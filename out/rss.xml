<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Dec 2025 15:45:25 +0000</lastBuildDate><item><title>Show HN: WhatHappened ‚Äì HN summaries, heatmaps, and contrarian picks</title><link>https://www.whathappened.tech/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46196796</guid><pubDate>Mon, 08 Dec 2025 19:55:08 +0000</pubDate></item><item><title>How Google Maps allocates survival across London's restaurants</title><link>https://laurenleek.substack.com/p/how-google-maps-quietly-allocates</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46203343</guid><pubDate>Tue, 09 Dec 2025 10:20:02 +0000</pubDate></item><item><title>Show HN: Local Privacy Firewall-blocks PII and secrets before ChatGPT sees them</title><link>https://github.com/privacyshield-ai/privacy-firewall</link><description>&lt;doc fingerprint="c2143cd0f4c98a72"&gt;
  &lt;main&gt;
    &lt;p&gt;üëã If you're trying PrivacyFirewall, please star the repo!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It helps others discover the project and motivates development. Takes 2 seconds ‚Üí ‚≠ê (top right)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;PrivacyFirewall is a local-first PII and secrets firewall for AI tools like ChatGPT, Claude, and Gemini.It blocks risky paste events, warns as you type, and (optionally) uses a lightweight on-device Transformer model for deeper PII detection.&lt;/p&gt;
    &lt;p&gt;üîí **No data ever leaves your machine.**Everything runs locally in your browser or through an optional local API.You can verify this by inspecting the network panel and reading the open-source code.&lt;/p&gt;
    &lt;p&gt;Modern AI tools make it extremely easy to leak sensitive information:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Emails &amp;amp; phone numbers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API keys &amp;amp; credentials&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customer or employee data&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IP &amp;amp; MAC address&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Internal logs &amp;amp; stack traces&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Regulated personal information (PII/PHI)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Traditional enterprise DLP tools don‚Äôt cover AI chat prompts.&lt;/p&gt;
    &lt;p&gt;PrivacyFirewall adds a zero-trust privacy shield BEFORE your text ever reaches a third-party AI system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;‚úã Human-in-the-loop protection for accidental leaks&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;üîí 100% local processing (browser + localhost only)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;‚ö° Practical protection (regex + optional transformer NER)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;üß© Friendly UX (warnings, paste-block modals, override options)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;üõ† OSS and auditable (MV3 + FastAPI + Hugging Face stack)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;**Lite Mode (regex-only)**Runs instantly in the extension ‚Äî no setup needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;**AI Mode (optional, local LLM)**Uses a local FastAPI agent + transformer model for deeper detection(People, organizations, locations, contextual entities).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;graph TD
    A[User Pastes/Types Text]:::blueNode --&amp;gt;|Intercept| B(Chrome Extension):::blueNode
    B --&amp;gt;|Regex Check| C{Contains Secrets/PII?}
    C --&amp;gt;|Yes &amp;amp; Paste| D[BLOCK &amp;amp; WARN]:::redNode
    C --&amp;gt;|Yes &amp;amp; Typing| E[SHOW WARNING BANNER]:::redNode
    C --&amp;gt;|No| F{Local Engine Online?}
    F --&amp;gt;|No| G[Allow]:::blueNode
    F --&amp;gt;|Yes| H[Python Local Engine]:::blueNode
    H --&amp;gt;|BERT Model| I{AI Detected PII?}
    I --&amp;gt;|Yes &amp;amp; Paste| D
    I --&amp;gt;|Yes &amp;amp; Typing| E
    I --&amp;gt;|No| G

    classDef blueNode fill:#2563eb,stroke:#1e40af,color:#fff
    classDef redNode fill:#dc2626,stroke:#b91c1c,color:#fff
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Regex Mode covers secrets quickly&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AI Mode enhances detection when the local engine is running&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If the agent goes offline ‚Üí extension falls back automatically&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Python 3.10+&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Chrome/Chromium/Edge&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Git&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ git clone https://github.com/privacyshield-ai/privacy-firewall.git

$ cd privacy-firewall

&lt;/code&gt;
    &lt;code&gt;$ cd src/engine  python -m venv .venv  

$ source .venv/bin/activate       # Windows: .venv\Scripts\activate

$ pip install --upgrade 

$ pip install -r requirements.txt

$ uvicorn main:app --host 127.0.0.1 --port 8765   

&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;First run downloads dslim/bert-base-NER (~400MB) to ~/.cache/huggingface.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;http://127.0.0.1:8765/health ‚Üí {"status":"ok"}&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Visit: chrome://extensions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable Developer mode&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Click Load unpacked&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select: src/extension/&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You now have Lite Mode running with regex-based detection.&lt;/p&gt;
    &lt;p&gt;Go to:&lt;/p&gt;
    &lt;p&gt;Paste:&lt;/p&gt;
    &lt;code&gt;My email is john.doe@example.com   `

&lt;/code&gt;
    &lt;p&gt;‚Üí Paste is intercepted, modal appears.&lt;/p&gt;
    &lt;p&gt;Paste:&lt;/p&gt;
    &lt;code&gt;AKIAIOSFODNN7EXAMPLE

&lt;/code&gt;
    &lt;p&gt;‚Üí Detected as AWS key ‚Üí blocked.&lt;/p&gt;
    &lt;p&gt;Enable AI Mode (when popup UI is ready), type:&lt;/p&gt;
    &lt;code&gt; Meeting notes from Sarah Thompson at HR...   

&lt;/code&gt;
    &lt;p&gt;‚Üí Local transformer flags PERSON ‚Üí warns you.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Email address&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Phone number&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Credit card candidate&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MAC address&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IPv4 address&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AWS access keys&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JWT tokens&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Private key blocks&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Generic API key / hash patterns&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;US SSN (basic pattern)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Powered by dslim/bert-base-NER:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;PERSON&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ORGANIZATION&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LOCATION&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Additional named entities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Helpful for ambiguous or context-based leakage&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;src/extension/        Chrome MV3 extension (content script, background worker, UI assets)
src/engine/           FastAPI service + transformer model wrapper
src/engine/models/    Model utilities (Hugging Face pipeline)
src/engine/tests/     Basic test harness for detection
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;HuggingFace models live in ~/.cache/huggingface/&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Delete this directory to force a fresh download&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Extension settings UI (enable/disable regex/AI modes)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add per-site allow/deny lists&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add secret-type redaction instead of full block&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Package engine as a binary or desktop app&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explore transformer.js for in-browser inference.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automated CI + browser testing&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Ensure the Python engine is running&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Confirm nothing else uses port 8765&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lite mode will still block regex-based secrets&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Ensure AI Mode is enabled + engine is online&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;NER models are probabilistic; long names work best&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Confidence threshold is tunable in transformer_detector.py&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PRs and issues are welcome!Please include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;OS &amp;amp; browser version&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reproduction steps&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Model version (if reporting AI false positives/negatives)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;No prompts or text ever leave your machine&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Extension communicates only with:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Browser local context&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Optional localhost API at 127.0.0.1:8765&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No analytics, telemetry, or external logging&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Review src/extension/content-script.js and DevTools ‚Üí Network tabto verify behavior&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License.See LICENSE for full text.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46206591</guid><pubDate>Tue, 09 Dec 2025 16:10:37 +0000</pubDate></item><item><title>Australia begins enforcing world-first teen social media ban</title><link>https://www.reuters.com/legal/litigation/australia-social-media-ban-takes-effect-world-first-2025-12-09/</link><description>&lt;doc fingerprint="2d87985519e0dfd9"&gt;
  &lt;main&gt;
    &lt;p&gt;SYDNEY, Dec 10 (Reuters) - Australia on Wednesday became the first country to ban social media for children under 16, blocking access in a move welcomed by many parents and child advocates but criticised by major technology companies and free-speech advocates.&lt;/p&gt;
    &lt;p&gt;Starting at midnight (1300 GMT on Tuesday), 10 of the largest platforms including TikTok, Alphabet's (GOOGL.O) YouTube and Meta's (META.O) Instagram and Facebook were ordered to block children or face fines of up to A$49.5 million ($33 million) under the new law, which is being closely watched by regulators worldwide.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;Prime Minister Anthony Albanese called it "a proud day" for families and cast the law as proof that policymakers can curb online harms that have outpaced traditional safeguards.&lt;/p&gt;
    &lt;p&gt;"This will make an enormous difference. It is one of the biggest social and cultural changes that our nation has faced," Albanese told a news conference on Wednesday.&lt;/p&gt;
    &lt;p&gt;"It's a profound reform which will continue to reverberate around the world."&lt;/p&gt;
    &lt;head rend="h2"&gt;READ A BOOK INSTEAD, PM TELLS YOUNGSTERS&lt;/head&gt;
    &lt;p&gt;In a video message, Albanese urged children to "start a new sport, new instrument, or read that book that has been sitting there for some time on your shelf," ahead of Australia's summer school break starting later this month.&lt;/p&gt;
    &lt;p&gt;Some of those below the cut-off age of 16 were anxious about adjusting to life without social media, but others were less concerned.&lt;/p&gt;
    &lt;p&gt;"I'm not really that emotional about it," said 14-year-old Claire Ni. "I'm kind of just, like, neutral."&lt;/p&gt;
    &lt;p&gt;Luna Dizon, 15, said she still had access to her TikTok, Instagram and Snapchat accounts, but worried about "culture shock" once the ban took full effect.&lt;/p&gt;
    &lt;p&gt;"I think eventually, without (social media), we'll learn how to adapt to it," she added.&lt;/p&gt;
    &lt;head rend="h2"&gt;TEENAGER SIGNS OFF WITH 'SEE YOU WHEN I'M 16'&lt;/head&gt;
    &lt;p&gt;While the government has said the ban would not be perfect in its operation, about 200,000 accounts were deactivated by Wednesday on TikTok alone, with "hundreds of thousands" more to be blocked in the next few days.&lt;/p&gt;
    &lt;p&gt;Many of the estimated 1 million children affected by the legislation also posted goodbye messages on social media.&lt;/p&gt;
    &lt;p&gt;"No more social media ... no more contact with the rest of the world," one teen wrote on TikTok.&lt;/p&gt;
    &lt;p&gt;"#seeyouwhenim16," said another.&lt;/p&gt;
    &lt;p&gt;Others said they would learn how to get round the ban.&lt;/p&gt;
    &lt;p&gt;"It's just kind of pointless, we're just going to create new ways to get on these platforms, so what's the point," said 14-year-old Claire Ni.&lt;/p&gt;
    &lt;head rend="h2"&gt;BAN HAS GLOBAL IMPLICATIONS&lt;/head&gt;
    &lt;p&gt;The rollout caps a year of debate over whether any country could practically stop children from using platforms embedded in daily life, and begins a live test for governments frustrated that social media firms have been slow to implement harm-reduction measures.&lt;/p&gt;
    &lt;p&gt;"I'm happy that they want to protect kids, and I'm happy that we have a chance to see how they do it and see if we can learn from them," said European Union lawmaker Christel Schaldemose, who wants to see greater protection for the bloc's children.&lt;/p&gt;
    &lt;p&gt;Albanese's centre-left government proposed the landmark law citing research showing harms to mental health from the overuse of social media among young teens, including misinformation, bullying and harmful depictions of body image.&lt;/p&gt;
    &lt;p&gt;Several countries from Denmark to New Zealand to Malaysia have signalled they may study or emulate Australia's model.&lt;/p&gt;
    &lt;p&gt;At a school in the German city of Bonn, students spoke favourably of a ban.&lt;/p&gt;
    &lt;p&gt;"Social media is highly addictive and doesn't really have any real advantages. I mean, there are advantages, such as being able to spread your opinion, but I think the disadvantages, especially the addiction, are much worse," said 15-year-old pupil Arian Klaar.&lt;/p&gt;
    &lt;p&gt;Julie Inman Grant, the U.S.-born eSafety Commissioner who is overseeing the ban, told Reuters on Wednesday a groundswell of American parents wanted similar measures.&lt;/p&gt;
    &lt;p&gt;"I hear from the parents and the activists and everyday people in America, 'we wish we had an eSafety commissioner like you in America, we wish we had a government that was going to put tween and teen safety before technology profits,'" she said in an interview at her office in Sydney.&lt;/p&gt;
    &lt;p&gt;'NOT OUR CHOICE': X SAYS WILL COMPLY&lt;/p&gt;
    &lt;p&gt;Elon Musk's X became the last of the 10 major platforms to take measures to cut off access to underage teens after publicly acknowledging on Wednesday that it would comply.&lt;/p&gt;
    &lt;p&gt;"It's not our choice - it's what the Australian law requires," X said on its website.&lt;/p&gt;
    &lt;p&gt;Australia has said the initial list of covered platforms would change as new products emerge and young users migrate.&lt;/p&gt;
    &lt;p&gt;Companies have told Canberra they will deploy a mix of age inference - estimating a user's age from their behaviour - and age estimation based on a selfie, alongside checks that could include uploaded identification documents.&lt;/p&gt;
    &lt;p&gt;For social media businesses, the implementation marks a new era of structural stagnation as user numbers flatline and time spent on platforms shrinks, studies show.&lt;/p&gt;
    &lt;p&gt;Platforms say they earn little from advertising to under-16s, but warn the ban disrupts a pipeline of future users. Just before the ban took effect, 86% of Australians aged eight to 15 used social media, the government said.&lt;/p&gt;
    &lt;p&gt;($1 = 1.5097 Australian dollars)&lt;/p&gt;
    &lt;p&gt;Reporting by Byron Kaye and Renju Jose; Additional reporting by James Redmayne and Cordelia Hsu; Writing by Alasdair Pal, Alexandra Hudson and Christine Chen; Editing by Andrew Heavens, Mark Potter, Lincoln Feast and Deepa Babington&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46208348</guid><pubDate>Tue, 09 Dec 2025 18:12:29 +0000</pubDate></item><item><title>Rubio stages font coup: Times New Roman ousts Calibri</title><link>https://www.reuters.com/world/us/rubio-stages-font-coup-times-new-roman-ousts-calibri-2025-12-09/</link><description>&lt;doc fingerprint="f1be8f403c685bdb"&gt;
  &lt;main&gt;
    &lt;p&gt;WASHINGTON, Dec 9 (Reuters) - U.S. Secretary of State Marco Rubio on Tuesday ordered diplomats to return to using Times New Roman font in official communications, calling his predecessor Antony Blinken's decision to adopt Calibri a "wasteful" diversity move, according to an internal department cable seen by Reuters.&lt;/p&gt;
    &lt;p&gt;The department under Blinken in early January 2023 had switched to Calibri, a modern sans-serif font, saying this was a more accessible font for people with disabilities because it did not have the decorative angular features and was the default in Microsoft products.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;A cable dated December 9 sent to all U.S. diplomatic posts said that typography shapes the professionalism of an official document and Calibri is informal compared to serif typefaces.&lt;/p&gt;
    &lt;p&gt;"To restore decorum and professionalism to the Department‚Äôs written work products and abolish yet another wasteful DEIA program, the Department is returning to Times New Roman as its standard typeface," the cable said.&lt;/p&gt;
    &lt;p&gt;"This formatting standard aligns with the President‚Äôs One Voice for America‚Äôs Foreign Relations directive, underscoring the Department‚Äôs responsibility to present a unified, professional voice in all communications," it added.&lt;/p&gt;
    &lt;p&gt;The State Department did not immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Some studies suggest that sans-serif fonts, such as Calibri, are easier to read for those with certain visual disabilities.&lt;/p&gt;
    &lt;p&gt;Trump, a Republican, moved quickly after taking office in January to eradicate federal DEI programs and discourage them in the private sector and education, including by directing the firing of diversity officers at federal agencies and pulling grant funding for a wide range of programs.&lt;/p&gt;
    &lt;p&gt;DEI policies became more widespread after nationwide protests in 2020 against police killings of unarmed Black people, spurring a conservative backlash. Trump and other critics of diversity initiatives say they are discriminatory against white people and men and have eroded merit-based decision making.&lt;/p&gt;
    &lt;p&gt;Reporting by Humeyra Pamuk; Editing by Don Durfee and Lisa Shumaker&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46212438</guid><pubDate>Wed, 10 Dec 2025 00:08:34 +0000</pubDate></item><item><title>Show HN: Wirebrowser ‚Äì A JavaScript debugger with breakpoint-driven heap search</title><link>https://github.com/fcavallarin/wirebrowser</link><description>&lt;doc fingerprint="ac2b8e0449babf04"&gt;
  &lt;main&gt;
    &lt;p&gt;Wirebrowser is a debugging, interception, and memory-inspection toolkit powered by the Chrome DevTools Protocol (CDP). It unifies network manipulation, API testing, automation scripting, and deep JavaScript memory inspection into one interface.&lt;lb/&gt; With features like Breakpoint-Driven Heap Search and real-time Live Object Search, Wirebrowser provides researchers and engineers with precise, high-visibility tools for client-side analysis, reverse engineering, and complex application debugging.&lt;/p&gt;
    &lt;p&gt;Intercept, block, rewrite, and replay HTTP requests and responses in real time.&lt;/p&gt;
    &lt;p&gt;Inspect, search, and modify JavaScript memory using both live heap analysis and heap snapshots, with full support for object identity search, primitive search (via snapshots), structural matching, and runtime patching.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Live Object Search ‚Äî Search all live JavaScript objects using regex or structural matching, and patch matched objects at runtime to alter state or behavior dynamically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Static Heap Snapshot Search Capture a full V8 heap snapshot and search all objects and primitives, including strings and closure-captured values that are unreachable through the Runtime domain.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Origin Trace (BDHS) ‚Äî Performs automatic debugger pauses and captures a full heap snapshot at each stop. Every snapshot is searched to identify the user-land function responsible for creating or mutating the target value. Framework and vendor scripts are filtered out via heuristics.&lt;/p&gt;&lt;lb/&gt;BDHS also includes a tolerance window that samples snapshots before and after the first match, providing contextual insight into when and how a value is introduced or mutated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A shared similarity engine used across Live Object Search, Heap Snapshots, and BDHS timelines. Enables shape-based searches, clustering, and origin tracing for objects that evolve over time.&lt;/p&gt;
    &lt;p&gt;Create, edit, and execute API requests with variable substitution and structured collections, integrating Postman-style workflows directly into the debugging environment.&lt;/p&gt;
    &lt;p&gt;A full technical deep-dive is available here: üëâ https://fcavallarin.github.io/wirebrowser/BDHS-Origin-Trace&lt;/p&gt;
    &lt;p&gt;Below is a quick visual tour of Wirebrowser‚Äôs most distinctive capabilities.&lt;/p&gt;
    &lt;p&gt;A short walkthrough of Wirebrowser‚Äôs advanced memory-analysis capabilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Live Object Search ‚Äî real-time search and runtime patching of live JS objects.&lt;/item&gt;
      &lt;item&gt;Origin Trace (BDHS) ‚Äî identify the user-land function responsible for creating or mutating the object during debugging.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Intercept, rewrite, block, and replay HTTP requests and responses.&lt;/p&gt;
    &lt;p&gt;Search and patch live JS objects using regex or structural matching.&lt;/p&gt;
    &lt;p&gt;Capture snapshots on each debugger pause to locate the user-land function responsible for object creation or mutation.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/fcavallarin/wirebrowser.git
cd wirebrowser
npm install
npm run build&lt;/code&gt;
    &lt;code&gt;npm run wirebrowser&lt;/code&gt;
    &lt;p&gt;On some Linux distributions, Electron may fail to start due to process sandboxing restrictions, showing errors such as:&lt;/p&gt;
    &lt;code&gt;The SUID sandbox helper binary was found, but is not configured correctly.
&lt;/code&gt;
    &lt;p&gt;This is a known issue in Electron ([electron/electron#42510]).&lt;lb/&gt; The most common solution is to disable AppArmor restrictions:&lt;/p&gt;
    &lt;code&gt;sudo sysctl -w kernel.apparmor_restrict_unprivileged_userns=0
&lt;/code&gt;
    &lt;p&gt;Beyond the core Network and Memory workflows, Wirebrowser offers several supporting modules that enhance debugging, testing, and automation workflows.&lt;/p&gt;
    &lt;p&gt;Create, edit, and execute API requests with variable substitution and organized collections.&lt;lb/&gt; Useful for testing endpoints, iterating on backend logic, or interacting with APIs directly from the same environment used for debugging the client.&lt;/p&gt;
    &lt;p&gt;Run browser-side or Node.js scripts, either manually or triggered by events such as page load.&lt;lb/&gt; Automation scripts have access to an &lt;code&gt;Utils&lt;/code&gt; object that exposes helpers for interacting with the browser, pages, variables, iterators, and HTTP utilities.&lt;/p&gt;
    &lt;code&gt;const userId = Utils.getVar("userId");
const page = Utils.getPage(1);
page.on("request", req =&amp;gt; req.continue());
await page.goto(`https://example.com/${userId}`);&lt;/code&gt;
    &lt;p&gt;A collection of small tools frequently needed during debugging and analysis, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encode or decode strings in multiple formats:&lt;/item&gt;
      &lt;item&gt;Create, verify, and decode JSON Web Tokens (JWTs).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most Wirebrowser actions can be performed either globally (across all open tabs/pages) or targeted to a single tab. This lets you choose whether a rule or inspection should affect the whole browser session or only a specific page.&lt;lb/&gt; Every tab/page opened by Wirebrowser has a unique integer &lt;code&gt;tabId&lt;/code&gt;. Use this &lt;code&gt;tabId&lt;/code&gt; to scope actions.&lt;/p&gt;
    &lt;p&gt;UI Notes&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many panels offer a scope selector (Global / Specific Tab ID) for quick changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wirebrowser is built with React and Node.js, using plain JavaScript to keep the codebase lightweight and hackable.&lt;lb/&gt; TypeScript or JSDoc-based typing may be introduced in the future for enhanced maintainability.&lt;/p&gt;
    &lt;p&gt;The following areas are being explored for future development:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SPA crawling ‚Äî automated crawling of single-page applications to map navigation flows and surface client-side behaviors.&lt;/item&gt;
      &lt;item&gt;DOM XSS scanning ‚Äî analysis of potential DOM-based XSS injection points during crawls or on-demand checks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wirebrowser is being built in the open ‚Äî contributions and feedback are welcome!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üí¨ Chat coming soon (Discord or Matrix)&lt;/item&gt;
      &lt;item&gt;üê¶ Follow updates on X/Twitter: https://x.com/wirebrowser&lt;/item&gt;
      &lt;item&gt;üß† Issues &amp;amp; Ideas: https://github.com/fcavallarin/wirebrowser/issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions and pull requests are welcome!&lt;lb/&gt; Open an issue or pull request ‚Äî even small suggestions help improve Wirebrowser.&lt;/p&gt;
    &lt;p&gt;Wirebrowser‚Ñ¢ is distributed under the MIT License.&lt;lb/&gt; See the LICENSE file for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46218101</guid><pubDate>Wed, 10 Dec 2025 14:30:43 +0000</pubDate></item><item><title>Size of Life</title><link>https://neal.fun/size-of-life/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219346</guid><pubDate>Wed, 10 Dec 2025 16:02:57 +0000</pubDate></item><item><title>Qwen3-Omni-Flash-2025-12-01Ôºöa next-generation native multimodal large model</title><link>https://qwen.ai/blog?id=qwen3-omni-flash-20251201</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219538</guid><pubDate>Wed, 10 Dec 2025 16:13:38 +0000</pubDate></item><item><title>Auto-grading decade-old Hacker News discussions with hindsight</title><link>https://karpathy.bearblog.dev/auto-grade-hn/</link><description>&lt;doc fingerprint="a207dbd71fe07fd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Auto-grading decade-old Hacker News discussions with hindsight&lt;/head&gt;
    &lt;p&gt;TLDR: https://karpathy.ai/hncapsule/&lt;/p&gt;
    &lt;p&gt;Yesterday I stumbled on this HN thread Show HN: Gemini Pro 3 hallucinates the HN front page 10 years from now, where Gemini 3 was hallucinating the frontpage of 10 years from now. One of the comments struck me a bit more though - Bjartr linked to the HN frontpage from exactly 10 years ago, i.e. December 2015. I was reading through the discussions of 10 years ago and mentally grading them for prescience when I realized that an LLM might actually be a lot better at this task. I copy pasted one of the article+comment threads manually into ChatGPT 5.1 Thinking and it gave me a beautiful analysis of what people thought + what actually happened in retrospect, even better and significantly more detailed than what I was doing manually. I realized that this task is actually a really good fit for LLMs and I was looking for excuses to vibe code something with the newly released Opus 4.5, so I got to work. I'm going to get all the front pages of December (31 days, 30 articles per day), get ChatGPT 5.1 Thinking to do the analysis, and present everything in a nice way for historical reading.&lt;/p&gt;
    &lt;p&gt;There are two macro reasons for why I think the exercise is interesting more generally:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I believe it is quite possible and desirable to train your forward future predictor given training and effort.&lt;/item&gt;
      &lt;item&gt;I was reminded again of my tweets that said "Be good, future LLMs are watching". You can take that in many directions, but here I want to focus on the idea that future LLMs are watching. Everything we do today might be scrutinized in great detail in the future because doing so will be "free". A lot of the ways people behave currently I think make an implicit "security by obscurity" assumption. But if intelligence really does become too cheap to meter, it will become possible to do a perfect reconstruction and synthesis of everything. LLMs are watching (or humans using them might be). Best to be good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vibe coding the actual project was relatively painless and took about 3 hours with Opus 4.5, with a few hickups but overall very impressive. The repository is on GitHub here: karpathy/hn-time-capsule. Here is the progression of what the code does:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Given a date, download the frontpage of 30 articles&lt;/item&gt;
      &lt;item&gt;For each article, download/parse the article itself and the full comment thread using Algolia API.&lt;/item&gt;
      &lt;item&gt;Package up everything into a markdown prompt asking for the analysis. Here is the prompt prefix I used:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;The following is an article that appeared on Hacker News 10 years ago, and the discussion thread.

Let's use our benefit of hindsight now in 6 sections:

1. Give a brief summary of the article and the discussion thread.
2. What ended up happening to this topic? (research the topic briefly and write a summary)
3. Give out awards for "Most prescient" and "Most wrong" comments, considering what happened.
4. Mention any other fun or notable aspects of the article or discussion.
5. Give out grades to specific people for their comments, considering what happened.
6. At the end, give a final score (from 0-10) for how interesting this article and its retrospect analysis was.

As for the format of Section 5, use the header "Final grades" and follow it with simply an unordered list of people and their grades in the format of "name: grade (optional comment)". Here is an example:

Final grades
- speckx: A+ (excellent predictions on ...)
- tosh: A (correctly predicted this or that ...)
- keepamovin: A
- bgwalter: D
- fsflover: F (completely wrong on ...)

Your list may contain more people of course than just this toy example. Please follow the format exactly because I will be parsing it programmatically. The idea is that I will accumulate the grades for each account to identify the accounts that were over long periods of time the most prescient or the most wrong.

As for the format of Section 6, use the prefix "Article hindsight analysis interestingness score:" and then the score (0-10) as a number. Give high scores to articles/discussions that are prominent, notable, or interesting in retrospect. Give low scores in cases where few predictions are made, or the topic is very niche or obscure, or the discussion is not very interesting in retrospect.

Here is an example:
Article hindsight analysis interestingness score: 8
---
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Submit prompt to GPT 5.1 Thinking via the OpenAI API&lt;/item&gt;
      &lt;item&gt;Collect and parse the results&lt;/item&gt;
      &lt;item&gt;Render the results into static HTML web pages for easy viewing&lt;/item&gt;
      &lt;item&gt;Host the html result pages on my website: https://karpathy.ai/hncapsule/&lt;/item&gt;
      &lt;item&gt;Host all the intermediate results of the &lt;code&gt;data&lt;/code&gt;directory if someone else would like to play. It's the file&lt;code&gt;data.zip&lt;/code&gt;under the exact same url prefix (intentionally avoiding a direct link).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I spent a few hours browsing around and found it to be very interesting. A few example threads just for fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 3 2015 Swift went open source.&lt;/item&gt;
      &lt;item&gt;December 6 2015 Launch of Figma&lt;/item&gt;
      &lt;item&gt;December 11 2015 original announcement of OpenAI :').&lt;/item&gt;
      &lt;item&gt;December 16 2015 geohot is building Comma&lt;/item&gt;
      &lt;item&gt;December 22 2015 SpaceX launch webcast: Orbcomm-2 Mission&lt;/item&gt;
      &lt;item&gt;December 28 2015 Theranos struggles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then when you navigate over to the Hall of Fame, you can find the top commenters of Hacker News in December 2015, sorted by imdb-style score of their grade point average. In particular, congratulations to pcwalton, tptacek, paulmd, cstross, greglindahl, moxie, hannob, 0xcde4c3db, Manishearth, johncolanduoni - GPT 5.1 Thinking found your comments very insightful and prescient. You can also scroll all the way down to find the noise of HN, which I think we're all familiar with too :)&lt;/p&gt;
    &lt;p&gt;My code (wait, Opus' code?) on GitHub can be used to reproduce or tweak the results. Running 31 days of 30 articles through GPT 5.1 Thinking meant &lt;code&gt;31 * 30 =&lt;/code&gt; 930 LLM queries and cost about $58 and somewhere around ~1 hour. The LLM megaminds of the future might find this kind of a thing a lot easier, a lot faster and a lot cheaper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220540</guid><pubDate>Wed, 10 Dec 2025 17:23:53 +0000</pubDate></item><item><title>Show HN: Automated license plate reader coverage in the USA</title><link>https://alpranalysis.com</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220794</guid><pubDate>Wed, 10 Dec 2025 17:42:30 +0000</pubDate></item><item><title>Super Mario 64 for the PS1</title><link>https://github.com/malucard/sm64-psx</link><description>&lt;doc fingerprint="57cb9c197ca5b5c8"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This is a fork of the full decompilation of Super Mario 64 (J), (U), (E), and (SH).&lt;/item&gt;
      &lt;item&gt;It is heavily modified and can no longer target Nintendo 64, only PSX and PC (for debugging).&lt;/item&gt;
      &lt;item&gt;There are still many limitations.&lt;/item&gt;
      &lt;item&gt;For now, it can only build from the US version.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This repo does not include all assets necessary for compiling the game. An original copy of the game is required to extract the assets.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cool "DUAL SHOCK‚Ñ¢ Compatible" graphic mimicking the original "ÊåØÂãï„Éë„ÉÉ„ÇØÂØæÂøú" (Rumble Pak Compatible) graphic&lt;/item&gt;
      &lt;item&gt;An analog rumble signal is now produced for the DualShock's large motor, in addition to the original modulated digital signal for the small motor and for the SCPH-1150 Dual Analog Controller&lt;/item&gt;
      &lt;item&gt;Low-precision soft float implementation specially written for PSX to reduce the performance impact of floats&lt;/item&gt;
      &lt;item&gt;Large amounts of code have been adapted to use fixed point math, including the 16-bit integer vectors and matrices that are standard on PSX&lt;/item&gt;
      &lt;item&gt;Simplified rewritten render graph walker&lt;/item&gt;
      &lt;item&gt;Tessellation (up to 2x) to reduce issues with large polygons&lt;/item&gt;
      &lt;item&gt;RSP display lists are compiled just-in-time into a custom display list format that is more compact and faster to process&lt;/item&gt;
      &lt;item&gt;Display list preprocessor that removes commands we won't use and optimizes meshes (TODO: make it fix more things)&lt;/item&gt;
      &lt;item&gt;Mario's animations are compressed (from 580632 to 190324 bytes) and placed in a corner of VRAM rather than being loaded from storage (we don't have the luxury of a fast cartridge to read from in the middle of a frame)&lt;/item&gt;
      &lt;item&gt;Custom profiler&lt;/item&gt;
      &lt;item&gt;Custom texture encoder that quantizes all textures to 4 bits per pixel&lt;/item&gt;
      &lt;item&gt;Translucent circle-texture shadows replaced with subtractive hexagonal shadows, as the PSX doesn't support arbitrary translucency&lt;/item&gt;
      &lt;item&gt;(TODO) Camera system adapted to rotate with the right analog stick&lt;/item&gt;
      &lt;item&gt;(TODO) Simplified rewritten Goddard subsystem&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Floating trees (temporary issue due caused by a math rewrite)&lt;/item&gt;
      &lt;item&gt;Some of Mario's animations do not play, and may even crash the game&lt;/item&gt;
      &lt;item&gt;Music cannot be generated at build time without manually obtaining the tracks&lt;/item&gt;
      &lt;item&gt;Sound effects work but sometimes sound odd or are missing notes&lt;/item&gt;
      &lt;item&gt;The camera cannot be controlled in many levels due to the unfinished camera control implementation&lt;/item&gt;
      &lt;item&gt;Crashes when entering certain levels (due to insufficient memory?)&lt;/item&gt;
      &lt;item&gt;Ending sequence crashes on load&lt;/item&gt;
      &lt;item&gt;When reaching the bridge in the castle grounds, Mario looks up but Lakitu never comes over&lt;/item&gt;
      &lt;item&gt;Poles do not go down when pounded&lt;/item&gt;
      &lt;item&gt;Textures are loaded individually, causing long stutters and loading times&lt;/item&gt;
      &lt;item&gt;Stretched textures due to PSX limitations (the graphics preprocessor could help)&lt;/item&gt;
      &lt;item&gt;Tessellation is not good enough to fix all large polygons (the graphics preprocessor could help)&lt;/item&gt;
      &lt;item&gt;Some textures are rendered incorrectly (RSP JIT issues?)&lt;/item&gt;
      &lt;item&gt;Title screen is unfinished&lt;/item&gt;
      &lt;item&gt;Pause menu doesn't work&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build and install the mipsel-none-elf-gcc toolchain. For Arch users, it is available on AUR. (You can also install it on your system from https://github.com/malucard/poeng by running &lt;code&gt;make install-gcc&lt;/code&gt;from there. This may take a long time.)&lt;/item&gt;
      &lt;item&gt;Clone the repo: &lt;code&gt;git clone https://github.com/malucard/sm64-psx&lt;/code&gt;, which will create a directory&lt;code&gt;sm64-port&lt;/code&gt;and then enter it&lt;code&gt;cd sm64-port&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Place a Super Mario 64 ROM called &lt;code&gt;baserom.&amp;lt;VERSION&amp;gt;.z64&lt;/code&gt;into the repository's root directory for asset extraction,&lt;del rend="overstrike"&gt;where&lt;/del&gt;. (For now, only&lt;code&gt;VERSION&lt;/code&gt;can be&lt;code&gt;us&lt;/code&gt;,&lt;code&gt;jp&lt;/code&gt;, or&lt;code&gt;eu&lt;/code&gt;&lt;code&gt;us&lt;/code&gt;is supported.)&lt;/item&gt;
      &lt;item&gt;(Optional) Create a folder named &lt;code&gt;.local&lt;/code&gt;in the root of the repo and place every track of the soundtrack in it as a .wav file, numbered from 0 to 37 (0.wav, 1.wav, etc).&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;make&lt;/code&gt;to build. To build the benchmark version without music, run&lt;code&gt;make BENCH=1&lt;/code&gt;. The disc image will be located at&lt;code&gt;build/&amp;lt;VERSION&amp;gt;_psx/sm64.&amp;lt;VERSION&amp;gt;.iso&lt;/code&gt;. The benchmark version will not generate an iso, only an elf and an exe, and it will require a PSX with 8MB of RAM (an emulator or a debug unit).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install and update MSYS2, following all the directions listed on https://www.msys2.org/.&lt;/item&gt;
      &lt;item&gt;From the start menu, launch MSYS2 MinGW and install required packages depending on your machine (do NOT launch "MSYS2 MSYS"):&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;64-bit: Launch "MSYS2 MinGW 64-bit" and install: &lt;code&gt;pacman -S git make python3 mingw-w64-x86_64-gcc mingw-w64-x86_64-meson mingw-w64-x86_64-ffmpeg unzip&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;32-bit (will also work on 64-bit machines): Launch "MSYS2 MinGW 32-bit" and install: &lt;code&gt;pacman -S git make python3 mingw-w64-i686-gcc mingw-w64-i686-meson mingw-w64-i686-ffmpeg unzip&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Do NOT by mistake install the packages called simply &lt;code&gt;gcc&lt;/code&gt;and&lt;code&gt;meson&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install the mipsel-none-elf-gcc toolchain.&lt;/item&gt;
      &lt;item&gt;The MSYS2 terminal has a current working directory that initially is &lt;code&gt;C:\msys64\home\&amp;lt;username&amp;gt;&lt;/code&gt;(home directory). At the prompt, you will see the current working directory in yellow.&lt;code&gt;~&lt;/code&gt;is an alias for the home directory. You can change the current working directory to&lt;code&gt;My Documents&lt;/code&gt;by entering&lt;code&gt;cd /c/Users/&amp;lt;username&amp;gt;/Documents&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Clone the repo: &lt;code&gt;git clone https://github.com/malucard/sm64-psx&lt;/code&gt;, which will create a directory&lt;code&gt;sm64-psx&lt;/code&gt;and then enter it&lt;code&gt;cd sm64-psx&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Place a Super Mario 64 ROM called &lt;code&gt;baserom.&amp;lt;VERSION&amp;gt;.z64&lt;/code&gt;into the repository's root directory for asset extraction,&lt;del rend="overstrike"&gt;where&lt;/del&gt;. (For now, only&lt;code&gt;VERSION&lt;/code&gt;can be&lt;code&gt;us&lt;/code&gt;,&lt;code&gt;jp&lt;/code&gt;, or&lt;code&gt;eu&lt;/code&gt;&lt;code&gt;us&lt;/code&gt;is supported.)&lt;/item&gt;
      &lt;item&gt;(Optional) Create a folder named &lt;code&gt;.local&lt;/code&gt;in the root of the repo and place every track of the soundtrack in it as a .wav file, numbered from 0 to 37 (0.wav, 1.wav, etc).&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;make&lt;/code&gt;to build. To build the benchmark version, run&lt;code&gt;make BENCH=1&lt;/code&gt;. The disc image will be located at&lt;code&gt;build/&amp;lt;VERSION&amp;gt;_psx/sm64.&amp;lt;VERSION&amp;gt;.iso&lt;/code&gt;. The benchmark version will not generate an iso, only an elf and an exe, and it will require a PSX with 8MB of RAM (an emulator or a debug unit).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If you get &lt;code&gt;make: gcc: no suitable C and C++ compiler found&lt;/code&gt;,&lt;code&gt;make: gcc: command not found&lt;/code&gt;,&lt;code&gt;make: gcc: No such file or directory&lt;/code&gt;although the packages did successfully install, you probably launched the wrong MSYS2. Read the instructions again. The terminal prompt should contain "MINGW32" or "MINGW64" in purple text, and NOT "MSYS".&lt;/item&gt;
      &lt;item&gt;If you get &lt;code&gt;Failed to open baserom.us.z64!&lt;/code&gt;you failed to place the baserom in the repository. You can write&lt;code&gt;ls&lt;/code&gt;to list the files in the current working directory. If you are in the&lt;code&gt;sm64-psx&lt;/code&gt;directory, make sure you see it here.&lt;/item&gt;
      &lt;item&gt;If you get &lt;code&gt;make: *** No targets specified and no makefile found. Stop.&lt;/code&gt;, you are not in the correct directory. Make sure the yellow text in the terminal ends with&lt;code&gt;sm64-psx&lt;/code&gt;. Use&lt;code&gt;cd &amp;lt;dir&amp;gt;&lt;/code&gt;to enter the correct directory. If you write&lt;code&gt;ls&lt;/code&gt;you should see all the project files, including&lt;code&gt;Makefile&lt;/code&gt;if everything is correct.&lt;/item&gt;
      &lt;item&gt;If you get any error, be sure MSYS2 packages are up to date by executing &lt;code&gt;pacman -Syu&lt;/code&gt;and&lt;code&gt;pacman -Su&lt;/code&gt;. If the MSYS2 window closes immediately after opening it, restart your computer.&lt;/item&gt;
      &lt;item&gt;Check if mipsel gcc is working by executing &lt;code&gt;mipsel-none-elf-gcc -v&lt;/code&gt;. If it doesn't work, you either opened the wrong MSYS start menu entry or installed the incorrect gcc package.&lt;/item&gt;
      &lt;item&gt;When switching between building on other platforms, run &lt;code&gt;make -C tools clean&lt;/code&gt;first to allow for the tools to recompile on the new platform. This also helps when switching between shells like WSL and MSYS2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sm64
‚îú‚îÄ‚îÄ actors: object behaviors, geo layout, and display lists
‚îú‚îÄ‚îÄ assets: animation and demo data
‚îÇ   ‚îú‚îÄ‚îÄ anims: animation data
‚îÇ   ‚îî‚îÄ‚îÄ demos: demo data
‚îú‚îÄ‚îÄ bin: C files for ordering display lists and textures
‚îú‚îÄ‚îÄ build: output directory
‚îú‚îÄ‚îÄ data: behavior scripts, misc. data
‚îú‚îÄ‚îÄ doxygen: documentation infrastructure
‚îú‚îÄ‚îÄ enhancements: example source modifications
‚îú‚îÄ‚îÄ include: header files
‚îú‚îÄ‚îÄ levels: level scripts, geo layout, and display lists
‚îú‚îÄ‚îÄ lib: N64 SDK code
‚îú‚îÄ‚îÄ sound: sequences, sound samples, and sound banks
‚îú‚îÄ‚îÄ src: C source code for game
‚îÇ   ‚îú‚îÄ‚îÄ audio: audio code
‚îÇ   ‚îú‚îÄ‚îÄ buffers: stacks, heaps, and task buffers
‚îÇ   ‚îú‚îÄ‚îÄ engine: script processing engines and utils
‚îÇ   ‚îú‚îÄ‚îÄ game: behaviors and rest of game source
‚îÇ   ‚îú‚îÄ‚îÄ goddard: rewritten Mario intro screen
‚îÇ   ‚îú‚îÄ‚îÄ goddard_og: backup of original Mario intro screen
‚îÇ   ‚îú‚îÄ‚îÄ menu: title screen and file, act, and debug level selection menus
‚îÇ   ‚îî‚îÄ‚îÄ port: port code, audio and video renderer
‚îú‚îÄ‚îÄ text: dialog, level names, act names
‚îú‚îÄ‚îÄ textures: skybox and generic texture data
‚îî‚îÄ‚îÄ tools: build tools
&lt;/code&gt;
    &lt;p&gt;Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46221925</guid><pubDate>Wed, 10 Dec 2025 18:58:55 +0000</pubDate></item><item><title>Getting a Gemini API key is an exercise in frustration</title><link>https://ankursethi.com/blog/gemini-api-key-frustration/</link><description>&lt;doc fingerprint="3956b1cd9b3799d1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Getting a Gemini API key is an exercise in frustration&lt;/head&gt;
    &lt;p&gt;Last week, I started working on a new side-project. It‚Äôs a standard React app partly made up of run-of-the-mill CRUD views‚Äîa perfect fit for LLM-assisted programming. I reasoned that if I could get an LLM to quickly write the boring code for me, I‚Äôd have more time to focus on the interesting problems I wanted to solve.&lt;/p&gt;
    &lt;p&gt;I‚Äôve pretty much settled on Claude Code as my coding assistant of choice, but I‚Äôd been hearing great things about Google‚Äôs Gemini 3 Pro. Despite my aversion to Google products, I decided to try it out on my new codebase.&lt;/p&gt;
    &lt;p&gt;I already had Gemini CLI installed, but that only gave me access to Gemini 2.5 with rate limits. I wanted to try out Gemini 3 Pro, and I wanted to avoid being rate limited. I had some spare cash to burn on this experiment, so I went looking for ways to pay for a Gemini Pro plan, if such a thing existed.&lt;/p&gt;
    &lt;p&gt;Thus began my grand adventure in trying to give Google my money.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a Gemini, really?&lt;/head&gt;
    &lt;p&gt;The name ‚ÄúGemini‚Äù is so overloaded that it barely means anything. Based on the context, Gemini could refer to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The chatbot available at gemini.google.com.&lt;/item&gt;
      &lt;item&gt;The mobile app that lets you use the same Gemini chatbot on your iPhone or Android.&lt;/item&gt;
      &lt;item&gt;The voice assistant on Android phones.&lt;/item&gt;
      &lt;item&gt;The AI features built into Google Workspace, Firebase, Colab, BigQuery, and other Google products.&lt;/item&gt;
      &lt;item&gt;Gemini CLI, an agentic coding tool for your terminal that works the same way as Claude Code or OpenAI Codex.&lt;/item&gt;
      &lt;item&gt;The Gemini Code Assist suite of products, which includes extensions for various IDEs, a GitHub app, and Gemini CLI.&lt;/item&gt;
      &lt;item&gt;The underlying LLM powering all these products.&lt;/item&gt;
      &lt;item&gt;Probably three more products by the time I finish writing this blog post.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make things even more confusing, Google has at least three different products just for agentic coding: Gemini Code Assist (Gemini CLI is a part of this suite of products), Jules, and Antigravity.&lt;/p&gt;
    &lt;p&gt;And then there‚Äôs a bunch of other GenAI stuff that is powered by Gemini but doesn‚Äôt have the word Gemini in the name: Vertex AI Platform, Google AI Studio, NotebookLM, and who knows what else.&lt;/p&gt;
    &lt;p&gt;I just wanted to plug my credit card information into a form and get access to a coding assistant. Instead, I was dunked into an alphabet soup of products that all seemed to do similar things and, crucially, didn‚Äôt have any giant ‚ÄúBuy Now!‚Äù buttons for me to click.&lt;/p&gt;
    &lt;p&gt;In contrast, both Anthropic and OpenAI have two primary ways you can access their products: via their consumer offerings at claude.ai and chatgpt.com respectively, or via API credits that you can buy through their respective developer consoles. In each case, there is a form field where you can plug in your credit card details, and a big, friendly ‚ÄúBuy Now!‚Äù button to click.&lt;/p&gt;
    &lt;p&gt;After half an hour of searching the web, I did the obvious thing and asked the free version of Gemini (the chatbot, not one of those other Geminis) what to do:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How do I pay for the pro version of Gemini so i can use it in the terminal for writing code? I specifically want to use the Gemini 3 Pro model.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It thought for a suspiciously long time and told me that Gemini 3 Pro required a developer API key to use. Since the new model is still in preview, it‚Äôs not yet available on any of the consumer plans. When I asked follow up questions about pricing, it told me that ‚ÄúSomething went wrong‚Äù. Which translates to: we broke something, but we won‚Äôt tell you how to fix it.&lt;/p&gt;
    &lt;p&gt;So I asked Claude for help. Between the two LLMs, I was able to figure out how to create an API key for the Gemini I wanted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating an API key is easy&lt;/head&gt;
    &lt;p&gt;Google AI Studio is supposed to be the all-in-one dashboard for Google‚Äôs generative AI models. This is where you can experiment with model parameters, manage API keys, view logs, and manage billing for your projects.&lt;/p&gt;
    &lt;p&gt;I logged into Google AI Studio and created a new API key. This part was pretty straightforward: I followed the on-screen instructions and had a fresh new key housed under a project in a few seconds. I then verified that my key was working with Gemini CLI.&lt;/p&gt;
    &lt;p&gt;It worked! Now all that was left to do was to purchase some API credits. Back in Google AI Studio, I saw a link titled ‚ÄúSet up billing‚Äù next to my key. It looked promising, so I clicked it.&lt;/p&gt;
    &lt;p&gt;That‚Äôs where the fun really began.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google doesn‚Äôt want my money&lt;/head&gt;
    &lt;p&gt;The ‚ÄúSet up billing‚Äù link kicked me out of Google AI Studio and into Google Cloud Console, and my heart sank. Every time I‚Äôve logged into Google Cloud Console or AWS, I‚Äôve wasted hours upon hours reading outdated documentation, gazing in despair at graphs that make no sense, going around in circles from dashboard to dashboard, and feeling a strong desire to attain freedom from this mortal coil.&lt;/p&gt;
    &lt;p&gt;Turns out I can‚Äôt just put $100 into my Gemini account. Instead, I must first create a Billing Account. After I‚Äôve done that, I must associate it with a project. Then I‚Äôm allowed to add a payment method to the Billing Account. And then, if I‚Äôm lucky, my API key will turn into a paid API key with Gemini Pro privileges.&lt;/p&gt;
    &lt;p&gt;So I did the thing. The whole song and dance. Including the mandatory two-factor OTP verification that every Indian credit card requires. At the end of the process, I was greeted with a popup telling me I had to verify my payment method before I‚Äôd be allowed to use it.&lt;/p&gt;
    &lt;p&gt;Wait. Didn‚Äôt I just verify my payment method? When I entered the OTP from my bank?&lt;/p&gt;
    &lt;p&gt;Nope, turns out Google hungers for more data. Who‚Äôd have thunk it?&lt;/p&gt;
    &lt;p&gt;To verify my payment method for reals, I had to send Google a picture of my government-issued ID and the credit card I‚Äôd just associated with my Billing Account. I had to ensure all the numbers on my credit card were redacted by manually placing black bars on top of them in an image editor, leaving only my name and the last four digits of the credit card number visible.&lt;/p&gt;
    &lt;p&gt;This felt unnecessarily intrusive. But by this point, I was too deep in the process to quit. I was invested. I needed my Gemini 3 Pro, and I was willing to pay any price.&lt;/p&gt;
    &lt;p&gt;The upload form for the government ID rejected my upload twice before it finally accepted it. It was the same exact ID every single time, just in different file formats. It wanted a PNG file. Not a JPG file, nor a PDF file, but a PNG file. Did the upload form mention that in the instructions? Of course not.&lt;/p&gt;
    &lt;p&gt;After jumping through all these hoops, I received an email from Google telling me that my verification will be completed in a few days.&lt;/p&gt;
    &lt;p&gt;A few days? Nothing to do but wait, I suppose.&lt;/p&gt;
    &lt;head rend="h2"&gt;403 Forbidden&lt;/head&gt;
    &lt;p&gt;At this point, I closed all my open Cloud Console tabs and went back to work. But when I was fifteen minutes into writing some code by hand like a Neanderthal, I received a second email from Google telling me that my verification was complete.&lt;/p&gt;
    &lt;p&gt;So for the tenth time that day, I navigated to AI Studio. For the tenth time I clicked ‚ÄúSet up billing‚Äù on the page listing my API keys. For the tenth time I was told that my project wasn‚Äôt associated with a billing account. For the tenth time I associated the project with my new billing account. And finally, after doing all of this, the ‚ÄúQuota tier‚Äù column on the page listing my API keys said ‚ÄúTier 1‚Äù instead of ‚ÄúSet up billing‚Äù.&lt;/p&gt;
    &lt;p&gt;Wait, Tier 1? Did that mean there were other tiers? What were tiers, anyway? Was I already on the best tier? Or maybe I was on the worst one? Not important. The important part was that I had my API key and I‚Äôd managed to convince Google to charge me for it.&lt;/p&gt;
    &lt;p&gt;I went back to the Gemini CLI, ran the &lt;code&gt;/settings&lt;/code&gt; command, and turned on the ‚ÄúEnable experimental features‚Äù option. I ran the &lt;code&gt;/models&lt;/code&gt; command, which told me that Gemini 3 Pro was now available.&lt;/p&gt;
    &lt;p&gt;Success? Not yet.&lt;/p&gt;
    &lt;p&gt;When I tried sending a message to the LLM, it failed with this 403 error:&lt;/p&gt;
    &lt;code&gt;{
  "error": {
    "message": "{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"The caller does not have permission\",\n    \"status\":\"PERMISSION_DENIED\"\n  }\n}\n",
    "code": 403,
    "status": "Forbidden"
  }
}&lt;/code&gt;
    &lt;p&gt;Is that JSON inside a string inside JSON? Yes. Yes it is.&lt;/p&gt;
    &lt;p&gt;To figure out if my key was even working, I tried calling the Gemini API from JavaScript, reproducing the basic example from Google‚Äôs own documentation.&lt;/p&gt;
    &lt;p&gt;No dice. I ran into the exact same error.&lt;/p&gt;
    &lt;p&gt;I then tried talking to Gemini 3 Pro using the Playground inside Google AI Studio. It showed me a toast message saying &lt;code&gt;Failed to generate content. Please try again.&lt;/code&gt; The chat transcript said &lt;code&gt;An internal error has occurred.&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;At this point I gave up and walked away from my computer. It was already 8pm. I‚Äôd been trying to get things to work since 5pm. I needed to eat dinner, play Clair Obscur, and go to bed. I had no more time to waste and no more fucks to give.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your account is in good standing at this time&lt;/head&gt;
    &lt;p&gt;Just as I was getting into bed, I received an email from Google with this subject line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Your Google Cloud and APIs billing account XXXXXX-XXXXXX-XXXXXX is in good standing at this time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With the message inside saying:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on the information you provided and further analysis by Google, we have reinstated your billing account XXXXXX-XXXXXX-XXXXXX. Your account is in good standing, and you should now have full access to your account and related Project(s) and Service(s).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have no idea what any of this means, but Gemini 3 Pro started working correctly after I received this email. It worked in the Playground, directly by calling the API from JavaScript, and with Gemini CLI.&lt;/p&gt;
    &lt;p&gt;Problem solved, I guess. Until Google mysteriously decides that my account is no longer in good standing.&lt;/p&gt;
    &lt;head rend="h2"&gt;This was a waste of time&lt;/head&gt;
    &lt;p&gt;This was such a frustrating experience that I still haven‚Äôt tried using Gemini with my new codebase, nearly a week after I made all those sacrifices to the Gods of Billing Account.&lt;/p&gt;
    &lt;p&gt;I understand why the process for getting a Gemini API key is so convoluted. It‚Äôs designed for large organizations, not an individual developers trying to get work done; it serves the bureaucracy, not the people doing the work; it‚Äôs designed for maximum compliance with government regulations, not for efficiency or productivity.&lt;/p&gt;
    &lt;p&gt;Google doesn‚Äôt want my money unless I‚Äôm an organization that employs ten thousand people.&lt;/p&gt;
    &lt;p&gt;In contrast to Google, Anthropic and OpenAI are much smaller and much more nimble. They‚Äôre able to make the process of setting up a developer account quick and easy for those of us who just want to get things done. Unlike Google, they haven‚Äôt yet become complacent. They need to compete for developer mindshare if they are to survive a decade into the future. Maybe they‚Äôll add the same level of bureaucracy to their processes as they become larger, but for now they‚Äôre fairly easy to deal with.&lt;/p&gt;
    &lt;p&gt;I‚Äôm still going to try using Gemini 3 Pro with Gemini CLI as my coding assistant, but I‚Äôll probably cap the experiment to a month. Unless Gemini 3 Pro is a massive improvement over its competitors, I‚Äôll stick to using tools built by organizations that want me as a customer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46223311</guid><pubDate>Wed, 10 Dec 2025 20:29:12 +0000</pubDate></item><item><title>Patterns.dev</title><link>https://www.patterns.dev/</link><description>&lt;doc fingerprint="ed186110298694bb"&gt;
  &lt;main&gt;
    &lt;p&gt;Interested in our next book? Learn more about Building Large-scale JavaScript Web Apps with React&lt;/p&gt;
    &lt;p&gt;Patterns.dev is a free online resource on design, rendering, and performance patterns for building powerful web apps with vanilla JavaScript or modern frameworks.&lt;/p&gt;
    &lt;p&gt;We publish patterns, tips and tricks for improving how you architect apps for free. Keep in mind, design patterns are descriptive, not prescriptive . They can guide you when facing a problem other developers have encountered many times before, but are not a blunt tool for jamming into every scenario. Patterns.dev aims to be a catalog of patterns (for increasing awareness) rather than a checklist (what you must do).&lt;/p&gt;
    &lt;p&gt;Design patterns are a fundamental part of software development, as they provide typical solutions to commonly recurring problems in software design.&lt;/p&gt;
    &lt;p&gt;A common critique of design patterns is that they needlessly add complexity.&lt;/p&gt;
    &lt;p&gt;Our perspective is that patterns are valuable for solving specific problems, often helping to communicate comminalities in code problems for humans. If a project doesn't have those problems, there isn't a need to apply them. Patterns can also be very language or framework-specific (e.g. React), which can often mean thinking beyond the scope of just the original GoF design patterns.&lt;/p&gt;
    &lt;p&gt;Learn about web performance patterns for loading your code more efficiently. Unsure how to think about modern approaches to loading or rendering user-experiences? We've got you covered.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46226483</guid><pubDate>Thu, 11 Dec 2025 01:18:55 +0000</pubDate></item><item><title>Incomplete list of mistakes in the design of CSS</title><link>https://wiki.csswg.org/ideas/mistakes</link><description>&lt;doc fingerprint="3f4ffa43dd2d5fa3"&gt;
  &lt;main&gt;&lt;p&gt;That should be corrected if anyone invents a time machine. :P&lt;/p&gt;&lt;code&gt;white-space: nowrap&lt;/code&gt; should be &lt;code&gt;white-space: no-wrap&lt;/code&gt;&lt;code&gt;white-space&lt;/code&gt;&lt;code&gt;animation-iteration-count&lt;/code&gt; should just have been &lt;code&gt;animation-count&lt;/code&gt; (like &lt;code&gt;column-count&lt;/code&gt;!)&lt;code&gt;vertical-align&lt;/code&gt; should not apply to table cells. Instead the CSS3 alignment properties should exist in Level 1.&lt;code&gt;vertical-align: middle&lt;/code&gt; should be &lt;code&gt;text-middle&lt;/code&gt; or &lt;code&gt;x-middle&lt;/code&gt; because it's not really in the middle, and such a name would better describes what it does.&lt;code&gt;fill-available&lt;/code&gt; rather than being undefined in auto situations.&lt;code&gt;border-box&lt;/code&gt; by default.&lt;code&gt;background-size&lt;/code&gt; with one value should duplicate its value, not default the second one to &lt;code&gt;auto&lt;/code&gt;. Ditto &lt;code&gt;translate()&lt;/code&gt;.&lt;code&gt;background-position&lt;/code&gt; and &lt;code&gt;border-spacing&lt;/code&gt; (all 2-axis properties) should take *vertical* first, to match with the 4-direction properties like &lt;code&gt;margin&lt;/code&gt;.&lt;code&gt;margin&lt;/code&gt; should go counter-clockwise (so that the inline-start value is before the block-end and inline-end values instead of after them).&lt;code&gt;z-index&lt;/code&gt; should be called &lt;code&gt;z-order&lt;/code&gt; or &lt;code&gt;depth&lt;/code&gt; and should Just Work on all elements (like it does on flex items).&lt;code&gt;word-wrap&lt;/code&gt;/&lt;code&gt;overflow-wrap&lt;/code&gt; should not exist. Instead, &lt;code&gt;overflow-wrap&lt;/code&gt; should be a keyword on 'white-space', like &lt;code&gt;nowrap&lt;/code&gt; (&lt;code&gt;no-wrap&lt;/code&gt;).&lt;code&gt;currentColor&lt;/code&gt; keyword should have retained the dash, &lt;code&gt;current-color&lt;/code&gt;, as originally specified. Likewise all other color multi-word keyword names.&lt;code&gt;border-radius&lt;/code&gt; should have been &lt;code&gt;corner-radius&lt;/code&gt;.&lt;code&gt;hyphens&lt;/code&gt; property should be called &lt;code&gt;hyphenate&lt;/code&gt;. (It's called &lt;code&gt;hyphens&lt;/code&gt; because the XSL:FO people objected to &lt;code&gt;hyphenate&lt;/code&gt;.)&lt;code&gt;rgba()&lt;/code&gt; and &lt;code&gt;hsla()&lt;/code&gt; should not exist, &lt;code&gt;rgb()&lt;/code&gt; and &lt;code&gt;hsl()&lt;/code&gt;  should have gotten an optional fourth parameter instead (and the alpha value should have used the same format as R, G, and B or S and L).&lt;code&gt;¬ª&lt;/code&gt; and indirect sibling combinator should have been &lt;code&gt;++&lt;/code&gt;, so there's some logical relationships among the selectors' ascii art&lt;code&gt;*-blend-mode&lt;/code&gt; properties should've just been &lt;code&gt;*-blend&lt;/code&gt;&lt;code&gt;u0001-u00c8&lt;/code&gt;.&lt;code&gt;font-family&lt;/code&gt; should have required the font name to be quoted (like all other values that come from ‚Äúoutside‚Äù CSS).  The rules for handling unquoted font names make parsing &lt;code&gt;font&lt;/code&gt; stupid, as it requires a &lt;code&gt;font-size&lt;/code&gt; value for disambiguation.&lt;code&gt;flex-basis&lt;/code&gt; vs &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt;.  Perhaps: if &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt; is &lt;code&gt;auto&lt;/code&gt;, use &lt;code&gt;flex-basis&lt;/code&gt;; otherwise, stick with &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt; as an inflexible size.  (This also makes min/max width/height behavior fall out of the generic definition.)&lt;code&gt;:empty&lt;/code&gt; should have been &lt;code&gt;:void&lt;/code&gt;, and &lt;code&gt;:empty&lt;/code&gt; should select items that contain only white space&lt;code&gt;table-layout: fixed; width: auto&lt;/code&gt; should result in a fill-available table with fixed-layout columns.&lt;code&gt;text-orientation&lt;/code&gt; should have had &lt;code&gt;upright&lt;/code&gt; as the initial value (given the latest changes to 'writing-mode').&lt;code&gt;@import&lt;/code&gt; rule is required to (a) always hit the network unless you specify cache headers, and (b) construct fresh CSSStyleSheet objects for every import, even if they're identical. It should have had more aggressive URL-based deduping and allowed sharing of stylesheet objects.&lt;code&gt;:link&lt;/code&gt; should have had the &lt;code&gt;:any-link&lt;/code&gt; semantics all along.&lt;code&gt;flex&lt;/code&gt; shorthand (and &lt;code&gt;flex-shrink&lt;/code&gt; and &lt;code&gt;flex-grow&lt;/code&gt; longhands) should accept &lt;code&gt;fr&lt;/code&gt; units instead of bare numbers to represent flex fractions.&lt;code&gt;display&lt;/code&gt; property should be called &lt;code&gt;display-type&lt;/code&gt;.&lt;code&gt;list-style&lt;/code&gt; properties should be called &lt;code&gt;marker-style&lt;/code&gt;, and &lt;code&gt;list-item&lt;/code&gt; renamed to &lt;code&gt;marked-block&lt;/code&gt; or something.&lt;code&gt;text-overflow&lt;/code&gt; property should always apply, not be dependent on &lt;code&gt;overflow&lt;/code&gt;&lt;code&gt;line-height: &amp;lt;percentage&amp;gt;&lt;/code&gt; should compute to the equivalent &lt;code&gt;line-height: &amp;lt;number&amp;gt;&lt;/code&gt;, so that it effectively inherits as a percentage not a length&lt;code&gt;::placeholder&lt;/code&gt; should be &lt;code&gt;::placeholder-text&lt;/code&gt; and &lt;code&gt;:placeholder-shown&lt;/code&gt; should be &lt;code&gt;:placeholder&lt;/code&gt;&lt;code&gt;overflow: scroll&lt;/code&gt; should introduce a stacking context&lt;code&gt;size&lt;/code&gt; should have been a shorthand for &lt;code&gt;width&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; instead of an &lt;code&gt;@page&lt;/code&gt; property with a different definition&lt;code&gt;span&lt;/code&gt;) with idents in the grid properties, possibly by using functional notation (like &lt;code&gt;span(2)&lt;/code&gt;).&lt;code&gt;align-inline-*&lt;/code&gt; and &lt;code&gt;align-block-*&lt;/code&gt;.&lt;code&gt;shape-outside&lt;/code&gt; should have had &lt;code&gt;wrap-&lt;/code&gt; in the name somehow, as people assume the shape should also clip the content as in &lt;code&gt;clip-path&lt;/code&gt;.&lt;code&gt;!important&lt;/code&gt; ‚Äî¬†that reads to engineers as ‚Äúnot important‚Äù. We should have picked another way to write this.&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46227619</guid><pubDate>Thu, 11 Dec 2025 04:20:52 +0000</pubDate></item><item><title>The Cost of a Closure in C</title><link>https://thephd.dev/the-cost-of-a-closure-in-c-c2y</link><description>&lt;doc fingerprint="ee43ba392a6d308c"&gt;
  &lt;main&gt;
    &lt;p&gt;I had a vague idea that closures could have a variety of performance implications; I did not believe that so many of the chosen and potential designs for C and C++ extensions ones, however, were so‚Ä¶ suboptimal.&lt;/p&gt;
    &lt;p&gt;But, before we get into how these things perform and what the cost of their designs are, we need to talk about what Closures are.&lt;/p&gt;
    &lt;head rend="h1"&gt;‚ÄúClosures‚Äù?&lt;/head&gt;
    &lt;p&gt;Closures in this instance are programming language constructs that includes data alongside instructions that are not directly related to their input (arguments) and their results (return values). They can be seen as a ‚Äúgeneralization‚Äù of the concept of a function or function call, in that a function call is a ‚Äúsubset‚Äù of closures (e.g., the set of closures that do not include this extra, spicy data that comes from places outside of arguments and returns). These generalized functions and generalized function objects hold the ability to do things like work with ‚Äúinstance‚Äù data that is not passed to it directly (i.e., variables surrouding the closure off the stack) and, usually, some way to carry around more data than is implied by their associated function signature.&lt;/p&gt;
    &lt;p&gt;Pretty much all recent and modern languages include something for Closures unless they are deliberately developing for a target audience or for a source code design that is too ‚Äúlow level‚Äù for such a concept (such as Stack programming languages, Bytecode languages, or ones that fashion themselves as assembly-like or close to it). However, we‚Äôre going to be focusing on and looking specifically at Closures in C and C++, since this is going to be about trying to work with and ‚Äì eventually ‚Äì standardize something for ISO C that works for everyone.&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs show a typical problem that arises in C code to show why closure solutions have popped up all over the C ecosystem, then talk about it in the context of the various solutions.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Closure Problem&lt;/head&gt;
    &lt;p&gt;The closure problem can be neatly described by as ‚Äúhow do I get extra data to use within this &lt;code&gt;qsort&lt;/code&gt; call?‚Äù. For example, consider setting this variable, &lt;code&gt;in_reverse&lt;/code&gt;, as part of a bit of command line shenanigans, to change how a sort happens:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

static int in_reverse = 0;

int compare(const void* untyped_left, const void* untyped_right) {
  const int* left = untyped_left;
  const int* right = untyped_right;
  return (in_reverse) ? *right - *left : *left - *right;
}

int main(int argc, char* argv[]) {
  if (argc &amp;gt; 1) {
    char* r_loc = strchr(argv[1], 'r');
    if (r_loc != NULL) {
      ptrdiff_t r_from_start = (r_loc - argv[1]);
      if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
        in_reverse = 1;
      } 
    }
  }
  int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
  qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
  return list[0];
}
&lt;/code&gt;
    &lt;p&gt;This uses a &lt;code&gt;static&lt;/code&gt; variable to have it persist between both the &lt;code&gt;compare&lt;/code&gt; function calls that &lt;code&gt;qsort&lt;/code&gt; makes and the &lt;code&gt;main&lt;/code&gt; call which (potentially) changes its value to be &lt;code&gt;1&lt;/code&gt; instead of &lt;code&gt;0&lt;/code&gt;. Unfortunately, this isn‚Äôt always the best idea for more complex programs that don‚Äôt fit within a single snippet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;it is impossible to have different ‚Äúcopies‚Äù of a &lt;code&gt;static&lt;/code&gt;variable, meaning all mutations done in all parts of the program that can see&lt;code&gt;in_reverse&lt;/code&gt;are responsible for knowing the state before and after (e.g., heavily stateful programming of state that you may not own / cannot see);&lt;/item&gt;
      &lt;item&gt;working on &lt;code&gt;static&lt;/code&gt;data may produce thread contention/race conditions in more complex programs;&lt;/item&gt;
      &lt;item&gt;using &lt;code&gt;_Thread_local&lt;/code&gt;instead of&lt;code&gt;static&lt;/code&gt;only solves the race condition problem but does not solve the ‚Äúshared across several places on the same thread‚Äù problem;&lt;/item&gt;
      &lt;item&gt;referring to specific pieces of data or local pieces of data (like &lt;code&gt;list&lt;/code&gt;itself) become impossible;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and so on, and so forth. This is the core of the problem here. It becomes more pronounced when you want to do things with function and data that are a bit more complex, such as Donald Knuth‚Äôs ‚ÄúMan-or-Boy‚Äù test code.&lt;/p&gt;
    &lt;p&gt;The solutions to these problems come in 4 major flavors in C and C++ code.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Just reimplement the offending function to take a userdata pointer so you can pass whatever data you want (typical C solution, e.g. going from &lt;code&gt;qsort&lt;/code&gt;as the sorting function to BSD‚Äôs&lt;code&gt;qsort_r&lt;/code&gt;1 or Annex K‚Äôs&lt;code&gt;qsort_s&lt;/code&gt;2).&lt;/item&gt;
      &lt;item&gt;Use GNU Nested Functions to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use Apple Blocks to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use C++ Lambdas and some elbow grease to just Refer To What You Want Anyways.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each solution has drawbacks and benefits insofar as usability and design, but as a quick overview we‚Äôll show what it‚Äôs like using &lt;code&gt;qsort&lt;/code&gt; (or &lt;code&gt;qsort_r&lt;/code&gt;/&lt;code&gt;qsort_s&lt;/code&gt;, where applicable). Apple Blocks, for starters, looks like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// value changed in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	qsort_b(list, (sizeof(list)/sizeof(*list)), sizeof(*list),
		// Apple Blocks are Block Expressions, meaning they do not have to be stored
		// in a variable first
		^(const void* untyped_left, const void* untyped_right) {
			const int* left = untyped_left;
			const int* right = untyped_right;
			return (in_reverse) ? *right - *left : *left - *right;
		}
	);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;and GNU Nested Functions look like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// modify variable in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	// GNU Nested Function definition, can reference `in_reverse` directly
	// is a declaration/definition, and cannot be used directly inside of `qsort`
	int compare(const void* untyped_left, const void* untyped_right) {
		const int* left = untyped_left;
		const int* right = untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	}
	// use in the sort function without the need for a `void*` parameter
	qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;or, finally, C++-style Lambdas:&lt;/p&gt;
    &lt;code&gt;#define __STDC_WANT_LIB_EXT1__ 1

#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	int in_reverse = 0;
	
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	// lambdas are expressions, but we can assign their unique variable types with `auto`
	auto compare = [&amp;amp;](const void* untyped_left, const void* untyped_right) {
		const int* left = (const int*)untyped_left;
		const int* right = (const int*)untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	};

	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };	

	// C++ Lambdas don't automatically make a trampoline, so we need to provide
	// one ourselves for the `qsort_s/r` case so we can call the lambda
	auto compare_trampoline = [](const void* left, const void* right, void* user) {
		typeof(compare)* p_compare = user;
		return (*p_compare)(left, right);
	};
	qsort_s(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare_trampoline, &amp;amp;compare);

	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;To solve this gaggle of problems, pretty much every semi-modern language (that isn‚Äôt assembly-adjacent or based on some kind of state/stack programming) provide some idea of being able to associate some set of data with one or more function calls. And, particularly for Closures, this is done in a local way without passing it as an explicit argument. As it turns out, all of those design choices ‚Äì including the ones in C ‚Äì have pretty significant consequences on not just usability, but performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Not A Big Overview&lt;/head&gt;
    &lt;p&gt;This article is NOT going to talk in-depth about the design of all of the alternatives or other languages. We‚Äôre focused on the actual cost of the extensions and what they mean. A detailed overview of the design tradeoffs, their security implications, and other problems, can be read at the ISO C Proposal for Functions with Closures here; it also gets into things like Security Implications, ABI, current implementation impact, and more of the various designs. The discussion in the paper is pretty long and talks about the dozens of aspects of each solution down to both the design aspect and the implementation quirks. We encourage you to dive into that proposal and read it to figure out if there‚Äôs something more specific you care about insofar as some specific design portion. But, this article is going to be concerned about one thing and one thing only:&lt;/p&gt;
    &lt;head rend="h1"&gt;Purrrrrrrformance :3!&lt;/head&gt;
    &lt;p&gt;In order to measure this cost, we are going to take Knuth‚Äôs Man-or-Boy test and benchmark various styles of implementation in C and C++ using various different extensions / features for the Closure problem. The Man-or-Boy test is an efficient measure of how well your programming language can handle referring to specific entities while engaging in a large degree of recursion and self-reference. It can stress test various portions of how your program creates and passes around data associated with a function call, and if your programming language design is so goofy that it can‚Äôt refer to a specific instance of a variable or function argument, it will end up producing the wrong answer and breaking horrifically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anatomy of a Benchmark: Raw C&lt;/head&gt;
    &lt;p&gt;Here is the core of the Man-or-Boy test, as implemented in raw C. This implementation3 and all the others are available online for us all to scrutinize and yell at me for messing up, to make sure I‚Äôm not slandering your favorite solution for Closures in this space.&lt;/p&gt;
    &lt;code&gt;// ...

static int eval(ARG* a) {
	return a-&amp;gt;fn(a);
}

static int B(ARG* a) {
	int k    = *a-&amp;gt;k -= 1;
	ARG args = { B, &amp;amp;k, a, a-&amp;gt;x1, a-&amp;gt;x2, a-&amp;gt;x3, a-&amp;gt;x4 };
	return A(&amp;amp;args);
}

static int A(ARG* a) {
	return *a-&amp;gt;k &amp;lt;= 0 ? eval(a-&amp;gt;x4) + eval(a-&amp;gt;x5) : B(a);
}

// ...
&lt;/code&gt;
    &lt;p&gt;You will notice that there is a big, fat, ugly &lt;code&gt;ARG*&lt;/code&gt; parameter hanging around all of these functions. That is because, as stated before, plain ISO C cannot handle passing the data around unless it‚Äôs part of a function‚Äôs arguments. Because the actual core of the Man-or-Boy experiment is the ability to refer to specific values of &lt;code&gt;k&lt;/code&gt; that exist during the recursive run of the program, we need to actually modify the function signature and thereby cheat some of the implicit Man-or-Boy requirements of not passing the value in directly. Here‚Äôs what &lt;code&gt;ARG&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;typedef struct arg {
	int (*fn)(struct arg*);
	int* k;
	struct arg *x1, *x2, *x3, *x4, *x5;
} ARG;

static int f_1(ARG* _) {
	return -1;
}

static int f0(ARG* _) {
	return 0;
}

static int f1(ARG* _) {
	return 1;
}

static int eval(ARG* a) {
	// ...
}
// ...
&lt;/code&gt;
    &lt;p&gt;And this is how it gets used in the main body of the function in order to compute the right answer and benchmark it:&lt;/p&gt;
    &lt;code&gt;static void normal_functions_rosetta(benchmark::State&amp;amp; state) {
	const int initial_k  = k_value();
	const int expected_k = expected_k_value();
	int64_t result       = 0;

	for (auto _ : state) {
		int k     = initial_k;
		ARG arg1  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg2  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg3  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg4  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg5  = { f0, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG args  = { B, &amp;amp;k, &amp;amp;arg1, &amp;amp;arg2, &amp;amp;arg3, &amp;amp;arg4, &amp;amp;arg5 };
		int value = A(&amp;amp;args);
		result += value == expected_k ? 1 : 0;
	}

	if (result != state.iterations()) {
		state.SkipWithError("failed: did not produce the right answer!");
	}
}

BENCHMARK(normal_functions_rosetta);
&lt;/code&gt;
    &lt;p&gt;Everything within the &lt;code&gt;for (auto _ : state) { ... }&lt;/code&gt; is benchmarked. For those paying attention to the code and find it looking familiar, it‚Äôs because that code is the basic structure all Google Benchmark4 code finds itself looking like. I‚Äôve wanted to swap to Catch25 for a long time now to change to their benchmarking infrastructure, but I‚Äôve been stuck on Google Benchmark because I‚Äôve made a lot of graph-making tools based on its JSON output and I have not vetted Catch2‚Äôs JSON output yet to see if it has all of the necessary bits ‚Äòn‚Äô bobbles I use to de-dedup runs and compute statistics.&lt;/p&gt;
    &lt;p&gt;Everything outside is setup (the part above the &lt;code&gt;for&lt;/code&gt; loop) or teardown/test correction (the part below the &lt;code&gt;for&lt;/code&gt; loop). The initialization of the &lt;code&gt;ARG args&lt;/code&gt;s cannot be moved outside of the measuring loop because each invocation of &lt;code&gt;A&lt;/code&gt; ‚Äì the core of the Man-or-Boy experiment ‚Äì modifies the &lt;code&gt;k&lt;/code&gt; of the ARG parameter, so all of them have to be inside. Conceivably, &lt;code&gt;arg1 .. 5&lt;/code&gt; could be moved out of the loop, but I am very tired of looking at the eight or nine variations of this code so someone else can move it and tell me if Clang or GCC has lots of compiler optimization sauce and doesn‚Äôt understand that those 5 &lt;code&gt;argI&lt;/code&gt;s can be hoisted out of the loop.&lt;/p&gt;
    &lt;p&gt;The value &lt;code&gt;k&lt;/code&gt; is &lt;code&gt;10&lt;/code&gt;, and &lt;code&gt;expected_k&lt;/code&gt; is &lt;code&gt;-67&lt;/code&gt;. The expected, returned &lt;code&gt;k&lt;/code&gt; value is dependent on the input &lt;code&gt;k&lt;/code&gt; value, which controls how deep the Man-or-Boy test would recurse on itself to produce its answer. Therefore, to prevent GCC and Clang and other MEGA POWERFUL PILLAR COMPILERS from optimizing the entire thing out and just replacing the benchmark loop with &lt;code&gt;ret -67&lt;/code&gt;, both &lt;code&gt;k_value()&lt;/code&gt; and &lt;code&gt;expected_k_value()&lt;/code&gt; come from a Dynamic Link Library (&lt;code&gt;.dylib&lt;/code&gt; on MacOS, &lt;code&gt;.so&lt;/code&gt; on *nix platforms, &lt;code&gt;.dll&lt;/code&gt; on Windows platforms) to make sure that NO amount of optimization (Link Time Optimization/Link Time Code Generation, Inlining Optimization, Cross-Translation Unit Optimization, and Automatic Constant Expression Optimization) from C or C++ compilers could fully preempt all forms of computation.&lt;/p&gt;
    &lt;p&gt;This allows us to know, for sure, that we‚Äôre actually measuring something and not just testing how fast a compiler can load a number into a register and test it against &lt;code&gt;state.iterations()&lt;/code&gt;. And, since we know for sure, we can now talk the general methodology.&lt;/p&gt;
    &lt;head rend="h1"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;The tests were ran on a dying 13-inch 2020 MacBook Pro M1 that has suffered several toddler spills and two severe falls. It has 16 GB of RAM and is son MacOS 15.7.2 Sequoia at the time the test was taken, using the stock MacOS AppleClang Compiler and the stock &lt;code&gt;brew install gcc&lt;/code&gt; compiler in order to produce the numbers seen on December 6th, 2025.&lt;/p&gt;
    &lt;p&gt;There 2 measures being conducted: Real Time and CPU Time. The time is gathered by running a single iteration of the code within the &lt;code&gt;for&lt;/code&gt; loop anywhere from a couple thousand to hundreds of thousands of times to produce confidence in that run of the benchmark. This is then averaged to produce the first point. The process is repeated 50 times, repeating that many iterations to build further confidence in the measurement. All 50 means are used as the points for the values, and the average of all of those 50 means is then used as the height of a bar in a bar graph.&lt;/p&gt;
    &lt;p&gt;The bars are presented side-by-side as a horizontal bar chart with 11 categories of C or C++ code being measured. The 11 categories are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;no-op&lt;/code&gt;: Literally doing nothing. It‚Äôs just there to test environmental noise and make sure none of our benchmarks are so off-base that we‚Äôre measuring noise rather than computation. Helps keep us grounded in reality.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (No Function Helpers)&lt;/code&gt;: a solution using C++-style lambdas. Rather than using helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;, we compute a raw lambda that stores the value meant to be returned for the Man-or-Boy test (&lt;code&gt;return i;&lt;/code&gt;) in the lambda itself and then pass that uniquely-typed lambda to the core of the test. The entire test is templated and uses a fake&lt;code&gt;recursion&lt;/code&gt;template parameter to halt the recursion after a certain depth.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas&lt;/code&gt;: The same as above but actually using&lt;code&gt;int f0(void)&lt;/code&gt;, etc. helper functions at the start rather than lambdas. Reduces inliner pressure by using ‚Äúnormal‚Äù types which do not add to the generated number of lambda-typed, recursive, templated function calls.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function_ref)&lt;/code&gt;: The same as above, but rather than using a function template to handle each uniquely-typed lambda like a precious baby bird, it instead erases the lambda behind a&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This allows the recursive function to retain exactly one signature.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function)&lt;/code&gt;: The same as above, but replaces&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;with&lt;code&gt;std::function&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This is its allocating, C++03-style type.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (Rosetta Code)&lt;/code&gt;: The code straight out of the C++11 Rosetta Code Lambda section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Apple Blocks&lt;/code&gt;: Uses Apple Blocks to implement the test, along with the&lt;code&gt;__block&lt;/code&gt;specifier to refer directly to certain variables on the stack.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions (Rosetta Code)&lt;/code&gt;: The code straight out of the C Rosetta Code section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions&lt;/code&gt;: GNU Nested Functions similar to the Rosetta Code implementation, but with some slight modifications in a hope to potentially alleviate some stack pressure if possible by using regular helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Custom C++ Class&lt;/code&gt;: A custom-written C++ class using a discriminated union to decide whether its doing a straight function call or attemping to engage in the Man-or-Boy recursion.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;C++03 shared_ptr (Rosetta Code)&lt;/code&gt;: A C++ class using&lt;code&gt;std::enable_shared_from_this&lt;/code&gt;and&lt;code&gt;std::shared_ptr&lt;/code&gt;with a virtual function call to invoke the ‚Äúright‚Äù function call during recursion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The two compilers tested are Apple Clang 17 and GCC 15. There are two graph images because one is for Apple Clang and the other is for GCC. This is particularly important because neither compiler implements the other‚Äôs closure extension (Clang does Apple Blocks but not Nested Functions, while GCC does Nested Functions in exclusively its C frontend but does not implement Apple Blocks6).&lt;/p&gt;
    &lt;head rend="h1"&gt;The Results&lt;/head&gt;
    &lt;p&gt;Ta-da!&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;‚Ä¶ Oh. That looks awful.&lt;/p&gt;
    &lt;p&gt;It turns out that some solutions are so dogwater that it completely screws up our viewing graphs. But, it does let us know that Lambdas used the Rosetta Code style are so unbelievably awful that it is several orders of magnitude more expensive than any other solution presented! One has to wonder what the hell is going on in the code snippet there, but first we need to make the graphs more legible. To do this we‚Äôre going to be using the (slightly deceptive) LOGARITHMIC SCALING. This is a bit deadly to do because it tends to mislead people about how much of a change there is, so please pay attention to the potential order of magnitude gains and losses when going from one bar graph to another.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;There we go. Now we can talk about the various solutions and ‚Äì in particular ‚Äì why ‚Äúlambdas‚Äù have 4 different entries with such wildly differing performance profiles. First up, let‚Äôs talk about the clear performance winners.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lambdas: On Top!&lt;/head&gt;
    &lt;p&gt;Not surprising to anyone who has been checked in to C++, lambdas that are used directly and not type-erased are on top. This means there‚Äôs a one-to-one mapping between a function call and a given bit of execution. We are cheating by using a constant parameter to stop the uniquely-typed lambdas being passed into the functions from recursing infinitely, which makes the Man-or-Boy function look like this:&lt;/p&gt;
    &lt;code&gt;template &amp;lt;int recursion = 0&amp;gt;
static int a(int k, const auto&amp;amp; x1, const auto&amp;amp; x2, const auto&amp;amp; x3, const auto&amp;amp; x4, const auto&amp;amp; x5) {
	if constexpr (recursion == 11) {
		::std::cerr &amp;lt;&amp;lt; "This should never happen and this code should never have been generated." &amp;lt;&amp;lt; std::endl;
		::std::terminate();
		return 0;
	}
	else {
		auto B = [&amp;amp;](this const auto&amp;amp; self) { return a&amp;lt;recursion + 1&amp;gt;(--k, self, x1, x2, x3, x4); };
		return k &amp;lt;= 0 ? x4() + x5() : B();
	}
}
&lt;/code&gt;
    &lt;p&gt;Every &lt;code&gt;B&lt;/code&gt; is its own unique type and we are not erasing that unique type when using the expression as an initializer to &lt;code&gt;B&lt;/code&gt;. This means that when we call &lt;code&gt;a&lt;/code&gt; again with &lt;code&gt;B&lt;/code&gt; (the &lt;code&gt;self&lt;/code&gt; in this lambda here using Deduced This, a C++23 feature that cannot be part of the C version of lambdas) which means we need to use &lt;code&gt;auto&lt;/code&gt; parameters (a shortcut way of writing template parameters) to take it. But, since every parameter is unique, and every &lt;code&gt;B&lt;/code&gt; is unique, calling this recursively means that, eventually, C++ compilers will actually just completely crash out/toss out-of-memory errors/say we‚Äôve compile-time recursed too hard, or similar. That‚Äôs why the compile-time &lt;code&gt;if constexpr&lt;/code&gt; on the extra, templated &lt;code&gt;recursion&lt;/code&gt; parameter needs to have some arbitrary limit. Because we know &lt;code&gt;k&lt;/code&gt; starts at 10 for this test, we just have some bogus limit of ‚Äú11‚Äù.&lt;/p&gt;
    &lt;p&gt;This results in a very spammy recursive chain of function calls, where the actual generated names of these template functions are far more complex than &lt;code&gt;a&lt;/code&gt; and can run the compiler into the ground / cause quite a bit of instantiations if you let &lt;code&gt;recursion&lt;/code&gt; get to a high enough value. But, once you add the limit, the compiler gets perfect information about this recursive call all the way to every leaf, and thus is able to not only optimize the hell out of it, but refuse to generate the other frivolous code it knows won‚Äôt be useful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas are also Fast, even when Type-Erased&lt;/head&gt;
    &lt;p&gt;You can observe a slight bump up in performance penalty when a Lambda is erased by a &lt;code&gt;std::function_ref&lt;/code&gt;. This is a low-level, non-allocating, non-owning, slim ‚Äúview‚Äù type that is analogous to what a language-based wide function pointer type would be in C. From this, it allows us to guess how good Lambdas in C would be even if you had to hide them behind a non-unique type.&lt;/p&gt;
    &lt;p&gt;The performance metrics are about equivalent to if you hand-wrote a C++ class with a custom &lt;code&gt;operator()&lt;/code&gt; that uses a discriminated union, no matter which compiler gets used to do it. It‚Äôs obviously not as fast as having access to a direct function call and being able to slurp-inline optimize, but the performance difference is acceptable when you do not want to engage in a large degree of what is called ‚Äúmonomorphisation‚Äù of a generic routine or type. And, indeed, outside of macros, C has no way of doing this innately that isn‚Äôt runtime-based.&lt;/p&gt;
    &lt;p&gt;A very strong contender for a good solution!&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas: On‚Ä¶. Bottom, too?&lt;/head&gt;
    &lt;p&gt;One must wonder, then, why the &lt;code&gt;std::function&lt;/code&gt; Lambdas and the Rosetta Code Lambdas are either bottom-middle-of-the-road or absolutely-teary-eyed-awful.&lt;/p&gt;
    &lt;p&gt;Starting off, the &lt;code&gt;std::function&lt;/code&gt; Lambdas are bad because of exactly that: &lt;code&gt;std::function&lt;/code&gt;. &lt;code&gt;std::function&lt;/code&gt; is not a ‚Äúcheap‚Äù closure; it is a potentially-allocating, meaty, owning function abstraction. This means that it‚Äôs safe to make one and pass it around and store it and call it later; the cost of this is, obviously, that you‚Äôre allocating (when the type is big enough) for that internal storage. Part of this is alleviated by using &lt;code&gt;const std::function&amp;lt;int(void)&amp;gt;&amp;amp;&lt;/code&gt; parameters, taking things by reference and only generating a new object when necessary. This prevents copying on every function call. Both the Rosetta Lambdas and regular &lt;code&gt;std::function&lt;/code&gt; Lambdas code do the by-reference parameters bit, though, so where does the difference come in? It actually has to do with the Captures. Here‚Äôs how &lt;code&gt;std::function&lt;/code&gt; Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [&amp;amp;] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;And, here is how the Rosetta Code Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [=, &amp;amp;k, &amp;amp;B] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;The big problem here is in the use of the &lt;code&gt;=&lt;/code&gt;. What &lt;code&gt;=&lt;/code&gt; by itself in the front of a lambda capture clause means is ‚Äúcopy all the visible variables in and hold onto that copy‚Äù (unless the capture for that following variable is ‚Äúoverridden‚Äù by a &lt;code&gt;&amp;amp;var&lt;/code&gt;, address capture). Meanwhile, the &lt;code&gt;&amp;amp;&lt;/code&gt; is the opposite: it means ‚Äúrefer to all the visible variables directly by their address and do not copy them in‚Äù. So, while the &lt;code&gt;std::function&lt;/code&gt; Lambda is (smartly) referring to stuff directly without copying because we know for the Man-or-Boy test that referring to things directly is not an unsafe operation, the general &lt;code&gt;=&lt;/code&gt; causes that for the several dozen recursive iterations through the function, it is copying all five allocating &lt;code&gt;std::function&lt;/code&gt; arguments. So the first call creates a &lt;code&gt;B&lt;/code&gt; that copies everything in, and then passes that in, and then the next call copies the previous &lt;code&gt;B&lt;/code&gt; and the 4 normal functions, and then passes that in to the next &lt;code&gt;B&lt;/code&gt;, and then it copies both previous &lt;code&gt;B&lt;/code&gt;‚Äôs, and this stacks for the depth of the callgraph (some 10 times since &lt;code&gt;k = 10&lt;/code&gt; to start).&lt;/p&gt;
    &lt;p&gt;You can imagine how much that completely screws with the performance, and it explains why the Rosetta Code Lambdas code behaves so poorly in terms of performance. But, this also raises a question: if referring to everything by-reference saves so much speed, then why does GNU Nested Functions ‚Äì in all its variants ‚Äì perform so poorly? After all, Nested Functions capture everything by reference / by address, exactly like a lambda does with &lt;code&gt;[&amp;amp;]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similarly, if allocating over and over again was so expensive, how come Apple Blocks and C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code-style versions of the Man-or-Boy test don‚Äôt perform nearly as badly as the Rosetta Code Lambdas? Are we not copying the value of the arguments into a newly created Apple Block and, thusly, tanking the performance metrics? Well, as it turns out, there‚Äôs many reasons for these things, so let‚Äôs start with GNU Nested Functions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nested Functions and The Stack&lt;/head&gt;
    &lt;p&gt;I‚Äôve written about it dozens of times now, but the prevailing and most common implementation of Nested Functions is with an executable stack. The are a lot of security and other implications for this, but all you need to understand is that the reason GCC did this is because it was an at-the-time slick encoding of both the location of the variables and the routine itself. Allocating a chunk of data off of the current programming stack means that the ‚Äúenvironment context‚Äù/‚Äùthis closure‚Äù pointer has the same anchoring address as the routine itself. This means you can encode both the location of the data to know what to access and the address of a function‚Äôs entry point into a single thing that works with your typical setup-and-call convention that comes with invoking a standard ISO C function pointer.&lt;/p&gt;
    &lt;p&gt;But think about that, briefly, in terms of optimization.&lt;/p&gt;
    &lt;p&gt;You are using the function‚Äôs stack frame at that precise point in the program as the ‚Äúbase address‚Äù for this executable code. That base address also means that all the variables associated with it need to be reachable from that base address: i.e., that things are not stuffed in registers, but that you are referring to the same variables as modified by the enclosing function around your nested function. Principally, this means that your function needs to have all of the following now so that GNU Nested Functions actually work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that is executable so that the base address used for the trampoline can be run succinctly.&lt;/item&gt;
      &lt;item&gt;A real function frame that exists somewhere in memory to serve as the base address for the trampoline.&lt;/item&gt;
      &lt;item&gt;Real objects in memory backing the names of the captured variables to be accessed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all seems like regular consequences, until you tack on the second order affects from the point of optimization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that now has both data and instructions all blended into itself.&lt;/item&gt;
      &lt;item&gt;A real function frame, which means no omission of a frame pointer and no collapsing / inlining of that function frame.&lt;/item&gt;
      &lt;item&gt;Real objects that all have their address taken that are tied to the function frame, which must be memory-accessible and which the compiler now has a hard time telling if they can simply be exchanged through registers or if the need to actually sit somewhere in memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words: GNU Nested Functions have created the perfect little storm for what might be the best optimizer-murderer. The reason it performs so drastically poorly (worse than even allocating lambdas inside of a &lt;code&gt;std::function&lt;/code&gt; or C++03-style virtual function calls inside of a bulky, nasty C++ &lt;code&gt;std::shared_ptr&lt;/code&gt;) by a whole order of magnitude or more is that everything about Nested Functions and their current implementation is basically Optimizer Death. If the compiler can‚Äôt see through everything ‚Äì and the Man-or-Boy test with a non-constant value of &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt; ‚Äì GNU Nested Functions deteriorate rapidly. It takes every core optimization technique that we‚Äôve researched and maximized on in the last 30 years and puts a shotgun to the side of its head once it can‚Äôt pre-compute &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The good news is that GCC has completed a new backing implementation for GNU Nested Functions, which uses a heap-based trampoline. Such a trampoline does not interfere with the stack, would allow for omission of frame pointers while referring directly to the data itself (which may prevent the wrecking of specific kinds of inlining optimizations), and does not need an executable stack (just a piece of memory from ‚ú®somewhere‚ú® it can mark executable). This may have performance closer to Apple Blocks, but we don‚Äôt have a build of the latest GCC to test it with. But, when we do, we can simply add the compilation flag &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the two source files in CMake and then let the benchmarks run again to see how it stacks up!&lt;/p&gt;
    &lt;p&gt;Finally, there is a minor performance degredation because our benchmarking software is in C++ and this extension exists exclusively in the C frontend of GCC. That means I have to use an &lt;code&gt;extern&lt;/code&gt; function call within the benchmark loop to get to the actual code. Within the function call, however, all of this stuff should be optimized down, so the cost of a single function call‚Äôs stack frame shouldn‚Äôt be so awful, but I expect to try to dig into this better to help make sure the &lt;code&gt;extern&lt;/code&gt; of a C function call isn‚Äôt making things dramatically worse than they are. Given it‚Äôs a different translation unit and it‚Äôs not being compiled as a separate static or dynamic library, it should still link together and optimize cleanly, but given how bad it‚Äôs performing? Every possible issue is on the table.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about Apple Blocks?&lt;/head&gt;
    &lt;p&gt;Apple Blocks are not the fastest, but they the best of the C extensions while being the worst of the ‚Äúfast‚Äù solutions. They are not faster than just hacking the &lt;code&gt;ARG*&lt;/code&gt; into the function signature and using regular normal C function calls, unfortunately, and that‚Äôs likely due to their shared, heap-ish nature. The saddest part about Apple Blocks is that it works using a Blocks Runtime that is already as optimized as it can possibly be: Clang and Apple both document that while the Blocks Runtime does manage an Automatic Reference Counted (ARC) Heap of Block pointers, when a Block is first created it will literally have its memory stored on the stack rather than in the heap. In order to move it to the heap, one must call &lt;code&gt;Block_copy&lt;/code&gt; to trigger the ‚Äúnormal‚Äù heap-based shenanigans. We never call &lt;code&gt;Block_copy&lt;/code&gt;, so this is with as-fast-as-possible variable access and management with few allocations.&lt;/p&gt;
    &lt;p&gt;It‚Äôs very slightly disappointing that: normal C functions with an &lt;code&gt;ARG*&lt;/code&gt; blob; a custom C++ class using a discriminated union and &lt;code&gt;operator()&lt;/code&gt;; any mildly conscientious use of lambdas; and, any other such shenanigans perform better than the very best Apple Blocks has to offer. One has to imagine that all of the ARC management functions made to copy the &lt;code&gt;int^(void)&lt;/code&gt; ‚Äúhat-style‚Äù function pointers, even if they end up not doing much for the data stored on the stack, impacted the results here. But, this is also somewhat good news: because Apple Block hat pointers are cheaply-copiable entities (they are just pointers to a Block object), it means that even if we copy all of the arguments into the closure every function call, that copying is about as cheap as it can get. Obivously, as regular ‚ÄúLambdas‚Äù and ‚ÄúLambas (No Function Helpers)‚Äù demonstrate, being able to just slurp everything up by address/by reference ‚Äì including visible function arguments ‚Äì with &lt;code&gt;[&amp;amp;]&lt;/code&gt; saves us a teensy, tiny bit of time7.&lt;/p&gt;
    &lt;p&gt;The cheapness of &lt;code&gt;int^(void)&lt;/code&gt; hat-pointer function types is likely the biggest saving grace for Apple Blocks in this benchmark. In the one place we need to be careful, we rename the input argument &lt;code&gt;k&lt;/code&gt; to &lt;code&gt;arg_k&lt;/code&gt; and then make a &lt;code&gt;__block&lt;/code&gt; variable to actually refer to a shared &lt;code&gt;int k&lt;/code&gt; (and get the right answer):&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	__block fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;All of the &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;x3&lt;/code&gt; ‚Äì like the bad Lambda case ‚Äì are copied over and over and over again. One could change the name of all the arugments &lt;code&gt;arg_xI&lt;/code&gt; and then have an &lt;code&gt;xI&lt;/code&gt; variable inside that is marked &lt;code&gt;__block&lt;/code&gt;, but that‚Äôs more effort and very unlikely to have any serious impact on the code while possibly degrading performance for the setup of multiple shared variables that all have to also be ARC-reference-counted and be stored inside each and every new &lt;code&gt;b&lt;/code&gt; block that is created.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Brief Aside: Self-Referencing Functions/Closures&lt;/head&gt;
    &lt;p&gt;It‚Äôs also important to note that just writing this:&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;(no &lt;code&gt;__block&lt;/code&gt; on the &lt;code&gt;b&lt;/code&gt; variable) is actually a huge bug. Apple Blocks, like older C++ Lambdas, cannot technically refer to ‚Äúitself‚Äù inside. You have to refer to the ‚Äúself‚Äù by capturing the variable it is assigned to. For those who use C++ and are familiar with the lambdas over there, it‚Äôs like making sure you capture the variable you initialize with the lambda by reference while also making sure it has a concrete type. It can only be escaped by using &lt;code&gt;auto&lt;/code&gt; and Deducing This, or some other combination of referential-use. That is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;auto x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;does not compile, as the type&lt;code&gt;auto&lt;/code&gt;isn‚Äôt figured out yet;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function_ref&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles but due to C++ shenanigans produces a dangling reference to a temporary lambda that dies after the full expression (the initialization);&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles and works with no segfaults because&lt;code&gt;std::function&lt;/code&gt;allocates, and the reference to itself&lt;code&gt;&amp;amp;x&lt;/code&gt;is just fine.&lt;/item&gt;
      &lt;item&gt;and, finally, &lt;code&gt;auto x = [](this const auto&amp;amp; self, int v) { if (v != limit) self(v + 1); return v + 8; }&lt;/code&gt;which compiles and works with no segfaults because the invisible&lt;code&gt;self&lt;/code&gt;parameter is just a reference to the current object.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem with the most recent Apple Blocks snippet just above is that it‚Äôs the equivalent of doing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice that there‚Äôs no &lt;code&gt;&amp;amp;x&lt;/code&gt; in the lambda initializer‚Äôs capture list. It‚Äôs copying an (uninitialized) variable by-value into the lambda. This is what Apple Blocks set into a variable that does not have a &lt;code&gt;__block&lt;/code&gt; specifier, like in our bad code case with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All variations of this on all implementations which allow for self-referencing allow this and compile some form of this. You would imagine some implementations would warn about this, but this is leftover nonsense from allowing a variable to refer to itself in its initialization. The obvious reason this happens in C and C++ is because you can create self-referential structures, but unfortunately neither language provided a safe way to do this generally. C++23‚Äôs Deducing This does not work inside of regular functions and non-objects, so good luck applying it to other places and other extensions8. The only extension which does not suffer this problem is GNU Nested Functions, because it creates a function declaration / definition rather than a variable with an initializer. Thus, this code from the benchmarks works:&lt;/p&gt;
    &lt;code&gt;inline static int gnu_nested_functions_a(int k, int xl(void), int x2(void), int x3(void), int x4(void), int x5(void)) {
	int b(void) {
		return gnu_nested_functions_a(--k, b, xl, x2, x3, x4);
	}
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;And it has the semantics one would expect, unlike how Blocks, Lambdas, or others with default by-value copying work.&lt;/p&gt;
    &lt;p&gt;In the general case, this is what the paper &lt;code&gt;__self_func&lt;/code&gt; was going to solve9, but‚Ä¶ that‚Äôs going to need some time for me to convince WG14 that maybe it IS actually a good idea. We can probably just keep writing the buggy code a few dozen more times for the recursion case and keep leaving it error prone, but I‚Äôll try my best to convince them one more time that the above situation is very not-okay.&lt;/p&gt;
    &lt;head rend="h1"&gt;Thinking It Over&lt;/head&gt;
    &lt;p&gt;While the Man-or-Boy test isn‚Äôt exactly the end-all, be-all performance test, due to flexing both (self)-referential data and utilization of local copies with recursion, it is surprisingly suitable for figuring out if a closure design is decent enough in a mid to high-level programming language. It also gives me some confidence that, at the very least, the baseline for performance of statically-known, compile-time understood, non type-erased, callable Closure objects will have the best implementation quality and performance tradeoffs for a language like ISO C no matter the compiler implementation.&lt;/p&gt;
    &lt;p&gt;In the future, at some point, I‚Äôll have to write about why that is. It‚Äôs a bit upside down from the perspective of readers of this blog to first address performance and then later write about the design, but it‚Äôs nice to make sure we‚Äôre not designing ourselves into a bad performance corner at the outset of this whole adventure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learned Insights&lt;/head&gt;
    &lt;p&gt;Surprising nobody, the more information the compiler is allowed to accrue (the Lambda design), the better its ability to make the code fast. What might be slightly more surprising is that a slim, compact layer of type erasure ‚Äì not a bulky set of Virtual Function Calls (C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code design) ‚Äì does not actually cost much at all (Lambdas with &lt;code&gt;std::function_ref&lt;/code&gt;). This points out something else that‚Äôs part of the ISO C proposal for Closures (but not formally in its wording): Wide Function Pointers.&lt;/p&gt;
    &lt;p&gt;The ability to make a thin &lt;code&gt;{ some_function_type* func; void* context; }&lt;/code&gt; type backed by the compiler in C would be extremely powerful. Martin Uecker has a proposal that has received interest and passing approval in the Committee, but it would be nice to move it along in a nice direction. My suggestion is having &lt;code&gt;%&lt;/code&gt; as a modifier, so it can be used easily since wide function pointers are an extremely prevalent concept. Being able to write something like the following would be very easy and helpful.&lt;/p&gt;
    &lt;code&gt;typedef int(compute_fn_t)(int);

int do_computation(int num, compute_fn_t% success_modification);
&lt;/code&gt;
    &lt;p&gt;A wide function pointer type like this would also be traditionally convertible from a number of already existing extensions, too, where GNU Nested Functions, Apple Blocks, C++-style Lambdas, and more could create the appropriate wide function pointer type to be cheaply used. Additionally, it also works for FFI: things like Go closures already use GCC‚Äôs &lt;code&gt;__builtin_call_with_static_chain&lt;/code&gt; to transport through their Go functions in C. Many other functions from other languages could be cheaply and efficiently bridged with this, without having to come up with harebrained schemes about where to put a &lt;code&gt;void* userdata&lt;/code&gt; or some kind of implicit context pointer / implicit environment pointer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Extensions?&lt;/head&gt;
    &lt;p&gt;Unfortunately ‚Äì except for the Borland closure annotation ‚Äì there‚Äôs too many things that are performance-stinky about existing C extensions to this problem. It‚Äôs no wonder GCC is trying to add &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the story of GNU Nested Functions; they might be able to tighten up that performance and make it more competitive with Apple Blocks. But, unfortunately, since it is heap-based, there‚Äôs a real chance that its maximum performance ceiling is only as good as Apple Blocks, and not as good as a C++-style Lambda.&lt;/p&gt;
    &lt;p&gt;Both GNU Nested Functions and Apple Blocks ‚Äì as they are implemented ‚Äì do not really work well in ISO C. GNU Nested Functions because their base design and most prevalent implementation are performance-awful, but also Apple Blocks because of the copying and indirection runtime of Blocks that manage ARC pointers providing a hard upper limit on how good the performance can actually be in complex cases.&lt;/p&gt;
    &lt;p&gt;Regular C code, again, performs middle-of-the-road here. It‚Äôs not the worst of it, but it‚Äôs not the best at all, which means there‚Äôs some room beneath how we could go having the C code run. While it‚Äôs hard to fully trust the Rosetta Code Man-or-Boy code for C as the best, it is a pretty clear example of how a ‚Äúnormal‚Äù C developer would do it and how it‚Äôs not actually able to hit maximum performance for this situation.&lt;/p&gt;
    &lt;p&gt;I wanted to add a version of regular C code that used a dynamic array with &lt;code&gt;static&lt;/code&gt;s to transfer data, or a bunch of &lt;code&gt;thread_local&lt;/code&gt;s, but I could not bring myself to actually care enough to write a complex association scheme from a specific invocation of the recursive function &lt;code&gt;a&lt;/code&gt; and the slot of dynamic data that represented the closure‚Äôs data. I‚Äôm sure there‚Äôs schemes for it and I could think of a few, but at that point it‚Äôs such a violent contortion to get a solution going that I figured it simply wasn‚Äôt worth the effort. But, as always,&lt;/p&gt;
    &lt;p&gt;pull requests are welcome. üíö&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Banner and Title Photo by Lukas, from Pexels&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;See: https://github.com/soasis/idk/tree/main/benchmarks/closures. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See https://github.com/catchorg/Catch2/blob/devel/docs/benchmarks.md. And try it out. It‚Äôs pretty good, I just haven‚Äôt gotten off my butt to make the swap to it yet. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apple Blocks used to have an implementation in GCC that could be turned on and it used a Blocks Runtime to achieve it. But, I think it was gutted when some NeXT support and Objective-C stuff was wiped out after being unmaintained for some time. There‚Äôs been talk of reintroducing it, but obviously someone has to actually sit down and either redo it from scratch (advantageous because Apple has changed the ABI of Blocks) or try to ressurect / fix the old support for this stuff. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Apple Blocks cannot have the ‚Äúby address‚Äù capturing mechanism it has ‚Äì the&lt;/p&gt;&lt;code&gt;__block&lt;/code&gt;storage class modifier ‚Äì applied to function arguments, for some reason. So, all function arguments are de-facto copied into a Block Expression unless someone saves a tempory inside the body of the function before the Block and then uses&lt;code&gt;__block&lt;/code&gt;on that to make it a by-reference capture. ‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;It also works on a template basis in order to deduce&lt;/p&gt;&lt;code&gt;this&lt;/code&gt;‚Äì the&lt;code&gt;const auto&amp;amp;&lt;/code&gt;is a templated parameter and is usually used to do things like allow a member function to be both&lt;code&gt;const&lt;/code&gt;and non-&lt;code&gt;const&lt;/code&gt;where possible when generated. ‚Ü©&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WG14 rejected the paper last meeting, unfortunately, as not motivated enough. Funnily enough, it was immediately after this meeting that I got slammed in the face with this bug. Foresight and ‚Äúbeing prepared‚Äù is just not something even the most diehard C enthusiasts really embodies, unfortunately, and most industry vendors tend to take a more strongly conservative position over a bigger one. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46228597</guid><pubDate>Thu, 11 Dec 2025 07:21:33 +0000</pubDate></item><item><title>A ‚Äúfrozen‚Äù dictionary for Python</title><link>https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/</link><description>&lt;doc fingerprint="9f1980339ca5cbf8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A "frozen" dictionary for Python&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Dictionaries are ubiquitous in Python code; they are the data structure of choice for a wide variety of tasks. But dictionaries are mutable, which makes them problematic for sharing data in concurrent code. Python has added various concurrency features to the language over the last decade or so‚Äîasync, free threading without the global interpreter lock (GIL), and independent subinterpreters‚Äîbut users must work out their own solution for an immutable dictionary that can be safely shared by concurrent code. There are existing modules that could be used, but a recent proposal, PEP 814 ("Add frozendict built-in type"), looks to bring the feature to the language itself.&lt;/p&gt;
    &lt;p&gt;Victor Stinner announced the PEP that he and Donghee Na have authored in a post to the PEPs category of the Python discussion forum on November 13. The idea has come up before, including in PEP 416, which has essentially the same title as 814 and was authored by Stinner back in 2012. It was rejected by Guido van Rossum at the time, in part due to its target: a Python sandbox that never really panned out.&lt;/p&gt;
    &lt;head rend="h4"&gt;frozendict&lt;/head&gt;
    &lt;p&gt;The idea is fairly straightforward: add frozendict as a new immutable type to the language's builtins module. As Stinner put it:&lt;/p&gt;
    &lt;quote&gt;We expect frozendict to be safe by design, as it prevents any unintended modifications. This addition benefits not only CPython's standard library, but also third-party maintainers who can take advantage of a reliable, immutable dictionary type.&lt;/quote&gt;
    &lt;p&gt;While frozendict has a lot in common with the dict built-in type, it is not a subclass of dict; instead, it is a subclass of the base object type. The frozendict() constructor can be used to create one in various ways:&lt;/p&gt;
    &lt;quote&gt;fd = frozendict() # empty fd = frozendict(a=1, b=2) # frozen { 'a' : 1, 'b' : 2 } d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) # same l = [ ( 'a', 1 ), ( 'b', 2 ) ] fd = frozendict(l) # same fd2 = frozendict(fd) # same assert d == fd == fd2 # True&lt;/quote&gt;
    &lt;p&gt;As with dictionaries, the keys for a frozendict must be immutable, thus hashable, but the values may or may not be. For example, a list is a legitimate type for a value in either type of dictionary, but it is mutable, making the dictionary as a whole (frozen or not) mutable. However, if all of the values stored in a frozendict are immutable, it is also immutable, so it can be hashed and used in places where that is required (e.g. dictionary keys, set elements, or entries in a functools.lru_cache).&lt;/p&gt;
    &lt;p&gt;As might be guessed, based on the last line of the example above, frozen dictionaries that are hashable can be compared for equality with other dictionaries of either type. In addition, neither the hash() value nor the equality test depend on the insertion order of the dictionary, though that order is preserved in a frozen dictionary (as it is in the regular variety). So:&lt;/p&gt;
    &lt;quote&gt;d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) d2 = { 'b' : 2, 'a' : 1 } fd2 = frozendict(d2) assert d == d2 == fd == fd2 # frozendict unions work too, from the PEP &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | frozendict(y=1) frozendict({'x': 1, 'y': 1}) &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | dict(y=1) frozendict({'x': 1, 'y': 1})For the unions, a new frozen dictionary is created in both cases; the "|=" union-assignment operator also works by generating a new frozendict for the result.&lt;/quote&gt;
    &lt;p&gt; Iteration over a frozendict works as expected; the type implements the collections.abc.Mapping abstract base class, so .items() returns an iterable of key-value tuples, while .keys() and .values() provide the keys and values of the frozen dictionary. For the most part, a frozendict acts like a dict that cannot change; the specific differences between the two are listed in the PEP. It also contains a lengthy list of places in the standard library where a dict could be switched to a frozendict to "&lt;quote&gt;enhance safety and prevent unintended modifications&lt;/quote&gt;". &lt;/p&gt;
    &lt;head rend="h4"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;The reaction to the PEP was generally positive, with the usual suggestions for tweaks and more substantive additions to the proposal. Stinner kept the discussion focused on the proposal at hand for the most part. One part of the proposal was troubling to some: converting a dict to a frozendict was described as an O(n) shallow copy. Daniel F Moisset thought that it would make sense to have an in-place transformation that could be O(1) instead. He proposed adding a .freeze() method that would essentially just change the type of a dict object to frozendict.&lt;/p&gt;
    &lt;p&gt;However, changing the type of an existing object is fraught with peril, as Brett Cannon described:&lt;/p&gt;
    &lt;quote&gt;But now you have made that dictionary frozen for everyone who holds a reference to it, which means side-effects at a distance in a way that could be unexpected (e.g. context switch in a thread and now suddenly you're going to get an exception trying to mutate what was a dict a microsecond ago but is now frozen). That seems like asking for really nasty debugging issues just to optimize some creation time.&lt;/quote&gt;
    &lt;p&gt; The PEP is not aimed at performance, he continued, but is meant to help "&lt;quote&gt;lessen bugs in concurrent code&lt;/quote&gt;". Moisset noted, that dictionaries can already change in unexpected ways via .clear() or .update(), thus the debugging issues already exist. He recognized that the authors may not want to tackle that as part of the PEP, but wanted to try to ensure that an O(1) transformation was not precluded in the future. &lt;/p&gt;
    &lt;p&gt; Cannon's strong objection is to changing the type of the object directly. Ben Hsing and "Nice Zombies" proposed ways to construct a new frozendict without requiring the shallow copy‚Äîthus O(1)‚Äîby either moving the hash table to a newly created frozendict, while clearing the dictionary, or by using a copy-on-write scheme for the table. As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n), which would be a silly thing to do, but that it sometimes happens "&lt;quote&gt;because it makes people stop complaining&lt;/quote&gt;", he said in a footnote. In light of the discussion, the PEP specifically defers that optimization to a later time, suggesting that it could also be done for other frozen types (tuple and frozenset), perhaps by resurrecting PEP 351 ("The freeze protocol"). &lt;/p&gt;
    &lt;p&gt;On December 1, Stinner announced that the PEP had been submitted to the steering council for pronouncement. Given that Na is on the council, though will presumably recuse himself from deciding on this PEP, he probably has a pretty good sense for how it might be received by the group. So it seems likely that the PEP has a good chance of being approved. The availability of the free-threaded version of the language (i.e. without the GIL) means that more multithreaded Python programs are being created, so having a safe way to share dictionaries between threads will be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Dictionaries&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Python Enhancement Proposals (PEP)/PEP 814&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Dec 5, 2025 9:01 UTC (Fri) by jeeger (subscriber, #104979) [Link] (7 responses) Posted Dec 5, 2025 9:29 UTC (Fri) by taladar (subscriber, #68407) [Link] (6 responses) Posted Dec 5, 2025 9:55 UTC (Fri) by intelfx (subscriber, #130118) [Link] (5 responses) Obviously, yes. What the GP is saying is that functions that are O(1) are also, strictly speaking, O(n), since the big-O notation only defines, informally, an "upper bound" on the algorithmic complexity. (The answer here is that engineers tend to casually use the big-O notation where they really mean Knuth's Œò notation instead.) Posted Dec 7, 2025 12:29 UTC (Sun) by Baughn (subscriber, #124425) [Link] (4 responses) If he'd used uppercase-T instead, then we'd use it. Posted Dec 7, 2025 13:21 UTC (Sun) by excors (subscriber, #95769) [Link] (3 responses) As is often the case, Knuth solved that problem too, by inventing TeX half a century ago. Now we just need LWN to implement server-side KaTeX rendering. Posted Dec 7, 2025 13:57 UTC (Sun) by dskoll (subscriber, #1630) [Link] (2 responses) I solved it in a horrible way. Look up "theta" on Wikipedia, then paste the result: Œò Posted Dec 7, 2025 15:10 UTC (Sun) by adobriyan (subscriber, #30858) [Link] (1 responses) Posted Dec 10, 2025 1:56 UTC (Wed) by raven667 (subscriber, #5198) [Link] Posted Dec 5, 2025 11:45 UTC (Fri) by iabervon (subscriber, #722) [Link] (3 responses) Next, I want a flag to json.loads() that causes it to return hashable values instead of mutable ones (without the caller needing to know how to accomplish that). Posted Dec 6, 2025 0:49 UTC (Sat) by AdamW (subscriber, #48457) [Link] (2 responses) It says frozendicts will be ordered, but hashes and comparisons will not care about the order. So frozendict({"a": "b", "c": "d"}) and frozendict({"c": "d", "a": "b"}) will have the same hash and compare as equal, but they're not really the same? I don't know how I feel about that! Posted Dec 6, 2025 5:02 UTC (Sat) by NYKevin (subscriber, #129325) [Link] Whether this is a problem is debatable, but it is also moot. Non-frozen dicts have behaved this way forever, so making frozendict behave differently would be pretty terrible language design. Posted Dec 6, 2025 5:23 UTC (Sat) by iabervon (subscriber, #722) [Link] The history is that the iterator order used to be unpredictable, so the same object might give different orders when traversed multiple times and objects constructed by adding the items in different order might give the same order when traversed multiple times. However, a more recent implementation of dict started to traverse the items in the order the keys were first added, just because that was more convenient, and then the language changed to guarantee this. Of course, that meant that there was now something you could reliably determine about dicts that wasn't included in the equality rules that had always existed. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;quote&gt; As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n) &lt;/quote&gt; I might be misremembering from my Uni days, but all O(1) algorithms are also O(n), so the statement doesn't make sense. I'd be happy for someone to correct me though. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46229467</guid><pubDate>Thu, 11 Dec 2025 09:51:47 +0000</pubDate></item><item><title>French supermarket's Christmas advert is worldwide hit (without AI) [video]</title><link>https://www.youtube.com/watch?v=Na9VmMNJvsA</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46231187</guid><pubDate>Thu, 11 Dec 2025 13:35:55 +0000</pubDate></item><item><title>Craft software that makes people feel something</title><link>https://rapha.land/craft-software-that-makes-people-feel-something/</link><description>&lt;doc fingerprint="2d5e191f3bee3c11"&gt;
  &lt;main&gt;
    &lt;p&gt;So, I woke up today. Got my coffee, family went to sleep, and I have a free afternoon.&lt;/p&gt;
    &lt;p&gt;I thought about writing something. I may delete this article, but if you are reading this, it means I went through with it.&lt;/p&gt;
    &lt;p&gt;Recently, people have been asking me why I‚Äôm pausing Boo to work on a programming language. I think it would actually be cool to write down how I feel.&lt;/p&gt;
    &lt;p&gt;Boo is a code editor I created solely for myself; I never had the intention of making it a mainstream editor. Of course, it would be fun if people used it, but that was never my goal. This year I got it working in a functional state, where I can actually use it for my daily work. It has innovative human-keyboard navigation and replaces the LSP system with something faster and less costly for the OS. So why on earth am I not open-sourcing it? That‚Äôs what people keep asking me.&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs go step by step.&lt;/p&gt;
    &lt;p&gt;My mind isn‚Äôt really moved by the idea that it would be a success or a failure ‚Äî the end user of Boo is me. I don‚Äôt feel it‚Äôs there yet; in fact, I think software should inspire us. Working on Rio Terminal and Boo in my free time ‚Äî both written in Rust and sharing many similarities ‚Äî affects my joy, because it starts to become something automatic. Both have similar architecture, language, release process, and etcetera.&lt;/p&gt;
    &lt;p&gt;Since I was a kid, I liked to build Lego blocks. That‚Äôs probably what I did the most besides playing football or video games. The fun thing about Lego is that one day you can build a castle, and the next day you can build a ship. Not necessarily using the same pieces and colors ‚Äî you can actually add a lot of stuff that‚Äôs external to what you have, like a wood stick.&lt;/p&gt;
    &lt;p&gt;When programming becomes repetitive, the odds of you creating something that makes people go ‚Äúwow‚Äù are reduced quite a bit. It isn‚Äôt a rule, of course. You need to be inspired to make inspiring software.&lt;/p&gt;
    &lt;p&gt;I always use the example of The Legend of Zelda: Breath of the Wild. This game is so well crafted that I know people who don‚Äôt even like video games but bought a console just to play it ‚Äî and once they finished, they sold everything. This is what I‚Äôm talking about: taking time to build something so that once people try it, they remember it for as long as they live.&lt;/p&gt;
    &lt;p&gt;Boo isn‚Äôt a business. I don‚Äôt need or want to make money out of it. I don‚Äôt have a deadline, nor do I want to create another VS Code. I don‚Äôt feel like forcing it to happen.&lt;/p&gt;
    &lt;p&gt;In that case, I don‚Äôt necessarily need to stop building Lego blocks, right? I‚Äôll just park it there, and when the inspiration comes back, I‚Äôll pick it up where it was. That being said, I paused Boo, and I am working on my own programming language. Eventually, my idea is to rewrite Boo to use it.&lt;/p&gt;
    &lt;p&gt;‚ÄúWow! That‚Äôs a lot of work.‚Äù Indeed. But it‚Äôs my hobby stuff. I‚Äôve always loved programming languages, and I am having a blast learning more about binaries and compilers. So, I don‚Äôt really feel I need to follow people‚Äôs cake recipe for success. That‚Äôs how my mind works, and I will stick with it.&lt;/p&gt;
    &lt;p&gt;By the way, this article was written using Boo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46231274</guid><pubDate>Thu, 11 Dec 2025 13:45:08 +0000</pubDate></item><item><title>The Walt Disney Company and OpenAI Partner on Sora</title><link>https://openai.com/index/disney-sora-agreement/</link><description>&lt;doc fingerprint="ad612f58d4a4adfb"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;As part of this three-year licensing agreement, Sora will be able to generate short, user-prompted social videos that can be viewed and shared by fans, drawing on more than 200 Disney, Marvel, Pixar and Star Wars characters.&lt;/item&gt;
      &lt;item&gt;Agreement will make a selection of these fan-inspired Sora short form videos available to stream on Disney+.&lt;/item&gt;
      &lt;item&gt;Disney and OpenAI affirm a shared commitment to responsible use of AI that protects the safety of users and the rights of creators.&lt;/item&gt;
      &lt;item&gt;Alongside the licensing agreement, Disney will become a major customer of OpenAI, using its APIs to build new products, tools, and experiences, including for Disney+, and deploying ChatGPT for its employees.&lt;/item&gt;
      &lt;item&gt;As part of the agreement, Disney will make a $1 billion equity investment in OpenAI, and receive warrants to purchase additional equity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Walt Disney Company and OpenAI have reached an agreement for Disney to become the first major content licensing partner on Sora, OpenAI‚Äôs short-form generative AI video platform, bringing these leaders in creativity and innovation together to unlock new possibilities in imaginative storytelling.&lt;/p&gt;
    &lt;p&gt;As part of this new, three-year licensing agreement, Sora will be able to generate short, user-prompted social videos that can be viewed and shared by fans, drawing from a set of more than 200 animated, masked and creature characters from Disney, Marvel, Pixar and Star Wars, including costumes, props, vehicles, and iconic environments. In addition, ChatGPT Images will be able to turn a few words by the user into fully generated images in seconds, drawing from the same intellectual property. The agreement does not include any talent likenesses or voices.&lt;/p&gt;
    &lt;p&gt;Alongside the licensing agreement, Disney will become a major customer of OpenAI, using its APIs to build new products, tools, and experiences, including for Disney+, and deploying ChatGPT for its employees.&lt;/p&gt;
    &lt;p&gt;As part of the agreement, Disney will make a $1 billion equity investment in OpenAI, and receive warrants to purchase additional equity.&lt;/p&gt;
    &lt;p&gt;Under the agreement, Disney and OpenAI are affirming a shared commitment to the responsible use of AI that protects user safety and the rights of creators. Together, the companies will advance human-centered AI that respects the creative industries and expands what is possible for storytelling.&lt;/p&gt;
    &lt;p&gt;The transaction is subject to the negotiation of definitive agreements, required corporate and board approvals, and customary closing conditions.&lt;/p&gt;
    &lt;p&gt;‚ÄúTechnological innovation has continually shaped the evolution of entertainment, bringing with it new ways to create and share great stories with the world,‚Äù said Robert A. Iger, CEO, The Walt Disney Company. ‚ÄúThe rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works. Bringing together Disney‚Äôs iconic stories and characters with OpenAI‚Äôs groundbreaking technology puts imagination and creativity directly into the hands of Disney fans in ways we‚Äôve never seen before, giving them richer and more personal ways to connect with the Disney characters and stories they love.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúDisney is the global gold standard for storytelling, and we‚Äôre excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content,‚Äù said Sam Altman, co-founder and CEO of OpenAI. ‚ÄúThis agreement shows how AI companies and creative leaders can work together responsibly to promote innovation that benefits society, respect the importance of creativity, and help works reach vast new audiences.‚Äù&lt;/p&gt;
    &lt;p&gt;Under the license, fans will be able to watch curated selections of Sora-generated videos on Disney+, and OpenAI and Disney will collaborate to utilize OpenAI‚Äôs models to power new experiences for Disney + subscribers, furthering innovative and creative ways to connect with Disney‚Äôs stories and characters. Sora and ChatGPT Images are expected to start generating fan-inspired videos with Disney‚Äôs multi-brand licensed characters in early 2026.&lt;/p&gt;
    &lt;p&gt;Among the characters fans will be able to use in their creations are Mickey Mouse, Minnie Mouse, Lilo, Stitch, Ariel, Belle, Beast, Cinderella, Baymax, Simba, Mufasa, as well as characters from the worlds of Encanto, Frozen, Inside Out, Moana, Monsters Inc., Toy Story, Up, Zootopia, and many more; plus iconic animated or illustrated versions of Marvel and Lucasfilm characters like Black Panther, Captain America, Deadpool, Groot, Iron Man, Loki, Thor, Thanos, Darth Vader, Han Solo, Luke Skywalker, Leia, the Mandalorian, Stormtroopers, Yoda and more.&lt;/p&gt;
    &lt;p&gt;As part of the agreement, OpenAI has committed to continuing its industry leadership in implementing responsible measures to further address trust and safety, including age-appropriate policies and other reasonable controls across the service. In addition, OpenAI and Disney have affirmed a shared commitment to maintaining robust controls to prevent the generation of illegal or harmful content, to respect the rights of content owners in relation to the outputs of models, and to respect the rights of individuals to appropriately control the use of their voice and likeness.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46231493</guid><pubDate>Thu, 11 Dec 2025 14:05:16 +0000</pubDate></item><item><title>Disney making $1B investment in OpenAI, will allow characters on Sora AI</title><link>https://www.cnbc.com/2025/12/11/disney-openai-sora-characters-video.html</link><description>&lt;doc fingerprint="a5259f189c24f9fa"&gt;
  &lt;main&gt;
    &lt;p&gt;The Walt Disney Company on Thursday announced it will make a $1 billion equity investment in OpenAI and will allow users to make videos with its copyrighted characters on its Sora app.&lt;/p&gt;
    &lt;p&gt;OpenAI launched Sora in September, and it allows users to create short videos by simply typing in a prompt.&lt;/p&gt;
    &lt;p&gt;As part of the startup's new three-year licensing agreement with Disney, Sora users will be able make content with more than 200 characters across Disney, Marvel, Pixar and Star Wars starting next year.&lt;/p&gt;
    &lt;p&gt;"The rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works," Disney CEO Bob Iger said in a statement.&lt;/p&gt;
    &lt;p&gt;Tune in at 10:30 a.m. ET as Disney CEO Bob Iger and OpenAI CEO Sam Altman joins CNBC TV to discuss the media giant's investment. Watch in real time on CNBC+ or the CNBC Pro stream.&lt;/p&gt;
    &lt;p&gt;As part of the agreement, Disney said it will receive warrants to purchase additional equity and will become a major OpenAI customer.&lt;/p&gt;
    &lt;p&gt;Disney is deploying OpenAI's chatbot ChatGPT to its employees and will work with its technology to build new tools and experiences, according to a release.&lt;/p&gt;
    &lt;p&gt;When Sora launched this fall, the app rocketed to the top of Apple's App Store and generated a storm of controversy as users flooded the platform with videos of popular brands and characters.&lt;/p&gt;
    &lt;p&gt;The Motion Picture Association said in October that OpenAI needed to take "immediate and decisive action" to prevent copyright infringement on Sora.&lt;/p&gt;
    &lt;p&gt;OpenAI CEO Sam Altman said more "granular control" over character generation was coming, according to a blog post following the launch.&lt;/p&gt;
    &lt;p&gt;As AI startups have rapidly changed the way that people can interact with content online, media companies, including Disney, have kicked off a series of fresh legal battles to try and protect their intellectual property.&lt;/p&gt;
    &lt;p&gt;Disney sent a cease and desist letter to Google late on Wednesday alleging the company infringed its copyrights on a "massive scale." In the letter, which was viewed by CNBC, Disney said Google has been using its copyrighted works to train models and distributing copies of its protected content without authorization.&lt;/p&gt;
    &lt;p&gt;Universal and Disney have sued the AI image creator Midjourney, alleging that the company improperly used and distributed AI-generated characters from their movies. Disney also sent a cease and desist letter to Character.AI in September, warning the startup to stop using its copyrighted characters without authorization.&lt;/p&gt;
    &lt;p&gt;Disney's deal with OpenAI suggests the company isn't ruling out AI platforms entirely.&lt;/p&gt;
    &lt;p&gt;The companies said they have affirmed a commitment to the use of AI that "protects user safety and the rights of creators" and "respects the creative industries," according to the release.&lt;/p&gt;
    &lt;p&gt;OpenAI has also agreed to maintain "robust controls" to prevent illegal or harmful content from being generated on its platforms.&lt;/p&gt;
    &lt;p&gt;Some of the characters available through the deal include Mickey Mouse, Ariel, Cinderella, Iron Man and Darth Vader. Disney and OpenAI said the agreement does not include any talent likeness or voices.&lt;/p&gt;
    &lt;p&gt;Users will also be able to draw from the same intellectual property while using ChatGPT Images, where they can use natural language prompts to create images.&lt;/p&gt;
    &lt;p&gt;"Disney is the global gold standard for storytelling, and we're excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content," Altman said in a statement.&lt;/p&gt;
    &lt;p&gt;Curated selections of Sora videos will also be available to watch on Disney's streaming platform Disney+.&lt;/p&gt;
    &lt;p&gt;WATCH: We tested OpenAI‚Äôs Sora 2 AI-video app to find out why Hollywood is worried&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46231585</guid><pubDate>Thu, 11 Dec 2025 14:12:14 +0000</pubDate></item></channel></rss>