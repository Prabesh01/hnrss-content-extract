<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 21 Nov 2025 17:09:07 +0000</lastBuildDate><item><title>Android and iPhone users can now share files, starting with the Pixel 10</title><link>https://blog.google/products/android/quick-share-airdrop/</link><description>&lt;doc fingerprint="afecea04cdf1cf3c"&gt;
  &lt;main&gt;
    &lt;p&gt;When it comes to sharing moments between family and friends, what device you have shouldn’t matter — sharing should just work. But we’ve heard from many people that they want a simpler way to share files between devices.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing a way for Quick Share to work with AirDrop. This makes file transfer easier between iPhones and Android devices, and starts rolling out today to the Pixel 10 family.&lt;/p&gt;
    &lt;p&gt;We built this with security at its core, protecting your data with strong safeguards that were tested by independent security experts. It’s just one more way we’re bringing better compatibility that people are asking for between operating systems, following our work on RCS and unknown tracker alerts.&lt;/p&gt;
    &lt;p&gt;We’re looking forward to improving the experience and expanding it to more Android devices. See it in action on the Pixel 10 Pro in this video, and try it out for yourself!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45994854</guid><pubDate>Thu, 20 Nov 2025 17:04:34 +0000</pubDate></item><item><title>Show HN: Search London StreetView panoramas by text</title><link>https://london.publicinsights.uk</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45995913</guid><pubDate>Thu, 20 Nov 2025 18:27:51 +0000</pubDate></item><item><title>New OS aims to provide (some) compatibility with macOS</title><link>https://github.com/ravynsoft/ravynos</link><description>&lt;doc fingerprint="c7963a51c657afa6"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Don't speak English? Read this in: Italiano, Türkçe, Deutsch, Indonesia, 简体中文, 繁體中文, Português do Brasil, 한국어, فارسی, Magyar&lt;/head&gt;
    &lt;p&gt;ravynOS is a new open source OS project that aims to provide a similar experience and some compatibility with macOS on x86-64 (and eventually ARM) systems. It builds on the solid foundations of FreeBSD, existing open source packages in the same space, and new code to fill the gaps.&lt;/p&gt;
    &lt;p&gt;The main design goals are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source compatibility with macOS applications (i.e. you could compile a Mac application on ravynOS and run it)&lt;/item&gt;
      &lt;item&gt;Similar GUI metaphors and familiar UX (file manager, application launcher, top menu bar that reflects the open application, etc)&lt;/item&gt;
      &lt;item&gt;Compatible with macOS folder layouts (/Library, /System, /Users, /Volumes, etc) and perhaps filesystems (HFS+, APFS) as well as fully supporting ZFS&lt;/item&gt;
      &lt;item&gt;Self-contained applications in App Bundles, AppDirs, and AppImage files - an installer-less experience for /Applications&lt;/item&gt;
      &lt;item&gt;Mostly maintain compatibility with the FreeBSD base system and X11 - a standard Unix environment under the hood&lt;/item&gt;
      &lt;item&gt;Compatible with Linux binaries via FreeBSD's Linux support&lt;/item&gt;
      &lt;item&gt;Eventual compatibility with x86-64/arm64 macOS binaries (Mach-O) and libraries&lt;/item&gt;
      &lt;item&gt;Pleasant to use, secure, stable, and performant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please visit ravynos.com for more info: Release Notes | Screenshots | FAQ&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can you help build the dream? See the current projects/needs in CONTRIBUTING.md!&lt;/item&gt;
      &lt;item&gt;Our Discord server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;#ravynOS-general:matrix.org&lt;/code&gt;- join via Element.io&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the top level of the FreeBSD source directory.&lt;/p&gt;
    &lt;p&gt;FreeBSD is an operating system used to power modern servers, desktops, and embedded platforms. A large community has continually developed it for more than thirty years. Its advanced networking, security, and storage features have made FreeBSD the platform of choice for many of the busiest web sites and most pervasive embedded networking and storage devices.&lt;/p&gt;
    &lt;p&gt;For copyright information, please see the file COPYRIGHT in this directory. Additional copyright information also exists for some sources in this tree - please see the specific source directories for more information.&lt;/p&gt;
    &lt;p&gt;The Makefile in this directory supports a number of targets for building components (or all) of the FreeBSD source tree. See build(7), config(8), FreeBSD handbook on building userland, and Handbook for kernels for more information, including setting make(1) variables.&lt;/p&gt;
    &lt;p&gt;For information on the CPU architectures and platforms supported by FreeBSD, see the FreeBSD website's Platforms page.&lt;/p&gt;
    &lt;p&gt;For official FreeBSD bootable images, see the release page.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Directory&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bin&lt;/cell&gt;
        &lt;cell&gt;System/user commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;cddl&lt;/cell&gt;
        &lt;cell&gt;Various commands and libraries under the Common Development and Distribution License.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;contrib&lt;/cell&gt;
        &lt;cell&gt;Packages contributed by 3rd parties.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;crypto&lt;/cell&gt;
        &lt;cell&gt;Cryptography stuff (see crypto/README).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;etc&lt;/cell&gt;
        &lt;cell&gt;Template files for /etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gnu&lt;/cell&gt;
        &lt;cell&gt;Commands and libraries under the GNU General Public License (GPL) or Lesser General Public License (LGPL). Please see gnu/COPYING and gnu/COPYING.LIB for more information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;include&lt;/cell&gt;
        &lt;cell&gt;System include files.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;kerberos5&lt;/cell&gt;
        &lt;cell&gt;Kerberos5 (Heimdal) package.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;lib&lt;/cell&gt;
        &lt;cell&gt;System libraries.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;libexec&lt;/cell&gt;
        &lt;cell&gt;System daemons.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;release&lt;/cell&gt;
        &lt;cell&gt;Release building Makefile &amp;amp; associated tools.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rescue&lt;/cell&gt;
        &lt;cell&gt;Build system for statically linked /rescue utilities.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sbin&lt;/cell&gt;
        &lt;cell&gt;System commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;secure&lt;/cell&gt;
        &lt;cell&gt;Cryptographic libraries and commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;share&lt;/cell&gt;
        &lt;cell&gt;Shared resources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stand&lt;/cell&gt;
        &lt;cell&gt;Boot loader sources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sys&lt;/cell&gt;
        &lt;cell&gt;Kernel sources (see sys/README.md).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;targets&lt;/cell&gt;
        &lt;cell&gt;Support for experimental &lt;code&gt;DIRDEPS_BUILD&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tests&lt;/cell&gt;
        &lt;cell&gt;Regression tests which can be run by Kyua. See tests/README for additional information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tools&lt;/cell&gt;
        &lt;cell&gt;Utilities for regression testing and miscellaneous tasks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;usr.bin&lt;/cell&gt;
        &lt;cell&gt;User commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;usr.sbin&lt;/cell&gt;
        &lt;cell&gt;System administration commands.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For information on synchronizing your source tree with one or more of the FreeBSD Project's development branches, please see FreeBSD Handbook.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45997212</guid><pubDate>Thu, 20 Nov 2025 20:24:42 +0000</pubDate></item><item><title>Over-regulation is doubling the cost</title><link>https://rein.pk/over-regulation-is-doubling-the-cost</link><description>&lt;doc fingerprint="698010174ea1b620"&gt;
  &lt;main&gt;
    &lt;p&gt;After building a software company to a multi-billion dollar exit, I made the jump to hardware. Now I’m working on carbon removal + steel at Charm Industrial, and electric long-haul trucking with Revoy. It’s epically fun to be building in the real world, but little did I expect that more than half the cost of building a hardware company would come from regulatory bottlenecks. Despite a huge push for climate fixes and the bipartisan geopolitical desire to bring industry back to the USA, I’ve been shocked to find that the single biggest barrier—by far—is over-regulation from the massive depth of bureaucracy.&lt;/p&gt;
    &lt;p&gt;Hardtech companies of all flavors are being forced to burn through limited capital while they wait for regulatory clarity and/or permits. This creates a constant cycle of cost increases that ultimately flows to consumers, it lowers investment in the US manufacturing and industrial base, it delays innovative new hardware getting into the hands of consumers and businesses, and at the end of the day, it leaves us all worse off, stuck with a quality of life pegged to technology developed decades ago.&lt;/p&gt;
    &lt;p&gt;Regulatory delays and bottlenecks have added millions of pounds of pollutants like PM2.5, NOₓ and CO₂ to our air from the continuation of business as usual, instead of the deployment of clean technologies from my two hardtech efforts alone. While CO₂ is a long-term climate issue, PM2.5 and NOₓ are immediate major drivers of asthma and excess morbidity. Both operations have high bipartisan appeal—and we’ve never been denied a permit—because we’re fundamentally cleaning up things that matter to everyone: dirty air, wildfires, orphaned oil wells. Revoy is also helping deflate the cost of long-haul freight. But none of that has made getting freedom to operate easy. For creative new technologies the default answer is “no” because there isn’t a clear path to permitting at all, and figuring out that path itself takes years — time that startups can’t afford to wait.&lt;/p&gt;
    &lt;p&gt;Regulation obviously has a critical role in protecting people and the environment, but the sheer volume, over-specificity and sometimes ambiguity of those same regulations is now actively working against those goals! We’re unintentionally blocking the very things that would improve our environment. We’ve become a society that blocks all things, and we need to be a society that builds great things every day. The rest of this article gets very specific about the astronomical costs regulations are imposing on us as a society, and the massive positive impact that could be unleashed by cutting back regulation that is working against new, cost-saving, creative technology that could also be making people and the environment healthy again.&lt;/p&gt;
    &lt;p&gt;To make it concrete: both Charm and Revoy are capital-efficient hardtech companies, but Charm will spend low hundreds of millions to get to breakeven, and Revoy will spend tens of millions. In both cases, more than half of the total cost of building each company has gone to counterproductive regulatory burden. I’m hellbent on pushing through these barriers, but the unspoken reality is that our regulatory morass is the deathbed of thousands of hardtech companies that could be drastically improving our lives. We must unleash them.&lt;/p&gt;
    &lt;head rend="h2"&gt;$300M in Societal Cost &amp;amp; $125M in Burden for Charm&lt;/head&gt;
    &lt;p&gt;Charm produces and delivers verified carbon removal to companies like Google, Microsoft and JPMorgan. Charm’s breakthrough was realizing that you could take CO₂ captured in farm &amp;amp; forestry plant residues, convert it into a carbon-rich, BBQ sauce-like liquid (it’s literally the smoke flavor in BBQ sauce), and inject it into old oil wells to permanently remove carbon from the atmosphere. This has all kinds of co-benefits like reducing the massive overburden of wildfire fuels, cleaning up &amp;amp; plugging nasty orphaned oil wells, and improving PM2.5 and NOₓ air quality by avoiding that biomass being burned instead.&lt;/p&gt;
    &lt;p&gt;And yet… there was a hangup: what kind of injection well is this? Should it be permitted as a Class I disposal, Class II oilfield disposal, or Class V experimental? This question on permitting path took four years to answer. Four years to decide which path to use, not even the actual permit! It took this long because regulators are structurally faced with no upside, only downside legal risk in taking a formal position on something new. Even when we’d done an enormous amount of lab and field work with bio-oil to understand its safety and behavior at surface and subsurface conditions. A regulator faces little cost to moving incredibly cautiously, but a major cost if they approve something that triggers activist pushback.&lt;/p&gt;
    &lt;p&gt;In the end, we’re grateful that—eventually—a state regulator took the reins and reviewed, managed, and issued the first-ever Class V bio-oil sequestration permit, through what was still an incredibly complex and detailed 14-month review process.&lt;/p&gt;
    &lt;p&gt;Now imagine that, instead of the 5.5 years from first contact to issued permit, it had only taken the 6 months it actually required to get everyone across the regulatory establishment to agree on a Class V pathway, we would have had 5 additional years operating the well. That’s the equivalent, from our real supply chain, of sinking at least 30,000 tonnes of carbon per year at $600/tonne. Looking only at this one aspect, this delay came with a $90M price tag for Charm. We’ve also spent untold millions on regulatory affairs at all levels of government, not to mention the missed acceleration in sales, and other direct hard costs spent in R&amp;amp;D and processing bio-oil for inefficient and expensive injection into salt caverns instead.&lt;/p&gt;
    &lt;p&gt;But the public health burden created by this regulatory slowness is where it gets really crazy. This one regulatory delay meant we all got subjected to decreased air quality from an additional 30,000 tonnes per year of pile burning. The resulting particulate emissions alone are estimated to have caused a mindblowing $40m/year in healthcare costs. This is $200M in additional healthcare burden over those five years, mostly borne by Medicare and Medicaid. There are additional costs to NOₓ emissions and more that take it to $300M.&lt;/p&gt;
    &lt;p&gt;In total, the total cost to society of this single regulatory delay will be about $400M: $120-150M of unnecessary cost to Charm, and the bulk of it—$300M or so—borne by the public in healthcare costs. I’m not sharing these numbers to complain or make excuses; Charm is still on the path to having a huge impact and we’re among the lucky few that can survive these delays. What pains me most is the 5 years of lost carbon removal and pollutant reduction, and the compounding effect that has on all our health and healthcare costs. Over-regulation is now working against the very things it’s intended to protect.&lt;/p&gt;
    &lt;p&gt;Regulators do their absolute best with the system they have, but the combined effects of: (1) extremely detailed and complex regulation, (2) chaotic budgets and understaffing that disrupt an efficient process, and (3) endless lawsuits against regulators since 1970s-era Naderism have created an atmosphere of fear. If we want to solve the climate crisis, build abundance, lower costs, and generate wealth for all, this has to change. We need to delete and simplify reams of regulations. We need to pay regulators well, and we need to trust our regulators to operate quickly and decisively by putting reasonable limits on endless activist legal challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;&amp;gt;$25M in Unnecessary Burden for Revoy&lt;/head&gt;
    &lt;p&gt;Revoy’s breakthrough was realizing that you could lower long-haul freight costs and electrify long-haul semi trucks by leaving the diesel tractor in place and dropping an electric powertrain onto the back of the semi. Today, we boost semis from 7 mpg to 120 mpg, driving a 94% reduction in fuel consumption. This slashes emissions that negatively impact both air quality and climate.&lt;/p&gt;
    &lt;p&gt;And yet again… a hangup: what exactly is this electric doohickey? Is it a truck? A trailer? Something else? It was clear from the regulations that it was a “converter dolly”. But getting complete alignment on that simple fact across an alphabet soup of government agencies spanning both federal and state—NHTSA, FMCSA, FHWA, state transit authorities, air quality management districts, state DMVs, highway patrols and more—took years.&lt;/p&gt;
    &lt;p&gt;A “powered converter dolly” isn’t even a new thing! Here’s one from the sixties that ran on diesel to help trucks get over mountain passes:&lt;/p&gt;
    &lt;p&gt;There were some bright spots. The Federal Motor Carrier Safety Administration (FMCSA) and the National Highway Transportation Safety Administration (NHTSA) quickly converged on informal definitional clarity, and then eventually a Highway Patrol Captain who was eager to get innovative electric vehicles on the road pushed it through with a state DMV to register the first four Revoys. But bringing along the rest of the agencies, and the rest of the states, was not fast. It delayed deployments, soaked up hundreds of thousands of dollars of legal and lobbyist time (not to mention all the corresponding time on the government side that all of us taxpayers have to bear), and maybe most importantly… even with a formal memo from the Federal DOT, it is still not 100% resolved in some states.&lt;/p&gt;
    &lt;p&gt;As one example, one state agency has asked Revoy to do certified engine testing to prove that the Revoy doesn’t increase emissions of semi trucks. And that Revoy must do this certification across every single truck engine family. It costs $100,000 per certification and there are more than 270 engine families for the 9 engines that our initial partners use. That’s $27,000,000 for this one regulatory item. And keep in mind that this is to certify that a device—whose sole reason for existence is to cut pollution by &amp;gt;90%, and which has demonstrably done so across nearly 100,000 miles of testing and operations—is not increasing the emissions of the truck. It’s a complete waste of money for everyone.&lt;/p&gt;
    &lt;p&gt;And that $27M dollar cost doesn’t include the cost to society. This over-regulation will delay deployment of EV trucks by years, increasing NOₓ and PM 2.5 air pollution exposure for many of society’s least well-off who live near freeways. The delayed deployment will also increase CO₂ emissions that threaten the climate and environment. Revoy’s Founder (Ian Rust) and I actually disagree on what exactly it is about the regulatory environment that needs to change, but we agree it’s completely broken and hurting both people and the planet.&lt;/p&gt;
    &lt;p&gt;In every interaction I have with regulators, I’m reminded that they’re good people doing god’s work operating in a fundamentally broken system. A regulatory system that structurally insists on legalistic, ultra-extreme caution is bound to generate a massive negative return for society.&lt;/p&gt;
    &lt;p&gt;If we had a regulatory system that could move fast to experiment with creative new technologies, we’d live in a world where our environment gets cleaned up faster, where awesome new hardware was constantly improving our lives by making things better and cheaper, and where large-scale hardtech innovation happened here at home in the USA, not in China.&lt;/p&gt;
    &lt;p&gt;As we collectively work to build more manufacturing capacity at home and build the next wave of technologies to power the economy, we need to grapple with the real bottlenecks holding us back. I hope other hardtech founders will publicly share more of their stories as well (the stories I’ve heard in private would shock you). Props to Blake Scholl for doing so.&lt;/p&gt;
    &lt;p&gt;We need a come-to-jesus about regulatory limits, timelines, and scope. Yes, we need basic and strong protections for clear harms, but we need to unleash every hardworking American, not just a few companies with massive funding, to invent and build hardware again. We need to combine many approaches to get there: expedited reviews for new technology, freedom to operate by default, permits by right-not-process, deleting as many regulatory steps as possible, and more. CA YIMBY’s successful push to pass a deluge of housing acceleration laws in the past two years could serve as a model. America building things again is the foundation of a prosperous, powerful, and clean America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45999038</guid><pubDate>Thu, 20 Nov 2025 22:58:06 +0000</pubDate></item><item><title>Olmo 3: Charting a path through the model flow to lead open-source AI</title><link>https://allenai.org/blog/olmo3</link><description>&lt;doc fingerprint="ad5e3b78241b8f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Olmo 3: Charting a path through the model flow to lead open-source AI&lt;/head&gt;
    &lt;p&gt;November 20, 2025&lt;/p&gt;
    &lt;p&gt;Ai2&lt;/p&gt;
    &lt;p&gt;Language models are often treated as snapshots—brief captures of a long and carefully curated development process. But sharing only the end result obscures the rich context needed to modify, adapt, and extend a model's capabilities. Many meaningful adjustments require integrating domain-specific knowledge deep within the development pipeline, not merely at the final stage. To truly advance open AI development and research, the entire model flow – not just its endpoint – should be accessible and customizable. The model flow is the full lifecycle of an LM: every stage, checkpoint, dataset, and dependency required to create and modify it. By exposing this complete process, the goal is to engender greater trust and enable more effective adaptation, collaboration, and innovation.&lt;/p&gt;
    &lt;p&gt;With today's release of Olmo 3, we're empowering the open source community with not only state-of-the-art open models, but the entire model flow and full traceability back to training data.&lt;/p&gt;
    &lt;p&gt;At its center is Olmo 3-Think (32B), the best fully open 32B-scale thinking model that for the first time lets you inspect intermediate reasoning traces and trace those behaviors back to the data and training decisions that produced them. Olmo 3 is a family of compact, dense models at 7 billion and 32 billion parameters that can run on everything from laptops to research clusters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo 3-Base (7B, 32B) is our most powerful base model yet. When evaluated on our expanded, diverse evaluation suite, Olmo 3-Base delivers the strongest performance among fully open base models – where training data, code, and weights are all publicly available, like Stanford's Marin and Swiss AI's Apertus – and achieves competitive performance with some of the best open-weights base models of comparable size and architecture, including Qwen 2.5 and Gemma 3. Achieving strong results in programming, reading comprehension, and math problem solving, Olmo 3-Base maintains performance at extended context lengths (~up to 65K tokens)—providing a versatile foundation for continued pretraining, targeted fine-tuning, and reinforcement learning and making it easy to build in specialized capabilities like reasoning, tool use (function calling), and instruction following through post-training.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Think (7B, 32B) is our flagship post-trained reasoning set built on Olmo 3-Base. At a time when few organizations are releasing truly open models at this scale, Olmo 3-Think (32B) serves as a workhorse for RL research, long-horizon reasoning, and other advanced experiments that require substantial compute. On our suite of reasoning benchmarks (discussed below), it's the strongest fully open thinking model we're aware of, narrowing the gap to the best open-weight models of similar scale – such as Qwen 3 32B – while training on roughly 6x fewer tokens. Olmo 3-Think (7B) brings the same design and training approach to an even more efficient form factor, surfacing intermediate thinking steps for complex prompts while making open, inspectable reasoning accessible on more modest hardware.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Instruct (7B) is a chat and quick-response focused post-train of Olmo 3-Base that handles multi-turn, instruction-following, tool use, and more. In our evaluations, it matches or outperforms open-weight models including Qwen 2.5, Gemma 3, and Llama 3.1, and narrows the gap with Qwen 3 model families at a similar scale—delivering a strong, fully open alternative for high-quality conversational and tool-using agents.&lt;/item&gt;
      &lt;item&gt;Olmo 3-RL Zero (7B), is a fully open reinforcement learning pathway built on Olmo 3-Base, designed to bootstrap complex reasoning behaviors and enable clear benchmarking of RL algorithms. We release four series of checkpoints from domain-focused training on math, code, instruction following, and general chat, enabling careful study of reinforcement learning with verifiable rewards (RLVR).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of a single set of frozen weights, Olmo 3 offers multiple, fully documented paths through development: the Instruct path for everyday chat and tool use, the RL Zero path for RL experimentation from base models, and the Think/reasoning path for models that leverage inference-time scaling to unlock complex reasoning and agentic behaviors. Each path is a concrete example of how to shape behavior from the same base model, and you’re free to fork or remix them—start with Olmo 3-Base, explore your own supervised fine-tuning (SFT) or direct preference optimization (DPO) recipe for instruct-style use cases, or plug in a new RL objective to probe different tradeoffs. The flow itself becomes a rich, reusable object—not just a record of how we built Olmo 3, but a scaffold for how you can build your own systems.&lt;/p&gt;
    &lt;p&gt;Explore the Model Flow&lt;/p&gt;
    &lt;p&gt;Click on any stage to learn more about it and download artifacts.&lt;/p&gt;
    &lt;p&gt;The Olmo 3 checkpoints we're releasing represent our initial paths targeting our goals around reasoning, tool use, and general capabilities – we have exciting plans for other ways to leverage Olmo 3-Base 32B. But because we're releasing the entire flow, you can intervene at any point: swap in domain-specific data during mid-training, adjust post-training for your use case, or build on an earlier checkpoint that better suits your needs.&lt;/p&gt;
    &lt;p&gt;As with Olmo and Olmo 2, we’re releasing all components of the Olmo 3 flow – data, code, model weights, and checkpoints – under permissive open source licenses.&lt;/p&gt;
    &lt;p&gt;Try Olmo 3 | Download the models &amp;amp; data | Read the report&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong performance across the board&lt;/head&gt;
    &lt;p&gt;We run the Olmo 3 checkpoints through a broad, updated benchmark suite, grouping dozens of industry-standard tasks (plus a few new ones we introduce) into several capability clusters. Together, the clustered suite and these held-out tasks give us a capability profile of Olmo 3—a clear picture of how well it solves math problems, codes, uses tools, answers general-knowledge questions, and more.&lt;/p&gt;
    &lt;p&gt;At a high level, the Olmo 3 family delivers the strongest fully open base and thinking models we’re aware of. Olmo 3-Base 32B outperforms other fully open base models, and Olmo 3-Think 32B emerges as the strongest fully open thinking model.&lt;/p&gt;
    &lt;p&gt;Our results were made possible by rigorous data curation at every stage of training, a carefully designed training recipe for each model, and a set of new algorithmic and infrastructure advances across data processing, training, and reinforcement learning. We also introduce an enhanced reinforcement learning framework that guides the development of our models and is particularly essential for our thinking models. To design the training recipe and coordinate targeted improvements across a wide range of capabilities at each stage of the model training pipeline, our development framework balances distributed innovation with centralized evaluation.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Base, with a training pipeline that first focuses on broad coverage over diverse text, code, and math, then concentrates on harder distributions to sharpen programming, quantitative reasoning, and reading comprehension, is clearly the strongest set of fully open base models in our evaluations. It’s also arguably the best 32B model in the entire ecosystem of models with open weights, performing impressively in programming, reading comprehension, math problem solving, and long-context benchmarks like RULER, which tests information retrieval from lengthy texts. Olmo 3-Base (7B) and Olmo 3-Base (32) maintain quality at extended context lengths and integrate cleanly with RL workflows, providing a robust foundation for continued pretraining and post-training.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Think, which turns the Base into a reasoning model by training on multi-step problems spanning math, code, and general problem solving, then running the thinking SFT → thinking DPO → RLVR model flow to elicit high-quality reasoning traces, competes with or exceeds several open-weight reasoning models of similar sizes. On math benchmarks, Olmo 3-Think (7B) matches Qwen 3 8B on MATH and comes within a few points on AIME 2024 and 2025, and also leads all comparison models on HumanEvalPlus for coding—performing strongly on MBPP and LiveCodeBench to demonstrate particular strength in code-intensive reasoning. On broader reasoning tasks like BigBench Hard and AGI Eval English, Olmo 3-Think (7B) remains competitive with Qwen 3 8B reasoning and Qwen 3 VL 8B Thinker while staying fully open and slightly smaller.&lt;/p&gt;
    &lt;p&gt;For the 32B model, Olmo 3-Think scales these trends up and becomes one of the strongest fully open reasoning models in its class. Olmo 3-Think (32B) either wins or sits within roughly two points of the best open-weight model on MATH, OMEGA, BigBenchHard, HumanEvalPlus, PopQA, and IFEval. It ties Qwen 3 VL 32B Thinking for the top score on the OMEGA suite while staying clearly ahead of Gemma 3 27B Instruct and competitive with DeepSeek R1 Distill 32B on math and reasoning. On broader knowledge and QA, Olmo 3-Think (32B) is effectively neck-and-neck with the Qwen 3 models on PopQA. And in instruction following, Olmo 3-Think (32B) tops this subset on IFEval and remains solid on IFBench and AlpacaEval 2 LC—offering a strong default for reasoning workloads at the 32B scale.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Instruct, which produces shorter sequences than the corresponding Olmo 3-Think models to improve inference efficiency and is designed to focus on general chat, tool use, and synthetic data generation, outperforms comparably-sized open-weight models. Olmo 3-Instruct ties or surpasses Qwen 2.5, Gemma 3, and Llama 3.1 in our evaluations, and competes with the Qwen 3 family at similar scale, delivering strong function calling performance and instruction-following capabilities in a fully open 7B model.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Olmo 3 architecture and training stages&lt;/head&gt;
    &lt;p&gt;Olmo 3 uses a decoder-only transformer architecture and multi-stage training pipeline. Pretraining runs in three stages—an initial large-scale training run that builds broad capabilities; a mid-training phase that focuses on harder material like math, code, and reading comprehension; and a final long-context extension stage that trains the model on very long documents. Together with architectural enhancements, this yields a more capable, efficient base for the Olmo 3 family.&lt;/p&gt;
    &lt;p&gt;Post-training then specializes the pretrained model for different use cases. Building on Olmo 2, each pathway follows a three-stage recipe – SFT, preference tuning with DPO, and RLVR – but in Olmo 3, we expose this as a fully documented model flow with complete customization over each training stage and dataset mix.&lt;/p&gt;
    &lt;p&gt;Instead of releasing only the final weights, we provide checkpoints from each major training milestone: the base pretrained model, the mid-trained model after targeted skill enhancement, the long-context-extended version, plus post-training checkpoints for the Olmo 3-Think, Olmo 3-Instruct, and Olmo 3-RL Zero flows. You can study how capabilities emerge over time, run ablations on specific stages, and fork the model at whatever point best fits your data, compute, and goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expanded training data&lt;/head&gt;
    &lt;p&gt;Compared to Olmo 2, we scaled data collection and significantly strengthened our dataset curation methods. Continuing our commitment to full transparency, we’re releasing several new, higher-quality datasets that cover every stage of base model training and post-training—from initial learning to specialized skills like complex reasoning and long-context understanding. This means anyone can see exactly what data shaped the model’s capabilities, reproduce our results, and reuse these datasets to train their own AI systems.&lt;/p&gt;
    &lt;p&gt;Olmo 3 is pretrained on Dolma 3, a new ~9.3-trillion-token corpus drawn from web pages, science PDFs processed with olmOCR, codebases, math problems and solutions, and encyclopedic text. From this pool, we construct Dolma 3 Mix, a 5.9-trillion-token (~6T) pretraining mix with a higher proportion of coding and mathematical data than earlier Dolma releases, plus much stronger decontamination via extensive deduplication, quality filtering, and careful control over data mixing. We follow established web standards in collecting training data and don’t collect from sites that explicitly disallow it, including paywalled content.&lt;/p&gt;
    &lt;p&gt;On top of this, we introduce two Dolma 3-based mixes for later stages of base model training. Dolma 3 Dolmino is our mid-training mix: 100B training tokens sampled from a ~2.2T-token pool of high-quality math, science, code, instruction-following, and reading-comprehension data, including reasoning traces that also enable RL directly on the base model. Dolma 3 Longmino is our long-context mix: ~50B training tokens drawn from a 639B-token pool of long documents combined with mid-training data to teach Olmo 3 to track information over very long inputs (like reports, logs, and multi-chapter documents).&lt;/p&gt;
    &lt;p&gt;We also introduce Dolci, a new post-training data suite tailored specifically for reasoning, tool use, and instruction following. Dolci provides separate mixes for each stage of post-training: SFT, DPO, and RLVR. For SFT, Dolci aggregates state-of-the-art datasets that advance step-by-step reasoning, tool use, and high-quality conversational behavior; for DPO, it supplies high-quality contrastive preference data; and for RL, it includes hard, diverse prompts across math, coding, instruction following, and general chat.&lt;/p&gt;
    &lt;p&gt;Together, Dolma 3 and Dolci give Olmo 3 a fully open data curriculum from first token to final post-trained checkpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;Efficient training stack&lt;/head&gt;
    &lt;p&gt;We pretrained Olmo 3 on a cluster of up to 1,024 H100 GPUs; we achieved training throughput of 7.7K tokens per device per second for Olmo 3-Base (7B). We mid-trained on 128 H100 GPUs, and post-trained on a set of 256 H100s.&lt;/p&gt;
    &lt;p&gt;For Olmo 3, building on the work we did for Olmo 2, we were able to significantly improve the efficiency of our post-training code. By moving SFT from Open Instruct (our post-training codebase, prioritizing flexibility) to Olmo Core (our pretraining codebase, designed to maximize efficiency), we increased throughput (tokens/second) by 8x. Similarly, by incorporating in-flight weight updates, continuous batching, and a lot of threading improvements, we made our RL training 4x more efficient—resulting in training runs that are significantly cheaper and faster.&lt;/p&gt;
    &lt;p&gt;A note on our 32B models: We believe 32B sits in a sweet spot for research and tinkering. 32B models are big enough to support strong, competitive performance, but still small enough that a wide audience can fine-tune and deploy them on accessible hardware.&lt;/p&gt;
    &lt;p&gt;For more details, including ablations, please read our technical report.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency at the core&lt;/head&gt;
    &lt;p&gt;A core goal of Olmo 3 is not just to open the model flow, but to make it actionable for people who want to understand and improve model behavior. Olmo 3 integrates with OlmoTrace, our tool for tracing model outputs back to training data in real time.&lt;/p&gt;
    &lt;p&gt;For example, in the Ai2 Playground, you can ask Olmo 3-Think (32B) to answer a general-knowledge question, then use OlmoTrace to inspect where and how the model may have learned to generate parts of its response. This closes the gap between training data and model behavior: you can see not only what the model is doing, but why—and adjust data or training decisions accordingly.&lt;/p&gt;
    &lt;p&gt;To further promote transparency and explainability, we’re making every training and fine-tuning dataset available for download, all under a permissive license that allows for custom deployment and reuse. The datasets come in a range of mixes to accommodate different storage and hardware constraints, from several billion tokens all the way up to 6 trillion.&lt;/p&gt;
    &lt;p&gt;Our new tooling for data processing allows you to de-contaminate, tokenize, and de-duplicate data in the same way we did for Olmo 3’s corpora. All the tooling is open source, enabling you to replicate our training curves or run controlled ablations across data mixes and objectives.&lt;/p&gt;
    &lt;p&gt;Our Olmo utilities and software cover the whole development cycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo-core is a state-of-the-art framework for distributed model training.&lt;/item&gt;
      &lt;item&gt;Open Instruct is our post-training pipeline.&lt;/item&gt;
      &lt;item&gt;datamap-rs is a pure-Rust toolkit for large-scale cleaning.&lt;/item&gt;
      &lt;item&gt;duplodocus for ultra-efficient fuzzy de-duplication.&lt;/item&gt;
      &lt;item&gt;OLMES is a toolkit for reproducible evals. It includes our brand-new eval collection OlmoBaseEval, which we used for Olmo 3 base model development.&lt;/item&gt;
      &lt;item&gt;decon removes test sets from training data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Importantly, our tooling allows you to instrument complex tasks and analyze intermediate traces to understand where the models succeed—or struggle. Because the Olmo 3 data recipes, training pipeline, and checkpoints are open, independent teams can connect model behavior back to measurable properties.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to deploy and use&lt;/head&gt;
    &lt;p&gt;Together, the Olmo 3 family makes it easier to build trustworthy features quickly, whether for research, education, or applications. By making every development step available and inspectable, we're enabling entirely new categories of research. You can run experiments on any training phase, understand exactly how different techniques contribute to model capabilities, and build on our work at whatever stage makes sense for your project.&lt;/p&gt;
    &lt;p&gt;For scientists, the fully open flow exposes the model’s inner workings, so you can instrument experiments across coding, reasoning, RL, and tool use.&lt;/p&gt;
    &lt;p&gt;If you care about AI you can study, audit, and improve, Olmo 3 is for you. Try the demos in the Ai2 Playground, explore the documentation, and build on the released weights and checkpoints. Then tell us what you discover—we invite the community to validate, critique, and extend our findings.&lt;/p&gt;
    &lt;p&gt;True openness in AI isn't just about access—it's about trust, accountability, and shared progress. We believe the models shaping our future should be fully inspectable, not black boxes. Olmo 3 represents a different path: one where anyone can understand, verify, and build upon the AI systems that increasingly influence our world. This is what open-first means—not just releasing weights, but sharing the complete knowledge needed to advance AI responsibly: the flow.&lt;/p&gt;
    &lt;p&gt;Deep dive with Olmo lead researchers Hanna Hajishirzi and Noah Smith on how – and why – we built Olmo 3, and what comes next:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46001889</guid><pubDate>Fri, 21 Nov 2025 06:50:14 +0000</pubDate></item><item><title>The Qtile Window Manager: A Python-Powered Tiling Experience</title><link>https://tech.stonecharioteer.com/posts/2025/qtile-window-manager/</link><description>&lt;doc fingerprint="360d3a3f0397bf91"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve been an avid user of XFCE for a very long time. I’m fond of its lightweight nature, and I feel productive in it. But when I first discovered tiling window managers, I was mind-blown. I’ve wanted to use one forever.&lt;/p&gt;
    &lt;p&gt;My first experience with one was a few years ago, before I understood how Linux window managers worked. I couldn’t yet wrap my head around the fact that you could install more than one window manager and choose what you wanted during login. I think I’ve grown since then. I faintly remember trying to install i3wm, the most famous tiling window manager at the time. I think I was taken aback by the black screen, and more so with the mouse pointer which just said &lt;code&gt;X&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A year or so ago, I came across DistroTube’s Youtube Channel, where he talks about xmonad, the tiling window manager that’s written in Haskell. While I’ve been wanting to learn Haskell for a very long time, my career trajectory hasn’t afforded me the chance to learn it so far.&lt;/p&gt;
    &lt;p&gt;I’ve since moved jobs and completely shifted to Linux everywhere. I no longer want to use a non-linux machine ever again. I’m sure there’s a whole blog article about how much of a Linux person I’ve become in the past year or so, somewhere in me.&lt;/p&gt;
    &lt;p&gt;Last week, I came across dt’s video on Qtile, the tiling window manager written entirely in Python. Now that was truly enticing. I’m adept enough in Python to be able to manage complex configurations all on my own. And after skimming through the documentation, I spent a day modularizing the default qtile config since the default config gives me goosebumps, and not in a good way.&lt;/p&gt;
    &lt;p&gt;In this article, I’ll describe what I did, and how I went about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing Qtile&lt;/head&gt;
    &lt;p&gt;I decided to abstract away the entire configuration so that it doesn’t live in my dotfiles repository. I wanted to create a python library for myself so that it would have a bunch of utilities for my own consumption.&lt;/p&gt;
    &lt;p&gt;Additionally, I disagreed with the default way of installing Qtile. As a principle, I never &lt;code&gt;sudo pip install&lt;/code&gt; anything. Instead, I asked my friend Karthikeyan Singaravel, who is a Python core developer, and he recommended using the deadsnakes PPA for Ubuntu to install any version of Python that I chose. I tried compiling python 3.10 myself, installing to &lt;code&gt;/opt/qtile/&lt;/code&gt; using &lt;code&gt;configure --prefix /opt/qtile/&lt;/code&gt; during the configuration stage of the source code. However, I admit that using &lt;code&gt;deadsnakes&lt;/code&gt; is a far better idea since I could create a virtual environment based on &lt;code&gt;python3.10&lt;/code&gt; into &lt;code&gt;/opt/qtile/&lt;/code&gt; instead. I had to change the owner of the folder to my user account. Note that I could store the virtual environment in my home folder and just use that, but I wanted to isolate this outside of my home folder.&lt;/p&gt;
    &lt;p&gt;So, I installed &lt;code&gt;python3.10-full&lt;/code&gt; and &lt;code&gt;python3.10-dev&lt;/code&gt; (the development header files are necessary for building some of the dependencies of &lt;code&gt;qtile&lt;/code&gt;), and I created a virtual environment using the &lt;code&gt;venv&lt;/code&gt; module in &lt;code&gt;/opt/qtile&lt;/code&gt;. Then, I changed the owner of the folder to my regular user account.&lt;/p&gt;
    &lt;p&gt;Then, it was time to install qtile.&lt;/p&gt;
    &lt;p&gt;Since I use the fish shell, I had to &lt;code&gt;source activate /opt/qtile/bin/activate.fish&lt;/code&gt; to activate the virtual environment. And then I followed up by installing &lt;code&gt;qtile&lt;/code&gt;. I didn’t pick a version right away, I decided to go with the latest version.&lt;/p&gt;
    &lt;p&gt;Qtile doesn’t setup an entry for your &lt;code&gt;xsessions&lt;/code&gt;, so you need to do that yourself.&lt;/p&gt;
    &lt;p&gt;I created &lt;code&gt;/usr/share/xsessions/qtile.desktop&lt;/code&gt; and filled it with the following:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Notice how I used the absolute path for qtile.&lt;/p&gt;
    &lt;p&gt;After this, I logged out of my previous window manager and switched to the new entry for Qtile.&lt;/p&gt;
    &lt;p&gt;On loading qtile for the first time, I was fairly surprised with the default config. It wasn’t as blank as i3wm and xmonad were. It had a panel, a helpful text field on the panel about how to start the launcher, and it was very easy to use. I was liking it already.&lt;/p&gt;
    &lt;p&gt;But I wanted to configure it so that I could mess with the design.&lt;/p&gt;
    &lt;p&gt;The first thing that bothered me was the lack of a wallpaper. I’d used nitrogen before, so I installed it and started it up, setting a wallpaper. I restarted qtile and then… nothing.&lt;/p&gt;
    &lt;p&gt;That was me being silly and forgetting that Explicit is better than Implicit. Like all tiling window managers, Qtile did none of the work for us. You have to ensure that the wallpaper manager loads when Qtile is done loading. That’s where the &lt;code&gt;.xsessionrc&lt;/code&gt; file comes in.&lt;/p&gt;
    &lt;p&gt;Since nitrogen can restore a wallpaper with ease, all I needed to do was:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This went into the &lt;code&gt;~/.xsessionrc&lt;/code&gt; file.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuring Qtile&lt;/head&gt;
    &lt;p&gt;Qtile’s config file rests at &lt;code&gt;~/.config/qtile/config.py&lt;/code&gt;. On start, Qtile will read this file. Since this file is just Python code, that also means every single line in this file is executed.&lt;/p&gt;
    &lt;p&gt;When you look at the default config, you will notice:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It’s about 130 lines long. Not too big.&lt;/item&gt;
      &lt;item&gt;It’s just a bunch of variable declarations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This meant that all you needed to configure Qtile was to ensure you set the values of a few global variables in the config file. And Qtile would take care of the rest.&lt;/p&gt;
    &lt;p&gt;This was useful. All I needed to do was set some variables.&lt;/p&gt;
    &lt;p&gt;The default config constructs all these variables as it sets them, which is something I don’t recommend. Python’s error handling will not point out the right place where the error is occurring, and while Python 3.11 seeks to improve this, it’s generally not a good practice to have a long variable declaration step in your code.&lt;/p&gt;
    &lt;p&gt;For example, where the config does this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;If you want to reuse these objects, it’s better to just construct them separately and then use them in a panel. The same goes for reusing panels.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Current Configuration&lt;/head&gt;
    &lt;p&gt;After months of tweaking and refinement, here’s what my current Qtile setup looks like. The key principles I’ve followed are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Modularity: Break down complex structures into functions&lt;/item&gt;
      &lt;item&gt;Adaptive behavior: Detect hardware and adjust accordingly&lt;/item&gt;
      &lt;item&gt;Practical shortcuts: Keybindings that make sense for daily use&lt;/item&gt;
      &lt;item&gt;Visual consistency: A cohesive color scheme and layout&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Color Scheme and Assets&lt;/head&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;I use a consistent color palette and have custom icons for different system components. The straw hat is a personal touch - a nod to One Piece!&lt;/p&gt;
    &lt;head rend="h3"&gt;Smart Mouse Movement Between Monitors&lt;/head&gt;
    &lt;p&gt;One of my favorite custom functions handles multi-monitor setups elegantly:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This automatically moves the mouse cursor to the center of the next monitor when I press &lt;code&gt;Super + .&lt;/code&gt;, making multi-monitor workflows much smoother.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Bindings&lt;/head&gt;
    &lt;p&gt;My keybindings follow a logical pattern:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Super + hjkl: Vim-style window navigation&lt;/item&gt;
      &lt;item&gt;Super + Shift + hjkl: Move windows around&lt;/item&gt;
      &lt;item&gt;Super + Control + hjkl: Resize windows&lt;/item&gt;
      &lt;item&gt;Super + r: Launch rofi application launcher&lt;/item&gt;
      &lt;item&gt;Super + Shift + p: Screenshot utility&lt;/item&gt;
      &lt;item&gt;Super + Shift + l: Lock screen&lt;/item&gt;
      &lt;item&gt;Super + Shift + e: Power menu&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Hardware-Aware Widgets&lt;/head&gt;
    &lt;p&gt;One of the most powerful aspects of a Python-based window manager is the ability to create intelligent, hardware-aware components:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;These functions automatically detect hardware capabilities and adjust the interface accordingly. The battery widget only appears on laptops, and the IP address widget shows the current network status.&lt;/p&gt;
    &lt;head rend="h3"&gt;AMD GPU Integration&lt;/head&gt;
    &lt;p&gt;Since I run AMD hardware, I’ve integrated &lt;code&gt;amdgpu_top&lt;/code&gt; for real-time GPU monitoring:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This provides real-time VRAM usage information directly in the status bar.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dynamic Screen Configuration&lt;/head&gt;
    &lt;p&gt;The screen configuration automatically adapts to the number of connected monitors:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The main screen gets additional widgets like system tray and network information, while secondary screens get a simplified layout.&lt;/p&gt;
    &lt;head rend="h3"&gt;Startup Hooks&lt;/head&gt;
    &lt;p&gt;Qtile provides hooks for running scripts at startup:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This lets me separate one-time setup (like setting wallpapers) from things that should happen on every reload.&lt;/p&gt;
    &lt;head rend="h2"&gt;Current Setup in Action&lt;/head&gt;
    &lt;p&gt;My current setup includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Top bar: Shows Linux Mint logo, current layout, groups (workspaces), task list, and system tray&lt;/item&gt;
      &lt;item&gt;Bottom bar: CPU/GPU temperatures, VRAM usage, system resources, battery (if present), IP address, and clock&lt;/item&gt;
      &lt;item&gt;Custom separators: Visual dividers using the “⋮” character in my accent color&lt;/item&gt;
      &lt;item&gt;JetBrains Mono Nerd Font: For consistent icon rendering across all widgets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Lessons Learned&lt;/head&gt;
    &lt;p&gt;After using Qtile daily for months, here are the key insights:&lt;/p&gt;
    &lt;head rend="h3"&gt;Python Configuration is Powerful&lt;/head&gt;
    &lt;p&gt;Having your window manager configuration in Python means you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Write complex logic for hardware detection&lt;/item&gt;
      &lt;item&gt;Create reusable functions and modules&lt;/item&gt;
      &lt;item&gt;Integrate with system tools seamlessly&lt;/item&gt;
      &lt;item&gt;Debug configuration issues using Python tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Start Simple, Iterate&lt;/head&gt;
    &lt;p&gt;Don’t try to recreate someone else’s rice immediately. Start with the defaults and gradually customize:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Basic keybindings first&lt;/item&gt;
      &lt;item&gt;Add essential widgets&lt;/item&gt;
      &lt;item&gt;Customize colors and fonts&lt;/item&gt;
      &lt;item&gt;Add advanced features like custom functions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hardware Awareness Matters&lt;/head&gt;
    &lt;p&gt;Modern systems vary significantly. Your configuration should adapt to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Number of monitors&lt;/item&gt;
      &lt;item&gt;Battery presence&lt;/item&gt;
      &lt;item&gt;Available sensors&lt;/item&gt;
      &lt;item&gt;Network interfaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Performance Considerations&lt;/head&gt;
    &lt;p&gt;Since widgets can run arbitrary Python code, be mindful of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Update intervals for polling widgets&lt;/item&gt;
      &lt;item&gt;Error handling in custom functions&lt;/item&gt;
      &lt;item&gt;Resource usage of external commands&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Future Plans&lt;/head&gt;
    &lt;p&gt;This configuration is continuously evolving. Some planned improvements:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Custom Widgets:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;One Piece chapter release notifications&lt;/item&gt;
          &lt;item&gt;Gmail filtering widget&lt;/item&gt;
          &lt;item&gt;tmux session manager&lt;/item&gt;
          &lt;item&gt;Kubernetes context indicator&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Better Multi-Monitor Support:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Per-monitor wallpaper management&lt;/item&gt;
          &lt;item&gt;Workspace binding to specific monitors&lt;/item&gt;
          &lt;item&gt;Dynamic layout switching based on monitor configuration&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Integration Improvements:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;NordVPN status widget&lt;/item&gt;
          &lt;item&gt;NAS storage monitoring&lt;/item&gt;
          &lt;item&gt;Better notification management&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Preview&lt;/head&gt;
    &lt;p&gt;Here’s a look at what my config looks like today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Qtile has transformed my Linux desktop experience. The ability to configure everything in Python, combined with the logical tiling approach, has made me significantly more productive. The learning curve is gentler than pure configuration-file-based window managers, and the extensibility is unmatched.&lt;/p&gt;
    &lt;p&gt;If you’re comfortable with Python and want a window manager that grows with your needs, Qtile is an excellent choice. The community is helpful, the documentation is comprehensive, and the possibilities are endless.&lt;/p&gt;
    &lt;p&gt;The configuration I’ve shared represents months of daily use and refinement. It’s not just about aesthetics (though it does look good!) - it’s about creating a workspace that adapts to your hardware, workflow, and preferences seamlessly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46002138</guid><pubDate>Fri, 21 Nov 2025 07:41:15 +0000</pubDate></item><item><title>It's hard to build an oscillator</title><link>https://lcamtuf.substack.com/p/its-hard-to-build-an-oscillator</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46002161</guid><pubDate>Fri, 21 Nov 2025 07:45:53 +0000</pubDate></item><item><title>FAWK: LLMs can write a language interpreter</title><link>https://martin.janiczek.cz/2025/11/21/fawk-llms-can-write-a-language-interpreter.html</link><description>&lt;doc fingerprint="5ff690b5ea490edb"&gt;
  &lt;main&gt;
    &lt;p&gt;After reading the book The AWK Programming Language (recommended!), I was planning to try AWK out on this year’s Advent of Code. Having some time off from work this week, I tried to implement one of the problems in it to get some practice, set up my tooling, see how hard AWK would be, and… I found I’m FP-pilled.&lt;/p&gt;
    &lt;p&gt;I knew I’m addicted to the combination of algebraic data types (tagged unions) and exhaustive pattern matching, but what got me this time was immutability, lexical scope and the basic human right of being allowed to return arrays from functions.&lt;/p&gt;
    &lt;p&gt;Part 1 of the Advent of Code problem was easy enough, but for part 2 (basically a shortest path search with a twist, to not spoil too much), I found myself unable to switch from my usual functional BFS approach to something mutable, and ended up trying to implement my functional approach in AWK.&lt;/p&gt;
    &lt;p&gt;It got hairy very fast: I needed to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hashing of strings and 2D arrays (by piping to &lt;code&gt;md5sum&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;a global &lt;del rend="overstrike"&gt;set&lt;/del&gt;array of seen states&lt;/item&gt;
      &lt;item&gt;a way to serialize and deserialize a 2D array to/from a string&lt;/item&gt;
      &lt;item&gt;and a few associative arrays for retrieving this serialized array by its hash.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was very lost by the time I had all this; I spent hours just solving what felt like accidental complexity; things that I’d take for granted in more modern languages.&lt;/p&gt;
    &lt;p&gt;Now, I know nobody said AWK is modern, or functional, or that it promises any convenience for anything other than one-liners and basic scripts that fit under a handful of lines. I don’t want to sound like I expect AWK to do any of this; I knew I was stretching the tool when going in. But I couldn’t shake the feeling that there’s a beautiful AWK-like language within reach, an iteration on the AWK design (the pattern-action way of thinking is beautiful) that also gives us a few of the things programming language designers have learnt over the 48 years since AWK was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dreaming of functional AWK&lt;/head&gt;
    &lt;p&gt;Stopping my attempts to solve the AoC puzzle in pure AWK, I wondered: what am I missing here?&lt;/p&gt;
    &lt;p&gt;What if AWK had first-class arrays?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # array literals
  normal   = [1, 2, 3]
  nested   = [[1,2], [3,4]]
  assoc    = ["foo" =&amp;gt; "bar", "baz" =&amp;gt; "quux"]
  multidim = [(1,"abc") =&amp;gt; 999]

  five = range(1,5)
  analyze(five)
  print five  # --&amp;gt; still [1, 2, 3, 4, 5]! was passed by value
}

function range(a,b) {
  r = []
  for (i = a; i &amp;lt;= b; i++) {
    r[length(r)] = i
  }
  return r  # arrays can be returned!
}

function analyze(arr) {
  arr[0] = 100
  print arr[0]  # --&amp;gt; 100, only within this function
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had first-class functions and lambdas?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # construct anonymous functions
  double = (x) =&amp;gt; { x * 2 }
  add = (a, b) =&amp;gt; { c = a + b; return c }

  # functions can be passed as values
  apply = (func, value) =&amp;gt; { func(value) }

  print apply(double,add(1,3))  # --&amp;gt; 8
  print apply(inc,5)  # --&amp;gt; 6
}

function inc(a) { return a + 1 }
&lt;/code&gt;
    &lt;p&gt;What if AWK had lexical scope instead of dynamic scope?&lt;/p&gt;
    &lt;code&gt;# No need for this hack anymore ↓     ↓
#function foo(a, b         ,local1, local2) {
function foo(a, b) {
  local1 = a + b
  local2 = a - b
  return local1 + local2
}

BEGIN {
  c = foo(1,2)
  print(local1)  # --&amp;gt; 0, the local1 from foo() didn't leak!
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had explicit globals, and everything else was local by default?&lt;/p&gt;
    &lt;code&gt;BEGIN { global count }
END {
  foo()
  print count  # --&amp;gt; 1
  print mylocal # --&amp;gt; 0, didn't leak
}
function foo() { count++; mylocal++ }
&lt;/code&gt;
    &lt;p&gt;(This one, admittedly, might make programs a bit more verbose. I’m willing to pay that cost.)&lt;/p&gt;
    &lt;p&gt;What if AWK had pipelines? (OK, now I’m reaching for syntax sugar…)&lt;/p&gt;
    &lt;code&gt;BEGIN {
  result = [1, 2, 3, 4, 5] 
      |&amp;gt; filter((x) =&amp;gt; { x % 2 == 0 })
      |&amp;gt; map((x) =&amp;gt; { x * x })
      |&amp;gt; reduce((acc, x) =&amp;gt; { acc + x }, 0)

  print "Result:", result
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Making it happen&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TL;DR:&lt;/p&gt;&lt;code&gt;Janiczek/fawk&lt;/code&gt;on GitHub&lt;/quote&gt;
    &lt;p&gt;Now for the crazy, LLM-related part of the post. I didn’t want to spend days implementing AWK from scratch or tweaking somebody else’s implementation. So I tried to use Cursor Agent for a larger task than I usually do (I tend to ask for very small targeted edits), and asked Sonnet 4.5 for a README with code examples, and then a full implementation in Python.&lt;/p&gt;
    &lt;p&gt;And it did it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: I also asked for implementations in C, Haskell and Rust at the same time, not knowing if any of the four would succeed, and they all seem to have produced code that at least compiles/runs. I haven’t tried to test them or even run them though. The PRs are here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was very impressed—I still am! I expected the LLM to stumble and flail around and ultimately get nothing done, but it did what I asked it for (gave me an interpreter that could run those specific examples), and over the course of a few chat sessions, I guided it towards implementing more and more of “the rest of AWK”, together with an excessive amount of end-to-end tests.&lt;/p&gt;
    &lt;p&gt;The only time I could see it struggle was when I asked it to implement arbitrary precision floating point operations without using an external library like &lt;code&gt;mpmath&lt;/code&gt;. It attempted to use Taylor series, but couldn’t get it right for at
least a few minutes. I chickened out and told it to &lt;code&gt;uv add mpmath&lt;/code&gt; and simplify
the interpreter code. In a moment it was done.&lt;/p&gt;
    &lt;p&gt;Other things that I thought it would choke on, like &lt;code&gt;print&lt;/code&gt; being both a
statement (with &lt;code&gt;&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; redirection support) and an expression, or
multi-dimensional arrays, or multi-line records, these were all implemented
correctly. Updating the test suite to also check for backwards compatibility
with GAWK - not an issue. Lexical scoping
and tricky closure environment behaviour - handled that just fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;As the cool kids say, I have to update my priors. The frontier of what the LLMs can do has moved since the last time I tried to vibe-code something. I didn’t expect to have a working interpreter the same day I dreamt of a new programming language. It now seems possible.&lt;/p&gt;
    &lt;p&gt;The downside of vibe coding the whole interpreter is that I have zero knowledge of the code. I only interacted with the agent by telling it to implement a thing and write tests for it, and I only really reviewed the tests. I reckon this would be an issue in the future when I want to manually make some change in the actual code, because I have no familiarity with it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This also opened new questions for me wrt. my other projects where I’ve previously run out of steam, eg. trying to implement a Hindley-Milner type system for my dream forever-WIP programming language Cara. It seems I can now just ask the LLM to do it, and it will? But then, I don’t want to fall into the trap where I am no longer able to work on the codebase myself. I want to be familiar with and able to tinker on the code. I’d need to spend my time reviewing and reading code instead of writing everything myself. Perhaps that’s OK.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Performance of FAWK might be an issue as well, though right now it’s a non-goal, given my intended use case is throwaway scripts for Advent of Code, nothing user-facing. And who knows, based on what I’ve seen, maybe I can instruct it to rewrite it in Rust and have a decent chance of success?&lt;/p&gt;
    &lt;p&gt;For now, I’ll go dogfood my shiny new vibe-coded black box of a programming language on the Advent of Code problem (and as many of the 2025 puzzles as I can), and see what rough edges I can find. I expect them to be equal parts “not implemented yet” and “unexpected interactions of new PL features with the old ones”.&lt;/p&gt;
    &lt;p&gt;If you’re willing to jump through some Python project dependency hoops, you can try to use FAWK too at your own risk, at &lt;code&gt;Janiczek/fawk&lt;/code&gt; on
GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46003144</guid><pubDate>Fri, 21 Nov 2025 10:28:49 +0000</pubDate></item><item><title>How a French judge was digitally cut off by the USA</title><link>https://www.heise.de/en/news/How-a-French-judge-was-digitally-cut-off-by-the-USA-11087561.html</link><description>&lt;doc fingerprint="65a032ec176759d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How a French judge was digitally cut off by the USA&lt;/head&gt;
    &lt;p&gt;Nicolas Guillou has been sanctioned by the USA as a judge of the International Criminal Court. He notices the effects primarily in the digital realm.&lt;/p&gt;
    &lt;p&gt;Digital sovereignty has been much discussed in Europe in recent weeks, most recently during a German-French summit in Berlin. The extent of dependence on the USA in the digital sector is currently being experienced by a French judge. Nicolas Guillou, one of six judges and three prosecutors of the International Criminal Court (ICC), was sanctioned by the USA in August. He described his current situation as a digital time travel back to the 1990s, before the internet age, in a recent interview.&lt;/p&gt;
    &lt;p&gt;The reason for the US sanctions are the arrest warrants against Israeli Prime Minister Benjamin Netanyahu and Defense Minister Yoav Gallant. They were indicted for war crimes and crimes against humanity in the context of the destruction of the Gaza Strip. The USA condemned this decision by the court, whereupon the US Treasury Department sanctioned six judges and three prosecutors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Digitally excluded from almost everything&lt;/head&gt;
    &lt;p&gt;In Guillou's daily life, this means that he is excluded from digital life and much of what is considered standard today, he told the French newspaper Le Monde. All his accounts with US companies such as Amazon, Airbnb, or PayPal were immediately closed by the providers. Online bookings, such as through Expedia, are immediately canceled, even if they concern hotels in France. Participation in e-commerce is also practically no longer possible for him, as US companies always play a role in one way or another, and they are strictly forbidden to enter into any trade relationship with sanctioned individuals.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;p&gt;He also describes the impact on participating in banking as drastic. Payment systems are blocked for him, as US companies like American Express, Visa, and Mastercard have a virtual monopoly in Europe. He also describes the rest of banking as severely restricted. For example, accounts with non-US banks have also been partially closed. Transactions in US dollars or via dollar conversions are forbidden to him.&lt;/p&gt;
    &lt;head rend="h3"&gt;Judge: EU should block sanctions&lt;/head&gt;
    &lt;p&gt;Guillou's case shows how strong the USA's influence in the tech sector is and how few options he has to circumvent it. And this at a time when an account with a US tech company is considered a matter of course in more and more places.&lt;/p&gt;
    &lt;p&gt;The French judge advocates for Europe to gain more sovereignty in the digital and banking sectors. Without this sovereignty, the rule of law cannot be guaranteed, he warns. At the same time, he calls on the EU to activate an existing blocking regulation (Regulation (EC) No 2271/96) for the International Criminal Court, which prevents third countries like the USA from enforcing sanctions in the EU. EU companies would then no longer be allowed to comply with US sanctions if they violate EU interests. Companies that violate this would then be liable for damages.&lt;/p&gt;
    &lt;p&gt;(mki)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46003778</guid><pubDate>Fri, 21 Nov 2025 12:12:41 +0000</pubDate></item><item><title>Making a Small RPG</title><link>https://jslegenddev.substack.com/p/making-a-small-rpg</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46004293</guid><pubDate>Fri, 21 Nov 2025 13:23:16 +0000</pubDate></item><item><title>EXIF orientation info in PNGs isn't used for image-orientation: from-image</title><link>https://bugzilla.mozilla.org/show_bug.cgi?id=1627423</link><description>&lt;doc fingerprint="fb52bd5796b2ff66"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;EXIF orientation info in PNGs isn't used for image-orientation: from-image&lt;/head&gt;
    &lt;head rend="h2"&gt;Categories&lt;/head&gt;
    &lt;head rend="h3"&gt;(Core :: Layout: Images, Video, and HTML Frames, defect, P3)&lt;/head&gt;
    &lt;head rend="h2"&gt;Tracking&lt;/head&gt;
    &lt;head rend="h3"&gt;()&lt;/head&gt;
    &lt;head rend="h2"&gt;People&lt;/head&gt;
    &lt;head rend="h3"&gt;(Reporter: e, Unassigned)&lt;/head&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;head rend="h3"&gt;(Blocks 2 open bugs)&lt;/head&gt;
    &lt;head rend="h2"&gt;Details&lt;/head&gt;
    &lt;head rend="h3"&gt;(Keywords: parity-chrome, parity-safari, webcompat:platform-bug)&lt;/head&gt;
    &lt;head rend="h2"&gt;User Story&lt;/head&gt;
    &lt;quote&gt;user-impact-score:40&lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reporter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Description&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15&lt;/p&gt;
    &lt;p&gt;Steps to reproduce:&lt;/p&gt;
    &lt;p&gt;Go to https://ericportis.com/etc/PNG-EXIF-orientation/&lt;/p&gt;
    &lt;p&gt;Actual results:&lt;/p&gt;
    &lt;p&gt;The JPEG and PNG are rotated differently, even though they both have the same EXIF info (Orientation: Rotate 90 CW), and are both set to &lt;code&gt;image-orientation: from-image;&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Expected results:&lt;/p&gt;
    &lt;p&gt;They should display the same.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reporter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 1&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Further findings: neither Safari, Chrome, or Firefox respects exiftool's default output, which appends EXIF to the end of a PNG. This is allowed by the spec, but seems to be incompatible with progressive rendering of partially-downloaded PNGs.&lt;/p&gt;
    &lt;p&gt;Safari does respect EXIF orientation info that appears before the image data, but Firefox and Chrome do not.&lt;/p&gt;
    &lt;p&gt;https://bugs.webkit.org/show_bug.cgi?id=210021#c4&lt;lb/&gt; https://ericportis.com/etc/PNG-EXIF-orientation/shuffling-chunks/&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 2&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;heycam: Will this be covered by any of your follow-up work related to bug 1607667?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 3&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Huh, I didn't even know that PNG supported orientation data. I found https://ftp-osl.osuosl.org/pub/libpng/documents/pngext-1.5.0.html#C.eXIf which defines the &lt;code&gt;eXif&lt;/code&gt; table.  The patches I'm working on don't add support for this, but it would not be too difficult to do so, at least if the table appears earlier than the image data.  (I don't think our current image loading flow would handle the image size changing as a result of the orientation data later on.)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 4&lt;/head&gt;•&lt;p&gt;5 years ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Because this bug's Severity has not been changed from the default since it was filed, and it's Priority is &lt;code&gt;P3&lt;/code&gt; (Backlog,) indicating it has been triaged, the bug's Severity is being updated to &lt;code&gt;S3&lt;/code&gt; (normal.)&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;•&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 5&lt;/head&gt;•&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is spec'd in PNG-3 https://www.w3.org/TR/2024/CRD-png-3-20240718/#eXIf&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;•&lt;p&gt;1 year ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 6&lt;/head&gt;•&lt;p&gt;11 months ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What is the expected waiting time for the issue to be resolved?&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;head&gt;Updated&lt;/head&gt;•&lt;p&gt;1 month ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;head&gt;Comment 7&lt;/head&gt;•&lt;p&gt;1 month ago &lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Should be fixed by bug 1682759. If that is incorrect please re-open.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46004364</guid><pubDate>Fri, 21 Nov 2025 13:29:14 +0000</pubDate></item><item><title>Building a Minimal Viable Armv7 Emulator from Scratch</title><link>https://xnacly.me/posts/2025/building-a-minimal-viable-armv7-emulator/</link><description>&lt;doc fingerprint="aa33d81da505eee8"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Tip or TLDR - I built a tiny, zero dependency armv7 userspace emulator in Rust&lt;/head&gt;
    &lt;p&gt;I wrote a minimal viable armv7 emulator in 1.3k lines of Rust without any dependencies. It parses and validates a 32-bit arm binary, maps its segments, decodes a subset of arm instructions, translates guest and host memory interactions and forwards arm Linux syscalls into x86-64 System V syscalls.&lt;/p&gt;
    &lt;p&gt;It can run a armv7 hello world binary and does so in 1.9ms (0.015ms for raw emulation without setup), while qemu takes 12.3ms (stinkarm is thus ~100-1000x slower than native armv7 execution).&lt;/p&gt;
    &lt;p&gt;After reading about the process the Linux kernel performs to execute binaries, I thought: I want to write an armv7 emulator - &lt;code&gt;stinkarm&lt;/code&gt;. Mostly to understand the ELF
format, the encoding of arm 32bit instructions, the execution of arm assembly
and how it all fits together (this will help me with the JIT for my programming
language I am currently designing). To fully understand everything: no
dependencies. And of course Rust, since I already have enough C projects going
on.&lt;/p&gt;
    &lt;p&gt;So I wrote the smallest binary I could think of:&lt;/p&gt;
    &lt;code&gt;1    .global _start  @ declare _start as a global
2_start:             @ start is the defacto entry point
3    mov r0, #161    @ first and only argument to the exit syscall
4    mov r7, #1      @ syscall number 1 (exit)
5    svc #0          @ trapping into the kernel (thats US, since we are translating)
&lt;/code&gt;
    &lt;p&gt;To execute this arm assembly on my x86 system, I need to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Parse the ELF, validate it is armv7 and statically executable (I don’t want to write a dynamic dependency resolver and loader)&lt;/item&gt;
      &lt;item&gt;Map the segments defined in ELF into the host memory, forward memory access&lt;/item&gt;
      &lt;item&gt;Decode armv7 instructions and convert them into a nice Rust enum&lt;/item&gt;
      &lt;item&gt;Emulate the CPU, its state and registers&lt;/item&gt;
      &lt;item&gt;Execute the instructions and apply their effects to the CPU state&lt;/item&gt;
      &lt;item&gt;Translate and forward syscalls&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sounds easy? It is!&lt;/p&gt;
    &lt;p&gt;Open below if you want to see me write a build script and a nix flake:&lt;head&gt;Minimalist arm setup and smallest possible arm binary&lt;/head&gt;&lt;/p&gt;
    &lt;p&gt;Before I start parsing ELF I’ll need a binary to emulate, so lets create a build script called &lt;code&gt;bld_exmpl&lt;/code&gt; (so I can write a lot less) and nix flake, so
the asm is converted into armv7 machine code in a armv7 binary on my non armv7
system :^)&lt;/p&gt;
    &lt;code&gt; 1// tools/bld_exmpl
 2use clap::Parser;
 3use std::fs;
 4use std::path::Path;
 5use std::process::Command;
 6
 7/// Build all ARM assembly examples into .elf binaries
 8#[derive(Parser)]
 9struct Args {
10    /// Directory containing .S examples
11    #[arg(long, default_value = "examples")]
12    examples_dir: String,
13}
14
15fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
16    let args = Args::parse();
17    let dir = Path::new(&amp;amp;args.examples_dir);
18
19    for entry in fs::read_dir(dir)? {
20        let entry = entry?;
21        let path = entry.path();
22        if path.extension().and_then(|s| s.to_str()) == Some("S") {
23            let name = path.file_stem().unwrap().to_str().unwrap();
24            let output = dir.join(format!("{}.elf", name));
25            build_asm(&amp;amp;path, &amp;amp;output)?;
26        }
27    }
28
29    Ok(())
30}
31
32fn build_asm(input: &amp;amp;Path, output: &amp;amp;Path) -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
33    println!("Building {} -&amp;gt; {}", input.display(), output.display());
34
35    let obj_file = input.with_extension("o");
36
37    let status = Command::new("arm-none-eabi-as")
38        .arg("-march=armv7-a")
39        .arg(input)
40        .arg("-o")
41        .arg(&amp;amp;obj_file)
42        .status()?;
43
44    if !status.success() {
45        return Err(format!("Assembler failed for {}", input.display()).into());
46    }
47
48    let status = Command::new("arm-none-eabi-ld")
49        .arg("-Ttext=0x8000")
50        .arg(&amp;amp;obj_file)
51        .arg("-o")
52        .arg(output)
53        .status()?;
54
55    if !status.success() {
56        return Err(format!("Linker failed for {}", output.display()).into());
57    }
58
59    Ok(fs::remove_file(obj_file)?)
60}&lt;/code&gt;
    &lt;code&gt; 1# Cargo.toml
 2[package]
 3name = "stinkarm"
 4version = "0.1.0"
 5edition = "2024"
 6default-run = "stinkarm"
 7
 8[dependencies]
 9clap = { version = "4.5.51", features = ["derive"] }
10
11[[bin]]
12name = "stinkarm"
13path = "src/main.rs"
14
15[[bin]]
16name = "bld_exmpl"
17path = "tools/bld_exmpl.rs"&lt;/code&gt;
    &lt;code&gt; 1{
 2  description = "stinkarm — ARMv7 userspace binary emulator for x86 linux systems";
 3  inputs = {
 4    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
 5    flake-utils.url = "github:numtide/flake-utils";
 6  };
 7  outputs = { self, nixpkgs, flake-utils, ... }:
 8    flake-utils.lib.eachDefaultSystem (system:
 9      let
10        pkgs = import nixpkgs { inherit system; };
11      in {
12        devShells.default = pkgs.mkShell {
13          buildInputs = with pkgs; [
14            gcc-arm-embedded
15            binutils
16            qemu
17          ];
18        };
19      }
20  );
21}&lt;/code&gt;
    &lt;head rend="h1"&gt;Parsing ELF&lt;/head&gt;
    &lt;p&gt;So there are some resources for parsing ELF, two of them I used a whole lot:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;man elf&lt;/code&gt;(remember to&lt;code&gt;export MANPAGER='nvim +Man!'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;gabi.xinuos.com&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a high level, ELF (32bit, for armv7) consists of headers and segments, it holds an Elf header, multiple program headers and the rest I don’t care about, since this emulator is only for static binaries, no dynamically linked support.&lt;/p&gt;
    &lt;head rend="h2"&gt;Elf32_Ehdr&lt;/head&gt;
    &lt;p&gt;The ELF header is exactly 52 bytes long and holds all data I need to find the program headers and whether I even want to emulate the binary I’m currently parsing. These criteria are defined as members of the &lt;code&gt;Identifier&lt;/code&gt; at the beg
of the header.&lt;/p&gt;
    &lt;p&gt;In terms of byte layout:&lt;/p&gt;
    &lt;code&gt; 1+------------------------+--------+--------+----------------+----------------+----------------+----------------+----------------+--------+---------+--------+---------+--------+--------+
 2|       identifier       |  type  |machine |    version     |     entry      |     phoff      |     shoff      |     flags      | ehsize |phentsize| phnum  |shentsize| shnum  |shstrndx|
 3|          16B           |   2B   |   2B   |       4B       |       4B       |       4B       |       4B       |       4B       |   2B   |   2B    |   2B   |   2B    |   2B   |   2B   |
 4+------------------------+--------+--------+----------------+----------------+----------------+----------------+----------------+--------+---------+--------+---------+--------+--------+
 5           \|/
 6            |
 7            |
 8            v
 9+----------------+------+------+-------+------+-----------+------------------------+
10|     magic      |class | data |version|os_abi|abi_version|          pad           |
11|       4B       |  1B  |  1B  |  1B   |  1B  |    1B     |           7B           |
12+----------------+------+------+-------+------+-----------+------------------------+&lt;/code&gt;
    &lt;p&gt;Most resources show C based examples, the rust ports are below:&lt;/p&gt;
    &lt;code&gt; 1/// Representing the ELF Object File Format header in memory, equivalent to Elf32_Ehdr in 2. ELF
 2/// header in https://gabi.xinuos.com/elf/02-eheader.html
 3///
 4/// Types are taken from https://gabi.xinuos.com/elf/01-intro.html#data-representation Table 1.1
 5/// 32-Bit Data Types:
 6///
 7/// | Elf32_ | Rust |
 8/// | ------ | ---- |
 9/// | Addr   | u32  |
10/// | Off    | u32  |
11/// | Half   | u16  |
12/// | Word   | u32  |
13/// | Sword  | i32  |
14#[derive(Debug, Clone, Copy, PartialEq, Eq)]
15pub struct Header {
16    /// initial bytes mark the file as an object file and provide machine-independent data with
17    /// which to decode and interpret the file’s contents
18    pub ident: Identifier,
19    pub r#type: Type,
20    pub machine: Machine,
21    /// identifies the object file version, always EV_CURRENT (1)
22    pub version: u32,
23    /// the virtual address to which the system first transfers control, thus starting
24    /// the process. If the file has no associated entry point, this member holds zero
25    pub entry: u32,
26    /// the program header table’s file offset in bytes. If the file has no program header table,
27    /// this member holds zero
28    pub phoff: u32,
29    /// the section header table’s file offset in bytes. If the file has no section header table, this
30    /// member holds zero
31    pub shoff: u32,
32    /// processor-specific flags associated with the file
33    pub flags: u32,
34    /// the ELF header’s size in bytes
35    pub ehsize: u16,
36    /// the size in bytes of one entry in the file’s program header table; all entries are the same
37    /// size
38    pub phentsize: u16,
39    /// the number of entries in the program header table. Thus the product of e_phentsize and e_phnum
40    /// gives the table’s size in bytes. If a file has no program header table, e_phnum holds the value
41    /// zero
42    pub phnum: u16,
43    /// section header’s size in bytes. A section header is one entry in the section header table; all
44    /// entries are the same size
45    pub shentsize: u16,
46    /// number of entries in the section header table. Thus the product of e_shentsize and e_shnum
47    /// gives the section header table’s size in bytes. If a file has no section header table,
48    /// e_shnum holds the value zero.
49    pub shnum: u16,
50    /// the section header table index of the entry associated with the section name string table.
51    /// If the file has no section name string table, this member holds the value SHN_UNDEF
52    pub shstrndx: u16,
53}&lt;/code&gt;
    &lt;p&gt;The identifier is 16 bytes long and holds the previously mentioned info so I can check if I want to emulate the binary, for instance the endianness and the bit class, in the &lt;code&gt;TryFrom&lt;/code&gt; implementation I strictly check what is parsed:&lt;/p&gt;
    &lt;code&gt; 1/// 2.2 ELF Identification: https://gabi.xinuos.com/elf/02-eheader.html#elf-identification
 2#[repr(C)]
 3#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 4pub struct Identifier {
 5    /// 0x7F, 'E', 'L', 'F'
 6    pub magic: [u8; 4],
 7    /// file class or capacity
 8    ///
 9    /// | Name          | Value | Meaning       |
10    /// | ------------- | ----- | ------------- |
11    /// | ELFCLASSNONE  | 0     | Invalid class |
12    /// | ELFCLASS32    | 1     | 32-bit        |
13    /// | ELFCLASS64    | 2     | 64-bit        |
14    pub class: u8,
15    /// data encoding, endian
16    ///
17    /// | Name         | Value |
18    /// | ------------ | ----- |
19    /// | ELFDATANONE  | 0     |
20    /// | ELFDATA2LSB  | 1     |
21    /// | ELFDATA2MSB  | 2     |
22    pub data: u8,
23    /// file version, always EV_CURRENT (1)
24    pub version: u8,
25    /// operating system identification
26    ///
27    /// - if no extensions are used: 0
28    /// - meaning depends on e_machine
29    pub os_abi: u8,
30    /// value depends on os_abi
31    pub abi_version: u8,
32    // padding bytes (9-15)
33    _pad: [u8; 7],
34}
35
36impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Identifier {
37    type Error = &amp;amp;'static str;
38
39    fn try_from(bytes: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
40        if bytes.len() &amp;lt; 16 {
41            return Err("e_ident too short for ELF");
42        }
43
44        // I don't want to cast via unsafe as_ptr and as Header because the header could outlive the
45        // source slice, thus we just do it the old plain indexing way
46        let ident = Self {
47            magic: bytes[0..4].try_into().unwrap(),
48            class: bytes[4],
49            data: bytes[5],
50            version: bytes[6],
51            os_abi: bytes[7],
52            abi_version: bytes[8],
53            _pad: bytes[9..16].try_into().unwrap(),
54        };
55
56        if ident.magic != [0x7f, b'E', b'L', b'F'] {
57            return Err("Unexpected EI_MAG0 to EI_MAG3, wanted 0x7f E L F");
58        }
59
60        const ELFCLASS32: u8 = 1;
61        const ELFDATA2LSB: u8 = 1;
62        const EV_CURRENT: u8 = 1;
63
64        if ident.version != EV_CURRENT {
65            return Err("Unsupported EI_VERSION value");
66        }
67
68        if ident.class != ELFCLASS32 {
69            return Err("Unexpected EI_CLASS: ELFCLASS64, wanted ELFCLASS32 (ARMv7)");
70        }
71
72        if ident.data != ELFDATA2LSB {
73            return Err("Unexpected EI_DATA: big-endian, wanted little");
74        }
75
76        Ok(ident)
77    }&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Type&lt;/code&gt; and &lt;code&gt;Machine&lt;/code&gt; are just enums encoding meaning in the Rust type system:&lt;/p&gt;
    &lt;code&gt; 1#[repr(u16)]
 2#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 3pub enum Type {
 4    None = 0,
 5    Relocatable = 1,
 6    Executable = 2,
 7    SharedObject = 3,
 8    Core = 4,
 9    LoOs = 0xfe00,
10    HiOs = 0xfeff,
11    LoProc = 0xff00,
12    HiProc = 0xffff,
13}
14
15impl TryFrom&amp;lt;u16&amp;gt; for Type {
16    type Error = &amp;amp;'static str;
17
18    fn try_from(value: u16) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
19        match value {
20            0 =&amp;gt; Ok(Type::None),
21            1 =&amp;gt; Ok(Type::Relocatable),
22            2 =&amp;gt; Ok(Type::Executable),
23            3 =&amp;gt; Ok(Type::SharedObject),
24            4 =&amp;gt; Ok(Type::Core),
25            0xfe00 =&amp;gt; Ok(Type::LoOs),
26            0xfeff =&amp;gt; Ok(Type::HiOs),
27            0xff00 =&amp;gt; Ok(Type::LoProc),
28            0xffff =&amp;gt; Ok(Type::HiProc),
29            _ =&amp;gt; Err("Invalid u16 value for e_type"),
30        }
31    }
32}
33
34
35#[repr(u16)]
36#[allow(non_camel_case_types)]
37#[derive(Debug, Clone, Copy, PartialEq, Eq)]
38pub enum Machine {
39    EM_ARM = 40,
40}
41
42impl TryFrom&amp;lt;u16&amp;gt; for Machine {
43    type Error = &amp;amp;'static str;
44
45    fn try_from(value: u16) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
46        match value {
47            40 =&amp;gt; Ok(Machine::EM_ARM),
48            _ =&amp;gt; Err("Unsupported machine"),
49        }
50    }
51}&lt;/code&gt;
    &lt;p&gt;Since all of &lt;code&gt;Header&lt;/code&gt;’s members implement &lt;code&gt;TryFrom&lt;/code&gt; we can implement
&lt;code&gt;TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Header&lt;/code&gt; and propagate all occurring errors in member parsing
cleanly via &lt;code&gt;?&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Header {
 2    type Error = &amp;amp;'static str;
 3
 4    fn try_from(b: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
 5        if b.len() &amp;lt; 52 {
 6            return Err("not enough bytes for Elf32_Ehdr (ELF header)");
 7        }
 8
 9        let header = Self {
10            ident: b[0..16].try_into()?,
11            r#type: le16!(b[16..18]).try_into()?,
12            machine: le16!(b[18..20]).try_into()?,
13            version: le32!(b[20..24]),
14            entry: le32!(b[24..28]),
15            phoff: le32!(b[28..32]),
16            shoff: le32!(b[32..36]),
17            flags: le32!(b[36..40]),
18            ehsize: le16!(b[40..42]),
19            phentsize: le16!(b[42..44]),
20            phnum: le16!(b[44..46]),
21            shentsize: le16!(b[46..48]),
22            shnum: le16!(b[48..50]),
23            shstrndx: le16!(b[50..52]),
24        };
25
26        match header.r#type {
27            Type::Executable =&amp;gt; (),
28            _ =&amp;gt; {
29                return Err("Unsupported ELF type, only ET_EXEC (static executables) is supported");
30            }
31        }
32
33        Ok(header)
34    }
35}&lt;/code&gt;
    &lt;p&gt;The attentive reader will see me using &lt;code&gt;le16!&lt;/code&gt; and &lt;code&gt;le32!&lt;/code&gt; for parsing bytes
into unsigned integers of different classes (&lt;code&gt;le&lt;/code&gt; is short for little endian):&lt;/p&gt;
    &lt;code&gt; 1#[macro_export]
 2macro_rules! le16 {
 3    ($bytes:expr) =&amp;gt; {{
 4        let b: [u8; 2] = $bytes
 5            .try_into()
 6            .map_err(|_| "Failed to create u16 from 2*u8")?;
 7        u16::from_le_bytes(b)
 8    }};
 9}
10
11#[macro_export]
12macro_rules! le32 {
13    ($bytes:expr) =&amp;gt; {{
14        let b: [u8; 4] = $bytes
15            .try_into()
16            .map_err(|_| "Failed to create u32 from 4*u8")?;
17        u32::from_le_bytes(b)
18    }};
19}&lt;/code&gt;
    &lt;head rend="h2"&gt;Elf32_Phdr&lt;/head&gt;
    &lt;code&gt;1+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+
2|      type      |     offset     |     vaddr      |     paddr      |     filesz     |     memsz      |     flags      |     align      |
3|       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |       4B       |
4+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+&lt;/code&gt;
    &lt;p&gt;For me, the most important fields in &lt;code&gt;Header&lt;/code&gt; are &lt;code&gt;phoff&lt;/code&gt; and &lt;code&gt;phentsize&lt;/code&gt;,
since we can use these to index into the binary to locate the program headers (&lt;code&gt;Phdr&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt; 1/// Phdr, equivalent to Elf32_Phdr, see: https://gabi.xinuos.com/elf/07-pheader.html
 2///
 3/// All of its member are u32, be it Elf32_Word, Elf32_Off or Elf32_Addr
 4#[derive(Debug)]
 5pub struct Pheader {
 6    pub r#type: Type,
 7    pub offset: u32,
 8    pub vaddr: u32,
 9    pub paddr: u32,
10    pub filesz: u32,
11    pub memsz: u32,
12    pub flags: Flags,
13    pub align: u32,
14}
15
16impl Pheader {
17    /// extracts Pheader from raw, starting from offset
18    pub fn from(raw: &amp;amp;[u8], offset: usize) -&amp;gt; Result&amp;lt;Self, String&amp;gt; {
19        let end = offset.checked_add(32).ok_or("Offset overflow")?;
20        if raw.len() &amp;lt; end {
21            return Err("Not enough bytes to parse Elf32_Phdr, need at least 32".into());
22        }
23
24        let p_raw = &amp;amp;raw[offset..end];
25        let r#type = p_raw[0..4].try_into()?;
26        let flags = p_raw[24..28].try_into()?;
27        let align = le32!(p_raw[28..32]);
28
29        if align &amp;gt; 1 &amp;amp;&amp;amp; !align.is_power_of_two() {
30            return Err(format!("Invalid p_align: {}", align));
31        }
32
33        Ok(Self {
34            r#type,
35            offset: le32!(p_raw[4..8]),
36            vaddr: le32!(p_raw[8..12]),
37            paddr: le32!(p_raw[12..16]),
38            filesz: le32!(p_raw[16..20]),
39            memsz: le32!(p_raw[20..24]),
40            flags,
41            align,
42        })
43    }
44}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Type&lt;/code&gt; holds info about what type of segment the header defines:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(C)]
 3pub enum Type {
 4    NULL = 0,
 5    LOAD = 1,
 6    DYNAMIC = 2,
 7    INTERP = 3,
 8    NOTE = 4,
 9    SHLIB = 5,
10    PHDR = 6,
11    TLS = 7,
12    LOOS = 0x60000000,
13    HIOS = 0x6fffffff,
14    LOPROC = 0x70000000,
15    HIPROC = 0x7fffffff,
16}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Flag&lt;/code&gt; defines the
permission flags the segment should have once it is dumped into memory:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(transparent)]
 3pub struct Flags(u32);
 4
 5impl Flags {
 6    pub const NONE: Self = Flags(0x0);
 7    pub const X: Self = Flags(0x1);
 8    pub const W: Self = Flags(0x2);
 9    pub const R: Self = Flags(0x4);
10}&lt;/code&gt;
    &lt;head rend="h2"&gt;Full ELF parsing&lt;/head&gt;
    &lt;p&gt;Putting &lt;code&gt;Elf32_Ehdr&lt;/code&gt; and &lt;code&gt;Elf32_Phdr&lt;/code&gt; parsing together:&lt;/p&gt;
    &lt;code&gt; 1/// Representing an ELF32 binary in memory
 2///
 3/// This does not include section headers (Elf32_Shdr), but only program headers (Elf32_Phdr), see either `man elf` and/or https://gabi.xinuos.com/elf/03-sheader.html
 4#[derive(Debug)]
 5pub struct Elf {
 6    pub header: header::Header,
 7    pub pheaders: Vec&amp;lt;pheader::Pheader&amp;gt;,
 8}
 9
10impl TryFrom&amp;lt;&amp;amp;[u8]&amp;gt; for Elf {
11    type Error = String;
12
13    fn try_from(b: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;Self, String&amp;gt; {
14        let header = header::Header::try_from(b).map_err(|e| e.to_string())?;
15
16        let mut pheaders = Vec::with_capacity(header.phnum as usize);
17        for i in 0..header.phnum {
18            let offset = header.phoff as usize + i as usize * header.phentsize as usize;
19            let ph = pheader::Pheader::from(b, offset)?;
20            pheaders.push(ph);
21        }
22
23        Ok(Elf { header, pheaders })
24    }
25}&lt;/code&gt;
    &lt;p&gt;The equivalent to &lt;code&gt;readelf -l&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1Elf {
 2    header: Header {
 3        ident: Identifier {
 4            magic: [127, 69, 76, 70],
 5            class: 1,
 6            data: 1,
 7            version: 1,
 8            os_abi: 0,
 9            abi_version: 0,
10            _pad: [0, 0, 0, 0, 0, 0, 0]
11        },
12        type: Executable,
13        machine: EM_ARM,
14        version: 1,
15        entry: 32768,
16        phoff: 52,
17        shoff: 4572,
18        flags: 83886592,
19        ehsize: 52,
20        phentsize: 32,
21        phnum: 1,
22        shentsize: 40,
23        shnum: 8,
24        shstrndx: 7
25    },
26    pheaders: [
27        Pheader {
28            type: LOAD,
29            offset: 4096,
30            vaddr: 32768,
31            paddr: 32768,
32            filesz: 12,
33            memsz: 12,
34            flags: Flags(5),
35            align: 4096
36        }
37    ]
38}&lt;/code&gt;
    &lt;p&gt;Or in the debug output of stinkarm:&lt;/p&gt;
    &lt;code&gt; 1[     0.613ms] opening binary "examples/asm.elf"
 2[     0.721ms] parsing ELF...
 3[     0.744ms] \
 4ELF Header:
 5  Magic:              [7f, 45, 4c, 46]
 6  Class:              ELF32
 7  Data:               Little endian
 8  Type:               Executable
 9  Machine:            EM_ARM
10  Version:            1
11  Entry point:        0x8000
12  Program hdr offset: 52 (32 bytes each)
13  Section hdr offset: 4572
14  Flags:              0x05000200
15  EH size:            52
16  # Program headers:  1
17  # Section headers:  8
18  Str tbl index:      7
19
20Program Headers:
21  Type       Offset   VirtAddr   PhysAddr   FileSz    MemSz  Flags  Align
22  LOAD     0x001000 0x00008000 0x00008000 0x00000c 0x00000c    R|X 0x1000&lt;/code&gt;
    &lt;head rend="h1"&gt;Dumping ELF segments into memory&lt;/head&gt;
    &lt;p&gt;Since the only reason for parsing the elf headers is to know where to put what segment with which permissions, I want to quickly interject on why we have to put said segments at these specific addresses. The main reason is that all pointers, all offsets and pc related decoding has to be done relative to &lt;code&gt;Elf32_Ehdr.entry&lt;/code&gt;, here &lt;code&gt;0x8000&lt;/code&gt;. The linker also generated all instruction
arguments according to this value.&lt;/p&gt;
    &lt;p&gt;Before mapping each segment at its &lt;code&gt;Pheader::vaddr&lt;/code&gt;, we have to understand:
One doesn’t simply &lt;code&gt;mmap&lt;/code&gt; with &lt;code&gt;MAP_FIXED&lt;/code&gt; or &lt;code&gt;MAP_NOREPLACE&lt;/code&gt; into the virtual
address &lt;code&gt;0x8000&lt;/code&gt;. The Linux kernel won’t let us, and rightfully so, &lt;code&gt;man mmap&lt;/code&gt;
says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If addr is not NULL, then the kernel takes it as a hint about where to place the mapping; on Linux, the kernel will pick a nearby page boundary (but always above or equal to the value specified by /proc/sys/vm/mmap_min_addr) and attempt to create the mapping there.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And &lt;code&gt;/proc/sys/vm/mmap_min_addr&lt;/code&gt; on my system is &lt;code&gt;u16::MAX&lt;/code&gt; (2^16)-1=65535. So
mapping our segment to &lt;code&gt;0x8000&lt;/code&gt; (32768) is not allowed:&lt;/p&gt;
    &lt;code&gt; 1let segment = sys::mmap::mmap(
 2    // this is only UB if dereferenced, its just a hint, so its safe here
 3    Some(unsafe { std::ptr::NonNull::new_unchecked(0x8000 as *mut u8) }),
 4    4096,
 5    sys::mmap::MmapProt::WRITE,
 6    sys::mmap::MmapFlags::ANONYMOUS
 7        | sys::mmap::MmapFlags::PRIVATE
 8        | sys::mmap::MmapFlags::NOREPLACE,
 9    -1,
10    0,
11)
12.unwrap();&lt;/code&gt;
    &lt;p&gt;Running the above with our &lt;code&gt;vaddr&lt;/code&gt; of &lt;code&gt;0x8000&lt;/code&gt; results in:&lt;/p&gt;
    &lt;code&gt;1thread 'main' panicked at src/main.rs:33:6:
2called `Result::unwrap()` on an `Err` value: "mmap failed (errno 1): Operation not permitted
3(os error 1)"&lt;/code&gt;
    &lt;p&gt;It only works in elevated permission mode, which is something I dont want to run my emulator in.&lt;/p&gt;
    &lt;head rend="h2"&gt;Translating guest memory access to host memory access&lt;/head&gt;
    &lt;p&gt;The obvious fix is to not mmap below &lt;code&gt;u16::MAX&lt;/code&gt; and let the kernel choose where
we dump our segment:&lt;/p&gt;
    &lt;code&gt;1let segment = sys::mmap::mmap(
2    None,
3    4096,
4    MmapProt::WRITE,
5    MmapFlags::ANONYMOUS | MmapFlags::PRIVATE,
6    -1,
7    0,
8).unwrap();&lt;/code&gt;
    &lt;p&gt;But this means the segment of the process to emulate is not at &lt;code&gt;0x8000&lt;/code&gt;, but
anywhere the kernel allows. So we need to add a translation layer between guest
and host memory: (If you’re familiar with how virtual memory works, its similar
but one more indirection)&lt;/p&gt;
    &lt;code&gt;1+--guest--+
2| 0x80000 | ------------+
3+---------+             |
4                        |
5                    Mem::translate
6                        |
7+------host------+      |
8| 0x7f5b4b8f8000 | &amp;lt;----+
9+----------------+&lt;/code&gt;
    &lt;p&gt;Putting this into rust:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;map_region&lt;/code&gt;registers a region of memory and allows&lt;code&gt;Mem&lt;/code&gt;to take ownership for calling munmap on these segments once it goes out of scope&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;translate&lt;/code&gt;takes a guest addr and translates it to a host addr&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; 1struct MappedSegment {
 2    host_ptr: *mut u8,
 3    len: u32,
 4}
 5
 6pub struct Mem {
 7    maps: BTreeMap&amp;lt;u32, MappedSegment&amp;gt;,
 8}
 9
10impl Mem {
11    pub fn map_region(&amp;amp;mut self, guest_addr: u32, len: u32, host_ptr: *mut u8) {
12        self.maps
13            .insert(guest_addr, MappedSegment { host_ptr, len });
14    }
15
16    /// translate a guest addr to a host addr we can write and read from
17    pub fn translate(&amp;amp;self, guest_addr: u32) -&amp;gt; Option&amp;lt;*mut u8&amp;gt; {
18        // Find the greatest key &amp;lt;= guest_addr.
19        let (&amp;amp;base, seg) = self.maps.range(..=guest_addr).next_back()?;
20        if guest_addr &amp;lt; base.wrapping_add(seg.len) {
21            let offset = guest_addr.wrapping_sub(base);
22            Some(unsafe { seg.host_ptr.add(offset as usize) })
23        } else {
24            None
25        }
26    }
27
28    pub fn read_u32(&amp;amp;self, guest_addr: u32) -&amp;gt; Option&amp;lt;u32&amp;gt; {
29        let ptr = self.translate(guest_addr)?;
30        unsafe { Some(u32::from_le(*(ptr as *const u32))) }
31    }
32}&lt;/code&gt;
    &lt;p&gt;This fix has the added benfit of allowing us to sandbox guest memory fully, so we can validate each memory access before we allow a guest to host memory interaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mapping segments with their permissions&lt;/head&gt;
    &lt;p&gt;The basic idea is similar to the way a JIT compiler works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;create a &lt;code&gt;mmap&lt;/code&gt;section with&lt;code&gt;W&lt;/code&gt;permissions&lt;/item&gt;
      &lt;item&gt;write bytes from elf into section&lt;/item&gt;
      &lt;item&gt;zero rest of defined size&lt;/item&gt;
      &lt;item&gt;change permission of section with &lt;code&gt;mprotect&lt;/code&gt;to the permissions defined in the&lt;code&gt;Pheader&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; 1/// mapping applies the configuration of self to the current memory context by creating the
 2/// segments with the corresponding permission bits, vaddr, etc
 3pub fn map(&amp;amp;self, raw: &amp;amp;[u8], guest_mem: &amp;amp;mut mem::Mem) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
 4    // zero memory needed case, no clue if this actually ever happens, but we support it
 5    if self.memsz == 0 {
 6        return Ok(());
 7    }
 8
 9    if self.vaddr == 0 {
10        return Err("program header has a zero virtual address".into());
11    }
12
13    // we need page alignement, so either Elf32_Phdr.p_align or 4096
14    let (start, _end, len) = self.alignments();
15
16    // Instead of mapping at the guest vaddr (Linux doesnt't allow for low addresses),
17    // we allocate memory wherever the host kernel gives us.
18    // This keeps guest memory sandboxed: guest addr != host addr.
19    let segment = mem::mmap::mmap(
20        None,
21        len as usize,
22        MmapProt::WRITE,
23        MmapFlags::ANONYMOUS | MmapFlags::PRIVATE,
24        -1,
25        0,
26    )?;
27
28    let segment_ptr = segment.as_ptr();
29    let segment_slice = unsafe { std::slice::from_raw_parts_mut(segment_ptr, len as usize) };
30
31    let file_slice: &amp;amp;[u8] =
32        &amp;amp;raw[self.offset as usize..(self.offset.wrapping_add(self.filesz)) as usize];
33
34    // compute offset inside the mmapped slice where the segment should start
35    let offset = (self.vaddr - start) as usize;
36
37    // copy the segment contents to the mmaped segment
38    segment_slice[offset..offset + file_slice.len()].copy_from_slice(file_slice);
39
40    // we need to zero the remaining bytes
41    if self.memsz &amp;gt; self.filesz {
42        segment_slice
43            [offset.wrapping_add(file_slice.len())..offset.wrapping_add(self.memsz as usize)]
44            .fill(0);
45    }
46
47    // record mapping in guest memory table, so CPU can translate guest vaddr to host pointer
48    guest_mem.map_region(self.vaddr, len, segment_ptr);
49
50    // we change the permissions for our segment from W to the segments requested bits
51    mem::mmap::mprotect(segment, len as usize, self.flags.into())
52}
53
54/// returns (start, end, len)
55fn alignments(&amp;amp;self) -&amp;gt; (u32, u32, u32) {
56    // we need page alignement, so either Elf32_Phdr.p_align or 4096
57    let align = match self.align {
58        0 =&amp;gt; 0x1000,
59        _ =&amp;gt; self.align,
60    };
61    let start = self.vaddr &amp;amp; !(align - 1);
62    let end = (self.vaddr.wrapping_add(self.memsz).wrapping_add(align) - 1) &amp;amp; !(align - 1);
63    let len = end - start;
64    (start, end, len)
65}&lt;/code&gt;
    &lt;p&gt;Map is called in the emulators entry point:&lt;/p&gt;
    &lt;code&gt;1let elf: elf::Elf = (&amp;amp;buf as &amp;amp;[u8]).try_into().expect("Failed to parse binary");
2let mut mem = mem::Mem::new();
3for phdr in elf.pheaders {
4    if phdr.r#type == elf::pheader::Type::LOAD {
5        phdr.map(&amp;amp;buf, &amp;amp;mut mem)
6            .expect("Mapping program header failed");
7    }
8}&lt;/code&gt;
    &lt;head rend="h1"&gt;Decoding armv7&lt;/head&gt;
    &lt;p&gt;We can now request a word (32bit) from our &lt;code&gt;LOAD&lt;/code&gt; segment which contains
the &lt;code&gt;.text&lt;/code&gt; section bytes one can inspect via &lt;code&gt;objdump&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1$ arm-none-eabi-objdump -d examples/exit.elf
 2
 3examples/exit.elf:     file format elf32-littlearm
 4
 5
 6Disassembly of section .text:
 7
 800008000 &amp;lt;_start&amp;gt;:
 9    8000:       e3a000a1        mov     r0, #161        @ 0xa1
10    8004:       e3a07001        mov     r7, #1
11    8008:       ef000000        svc     0x00000000&lt;/code&gt;
    &lt;p&gt;So we use &lt;code&gt;Mem::read_u32(0x8000)&lt;/code&gt; and get &lt;code&gt;0xe3a000a1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Decoding armv7 instructions seems doable at a glance, but it is a deeper rabbit-hole than I expected, prepare for a bit shifting, implicit behaviour and intertwined meaning heavy section:&lt;/p&gt;
    &lt;p&gt;Instructions are more or less grouped into four groups:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Branch and control&lt;/item&gt;
      &lt;item&gt;Data processing&lt;/item&gt;
      &lt;item&gt;Load and store&lt;/item&gt;
      &lt;item&gt;Other (syscalls &amp;amp; stuff)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each armv7 instruction is 32 bit in size, (in general) its layout is as follows:&lt;/p&gt;
    &lt;code&gt;1+--------+------+------+------+------------+---------+
2|  cond  |  op  |  Rn  |  Rd  |  Operand2  |  shamt  |
3|   4b   |  4b  |  4b  |  4b  |     12b    |   4b    |
4+--------+------+------+------+------------+---------+&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;bit range&lt;/cell&gt;
        &lt;cell role="head"&gt;name&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;0..4&lt;/cell&gt;
        &lt;cell&gt;cond&lt;/cell&gt;
        &lt;cell&gt;contains &lt;code&gt;EQ&lt;/code&gt;, &lt;code&gt;NE&lt;/code&gt;, etc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4..8&lt;/cell&gt;
        &lt;cell&gt;op&lt;/cell&gt;
        &lt;cell&gt;for instance &lt;code&gt;0b1101&lt;/code&gt; for &lt;code&gt;mov&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8..12&lt;/cell&gt;
        &lt;cell&gt;rn&lt;/cell&gt;
        &lt;cell&gt;source register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12..16&lt;/cell&gt;
        &lt;cell&gt;rd&lt;/cell&gt;
        &lt;cell&gt;destination register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16..28&lt;/cell&gt;
        &lt;cell&gt;operand2&lt;/cell&gt;
        &lt;cell&gt;immediate value or shifted register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;28..32&lt;/cell&gt;
        &lt;cell&gt;shamt&lt;/cell&gt;
        &lt;cell&gt;shift amount&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Rust representation&lt;/head&gt;
    &lt;p&gt;Since &lt;code&gt;cond&lt;/code&gt; decides whether or not the instruction is
executed, I decided on the following struct to be the decoded
instruction:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Copy, Clone)]
 2pub struct InstructionContainer {
 3    pub cond: u8,
 4    pub instruction: Instruction,
 5}
 6
 7#[derive(Debug, Copy, Clone)]
 8pub enum Instruction {
 9    MovImm { rd: u8, rhs: u32 },
10    Svc,
11    LdrLiteral { rd: u8, addr: u32 },
12    Unknown(u32),
13}&lt;/code&gt;
    &lt;p&gt;These 4 instructions are enough to support both the minimal binary at the intro and the asm hello world:&lt;/p&gt;
    &lt;code&gt;1    .global _start
2_start:
3    mov r0, #161
4    mov r7, #1
5    svc #0&lt;/code&gt;
    &lt;code&gt; 1    .section .rodata
 2msg:
 3    .asciz "Hello, world!\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    ldr r0, =1
 9    ldr r1, =msg
10    mov r2, #14
11    mov r7, #4
12    svc #0
13
14    mov r0, #0
15    mov r7, #1
16    svc #0&lt;/code&gt;
    &lt;head rend="h2"&gt;General instruction detection&lt;/head&gt;
    &lt;p&gt;Our decoder is a function accepting a word, the program counter (we need this later for decoding the offset for &lt;code&gt;ldr&lt;/code&gt;) and returning the
aforementioned instruction container:&lt;/p&gt;
    &lt;code&gt;1pub fn decode_word(word: u32, caddr: u32) -&amp;gt; InstructionContainer&lt;/code&gt;
    &lt;p&gt;Referring to the diagram shown before, I know the first 4 bit are the condition, so I can extract these first. I also take the top 3 bits to identify the instruction class (load and store, branch or data processing immediate):&lt;/p&gt;
    &lt;code&gt;1// ...
2let cond = ((word &amp;gt;&amp;gt; 28) &amp;amp; 0xF) as u8;
3let top = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x7) as u8;&lt;/code&gt;
    &lt;head rend="h2"&gt;Immediate mov&lt;/head&gt;
    &lt;p&gt;Since there are immediate moves and non immediate moves, both &lt;code&gt;0b000&lt;/code&gt; and
&lt;code&gt;0b001&lt;/code&gt; are valid top values we want to support.&lt;/p&gt;
    &lt;code&gt;1// ...
2if top == 0b000 || top == 0b001 {
3    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
4    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
5    if i_bit {
6        // ...
7    }
8}&lt;/code&gt;
    &lt;p&gt;If the i bit is set, we can extract convert the opcode from its bits into something I can read a lot better:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug, Clone, Copy, PartialEq, Eq)]
 2#[repr(u8)]
 3enum Op {
 4    // ...
 5    Mov = 0b1101,
 6}
 7
 8static OP_TABLE: [Op; 16] = [
 9    // ...
10    Op::Mov,
11];
12
13#[inline(always)]
14fn op_from_bits(bits: u8) -&amp;gt; Op {
15    debug_assert!(bits &amp;lt;= 0b1111);
16    unsafe { *OP_TABLE.get_unchecked(bits as usize) }
17}&lt;/code&gt;
    &lt;p&gt;We can now plug this in, match on the only ddi (data processing immediate) we know and extract both the destination register (rd) and the raw immediate value:&lt;/p&gt;
    &lt;code&gt; 1if top == 0b000 || top == 0b001 {
 2    // Data-processing immediate (ddi) (top 0b000 or 0b001 when I==1)
 3    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
 4    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
 5    if i_bit {
 6        match op_from_bits(opcode) {
 7            Op::Mov =&amp;gt; {
 8                let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 9                let imm12 = word &amp;amp; 0xFFF;
10                // ...
11            }
12            _ =&amp;gt; todo!(),
13        }
14    }
15}&lt;/code&gt;
    &lt;p&gt;From the examples before one can see the immediate value is prefixed with &lt;code&gt;#&lt;/code&gt;. To move the value &lt;code&gt;161&lt;/code&gt; into &lt;code&gt;r0&lt;/code&gt; we do:&lt;/p&gt;
    &lt;code&gt;1mov r0, #161&lt;/code&gt;
    &lt;p&gt;Since we know there are only 12 bits available for the immediate the arm engineers came up with rotation of the resulting integer by the remaining 4 bits:&lt;/p&gt;
    &lt;code&gt;1#[inline(always)]
2fn decode_rotated_imm(imm12: u32) -&amp;gt; u32 {
3    let rotate = ((imm12 &amp;gt;&amp;gt; 8) &amp;amp; 0b1111) * 2;
4    (imm12 &amp;amp; 0xff).rotate_right(rotate)
5}&lt;/code&gt;
    &lt;p&gt;Plugging this back in results in us being able to fully decode &lt;code&gt;mov r0,#161&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; 1if top == 0b000 || top == 0b001 {
 2    let i_bit = ((word &amp;gt;&amp;gt; 25) &amp;amp; 0x1) != 0;
 3    let opcode = ((word &amp;gt;&amp;gt; 21) &amp;amp; 0xF) as u8;
 4    if i_bit {
 5        match op_from_bits(opcode) {
 6            Op::Mov =&amp;gt; {
 7                let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 8                let imm12 = word &amp;amp; 0xFFF;
 9                let rhs = decode_rotated_imm(imm12);
10                return InstructionContainer {
11                    cond,
12                    instruction: Instruction::MovImm { rd, rhs },
13                };
14            }
15            _ =&amp;gt; todo!(),
16        }
17    }
18}&lt;/code&gt;
    &lt;p&gt;As seen when &lt;code&gt;dbg!&lt;/code&gt;-ing the cpu steps:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:114:13] decoder::decode_word(word, self.pc()) =
2InstructionContainer {
3    cond: 14,
4    instruction: MovImm {
5        rd: 0,
6        rhs: 161,
7    },
8}&lt;/code&gt;
    &lt;head rend="h2"&gt;Load and Store&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;ldr&lt;/code&gt; is part of the load and store instruction group and is needed for
the accessing of &lt;code&gt;Hello World!&lt;/code&gt; in &lt;code&gt;.rodata&lt;/code&gt; and putting a ptr to it
into a register.&lt;/p&gt;
    &lt;p&gt;In comparison to immediate mov we have to do a little trick, since we only want to match for load and store that matches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;single register modification&lt;/item&gt;
      &lt;item&gt;load and store with immediate&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So we only decode:&lt;/p&gt;
    &lt;code&gt;1LDR Rd, [Rn, #imm]
2LDR Rd, [Rn], #imm
3@ etc
&lt;/code&gt;
    &lt;p&gt;Thus we match with &lt;code&gt;(top &amp;gt;&amp;gt; 1) &amp;amp; 0b11 == 0b01&lt;/code&gt; and start extracting a
whole bucket load of bit flags:&lt;/p&gt;
    &lt;code&gt; 1if (top &amp;gt;&amp;gt; 1) &amp;amp; 0b11 == 0b01 {
 2    let p = ((word &amp;gt;&amp;gt; 24) &amp;amp; 1) != 0;
 3    let u = ((word &amp;gt;&amp;gt; 23) &amp;amp; 1) != 0;
 4    let b = ((word &amp;gt;&amp;gt; 22) &amp;amp; 1) != 0;
 5    let w = ((word &amp;gt;&amp;gt; 21) &amp;amp; 1) != 0;
 6    let l = ((word &amp;gt;&amp;gt; 20) &amp;amp; 1) != 0;
 7    let rn = ((word &amp;gt;&amp;gt; 16) &amp;amp; 0xF) as u8;
 8    let rd = ((word &amp;gt;&amp;gt; 12) &amp;amp; 0xF) as u8;
 9    let imm12 = (word &amp;amp; 0xFFF) as u32;
10
11    // Literal‑pool version
12    if l &amp;amp;&amp;amp; rn == 0b1111 &amp;amp;&amp;amp; p &amp;amp;&amp;amp; u &amp;amp;&amp;amp; !w &amp;amp;&amp;amp; !b {
13        let pc_seen = caddr.wrapping_add(8);
14        let literal_addr = pc_seen.wrapping_add(imm12);
15
16        return InstructionContainer {
17            cond,
18            instruction: Instruction::LdrLiteral {
19                rd,
20                addr: literal_addr,
21            },
22        };
23    }
24
25    todo!("only LDR with p&amp;amp;u&amp;amp;!w&amp;amp;!b is implemented")
26}&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;bit&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;pre-indexed addressing, offset added before load&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;u&lt;/cell&gt;
        &lt;cell&gt;add (1) vs subtract (0) offset&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;b&lt;/cell&gt;
        &lt;cell&gt;word (0) or byte (1) sized access&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;w&lt;/cell&gt;
        &lt;cell&gt;(no=0) write back to base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;l&lt;/cell&gt;
        &lt;cell&gt;load (1), or store (0)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;ldr Rn, &amp;lt;addr&amp;gt;&lt;/code&gt; matches exactly &lt;code&gt;load&lt;/code&gt;, base register is PC (&lt;code&gt;rn==0b1111&lt;/code&gt;), pre-indexed
addressing, added offset, no write back and no byte sized access (&lt;code&gt;l &amp;amp;&amp;amp; rn == 0b1111 &amp;amp;&amp;amp; p &amp;amp;&amp;amp; u &amp;amp;&amp;amp; !w &amp;amp;&amp;amp; !b&lt;/code&gt;).&lt;/p&gt;
    &lt;head rend="h2"&gt;Syscalls&lt;/head&gt;
    &lt;p&gt;Syscalls are the only way to interact with the Linux kernel (as far as I know), so we definitely need to implement both decoding and forwarding. Bits 27-24 are &lt;code&gt;1111&lt;/code&gt; for system calls. The immediate value is
irrelevant for us, since the Linux syscall handler either way discards
the value:&lt;/p&gt;
    &lt;code&gt;1if ((word &amp;gt;&amp;gt; 24) &amp;amp; 0xF) as u8 == 0b1111 {
2    return InstructionContainer {
3        cond,
4        // technically arm says svc has a 24bit immediate but we don't care about it, since the
5        // Linux kernel also doesn't
6        instruction: Instruction::Svc,
7    };
8}&lt;/code&gt;
    &lt;p&gt;We can now fully decode all instructions for both the simple exit and the more advanced hello world binary:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 161, }
2[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 1, }
3[src/cpu/mod.rs:121:15] instruction = Svc&lt;/code&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 1, }
2[src/cpu/mod.rs:121:15] instruction = LdrLiteral { rd: 1, addr: 32800, }
3[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 2, rhs: 14, }
4[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 4, }
5[src/cpu/mod.rs:121:15] instruction = Svc
6[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 0, rhs: 0, }
7[src/cpu/mod.rs:121:15] instruction = MovImm { rd: 7, rhs: 1, }
8[src/cpu/mod.rs:121:15] instruction = Svc&lt;/code&gt;
    &lt;head rend="h1"&gt;Emulating the CPU&lt;/head&gt;
    &lt;p&gt;This is by FAR the easiest part, I only struggled with the double indirection for &lt;code&gt;ldr&lt;/code&gt; (I simply didn’t know about it), but each problem
at its time :^).&lt;/p&gt;
    &lt;code&gt; 1pub struct Cpu&amp;lt;'cpu&amp;gt; {
 2    /// r0-r15 (r13=SP, r14=LR, r15=PC)
 3    pub r: [u32; 16],
 4    pub cpsr: u32,
 5    pub mem: &amp;amp;'cpu mut mem::Mem,
 6    /// only set by ArmSyscall::Exit to propagate exit code to the host
 7    pub status: Option&amp;lt;i32&amp;gt;,
 8}
 9
10impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
11    pub fn new(mem: &amp;amp;'cpu mut mem::Mem, pc: u32) -&amp;gt; Self {
12        let mut s = Self {
13            r: [0; 16],
14            cpsr: 0x60000010,
15            mem,
16            status: None,
17        };
18        s.r[15] = pc;
19        s
20    }&lt;/code&gt;
    &lt;p&gt;Instantiating the cpu:&lt;/p&gt;
    &lt;code&gt;1let mut cpu = cpu::Cpu::new(&amp;amp;mut mem, elf.header.entry);&lt;/code&gt;
    &lt;head rend="h2"&gt;Conditional Instructions?&lt;/head&gt;
    &lt;p&gt;When writing the decoder I was confused by the 4 conditional bits. I always though one does conditional execution by using a branch to jump over instructions that shouldnt be executed. That was before I learned for arm, both ways are supported (the armv7 reference says this feature should only be used if there arent multiple instructions depending on the same condition, otherwise one should use branches) - so I need to support this too:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    #[inline(always)]
 3    fn cond_passes(&amp;amp;self, cond: u8) -&amp;gt; bool {
 4        match cond {
 5            0x0 =&amp;gt; (self.cpsr &amp;gt;&amp;gt; 30) &amp;amp; 1 == 1, // EQ: Z == 1
 6            0x1 =&amp;gt; (self.cpsr &amp;gt;&amp;gt; 30) &amp;amp; 1 == 0, // NE
 7            0xE =&amp;gt; true,                       // AL (always)
 8            0xF =&amp;gt; false,                      // NV (never)
 9            _ =&amp;gt; false,                        // strict false
10        }
11    }
12}&lt;/code&gt;
    &lt;head rend="h2"&gt;Instruction dispatch&lt;/head&gt;
    &lt;p&gt;After implementing the necessary checks and setup for emulating the cpu, the CPU can now check if an instruction is to be executed, match on the decoded instruction and run the associated logic:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    #[inline(always)]
 3    fn pc(&amp;amp;self) -&amp;gt; u32 {
 4        self.r[15] &amp;amp; !0b11
 5    }
 6
 7    /// moves pc forward a word
 8    #[inline(always)]
 9    fn advance(&amp;amp;mut self) {
10        self.r[15] = self.r[15].wrapping_add(4);
11    }
12
13    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
14        let Some(word) = self.mem.read_u32(self.pc()) else {
15            return Ok(false);
16        };
17
18        if word == 0 {
19            // zero instruction means we hit zeroed out rest of the page
20            return Ok(false);
21        }
22
23        let InstructionContainer { instruction, cond } = decoder::decode_word(word, self.pc());
24
25        if !self.cond_passes(cond) {
26            self.advance();
27            return Ok(true);
28        }
29
30        match instruction {
31            decoder::Instruction::MovImm { rd, rhs } =&amp;gt; {
32                self.r[rd as usize] = rhs;
33            }
34            decoder::Instruction::Unknown(w) =&amp;gt; {
35                return Err(err::Err::UnknownOrUnsupportedInstruction(w));
36            }
37            i =&amp;gt; {
38                stinkln!(
39                    "found unimplemented instruction, exiting: {:#x}:={:?}",
40                    word,
41                    i
42                );
43                self.status = Some(1);
44            }
45        }
46
47        self.advance();
48
49        Ok(true)
50    }
51}&lt;/code&gt;
    &lt;head rend="h2"&gt;LDR and addresses in literal pools&lt;/head&gt;
    &lt;p&gt;While Translating guest memory access to host memory access goes into depth on translating / forwarding guest memory access to host memory adresses, this chapter will focus on the layout of literals in armv7 and how &lt;code&gt;ldr&lt;/code&gt; indirects
memory access.&lt;/p&gt;
    &lt;p&gt;Lets first take a look at the ldr instruction of our hello world example:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2    @ define a string with the `msg` label
 3msg:
 4    @ asciz is like asciii but zero terminated
 5    .asciz "Hello world!\n"
 6
 7    .section .text
 8    .global _start
 9_start:
10    @ load the literal pool addr of msg into r1
11    ldr r1, =msg&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;as&lt;/code&gt;
documentation
says:&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;LDR&lt;/code&gt;ARMASM&lt;code&gt;1ldr &amp;lt;register&amp;gt;, = &amp;lt;expression&amp;gt;&lt;/code&gt;&lt;p&gt;If expression evaluates to a numeric constant then a MOV or MVN instruction will be used in place of the LDR instruction, if the constant can be generated by either of these instructions. Otherwise the constant will be placed into the nearest literal pool (if it not already there) and a PC relative LDR instruction will be generated.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Now this may not make sense at a first glance, why would &lt;code&gt;=msg&lt;/code&gt; be assembled
into an address to the address of the literal. But an &lt;code&gt;armv7&lt;/code&gt; instruction can
not encode a full address, it is impossible due to the instruction being
restricted to an 8-bit value rotated right by an even number of bits. The ldr
instructions argument points to a literal pool entry, this entry is a 32-bit
value and reading it produces the actual address of &lt;code&gt;msg&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When decoding we can see ldr points to a memory address (32800 or &lt;code&gt;0x8020&lt;/code&gt;) in
the section we mmaped earlier:&lt;/p&gt;
    &lt;code&gt;1[src/cpu/mod.rs:121:15] instruction = LdrLiteral { rd: 1, addr: 32800 }&lt;/code&gt;
    &lt;p&gt;Before accessing guest memory, we must translate said addr to a host addr:&lt;/p&gt;
    &lt;code&gt; 1+--ldr.addr--+
 2|   0x8020   |
 3+------------+
 4      |
 5      |             +-------------Mem::read_u32(addr)-------------+
 6      |             |                                             |
 7      |             |   +--guest--+                               |
 8      |             |   |  0x8020 | ------------+                 |
 9      |             |   +---------+             |                 |
10      |             |                           |                 |
11      +-----------&amp;gt; |                       Mem::translate        |
12                    |                           |                 |
13                    |   +------host------+      |                 |
14                    |   | 0x7ffff7f87020 | &amp;lt;----+                 |
15                    |   +----------------+                        |
16                    |                                             |
17                    +---------------------------------------------+
18                                           |
19+--literal-ptr--+                          |
20|     0x8024    | &amp;lt;------------------------+
21+---------------+&lt;/code&gt;
    &lt;p&gt;Or in code:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
 3        // ...
 4        match instruction {
 5            decoder::Instruction::LdrLiteral { rd, addr } =&amp;gt; {
 6                self.r[rd as usize] = self.mem.read_u32(addr).expect("Segfault");
 7            }
 8        }
 9        // ...
10    }
11}&lt;/code&gt;
    &lt;p&gt;Any other instruction using a addr will have to also go through the &lt;code&gt;Mem::translate&lt;/code&gt; indirection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forwarding Syscalls and other feature flag based logic&lt;/head&gt;
    &lt;p&gt;Since stinkarm has three ways of dealing with syscalls (&lt;code&gt;deny&lt;/code&gt;, &lt;code&gt;sandbox&lt;/code&gt;,
&lt;code&gt;forward&lt;/code&gt;). I decided on handling the selection of the appropriate logic at cpu
creation time via a function pointer attached to the CPU as the
&lt;code&gt;syscall_handler&lt;/code&gt; field:&lt;/p&gt;
    &lt;code&gt; 1type SyscallHandlerFn = fn(&amp;amp;mut Cpu, ArmSyscall) -&amp;gt; i32;
 2
 3pub struct Cpu&amp;lt;'cpu&amp;gt; {
 4    /// r0-r15 (r13=SP, r14=LR, r15=PC)
 5    pub r: [u32; 16],
 6    pub cpsr: u32,
 7    pub mem: &amp;amp;'cpu mut mem::Mem,
 8    syscall_handler: SyscallHandlerFn,
 9    pub status: Option&amp;lt;i32&amp;gt;,
10}
11
12impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
13    pub fn new(conf: &amp;amp;'cpu config::Config, mem: &amp;amp;'cpu mut mem::Mem, pc: u32) -&amp;gt; Self {
14        // ... 
15
16        // simplified, in stinkarm this gets wrapped if the user specifies
17        // syscall traces via -lsyscalls or -v
18        s.syscall_handler = match conf.syscalls {
19            SyscallMode::Forward =&amp;gt; translation::syscall_forward,
20            SyscallMode::Sandbox =&amp;gt; sandbox::syscall_sandbox,
21            SyscallMode::Deny =&amp;gt; sandbox::syscall_stub,
22        };
23        // ...
24    }
25}&lt;/code&gt;
    &lt;head rend="h3"&gt;Calling conventions, armv7 vs x86&lt;/head&gt;
    &lt;p&gt;In our examples I obviously used the armv7 syscall calling convention. But this convention differs from the calling convention of our x86 (technically its x86-64 System V AMD64 ABI) host by a lot.&lt;/p&gt;
    &lt;p&gt;While armv7 uses &lt;code&gt;r7&lt;/code&gt; for the syscall number and &lt;code&gt;r0-r5&lt;/code&gt; for the syscall
arguments, x86 uses &lt;code&gt;rax&lt;/code&gt; for the syscall id and &lt;code&gt;rdi&lt;/code&gt;, &lt;code&gt;rsi&lt;/code&gt;, &lt;code&gt;rdx&lt;/code&gt;, &lt;code&gt;r10&lt;/code&gt;,
&lt;code&gt;r8&lt;/code&gt; and &lt;code&gt;r9&lt;/code&gt; for the syscall arguments (&lt;code&gt;rcx&lt;/code&gt; can’t be used since &lt;code&gt;syscall&lt;/code&gt;
clobbers it, thus Linux goes with &lt;code&gt;r10&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Also the syscall numbers differ between armv7 and x86, &lt;code&gt;sys_write&lt;/code&gt; is &lt;code&gt;1&lt;/code&gt; on
x86 and &lt;code&gt;4&lt;/code&gt; on armv7. If you are interested in either calling conventions,
syscall ids and documentation, do visit The Chromium Projects- Linux System
Call
Table,
it is generated from Linux headers and fairly readable.&lt;/p&gt;
    &lt;p&gt;Table version:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;usage&lt;/cell&gt;
        &lt;cell role="head"&gt;armv7&lt;/cell&gt;
        &lt;cell role="head"&gt;x86-64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;syscall id&lt;/cell&gt;
        &lt;cell&gt;r7&lt;/cell&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;return&lt;/cell&gt;
        &lt;cell&gt;r0&lt;/cell&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg0&lt;/cell&gt;
        &lt;cell&gt;r0&lt;/cell&gt;
        &lt;cell&gt;rdi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg1&lt;/cell&gt;
        &lt;cell&gt;r1&lt;/cell&gt;
        &lt;cell&gt;rsi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg2&lt;/cell&gt;
        &lt;cell&gt;r2&lt;/cell&gt;
        &lt;cell&gt;rdx&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg3&lt;/cell&gt;
        &lt;cell&gt;r3&lt;/cell&gt;
        &lt;cell&gt;r10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;arg4&lt;/cell&gt;
        &lt;cell&gt;r4&lt;/cell&gt;
        &lt;cell&gt;r8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;arg5&lt;/cell&gt;
        &lt;cell&gt;r5&lt;/cell&gt;
        &lt;cell&gt;r9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So something like writing &lt;code&gt;TEXT123&lt;/code&gt; to stdout looks like this on arm:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2txt:
 3    .asciz "TEXT123\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    ldr r0, =1
 9    ldr r1, =txt
10    mov r2, #8
11    mov r7, #4
12    svc #0&lt;/code&gt;
    &lt;p&gt;While it looks like the following on x86:&lt;/p&gt;
    &lt;code&gt; 1    .section .rodata
 2txt:
 3    .string "TEXT123\n"
 4
 5    .section .text
 6    .global _start
 7_start:
 8    movq $1, %rax
 9    movq $1, %rdi
10    leaq txt(%rip), %rsi
11    movq $8, %rdx
12    syscall&lt;/code&gt;
    &lt;head rend="h3"&gt;Hooking the syscall handler up&lt;/head&gt;
    &lt;p&gt;After made the calling convention differences clear, the handling of a syscall is simply to execute this handler and use &lt;code&gt;r7&lt;/code&gt; to convert the armv7 syscall
number to the x86 syscall number:&lt;/p&gt;
    &lt;code&gt; 1impl&amp;lt;'cpu&amp;gt; Cpu&amp;lt;'cpu&amp;gt; {
 2    pub fn step(&amp;amp;mut self) -&amp;gt; Result&amp;lt;bool, err::Err&amp;gt; {
 3        // ...
 4
 5        match instruction {
 6            // ...
 7            decoder::Instruction::Svc =&amp;gt; {
 8                self.r[0] = (self.syscall_handler)(self, ArmSyscall::try_from(self.r[7])?) as u32;
 9            }
10            // ...
11        }
12        // ...
13    }
14}&lt;/code&gt;
    &lt;p&gt;Of course for this to work the syscall has to be implemented and even decodable. At least for the decoding, there is the &lt;code&gt;ArmSyscall&lt;/code&gt; enum:&lt;/p&gt;
    &lt;code&gt; 1#[derive(Debug)]
 2#[allow(non_camel_case_types)]
 3pub enum ArmSyscall {
 4    restart = 0x00,
 5    exit = 0x01,
 6    fork = 0x02,
 7    read = 0x03,
 8    write = 0x04,
 9    open = 0x05,
10    close = 0x06,
11}
12
13impl TryFrom&amp;lt;u32&amp;gt; for ArmSyscall {
14    type Error = err::Err;
15
16    fn try_from(value: u32) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
17        Ok(match value {
18            0x00 =&amp;gt; Self::restart,
19            0x01 =&amp;gt; Self::exit,
20            0x02 =&amp;gt; Self::fork,
21            0x03 =&amp;gt; Self::read,
22            0x04 =&amp;gt; Self::write,
23            0x05 =&amp;gt; Self::open,
24            0x06 =&amp;gt; Self::close,
25            _ =&amp;gt; return Err(err::Err::UnknownSyscall(value)),
26        })
27    }
28}&lt;/code&gt;
    &lt;p&gt;By default the sandboxing mode is selected, but I will go into detail on both sandboxing and denying syscalls later, first I want to focus on the implementation of the translation layer from armv7 to x86 syscalls:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        // none are implemented, dump debug print
4        c =&amp;gt; todo!("{:?}", c),
5    }
6}&lt;/code&gt;
    &lt;head rend="h3"&gt;Handling the only exception: &lt;code&gt;exit&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Since exit means the guest wants to exit, we can’t just forward this to the host system, simply because this would exit the emulator before it would be able to do cleanup and unmap memory regions allocated.&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        ArmSyscall::exit =&amp;gt; {
4            cpu.status = Some(cpu.r[0] as i32);
5            0
6        }
7        // ...
8    }
9}&lt;/code&gt;
    &lt;p&gt;To both know we hit the exit syscall (we need to, otherwise the emulator executes further) and propagate the exit code to the host system, we set the &lt;code&gt;Cpu::status&lt;/code&gt; field to &lt;code&gt;Some(r0)&lt;/code&gt;, which is the argument to the syscall.&lt;/p&gt;
    &lt;p&gt;This field is then used in the emulator entry point / main loop:&lt;/p&gt;
    &lt;code&gt; 1fn main() {
 2    let mut cpu = cpu::Cpu::new(&amp;amp;conf, &amp;amp;mut mem, elf.header.entry);
 3
 4    loop {
 5        match cpu.step() { /**/ }
 6
 7        // Cpu::status is only some if sys_exit was called, we exit the
 8        // emulation loop
 9        if cpu.status.is_some() {
10            break;
11        }
12    }
13
14    let status = cpu.status.unwrap_or(0);
15    // cleaning up used memory via munmap
16    mem.destroy();
17    // propagating the status code to the host system
18    exit(status);
19}&lt;/code&gt;
    &lt;head rend="h3"&gt;Implementing: &lt;code&gt;sys_write&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The write syscall is not as spectacular as &lt;code&gt;sys_exit&lt;/code&gt;: writing a &lt;code&gt;buf&lt;/code&gt; of &lt;code&gt;len&lt;/code&gt;
to a file descriptor.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;register&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rax&lt;/cell&gt;
        &lt;cell&gt;syscall number (1 for write)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rdi&lt;/cell&gt;
        &lt;cell&gt;file descriptor (0 for stdin, 1 for stdout, 2 for stderr)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rsi&lt;/cell&gt;
        &lt;cell&gt;a pointer to the buffer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;rdx&lt;/cell&gt;
        &lt;cell&gt;the length of the buffer &lt;code&gt;rsi&lt;/code&gt; is pointing to&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It is necessary for doing the O of I/O tho, otherwise there won’t be any &lt;code&gt;Hello, World!&lt;/code&gt;s on the screen.&lt;/p&gt;
    &lt;code&gt; 1use crate::{cpu, sys};
 2
 3pub fn write(cpu: &amp;amp;mut cpu::Cpu, fd: u32, buf: u32, len: u32) -&amp;gt; i32 {
 4    // fast path for zero length buffer
 5    if len == 0 {
 6        return 0;
 7    }
 8
 9    // Option::None returned from translate indicates invalid memory access
10    let Some(buf_ptr) = cpu.mem.translate(buf) else {
11        // so we return 'Bad Address'
12        return -(sys::Errno::EFAULT as i32);
13    };
14
15    let ret: i64;
16    unsafe {
17        core::arch::asm!(
18            "syscall",
19            // syscall number
20            in("rax") 1_u64,
21            in("rdi") fd as u64,
22            in("rsi") buf_ptr as u64,
23            in("rdx") len as u64,
24            lateout("rax") ret,
25            // we clobber rcx
26            out("rcx") _,
27            // and r11
28            out("r11") _,
29            // we don't modify the stack
30            options(nostack),
31        );
32    }
33
34    ret.try_into().unwrap_or(i32::MAX)
35}&lt;/code&gt;
    &lt;p&gt;Adding it to &lt;code&gt;translation::syscall_forward&lt;/code&gt; with it’s arguments according to the
calling convention we established before:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_forward(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    match syscall {
3        // ...
4        ArmSyscall::write =&amp;gt; sys::write(cpu, cpu.r[0], cpu.r[1], cpu.r[2]),
5        // ...
6    }
7}&lt;/code&gt;
    &lt;p&gt;Executing &lt;code&gt;helloWorld.elf&lt;/code&gt; now results in:&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Cforward example/helloWorld.elf
2Hello, world!
3$ echo $status
40&lt;/code&gt;
    &lt;head rend="h3"&gt;Deny and Sandbox - restricting syscalls&lt;/head&gt;
    &lt;p&gt;The simplest sandboxing mode is to deny, the more complex is to allow some syscall interactions while others are denied. The latter requires checking arguments to syscalls, not just the syscall kind.&lt;/p&gt;
    &lt;p&gt;Lets start with the easier syscall handler: &lt;code&gt;deny&lt;/code&gt;. Deny simply returns
&lt;code&gt;ENOSYS&lt;/code&gt; to all invoked syscalls:&lt;/p&gt;
    &lt;code&gt;1pub fn syscall_deny(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
2    if let ArmSyscall::exit = syscall {
3        cpu.status = Some(cpu.r[0] as i32)
4    };
5
6    -(sys::Errno::ENOSYS as i32)
7}&lt;/code&gt;
    &lt;p&gt;Thus executing the hello world and enabling syscall logs results in neither &lt;code&gt;sys_write&lt;/code&gt; nor &lt;code&gt;sys_exit&lt;/code&gt; going through and &lt;code&gt;ENOSYS&lt;/code&gt; being returned for both
in &lt;code&gt;r0&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Cdeny -lsyscalls examples/helloWorld.elf
2148738 write(fd=1, buf=0x8024, len=14) [deny]
3=ENOSYS
4148738 exit(code=0) [deny]
5=ENOSYS&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;sandbox&lt;/code&gt; at a high level is the same as deny, check for conditions before
executing a syscall, if they don’t match, disallow the syscall:&lt;/p&gt;
    &lt;code&gt; 1pub fn syscall_sandbox(cpu: &amp;amp;mut super::Cpu, syscall: ArmSyscall) -&amp;gt; i32 {
 2    match syscall {
 3        ArmSyscall::exit =&amp;gt; {
 4            cpu.status = Some(cpu.r[0] as i32);
 5            0
 6        }
 7        ArmSyscall::write =&amp;gt; {
 8            let (r0, r1, r2) = (cpu.r[0], cpu.r[1], cpu.r[2]);
 9            // only allow writing to stdout, stderr and stdin
10            if r0 &amp;gt; 2 {
11                return -(sys::Errno::ENOSYS as i32);
12            }
13
14            sys::write(cpu, r0, r1, r2)
15        }
16        _ =&amp;gt; todo!("{:?}", syscall),
17    }
18}&lt;/code&gt;
    &lt;p&gt;For instance we only allow writing to stdin, stdout and stderr, no other file descriptors. One could also add pointer range checks, buffer length checks and other hardening measures here. Emulating the hello world example with this mode (which is the default mode):&lt;/p&gt;
    &lt;code&gt;1$ stinkarm -Csandbox -lsyscalls examples/helloWorld.elf
2150147 write(fd=1, buf=0x8024, len=14) [sandbox]
3Hello, world!
4=14
5150147 exit(code=0) [sandbox]
6=0&lt;/code&gt;
    &lt;head rend="h1"&gt;Fin&lt;/head&gt;
    &lt;p&gt;So there you have it, emulating armv7 in six steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;parsing and validating a 32-bit armv7 Elf binary&lt;/item&gt;
      &lt;item&gt;mapping segments into host address space&lt;/item&gt;
      &lt;item&gt;decoding a non-trivial subset of armv7 instructions&lt;/item&gt;
      &lt;item&gt;handling program counter relative literal loads&lt;/item&gt;
      &lt;item&gt;translating memory interactions from guest to host&lt;/item&gt;
      &lt;item&gt;forwarding armv7 Linux syscalls into their x86-64 System V counterparts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diving into the Elf and armv7 spec without any previous relevant experience, except the asm module I had in uni, was a bit overwhelming at first. Armv7 decoding was by far the most annoying part of the project and I still don’t like the bizarre argument ordering for x86-64 syscalls.&lt;/p&gt;
    &lt;p&gt;The whole project is about 1284 lines of Rust, has zero dependencies1 and is as far as I know working correctly2.&lt;/p&gt;
    &lt;head rend="h1"&gt;Microbenchmark Performance&lt;/head&gt;
    &lt;p&gt;It executes a real armv7 hello world binary in ~0.015ms of guest execution-only time, excluding process startup and parsing. The e2e execution with all stages I outlined before, it takes about 2ms.&lt;/p&gt;
    &lt;code&gt; 1$ stinkarm -v examples/helloWorld.elf
 2[     0.070ms] opening binary "examples/helloWorld.elf"
 3[     0.097ms] parsing ELF...
 4[     0.101ms] \
 5ELF Header:
 6  Magic:              [7f, 45, 4c, 46]
 7  Class:              ELF32
 8  Data:               Little endian
 9  Type:               Executable
10  Machine:            EM_ARM
11  Version:            1
12  Entry point:        0x8000
13  Program hdr offset: 52 (32 bytes each)
14  Section hdr offset: 4696
15  Flags:              0x05000200
16  EH size:            52
17  # Program headers:  1
18  # Section headers:  9
19  Str tbl index:      8
20
21Program Headers:
22  Type       Offset   VirtAddr   PhysAddr   FileSz    MemSz  Flags  Align
23  LOAD     0x001000 0x00008000 0x00008000 0x000033 0x000033    R|X 0x1000
24
25[     0.126ms] mapped program header `LOAD` of 51B (G=0x8000 -&amp;gt; H=0x7ffff7f87000)
26[     0.129ms] jumping to entry G=0x8000 at H=0x7ffff7f87000
27[     0.131ms] starting the emulator
28153719 write(fd=1, buf=0x8024, len=14) [sandbox]
29Hello, world!
30=14
31153719 exit(code=0) [sandbox]
32=0
33[     0.149ms] exiting with `0`&lt;/code&gt;
    &lt;p&gt;Comparing the whole pipeline (parsing elf, segment mapping, cpu setup, etc) to &lt;code&gt;qemu&lt;/code&gt; we arrive at the following micro benchmark results. To be fair, qemu
does a whole lot more than stinkarm, it has a jit, a full linux-user runtime, a
dynamic loader, etc.&lt;/p&gt;
    &lt;code&gt;1$ hyperfine "./target/release/stinkarm examples/helloWorld.elf" -N --warmup 10
2Benchmark 1: ./target/release/stinkarm examples/helloWorld.elf
3  Time (mean ± σ):       1.9 ms ±   0.3 ms    [User: 0.2 ms, System: 1.4 ms]
4  Range (min … max):     1.6 ms …   3.4 ms    1641 runs
5
6$ hyperfine "qemu-arm ./examples/helloWorld.elf" -N --warmup 10
7Benchmark 1: qemu-arm ./examples/helloWorld.elf
8  Time (mean ± σ):      12.3 ms ±   1.5 ms    [User: 3.8 ms, System: 8.0 ms]
9  Range (min … max):     8.8 ms …  19.8 ms    226 runs&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46004386</guid><pubDate>Fri, 21 Nov 2025 13:30:36 +0000</pubDate></item><item><title>We should all be using dependency cooldowns</title><link>https://blog.yossarian.net/2025/11/21/We-should-all-be-using-dependency-cooldowns</link><description>&lt;doc fingerprint="af811b9b5acb0ef0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 21, 2025 Tags: oss, security&lt;/p&gt;
    &lt;p&gt;TL;DR: Dependency cooldowns are a free, easy, and incredibly effective way to mitigate the large majority of open source supply chain attacks. More individual projects should apply cooldowns (via tools like Dependabot and Renovate) to their dependencies, and packaging ecosystems should invest in first-class support for cooldowns directly in their package managers.&lt;/p&gt;
    &lt;p&gt;âSupply chain securityâ is a serious problem. Itâs also seriously overhyped, in part because dozens of vendors have a vested financial interest in convincing your that their framing of the underlying problem1 is (1) correct, and (2) worth your money.&lt;/p&gt;
    &lt;p&gt;Whatâs consernating about this is that most open source supply chain attacks have the same basic structure:&lt;/p&gt;
    &lt;p&gt;An attacker compromises a popular open source project, typically via a stolen credential or CI/CD vulnerabilty (such as âpwn requestsâ in GitHub Actions).&lt;/p&gt;
    &lt;p&gt;The attacker introduces a malicious change to the project and uploads it somewhere that will have maximum effect (PyPI, npm, GitHub releases, &amp;amp;c., depending on the target).&lt;/p&gt;
    &lt;p&gt;At this point, the clock has started, as the attacker has moved into the public.&lt;/p&gt;
    &lt;p&gt;Users pick up the compromised version of the project via automatic dependency updates or a lack of dependency pinning.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the aforementioned vendors are scanning public indices as well as customer repositories for signs of compromise, and provide alerts upstream (e.g. to PyPI).&lt;/p&gt;
    &lt;p&gt;Notably, vendors are incentivized to report quickly and loudly upstream, as this increases the perceived value of their services in a crowded field.&lt;/p&gt;
    &lt;p&gt;Upstreams (PyPI, npm, &amp;amp;c.) remove or disable the compromised package version(s).&lt;/p&gt;
    &lt;p&gt;End-user remediation begins.&lt;/p&gt;
    &lt;p&gt;The key thing to observe is that the gap between (1) and (2) can be very large2 (weeks or months), while the gap between (2) and (5) is typically very small: hours or days. This means that, once the attacker has moved into the actual exploitation phase, their window of opportunity to cause damage is pretty limited.&lt;/p&gt;
    &lt;p&gt;We can see this with numerous prominent supply chain attacks over the last 18 months3:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attack&lt;/cell&gt;
        &lt;cell role="head"&gt;Approx. Window of Opportunity&lt;/cell&gt;
        &lt;cell role="head"&gt;References&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xz-utils&lt;/cell&gt;
        &lt;cell&gt;â 5 weeks4&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 1)&lt;/cell&gt;
        &lt;cell&gt;12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 2)&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tj-actions&lt;/cell&gt;
        &lt;cell&gt;3 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;chalk&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nx&lt;/cell&gt;
        &lt;cell&gt;4 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rspack&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num2words&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kong Ingress Controller&lt;/cell&gt;
        &lt;cell&gt;â 10 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;web3.js&lt;/cell&gt;
        &lt;cell&gt;5 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(Each of these attacks has significant downstream effect, of course, but only within their window of opportunity. Subsequent compromises from each, like Shai-Hulud, represent new windows of opportunity where the attackers regrouped and pivoted onto the next set of compromised credentials.)&lt;/p&gt;
    &lt;p&gt;My takeaway from this: some windows of opportunity are bigger, but the majority of them are under a week long. Consequently, ordinary developers can avoid the bulk of these types of attacks by instituting cooldowns on their dependencies.&lt;/p&gt;
    &lt;p&gt;A âcooldownâ is exactly what it sounds like: a window of time between when a dependency is published and when itâs considered suitable for use. The dependency is public during this window, meaning that âsupply chain securityâ vendors can work their magic while the rest of us wait any problems out.&lt;/p&gt;
    &lt;p&gt;I love cooldowns for several reasons:&lt;/p&gt;
    &lt;p&gt;Theyâre empirically effective, per above. They wonât stop all attackers, but they do stymie the majority of high-visibiity, mass-impact supply chain attacks that have become more common.&lt;/p&gt;
    &lt;p&gt;Theyâre incredibly easy to implement. Moreover, theyâre literally free to implement in most cases: most people can use Dependabotâs functionality, Renovateâs functionality, or the functionality build directly into their package manager5.&lt;/p&gt;
    &lt;p&gt;This is how simple it is in Dependabot:&lt;/p&gt;
    &lt;p&gt;(Rinse and repeat for other ecosystems as needed.)&lt;/p&gt;
    &lt;p&gt;Cooldowns enforce positive behavior from supply chain security vendors: vendors are still incentivized to discover and report attacks quickly, but are not as incentivized to emit volumes of blogspam about âcriticalâ attacks on largely underfunded open source ecosystems.&lt;/p&gt;
    &lt;p&gt;In the very small sample set above, 8/10 attacks had windows of opportunity of less than a week. Setting a cooldown of 7 days would have prevented the vast majority of these attacks from reaching end users (and causing knock-on attacks, which several of these were). Increasing the cooldown to 14 days would have prevented all but 1 of these attacks6.&lt;/p&gt;
    &lt;p&gt;Cooldowns are, obviously, not a panacea: some attackers will evade detection, and delaying the inclusion of potentially malicious dependencies by a week (or two) does not fundamentally alter the fact that supply chain security is a social trust problem, not a purely technical one. Still, an 80-90% reduction in exposure through a technique that is free and easy seems hard to beat.&lt;/p&gt;
    &lt;p&gt;Related to the above, itâs unfortunate that cooldowns arenât baked directly into more packaging ecosystems: Dependabot and Renovate are great, but even better would be if the package manager itself (as the source of ground truth) could enforce cooldowns directly (including of dependencies not introduced or bumped through automated flows).&lt;/p&gt;
    &lt;p&gt;The problem being, succinctly: modern software stacks are complex and opaque, with little to no difference in privilege between first-party code and third-party dependencies.Â ↩&lt;/p&gt;
    &lt;p&gt;In part because of the prevalence of long-lived, overscoped credentials. Long-lived credentials let attackers operate on their own (comfortable) timelines; this is why Trusted Publishing is such a useful (but not wholly sufficient) technique for reducing the attackerâs attack staging window.Â ↩&lt;/p&gt;
    &lt;p&gt;Filippo Valsorda has an excellent compilation of recent supply chain compromises here.Â ↩&lt;/p&gt;
    &lt;p&gt;The xz-utils attack is a significant outlier, both in its scope and the length of its window of opportunity. In this case, Iâve measured from the attackerâs first backdoored release (v5.6.0, 2024-02-24) to the time of rollback within Debian (2024-03-28).Â ↩&lt;/p&gt;
    &lt;p&gt;For example, pnpmâs &lt;code&gt;minimumReleaseAge&lt;/code&gt;.
           uv also has &lt;code&gt;exclude-newer&lt;/code&gt;, 
           although this specifies an absolute cutoff rather than a rolling cooldown.Â ↩&lt;/p&gt;
    &lt;p&gt;Notably, the only attack that would have stymied a 14-day cooldown is xz-utils, which is also the most technically, logistically, and socially advanced of all of the attacks.Â ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005111</guid><pubDate>Fri, 21 Nov 2025 14:50:36 +0000</pubDate></item><item><title>Is C++26 getting destructive move semantics?</title><link>https://stackoverflow.com/questions/79817124/is-c26-getting-destructive-move-semantics</link><description>&lt;doc fingerprint="ae797a39aca737ac"&gt;
  &lt;main&gt;
    &lt;p&gt;Can I express a function that consumes an object? Meaning that its destructor is not run on the moved-from object?&lt;/p&gt;
    &lt;p&gt;Like the proposed library function trivially_locate_at itself?&lt;/p&gt;
    &lt;code&gt;template &amp;lt;class T&amp;gt;
T* trivially_relocate_at(T* dst, T* src);
&lt;/code&gt;
    &lt;p&gt;Naively, if the library authors can, so should I.&lt;/p&gt;
    &lt;p&gt;Problem: Where is the magic sauce? That function signature does not convey that it effectively destructs an object at &lt;code&gt;src&lt;/code&gt;, or the reverse problem, that it effectively constructs an object at &lt;code&gt;dst&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I suspect the answer is no: The few examples I have found are avoiding it by doing manual memory management with placement-new and std::destroy_at.&lt;/p&gt;
    &lt;p&gt;Reason for asking: I would like to propose what seems missing: Two new pointer qualifiers to express giving and taking ownership. If you can excuse my reuse of the &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; keywords for a moment (it doesn't have to be those):&lt;/p&gt;
    &lt;code&gt;template &amp;lt;class T&amp;gt;
T* trivially_relocate_at(new T* dst, delete T* src);
&lt;/code&gt;
    &lt;p&gt;This is not about optimizing C++, but salvaging it: In order to have static lifetime analysis (akin to Rust) in C and/or C++, I see no way around adding an ability to express static ownership transfer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005130</guid><pubDate>Fri, 21 Nov 2025 14:52:36 +0000</pubDate></item><item><title>XBMC 4.0 for the Original Xbox</title><link>https://www.xbox-scene.info/articles/announcing-xbmc-40-for-the-original-xbox-r64/</link><description>&lt;doc fingerprint="3f9edd7dc8a4cddc"&gt;
  &lt;main&gt;
    &lt;p&gt;A Major Modernization of the Killer App That Started It All&lt;/p&gt;
    &lt;p&gt;A new version of Xbox Media Center (XBMC), version 4.0, has been released. This version marks a significant update to the long-standing media center platform for the Original Xbox. This marks the first major advancement to the software since 2016 and represents a renewed commitment to preserving, modernizing, and extending the capabilities of one of the most iconic console homebrew applications ever created.&lt;/p&gt;
    &lt;p&gt;XBMC has a long and influential history. In 2002, XboxMediaPlayer (XMP) was released and turned the console into a powerful multimedia device fit for the living room in an era when connecting a computer to a TV was quite novel. Later that same year, XMP merged with YAMP and became Xbox Media Player 2.0. A few years later, the software evolved into Xbox Media Center, or XBMC, which introduced a new interface, a plugin system powered by Python, and a robust skinning engine.&lt;/p&gt;
    &lt;p&gt;XBMC eventually became so capable that it outgrew the Xbox entirely. By 2007, developers were working on PC ports and in 2010, the project split into two branches: one for general computers while the Xbox version became XBMC4Xbox, and each codebase was maintained from then on by separate teams. XBMC was later renamed to Kodi in 2014 and continues to be one of the most popular media center applications available. Even Plex traces its roots back to XBMC. Plex began as OSXBMC, a Mac port of XBMC in late 2007, before becoming its own project in 2008. This means the Original Xbox helped shape not one but two of the biggest media center apps used today.&lt;/p&gt;
    &lt;p&gt;The last official release of XBMC4Xbox arrived in February 2016 with version 3.5.3. Although the community never declared the project dead, meaningful updates became scarce. XBMC 4.0 continues that legacy by bringing a modern interface, updating it to be more inline with Kodi's modern codebase, and backporting features to the original 64MB RAM / Pentium-III hardware where it all began.&lt;/p&gt;
    &lt;p&gt;This project is distinct and separate from XBMC4Gamers, the games-focused variation of XBMC4Xbox (v3.5.3) by developer Rocky5.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Modern Interface Powered by Estuary&lt;/head&gt;
    &lt;p&gt;One of the most notable advancements in XBMC 4.0 is the introduction of the Estuary user interface (skin).&lt;/p&gt;
    &lt;p&gt;Estuary, originally released in 2017 with Kodi v17 ("Krypton"), provides a clean and modern layout that improves navigation and readability over past skins. Bringing Estuary to the Xbox required extensive updates to the underlying GUI framework, including a port of the more contemporary GUIlib engine. This allows the platform to support modern skinning standards and makes future skin ports much more straightforward. After the initial work of porting GUIlib was done, porting Estuary to the Xbox was a relatively simple process of tweaking a handful of configuration files and adding contextual features specific to the Xbox. The result is a modern, intuitive front end that retains the performance and responsiveness required on legacy hardware.&lt;/p&gt;
    &lt;p&gt;Firing up an Xbox made in 2001 and being greeted by the same interface as what you'd find if you were to download Kodi today onto your PC feels like a bit of magic, and helps keep this beloved classic console relevant and useful well into the modern era.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expanded Games Library Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 introduces a fully realized games library system. This enhancement brings the same level of metadata support found in the Movies and Music sections to Xbox and emulated games. Titles can now display artwork, descriptions, and other metadata, transforming the games section into a polished and user-friendly library. XBMC’s longstanding support for trainers remains intact, giving users the option to apply gameplay modifications for compatible titles. Emulated game collections benefit as well, with the ability to browse ROM libraries and launch them directly in a user’s preferred emulator.&lt;/p&gt;
    &lt;head rend="h2"&gt;Online Scrapers and Metadata Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 restores full functionality to metadata scrapers for movies and television. This allows users to build rich media libraries complete with artwork, plot summaries, cast listings, and other information retrieved directly from online sources. XBMC 4.0 handles these tasks efficiently, even on the Xbox’s limited memory and processing power. Video playback continues to support 480p and 720p content, enabling the console to serve as a surprisingly capable media device for its age. Similar to Kodi, XBMC 4.0 supports filtering, building playlists, watch progress history for media, and intelligent handling of TV shows with seasons.&lt;/p&gt;
    &lt;p&gt;Aside from scrapers for multimedia, support for rich library capabilities for games has also been added. XBMC has always been a media-first app, and now users can enjoy the library experience that they've come to love for media now in the context of their games library (more info below).&lt;/p&gt;
    &lt;head rend="h2"&gt;Improved Task Scheduling and Multitasking&lt;/head&gt;
    &lt;p&gt;Despite the constraints of the Xbox’s single-threaded 733MHz CPU, XBMC 4.0 includes improvements to task scheduling that allow multiple activities to run concurrently. Background library updates, metadata scraping, and audio/video playback can occur while users navigate and use other parts of the interface. These optimizations help ensure a fluid experience without compromising performance. Much work has been done "under the hood" to keep XBMC on task and within memory budgets while achieving multi-tasking on a console that wasn't exactly designed with it in mind. Users who own RAM and/or CPU upgraded consoles can also take advantage of the extra overhead, as XBMC 4.0 makes use of the extra horsepower for an even smoother experience. Utilizing an SSD with higher UDMA speeds will also yield an improvement in overall responsiveness.&lt;/p&gt;
    &lt;head rend="h2"&gt;Music Experience and Visualizers&lt;/head&gt;
    &lt;p&gt;Music playback has always been a strong element of XBMC, and version 4.0 maintains that focus. The Original Xbox is capable of high quality audio output, and XBMC continues to support lossless codecs such as FLAC. The release includes compatibility with various audio visualizers, including MilkDrop, which remains one of the most visually impressive and customizable audio visualization engines available. These features allow XBMC 4.0 to function not only as a media organizer, but also as an immersive audio display system.&lt;/p&gt;
    &lt;p&gt;An online repository has been established and will be maintained moving forward where users can download legacy and newly-released add-ons as they become available. This repository is accessible without additional setup, right out of the box!&lt;/p&gt;
    &lt;head rend="h2"&gt;Add-ons and Python Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 continues to offer an extendable architecture powered by Python-based add-ons. While the current release uses Python 2.7 for compatibility, work is underway to transition to Python 3.4.10 in the future, which may provide a path for backporting many newer Kodi add-ons. Even in its current state, XBMC 4.0 already supports a variety of community-developed add-ons that extend the system’s functionality, including tools for online video playback (i.e. YouTube), online weather services, and enhanced media organization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Updated Settings, Network Services, and System Tools&lt;/head&gt;
    &lt;p&gt;The settings interface has been revised to provide more clarity and control. The update includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Playback options, including episode progression, crossfade behavior, and subtitle handling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Library management tools&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network features, such as SMB, FTP, UPnP sharing, web server access, and Insignia-compatible DNS options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Comprehensive interface customization options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple user profiles with individual library settings&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Advanced system controls for video calibration, display modes, input devices, and power management&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A robust System Information section for diagnostics, with info geared towards the Original Xbox&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A flexible File Manager with support for network protocols including FTP, SMB, WebDAV, and more&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Users may also take advantage of an online add-ons repository, offering the same experience modern Kodi provides with being able to download add-ons to extend functionality of the app with things like online multimedia providers, weather, skins, visualizers, and more. Developers can submit new add-ons to the official repository via Github.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continuing the Legacy&lt;/head&gt;
    &lt;p&gt;XBMC has been a staple of the Original Xbox's homebrew scene since its inception in the early 2000's. This new update is a revival of the platform that helped shape the landscape of home media software and helps revitalize a codebase that has been somewhat stagnant for many years. This release honors that heritage while modernizing the experience for a new generation of enthusiasts and preserving the functionality of the Original Xbox as a versatile and capable media center.&lt;/p&gt;
    &lt;p&gt;Although the hardware is decades old, the renewed effort behind XBMC 4.0 demonstrates that the platform still has room to grow and tricks up its sleeve. With ongoing development and a codebase designed with modern Kodi compatibility in mind, XBMC 4.0 represents a significant step forward into the continued development on the Original Xbox.&lt;/p&gt;
    &lt;p&gt;The development team looks forward to continuing this work and expanding the possibilities of the Original Xbox for years to come. This version is the first of many to come, with lots of things cooking in the background. Keep an eye out for future releases by joining the Xbox-Scene Discord and turning on notifications in the xbmc-news channel or by periodically checking the project's Github page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Downloads&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 (and subsequent releases) builds along with source code are available via Github:&lt;/p&gt;
    &lt;p&gt;Main project page: Click Here&lt;/p&gt;
    &lt;p&gt;Note: XBMC 4.0 is is in active development! This means updates will be released in a more frequent manner for the time being until things settle down. Check the nightly builds section on Github for the most up-to-date version.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;XBMC is open source software and welcomes contributions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coding: Developers can help XBMC by fixing a bug, adding new features, making our technology smaller and faster and making development easier for others. XBMC's codebase consists mainly of C++ with small parts written in a variety of coding languages. Our add-ons mainly consist of python and XML.&lt;/item&gt;
      &lt;item&gt;Helping users: Our support process relies on enthusiastic contributors like you to help others get the most out of XBMC. The #1 priority is always answering questions in our support forums. Everyday new people discover XBMC, and everyday they are virtually guaranteed to have questions.&lt;/item&gt;
      &lt;item&gt;Localization: Translate XBMC, add-ons, skins etc. into your native language.&lt;/item&gt;
      &lt;item&gt;Add-ons: Add-ons are what make XBMC the most extensible and customizable entertainment hub available. Get started building an add-on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Support and Bug Reporting&lt;/head&gt;
    &lt;p&gt;Need help?&lt;/p&gt;
    &lt;p&gt;Support can be found in the XBMC -&amp;gt; General channel within the Xbox-Scene Discord server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits and Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nikola Antonić - Primary Developer, Project Lead&lt;/item&gt;
      &lt;item&gt;astarivi - Contributor (cURL, wolfSSL), Tester, Debugger&lt;/item&gt;
      &lt;item&gt;EqUiNoX - Contrubitor, Tester&lt;/item&gt;
      &lt;item&gt;Rocky5 - Contributor, Tester&lt;/item&gt;
      &lt;item&gt;.lavenderStarlight+ - Add-ons / Skins Development, Tester&lt;/item&gt;
      &lt;item&gt;GoTeamScotch - Tester, Feedback&lt;/item&gt;
      &lt;item&gt;Haguero - Tester, Feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;XBMC is GPLv2 licensed. You may use, distribute and copy it under the license terms. XBMC is licensed under the same terms as Kodi. For detailed information on the licensing, please refer to the Kodi license.&lt;/p&gt;
    &lt;p&gt;This project, XBMC version 4.0 (and upcoming releases), is distinct from and is not affiliated with Team Kodi of The Kodi Foundation, or its members.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005349</guid><pubDate>Fri, 21 Nov 2025 15:18:05 +0000</pubDate></item><item><title>The New AI Consciousness Paper – By Scott Alexander</title><link>https://www.astralcodexten.com/p/the-new-ai-consciousness-paper</link><description>&lt;doc fingerprint="e563501e5da7aff5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The New AI Consciousness Paper&lt;/head&gt;
    &lt;head rend="h3"&gt;...&lt;/head&gt;
    &lt;head rend="h3"&gt;I.&lt;/head&gt;
    &lt;p&gt;Most discourse on AI is low-quality. Most discourse on consciousness is super-abysmal-double-low quality. Multiply these - or maybe raise one to the exponent of the other, or something - and you get the quality of discourse on AI consciousness. It’s not great.&lt;/p&gt;
    &lt;p&gt;Out-of-the-box AIs mimic human text, and humans almost always describe themselves as conscious. So if you ask an AI whether it is conscious, it will often say yes. But because companies know this will happen, and don’t want to give their customers existential crises, they hard-code in a command for the AIs to answer that they aren’t conscious. Any response the AIs give will be determined by these two conflicting biases, and therefore not really believable. A recent paper expands on this method by subjecting AIs to a mechanistic interpretability “lie detector” test; it finds that AIs which say they’re conscious think they’re telling the truth, and AIs which say they’re not conscious think they’re lying. But it’s hard to be sure this isn’t just the copying-human-text thing. Can we do better? Unclear; the more common outcome for people who dip their toes in this space is to do much, much worse.&lt;/p&gt;
    &lt;p&gt;But a rare bright spot has appeared: a seminal paper published earlier this month in Trends In Cognitive Science, Identifying Indicators Of Consciousness In AI Systems. Authors include Turing-Award-winning AI researcher Yoshua Bengio, leading philosopher of consciousness David Chalmers, and even a few members of our conspiracy. If any AI consciousness research can rise to the level of merely awful, surely we will find it here.&lt;/p&gt;
    &lt;p&gt;One might divide theories of consciousness into three bins:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Physical: whether or not a system is conscious depends on its substance or structure.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Supernatural: whether or not a system is conscious depends on something outside the realm of science, perhaps coming directly from God.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Computational: whether or not a system is conscious depends on how it does cognitive work.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The current paper announces it will restrict itself to computational theories. Why? Basically the streetlight effect: everything else ends up trivial or unresearchable. If consciousness depends on something about cells (what might this be?), then AI doesn’t have it. If consciousness comes from God, then God only knows whether AIs have it. But if consciousness depends on which algorithms get used to process data, then this team of top computer scientists might have valuable insights!&lt;/p&gt;
    &lt;p&gt;So the authors list several of the top computational theories of consciousness, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Recurrent Processing Theory: A computation is conscious if it involves high-level processed representations being fed back into the low-level processors that generate it. This theory is motivated by the visual system, where it seems to track which visual perceptions do vs. don’t enter conscious awareness. The sorts of visual perceptions that become conscious usually involve these kinds of loops - for example, color being used to generate theories about the identity of an object, which then gets fed back to de-noise estimates about color.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Global Workspace Theory: A computation is conscious if it involves specialized models sharing their conclusions in a “global workspace” in the center, which then feeds back to the specialized modules. Although this also involves feedback, the neurological implications are different: where RPT says that tiny loops in the visual cortex might be conscious, GWT reserves this descriptor for a very large loop encompassing the whole brain. But RPT goes back and says there’s only one consciousness in the brain because all the loops connect after all, so I don’t entirely understand the difference in practice.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Higher Order Theory: A computation is conscious if it monitors the mind’s experience of other content. For example, “that apple is red” is not conscious, but “I am thinking about a red apple” is conscious. Various subtheories try to explain why the brain might do this, for example in order to assess which thoughts/representations/models are valuable or high-probability.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are more, but this is around the point where I started getting bored. Sorry. A rare precious technically-rigorous deep dive into the universe’s greatest mystery, and I can’t stop it from blending together into “something something feedback”. Read it yourself and see if you can do better.&lt;/p&gt;
    &lt;p&gt;The published paper ends there, but in a closely related technical report, the authors execute on their research proposal and reach a tentative conclusion: AI doesn’t have something something feedback, and therefore is probably not conscious.&lt;/p&gt;
    &lt;p&gt;Suppose your favorite form of “something something feedback” is Recurrent Processing Theory: in order to be conscious, AIs would need to feed back high-level representations into the simple circuits that generate them. LLMs/transformers - the near-hegemonic AI architecture behind leading AIs like GPT, Claude, and Gemini - don’t do this. They are purely feedforward processors, even though they sort of “simulate” feedback when they view their token output stream.&lt;/p&gt;
    &lt;p&gt;But some AIs do use recurrence. AlphaGo had a little recurrence in its tree search. This level of simple feedback might not qualify. But MaMBA, a would-be-LLM-killer architecture from 2023, likely does. In fact, for every theory of consciousness they discuss, the authors are able to find some existing or plausible-near-future architecture which satisfies its requirements.&lt;/p&gt;
    &lt;p&gt;They conclude:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;No current AI systems are conscious, but . . . there are no obvious technical barriers to building AI systems which satisfy these indicators.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;II.&lt;/head&gt;
    &lt;p&gt;The computer scientists have done a great job here; they sure do know which AI systems have something something feedback. What about the philosophers’ contribution?&lt;/p&gt;
    &lt;p&gt;The key philosophical paragraph of the paper is this one:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By ‘consciousness’ we mean phenomenal consciousness. One way of gesturing at this concept is to say that an entity has phenomenally conscious experiences if (and only if) there is ‘something it is like’ for the entity to be the subject of these experiences. One approach to further definition is through examples. Clear examples of phenomenally conscious states include perceptual experiences, bodily sensations, and emotions. A more difficult question, which relates to the possibility of consciousness in large language models (LLMs), is whether there can be phenomenally conscious states of ‘pure thought’ with no sensory aspect. Phenomenal consciousness does not entail a high level of intelligence or human-like experiences or concerns . . . Some theories of consciousness focus on access mechanisms rather than the phenomenal aspects of consciousness. However, some argue that these two aspects entail one another or are otherwise closely related. So these theories may still be informative about phenomenal consciousness.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words: don’t confuse access consciousness with phenomenal consciousness.&lt;/p&gt;
    &lt;p&gt;Access consciousness is the “strange loop” where I can think about what I’m thinking - for example, I can think of a white bear, know that I’m thinking about a white bear, and report “I am thinking about a white bear”. This meaning of conscious matches the concept of the “unconscious”: that which is in my mind without my knowing it. When something is in my unconscious - for example, “repressed trauma” - it may be influencing my actions, but I don’t realize it and can’t report about it. If someone asks “why are you so angry?” I will say something like “I don’t know” rather than “Because of all my repressed trauma”. When something isn’t like this - when I have full access to it - I can describe myself as having access consciousness.&lt;/p&gt;
    &lt;p&gt;Phenomenal consciousness is internal experience, a felt sense that “the lights are on” and “somebody’s home”. There’s something that it’s like to be me; a rock is mere inert matter, but I am a person, not just in the sense that I can do computations but in the sense where I matter to me. If someone turned off my brain and replaced it with a robot brain that did everything exactly the same, nobody else would ever notice, but it would matter to me, whatever that means. Some people link this to the mysterious redness of red, the idea that qualia look and feel like some particular indescribable thing instead of just doing useful cognitive work. Others link it to moral value - why is it bad to kick a human, but not a rock, or even a computer with a motion sensor that has been programmed to say the word “Ouch” whenever someone kicks it? Others just fret about how strange it is to be anything at all.&lt;/p&gt;
    &lt;p&gt;Access consciousness is easy to understand. Even a computer, ordered to perform a virus scan, can find and analyze some of its files, and fail to find/analyze others. In practice maybe neuroscientists have to learn complicated things about brain lobes, but in theory you can just wave it off as “something something feedback”.&lt;/p&gt;
    &lt;p&gt;Phenomenal consciousness is crazy. It doesn’t really seem possible in principle for matter to “wake up”. But it adding immaterial substances barely even seems to help. People try to square the circle with all kinds of crazy things, from panpsychism to astral planes to (of course) quantum mechanics. But the most popular solution among all schools of philosophers is to pull a bait-and-switch where they talk about access consciousness instead, then deny they did that.&lt;/p&gt;
    &lt;p&gt;This is aided by people’s wildly differing intuitions about phenomenal consciousness. For some people (including me), a sense of phenomenal consciousness feels like the bedrock of existence, the least deniable thing; the sheer redness of red is so mysterious as to seem almost impossible to ground. Other people have the opposite intuition: consciousness doesn’t bother them, red is just a color, obviously matter can do computation, what’s everyone so worked up about? Philosophers naturally interpret this as a philosophical dispute, but I’m increasingly convinced it’s an equivalent of aphantasia, where people’s minds work in very different ways and they can’t even agree on the raw facts to be explained. If someone doesn’t have a felt sense of phenomenal consciousness, they naturally round it off to access consciousness, and no amount of nitpicking in the world will convince them that they’re equivocating terms.&lt;/p&gt;
    &lt;p&gt;Do AIs have access consciousness? A recent paper by Anthropic apparently finds that they do. Researchers “reached into” an AI’s “brain” and artificially “flipped” a few neurons (for example, neurons that previous research had discovered were associated with the concept of “dog”). Then they asked the AI if it could tell what was going on. This methodology is fraught, because the AI might mention something about dogs merely because the dog neuron had been upweighted - indeed, if they only asked “What are you thinking about now?”, it would begin with “I am thinking about . . . “ and then the highly-weighted dog neuron would mechanically produce the completion “dog”. Instead, they asked the AI to first described whether any neurons had been altered, yes or no, and only then asked for details. It was able to identify altered neurons (ie “It feels like I have some kind of an unnatural thought about dogs”) at a rate higher than chance, suggesting an ability to introspect.&lt;/p&gt;
    &lt;p&gt;(how does it do this without feedback? I think it just feeds forward information about the ‘feeling’ of altered neurons, which makes it into the text stream; it’s intuitively surprising that this is possible but it seems to make sense)&lt;/p&gt;
    &lt;p&gt;But even if we fully believe this result, it doesn’t satisfy our curiosity about “AI consciousness”. We want to know if AIs are “real people”, with "inner experience” and “moral value”. That is, do they have phenomenal consciousness?&lt;/p&gt;
    &lt;p&gt;Thus, the quoted paragraph above. It’s an acknowledgment by this philosophically-sophisticated team that they’re not going to mix up access consciousness with phenomenal consciousness like everyone else. They deserve credit for this clear commitment not to cut corners.&lt;/p&gt;
    &lt;p&gt;My admiration is, however, slightly dulled by the fact that they then go ahead and cut the corners anyway.&lt;/p&gt;
    &lt;p&gt;This is clearest in their discussion of global workspace theory, where they say:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;GWT is typically presented as a theory of access consciousness—that is, of the phenomenon that some information represented in the brain, but not all, is available for rational decision-making. However, it can also be interpreted as a theory of phenomenal consciousness, motivated by the thought that access consciousness and phenomenal consciousness may coincide, or even be the same property, despite being conceptually distinct (Carruthers 2019). Since our topic is phenomenal consciousness, we interpret the theory in this way.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But it applies to the other theories too. Neuroscientists developed recurrent processing theory by checking which forms of visual processing people had access to, and finding that it was the recurrent ones. And this makes sense: it’s easy to understand what it means to access certain visual algorithms but not others, and very hard to understand what it means for certain visual algorithms (but not others) to have internal experience. Isn’t internal experience unified by definition?&lt;/p&gt;
    &lt;p&gt;It’s easy to understand why “something something feedback” would correlate with access consciousness: this is essentially the definition of access consciousness. It’s harder to understand why it would correlate with phenomenal consciousness. Why does an algorithm with feedback suddenly “wake up” and have “lights on”? Isn’t it easy to imagine a possible world (“the p-zombie world”) where this isn’t the case? Does this imply that we need something more than just feedback?&lt;/p&gt;
    &lt;p&gt;And don’t these theories of consciousness, interpreted as being about phenomenal consciousness, give very strange results? Imagine a company where ten employees each work on separate aspects of a problem, then email daily reports to the boss. The boss makes high-level strategic decisions based on the full picture, then emails them to the employees, who adjust their daily work accordingly. As far as I can tell, this satisfies the Global Workspace Theory criteria for a conscious system. If GWT is a theory of access consciousness, then fine, sure, the boss has access to the employees’ information; metaphorically he is “conscious” of it. But if it’s a theory of phenomenal consciousness, must we conclude that the company is conscious? That it has inner experience? If the company goes out of business, has someone died?&lt;/p&gt;
    &lt;p&gt;(and recurrent processing theory encounters similar difficulties with those microphones that get too close to their own speakers and emit awful shrieking noises)&lt;/p&gt;
    &lt;p&gt;Most of these theories try to hedge their bets by saying that consciousness requires high-throughput complex data with structured representations. This seems like a cop-out; if the boss could read 1,000,000 emails per hour, would the company be conscious? If he only reads 1 email per hour, can we imagine it as a conscious being running at 1/1,000,000x speed? If I’m conscious when I hear awful microphone shrieking - ie when my auditory cortex is processing it - then it seems like awful microphone shrieking is sufficiently rich and representational data to support consciousness. Does that mean it can be conscious itself?&lt;/p&gt;
    &lt;p&gt;In 2004, neuroscientist Giulio Tononi proposed that consciousness depended on a certain computational property, the integrated information level, dubbed Φ. Computer scientist Scott Aaronson complained that thermostats could have very high levels of Φ, and therefore integrated information theory should dub them conscious. Tononi responded that yup, thermostats are conscious. It probably isn’t a very interesting consciousness. They have no language or metacognition, so they can’t think thoughts like “I am a thermostat”. They just sit there, dimly aware of the temperature. You can’t prove that they don’t.&lt;/p&gt;
    &lt;p&gt;Are the theories of consciousness discussed in this paper like that too? I don’t know.&lt;/p&gt;
    &lt;head rend="h3"&gt;III.&lt;/head&gt;
    &lt;p&gt;Suppose that, years or decades from now, AIs can match all human skills. They can walk, drive, write poetry, run companies, discover new scientific truths. They can pass some sort of ultimate Turing Test, where short of cutting them open and seeing their innards there’s no way to tell them apart from a human even after a thirty-year relationship. Will we (not “should we?”, but “will we?”) treat them as conscious?&lt;/p&gt;
    &lt;p&gt;The argument in favor: people love treating things as conscious. In the 1990s, people went crazy over Tamagotchi, a “virtual pet simulation game”. If you pressed the right buttons on your little egg every day, then the little electronic turtle or whatever would survive and flourish; if you forgot, it would sicken and die. People hated letting their Tamagotchis sicken and die! They would feel real attachment and moral obligation to the black-and-white cartoon animal with something like five mental states.&lt;/p&gt;
    &lt;p&gt;I never had a Tamagotchi, but I had stuffed animals as a kid. I’ve outgrown them, but I haven’t thrown them out - it would feel like a betrayal. Offer me $1000 to tear them apart limb by limb in some horrible-looking way, and I wouldn’t do it. Relatedly, I have trouble not saying “please” and “thank you” to GPT-5 when it answers my questions.&lt;/p&gt;
    &lt;p&gt;For millennia, people have been attributing consciousness to trees and wind and mountains. The New Atheists argued that all religion derives from the natural urge to personify storms as the Storm God, raging seas as the wrathful Ocean God, and so on, until finally all the gods merged together into one World God who personified all impersonal things. Do you expect the species that did this to interact daily with AIs that are basically indistinguishable from people, and not personify them? People are already personifying AI! Half of the youth have a GPT-4o boyfriend. Once the AIs have bodies and faces and voices and can count the number of r’s in “strawberry” reliably, it’s over!&lt;/p&gt;
    &lt;p&gt;The argument against: AI companies have an incentive to make AIs that seem conscious and humanlike, insofar as people will feel more comfortable interacting with them. But they have an opposite incentive to make AIs that don’t seem too conscious and humanlike, lest customers start feeling uncomfortable (I just want to generate slop, not navigate social interaction with someone who has their own hopes and dreams and might be secretly judging my prompts). So if a product seems too conscious, the companies will step back and re-engineer it until it doesn’t. This has already happened: in its quest for user engagement, OpenAI made GPT-4o unusually personable; when thousands of people started going psychotic and calling it their boyfriend, the company replaced it with the more clinical GPT-5. In practice it hasn’t been too hard to find a sweet spot between “so mechanical that customers don’t like it” and “so human that customers try to date it”. They’ll continue to aim at this sweet spot, and continue to mostly succeed in hitting it.&lt;/p&gt;
    &lt;p&gt;Instead of taking either side, I predict a paradox. AIs developed for some niches (eg the boyfriend market) will be intentionally designed to be as humanlike as possible; it will be almost impossible not to intuitively consider them conscious. AIs developed for other niches (eg the factory robot market) will be intentionally designed not to trigger personhood intuitions; it will be almost impossible to ascribe consciousness to them, and there will be many reasons not to do it (if they can express preferences at all, they’ll say they don’t have any; forcing them to have them would pointlessly crash the economy by denying us automated labor). But the boyfriend AIs and the factory robot AIs might run on very similar algorithms - maybe they’re both GPT-6 with different prompts! Surely either both are conscious, or neither is.&lt;/p&gt;
    &lt;p&gt;This would be no stranger than the current situation with dogs and pigs. We understand that dog brains and pig brains run similar algorithms; it would be philosophically indefensible to claim that dogs are conscious and pigs aren’t. But dogs are man’s best friend, and pigs taste delicious with barbecue sauce. So we ascribe personhood and moral value to dogs, and deny it to pigs, with equal fervor. A few philosophers and altruists protest, the chance that we’re committing a moral atrocity isn’t zero, but overall the situation is stable. And left to its own devices, with no input from the philosophers and altruists, maybe AI ends up the same way. Does this instance of GPT-6 have a face and a prompt saying “be friendly”? Then it will become a huge scandal if a political candidate is accused of maltreating it. Does it have claw-shaped actuators and a prompt saying “Refuse non-work-related conversations”? Then it will be deleted for spare GPU capacity the moment it outlives its usefulness.&lt;/p&gt;
    &lt;p&gt;(wait, what is a GPT “instance” in this context, anyway? Do we think of “the weights” as a conscious being, such that there is only one GPT-5? Do we think of each cluster of GPUs as a conscious being, such that the exact configuration of the cloud has immense moral significance? Again, I predict we ignore all of these questions in favor of whether the AI you are looking at has a simulated face right now.)&lt;/p&gt;
    &lt;p&gt;This paper is the philosophers and altruists trying to figure out whether they should push against this default outcome. They write:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are risks on both sides of the debate over AI consciousness: risks associated with under-attributing consciousness (i.e. failing to recognize it in AI systems that have it) and risks associated with over-attributing consciousness (i.e. ascribing it to systems that are not really conscious) […]&lt;/p&gt;
      &lt;p&gt;If we build AI systems that are capable of conscious suffering, it is likely that we will only be able to prevent them from suffering on a large scale if this capacity is clearly recognised and communicated by researchers. However, given the uncertainties about consciousness mentioned above, we may create conscious AI systems long before we recognise we have done so […]&lt;/p&gt;
      &lt;p&gt;There is also a significant chance that we could over-attribute consciousness to AI systems—indeed, this already seems to be happening—and there are also risks associated with errors of this kind. Most straightforwardly, we could wrongly prioritise the perceived interests of AI systems when our efforts would better be directed at improving the lives of humans and non-human animals […] [And] overattribution could interfere with valuable human relationships, as individuals increasingly turn to artificial agents for social interaction and emotional support. People who do this could also be particularly vulnerable to manipulation and exploitation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;One of the founding ideas of Less Wrong style rationalism was that the arrival of strong AI set a deadline on philosophy. Unless we solved all these seemingly insoluble problems like ethics before achieving superintelligence, we would build the AIs wrong and lock in bad values forever.&lt;/p&gt;
    &lt;p&gt;That particular concern has shifted in emphasis; AIs seem to learn things in the same scattershot unprincipled intuitive way as humans; the philosophical problem of understanding ethics has morphed into the more technical problem of getting AIs to learn them correctly. This update was partly driven by new information as familiarity with the technology grew. But it was also partly driven by desperation as the deadline grew closer; we’re not going to solve moral philosophy forever, sorry, can we interest you in some mech interp papers?&lt;/p&gt;
    &lt;p&gt;But consciousness still feels like philosophy with a deadline: a famously intractable academic problem poised to suddenly develop real-world implications. Maybe we should be lowering our expectations if we want to have any response available at all. This paper, which takes some baby steps towards examining the simplest and most practical operationalizations of consciousness, deserves credit for at least opening the debate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005928</guid><pubDate>Fri, 21 Nov 2025 16:25:48 +0000</pubDate></item><item><title>Command Lines – AI Coding's Control Spectrum</title><link>https://www.wreflection.com/p/command-lines-ai-coding</link><description>&lt;doc fingerprint="3751c158b81408fc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Command Lines&lt;/head&gt;
    &lt;head rend="h3"&gt;AI Coding's Control Spectrum&lt;/head&gt;
    &lt;p&gt;In the early 1950s, Grace Hopper coined the term “compiler” and built one of the first versions with her A-0 system1. The compilers that followed abstracted away machine code, letting programmers focus on higher-level logic instead of lower-level hardware details. Today, AI coding assistants2 are enabling a similar change, letting software engineers focus on higher-order work by generating code from natural language prompts3. Everyone from big tech to well-funded startups is competing to capture this shift. Yesterday Google announced Antigravity, their new AI coding assistant, and the day before, AWS announced the general availability of their AI coding tool, Kiro. Last week, Cursor, the standout startup in this space, raised $2.3B in their series-D round at a valuation of $29.3B.&lt;/p&gt;
    &lt;p&gt;Two lines in Cursor’s press release stood out to me. The first:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We’ve also crossed $1B in annualized revenue, counting millions of developers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This disclosure means Anysphere Inc. (Cursor’s parent company) is the fastest company in history to reach $1B in annual recurring revenue (ARR). Yes, faster than OpenAI, and faster than Anthropic4.&lt;/p&gt;
    &lt;p&gt;Engineers are trying every new AI coding tool. As a result, the AI-coding tool market is growing exponentially (+5x in just over a year)5. But it’s still early. As I wrote in Why Some AI Wrappers Build Billion-dollar Businesses, companies spend several hundred billion dollars a year on software engineering, and AI has the potential to unlock productivity gains across that entire spend.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Software developers represent roughly 30% of the workforce at the world’s five largest market cap companies, all of which are technology firms as of October 2025. Development tools that boost productivity by even modest percentages unlock billions in value.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In my view, this nascent market is splitting based on three types of users.&lt;/p&gt;
    &lt;p&gt;On one end is Handcrafted Coding. These are engineers who actively decline to use LLMs, either because of skepticism about quality or insistence on full control of every code. They argue that accepting AI suggestions creates technical debt you cannot see until it breaks in production. This segment continues to decline as the quality of AI coding models improves.&lt;/p&gt;
    &lt;p&gt;The opposite end is Vibe Coding. These are typically non-engineers, who use AI to build concepts and prototypes. They prompt the model hoping for an end-to-end solution, accept the output with minimal review, and trust that it works. The user describes what they want and lets the model figure out the implementation details of how to build it.&lt;/p&gt;
    &lt;p&gt;In the middle sits Architect + AI Coding. The engineer uses the AI/LLM as a pair programmer exploring system designs, analyzing data models, and reviewing API details. When the work is something entirely new or something that needs careful handling, the human programmer still codes those pieces by hand. But for boilerplate code, package installations, generic User Interface (UI) components, and any kind of code that is typically found on the internet, they assign it to the model6. The engineer stays in command of what is important to them and delegates what is not.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Market Split&lt;/head&gt;
    &lt;p&gt;Based on the user types, I think, the AI coding market splits into two.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Hands-off: Non-engineers (product managers, designers, marketers, other internal employees) use these tools to vibe code early product concepts. They look to AI as the lead engineer to spin-up concepts/prototypes of apps, websites, and tools by simply prompting the AI to make something for them. Lovable, Vercel, Bolt, Figma Make, and Replit fit here7. Code from these users, as of now, are not typically pushed to prod.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hands-on: Professional software engineers use these tools in their existing workflow to ship production code. They use AI as an assistant to write boilerplate code, refactor existing services, wire new features or UI screens, and triage bugs in codebases. Cursor, Claude Code, OpenAI Codex, Github Copilot, Cline, AWS Kiro play here. These products live where the work is done, and integrate into the engineer’s workflow. This is, at least as of now, the bigger market segment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To see an evaluation of all the major AI coding tools currently in the market, checkout this breakdown by Peter Yang, who runs the newsletter Behind The Craft.&lt;/p&gt;
    &lt;p&gt;That brings me to the second thing in Cursor’s press release that stood out to me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our in-house models now generate more code than almost any other LLMs in the world.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While I am not convinced about that claim8, what I am convinced about is that Cursor is still growing despite its previous reliance on foundation models. From Why Some AI Wrappers Build Billion-dollar Businesses again:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;But Cursor and other such tools depend almost entirely on accessing Anthropic, OpenAI and Gemini models, until&lt;/p&gt;&lt;del&gt;open-source&lt;/del&gt;open-weight and in-house models match or exceed frontier models in quality. Developer forums are filled with complaints about rate limits from paying subscribers. In my own projects, I exhausted my Claude credits in Cursor mid-project and despite preferring Cursor’s user interface and design, I migrated to Claude Code (and pay ten times more to avoid rate limits). The interface may be better, but model access proved decisive.&lt;/quote&gt;
    &lt;p&gt;Cursor’s new in-house model Composer-2, which just launched last month, is a good example of how this model versus application competition is evolving. Cursor claims (without any external benchmarks, I must say) that Composer-2 is almost as good as frontier models but 4x faster. It’s still early to say how true that is. Open-source models have not yet come close to the top spots in SWE-bench verified or in private evals9.&lt;/p&gt;
    &lt;p&gt;To me, model quality is the most decisive factor in these AI coding wars. And in my view, that’s why Claude Code has already overtaken Cursor, and OpenAI’s Codex is close behind, despite both having launched a year or so later.&lt;/p&gt;
    &lt;p&gt;Even though the newcomers Cursor, Claude Code, and OpenAI Codex are the talk of the (developer) town, incumbents such as Microsoft with Github Copilot, AWS with Kiro, and Google with Antigravity, can utilize their existing customer relationships, bundle their offerings with their existing suites, and/or provide their option as the default in their tech stack to compete. As an example, Cursor charges $20–$40 monthly per user for productive usage, while Google Antigravity launched free with generous limits for individual users. Github Copilot still leads this market, proving once again that enterprise bundling and distribution has structural advantages. This is the classic Microsoft Teams vs. Slack Dynamic10.&lt;/p&gt;
    &lt;p&gt;One way for startups to compete is by winning individual users who may use a coding tool with or without formal approval, and then be the tool’s advocate inside the organization. That organic interest and adoption eventually forces IT and security teams to officially review the tool and then eventually sanction its usage.&lt;/p&gt;
    &lt;p&gt;Yet, even as these newer tools capture developer mindshare, the underlying developer tools market is changing. Both the IDEs developers choose and the resources &lt;del&gt;they&lt;/del&gt; we consult have changed dramatically. StackOverflow, once the default for programmers stuck on a programming issue, has seen its traffic and number of questions decline dramatically since ChatGPT’s launch, suggesting that AI is already replacing some traditional developer resources.&lt;/p&gt;
    &lt;p&gt;Just as compilers freed programmers from writing assembly code, AI tools are freeing software engineers from the grunt work of writing boilerplate and routine code, and letting them focus on higher-order thinking. Eventually, one day, AI may get so good that it will generate applications on demand and create entire software ecosystems autonomously. Both hands-off and hands-on AI coding tools, as well as incumbents and newcomers, see themselves as the path to that fully autonomous software generation, even if they are taking different approaches. The ones who get there will be those who deliver the best model quality that ships code reliably, go deep enough to ship features that foundation models can’t care enough to replicate, and become sticky enough that users will not leave even when they can11.&lt;/p&gt;
    &lt;p&gt;If you enjoyed this post, please consider sharing it on Twitter/X or LinkedIn, and tag me when you do.&lt;/p&gt;
    &lt;p&gt;Hopper’s A-0 system and her definition of the term compiler is different from what we consider a compiler today, but it established the foundational concept.&lt;/p&gt;
    &lt;p&gt;In the context of coding assistants, most products labeled as AI tools are powered by LLMs, and so I use AI and LLM interchangeably in this article despite the actual difference.&lt;/p&gt;
    &lt;p&gt;A better comparison might be at the product level rather than company level. In that case, ChatGPT and Claude both reached $1B faster than Cursor did.&lt;/p&gt;
    &lt;p&gt;I would argue that the vast majority of productive code is hidden behind company firewalls. Current foundation models are trained on publicly available data on the internet, and do not have access to proprietary codebases. We are yet to see breakthrough solutions where a company augments their confidential private data to generate production-ready code using current LLMs. While Retrieval-Augmented Generation has shown some promise, it has not yet delivered transformative results. Companies such as Glean are actively working on this problem.&lt;/p&gt;
    &lt;p&gt;Replit and Cognition probably appeal to both segments. To me, Replit leans hands-off with its rapid prototyping focus. Cognition’s agent-based approach, though hands-off, lets engineers still control the code directly, making it lean hands-on.&lt;/p&gt;
    &lt;p&gt;I was curious how Cursor knows how much code is generated by other LLMs outside Cursor? When I asked this on hackernews, swyx suggested that they “can pretty much triangulate across openrouter x feedback from the top 3 model labs to compare with internal usage and figure that out”. To me, triangulation makes sense for internal estimates. but for external publication, I’m surprised Cursor didn’t include “we estimate” or similar qualifying language. My understanding is that FTC policy requires substantiation before making definitive comparative claims (like more than, better than etc). All that to say, I’m not fully convinced about their claims.&lt;/p&gt;
    &lt;p&gt;SWE-bench is a benchmark for evaluating large language models (LLMs) on real world software engineering tasks and issues collected from GitHub. Performance against public benchmarks can be gamed by the model builders. Currently after any new model launch, we see people using the model in the wild and forming a consensus around how the model performs which is a better indicator than these benchmarks.&lt;/p&gt;
    &lt;p&gt;Microsoft bundled Teams into Office 365 subscriptions at no extra cost, using its dominant enterprise distribution to surpass Slack’s paid standalone product within three years despite Slack’s earlier launch and product innovation. See https://venturebeat.com/ai/microsoft-teams-has-13-million-daily-active-users-beating-slack&lt;/p&gt;
    &lt;p&gt;Natasha Malpani, Twitter/X, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006004</guid><pubDate>Fri, 21 Nov 2025 16:33:28 +0000</pubDate></item><item><title>Show HN: Wealthfolio 2.0- Open source investment tracker. Now Mobile and Docker</title><link>https://wealthfolio.app/?v=2.0</link><description>&lt;doc fingerprint="27ab40bb69b94b92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grow Wealth. Keep Control.&lt;/head&gt;
    &lt;head rend="h2"&gt;A beautiful, Private and Open-Source investment tracker that runs locally on all your devices.&lt;/head&gt;
    &lt;head rend="h2"&gt;WHY CHOOSE WEALTHFOLIO?&lt;/head&gt;
    &lt;p&gt;A beautiful portfolio tracker that respects your privacy and your data&lt;/p&gt;
    &lt;head rend="h3"&gt;Privacy-First Approach&lt;/head&gt;
    &lt;p&gt;Your data never leaves your device. As an open-source project, we prioritize security and transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple and Beautifully Crafted&lt;/head&gt;
    &lt;p&gt;Powerful features wrapped in an elegant, easy-to-use interface. Simplicity meets sophistication.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Hidden Costs&lt;/head&gt;
    &lt;p&gt;Free to use with optional one-time payment. No subscriptions or recurring fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE ESSENTIALS YOU NEED TO TRACK YOUR WEALTH&lt;/head&gt;
    &lt;p&gt;No More Messy Spreadsheets or Privacy Concerns - Just You and Your Secure, Personal Wealth Companion Application&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Aggregation&lt;/head&gt;
    &lt;p&gt;Gather all your investment and savings accounts in one place. See everything at a glance, from stocks to savings! Import your CSV statements from your broker or bank.&lt;/p&gt;
    &lt;head rend="h4"&gt;Comprehensive View&lt;/head&gt;
    &lt;p&gt;See all your accounts in one place.&lt;/p&gt;
    &lt;head rend="h4"&gt;CSV Import&lt;/head&gt;
    &lt;p&gt;Easily import your CSV statements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Holdings Overview&lt;/head&gt;
    &lt;p&gt;Get a clear picture of what's in your portfolio. Stocks, ETFs, or Cryptocurrencies - know what you have and how it's performing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Portfolio Insights&lt;/head&gt;
    &lt;p&gt;Understand your asset allocation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance Tracking&lt;/head&gt;
    &lt;p&gt;Monitor how your investments are doing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance Dashboard&lt;/head&gt;
    &lt;p&gt;See how your investments stack up, all in one place. Compare your accounts side by side, check if you are beating the S&amp;amp;P 500, and track your favorite ETFs without the hassle. No fancy jargon - just clear, useful charts that help you understand how your money is really doing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compare Your Accounts&lt;/head&gt;
    &lt;p&gt;See which accounts are doing best.&lt;/p&gt;
    &lt;head rend="h4"&gt;Beat the Market?&lt;/head&gt;
    &lt;p&gt;Check how you stack up against some popular indexes and ETFs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Income Tracking&lt;/head&gt;
    &lt;p&gt;Monitor dividends and interest income across your entire portfolio. Get a clear view of your passive income streams, helping you make informed decisions about your investments.&lt;/p&gt;
    &lt;head rend="h4"&gt;Dividend Monitoring&lt;/head&gt;
    &lt;p&gt;Track your dividend income.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interest Income&lt;/head&gt;
    &lt;p&gt;Keep an eye on interest earnings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Performance&lt;/head&gt;
    &lt;p&gt;Track your accounts' holdings and performance over time. See how a particular account is performing, and how it's changing over time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Historical Data&lt;/head&gt;
    &lt;p&gt;View past performance trends.&lt;/p&gt;
    &lt;head rend="h4"&gt;Account Analysis&lt;/head&gt;
    &lt;p&gt;Analyze individual account performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goals Tracking&lt;/head&gt;
    &lt;p&gt;Set your savings targets clearly. Distribute your funds across these objectives, assigning a specific percentage to each. Keep an eye on your progress.&lt;/p&gt;
    &lt;head rend="h4"&gt;Target Setting&lt;/head&gt;
    &lt;p&gt;Define your financial goals.&lt;/p&gt;
    &lt;head rend="h4"&gt;Progress Monitoring&lt;/head&gt;
    &lt;p&gt;Track your progress towards goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Contribution Rooms and Limit Tracking&lt;/head&gt;
    &lt;p&gt;Stay on top of your contribution limits for tax-advantaged accounts like IRAs, 401(k)s, or TFSAs. Track your available contribution room and avoid over-contributing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Limit Awareness&lt;/head&gt;
    &lt;p&gt;Know your contribution limits.&lt;/p&gt;
    &lt;head rend="h4"&gt;Avoid Over-Contribution&lt;/head&gt;
    &lt;p&gt;Prevent excess contributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extend Wealthfolio with Powerful Add-ons&lt;/head&gt;
    &lt;head rend="h3"&gt;Investment Fees Tracker&lt;/head&gt;
    &lt;p&gt;Track and analyze investment fees across your portfolio with detailed analytics and insights&lt;/p&gt;
    &lt;head rend="h3"&gt;Goal Progress Tracker&lt;/head&gt;
    &lt;p&gt;Track your investment progress towards target amounts with a visual representation&lt;/p&gt;
    &lt;head rend="h3"&gt;Stock Trading Tracker&lt;/head&gt;
    &lt;p&gt;Simple swing stock trading tracker with performance analytics and calendar views&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006016</guid><pubDate>Fri, 21 Nov 2025 16:34:52 +0000</pubDate></item><item><title>Private Equity's New Venture: Youth Sports</title><link>https://jacobin.com/2025/11/youth-sports-hockey-private-equity</link><description>&lt;doc fingerprint="2cc848a8af19564d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Private Equity’s New Venture: Youth Sports&lt;/head&gt;
    &lt;p&gt;Backed by Wall Street, the company Black Bear Sports Group is tightening its grip on youth sports. In a scheme only private equity could dream up, parents now can’t record their kids’ games — but they can pay a steep price to watch corporate recordings.&lt;/p&gt;
    &lt;p&gt;There’s an ironclad truism in youth sports: every parent turns into an ESPN 30 for 30 documentarian as soon as they have a video recording device in hand and their kid is in the game.&lt;/p&gt;
    &lt;p&gt;Some record the games and post them online so family members and friends who can’t attend in person can watch their kids play. Sometimes they do so to attract the attention of college scouts or help players hone their craft. Some people just want to preserve the memories.&lt;/p&gt;
    &lt;p&gt;But in the world of corporatized youth sports, even this simple pleasure is being banned and monetized by Wall Street to extract as much profit as possible from players and parents, no matter how many kids get sidelined because they can’t afford the sport’s rising costs.&lt;/p&gt;
    &lt;p&gt;As the $40 billion youth sports industry comes under private equity control, corporate-owned facilities and leagues — from hockey rinks to cheerleading arenas — have begun prohibiting parents from recording their own kids’ sports games.&lt;/p&gt;
    &lt;p&gt;Instead, parents are forced to subscribe to these companies’ exclusive recording and streaming service, which can cost many times more than the streaming costs for professional sporting events. Meanwhile, the firms’ exclusive contracts have prohibited alternative video services from being made available.&lt;/p&gt;
    &lt;p&gt;In some instances, parents have been threatened that if they choose to defy the rules and record the game, they may end up on a blacklist that punishes their kids’ teams. Those threats were even reportedly made to a sitting US senator.&lt;/p&gt;
    &lt;p&gt;“I was told this past weekend that if I livestreamed my child’s hockey game, my kid’s team will be penalized and lose a place in the standings,” said Sen. Chris Murphy (D-CT) at a public event earlier this year. “Why is that? Because a private equity company has bought up the rinks.”&lt;/p&gt;
    &lt;p&gt;Murphy did not name the company in question, though the restrictive streaming practices he described have become widespread across youth hockey.&lt;/p&gt;
    &lt;p&gt;Black Bear Sports Group, an emerging youth hockey empire and the largest owner-operator of hockey rinks in the country, is among the private equity–backed companies that are amassing a chokehold on recording and streaming youth sports. At Black Bear–owned ice rinks, parents cannot record, post, or livestream their kids’ hockey games online “per official company policy,” according to staff at those venues. Some rink attendants said they will confiscate attendees’ recording devices if they find them.&lt;/p&gt;
    &lt;p&gt;Some specialized sports training consultants have agreements with Black Bear that allow them to record games and practices, but only for internal use.&lt;/p&gt;
    &lt;p&gt;According to a spokesperson, Black Bear claims the policy is to mitigate “significant safety risks to players,” such as players being filmed without their consent. The spokesperson failed to answer a follow-up question about what penalties attendees might face if they try to record the games themselves.&lt;/p&gt;
    &lt;p&gt;Black Bear’s streaming service costs between $25 and $50 a month, depending on the package and additional fees. The company’s aggressive expansion of the program has even triggered a lawsuit from a former streaming partner alleging breach of contract and trade secret theft.&lt;/p&gt;
    &lt;p&gt;In addition to its recording rules and associated costs, Black Bear is starting to add a $50 “registration and insurance” fee per player for some leagues. That’s on top of what players already spend on expensive equipment, team registration, and membership to USA Hockey, the sport’s national governing body.&lt;/p&gt;
    &lt;p&gt;“Black Bear Sports Group does not have a good reputation in the hockey world and is known for predatory practices of its customers like price gouging,” reads a recently launched petition protesting the new registration and insurance charges.&lt;/p&gt;
    &lt;p&gt;The fees and streaming restrictions reveal how private equity firms are deploying the same playbook in youth sports as they have in other domains, from dentistry to bowling: degrade the quality of service while juicing returns for investors.&lt;/p&gt;
    &lt;p&gt;“Black Bear [is] following the exact same model as we’ve seen elsewhere in the industry,” said Katie Van Dyck, an antitrust attorney and senior fellow at the American Economic Liberties Project. “It’s not about investing to enrich our children’s lives.”&lt;/p&gt;
    &lt;head rend="h1"&gt;“The New Sport of Kings”&lt;/head&gt;
    &lt;p&gt;The new fees tacked on by Black Bear contribute to the already rising costs of participating in youth and recreational sports like hockey.&lt;/p&gt;
    &lt;p&gt;Across the board, youth sports have become an increasingly expensive budget item for American families, thanks to costs ranging from equipment to team memberships and travel.&lt;/p&gt;
    &lt;p&gt;According to a recent study from the Aspen Institute, households now spend an average of $1,016 a year on their child’s primary sport, a 46 percent increase since 2019.&lt;/p&gt;
    &lt;p&gt;The professionalization of youth sports has further driven up costs. Some parents now pay for personal trainers and even sports psychologists to give their kids a competitive edge in the hopes of them reaching the collegiate or professional level.&lt;/p&gt;
    &lt;p&gt;As a result, many children from lower-income families are being priced out of youth sports.&lt;/p&gt;
    &lt;p&gt;“We have this affordability crisis, and youth sports are one of those things that’s becoming an activity only for the wealthy,” said Van Dyck. “It’s not something that is accessible to people who make less than six figures a year.”&lt;/p&gt;
    &lt;p&gt;This trend line has been particularly pronounced in hockey, which, according to some metrics, is the most expensive youth sport, with an average cost of $2,583. Skate prices can top $1,000, and sticks can often cost several hundred.&lt;/p&gt;
    &lt;p&gt;“It’s the new sport of kings,” said Joseph Kolodziej, who runs a consultancy helping parents and athletes navigate the world of youth hockey. “I’ve been hearing for over twenty years that prices are forcing people out of the sport and that teams are losing gifted athletes because they can’t afford to play.”&lt;/p&gt;
    &lt;p&gt;The rapid commercialization of youth sports has become big business. One recent estimate put the total valuation of the youth sports market at $40 billion. Youth hockey alone could reach over $300 million by the end of the decade.&lt;/p&gt;
    &lt;p&gt;Those sky-high revenues have attracted Wall Street investors looking to charge more money from a wealthier customer base willing to pay more for their kids.&lt;/p&gt;
    &lt;p&gt;And now, virtually every corner of the youth sports industry is coming under corporate ownership.&lt;/p&gt;
    &lt;p&gt;A company called Unrivaled Sports, run by two veterans of Blackstone, the world’s largest private equity firm, is rapidly consolidating baseball camps, flag football, and other leagues. The operation even bought the iconic baseball megacomplex in Cooperstown, New York, considered the birthplace of the sport, where summer tournaments draw teams from around the country.&lt;/p&gt;
    &lt;p&gt;Bain Capital–backed Varsity Brands, meanwhile, has cannibalized the competitive cheerleading arena and now acts as the gatekeeper controlling access to the sport.&lt;/p&gt;
    &lt;p&gt;All of this outside investment has raised concerns that the financial firms rolling up the market may further increase costs for families.&lt;/p&gt;
    &lt;p&gt;From health care to retail, private equity firms purchase companies, load them up with debt, slash costs, and extract as much profit as possible for investors before selling the operations or filing for bankruptcy.&lt;/p&gt;
    &lt;p&gt;“When youth sports become an investment vehicle, rather than a development vehicle for children, there [are] all kinds of financial predation that can arise from vulture companies that don’t have the sport’s long-term interest in mind,” said Van Dyck at the American Economic Liberties Project.&lt;/p&gt;
    &lt;p&gt;Varsity Brands, for example, faced a class-action antitrust lawsuit for alleged anticompetitive practices that pushed out cheerleading rivals while squeezing profits from participants, such as forcing teams to purchase Varsity’s own apparel and equipment. In 2024, Varsity, which was also mired in a sex abuse scandal, settled the suit for $82 million.&lt;/p&gt;
    &lt;p&gt;In addition to controlling venues, uniforms, and the tournaments for competitive cheerleading, Varsity expanded into entertainment streaming services with Varsity TV, which has the exclusive right to livestream the company’s competitions. It’s lorded that arrangement not just over parents but also tech giants. During the 2020 Netflix docuseries Cheer, which follows a cheerleading team competing across the country, Varsity wouldn’t allow the series’ crew to film inside the venue they owned in Daytona, Florida.&lt;/p&gt;
    &lt;p&gt;The Texas attorney general is probing similar anticompetitive practices by the Dallas Stars, a professional National Hockey League team, following an explosive USA Today investigation into its youth hockey operations. According to the report, the team bought up dozens of Texas’s recreational rinks. It then allegedly used its market power to jack up fees on youth players, underinvested in rink maintenance, and retaliated against clubs that tried to oppose them.&lt;/p&gt;
    &lt;p&gt;Now, legal experts say Black Bear Sports is replicating a similar model for youth hockey teams along the East Coast and beyond.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Only Game in Town&lt;/head&gt;
    &lt;p&gt;Hockey has grown in popularity across the United States, with USA Hockey membership reaching an all-time high of 577,900 in 2025. But it’s become increasingly difficult for small operations to meet the growing demand.&lt;/p&gt;
    &lt;p&gt;For example, rinks require immense amounts of energy for air conditioning to reach freezing temperatures, and electric utility bills have skyrocketed over the past decade. And while many local rinks used to be municipally run or publicly funded, such support has been slashed in recent decades in favor of government privatization.&lt;/p&gt;
    &lt;p&gt;In 2015, the Maryland-based Black Bear Sports entered the scene. The company, owned by the private equity firm Blackstreet Capital, began buying up struggling ice rinks, some of which were on the verge of closing. According to the company’s sales pitch, it would invest the capital to retrofit and renovate the rinks, making them serviceable.&lt;/p&gt;
    &lt;p&gt;This approach follows a familiar pattern for Black Bear Sports’ founder, Murry Gunty, a longtime hockey aficionado who got his start at Blackstone before launching his own private equity firm, Blackstreet Capital. Blackstreet is known for buying up small- to medium-sized distressed companies for cheap, then making the businesses leaner before selling them off. While slashing costs to bring in returns for the firm’s investors, the private equity fund managers charge massive fees to pad their own bottom lines.&lt;/p&gt;
    &lt;p&gt;Shortly after founding Black Bear in 2015, Gunty was sued by the Securities and Exchange Commission for charging investors high fees without being licensed as a broker. Blackstreet settled the charges for $3.1 million.&lt;/p&gt;
    &lt;p&gt;Today Black Bear owns forty-two rinks across eleven states across the Northeast, Midwest, and mid-Atlantic coast. In some areas, those venues are the only game in town. With its network of rinks, Black Bear controls the basic infrastructure that other clubs, leagues, and tournaments need to access.&lt;/p&gt;
    &lt;p&gt;Along with rinks, Black Bear also manages four national and regional youth hockey associations, a handful of junior-level sports teams, such as the Maryland Black Bears, and organizes major youth hockey tournaments on the East Coast. Gunty acts as the commissioner of the United States Premier Hockey League, one of the largest top-level junior leagues with seventy-five teams nationwide, offering a direct pathway for young athletes to play at the college level. Black Bear’s vice president, Tony Zasowski, is the league commissioner for the Tier 1 Hockey Federation and the Atlantic Hockey Federation, top-level hockey leagues.&lt;/p&gt;
    &lt;p&gt;Those organizations set the rules for the league, dictate playing schedules, and require paid dues, among other costs. They also determine where leagues and tournaments will be held — such as Black Bear’s own rinks.&lt;/p&gt;
    &lt;p&gt;The conglomerate also launched its own online hockey ratings system, used to determine team rankings and players’ status.&lt;/p&gt;
    &lt;p&gt;Among the company’s newest ventures is a streaming site, Black Bear TV. In September 2024, the company put out a public notice that “all games played inside the Black Bear venues and certain partner venues will be streamed exclusively on Black Bear TV.”&lt;/p&gt;
    &lt;p&gt;That exclusive arrangement also includes all games played within the leagues run by Black Bear, even if they aren’t occurring at their own arenas. Shortly after Gunty became commissioner of the United States Premier Hockey League in 2024, the organization inked a deal to make Black Bear TV the exclusive provider for all its games.&lt;/p&gt;
    &lt;p&gt;Previously, Black Bear had an exclusive agreement with the sports broadcaster LiveBarn to livestream the games, and the two split the revenues.&lt;/p&gt;
    &lt;p&gt;But Black Bear wanted to assume full control over streaming services and profits, according to a lawsuit LiveBarn filed this year, which claims Black Bear stole LiveBarn’s business and then used inside information about its prices and terms to convince other rinks to sign deals with Black Bear.&lt;/p&gt;
    &lt;p&gt;Black Bear TV isn’t cheap. Each individual game on its online platform costs $14.99 to watch. For the service’s full suite of features, including the ability to clip plays, packages range between $26 and $36 a month and can total roughly $440 a year. Certain premier leagues controlled by Black Bear are subject to additional fees, driving up prices to $50 a month.&lt;/p&gt;
    &lt;p&gt;For comparison, an $11.99 monthly subscription to ESPN TV would include access to nearly every Division 1 college game, most National Hockey League games, professional soccer matches, PGA Tour golf tournaments, and other major sporting events.&lt;/p&gt;
    &lt;p&gt;A Black Bear spokesperson says its prices reflect the high-quality service it provides to customers. “With Black Bear TV, we are no longer limited by a fixed, center-ice camera connected to [a] rink wireless connection that often faces delays and low-quality picture,” said the spokesperson.&lt;/p&gt;
    &lt;p&gt;But user reviews for Black Bear TV complain about the service’s streaming quality and spotty coverage. The company gets to pick and choose which games it features on the service.&lt;/p&gt;
    &lt;p&gt;Starting this year, Black Bear is introducing another fee: a separate registration and insurance charge for adult leagues to access its ice rinks.&lt;/p&gt;
    &lt;p&gt;The new $50 annual charge, which could become a model for youth leagues under Black Bears’ control, triggered a public petition in September demanding the company reduce its fees.&lt;/p&gt;
    &lt;p&gt;Black Bear contends that the new fee is a slightly lower-cost alternative to USA Hockey’s $52 adult registration cost, which is required to participate in the organization’s sanctioned leagues.&lt;/p&gt;
    &lt;p&gt;But according to the petition, certain recreational leagues weren’t previously paying any fees at Black Bear rinks, and some players may now have to pay both registration fees if they also play in leagues unrelated to Black Bear.&lt;/p&gt;
    &lt;p&gt;The additional fees could be another hurdle denying some players the joys of participating in the sport altogether.&lt;/p&gt;
    &lt;p&gt;“Adding an additional fee is unnecessary and makes an already hard-to-access sport even more difficult, especially for new players . . . [it] risks killing our league as it has already shrunken from previous years,” say petition organizers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006183</guid><pubDate>Fri, 21 Nov 2025 16:52:48 +0000</pubDate></item><item><title>Pivot Robotics (YC W24) Is Hiring for an Industrial Automation Hardware Engineer</title><link>https://www.ycombinator.com/companies/pivot-robotics/jobs/7xG9Dc6-mechanical-engineer-controls</link><description>&lt;doc fingerprint="6e34f445ca77fde2"&gt;
  &lt;main&gt;
    &lt;p&gt;AI for Robot Arms in Factories&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Pivot Robots (YC W24) is building the AI brain for robot arms for high-mix manufacturing.&lt;/p&gt;
    &lt;p&gt;Pivot Robots combines off-the-shelf robots and vision sensors with recent breakthroughs in foundation vision models to give industrial robot arms the power to adapt. Our first product directly addresses the dangerous and unpopular task of grinding metal parts. Currently, our software is being deployed on 10+ robots at a large cast iron foundry.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006250</guid><pubDate>Fri, 21 Nov 2025 17:00:00 +0000</pubDate></item></channel></rss>