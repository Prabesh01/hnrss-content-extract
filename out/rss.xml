<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 19 Oct 2025 20:10:39 +0000</lastBuildDate><item><title>Show HN: Pyversity – Fast Result Diversification for Retrieval and RAG</title><link>https://github.com/Pringled/pyversity</link><description>&lt;doc fingerprint="f4020924a335382"&gt;
  &lt;main&gt;
    &lt;p&gt;Pyversity is a fast, lightweight library for diversifying retrieval results. Retrieval systems often return highly similar items. Pyversity efficiently re-ranks these results to encourage diversity, surfacing items that remain relevant but less redundant.&lt;/p&gt;
    &lt;p&gt;It implements several popular diversification strategies such as MMR, MSD, DPP, and Cover with a clear, unified API. More information about the supported strategies can be found in the supported strategies section. The only dependency is NumPy, making the package very lightweight.&lt;/p&gt;
    &lt;p&gt;Install &lt;code&gt;pyversity&lt;/code&gt; with:&lt;/p&gt;
    &lt;code&gt;pip install pyversity&lt;/code&gt;
    &lt;p&gt;Diversify retrieval results:&lt;/p&gt;
    &lt;code&gt;import numpy as np
from pyversity import diversify, Strategy

# Define embeddings and scores (e.g. cosine similarities of a query result)
embeddings = np.random.randn(100, 256)
scores = np.random.rand(100)

# Diversify the result
diversified_result = diversify(
    embeddings=embeddings,
    scores=scores,
    k=10, # Number of items to select
    strategy=Strategy.MMR, # Diversification strategy to use
    diversity=0.5 # Diversity parameter (higher values prioritize diversity)
)

# Get the indices of the diversified result
diversified_indices = diversified_result.indices&lt;/code&gt;
    &lt;p&gt;The returned &lt;code&gt;DiversificationResult&lt;/code&gt; can be used to access the diversified &lt;code&gt;indices&lt;/code&gt;, as well as the &lt;code&gt;selection_scores&lt;/code&gt; of the selected strategy and other useful info. The strategies are extremely fast and scalable: this example runs in milliseconds.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;diversity&lt;/code&gt; parameter tunes the trade-off between relevance and diversity: 0.0 focuses purely on relevance (no diversification), while 1.0 maximizes diversity, potentially at the cost of relevance.&lt;/p&gt;
    &lt;p&gt;The following table describes the supported strategies, how they work, their time complexity, and when to use them. The papers linked in the references section provide more in-depth information on the strengths/weaknesses of the supported strategies.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Strategy&lt;/cell&gt;
        &lt;cell role="head"&gt;What It Does&lt;/cell&gt;
        &lt;cell role="head"&gt;Time Complexity&lt;/cell&gt;
        &lt;cell role="head"&gt;When to Use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMR (Maximal Marginal Relevance)&lt;/cell&gt;
        &lt;cell&gt;Keeps the most relevant items while down-weighting those too similar to what’s already picked.&lt;/cell&gt;
        &lt;cell&gt;O(k · n · d)&lt;/cell&gt;
        &lt;cell&gt;Good default. Fast, simple, and works well when you just want to avoid near-duplicates.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MSD (Max Sum of Distances)&lt;/cell&gt;
        &lt;cell&gt;Prefers items that are both relevant and far from all previous selections.&lt;/cell&gt;
        &lt;cell&gt;O(k · n · d)&lt;/cell&gt;
        &lt;cell&gt;Use when you want stronger spread, i.e. results that cover a wider range of topics or styles.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DPP (Determinantal Point Process)&lt;/cell&gt;
        &lt;cell&gt;Samples diverse yet relevant items using probabilistic “repulsion.”&lt;/cell&gt;
        &lt;cell&gt;O(k · n · d + n · k²)&lt;/cell&gt;
        &lt;cell&gt;Ideal when you want to eliminate redundancy or ensure diversity is built-in to selection.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;COVER (Facility-Location)&lt;/cell&gt;
        &lt;cell&gt;Ensures selected items collectively represent the full dataset’s structure.&lt;/cell&gt;
        &lt;cell&gt;O(k · n²)&lt;/cell&gt;
        &lt;cell&gt;Great for topic coverage or clustering scenarios, but slower for large &lt;code&gt;n&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Traditional retrieval systems rank results purely by relevance (how closely each item matches the query). While effective, this can lead to redundancy: top results often look nearly identical, which can create a poor user experience.&lt;/p&gt;
    &lt;p&gt;Diversification techniques like MMR, MSD, COVER, and DPP help balance relevance and variety. Each new item is chosen not only because it’s relevant, but also because it adds new information that wasn’t already covered by earlier results.&lt;/p&gt;
    &lt;p&gt;This improves exploration, user satisfaction, and coverage across many domains, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;E-commerce: Show different product styles, not multiple copies of the same black pants.&lt;/item&gt;
      &lt;item&gt;News search: Highlight articles from different outlets or viewpoints.&lt;/item&gt;
      &lt;item&gt;Academic retrieval: Surface papers from different subfields or methods.&lt;/item&gt;
      &lt;item&gt;RAG / LLM contexts: Avoid feeding the model near-duplicate passages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The implementations in this package are based on the following research papers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;MMR: Carbonell, J., &amp;amp; Goldstein, J. (1998). The use of MMR, diversity-based reranking for reordering documents and producing summaries. Link&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MSD: Borodin, A., Lee, H. C., &amp;amp; Ye, Y. (2012). Max-sum diversification, monotone submodular functions and dynamic updates. Link&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;COVER: Puthiya Parambath, S. A., Usunier, N., &amp;amp; Grandvalet, Y. (2016). A coverage-based approach to recommendation diversity on similarity graph. Link&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DPP: Kulesza, A., &amp;amp; Taskar, B. (2012). Determinantal Point Processes for Machine Learning. Link&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DPP (efficient greedy implementation): Chen, L., Zhang, G., &amp;amp; Zhou, H. (2018). Fast greedy MAP inference for determinantal point process to improve recommendation diversity. Link&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thomas van Dongen&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45634310</guid><pubDate>Sun, 19 Oct 2025 14:16:12 +0000</pubDate></item><item><title>Xubuntu.org Might Be Compromised</title><link>https://old.reddit.com/r/Ubuntu/comments/1oa4549/xubuntuorg_might_be_compromised/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45634367</guid><pubDate>Sun, 19 Oct 2025 14:25:45 +0000</pubDate></item><item><title>Scheme Reports at Fifty</title><link>https://crumbles.blog/posts/2025-10-18-scheme-reports-at-fifty.html</link><description>&lt;doc fingerprint="a763db7f1fbb9dd4"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Based on my talk at the Scheme Workshop 2025. You might prefer to have me talk it at you instead.&lt;/p&gt;
      &lt;p&gt;We are holding an election to the Scheme Steering Committee. Register to vote and nominate candidates!&lt;/p&gt;
      &lt;p&gt;In December this year, the Scheme reports will turn fifty. As chair of the working group entrusted with the next major revision of the report, I want to start a discussion in the Scheme community about what a good Scheme report looks like in 2025, and how it’s different from what it looked like in 1975, and from other times when the Scheme report was revised.&lt;/p&gt;
      &lt;head rend="h2"&gt;Who is the Scheme report for?&lt;/head&gt;
      &lt;p&gt;When making any document, we have to consider who we’re writing for, how they’re going to use it, and what they want and need from it in order to do their thing. For the Scheme report, the obvious two groups are users and implementers. These are very vague groupings, and like any attempt to divide up a group as large and diverse as the Scheme user base, one can’t at all pretend like every member of these groupings has the same views on every issue. Nonetheless, a rule of thumb is that users want their pet features added, creating pressure to grow the report; implementers want to be able to actually create a compiler, creating pressure to keep the report smaller and simpler. But there are also clear subdivisions where even this clear breakdown falls apart, like in the embedded space, where both users and implementers want something that can work on limited-resource environments.&lt;/p&gt;
      &lt;p&gt;Another tension still rarely recognized is between users in education – students learning the language – and long-time Schemers who know it very well. The How to Design Programs approach seems to have worked for many more beginning students than Structure and Interpretation of Computer Programs ever did, because HtDP acknowledges from the start that assuming brand new students will be able to learn with a tool designed for people who are already experts means that tool can’t be as helpful to those just finding their feet in a new way of thinking.&lt;/p&gt;
      &lt;p&gt;But in Scheme we often get into philosophical and even political debates when we talk about what the Scheme report should be like; debates which have much more to do with an ideological belief about the nature of the language than with the direct, practical needs of the debaters.&lt;/p&gt;
      &lt;head rend="h2"&gt;Big and little languages&lt;/head&gt;
      &lt;p&gt;A classic such argument is over whether certain versions of Scheme – usually R6RS, and sometimes also the one that my working group is producing – are ‘too big’. On the other hand, Scheme reports that don’t have this quality can be seen as being too ‘little’ for doing real programming work in.&lt;/p&gt;
      &lt;p&gt;Earlier, littler Scheme reports are often described by the makers of these arguments as a ‘diamond-like jewel’, or similar. The ‘diamond-like jewel’ wording is, I think, from the infamous ‘Worse is Better’ section of Richard P. Gabriel’s paper ‘Lisp: Good News, Bad News, and How to Win Big’.&lt;/p&gt;
      &lt;p&gt;In 1998 Guy L. Steele, co-inventor of Scheme, read a keynote paper at OOPSLA entitled ‘Growing a Language’ (transcript). In the paper, he talks a lot about the tension between the qualities of a big language vs a little language, and in particular talks about Scheme, saying:&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;Could not “The Right Thing” be a small language, not hard to port but with no warts?&lt;/p&gt;
        &lt;p&gt;I guess it could be done, but I, for one, am not that smart (or have not had that much luck). But in fact I think it can not be done. Five-and-twenty years ago, when users did not want that much from a programming language, one could try. Scheme was my best shot at it.&lt;/p&gt;
        &lt;p&gt;But users will not now with glad cries glom on to a language that gives them no more than what Scheme or Pascal gave them. They need to paint bits, lines, and boxes on the screen in hues bright and wild; they need to talk to printers and servers through the net; they need to load code on the fly; they need their programs to work with other code they don’t trust; they need to run code in many threads and on many machines; they need to deal with text and sayings in all the world’s languages. A small programming language just won’t cut it.&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;From the things he names as examples of what users want to do with languages, it’s clear that Steele’s mind was very much on Java, and Java in 1998 at that. Some of these things just aren’t in fashion any more: some were thought to be good ideas at the time, but now seen as bad ideas; some of them just never caught on. But some of them are just as important as ever.&lt;/p&gt;
      &lt;p&gt;Of the things that are just as important as ever, though, many of those no longer seem like things a language should have to take care of; they are, rather, the province of third-party libraries.&lt;/p&gt;
      &lt;p&gt;That, in fact, is how Steele concludes ‘Growing a Language’: users should be able to grow the language itself by adding libraries.&lt;/p&gt;
      &lt;p&gt;But, it seems, the base of Scheme as it was in 1975, and even in 1998, was not enough. Most people today would agree that a programming language without even a standard error-handling mechanism is quite deficient, even if it does give one the ability to build one for oneself.&lt;/p&gt;
      &lt;p&gt;And, in fact, most Schemers agree that the best way for Scheme to go forward is to find better ways for Scheme users to grow the language. Our disagreement, then, is not about ‘bigness’ per se, but where the bigness comes from. Few would swallow a thousand-page Scheme report; setting aside the actual labour of designing and implementing it, most people would be perfectly content with using their own personal library of language extensions whose documentation would run to a thousand pages if it were all documented. Our disagreement is over how big the base needs to be in order to provide a foundation to grow from; and over what needs to be in the foundation, to enable users to give themselves this bigness.&lt;/p&gt;
      &lt;head rend="h2"&gt;Dynamic systems, static languages&lt;/head&gt;
      &lt;p&gt;So big vs little is one axis of disagreement within the Scheme community. How about another?&lt;/p&gt;
      &lt;p&gt;In 2012 Richard P. Gabriel wrote an essay paper, ‘The Structure of a Programming Language Revolution’. He talks about how he took a break from programming languages in the early 1990s in order to do a fine arts degree. When he came back in the late 1990s, everything had changed. When he left, people in programming languages talked about dynamic, evolving systems like Common Lisp and Smalltalk. By the time he returned, the conversation had changed to static languages like Haskell.&lt;/p&gt;
      &lt;p&gt;I think one of the other axes of division in the Scheme community is around this matter, and the reason is that our community lived through this paradigm shift without, collectively, noticing it. Some of the community moved on to the new paradigm; others continued to think in terms of the old. The two sides do not understand one another’s concerns.&lt;/p&gt;
      &lt;p&gt;The dynamic systems viewpoint is epitomized by the R5RS, whose only entrypoint for programs is the interactive top level, and which even began to lay the groundwork for a library and namespacing system that would have been based on first-class environments and the &lt;code&gt;eval&lt;/code&gt; procedure. Implementations which follow the dynamic systems model include MIT Scheme and Guile.&lt;/p&gt;
      &lt;p&gt;The static languages viewpoint is epitomized by the R6RS, which doesn’t have an interactive top level at all. Implementations which adhere to this model include Chez, Racket, and Loko, but also niche implementations like Stalin which don’t have their own interactive top level at all.&lt;/p&gt;
      &lt;p&gt;Put briefly, the dynamic languages people see Scheme as a dynamic language, plain and simple, and want to be able to do all the run-time modification that they should be able to do in any such language. The static languages people see Scheme as a static language in all regards except its type system.&lt;/p&gt;
      &lt;head rend="h2"&gt;And yet more&lt;/head&gt;
      &lt;p&gt;Already with these two dimensions of belief about what kind of language Scheme is – big vs little, static vs dynamic – we have a huge range of potential expectations for what the Scheme report might be like. When we compare these viewpoints in terms of the models their advocates might prefer to take as guidance for the future development of the language, we can see just how far apart these ideas are.&lt;/p&gt;
      &lt;p&gt;Those who want a little, dynamic language will look to earlier versions of the report, and maybe to the cores of languages like Lua and JavaScript. A little static language might look like SML. Common Lisp might be a model to follow for a big dynamic language, or Haskell for a big static language. I’ve seen almost every one of these languages used to suggest what a suitable future direction for Scheme might look like.&lt;/p&gt;
      &lt;p&gt;These are hugely different rôle models to follow – and these are only two matters on which people have different feelings. I could add more for more philosophical disagreements about the nature of Scheme, ideas about what the right way to make and publish a Scheme report should be, and even very specific individual language features. Clearly, we can’t satisfy all of these expectations at once.&lt;/p&gt;
      &lt;head rend="h2"&gt;Does a Scheme report still make sense?&lt;/head&gt;
      &lt;p&gt;Given these deep divisions over the essential nature of the Scheme language, does it even make sense that we still keep making a Scheme report?&lt;/p&gt;
      &lt;p&gt;‘No’ is an entirely possible answer to this question. Already in the R6RS and R7RS small days, people were arguing that Scheme standardization should stop.&lt;/p&gt;
      &lt;p&gt;If we went this way then, just like Racket in its default mode no longer claims to be a Scheme report implementation, Schemes would slowly diverge into different languages. Guile Scheme would one day simply be Guile; Chicken Scheme would be Chicken, and so on. Like the many descendants of Algol 60 and 68, and the many dialects of those descendants, each of these languages would have a strongly recognizable common ancestor, but each would still be distinct and, ultimately, likely incompatible.&lt;/p&gt;
      &lt;p&gt;It shouldn’t surprise you that I don’t think this is the way to go. As a user of Scheme, I can still see ways I want to extend the common core of all of these implementations. It’s much better to be able to do this once and have my extension available on many implementations, than to have to work hard to port my extension to multiple different languages. All implementations benefit by agreeing on a common core that works to be able to grow the language.&lt;/p&gt;
      &lt;p&gt;Racket is already a cautionary tale. It has some amazing features, but those of us on other implementations can’t take advantage of them because Racket’s implementations of those features aren’t portable. Even Guile has this problem, because it’s still culturally normal among users of that implementation to use its own &lt;code&gt;define-module&lt;/code&gt; declaration instead of the standard R6RS or R7RS &lt;code&gt;library&lt;/code&gt; or &lt;code&gt;define-library&lt;/code&gt;, even if your code only uses standard Scheme features and maybe some common SRFIs. There’s much Guile code I’d like to use on Chez for more speed, or on Chibi for a smaller implementation footprint, that currently requires at least some porting effort for not much good reason.&lt;/p&gt;
      &lt;p&gt;This is more or less the same rationale for Scheme reports that there has always been, and I think it continues to apply today.&lt;/p&gt;
      &lt;head rend="h2"&gt;R6RS and R7RS small considered more alike than widely believed&lt;/head&gt;
      &lt;p&gt;So our thoughts must naturally turn to unification. The co-existence and incompatibility of R6RS and R7RS small have become emblematic of the splits in our community. The R6RS editors and community are, to some extent, justified in feeling that R7RS small unfairly abandoned their work. But in fact the two are more similar than is widely recognized, and in many cases the design of R7RS small in fact vindicates the decisions of the R6RS editors.&lt;/p&gt;
      &lt;p&gt;This may seem a shocking claim about a division which some predicted would end up killing Scheme as a programming language. But, I think, WG1 stumbled their way towards a realization, which I’ll get to just below, which led them to a develop a library system working on basically the same model as R6RS. R7RS small library declarations support conditional compilation and other features which, in R6RS, is exclusively the province of the ‘main’ level language; but once all the &lt;code&gt;cond-expand&lt;/code&gt;s in &lt;code&gt;define-library&lt;/code&gt; are dealt with, the two are essentially the same. R7RS small’s &lt;code&gt;define-library&lt;/code&gt; declarations can be compiled entirely ahead of time to R6RS’s &lt;code&gt;library&lt;/code&gt; style – there is already a program which does this, Akku.&lt;/p&gt;
      &lt;p&gt;Between 2007 and 2013 the landscape of programming tools also shifted so some of the things which were criticized in R6RS’s day – bytevectors for binary data, and Unicode throughout – now seem perfectly normal. It now seems wrong for a programming language not to support these. R7RS small also adopted, for example, the same exception raising and handling mechanism as R6RS.&lt;/p&gt;
      &lt;p&gt;The changing landscape of programming tools around Scheme means that programmers now have a much different sense of the ‘littleness’ of a language than in 2007. Since Rust there’s increasing recognition that memory safety should be treated as non-optional even in most low-level programming, and is cheap to implement; it now seems strange that R5RS and below, and R7RS small, make bounds violations undefined behaviour in the C sense.&lt;/p&gt;
      &lt;p&gt;Moreover, what R7RS small’s authors seemed to realize is that the report will always be a minimum, and not a maximum. I think this is why, although it came from a group of users who started out highly critical of the R6RS for its static model of libraries and programs, R7RS small adopted essentially that model. It doesn’t restrict implementations from providing more dynamic library features.&lt;/p&gt;
      &lt;p&gt;The small/large report split helps even more, because now the report offers two different minimums for implementations to choose from, according to what their needs are. The minimum for ‘the practical needs of mainstream software development’ is, as Guy Steele says, much bigger than it used to be, and that’s fine!&lt;/p&gt;
      &lt;head rend="h2"&gt;Join, or die&lt;/head&gt;
      &lt;p&gt;This is my appeal to the Scheme community, then. If we don’t have a Scheme report that the majority of implementations and users accept, Scheme as a coherent single language will simply die. Not all implementations – it’ll be like if, as above, we stopped making Scheme reports and everyone went their own way. But even the implementations which survived would clearly be weaker for not being able to share the libraries of the others.&lt;/p&gt;
      &lt;p&gt;R7RS is our best shot at joining together to still be able to share code with one another, and to grow the language together – not only in the report itself, but much more in the tools we build with it.&lt;/p&gt;
      &lt;head rend="h2"&gt;The sacred craft&lt;/head&gt;
      &lt;p&gt;Joining, however, means compromising.&lt;/p&gt;
      &lt;p&gt;Nobody will look at any Scheme report and say it is perfect. I can’t look at any version of the report, not even R5RS, and say it was a ‘diamond-like jewel’. Even if it were perfect for one person, it cannot be all things to all people. Scheme has historically been too shy of compromise; the uncompromising pursuit of perfection, even though perfection is a quality nobody will ever agree on, may be why we have struggled so much to move onwards while other languages have not.&lt;/p&gt;
      &lt;p&gt;‘Compromise’ is a dirty word for some people, but I would like to offer another perspective. Although R7RS is sometimes perceived as anti-R6RS, two of the small report’s editors actually voted in favour of R6RS ratification. One of them, John Cowan – also my predecessor as chair of WG2 for the large language – wrote in the rationale to his vote:&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;a well-crafted compromise is in my opinion the most sacred thing that a secular age knows&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;We may not be able to have a diamond-like jewel, but we can have the most sacred thing a secular age can make.&lt;/p&gt;
      &lt;p&gt;As mentioned at the top, I hope this essay will really open a conversation about these matters. You can write a comment on Mastodon, or if that’s too limiting, on the Scheme Reports mailing list.&lt;/p&gt;
      &lt;p&gt;And don’t forget the election to the Scheme Steering Committee!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45634528</guid><pubDate>Sun, 19 Oct 2025 14:45:03 +0000</pubDate></item><item><title>Why an abundance of choice is not the same as freedom</title><link>https://aeon.co/essays/why-an-abundance-of-choice-is-not-the-same-as-freedom</link><description>&lt;doc fingerprint="f251d10d28e319bd"&gt;
  &lt;main&gt;
    &lt;p&gt;By the time you read this essay, no matter the hour of the day, you will likely have already made some kind of choice: coffee with skimmed milk, whole milk, cream, or black? Sugar or no sugar? Tea instead? Personalised, preference-based choice is, at present, a deeply familiar aspect of life in much of the world, though perhaps most markedly so in the United States, where I live and work. It is also something people don’t generally spend a lot of time discussing, in part because it feels so ordinary. People around the globe shop for everything from housing to vacations to, yes, caffeinated drinks. They pick what they want to read, what they want to listen to, and what they want to believe. They vote for favourite candidates for office. They select friends and lovers, fields to study, professions and jobs, places to live, even insurance plans to hedge their bets when something they cannot choose occurs.&lt;/p&gt;
    &lt;p&gt;Perusing a menu of options to decide what best matches individual desires and values – which is what we generally mean today by making a choice – is a key feature of modern democratic and consumer culture alike. It is also an exalted one. People may disagree about what the possibilities should be, but rarely about the principle of maximising arenas for choice-making or the options themselves. For many of the world’s citizens, this is simply what freedom feels like.&lt;/p&gt;
    &lt;p&gt;Yet, as you may have also felt at various moments, abundant choice isn’t always so straightforward. Behavioural economists point out that most people are actually pretty bad at making decisions of this kind (which explains the appeal of return departments and divorces for when things don’t go as hoped). Philosophers and political theorists say it promotes selfish individualism and discourages collective action around issues that affect us all. And sociologists add that societies that prize choice too much tend to blame those with only poor or limited options for their own misfortunes. So much for choice as consistently synonymous with freedom.&lt;/p&gt;
    &lt;p&gt;What is strange, though, is that few of these critics ever really question either the centrality or the value of choice-making in contemporary life. On the contrary, they tend to make their case as if people everywhere had always spent their days doing things that feel commonplace in capitalist democracies and, indeed, hankering for more such chances. But for the historian, it is obvious that this entire phenomenon is culturally specific. There are people around the globe even today who actively resist this framing of freedom. What may be more surprising is that granting this special status to choice-making is also a relatively recent development even in Western Europe and the United States, not to mention the rest of the world.&lt;/p&gt;
    &lt;p&gt;So how did we get to this point? How did choice become a proxy for freedom in so many domains in modern life? As we discover more and more about our troubles navigating it, we might also wonder if there are other, better ways to be free.&lt;/p&gt;
    &lt;p&gt;Though the explosion of choice has largely been a 20th-century phenomenon, the full story is a long one, going all the way back to the 17th and 18th centuries. Personal choice, as both experience and term of art, got its start in two quite distinct early modern spaces.&lt;/p&gt;
    &lt;p&gt;One is the realm of the shop. Fuelled by the building of colonial and interior trade networks, new goods started to enter cities and towns as early as the 17th century, first in Western Europe, then in the New World, and gradually in their hinterlands too. Particularly significant among those goods were patterned and brightly coloured textiles called calicoes, originally from South Asia, whose price point made it possible for ordinary people to have the novel experience of selecting from among different designs for clothing or home furnishings. Checks? Flowers? Stripes? Purple or green? The decision could, distinctively, be based on nothing more than personal preference. For at the same time, a leisure-time activity blossomed, first at auctions in temporary locations and then increasingly in fixed destinations called shops, in which consumers were invited to peruse a display of the options for sale before ever opening their purses.&lt;/p&gt;
    &lt;p&gt;Even people with limited means started to engage in such new activities as trying out different preachers&lt;/p&gt;
    &lt;p&gt;The English-language neologism ‘shopping’, as opposed to provisioning, took off in the second half of the 18th century precisely to describe this newfangled business. We now also call it consumer choice. The customer learned from all of this browsing and weighing the possibilities to ‘make a choice’ – which is to say, an aesthetic as well as a practical determination en route to purchasing – from what were often already described as a set of ‘choice’, or pre-selected, goods ripe for picking.&lt;/p&gt;
    &lt;p&gt;The post-Reformation fracturing of Christianity, combined with the Protestant tradition of ‘freedom of conscience’ or ‘religious choice’, gradually produced a sense of ideas and beliefs as being similarly up for selection in a pluralist world. With the double emergence of Enlightenment notions of tolerance in Europe and of the Great Awakening religious revival in the British colonies, even people with limited means started, on both sides of the Atlantic, to engage in such new activities as trying out different preachers and churches where congregations had become voluntary communities, attending varieties of public lectures, and picking books from lending libraries and sales catalogues. These, too, were learned recreations, ones that soon revolved around secular as well as sacred notions. Consider Jane Austen’s fictional heroine in Mansfield Park (1814) who, when she gets up the nerve to subscribe to a lending library, is, in Austen’s lightly satirical telling, ‘amazed at her own doings in every way, to be a renter, a chuser [sic] of books!’ From such actions, the stage was set for intellectual choice as well.&lt;/p&gt;
    &lt;p&gt;Between the late 18th century and the First World War, choice continued to expand its domain, encompassing the selection of other people, from marriage partners to employees to political representatives, too. At the same time, it became subject to ever more rules and strictures, formal and not, so as both to tame its potential for undermining the social order and to make it work.&lt;/p&gt;
    &lt;p&gt;In the course of the 19th century, choice increasingly entered the romantic and sexual lives of urban men and women, though with significant gender distinctions when it came to the rules, creating affective and bodily choice as well. This was a development tied very much to the rise of both the idea of companionate marriage – spouses who consent to marry out of mutual affection or even attraction – and an elaborate etiquette about how to identify and court possible partners in a world in which everyone didn’t already know everyone else. From Santiago and Chicago to Paris and Stockholm, and from working-class ticketed dance halls to private soirées for the elite, balls, in particular, became places for organising and evaluating the options in a world in which both young men and young women had been given the power to contract freely for a spin around the dance floor – and also, potentially, for permanent coupledom in the form of a marriage (aka ‘The Choice’, though a marriage contract would technically mean the end of sexual choice once it was signed and sealed). Employment saw a similar kind of transition insofar as it, too, became increasingly a matter of sorting mechanisms, markets and contracts.&lt;/p&gt;
    &lt;p&gt;Finally (and surprisingly late), a similar form of choice came to politics in the form of new voting practices as well as an increase in formal laws to go with them. It is well known that the 19th century was marked by intense debates, from Central Europe to Latin America, about who should be able to vote as new democratic norms spread to many parts of the world. Much less well remembered is the rise of intense discussions about how all these new voters should go about the business of suffrage, especially when it came to the moment of choice itself. Voices in favour of secret balloting, like the Sons of Liberty in New York City, had made themselves heard by the end of the 1760s. But it wasn’t until another century had passed that this mode of voting, rooted in the idea of the protection of internal personal preferences from outside pressure, became the international gold standard, instituted first in Australia in the 1850s (hence what is sometimes still called the Australian ballot) and then by a host of other nations around the globe in the decades just preceding the First World War.&lt;/p&gt;
    &lt;p&gt;Psychiatrists, marketing experts and economists devoted themselves, in different ways, to the study of choice-making&lt;/p&gt;
    &lt;p&gt;Even at the time, commentators were amazed that this transformation took place with so little upheaval. That may well be because the change to secret, individualised voting diminished the rowdiness and violence so often previously associated in many places with popular and considerably more communal elections. But surely it was also because, by the time this shift occurred, it seemed to bring elections into line with so many other kinds of 19th-century leisure-time activities. The secret ballot allowed for the same sorts of choice-making to be enacted when it came to candidates, though with the results eventually aggregated into group choice, as it did for other forms of picking – overcoming the longstanding objections of even liberals like John Stuart Mill, who worried that the last stronghold of public life would in this way be privatised. Only the workplace would remain largely immune. In effect, if the initial age of revolutions in the 18th century introduced popular sovereignty based on elections in the first place, we might think of this as the moment of a second age of democratic revolution.&lt;/p&gt;
    &lt;p&gt;But the 20th century added its own finishing touches to the story of choice. The ranks of choosers continued to expand, albeit highly unevenly, to include women, poor people, sometimes even children, especially in places where mass goods, from newspapers to chewing gum, became widely available. So did the ranks of ‘choice agents’, the people creating the menus of options, inventing the rules, and directing the activity itself. Beyond shop owners, itinerant preachers, dancing masters and political party officials, now new kinds of social scientists came to the fore. Psychiatrists, marketing experts, economists: in different ways, they all devoted themselves to the study of choice-making, exploring who makes what choices under what conditions and with what effects, along with how individuals and groups could be steered to make better ones. Ordinary people participated in this work every time they sat on a couch for a therapy session or filled out a survey card or took a multiple-choice exam. Together, researchers and their everyday subjects, male and female, invented sciences of choice, further entrenching the idea of humans as, fundamentally, choosers.&lt;/p&gt;
    &lt;p&gt;Needless to say, the rise of the internet has only expanded this model. Today, the sheer number of both choice-making opportunities and options has grown exponentially, whether we are talking about music or vacuum cleaners. The nature of our choices has also changed. Before the age of shopping for goods and selecting ideas had really gotten underway, most choices were structured around doing the right thing rather than the wrong. Since then, choice has increasingly become value-neutral, a matter of one’s own interior preferences being externalised in the act of selection. Moreover, choice has become more and more important to conceptions of human flourishing. Once, picking from menus was of relatively little significance, especially since freedom was imagined in the Western tradition well into the 18th century more often as a matter of not having to make many choices or strive too much thanks to being born with the status of an independent person. Over time, however, choice became a means of achieving the liberty to shape one’s life as one saw fit. It also became a key signifier of being a full-fledged, autonomous person worthy of respect by others. Since the end of the Second World War, we might say that it has become a value unto itself, widely celebrated from billboards to international human rights decrees as the meeting point of capitalism and democracy. When then French presidential candidate Emmanuel Macron said, in 2016, ‘I believe deeply in a society governed by choice,’ he was in a certain sense uttering what had become a banality.&lt;/p&gt;
    &lt;p&gt;This is a story that has not been told before, even as some of the details may feel familiar from lived experience. It is also an essential story to grapple with if we want to try to understand what has been gained and lost from an investment in choice as the defining feature of ‘free time’ as well as a basic understanding of freedom. How have people, individually and collectively, benefitted from this mushrooming of options and opportunities for choice – and how and when has choice led us astray? This is a question that behavioural economists and all others who take our current actions and investments as constants have largely failed to ask.&lt;/p&gt;
    &lt;p&gt;On the one hand, it is easy to read this narrative as a tale of liberation. Take feminism. Women in Europe and its outposts got their first real taste of the modern form of choosing as shoppers for ribbons, fabrics and other sundries, as late-18th-century novels – and especially those written by and for women – make very clear. Certainly, both women and this new kind of value-neutral, preference-based choice-making were quickly tainted by association with each other; the coquette was a much-mocked figure of the 18th and 19th centuries, a stereotype of a woman who relishes her own choices a little too much and isn’t very good at them either. But as new forms of individualised selection became more important to life beyond the textile purveyor’s shop, and as men got in on the game as well, an expanded repertoire of choice, including in ideas, reading matter, marriage, children, career and finally politics, became one of the key aspirations for women looking to break free of their traditional constraints.&lt;/p&gt;
    &lt;p&gt;Female suffrage, for example, could be advocated at the start of the 20th century as simply an extension of women’s already existing capacity to make a selection from a list-like menu. By the start of the 1960s, the American feminist Betty Friedan could argue that women’s full liberation required them to follow their male counterparts in seizing ‘the power to choose’ in their personal lives too, including in forging ‘an identity’ beyond housewife. And a decade later, mainstream abortion rights advocates, not surprisingly, took the same tack, imagining limited resistance to a focus on choice. Who, after all, could object? Behind the 1970s feminist idea of ‘a right to choice’ when it came to motherhood was the argument that no one should be compelled to pick this solution – abortion – for themselves; the law now simply ensured that everybody could, as needed, determine which of the possibilities on offer seemed like the best option by their own criteria. Plus, having choices meant the essential opportunity to reassert one’s standing as author of one’s own destiny – a point sometimes made by stateless people today as well. No wonder the ability to choose has become a key factor in global happiness indexes.&lt;/p&gt;
    &lt;p&gt;A market model for governance would, ironically, mean the end of democracy as we know it&lt;/p&gt;
    &lt;p&gt;But the abortion debate of the 1970s also illustrates some of the limitations of this framing. Soon after the passage of the Roe v Wade US Supreme Court Ruling legalising abortion in the early months of pregnancy, an emerging Right-wing coalition landed on a clever strategy for opposition, arguing that the ‘life’ of the fetus outweighed mere ‘choice’ on the part of the mother. In other words, in feminists’ enthusiasm for having choices, the moral dimension of what was being chosen had been pushed to the side, leaving behind a very thin foundation for a major policy matter. And from the Left, and especially from Black feminists, came the argument that choice itself was meaningless, an empty promise, unless it was to be accompanied by a commitment to meeting women’s basic needs, whether that meant the money necessary to travel and pay for an abortion, or greater financial and institutional support for mothers after their children were born. In this sense, they too warned of the dangers of the shopper and shopping as models for all our activities, even when couched as rights.&lt;/p&gt;
    &lt;p&gt;Now we are seeing some of the fallout of this debate writ large. Today, a far-Right ‘dark enlightenment’ movement imagines a market model for everything, including governance, which would, ironically, mean the end of democracy as we know it. At the same time, significant pushback around the world against feminism, and now against gay and trans rights as well, has become emblematic of the rejection of a larger vision of freedom rooted in personal choice. As a result of democracy-promotion and global capitalism, almost no one in the world currently stands entirely outside the choice-as-freedom paradigm; it has gradually enveloped even those with very limited ability or opportunity to choose or with only rotten choices before them. Voting, for example, is near universal today even in places like Russia, where it is a sham. But an emphasis on choice as a form of liberation has occasioned serious resentments in different sectors and geographies, where it can seem a direct threat to other, more communal values and needs.&lt;/p&gt;
    &lt;p&gt;Indeed, even in democracies, choice can sometimes seem to be not only an illusion (is there any real difference between the scores of toothpastes or breakfast cereals in contemporary supermarkets?) or a headache to contend with, but a regressive force. Think, for example, of people who took up the pro-abortion rights phrase ‘My body, my choice’ to protest mask or vaccine mandates during the COVID-19 pandemic, even as they were told that the point of both actions was to limit the spread of the disease and advance public health more broadly. Or consider how the US president Donald Trump’s current claims to be restoring the American people’s ‘freedom to choose’ in the market for cars and appliances will require gutting environmental regulations and thus advancing climate change in ways that will negatively impact all of us. It’s not just that we don’t always know our minds. It’s that choice in its current incarnation isn’t, in fact, always freeing.&lt;/p&gt;
    &lt;p&gt;So where does this leave us? The answer is not with one or the other of these visions. But considering the history of choice should make us more self-conscious the next time we are fretting over whether to pick the oat milk rather than the half-and-half, not to mention one train ticket or candidate for office or college course over another. We might instead ask ourselves: when, collectively, should we be invested in individual choice as a good way to solve a shared problem, and when not? And when should I, as an individual, try to maximise the opportunity to make choices about my own life versus not doing so? Most of all, though – as we struggle with both choice overload and the failures of personal choice to help us solve some of our biggest problems, including the rise of forms of authoritarianism directed squarely against choice – thinking about our attachment to choosing off menus should make us wonder what other possibilities for defining freedom might be lurking out there. In the past, for example, freedom has sometimes been imagined as a release from oppression or as an act of pure imagination, alternative visions we might want to bring back into circulation. As it turns out, choice doesn’t always produce freedom, and freedom itself often looks very different.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45634641</guid><pubDate>Sun, 19 Oct 2025 14:58:16 +0000</pubDate></item><item><title>RFCs: Blueprints of the Internet</title><link>https://ackreq.github.io/posts/what-are-rfcs/</link><description>&lt;doc fingerprint="bb881b5309e59fce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What Are RFCs? The Forgotten Blueprints of the Internet&lt;/head&gt;
    &lt;p&gt;Think about it for a second: could the internet exist without standards and protocols? Of course not! Computers need shared rules and agreements to communicate with one another. Even human languages, like English, work much the same way. They function as a kind of communication protocol because we’ve all agreed on words and grammar that carry shared meaning. In both cases, whether among machines or people, communication depends on common understanding.&lt;/p&gt;
    &lt;p&gt;This is where RFCs come in. They’re the blueprints and proposals that define how the internet operates and how systems interact. In this post, we’ll take a closer look at RFCs and uncover some of the fascinating history behind the internet.&lt;/p&gt;
    &lt;p&gt;Before we dive in, here’s a quick overview: RFCs, or Requests for Comments, are official documents that explain how Internet technologies work. They outline how systems are expected to behave and interact. Think of them as the reference guides for anyone who wants to build, understand, or improve the Internet.&lt;/p&gt;
    &lt;head rend="h2"&gt;the Birth of the Internet&lt;/head&gt;
    &lt;p&gt;The history of the internet is a long and amazing tale — one that deserves its own post (and I’ll probably write about it). But for now, let’s focus on how it all began.&lt;/p&gt;
    &lt;p&gt;I’m not going to answer “Who invented the Internet?” because that’s the wrong question. The internet didn’t appear overnight; it took decades to mature. Instead, we can highlight the people who played key roles in shaping today’s internet.&lt;/p&gt;
    &lt;p&gt;It all started in the USA in 1958, when the government created the Advanced Research Projects Agency (ARPA) to fund research in new technologies — partly driven by Cold War tensions. The US government was worried that a nuclear first strike from the Soviets could wipe out their communication. To prevent that, they established computer research centers at leading universities. The goal was to create a reliable, distributed communication system that could continue operating even if parts of it were damaged by a nuclear attack.&lt;/p&gt;
    &lt;p&gt;Computers — or better to say, mainframes back in those days — were gigantic and could fill an entire room. Here’s what an IBM 7090 mainframe looked like in the early 60s:&lt;/p&gt;
    &lt;p&gt;Fernando Corbató with MIT’s IBM 7090&lt;/p&gt;
    &lt;p&gt;Back then, transferring data was nothing like the internet file uploads we know today. First, everything was physical: punch cards or paper tapes had to be loaded manually into machines. Then came magnetic tapes (big reels of tape containing data) which you’d physically transport to other machines:&lt;/p&gt;
    &lt;p&gt;Set of punch cards + punched paper tape + magnetic tape&lt;/p&gt;
    &lt;p&gt;Since these computers were geographically separated, they needed a way to connect and exchange information reliably and fast. The solution was to develop a packet-switching network, which could send data in small blocks called “packets” that could travel independently across the network and be reassembled at their destination. This system eventually became the ARPANET, the first network to implement packet switching, laying the foundation for the modern internet.&lt;/p&gt;
    &lt;p&gt;In fact, the first use of the term protocol in a modern data communication context appeared in April 1967, in a memorandum titled “A Protocol for Use in the NPL Data Communications Network”. It was written under the direction of Donald Davies, who pioneered the concept of packet switching.&lt;/p&gt;
    &lt;p&gt;In 1969, the first message was sent over ARPANET from UCLA to Stanford university. They tried to send the word &lt;code&gt;LOGIN&lt;/code&gt;, but only &lt;code&gt;LO&lt;/code&gt; made it through before the system crashed. About an hour later, after recovering from the crash, the full message was successfully transmitted.&lt;/p&gt;
    &lt;p&gt;by 1970, there were around 15 nodes (or computers), and by 1972, 19 nodes were connected. In 1973 they even created a map of ARPANET — the same one you see in the post preview image. ARPANET was considered a major success because it showed that packet-switching technology worked in practice and made it possible for distant computers to share information reliably. However, access was still limited to universities and research organizations that held contracts with the U.S. Department of Defense.&lt;/p&gt;
    &lt;p&gt;As you can see, the network was growing rapidly and that didn’t happen by chance. It was the result of coordination and collaboration. Every node and computer had to follow the same rules and standards to communicate effectively.&lt;/p&gt;
    &lt;p&gt;By the late 1980s, the foundations laid by ARPANET and early networking experiments made it possible for something revolutionary: in 1989, Tim Berners-Lee proposed the World Wide Web (WWW), which went public in 1991, opening the internet to everyone.&lt;/p&gt;
    &lt;p&gt;We’ll pause the story of the internet here, having covered the key parts. If you’re interested to know more, you can check out this infographic:&lt;/p&gt;
    &lt;p&gt;Internet History Timeline (Infographic from Behance)&lt;/p&gt;
    &lt;p&gt;Now it’s time to explore the technical documents that shape and standardize Internet operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Are RFCs?&lt;/head&gt;
    &lt;p&gt;Request for Comments (RFCs) are a series of numbered documents that describe how the internet works and how different systems communicate. They address a variety of topics, including core standards, communication protocols, guidelines, design ideas, and concepts that help keep the global network running smoothly. Each RFC is written by engineers and computer scientists as a memorandum presenting new ideas, research findings, proposed methods, or other concepts related to internet technologies.&lt;/p&gt;
    &lt;p&gt;The RFC system was created in 1969 by Steve Crocker to record and share informal notes on the development of ARPANET. The goal was to help researchers share ideas about how the network should operate, how computers (hosts) should communicate, and how software running on these hosts should behave.&lt;/p&gt;
    &lt;p&gt;The very first RFC, titled “Host Software”, was published by Crocker himself on April 7, 1969. In this context, “host software” referred to the programs and protocols that computers needed to communicate over ARPANET, essentially the foundational rules for networked computing at the time:&lt;/p&gt;
    &lt;p&gt;RFC 1: The first Request for Comments document published in 1969&lt;/p&gt;
    &lt;p&gt;Every RFC is assigned a unique number upon publication — starting with RFC 1 — and these numbers are permanent. Once a document receives its number, it never changes or gets reused, even if that RFC later becomes obsolete or is replaced by a newer one. This numbering system helps maintain a consistent historical record of the internet’s evolution and ensures that every RFC can be precisely referenced.&lt;/p&gt;
    &lt;p&gt;Today, RFCs are maintained and published by the Internet Engineering Task Force (IETF), which continues to develop and expand them. While many RFCs are experimental in nature and never become official standards, others have become the backbone of the Internet’s architecture. These include the core technologies we rely on every day, such as TCP/IP, HTTP, and DNS. RFCs not only define how these protocols operate but also reveal the reasoning behind their design, helping us understand how the global network actually operates.&lt;/p&gt;
    &lt;p&gt;RFCs are often described as the blueprints of the internet. Yet, in an age where artificial intelligence and higher-level tools make technology more accessible, fewer people explore the underlying systems and details of how things actually work (at least, that’s my perspective). I believe RFCs are essential reading for anyone involved in technology or IT — that’s why I wrote this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why They Still Matter Today?&lt;/head&gt;
    &lt;p&gt;RFCs remain the official source of truth for Internet standards, ensuring consistency across the global network. By studying them, developers learn not just the rules, but also why they exist, gaining the knowledge needed to build software and systems that communicate reliably with other computers.&lt;/p&gt;
    &lt;p&gt;You can’t create something truly dependable without understanding its foundations — just as you couldn’t design a beautiful building without knowing architecture, the same principle applies to apps and networked systems.&lt;/p&gt;
    &lt;p&gt;For instance, if you ever wanted to build your own DNS server, the first step isn’t writing code from scratch or copying an online tutorial. You’d start by reading relevant RFCs, which define the domain name system and how queries and responses should work. By understanding the protocol from the original source, you can ensure your implementation is reliable, interoperable, and standards-compliant. This is the power of RFCs: they let you build on solid foundations rather than reinventing the wheel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding and Reading Them&lt;/head&gt;
    &lt;p&gt;The official source for RFCs is the RFC Editor, which manages the publication, editing, and archiving of all RFC documents. If you want to explore the development process behind RFCs — including drafts, authors, and approval stages — the IETF Datatracker provides detailed information on every document’s history and current status. Also, for a more comfortable reading experience, RFC Reader offers an online viewer with features like an automatic table of contents, note-taking, and search capabilities.&lt;/p&gt;
    &lt;p&gt;For guidance on how to read RFCs, the IETF provides a helpful article titled “How to Read an RFC”. I strongly recommend reading it. The article explains how to search for the right documents, understand the structure of RFCs, and identify the most relevant information on the first page.&lt;/p&gt;
    &lt;p&gt;Some RFCs are informational or experimental, so you should be careful about which ones you read. For example, RFC 1149 literally describes a method for transmitting IP packets using pigeons! Its humorous follow-up, RFC 2549, improves on the idea. You can think of it like something out of a Harry Potter movie — sending messages via birds — but applied, jokingly, to the Internet:&lt;/p&gt;
    &lt;p&gt;RFC 1149: IP over Avian Carriers&lt;/p&gt;
    &lt;p&gt;Also, RFCs are archival documents, which means they cannot be updated once published. As a result, older RFCs may be obsolete or superseded by newer versions, and it’s important to ensure you are reading the correct, up-to-date document. The IETF article linked above explains how to identify the most relevant RFCs and determine which ones are current.&lt;/p&gt;
    &lt;p&gt;You can also check the references at the end of a Wikipedia article on a given topic, where several related RFCs are often listed for further reading.&lt;/p&gt;
    &lt;p&gt;When reading these documents, you might come across words like “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY”, and “OPTIONAL”. These are not just casual suggestions — they have precise and standardized meanings in the context of RFCs. These terms are defined in RFC 2119, which provides guidance for specifying requirement levels in technical documents. Here’s a quick summary of what they mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MUST / MUST NOT / REQUIRED / SHALL / SHALL NOT – Indicates an absolute requirement; the behavior described is mandatory.&lt;/item&gt;
      &lt;item&gt;SHOULD / SHOULD NOT / RECOMMENDED – Indicates a strong recommendation, but there may be valid reasons to deviate.&lt;/item&gt;
      &lt;item&gt;MAY / OPTIONAL – Indicates a truly optional behavior; implementers have complete discretion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Understanding these words is crucial because RFCs use them to clearly communicate which rules are mandatory and which are flexible, ensuring interoperability and consistency across the Internet. If you plan to implement protocols, don’t skim RFCs or read them selectively. It’s easy to misinterpret a specification if you only look at part of it. You should read not just the sections that seem directly relevant to what you’re working on, but also any referenced material, to fully understand its requirements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;The early developers built much of the technology we rely on today without the Internet or Stack Overflow, relying purely on their skill, curiosity, and persistence. It’s easy to copy and paste something that works — but doing so only makes you one of many. The ones who truly understand are those who push limits and create what has never existed before.&lt;/p&gt;
    &lt;p&gt;Every protocol, every standard, every “MUST” or “SHOULD” is part of a story crafted by engineers over decades. So don’t be intimidated — explore, read carefully, and let these documents guide you. And if you ever discover a better idea or approach, share it with the world — perhaps even as an RFC. Who knows? The next specification you write could help shape the Internet of tomorrow.&lt;/p&gt;
    &lt;p&gt;I’ll end this post with a quote from one of my all-time favorite animated films, Ratatouille — a reminder that mastery comes from courage and curiosity:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“You must try things that may not work, and you must not let anyone define your limits because of where you come from. Your only limit is your soul. What I say is true — anyone can cook… but only the fearless can be great.”&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45634678</guid><pubDate>Sun, 19 Oct 2025 15:03:29 +0000</pubDate></item><item><title>The Spherical Cows of Programming</title><link>https://programmingsimplicity.substack.com/p/the-spherical-cows-of-programming</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45634811</guid><pubDate>Sun, 19 Oct 2025 15:18:58 +0000</pubDate></item><item><title>GNU Octave Meets JupyterLite: Compute Anywhere, Anytime</title><link>https://blog.jupyter.org/gnu-octave-meets-jupyterlite-compute-anywhere-anytime-8b033afbbcdc</link><description>&lt;doc fingerprint="469708be47d97371"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GNU Octave Meets JupyterLite: Compute Anywhere, Anytime!&lt;/head&gt;
    &lt;p&gt;We are thrilled to announce the newest member of our JupyterLite kernel ecosystem: Xeus-Octave. Xeus-Octave allows you to run GNU Octave code directly on your browser. GNU Octave is a free and open-source Scientific Programming Language that can be used to run Matlab scripts. In this article, we present the challenges encountered when targeting WebAssembly, the current state of the Xeus-Octave kernel, and the future plans for expanding the GNU Octave ecosystem.&lt;/p&gt;
    &lt;p&gt;Earlier this year, we introduced the JupyterLite kernel for R, Xeus-R-Lite. Much like R, cross-compiling GNU Octave to WebAssembly required the same custom toolchain to enable the compilation of Fortran code, combining LLVM Flang and Emscripten.&lt;/p&gt;
    &lt;p&gt;Similar to many other mathematically oriented language packages, GNU Octave requires a BLAS/LAPACK implementation. Fortunately, OpenBLAS and the Netlib implementations of BLAS/LAPACK had already been added to the emscripten-forge WebAssembly distribution. Initially, OpenBLAS was the preferred implementation, but for the successful compilation of Octave, Netlib LAPACK was selected as it presented fewer hurdles during the build process.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cross-Compilation of GNU Octave&lt;/head&gt;
    &lt;p&gt;One of the complications of cross-compiling Octave to WebAssembly, which had not been encountered with the R source code, was the extensive use of Fortran common symbols blocks in the internal libraries of Octave such as odepack.&lt;/p&gt;
    &lt;code&gt;C Source: liboctave/external/odepack/slsode.f&lt;lb/&gt;C-----------------------------------------------------------------------&lt;lb/&gt;C The following internal Common block contains&lt;lb/&gt;C (a) variables which are local to any subroutine but whose values must&lt;lb/&gt;C     be preserved between calls to the routine ("own" variables), and&lt;lb/&gt;C (b) variables which are communicated between subroutines.&lt;lb/&gt;C The block SLS001 is declared in subroutines SLSODE, SINTDY, SSTODE,&lt;lb/&gt;C SPREPJ, and SSOLSY.&lt;lb/&gt;C Groups of variables are replaced by dummy arrays in the Common&lt;lb/&gt;C declarations in routines where those variables are not used.&lt;lb/&gt;C-----------------------------------------------------------------------&lt;lb/&gt;     COMMON /SLS001/ CONIT, CRATE, EL(13), ELCO(13,12),&lt;lb/&gt;    1   HOLD, RMAX, TESCO(3,12),&lt;lb/&gt;    1   CCMAX, EL0, H, HMIN, HMXI, HU, RC, TN, UROUND,&lt;lb/&gt;    2   INIT, MXSTEP, MXHNIL, NHNIL, NSLAST, NYH,&lt;lb/&gt;    3   IALTH, IPUP, LMAX, MEO, NQNYH, NSLP,&lt;lb/&gt;    3   ICF, IERPJ, IERSL, JCUR, JSTART, KFLAG, L,&lt;lb/&gt;    4   LYH, LEWT, LACOR, LSAVF, LWM, LIWM, METH, MITER,&lt;lb/&gt;    5   MAXORD, MAXCOR, MSBP, MXNCF, N, NQ, NST, NFE, NJE, NQU&lt;/code&gt;
    &lt;p&gt;Initially, it was not possible to cross-compile these common blocks to WebAssembly because the latest version of LLVM (v20 at the time of testing) did not support common symbol linkage.&lt;/p&gt;
    &lt;code&gt;// Source: llvm/lib/MC/MCWasmStreamer.cpp&lt;lb/&gt;void MCWasmStreamer::emitCommonSymbol(MCSymbol *S, uint64_t Size,&lt;lb/&gt;                                      Align ByteAlignment) {&lt;lb/&gt;  llvm_unreachable("Common symbols are not yet implemented for Wasm");&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;As a temporary solution, LLVM was patched with the help of Serge Guelton to simulate common symbols as weak symbols.&lt;/p&gt;
    &lt;code&gt;void MCWasmStreamer::emitCommonSymbol(MCSymbol *S, uint64_t Size,&lt;lb/&gt;                                      Align ByteAlignment) {&lt;lb/&gt;-  llvm_unreachable("Common symbols are not yet implemented for Wasm");&lt;lb/&gt;+  auto *Symbol = cast&amp;lt;mcsymbolwasm&amp;gt;(S);&lt;lb/&gt;+  getAssembler().registerSymbol(*Symbol);&lt;lb/&gt;+  Symbol-&amp;gt;setWeak(true);&lt;lb/&gt;+  Symbol-&amp;gt;setExternal(true);&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;A proper solution to enable support of common symbols is currently in progress and will likely be included in the next release of LLVM v22 (see llvm-project/pull/151478). For curious readers, the patched version of LLVM can be found here (linux only).&lt;/p&gt;
    &lt;p&gt;In addition to the patches for LLVM, GNU Octave required a few minor modifications to target WebAssembly; mainly this entailed disabling the GUI functionalities and consolidating the Fortran function signatures and calling conventions. A full list of patches can be found in the recipe directory on emscripten-forge.&lt;/p&gt;
    &lt;head rend="h2"&gt;Xeus-Octave&lt;/head&gt;
    &lt;p&gt;Once GNU Octave had been successfully packaged for WebAssembly, bringing Xeus-Octave to JupyterLite was a simple matter of adding a recipe to emscripten-forge!&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Work&lt;/head&gt;
    &lt;p&gt;For our next steps, the team is planning on expanding the Octave ecosystem by adding Octave packages to both conda-forge and emscripten-forge. The packaging work will require defining a process where Octave packages can be installed in predetermined conda environments, perhaps with some minor modifications to the existing pkg utility.&lt;/p&gt;
    &lt;head rend="h2"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;Isabel Paredes, who led the charge on bringing GNU Octave to emscripten-forge, is a senior scientific software developer at QuantStack. Prior to working on this project, she focused on porting the R programming language and the Robot Operating System (ROS) framework to WebAssembly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;This project synthesizes work from many open-source contributors.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Emscripten-forge, the distribution of conda packages for WebAssembly, was created by Thorsten Beier, who continues to lead the project. Many recipes were contributed by Isabel Paredes, Anutosh Bhat, Martin Renou, Ian Thomas, Wolf Vollprecht, and Johan Mabille.&lt;/item&gt;
      &lt;item&gt;JupyterLite, the Jupyter distribution that runs entirely in the web browser, was created by Jeremy Tuloup.&lt;/item&gt;
      &lt;item&gt;Xeus, the C++ library implementing the Jupyter kernel protocol, enabling a custom communication layer, and is foundational to kernels like xeus-r, xeus-python, running in JupyterLite, was created by Johan Mabille and is maintained by a broader team including Martin Renou, Sylvain Corlay, and Thorsten Beier, who worked on the first integration with JupyterLite.&lt;/item&gt;
      &lt;item&gt;Xeus-Octave, the Xeus-based Jupyter kernel for GNU Octave, was created by Giulio Girardi and Antoine Prouvost.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45635069</guid><pubDate>Sun, 19 Oct 2025 15:48:52 +0000</pubDate></item><item><title>Show HN: Notepad.exe – macOS editor for Swift and Python (now Linux runtime)</title><link>https://notepadexe.com/</link><description>&lt;doc fingerprint="740e75936f67bcaa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stop Fighting With IDEs.&lt;lb/&gt;Start Coding.&lt;/head&gt;
    &lt;p&gt; The lightweight editor for rapid prototyping.&lt;lb/&gt; Launch instantly. Execute immediately. &lt;/p&gt;
    &lt;p&gt;macOS 15.0+&lt;/p&gt;
    &lt;head rend="h2"&gt;See It In Action&lt;/head&gt;
    &lt;head rend="h2"&gt;What Makes It Special&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero Setup, Infinite Possibilities: Write Swift or Python code and hit run. No Xcode projects, no configuration files needed.&lt;/item&gt;
      &lt;item&gt;Swift, Python &amp;amp; iOS Development: Full language support with intelligent code completion and automatic iOS simulator integration for SwiftUI apps. Read about our latest updates.&lt;/item&gt;
      &lt;item&gt;Swift on Linux From Your Mac: Run and test Swift code for Linux platforms directly from macOS. Everything is handled automatically - just write your code and run it.&lt;/item&gt;
      &lt;item&gt;Smart Notes Library: Every code snippet saved and searchable. Filter by language, tags, or recent edits in your personal code library.&lt;/item&gt;
      &lt;item&gt;AI-Powered, Privacy-First: Local AI code assistance that respects your privacy. What happens on your Mac, stays on your Mac. Learn about our privacy commitment.&lt;/item&gt;
      &lt;item&gt;Built for Focus: Clean native macOS interface without the IDE bloat. Just you and your Swift or Python code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;And So Much More&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swift &amp;amp; Python Language Support&lt;/item&gt;
      &lt;item&gt;Syntax Highlighting&lt;/item&gt;
      &lt;item&gt;Intelligent Code Completion&lt;/item&gt;
      &lt;item&gt;Instant Code Execution&lt;/item&gt;
      &lt;item&gt;iOS Simulator Integration&lt;/item&gt;
      &lt;item&gt;Run on Linux Support&lt;/item&gt;
      &lt;item&gt;Mac Application Development&lt;/item&gt;
      &lt;item&gt;Real-time Error Detection&lt;/item&gt;
      &lt;item&gt;Code Formatting&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smart Notes Library&lt;/item&gt;
      &lt;item&gt;Search &amp;amp; Filter Notes&lt;/item&gt;
      &lt;item&gt;Tag Organization&lt;/item&gt;
      &lt;item&gt;Pin Favorite Notes&lt;/item&gt;
      &lt;item&gt;Recent Files Access&lt;/item&gt;
      &lt;item&gt;Export Code as Image&lt;/item&gt;
      &lt;item&gt;Export Mac Applications&lt;/item&gt;
      &lt;item&gt;Share Code Snippets&lt;/item&gt;
      &lt;item&gt;Dark &amp;amp; Light Themes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI Coding Assistant&lt;/item&gt;
      &lt;item&gt;Local AI Processing&lt;/item&gt;
      &lt;item&gt;Privacy-First Design&lt;/item&gt;
      &lt;item&gt;Multiple Cursors&lt;/item&gt;
      &lt;item&gt;Line Numbers &amp;amp; Guides&lt;/item&gt;
      &lt;item&gt;Keyboard Shortcuts&lt;/item&gt;
      &lt;item&gt;Spotlight Integration&lt;/item&gt;
      &lt;item&gt;Auto-Save&lt;/item&gt;
      &lt;item&gt;Native macOS Performance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What Developers Are Saying&lt;/head&gt;
    &lt;p&gt;I'm crafting Notepad.exe as the best lightweight code editor for macOS developers. Whether you're exploring Swift APIs, testing Python scripts, or prototyping iOS apps, this native Mac app provides the perfect environment without the complexity of full IDEs like Xcode or PyCharm. Students and educators can apply for free academic access. Check out the [FAQ] section for answers to the most common questions, or visit our press kit for media resources.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Q: How do I get Notepad.exe?&lt;/item&gt;
      &lt;item rend="dd-1"&gt;A: Download and purchase Notepad.exe or try the free version with core features. You can also subscribe to receive information about releases.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Q: When will you support language X?&lt;/item&gt;
      &lt;item rend="dd-2"&gt;A: Sooner than later. I'm starting with Swift, though, because I'm classy like that.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Q: Can't I just use (insert tool you have an opinion about)?&lt;/item&gt;
      &lt;item rend="dd-3"&gt;A: Sure, why not? Go ahead and be a rebel!&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Q: (Tool you have an opinion about) does it better.&lt;/item&gt;
      &lt;item rend="dd-4"&gt;A: That's fantastic news! More power to you and your tool-wielding skills!&lt;/item&gt;
      &lt;item rend="dt-5"&gt;Q: Why?&lt;/item&gt;
      &lt;item rend="dd-5"&gt;A: Because I can... and also because unicorns told me so.&lt;/item&gt;
      &lt;item rend="dt-6"&gt;Q: Is this Swift Studio?&lt;/item&gt;
      &lt;item rend="dd-6"&gt;A: It might transform into one after midnight. Who knows?&lt;/item&gt;
      &lt;item rend="dt-7"&gt;Q: Why isn't it open source?&lt;/item&gt;
      &lt;item rend="dd-7"&gt;A: For very mysterious reasons, like protecting the last piece of grandma's secret pie recipe. Plus, parts are open source on GitHub, so I'm not a total villain!&lt;/item&gt;
      &lt;item rend="dt-8"&gt;Q: I could build it over the weekend myself?&lt;/item&gt;
      &lt;item rend="dd-8"&gt;A: Go for it! Just make sure to send me a postcard when you're done.&lt;/item&gt;
      &lt;item rend="dt-9"&gt;Q: Why don't you just...?&lt;/item&gt;
      &lt;item rend="dd-9"&gt;A: Yes! Wait, what was the question again?&lt;/item&gt;
      &lt;item rend="dt-10"&gt;Q: It looks like CodeRunner with some AI sprinkled in?&lt;/item&gt;
      &lt;item rend="dd-10"&gt;A: Sure, and my toaster's a space shuttle. But hey, great detective work, Sherlock!&lt;/item&gt;
      &lt;item rend="dt-11"&gt;Q: Do I need Xcode installed?&lt;/item&gt;
      &lt;item rend="dd-11"&gt;A: Notepad.exe works with just Swift Toolchain installed, but having the SDK from Xcode allows you to run applications. It's like having both the recipe and the oven!&lt;/item&gt;
      &lt;item rend="dt-12"&gt;Q: Can I develop iOS apps with Notepad.exe?&lt;/item&gt;
      &lt;item rend="dd-12"&gt;A: I spent months adding iOS simulators just so you could ask this question. Yes, it runs iOS code now. Your iPhone apps can finally escape Xcode's loving embrace!&lt;/item&gt;
      &lt;item rend="dt-13"&gt;Q: Did anyone actually ask these questions?&lt;/item&gt;
      &lt;item rend="dd-13"&gt;A: Nope, but it feels good to pretend they did!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;[ made by marcin ]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45635147</guid><pubDate>Sun, 19 Oct 2025 15:56:12 +0000</pubDate></item><item><title>Windows 11 25H2 October Update Bug Renders Recovery Environment Unusable</title><link>https://www.techpowerup.com/342032/windows-11-25h2-october-update-bug-renders-recovery-environment-unusable</link><description>&lt;doc fingerprint="9443181d0f650e9c"&gt;
  &lt;main&gt;
    &lt;p&gt;a guide by Andrej Karpathy&lt;/p&gt;
    &lt;p&gt;Here is some advice I would give to younger students if they wish to do well in their undergraduate courses.&lt;lb/&gt; Having been tested for many years of my life (with pretty good results), here are some rules of thumb that I feel helped me:&lt;/p&gt;
    &lt;p&gt; All-nighters are not worth it. &lt;lb/&gt; Sleep does wonders. Optimal sleep time for me is around 7.5 hours, with an absolute minimum of around 4hrs.&lt;lb/&gt; It has happened to me several times that I was stuck on some problem for an hour in the night, but was able to solve it in 5 minutes in the morning. I feel like the brain "commits" a lot of shaky short-term memories to stable long-term memories during the night. I try to start studying for any big tests well in advance (several days), even if for short periods of time, to maximize the number of nights that my brain gets for the material. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Attend tutorials or review sessions.&lt;lb/&gt; Even if they are bad. The fact that they get you to think about the material is what counts. If its too boring, you can always work on something else. Remember that you can also try to attend a different tutorial with a different TA.&lt;/p&gt;
    &lt;p&gt; Considering the big picture and organisation is the key. &lt;lb/&gt; Create schedule of study, even if you dont stick to it. For me this usually involves getting an idea of everything I need to know and explicitly writing it down in terms of bullet points. Consider all points carefully and think about how long it will take you to get them down. If you don't do this, there is a tendency to spend too much time on beginning of material and then skim through the (most important) later material due to lack of time.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to look at previous tests BEFORE starting to study.&lt;lb/&gt; Especially if the past tests were written by the same professor. This will give you strong hints about how you should study. Every professor has a different evaluation style. Don't actually attempt to complete the questions in the beginning, but take careful note of the type of questions.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Reading and understanding IS NOT the same as replicating the content.&lt;lb/&gt; Even I often make this mistake still: You read a formula/derivation/proof in the book and it makes perfect sense. Now close the book and try to write it down. You will find that this process is completely different and it will amaze you that many times you won't actually be able to do this! Somehow the two things use different parts of the memory. Make it a point to make sure that you can actually write down the most important bits, and that you can re-derive them at will. Feynman famously knew this very well.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to collaborate with others, but near the end. &lt;lb/&gt; Study alone first because in the early stages of studying others can only serve as a distraction. But near the end get together with others: they will often point out important pitfalls, bring up good issues, and sometimes give you an opportunity to teach. Which brings me to:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Don't only hang out only with stronger students.&lt;lb/&gt; Weaker students will have you explain things to them and you will find that teaching the material helps A LOT with understanding.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Go to the prof before final exam at least once for office hours. &lt;lb/&gt; Even if you have no questions (make something up!) Profs will sometimes be willing to say more about a test in 1on1 basis (things they would not disclose in front of the entire class). Don't expect it, but when this does happen, it helps a lot. Does this give you an unfair advantage over other students? Sometimes. It's a little shady :)&lt;lb/&gt; But in general it is a good idea to let the prof get to know you at least a little.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study well in advance. &lt;lb/&gt; Did I mention this already? Maybe I should stress it again. The brain really needs time to absorb material. Things that looked hard become easier with time. &lt;lb/&gt; You want to alocate ~3 days for midterms, ~6 days for exams.&lt;lb/&gt; If things are going badly and you get too tired, in emergency situations, jug an energy drink.&lt;lb/&gt; They work. It's just chemistry.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; For things like math: Exercise &amp;gt; Reading.&lt;lb/&gt; It is good to study to the point where you are reasonably ready to start the exercises, but then fill in the gaps through doing exercises, especially if you have many available to you. The exercises will also make you go back and read things you don't know.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Make yourself cheat sheet. &lt;lb/&gt; Even if you're not allowed to bring it to the exam. Writing things down helps. What you want is to cram the entire course on 1 or more pages that you can in the end tile in front of you and say with high degree of confidence "This is exactly everything I must know"&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study in places where other people study as well, even if not the same thing. &lt;lb/&gt; This makes you feel bad when you are the one not studying. It works for me :)&lt;lb/&gt; Places with a lot of background noise are bad and have a research-supported negative impact on learning. Libraries and Reading rooms work best.&lt;/p&gt;
    &lt;p&gt; Optimal eating/drinking habit is: T-2 hours get coffee and food.&lt;lb/&gt; For me, Coffee or Food RIGHT before the test is ALWAYS bad&lt;lb/&gt; Coffee right before any potentially stressful situation is ALWAYS bad.&lt;lb/&gt; No coffee at all is bad.&lt;lb/&gt; I realize the coffee bit may be subjective to me, but its something to think about for yourself.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study very intensely RIGHT before the test. &lt;lb/&gt; I see many people give up before the test and claim to "take a break". Short term memory is a wonderful thing, don't waste it! Study as intensely as possible right before the test. If you really feel you must take a break, take it about an hour before the test, but make sure you study really hard 30-45 minutes before the test.&lt;/p&gt;
    &lt;p&gt; Always use pencil for tests. &lt;lb/&gt; You want to be able to erase your garbage "solutions"&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Look over all questions very briefly before start. &lt;lb/&gt; A mere 1-3 second glance per question is good enough. Just absorb all key words, and get idea of the size of the entire test.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; On test, do easy questions first. &lt;lb/&gt; Do not allow yourself to get stuck on something too long. Come back to it later. I skip questions all the time... Sometimes I can complete as little as 30% of the test on my first pass. Some questions somehow become much easier once you're "warmed up", I can't explain it.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to be neat on the test. &lt;lb/&gt; Surprisingly few people actually realize this obvious fact: A human being will mark your test. A sad human being gives low marks. I suspected this as undergrad student and confirmed it strongly when I was TAing and actually marking. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always BOX IN/CIRCLE the answer&lt;lb/&gt; Especially when there is derivation around it. This allows the marker to give you a quick check mark for full marks and move on. Get in the mindset of a marker.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; NEVER. EVER. EVER. Leave test early. &lt;lb/&gt; You made a silly mistake (I guarantee it), find it and fix it. If you can't find it, try harder until time runs out. If you are VERY certain of no mistakes, work on making test more legible and easier to mark. Erase garbage, box in answers, add steps to proofs, etc.&lt;lb/&gt; I have no other way of putting this-- people who leave tests early are stupid. This is a clear example of a situation where potential benefits completely outweigh the cost. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Communicate with the marker. &lt;lb/&gt; Show the marker that you know more than what you put down. Ok you can't do a particular step, but make it clear that you know how to proceed if you did. Don't be afraid to leave notes when necessary. Believe it or not the markers often end up trying to find you more marks-- make it easy for them.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Consider number of points per question.&lt;lb/&gt; Many tests will tell you how many marks every question is worth. This can give you very strong hints when you are doing something wrong. It also gives you strong hints at what questions you should be working on. It is, of course, silly to spend too much time on questions worth little marks that are still relatively hard for you.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; If there are &amp;lt;5 minutes left and you are still stuck on some question, STOP. &lt;lb/&gt; Your time is better spent re-reading all questions and making absolutely sure you did not miss any secondary&lt;lb/&gt; questions, and that you answered everything. You wouldn't believe how many silly marks people lose this way.&lt;/p&gt;
    &lt;p&gt;Congratulations if you got all the way here! Now that you are here, here's my last (very important advice). It is something that I wish someone had told me when I was an undergraduate.&lt;/p&gt;
    &lt;p&gt;Undergrads tend to have tunnel vision about their classes. They want to get good grades, etc. The crucial fact to realize is that noone will care about your grades, unless they are bad. For example, I always used to say that the smartest student will get 85% in all of his courses. This way, you end up with somewhere around 4.0 score, but you did not over-study, and you did not under-study.&lt;/p&gt;
    &lt;p&gt;Your time is a precious, limited resource. Get to a point where you don't screw up on a test and then switch your attention to much more important endeavors. What are they?&lt;/p&gt;
    &lt;p&gt;Getting actual, real-world experience, working on real code base, projects or problems outside of silly course exercises is extremely imporant. Professors/People who know you and can write you a good reference letter saying that you have initiative, passion and drive are extremely important. Are you thinking of applying to jobs? Get a summer internship. Are you thinking of pursuing graduate school? Get research experience! Sign up for whatever programs your school offers. Or reach out to a professor/graduate student asking to get involved on a research project you like. This might work if they think you're driven and motivated enough. Do not underestimate the importance of this: A well-known professor who writes in their recommendation letter that you are driven, motivated and independent thinker completely dwarfs anything else, especially petty things like grades. It also helps a lot if you squeeze in at least one paper before you apply. Also, you should be aware that the biggest pet peeve from their side are over-excited undergrad students who sign up for a project, meet a few times, ask many questions, and then suddenly give up and disappear after all that time investment from the graduate student's or professor's side. Do not be this person (it damages your reputation) and do not give any indication that you might be.&lt;/p&gt;
    &lt;p&gt;Other than research projects, get involved with some group of people on side projects or better, start your own from scratch. Contribute to Open Source, make/improve a library. Get out there and create (or help create) something cool. Document it well. Blog about it. These are the things people will care about a few years down the road. Your grades? They are an annoyance you have to deal with along the way. Use your time well and good luck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45635287</guid><pubDate>Sun, 19 Oct 2025 16:09:43 +0000</pubDate></item><item><title>Doing well in your courses: Andrej's advice for success (2013)</title><link>https://cs.stanford.edu/people/karpathy/advice.html</link><description>&lt;doc fingerprint="9443181d0f650e9c"&gt;
  &lt;main&gt;
    &lt;p&gt;a guide by Andrej Karpathy&lt;/p&gt;
    &lt;p&gt;Here is some advice I would give to younger students if they wish to do well in their undergraduate courses.&lt;lb/&gt; Having been tested for many years of my life (with pretty good results), here are some rules of thumb that I feel helped me:&lt;/p&gt;
    &lt;p&gt; All-nighters are not worth it. &lt;lb/&gt; Sleep does wonders. Optimal sleep time for me is around 7.5 hours, with an absolute minimum of around 4hrs.&lt;lb/&gt; It has happened to me several times that I was stuck on some problem for an hour in the night, but was able to solve it in 5 minutes in the morning. I feel like the brain "commits" a lot of shaky short-term memories to stable long-term memories during the night. I try to start studying for any big tests well in advance (several days), even if for short periods of time, to maximize the number of nights that my brain gets for the material. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Attend tutorials or review sessions.&lt;lb/&gt; Even if they are bad. The fact that they get you to think about the material is what counts. If its too boring, you can always work on something else. Remember that you can also try to attend a different tutorial with a different TA.&lt;/p&gt;
    &lt;p&gt; Considering the big picture and organisation is the key. &lt;lb/&gt; Create schedule of study, even if you dont stick to it. For me this usually involves getting an idea of everything I need to know and explicitly writing it down in terms of bullet points. Consider all points carefully and think about how long it will take you to get them down. If you don't do this, there is a tendency to spend too much time on beginning of material and then skim through the (most important) later material due to lack of time.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to look at previous tests BEFORE starting to study.&lt;lb/&gt; Especially if the past tests were written by the same professor. This will give you strong hints about how you should study. Every professor has a different evaluation style. Don't actually attempt to complete the questions in the beginning, but take careful note of the type of questions.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Reading and understanding IS NOT the same as replicating the content.&lt;lb/&gt; Even I often make this mistake still: You read a formula/derivation/proof in the book and it makes perfect sense. Now close the book and try to write it down. You will find that this process is completely different and it will amaze you that many times you won't actually be able to do this! Somehow the two things use different parts of the memory. Make it a point to make sure that you can actually write down the most important bits, and that you can re-derive them at will. Feynman famously knew this very well.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to collaborate with others, but near the end. &lt;lb/&gt; Study alone first because in the early stages of studying others can only serve as a distraction. But near the end get together with others: they will often point out important pitfalls, bring up good issues, and sometimes give you an opportunity to teach. Which brings me to:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Don't only hang out only with stronger students.&lt;lb/&gt; Weaker students will have you explain things to them and you will find that teaching the material helps A LOT with understanding.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Go to the prof before final exam at least once for office hours. &lt;lb/&gt; Even if you have no questions (make something up!) Profs will sometimes be willing to say more about a test in 1on1 basis (things they would not disclose in front of the entire class). Don't expect it, but when this does happen, it helps a lot. Does this give you an unfair advantage over other students? Sometimes. It's a little shady :)&lt;lb/&gt; But in general it is a good idea to let the prof get to know you at least a little.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study well in advance. &lt;lb/&gt; Did I mention this already? Maybe I should stress it again. The brain really needs time to absorb material. Things that looked hard become easier with time. &lt;lb/&gt; You want to alocate ~3 days for midterms, ~6 days for exams.&lt;lb/&gt; If things are going badly and you get too tired, in emergency situations, jug an energy drink.&lt;lb/&gt; They work. It's just chemistry.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; For things like math: Exercise &amp;gt; Reading.&lt;lb/&gt; It is good to study to the point where you are reasonably ready to start the exercises, but then fill in the gaps through doing exercises, especially if you have many available to you. The exercises will also make you go back and read things you don't know.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Make yourself cheat sheet. &lt;lb/&gt; Even if you're not allowed to bring it to the exam. Writing things down helps. What you want is to cram the entire course on 1 or more pages that you can in the end tile in front of you and say with high degree of confidence "This is exactly everything I must know"&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study in places where other people study as well, even if not the same thing. &lt;lb/&gt; This makes you feel bad when you are the one not studying. It works for me :)&lt;lb/&gt; Places with a lot of background noise are bad and have a research-supported negative impact on learning. Libraries and Reading rooms work best.&lt;/p&gt;
    &lt;p&gt; Optimal eating/drinking habit is: T-2 hours get coffee and food.&lt;lb/&gt; For me, Coffee or Food RIGHT before the test is ALWAYS bad&lt;lb/&gt; Coffee right before any potentially stressful situation is ALWAYS bad.&lt;lb/&gt; No coffee at all is bad.&lt;lb/&gt; I realize the coffee bit may be subjective to me, but its something to think about for yourself.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study very intensely RIGHT before the test. &lt;lb/&gt; I see many people give up before the test and claim to "take a break". Short term memory is a wonderful thing, don't waste it! Study as intensely as possible right before the test. If you really feel you must take a break, take it about an hour before the test, but make sure you study really hard 30-45 minutes before the test.&lt;/p&gt;
    &lt;p&gt; Always use pencil for tests. &lt;lb/&gt; You want to be able to erase your garbage "solutions"&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Look over all questions very briefly before start. &lt;lb/&gt; A mere 1-3 second glance per question is good enough. Just absorb all key words, and get idea of the size of the entire test.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; On test, do easy questions first. &lt;lb/&gt; Do not allow yourself to get stuck on something too long. Come back to it later. I skip questions all the time... Sometimes I can complete as little as 30% of the test on my first pass. Some questions somehow become much easier once you're "warmed up", I can't explain it.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to be neat on the test. &lt;lb/&gt; Surprisingly few people actually realize this obvious fact: A human being will mark your test. A sad human being gives low marks. I suspected this as undergrad student and confirmed it strongly when I was TAing and actually marking. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always BOX IN/CIRCLE the answer&lt;lb/&gt; Especially when there is derivation around it. This allows the marker to give you a quick check mark for full marks and move on. Get in the mindset of a marker.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; NEVER. EVER. EVER. Leave test early. &lt;lb/&gt; You made a silly mistake (I guarantee it), find it and fix it. If you can't find it, try harder until time runs out. If you are VERY certain of no mistakes, work on making test more legible and easier to mark. Erase garbage, box in answers, add steps to proofs, etc.&lt;lb/&gt; I have no other way of putting this-- people who leave tests early are stupid. This is a clear example of a situation where potential benefits completely outweigh the cost. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Communicate with the marker. &lt;lb/&gt; Show the marker that you know more than what you put down. Ok you can't do a particular step, but make it clear that you know how to proceed if you did. Don't be afraid to leave notes when necessary. Believe it or not the markers often end up trying to find you more marks-- make it easy for them.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Consider number of points per question.&lt;lb/&gt; Many tests will tell you how many marks every question is worth. This can give you very strong hints when you are doing something wrong. It also gives you strong hints at what questions you should be working on. It is, of course, silly to spend too much time on questions worth little marks that are still relatively hard for you.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; If there are &amp;lt;5 minutes left and you are still stuck on some question, STOP. &lt;lb/&gt; Your time is better spent re-reading all questions and making absolutely sure you did not miss any secondary&lt;lb/&gt; questions, and that you answered everything. You wouldn't believe how many silly marks people lose this way.&lt;/p&gt;
    &lt;p&gt;Congratulations if you got all the way here! Now that you are here, here's my last (very important advice). It is something that I wish someone had told me when I was an undergraduate.&lt;/p&gt;
    &lt;p&gt;Undergrads tend to have tunnel vision about their classes. They want to get good grades, etc. The crucial fact to realize is that noone will care about your grades, unless they are bad. For example, I always used to say that the smartest student will get 85% in all of his courses. This way, you end up with somewhere around 4.0 score, but you did not over-study, and you did not under-study.&lt;/p&gt;
    &lt;p&gt;Your time is a precious, limited resource. Get to a point where you don't screw up on a test and then switch your attention to much more important endeavors. What are they?&lt;/p&gt;
    &lt;p&gt;Getting actual, real-world experience, working on real code base, projects or problems outside of silly course exercises is extremely imporant. Professors/People who know you and can write you a good reference letter saying that you have initiative, passion and drive are extremely important. Are you thinking of applying to jobs? Get a summer internship. Are you thinking of pursuing graduate school? Get research experience! Sign up for whatever programs your school offers. Or reach out to a professor/graduate student asking to get involved on a research project you like. This might work if they think you're driven and motivated enough. Do not underestimate the importance of this: A well-known professor who writes in their recommendation letter that you are driven, motivated and independent thinker completely dwarfs anything else, especially petty things like grades. It also helps a lot if you squeeze in at least one paper before you apply. Also, you should be aware that the biggest pet peeve from their side are over-excited undergrad students who sign up for a project, meet a few times, ask many questions, and then suddenly give up and disappear after all that time investment from the graduate student's or professor's side. Do not be this person (it damages your reputation) and do not give any indication that you might be.&lt;/p&gt;
    &lt;p&gt;Other than research projects, get involved with some group of people on side projects or better, start your own from scratch. Contribute to Open Source, make/improve a library. Get out there and create (or help create) something cool. Document it well. Blog about it. These are the things people will care about a few years down the road. Your grades? They are an annoyance you have to deal with along the way. Use your time well and good luck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45635533</guid><pubDate>Sun, 19 Oct 2025 16:31:57 +0000</pubDate></item><item><title>The Trinary Dream Endures</title><link>https://www.robinsloan.com/lab/trinary-dream/</link><description>&lt;doc fingerprint="e8fb7a68e471e35"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The trinary dream endures&lt;/head&gt;&lt;p&gt;Since the beginning, there has been an alternative vision for computing, not binary but trinary, also called ternary. (âTrinaryâ sounds so much better to me.)&lt;/p&gt;&lt;p&gt;Trinary didnât make any headway in the 20th century; binaryâs direct mapping to the âonâ/âoffâ states of electric current was just too effective, or seductive; but remember that electric current isnât actually âonâ or âoffâ. It has taken a ton of engineering across decades to âsimulateâ those abstract states in real, physical circuits.&lt;/p&gt;&lt;p&gt;Trinary is philosophically appealing because the ground-floor vocabulary isnât âyesâ and ânoâ, but rather: âyesâ, ânoâ, and âmaybeâ. Itâs probably a bit much to imagine that this architectural difference could cascade up through the layers of abstraction and tend to produce software with subtler, richer values … yet I do imagine it.&lt;/p&gt;&lt;p&gt;Trinary might still have its day. You can train a capable and super-efficient language model using weights of only -1, 0, and 1, and I believe many models in the future will use this architecture.&lt;/p&gt;&lt;p&gt;Viva la âmaybeâ!&lt;/p&gt;&lt;p&gt;P.S. I donât say this explicitly in Moonboundâs text, but I lay out a few numeric clues, and here I will confirm, for the curious, that the computer systems of the Anth at their apex were indeed trinary.&lt;/p&gt;To the blog home page&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45635734</guid><pubDate>Sun, 19 Oct 2025 16:57:49 +0000</pubDate></item><item><title>Infisical (YC W23) Is Hiring Full Stack Engineers</title><link>https://www.ycombinator.com/companies/infisical/jobs/0gY2Da1-full-stack-engineer-global</link><description>&lt;doc fingerprint="c11a68a47b7e12a"&gt;
  &lt;main&gt;
    &lt;p&gt;Unified platform for secrets, certs, and privileged access management&lt;/p&gt;
    &lt;p&gt;Infisical is looking to hire exceptional talent to join our teams in building the open source security infrastructure stack for the AI era.&lt;/p&gt;
    &lt;p&gt;We're building a generational company with a world-class engineering team. This isn’t a place to coast — but if you want to grow fast, take ownership, and solve tough problems, you’ll be challenged like nowhere else.&lt;/p&gt;
    &lt;p&gt;What We’re Looking For&lt;/p&gt;
    &lt;p&gt;We’re looking for an exceptional Full Stack Engineer to help us build, optimize, and expand the foundation of the platform.&lt;/p&gt;
    &lt;p&gt;We’ve kept our hiring standards exceptionally high since we expect engineers to tackle a broad range of challenges on a day-to-day basis. Examples of past engineering initiatives include developing strategies for secret rotation and dynamic secrets, a gateway to provide secure access to private resources, protocols like EST and KMIP, integrations for syncing secrets across cloud providers, and entire new product lines such as Infisical PKI and Infisical SSH.&lt;/p&gt;
    &lt;p&gt;You’ll be working closely with our CTO and the rest of the engineering team to:&lt;/p&gt;
    &lt;p&gt;Requirements&lt;/p&gt;
    &lt;p&gt;Bonus&lt;/p&gt;
    &lt;p&gt;How You’ll Grow&lt;/p&gt;
    &lt;p&gt;In this role, you’ll play a pivotal part in shaping Infisical’s future—making key technical decisions, establishing foundational processes, and tackling complex scalability challenges. As you gain experience and the team expands, you'll have the opportunity to take full ownership of specific areas of our platform, driving them end-to-end with autonomy and impact.&lt;/p&gt;
    &lt;p&gt;Overall, you’ll be one of the defining pieces of our team as we scale to thousands of customers over the next 18 months.&lt;/p&gt;
    &lt;p&gt;Team, Values &amp;amp; Benefits&lt;/p&gt;
    &lt;p&gt;Our team brings experience from companies like Figma, AWS, and Red Hat. We operate primarily as a remote team but maintain a strong presence in San Francisco, where we have an office. We also get together in person throughout the year for off-sites, conferences, and team gatherings.&lt;/p&gt;
    &lt;p&gt;At Infisical, we offer competitive compensation, including both salary and equity options. For this role, the salary range depends on location, experience, and seniority. Additional benefits, such as a lunch stipend and a work setup budget, are available with more details to be found on our careers page.&lt;/p&gt;
    &lt;p&gt;About Us&lt;/p&gt;
    &lt;p&gt;Infisical is the open source security infrastructure platform that engineers use for secrets management, internal PKI, key management, and SSH workflow orchestration. We help developers and organizations securely manage over 1.5 billion secrets each month including application configuration, database credentials, certificates, and more.&lt;/p&gt;
    &lt;p&gt;We’ve raised $19M from Y Combinator, Google, and Elad Gil, and our customers include Hugging Face, Lucid, and LG.&lt;/p&gt;
    &lt;p&gt;Join us on a mission to make security easier for all developers — starting with secrets management.&lt;/p&gt;
    &lt;p&gt;Infisical is the #1 open source secret management platform – used by tens of thousands of developers.&lt;/p&gt;
    &lt;p&gt;We raised $3M from Y Combinator, Gradient Ventures (Google's VC fund), and awesome angel investors like Elad Gil, Arash Ferdowsi (founder/ex-CTO of Dropbox), Paul Copplestone (founder/CEO of Supabase), James Hawkins (founder/CEO of PostHog), Andrew Miklas (founder/ex-CTO of PagerDuty), Diana Hu (GP at Y Combinator), and more.&lt;/p&gt;
    &lt;p&gt;We are default alive, and have signed many customers ranging from fastest growing startups to post-IPO enterprises.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45635757</guid><pubDate>Sun, 19 Oct 2025 17:00:42 +0000</pubDate></item><item><title>I wish SSDs gave you CPU performance style metrics about their activity</title><link>https://utcc.utoronto.ca/~cks/space/blog/tech/SSDWritePerfMetricsWish</link><description>&lt;doc fingerprint="36de10ce505722cb"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;I wish SSDs gave you CPU performance style metrics about their activity&lt;/head&gt;
    &lt;p&gt;Modern CPUs have an impressive collection of performance counters for detailed, low level information on things like cache misses, branch mispredictions, various sorts of stalls, and so on; on Linux you can use 'perf list' to see them all. Modern SSDs (NVMe, SATA, and SAS) are all internally quite complex, and their behavior under load depends on a lot of internal state. It would be nice to have CPU performance counter style metrics to expose some of those details. For a relevant example that's on my mind (cf), it certainly would be interesting to know how often flash writes had to stall while blocks were hastily erased, or the current erase rate.&lt;/p&gt;
    &lt;p&gt;Having written this, I checked some of our SSDs (the ones I'm most interested in at the moment) and I see that our SATA SSDs do expose some of this information as (vendor specific) SMART attributes, with things like 'block erase count' and 'NAND GB written' to TLC or SLC (as well as the host write volume and so on stuff you'd expect). NVMe does this in a different way that doesn't have the sort of easy flexibility that SMART attributes do, so a random one of ours that I checked doesn't seem to provide this sort of lower level information.&lt;/p&gt;
    &lt;p&gt;It's understandable that SSD vendors don't necessarily want to expose this sort of information, but it's quite relevant if you're trying to understand unusual drive performance. For example, for your workload do you need to TRIM your drives more often, or do they have enough pre-erased space available when you need it? Since TRIM has an overhead, you may not want to blindly do it on a frequent basis (and its full effects aren't entirely predictable since they depend on how much the drive decides to actually erase in advance).&lt;/p&gt;
    &lt;p&gt;(Having looked at SMART 'block erase count' information on one of our servers, it's definitely doing something when the server is under heavy fsync() load, but I need to cross-compare the numbers from it to other systems in order to get a better sense of what's exceptional and what's not.)&lt;/p&gt;
    &lt;p&gt;I'm currently more focused on write related metrics, but there's probably important information that could be exposed for reads and for other operations. I'd also like it if SSDs provided counters for how many of various sorts of operations they saw, because while your operating system can in theory provide this, it often doesn't (or doesn't provide them at the granularity of, say, how many writes with 'Force Unit Access' or how many 'Flush' operations were done).&lt;/p&gt;
    &lt;p&gt;(In Linux, I think I'd have to extract this low level operation information in an ad-hoc way with eBPF tracing.)&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45635870</guid><pubDate>Sun, 19 Oct 2025 17:13:11 +0000</pubDate></item><item><title>Could the XZ backdoor been detected with better Git/Deb packaging practices?</title><link>https://optimizedbyotto.com/post/xz-backdoor-debian-git-detection/</link><description>&lt;doc fingerprint="ad46ea3988cfcd83"&gt;
  &lt;main&gt;
    &lt;p&gt;The discovery of a backdoor in XZ Utils in the spring of 2024 shocked the open source community, raising critical questions about software supply chain security. This post explores whether better Debian packaging practices could have detected this threat, offering a guide to auditing packages and suggesting future improvements.&lt;/p&gt;
    &lt;p&gt;The XZ backdoor in versions 5.6.0/5.6.1 made its way briefly into many major Linux distributions such as Debian and Fedora, but luckily didn’t reach that many actual users, as the backdoored releases were quickly removed thanks to the heroic diligence of Andres Freund. We are all extremely lucky that he detected a half a second performance regression in SSH, cared enough to trace it down, discovered malicious code in the XZ library loaded by SSH, and reported promtly to various security teams for quick coordinated actions.&lt;/p&gt;
    &lt;p&gt;This episode makes software engineers pondering the following questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why didn’t any Linux distro packagers notice anything odd when importing the new XZ version 5.6.0/5.6.1 from upstream?&lt;/item&gt;
      &lt;item&gt;Is the current software supply-chain in the most popular Linux distros easy to audit?&lt;/item&gt;
      &lt;item&gt;Could we have similar backdoors lurking that haven’t been detected yet?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a Debian Developer, I decided to audit the xz package in Debian, share my methodology and findings in this post, and also suggest some improvements on how the software supply-chain security could be tightened in Debian specifically.&lt;/p&gt;
    &lt;p&gt;Note that the scope here is only to inspect how Debian imports software from its upstreams, and how they are distributed to Debian’s users. This excludes the whole story of how to assess if an upstream project is following software development security best practices. This post doesn’t discuss how to operate an individual computer running Debian to ensure it remains untampered as there are plenty of guides on that already.&lt;/p&gt;
    &lt;head rend="h2"&gt;Downloading Debian and upstream source packages&lt;/head&gt;
    &lt;p&gt;Let’s start by working backwards from what the Debian package repositories offer for download. As auditing binaries is extremely complicated, we skip that, and assume the Debian build hosts are trustworthy and reliably building binaries from the source packages, and the focus should be on auditing the source code packages.&lt;/p&gt;
    &lt;p&gt;As with everything in Debian, there are multiple tools and ways to do the same thing, but in this post only one (and hopefully the best) way to do something is presented for brevity.&lt;/p&gt;
    &lt;p&gt;The first step is to download the latest version and some past versions of the package from the Debian archive, which is easiest done with debsnap. The following command will download all Debian source packages of xz-utils from Debian release 5.2.4-1 onwards:&lt;/p&gt;
    &lt;code&gt;$ debsnap --verbose --first 5.2.4-1 xz-utils
Getting json https://snapshot.debian.org/mr/package/xz-utils/
...
Getting dsc file xz-utils_5.2.4-1.dsc: https://snapshot.debian.org/file/a98271e4291bed8df795ce04d9dc8e4ce959462d
Getting file xz-utils_5.2.4.orig.tar.xz.asc: https://snapshot.debian.org/file/59ccbfb2405abe510999afef4b374cad30c09275
Getting file xz-utils_5.2.4-1.debian.tar.xz: https://snapshot.debian.org/file/667c14fd9409ca54c397b07d2d70140d6297393f
source-xz-utils/xz-utils_5.2.4-1.dsc:
      Good signature found
   validating xz-utils_5.2.4.orig.tar.xz
   validating xz-utils_5.2.4.orig.tar.xz.asc
   validating xz-utils_5.2.4-1.debian.tar.xz
All files validated successfully.&lt;/code&gt;
    &lt;p&gt;Once debsnap completes there will be a subfolder &lt;code&gt;source-&amp;lt;package name&amp;gt;&lt;/code&gt; with the following types of files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;*.orig.tar.xz&lt;/code&gt;: source code from upstream&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;*.orig.tar.xz.asc&lt;/code&gt;: detached signature (if upstream signs their releases)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;*.debian.tar.xz&lt;/code&gt;: Debian packaging source, i.e. the&lt;code&gt;debian/&lt;/code&gt;subdirectory contents&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;*.dsc&lt;/code&gt;: Debian source control file, including signature by Debian Developer/Maintainer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;$ ls -1 source-xz-utils/
...
xz-utils_5.6.4.orig.tar.xz
xz-utils_5.6.4.orig.tar.xz.asc
xz-utils_5.6.4-1.debian.tar.xz
xz-utils_5.6.4-1.dsc
xz-utils_5.8.0.orig.tar.xz
xz-utils_5.8.0.orig.tar.xz.asc
xz-utils_5.8.0-1.debian.tar.xz
xz-utils_5.8.0-1.dsc
xz-utils_5.8.1.orig.tar.xz
xz-utils_5.8.1.orig.tar.xz.asc
xz-utils_5.8.1-1.1.debian.tar.xz
xz-utils_5.8.1-1.1.dsc
xz-utils_5.8.1-1.debian.tar.xz
xz-utils_5.8.1-1.dsc
xz-utils_5.8.1-2.debian.tar.xz
xz-utils_5.8.1-2.dsc&lt;/code&gt;
    &lt;head rend="h2"&gt;Verifying authenticity of upstream and Debian sources using OpenPGP signatures&lt;/head&gt;
    &lt;p&gt;As seen in the output of &lt;code&gt;debsnap&lt;/code&gt;, it already automatically verifies that the downloaded files match the OpenPGP signatures. To have full clarity on what files were authenticated with what keys, we should verify the Debian packagers signature with:&lt;/p&gt;
    &lt;code&gt;$ gpg --verify --auto-key-retrieve --keyserver hkps://keyring.debian.org xz-utils_5.8.1-2.dsc
gpg: Signature made Fri Oct  3 22:04:44 2025 UTC
gpg:                using RSA key 57892E705233051337F6FDD105641F175712FA5B
gpg: requesting key 05641F175712FA5B from hkps://keyring.debian.org
gpg: key 7B96E8162A8CF5D1: public key "Sebastian Andrzej Siewior" imported
gpg: Total number processed: 1
gpg:               imported: 1
gpg: Good signature from "Sebastian Andrzej Siewior" [unknown]
gpg:                 aka "Sebastian Andrzej Siewior &amp;lt;bigeasy@linutronix.de&amp;gt;" [unknown]
gpg:                 aka "Sebastian Andrzej Siewior &amp;lt;sebastian@breakpoint.cc&amp;gt;" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 6425 4695 FFF0 AA44 66CC  19E6 7B96 E816 2A8C F5D1
     Subkey fingerprint: 5789 2E70 5233 0513 37F6  FDD1 0564 1F17 5712 FA5B&lt;/code&gt;
    &lt;p&gt;The upstream tarball signature (if available) can be verified with:&lt;/p&gt;
    &lt;code&gt;$ gpg --verify --auto-key-retrieve xz-utils_5.8.1.orig.tar.xz.asc
gpg: assuming signed data in 'xz-utils_5.8.1.orig.tar.xz'
gpg: Signature made Thu Apr  3 11:38:23 2025 UTC
gpg:                using RSA key 3690C240CE51B4670D30AD1C38EE757D69184620
gpg: key 38EE757D69184620: public key "Lasse Collin &amp;lt;lasse.collin@tukaani.org&amp;gt;" imported
gpg: Total number processed: 1
gpg:               imported: 1
gpg: Good signature from "Lasse Collin &amp;lt;lasse.collin@tukaani.org&amp;gt;" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: 3690 C240 CE51 B467 0D30  AD1C 38EE 757D 6918 4620&lt;/code&gt;
    &lt;p&gt;Note that this only proves that there is a key that created a valid signature for this content. The authenticity of the keys themselves need to be validated separately before trusting they in fact are the keys of these people. That can be done by checking e.g. the upstream website for what key fingerprints they published, or the Debian keyring for Debian Developers and Maintainers, or by relying on the OpenPGP “web-of-trust”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Verifying authenticity of upstream sources by comparing checksums&lt;/head&gt;
    &lt;p&gt;In case the upstream in question does not publish release signatures, the second best way to verify the authenticity of the sources used in Debian is to download the sources directly from upstream and compare that the sha256 checksums match.&lt;/p&gt;
    &lt;p&gt;This should be done using the &lt;code&gt;debian/watch&lt;/code&gt; file inside the Debian packaging, which defines where the upstream source is downloaded from. Continuing on the example situation above, we can unpack the latest Debian sources, enter and then run uscan to download:&lt;/p&gt;
    &lt;code&gt;$ tar xvf xz-utils_5.8.1-2.debian.tar.xz
...
debian/rules
debian/source/format
debian/source.lintian-overrides
debian/symbols
debian/tests/control
debian/tests/testsuite
debian/upstream/signing-key.asc
debian/watch
...

$ uscan --download-current-version --destdir /tmp
Newest version of xz-utils on remote site is 5.8.1, specified download version is 5.8.1
gpgv: Signature made Thu Apr  3 11:38:23 2025 UTC
gpgv:                using RSA key 3690C240CE51B4670D30AD1C38EE757D69184620
gpgv: Good signature from "Lasse Collin &amp;lt;lasse.collin@tukaani.org&amp;gt;"
Successfully symlinked /tmp/xz-5.8.1.tar.xz to /tmp/xz-utils_5.8.1.orig.tar.xz.&lt;/code&gt;
    &lt;p&gt;The original files downloaded from upstream are now in &lt;code&gt;/tmp&lt;/code&gt; along with the files renamed to follow Debian conventions. Using everything downloaded so far the sha256 checksums can be compared across the files and also to what the &lt;code&gt;.dsc&lt;/code&gt; file advertised:&lt;/p&gt;
    &lt;code&gt;$ ls -1 /tmp/
xz-5.8.1.tar.xz
xz-5.8.1.tar.xz.sig
xz-utils_5.8.1.orig.tar.xz
xz-utils_5.8.1.orig.tar.xz.asc

$ sha256sum xz-utils_5.8.1.orig.tar.xz /tmp/xz-5.8.1.tar.xz
0b54f79df85912504de0b14aec7971e3f964491af1812d83447005807513cd9e  xz-utils_5.8.1.orig.tar.xz
0b54f79df85912504de0b14aec7971e3f964491af1812d83447005807513cd9e  /tmp/xz-5.8.1.tar.xz

$ grep -A 3 Sha256 xz-utils_5.8.1-2.dsc
Checksums-Sha256:
 0b54f79df85912504de0b14aec7971e3f964491af1812d83447005807513cd9e 1461872 xz-utils_5.8.1.orig.tar.xz
 4138f4ceca1aa7fd2085fb15a23f6d495d27bca6d3c49c429a8520ea622c27ae 833 xz-utils_5.8.1.orig.tar.xz.asc
 3ed458da17e4023ec45b2c398480ed4fe6a7bfc1d108675ec837b5ca9a4b5ccb 24648 xz-utils_5.8.1-2.debian.tar.xz&lt;/code&gt;
    &lt;p&gt;In the example above the checksum &lt;code&gt;0b54f79df85...&lt;/code&gt; is the same across the files, so it is a match.&lt;/p&gt;
    &lt;head rend="h3"&gt;Repackaged upstream sources can’t be verified as easily&lt;/head&gt;
    &lt;p&gt;Note that &lt;code&gt;uscan&lt;/code&gt; may in rare cases repackage some upstream sources, for example to exclude files that don’t adhere to Debian’s copyright and licensing requirements. Those files and paths would be listed under the &lt;code&gt;Files-Excluded&lt;/code&gt; section in the &lt;code&gt;debian/copyright&lt;/code&gt; file. There are also other situations where the file that represents the upstream sources in Debian isn’t bit-by-bit the same as what upstream published. If checksums don’t match, an experienced Debian Developer should review all package settings (e.g. &lt;code&gt;debian/source/options&lt;/code&gt;) to see if there was a valid and intentional reason for divergence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reviewing changes between two source packages using diffoscope&lt;/head&gt;
    &lt;p&gt;Diffoscope is an incredibly capable and handy tool to compare arbitrary files. For example, to view a report in HTML format of the differences between two XZ releases, run:&lt;/p&gt;
    &lt;code&gt;diffoscope --html-dir xz-utils-5.6.4_vs_5.8.0 xz-utils_5.6.4.orig.tar.xz xz-utils_5.8.0.orig.tar.xz
browse xz-utils-5.6.4_vs_5.8.0/index.html&lt;/code&gt;
    &lt;p&gt;If the changes are extensive, and you want to use a LLM to help spot potential security issues, generate the report of both the upstream and Debian packaging differences in Markdown with:&lt;/p&gt;
    &lt;code&gt;diffoscope --markdown diffoscope-debian.md xz-utils_5.6.4-1.debian.tar.xz xz-utils_5.8.1-2.debian.tar.xz
diffoscope --markdown diffoscope.md xz-utils_5.6.4.orig.tar.xz xz-utils_5.8.0.orig.tar.xz&lt;/code&gt;
    &lt;p&gt;The Markdown files created above can then be passed to your favorite LLM, along with a prompt such as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on the attached diffoscope output for a new Debian package version compared with the previous one, list all suspicious changes that might have introduced a backdoor, followed by other potential security issues. If there are none, list a short summary of changes as the conclusion.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Reviewing Debian source packages in version control&lt;/head&gt;
    &lt;p&gt;As of today only 93% of all Debian source packages are tracked in git on Debian’s GitLab instance at salsa.debian.org. Some key packages such as Coreutils and Bash are not using version control at all, as their maintainers apparently don’t see value in using git for Debian packaging, and the Debian Policy does not require it. Thus, the only reliable and consistent way to audit changes in Debian packages is to compare the full versions from the archive as shown above.&lt;/p&gt;
    &lt;p&gt;However, for packages that are hosted on Salsa, one can view the git history to gain additional insight into what exactly changed, when and why. For packages that are using version control, their location can be found in the &lt;code&gt;Git-Vcs&lt;/code&gt; header in the &lt;code&gt;debian/control&lt;/code&gt; file. For xz-utils the location is salsa.debian.org/debian/xz-utils.&lt;/p&gt;
    &lt;p&gt;Note that the Debian policy does not state anything about how Salsa should be used, or what git repository layout or development practices to follow. In practice most packages follow the DEP-14 proposal, and use git-buildpackage as the tool for managing changes and pushing and pulling them between upstream and salsa.debian.org.&lt;/p&gt;
    &lt;p&gt;To get the XZ Utils source, run:&lt;/p&gt;
    &lt;code&gt;$ gbp clone https://salsa.debian.org/debian/xz-utils.git
gbp:info: Cloning from 'https://salsa.debian.org/debian/xz-utils.git'&lt;/code&gt;
    &lt;p&gt;At the time of writing this post the git history shows:&lt;/p&gt;
    &lt;code&gt;$ git log --graph --oneline
* bb787585 (HEAD -&amp;gt; debian/unstable, origin/debian/unstable, origin/HEAD) Prepare 5.8.1-2
* 4b769547 d: Remove the symlinks from -dev package.
* a39f3428 Correct the nocheck build profile
* 1b806b8d Import Debian changes 5.8.1-1.1
* b1cad34b Prepare 5.8.1-1
* a8646015 Import 5.8.1
*   2808ec2d Update upstream source from tag 'upstream/5.8.1'
|\
| * fa1e8796 (origin/upstream/v5.8, upstream/v5.8) New upstream version 5.8.1
| * a522a226 Bump version and soname for 5.8.1
| * 1c462c2a Add NEWS for 5.8.1
| * 513cabcf Tests: Call lzma_code() in smaller chunks in fuzz_common.h
| * 48440e24 Tests: Add a fuzzing target for the multithreaded .xz decoder
| * 0c80045a liblzma: mt dec: Fix lack of parallelization in single-shot decoding
| * 81880488 liblzma: mt dec: Don't modify thr-&amp;gt;in_size in the worker thread
| * d5a2ffe4 liblzma: mt dec: Don't free the input buffer too early (CVE-2025-31115)
| * c0c83596 liblzma: mt dec: Simplify by removing the THR_STOP state
| * 831b55b9 liblzma: mt dec: Fix a comment
| * b9d168ee liblzma: Add assertions to lzma_bufcpy()
| * c8e0a489 DOS: Update Makefile to fix the build
| * 307c02ed sysdefs.h: Avoid &amp;lt;stdalign.h&amp;gt; even with C11 compilers
| * 7ce38b31 Update THANKS
| * 688e51bd Translations: Update the Croatian translation
* | a6b54dde Prepare 5.8.0-1.
* | 77d9470f Add 5.8 symbols.
* | 9268eb66 Import 5.8.0
* |   6f85ef4f Update upstream source from tag 'upstream/5.8.0'
|\ \
| * | afba662b New upstream version 5.8.0
| |/
| * 173fb5c6 doc/SHA256SUMS: Add 5.8.0
| * db9258e8 Bump version and soname for 5.8.0
| * bfb752a3 Add NEWS for 5.8.0
| * 6ccbb904 Translations: Run "make -C po update-po"
| * 891a5f05 Translations: Run po4a/update-po
| * 4f52e738 Translations: Partially fix overtranslation in Serbian man pages
| * ff5d9447 liblzma: Count the extra bytes in LZMA/LZMA2 decoder memory usage
| * 943b012d liblzma: Use SSE2 intrinsics instead of memcpy() in dict_repeat()&lt;/code&gt;
    &lt;p&gt;This shows both the changes on the &lt;code&gt;debian/unstable&lt;/code&gt; branch as well as the intermediate upstream import branch, and the actual real upstream development branch. See my Debian source packages in git explainer for details of what these branches are used for.&lt;/p&gt;
    &lt;p&gt;To only view changes on the Debian branch, run &lt;code&gt;git log --graph --oneline --first-parent&lt;/code&gt; or &lt;code&gt;git log --graph --oneline -- debian&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The Debian branch should only have changes inside the &lt;code&gt;debian/&lt;/code&gt; subdirectory, which is easy to check with:&lt;/p&gt;
    &lt;code&gt;$ git diff --stat upstream/v5.8
 debian/README.source             |  16 +++
 debian/autogen.sh                |  32 +++++
 debian/changelog                 | 949 ++++++++++++++++++++++++++
 ...
 debian/upstream/signing-key.asc  |  52 +++++++++
 debian/watch                     |   4 +
 debian/xz-utils.README.Debian    |  47 ++++++++
 debian/xz-utils.docs             |   6 +
 debian/xz-utils.install          |  28 +++++
 debian/xz-utils.postinst         |  19 +++
 debian/xz-utils.prerm            |  10 ++
 debian/xzdec.docs                |   6 +
 debian/xzdec.install             |   4 +
 33 files changed, 2014 insertions(+)&lt;/code&gt;
    &lt;p&gt;All the files outside the &lt;code&gt;debian/&lt;/code&gt; directory originate from upstream, and for example running &lt;code&gt;git blame&lt;/code&gt; on them should show only upstream commits:&lt;/p&gt;
    &lt;code&gt;$ git blame CMakeLists.txt
22af94128 (Lasse Collin 2024-02-12 17:09:10 +0200  1) # SPDX-License-Identifier: 0BSD
22af94128 (Lasse Collin 2024-02-12 17:09:10 +0200  2)
7e3493d40 (Lasse Collin 2020-02-24 23:38:16 +0200  3) ###############
7e3493d40 (Lasse Collin 2020-02-24 23:38:16 +0200  4) #
426bdc709 (Lasse Collin 2024-02-17 21:45:07 +0200  5) # CMake support for building XZ Utils&lt;/code&gt;
    &lt;p&gt;If the upstream in question signs commits or tags, they can be verified with e.g.:&lt;/p&gt;
    &lt;code&gt;$ git verify-tag v5.6.2
gpg: Signature made Wed 29 May 2024 09:39:42 AM PDT
gpg:                using RSA key 3690C240CE51B4670D30AD1C38EE757D69184620
gpg:                issuer "lasse.collin@tukaani.org"
gpg: Good signature from "Lasse Collin &amp;lt;lasse.collin@tukaani.org&amp;gt;" [expired]
gpg: Note: This key has expired!&lt;/code&gt;
    &lt;p&gt;The main benefit of reviewing changes in git is the ability to see detailed information about each individual change, instead of just staring at a massive list of changes without any explanations. In this example, to view all the upstream commits since the previous import to Debian, one would view the commit range from afba662b New upstream version 5.8.0 to fa1e8796 New upstream version 5.8.1 with &lt;code&gt;git log --reverse -p afba662b...fa1e8796&lt;/code&gt;. However, a far superior way to review changes would be to browse this range using a visual git history viewer, such as gitk. Either way, looking at one code change at a time and reading the git commit message makes the review much easier.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing Debian source packages to git contents&lt;/head&gt;
    &lt;p&gt;As stated in the beginning of the previous section, and worth repeating, there is no guarantee that the contents in the Debian packaging git repository matches what was actually uploaded to Debian. While the tag2upload project in Debian is getting more and more popular, Debian is still far from having any system to enforce that the git repository would be in sync with the Debian archive contents.&lt;/p&gt;
    &lt;p&gt;To detect such differences we can run diff across the Debian source packages downloaded with debsnap earlier (path &lt;code&gt;source-xz-utils/xz-utils_5.8.1-2.debian&lt;/code&gt;) and the git repository cloned in the previous section (path &lt;code&gt;xz-utils&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;$ diff -u source-xz-utils/xz-utils_5.8.1-2.debian/ xz-utils/debian/
diff -u source-xz-utils/xz-utils_5.8.1-2.debian/changelog xz-utils/debian/changelog
--- debsnap/source-xz-utils/xz-utils_5.8.1-2.debian/changelog	2025-10-03 09:32:16.000000000 -0700
+++ xz-utils/debian/changelog	2025-10-12 12:18:04.623054758 -0700
@@ -5,7 +5,7 @@
   * Remove the symlinks from -dev, pointing to the lib package.
     (Closes: #1109354)

- -- Sebastian Andrzej Siewior &amp;lt;sebastian@breakpoint.cc&amp;gt;  Fri, 03 Oct 2025 18:32:16 +0200
+ -- Sebastian Andrzej Siewior &amp;lt;sebastian@breakpoint.cc&amp;gt;  Fri, 03 Oct 2025 18:36:59 +0200
&lt;/code&gt;
    &lt;p&gt;In the case above &lt;code&gt;diff&lt;/code&gt; revealed that the timestamp in the changelog in the version uploaded to Debian is different from what was committed to git. This is not malicious, just a mistake by the maintainer who probably didn’t run &lt;code&gt;gbp tag&lt;/code&gt; immediately after upload, but instead some &lt;code&gt;dch&lt;/code&gt; command and ended up with having a different timestamps in the git compared to what was actually uploaded to Debian.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating syntetic Debian packaging git repositories&lt;/head&gt;
    &lt;p&gt;If no Debian packaging git repository exists, or if it is lagging behind what was uploaded to Debian’s archive, one can use git-buildpackage’s import-dscs feature to create synthetic git commits based on the files downloaded by debsnap, ensuring the git contents fully matches what was uploaded to the archive. To import a single version there is gbp import-dsc (no ’s’ at the end), of which an example invocation would be:&lt;/p&gt;
    &lt;code&gt;$ gbp import-dsc --verbose ../source-xz-utils/xz-utils_5.8.1-2.dsc
Version '5.8.1-2' imported under '/home/otto/debian/xz-utils-2025-09-29'&lt;/code&gt;
    &lt;p&gt;Example commit history from a repository with commits added with &lt;code&gt;gbp import-dsc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;$ git log --graph --oneline
* 86aed07b (HEAD -&amp;gt; debian/unstable, tag: debian/5.8.1-2, origin/debian/unstable) Import Debian changes 5.8.1-2
* f111d93b (tag: debian/5.8.1-1.1) Import Debian changes 5.8.1-1.1
*   1106e19b (tag: debian/5.8.1-1) Import Debian changes 5.8.1-1
|\
| *   08edbe38 (tag: upstream/5.8.1, origin/upstream/v5.8, upstream/v5.8) Import Upstream version 5.8.1
| |\
| | * a522a226 (tag: v5.8.1) Bump version and soname for 5.8.1
| | * 1c462c2a Add NEWS for 5.8.1
| | * 513cabcf Tests: Call lzma_code() in smaller chunks in fuzz_common.h&lt;/code&gt;
    &lt;p&gt;An online example repository with only a few missing uploads added using &lt;code&gt;gbp import-dsc&lt;/code&gt; can be viewed at salsa.debian.org/otto/xz-utils-2025-09-29/-/network/debian%2Funstable&lt;/p&gt;
    &lt;p&gt;An example repository that was fully crafted using &lt;code&gt;gbp import-dscs&lt;/code&gt; can be viewed at salsa.debian.org/otto/xz-utils-gbp-import-dscs-debsnap-generated/-/network/debian%2Flatest.&lt;/p&gt;
    &lt;p&gt;There exists also dgit, which in a similar way creates a synthetic git history to allow viewing the Debian archive contents via git tools. However, its focus is on producing new package versions, so fetching a package with dgit that has not had the history recorded in dgit earlier will only show the latest versions:&lt;/p&gt;
    &lt;code&gt;$ dgit clone xz-utils
canonical suite name for unstable is sid
starting new git history
last upload to archive: NO git hash
downloading http://ftp.debian.org/debian//pool/main/x/xz-utils/xz-utils_5.8.1.orig.tar.xz...
downloading http://ftp.debian.org/debian//pool/main/x/xz-utils/xz-utils_5.8.1.orig.tar.xz.asc...
downloading http://ftp.debian.org/debian//pool/main/x/xz-utils/xz-utils_5.8.1-2.debian.tar.xz...
dpkg-source: info: extracting xz-utils in unpacked
dpkg-source: info: unpacking xz-utils_5.8.1.orig.tar.xz
dpkg-source: info: unpacking xz-utils_5.8.1-2.debian.tar.xz
synthesised git commit from .dsc 5.8.1-2
HEAD is now at f9bcaf7 xz-utils (5.8.1-2) unstable; urgency=medium
dgit ok: ready for work in xz-utils

$ dgit/sid ± git log --graph --oneline
*   f9bcaf7 xz-utils (5.8.1-2) unstable; urgency=medium 9 days ago (HEAD -&amp;gt; dgit/sid, dgit/dgit/sid)
|\
| * 11d3a62 Import xz-utils_5.8.1-2.debian.tar.xz 9 days ago
* 15dcd95 Import xz-utils_5.8.1.orig.tar.xz 6 months ago&lt;/code&gt;
    &lt;p&gt;Unlike git-buildpackage managed git repositories, the dgit managed repositories cannot incorporate the upstream git history and are thus less useful for auditing the full software supply-chain in git.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing upstream source packages to git contents&lt;/head&gt;
    &lt;p&gt;Equally important to the note in the beginning of the previous section, one must also keep in mind that the upstream release source packages, often called release tarballs, are not guaranteed to have the exact same contents as the upstream git repository. Projects might strip out test data or extra development files from their release tarballs to avoid shipping unnecessary files to users, or projects might add documentation files or versioning information into the tarball that isn’t stored in git. While a small minority, there are also upstreams that don’t use git at all, so the plain files in a release tarball is still the lowest common denominator for all open source software projects, and exporting and importing source code needs to interface with it.&lt;/p&gt;
    &lt;p&gt;In the case of XZ, the release tarball has additional version info and also a sizeable amount of pregenerated compiler configuration files. Detecting and comparing differences between git contents and tarballs can of course be done manually by running diff across an unpacked tarball and a checked out git repository. If using git-buildpackage, the difference between the git contents and tarball contents can be made visible directly in the import commit.&lt;/p&gt;
    &lt;p&gt;In this XZ example, consider this git history:&lt;/p&gt;
    &lt;code&gt;* b1cad34b Prepare 5.8.1-1
* a8646015 Import 5.8.1
*   2808ec2d Update upstream source from tag 'upstream/5.8.1'
|\
| * fa1e8796 (debian/upstream/v5.8, upstream/v5.8) New upstream version 5.8.1
| * a522a226 (tag: v5.8.1) Bump version and soname for 5.8.1
| * 1c462c2a Add NEWS for 5.8.1&lt;/code&gt;
    &lt;p&gt;The commit a522a226 was the upstream release commit, which upstream also tagged v5.8.1. The merge commit 2808ec2d applied the new upstream import branch contents on the Debian branch. Between these is the special commit fa1e8796 New upstream version 5.8.1 tagged upstream/v5.8. This commit and tag exists only in the Debian packaging repository, and they show what is the contents imported into Debian. This is generated automatically by git-buildpackage when running &lt;code&gt;git import-orig --uscan&lt;/code&gt; for Debian packages with the correct settings in &lt;code&gt;debian/gbp.conf&lt;/code&gt;. By viewing this commit one can see exactly how the upstream release tarball differs from the upstream git contents (if at all).&lt;/p&gt;
    &lt;p&gt;In the case of XZ, the difference is substantial, and shown below in full as it is very interesting:&lt;/p&gt;
    &lt;code&gt;$ git show --stat fa1e8796
commit fa1e8796dabd91a0f667b9e90f9841825225413a
       (debian/upstream/v5.8, upstream/v5.8)
Author: Sebastian Andrzej Siewior &amp;lt;sebastian@breakpoint.cc&amp;gt;
Date:   Thu Apr 3 22:58:39 2025 +0200

    New upstream version 5.8.1

 .codespellrc                     |    30 -
 .gitattributes                   |     8 -
 .github/workflows/ci.yml         |   163 -
 .github/workflows/freebsd.yml    |    32 -
 .github/workflows/netbsd.yml     |    32 -
 .github/workflows/openbsd.yml    |    35 -
 .github/workflows/solaris.yml    |    32 -
 .github/workflows/windows-ci.yml |   124 -
 .gitignore                       |   113 -
 ABOUT-NLS                        |     1 +
 ChangeLog                        | 17392 +++++++++++++++++++++
 Makefile.in                      |  1097 +++++++
 aclocal.m4                       |  1353 ++++++++
 build-aux/ci_build.bash          |   286 --
 build-aux/compile                |   351 ++
 build-aux/config.guess           |  1815 ++++++++++
 build-aux/config.rpath           |   751 +++++
 build-aux/config.sub             |  2354 +++++++++++++
 build-aux/depcomp                |   792 +++++
 build-aux/install-sh             |   541 +++
 build-aux/ltmain.sh              | 11524 ++++++++++++++++++++++
 build-aux/missing                |   236 ++
 build-aux/test-driver            |   160 +
 config.h.in                      |   634 ++++
 configure                        | 26434 ++++++++++++++++++++++
 debug/Makefile.in                |   756 +++++
 doc/SHA256SUMS                   |   236 --
 doc/man/txt/lzmainfo.txt         |    36 +
 doc/man/txt/xz.txt               |  1708 ++++++++++
 doc/man/txt/xzdec.txt            |    76 +
 doc/man/txt/xzdiff.txt           |    39 +
 doc/man/txt/xzgrep.txt           |    70 +
 doc/man/txt/xzless.txt           |    36 +
 doc/man/txt/xzmore.txt           |    31 +
 lib/Makefile.in                  |   623 ++++
 m4/.gitignore                    |    40 -
 m4/build-to-host.m4              |   274 ++
 m4/gettext.m4                    |   392 +++
 m4/host-cpu-c-abi.m4             |   529 +++
 m4/iconv.m4                      |   324 ++
 m4/intlmacosx.m4                 |    71 +
 m4/lib-ld.m4                     |   170 +
 m4/lib-link.m4                   |   815 +++++
 m4/lib-prefix.m4                 |   334 ++
 m4/libtool.m4                    |  8488 +++++++++++++++++++++
 m4/ltoptions.m4                  |   467 +++
 m4/ltsugar.m4                    |   124 +
 m4/ltversion.m4                  |    24 +
 m4/lt~obsolete.m4                |    99 +
 m4/nls.m4                        |    33 +
 m4/po.m4                         |   456 +++
 m4/progtest.m4                   |    92 +
 po/.gitignore                    |    31 -
 po/Makefile.in.in                |   517 +++
 po/Rules-quot                    |    66 +
 po/boldquot.sed                  |    21 +
 po/ca.gmo                        |   Bin 0 -&amp;gt; 15587 bytes
 po/cs.gmo                        |   Bin 0 -&amp;gt; 7983 bytes
 po/da.gmo                        |   Bin 0 -&amp;gt; 9040 bytes
 po/de.gmo                        |   Bin 0 -&amp;gt; 29882 bytes
 po/en@boldquot.header            |    35 +
 po/en@quot.header                |    32 +
 po/eo.gmo                        |   Bin 0 -&amp;gt; 15060 bytes
 po/es.gmo                        |   Bin 0 -&amp;gt; 29228 bytes
 po/fi.gmo                        |   Bin 0 -&amp;gt; 28225 bytes
 po/fr.gmo                        |   Bin 0 -&amp;gt; 10232 bytes&lt;/code&gt;
    &lt;p&gt;To be able to easily inspect exactly what changed in the release tarball compared to git release tag contents, the best tool for the job is Meld, invoked via &lt;code&gt;git difftool --dir-diff fa1e8796^..fa1e8796&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To compare changes across the new and old upstream tarball, one would need to compare commits afba662b New upstream version 5.8.0 and fa1e8796 New upstream version 5.8.1 by running &lt;code&gt;git difftool --dir-diff afba662b..fa1e8796&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;With all the above tips you can now go and try to audit your own favorite package in Debian and see if it is identical with upstream, and if not, how it differs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Should the XZ backdoor have been detected using these tools?&lt;/head&gt;
    &lt;p&gt;The famous XZ Utils backdoor (CVE-2024-3094) consisted of two parts: the actual backdoor inside two binary blobs masqueraded as a test files (&lt;code&gt;tests/files/bad-3-corrupt_lzma2.xz&lt;/code&gt;, &lt;code&gt;tests/files/good-large_compressed.lzma&lt;/code&gt;), and a small modification in the build scripts (&lt;code&gt;m4/build-to-host.m4&lt;/code&gt;) to extract the backdoor and plant it into the built binary. The build script was not tracked in version control, but generated with GNU Autotools at release time and only shipped as additional files in the release tarball.&lt;/p&gt;
    &lt;p&gt;The entire reason for me to write this post was to ponder if a diligent engineer using git-buildpackage best practices could have reasonably spotted this while importing the new upstream release into Debian. The short answer is “no”. The malicious actor here clearly anticipated all the typical ways anyone might inspect both git commits, and release tarball contents, and masqueraded the changes very well and over a long timespan.&lt;/p&gt;
    &lt;p&gt;First of all, XZ has for legitimate reasons for several carefully crafted &lt;code&gt;.xz&lt;/code&gt; files as test data to help catch regressions in the decompression code path. The test files are shipped in the release so users can run the test suite and validate that the binary is built correctly and xz works properly. Debian famously runs massive amounts of testing in its CI and autopkgtest system across tens of thousands of packages to uphold high quality despite frequent upgrades of the build toolchain and while supporting more CPU architectures than any other distro. Test data is useful and should stay.&lt;/p&gt;
    &lt;p&gt;When git-buildpackage is used correctly, the upstream commits are visible in the Debian packaging for easy review, but the commit cf44e4b that introduced the test files does not deviate enough from regular sloppy coding practices to really stand out. It is unfortunately very common for git commit to lack a message body explaining why the change was done, and to not be properly atomic with test code and test data together in the same commit, and for commits to be pushed directly to mainline without using code reviews (the commit was not part of any PR in this case). Only another upstream developer could have spotted that this change is not on par to what the project expects, and that the test code was never added, only test data, and thus that this commit was not just a sloppy one but potentially malicious.&lt;/p&gt;
    &lt;p&gt;Secondly, the fact that a new Autotools file appeared (&lt;code&gt;m4/build-to-host.m4&lt;/code&gt;) in the XZ Utils 5.6.0 is not suspicious. This is perfectly normal for Autotools. In fact, starting from XZ Utils version 5.8.1 it is now shipping a &lt;code&gt;m4/build-to-host.m4&lt;/code&gt; file that it actually uses now.&lt;/p&gt;
    &lt;p&gt;Spotting that there is anything fishy is practically impossible by simply reading the code, as Autotools files are full custom m4 syntax interwoven with shell script, and there are plenty of backticks (&lt;code&gt;`&lt;/code&gt;) that spawn subshells and &lt;code&gt;evals&lt;/code&gt; that execute variable contents further, which is just normal for Autotools. Russ Cox’s XZ post explains how exactly the Autotools code fetched the actual backdoor from the test files and injected it into the build.&lt;/p&gt;
    &lt;p&gt;There is only one tiny thing that maybe a very experienced Autotools user could potentially have noticed: the &lt;code&gt;serial 30&lt;/code&gt; in the version header is way too high. In theory one could also have noticed this Autotools file deviates from what other packages in Debian ship with the same filename, such as e.g. the serial 3, serial 5a or 5b versions. That would however require and an insane amount extra checking work, and is not something we should plan to start doing. A much simpler solution would be to simply strongly recommend all open source projects to stop using Autotools to eventually get rid of it entirely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Not detectable with reasonable effort&lt;/head&gt;
    &lt;p&gt;While planting backdoors is evil, it is hard not to feel some respect to the level of skill and dedication of the people behind this. I’ve been involved in a bunch of security breach investigations during my IT career, and never have I seen anything this well executed.&lt;/p&gt;
    &lt;p&gt;If it hadn’t slowed down SSH by ~500 milliseconds and been discovered due to that, it would most likely have stayed undetected for months or years. Hiding backdoors in closed source software is relatively trivial, but hiding backdoors in plain sight in a popular open source project requires some unusual amount of expertise and creativity as shown above.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is the software supply-chain in Debian easy to audit?&lt;/head&gt;
    &lt;p&gt;While maintaining a Debian package source using git-buildpackage can make the package history a lot easier to inspect, most packages have incomplete configurations in their &lt;code&gt;debian/gbp.conf&lt;/code&gt;, and thus their package development histories are not always correctly constructed or uniform and easy to compare. The Debian Policy does not mandate git usage at all, and there are many important packages that are not using git at all. Additionally the Debian Policy also allows for non-maintainers to upload new versions to Debian without committing anything in git even for packages where the original maintainer wanted to use git. Uploads that “bypass git” unfortunately happen surpisingly often.&lt;/p&gt;
    &lt;p&gt;Because of the situation, I am afraid that we could have multiple similar backdoors lurking that simply haven’t been detected yet. More audits, that hopefully also get published openly, would be welcome! More people auditing the contents of the Debian archives would probably also help surface what tools and policies Debian might be missing to make the work easier, and thus help improve the security of Debian’s users, and improve trust in Debian.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is Debian currently missing some software that could help detect similar things?&lt;/head&gt;
    &lt;p&gt;To my knowledge there is currently no system in place as part of Debian’s QA or security infrastructure to verify that the upstream source packages in Debian are actually from upstream. I’ve come across a lot of packages where the &lt;code&gt;debian/watch&lt;/code&gt; or other configs are incorrect and even cases where maintainers have manually created upstream tarballs as it was easier than configuring automation to work. It is obvious that for those packages the source tarball now in Debian is not at all the same as upstream. I am not aware of any malicious cases though (if I was, I would report them of course).&lt;/p&gt;
    &lt;p&gt;I am also aware of packages in the Debian repository that are misconfigured to be of type &lt;code&gt;1.0 (native)&lt;/code&gt; packages, mixing the upstream files and debian/ contents and having patches applied, while they actually should be configured as &lt;code&gt;3.0 (quilt)&lt;/code&gt;, and not hide what is the true upstream sources. Debian should extend the QA tools to scan for such things. If I find a sponsor, I might build it myself as my next major contribution to Debian.&lt;/p&gt;
    &lt;p&gt;In addition to better tooling for finding mismatches in the source code, Debian could also have better tooling for tracking in built binaries what their source files were, but solutions like Fraunhofer-AISEC’s supply-graph or Sony’s ESSTRA are not practical yet. Julien Malka’s post about NixOS discusses the role of reproducible builds, which may help in some cases across all distros.&lt;/p&gt;
    &lt;head rend="h2"&gt;Or, is Debian missing some policies or practices to mitigate this?&lt;/head&gt;
    &lt;p&gt;Perhaps more importantly than more security scanning, the Debian Developer community should switch the general mindset from “anyone is free to do anything” to valuing having more shared workflows. The ability to audit anything is severely hampered by the fact that there are so many ways to do the same thing, and distinguishing what is a “normal” deviation from a malicious deviation is too hard, as the “normal” can basically be almost anything.&lt;/p&gt;
    &lt;p&gt;Also, as there is no documented and recommended “default” workflow, both those who are old and new to Debian packaging might never learn any one optimal workflow, and end up doing many steps in the packaging process in a way that kind of works, but is actually wrong or unnecessary, causing process deviations that look malicious, but turn out to just be a result of not fully understanding what would have been the right way to do something.&lt;/p&gt;
    &lt;p&gt;In the long run, once individual developers’ workflows are more aligned, doing code reviews will become a lot easier and smoother as the excess noise of workflow differences diminishes and reviews will feel much more productive to all participants. Debian fostering a culture of code reviews would allow us to slowly move from the current practice of mainly solo packaging work towards true collaboration forming around those code reviews.&lt;/p&gt;
    &lt;p&gt;I have been promoting increased use of Merge Requests in Debian already for some time, for example by proposing DEP-18: Encourage Continuous Integration and Merge Request based Collaboration for Debian packages. If you are involved in Debian development, please give a thumbs up in dep-team/deps!21 if you want me to continue promoting it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Can we trust open source software?&lt;/head&gt;
    &lt;p&gt;Yes — and I would argue that we can only trust open source software. There is no way to audit closed source software, and anyone using e.g. Windows or MacOS just have to trust the vendor’s word when they say they have no intentional or accidental backdoors in their software. Or, when the news gets out that the systems of a closed source vendor was compromised, like Crowdstrike some weeks ago, we can’t audit anything, and time after time we simply need to take their word when they say they have properly cleaned up their code base.&lt;/p&gt;
    &lt;p&gt;In theory, a vendor could give some kind of contractual or financial guarantee to its customer that there are no preventable security issues, but in practice that never happens. I am not aware of a single case of e.g. Microsoft or Oracle would have paid damages to their customers after a security flaw was found in their software. In theory you could also pay a vendor more to have them focus more effort in security, but since there is no way to verify what they did, or to get compensation when they didn’t, any increased fees are likely just pocketed as increased profit.&lt;/p&gt;
    &lt;p&gt;Open source is clearly better overall. You can, if you are an individual with the time and skills, audit every step in the supply-chain, or you could as an organization make investments in open source security improvements and actually verify what changes were made and how security improved.&lt;/p&gt;
    &lt;p&gt;If your organisation is using Debian (or derivatives, such as Ubuntu) and you are interested in sponsoring my work to improve Debian, please reach out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45636116</guid><pubDate>Sun, 19 Oct 2025 17:38:11 +0000</pubDate></item><item><title>Airliner hit by possible space debris</title><link>https://avbrief.com/united-max-hit-by-falling-object-at-36000-feet/</link><description>&lt;doc fingerprint="f8c8cb57479ac763"&gt;
  &lt;main&gt;
    &lt;p&gt;Authorities are now considering whether a falling object, possibly from space, caused damage to the windshield and frame on a United 737 MAX over Colorado on Thursday. Various reports that include watermarked photos of the damage suggest the plane was struck by a falling object not long after taking off from Denver for Los Angeles. One of the photos shows a pilot’s arm peppered with small cuts and scratches. In his remarks after the incident, the captain reportedly described the object that hit the plane as “space debris,” which would suggest it was from a rocket or satellite or some other human-made object. Some reports say it was possibly a meteorite.&lt;/p&gt;
    &lt;p&gt;Whatever hit the plane, it was an enormously rare event and likely the first time it’s ever happened. The plane diverted without incident to Salt Lake City where the approximately 130 passengers were put on another plane to finish the last half of the 90-minute flight. Apparently only one layer of the windshield was damaged, and there was no depressurization. The crew descended from 36,000 feet to 26,000 feet for the diversion, likely to ease the pressure differential on the remaining layers of windshield. Neither the airline nor FAA have commented.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45636285</guid><pubDate>Sun, 19 Oct 2025 17:54:21 +0000</pubDate></item><item><title>Compare Single Board Computers</title><link>https://sbc.compare/</link><description>&lt;doc fingerprint="468f7cd727be4f3a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compare Single Board Computers&lt;/head&gt;
    &lt;p&gt;Find the perfect SBC for your project with comprehensive benchmarks, specifications, and real-world performance data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Search SBCs to Compare&lt;/head&gt;
    &lt;head rend="h2"&gt;Popular Comparisons&lt;/head&gt;
    &lt;head rend="h2"&gt;Quick Start Guide&lt;/head&gt;
    &lt;head rend="h3"&gt;Search&lt;/head&gt;
    &lt;p&gt;Search for single board computers by name, manufacturer, or specifications.&lt;/p&gt;
    &lt;head rend="h3"&gt;Select&lt;/head&gt;
    &lt;p&gt;Add up to 3 boards to your comparison list by clicking on them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compare&lt;/head&gt;
    &lt;p&gt;View detailed comparisons with benchmarks, specifications, and performance data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45636365</guid><pubDate>Sun, 19 Oct 2025 18:02:00 +0000</pubDate></item><item><title>US Government Uptime Monitor</title><link>https://usa-status.com/</link><description>&lt;doc fingerprint="73ee6184cf780a96"&gt;
  &lt;main&gt;
    &lt;p&gt;Federal Government Uptime Monitor*&lt;/p&gt;
    &lt;p&gt;down for 18d 15h 24m 27s&lt;/p&gt;
    &lt;p&gt;98.120520501%&lt;/p&gt;
    &lt;p&gt;Uptime over the last 15 years&lt;/p&gt;
    &lt;p&gt;* Not an official service of the United States of America. Your tax dollars at rest.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45637049</guid><pubDate>Sun, 19 Oct 2025 19:16:59 +0000</pubDate></item><item><title>Dosbian: Boot to DOSBox on Raspberry Pi</title><link>https://cmaiolino.wordpress.com/dosbian/</link><description>&lt;doc fingerprint="510529795c96c4c7"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;09/01/2025 released DOSBIAN 3.0 for Raspberry Pi 3/4/400/5/500&lt;/head&gt;
    &lt;p&gt;WHAT’S NEW IN VERSION 3.0&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Latest distro updates applied to run in Raspberry Pi 5/500.&lt;/item&gt;
      &lt;item&gt;Dosbox Staging updated to version 0.82, now with support for MMX instructions (Please see official sites for all the changements).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Incredible performances expecially with Raspberry Pi 5/500, Dosbian V3.0 guarantees you an incredible DOS experience.&lt;/p&gt;
    &lt;p&gt;Rewritten from scratch starting from the new Bookworm OS for Raspberry Pi, Dosbian is the first distro totally dedicated to the DOS world. It boots straight to Dosbox, from there, you can install whatever you want and building your retro-pc 🙂&lt;lb/&gt;Whether you love DOS games or you’re just fond of all the DOS retro software, this is the distro for you.&lt;lb/&gt;Just switch on your Raspberry Pi and in few seconds your Dos prompt will be ready to use. No configuration needed, just an old school command like based machine to enjoy!&lt;/p&gt;
    &lt;p&gt;What you can do with your Dosbian distro:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run all retro Pc Sofware (DOS / Win 3.1 / Win 95 / Win98)&lt;/item&gt;
      &lt;item&gt;Run most of 90’s retro games&lt;/item&gt;
      &lt;item&gt;Run games from LaunchBox frontend&lt;/item&gt;
      &lt;item&gt;Run ScummVM Games&lt;/item&gt;
      &lt;item&gt;Create empty floppy of size: 320KB, 720KB, 1,44MB&lt;/item&gt;
      &lt;item&gt;Create empty HDDs of size: 256MB, 512MB, 1GB, 2GB&lt;/item&gt;
      &lt;item&gt;Mount Floppy disk, CD-ROM or HDD using a GUI driven utility&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;PLEASE NOTE&lt;/head&gt;
    &lt;head rend="h3"&gt;Dosbian doesn’t contains any copyrighted material.&lt;lb/&gt;It’s up to you to install games/software or the operating system. &lt;lb/&gt;I knew someone on the web is selling my distro with OS pre-installed (that’s illegal). I’m not involved in to this, so please, if you want a genuine free Dosbian image, download the distro only from my blog.&lt;/head&gt;
    &lt;head rend="h3"&gt;The images below are just examples on what you can run on Dosbian, but nothing is included inside the distribution.&lt;/head&gt;
    &lt;p&gt;Example games running on Dosbian&lt;/p&gt;
    &lt;p&gt;Some software Dosbian is able to run&lt;/p&gt;
    &lt;head rend="h2"&gt;Terms of use and distribution&lt;/head&gt;
    &lt;p&gt;Dosbian is a donationware project, this means you can modify, improve, customise it as you like for your own use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dosbian Facebook group&lt;/head&gt;
    &lt;p&gt;Join the official Facebook group, a place where you can meet other friends and discuss about games, configurations, issues, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download&lt;/head&gt;
    &lt;p&gt;Please note: The distro doesn’t contain any copyrighted material.&lt;lb/&gt;Dosbian is compatible with the following Raspberry Pi models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raspberry Pi 3B&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 3B+&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 3A+&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 4B&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 400&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 5&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 500&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Do you like the project? Please consider to make a free donation using the button below&lt;/p&gt;
    &lt;head rend="h2"&gt;For Raspberry Pi 3B/3B+/4B/400/5/500&lt;lb/&gt;Download Dosbian 3.0&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;Note: Unzip the image with 7zip and use Win32DiskImager or Balena Etcher to flash it.&lt;/p&gt;
    &lt;p&gt;Did you like Dosbian? &lt;lb/&gt;Try Combian64, a dedicated distro that boots straight in to one of the old glory Commodore machines (64,128, Vic 20, PET, ecc).&lt;/p&gt;
    &lt;head rend="h2"&gt;Where to start from?&lt;/head&gt;
    &lt;p&gt;Here you can find some useful guide, link and tutorial:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dosbian a “Quick start guide” by Gary Marsh&lt;/item&gt;
      &lt;item&gt;The Definitive Guide on installing Windows 95 on Raspberry Pi 3B/4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;Guide and drivers: Installing Windows 95 on Raspberry Pi 3B/4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;How to install Windows 98 on Raspberry Pi 4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;mTCP – TCP/IP applications for DOS&lt;/item&gt;
      &lt;item&gt;List of games running smoothly on Dosbian 1.5 Rpi4 by Daniel Řepka&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Do you have anything to tell? Write a comment 🙂&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45637133</guid><pubDate>Sun, 19 Oct 2025 19:26:54 +0000</pubDate></item><item><title>What Unix pipelines got right and how we can do better</title><link>https://programmingsimplicity.substack.com/p/what-unix-pipelines-got-right-and</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45637242</guid><pubDate>Sun, 19 Oct 2025 19:40:26 +0000</pubDate></item><item><title>We Need Arabic Language Models</title><link>https://www.natureasia.com/en/nmiddleeast/article/10.1038/nmiddleeast.2025.142</link><description>&lt;doc fingerprint="dd35baf98fb9db9f"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Why We Need Arabic Language Models&lt;/head&gt;
    &lt;p&gt;24 August 2025&lt;/p&gt;
    &lt;p&gt;Published online 24 August 2025&lt;/p&gt;
    &lt;p&gt;Building strong Arabic language models is a strategic step to ensure the Arab world’s active role in shaping the future of artificial intelligence.&lt;/p&gt;
    &lt;p&gt;In the global race to develop generative AI models, attention tends to center on major companies and research institutes in the West and China. Flagship models, including OpenAI’s GPT-4 and Google’s Gemini, are trained on vast amounts of data, predominantly in English and other Western languages, and therefore tend to reflect the cultural assumptions and values of the context in which they were developed.&lt;/p&gt;
    &lt;p&gt;The growing reliance on language models that do not necessarily reflect the richness and diversity of Arabic language poses a significant challenge. It’s not simply a matter of technical preference, but one that raises questions of cultural sovereignty, technological independence, and national identity.&lt;/p&gt;
    &lt;p&gt;These widely used models such as ChatGPT have the potential to shape perceptions and ideas. When trained on data from different cultural contexts, these models can generate responses that sideline core Arab values or remain vague on critical issues.&lt;/p&gt;
    &lt;p&gt;Clear examples emerge when global language models address culturally sensitive issues, such as social relationships or political debates. They often adopt ambiguous positions that overlook the Arab cultural context, creating a gap between these digital tools and the values and lived experiences of Arab users.&lt;/p&gt;
    &lt;p&gt;The lack of robust and competitive Arabic language models forces researchers and developers across the region to rely on tools that fail to capture the linguistic complexity of Arabic, its dialects, or cultural contexts. This dependence constrains the ability to design AI applications and services tailored to local needs, while also weakens the Arab world’s contribution to global AI advancement. In many ways, language models serve as a mirror of our research and innovation capacity.&lt;/p&gt;
    &lt;p&gt;In response to this challenge, promising initiatives have emerged across the Arab world. For instance, the UAE’s ‘Jais,’ Saudi Arabia’s ‘ALLaM,’ and Qatar’s ‘Fanar,’ which was developed by the Qatar Computing Research Institute (QCRI) at Hamad Bin Khalifa University (HBKU) in collaboration with government partners. These initiatives are part of broader strategic efforts to localize technology, safeguard cultural identity, and build technological self-reliance.&lt;/p&gt;
    &lt;p&gt;Developing such models, however, comes with significant challenges. One of the most persistent is the scarcity of high-quality Arabic content online compared with English. While Fanar was trained on more than half a trillion Arabic words, this remains modest when compared to global models trained on trillions of tokens. The quality of available Arabic data also varies widely, due to accuracy issues, linguistic style, and considerable diversity between Modern Standard Arabic and regional dialects, making data collection and representation more complex.&lt;/p&gt;
    &lt;p&gt;Another major challenge is the high cost of training large language models. For example, training a 7-billion-parameter model on a trillion words requires more than 220 H100 GPUs running continuously for over a month, and these resources are often beyond the reach of most research institutions in the Arab world. This inspired the Fanar team to focus on developing smaller models with seven and nine billion parameters, prioritizing improvements in data quality and optimization techniques to deliver the best possible performance with the resources available.&lt;/p&gt;
    &lt;p&gt;Addressing the challenges of cultural and technological dependency requires collaboration across multiple sectors. Academic and research institutions need to invest in Arabic language processing and build international partnerships to maximize resources and expertise. Governments and policymakers, in return, should provide sustained funding; support data infrastructure; promote policies that facilitate the collection and organization of high-quality Arabic datasets; and foster collaboration between the public and private sectors, as it is essential for building a supportive ecosystem for technological innovation.&lt;/p&gt;
    &lt;p&gt;Startups and developers in the region also have a role to play. Both should adopt Arabic language models to build applications that respond to local needs, from AI personalized education platforms to voice assistants in regional dialects. Cultural, educational, and media institutions, meanwhile, can contribute by generating diverse, high-quality Arabic digital content that can be used to train these models.&lt;/p&gt;
    &lt;p&gt;Building robust Arabic language models is not a technological luxury, but a strategic necessity to ensure that the Arab world has a voice in shaping the future of AI. While significant progress has been made, the path ahead requires sustained investment and collective effort from stakeholders across the region.&lt;/p&gt;
    &lt;p&gt;This is a translation of the Arabic article published on 3rd August 2025&lt;/p&gt;
    &lt;p&gt;doi:10.1038/nmiddleeast.2025.142&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45637347</guid><pubDate>Sun, 19 Oct 2025 19:50:46 +0000</pubDate></item></channel></rss>