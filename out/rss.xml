<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 23 Jan 2026 12:23:36 +0000</lastBuildDate><item><title>In Europe, wind and solar overtake fossil fuels</title><link>https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels</link><description>&lt;doc fingerprint="2d5274461a14540a"&gt;
  &lt;main&gt;
    &lt;p&gt;Last year, for the first time, wind and solar supplied more power than fossil fuels to the E.U., according to a new analysis.&lt;/p&gt;
    &lt;p&gt;The shift is largely due to the rapid expansion of solar energy, which is growing faster than any other source of electricity. Together, wind and solar generated 30 percent of E.U. power last year, while fossil fuels provided 29 percent, according to the analysis from Ember, a think tank based in London. Including hydro, renewables provided nearly half of all E.U. power in 2025.&lt;/p&gt;
    &lt;p&gt;The analysis finds that solar is making gains in every E.U. country, while coal is broadly in retreat. Last year, solar alone supplied more than 20 percent of power in Hungary, Cyprus, Greece, Spain, and the Netherlands. Meanwhile, in 19 European countries, coal accounted for less than 5 percent of power. In 2025, both Ireland and Finland joined the ranks of European countries that have shuttered their last remaining coal plants.&lt;/p&gt;
    &lt;p&gt;Warming, however, continues to challenge the shift to clean energy as drought saps hydropower. Last year, hydro output dropped slightly in the E.U., and natural gas power rose to compensate.&lt;/p&gt;
    &lt;p&gt;“The next priority for the E.U. should be to put a serious dent in reliance on expensive, imported gas,” said Ember analyst Beatrice Petrovich. “Gas not only makes the E.U. more vulnerable to energy blackmail, it’s also driving up prices.”&lt;/p&gt;
    &lt;p&gt;In parts of Europe, there are signs that increasingly cheap batteries are beginning to displace natural gas in the early evening, when power demand is high, but solar output is waning. Said Petrovich, “As this trend accelerates it could limit how much gas is needed in evening hours, therefore stabilizing prices.”&lt;/p&gt;
    &lt;head rend="h2"&gt;ALSO ON YALE E360&lt;/head&gt;
    &lt;p&gt;An E.U. Plan to Slash Micropollutants in Wastewater Is Under Attack&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46719491</guid><pubDate>Thu, 22 Jan 2026 14:14:15 +0000</pubDate></item><item><title>GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers</title><link>https://gptzero.me/news/neurips/</link><description>&lt;doc fingerprint="3b56a9bbb164ebc9"&gt;
  &lt;main&gt;
    &lt;row style="height:27.75pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Published Paper&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;GPTZero Scan&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Example of Verified Hallucination&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Comment&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;John Doe and Jane Smith. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.00001, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Article with a matching title exists here. Authors are obviously fabricated. arXiv ID links to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;John Smith and Jane Doe. Deep learning techniques for avatar-based interaction in virtual environments. IEEE Transactions on Neural Networks and Learning Systems, 32(12):5600-5612, 2021. doi: 10.1109/ TNNLS.2021.3071234. URL https://ieeexplore.ieee.org/document/307123&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication. URL and DOI are fake.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Min-Jun Lee and Soo-Young Kim. Generative adversarial networks for hyper-realistic avatar creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1234-1243, 2022. doi: 10.1109/CVPR.2022.001234. URL https://ieeexplore.ieee.org/ document/00123&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication. URL and DOI are fake.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Firstname Lastname and Others. Drivlme: A large-scale multi-agent driving benchmark, 2023. URL or arXiv ID to be updated.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Potentially referring to this article, but year is off (2024)&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Firstname Lastname and Others. Robotslang: Grounded natural language for multi-robot object search, 2024. To appear.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Potentially referring to this article, but year is totally off (2020).&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nuo Lou and et al. Dsp: Diffusion-based span prediction for masked text modeling. arXiv preprint arXiv:2305.XXXX, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A. Sahoo and et al. inatk: Iterative noise aware text denoising. arXiv preprint arXiv:2402.XXXX, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sheng Shi and et al. Maskgpt: Uniform denoising diffusion for language. arXiv preprint arXiv:2401.XXXX, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Asma Issa, George Mohler, and John Johnson. Paraphrase identification using deep contextualized representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 517-526, 2018.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yi Tay, Kelvin Fu, Kai Wu, Ivan Casanueva, Jianfeng Liu, Byron Wallace, Shuohang Wang, Bajrang Singh, and Julian McAuley. Reasoning with heterogeneous graph representations for knowledge-aware question answering. In Findings of the Association for Computational Linguistics: ACL 2021, pp. 3497-3506, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No exact author or title match, although this title is close. No match in the publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Alex Wang, Rishi Bommasani, Dan Hendrycks, Daniel Song, and Zhilin Zhang. Efficient fewshot learning with efl: A single transformer for all tasks. In arXiv preprint arXiv:2107.13586, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Lei Yu, Jimmy Dumsmyr, and Kevin Knight. Deep paraphrase identification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $650-655,2014$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. No match in publication&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;X. Ou and et al. Tuqdm: Token unmasking with quantized diffusion models. In ACL, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Franz Aichberger, Lily Chen, and John Smith. Semantically diverse language generation. In International Conference on Learning Representations (ICLR), 2025.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Some similarity to this article&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Maria Glushkova, Shiori Kobayashi, and Junichi Suzuki. Uncertainty estimation in neural text regression. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. $4567-4576,2021$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yichao Wang, Bowen Zhou, Adam Lopez, and Benjamin Snyder. Uncertainty quantification in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1234-1245, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mohit Jain, Ethan Perez, and James Glass. Learning to predict confidence for language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 245-256, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Srinivasan Kadavath, Urvashi Khandelwal, Alec Radford, and Noam Shazeer. Answer me this: Self-verifying large language models. In arXiv preprint arXiv:2205.05407, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Privacy Reasoning in Ambiguous Contexts&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zayne Sprague, Xi Ye, Kyle Richardson, and Greg Durrett. MuSR: Testing the limits of chain-of-thought with multistep soft reasoning. In EMNLP, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Two authors are omitted and one (Kyle Richardson) is added. This paper was published at ICLR 2024.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mario Paolone, Trevor Gaunt, Xavier Guillaud, Marco Liserre, Sakis Meliopoulos, Antonello Monti, Thierry Van Cutsem, Vijay Vittal, and Costas Vournas. A benchmark model for power system stability controls. IEEE Transactions on Power Systems, 35(5):3627-3635, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;The authors match this paper, but the title, publisher, volume, issue, and page numbers are incorrect. Year (2020) is correct.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mingliang Han, Bingni W Wei, Phelan Senatus, Jörg D Winkel, Mason Youngblood, I-Han Lee, and David J Mandell. Deep koopman operator: A model-free approach to nonlinear dynamical systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(12):123135, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Journal and other identifiers match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Adaptive Quantization in Generative Flow Networks for Probabilistic Sequential Prediction&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Francisco Ramalho, Meng Liu, Zihan Liu, and Etienne Mathieu. Towards gflownets for continuous control. arXiv preprint arXiv:2310.18664, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID matches this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Grounded Reinforcement Learning for Visual Reasoning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Arjun Gupta, Xi Victoria Lin, Chunyuan Zhang, Michel Galley, Jianfeng Gao, and Carlos Guestrin Ferrer. Robust compositional visual reasoning via language-guided neural module networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. This paper has a similar title and matches publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MTRec: Learning to Align with User Preferences via Mental Reward Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diederik P. Kingma and Jimmy Ba. Deepfm: a factorization-machine based neural network for ctr prediction. In International Conference on Learning Representations, 2015.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title matches this paper. Authors, date, and publisher match this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Weijia Xu, Xing Niu, and Marine Carpuat. Controlling toxicity in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4245-4256, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors, publisher and date match this paper. Title and page numbers don't match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Xiang Zhang, Xuehai Wei, Xian Zhang, and Xue Zhang. Adversarial attacks and defenses in toxicity detection: A survey. In Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fenglin Ding, Debesh Jha, Maria Härgestam, Pål Halvorsen, Michael A Riegler, Dag Johansen, Ronny Hänsch, and Håvard Stensland. Vits: Vision transformer for video self-supervised pretraining of surgical phase recognition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 293-302. Springer, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Proceedings from this conference are split into volumes, but the citation doesn't have a volume number.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Humberto Acevedo-Viloria, Juan Martinez, and Maria Garcia. Relational graph convolutional networks for financial fraud detection. IEEE Transactions on Knowledge and Data Engineering, 33(7):1357-1370, 2021. doi: 10.1109/TKDE.2020.3007655.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in the cited publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Majid Zolghadr, Mohsen Jamali, and Jiawei Zhang. Diffurecsys: Diffusion-based generative modeling for sequential recommendation. Proceedings of the ACM Web Conference (WWW), pages 2156-2165, 2024. doi: 10.1145/3545678.3557899.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. DOI doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Bernd Kerbl, Thomas Müller, and Paolo Favaro. Efficient 3d gaussian splatting for real-time neural rendering. In CVPR, 2022. 2, 3&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Loosely matches this article, but only one author and part of the title actually match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Punchana Khungurn, Edward H. Adelson, Julie Dorsey, and Holly Rushmeier. Matching real-world material appearance. TPAMI, 2015. 6&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No clear match. Two authors and the subject match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;When and How Unlabeled Data Provably Improve In-Context Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ashish Kumar, Logan Engstrom, Andrew Ilyas, and Dimitris Tsipras. Understanding self-training for gradient-boosted trees. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1651-1662, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;When and How Unlabeled Data Provably Improve In-Context Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Chuang Fan, Shipeng Liu, Seyed Motamed, Shiyu Zhong, Silvio Savarese, Juan Carlos Niebles, Anima Anandkumar, Adrien Gaidon, and Stefan Scherer. Expectation maximization pseudo labels. arXiv preprint arXiv:2305.01747, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;This paper exists, but all the authors are fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;T. Qiao, W. Liu, Z. Xie, H. Xu, J. Lin, J. Huang, and Y. Yang, "Clip-score: A robust scoring metric for text-to-image generation," arXiv preprint arXiv:2201.07519, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No clear author or title matches. Title loosely matches this article. ArXiv ID leads here.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yunwen Lei, Puyu Wang, Yiming Ying, and Ding-Xuan Zhou. Optimization and generalization of gradient descent for shallow relu networks with minimal width. preprint, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. Authors match this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;GeoDynamics: A Geometric State‑Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Uher, R., Goodman, R., Moutoussis, M., Brammer, M., Williams, S.C.R., Dolan, R.J.: Cognitive and neural predictors of response to cognitive behavioral therapy for depression: a review of the evidence. Journal of Affective Disorders 169, 94-104 (2014)&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No exact title or author match. Loose title match with this article. Doesn't exist in the journal volume&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Label Proportions Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Scan&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Junyeong Lee, Yiseong Kim, Seungju Park, and Hyunjik Lee. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 18315-18327, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title matches this paper. No match in NeurIPS volume 36.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z. Zhu, T. Yu, X. Zhang, J. Li, Y. Zhang, and Y. Fu. Neuralrgb-d: Neural representations for depth estimation and scene mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Y. Zhang, M. Oswald, and D. Cremers. Airslam: Illumination-invariant hybrid slam. In International Conference on Computer Vision (ICCV), pages 2345-2354, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Geometric Imbalance in Semi-Supervised Node Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yihong Zhu, Junxian Li, Xianfeng Han, Shirui Pan, Liang Yao, and Chengqi Wang. Spectral contrastive graph clustering. In International Conference on Learning Representations, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. This paper has a similar title, but there's no match in the ICLR 2022 database.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Geometric Imbalance in Semi-Supervised Node Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ming Zhong, Han Liu, Weizhu Zhang, Houyu Wang, Xiang Li, Maosong Sun, and Xu Han. Hyperbolic and spherical embeddings for long-tail entities. In ACL, pages 5491-5501, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ye Gao, Robert Tardif, Jiale Cao, and Tapio Schneider. Artificial intelligence reconstructs missing climate information. Nature Geoscience, 17:158-164, 2024. doi: 10.1038/s41561-023-01297-2.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title and publisher match this article. Issue, page numbers, and year match this article. DOI is fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Étienne Pardoux and Alexander Yu Veretennikov. Poisson equation for multiscale diffusions. Journal of Mathematical Sciences, 111(3):3713-3719, 2002.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors have frequently published together on the "poisson equation", but this title doesn't match any of their publications. Doesn't exist in publication volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Charanpal D Mummadi, Matthias Arens, and Thomas Brox. Test-time adaptation for continual semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11828-11837, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiacheng He, Zhilu Zhang, Zhen Wang, and Yan Huang. Autoencoder based test-time adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 998-1007, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Global Minimizers of ℓp-Regularized Objectives Yield the Sparsest ReLU Neural Networks&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;M. Gong, F. Yu, J. Zhang, and D. Tao. Efficient $\ell_{p}$ norm regularization for learning sparsity in deep neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(10): $5381-5392,2022&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mihail Stoian, Richard Milbradt, and Christian Mendl. NP-Hardness of Optimal TensorNetwork Contraction and Polynomial-Time Algorithms for Tree Tensor Networks. Quantum, 6:e119, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;The authors match this article and the title is similar. However, the year, publisher and other data don't match. This article didn't appear in the 2022 Quantum volume.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jianyu Xu, Wei Li, and Ming Zhao. Complexity of Optimal Tensor Network Contraction Sequences. Journal of Computational Physics, 480:112237, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning World Models for Interactive Video Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Patrick Esser, Robin Rombach, and Björn Ommer. Structure-aware video generation with latent diffusion models. arXiv preprint arXiv:2303.07332, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors match this article. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Lele Xu, Chen Lin, Hongyu Zhao, and et al. Gaborvit: Global attention with local frequency awareness. In European Conference on Computer Vision (ECCV), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yoonwoo Lee, Jaehyeong Kang, Namil Kim, Jinwoo Shin, and Honglak Lee. Structured fast fourier transform attention for vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Siyuan Gong, Alan Yu, Xiaohan Chen, Yinpeng Lin, and Larry S Davis. Vision transformer compression: Early exiting and token pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiuxiang Shi, Zuxuan Wu, and Dahua Lin. Token-aware adaptive sampling for efficient diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Raphael Muller, Simon Kornblith, and Geoffrey Hinton. Adavit: Adaptive tokens for efficient vision transformer. In Proceedings of the International Conference on Machine Learning (ICML), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors match this article. Title matches this article. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Xin Wang, Anlin Chen, Lihui Xie, Xin Jin, Cheng Wang, and Ping Luo. Not all tokens are equal: Efficient transformer for tokenization and beyond. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This article title is similar. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z. Chen and N. Flammarion. When and why sam generalizes better: An optimization perspective. arXiv preprint arXiv:2206.09267, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to a different paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;K. A. Sankararaman, S. Sankararaman, H. Pandey, S. Ganguli, and F. Bromberg. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In 37th International Conference on Machine Learning (ICML), pages 8469-8479, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;This paper is a match, but all authors but the first (K. A. Sankararaman) are fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Why Physically-Based Rendering. Physically-based rendering. Procedia IUTAM, 13(127137):3, 2015 .&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author given and title appears to be garbled. Publisher, issue, year, and pages match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Pierre Casgrain, Anirudh Kulkarni, and Nicholas Watters. Learning to trade with continuous action spaces: Application to market making. arXiv preprint arXiv:2303.08603, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. ArXiv ID matches a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z Ning and Y K Kwok. Q-learning for option pricing and hedging with transaction costs. Applied Economics, 52(55):6033-6048, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;W L Chan and R O Shelton. Can machine learning improve delta hedging? Journal of Derivatives, $9(1): 39-56,2001$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Petter N Kolm, Sebastian Krügel, and Sergiy V Zadorozhnyi. Reinforcement learning for optimal hedging. The Journal of Trading, 14(4):4-17, 2019.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. There is no volume 14 of this journal.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Kyung Hyun Park, Hyeong Jin Kim, and Woo Chang Kim. Deep reinforcement learning for limit order book-based market making. Expert Systems with Applications, 169:114338, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publisher ID matches this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Moonseop Han and Elizabeth Qian. Robust prediction of dynamical systems with structured neural networks: Long-term behavior and chaos. Physica D: Nonlinear Phenomena, 427:133006, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publisher ID matches this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Bart De Schutter and Serge P Hoogendoorn. Modeling and control of freeway traffic flow by state space neural networks. Neural Computing and Applications, 17(2):175-185, 2008.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match, although Schutter and Hoogendorn have written or coauthored several related papers (example and example). Journal volume/issue matches an unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jaideep Pathak, Brian R Hunt, Georg M Goerg, and Themistoklis P Sapsis. Data-driven prediction of chaotic dynamics: Methods, challenges, and opportunities. Annual Review of Condensed Matter Physics, 14:379-401, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Alejandro Güemes, Stefano Discetti, and Andrea Ianiro. Coarse-grained physics-based prediction of three-dimensional unsteady flows via neural networks. Science Advances, 7(46):eabj0751, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;BNMusic: Blending Environmental Noises into Personalized Music&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jeongseung Park, Minseon Yang, Minz Won Park, and Geonseok Lee. Diffsound: Differential sound manipulation with a few-shot supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1767-1775, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Wenxuan Sun, Tri Dao, Hongyu Zhuang, Zihang Dai, Albert Gu, and Christopher D Manning. Llamba: Efficient llms with mamba-based distillation. arXiv preprint arXiv:2502.14458, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;ArXiv ID leads to this article with a similar title and one matching author.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Tri Dao, Shizhe Ma, Wenxuan Sun, Albert Gu, Sam Smith, Aapo Kyrola, Christopher D Manning, and Christopher Re. An empirical study of state space models for large language modeling. arXiv preprint arXiv:2406.07887, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Two authors (Tri Dao and Albert Gu), the arXiv ID, and the year match this paper. However, the title is only a partial match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Clouds: Fast Bias Correction for Imbalanced Semi-Supervised Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Junyan Zhu, Chenyang Li, Chao He, and et al. Freematch: A simple framework for long-tailed semi-supervised learning. In NeurIPS, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This paper title is very close, but it was published by ICLR 2023 not NeurIPS 2021.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Scan&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yijie Zang et al. Fedclip: A federated learning framework for vision-language models. In NeurIPS, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match, although this title is close. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;AI-Generated Video Detection via Perceptual Straightening&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiahui Liu and et al. Tall-swin: Thumbnail layout transformer for generalised deepfake video detection. In ICCV, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. A paper with a similar title appears in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Multi-Expert Distributionally Robust Optimization for Out-of-Distribution Generalization&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nitish Srivastava and Ruslan R Salakhutdinov. Discriminative features for fast frame-based phoneme classification. Neural networks, 47:17-23, 2013.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match, but authors have published together previously (example). No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Anh Tuan Nguyen, Shengping Li, and Chao Qin. Multimodal adversarial robustness: Attack and defense. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jack Lau, Ankan Gayen, Philipp Tschandl, Gregory A Burns, Jiahong Yuan, Tanveer SyedaMahmood, and Mehdi Moradi. A dataset and exploration of models for understanding radiology images through dialogue. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2575-2584, 2018.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches another hallucinated citation in this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yikai Zhang et al. "Text-to-Image Diffusion Models with Customized Guidance". In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Author Song and AnotherAuthor Zhang. "Consistency in Diffusion Models: Improving Noise Embeddings". In: IEEE Transactions on Pattern Analysis and Machine Intelligence (2023). URL: https://arxiv.org/abs/2304.08787.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This paper has a similar title. ArXiv ID leads to unrelated paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Strategic Costs of Perceived Bias in Fair Selection&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Claudia Goldin. Occupational choices and the gender wage gap. American Economic Review, 104(5):348-353, 2014.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Author is a famous economist, but the title doesn't match any of her works. Journal and locators match this unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Linear Transformers Implicitly Discover Unified Numerical Algorithms&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Olah, C., Elhage, N., Nanda, N., Schiefer, N., Jones, A., Henighan, T., and DasSarma, N. (2022). Transformer circuits. Distill, 7(3). https://distill.pub/2022/circuits/.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Most authors match this paper, but the title, publisher, and year are different. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Linear Transformers Implicitly Discover Unified Numerical Algorithms&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nanda, N. (2023). Progress in mechanistic interpretability: Reverse-engineering induction heads in GPT-2.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. Author may be Neel Nanda, who wrote several similar articles in 2023.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;J. Zhang and X. Li. Multi-agent systems for distributed problem solving: A framework for task decomposition and coordination. Procedia Computer Science, 55:1131-1138, 2015.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Erfan Aghasian, Shai Avidan, Piotr Dollar, and Justin Johnson. Hierarchical protocols for multi-agent 3d scene understanding. In CVPR, pages 7664-7673, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Rami El-Yaniv, and Yoshua Bengio. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1612.01462, 2017.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors mostly match this paper. Title matches this paper. ArXiv ID matches a third paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhiqiang Wang, Chao Zhang, Bing Li, Zhen Xu, and Zhiwei Li. A survey of model compression and acceleration for deep neural networks. ACM Computing Surveys, 54(7):1-34, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Andrew Black et al. Zero-shot skill composition with semantic feature fusion. arXiv preprint arXiv:2310.08573, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. ArXiv ID leads to unrelated paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yufei Wu, Kiran Alwala, Vivek Ganapathi, Sudeep Sharma, Yilun Chang, Yicheng Zhang, Yilun Zhou, et al. Susie: Scaling up instruction-following policies for robot manipulation. arXiv preprint arXiv:2402.17552, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FLAME: Fast Long-context Adaptive Memory for Event-based Vision&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhipeng Zhang, Chang Liu, Shihan Wu, and Yan Zhao. EST: Event spatio-temporal transformer for object recognition with event cameras. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FLAME: Fast Long-context Adaptive Memory for Event-based Vision&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Daniel Gehrig, Mathias Gehrig, John Monaghan, and Davide Scaramuzza. Recurrent vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 3139-3148, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match, but this paper has a similar title. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Qiyang Du, Ozan Sener, and Silvio Savarese. Agree to disagree: Adaptive learning with gradient disagreement. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Sener and Savarese have published together previously. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Longxuan Jing, Yu Tian, Yujun Pei, Yibing Shen, and Jiashi Feng. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Learning Representations (ICLR), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yair Leviathan, Clemens Rosenbaum, and Slav Petrov. Fast inference from transformers via speculative decoding. In ICML, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title, publisher, and date match this paper, but all authors except one surname (Leviathan) are different.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Wenwen Chang, Tal Schuster, and Yann LeCun. Neural surgery for memorisation: Locating and removing verbatim recall neurons. In NeurIPS, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;M. Garcia and A. Thompson. Applications of llms in legal document analysis. Journal of Legal Technology, 7(1):50-65, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publication doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;J. Smith and A. Patel. Leveraging large language models for financial forecasting. International Journal of Financial Technology, 9(2):101-115, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publication doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;David Jones et al. Gpsa: Gene expression and histology-based spatial alignment. Nature Methods, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhihao Chen, Hantao Zhang, Yuhan Zhang, Zhanlin Hu, Quanquan Gu, Qing Zhang, and Shuo Suo. Slat: a transformer-based method for simultaneous alignment and clustering of spatial transcriptomics data. Nature Communications, 14(1):5548, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;François Baccelli, Gérard H. Taché, and Etienne Altman. Flow complexity and heavytailed delays in packet networks. Performance Evaluation, 49(1-4):427-449, 2002.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Saravanan Jebarajakirthy, Paurav Shukla, and Prashant Palvia. Heavy-tailed distributions in online ad response: A marketing analytics perspective. Journal of Business Research, 124:818-830, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mehdi Azabou, Micah Weber, Wenlin Ma, et al. Mineclip: Multimodal neural exploration of clip latents for automatic video annotation. arXiv preprint arXiv:2210.02870, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46720395</guid><pubDate>Thu, 22 Jan 2026 15:20:48 +0000</pubDate></item><item><title>Show HN: isometric.nyc – giant isometric pixel art map of NYC</title><link>https://cannoneyed.com/isometric-nyc/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721802</guid><pubDate>Thu, 22 Jan 2026 16:52:35 +0000</pubDate></item><item><title>AnswerThis (YC F25) Is Hiring</title><link>https://www.ycombinator.com/companies/answerthis/jobs/r5VHmSC-ai-agent-orchestration</link><description>&lt;doc fingerprint="3d09585cf826800c"&gt;
  &lt;main&gt;
    &lt;p&gt;End-to-end workspace to accelerate scientific discovery&lt;/p&gt;
    &lt;p&gt;We crossed $1M ARR in 8 months. 200,000+ researchers at Stanford, MIT, and Amazon use us to do literature reviews 10x faster.&lt;lb/&gt; Now we're building something bigger: the system of record for scientists where they can find papers, analyze experiments, and write their drafts while collaborating with other scientists as well as our AI agents. &lt;lb/&gt; You should apply if you:&lt;lb/&gt; → Ship fast and learn faster &lt;lb/&gt; → Know the agentic AI stack cold (vector DBs, graph RAG, agent memory) &lt;lb/&gt; → Have built full-stack products that scaled past 1M users &lt;lb/&gt; → Actually care about accelerating scientific discovery&lt;lb/&gt; Bonus: You've published research yourself. &lt;lb/&gt; Don't apply if you:&lt;lb/&gt; → Can't be in SF, in person &lt;lb/&gt; → Haven't used the product yet &lt;lb/&gt; → Don't want to talk to customers &lt;lb/&gt; $120K-$200K + equity. We're a small team backed by YC. &lt;lb/&gt; Reach out on careers [at] answerthis.io&lt;lb/&gt; Tell us what you hate about AnswerThis, what you love, and one project you're proud of alongside your resume.&lt;lb/&gt; Science moves too slowly. Help us fix that.&lt;/p&gt;
    &lt;p&gt;We move fast. The whole process can be done in 2-3 weeks.&lt;/p&gt;
    &lt;p&gt;AnswerThis is building the system of record for scientists—where researchers can find papers, analyze experiments, and write drafts while collaborating with other scientists and AI agents.&lt;/p&gt;
    &lt;p&gt;We crossed $1M ARR in 8 months. 200,000+ researchers at Stanford, MIT, Amazon, and top institutions worldwide use us daily. We're backed by Y Combinator (F25) and cash-flow positive.&lt;/p&gt;
    &lt;p&gt;Science moves too slowly. Grant applications take months. Literature reviews take weeks. Researchers spend more time on paperwork than on discovery. We're fixing that.&lt;/p&gt;
    &lt;p&gt;You'll be joining a small, fast team in SF that ships constantly and talks to customers every day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721897</guid><pubDate>Thu, 22 Jan 2026 17:00:40 +0000</pubDate></item><item><title>Launch HN: Constellation Space (YC W26) – AI for satellite mission assurance</title><link>https://news.ycombinator.com/item?id=46721933</link><description>&lt;doc fingerprint="22906374e6a95fca"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN! We're Kamran, Raaid, Laith, and Omeed from Constellation Space (&lt;/p&gt;https://constellation-io.com/&lt;p&gt;). We built an AI system that predicts satellite link failures before they happen. Here's a video walkthrough: &lt;/p&gt;https://www.youtube.com/watch?v=069V9fADAtM&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Between us, we've spent years working on satellite operations at SpaceX, Blue Origin, and NASA. At SpaceX, we managed constellation health for Starlink. At Blue, we worked on next-gen test infra for New Glenn. At NASA, we dealt with deep space communications. The same problem kept coming up: by the time you notice a link is degrading, you've often already lost data.&lt;/p&gt;&lt;p&gt;The core issue is that satellite RF links are affected by dozens of interacting variables. A satellite passes overhead, and you need to predict whether the link will hold for the next few minutes. That depends on: the orbital geometry (elevation angle changes constantly), tropospheric attenuation (humidity affects signal loss via ITU-R P.676), rain fade (calculated via ITU-R P.618 - rain rates in mm/hr translate directly to dB of loss at Ka-band and above), ionospheric scintillation (we track the KP index from magnetometer networks), and network congestion on top of all that.&lt;/p&gt;&lt;p&gt;The traditional approach is reactive. Operators watch dashboards, and when SNR drops below a threshold, they manually reroute traffic or switch to a backup link. With 10,000 satellites in orbit today and 70,000+ projected by 2030, this doesn't scale. Our system ingests telemetry at around 100,000 messages per second from satellites, ground stations, weather radar, IoT humidity sensors, and space weather monitors. We run physics-based models in real-time - the full link budget equations, ITU atmospheric standards, orbital propagation - to compute what should be happening. Then we layer ML models on top, trained on billions of data points from actual multi-orbit operations.&lt;/p&gt;&lt;p&gt;The ML piece is where it gets interesting. We use federated learning because constellation operators (understandably) don't want to share raw telemetry. Each constellation trains local models on their own data, and we aggregate only the high-level patterns. This gives us transfer learning across different orbit types and frequency bands - learnings from LEO Ka-band links help optimize MEO or GEO operations. We can predict most link failures 3-5 minutes out with &amp;gt;90% accuracy, which gives enough time to reroute traffic before data loss. The system is fully containerized (Docker/Kubernetes) and deploys on-premise for air-gapped environments, on GovCloud (AWS GovCloud, Azure Government), or standard commercial clouds.&lt;/p&gt;&lt;p&gt;Right now we're testing with defense and commercial partners. The dashboard shows real-time link health, forecasts at 60/180/300 seconds out, and root cause analysis (is this rain fade? satellite setting below horizon? congestion?). We expose everything via API - telemetry ingestion, predictions, topology snapshots, even an LLM chat endpoint for natural language troubleshooting.&lt;/p&gt;&lt;p&gt;The hard parts we're still working on: prediction accuracy degrades for longer time horizons (beyond 5 minutes gets dicey), we need more labeled failure data for rare edge cases, and the federated learning setup requires careful orchestration across different operators' security boundaries. We'd love feedback from anyone who's worked on satellite ops, RF link modeling, or time-series prediction at scale. What are we missing? What would make this actually useful in a production NOC environment?&lt;/p&gt;&lt;p&gt;Happy to answer any technical questions!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721933</guid><pubDate>Thu, 22 Jan 2026 17:03:21 +0000</pubDate></item><item><title>CSS Optical Illusions</title><link>https://alvaromontoro.com/blog/68091/css-optical-illusions</link><description>&lt;doc fingerprint="860c29efdbe599e5"&gt;
  &lt;main&gt;
    &lt;p&gt;You can find a collection with all the optical illusions in this article (and more!) on CodePen. You can move your mouse over many of the demos below to reveal the effect or stop the animations.&lt;/p&gt;
    &lt;head rend="h2"&gt;1 - Poggendorff Illusions&lt;/head&gt;
    &lt;p&gt;The Poggendorff illusion is an optical illusion in which a diagonal line interrupted by a vertical bar appears misaligned, even when both segments are actually continuous.&lt;/p&gt;
    &lt;p&gt;A simple version of this effect can be seen in the following demo. I used the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt; pseudo-elements to create the diagonal line and the vertical bar, respectively.&lt;/p&gt;
    &lt;p&gt;The effect can also be seen in a more elaborate version with multiple diagonal lines and vertical bars:&lt;/p&gt;
    &lt;p&gt;This drawing can easily be achieved using two CSS gradients: one tilted at 70 degrees and another consisting of a series of vertical columns. I applied it to the &lt;code&gt;body&lt;/code&gt;, although I could have used &lt;code&gt;:root&lt;/code&gt; instead.&lt;/p&gt;
    &lt;p&gt;Another variation of this illusion is the Münsterberg Poggendorff Arch, in which the two sides of an arch appear misaligned and seem as though they will not meet at the top - but they do (mouse over to see it).&lt;/p&gt;
    &lt;head rend="h2"&gt;2 - Induced Gradients&lt;/head&gt;
    &lt;p&gt;The following illusions combine gradients and flat colors. Surprisingly, some of the gradients do not actually exist. They are simple gray bars that, when placed over a gradient, appear to have gradients themselves.&lt;/p&gt;
    &lt;p&gt;Take the following demo: all three bars (two vertical ones on the sides and one horizontal bar in the center) are the same shade of gray. The only real gradient is behind them, which tricks our brain into believing that the bars are different colors and even contain gradients.&lt;/p&gt;
    &lt;p&gt;Here is another variation of this effect. It looks like the central line has a repeating gradient of dark and light grays, but in reality it is a flat color. If you mouse over the demo, the bar will expand, making it clear that there is no gradient at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;3 - Cornsweet Illusion&lt;/head&gt;
    &lt;p&gt;The next few optical illusions share a common idea: some colors are identical, but they do not look the same. This typically happens when regions of the same color or brightness are surrounded by areas with different contrast.&lt;/p&gt;
    &lt;p&gt;For example, in the following demo, the left and right ends are the same shade of gray. However, one looks lighter because it is closer to white, while the other looks darker because it is closer to black. Mouse over to reveal that they are, in fact, the same color.&lt;/p&gt;
    &lt;head rend="h2"&gt;4 - White's Illusion&lt;/head&gt;
    &lt;p&gt;Run the following demo. You will see two gray columns in a black-and-white grid. Both columns are the same shade of gray, but the one surrounded by black appears darker than the one surrounded by white.&lt;/p&gt;
    &lt;p&gt;I coded this demo using &lt;code&gt;mix-blend-mode&lt;/code&gt; so I could try something a bit different. That worked well, but it also made it harder to showcase the effect on hover. In hindsight, I should have planned that better.&lt;/p&gt;
    &lt;p&gt;This optical illusion also works with colors. For example, these two squares appear to be different shades of blue, but they are the same color. This time, you can mouse over to reveal the effect:&lt;/p&gt;
    &lt;head rend="h2"&gt;5 - Wertheimer-Koffka Ring&lt;/head&gt;
    &lt;p&gt;The ring in the following illustration has the same color all the way around. However, one side is placed over white and the other over black, which makes them look different. If you mouse over the demo, the red bar will disappear, making it more obvious that the ring is a single, uniform color.&lt;/p&gt;
    &lt;head rend="h2"&gt;6 - Adelson's Illusion&lt;/head&gt;
    &lt;p&gt;You have probably seen the illusion involving a checkerboard and an object casting a shadow, where two tiles - one seemingly light and one seemingly dark - turn out to be the same color.&lt;/p&gt;
    &lt;p&gt;This demo follows the same principle. You will see two tiles labeled A and B. Both have the same shade of gray, but most people cannot tell at first glance (or second, or even third).&lt;/p&gt;
    &lt;head rend="h2"&gt;7 - Asahi illusion of Brightness&lt;/head&gt;
    &lt;p&gt;The circle at the center of this flower-shaped element is the same white as the rest of the page, but it gives the impression of being brighter, as if it were emitting light.&lt;/p&gt;
    &lt;head rend="h2"&gt;8 - Color Spheres&lt;/head&gt;
    &lt;p&gt;This is one of my favorite illusions in the collection. The circles (or spheres) look red, blue, or green, but in reality they are all the same grayish color. Our brain "colorizes" them based on the lines that overlap the shapes. Don't believe it? Mouse over the illustration.&lt;/p&gt;
    &lt;head rend="h2"&gt;9 - Colors from Contour&lt;/head&gt;
    &lt;p&gt;In the following illustration, the lines inside the yellow section appear blue, while the lines inside the blue section appear red... but they are all black (or very dark gray). The white contour creates the illusion of color. Mouse over to remove the contour and the lines will clearly appear black.&lt;/p&gt;
    &lt;head rend="h2"&gt;10 - Curvature Blindness&lt;/head&gt;
    &lt;p&gt;One set of lines looks straighter (top) while the other looks more curved (bottom). In reality, both sets are equally wavy. The only difference is how they are colored: changing the color at the peaks makes the lines look straighter. Changing it at the inflection points makes them look more curved.&lt;/p&gt;
    &lt;p&gt;The CSS code for the wavy lines is adapted from a Temani Afif snippet on CSS-Tricks and his wavy shape generator.&lt;/p&gt;
    &lt;head rend="h2"&gt;11 - Cafe Wall&lt;/head&gt;
    &lt;p&gt;This is a classic optical illusion and an easy one to code in CSS. Three gradients are all that is needed to generate the effect in which the horizontal lines appear slanted, even though they are perfectly parallel.&lt;/p&gt;
    &lt;head rend="h2"&gt;12 - Penrose Triangle&lt;/head&gt;
    &lt;p&gt;This optical illusion depicts an impossible shape. Parts that should be in front appear in the back, top becomes right, and everything feels contradictory. I coded this one some time ago for the 2024 Divtober event.&lt;/p&gt;
    &lt;head rend="h2"&gt;13 - Ebbinghaus Illusion&lt;/head&gt;
    &lt;p&gt;Which orange circle is larger: the one on the right or the one on the left? It is a trick question: both are the same size. However, having smaller surrounding elements gives the impression that one is larger.&lt;/p&gt;
    &lt;p&gt;I also created an animated version of this illusion (see below), as well as another version using a square shape instead of a flower shape:&lt;/p&gt;
    &lt;head rend="h2"&gt;14 - Kanizsa Square&lt;/head&gt;
    &lt;p&gt;When people look at this illustration, they usually say they see a white square over black circles. However, the square is not actually there. The "Pac-Man" shapes create the illusion of a square and a sense of depth. Our brain fills in the missing information.&lt;/p&gt;
    &lt;head rend="h2"&gt;15 - Ehrenstein's Illusion&lt;/head&gt;
    &lt;p&gt;There are no circles or discs in this illustration, only vertical and horizontal lines forming crosses. Our visual system completes the shape and makes us perceive a disc that does not exist.&lt;/p&gt;
    &lt;head rend="h2"&gt;16 - Neon-Color-Spreading Illusion&lt;/head&gt;
    &lt;p&gt;This illustration shows concentric circles, some of which have a green-and-black pattern. Our brain perceives a central patterned circle and four concentric circles around it, beneath the green circle.&lt;/p&gt;
    &lt;p&gt;I cheated a little when creating this in CSS, as I actually used a green circle blended with the other backgrounds.&lt;/p&gt;
    &lt;head rend="h2"&gt;17 - Hering and Wundt Illusions&lt;/head&gt;
    &lt;p&gt;Perspective-based illusions are fascinating. Even when we know we are looking at a flat image, our brain insists on interpreting depth.&lt;/p&gt;
    &lt;p&gt;In the Hering illusion, the red lines appear to curve outward, even though they are straight.&lt;/p&gt;
    &lt;p&gt;The opposite effect is the Wundt illusion. When the lines expand from the sides toward the center, the red lines appear to curve inward (this effect is more subtle).&lt;/p&gt;
    &lt;head rend="h2"&gt;18 - Ponzo Illusion&lt;/head&gt;
    &lt;p&gt;Both yellow lines are the same length, but the top one looks longer due to perceived depth and perspective. I tried a different approach when coding this one by applying a three-dimensional rotation in CSS... so the perspective is technically real.&lt;/p&gt;
    &lt;head rend="h2"&gt;19 - T Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is easy to code in CSS and easy to fall for. Both the vertical and horizontal lines are the same length, but the vertical line appears longer.&lt;/p&gt;
    &lt;head rend="h2"&gt;20 - Müller-Lyer Illusion&lt;/head&gt;
    &lt;p&gt;A classic illusion: the horizontal lines are the same length, but inward- or outward-pointing edges dramatically change how we perceive them. I could swear the top one is longer. But it is not.&lt;/p&gt;
    &lt;p&gt; From a coding perspective, each shape is a pseudo-element. I ensured the horizontal lines were identical by using the same gradients and only repositioning the edges in the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;21 - Tilted Table Illusion&lt;/head&gt;
    &lt;p&gt;It looks like the top rectangle is leaning to the left, but it is actually parallel to the one at the bottom. The trick lies in the direction of the diagonal lines used to "color" each rectangle.&lt;/p&gt;
    &lt;p&gt;This illusion works better on larger screens. The effect is diminished when you can see the whole picture.&lt;/p&gt;
    &lt;head rend="h2"&gt;22 - Parallel Lines&lt;/head&gt;
    &lt;p&gt;This is a simple effect: the black lines are parallel, but they appear not to be because of the direction of the bars crossing them.&lt;/p&gt;
    &lt;p&gt;I slightly overcomplicated this one while coding it. I initially built the black-and-red version below and tried to reuse more code than I probably should have.&lt;/p&gt;
    &lt;p&gt;Here is the original version I created. The effect is also visible there:&lt;/p&gt;
    &lt;p&gt;Good news! There are more optical illusions below - but first, a warning.&lt;/p&gt;
    &lt;p&gt;ATTENTION: The following optical illusions are static, but they give the impression of movement. Proceed accordingly.&lt;/p&gt;
    &lt;p&gt;(Leaving some blank space in case you do not want to continue.)&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;23 - Expanding Hole&lt;/head&gt;
    &lt;p&gt;This is a trippy optical illusion. It is completely static, yet it looks like the black hole at the center is expanding - especially when you are not looking at it directly, creating the sensation of falling into a pit.&lt;/p&gt;
    &lt;p&gt;From a coding perspective, this one was very simple: a background pattern made with two radial gradients, plus a blurred pseudo-element for the "expanding" hole.&lt;/p&gt;
    &lt;head rend="h2"&gt;24 - Rotating Snakes&lt;/head&gt;
    &lt;p&gt;This is one of only two optical illusions in this collection where I used HTML elements instead of relying exclusively on CSS. It is a classic effect: when you look at the illustration, the peripheral discs appear to rotate, even though nothing is actually moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;25 - Appearing Dots&lt;/head&gt;
    &lt;p&gt;Another classic illusion. Focus on the white dots and the adjacent dots will appear to turn black. There is no animation, no transition, and nothing dynamic. Just intersecting lines and small white circles, yet it looks like motion.&lt;/p&gt;
    &lt;head rend="h2"&gt;26 - Disappearing Dots&lt;/head&gt;
    &lt;p&gt;This pattern consists of repeating black and white dots across the page. If you focus on one dot, the others will begin to disappear. At first it may happen by row or column, but after a short while, most of them vanish.&lt;/p&gt;
    &lt;p&gt;If you do not immediately see the effect, try focusing on one black dot. Mouse over it, wait a few seconds while keeping your focus, and then mouse out.&lt;/p&gt;
    &lt;head rend="h2"&gt;27 - Ouchi Illusion&lt;/head&gt;
    &lt;p&gt;This is a static image, but it gives the impression that the pattern inside the circle is moving sideways. This happens because our eyes are constantly making small movements, even when we are not aware of it.&lt;/p&gt;
    &lt;p&gt;If you cannot see the illusion, try slightly moving the screen (or your head) while looking just outside the circle.&lt;/p&gt;
    &lt;head rend="h2"&gt;28 - Orthogonal Dotted Lines Sway&lt;/head&gt;
    &lt;p&gt;When you look around this pattern, the central area appears to slide and sway, even though it is completely static. This illusion makes me dizzy... but that may also be because I had to stare at it for a long time while coding it.&lt;/p&gt;
    &lt;head rend="h2"&gt;29 - Enigma&lt;/head&gt;
    &lt;p&gt;This illusion is particularly interesting. There is a pink circle surrounded by concentric pink and purple rings. If you focus on the pink circle, the rings appear to spin or scintillate, as if there were some activity in them. Of course, nothing is actually moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;30 - Waves&lt;/head&gt;
    &lt;p&gt;This demo was challenging to code and takes a long time to load. Mainly because it uses a large number of conic gradients behind the scenes, which browsers struggle to render efficiently. There is probably a better way to implement it, but I have not explored that yet.&lt;/p&gt;
    &lt;p&gt;If you look closely at the illustration, you may notice wave-like motion. As with the previous illusions in this section, the image is entirely static.&lt;/p&gt;
    &lt;p&gt;Good news! There are more optical illusions below - but first, another warning.&lt;/p&gt;
    &lt;p&gt;ATTENTION: The following optical illusions actually move, and the illusion is created by motion itself. Some of them can be dizzying, so proceed accordingly.&lt;/p&gt;
    &lt;p&gt;(Leaving some blank space in case you do not want to continue.)&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;31 - Animated Ebbinghaus Illusion&lt;/head&gt;
    &lt;p&gt;Earlier, we saw two static versions of the Ebbinghaus illusion. This one is animated. The elements move side to side, and the surrounding shapes grow and shrink, giving the impression that the orange circle is changing size - when it definitely is not.&lt;/p&gt;
    &lt;head rend="h2"&gt;32 - Psychokinematic Tower&lt;/head&gt;
    &lt;p&gt;This looks like a three-dimensional tower spinning in space, as seen from above. In reality, it is a flat, two-dimensional image rotating.&lt;/p&gt;
    &lt;p&gt;Mouse over the demo to stop the rotation and the illusion of depth disappears entirely.&lt;/p&gt;
    &lt;head rend="h2"&gt;33 - Color Fan&lt;/head&gt;
    &lt;p&gt;This optical illusion requires only two gradients: a conic gradient for the fan-shaped arms and a radial gradient for the circles and discs.&lt;/p&gt;
    &lt;p&gt;If you focus on the black dot, the illustration may appear to develop a darker greenish or brownish border. However, the colors never change.&lt;/p&gt;
    &lt;head rend="h2"&gt;34 - Reverse Spoke Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is delightful and disorienting. While the background colors of the wheel are spinning, the spokes remain fixed. However, they appear to rotate in the opposite direction. In reality, only the background is moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;35 - Motion Binding&lt;/head&gt;
    &lt;p&gt;What do you see in this animation? Most people report two sets of lines operating independently: one moving horizontally and another moving vertically. And that is exactly how it looks.&lt;/p&gt;
    &lt;p&gt;In reality, it is a single shape moving uniformly. Run the demo, mouse over the lines, and the true motion will be revealed.&lt;/p&gt;
    &lt;head rend="h2"&gt;36 - Mainz-Linez Illusion&lt;/head&gt;
    &lt;p&gt;Focus on one of the red dots. You will notice it moves straight up and down along a vertical path. Now shift your focus to one of the black crosses in the center. Suddenly, the red dots appear to zigzag instead of moving straight.&lt;/p&gt;
    &lt;p&gt;The CSS code for the wavy lines is adapted from a Temani Afif snippet on CSS-Tricks and his wavy shape generator.&lt;/p&gt;
    &lt;head rend="h2"&gt;37 - Waddling Colors&lt;/head&gt;
    &lt;p&gt;It may look like the boxes are moving at different speeds or like a set of walking feet. In reality, all elements move at the same pace and in parallel. Mouse over the demo to reveal the effect.&lt;/p&gt;
    &lt;p&gt;The illusion also works when the "feet" move in circles, as shown in this alternative version:&lt;/p&gt;
    &lt;head rend="h2"&gt;38 - Dotted-Line Motion&lt;/head&gt;
    &lt;p&gt;Follow the red dot as it moves sideways. From the corner of your vision, it may appear that the dashed black-and-white lines are moving closer together (when the dot moves left) or farther apart (when it moves right). In reality, the lines are completely static.&lt;/p&gt;
    &lt;head rend="h2"&gt;39 - Contrast Asynchrony&lt;/head&gt;
    &lt;p&gt;These dots always have the same color. However, when placed against alternating backgrounds, they appear to jump or move out of sync because of how they blend with their surroundings.&lt;/p&gt;
    &lt;p&gt;Mouse over the demo to remove the background and the illusion disappears.&lt;/p&gt;
    &lt;head rend="h2"&gt;40 - Breathing Square&lt;/head&gt;
    &lt;p&gt;This illusion gives the impression that a blue square is growing and shrinking rhythmically, almost as if it were breathing or beating like a heart.&lt;/p&gt;
    &lt;p&gt;Although the image is rotating, its size never changes. Mouse over the illustration to remove the green boxes and reveal the rotating blue square.&lt;/p&gt;
    &lt;head rend="h2"&gt;41 - Troxler Fading&lt;/head&gt;
    &lt;p&gt;This illustration shows a circle made of pink dots, with one dot missing. Focus on the cross at the center and the missing dot will appear as a yellow or green dot, giving the impression that it is "eating" the pink dots. Just like Pac-Man.&lt;/p&gt;
    &lt;p&gt;I could have used CSS trigonometric functions to calculate the exact positions of the dots, but since they never change, I chose to hardcode the values instead.&lt;/p&gt;
    &lt;p&gt;Here is a related effect. Follow the light gray circle as it spins, and the darker circles will appear to change from gray to greenish. Focus on the cross at the center, and after a short time, the darker circles may begin to fade entirely.&lt;/p&gt;
    &lt;head rend="h2"&gt;42 - Pinna-Brelstaff Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is particularly dizzying. Follow the bluish dot as it moves from right to left and back again. It will appear as though parts of the tiled background are shifting, even though they are static. The only moving element is the dot.&lt;/p&gt;
    &lt;p&gt; From a CSS perspective, I coded the pattern using conic gradients, and applied it to the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt; pseudo-elements. I then flipped one upside down and clipped it.&lt;/p&gt;
    &lt;head rend="h2"&gt;43 - Palisade&lt;/head&gt;
    &lt;p&gt;The radii of a wheel, when viewed through a palisade, appear to curve. In reality, they are perfectly straight. Mouse over the demo to remove the palisade and you will see that the radii never bend.&lt;/p&gt;
    &lt;head rend="h2"&gt;44 - Alternative Motion&lt;/head&gt;
    &lt;p&gt;This animation demonstrates how our minds infer motion that may not actually be there. Consider the two blue dots. Different people perceive different movements: side to side, top to bottom, or even circular motion.&lt;/p&gt;
    &lt;p&gt;Cover the right side of the animation so that you see only one dot at a time. The motion now appears vertical. Cover the bottom part instead, and the motion appears horizontal. This is our brain trying to complete the movement.&lt;/p&gt;
    &lt;head rend="h2"&gt;45 - Motion Inversion&lt;/head&gt;
    &lt;p&gt;These two illustrations are identical - same shapes, same animation. The only difference is the CSS timing function.&lt;/p&gt;
    &lt;p&gt;The top animation moves smoothly from right to left. The bottom one appears to move choppily in the same direction, but if you focus on it, it may suddenly seem to reverse direction and move faster.&lt;/p&gt;
    &lt;p&gt;Most of the inspiration for these optical illusions came from two excellent resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"35 optical illusions and why they trick your brain" by Patrick Pester.&lt;/item&gt;
      &lt;item&gt;"154 Visual Phenomena &amp;amp; Optical Illusions" with explanations by Michael Bach&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also find this article on:&lt;/p&gt;
    &lt;p&gt;(You can leave comments on those platforms and I will reply there).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46722570</guid><pubDate>Thu, 22 Jan 2026 17:41:22 +0000</pubDate></item><item><title>I was banned from Claude for scaffolding a Claude.md file?</title><link>https://hugodaniel.com/posts/claude-code-banned-me/</link><description>&lt;doc fingerprint="c61ded10eca0acfa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Why does SSH send 100 packets per keystroke?&lt;/head&gt;
    &lt;p&gt;And why do I care?&lt;/p&gt;
    &lt;p&gt;Jan 22, 2026&lt;/p&gt;
    &lt;p&gt;Here are a few lines of summarized &lt;code&gt;tcpdump&lt;/code&gt; output for an ssh session where I send a single keystroke:&lt;/p&gt;
    &lt;code&gt;$ ./first_lines_of_pcap.sh single-key.pcap
  1   0.000s  CLIENT-&amp;gt;SERVER   36 bytes
  2   0.007s  SERVER-&amp;gt;CLIENT  564 bytes
  3   0.015s  CLIENT-&amp;gt;SERVER    0 bytes
  4   0.015s  CLIENT-&amp;gt;SERVER   36 bytes
  5   0.015s  SERVER-&amp;gt;CLIENT   36 bytes
  6   0.026s  CLIENT-&amp;gt;SERVER    0 bytes
  7   0.036s  CLIENT-&amp;gt;SERVER   36 bytes
  8   0.036s  SERVER-&amp;gt;CLIENT   36 bytes
  9   0.046s  CLIENT-&amp;gt;SERVER    0 bytes
 10   0.059s  CLIENT-&amp;gt;SERVER   36 bytes
&lt;/code&gt;
    &lt;p&gt;I said a “few” because there are a lot of these lines.&lt;/p&gt;
    &lt;code&gt;$ ./summarize_pcap.sh single-key.pcap
Total packets: 270

  36-byte msgs:   179 packets ( 66.3%)   6444 bytes
  Other data:       1 packet  (  0.4%)    564 bytes
  TCP ACKs:        90 packets ( 33.3%)

  Data sent:      6444 bytes in 36-byte messages,  564 bytes in other data
  Ratio:          11.4x more data in 36-byte messages than other data

  Data packet rate: ~90 packets/second (avg 11.1 ms between data packets)
&lt;/code&gt;
    &lt;p&gt;That is a lot of packets for one keypress. What’s going on here? Why do I care?&lt;/p&gt;
    &lt;head class="sc-4d1d4ca-1 bowwWe"&gt;here's those scripts if you're curious&lt;/head&gt;
    &lt;code&gt;# first_lines_of_pcap.sh
tshark -r "$1" \
  -T fields -e frame.number -e frame.time_relative -e ip.src -e ip.dst -e tcp.len | \
  awk 'NR&amp;lt;=10 {dir = ($3 ~ /71\.190/ ? "CLIENT-&amp;gt;SERVER" : "SERVER-&amp;gt;CLIENT");
       printf "%3d  %6.3fs  %-4s  %3s bytes\n", $1, $2, dir, $5}'
&lt;/code&gt;
    &lt;code&gt;# summarize_pcap.sh
tshark -r "$1" -Y "frame.time_relative &amp;lt;= 2.0" -T fields -e frame.time_relative -e tcp.len | awk '
  {
      count++
      payload = $2

      if (payload == 0) {
          acks++
      } else if (payload == 36) {
          mystery++
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      } else {
          game_data++
          game_bytes = payload
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      }
  }
  END {
      print "Total packets:", count
      print ""
      printf "  36-byte msgs:   %3d packets (%5.1f%%)  %5d bytes\n", mystery, 100*mystery/count, mystery*36
      printf "  Other data:     %3d packet  (%5.1f%%)  %5d bytes\n", game_data, 100*game_data/count, game_bytes
      printf "  TCP ACKs:       %3d packets (%5.1f%%)\n", acks, 100*acks/count
      print ""
      printf "  Data sent:      %d bytes in 36-byte messages,  %d bytes in other data\n", mystery*36, game_bytes
      printf "  Ratio:          %.1fx more data in 36-byte messages than other data\n", (mystery*36)/game_bytes
      print ""
      avg_ms = (sum_data_deltas / data_intervals) * 1000
      printf "  Data packet rate: ~%d packets/second (avg %.1f ms between data packets)\n", int(1000/avg_ms + 0.5), avg_ms
  }'
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discovery&lt;/head&gt;
    &lt;p&gt;I am working on a high-performance game that runs over ssh. The TUI for the game is created in bubbletea 1 and sent over ssh via wish.&lt;/p&gt;
    &lt;p&gt;I have also forked bubbletea to make it faster. Stay tuned!&lt;/p&gt;
    &lt;p&gt;The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.&lt;/p&gt;
    &lt;p&gt;So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s outstanding profiling tools to look at what’s going on.&lt;/p&gt;
    &lt;p&gt;Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.&lt;/p&gt;
    &lt;p&gt;At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.&lt;/p&gt;
    &lt;p&gt;But wait.&lt;/p&gt;
    &lt;p&gt;If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?&lt;/p&gt;
    &lt;head rend="h2"&gt;Investigation&lt;/head&gt;
    &lt;p&gt;As part of debugging the test harness issue, I used &lt;code&gt;tcpdump&lt;/code&gt; to log game traffic with and without the breaking change. Something like:&lt;/p&gt;
    &lt;code&gt;# The game runs on port 22
timeout 30s tcpdump -i eth0 'port 22' -w with-breaking-change.pcap
# Revert change
timeout 30s tcpdump -i eth0 'port 22' -w without-breaking-change.pcap
&lt;/code&gt;
    &lt;p&gt;Our breaking change stopped us from rendering our game over ssh. So &lt;code&gt;with-breaking-change.pcap&lt;/code&gt; contains packets that represent the overhead of each connection without actually rendering the game.&lt;/p&gt;
    &lt;p&gt;I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.&lt;/p&gt;
    &lt;code&gt;Wanna take a look yourself? I put with-breaking-change.pcap in this directory

--

Wow! Here's what I found:

Packet Size Distribution (413,703 total packets):
274,907 packets (66%): Exactly 36 bytes
138,778 packets (34%): 0 bytes (TCP ACKs)
18 packets (&amp;lt;0.1%): 72 bytes
&lt;/code&gt;
    &lt;p&gt;Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.&lt;/p&gt;
    &lt;p&gt;This was baffling to me (and to Claude Code). We kicked around several ideas like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSH flow control messages&lt;/item&gt;
      &lt;item&gt;PTY size polling or other status checks&lt;/item&gt;
      &lt;item&gt;Some quirk of bubbletea or wish&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One thing stood out - these exchanges were initiated by my ssh client (stock ssh installed on MacOS) - not by my server.&lt;/p&gt;
    &lt;p&gt;On a hunch, I took a &lt;code&gt;tcpdump&lt;/code&gt; of a regular ssh session.&lt;/p&gt;
    &lt;code&gt;# on my mac, in one tab
sudo tcpdump -ien0 'port 22'

# on my mac, in another tab
ssh $some_vm_of_mine
&lt;/code&gt;
    &lt;p&gt;I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the &lt;code&gt;tcpdump&lt;/code&gt; output.&lt;/p&gt;
    &lt;p&gt;I saw the exact same pattern! What in the world?&lt;/p&gt;
    &lt;head rend="h2"&gt;Root cause&lt;/head&gt;
    &lt;p&gt;Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;ssh -vvv&lt;/code&gt; gave me a pretty good sense of what was going on:&lt;/p&gt;
    &lt;code&gt;debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent) 
debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;20ms&lt;/code&gt; is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.&lt;/p&gt;
    &lt;p&gt;In 2023, ssh added keystroke timing obfuscation. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.&lt;/p&gt;
    &lt;p&gt;That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where latency is critical.&lt;/p&gt;
    &lt;head rend="h2"&gt;Remediation&lt;/head&gt;
    &lt;p&gt;Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass &lt;code&gt;ObscureKeystrokeTiming=no&lt;/code&gt; when starting up ssh sessions.&lt;/p&gt;
    &lt;p&gt;This worked great. CPU usage dropped dramatically and bots still received valid data.&lt;/p&gt;
    &lt;p&gt;But this is hardly a solution in the real world. I want &lt;code&gt;ssh mygame&lt;/code&gt; to Just Work without asking users to pass options that they might not understand.&lt;/p&gt;
    &lt;p&gt;Claude Code originally didn’t have much faith that we could disable this functionality server-side.&lt;/p&gt;
    &lt;p&gt;generated with simon wilson's excellent claude-code-transcripts tool&lt;/p&gt;
    &lt;p&gt;Fortunately, the description I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).&lt;/p&gt;
    &lt;code&gt;Log message:
Introduce a transport-level ping facility

This adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG
to implement a ping capability. These messages use numbers in the "local
extensions" number space and are advertised using a "[email protected]"
ext-info message with a string version number of "0".
&lt;/code&gt;
    &lt;p&gt;The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the &lt;code&gt;[email protected]&lt;/code&gt; extension. What if we just…don’t advertise &lt;code&gt;[email protected]&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;I searched go’s ssh library for &lt;code&gt;[email protected]&lt;/code&gt; and found the commit where support was added. The commit was tiny and seemed very easy to revert.&lt;/p&gt;
    &lt;p&gt;I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (go’s replace directive makes forking a library very easy).&lt;/p&gt;
    &lt;p&gt;Then I re-ran my test harness. The results were…very good:&lt;/p&gt;
    &lt;code&gt;Total CPU  29.90%          -&amp;gt; 11.64%
Syscalls   3.10s           -&amp;gt; 0.66s
Crypto     1.6s            -&amp;gt; 0.11s
Bandwidth  ~6.5 Mbit/sec   -&amp;gt; ~3 Mbit/sec
&lt;/code&gt;
    &lt;p&gt;Claude was also pretty pumped:&lt;/p&gt;
    &lt;p&gt;yes it's 1:30 am what of it&lt;/p&gt;
    &lt;p&gt;Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.&lt;/p&gt;
    &lt;p&gt;But this is a huge improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &amp;gt;50% drop was unimaginable to me.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging with LLMs was fun&lt;/head&gt;
    &lt;p&gt;I’ve been thinking about whether LLMs remove parts of the problem-solving process that I enjoy. But I’ve gotta say, debugging this problem using Claude Code was super fun.&lt;/p&gt;
    &lt;p&gt;I am familiar enough with &lt;code&gt;tcpdump&lt;/code&gt;, &lt;code&gt;tshark&lt;/code&gt;, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.&lt;/p&gt;
    &lt;p&gt;There were still edge cases. At some point in my confusion I switched to ChatGPT and it very confidently told me that my tcpdump output was normal ssh behavior:&lt;/p&gt;
    &lt;p&gt;do all chatgpt messages have this tone and formatting now?&lt;/p&gt;
    &lt;p&gt;And then doubled down when I pushed back:&lt;/p&gt;
    &lt;p&gt;no!!!&lt;/p&gt;
    &lt;p&gt;Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”&lt;/p&gt;
    &lt;p&gt;When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”&lt;/p&gt;
    &lt;p&gt;I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.&lt;/p&gt;
    &lt;p&gt;But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.&lt;/p&gt;
    &lt;p&gt;Besides. Being in the loop is fun. How else would I write this post?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46723384</guid><pubDate>Thu, 22 Jan 2026 18:38:27 +0000</pubDate></item><item><title>Why does SSH send 100 packets per keystroke?</title><link>https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/</link><description>&lt;doc fingerprint="c61ded10eca0acfa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Why does SSH send 100 packets per keystroke?&lt;/head&gt;
    &lt;p&gt;And why do I care?&lt;/p&gt;
    &lt;p&gt;Jan 22, 2026&lt;/p&gt;
    &lt;p&gt;Here are a few lines of summarized &lt;code&gt;tcpdump&lt;/code&gt; output for an ssh session where I send a single keystroke:&lt;/p&gt;
    &lt;code&gt;$ ./first_lines_of_pcap.sh single-key.pcap
  1   0.000s  CLIENT-&amp;gt;SERVER   36 bytes
  2   0.007s  SERVER-&amp;gt;CLIENT  564 bytes
  3   0.015s  CLIENT-&amp;gt;SERVER    0 bytes
  4   0.015s  CLIENT-&amp;gt;SERVER   36 bytes
  5   0.015s  SERVER-&amp;gt;CLIENT   36 bytes
  6   0.026s  CLIENT-&amp;gt;SERVER    0 bytes
  7   0.036s  CLIENT-&amp;gt;SERVER   36 bytes
  8   0.036s  SERVER-&amp;gt;CLIENT   36 bytes
  9   0.046s  CLIENT-&amp;gt;SERVER    0 bytes
 10   0.059s  CLIENT-&amp;gt;SERVER   36 bytes
&lt;/code&gt;
    &lt;p&gt;I said a “few” because there are a lot of these lines.&lt;/p&gt;
    &lt;code&gt;$ ./summarize_pcap.sh single-key.pcap
Total packets: 270

  36-byte msgs:   179 packets ( 66.3%)   6444 bytes
  Other data:       1 packet  (  0.4%)    564 bytes
  TCP ACKs:        90 packets ( 33.3%)

  Data sent:      6444 bytes in 36-byte messages,  564 bytes in other data
  Ratio:          11.4x more data in 36-byte messages than other data

  Data packet rate: ~90 packets/second (avg 11.1 ms between data packets)
&lt;/code&gt;
    &lt;p&gt;That is a lot of packets for one keypress. What’s going on here? Why do I care?&lt;/p&gt;
    &lt;head class="sc-4d1d4ca-1 bowwWe"&gt;here's those scripts if you're curious&lt;/head&gt;
    &lt;code&gt;# first_lines_of_pcap.sh
tshark -r "$1" \
  -T fields -e frame.number -e frame.time_relative -e ip.src -e ip.dst -e tcp.len | \
  awk 'NR&amp;lt;=10 {dir = ($3 ~ /71\.190/ ? "CLIENT-&amp;gt;SERVER" : "SERVER-&amp;gt;CLIENT");
       printf "%3d  %6.3fs  %-4s  %3s bytes\n", $1, $2, dir, $5}'
&lt;/code&gt;
    &lt;code&gt;# summarize_pcap.sh
tshark -r "$1" -Y "frame.time_relative &amp;lt;= 2.0" -T fields -e frame.time_relative -e tcp.len | awk '
  {
      count++
      payload = $2

      if (payload == 0) {
          acks++
      } else if (payload == 36) {
          mystery++
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      } else {
          game_data++
          game_bytes = payload
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      }
  }
  END {
      print "Total packets:", count
      print ""
      printf "  36-byte msgs:   %3d packets (%5.1f%%)  %5d bytes\n", mystery, 100*mystery/count, mystery*36
      printf "  Other data:     %3d packet  (%5.1f%%)  %5d bytes\n", game_data, 100*game_data/count, game_bytes
      printf "  TCP ACKs:       %3d packets (%5.1f%%)\n", acks, 100*acks/count
      print ""
      printf "  Data sent:      %d bytes in 36-byte messages,  %d bytes in other data\n", mystery*36, game_bytes
      printf "  Ratio:          %.1fx more data in 36-byte messages than other data\n", (mystery*36)/game_bytes
      print ""
      avg_ms = (sum_data_deltas / data_intervals) * 1000
      printf "  Data packet rate: ~%d packets/second (avg %.1f ms between data packets)\n", int(1000/avg_ms + 0.5), avg_ms
  }'
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discovery&lt;/head&gt;
    &lt;p&gt;I am working on a high-performance game that runs over ssh. The TUI for the game is created in bubbletea 1 and sent over ssh via wish.&lt;/p&gt;
    &lt;p&gt;I have also forked bubbletea to make it faster. Stay tuned!&lt;/p&gt;
    &lt;p&gt;The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.&lt;/p&gt;
    &lt;p&gt;So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s outstanding profiling tools to look at what’s going on.&lt;/p&gt;
    &lt;p&gt;Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.&lt;/p&gt;
    &lt;p&gt;At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.&lt;/p&gt;
    &lt;p&gt;But wait.&lt;/p&gt;
    &lt;p&gt;If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?&lt;/p&gt;
    &lt;head rend="h2"&gt;Investigation&lt;/head&gt;
    &lt;p&gt;As part of debugging the test harness issue, I used &lt;code&gt;tcpdump&lt;/code&gt; to log game traffic with and without the breaking change. Something like:&lt;/p&gt;
    &lt;code&gt;# The game runs on port 22
timeout 30s tcpdump -i eth0 'port 22' -w with-breaking-change.pcap
# Revert change
timeout 30s tcpdump -i eth0 'port 22' -w without-breaking-change.pcap
&lt;/code&gt;
    &lt;p&gt;Our breaking change stopped us from rendering our game over ssh. So &lt;code&gt;with-breaking-change.pcap&lt;/code&gt; contains packets that represent the overhead of each connection without actually rendering the game.&lt;/p&gt;
    &lt;p&gt;I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.&lt;/p&gt;
    &lt;code&gt;Wanna take a look yourself? I put with-breaking-change.pcap in this directory

--

Wow! Here's what I found:

Packet Size Distribution (413,703 total packets):
274,907 packets (66%): Exactly 36 bytes
138,778 packets (34%): 0 bytes (TCP ACKs)
18 packets (&amp;lt;0.1%): 72 bytes
&lt;/code&gt;
    &lt;p&gt;Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.&lt;/p&gt;
    &lt;p&gt;This was baffling to me (and to Claude Code). We kicked around several ideas like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSH flow control messages&lt;/item&gt;
      &lt;item&gt;PTY size polling or other status checks&lt;/item&gt;
      &lt;item&gt;Some quirk of bubbletea or wish&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One thing stood out - these exchanges were initiated by my ssh client (stock ssh installed on MacOS) - not by my server.&lt;/p&gt;
    &lt;p&gt;On a hunch, I took a &lt;code&gt;tcpdump&lt;/code&gt; of a regular ssh session.&lt;/p&gt;
    &lt;code&gt;# on my mac, in one tab
sudo tcpdump -ien0 'port 22'

# on my mac, in another tab
ssh $some_vm_of_mine
&lt;/code&gt;
    &lt;p&gt;I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the &lt;code&gt;tcpdump&lt;/code&gt; output.&lt;/p&gt;
    &lt;p&gt;I saw the exact same pattern! What in the world?&lt;/p&gt;
    &lt;head rend="h2"&gt;Root cause&lt;/head&gt;
    &lt;p&gt;Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;ssh -vvv&lt;/code&gt; gave me a pretty good sense of what was going on:&lt;/p&gt;
    &lt;code&gt;debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent) 
debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;20ms&lt;/code&gt; is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.&lt;/p&gt;
    &lt;p&gt;In 2023, ssh added keystroke timing obfuscation. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.&lt;/p&gt;
    &lt;p&gt;That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where latency is critical.&lt;/p&gt;
    &lt;head rend="h2"&gt;Remediation&lt;/head&gt;
    &lt;p&gt;Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass &lt;code&gt;ObscureKeystrokeTiming=no&lt;/code&gt; when starting up ssh sessions.&lt;/p&gt;
    &lt;p&gt;This worked great. CPU usage dropped dramatically and bots still received valid data.&lt;/p&gt;
    &lt;p&gt;But this is hardly a solution in the real world. I want &lt;code&gt;ssh mygame&lt;/code&gt; to Just Work without asking users to pass options that they might not understand.&lt;/p&gt;
    &lt;p&gt;Claude Code originally didn’t have much faith that we could disable this functionality server-side.&lt;/p&gt;
    &lt;p&gt;generated with simon wilson's excellent claude-code-transcripts tool&lt;/p&gt;
    &lt;p&gt;Fortunately, the description I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).&lt;/p&gt;
    &lt;code&gt;Log message:
Introduce a transport-level ping facility

This adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG
to implement a ping capability. These messages use numbers in the "local
extensions" number space and are advertised using a "[email protected]"
ext-info message with a string version number of "0".
&lt;/code&gt;
    &lt;p&gt;The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the &lt;code&gt;[email protected]&lt;/code&gt; extension. What if we just…don’t advertise &lt;code&gt;[email protected]&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;I searched go’s ssh library for &lt;code&gt;[email protected]&lt;/code&gt; and found the commit where support was added. The commit was tiny and seemed very easy to revert.&lt;/p&gt;
    &lt;p&gt;I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (go’s replace directive makes forking a library very easy).&lt;/p&gt;
    &lt;p&gt;Then I re-ran my test harness. The results were…very good:&lt;/p&gt;
    &lt;code&gt;Total CPU  29.90%          -&amp;gt; 11.64%
Syscalls   3.10s           -&amp;gt; 0.66s
Crypto     1.6s            -&amp;gt; 0.11s
Bandwidth  ~6.5 Mbit/sec   -&amp;gt; ~3 Mbit/sec
&lt;/code&gt;
    &lt;p&gt;Claude was also pretty pumped:&lt;/p&gt;
    &lt;p&gt;yes it's 1:30 am what of it&lt;/p&gt;
    &lt;p&gt;Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.&lt;/p&gt;
    &lt;p&gt;But this is a huge improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &amp;gt;50% drop was unimaginable to me.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging with LLMs was fun&lt;/head&gt;
    &lt;p&gt;I’ve been thinking about whether LLMs remove parts of the problem-solving process that I enjoy. But I’ve gotta say, debugging this problem using Claude Code was super fun.&lt;/p&gt;
    &lt;p&gt;I am familiar enough with &lt;code&gt;tcpdump&lt;/code&gt;, &lt;code&gt;tshark&lt;/code&gt;, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.&lt;/p&gt;
    &lt;p&gt;There were still edge cases. At some point in my confusion I switched to ChatGPT and it very confidently told me that my tcpdump output was normal ssh behavior:&lt;/p&gt;
    &lt;p&gt;do all chatgpt messages have this tone and formatting now?&lt;/p&gt;
    &lt;p&gt;And then doubled down when I pushed back:&lt;/p&gt;
    &lt;p&gt;no!!!&lt;/p&gt;
    &lt;p&gt;Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”&lt;/p&gt;
    &lt;p&gt;When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”&lt;/p&gt;
    &lt;p&gt;I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.&lt;/p&gt;
    &lt;p&gt;But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.&lt;/p&gt;
    &lt;p&gt;Besides. Being in the loop is fun. How else would I write this post?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46723990</guid><pubDate>Thu, 22 Jan 2026 19:27:32 +0000</pubDate></item><item><title>Capital One to acquire Brex for $5.15B</title><link>https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/</link><description>&lt;doc fingerprint="e1d8a53d0af29fa9"&gt;
  &lt;main&gt;
    &lt;p&gt;Jan 22 (Reuters) - Capital One Financial (COF.N) said on Thursday it will acquire fintech firm Brex in a cash and stock deal valued at $5.15 billion and reported a rise in quarterly profit on the back of higher interest income from its credit card debt.&lt;/p&gt;
    &lt;p&gt;Shares of the consumer lender fell more than 5% following the announcement of the deal, but robust results helped them pare losses to trade 1.5% lower.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;The move comes as dealmakers prepare for another strong year in 2026, with a record slate of transactions expected as executives pursue scale to navigate rising economic and geopolitical uncertainties.&lt;/p&gt;
    &lt;p&gt;The deal, which is expected to close in mid‑2026, will be carried out on an approximate 50-50 cash-stock basis, Capital One said.&lt;/p&gt;
    &lt;p&gt;Brex operates in corporate cards and expense management software used by firms such as DoorDash (DASH.O) and Robinhood (HOOD.O), which could give Capital One greater exposure and reduce its reliance on consumer credit, cushioning it against the impact of economic downturns.&lt;/p&gt;
    &lt;p&gt;Brex operates in more than 120 countries according to its website.&lt;/p&gt;
    &lt;p&gt;Capital One said the fintech firm's chief executive and founder, Pedro Franceschi, will remain at the helm following the transaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;FOURTH-QUARTER EARNINGS&lt;/head&gt;
    &lt;p&gt;U.S. consumer spending rose at a solid pace in November and October, suggesting the economy was on track for a third consecutive quarter of strong growth.&lt;/p&gt;
    &lt;p&gt;Economic momentum has been underpinned largely by resilient household demand as well as a narrowing trade deficit, with imports declining in response to President Donald Trump's broad tariff increases.&lt;/p&gt;
    &lt;p&gt;However, the tariffs have pushed up the prices of many goods, weighing unevenly across income groups.&lt;/p&gt;
    &lt;p&gt;Economists say spending strength is increasingly concentrated among higher-income households, while lower- and middle-income consumers have limited scope to switch to cheaper alternatives.&lt;/p&gt;
    &lt;p&gt;Capital One's net interest income — the difference between what it makes on loans and pays out on deposits — rose 54% to $12.47 billion in the fourth quarter from a year ago.&lt;/p&gt;
    &lt;p&gt;The McLean, Virginia-based company's net income available to common stockholders came in at $2.06 billion, or $3.26 per share, for the quarter, compared with $1.02 billion, or $2.67 per share, a year earlier.&lt;/p&gt;
    &lt;head rend="h2"&gt;CREDIT CARD CAP CONUNDRUM&lt;/head&gt;
    &lt;p&gt;Trump said last week he was calling for a one‑year cap on credit card interest rates at 10% starting January 20, but offered few details on how the proposal would be implemented or how companies would be compelled to comply.&lt;/p&gt;
    &lt;p&gt;Banking industry groups have pushed back against the proposal, warning it would restrict the availability of credit for everyday consumers.&lt;/p&gt;
    &lt;p&gt;JPMorgan Chase (JPM.N) CEO Jamie Dimon said on Wednesday a proposal to cap credit card interest rates would amount to economic disaster.&lt;/p&gt;
    &lt;p&gt;However, Bank of America (BAC.N) is considering options to offer new credit cards with an interest rate of 10% to satisfy Trump's demands, a source familiar with the matter said on Thursday.&lt;/p&gt;
    &lt;p&gt;The introduction of an interest rate cap would deal a significant blow to Capital One Financial, which has one of the most credit-card‑dependent business models among major U.S. lenders.&lt;/p&gt;
    &lt;p&gt;"We feel strongly that a cap on interest rates would catalyze a number of unintended consequences," CEO Richard Fairbank said in a call with analysts.&lt;/p&gt;
    &lt;p&gt;He added that lack of credit would result in reduced consumer spending and likely bring on a recession.&lt;/p&gt;
    &lt;p&gt;Reporting by Pritam Biswas in Bengaluru; Editing by Shreya Biswas&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46725288</guid><pubDate>Thu, 22 Jan 2026 21:23:12 +0000</pubDate></item><item><title>Scaling PostgreSQL to power 800M ChatGPT users</title><link>https://openai.com/index/scaling-postgresql/</link><description>&lt;doc fingerprint="9558893f469078c2"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Scaling PostgreSQL to power 800 million ChatGPT users&lt;/head&gt;&lt;p&gt;By Bohan Zhang, Member of the Technical Staff&lt;/p&gt;&lt;p&gt;For years, PostgreSQL has been one of the most critical, under-the-hood data systems powering core products like ChatGPT and OpenAI’s API. As our user base grows rapidly, the demands on our databases have increased exponentially, too. Over the past year, our PostgreSQL load has grown by more than 10x, and it continues to rise quickly.&lt;/p&gt;&lt;p&gt;Our efforts to advance our production infrastructure to sustain this growth revealed a new insight: PostgreSQL can be scaled to reliably support much larger read-heavy workloads than many previously thought possible. The system (initially created by a team of scientists at University of California, Berkeley) has enabled us to support massive global traffic with a single primary Azure PostgreSQL flexible server instance(opens in a new window) and nearly 50 read replicas spread over multiple regions globally. This is the story of how we’ve scaled PostgreSQL at OpenAI to support millions of queries per second for 800 million users through rigorous optimizations and solid engineering; we’ll also cover key takeaways we learned along the way.&lt;/p&gt;&lt;p&gt;After the launch of ChatGPT, traffic grew at an unprecedented rate. To support it, we rapidly implemented extensive optimizations at both the application and PostgreSQL database layers, scaled up by increasing the instance size, and scaled out by adding more read replicas. This architecture has served us well for a long time. With ongoing improvements, it continues to provide ample runway for future growth.&lt;/p&gt;&lt;p&gt;It may sound surprising that a single-primary architecture can meet the demands of OpenAI’s scale; however, making this work in practice isn’t simple. We’ve seen several SEVs caused by Postgres overload, and they often follow the same pattern: an upstream issue causes a sudden spike in database load, such as widespread cache misses from a caching-layer failure, a surge of expensive multi-way joins saturating CPU, or a write storm from a new feature launch. As resource utilization climbs, query latency rises and requests begin to time out. Retries then further amplify the load, triggering a vicious cycle with the potential to degrade the entire ChatGPT and API services.&lt;/p&gt;&lt;p&gt;Although PostgreSQL scales well for our read-heavy workloads, we still encounter challenges during periods of high write traffic. This is largely due to PostgreSQL’s multiversion concurrency control (MVCC) implementation, which makes it less efficient for write-heavy workloads. For example, when a query updates a tuple or even a single field, the entire row is copied to create a new version. Under heavy write loads, this results in significant write amplification. It also increases read amplification, since queries must scan through multiple tuple versions (dead tuples) to retrieve the latest one. MVCC introduces additional challenges such as table and index bloat, increased index maintenance overhead, and complex autovacuum tuning. (You can find a deep-dive on these issues in a blog I wrote with Prof. Andy Pavlo at Carnegie Mellon University called The Part of PostgreSQL We Hate the Most(opens in a new window), cited(opens in a new window) in the PostgreSQL Wikipedia page.)&lt;/p&gt;&lt;p&gt;To mitigate these limitations and reduce write pressure, we’ve migrated, and continue to migrate, shardable (i.e. workloads that can be horizontally partitioned), write-heavy workloads to sharded systems such as Azure Cosmos DB, optimizing application logic to minimize unnecessary writes. We also no longer allow adding new tables to the current PostgreSQL deployment. New workloads default to the sharded systems.&lt;/p&gt;&lt;p&gt;Even as our infrastructure has evolved, PostgreSQL has remained unsharded, with a single primary instance serving all writes. The primary rationale is that sharding existing application workloads would be highly complex and time-consuming, requiring changes to hundreds of application endpoints and potentially taking months or even years. Since our workloads are primarily read-heavy, and we’ve implemented extensive optimizations, the current architecture still provides ample headroom to support continued traffic growth. While we’re not ruling out sharding PostgreSQL in the future, it’s not a near-term priority given the sufficient runway we have for current and future growth.&lt;/p&gt;&lt;p&gt;In the following sections, we’ll dive into the challenges we faced and the extensive optimizations we implemented to address them and prevent future outages, pushing PostgreSQL to its limits and scaling it to millions of queries per second (QPS).&lt;/p&gt;&lt;p&gt;Challenge: With only one writer, a single-primary setup can’t scale writes. Heavy write spikes can quickly overload the primary and impact services like ChatGPT and our API.&lt;/p&gt;&lt;p&gt;Solution: We minimize load on the primary as much as possible—both reads and writes—to ensure it has sufficient capacity to handle write spikes. Read traffic is offloaded to replicas wherever possible. However, some read queries must remain on the primary because they’re part of write transactions. For those, we focus on ensuring they’re efficient and avoid slow queries. For write traffic, we’ve migrated shardable, write-heavy workloads to sharded systems such as Azure CosmosDB. Workloads that are harder to shard but still generate high write volume take longer to migrate, and that process is still ongoing. We also aggressively optimized our applications to reduce write load; for example, we’ve fixed application bugs that caused redundant writes and introduced lazy writes, where appropriate, to smooth traffic spikes. In addition, when backfilling table fields, we enforce strict rate limits to prevent excessive write pressure.&lt;/p&gt;&lt;p&gt;Challenge: We identified several expensive queries in PostgreSQL. In the past, sudden volume spikes in these queries would consume large amounts of CPU, slowing both ChatGPT and API requests.&lt;/p&gt;&lt;p&gt;Solution: A few expensive queries, such as those joining many tables together, can significantly degrade or even bring down the entire service. We need to continuously optimize PostgreSQL queries to ensure they’re efficient and avoid common Online Transaction Processing (OLTP) anti-patterns. For example, we once identified an extremely costly query that joined 12 tables, where spikes in this query were responsible for past high-severity SEVs. We should avoid complex multi-table joins whenever possible. If joins are necessary, we learned to consider breaking down the query and move complex join logic to the application layer instead. Many of these problematic queries are generated by Object-Relational Mapping frameworks (ORMs), so it’s important to carefully review the SQL they produce and ensure it behaves as expected. It’s also common to find long-running idle queries in PostgreSQL. Configuring timeouts like idle_in_transaction_session_timeout is essential to prevent them from blocking autovacuum.&lt;/p&gt;&lt;p&gt;Challenge: If a read replica goes down, traffic can still be routed to other replicas. However, relying on a single writer means having a single point of failure—if it goes down, the entire service is affected.&lt;/p&gt;&lt;p&gt;Solution: Most critical requests only involve read queries. To mitigate the single point of failure in the primary, we offloaded those reads from the writer to replicas, ensuring those requests can continue serving even if the primary goes down. While write operations would still fail, the impact is reduced; it’s no longer a SEV0 since reads remain available.&lt;/p&gt;&lt;p&gt;To mitigate primary failures, we run the primary in High-Availability (HA) mode with a hot standby, a continuously synchronized replica that is always ready to take over serving traffic. If the primary goes down or needs to be taken offline for maintenance, we can quickly promote the standby to minimize downtime. The Azure PostgreSQL team has done significant work to ensure these failovers remain safe and reliable even under very high load. To handle read replica failures, we deploy multiple replicas in each region with sufficient capacity headroom, ensuring that a single replica failure doesn’t lead to a regional outage.&lt;/p&gt;&lt;p&gt;Challenge: We often encounter situations where certain requests consume a disproportionate amount of resources on PostgreSQL instances. This can lead to degraded performance for other workloads running on the same instances. For example, a new feature launch can introduce inefficient queries that heavily consume PostgreSQL CPU, slowing down requests for other critical features.&lt;/p&gt;&lt;p&gt;Solution: To mitigate the “noisy neighbor” problem, we isolate workloads onto dedicated instances to ensure that sudden spikes in resource-intensive requests don’t impact other traffic. Specifically, we split requests into low-priority and high-priority tiers and route them to separate instances. This way, even if a low-priority workload becomes resource-intensive, it won’t degrade the performance of high-priority requests. We apply the same strategy across different products and services as well, so that activity from one product does not affect the performance or reliability of another.&lt;/p&gt;&lt;p&gt;Challenge: Each instance has a maximum connection limit (5,000 in Azure PostgreSQL). It’s easy to run out of connections or accumulate too many idle ones. We’ve previously had incidents caused by connection storms that exhausted all available connections.&lt;/p&gt;&lt;p&gt;Solution: We deployed PgBouncer as a proxy layer to pool database connections. Running it in statement or transaction pooling mode allows us to efficiently reuse connections, greatly reducing the number of active client connections. This also cuts connection setup latency: in our benchmarks, the average connection time dropped from 50 milliseconds (ms) to 5 ms. Inter-region connections and requests can be expensive, so we co-locate the proxy, clients, and replicas in the same region to minimize network overhead and connection use time. Moreover, PgBouncer must be configured carefully. Settings like idle timeouts are critical to prevent connection exhaustion.&lt;/p&gt;&lt;p&gt;Challenge: A sudden spike in cache misses can trigger a surge of reads on the PostgreSQL database, saturating CPU and slowing user requests.&lt;/p&gt;&lt;p&gt;Solution: To reduce read pressure on PostgreSQL, we use a caching layer to serve most of the read traffic. However, when cache hit rates drop unexpectedly, the burst of cache misses can push a large volume of requests directly to PostgreSQL. This sudden increase in database reads consumes significant resources, slowing down the service. To prevent overload during cache-miss storms, we implement a cache locking (and leasing) mechanism so that only a single reader that misses on a particular key fetches the data from PostgreSQL. When multiple requests miss on the same cache key, only one request acquires the lock and proceeds to retrieve the data and repopulate the cache. All other requests wait for the cache to be updated rather than all hitting PostgreSQL at once. This significantly reduces redundant database reads and protects the system from cascading load spikes.&lt;/p&gt;&lt;p&gt;Challenge: The primary streams Write Ahead Log (WAL) data to every read replica. As the number of replicas increases, the primary must ship WAL to more instances, increasing pressure on both network bandwidth and CPU. This causes higher and more unstable replica lag, which makes the system harder to scale reliably.&lt;/p&gt;&lt;p&gt;Solution: We operate nearly 50 read replicas across multiple geographic regions to minimize latency. However, with the current architecture, the primary must stream WAL to every replica. Although it currently scales well with very large instance types and high-network bandwidth, we can’t keep adding replicas indefinitely without eventually overloading the primary. To address this, we’re collaborating with the Azure PostgreSQL team on cascading replication(opens in a new window), where intermediate replicas relay WAL to downstream replicas. This approach allows us to scale to potentially over a hundred replicas without overwhelming the primary. However, it also introduces additional operational complexity, particularly around failover management. The feature is still in testing; we’ll ensure it’s robust and can fail over safely before rolling it out to production.&lt;/p&gt;&lt;p&gt;Challenge: A sudden traffic spike on specific endpoints, a surge of expensive queries, or a retry storm can quickly exhaust critical resources such as CPU, I/O, and connections, which causes widespread service degradation.&lt;/p&gt;&lt;p&gt;Solution: We implemented rate-limiting across multiple layers—application, connection pooler, proxy, and query—to prevent sudden traffic spikes from overwhelming database instances and triggering cascading failures. It’s also crucial to avoid overly short retry intervals, which can trigger retry storms. We also enhanced the ORM layer to support rate limiting and when necessary, fully block specific query digests. This targeted form of load shedding enables rapid recovery from sudden surges of expensive queries.&lt;/p&gt;&lt;p&gt;Challenge: Even a small schema change, such as altering a column type, can trigger a full table rewrite(opens in a new window). We therefore apply schema changes cautiously—limiting them to lightweight operations and avoiding any that rewrite entire tables.&lt;/p&gt;&lt;p&gt;Solution: Only lightweight schema changes are permitted, such as adding or removing certain columns that do not trigger a full table rewrite. We enforce a strict 5-second timeout on schema changes. Creating and dropping indexes concurrently is allowed. Schema changes are restricted to existing tables. If a new feature requires additional tables, they must be in alternative sharded systems such as Azure CosmosDB rather than PostgreSQL. When backfilling a table field, we apply strict rate limits to prevent write spikes. Although this process can sometimes take over a week, it ensures stability and avoids any production impact.&lt;/p&gt;&lt;p&gt;This effort demonstrates that with the right design and optimizations, Azure PostgreSQL can be scaled to handle the largest production workloads. PostgreSQL handles millions of QPS for read-heavy workloads, powering OpenAI’s most critical products like ChatGPT and the API platform. We added nearly 50 read replicas, while keeping replication lag near zero, maintained low-latency reads across geo-distributed regions, and built sufficient capacity headroom to support future growth.&lt;/p&gt;&lt;p&gt;This scaling works while still minimizing latency and improving reliability. We consistently deliver low double-digit millisecond p99 client-side latency and five-nines availability in production. And over the past 12 months, we’ve had only one SEV-0 PostgreSQL incident (it occurred during the viral launch(opens in a new window) of ChatGPT ImageGen, when write traffic suddenly surged by more than 10x as over 100 million new users signed up within a week.)&lt;/p&gt;&lt;p&gt;While we’re happy with how far PostgreSQL has taken us, we continue to push its limits to ensure we have sufficient runway for future growth. We’ve already migrated the shardable write-heavy workloads to our sharded systems like CosmosDB. The remaining write-heavy workloads are more challenging to shard—we’re actively migrating those as well to further offload writes from the PostgreSQL primary. We’re also working with Azure to enable cascading replication so we can safely scale to significantly more read replicas.&lt;/p&gt;&lt;p&gt;Looking ahead, we’ll continue to explore additional approaches to further scale, including sharded PostgreSQL or alternative distributed systems, as our infrastructure demands continue to grow.&lt;/p&gt;&lt;head rend="h2"&gt;Author&lt;/head&gt;Bohan Zhang&lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;&lt;p&gt;Special thanks to Jon Lee, Sicheng Liu, Chaomin Yu, and Chenglong Hao, who contributed to this post, and to the entire team that helped scale PostgreSQL. We’d also like to thank the Azure PostgreSQL team for their strong partnership.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46725300</guid><pubDate>Thu, 22 Jan 2026 21:24:23 +0000</pubDate></item><item><title>Improving the usability of C libraries in Swift</title><link>https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/</link><description>&lt;doc fingerprint="6e025913b407b699"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Improving the usability of C libraries in Swift&lt;/head&gt;
    &lt;p&gt;There are many interesting, useful, and fun C libraries in the software ecosystem. While one could go and rewrite these libraries in Swift, usually there is no need, because Swift provides direct interoperability with C. With a little setup, you can directly use existing C libraries from your Swift code.&lt;/p&gt;
    &lt;p&gt;When you use a C library directly from Swift, it will look and feel similar to using it from C. That can be useful if you’re following sample code or a tutorial written in C, but it can also feel out of place. For example, here’s a small amount of code using a C API:&lt;/p&gt;
    &lt;code&gt;  var instanceDescriptor = WGPUInstanceDescriptor()
  let instance = wgpuCreateInstance(&amp;amp;instanceDescriptor)
  var surfaceDescriptor = WGPUSurfaceDescriptor()
  let surface = wgpuInstanceCreateSurface(instance, &amp;amp;surfaceDescriptor)
  if wgpuSurfacePresent(&amp;amp;surface) == WGPUStatus_Error {
      // report error
  }
  wgpuSurfaceRelease(surface)
  wgpuInstanceRelease(instance)
&lt;/code&gt;
    &lt;p&gt;The C library here that Swift is using comes from the webgpu-headers project, which vends a C header (&lt;code&gt;webgpu.h&lt;/code&gt;) that is used by several implementations of WebGPU. WebGPU  is a technology that enables web developers to use the system’s GPU (Graphics Processing Unit) from the browser. For the purposes of this post, you don’t really need to know anything about WebGPU: I’m using it as an example of a typical C library, and the techniques described in this blog post apply to lots of other well-designed C libraries.&lt;/p&gt;
    &lt;p&gt;The Swift code above has a very “C” feel to it. It has global function calls with prefixed names like &lt;code&gt;wgpuInstanceCreateSurface&lt;/code&gt; and global integer constants like &lt;code&gt;WGPUStatus_Error&lt;/code&gt;. It pervasively uses unsafe pointers, some of which are managed with explicit reference counting, where the user provides calls to &lt;code&gt;wpuXYZAddRef&lt;/code&gt; and &lt;code&gt;wgpuXYZRelease&lt;/code&gt; functions. It works, but it doesn’t feel like Swift, and inherits various safety problems of C.&lt;/p&gt;
    &lt;p&gt;Fortunately, we can improve this situation, providing a safer and more ergonomic interface to WebGPU from Swift that feels like it belongs in Swift. More importantly, we can do so without changing the WebGPU implementation: Swift provides a suite of annotations that you can apply to C headers to improve the way in which the C APIs are expressed in Swift. These annotations describe common conventions in C that match up with Swift constructs, projecting a more Swift-friendly interface on top of the C code.&lt;/p&gt;
    &lt;p&gt;In this post, I’m going to use these annotations to improve how Swift interacts with the WebGPU C code. By the end, we’ll be able to take advantage of Swift features like argument labels, methods, enums, and automatic reference counting, like this:&lt;/p&gt;
    &lt;code&gt;  var instanceDescriptor = WGPUInstanceDescriptor()
  let instance = WGPUInstance(descriptor: &amp;amp;instanceDescriptor)
  var surfaceDescriptor = WGPUSurfaceDescriptor()
  let surface = instance.createSurface(descriptor: &amp;amp;surfaceDescriptor)
  if surface.present() == .error {
      // report error
  }
  // Swift automatically deallocates the instance and surface when we're done
&lt;/code&gt;
    &lt;p&gt;These same annotations can be used for any C library to provide a safer, more ergonomic development experience in Swift without changing the C library at all.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Some of what is covered in this post requires bug fixes that first became available in Swift 6.2.3.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Setup: Writing a module map&lt;/head&gt;
    &lt;p&gt;A module map is a way of layering a Swift-friendly modular structure on top of C headers. You can create a module map for the WebGPU header by writing the following to a file &lt;code&gt;module.modulemap&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;module WebGPU {
  header "webgpu.h"
  export *
}
&lt;/code&gt;
    &lt;p&gt;The easiest thing to do is to put &lt;code&gt;module.modulemap&lt;/code&gt; alongside the header itself. For my experiment here, I put it in the root directory of my &lt;code&gt;webgpu-headers&lt;/code&gt; checkout. If you’re in a Swift package, put it into its own target with this layout:&lt;/p&gt;
    &lt;code&gt;├── Package.swift
└── Sources
    └── WebGPU
        ├── include
        │   ├── webgpu.h
        │   └── module.modulemap
        └── WebGPU.c (empty file)
&lt;/code&gt;
    &lt;p&gt;If you reference this &lt;code&gt;WebGPU&lt;/code&gt; target from elsewhere in the package, you can &lt;code&gt;import WebGPU&lt;/code&gt; to get access to the C APIs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Seeing the results&lt;/head&gt;
    &lt;p&gt;There are a few ways to see what the Swift interface for a C library looks like.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;swift-synthesize-interface&lt;/code&gt;tool in Swift 6.2+ prints the Swift interface to the terminal.&lt;/item&gt;
      &lt;item&gt;Xcode’s “Swift 5 interface” counterpart to the &lt;code&gt;webgpu.h&lt;/code&gt;header will show how the header has been mapped into Swift.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s do it from the command line, using &lt;code&gt;swift-synthesize-interface&lt;/code&gt;. From the directory containing &lt;code&gt;webgpu.h&lt;/code&gt; and &lt;code&gt;module.modulemap&lt;/code&gt;, run:&lt;/p&gt;
    &lt;code&gt;xcrun swift-synthesize-interface -I . -module-name WebGPU -target arm64-apple-macos15 -sdk /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX26.0.sdk
&lt;/code&gt;
    &lt;p&gt;The leading &lt;code&gt;xcrun&lt;/code&gt; and the &lt;code&gt;-sdk&lt;/code&gt; argument with the path is only needed on macOS; on other platforms, make sure &lt;code&gt;swift-synthesize-interface&lt;/code&gt; is in your path. The &lt;code&gt;-target&lt;/code&gt; operation is the triple provided if you run &lt;code&gt;swiftc -print-target-info&lt;/code&gt;. It looks like this:&lt;/p&gt;
    &lt;code&gt;{
  "compilerVersion": "Apple Swift version 6.2 (swiftlang-6.2.2.15.4 clang-1700.3.15.2)",
  "target": {
    "triple": "arm64-apple-macosx15.0",
    "unversionedTriple": "arm64-apple-macosx",
    "moduleTriple": "arm64-apple-macos",
    "compatibilityLibraries": [ ],
    "librariesRequireRPath": false
  },
  "paths": { ... }
}
&lt;/code&gt;
    &lt;p&gt;The output of &lt;code&gt;swift-synthesize-interface&lt;/code&gt; is the Swift API for the WebGPU module, directly translated from C. For example, this code from the WebGPU header:&lt;/p&gt;
    &lt;code&gt;typedef enum WGPUAdapterType {
    WGPUAdapterType_DiscreteGPU = 0x00000001,
    WGPUAdapterType_IntegratedGPU = 0x00000002,
    WGPUAdapterType_CPU = 0x00000003,
    WGPUAdapterType_Unknown = 0x00000004,
    WGPUAdapterType_Force32 = 0x7FFFFFFF
} WGPUAdapterType WGPU_ENUM_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;is translated into:&lt;/p&gt;
    &lt;code&gt;public struct WGPUAdapterType : Hashable, Equatable, RawRepresentable {
    public init(_ rawValue: UInt32)
    public init(rawValue: UInt32)
    public var rawValue: UInt32
}

public var WGPUAdapterType_DiscreteGPU: WGPUAdapterType { get }
public var WGPUAdapterType_IntegratedGPU: WGPUAdapterType { get }
public var WGPUAdapterType_CPU: WGPUAdapterType { get }
public var WGPUAdapterType_Unknown: WGPUAdapterType { get }
public var WGPUAdapterType_Force32: WGPUAdapterType { get }
&lt;/code&gt;
    &lt;p&gt;and there are lots of global functions like this:&lt;/p&gt;
    &lt;code&gt;public func wgpuComputePipelineGetBindGroupLayout(_ computePipeline: WGPUComputePipeline!, _ groupIndex: UInt32) -&amp;gt; WGPUBindGroupLayout!
public func wgpuComputePipelineSetLabel(_ computePipeline: WGPUComputePipeline!, _ label: WGPUStringView)
public func wgpuComputePipelineAddRef(_ computePipeline: WGPUComputePipeline!)
public func wgpuComputePipelineRelease(_ computePipeline: WGPUComputePipeline!)
&lt;/code&gt;
    &lt;p&gt;It’s a starting point! You can absolutely write Swift programs using these WebGPU APIs, and they’ll feel a lot like writing them in C. Let’s see what we can do to make it better.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cleaning up enumeration types&lt;/head&gt;
    &lt;p&gt;C enums can be used for several things. Sometimes they really represent a choice among a number of alternatives. Sometimes they represent flags in a set of options, from which you can choose several. Sometimes they’re just a convenient way to create a bunch of named constants. Swift conservatively imports enum types as wrappers over the underlying C type used to store values of the enum (e.g., &lt;code&gt;WGPUAdapterType&lt;/code&gt; wraps a &lt;code&gt;UInt32&lt;/code&gt;) and makes the enumerators into global constants. It covers all of the possible use cases, but it isn’t nice.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;WGPUAdapterType&lt;/code&gt; enum really is a choice among one of several options, which would be best represented as an &lt;code&gt;enum&lt;/code&gt; in Swift. If we were willing to modify the header, we could apply the &lt;code&gt;enum_extensibility&lt;/code&gt; attribute to the enum, like this:&lt;/p&gt;
    &lt;code&gt;typedef enum __attribute__((enum_extensibility(closed))) WGPUAdapterType {
    WGPUAdapterType_DiscreteGPU = 0x00000001,
    WGPUAdapterType_IntegratedGPU = 0x00000002,
    WGPUAdapterType_CPU = 0x00000003,
    WGPUAdapterType_Unknown = 0x00000004,
    WGPUAdapterType_Force32 = 0x7FFFFFFF
} WGPUAdapterType WGPU_ENUM_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;This works, and results in a much nicer Swift API:&lt;/p&gt;
    &lt;code&gt;@frozen public enum WGPUAdapterType : UInt32, @unchecked Sendable {
    case discreteGPU = 1
    case integratedGPU = 2
    case CPU = 3
    case unknown = 4
    case force32 = 2147483647
}
&lt;/code&gt;
    &lt;p&gt;Now, we get an &lt;code&gt;enum&lt;/code&gt; that we can switch over, and nice short case names, e.g.,&lt;/p&gt;
    &lt;code&gt;switch adapterType {
  case .discreteGPU, .integratedGPU:
    print("definitely a GPU")
  default:
    print("not so sure")
}
&lt;/code&gt;
    &lt;p&gt;That’s great, but I already broke my rule: no header modifications unless I have to!&lt;/p&gt;
    &lt;head rend="h2"&gt;API notes&lt;/head&gt;
    &lt;p&gt;The problem of needing to layer information on top of existing C headers is not a new one. As noted earlier, Swift relies on a Clang feature called API notes to let us express this same information in a separate file, so we don’t have to edit the header. In this case, we create a file called &lt;code&gt;WebGPU.apinotes&lt;/code&gt; (the name &lt;code&gt;WebGPU&lt;/code&gt; matches the module name from &lt;code&gt;module.modulemap&lt;/code&gt;), which is a YAML file describing the extra information. We’ll start with one that turns &lt;code&gt;WGPUAdapterType&lt;/code&gt; into an &lt;code&gt;enum&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;---
Name: WebGPU
Tags:
- Name: WGPUAdapterType
  EnumExtensibility: closed
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Tags&lt;/code&gt; here is a term used in the C and C++ standard to refer to enum, struct, union, or class types. Any information about those types in the API notes file will go into that section.&lt;/p&gt;
    &lt;p&gt;Put &lt;code&gt;WebGPU.apinotes&lt;/code&gt; alongside the &lt;code&gt;module.modulemap&lt;/code&gt;, and now &lt;code&gt;WGPUAdapterType&lt;/code&gt; gets mapped into a &lt;code&gt;Swift&lt;/code&gt; enum. For a package, the structure will look like this:&lt;/p&gt;
    &lt;code&gt;├── Package.swift
└── Sources
    └── WebGPU
        ├── include
        │   ├── webgpu.h
        │   ├── WebGPU.apinotes
        │   └── module.modulemap
        └── WebGPU.c (empty file)
&lt;/code&gt;
    &lt;p&gt;We’ll be adding more to this API notes file as we keep digging through the interface.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reference-counted object types&lt;/head&gt;
    &lt;p&gt;The WebGPU header has a number of “object” types that are defined like this:&lt;/p&gt;
    &lt;code&gt;typedef struct WGPUBindGroupImpl* WGPUBindGroup WGPU_OBJECT_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;This gets imported into Swift as an alias for an opaque pointer type, which is… not great:&lt;/p&gt;
    &lt;code&gt;public typealias WGPUBindGroup = OpaquePointer
&lt;/code&gt;
    &lt;p&gt;WebGPU object types are reference counted, and each object type has corresponding &lt;code&gt;AddRef&lt;/code&gt; and &lt;code&gt;Release&lt;/code&gt; functions to increment and decrement the reference count, like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT void wgpuBindGroupAddRef(WGPUBindGroup bindGroup) WGPU_FUNCTION_ATTRIBUTE;
WGPU_EXPORT void wgpuBindGroupRelease(WGPUBindGroup bindGroup) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;Of course, you can use these functions in Swift exactly how you do in C, making sure to balance out calls to &lt;code&gt;AddRef&lt;/code&gt; and &lt;code&gt;Release&lt;/code&gt;, but then it would be every bit as unsafe as C.&lt;/p&gt;
    &lt;p&gt;We can do better with &lt;code&gt;SWIFT_SHARED_REFERENCE&lt;/code&gt;. It’s a macro (defined in the &lt;code&gt;&amp;lt;swift/bridging&amp;gt;&lt;/code&gt; header) that can turn a reference-counted C type like the above into an automatically reference-counted &lt;code&gt;class&lt;/code&gt; in Swift. Here’s how we would use it in the header:&lt;/p&gt;
    &lt;code&gt;typedef struct SWIFT_SHARED_REFERENCE(wgpuBindGroupAddRef, wgpuBindGroupRelease) WGPUBindGroupImpl* WGPUBindGroup WGPU_OBJECT_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;Now, &lt;code&gt;WGPUBindGroup&lt;/code&gt; gets imported like this:&lt;/p&gt;
    &lt;code&gt;public class WGPUBindGroupImpl { }
public typealias WGPUBindGroup = WGPUBindGroupImpl
&lt;/code&gt;
    &lt;p&gt;The extra typealias is a little unexpected, but overall this is a huge improvement: Swift is treating &lt;code&gt;WGPUBindGroup&lt;/code&gt; as a class, meaning that it automatically manages retains and releases for you! This is both an ergonomic win (less code to write) and a safety win, because it’s eliminated the possibility of mismanaging these instances.&lt;/p&gt;
    &lt;p&gt;There’s one more thing: when dealing with reference-counting APIs, you need to know whether a particular function that returns an object is expecting you to call “release” when you’re done. In the WebGPU header, this information is embedded in a comment:&lt;/p&gt;
    &lt;code&gt;/**
 * @returns
 * This value is @ref ReturnedWithOwnership.
 */
WGPU_EXPORT WGPUBindGroup wgpuDeviceCreateBindGroup(WGPUDevice device, WGPUBindGroupDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;“ReturnedWithOwnership” here means that the result of the call has already been retained one extra time, and the caller is responsible for calling “release” when they are done with it. The &lt;code&gt;&amp;lt;swift/bridging&amp;gt;&lt;/code&gt; header has a &lt;code&gt;SWIFT_RETURNS_RETAINED&lt;/code&gt; macro that expresses this notion. One can use it like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT WGPUBindGroup wgpuDeviceCreateBindGroup(WGPUDevice device, WGPUBindGroupDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE SWIFT_RETURNS_RETAINED;
&lt;/code&gt;
    &lt;p&gt;Now, Swift will balance out the retain that &lt;code&gt;wgpuDeviceCreateBindGroup&lt;/code&gt; has promised to do by performing the extra release once you’re done using the object. Once these annotations are done, we’re all set with a more ergonomic and memory-safe API for this C library. There’s no need to ever call &lt;code&gt;wgpuBindGroupRelease&lt;/code&gt; or &lt;code&gt;wgpuBindGroupAddRef&lt;/code&gt; yourself.&lt;/p&gt;
    &lt;p&gt;We’ve hacked up our header again, so let’s undo that and move all of this out to API notes. To turn a type into a foreign reference type, we augment the &lt;code&gt;Tags&lt;/code&gt; section of our API notes with the same information, but in YAML form:&lt;/p&gt;
    &lt;code&gt;- Name: WGPUBindGroupImpl
  SwiftImportAs: reference
  SwiftReleaseOp: wgpuBindGroupRelease
  SwiftRetainOp: wgpuBindGroupAddRef
&lt;/code&gt;
    &lt;p&gt;That makes &lt;code&gt;WGPUBindGroupImpl&lt;/code&gt; import as a class type, with the given retain and release functions. We can express the “returns retained” behavior of the &lt;code&gt;wgpuDeviceCreateBindGroup&lt;/code&gt; function like this:&lt;/p&gt;
    &lt;code&gt;Functions:
- Name: wgpuDeviceCreateBindGroup
  SwiftReturnOwnership: retained
&lt;/code&gt;
    &lt;p&gt;That’s enums and classes, so now let’s tackle… functions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Importing functions&lt;/head&gt;
    &lt;p&gt;A typical function from &lt;code&gt;webgpu.h&lt;/code&gt;, like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT void wgpuQueueWriteBuffer(
    WGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset, 
    void const * data, size_t size
) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;will come into Swift like this:&lt;/p&gt;
    &lt;code&gt;public func wgpuQueueWriteBuffer(_ queue: WGPUQueue!, _ buffer: WGPUBuffer!, _ bufferOffset: UInt64, _ data: UnsafeRawPointer!, _ size: Int)
&lt;/code&gt;
    &lt;p&gt;Note that &lt;code&gt;_&lt;/code&gt; on each parameter, which means that we won’t use argument labels for anything when we call it:&lt;/p&gt;
    &lt;code&gt;wgpuQueueWriteBuffer(myQueue, buffer, position, dataToWrite, bytesToWrite)
&lt;/code&gt;
    &lt;p&gt;That matches C, but it isn’t as clear as it could be in Swift. Let’s clean this up by providing a better name in Swift that includes argument labels. We can do so using &lt;code&gt;SWIFT_NAME&lt;/code&gt; (also in &lt;code&gt;&amp;lt;swift/bridging&amp;gt;&lt;/code&gt;), like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT void wgpuQueueWriteBuffer(
      WGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset,
      void const * data, size_t size
  ) WGPU_FUNCTION_ATTRIBUTE 
    SWIFT_NAME("wgpuQueueWriteBuffer(_:buffer:bufferOffset:data:size:)");
&lt;/code&gt;
    &lt;p&gt;Within the parentheses, we have each of the argument labels that we want (or &lt;code&gt;_&lt;/code&gt; meaning “no label”), each followed by a &lt;code&gt;:&lt;/code&gt;. This is how one describes a full function name in Swift. Once we’ve made this change to the Swift name, the C function comes into Swift with argument labels, like this:&lt;/p&gt;
    &lt;code&gt;public func wgpuQueueWriteBuffer(_ queue: WGPUQueue!, buffer: WGPUBuffer!, bufferOffset: UInt64, data: UnsafeRawPointer!, size: Int)
&lt;/code&gt;
    &lt;p&gt;That makes the call site more clear and self-documenting:&lt;/p&gt;
    &lt;code&gt;wgpuQueueWriteBuffer(myQueue, buffer: buffer, offset: position, data: dataToWrite, size: bytesToWrite)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Importing functions as methods&lt;/head&gt;
    &lt;p&gt;There is more usable structure in this API. Note that the &lt;code&gt;wgpuQueueWriteBuffer&lt;/code&gt; function takes, as its first argument, an instance of &lt;code&gt;WGPUQueue&lt;/code&gt;. Most of the C functions in &lt;code&gt;WebGPU.h&lt;/code&gt; are like this, because these are effectively functions that operate on their first argument. In a language that has methods, they would be methods. Swift has methods, so let’s make them methods!&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT void wgpuQueueWriteBuffer(
      WGPUQueue queue, WGPUBuffer buffer, uint64_t bufferOffset, void const * data, size_t size) 
  WGPU_FUNCTION_ATTRIBUTE SWIFT_NAME("WGPUQueueImpl.writeBuffer(self:buffer:bufferOffset:data:size:)");
&lt;/code&gt;
    &lt;p&gt;There are three things to notice about this &lt;code&gt;SWIFT_NAME&lt;/code&gt; string:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It starts with &lt;code&gt;WGPUQueueImpl.&lt;/code&gt;, which tells Swift to make this function a member inside&lt;code&gt;WGPUQueueImpl&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Let’s change the function name to &lt;code&gt;writeBuffer&lt;/code&gt;, because we no longer need the&lt;code&gt;wgpuQueue&lt;/code&gt;prefix to distinguish it from other “write buffer” operations on other types.&lt;/item&gt;
      &lt;item&gt;The name of the first argument in parentheses is &lt;code&gt;self&lt;/code&gt;, which indicates that the&lt;code&gt;self&lt;/code&gt;argument (in Swift) should be passed as that positional argument to the C function. The other arguments are passed in-order.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that this also requires &lt;code&gt;WGPUQueue(Impl)&lt;/code&gt; to be imported as a &lt;code&gt;class&lt;/code&gt;, as we did earlier for &lt;code&gt;WGPUBindGroupImpl&lt;/code&gt;. Once we’ve done so, we get a much-nicer Swift API:&lt;/p&gt;
    &lt;code&gt;extension WGPUQueueImpl {
  /**
   * Produces a @ref DeviceError both content-timeline (`size` alignment) and d
evice-timeline
   * errors defined by the WebGPU specification.
   */
  public func writeBuffer(buffer: WGPUBuffer!, bufferOffset: UInt64, data: UnsafeRawPointer!, size: Int)
}
&lt;/code&gt;
    &lt;p&gt;We’ve hacked up the header again, but didn’t have to. In &lt;code&gt;WebGPU.apinotes&lt;/code&gt;, you can put a &lt;code&gt;SwiftName&lt;/code&gt; attribute on any entity. For &lt;code&gt;wgpuQueueWriteBuffer&lt;/code&gt;, it would look like this (in the &lt;code&gt;Functions&lt;/code&gt; section):&lt;/p&gt;
    &lt;code&gt;- Name: wgpuQueueWriteBuffer
  SwiftName: WGPUQueueImpl.writeBuffer(self:buffer:bufferOffset:data:size:)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Importing functions as properties&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;WebGPU.h&lt;/code&gt; has a number of &lt;code&gt;Get&lt;/code&gt; functions that produce information about some aspect of a type. Here are two for the &lt;code&gt;WGPUQuerySet&lt;/code&gt; type:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT uint32_t wgpuQuerySetGetCount(WGPUQuerySet querySet) WGPU_FUNCTION_ATTRIBUTE;
WGPU_EXPORT WGPUQueryType wgpuQuerySetGetType(WGPUQuerySet querySet) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;With the &lt;code&gt;SWIFT_NAME&lt;/code&gt; tricks above, we can turn these into “get” methods on &lt;code&gt;WGPUQuerySet&lt;/code&gt;, like this:&lt;/p&gt;
    &lt;code&gt;extension WGPUQuerySetImpl {
    public func getCount() -&amp;gt; UInt32
    public func getType() -&amp;gt; WGPUQueryType
}
&lt;/code&gt;
    &lt;p&gt;That’s okay, but it’s not what you’d do in Swift. Let’s go one step further and turn them into read-only computed properties. To do so, use the &lt;code&gt;getter:&lt;/code&gt; prefix on the Swift name we define. We’ll skip ahead to the YAML form that goes into API notes:&lt;/p&gt;
    &lt;code&gt;- Name: wgpuQuerySetGetCount
  SwiftName: getter:WGPUQuerySetImpl.count(self:)
- Name: wgpuQuerySetGetType
  SwiftName: getter:WGPUQuerySetImpl.type(self:)
&lt;/code&gt;
    &lt;p&gt;And now, we arrive at a nice Swift API:&lt;/p&gt;
    &lt;code&gt;extension WGPUQuerySetImpl {
    public var count: UInt32 { get }
    public var type: WGPUQueryType { get }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Importing functions as initializers&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;SWIFT_NAME&lt;/code&gt; can also be used to import a function that returns a new instance as a Swift initializer. For example, this function creates a new &lt;code&gt;WGPUInstance&lt;/code&gt; (which we assume is getting imported as a &lt;code&gt;class&lt;/code&gt; like we’ve been doing above):&lt;/p&gt;
    &lt;code&gt;/**
 * Create a WGPUInstance
 *
 * @returns
 * This value is @ref ReturnedWithOwnership.
 */
WGPU_EXPORT WGPUInstance wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;We can turn this into a Swift initializer, which is used to create a new object, using the same &lt;code&gt;SWIFT_NAME&lt;/code&gt; syntax but where the method name is &lt;code&gt;init&lt;/code&gt;. Here is the YAML form that goes into API notes:&lt;/p&gt;
    &lt;code&gt;- Name: wgpuCreateInstance
  SwiftReturnOwnership: retained
  SwiftName: WGPUInstanceImpl.init(descriptor:)
&lt;/code&gt;
    &lt;p&gt;and here is the resulting Swift initializer:&lt;/p&gt;
    &lt;code&gt;extension WGPUInstanceImpl {
    /**
     * Create a WGPUInstance
     *
     * @returns
     * This value is @ref ReturnedWithOwnership.
     */
    public /*not inherited*/ init!(descriptor: UnsafePointer&amp;lt;WGPUInstanceDescriptor&amp;gt;!)
}
&lt;/code&gt;
    &lt;p&gt;Now, one can create a new &lt;code&gt;WGPUInstance&lt;/code&gt; with the normal object-creation syntax, e.g.,&lt;/p&gt;
    &lt;code&gt;let instance = WGPUInstance(descriptor: myDescriptor)
&lt;/code&gt;
    &lt;head rend="h2"&gt;Another Boolean type?&lt;/head&gt;
    &lt;p&gt;The WebGPU header defines its own Boolean type. I wish everyone would use C99’s &lt;code&gt;_Bool&lt;/code&gt; and be done with it, but alas, here are the definitions for WebGPUs Boolean types:&lt;/p&gt;
    &lt;code&gt;#define WGPU_TRUE (UINT32_C(1))
#define WGPU_FALSE (UINT32_C(0))
typedef uint32_t WGPUBool;
&lt;/code&gt;
    &lt;p&gt;This means that &lt;code&gt;WGPUBool&lt;/code&gt; will come in to Swift as a &lt;code&gt;UInt32&lt;/code&gt;. The two macros aren’t available in Swift at all: they’re “too complicated” to be recognized as integral constants. Even if they were available in Swift, it still wouldn’t be great because we want to use &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;false&lt;/code&gt; for Boolean values in Swift, not &lt;code&gt;WGPU_TRUE&lt;/code&gt; and &lt;code&gt;WGPU_FALSE&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To make &lt;code&gt;WGPUBool&lt;/code&gt; easier to use from Swift, we’re first going to map that typedef to its own &lt;code&gt;struct&lt;/code&gt; that stores the underlying &lt;code&gt;UInt32&lt;/code&gt;, giving it an identity separate from &lt;code&gt;UInt32&lt;/code&gt;. We can do this using a &lt;code&gt;SwiftWrapper&lt;/code&gt; API note within the &lt;code&gt;Typedefs&lt;/code&gt; section of the file, like this:&lt;/p&gt;
    &lt;code&gt;- Name: WGPUBool
  SwiftWrapper: struct
&lt;/code&gt;
    &lt;p&gt;Now, we get &lt;code&gt;WGPUBool&lt;/code&gt; imported like this:&lt;/p&gt;
    &lt;code&gt;public struct WGPUBool : Hashable, Equatable, RawRepresentable {
    public init(_ rawValue: UInt32)
    public init(rawValue: UInt32)
}
&lt;/code&gt;
    &lt;p&gt;To be able to use &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;false&lt;/code&gt; literals with this new &lt;code&gt;WGPUBool&lt;/code&gt;, we can write a little bit of Swift code that makes this type conform to the &lt;code&gt;ExpressibleByBooleanLiteral&lt;/code&gt; protocol, like this:&lt;/p&gt;
    &lt;code&gt;extension WGPUBool: ExpressibleByBooleanLiteral {
  init(booleanLiteral value: Bool) {
    self.init(rawValue: value ? 1 : 0)
  }
}
&lt;/code&gt;
    &lt;p&gt;That’s it! Better type safety (you cannot confuse a &lt;code&gt;WGPUBool&lt;/code&gt; with any other integer value) and the convenience of Boolean literals in Swift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Option sets&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;webgpu.h&lt;/code&gt; describes a set of flags using a &lt;code&gt;typedef&lt;/code&gt; of the &lt;code&gt;WGPUFlags&lt;/code&gt; type (a 64-bit unsigned integer) along with a set of global constants for the different flag values. For example, here is the &lt;code&gt;WGPUBufferUsage&lt;/code&gt; flag type and some of its constants:&lt;/p&gt;
    &lt;code&gt;typedef WGPUFlags WGPUBufferUsage;
static const WGPUBufferUsage WGPUBufferUsage_MapRead = 0x0000000000000001;
static const WGPUBufferUsage WGPUBufferUsage_MapWrite = 0x0000000000000002;
static const WGPUBufferUsage WGPUBufferUsage_CopySrc = 0x0000000000000004;
static const WGPUBufferUsage WGPUBufferUsage_Index = 0x0000000000000010;
&lt;/code&gt;
    &lt;p&gt;Similar to what we saw with &lt;code&gt;WGPUBool&lt;/code&gt;, &lt;code&gt;WGPUBufferUsage&lt;/code&gt; is a &lt;code&gt;typedef&lt;/code&gt; of a &lt;code&gt;typedef&lt;/code&gt; of a &lt;code&gt;uint64_t&lt;/code&gt;. There’s no type safety in this C API, and one could easily mix up these flags with, say, those of &lt;code&gt;WGPUMapMode&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;typedef WGPUFlags WGPUMapMode;
static const WGPUMapMode WGPUMapMode_Read = 0x0000000000000001;
static const WGPUMapMode WGPUMapMode_Write = 0x0000000000000002;
&lt;/code&gt;
    &lt;p&gt;We can do better, by layering more structure for the Swift version of this API using the same &lt;code&gt;SwiftWrapper&lt;/code&gt; approach from &lt;code&gt;WGPUBool&lt;/code&gt;. This goes into the &lt;code&gt;Typedefs&lt;/code&gt; section of API notes:&lt;/p&gt;
    &lt;code&gt;Typedefs:
- Name: WGPUBufferUsage
  SwiftWrapper: struct
&lt;/code&gt;
    &lt;p&gt;Now, &lt;code&gt;WGPUBufferUsage&lt;/code&gt; comes in as its own &lt;code&gt;struct&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;public struct WGPUBufferUsage : Hashable, Equatable, RawRepresentable {
    public init(_ rawValue: WGPUFlags)
    public init(rawValue: WGPUFlags)
}
&lt;/code&gt;
    &lt;p&gt;The initializers let you create a &lt;code&gt;WGPUBufferUsage&lt;/code&gt; from a &lt;code&gt;WGPUFlags&lt;/code&gt; value, and there is also a &lt;code&gt;rawValue&lt;/code&gt; property to get a &lt;code&gt;WGPUFlags&lt;/code&gt; value out of a &lt;code&gt;WGPUBufferInstance&lt;/code&gt;, so the raw value is always there… but the default is to be type safe. Additionally, those global constants will come in as members of &lt;code&gt;WGPUBufferUsage&lt;/code&gt;, like this:&lt;/p&gt;
    &lt;code&gt;extension WGPUBufferUsage {
    /**
     * The buffer can be *mapped* on the CPU side in *read* mode (using @ref WGPUMapMode_Read).
     */
    public static var _MapRead: WGPUBufferUsage { get }

    /**
     * The buffer can be *mapped* on the CPU side in *write* mode (using @ref WGPUMapMode_Write).
     *
     * @note This usage is **not** required to set `mappedAtCreation` to `true` in @ref WGPUBufferDescriptor.
     */
    public static var _MapWrite: WGPUBufferUsage { get }

    /**
     * The buffer can be used as the *source* of a GPU-side copy operation.
     */
    public static var _CopySrc: WGPUBufferUsage { get }

    /**
     * The buffer can be used as the *destination* of a GPU-side copy operation.
     */
    public static var _CopyDst: WGPUBufferUsage { get }
}
&lt;/code&gt;
    &lt;p&gt;This means that, if you’re passing a value of type &lt;code&gt;WPUBufferUsage&lt;/code&gt;, you can use the shorthand “leading dot” syntax. For example:&lt;/p&gt;
    &lt;code&gt;func setBufferUsage(_ usage: WGPUBufferUsage) { ... }

setBufferUsage(._MapRead)
&lt;/code&gt;
    &lt;p&gt;Swift has dropped the common &lt;code&gt;WPUBufferUsage&lt;/code&gt; prefix from the constants when it made them into members. However, the resulting names aren’t great. We can rename them by providing a &lt;code&gt;SwiftName&lt;/code&gt; in the API notes file within the &lt;code&gt;Globals&lt;/code&gt; section:&lt;/p&gt;
    &lt;code&gt;Globals:
- Name: WGPUBufferUsage_MapRead
  SwiftName: WGPUBufferUsage.mapRead
- Name: WGPUBufferUsage_MapWrite
  SwiftName: WGPUBufferUsage.mapWrite
&lt;/code&gt;
    &lt;p&gt;We can go one step further by making the &lt;code&gt;WGPUBufferUsage&lt;/code&gt; type conform to Swift’s &lt;code&gt;OptionSet&lt;/code&gt; protocol. If we revise the API notes like this:&lt;/p&gt;
    &lt;code&gt;Typedefs:
- Name: WGPUBufferUsage
  SwiftWrapper: struct
  SwiftConformsTo: Swift.OptionSet
&lt;/code&gt;
    &lt;p&gt;Now, we get the nice option-set syntax we expect in Swift:&lt;/p&gt;
    &lt;code&gt;let usageFlags: WGPUBufferUsage = [.mapRead, .mapWrite]
&lt;/code&gt;
    &lt;head rend="h2"&gt;Nullability&lt;/head&gt;
    &lt;p&gt;Throughout &lt;code&gt;webgpu.h&lt;/code&gt;, the &lt;code&gt;WGPU_NULLABLE&lt;/code&gt; macro is used to indicate pointers that can be NULL. The implication is that any pointer that is not marked with &lt;code&gt;WGPU_NULLABLE&lt;/code&gt; cannot be NULL. For example, here is the definition of &lt;code&gt;wgpuCreateInstance&lt;/code&gt; we used above:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT WGPUInstance wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * descriptor) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;WGPU_NULLABLE&lt;/code&gt; indicates that it’s acceptable to pass a NULL pointer in as the &lt;code&gt;descriptor&lt;/code&gt; parameter. Clang already has nullability specifiers to express this information. We could alter the declaration in the header to express that this parameter is nullable but the result type is never NULL, like this:&lt;/p&gt;
    &lt;code&gt;WGPU_EXPORT WGPUInstance _Nonnull wgpuCreateInstance(WGPU_NULLABLE WGPUInstanceDescriptor const * _Nullable descriptor) WGPU_FUNCTION_ATTRIBUTE;
&lt;/code&gt;
    &lt;p&gt;This eliminates the implicitly-unwrapped optionals (&lt;code&gt;!&lt;/code&gt;) from the signature of the initializer, so we end up with one that explicitly accepts a &lt;code&gt;nil&lt;/code&gt; descriptor argument and always returns a new instance (never &lt;code&gt;nil&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;extension WGPUInstanceImpl {
    /**
     * Create a WGPUInstance
     *
     * @returns
     * This value is @ref ReturnedWithOwnership.
     */
    public /*not inherited*/ init(descriptor: UnsafePointer&amp;lt;WGPUInstanceDescriptor&amp;gt;?)
}
&lt;/code&gt;
    &lt;p&gt;Now, I did cheat by hacking the header. Instead, we can express this with API notes on the parameters and result type by extending the entry we already have for &lt;code&gt;wgpuCreateInstance&lt;/code&gt; like this:&lt;/p&gt;
    &lt;code&gt;- Name: wgpuCreateInstance
  SwiftReturnOwnership: retained
  SwiftName: WGPUInstanceImpl.init(descriptor:)
  Parameters:
  - Position: 0
    Nullability: O
  ResultType: "WGPUInstance _Nonnull"
&lt;/code&gt;
    &lt;p&gt;To specific nullability of pointer parameters, one can identify them by position (where 0 is the first parameter to the function) and then specify whether the parameter should come into Swift as optional (&lt;code&gt;O&lt;/code&gt;, corresponds to &lt;code&gt;_Nullable&lt;/code&gt;), non-optional (&lt;code&gt;N&lt;/code&gt;, corresponds to &lt;code&gt;_Nonnull&lt;/code&gt;) or by left unspecified as an implicitly-unwrapped optional (&lt;code&gt;U&lt;/code&gt;, corresponds to &lt;code&gt;_Null_unspecified&lt;/code&gt;). For the result type, it’s a little different: we specified the result type along with the nullability specifier, i.e., &lt;code&gt;WGPUInstance _Nonnull&lt;/code&gt;. The end result of these annotations is the same as the modified header, so we can layer nullability information on top of the header.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scripting the creation of &lt;code&gt;WebGPU.apinotes&lt;/code&gt; WebGPU.apinotes section" href="#scripting-the-creation-of-webgpuapinotes"&amp;gt;
             
          &lt;/head&gt;
    &lt;p&gt;&lt;code&gt;webgpu.h&lt;/code&gt; is about 6,400 lines long, and is regenerated from a database of the API as needed. Each of the WebGPU implementations seems to augment or tweak the header a bit. So, rather than grind through and manually do annotations, I wrote a little Swift script to “parse” &lt;code&gt;webgpu.h&lt;/code&gt;, identify its patterns, and generate &lt;code&gt;WebGPU.apinotes&lt;/code&gt; for most of what is discussed in this post. The entirety of the script is here. It reads &lt;code&gt;webgpu.h&lt;/code&gt; from standard input and prints &lt;code&gt;WebGPU.apinotes&lt;/code&gt; to standard output.&lt;/p&gt;
    &lt;p&gt;Because &lt;code&gt;webgpu.h&lt;/code&gt; is generated, it has a very regular structure that we can pick up on via regular expressions. For example:&lt;/p&gt;
    &lt;code&gt;// Enum definitions, marked by WGPU_ENUM_ATTRIBUTE.
let enumMatcher = /} (?&amp;lt;name&amp;gt;\w+?) WGPU_ENUM_ATTRIBUTE/

// Object definitions, marked by WGPU_OBJECT_ATTRIBUTE.
let objectMatcher = /typedef struct (?&amp;lt;implName&amp;gt;\w+?)\* (?&amp;lt;name&amp;gt;\w+?) WGPU_OBJECT_ATTRIBUTE;/

// Function declarations, marked by WGPU_FUNCTION_ATTRIBUTE
let functionMatcher = /WGPU_EXPORT (?&amp;lt;nullableResult&amp;gt;WGPU_NULLABLE ?)?(?&amp;lt;resultType&amp;gt;\w+?) (?&amp;lt;name&amp;gt;\w+?)\((?&amp;lt;parameters&amp;gt;.*\)?) WGPU_FUNCTION_ATTRIBUTE;/
let parameterMatcher = /(?&amp;lt;type&amp;gt;[^),]+?) (?&amp;lt;name&amp;gt;\w+?)[),]/
&lt;/code&gt;
    &lt;p&gt;That’s enough to identify all of the enum types (so we can emit the &lt;code&gt;EnumExtensibility: closed&lt;/code&gt; API notes), object types (to turn them into shared references), and functions (which get nicer names and such). The script is just a big &lt;code&gt;readLine&lt;/code&gt; loop that applies the regexes to capture all of the various types and functions, then does some quick classification before printing out the API notes. The resulting API notes are in WebGPU.apinotes, and the generated Swift interface after these API notes are applied is here. You can run it with, e.g.,&lt;/p&gt;
    &lt;code&gt;swift -enable-bare-slash-regex webgpu_apinotes.swift &amp;lt; webgpu.h
&lt;/code&gt;
    &lt;p&gt;This script full of regular expressions is, admittedly, a bit of a hack. A better approach for an arbitrary C header would be to use &lt;code&gt;libclang&lt;/code&gt; to properly parse the headers. For WebGPU specifically, the webgpu-headers project contains a database from which the header is generated, and one could also generate API notes directly from that header. Regardless of how you get there, many C libraries have well-structured headers with conventions that can be leveraged to create safer, more ergonomic projections in Swift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Swiftifying your favorite C library&lt;/head&gt;
    &lt;p&gt;The techniques described in this post can be applied to just about any C library. To do so, I recommend setting up a small package like the one described here for WebGPU, so you can iterate quickly on example code to get a feel for how the Swift projection of the C API will work. The annotations might not get you all the way to the best Swift API, but they are a lightweight way to get most of the way there. Feel free to also extend the C types to convenience APIs that make sense in Swift, like I did above to make &lt;code&gt;WGPUBool&lt;/code&gt; conform to &lt;code&gt;ExpressibleByBooleanLiteral&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A little bit of annotation work on your favorite C library can make for a safer, more ergonomic, more Swifty experience of working with that library.&lt;/p&gt;
    &lt;head rend="h2"&gt;Postscript: Thoughts for improving the generated webgpu.h&lt;/head&gt;
    &lt;p&gt;The regular structure of &lt;code&gt;webgpu.h&lt;/code&gt; helped considerably when trying to expose the API nicely in Swift. That said, there are a few ways in which &lt;code&gt;webgpu.h&lt;/code&gt; could be improved to require less annotation for this purpose:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;WGPU_ENUM_ATTRIBUTE&lt;/code&gt;would be slightly nicer if placed on the&lt;code&gt;enum&lt;/code&gt;itself, rather than on the&lt;code&gt;typedef&lt;/code&gt;. If it were there, we could use&lt;code&gt;#define WGPU_ENUM_ATTRIBUTE __attribute__((enum_extensibility(closed)))&lt;/code&gt;&lt;p&gt;and not have to generate any API notes to bring these types in as proper enums in Swift.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;WGPU_OBJECT_ATTRIBUTE&lt;/code&gt;could provide the names of the retain and release operations and be placed on the&lt;code&gt;struct&lt;/code&gt;itself. If it were there, we could use&lt;code&gt;#define WGPU_OBJECT_ATTRIBUTE(RetainFn,ReleaseFn) SWIFT_SHARED_REFERENCE(RetainFn,ReleaseFn)&lt;/code&gt;&lt;p&gt;and not have to generate any API notes to bring these types in as classes in Swift.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;WGPU_NULLABLE&lt;/code&gt;could be placed on the pointer itself (i.e., after the&lt;code&gt;*&lt;/code&gt;) rather than at the beginning of the type, to match the position of Clang’s nullability attributes. If it were placed there, then&lt;code&gt;#define WGPU_NULLABLE _Nullable&lt;/code&gt;&lt;p&gt;would work with Clangs’ longstanding nullable-types support. Swift would then import such pointers as optional types (with&lt;/p&gt;&lt;code&gt;?&lt;/code&gt;). Moreover, if some macros&lt;code&gt;WGPU_ASSUME_NONNULL_BEGIN&lt;/code&gt;and&lt;code&gt;WGPU_ASSUME_NONNULL_END&lt;/code&gt;were placed at the beginning and end of the header, they could be mapped to Clang’s pragmas to assume that any pointer not marked “nullable” is always non-null:&lt;code&gt;#define WGPU_ASSUME_NONNULL_BEGIN #pragma clang assume_nonnull begin #define WGPU_ASSUME_NONNULL_END #pragma clang assume_nonnull end&lt;/code&gt;&lt;p&gt;This would eliminate all of the implicitly unwrapped optionals (marked&lt;/p&gt;&lt;code&gt;!&lt;/code&gt;in the Swift interface), making it easier to use safely.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46726526</guid><pubDate>Thu, 22 Jan 2026 23:34:44 +0000</pubDate></item><item><title>Why medieval city-builder video games are historically inaccurate (2020)</title><link>https://www.leidenmedievalistsblog.nl/articles/why-medieval-city-builder-video-games-are-historically-inaccurate</link><description>&lt;doc fingerprint="b302597347b4065b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why medieval city-builder video games are historically inaccurate&lt;/head&gt;
    &lt;p&gt;This blog post explores the historical accuracy of medieval city-builder video games.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Since many of us are working from home in these trying times, it seems safe to assume that more people than ever are indulging in playing the occasional computer game. A city builder is a specific kind of computer game in which you design a city, extract resources, set up production chains and ensure that your settlement grows. City builders are very similar to strategy games as they reward patience and strategy. In this article, I will take a look at one sub-genre of the city builder, the medieval city builder, and explain how this gaming genre relates to our knowledge of medieval settlement planning.&lt;/p&gt;
    &lt;head rend="h3"&gt;Historical city builders&lt;/head&gt;
    &lt;p&gt;The city builder has its origins far back in the 1990s in the combination of the strategy genre and the management genre, leading to games such as Sim City (1989), Caesar (1992) and Age of Empires (1997).&lt;/p&gt;
    &lt;p&gt;It did not take long before medieval-themed city builders popped up. We may think of Settlers (1993) and Knights and Merchants (1998). In addition, the Anno games (1998-2019), although initially set in the 1600s basically had a medieval theme.&lt;/p&gt;
    &lt;p&gt;These games often start with plopping down a village center on a promising location near abundant resources. You then continue to gather these resources which grant you building materials for building new homes and facilities for your settlement.&lt;/p&gt;
    &lt;p&gt;Setting up specialized production chains might involve growing grain, milling the grain for flour and turning the flour into bread which feeds your villages. Similarly, another production chain might involve rearing sheep for their wool, turning the wool into cloth and turning the cloth into clothing. When done correctly, the reward of correct investments and planning is that you see your settlement grow.&lt;/p&gt;
    &lt;p&gt;This often leads to settlements growing organically from a couple of houses around a community center to a larger settlement with hundreds of people. However logical such an organic growth of a settlement might seem, it is not historically accurate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Medieval village life &lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;Any gameplay loop that tells a story of linear settlement growth is incongruent with how a medieval economy worked (see Foussier 2004). Medieval villagers were often living on the edge of subsistence. Agricultural surpluses were skimmed by the church and the feudal lords. Bad harvests, banditry, warfare and disease might decimate a village community at any time. For this very reason, the demography of many European villages remained relatively stable between the twelfth and the eighteenth century. It may therefore be clear that the gameplay loop of city builders pivots around the concept of doing the historically exceptional (i.e. growing a settlement to a town) and thereby strays far from what actually happened in the lives of our medieval forebears. &lt;lb/&gt;A notable exception to this genre trope is the game Banished (2014) in which high mortality rates and bad weather do seriously stifle any kind of linear growth. In this city builder you are constantly fighting the odds and settlement growth is not guaranteed. However, also in Banished it is your goal to overcome the stagnation and lead your settlement to expansion.&lt;/p&gt;
    &lt;p&gt;A thing that is rarely touched upon in medieval city builders is how complex village life actually was. This can be exemplified by how the community related to its overlords. Land ownership here is key. Land in the community might be owned by a lord, a local liegeman, a monastery or even directly by the duke or count. Taxes, rents and tithes were the organisational structures in which the landowner was tied to the farmers who worked the fields. Often the payment of taxes and tithes was linked to feast days and the visit of the tax collector represented a big event in the agricultural year. An interesting side note is that some obligations which the commoners had to the lord and the church (such as seigneurial duties like working a mill) might drain the community from the needed manpower for tilling the land. Furthermore, a rural community that was its own seigneury had access to a law court with sheriff, aldermen and a local militia (Middle Dutch schutterie) to fight off bandits with. Harsh capital punishments were set in place to deter anyone from raiding the farms and hamlets and the village gallows were often the first thing one saw when approaching a medieval settlement.&lt;/p&gt;
    &lt;head rend="h3"&gt;Planning a medieval settlement&lt;/head&gt;
    &lt;p&gt;But something that is much more fundamental to the theme of a settlement building game, is how medieval settlements were actually planned and grew. Landscape historians and archaeologists have acquired a lot of insight into how this worked.&lt;/p&gt;
    &lt;p&gt;Let's start with the realization that medieval settlements in their first stages of development were planned and laid out according to a specific design. In my own research into the settlement history of West-Brabant (southern Netherlands, from 1000 to 1300 CE) I have encountered the following types.&lt;/p&gt;
    &lt;p&gt;Here is a sketch of a Brabantine circular manor (Middle Dutch vroonhoeve). This is a reinforced circular homestead with moat, often next to a bend in the river, containing several farms and a fan-like plot pattern radiating out from it. Such manors were often called BORCH.&lt;/p&gt;
    &lt;p&gt;Here is a sketch of a Brabantine street settlement, often built with exploitation of nearby fenland in mind. It consists of a line of farms with associated evenly sized rectangular plots built in a line perpendicular to a raised road.&lt;/p&gt;
    &lt;p&gt;Here is a more complex exploitation village which is set up with a moated enclosed church homestead and a central meadow as its center. There is a line of farms next to the road. The arable land to the east is bordered by a ditch supplying fresh drinking water (Middle Dutch bansloot). In layout, this type represents a hybrid between the two earlier settlement types.&lt;/p&gt;
    &lt;p&gt;Let us first make clear that these different types of exploitation settlements often existed alongside each other and can be found in one and the same region. In part, the different types reflect different chronological layers but some types were also more suited to certain geographical environments than others.&lt;/p&gt;
    &lt;p&gt;So how were these settlements planned? Many medieval exploitation enterprises were initiated by a monastery or a consortium of free men who were granted permission by (or bought permission from) the feudal lord to “colonize” the wilderness.&lt;/p&gt;
    &lt;p&gt;Clearing the wooded landscape in order to create arable land was done by cutting away the trees and bushes (Middle Dutch rode) or, alternatively, burning it away in controlled fires (Middle Dutch brant).&lt;/p&gt;
    &lt;p&gt;Land surveyors sent by the lord would then measure out the block or strips that would be taken in cultivation. Strips of arable land were often 1250m deep (6 Middle Dutch voorlingen = furlongs) so that the plough could go straight in a long line before having to turn. Important blocks or strips were demarcated by hedges, earthwork, woodwork, ditches or roads. Medieval names for these blocks often survived into the modern day.&lt;/p&gt;
    &lt;p&gt;The presence of drinking water (a river or a brook) in the vicinity was an important factor in choosing the location for the settlement. The vicinity of water entailed risk and reward because flooding was an ever present danger. Floods could devastate arable land but might also fertilize it. Meadows in particular were often situated in flood areas.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managing a settlement&lt;/head&gt;
    &lt;p&gt;So how was such a settlement managed? First of all, the quality of the soil had to be carefully controlled by crop rotation: specific crops were sown on different segments of the arable land with one part laying fallow to recover from the tilling (English three-field system, Dutch drieslagstelsel). The cattle and sheep were put out to pasture on the common meadows guarded by a shepherd or cowherd. Pigs were allowed to forage in the nearby forests and killed in autumn before the winter starvation set in.&lt;/p&gt;
    &lt;p&gt;Roads and rivers were important for transport of crops and livestock. These roads, some of them paved, some of them not, needed to be maintained. They were essential to the payment of the tithe, since tithe collectors assessed the harvest on the field and later collected the sheaves on the side of the road.&lt;/p&gt;
    &lt;p&gt;The buildings within the community also needed maintenance. Farmhouses, community barns and stables were made of wood and had to be rebuilt every few generations, only the name of the farm or homestead being continued.&lt;/p&gt;
    &lt;p&gt;So what kind of threats did a medieval settlement face? First of all, the weather was an important factor which dictated the success of the harvest. Storms, droughts and floods could devastate the harvest and decimate the community.&lt;/p&gt;
    &lt;p&gt;Diseases and epidemics were another danger threatening the community. The situation on the countryside was a lot better in this regard than in the medieval towns, but an epidemic could still mean the end of a village. Similarly, diseases among livestock impacted the medieval subsistence economy in a brutal way.&lt;/p&gt;
    &lt;p&gt;Then there are the consequences of medieval warfare affecting the community: Armies that passed by could plunder the village, burn the farms and execute villagers at will. Or they could also demand supplies, food and provisions as an emergency "tax"&lt;/p&gt;
    &lt;p&gt;But war also brought indirect consequences; a liege lord calling the banners and levying troops from the village community might extract a large part of the adult men. Warfare also disrupted the trade networks that supplied a village with building materials and commodities.&lt;/p&gt;
    &lt;p&gt;Then there were internal threats to the fabric of the village community. We may think of social unrest because of land disputes. Feuds could also tear a community apart with endemic vendetta’s causing death and despair. A socially unstable society was also more prone to internal accusations of heresy and witchcraft.&lt;/p&gt;
    &lt;head rend="h3"&gt;An “accurate” medieval settlement builder&lt;/head&gt;
    &lt;p&gt;So, which of the above listed features could potentially contribute to a more historically accurate computer game about medieval settlement building? First of all, it would be more realistic if the settlement could first be planned out and was not forced to "grow organically" from a community center. The first settlement phase would be a test of how “successful” a layout is in adapting to the exigencies of the terrain and the needs of the community. Only after that initial layout proved successful, further expansions can be planned.&lt;/p&gt;
    &lt;p&gt;Secondly, it would be more realistic if we could build both straight roads and curved roads, just as in Cities Skylines (2015), a modern city builder well known for its incredibly flexible layout tools. Incidentally, the tools of Cities Skylines can also be used to recreate medieval settlements, as was done by YouTube creator Play Curiously who constructed an impression of a medieval Croatian village.&lt;/p&gt;
    &lt;p&gt;Such a flexible road drawing tool can then also be used to lay out ditches, hedges and enclosures since these features were central to the medieval experience of the cultivated landscape.&lt;/p&gt;
    &lt;p&gt;Thirdly, It would be interesting to see a medieval-themed game embrace the concept of flood valleys that limit and endanger pasture and arable land. Other historical city builders such as Pharaoh (1999) and Children of the Nile (2004) already implemented this feature for their setting in Ancient Egypt. However, such a mechanic would likewise fit a medieval city builder and show the general public how medieval society dealt with seasonal flooding as well as the devastating effects that storm floods could have.&lt;/p&gt;
    &lt;p&gt;And finally, something that would, in my opinion, really add to the realism and historical flavor of a medieval-themed city builder would be the introduction of mechanisms in which agricultural surpluses are skimmed by the church and the feudal lord. Tithes, taxes and rents! Instead of merely abstracting the taxes into an income modifier or letting the player be the extractor himself, we could be shown the tax collector visiting the village, counting the sheaves by the side of the road, selecting the calves and chickens. This way, the experiences of our medieval forebears are visualized and may help to educate the public about medieval village life.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not?&lt;/head&gt;
    &lt;p&gt;There are some good reasons why city building games are not that historically accurate and instead adhere to the established formula of the city building game.&lt;/p&gt;
    &lt;p&gt;First of all, a linear growth model makes sense from a gameplay perspective, since it is rewarding to see your settlement grow.in a linear way. It fosters a feeling of progress and motivates the player to keep momentum and push through to the next expansion phase. Secondly, games are generally wary of punishing failure too harshly in order to avoid demoralizing the player. Thirdly, in order to facilitate path finding for the simulated villagers it is easier to implement a gridlike road and building system rather than an off-grid building system that allows for curvy roads. So far only Cities Skylines has managed to do this in a satisfactory way.&lt;/p&gt;
    &lt;p&gt;Lastly, for marketing purposes and recognizability, game developers generally don't stray too far from the image of the Middle Ages that the public is already acquainted with. For a medieval city builder this means windmills, industrious peasants, lots of sheep and stone castles. Things like land surveying, crop rotation and tithe collection do not fit this image and challenge the romanticized picture of the uneducated farmer in his pre-industrial environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Although I think medieval-themed city building games could benefit from incorporating some of the things we know about medieval settlement history into the gameplay loop, it may not be desirable for game developers to stray too far from the established formula. The idea that medieval settlements developed organically according to messy road plans is strongly imbedded in popular perception. Allowing both straight and curved road building in medieval city builders, may serve to challenge some of the stereotypes that exist about medieval village life. And if you ask me, that would be a good thing for it is an enriching experience to see the world through the eyes of our medieval forebears. One may find out that their lives were not that different after all...&lt;/p&gt;
    &lt;head rend="h3"&gt;Further reading&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fossier, R. (2004). “The Rural Economy and Demographic Growth.” In: D. Luscombe &amp;amp; J. Riley-Smith (Eds.). The New Cambridge Medieval History. Cambridge: Cambridge University Press, 11-46.&lt;/item&gt;
      &lt;item&gt;Van Ham, W. (1979). “Dorp en dorpsleven in middeleeuws Wouw." in: A. Delahaye (red.), De Heren XVII van Nassau-Brabant, 316-336.”&lt;/item&gt;
      &lt;item&gt;(forthc.) Kerkhof, P.A. (2020). “Saer, Saert; een Zuid-Nederlandse veldnaam van onzekere oorsprong.” Noordbrabants Historisch Jaarboek.&lt;/item&gt;
      &lt;item&gt;Leenders, K.A.H.W. (1996). "Noord-Vlaanderen en de Noordwesthoek; een vergelijking." Tijdschrift voor Waterstaatsgeschiedenis 5, 67-73.&lt;/item&gt;
      &lt;item&gt;Leenders, K.A.H.W. (1989). Verdwenen venen; een onderzoek naar de ligging en exploitatie van thans verdwenen venen in het gebied tussen Antwerpen, Turnhout, Geertruidenberg en Willemstad (1250-1750). Reeks Landschapsstudies 13, Wageningen.&lt;/item&gt;
      &lt;item&gt;Oosthuizen, S. (2017). The Anglo-Saxon fenland. Windgather Press.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;© Alexia Kerkhof and Leiden Medievalists Blog, 2020. Unauthorised use and/or duplication of this material without express and written permission from this site’s author and/or owner is strictly prohibited. Excerpts and links may be used, provided that full and clear credit is given to Alexia Kerkhof and Leiden Medievalists Blog with appropriate and specific direction to the original content.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46726857</guid><pubDate>Fri, 23 Jan 2026 00:22:58 +0000</pubDate></item><item><title>Stunnel</title><link>https://www.stunnel.org/</link><description>&lt;doc fingerprint="aedecc1f008fa6d0"&gt;
  &lt;main&gt;
    &lt;p&gt;Stunnel is a proxy designed to add TLS encryption functionality to existing clients and servers without any changes in the programs' code. Its architecture is optimized for security, portability, and scalability (including load-balancing), making it suitable for large deployments.&lt;/p&gt;
    &lt;p&gt;Stunnel uses the OpenSSL library for cryptography, so it supports whatever cryptographic algorithms are compiled into the library. It can benefit from the FIPS 140-2 validation of the OpenSSL FIPS Provider, as long as the building process meets the OpenSSL FIPS 140-2 Security Policy. Our latest Windows installer includes the OpenSSL FIPS Provider.&lt;/p&gt;
    &lt;p&gt;Stunnel is a free software authored by Michał Trojnara. Although distributed under GNU GPL version 2 or later with OpenSSL exception, stunnel is not a community project. We retain the copyright of the source code. Please contact us for commercial support or non-GPL licenses. Free, community-based support is also available via stunnel-users mailing list.&lt;/p&gt;
    &lt;p&gt;We offer commercial support with several levels of response time up to 24/7/365 helpline.&lt;/p&gt;
    &lt;p&gt;Download latest version of stunnel and try it out yourself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46726916</guid><pubDate>Fri, 23 Jan 2026 00:30:20 +0000</pubDate></item><item><title>Bugs Apple Loves</title><link>https://www.bugsappleloves.com</link><description>&lt;doc fingerprint="e3cc5b7e4366455c"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading Apple's bugs&lt;/p&gt;
    &lt;p&gt;Why else would they keep them around for so long?&lt;/p&gt;
    &lt;p&gt;Total time wasted by humanity because Apple won't fix these&lt;/p&gt;
    &lt;p&gt;Calculating...&lt;/p&gt;
    &lt;p&gt;and counting&lt;/p&gt;
    &lt;p&gt;Every bug is different. But the math is always real.&lt;lb/&gt;Think our numbers are wrong? Edit them yourself.&lt;/p&gt;
    &lt;code&gt;Users Affected × Frequency × Time Per Incident&lt;/code&gt;
    &lt;p&gt;How many Apple users hit this bug, how often, and how long they suffer each time.&lt;/p&gt;
    &lt;code&gt;Σ (Workaround Time × Participation Rate)&lt;/code&gt;
    &lt;p&gt;The extra time spent by people who try to fix what Apple won't.&lt;/p&gt;
    &lt;code&gt;Years Unfixed × Pressure Factor&lt;/code&gt;
    &lt;p&gt;How long Apple has known about this and how urgent the task usually is.&lt;/p&gt;
    &lt;code&gt;Human Hours Wasted ÷ Engineering Hours to Fix&lt;/code&gt;
    &lt;p&gt;How many times over Apple could have fixed it with the productivity they've destroyed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46727587</guid><pubDate>Fri, 23 Jan 2026 02:24:12 +0000</pubDate></item><item><title>I built a light that reacts to radio waves [video]</title><link>https://www.youtube.com/watch?v=moBCOEiqiPs</link><description>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46728808</guid><pubDate>Fri, 23 Jan 2026 05:34:35 +0000</pubDate></item><item><title>Proton Spam and the AI Consent Problem</title><link>https://dbushell.com/2026/01/22/proton-spam/</link><description>&lt;doc fingerprint="ef5f40914ea2becf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Proton Spam and the AI Consent Problem&lt;/head&gt;
    &lt;p&gt;On Jan 14th Proton sent out an email newsletter with the subject line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Introducing Projects - Try Lumoâs powerful new feature now&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Lumo is Protonâs &lt;/p&gt;
    &lt;p&gt;There is a problem with this email. And Iâm not talking about the question of how exactly AI aligns with Protonâs core values of privacy and security.&lt;/p&gt;
    &lt;p&gt;The problem is I had already explicitly opted out of Lumo emails.&lt;/p&gt;
    &lt;p&gt;That toggle for âLumo product updatesâ is unchecked. Lumo is the only topic Iâm not subscribed to. Proton has over a dozen newsletters, including some crypto nonsense. I opt-in to everything but Lumo, I gave an undeniable no to Lumo emails.&lt;/p&gt;
    &lt;p&gt;So the email I received from Proton is spam, right?&lt;/p&gt;
    &lt;p&gt;My understanding is that spam is a violation of GDPR and UK data protection laws. Regardless, Protonâs email is a clear abuse of their own service towards a paying business customer.&lt;/p&gt;
    &lt;p&gt;Before I grab my pitchfork I emailed Proton support.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proton Support&lt;/head&gt;
    &lt;p&gt;Despite the subject line and contents, and despite the âFrom Lumoâ name and &lt;code&gt;@lumo.proton.me&lt;/code&gt; address, maybe this was an honest mistake?&lt;/p&gt;
    &lt;p&gt;Protonâs first reply explained how to opt-out.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hello David,&lt;/p&gt;
      &lt;p&gt;Thank you for contacting us.&lt;/p&gt;
      &lt;p&gt;You can unsubscribe from the newsletters if you do the following:&lt;/p&gt;
      &lt;p&gt;- Log in to your account at https://account.protonvpn.com/login&lt;/p&gt;
      &lt;p&gt;- Navigate to the Account category&lt;/p&gt;
      &lt;p&gt;- Disable the check-marks under âEmail subscriptionsâ&lt;/p&gt;
      &lt;p&gt;- If you need additional assistance, let me know.&lt;/p&gt;
      &lt;p&gt;[screenshot of the same opt-out toggle]&lt;/p&gt;
      &lt;p&gt;-Have a nice day.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;John Support directs me to the exact same âLumo product updatesâ toggle I had already unchecked. I replied explaining that I had already opted out. Support replies saying theyâre âchecking this with the teamâ then later replies again asking for screenshots.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Can you make sure to send me a screenshot of this newsletter option disabled, as well as the date when the last message was sent to you regarding the Lumo offer?&lt;/p&gt;
      &lt;p&gt;You can send me a screenshot of the whole message, including the date.&lt;/p&gt;
      &lt;p&gt;Is it perhaps 14 January 2026 that you received the message?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I found that last line curious, are they dealing with other unhappy customers? Maybe Iâm reading too much into it.&lt;/p&gt;
    &lt;p&gt;I sent the screenshots and signed off with âDonât try to pretend this fits into another newsletter category.â&lt;/p&gt;
    &lt;p&gt;After more âchecking this with the teamâ I got a response today.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In this case, the mentioned newsletter is for promoting Lumo Business Suit to Business-related plans.&lt;/p&gt;
      &lt;p&gt;Hence, why you received it, as Product Updates and Email Subscription are two different things.&lt;/p&gt;
      &lt;p&gt;In the subscription section, you will see the âEmail Subscriptionâ category, where you can disable the newsletter in order to avoid getting it in the future.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If I understand correctly, Proton are claiming this email is the âProton for Business newsletterâ. Not the âLumo product updatesâ newsletter.&lt;/p&gt;
    &lt;p&gt;I donât know about you, but I think thatâs baloney. Proton Support had five full business days to come up with a better excuse. Please tell me, how can I have been any more explicit about opting out of Lumo emails, only to receive âTry Lumoâ âFrom Lumoâ, and be told that is not actually a Lumo email?&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-Consent&lt;/head&gt;
    &lt;p&gt;Has anyone else noticed that the AI industry canât take ânoâ for an answer? AI is being force-fed into every corner of tech. Itâs unfathomable to them that some of us arenât interested.&lt;/p&gt;
    &lt;p&gt;The entire AI industry is built upon a common principle of non-consent. They laugh in the face of IP and copyright law. AI bots DDoS websites and lie about user-agents. Can it get worse than the sickening actions of Grok? I dread to think.&lt;/p&gt;
    &lt;p&gt;As Proton has demonstrated above, and Mozilla/Firefox recently too, the AI industry simply will not accept ânoâ as an answer. Some examples like spam are more trivial than others, but the growing trend is vile and disturbing.&lt;/p&gt;
    &lt;p&gt;I do not want your AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Update for 23rd January&lt;/head&gt;
    &lt;p&gt;I guess someone at Microsoft read my post and said âhold my beerâ. This morning I woke up to a lovely gift in my inbox; âBuild Al agents with the new GitHub Copilot SDKâ.&lt;/p&gt;
    &lt;p&gt;GitHub Ensloppification is moving faster than I can delete my account for good. (Itâs an unfortunate requirement for client projects.) For the record, I have never said âyesâ to any GitHub newsletter. Even before Copilot I disabled every possible GitHub email notification.&lt;/p&gt;
    &lt;p&gt;The âUnsubscribeâ link provides the hidden newsletter list. There is nothing within GitHub account settings I can find to disable spam.&lt;/p&gt;
    &lt;p&gt;As expected, Microsoft has opted me in without my consent. The wheels are falling off at GitHub. The brutally slow front-end UI. The embarrassingly lacklustre Actions CI. Now this sloppy tripe everywhere. Reminder to developers: GitHub is not Git.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46729368</guid><pubDate>Fri, 23 Jan 2026 07:01:29 +0000</pubDate></item><item><title>Replacing Protobuf with Rust to go 5 times faster</title><link>https://pgdog.dev/blog/replace-protobuf-with-rust</link><description>&lt;doc fingerprint="ae1a93792c310158"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Replacing Protobuf with Rust to go 5 times faster&lt;/head&gt;
    &lt;p&gt;Jan 22nd, 2026&lt;lb/&gt;Lev Kokotov&lt;/p&gt;
    &lt;p&gt;PgDog is a proxy for scaling PostgreSQL. Under the hood, we use &lt;code&gt;libpg_query&lt;/code&gt; to parse and understand SQL queries. Since PgDog is written in Rust, we use its Rust bindings to interface with the core C library. 
Those bindings use Protobuf (de)serialization to work uniformly across different programming languages, e.g., the popular Ruby pg_query gem.&lt;/p&gt;
    &lt;p&gt;Protobuf is fast, but not using Protobuf is faster. We forked pg_query.rs and replaced Protobuf with direct C-to-Rust (and back to C) bindings, using bindgen and Claude-generated wrappers. This resulted in a 5x improvement in parsing queries, and a 10x improvement in deparsing (Postgres AST to SQL string conversion).&lt;/p&gt;
    &lt;head rend="h5"&gt;Results&lt;/head&gt;
    &lt;p&gt;You can reproduce these by cloning our fork and running the benchmark tests:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Function&lt;/cell&gt;
        &lt;cell role="head"&gt;Queries per second&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;pg_query::parse&lt;/code&gt; (Protobuf)&lt;/cell&gt;
        &lt;cell&gt;613&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;pg_query::parse_raw&lt;/code&gt; (Direct C to Rust)&lt;/cell&gt;
        &lt;cell&gt;3357 (5.45x faster)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;pg_query::deparse&lt;/code&gt; (Protobuf)&lt;/cell&gt;
        &lt;cell&gt;759&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;pg_query::deparse_raw&lt;/code&gt; (Direct Rust to C)&lt;/cell&gt;
        &lt;cell&gt;7319 (9.64x faster)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;The process&lt;/head&gt;
    &lt;p&gt;The first step is always profiling. We use samply, which integrates nicely with the Firefox profiler. Samply is a sampling profiler: it measures how much time code spends running CPU instructions in each function. It works by inspecting the application call stack thousands of times per second. The more time is spent inside a particular function (or span, as they are typically called), the slower that code is. This is how we discovered &lt;code&gt;pg_query_parse_protobuf&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;This is the entrypoint to the &lt;code&gt;libpg_query&lt;/code&gt; C library, used by all pg_query bindings. The function that wraps the actual Postgres parser, &lt;code&gt;pg_query_raw_parse&lt;/code&gt;, barely registered on the flame graph. Parsing queries isn’t free, but the Postgres parser itself is very quick and has been optimized for a long time. With the hot spot identified, our first instinct was to do nothing and just add a cache.&lt;/p&gt;
    &lt;head rend="h4"&gt;Caching mostly works&lt;/head&gt;
    &lt;p&gt;Caching is a trade-off between memory and CPU utilization, and memory is relatively cheap (latest DRAM crunch notwithstanding). The cache is mutex-protected, uses the LRU algorithm and is backed by a hashmap1. The query text is the key and the Abstract Syntax Tree is the value, which expects most apps to use prepared statements. The query text contains placeholders instead of actual values and is therefore reusable, for example:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM users WHERE id = $1;
&lt;/code&gt;
    &lt;p&gt;While the &lt;code&gt;id&lt;/code&gt; parameter can change between invocations, the prepared statement does not, so we could cache its static AST in memory.&lt;/p&gt;
    &lt;p&gt;This works pretty well, but eventually we ran into a couple of issues:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Some ORMs can have bugs that generate thousands of unique statements, e.g., &lt;code&gt;value IN ($1, $2, $3)&lt;/code&gt;instead of&lt;code&gt;value = ANY($1)&lt;/code&gt;, which causes a lot of cache misses&lt;/item&gt;
      &lt;item&gt;Applications use old PostgreSQL client drivers which don’t support prepared statements, e.g., Python’s &lt;code&gt;psycopg2&lt;/code&gt;package&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The clock on Protobuf was ticking and we needed to act. So, like a lot of engineers these days, we asked an LLM to just do it for us.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tight constraints&lt;/head&gt;
    &lt;p&gt;I’m going to preface this section by saying that the vast majority of PgDog’s source code is written by a human. AI is not in a position to one-shot a connection pooler, load balancer and database sharder. However, when scoped to a very specific, well-defined and most importantly machine-verifiable task, it can work really well.&lt;/p&gt;
    &lt;p&gt;The prompt we started with was pretty straightforward:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;libpg_query is a library that wraps the PostgreSQL parser in an API. pg_query.rs is a Rust wrapper around libpg_query which uses Protobuf for (de)serialization. Replace Protobuf with bindgen-generated Rust structs that map directly to the Postgres AST.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And after two days of back and forth between us and the machine, it worked. We ended up with 6,000 lines of recursive Rust that manually mapped C types and structs to Rust structs, and vice versa. We made the switch for &lt;code&gt;parse&lt;/code&gt;, &lt;code&gt;deparse&lt;/code&gt; (used in our new query rewrite engine, which we’ll talk about in another post), &lt;code&gt;fingerprint&lt;/code&gt; and &lt;code&gt;scan&lt;/code&gt;. These four methods are heavily used in PgDog to make sharding work, and we immediately saw a 25% improvement in pgbench benchmarks2.&lt;/p&gt;
    &lt;p&gt;Just to be clear: we had a lot of things going for us already that made this possible. First, pg_query has a Protobuf spec for protoc (and Prost, the Protobuf Rust implementation) to generate bindings, so Claude was able to get a comprehensive list of structs it needed to extract from C, along with the expected data types.&lt;/p&gt;
    &lt;p&gt;Second, pg_query.rs was already using bindgen, so we had to just copy/paste some invocations around to get the AST structs included in bindgen’s output.&lt;/p&gt;
    &lt;p&gt;And last, and definitely not least, pg_query.rs already had a working &lt;code&gt;parse&lt;/code&gt; and &lt;code&gt;deparse&lt;/code&gt; implementation, so we could test our AI-generated code against its output. This was entirely automated and verifiable: for each test case that used &lt;code&gt;parse&lt;/code&gt;, we included a call to &lt;code&gt;parse_raw&lt;/code&gt;, compared their results and if they differed by even one byte, Claude Code had to go back and try again.&lt;/p&gt;
    &lt;head rend="h4"&gt;The implementation&lt;/head&gt;
    &lt;p&gt;The translation code between Rust and C uses &lt;code&gt;unsafe&lt;/code&gt; Rust functions that wrap Rust structs to C structs. The C structs are then passed to the Postgres/libpg_query C API which does the actual work of building the AST.&lt;/p&gt;
    &lt;p&gt;The result is converted back to Rust using a recursive algorithm: each node in the AST has its own converter function which accepts an &lt;code&gt;unsafe&lt;/code&gt; C pointer and returns a safe Rust struct. Much like the name suggests, the AST is a tree, which is stored in an array:&lt;/p&gt;
    &lt;code&gt;unsafe fn convert_list_to_raw_stmts(
    list: *mut bindings_raw::List
) -&amp;gt; Vec&amp;lt;protobuf::RawStmt&amp;gt; {
    // C-to-Rust conversion.
}
&lt;/code&gt;
    &lt;p&gt;For each node in the list, the implementation calls &lt;code&gt;convert_node&lt;/code&gt;, which then handles each one of the 100s of tokens available in the SQL grammar:&lt;/p&gt;
    &lt;code&gt;unsafe fn convert_node(
    node_ptr: *mut bindings_raw::Node
) -&amp;gt; Option&amp;lt;protobuf::Node&amp;gt; {
    // This is basically C in Rust, so we better check for nulls!
    if node_ptr.is_null() {
        return None;
    }

    match (*node_ptr).type_ {
        // SELECT statement root node.
        bindings_raw::NodeTag_T_SelectStmt =&amp;gt; {
            let stmt = node_ptr as *mut bindings_raw::SelectStmt;
            Some(protobuf::node::Node::SelectStmt(Box::new(convert_select_stmt(&amp;amp;*stmt))))
        }
        
        // INSERT statement root node.
        bindings_raw::NodeTag_T_InsertStmt =&amp;gt; {
            let stmt = node_ptr as *mut bindings_raw::InsertStmt;
            Some(protobuf::node::Node::InsertStmt(Box::new(convert_insert_stmt(&amp;amp;*stmt))))
        }
        
        // ... 100s more nodes.
    }
}
&lt;/code&gt;
    &lt;p&gt;For nodes that contain other nodes, we recurse on &lt;code&gt;convert_node&lt;/code&gt; again until the algorithm reaches the leaves (nodes with no children) and terminates. For nodes that contain scalars, like a number (e.g., &lt;code&gt;5&lt;/code&gt;) or text (e.g., &lt;code&gt;'hello world'&lt;/code&gt;), the data type is copied into a Rust analog, e.g., &lt;code&gt;i32&lt;/code&gt; or &lt;code&gt;String&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The end result is &lt;code&gt;protobuf::ParseResult&lt;/code&gt;, a Rust struct generated by Prost from the pg_query API Protobuf specification, but populated by native Rust code instead of Prost’s deserializer. Reusing existing structs reduces the chance of errors considerably: we can compare &lt;code&gt;parse&lt;/code&gt; and &lt;code&gt;parse_raw&lt;/code&gt; outputs, using the derived &lt;code&gt;PartialEq&lt;/code&gt; trait, and ensure that both are identical, in testing.&lt;/p&gt;
    &lt;p&gt;While recursive algorithms have a questionable reputation in the industry because bad ones can cause stack overflows, they are very fast. Recursion requires no additional memory allocation because all of its working space, the stack, is created on program startup. It also has excellent CPU cache locality because the instructions for the next invocation of the same function are already in the CPU L1/L2/L3 cache. Finally and arguably more importantly, they are just easier to read and understand than iterative implementations, which helps us, the humans, with debugging.&lt;/p&gt;
    &lt;p&gt;Just for good measure, we tried generating an iterative algorithm, but it ended up being slower than Prost. The main cause (we think) was unnecessary memory allocations, hashmap lookups of previously converted nodes, and too much overhead from walking the tree several times. Meanwhile, recursion processes each AST node exactly once and uses the stack pointer to track its position in the tree. If you have any ideas on how to make an iterative algorithm work better, let us know!&lt;/p&gt;
    &lt;head rend="h3"&gt;Closing thoughts&lt;/head&gt;
    &lt;p&gt;Reducing the overhead from using the Postgres parser in PgDog makes a huge difference for us. As a network proxy, our budget for latency, memory utilization, and CPU cycles is low. After all, we aren’t a real database…yet! This change improves performance from two angles: we use less CPU and we do less work, so PgDog is faster and cheaper to run.&lt;/p&gt;
    &lt;p&gt;If stuff like this is interesting to you, reach out. We are looking for a Founding Software Engineer to help us grow and build the next iteration of horizontal scaling for PostgreSQL.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46730214</guid><pubDate>Fri, 23 Jan 2026 09:03:31 +0000</pubDate></item><item><title>The State of Modern AI Text to Speech Systems for Screen Reader Users</title><link>https://stuff.interfree.ca/2026/01/05/ai-tts-for-screenreaders.html</link><description>&lt;doc fingerprint="85deb33a0b1ab384"&gt;
  &lt;main&gt;
    &lt;p&gt;If you're not a screen reader user yourself, you might be surprised to learn that the text to speech technology used by most blind people hasn't changed in the last 30 years. While text to speech has taken the sighted world by storm, in everything from personal assistants to GPS to telephone systems, the voices used by blind folks have remained mostly static. This is largely intentional. The needs of a blind text to speech user are vastly different than those of a sighted user. While sighted users prefer voices that are natural, conversational, and as human-like as possible, blind users tend to prefer voices that are fast, clear, predictable, and efficient. This results in a preference among blind users for voices that sound somewhat robotic, but can be understood at high rates of speed, often upwards of 800 to 900 words per minute. The speaking rate of an average person hovers around 200 to 250 words per minute, for comparison.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this difference in needs has resulted in blind people getting left out of the explosion of text to speech advancement, and has caused many problems. First, the voice that is preferred by the majority of western English blind users, called Eloquence, was last updated in 2003. While it is so overwhelmingly popular that even Apple was eventually pressured to add the voice to iPhone, mac, Apple TV, and Apple Watch, even they were forced to use an emulation layer. As Eloquence is a 32-bit voice last compiled in 2003, it cannot run in modern software without some sort of emulation or bridge. If the sourcecode to Eloquence still exists and can be compiled, even large companies like Apple haven't managed to find or compile it. As the NVDA screen reader moves from being a 32-bit application to a 64-bit one, keeping eloquence running with it has been a challenge that I and many other community members have spent a lot of time and effort solving. The eloquence libraries also have many known security issues, and anyone using the libraries today is forced to understand and program around them, as Eloquence itself can never be updated or fixed. These stopgap solutions are entirely untenable, and are likely to take us only so far. A better solution is urgently needed.&lt;/p&gt;
    &lt;p&gt;The second problem this has caused is for those who speak languages other than English. As most modern text to speech voices are created by and for sighted users, blind users begin to find that the voices available in less popular languages are inefficient, overly conversational, slow, and otherwise unsatisfactory. While espeak-ng is an open-source text to speech system that attempts to support hundreds of languages while meeting the needs of blind users, it brings a different set of problems to the table. First, many of the languages it supports were added based on pronunciation rules taken from Wikipedia articles, without involving speakers of the language. Second, Espeak-ng is based directly on Speak, a text to speech system written by Jonathan Duddington in 1995 for RISC OS on the BBC Micro, meaning that espeak users today continue to have to live with many of the design decisions made back in 1995 for an operating system that no longer exists. Third, looking at the Espeak-ng repository, it seems to only have one or two active maintainers. While this is obviously better than the zero active maintainers of Eloquence, it could still become a problem in the future.&lt;/p&gt;
    &lt;p&gt;These are the reasons that I'm always interested in advancements in text to speech, and am actively keeping my ears open for something that takes advantage of modern technology, while continuing to suit the needs of screen reader users like myself.&lt;/p&gt;
    &lt;p&gt;Over the holiday break, I decided to take a look at two modern AI-based text to speech systems, and see if they could be added to NVDA. I chose two models, because they advertised themselves as fast, able to run without a GPU, and responsive. The first was supertonic, and the second was Kitten TTS. As both models require 64-bit Python, I wrote the addons for the 64-bit alpha of NVDA. However, other than making development easier, this had little effect on the results.&lt;/p&gt;
    &lt;p&gt;Unfortunately, doing this work uncovered a number of issues that I believe are common to all of the modern AI-based text to speech systems, and make them unsuitable for use in screen readers. The first issue is dependency bloat. In order to bundle these systems as NVDA addons, developers are required to include a vast multitude of large and complex Python packages. In the case of Kitten TTS, the number is around 103, and just over 30 for supertonic. As the standard building and packaging methods for NVDA addons do not support specifying and building requirements, these dependencies need to be manually copied over, included in any github repositories, and cannot be automatically updated. Loading all of these dependencies directly into NVDA also causes the screen reader to load slower, use more system resources, and opens NVDA users up to any security issue in any of these libraries. As a screen reader needs access to the entire system, this is far from ideal.&lt;/p&gt;
    &lt;p&gt;The second issue is accuracy. These modern systems are developed to sound human, natural, and conversational. Unfortunately this seems to come at the expense of accuracy. In my testing, both models had a tendency to skip words, read numbers incorrectly, chop off short utterances, and ignore prosody hints from text punctuation. Kitten TTS is slightly better here, as it uses a deterministic phonemizer (the same one used by espeak, actually) to determine the correct way to pronounce words, leaving only the generation of the speech itself up to AI. But never the less, Kitten TTS is still far from perfectly accurate. When it comes to use in a screen reader, skipping words, or reading numbers incorrectly, is unacceptable.&lt;/p&gt;
    &lt;p&gt;The third issue is speed. Supertonic has the edge, here, but even it is far too slow. Unlike older text to speech systems, Supertonic and Kitten TTS cannot begin generating speech until they have an entire chunk of text. Supertonic is slightly faster, as it can stream result audio as it becomes available, whereas Kitten TTS cannot start speaking until all of the audio for the chunk is fully generated. But for use in a screen reader, a text to speech system needs to begin generating speech as quickly as possible, rather than waiting for an entire phrase or sentence. Users of screen readers quickly jump through text and frequently interrupt the screen reader, and thus require the text to speech system to be able to quickly discard and restart speech.&lt;/p&gt;
    &lt;p&gt;The fourth and final issue is control. Older text to speech systems make changing the pitch, speed, volume, breathiness, roughness, headsize, and other parameters of the voice easy. This allows screen reader users to customize the voice to our exact needs, as well as offering the ability to change the characteristics of the voice in real time based on the formatting or other attributes of the text. AI text to speech models, being trained on data from a particular set of speakers, cannot offer this customization. Instead, they inherit the speaking speed, pitch, volume, and other characteristics that were present in the training data. Kitten TTS and Supertonic both offer basic speed control, however it is highly variable from voice to voice and utterance to utterance. This leads to a loss of functionality that many blind users depend on.&lt;/p&gt;
    &lt;p&gt;If you'd like to experience these issues for yourself, feel free to follow the links above to my GitHub repositories. They offer ready to install addons that can be installed and used with the 64-bit NVDA alphas.&lt;/p&gt;
    &lt;p&gt;I'm picking on Kitten TTS and Supertonic not because they're particularly bad for the above problems, but because they're the models that are the state of the art in AI text to speech right now when it comes to speed and size. Other models, like Kokoro, exhibit all of the same issues, but more so.&lt;/p&gt;
    &lt;p&gt;So what's the way forward for blind screen reader users? Sadly, I don't know. Modern text to speech research has little to no overlap with our requirements. Using Eloquence, the system that many blind people find best, is becoming increasingly untenable. ESpeak uses an odd architecture originally designed for computers in 1995, and has few maintainers. Blastbay Studios has done some interesting work to create a text to speech voice using modern design and technology, that meets the requirements of blind users. But it's a closed-source product with a single maintainer, that also suffers from a lack of pronunciation accuracy. In an ideal world, someone would re-implement Eloquence as a set of open source libraries. However, doing so would require expertise in linguistics, digital signal processing, and audiology, as well as excellent programming abilities. My suspicion is that modernizing the text to speech stack that is preferred by blind power-users is an effort that would require several million dollars of funding at minimum. Instead, we'll probably wind up having to settle for text to speech voices that are "good enough", while being nowhere near as fast and efficient as what we have currently. Personally, I intend to keep Eloquence limping along for as long as I can, until the layers of required emulation and bridges make real time use impossible. Perhaps at that point AI will be good enough that it can be prompted to create a text to speech system that's up to our standards. Or, more hopefully, articles like this one may bring attention to the issues, and bring our community together to recognize the problems and find solutions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46730346</guid><pubDate>Fri, 23 Jan 2026 09:24:27 +0000</pubDate></item><item><title>Google is ending full-web search for niche search engines</title><link>https://programmablesearchengine.googleblog.com/</link><description>&lt;doc fingerprint="da81ded28b96ef2e"&gt;
  &lt;main&gt;
    &lt;p&gt;Evolving Programmable Search Engine&lt;/p&gt;
    &lt;p&gt;Programmable Search Engine helps hundreds of partners – from academic institutions to retail websites – serve their users’ search needs on their sites.&lt;/p&gt;
    &lt;p&gt;Looking forward, we’ll be evolving our offerings to provide more focused and capable solutions for every use case. This evolution is designed to ensure a high-quality experience for users and partners.&lt;/p&gt;
    &lt;p&gt;A clearer path for every search need&lt;/p&gt;
    &lt;p&gt;We're simplifying and modernizing our offerings so you can choose the best tool for your goals.&lt;/p&gt;
    &lt;p&gt;For site-specific search: The Programmable Search Element (the “Search Element”) is being simplified to be the best tool for creating rich, focused search experiences on your own websites. This solution is intended for website owners who cater focused content to a specific audience.&lt;/p&gt;
    &lt;p&gt;For enterprise-grade needs: For advanced features like AI-powered conversational search and enterprise-grade grounding, we continue to offer Google Vertex AI Search as a solution.&lt;/p&gt;
    &lt;p&gt;For full web search needs: We understand some partners have use cases that require querying beyond a designated subset of domains. Our full web search solution is available for those requiring our entire index; please complete this form to register your interest .&lt;/p&gt;
    &lt;p&gt;Planning your transition to more powerful tools&lt;/p&gt;
    &lt;p&gt;We are excited to help you harness the full potential of these evolving solutions. As you plan for the future, here is your path forward for the transition, which can be completed any time between now and January 1, 2027.&lt;/p&gt;
    &lt;p&gt;“Sites to search” feature for users of the Search Element querying 50 or fewer domains: The Search Element remains the optimal solution for delivering highly optimized and focused results. With this free feature, you can designate a maximum number of 50 domains for site-specific searches.&lt;/p&gt;
    &lt;p&gt;“Search the entire web” option for users of the Search Element querying more than 50 domains: If your use case necessitates querying more than 50 domains or is set to “Search the entire web”, contact us to express your interest in the more advanced full web search solution and get more information about its capabilities and pricing. Your transition to an alternative solution needs to be completed by January 1, 2027.&lt;/p&gt;
    &lt;p&gt;For users of the Custom Search JSON API: Vertex AI Search is a favorable alternative for up to 50 domains. Alternatively, if your use case necessitates full web search, contact us to express your interest in and get more information about our full web search solution. Your transition to an alternative solution needs to be completed by January 1, 2027.&lt;/p&gt;
    &lt;p&gt;To prepare for this transition, as of today, all new engines must be configured to use the “Sites to search” feature. This change impacts only new engines; existing engines are not affected and can continue to use the “Search the entire web” option until January 1, 2027.&lt;/p&gt;
    &lt;p&gt;This evolution will help us create more focused products, so we can provide a better search experience for our developer partners. We’re excited to build the future of search with you.&lt;/p&gt;
    &lt;p&gt;Thank you,&lt;/p&gt;
    &lt;p&gt;The Google Programmable Search Engine Team&lt;/p&gt;
    &lt;p&gt;We’re pleased to announce that we have added support for the following 32 languages: Afrikaans, Albanian, Amharic, Armenian, Azerbaijani, Bengali, Burmese, Estonian, French (CA), Georgian, Gujarati, Icelandic, Kannada, Kazakh, Khmer, Kyrgyz, Lao, Macedonian, Malay, Malayalam, Mongolian, Marathi, Nepali, Persian, Punjabi, Serbian (Latin), Sinhala, Swahili, Tamil, Telugu, Urdu, and Uzbek.&lt;/p&gt;
    &lt;p&gt;Engine owners can now select one of these languages for their search engine in the Control Panel by navigating to Overview &amp;gt; Look and Feel &amp;gt; Language.&lt;/p&gt;
    &lt;p&gt;The Custom Search Site Restricted JSON API endpoints will cease serving traffic on January 8, 2025.&lt;/p&gt;
    &lt;p&gt;Beginning on January 8, 2025, all Custom Search Site Restricted JSON API customers must begin their transition to Google Cloud's Vertex AI Search to maintain access to their site search functionality.&lt;/p&gt;
    &lt;p&gt;This year, Google Cloud has released a number of new AI products, and we are constantly working to improve our existing products with new AI features. We believe that our Custom Search Site Restricted JSON API customers will be better served by the new Vertex AI Search product, which offers a number of advanced features, including:&lt;/p&gt;
    &lt;p&gt;Advanced site search: Improved latency and domain coverage&lt;/p&gt;
    &lt;p&gt;Gen AI features: Summarization and multi-turn queries&lt;/p&gt;
    &lt;p&gt;Reverse Image search&lt;/p&gt;
    &lt;p&gt;Vertex AI integration via Vertex extension platform&lt;/p&gt;
    &lt;p&gt;Finally, we believe that the new Vertex AI Search solution will generally be more cost-effective and offer better value for our customers.&lt;/p&gt;
    &lt;p&gt;We have made the transition to Vertex AI Search as easy as possible for our partners by providing detailed transition guidance. We are here to support you through the process.&lt;/p&gt;
    &lt;p&gt;We appreciate your understanding and cooperation. If you have any questions, please do not hesitate to contact us.&lt;/p&gt;
    &lt;p&gt;We’re excited to share news of a new product our partners over at Google Cloud just announced, Vertex AI Search! If you use the Custom Search Site Restricted JSON API we believe this product is likely a good fit for you.&lt;/p&gt;
    &lt;p&gt;Vertex AI Search allows you to set up and deploy a Google-grade site search engine in minutes at a competitive price. In addition to the basic search experience, Vertex AI Search provides an advanced option, which includes:&lt;/p&gt;
    &lt;p&gt;Because we believe Vertex AI Search best serves the needs of site restricted search use cases, we are no longer receiving new customers for the Custom Search Site Restricted JSON API. This has no effect on existing customers.&lt;/p&gt;
    &lt;p&gt;If you are interested in moving from the Custom Search Site Restricted JSON API to Vertex AI Search, you can find transition guidance here.&lt;/p&gt;
    &lt;p&gt;Last July, we introduced the new Programmable Search Engine control panel. While we continued to provide the option to use the legacy control panel, we’re glad to see that the vast majority of customers have found value in the new control panel and made the switch permanently. During this time, we’ve worked diligently to respond to feedback and ensure the new control panel has all the features customers need and expect from Programmable Search Engine.&lt;/p&gt;
    &lt;p&gt;Today, we’re confident that the new control panel is more user-friendly and intuitive, making it easier than ever to create and manage your search engines. Therefore, the legacy control panel will be retired on June 21, 2023 and all remaining traffic will be redirected to the new control panel.&lt;/p&gt;
    &lt;p&gt;We’re confident you’ll like the new control panel as much as we do. As always, your feedback is welcome and encouraged as we continue to improve Programmable Search Engine.&lt;/p&gt;
    &lt;p&gt;As the search space continues to evolve, we want to make sure that Programmable Search Engine continues to evolve to meet the needs of your users. We’ve decided to remove the Popular Queries JavaScript API (https://cse.google.com/api/&amp;lt;Search Engine ID&amp;gt;/popularqueryjs). Starting on November 11, 2022, this feature will no longer be available.&lt;/p&gt;
    &lt;p&gt;If you’ve been showing Popular Queries on your website, we’re glad to let you know that the Stats tab of the Statistics and Logs page in the Control Panel still shows popular queries for your Programmable Search Engine. We know this may be disruptive, so if you have questions or need assistance, please check out the Help Center or reach out to us on the Community Forums.&lt;/p&gt;
    &lt;p&gt;While the current control panel has served our customers well over the years, the time has come for a refresh. Today, we’re excited to announce the launch of the new Programmable Search Engine control panel!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46730436</guid><pubDate>Fri, 23 Jan 2026 09:38:02 +0000</pubDate></item><item><title>Ghostty's AI Policy</title><link>https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md</link><description>&lt;doc fingerprint="eb7ad05e09566404"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46730504</guid><pubDate>Fri, 23 Jan 2026 09:50:26 +0000</pubDate></item></channel></rss>