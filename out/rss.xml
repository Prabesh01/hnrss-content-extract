<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Dec 2025 13:10:51 +0000</lastBuildDate><item><title>Python Workers redux: fast cold starts, packages, and a uv-first workflow</title><link>https://blog.cloudflare.com/python-workers-advancements/</link><description>&lt;doc fingerprint="a2332701ecf0f4b1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Note: This post was updated with additional details regarding AWS Lambda.&lt;/p&gt;
      &lt;p&gt;Last year we announced basic support for Python Workers, allowing Python developers to ship Python to region: Earth in a single command and take advantage of the Workers platform.&lt;/p&gt;
      &lt;p&gt;Since then, we√¢ve been hard at work making the Python experience on Workers feel great. We√¢ve focused on bringing package support to the platform, a reality that√¢s now here √¢ with exceptionally fast cold starts and a Python-native developer experience.&lt;/p&gt;
      &lt;p&gt;This means a change in how packages are incorporated into a Python Worker. Instead of offering a limited set of built-in packages, we now support any package supported by Pyodide, the WebAssembly runtime powering Python Workers. This includes all pure Python packages, as well as many packages that rely on dynamic libraries. We also built tooling around uv to make package installation easy.&lt;/p&gt;
      &lt;p&gt;We√¢ve also implemented dedicated memory snapshots to reduce cold start times. These snapshots result in serious speed improvements over other serverless Python vendors. In cold start tests using common packages, Cloudflare Workers start over 2.4x faster than AWS Lambda without SnapStart and 3x faster than Google Cloud Run.&lt;/p&gt;
      &lt;p&gt;In this blog post, we√¢ll explain what makes Python Workers unique and share some of the technical details of how we√¢ve achieved the wins described above. But first, for those who may not be familiar with Workers or serverless platforms √¢ and especially those coming from a Python background √¢ let us share why you might want to use Workers at all.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Deploying Python globally in 2 minutes&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Part of the magic of Workers is simple code and easy global deployments. Let's start by showing how you can deploy a FastAPI app across the world with fast cold starts in less than two minutes.&lt;/p&gt;
      &lt;p&gt;A simple Worker using FastAPI can be implemented in a handful of lines:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;from fastapi import FastAPI
from workers import WorkerEntrypoint
import asgi

app = FastAPI()

@app.get("/")
async def root():
   return {"message": "This is FastAPI on Workers"}

class Default(WorkerEntrypoint):
   async def fetch(self, request):
       return await asgi.fetch(app, request.js_object, self.env)&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;To deploy something similar, just make sure you have &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;npm&lt;/code&gt; installed, then run the following:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ uv tool install workers-py
$ pywrangler init --template \
    https://github.com/cloudflare/python-workers-examples/03-fastapi
$ pywrangler deploy&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;With just a little code and a &lt;code&gt;pywrangler deploy&lt;/code&gt;, you√¢ve now deployed your application across Cloudflare√¢s edge network that extends to 330 locations across 125 countries. No worrying about infrastructure or scaling.&lt;/p&gt;
      &lt;p&gt;And for many use cases, Python Workers are completely free. Our free tier offers 100,000 requests per day and 10ms CPU time per invocation. For more information, check out the pricing page in our documentation.&lt;/p&gt;
      &lt;p&gt;For more examples, check out the repo in GitHub. And read on to find out more about Python Workers.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;So what can you do with Python Workers?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Now that you√¢ve got a Worker, just about anything is possible. You write the code, so you get to decide. Your Python Worker receives HTTP requests and can make requests to any server on the public Internet.&lt;/p&gt;
      &lt;p&gt;You can set up cron triggers, so your Worker runs on a regular schedule. Plus, if you have more complex requirements, you can make use of Workflows for Python Workers, or even long-running WebSocket servers and clients using Durable Objects.&lt;/p&gt;
      &lt;p&gt;Here are more examples of the sorts of things you can do using Python Workers:&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Faster package cold starts&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Serverless platforms like Workers save you money by only running your code when it√¢s necessary to do so. This means that if your Worker isn√¢t receiving requests, it may be shut down and will need to be restarted once a new request comes in. This typically incurs a resource overhead we refer to as the √¢cold start.√¢ It√¢s important to keep these as short as possible to minimize latency for end users.&lt;/p&gt;
      &lt;p&gt;In standard Python, booting the runtime is expensive, and our initial implementation of Python Workers focused on making the runtime boot fast. However, we quickly realized that this wasn√¢t enough. Even if the Python runtime boots quickly, in real-world scenarios the initial startup usually includes loading modules from packages, and unfortunately, in Python many popular packages can take several seconds to load.&lt;/p&gt;
      &lt;p&gt;We set out to make cold starts fast, regardless of whether packages were loaded.&lt;/p&gt;
      &lt;p&gt;To measure realistic cold start performance, we set up a benchmark that imports common packages, as well as a benchmark running a √¢hello world√¢ using a bare Python runtime. Standard Lambda is able to start just the runtime quickly, but once you need to import packages, the cold start times shoot up. In order to optimize for faster cold starts with packages, you can use SnapStart on Lambda (which we will be adding to the linked benchmarks shortly). This incurs a cost to store the snapshot and an additional cost on every restore. Python Workers will automatically apply memory snapshots for free for every Python Worker.&lt;/p&gt;
      &lt;p&gt;Here are the average cold start times when loading three common packages (httpx, fastapi and pydantic):&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Platform&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Mean Cold Start (secs)&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Cloudflare Python Workers&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;1.027&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;AWS Lambda (without SnapStart)&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;2.502&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Google Cloud Run&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;3.069&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;In this case, Cloudflare Python Workers have 2.4x faster cold starts than AWS Lambda without SnapStart and 3x faster cold starts than Google Cloud Run. We achieved these low cold start numbers by using memory snapshots, and in a later section we explain how we did so.&lt;/p&gt;
      &lt;p&gt;We are regularly running these benchmarks. Go here for up-to-date data and more info on our testing methodology.&lt;/p&gt;
      &lt;p&gt;We√¢re architecturally different from these other platforms √¢ namely, Workers is isolate-based. Because of that, our aims are high, and we are planning for a zero cold start future.&lt;/p&gt;
      &lt;p&gt;The diverse package ecosystem is a large part of what makes Python so amazing. That√¢s why we√¢ve been hard at work ensuring that using packages in Workers is as easy as possible.&lt;/p&gt;
      &lt;p&gt;We realised that working with the existing Python tooling is the best path towards a great development experience. So we picked the &lt;code&gt;uv&lt;/code&gt; package and project manager, as it√¢s fast, mature, and gaining momentum in the Python ecosystem.&lt;/p&gt;
      &lt;p&gt;We built our own tooling around &lt;code&gt;uv&lt;/code&gt; called pywrangler. This tool essentially performs the following actions:&lt;/p&gt;
      &lt;p&gt;Pywrangler calls out to &lt;code&gt;uv&lt;/code&gt; to install the dependencies in a way that is compatible with Python Workers, and calls out to &lt;code&gt;wrangler&lt;/code&gt; when developing locally or deploying Workers.√Ç¬†&lt;/p&gt;
      &lt;p&gt;Effectively this means that you just need to run &lt;code&gt;pywrangler dev&lt;/code&gt; and &lt;code&gt;pywrangler&lt;/code&gt; &lt;code&gt;deploy&lt;/code&gt; to test your Worker locally and deploy it.√Ç¬†&lt;/p&gt;
      &lt;p&gt;You can generate type hints for all of the bindings defined in your wrangler config using &lt;code&gt;pywrangler types&lt;/code&gt;. These type hints will work with Pylance or with recent versions of mypy.&lt;/p&gt;
      &lt;p&gt;To generate the types, we use wrangler types to create typescript type hints, then we use the typescript compiler to generate an abstract syntax tree for the types. Finally, we use the TypeScript hints √¢ such as whether a JS object has an iterator field √¢ to generate &lt;code&gt;mypy&lt;/code&gt; type hints that work with the Pyodide foreign function interface.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Decreasing cold start duration using snapshots&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Python startup is generally quite slow and importing a Python module can trigger a large amount of work. We avoid running Python startup during a cold start using memory snapshots.&lt;/p&gt;
      &lt;p&gt;When a Worker is deployed, we execute the Worker√¢s top-level scope and then take a memory snapshot and store it alongside your Worker. Whenever we are starting a new isolate for the Worker, we restore the memory snapshot and the Worker is ready to handle requests, with no need to execute any Python code in preparation. This improves cold start times considerably. For instance, starting a Worker that imports &lt;code&gt;fastapi&lt;/code&gt;, &lt;code&gt;httpx&lt;/code&gt; and &lt;code&gt;pydantic&lt;/code&gt; without snapshots takes around 10 seconds. With snapshots, it takes 1 second.&lt;/p&gt;
      &lt;p&gt;The fact that Pyodide is built on WebAssembly enables this. We can easily capture the full linear memory of the runtime and restore it.√Ç &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h4"&gt;Memory snapshots and Entropy&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;WebAssembly runtimes do not require features like address space layout randomization for security, so most of the difficulties with memory snapshots on a modern operating system do not arise. Just like with native memory snapshots, we still have to carefully handle entropy at startup to avoid using the XKCD random number generator (we√¢re very into actual randomness).&lt;/p&gt;
      &lt;p&gt;By snapshotting memory, we might inadvertently lock in a seed value for randomness. In this case, future calls for √¢random√¢ numbers would consistently return the same sequence of values across many requests.&lt;/p&gt;
      &lt;p&gt;Avoiding this is particularly challenging because Python uses a lot of entropy at startup. These include the libc functions &lt;code&gt;getentropy()&lt;/code&gt; and &lt;code&gt;getrandom()&lt;/code&gt; and also reading from &lt;code&gt;/dev/random&lt;/code&gt; and &lt;code&gt;/dev/urandom&lt;/code&gt;. All of these functions share the same implementation in terms of the JavaScript &lt;code&gt;crypto.getRandomValues()&lt;/code&gt; function.&lt;/p&gt;
      &lt;p&gt;In Cloudflare Workers, &lt;code&gt;crypto.getRandomValues()&lt;/code&gt; has always been disabled at startup in order to allow us to switch to using memory snapshots in the future. Unfortunately, the Python interpreter cannot bootstrap without calling this function. And many packages also require entropy at startup time. There are essentially two purposes for this entropy:&lt;/p&gt;
      &lt;p&gt;Hash randomization we do at startup time and accept the cost that each specific Worker has a fixed hash seed. Python has no mechanism to allow replacing the hash seed after startup.&lt;/p&gt;
      &lt;p&gt;For pseudorandom number generators (PRNG), we take the following approach:&lt;/p&gt;
      &lt;p&gt;At deploy time:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Seed the PRNG with a fixed √¢poison seed√¢, then record the PRNG state.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Replace all APIs that call into the PRNG with an overlay that fails the deployment with a user error.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Execute the top level scope of user code.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Capture the snapshot.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;At run time:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Assert that the PRNG state is unchanged. If it changed, we forgot the overlay for some method. Fail the deployment with an internal error.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;After restoring the snapshot, reseed the random number generator before executing any handlers.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;With this, we can ensure that PRNGs can be used while the Worker is running, but stop Workers from using them during initialization and pre-snapshot.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h4"&gt;Memory snapshots and WebAssembly state&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;An additional difficulty arises when creating memory snapshots on WebAssembly: The memory snapshot we are saving consists only of the WebAssembly linear memory, but the full state of the Pyodide WebAssembly instance is not contained in the linear memory.√Ç &lt;/p&gt;
      &lt;p&gt;There are two tables outside of this memory.&lt;/p&gt;
      &lt;p&gt;One table holds the values of function pointers. Traditional computers use a √¢Von Neumann√¢ architecture, which means that code exists in the same memory space as data, so that calling a function pointer is a jump to some memory address. WebAssembly has a √¢Harvard architecture√¢ where code lives in a separate address space. This is key to most of the security guarantees of WebAssembly and in particular why WebAssembly does not need address space layout randomization. A function pointer in WebAssembly is an index into the function pointer table.&lt;/p&gt;
      &lt;p&gt;A second table holds all JavaScript objects referenced from Python. JavaScript objects cannot be directly stored into memory because the JavaScript virtual machine forbids directly obtaining a pointer to a JavaScript object. Instead, they are stored into a table and represented in WebAssembly as an index into the table.&lt;/p&gt;
      &lt;p&gt;We need to ensure that both of these tables are in exactly the same state after we restore a snapshot as they were when we captured the snapshot.&lt;/p&gt;
      &lt;p&gt;The function pointer table is always in the same state when the WebAssembly instance is initialized and is updated by the dynamic loader when we load dynamic libraries √¢ native Python packages like numpy.√Ç &lt;/p&gt;
      &lt;p&gt;To handle dynamic loading:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;When taking the snapshot, we patch the loader to record the load order of dynamic libraries, the address in memory where the metadata for each library is allocated, and the function pointer table base address for relocations.√Ç &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;When restoring the snapshot, we reload the dynamic libraries in the same order, and we use a patched memory allocator to place the metadata in the same locations. We assert that the current size of the function pointer table matches the function pointer table base we recorded for the dynamic library.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of this ensures that each function pointer has the same meaning after we√¢ve restored the snapshot as it had when we took the snapshot.&lt;/p&gt;
      &lt;p&gt;To handle the JavaScript references, we implemented a fairly limited system. If a JavaScript object is accessible from globalThis by a series of property accesses, we record those property accesses and replay them when restoring the snapshot. If any reference exists to a JavaScript object that is not accessible in this way, we fail deployment of the Worker. This is good enough to deal with all the existing Python packages with Pyodide support, which do top level imports like:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;from js import fetch&lt;/code&gt;
      &lt;/quote&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Reducing cold start frequency using sharding&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Another important characteristic of our performance strategy for Python Workers is sharding. There is a very detailed description of what went into its implementation here. In short, we now route requests to existing Worker instances, whereas before we might have chosen to start a new instance.&lt;/p&gt;
      &lt;p&gt;Sharding was actually enabled for Python Workers first and proved to be a great test bed for it. A cold start is far more expensive in Python than in JavaScript, so ensuring requests are routed to an already-running isolate is especially important.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Where do we go from here?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;This is just the start. We have many plans to make Python Workers better:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;More developer-friendly tooling&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Even faster cold starts by utilising our isolate architecture&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Support for more packages&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Support for native TCP sockets, native WebSockets, and more bindings&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;To learn more about Python Workers, check out the documentation available here. To get help, be sure to join our Discord.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192748</guid><pubDate>Mon, 08 Dec 2025 14:42:01 +0000</pubDate></item><item><title>How Google Maps allocates survival across London's restaurants</title><link>https://laurenleek.substack.com/p/how-google-maps-quietly-allocates</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46203343</guid><pubDate>Tue, 09 Dec 2025 10:20:02 +0000</pubDate></item><item><title>Show HN: Local Privacy Firewall-blocks PII and secrets before ChatGPT sees them</title><link>https://github.com/privacyshield-ai/privacy-firewall</link><description>&lt;doc fingerprint="c2143cd0f4c98a72"&gt;
  &lt;main&gt;
    &lt;p&gt;üëã If you're trying PrivacyFirewall, please star the repo!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It helps others discover the project and motivates development. Takes 2 seconds ‚Üí ‚≠ê (top right)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;PrivacyFirewall is a local-first PII and secrets firewall for AI tools like ChatGPT, Claude, and Gemini.It blocks risky paste events, warns as you type, and (optionally) uses a lightweight on-device Transformer model for deeper PII detection.&lt;/p&gt;
    &lt;p&gt;üîí **No data ever leaves your machine.**Everything runs locally in your browser or through an optional local API.You can verify this by inspecting the network panel and reading the open-source code.&lt;/p&gt;
    &lt;p&gt;Modern AI tools make it extremely easy to leak sensitive information:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Emails &amp;amp; phone numbers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API keys &amp;amp; credentials&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customer or employee data&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IP &amp;amp; MAC address&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Internal logs &amp;amp; stack traces&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Regulated personal information (PII/PHI)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Traditional enterprise DLP tools don‚Äôt cover AI chat prompts.&lt;/p&gt;
    &lt;p&gt;PrivacyFirewall adds a zero-trust privacy shield BEFORE your text ever reaches a third-party AI system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;‚úã Human-in-the-loop protection for accidental leaks&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;üîí 100% local processing (browser + localhost only)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;‚ö° Practical protection (regex + optional transformer NER)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;üß© Friendly UX (warnings, paste-block modals, override options)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;üõ† OSS and auditable (MV3 + FastAPI + Hugging Face stack)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;**Lite Mode (regex-only)**Runs instantly in the extension ‚Äî no setup needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;**AI Mode (optional, local LLM)**Uses a local FastAPI agent + transformer model for deeper detection(People, organizations, locations, contextual entities).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;graph TD
    A[User Pastes/Types Text]:::blueNode --&amp;gt;|Intercept| B(Chrome Extension):::blueNode
    B --&amp;gt;|Regex Check| C{Contains Secrets/PII?}
    C --&amp;gt;|Yes &amp;amp; Paste| D[BLOCK &amp;amp; WARN]:::redNode
    C --&amp;gt;|Yes &amp;amp; Typing| E[SHOW WARNING BANNER]:::redNode
    C --&amp;gt;|No| F{Local Engine Online?}
    F --&amp;gt;|No| G[Allow]:::blueNode
    F --&amp;gt;|Yes| H[Python Local Engine]:::blueNode
    H --&amp;gt;|BERT Model| I{AI Detected PII?}
    I --&amp;gt;|Yes &amp;amp; Paste| D
    I --&amp;gt;|Yes &amp;amp; Typing| E
    I --&amp;gt;|No| G

    classDef blueNode fill:#2563eb,stroke:#1e40af,color:#fff
    classDef redNode fill:#dc2626,stroke:#b91c1c,color:#fff
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Regex Mode covers secrets quickly&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AI Mode enhances detection when the local engine is running&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If the agent goes offline ‚Üí extension falls back automatically&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Python 3.10+&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Chrome/Chromium/Edge&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Git&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ git clone https://github.com/privacyshield-ai/privacy-firewall.git

$ cd privacy-firewall

&lt;/code&gt;
    &lt;code&gt;$ cd src/engine  python -m venv .venv  

$ source .venv/bin/activate       # Windows: .venv\Scripts\activate

$ pip install --upgrade 

$ pip install -r requirements.txt

$ uvicorn main:app --host 127.0.0.1 --port 8765   

&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;First run downloads dslim/bert-base-NER (~400MB) to ~/.cache/huggingface.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;http://127.0.0.1:8765/health ‚Üí {"status":"ok"}&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Visit: chrome://extensions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable Developer mode&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Click Load unpacked&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select: src/extension/&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You now have Lite Mode running with regex-based detection.&lt;/p&gt;
    &lt;p&gt;Go to:&lt;/p&gt;
    &lt;p&gt;Paste:&lt;/p&gt;
    &lt;code&gt;My email is john.doe@example.com   `

&lt;/code&gt;
    &lt;p&gt;‚Üí Paste is intercepted, modal appears.&lt;/p&gt;
    &lt;p&gt;Paste:&lt;/p&gt;
    &lt;code&gt;AKIAIOSFODNN7EXAMPLE

&lt;/code&gt;
    &lt;p&gt;‚Üí Detected as AWS key ‚Üí blocked.&lt;/p&gt;
    &lt;p&gt;Enable AI Mode (when popup UI is ready), type:&lt;/p&gt;
    &lt;code&gt; Meeting notes from Sarah Thompson at HR...   

&lt;/code&gt;
    &lt;p&gt;‚Üí Local transformer flags PERSON ‚Üí warns you.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Email address&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Phone number&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Credit card candidate&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MAC address&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IPv4 address&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AWS access keys&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JWT tokens&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Private key blocks&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Generic API key / hash patterns&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;US SSN (basic pattern)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Powered by dslim/bert-base-NER:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;PERSON&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ORGANIZATION&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LOCATION&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Additional named entities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Helpful for ambiguous or context-based leakage&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;src/extension/        Chrome MV3 extension (content script, background worker, UI assets)
src/engine/           FastAPI service + transformer model wrapper
src/engine/models/    Model utilities (Hugging Face pipeline)
src/engine/tests/     Basic test harness for detection
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;HuggingFace models live in ~/.cache/huggingface/&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Delete this directory to force a fresh download&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Extension settings UI (enable/disable regex/AI modes)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add per-site allow/deny lists&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add secret-type redaction instead of full block&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Package engine as a binary or desktop app&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explore transformer.js for in-browser inference.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automated CI + browser testing&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Ensure the Python engine is running&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Confirm nothing else uses port 8765&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lite mode will still block regex-based secrets&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Ensure AI Mode is enabled + engine is online&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;NER models are probabilistic; long names work best&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Confidence threshold is tunable in transformer_detector.py&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PRs and issues are welcome!Please include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;OS &amp;amp; browser version&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reproduction steps&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Model version (if reporting AI false positives/negatives)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;No prompts or text ever leave your machine&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Extension communicates only with:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Browser local context&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Optional localhost API at 127.0.0.1:8765&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No analytics, telemetry, or external logging&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Review src/extension/content-script.js and DevTools ‚Üí Network tabto verify behavior&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License.See LICENSE for full text.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46206591</guid><pubDate>Tue, 09 Dec 2025 16:10:37 +0000</pubDate></item><item><title>Australia begins enforcing world-first teen social media ban</title><link>https://www.reuters.com/legal/litigation/australia-social-media-ban-takes-effect-world-first-2025-12-09/</link><description>&lt;doc fingerprint="2d87985519e0dfd9"&gt;
  &lt;main&gt;
    &lt;p&gt;SYDNEY, Dec 10 (Reuters) - Australia on Wednesday became the first country to ban social media for children under 16, blocking access in a move welcomed by many parents and child advocates but criticised by major technology companies and free-speech advocates.&lt;/p&gt;
    &lt;p&gt;Starting at midnight (1300 GMT on Tuesday), 10 of the largest platforms including TikTok, Alphabet's (GOOGL.O) YouTube and Meta's (META.O) Instagram and Facebook were ordered to block children or face fines of up to A$49.5 million ($33 million) under the new law, which is being closely watched by regulators worldwide.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;Prime Minister Anthony Albanese called it "a proud day" for families and cast the law as proof that policymakers can curb online harms that have outpaced traditional safeguards.&lt;/p&gt;
    &lt;p&gt;"This will make an enormous difference. It is one of the biggest social and cultural changes that our nation has faced," Albanese told a news conference on Wednesday.&lt;/p&gt;
    &lt;p&gt;"It's a profound reform which will continue to reverberate around the world."&lt;/p&gt;
    &lt;head rend="h2"&gt;READ A BOOK INSTEAD, PM TELLS YOUNGSTERS&lt;/head&gt;
    &lt;p&gt;In a video message, Albanese urged children to "start a new sport, new instrument, or read that book that has been sitting there for some time on your shelf," ahead of Australia's summer school break starting later this month.&lt;/p&gt;
    &lt;p&gt;Some of those below the cut-off age of 16 were anxious about adjusting to life without social media, but others were less concerned.&lt;/p&gt;
    &lt;p&gt;"I'm not really that emotional about it," said 14-year-old Claire Ni. "I'm kind of just, like, neutral."&lt;/p&gt;
    &lt;p&gt;Luna Dizon, 15, said she still had access to her TikTok, Instagram and Snapchat accounts, but worried about "culture shock" once the ban took full effect.&lt;/p&gt;
    &lt;p&gt;"I think eventually, without (social media), we'll learn how to adapt to it," she added.&lt;/p&gt;
    &lt;head rend="h2"&gt;TEENAGER SIGNS OFF WITH 'SEE YOU WHEN I'M 16'&lt;/head&gt;
    &lt;p&gt;While the government has said the ban would not be perfect in its operation, about 200,000 accounts were deactivated by Wednesday on TikTok alone, with "hundreds of thousands" more to be blocked in the next few days.&lt;/p&gt;
    &lt;p&gt;Many of the estimated 1 million children affected by the legislation also posted goodbye messages on social media.&lt;/p&gt;
    &lt;p&gt;"No more social media ... no more contact with the rest of the world," one teen wrote on TikTok.&lt;/p&gt;
    &lt;p&gt;"#seeyouwhenim16," said another.&lt;/p&gt;
    &lt;p&gt;Others said they would learn how to get round the ban.&lt;/p&gt;
    &lt;p&gt;"It's just kind of pointless, we're just going to create new ways to get on these platforms, so what's the point," said 14-year-old Claire Ni.&lt;/p&gt;
    &lt;head rend="h2"&gt;BAN HAS GLOBAL IMPLICATIONS&lt;/head&gt;
    &lt;p&gt;The rollout caps a year of debate over whether any country could practically stop children from using platforms embedded in daily life, and begins a live test for governments frustrated that social media firms have been slow to implement harm-reduction measures.&lt;/p&gt;
    &lt;p&gt;"I'm happy that they want to protect kids, and I'm happy that we have a chance to see how they do it and see if we can learn from them," said European Union lawmaker Christel Schaldemose, who wants to see greater protection for the bloc's children.&lt;/p&gt;
    &lt;p&gt;Albanese's centre-left government proposed the landmark law citing research showing harms to mental health from the overuse of social media among young teens, including misinformation, bullying and harmful depictions of body image.&lt;/p&gt;
    &lt;p&gt;Several countries from Denmark to New Zealand to Malaysia have signalled they may study or emulate Australia's model.&lt;/p&gt;
    &lt;p&gt;At a school in the German city of Bonn, students spoke favourably of a ban.&lt;/p&gt;
    &lt;p&gt;"Social media is highly addictive and doesn't really have any real advantages. I mean, there are advantages, such as being able to spread your opinion, but I think the disadvantages, especially the addiction, are much worse," said 15-year-old pupil Arian Klaar.&lt;/p&gt;
    &lt;p&gt;Julie Inman Grant, the U.S.-born eSafety Commissioner who is overseeing the ban, told Reuters on Wednesday a groundswell of American parents wanted similar measures.&lt;/p&gt;
    &lt;p&gt;"I hear from the parents and the activists and everyday people in America, 'we wish we had an eSafety commissioner like you in America, we wish we had a government that was going to put tween and teen safety before technology profits,'" she said in an interview at her office in Sydney.&lt;/p&gt;
    &lt;p&gt;'NOT OUR CHOICE': X SAYS WILL COMPLY&lt;/p&gt;
    &lt;p&gt;Elon Musk's X became the last of the 10 major platforms to take measures to cut off access to underage teens after publicly acknowledging on Wednesday that it would comply.&lt;/p&gt;
    &lt;p&gt;"It's not our choice - it's what the Australian law requires," X said on its website.&lt;/p&gt;
    &lt;p&gt;Australia has said the initial list of covered platforms would change as new products emerge and young users migrate.&lt;/p&gt;
    &lt;p&gt;Companies have told Canberra they will deploy a mix of age inference - estimating a user's age from their behaviour - and age estimation based on a selfie, alongside checks that could include uploaded identification documents.&lt;/p&gt;
    &lt;p&gt;For social media businesses, the implementation marks a new era of structural stagnation as user numbers flatline and time spent on platforms shrinks, studies show.&lt;/p&gt;
    &lt;p&gt;Platforms say they earn little from advertising to under-16s, but warn the ban disrupts a pipeline of future users. Just before the ban took effect, 86% of Australians aged eight to 15 used social media, the government said.&lt;/p&gt;
    &lt;p&gt;($1 = 1.5097 Australian dollars)&lt;/p&gt;
    &lt;p&gt;Reporting by Byron Kaye and Renju Jose; Additional reporting by James Redmayne and Cordelia Hsu; Writing by Alasdair Pal, Alexandra Hudson and Christine Chen; Editing by Andrew Heavens, Mark Potter, Lincoln Feast and Deepa Babington&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46208348</guid><pubDate>Tue, 09 Dec 2025 18:12:29 +0000</pubDate></item><item><title>Rubio stages font coup: Times New Roman ousts Calibri</title><link>https://www.reuters.com/world/us/rubio-stages-font-coup-times-new-roman-ousts-calibri-2025-12-09/</link><description>&lt;doc fingerprint="f1be8f403c685bdb"&gt;
  &lt;main&gt;
    &lt;p&gt;WASHINGTON, Dec 9 (Reuters) - U.S. Secretary of State Marco Rubio on Tuesday ordered diplomats to return to using Times New Roman font in official communications, calling his predecessor Antony Blinken's decision to adopt Calibri a "wasteful" diversity move, according to an internal department cable seen by Reuters.&lt;/p&gt;
    &lt;p&gt;The department under Blinken in early January 2023 had switched to Calibri, a modern sans-serif font, saying this was a more accessible font for people with disabilities because it did not have the decorative angular features and was the default in Microsoft products.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;A cable dated December 9 sent to all U.S. diplomatic posts said that typography shapes the professionalism of an official document and Calibri is informal compared to serif typefaces.&lt;/p&gt;
    &lt;p&gt;"To restore decorum and professionalism to the Department‚Äôs written work products and abolish yet another wasteful DEIA program, the Department is returning to Times New Roman as its standard typeface," the cable said.&lt;/p&gt;
    &lt;p&gt;"This formatting standard aligns with the President‚Äôs One Voice for America‚Äôs Foreign Relations directive, underscoring the Department‚Äôs responsibility to present a unified, professional voice in all communications," it added.&lt;/p&gt;
    &lt;p&gt;The State Department did not immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Some studies suggest that sans-serif fonts, such as Calibri, are easier to read for those with certain visual disabilities.&lt;/p&gt;
    &lt;p&gt;Trump, a Republican, moved quickly after taking office in January to eradicate federal DEI programs and discourage them in the private sector and education, including by directing the firing of diversity officers at federal agencies and pulling grant funding for a wide range of programs.&lt;/p&gt;
    &lt;p&gt;DEI policies became more widespread after nationwide protests in 2020 against police killings of unarmed Black people, spurring a conservative backlash. Trump and other critics of diversity initiatives say they are discriminatory against white people and men and have eroded merit-based decision making.&lt;/p&gt;
    &lt;p&gt;Reporting by Humeyra Pamuk; Editing by Don Durfee and Lisa Shumaker&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46212438</guid><pubDate>Wed, 10 Dec 2025 00:08:34 +0000</pubDate></item><item><title>Common Lisp, ASDF, and Quicklisp: packaging explained</title><link>https://cdegroot.com/programming/commonlisp/2025/11/26/cl-ql-asdf.html</link><description>&lt;doc fingerprint="b7209a1f8fa70f94"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Common Lisp, ASDF, and Quicklisp: packaging explained&lt;/head&gt;
    &lt;p&gt;If there is one thing that confuses newcomers to Common Lisp, it is the interplay of built-in CL functionality, add-ons like Quicklisp and ASDF, and what all the words mean.&lt;/p&gt;
    &lt;p&gt;Common Lisp is old, and its inspiration is even older. It was developed when there was zero consensus on how file systems worked, operating systems were more incompatible than you can probably imagine, and that age shows. It pinned down terminology way before other languages got to the same point, and, as it happens so often, the late arrivals decided that they needed different words and these words stuck.&lt;/p&gt;
    &lt;p&gt;So let√¢s do a bit of a deep dive and see how all the bits and pieces work and why they are there. All examples are using SBCL and might be SBCL-specific. Check your Lisp√¢s manual if you use something else. Also, I√¢m (still) linking to the old LispWorks-provided HyperSpec as I√¢m not sure that the newer versions are fully done yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common Lisp&lt;/head&gt;
    &lt;p&gt;Common Lisp comes with just the bare essentials to work with files. It has to, as that single specification had to work on microcomputers, mainframes, and all sorts of minicomputers. Even today with essentially just two branches of the operating system family alive, the difference are big between Unix derivatives with a single hierarchy (and one of them, macOS, by default with a case-insensitive interpretation) and MS-DOS derivaties with drive letters and backslashes but also the option to have network-style paths with double backslashes. So Common Lisp has a somewhat odd system of √¢namestrings√¢ (plain strings) and √¢pathnames√¢ (weird strings). It is not super important and the spec has details, the tl&amp;amp;dr is that sometimes you will see a special reader macro &lt;code&gt;#P"/foo/bar"&lt;/code&gt; instead of just &lt;code&gt;"/foo/bar"&lt;/code&gt; and the docs will tell
you which of these two is acceptable as an argument for what function. I just wanted to get
that out of the way first. They HyperSpec has all the details, of course.&lt;/p&gt;
    &lt;head rend="h3"&gt;Loading code from files.&lt;/head&gt;
    &lt;p&gt;With files out of the way, next up is &lt;code&gt;LOAD&lt;/code&gt;. It loads a file √¢into the Lisp environment√¢ (which
means your running image), but exactly how the file is named and whether it will load a source
file or a compiled file is system-dependent. So&lt;/p&gt;
    &lt;code&gt;(load "foo")
&lt;/code&gt;
    &lt;p&gt;can load &lt;code&gt;foo.lisp&lt;/code&gt; or &lt;code&gt;foo.fasl&lt;/code&gt; or maybe even &lt;code&gt;foo.obj&lt;/code&gt; if a Lisp implementation compiles to
C object files. If it is a source file, it√¢ll evaluate all the forms and do some system-specific
thing with them. The end result is that, well, everything in the file will now be ready for you
to use. So if we have:&lt;/p&gt;
    &lt;code&gt;(defun hello ()
  (print "Hello, world!"))

(print "Done loading!")
&lt;/code&gt;
    &lt;p&gt;and we open SBCL:&lt;/p&gt;
    &lt;code&gt;CL-USER(1): (load "test")

"Done loading"
T
CL-USER(2): (hello)

"Hello, world!"
"Hello, world!"
&lt;/code&gt;
    &lt;p&gt;Nothing too surprising there. In case we want to speed up loading, we can compile the file:&lt;/p&gt;
    &lt;code&gt;CL-USER(7): (compile-file "test")

; compiling file "/home/cees/tmp/test.lisp" (written 26 NOV 2025 09:03:19 PM):

; wrote /home/cees/tmp/test.fasl
; compilation finished in 0:00:00.004
#P"/home/cees/tmp/test.fasl"
NIL
NIL
&lt;/code&gt;
    &lt;p&gt;and the next time we ask to load &lt;code&gt;"test"&lt;/code&gt;, the FASL (√¢fast load√¢) file should be loaded. It is purely a time-saver
as the FASL file has been pre-parsed into your Lisp√¢s in-memory format so can be loaded very
quickly (bypassing &lt;code&gt;READ&lt;/code&gt; with all its bells and whistles). FASL files are implementation dependent and more often than not even version dependent. This
is pretty much everything that the standard has to say about getting code into the system, and as you
can see, it√¢s not much.&lt;/p&gt;
    &lt;p&gt;There is also &lt;code&gt;PROVIDE&lt;/code&gt; and &lt;code&gt;REQUIRE&lt;/code&gt;, which operate on something
that the standard calls modules (and which are kept in a variable called &lt;code&gt;*modules*&lt;/code&gt;) but the
standard designates this as deprecated so let√¢s skip it. Just know it is still lingering there. Don√¢t
use it (not even when packages √¢helpfully√¢ wrap it).&lt;/p&gt;
    &lt;head rend="h3"&gt;Packages&lt;/head&gt;
    &lt;p&gt;That &lt;code&gt;CL-USER&lt;/code&gt; in the prompt is the name of the package that you are in. Here is a pretty
bad choice of naming, and an endless source of confusion. A package is a namespace, nothing else, and
the spec says so much:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A package establishes a mapping from names to symbols.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;These days, we associate the concept of √¢package√¢ probably with more than that. A bundle of software, with files and maybe some metadata, a thing you can download from somewhere, most likely. But in Common Lisp, it√¢s just a tool to map symbol names (strings in your source code) to symbols (internal addresses in memory). It√¢s a pretty versatile facility and you should read the docs on &lt;code&gt;DEFPACKAGE&lt;/code&gt;. It√¢s is quite powerful, as it can &lt;code&gt;:use&lt;/code&gt; other packages, it can shadow symbols,
and whatnot, but at the end of the day, all that happens is that when you type:&lt;/p&gt;
    &lt;code&gt;(hello)
&lt;/code&gt;
    &lt;p&gt;The REPL will use the current package (in &lt;code&gt;*package*&lt;/code&gt;) to translate &lt;code&gt;hello&lt;/code&gt; to whatever function
is in memory, which should exist in the current package (here &lt;code&gt;COMMON-LISP-USER&lt;/code&gt;, commonly aliased to
&lt;code&gt;CL-USER&lt;/code&gt;) or in any packages it inherits from (√¢uses√¢). You can explicitly tell Lisp to look into
another package (&lt;code&gt;my-package:hello&lt;/code&gt; is a different function) and even ignore
that package√¢s explicit list of exported symbols by using a double colon (but don√¢t make a habit
out of prying into other packages, it breaks modularity). There are a ton of details, but
what counts is that a Common Lisp package is just an in-memory namespace thing, a bunch of
connected lookup tables that help the parser map the strings in the files you load to the
correct items inside your running image.&lt;/p&gt;
    &lt;p&gt;Nothing more, nothing less.&lt;/p&gt;
    &lt;head rend="h3"&gt;Systems&lt;/head&gt;
    &lt;p&gt;Common Lisp documentation often talks about systems in a general way like it is an intrinsic part of the language. However, the standard is vague. In the chapter on √¢System construction√¢ it deals with loading√¢the little bit of functionality we already discussed√¢and √¢features√¢, which are essentially just flags that are used by the &lt;code&gt;#+&lt;/code&gt; and &lt;code&gt;#-&lt;/code&gt; reader macros to make bits of code that
is loaded conditional on the presence of features.&lt;/p&gt;
    &lt;p&gt;That is all the standard has to say about systems. You can load files and you can make compilation of these files conditional on feature flags.&lt;/p&gt;
    &lt;head rend="h3"&gt;So, where does that leave us?&lt;/head&gt;
    &lt;p&gt;In a sense, this is all you need. I mean, you can take someone else√¢s files and &lt;code&gt;LOAD&lt;/code&gt; them, and
they can be made somewhat portable by using features and saying &lt;code&gt;#+sbcl&lt;/code&gt; (this code only to be
compiled on SBCL) or &lt;code&gt;#-linux&lt;/code&gt; (do not compile this on Linux), and the files can organize themselves
by using &lt;code&gt;DEFPACKAGE&lt;/code&gt; and friends to separate the code into namespaces so everybody can write
code using names like &lt;code&gt;HELLO&lt;/code&gt; and not step on each other√¢s toes.&lt;/p&gt;
    &lt;p&gt;Still, that Common Lisp √¢system√¢ thing√¢¬¶ it√¢s a bit vague and maybe there√¢s a hook there to build something more?&lt;/p&gt;
    &lt;head rend="h2"&gt;Another System Definition Facility&lt;/head&gt;
    &lt;p&gt;Some Common Lisp implementations come with a &lt;code&gt;DEFSYSTEM&lt;/code&gt;, but that is not portable. There
were early (we√¢re in 1989-ish now) attempts to have a common version, &lt;code&gt;MK:DEFSYSTEM&lt;/code&gt;, which
still works and is used by some projects. At the turn of a century, another version of &lt;code&gt;DEFSYSTEM&lt;/code&gt;
was created under the name &lt;code&gt;ASDF&lt;/code&gt;, which modernized things and quickly turned into the de facto
standard. It can do a lot of things and has extensive docs on its website, but we√¢ll focus here on the essentials.&lt;/p&gt;
    &lt;p&gt;So, what is a system? Well, a library? A, err, package? Well, it should be named a package and if Common Lisp were born a couple of decades later it might have been called a package, but we have already seen that that name has been given to something closer to what we would probably call √¢module√¢ today. √¢System√¢ it is, then, and ASDF √¢defines√¢ them.&lt;/p&gt;
    &lt;p&gt;Still, the closest analogy of a system is a package or a library: a bunch of Lisp code that together defines some functionality. It√¢s not a perfect comparison, because a lot of Lisp libraries contain multiple systems: at the very least, it is customary to have your code define separate systems for regular code and for test code, and often more systems are defined for, say, optional or contributed code. In any case, it is not intrinsic to Common Lisp, though, so ASDF strictly adds functionality:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It allows you to define a system (&lt;code&gt;ASDF:DEFSYSTEM&lt;/code&gt;). That√¢s the core function: you tell it that you have a system with a certain name, and description, and all sorts of metadata; and most importantly, what source files are part of the system.&lt;/item&gt;
      &lt;item&gt;It allows you to define dependencies between systems in your &lt;code&gt;DEFSYSTEM&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;It allows you to load such systems wholesale. Instead of the individual files, or a developer√¢s homebrew loading script, you can now work on a higher level and load a system by name.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A system still is not a √¢real√¢ Common Lisp thing: all that it does with respect to the standard is a bunch of &lt;code&gt;LOAD&lt;/code&gt; and &lt;code&gt;COMPILE-FILE&lt;/code&gt; calls. It will keep metadata in memory
about systems that are loaded, but under the hood, loading code is all it does. It comes
with extensive documentation and can do a lot of things like additional compilation steps, manage
test runs, etcetera, but if you squint, it just loads code.&lt;/p&gt;
    &lt;p&gt;An ASDF file, with the extension &lt;code&gt;.asd&lt;/code&gt;, is also just a Lisp source. The only special thing about
it is that the extension signals to ASDF that it is the file to look for when ASDF is searching
for systems, the one that has the system
definition in a given constellation of source files and directories.&lt;/p&gt;
    &lt;p&gt;It is important to realize that a √¢system√¢ and a package are entirely different things: one is an entity in an add-on tool, the other is intrinsic to Common Lisp√¢s namespacing. They can have the same name and often enough, they have the same name (your ASDF system √¢foo√¢ will likely define a package √¢FOO√¢ and it is helpful if that lives in a Git repository called √¢foo√¢ which has a file named √¢foo.asd√¢) but they are different things living in, well, different namespaces and should not be confused with each other. One is intrinsic, the other an optional (but widely used) add-on and they are fully orthogonal things.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where does ASDF gets its systems from?&lt;/head&gt;
    &lt;p&gt;Well, we have a √¢standard√¢, albeit a de facto one, to bundle Lisp code and describe how to load it. But if you say √¢this system here is called FOO and is dependent on BAR√¢, how does ASDF find BAR? The answer is very simple: it looks in predefined locations on your local disk (and nowhere else!). There are two predefined locations, one older and one currently preferred:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;~/common-lisp&lt;/code&gt;, the old one;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.local/share/common-lisp/source&lt;/code&gt;, the XDG-compliant currently preferred one. Use this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That√¢s all. You can extend that list by a very flexible but somewhat complicated mechanism called √¢source registries√¢, extensively documented, but essentially, the process looks like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You refer to a system called &lt;code&gt;foo&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;ASDF will look for &lt;code&gt;foo.asd&lt;/code&gt;under the configured directories;&lt;/item&gt;
      &lt;item&gt;If found, it will load that file, and the &lt;code&gt;DEFSYSTEM&lt;/code&gt;in there will do the rest.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This process recurses, so when system &lt;code&gt;foo&lt;/code&gt; depends on system &lt;code&gt;bar&lt;/code&gt; then the process will repeat, until
everything is loaded or an error occurs. All the systems will be found, defined (in your image/memory),
and loaded in the right order (depth-first so that dependencies are loaded, their packages defined and
functions and macros and variables ready for use, before dependents are).&lt;/p&gt;
    &lt;head rend="h3"&gt;So, where does that leave us?&lt;/head&gt;
    &lt;p&gt;We upgraded from √¢here are a couple of Lisp files, good luck!√¢ to √¢here is a library with dependencies√¢. Good progress. All you need to do now is download the library (as a Zip file or a tarball), unpack it under &lt;code&gt;~/.local/share/common-lisp/source&lt;/code&gt;, and load if with
&lt;code&gt;ASDF:LOAD-SYSTEM&lt;/code&gt;. Of course, the system may declare dependencies so you may get an error
message. Easy enough, hunt for the dependency on the Net, download and unpack that, try again, find the next one.&lt;/p&gt;
    &lt;p&gt;Not perfect, but, well, progress?&lt;/p&gt;
    &lt;head rend="h2"&gt;Quicklisp enters the stage&lt;/head&gt;
    &lt;p&gt;It√¢s still a bit primitive, though. I mean, when coders were sending each other QIC tapes this may have been sufficient, but then someone went and had to invent the Internet and now we just push data over the information superhighway. We should be able to do better, not? Like √¢Perl in 1995√¢ better, even?&lt;/p&gt;
    &lt;p&gt;Just ilke ASDF is an optional add-on to what Common Lisp provides, Quicklisp is an optional add-on to what ASDF offers. Essentially, it does two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It adds a new directory to the places where ASDF can find systems;&lt;/item&gt;
      &lt;item&gt;It offers some functions to download a system from √¢wherever√¢, which includes √¢the Internet√¢.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It hook into ASDF√¢s dependency resolution so that if there are more dependencies needed, Quicklisp will go and fetch them as well.&lt;/p&gt;
    &lt;p&gt;Tadaa: problem solved! We can just open SBCL and say&lt;/p&gt;
    &lt;code&gt;(ql:quickload "foo")
&lt;/code&gt;
    &lt;p&gt;and admire a scrolling list of systems being downloaded, unpacked, loaded, analyzed for dependencies, dependencies being loaded, and so on.&lt;/p&gt;
    &lt;head rend="h3"&gt;So, where does that leave us?&lt;/head&gt;
    &lt;p&gt;We have all the functionality, but there√¢s one final issue: it is √¢always on√¢. In a lot of other languages, if you start your REPL (say, in Python or in Ruby or in Elixir) in a certain directory, that carries significance. The language runtime will look for a special project file, probably, and set up search paths so that they work for that project. Common Lisp has no such concept, not even after you load ASDF and Quicklisp. So if you have a directory &lt;code&gt;~/my-code/my-awesome-lisp-project&lt;/code&gt; with
a &lt;code&gt;my-awesome-lisp-project.asd&lt;/code&gt; in there√¢¬¶. Neither Quicklisp nor ASDF is going to bother
about the current directory and magically find your system.&lt;/p&gt;
    &lt;p&gt;You must play with their rules. Luckily, the rules are simple: go to &lt;code&gt;~/.local/share/common-lisp/source&lt;/code&gt; and drop symlinks in there to your projects so that ASDF
can find them. That also means that it
doesn√¢t matter where you start &lt;code&gt;sbcl&lt;/code&gt; or Sly or SLIME from, your code will always be found. And
when you then load your system with &lt;code&gt;QL:QUICKLOAD&lt;/code&gt;, its dependencies will automatically be pulled
in (&lt;code&gt;ASDF:LOAD-SYSTEM&lt;/code&gt; will still operate locally. It will, of course, use dependencies that
Quicklisp found and downloaded in previous runs).&lt;/p&gt;
    &lt;head rend="h2"&gt;Final tips&lt;/head&gt;
    &lt;head rend="h3"&gt;Read the source, Luke&lt;/head&gt;
    &lt;code&gt;$ git clone https://gitlab.common-lisp.net/asdf/asdf.git
$ git clone https://github.com/quicklisp/quicklisp-client
$ guix shell cloc -- cloc quicklisp-client asdf
     363 text files.
     264 unique files.
     104 files ignored.

github.com/AlDanial/cloc v 2.06  T=0.13 s (1984.8 files/s, 332115.5 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Lisp                           219           3981           3889          31548
Markdown                         3            305              0           1173
HTML                             1             11             50            767
Bourne Shell                    10             40            120            526
Text                            18            105              0            357
make                             4             88             61            312
CSS                              1             60              8            236
YAML                             3             28             51            202
Perl                             2             22              8            117
DOS Batch                        2             23             13             74
C                                1              0              0              1
-------------------------------------------------------------------------------
SUM:                           264           4663           4200          35313
-------------------------------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;It√¢s not that much code, and 7400 lines of that is the &lt;code&gt;UIOP&lt;/code&gt; package that ASDF
includes. UIOP is a package that is very useful in its own right as it is full of
utilities that help you make your code less implementation-dependent, but it won√¢t
teach you much about ASDF. So 25KLOC, tops. Without tests and contrib and whatnot, each
package is around 5000 lines of well-written Lisp and worth learning. It√¢s helped
me more than once to understand especially ASDF-VM to just open the code and figure
out what exactly is going on.&lt;/p&gt;
    &lt;head rend="h3"&gt;KISS: Use package-inferred-system and a single source tree.&lt;/head&gt;
    &lt;p&gt;Put your Lisp code in directories under, I dunno, say &lt;code&gt;~/Code/CL&lt;/code&gt;. Symlink that directory to
&lt;code&gt;~/.local/share/common-lisp/source&lt;/code&gt; and ASDF will be able to find all your
systems. I√¢ve done some magic using GUIX Home and Stow and whatnot and had to dig
around into how things worked, not recommended. If you have dependencies that are not
in Quicklisp (or Ultralisp, which is worth adding), then check
them out in a central spot (I use &lt;code&gt;~/OpenSource&lt;/code&gt;) and symlink it into &lt;code&gt;~/quicklisp/local-projects&lt;/code&gt;.
That way, all your dependency management is in one spot, the Quicklisp directory, whether you
download them or Quicklisp did the job.&lt;/p&gt;
    &lt;p&gt;Read about ASDF√¢s package-inferred-system and use it. It√¢ll keep you from having to spend much time writing &lt;code&gt;.asd&lt;/code&gt; files.
As the docs say, ASDF itself uses
it
and since switching to it, there√¢s no going back for me. In a nutshell, every file is
now expected to be a package and a system, same name, so that bit of confusion
goes away. One of my project repos (my main monorepo as of lately, I√¢m slowly
moving all my other code to it) has a very short ASDF definition:&lt;/p&gt;
    &lt;code&gt;#-asdf3.1 (error "CA.BERKSOFT requires ASDF 3.1 or later.")
(asdf:defsystem "ca.berksoft"
  :class :package-inferred-system)
&lt;/code&gt;
    &lt;p&gt;It also has some necessary &lt;code&gt;REGISTER-SYSTEM-PACKAGES&lt;/code&gt; calls to register Coalton packages. Sometimes you
have dependencies that don√¢t work well with this scheme and this is the work-around, a small drawback
that is dwarved by the advantages. But essentially, these three lines are it.&lt;/p&gt;
    &lt;p&gt;With that setup, a library to calculate the color temperature of an RGB color, say, lives in &lt;code&gt;l/gfx/color-temperature.lisp&lt;/code&gt; and starts with:&lt;/p&gt;
    &lt;code&gt;(uiop:define-package :ca.berksoft/l/gfx/color-temperature
  (:use :cl :infix-math :try)
  (:export :temp-&amp;gt;rgb))
&lt;/code&gt;
    &lt;p&gt;Note that I use the UIOP version of &lt;code&gt;defpackage&lt;/code&gt;. It√¢s a good habit to use the UIOP versions of
functions where possible; it√¢ll increase portability and more often than not, the UIOP functions
clean up confusion or shortcomings of the standard.&lt;/p&gt;
    &lt;p&gt;And that is all. ASDF, when I instruct it to load the system √¢ca.berksoft/l/gfx/color-temperature√¢, will stumble upon the top level &lt;code&gt;.asd&lt;/code&gt; file, and then will start interpreting the rest (√¢l/gfx/color-temperature√¢) as
a relative path under its package-inferred-system functionality. It finds that file, registers it as an ASDF system and loads it, which creates the Common Lisp package. Very simple, very clean. Give it
a try.&lt;/p&gt;
    &lt;p&gt;Questions? Jump on Libera IRC and join the &lt;code&gt;#commonlisp&lt;/code&gt; channel, I usually keep a close eye on
it. You can also DM me on Mastodon or drop me a mail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46216446</guid><pubDate>Wed, 10 Dec 2025 11:10:58 +0000</pubDate></item><item><title>Show HN: Wirebrowser ‚Äì A JavaScript debugger with breakpoint-driven heap search</title><link>https://github.com/fcavallarin/wirebrowser</link><description>&lt;doc fingerprint="ac2b8e0449babf04"&gt;
  &lt;main&gt;
    &lt;p&gt;Wirebrowser is a debugging, interception, and memory-inspection toolkit powered by the Chrome DevTools Protocol (CDP). It unifies network manipulation, API testing, automation scripting, and deep JavaScript memory inspection into one interface.&lt;lb/&gt; With features like Breakpoint-Driven Heap Search and real-time Live Object Search, Wirebrowser provides researchers and engineers with precise, high-visibility tools for client-side analysis, reverse engineering, and complex application debugging.&lt;/p&gt;
    &lt;p&gt;Intercept, block, rewrite, and replay HTTP requests and responses in real time.&lt;/p&gt;
    &lt;p&gt;Inspect, search, and modify JavaScript memory using both live heap analysis and heap snapshots, with full support for object identity search, primitive search (via snapshots), structural matching, and runtime patching.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Live Object Search ‚Äî Search all live JavaScript objects using regex or structural matching, and patch matched objects at runtime to alter state or behavior dynamically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Static Heap Snapshot Search Capture a full V8 heap snapshot and search all objects and primitives, including strings and closure-captured values that are unreachable through the Runtime domain.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Origin Trace (BDHS) ‚Äî Performs automatic debugger pauses and captures a full heap snapshot at each stop. Every snapshot is searched to identify the user-land function responsible for creating or mutating the target value. Framework and vendor scripts are filtered out via heuristics.&lt;/p&gt;&lt;lb/&gt;BDHS also includes a tolerance window that samples snapshots before and after the first match, providing contextual insight into when and how a value is introduced or mutated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A shared similarity engine used across Live Object Search, Heap Snapshots, and BDHS timelines. Enables shape-based searches, clustering, and origin tracing for objects that evolve over time.&lt;/p&gt;
    &lt;p&gt;Create, edit, and execute API requests with variable substitution and structured collections, integrating Postman-style workflows directly into the debugging environment.&lt;/p&gt;
    &lt;p&gt;A full technical deep-dive is available here: üëâ https://fcavallarin.github.io/wirebrowser/BDHS-Origin-Trace&lt;/p&gt;
    &lt;p&gt;Below is a quick visual tour of Wirebrowser‚Äôs most distinctive capabilities.&lt;/p&gt;
    &lt;p&gt;A short walkthrough of Wirebrowser‚Äôs advanced memory-analysis capabilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Live Object Search ‚Äî real-time search and runtime patching of live JS objects.&lt;/item&gt;
      &lt;item&gt;Origin Trace (BDHS) ‚Äî identify the user-land function responsible for creating or mutating the object during debugging.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Intercept, rewrite, block, and replay HTTP requests and responses.&lt;/p&gt;
    &lt;p&gt;Search and patch live JS objects using regex or structural matching.&lt;/p&gt;
    &lt;p&gt;Capture snapshots on each debugger pause to locate the user-land function responsible for object creation or mutation.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/fcavallarin/wirebrowser.git
cd wirebrowser
npm install
npm run build&lt;/code&gt;
    &lt;code&gt;npm run wirebrowser&lt;/code&gt;
    &lt;p&gt;On some Linux distributions, Electron may fail to start due to process sandboxing restrictions, showing errors such as:&lt;/p&gt;
    &lt;code&gt;The SUID sandbox helper binary was found, but is not configured correctly.
&lt;/code&gt;
    &lt;p&gt;This is a known issue in Electron ([electron/electron#42510]).&lt;lb/&gt; The most common solution is to disable AppArmor restrictions:&lt;/p&gt;
    &lt;code&gt;sudo sysctl -w kernel.apparmor_restrict_unprivileged_userns=0
&lt;/code&gt;
    &lt;p&gt;Beyond the core Network and Memory workflows, Wirebrowser offers several supporting modules that enhance debugging, testing, and automation workflows.&lt;/p&gt;
    &lt;p&gt;Create, edit, and execute API requests with variable substitution and organized collections.&lt;lb/&gt; Useful for testing endpoints, iterating on backend logic, or interacting with APIs directly from the same environment used for debugging the client.&lt;/p&gt;
    &lt;p&gt;Run browser-side or Node.js scripts, either manually or triggered by events such as page load.&lt;lb/&gt; Automation scripts have access to an &lt;code&gt;Utils&lt;/code&gt; object that exposes helpers for interacting with the browser, pages, variables, iterators, and HTTP utilities.&lt;/p&gt;
    &lt;code&gt;const userId = Utils.getVar("userId");
const page = Utils.getPage(1);
page.on("request", req =&amp;gt; req.continue());
await page.goto(`https://example.com/${userId}`);&lt;/code&gt;
    &lt;p&gt;A collection of small tools frequently needed during debugging and analysis, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encode or decode strings in multiple formats:&lt;/item&gt;
      &lt;item&gt;Create, verify, and decode JSON Web Tokens (JWTs).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most Wirebrowser actions can be performed either globally (across all open tabs/pages) or targeted to a single tab. This lets you choose whether a rule or inspection should affect the whole browser session or only a specific page.&lt;lb/&gt; Every tab/page opened by Wirebrowser has a unique integer &lt;code&gt;tabId&lt;/code&gt;. Use this &lt;code&gt;tabId&lt;/code&gt; to scope actions.&lt;/p&gt;
    &lt;p&gt;UI Notes&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many panels offer a scope selector (Global / Specific Tab ID) for quick changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wirebrowser is built with React and Node.js, using plain JavaScript to keep the codebase lightweight and hackable.&lt;lb/&gt; TypeScript or JSDoc-based typing may be introduced in the future for enhanced maintainability.&lt;/p&gt;
    &lt;p&gt;The following areas are being explored for future development:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SPA crawling ‚Äî automated crawling of single-page applications to map navigation flows and surface client-side behaviors.&lt;/item&gt;
      &lt;item&gt;DOM XSS scanning ‚Äî analysis of potential DOM-based XSS injection points during crawls or on-demand checks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wirebrowser is being built in the open ‚Äî contributions and feedback are welcome!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üí¨ Chat coming soon (Discord or Matrix)&lt;/item&gt;
      &lt;item&gt;üê¶ Follow updates on X/Twitter: https://x.com/wirebrowser&lt;/item&gt;
      &lt;item&gt;üß† Issues &amp;amp; Ideas: https://github.com/fcavallarin/wirebrowser/issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions and pull requests are welcome!&lt;lb/&gt; Open an issue or pull request ‚Äî even small suggestions help improve Wirebrowser.&lt;/p&gt;
    &lt;p&gt;Wirebrowser‚Ñ¢ is distributed under the MIT License.&lt;lb/&gt; See the LICENSE file for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46218101</guid><pubDate>Wed, 10 Dec 2025 14:30:43 +0000</pubDate></item><item><title>Size of Life</title><link>https://neal.fun/size-of-life/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219346</guid><pubDate>Wed, 10 Dec 2025 16:02:57 +0000</pubDate></item><item><title>Qwen3-Omni-Flash-2025-12-01Ôºöa next-generation native multimodal large model</title><link>https://qwen.ai/blog?id=qwen3-omni-flash-20251201</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219538</guid><pubDate>Wed, 10 Dec 2025 16:13:38 +0000</pubDate></item><item><title>Valve: HDMI Forum Continues to Block HDMI 2.1 for Linux</title><link>https://www.heise.de/en/news/Valve-HDMI-Forum-Continues-to-Block-HDMI-2-1-for-Linux-11107440.html</link><description>&lt;doc fingerprint="8cfd1d1d0995dc70"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Valve: HDMI Forum Continues to Block HDMI 2.1 for Linux&lt;/head&gt;
    &lt;p&gt;Technically, the Steam Machine supports HDMI 2.1. However, Valve and AMD are not allowed to offer an open-source driver for it.&lt;/p&gt;
    &lt;p&gt;The HDMI Forum, responsible for the HDMI specification, continues to stonewall open source. Valve's Steam Machine theoretically supports HDMI 2.1, but the mini-PC is software-limited to HDMI 2.0. As a result, more than 60 frames per second at 4K resolution are only possible with limitations.&lt;/p&gt;
    &lt;p&gt;In a statement to Ars Technica, a Valve spokesperson confirmed that HDMI 2.1 support is "still a work-in-progress on the software side." "We‚Äôve been working on trying to unblock things there."&lt;/p&gt;
    &lt;p&gt;The Steam Machine uses an AMD Ryzen APU with a Radeon graphics unit. Valve strictly adheres to open-source drivers, but the HDMI Forum is unwilling to disclose the 2.1 specification. According to Valve, they have validated the HDMI 2.1 hardware under Windows to ensure basic functionality.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;head rend="h3"&gt;No Change After Almost Two Years&lt;/head&gt;
    &lt;p&gt;The restriction imposed by the HDMI Forum was already criticized in early 2024 by an AMD employee responsible for Linux. Even then, according to AMD, they had submitted a functional, HDMI 2.1-compatible driver, which the HDMI Forum rejected.&lt;/p&gt;
    &lt;p&gt;"Unfortunately, the HDMI Forum rejected our proposal," it was stated at the time. "At this time an open source HDMI 2.1 implementation is not possible without running afoul of the HDMI Forum requirements."&lt;/p&gt;
    &lt;p&gt;Only HDMI 2.1 offers sufficient bandwidth for 120 or 144 Hertz at 3840 √ó 2160 pixels without compression. Furthermore, this version introduced manufacturer-independent variable refresh rates (HDMI VRR). Valve enables 4K and 120 Hertz using chroma subsampling, a compression technique that is particularly noticeable with text. VRR functions in the form of AMD's Freesync, which requires compatible displays.&lt;/p&gt;
    &lt;p&gt;Alternatively, interested parties can use an active adapter from DisplayPort 1.4 to HDMI 2.1 to increase the frame rate without compression. However, they do not officially support VRR. Popular variants from Club3D are no longer available; offers from less well-known providers (starting from 35,67 ‚Ç¨) are still available in price comparisons.&lt;/p&gt;
    &lt;p&gt;(mma)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220488</guid><pubDate>Wed, 10 Dec 2025 17:20:06 +0000</pubDate></item><item><title>Auto-grading decade-old Hacker News discussions with hindsight</title><link>https://karpathy.bearblog.dev/auto-grade-hn/</link><description>&lt;doc fingerprint="a207dbd71fe07fd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Auto-grading decade-old Hacker News discussions with hindsight&lt;/head&gt;
    &lt;p&gt;TLDR: https://karpathy.ai/hncapsule/&lt;/p&gt;
    &lt;p&gt;Yesterday I stumbled on this HN thread Show HN: Gemini Pro 3 hallucinates the HN front page 10 years from now, where Gemini 3 was hallucinating the frontpage of 10 years from now. One of the comments struck me a bit more though - Bjartr linked to the HN frontpage from exactly 10 years ago, i.e. December 2015. I was reading through the discussions of 10 years ago and mentally grading them for prescience when I realized that an LLM might actually be a lot better at this task. I copy pasted one of the article+comment threads manually into ChatGPT 5.1 Thinking and it gave me a beautiful analysis of what people thought + what actually happened in retrospect, even better and significantly more detailed than what I was doing manually. I realized that this task is actually a really good fit for LLMs and I was looking for excuses to vibe code something with the newly released Opus 4.5, so I got to work. I'm going to get all the front pages of December (31 days, 30 articles per day), get ChatGPT 5.1 Thinking to do the analysis, and present everything in a nice way for historical reading.&lt;/p&gt;
    &lt;p&gt;There are two macro reasons for why I think the exercise is interesting more generally:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I believe it is quite possible and desirable to train your forward future predictor given training and effort.&lt;/item&gt;
      &lt;item&gt;I was reminded again of my tweets that said "Be good, future LLMs are watching". You can take that in many directions, but here I want to focus on the idea that future LLMs are watching. Everything we do today might be scrutinized in great detail in the future because doing so will be "free". A lot of the ways people behave currently I think make an implicit "security by obscurity" assumption. But if intelligence really does become too cheap to meter, it will become possible to do a perfect reconstruction and synthesis of everything. LLMs are watching (or humans using them might be). Best to be good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vibe coding the actual project was relatively painless and took about 3 hours with Opus 4.5, with a few hickups but overall very impressive. The repository is on GitHub here: karpathy/hn-time-capsule. Here is the progression of what the code does:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Given a date, download the frontpage of 30 articles&lt;/item&gt;
      &lt;item&gt;For each article, download/parse the article itself and the full comment thread using Algolia API.&lt;/item&gt;
      &lt;item&gt;Package up everything into a markdown prompt asking for the analysis. Here is the prompt prefix I used:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;The following is an article that appeared on Hacker News 10 years ago, and the discussion thread.

Let's use our benefit of hindsight now in 6 sections:

1. Give a brief summary of the article and the discussion thread.
2. What ended up happening to this topic? (research the topic briefly and write a summary)
3. Give out awards for "Most prescient" and "Most wrong" comments, considering what happened.
4. Mention any other fun or notable aspects of the article or discussion.
5. Give out grades to specific people for their comments, considering what happened.
6. At the end, give a final score (from 0-10) for how interesting this article and its retrospect analysis was.

As for the format of Section 5, use the header "Final grades" and follow it with simply an unordered list of people and their grades in the format of "name: grade (optional comment)". Here is an example:

Final grades
- speckx: A+ (excellent predictions on ...)
- tosh: A (correctly predicted this or that ...)
- keepamovin: A
- bgwalter: D
- fsflover: F (completely wrong on ...)

Your list may contain more people of course than just this toy example. Please follow the format exactly because I will be parsing it programmatically. The idea is that I will accumulate the grades for each account to identify the accounts that were over long periods of time the most prescient or the most wrong.

As for the format of Section 6, use the prefix "Article hindsight analysis interestingness score:" and then the score (0-10) as a number. Give high scores to articles/discussions that are prominent, notable, or interesting in retrospect. Give low scores in cases where few predictions are made, or the topic is very niche or obscure, or the discussion is not very interesting in retrospect.

Here is an example:
Article hindsight analysis interestingness score: 8
---
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Submit prompt to GPT 5.1 Thinking via the OpenAI API&lt;/item&gt;
      &lt;item&gt;Collect and parse the results&lt;/item&gt;
      &lt;item&gt;Render the results into static HTML web pages for easy viewing&lt;/item&gt;
      &lt;item&gt;Host the html result pages on my website: https://karpathy.ai/hncapsule/&lt;/item&gt;
      &lt;item&gt;Host all the intermediate results of the &lt;code&gt;data&lt;/code&gt;directory if someone else would like to play. It's the file&lt;code&gt;data.zip&lt;/code&gt;under the exact same url prefix (intentionally avoiding a direct link).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I spent a few hours browsing around and found it to be very interesting. A few example threads just for fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 3 2015 Swift went open source.&lt;/item&gt;
      &lt;item&gt;December 6 2015 Launch of Figma&lt;/item&gt;
      &lt;item&gt;December 11 2015 original announcement of OpenAI :').&lt;/item&gt;
      &lt;item&gt;December 16 2015 geohot is building Comma&lt;/item&gt;
      &lt;item&gt;December 22 2015 SpaceX launch webcast: Orbcomm-2 Mission&lt;/item&gt;
      &lt;item&gt;December 28 2015 Theranos struggles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then when you navigate over to the Hall of Fame, you can find the top commenters of Hacker News in December 2015, sorted by imdb-style score of their grade point average. In particular, congratulations to pcwalton, tptacek, paulmd, cstross, greglindahl, moxie, hannob, 0xcde4c3db, Manishearth, johncolanduoni - GPT 5.1 Thinking found your comments very insightful and prescient. You can also scroll all the way down to find the noise of HN, which I think we're all familiar with too :)&lt;/p&gt;
    &lt;p&gt;My code (wait, Opus' code?) on GitHub can be used to reproduce or tweak the results. Running 31 days of 30 articles through GPT 5.1 Thinking meant &lt;code&gt;31 * 30 =&lt;/code&gt; 930 LLM queries and cost about $58 and somewhere around ~1 hour. The LLM megaminds of the future might find this kind of a thing a lot easier, a lot faster and a lot cheaper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220540</guid><pubDate>Wed, 10 Dec 2025 17:23:53 +0000</pubDate></item><item><title>Show HN: Automated license plate reader coverage in the USA</title><link>https://alpranalysis.com</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220794</guid><pubDate>Wed, 10 Dec 2025 17:42:30 +0000</pubDate></item><item><title>Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise</title><link>https://arxiv.org/abs/2512.08309</link><description>&lt;doc fingerprint="cd3e340a91d592ce"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 9 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Alexander Goslin [view email]&lt;p&gt;[v1] Tue, 9 Dec 2025 07:10:35 UTC (12,075 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.CV&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46221594</guid><pubDate>Wed, 10 Dec 2025 18:37:27 +0000</pubDate></item><item><title>Super Mario 64 for the PS1</title><link>https://github.com/malucard/sm64-psx</link><description>&lt;doc fingerprint="57cb9c197ca5b5c8"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This is a fork of the full decompilation of Super Mario 64 (J), (U), (E), and (SH).&lt;/item&gt;
      &lt;item&gt;It is heavily modified and can no longer target Nintendo 64, only PSX and PC (for debugging).&lt;/item&gt;
      &lt;item&gt;There are still many limitations.&lt;/item&gt;
      &lt;item&gt;For now, it can only build from the US version.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This repo does not include all assets necessary for compiling the game. An original copy of the game is required to extract the assets.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cool "DUAL SHOCK‚Ñ¢ Compatible" graphic mimicking the original "ÊåØÂãï„Éë„ÉÉ„ÇØÂØæÂøú" (Rumble Pak Compatible) graphic&lt;/item&gt;
      &lt;item&gt;An analog rumble signal is now produced for the DualShock's large motor, in addition to the original modulated digital signal for the small motor and for the SCPH-1150 Dual Analog Controller&lt;/item&gt;
      &lt;item&gt;Low-precision soft float implementation specially written for PSX to reduce the performance impact of floats&lt;/item&gt;
      &lt;item&gt;Large amounts of code have been adapted to use fixed point math, including the 16-bit integer vectors and matrices that are standard on PSX&lt;/item&gt;
      &lt;item&gt;Simplified rewritten render graph walker&lt;/item&gt;
      &lt;item&gt;Tessellation (up to 2x) to reduce issues with large polygons&lt;/item&gt;
      &lt;item&gt;RSP display lists are compiled just-in-time into a custom display list format that is more compact and faster to process&lt;/item&gt;
      &lt;item&gt;Display list preprocessor that removes commands we won't use and optimizes meshes (TODO: make it fix more things)&lt;/item&gt;
      &lt;item&gt;Mario's animations are compressed (from 580632 to 190324 bytes) and placed in a corner of VRAM rather than being loaded from storage (we don't have the luxury of a fast cartridge to read from in the middle of a frame)&lt;/item&gt;
      &lt;item&gt;Custom profiler&lt;/item&gt;
      &lt;item&gt;Custom texture encoder that quantizes all textures to 4 bits per pixel&lt;/item&gt;
      &lt;item&gt;Translucent circle-texture shadows replaced with subtractive hexagonal shadows, as the PSX doesn't support arbitrary translucency&lt;/item&gt;
      &lt;item&gt;(TODO) Camera system adapted to rotate with the right analog stick&lt;/item&gt;
      &lt;item&gt;(TODO) Simplified rewritten Goddard subsystem&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Floating trees (temporary issue due caused by a math rewrite)&lt;/item&gt;
      &lt;item&gt;Some of Mario's animations do not play, and may even crash the game&lt;/item&gt;
      &lt;item&gt;Music cannot be generated at build time without manually obtaining the tracks&lt;/item&gt;
      &lt;item&gt;Sound effects work but sometimes sound odd or are missing notes&lt;/item&gt;
      &lt;item&gt;The camera cannot be controlled in many levels due to the unfinished camera control implementation&lt;/item&gt;
      &lt;item&gt;Crashes when entering certain levels (due to insufficient memory?)&lt;/item&gt;
      &lt;item&gt;Ending sequence crashes on load&lt;/item&gt;
      &lt;item&gt;When reaching the bridge in the castle grounds, Mario looks up but Lakitu never comes over&lt;/item&gt;
      &lt;item&gt;Poles do not go down when pounded&lt;/item&gt;
      &lt;item&gt;Textures are loaded individually, causing long stutters and loading times&lt;/item&gt;
      &lt;item&gt;Stretched textures due to PSX limitations (the graphics preprocessor could help)&lt;/item&gt;
      &lt;item&gt;Tessellation is not good enough to fix all large polygons (the graphics preprocessor could help)&lt;/item&gt;
      &lt;item&gt;Some textures are rendered incorrectly (RSP JIT issues?)&lt;/item&gt;
      &lt;item&gt;Title screen is unfinished&lt;/item&gt;
      &lt;item&gt;Pause menu doesn't work&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build and install the mipsel-none-elf-gcc toolchain. For Arch users, it is available on AUR. (You can also install it on your system from https://github.com/malucard/poeng by running &lt;code&gt;make install-gcc&lt;/code&gt;from there. This may take a long time.)&lt;/item&gt;
      &lt;item&gt;Clone the repo: &lt;code&gt;git clone https://github.com/malucard/sm64-psx&lt;/code&gt;, which will create a directory&lt;code&gt;sm64-port&lt;/code&gt;and then enter it&lt;code&gt;cd sm64-port&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Place a Super Mario 64 ROM called &lt;code&gt;baserom.&amp;lt;VERSION&amp;gt;.z64&lt;/code&gt;into the repository's root directory for asset extraction,&lt;del rend="overstrike"&gt;where&lt;/del&gt;. (For now, only&lt;code&gt;VERSION&lt;/code&gt;can be&lt;code&gt;us&lt;/code&gt;,&lt;code&gt;jp&lt;/code&gt;, or&lt;code&gt;eu&lt;/code&gt;&lt;code&gt;us&lt;/code&gt;is supported.)&lt;/item&gt;
      &lt;item&gt;(Optional) Create a folder named &lt;code&gt;.local&lt;/code&gt;in the root of the repo and place every track of the soundtrack in it as a .wav file, numbered from 0 to 37 (0.wav, 1.wav, etc).&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;make&lt;/code&gt;to build. To build the benchmark version without music, run&lt;code&gt;make BENCH=1&lt;/code&gt;. The disc image will be located at&lt;code&gt;build/&amp;lt;VERSION&amp;gt;_psx/sm64.&amp;lt;VERSION&amp;gt;.iso&lt;/code&gt;. The benchmark version will not generate an iso, only an elf and an exe, and it will require a PSX with 8MB of RAM (an emulator or a debug unit).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install and update MSYS2, following all the directions listed on https://www.msys2.org/.&lt;/item&gt;
      &lt;item&gt;From the start menu, launch MSYS2 MinGW and install required packages depending on your machine (do NOT launch "MSYS2 MSYS"):&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;64-bit: Launch "MSYS2 MinGW 64-bit" and install: &lt;code&gt;pacman -S git make python3 mingw-w64-x86_64-gcc mingw-w64-x86_64-meson mingw-w64-x86_64-ffmpeg unzip&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;32-bit (will also work on 64-bit machines): Launch "MSYS2 MinGW 32-bit" and install: &lt;code&gt;pacman -S git make python3 mingw-w64-i686-gcc mingw-w64-i686-meson mingw-w64-i686-ffmpeg unzip&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Do NOT by mistake install the packages called simply &lt;code&gt;gcc&lt;/code&gt;and&lt;code&gt;meson&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install the mipsel-none-elf-gcc toolchain.&lt;/item&gt;
      &lt;item&gt;The MSYS2 terminal has a current working directory that initially is &lt;code&gt;C:\msys64\home\&amp;lt;username&amp;gt;&lt;/code&gt;(home directory). At the prompt, you will see the current working directory in yellow.&lt;code&gt;~&lt;/code&gt;is an alias for the home directory. You can change the current working directory to&lt;code&gt;My Documents&lt;/code&gt;by entering&lt;code&gt;cd /c/Users/&amp;lt;username&amp;gt;/Documents&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Clone the repo: &lt;code&gt;git clone https://github.com/malucard/sm64-psx&lt;/code&gt;, which will create a directory&lt;code&gt;sm64-psx&lt;/code&gt;and then enter it&lt;code&gt;cd sm64-psx&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Place a Super Mario 64 ROM called &lt;code&gt;baserom.&amp;lt;VERSION&amp;gt;.z64&lt;/code&gt;into the repository's root directory for asset extraction,&lt;del rend="overstrike"&gt;where&lt;/del&gt;. (For now, only&lt;code&gt;VERSION&lt;/code&gt;can be&lt;code&gt;us&lt;/code&gt;,&lt;code&gt;jp&lt;/code&gt;, or&lt;code&gt;eu&lt;/code&gt;&lt;code&gt;us&lt;/code&gt;is supported.)&lt;/item&gt;
      &lt;item&gt;(Optional) Create a folder named &lt;code&gt;.local&lt;/code&gt;in the root of the repo and place every track of the soundtrack in it as a .wav file, numbered from 0 to 37 (0.wav, 1.wav, etc).&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;make&lt;/code&gt;to build. To build the benchmark version, run&lt;code&gt;make BENCH=1&lt;/code&gt;. The disc image will be located at&lt;code&gt;build/&amp;lt;VERSION&amp;gt;_psx/sm64.&amp;lt;VERSION&amp;gt;.iso&lt;/code&gt;. The benchmark version will not generate an iso, only an elf and an exe, and it will require a PSX with 8MB of RAM (an emulator or a debug unit).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If you get &lt;code&gt;make: gcc: no suitable C and C++ compiler found&lt;/code&gt;,&lt;code&gt;make: gcc: command not found&lt;/code&gt;,&lt;code&gt;make: gcc: No such file or directory&lt;/code&gt;although the packages did successfully install, you probably launched the wrong MSYS2. Read the instructions again. The terminal prompt should contain "MINGW32" or "MINGW64" in purple text, and NOT "MSYS".&lt;/item&gt;
      &lt;item&gt;If you get &lt;code&gt;Failed to open baserom.us.z64!&lt;/code&gt;you failed to place the baserom in the repository. You can write&lt;code&gt;ls&lt;/code&gt;to list the files in the current working directory. If you are in the&lt;code&gt;sm64-psx&lt;/code&gt;directory, make sure you see it here.&lt;/item&gt;
      &lt;item&gt;If you get &lt;code&gt;make: *** No targets specified and no makefile found. Stop.&lt;/code&gt;, you are not in the correct directory. Make sure the yellow text in the terminal ends with&lt;code&gt;sm64-psx&lt;/code&gt;. Use&lt;code&gt;cd &amp;lt;dir&amp;gt;&lt;/code&gt;to enter the correct directory. If you write&lt;code&gt;ls&lt;/code&gt;you should see all the project files, including&lt;code&gt;Makefile&lt;/code&gt;if everything is correct.&lt;/item&gt;
      &lt;item&gt;If you get any error, be sure MSYS2 packages are up to date by executing &lt;code&gt;pacman -Syu&lt;/code&gt;and&lt;code&gt;pacman -Su&lt;/code&gt;. If the MSYS2 window closes immediately after opening it, restart your computer.&lt;/item&gt;
      &lt;item&gt;Check if mipsel gcc is working by executing &lt;code&gt;mipsel-none-elf-gcc -v&lt;/code&gt;. If it doesn't work, you either opened the wrong MSYS start menu entry or installed the incorrect gcc package.&lt;/item&gt;
      &lt;item&gt;When switching between building on other platforms, run &lt;code&gt;make -C tools clean&lt;/code&gt;first to allow for the tools to recompile on the new platform. This also helps when switching between shells like WSL and MSYS2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sm64
‚îú‚îÄ‚îÄ actors: object behaviors, geo layout, and display lists
‚îú‚îÄ‚îÄ assets: animation and demo data
‚îÇ   ‚îú‚îÄ‚îÄ anims: animation data
‚îÇ   ‚îî‚îÄ‚îÄ demos: demo data
‚îú‚îÄ‚îÄ bin: C files for ordering display lists and textures
‚îú‚îÄ‚îÄ build: output directory
‚îú‚îÄ‚îÄ data: behavior scripts, misc. data
‚îú‚îÄ‚îÄ doxygen: documentation infrastructure
‚îú‚îÄ‚îÄ enhancements: example source modifications
‚îú‚îÄ‚îÄ include: header files
‚îú‚îÄ‚îÄ levels: level scripts, geo layout, and display lists
‚îú‚îÄ‚îÄ lib: N64 SDK code
‚îú‚îÄ‚îÄ sound: sequences, sound samples, and sound banks
‚îú‚îÄ‚îÄ src: C source code for game
‚îÇ   ‚îú‚îÄ‚îÄ audio: audio code
‚îÇ   ‚îú‚îÄ‚îÄ buffers: stacks, heaps, and task buffers
‚îÇ   ‚îú‚îÄ‚îÄ engine: script processing engines and utils
‚îÇ   ‚îú‚îÄ‚îÄ game: behaviors and rest of game source
‚îÇ   ‚îú‚îÄ‚îÄ goddard: rewritten Mario intro screen
‚îÇ   ‚îú‚îÄ‚îÄ goddard_og: backup of original Mario intro screen
‚îÇ   ‚îú‚îÄ‚îÄ menu: title screen and file, act, and debug level selection menus
‚îÇ   ‚îî‚îÄ‚îÄ port: port code, audio and video renderer
‚îú‚îÄ‚îÄ text: dialog, level names, act names
‚îú‚îÄ‚îÄ textures: skybox and generic texture data
‚îî‚îÄ‚îÄ tools: build tools
&lt;/code&gt;
    &lt;p&gt;Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46221925</guid><pubDate>Wed, 10 Dec 2025 18:58:55 +0000</pubDate></item><item><title>Getting a Gemini API key is an exercise in frustration</title><link>https://ankursethi.com/blog/gemini-api-key-frustration/</link><description>&lt;doc fingerprint="3956b1cd9b3799d1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Getting a Gemini API key is an exercise in frustration&lt;/head&gt;
    &lt;p&gt;Last week, I started working on a new side-project. It‚Äôs a standard React app partly made up of run-of-the-mill CRUD views‚Äîa perfect fit for LLM-assisted programming. I reasoned that if I could get an LLM to quickly write the boring code for me, I‚Äôd have more time to focus on the interesting problems I wanted to solve.&lt;/p&gt;
    &lt;p&gt;I‚Äôve pretty much settled on Claude Code as my coding assistant of choice, but I‚Äôd been hearing great things about Google‚Äôs Gemini 3 Pro. Despite my aversion to Google products, I decided to try it out on my new codebase.&lt;/p&gt;
    &lt;p&gt;I already had Gemini CLI installed, but that only gave me access to Gemini 2.5 with rate limits. I wanted to try out Gemini 3 Pro, and I wanted to avoid being rate limited. I had some spare cash to burn on this experiment, so I went looking for ways to pay for a Gemini Pro plan, if such a thing existed.&lt;/p&gt;
    &lt;p&gt;Thus began my grand adventure in trying to give Google my money.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a Gemini, really?&lt;/head&gt;
    &lt;p&gt;The name ‚ÄúGemini‚Äù is so overloaded that it barely means anything. Based on the context, Gemini could refer to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The chatbot available at gemini.google.com.&lt;/item&gt;
      &lt;item&gt;The mobile app that lets you use the same Gemini chatbot on your iPhone or Android.&lt;/item&gt;
      &lt;item&gt;The voice assistant on Android phones.&lt;/item&gt;
      &lt;item&gt;The AI features built into Google Workspace, Firebase, Colab, BigQuery, and other Google products.&lt;/item&gt;
      &lt;item&gt;Gemini CLI, an agentic coding tool for your terminal that works the same way as Claude Code or OpenAI Codex.&lt;/item&gt;
      &lt;item&gt;The Gemini Code Assist suite of products, which includes extensions for various IDEs, a GitHub app, and Gemini CLI.&lt;/item&gt;
      &lt;item&gt;The underlying LLM powering all these products.&lt;/item&gt;
      &lt;item&gt;Probably three more products by the time I finish writing this blog post.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make things even more confusing, Google has at least three different products just for agentic coding: Gemini Code Assist (Gemini CLI is a part of this suite of products), Jules, and Antigravity.&lt;/p&gt;
    &lt;p&gt;And then there‚Äôs a bunch of other GenAI stuff that is powered by Gemini but doesn‚Äôt have the word Gemini in the name: Vertex AI Platform, Google AI Studio, NotebookLM, and who knows what else.&lt;/p&gt;
    &lt;p&gt;I just wanted to plug my credit card information into a form and get access to a coding assistant. Instead, I was dunked into an alphabet soup of products that all seemed to do similar things and, crucially, didn‚Äôt have any giant ‚ÄúBuy Now!‚Äù buttons for me to click.&lt;/p&gt;
    &lt;p&gt;In contrast, both Anthropic and OpenAI have two primary ways you can access their products: via their consumer offerings at claude.ai and chatgpt.com respectively, or via API credits that you can buy through their respective developer consoles. In each case, there is a form field where you can plug in your credit card details, and a big, friendly ‚ÄúBuy Now!‚Äù button to click.&lt;/p&gt;
    &lt;p&gt;After half an hour of searching the web, I did the obvious thing and asked the free version of Gemini (the chatbot, not one of those other Geminis) what to do:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How do I pay for the pro version of Gemini so i can use it in the terminal for writing code? I specifically want to use the Gemini 3 Pro model.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It thought for a suspiciously long time and told me that Gemini 3 Pro required a developer API key to use. Since the new model is still in preview, it‚Äôs not yet available on any of the consumer plans. When I asked follow up questions about pricing, it told me that ‚ÄúSomething went wrong‚Äù. Which translates to: we broke something, but we won‚Äôt tell you how to fix it.&lt;/p&gt;
    &lt;p&gt;So I asked Claude for help. Between the two LLMs, I was able to figure out how to create an API key for the Gemini I wanted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating an API key is easy&lt;/head&gt;
    &lt;p&gt;Google AI Studio is supposed to be the all-in-one dashboard for Google‚Äôs generative AI models. This is where you can experiment with model parameters, manage API keys, view logs, and manage billing for your projects.&lt;/p&gt;
    &lt;p&gt;I logged into Google AI Studio and created a new API key. This part was pretty straightforward: I followed the on-screen instructions and had a fresh new key housed under a project in a few seconds. I then verified that my key was working with Gemini CLI.&lt;/p&gt;
    &lt;p&gt;It worked! Now all that was left to do was to purchase some API credits. Back in Google AI Studio, I saw a link titled ‚ÄúSet up billing‚Äù next to my key. It looked promising, so I clicked it.&lt;/p&gt;
    &lt;p&gt;That‚Äôs where the fun really began.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google doesn‚Äôt want my money&lt;/head&gt;
    &lt;p&gt;The ‚ÄúSet up billing‚Äù link kicked me out of Google AI Studio and into Google Cloud Console, and my heart sank. Every time I‚Äôve logged into Google Cloud Console or AWS, I‚Äôve wasted hours upon hours reading outdated documentation, gazing in despair at graphs that make no sense, going around in circles from dashboard to dashboard, and feeling a strong desire to attain freedom from this mortal coil.&lt;/p&gt;
    &lt;p&gt;Turns out I can‚Äôt just put $100 into my Gemini account. Instead, I must first create a Billing Account. After I‚Äôve done that, I must associate it with a project. Then I‚Äôm allowed to add a payment method to the Billing Account. And then, if I‚Äôm lucky, my API key will turn into a paid API key with Gemini Pro privileges.&lt;/p&gt;
    &lt;p&gt;So I did the thing. The whole song and dance. Including the mandatory two-factor OTP verification that every Indian credit card requires. At the end of the process, I was greeted with a popup telling me I had to verify my payment method before I‚Äôd be allowed to use it.&lt;/p&gt;
    &lt;p&gt;Wait. Didn‚Äôt I just verify my payment method? When I entered the OTP from my bank?&lt;/p&gt;
    &lt;p&gt;Nope, turns out Google hungers for more data. Who‚Äôd have thunk it?&lt;/p&gt;
    &lt;p&gt;To verify my payment method for reals, I had to send Google a picture of my government-issued ID and the credit card I‚Äôd just associated with my Billing Account. I had to ensure all the numbers on my credit card were redacted by manually placing black bars on top of them in an image editor, leaving only my name and the last four digits of the credit card number visible.&lt;/p&gt;
    &lt;p&gt;This felt unnecessarily intrusive. But by this point, I was too deep in the process to quit. I was invested. I needed my Gemini 3 Pro, and I was willing to pay any price.&lt;/p&gt;
    &lt;p&gt;The upload form for the government ID rejected my upload twice before it finally accepted it. It was the same exact ID every single time, just in different file formats. It wanted a PNG file. Not a JPG file, nor a PDF file, but a PNG file. Did the upload form mention that in the instructions? Of course not.&lt;/p&gt;
    &lt;p&gt;After jumping through all these hoops, I received an email from Google telling me that my verification will be completed in a few days.&lt;/p&gt;
    &lt;p&gt;A few days? Nothing to do but wait, I suppose.&lt;/p&gt;
    &lt;head rend="h2"&gt;403 Forbidden&lt;/head&gt;
    &lt;p&gt;At this point, I closed all my open Cloud Console tabs and went back to work. But when I was fifteen minutes into writing some code by hand like a Neanderthal, I received a second email from Google telling me that my verification was complete.&lt;/p&gt;
    &lt;p&gt;So for the tenth time that day, I navigated to AI Studio. For the tenth time I clicked ‚ÄúSet up billing‚Äù on the page listing my API keys. For the tenth time I was told that my project wasn‚Äôt associated with a billing account. For the tenth time I associated the project with my new billing account. And finally, after doing all of this, the ‚ÄúQuota tier‚Äù column on the page listing my API keys said ‚ÄúTier 1‚Äù instead of ‚ÄúSet up billing‚Äù.&lt;/p&gt;
    &lt;p&gt;Wait, Tier 1? Did that mean there were other tiers? What were tiers, anyway? Was I already on the best tier? Or maybe I was on the worst one? Not important. The important part was that I had my API key and I‚Äôd managed to convince Google to charge me for it.&lt;/p&gt;
    &lt;p&gt;I went back to the Gemini CLI, ran the &lt;code&gt;/settings&lt;/code&gt; command, and turned on the ‚ÄúEnable experimental features‚Äù option. I ran the &lt;code&gt;/models&lt;/code&gt; command, which told me that Gemini 3 Pro was now available.&lt;/p&gt;
    &lt;p&gt;Success? Not yet.&lt;/p&gt;
    &lt;p&gt;When I tried sending a message to the LLM, it failed with this 403 error:&lt;/p&gt;
    &lt;code&gt;{
  "error": {
    "message": "{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"The caller does not have permission\",\n    \"status\":\"PERMISSION_DENIED\"\n  }\n}\n",
    "code": 403,
    "status": "Forbidden"
  }
}&lt;/code&gt;
    &lt;p&gt;Is that JSON inside a string inside JSON? Yes. Yes it is.&lt;/p&gt;
    &lt;p&gt;To figure out if my key was even working, I tried calling the Gemini API from JavaScript, reproducing the basic example from Google‚Äôs own documentation.&lt;/p&gt;
    &lt;p&gt;No dice. I ran into the exact same error.&lt;/p&gt;
    &lt;p&gt;I then tried talking to Gemini 3 Pro using the Playground inside Google AI Studio. It showed me a toast message saying &lt;code&gt;Failed to generate content. Please try again.&lt;/code&gt; The chat transcript said &lt;code&gt;An internal error has occurred.&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;At this point I gave up and walked away from my computer. It was already 8pm. I‚Äôd been trying to get things to work since 5pm. I needed to eat dinner, play Clair Obscur, and go to bed. I had no more time to waste and no more fucks to give.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your account is in good standing at this time&lt;/head&gt;
    &lt;p&gt;Just as I was getting into bed, I received an email from Google with this subject line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Your Google Cloud and APIs billing account XXXXXX-XXXXXX-XXXXXX is in good standing at this time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With the message inside saying:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on the information you provided and further analysis by Google, we have reinstated your billing account XXXXXX-XXXXXX-XXXXXX. Your account is in good standing, and you should now have full access to your account and related Project(s) and Service(s).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have no idea what any of this means, but Gemini 3 Pro started working correctly after I received this email. It worked in the Playground, directly by calling the API from JavaScript, and with Gemini CLI.&lt;/p&gt;
    &lt;p&gt;Problem solved, I guess. Until Google mysteriously decides that my account is no longer in good standing.&lt;/p&gt;
    &lt;head rend="h2"&gt;This was a waste of time&lt;/head&gt;
    &lt;p&gt;This was such a frustrating experience that I still haven‚Äôt tried using Gemini with my new codebase, nearly a week after I made all those sacrifices to the Gods of Billing Account.&lt;/p&gt;
    &lt;p&gt;I understand why the process for getting a Gemini API key is so convoluted. It‚Äôs designed for large organizations, not an individual developers trying to get work done; it serves the bureaucracy, not the people doing the work; it‚Äôs designed for maximum compliance with government regulations, not for efficiency or productivity.&lt;/p&gt;
    &lt;p&gt;Google doesn‚Äôt want my money unless I‚Äôm an organization that employs ten thousand people.&lt;/p&gt;
    &lt;p&gt;In contrast to Google, Anthropic and OpenAI are much smaller and much more nimble. They‚Äôre able to make the process of setting up a developer account quick and easy for those of us who just want to get things done. Unlike Google, they haven‚Äôt yet become complacent. They need to compete for developer mindshare if they are to survive a decade into the future. Maybe they‚Äôll add the same level of bureaucracy to their processes as they become larger, but for now they‚Äôre fairly easy to deal with.&lt;/p&gt;
    &lt;p&gt;I‚Äôm still going to try using Gemini 3 Pro with Gemini CLI as my coding assistant, but I‚Äôll probably cap the experiment to a month. Unless Gemini 3 Pro is a massive improvement over its competitors, I‚Äôll stick to using tools built by organizations that want me as a customer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46223311</guid><pubDate>Wed, 10 Dec 2025 20:29:12 +0000</pubDate></item><item><title>Patterns.dev</title><link>https://www.patterns.dev/</link><description>&lt;doc fingerprint="ed186110298694bb"&gt;
  &lt;main&gt;
    &lt;p&gt;Interested in our next book? Learn more about Building Large-scale JavaScript Web Apps with React&lt;/p&gt;
    &lt;p&gt;Patterns.dev is a free online resource on design, rendering, and performance patterns for building powerful web apps with vanilla JavaScript or modern frameworks.&lt;/p&gt;
    &lt;p&gt;We publish patterns, tips and tricks for improving how you architect apps for free. Keep in mind, design patterns are descriptive, not prescriptive . They can guide you when facing a problem other developers have encountered many times before, but are not a blunt tool for jamming into every scenario. Patterns.dev aims to be a catalog of patterns (for increasing awareness) rather than a checklist (what you must do).&lt;/p&gt;
    &lt;p&gt;Design patterns are a fundamental part of software development, as they provide typical solutions to commonly recurring problems in software design.&lt;/p&gt;
    &lt;p&gt;A common critique of design patterns is that they needlessly add complexity.&lt;/p&gt;
    &lt;p&gt;Our perspective is that patterns are valuable for solving specific problems, often helping to communicate comminalities in code problems for humans. If a project doesn't have those problems, there isn't a need to apply them. Patterns can also be very language or framework-specific (e.g. React), which can often mean thinking beyond the scope of just the original GoF design patterns.&lt;/p&gt;
    &lt;p&gt;Learn about web performance patterns for loading your code more efficiently. Unsure how to think about modern approaches to loading or rendering user-experiences? We've got you covered.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46226483</guid><pubDate>Thu, 11 Dec 2025 01:18:55 +0000</pubDate></item><item><title>Incomplete list of mistakes in the design of CSS</title><link>https://wiki.csswg.org/ideas/mistakes</link><description>&lt;doc fingerprint="3f4ffa43dd2d5fa3"&gt;
  &lt;main&gt;&lt;p&gt;That should be corrected if anyone invents a time machine. :P&lt;/p&gt;&lt;code&gt;white-space: nowrap&lt;/code&gt; should be &lt;code&gt;white-space: no-wrap&lt;/code&gt;&lt;code&gt;white-space&lt;/code&gt;&lt;code&gt;animation-iteration-count&lt;/code&gt; should just have been &lt;code&gt;animation-count&lt;/code&gt; (like &lt;code&gt;column-count&lt;/code&gt;!)&lt;code&gt;vertical-align&lt;/code&gt; should not apply to table cells. Instead the CSS3 alignment properties should exist in Level 1.&lt;code&gt;vertical-align: middle&lt;/code&gt; should be &lt;code&gt;text-middle&lt;/code&gt; or &lt;code&gt;x-middle&lt;/code&gt; because it's not really in the middle, and such a name would better describes what it does.&lt;code&gt;fill-available&lt;/code&gt; rather than being undefined in auto situations.&lt;code&gt;border-box&lt;/code&gt; by default.&lt;code&gt;background-size&lt;/code&gt; with one value should duplicate its value, not default the second one to &lt;code&gt;auto&lt;/code&gt;. Ditto &lt;code&gt;translate()&lt;/code&gt;.&lt;code&gt;background-position&lt;/code&gt; and &lt;code&gt;border-spacing&lt;/code&gt; (all 2-axis properties) should take *vertical* first, to match with the 4-direction properties like &lt;code&gt;margin&lt;/code&gt;.&lt;code&gt;margin&lt;/code&gt; should go counter-clockwise (so that the inline-start value is before the block-end and inline-end values instead of after them).&lt;code&gt;z-index&lt;/code&gt; should be called &lt;code&gt;z-order&lt;/code&gt; or &lt;code&gt;depth&lt;/code&gt; and should Just Work on all elements (like it does on flex items).&lt;code&gt;word-wrap&lt;/code&gt;/&lt;code&gt;overflow-wrap&lt;/code&gt; should not exist. Instead, &lt;code&gt;overflow-wrap&lt;/code&gt; should be a keyword on 'white-space', like &lt;code&gt;nowrap&lt;/code&gt; (&lt;code&gt;no-wrap&lt;/code&gt;).&lt;code&gt;currentColor&lt;/code&gt; keyword should have retained the dash, &lt;code&gt;current-color&lt;/code&gt;, as originally specified. Likewise all other color multi-word keyword names.&lt;code&gt;border-radius&lt;/code&gt; should have been &lt;code&gt;corner-radius&lt;/code&gt;.&lt;code&gt;hyphens&lt;/code&gt; property should be called &lt;code&gt;hyphenate&lt;/code&gt;. (It's called &lt;code&gt;hyphens&lt;/code&gt; because the XSL:FO people objected to &lt;code&gt;hyphenate&lt;/code&gt;.)&lt;code&gt;rgba()&lt;/code&gt; and &lt;code&gt;hsla()&lt;/code&gt; should not exist, &lt;code&gt;rgb()&lt;/code&gt; and &lt;code&gt;hsl()&lt;/code&gt;  should have gotten an optional fourth parameter instead (and the alpha value should have used the same format as R, G, and B or S and L).&lt;code&gt;¬ª&lt;/code&gt; and indirect sibling combinator should have been &lt;code&gt;++&lt;/code&gt;, so there's some logical relationships among the selectors' ascii art&lt;code&gt;*-blend-mode&lt;/code&gt; properties should've just been &lt;code&gt;*-blend&lt;/code&gt;&lt;code&gt;u0001-u00c8&lt;/code&gt;.&lt;code&gt;font-family&lt;/code&gt; should have required the font name to be quoted (like all other values that come from ‚Äúoutside‚Äù CSS).  The rules for handling unquoted font names make parsing &lt;code&gt;font&lt;/code&gt; stupid, as it requires a &lt;code&gt;font-size&lt;/code&gt; value for disambiguation.&lt;code&gt;flex-basis&lt;/code&gt; vs &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt;.  Perhaps: if &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt; is &lt;code&gt;auto&lt;/code&gt;, use &lt;code&gt;flex-basis&lt;/code&gt;; otherwise, stick with &lt;code&gt;width&lt;/code&gt;/&lt;code&gt;height&lt;/code&gt; as an inflexible size.  (This also makes min/max width/height behavior fall out of the generic definition.)&lt;code&gt;:empty&lt;/code&gt; should have been &lt;code&gt;:void&lt;/code&gt;, and &lt;code&gt;:empty&lt;/code&gt; should select items that contain only white space&lt;code&gt;table-layout: fixed; width: auto&lt;/code&gt; should result in a fill-available table with fixed-layout columns.&lt;code&gt;text-orientation&lt;/code&gt; should have had &lt;code&gt;upright&lt;/code&gt; as the initial value (given the latest changes to 'writing-mode').&lt;code&gt;@import&lt;/code&gt; rule is required to (a) always hit the network unless you specify cache headers, and (b) construct fresh CSSStyleSheet objects for every import, even if they're identical. It should have had more aggressive URL-based deduping and allowed sharing of stylesheet objects.&lt;code&gt;:link&lt;/code&gt; should have had the &lt;code&gt;:any-link&lt;/code&gt; semantics all along.&lt;code&gt;flex&lt;/code&gt; shorthand (and &lt;code&gt;flex-shrink&lt;/code&gt; and &lt;code&gt;flex-grow&lt;/code&gt; longhands) should accept &lt;code&gt;fr&lt;/code&gt; units instead of bare numbers to represent flex fractions.&lt;code&gt;display&lt;/code&gt; property should be called &lt;code&gt;display-type&lt;/code&gt;.&lt;code&gt;list-style&lt;/code&gt; properties should be called &lt;code&gt;marker-style&lt;/code&gt;, and &lt;code&gt;list-item&lt;/code&gt; renamed to &lt;code&gt;marked-block&lt;/code&gt; or something.&lt;code&gt;text-overflow&lt;/code&gt; property should always apply, not be dependent on &lt;code&gt;overflow&lt;/code&gt;&lt;code&gt;line-height: &amp;lt;percentage&amp;gt;&lt;/code&gt; should compute to the equivalent &lt;code&gt;line-height: &amp;lt;number&amp;gt;&lt;/code&gt;, so that it effectively inherits as a percentage not a length&lt;code&gt;::placeholder&lt;/code&gt; should be &lt;code&gt;::placeholder-text&lt;/code&gt; and &lt;code&gt;:placeholder-shown&lt;/code&gt; should be &lt;code&gt;:placeholder&lt;/code&gt;&lt;code&gt;overflow: scroll&lt;/code&gt; should introduce a stacking context&lt;code&gt;size&lt;/code&gt; should have been a shorthand for &lt;code&gt;width&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; instead of an &lt;code&gt;@page&lt;/code&gt; property with a different definition&lt;code&gt;span&lt;/code&gt;) with idents in the grid properties, possibly by using functional notation (like &lt;code&gt;span(2)&lt;/code&gt;).&lt;code&gt;align-inline-*&lt;/code&gt; and &lt;code&gt;align-block-*&lt;/code&gt;.&lt;code&gt;shape-outside&lt;/code&gt; should have had &lt;code&gt;wrap-&lt;/code&gt; in the name somehow, as people assume the shape should also clip the content as in &lt;code&gt;clip-path&lt;/code&gt;.&lt;code&gt;!important&lt;/code&gt; ‚Äî¬†that reads to engineers as ‚Äúnot important‚Äù. We should have picked another way to write this.&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46227619</guid><pubDate>Thu, 11 Dec 2025 04:20:52 +0000</pubDate></item><item><title>The Cost of a Closure in C</title><link>https://thephd.dev/the-cost-of-a-closure-in-c-c2y</link><description>&lt;doc fingerprint="ee43ba392a6d3084"&gt;
  &lt;main&gt;
    &lt;p&gt;I had a vague idea that closures could have a variety of performance implications; I did not believe that so many of the chosen and potential designs for C and C++ extensions ones, however, were so‚Ä¶ suboptimal.&lt;/p&gt;
    &lt;p&gt;But, before we get into how these things perform and what the cost of their designs are, we need to talk about what Closures are.&lt;/p&gt;
    &lt;head rend="h1"&gt;‚ÄúClosures‚Äù?&lt;/head&gt;
    &lt;p&gt;Closures in this instance are programming language constructs that includes data alongside instructions that are not directly related to their input (arguments) and their results (return values). They can be seen as a ‚Äúgeneralization‚Äù of the concept of a function or function call, in that a function call is a ‚Äúsubset‚Äù of closures (e.g., the set of closures that do not include this extra, spicy data that comes from places outside of arguments and returns). These generalized functions and generalized function objects hold the ability to do things like work with ‚Äúinstance‚Äù data that is not passed to it directly (i.e., variables surrouding the closure off the stack) and, usually, some way to carry around more data than is implied by their associated function signature.&lt;/p&gt;
    &lt;p&gt;Pretty much all recent and modern languages include something for Closures unless they are deliberately developing for a target audience or for a source code design that is too ‚Äúlow level‚Äù for such a concept (such as Stack programming languages, Bytecode languages, or ones that fashion themselves as assembly-like or close to it). However, we‚Äôre going to be focusing on and looking specifically at Closures in C and C++, since this is going to be about trying to work with and ‚Äì eventually ‚Äì standardize something for ISO C that works for everyone.&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs show a typical problem that arises in C code to show why closure solutions have popped up all over the C ecosystem, then talk about it in the context of the various solutions.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Closure Problem&lt;/head&gt;
    &lt;p&gt;The closure problem can be neatly described by as ‚Äúhow do I get extra data to use within this &lt;code&gt;qsort&lt;/code&gt; call?‚Äù. For example, consider setting this variable, &lt;code&gt;in_reverse&lt;/code&gt;, as part of a bit of command line shenanigans, to change how a sort happens:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

static int in_reverse = 0;

int compare(const void* untyped_left, const void* untyped_right) {
  const int* left = untyped_left;
  const int* right = untyped_right;
  return (in_reverse) ? *right - *left : *left - *right;
}

int main(int argc, char* argv[]) {
  if (argc &amp;gt; 1) {
    char* r_loc = strchr(argv[1], 'r');
    if (r_loc != NULL) {
      ptrdiff_t r_from_start = (r_loc - argv[1]);
      if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
        in_reverse = 1;
      } 
    }
  }
  int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
  qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
  return list[0];
}
&lt;/code&gt;
    &lt;p&gt;This uses a &lt;code&gt;static&lt;/code&gt; variable to have it persist between both the &lt;code&gt;compare&lt;/code&gt; function calls that &lt;code&gt;qsort&lt;/code&gt; makes and the &lt;code&gt;main&lt;/code&gt; call which (potentially) changes its value to be &lt;code&gt;1&lt;/code&gt; instead of &lt;code&gt;0&lt;/code&gt;. Unfortunately, this isn‚Äôt always the best idea for more complex programs that don‚Äôt fit within a single snippet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;it is impossible to have different ‚Äúcopies‚Äù of a &lt;code&gt;static&lt;/code&gt;variable, meaning all mutations done in all parts of the program that can see&lt;code&gt;in_reverse&lt;/code&gt;are responsible for knowing the state before and after (e.g., heavily stateful programming of state that you may not own / cannot see);&lt;/item&gt;
      &lt;item&gt;working on &lt;code&gt;static&lt;/code&gt;data may produce thread contention/race conditions in more complex programs;&lt;/item&gt;
      &lt;item&gt;using &lt;code&gt;_Thread_local&lt;/code&gt;instead of&lt;code&gt;static&lt;/code&gt;only solves the race condition problem but does not solve the ‚Äúshared across several places on the same thread‚Äù problem;&lt;/item&gt;
      &lt;item&gt;referring to specific pieces of data or local pieces of data (like &lt;code&gt;list&lt;/code&gt;itself) become impossible;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and so on, and so forth. This is the core of the problem here. It becomes more pronounced when you want to do things with function and data that are a bit more complex, such as Donald Knuth‚Äôs ‚ÄúMan-or-Boy‚Äù test code.&lt;/p&gt;
    &lt;p&gt;The solutions to these problems come in 4 major flavors in C and C++ code.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Just reimplement the offending function to take a userdata pointer so you can pass whatever data you want (typical C solution, e.g. going from &lt;code&gt;qsort&lt;/code&gt;as the sorting function to BSD‚Äôs&lt;code&gt;qsort_r&lt;/code&gt;1 or Annex K‚Äôs&lt;code&gt;qsort_s&lt;/code&gt;2).&lt;/item&gt;
      &lt;item&gt;Use GNU Nested Functions to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use Apple Blocks to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use C++ Lambdas and some elbow grease to just Refer To What You Want Anyways.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each solution has drawbacks and benefits insofar as usability and design, but as a quick overview we‚Äôll show what it‚Äôs like using &lt;code&gt;qsort&lt;/code&gt; (or &lt;code&gt;qsort_r&lt;/code&gt;/&lt;code&gt;qsort_s&lt;/code&gt;, where applicable). Apple Blocks, for starters, looks like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// value changed in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	qsort_b(list, (sizeof(list)/sizeof(*list)), sizeof(*list),
		// Apple Blocks are Block Expressions, meaning they do not have to be stored
		// in a variable first
		^(const void* untyped_left, const void* untyped_right) {
			const int* left = untyped_left;
			const int* right = untyped_right;
			return (in_reverse) ? *right - *left : *left - *right;
		}
	);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;and GNU Nested Functions look like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// modify variable in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	// GNU Nested Function definition, can reference `in_reverse` directly
	// is a declaration/definition, and cannot be used directly inside of `qsort`
	int compare(const void* untyped_left, const void* untyped_right) {
		const int* left = untyped_left;
		const int* right = untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	}
	// use in the sort function without the need for a `void*` parameter
	qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;or, finally, C++-style Lambdas:&lt;/p&gt;
    &lt;code&gt;#define __STDC_WANT_LIB_EXT1__ 1

#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	int in_reverse = 0;
	
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	// lambdas are expressions, but we can assign their unique variable types with `auto`
	auto compare = [&amp;amp;](const void* untyped_left, const void* untyped_right) {
		const int* left = (const int*)untyped_left;
		const int* right = (const int*)untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	};

	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };	

	// C++ Lambdas don't automatically make a trampoline, so we need to provide
	// one ourselves for the `qsort_s/r` case so we can call the lambda
	auto compare_trampoline = [](const void* left, const void* right, void* user) {
		typeof(compare)* p_compare = user;
		return (*p_compare)(left, right);
	};
	qsort_s(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare_trampoline, &amp;amp;compare);

	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;To solve this gaggle of problems, pretty much every semi-modern language (that isn‚Äôt assembly-adjacent or based on some kind of state/stack programming) provide some idea of being able to associate some set of data with one or more function calls. And, particularly for Closures, this is done in a local way without passing it as an explicit argument. As it turns out, all of those design choices ‚Äì including the ones in C ‚Äì have pretty significant consequences on not just usability, but performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Not A Big Overview&lt;/head&gt;
    &lt;p&gt;This article is NOT going to talk in-depth about the design of all of the alternatives or other languages. We‚Äôre focused on the actual cost of the extensions and what they mean. A detailed overview of the design tradeoffs, their security implications, and other problems, can be read at the ISO C Proposal for Functions with Closures here; it also gets into things like Security Implications, ABI, current implementation impact, and more of the various designs. The discussion in the paper is pretty long and talks about the dozens of aspects of each solution down to both the design aspect and the implementation quirks. We encourage you to dive into that proposal and read it to figure out if there‚Äôs something more specific you care about insofar as some specific design portion. But, this article is going to be concerned about one thing and one thing only:&lt;/p&gt;
    &lt;head rend="h1"&gt;Purrrrrrrformance :3!&lt;/head&gt;
    &lt;p&gt;In order to measure this cost, we are going to take Knuth‚Äôs Man-or-Boy test and benchmark various styles of implementation in C and C++ using various different extensions / features for the Closure problem. The Man-or-Boy test is an efficient measure of how well your programming language can handle referring to specific entities while engaging in a large degree of recursion and self-reference. It can stress test various portions of how your program creates and passes around data associated with a function call, and if your programming language design is so goofy that it can‚Äôt refer to a specific instance of a variable or function argument, it will end up producing the wrong answer and breaking horrifically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anatomy of a Benhcmark: Raw C&lt;/head&gt;
    &lt;p&gt;Here is the core of the Man-or-Boy test, as implemented in raw C. This implementation3 and all the others are available online for us all to scrutinize and yell at me for messing up, to make sure I‚Äôm not slandering your favorite solution for Closures in this space.&lt;/p&gt;
    &lt;code&gt;// ...

static int eval(ARG* a) {
	return a-&amp;gt;fn(a);
}

static int B(ARG* a) {
	int k    = *a-&amp;gt;k -= 1;
	ARG args = { B, &amp;amp;k, a, a-&amp;gt;x1, a-&amp;gt;x2, a-&amp;gt;x3, a-&amp;gt;x4 };
	return A(&amp;amp;args);
}

static int A(ARG* a) {
	return *a-&amp;gt;k &amp;lt;= 0 ? eval(a-&amp;gt;x4) + eval(a-&amp;gt;x5) : B(a);
}

// ...
&lt;/code&gt;
    &lt;p&gt;You will notice that there is a big, fat, ugly &lt;code&gt;ARG*&lt;/code&gt; parameter hanging around all of these functions. That is because, as stated before, plain ISO C cannot handle passing the data around unless it‚Äôs part of a function‚Äôs arguments. Because the actual core of the Man-or-Boy experiment is the ability to refer to specific values of &lt;code&gt;k&lt;/code&gt; that exist during the recursive run of the program, we need to actually modify the function signature and thereby cheat some of the implicit Man-or-Boy requirements of not passing the value in directly. Here‚Äôs what &lt;code&gt;ARG&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;typedef struct arg {
	int (*fn)(struct arg*);
	int* k;
	struct arg *x1, *x2, *x3, *x4, *x5;
} ARG;

static int f_1(ARG* _) {
	return -1;
}

static int f0(ARG* _) {
	return 0;
}

static int f1(ARG* _) {
	return 1;
}

static int eval(ARG* a) {
	// ...
}
// ...
&lt;/code&gt;
    &lt;p&gt;And this is how it gets used in the main body of the function in order to compute the right answer and benchmark it:&lt;/p&gt;
    &lt;code&gt;static void normal_functions_rosetta(benchmark::State&amp;amp; state) {
	const int initial_k  = k_value();
	const int expected_k = expected_k_value();
	int64_t result       = 0;

	for (auto _ : state) {
		int k     = initial_k;
		ARG arg1  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg2  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg3  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg4  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg5  = { f0, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG args  = { B, &amp;amp;k, &amp;amp;arg1, &amp;amp;arg2, &amp;amp;arg3, &amp;amp;arg4, &amp;amp;arg5 };
		int value = A(&amp;amp;args);
		result += value == expected_k ? 1 : 0;
	}

	if (result != state.iterations()) {
		state.SkipWithError("failed: did not produce the right answer!");
	}
}

BENCHMARK(normal_functions_rosetta);
&lt;/code&gt;
    &lt;p&gt;Everything within the &lt;code&gt;for (auto _ : state) { ... }&lt;/code&gt; is benchmarked. For those paying attention to the code and find it looking familiar, it‚Äôs because that code is the basic structure all Google Benchmark4 code finds itself looking like. I‚Äôve wanted to swap to Catch25 for a long time now to change to their benchmarking infrastructure, but I‚Äôve been stuck on Google Benchmark because I‚Äôve made a lot of graph-making tools based on its JSON output and I have not vetted Catch2‚Äôs JSON output yet to see if it has all of the necessary bits ‚Äòn‚Äô bobbles I use to de-dedup runs and compute statistics.&lt;/p&gt;
    &lt;p&gt;Everything outside is setup (the part above the &lt;code&gt;for&lt;/code&gt; loop) or teardown/test correction (the part below the &lt;code&gt;for&lt;/code&gt; loop). The initialization of the &lt;code&gt;ARG args&lt;/code&gt;s cannot be moved outside of the measuring loop because each invocation of &lt;code&gt;A&lt;/code&gt; ‚Äì the core of the Man-or-Boy experiment ‚Äì modifies the &lt;code&gt;k&lt;/code&gt; of the ARG parameter, so all of them have to be inside. Conceivably, &lt;code&gt;arg1 .. 5&lt;/code&gt; could be moved out of the loop, but I am very tired of looking at the eight or nine variations of this code so someone else can move it and tell me if Clang or GCC has lots of compiler optimization sauce and doesn‚Äôt understand that those 5 &lt;code&gt;argI&lt;/code&gt;s can be hoisted out of the loop.&lt;/p&gt;
    &lt;p&gt;The value &lt;code&gt;k&lt;/code&gt; is &lt;code&gt;10&lt;/code&gt;, and &lt;code&gt;expected_k&lt;/code&gt; is &lt;code&gt;-67&lt;/code&gt;. The expected, returned &lt;code&gt;k&lt;/code&gt; value is dependent on the input &lt;code&gt;k&lt;/code&gt; value, which controls how deep the Man-or-Boy test would recurse on itself to produce its answer. Therefore, to prevent GCC and Clang and other MEGA POWERFUL PILLAR COMPILERS from optimizing the entire thing out and just replacing the benchmark loop with &lt;code&gt;ret -67&lt;/code&gt;, both &lt;code&gt;k_value()&lt;/code&gt; and &lt;code&gt;expected_k_value()&lt;/code&gt; come from a Dynamic Link Library (&lt;code&gt;.dylib&lt;/code&gt; on MacOS, &lt;code&gt;.so&lt;/code&gt; on *nix platforms, &lt;code&gt;.dll&lt;/code&gt; on Windows platforms) to make sure that NO amount of optimization (Link Time Optimization/Link Time Code Generation, Inlining Optimization, Cross-Translation Unit Optimization, and Automatic Constant Expression Optimization) from C or C++ compilers could fully preempt all forms of computation.&lt;/p&gt;
    &lt;p&gt;This allows us to know, for sure, that we‚Äôre actually measuring something and not just testing how fast a compiler can load a number into a register and test it against &lt;code&gt;state.iterations()&lt;/code&gt;. And, since we know for sure, we can now talk the general methodology.&lt;/p&gt;
    &lt;head rend="h1"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;The tests were ran on a dying 13-inch 2020 MacBook Pro M1 that has suffered several toddler spills and two severe falls. It has 16 GB of RAM and is son MacOS 15.7.2 Sequoia at the time the test was taken, using the stock MacOS AppleClang Compiler and the stock &lt;code&gt;brew install gcc&lt;/code&gt; compiler in order to produce the numbers seen on December 6th, 2025.&lt;/p&gt;
    &lt;p&gt;There 2 measures being conducted: Real Time and CPU Time. The time is gathered by running a single iteration of the code within the &lt;code&gt;for&lt;/code&gt; loop anywhere from a couple thousand to hundreds of thousands of times to produce confidence in that run of the benchmark. This is then averaged to produce the first point. The process is repeated 50 times, repeating that many iterations to build further confidence in the measurement. All 50 means are used as the points for the values, and the average of all of those 50 means is then used as the height of a bar in a bar graph.&lt;/p&gt;
    &lt;p&gt;The bars are presented side-by-side as a horizontal bar chart with 11 categories of C or C++ code being measured. The 11 categories are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;no-op&lt;/code&gt;: Literally doing nothing. It‚Äôs just there to test environmental noise and make sure none of our benchmarks are so off-base that we‚Äôre measuring noise rather than computation. Helps keep us grounded in reality.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (No Function Helpers)&lt;/code&gt;: a solution using C++-style lambdas. Rather than using helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;, we compute a raw lambda that stores the value meant to be returned for the Man-or-Boy test (&lt;code&gt;return i;&lt;/code&gt;) in the lambda itself and then pass that uniquely-typed lambda to the core of the test. The entire test is templated and uses a fake&lt;code&gt;recursion&lt;/code&gt;template parameter to halt the recursion after a certain depth.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas&lt;/code&gt;: The same as above but actually using&lt;code&gt;int f0(void)&lt;/code&gt;, etc. helper functions at the start rather than lambdas. Reduces inliner pressure by using ‚Äúnormal‚Äù types which do not add to the generated number of lambda-typed, recursive, templated function calls.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function_ref)&lt;/code&gt;: The same as above, but rather than using a function template to handle each uniquely-typed lambda like a precious baby bird, it instead erases the lambda behind a&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This allows the recursive function to retain exactly one signature.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function)&lt;/code&gt;: The same as above, but replaces&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;with&lt;code&gt;std::function&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This is its allocating, C++03-style type.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (Rosetta Code)&lt;/code&gt;: The code straight out of the C++11 Rosetta Code Lambda section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Apple Blocks&lt;/code&gt;: Uses Apple Blocks to implement the test, along with the&lt;code&gt;__block&lt;/code&gt;specifier to refer directly to certain variables on the stack.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions (Rosetta Code)&lt;/code&gt;: The code straight out of the C Rosetta Code section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions&lt;/code&gt;: GNU Nested Functions similar to the Rosetta Code implementation, but with some slight modifications in a hope to potentially alleviate some stack pressure if possible by using regular helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Custom C++ Class&lt;/code&gt;: A custom-written C++ class using a discriminated union to decide whether its doing a straight function call or attemping to engage in the Man-or-Boy recursion.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;C++03 shared_ptr (Rosetta Code)&lt;/code&gt;: A C++ class using&lt;code&gt;std::enable_shared_from_this&lt;/code&gt;and&lt;code&gt;std::shared_ptr&lt;/code&gt;with a virtual function call to invoke the ‚Äúright‚Äù function call during recursion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The two compilers tested are Apple Clang 17 and GCC 15. There are two graph images because one is for Apple Clang and the other is for GCC. This is particularly important because neither compiler implements the other‚Äôs closure extension (Clang does Apple Blocks but not Nested Functions, while GCC does Nested Functions in exclusively its C frontend but does not implement Apple Blocks6).&lt;/p&gt;
    &lt;head rend="h1"&gt;The Results&lt;/head&gt;
    &lt;p&gt;Ta-da!&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;‚Ä¶ Oh. That looks awful.&lt;/p&gt;
    &lt;p&gt;It turns out that some solutions are so dogwater that it completely screws up our viewing graphs. But, it does let us know that Lambdas used the Rosetta Code style are so unbelievably awful that it is several orders of magnitude more expensive than any other solution presented! One has to wonder what the hell is going on in the code snippet there, but first we need to make the graphs more legible. To do this we‚Äôre going to be using the (slightly deceptive) LOGARITHMIC SCALING. This is a bit deadly to do because it tends to mislead people about how much of a change there is, so please pay attention to the potential order of magnitude gains and losses when going from one bar graph to another.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;There we go. Now we can talk about the various solutions and ‚Äì in particular ‚Äì why ‚Äúlambdas‚Äù have 4 different entries with such wildly differing performance profiles. First up, let‚Äôs talk about the clear performance winners.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lambdas: On Top!&lt;/head&gt;
    &lt;p&gt;Not surprising to anyone who has been checked in to C++, lambdas that are used directly and not type-erased are on top. This means there‚Äôs a one-to-one mapping between a function call and a given bit of execution. We are cheating by using a constant parameter to stop the uniquely-typed lambdas being passed into the functions from recursing infinitely, which makes the Man-or-Boy function look like this:&lt;/p&gt;
    &lt;code&gt;template &amp;lt;int recursion = 0&amp;gt;
static int a(int k, const auto&amp;amp; x1, const auto&amp;amp; x2, const auto&amp;amp; x3, const auto&amp;amp; x4, const auto&amp;amp; x5) {
	if constexpr (recursion == 11) {
		::std::cerr &amp;lt;&amp;lt; "This should never happen and this code should never have been generated." &amp;lt;&amp;lt; std::endl;
		::std::terminate();
		return 0;
	}
	else {
		auto B = [&amp;amp;](this const auto&amp;amp; self) { return a&amp;lt;recursion + 1&amp;gt;(--k, self, x1, x2, x3, x4); };
		return k &amp;lt;= 0 ? x4() + x5() : B();
	}
}
&lt;/code&gt;
    &lt;p&gt;Every &lt;code&gt;B&lt;/code&gt; is its own unique type and we are not erasing that unique type when using the expression as an initializer to &lt;code&gt;B&lt;/code&gt;. This means that when we call &lt;code&gt;a&lt;/code&gt; again with &lt;code&gt;B&lt;/code&gt; (the &lt;code&gt;self&lt;/code&gt; in this lambda here using Deduced This, a C++23 feature that cannot be part of the C version of lambdas) which means we need to use &lt;code&gt;auto&lt;/code&gt; parameters (a shortcut way of writing template parameters) to take it. But, since every parameter is unique, and every &lt;code&gt;B&lt;/code&gt; is unique, calling this recursively means that, eventually, C++ compilers will actually just completely crash out/toss out-of-memory errors/say we‚Äôve compile-time recursed too hard, or similar. That‚Äôs why the compile-time &lt;code&gt;if constexpr&lt;/code&gt; on the extra, templated &lt;code&gt;recursion&lt;/code&gt; parameter needs to have some arbitrary limit. Because we know &lt;code&gt;k&lt;/code&gt; starts at 10 for this test, we just have some bogus limit of ‚Äú11‚Äù.&lt;/p&gt;
    &lt;p&gt;This results in a very spammy recursive chain of function calls, where the actual generated names of these template functions is far more complex than &lt;code&gt;a&lt;/code&gt; and can run the compiler into the ground / cause quite a bit of instantiations if you let &lt;code&gt;recursion&lt;/code&gt; get to a high enough value. But, once you add the limit, the compiler gets perfect information about this recursive call all the way to every leaf, and thus is able to not only optimize the hell out of it, but refuse to generate the other frivolous code it knows won‚Äôt be useful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas are also Fast, even when Type-Erased&lt;/head&gt;
    &lt;p&gt;You can observe a slight bump up in performance penalty when a Lambda is erased by a &lt;code&gt;std::function_ref&lt;/code&gt;. This is a low-level, non-allocating, non-owning, slim ‚Äúview‚Äù type that is analogous to what a language-based wide function pointer type would be in C. From this, it allows us to guess how good Lambdas in C would be even if you had to hide them behind a non-unique type.&lt;/p&gt;
    &lt;p&gt;The performance metrics are about equivalent to if you hand-wrote a C++ class with a custom &lt;code&gt;operator()&lt;/code&gt; that uses a discriminated union, no matter which compiler gets used to do it. It‚Äôs obviously not as fast as having access to a direct function call and being able to slurp-inline optimize, but the performance difference is acceptable when you do not want to engage in a large degree of what is called ‚Äúmonomorphisation‚Äù of a genric routine or type. And, indeed, outside of macros, C has no way of doing this innately that isn‚Äôt runtime-based.&lt;/p&gt;
    &lt;p&gt;A very strong contender for a good solution!&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas: On‚Ä¶. Bottom, too?&lt;/head&gt;
    &lt;p&gt;One must wonder, then, why the &lt;code&gt;std::function&lt;/code&gt; Lambdas and the Rosetta Code Lambdas are either bottom-middle-of-the-road or absolutely-teary-eyed-awful.&lt;/p&gt;
    &lt;p&gt;Starting off, the &lt;code&gt;std::function&lt;/code&gt; Lambdas are bad because of exactly that: &lt;code&gt;std::function&lt;/code&gt;. &lt;code&gt;std::function&lt;/code&gt; is not a ‚Äúcheap‚Äù closure; it is a potentially-allocating, meaty, owning function abstraction. This means that it‚Äôs safe to make one and pass it around and store it and call it later; the cost of this is, obviously, that you‚Äôre allocating (when the type is big enough) for that internal storage. Part of this is alleviated by using &lt;code&gt;const std::function&amp;lt;int(void)&amp;gt;&amp;amp;&lt;/code&gt; parameters, taking things by reference and only generating a new object when necessary. This prevents copying on every function call. Both the Rosetta Lambdas and regular &lt;code&gt;std::function&lt;/code&gt; Lambdas code does the by-reference parameters bit, though, so where does the difference come in? It actually has to do with the Captures. Here‚Äôs how &lt;code&gt;std::function&lt;/code&gt; Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [&amp;amp;] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;And, here is how the Rosetta Code Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [=, &amp;amp;k, &amp;amp;B] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;The big problem here is in the use of the &lt;code&gt;=&lt;/code&gt;. What &lt;code&gt;=&lt;/code&gt; by itself in the front of a lambda capture clause means is ‚Äúcopy all the visible variables in and hold onto that copy‚Äù (unless the capture for that following variable is ‚Äúoverridden‚Äù by a &lt;code&gt;&amp;amp;var&lt;/code&gt;, address capture). Meanwhile, the &lt;code&gt;&amp;amp;&lt;/code&gt; is the opposite: it means ‚Äúrefer to all the visible variables directly by their address and do not copy them in‚Äù. So, while the &lt;code&gt;std::function&lt;/code&gt; Lambda is (smartly) referring to stuff directly without copying because we know for the Man-or-Boy test that referring to things directly is not an unsafe operation, the general &lt;code&gt;=&lt;/code&gt; causes that for the several dozen recursive iterations through the function, it is copying all five allocating &lt;code&gt;std::function&lt;/code&gt; arguments. So the first call creates a &lt;code&gt;B&lt;/code&gt; that copies everything in, and then passes that in, and then the next call copies the previous &lt;code&gt;B&lt;/code&gt; and the 4 normal functions, and then passes that in to the next &lt;code&gt;B&lt;/code&gt;, and then it copies both previous &lt;code&gt;B&lt;/code&gt;‚Äôs, and this stacks for the depth of the callgraph (some 10 times since &lt;code&gt;k = 10&lt;/code&gt; to start).&lt;/p&gt;
    &lt;p&gt;You can imagine how much that completely screws with the performance, and it explains why the Rosetta Code Lambdas code behaves so poorly in terms of performance. But, this also raises a question: if referring to everything by-reference saves so much speed, then why does GNU Nested Functions ‚Äì in all its variants ‚Äì perform so poorly? After all, Nested Functions capture everything by reference / by address, exactly like a lambda does with &lt;code&gt;[&amp;amp;]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similarly, if allocating over and over again was so expensive, how come Apple Blocks and C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code-style versions of the Man-or-Boy test don‚Äôt perform nearly as badly as the Rosetta Code Lambdas? Are we not copying the value of the arguments into a newly created Apple Block and, thusly, tanking the performance metrics? Well, as it turns out, there‚Äôs many reasons for these things, so let‚Äôs start with GNU Nested Functions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nested Functions and The Stack&lt;/head&gt;
    &lt;p&gt;I‚Äôve written about it dozens of times now, but the prevailing and most common implementation of Nested Functions is with an executable stack. The are a lot of security and other implications for this, but all you need to understand is that the reason GCC did this is because it was an at-the-time slick encoding of both the location of the variables and the routine itself. Allocating a chunk of data off of the current programming stack means that the ‚Äúenvironment context‚Äù/‚Äùthis closure‚Äù pointer has the same anchoring address as the routine itself. This means you can encode both the location of the data to know what to access and the address of a function‚Äôs entry point into a single thing that works with your typical setup-and-call convention that comes with invoking a standard ISO C function pointer.&lt;/p&gt;
    &lt;p&gt;But think about that, briefly, in terms of optimization.&lt;/p&gt;
    &lt;p&gt;You are using the function‚Äôs stack frame at that precise point in the program as the ‚Äúbase address‚Äù for this executable code. That base address also means that all the variables associated with it need to be reachable from that base address: i.e., that things are not stuffed in registers, but that you are referring to the same variables as modified by the enclosing function around your nested function. Principally, this means that your function needs to have all of the following now so that GNU Nested Functions actually work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that is executable so that the base address used for the trampoline can be run succinctly.&lt;/item&gt;
      &lt;item&gt;A real function frame that exists somewhere in memory to serve as the base address for the trampoline.&lt;/item&gt;
      &lt;item&gt;Real objects in memory backing the names of the captured variables accesses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all seems like regular consequences, until you tack on the second order affects from the point of optimization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that now has both data and instructions all blended into itself.&lt;/item&gt;
      &lt;item&gt;A real function frame, which means no ommission of a frame pointer and no collapsing / inlining of that function frame.&lt;/item&gt;
      &lt;item&gt;Real objects that all have their address taken that are tied to the function frame, which must be memory-accessible and which the compiler now has a hard time telling if they can simply be exchanged through registers or if the need to actually sit somewhere in memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words: GNU Nested Functions have created the perfect little storm for what might be the best optimizer-murderer. The reason it performs so drastically poorly (worse than even allocating lambdas inside of a &lt;code&gt;std::function&lt;/code&gt; or C++03-style virtual function calls inside of a bulky, nasty C++ &lt;code&gt;std::shared_ptr&lt;/code&gt;) by a whole order of magnitude or more is that everything about Nested Functions and their current implementation is basically Optimizer Death. If the compiler can‚Äôt see through everything ‚Äì and the Man-or-Boy test with a non-constant value of &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt; ‚Äì GNU Nested Functions deteriorate rapidly. It takes every core optimization technique that we‚Äôve researched and maximized on in the last 30 years and puts a shotgun to the side of its head once it can‚Äôt pre-compute &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The good news is that GCC has completed a new backing implementation for GNU Nested Functions, which uses a heap-based trampoline. Such a trampoline does not interfere with the stack, would allow for omission of frame pointers while referring directly to the data itself (which may prevent the wrecking of specific kinds of inlining optimizations), and does not need an executable stack (just a piece of memory from ‚ú®somewhere‚ú® it can mark executable). This may have performance closer to Apple Blocks, but we don‚Äôt have a build of the latest GCC to test it with. But, when we do, we can simply add the compilation flag &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the two source files in CMake and then let the benchmarks run again to see how it stacks up!&lt;/p&gt;
    &lt;p&gt;Finally, there is a minor performance degredation because our benchmarking software is in C++ and this extension exists exclusively in the C frontend of GCC. That means I have to use an &lt;code&gt;extern&lt;/code&gt; function call within the benchmark loop to get to the actual code. Within the function call, however, all of this stuff should be optimized down, so the cost of a single function call‚Äôs stack frame shouldn‚Äôt be so awful, but I expect to try to dig into this better to help make sure the &lt;code&gt;extern&lt;/code&gt; of a C function call isn‚Äôt making things dramatically worse than they are. Given it‚Äôs a different translation unit and it‚Äôs not being compiled as a separate static or dynamic library, it should still link together and optimize cleanly, but given how bad it‚Äôs performing? Every possible issue is on the table.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about Apple Blocks?&lt;/head&gt;
    &lt;p&gt;Apple Blocks are not the fastest, but they the best of the C extensions while being the worst of the ‚Äúfast‚Äù solutions. They are not faster than just hacking the &lt;code&gt;ARG*&lt;/code&gt; into the function signature and using regular normal C function calls, unfortunately, and that‚Äôs likely due to their shared, heap-ish nature. The saddest part about Apple Blocks is that it works using a Blocks Runtime that is already as optimized as it can possibly be: Clang and Apple both document that whie the Blocks Runtime does manage an Automatic Reference Counted (ARC) Heap of Block pointers, when a Block is first created it will literally have its memory stored on the stack rather than in the heap. In order to move it to the heap, one must call &lt;code&gt;Block_copy&lt;/code&gt; to trigger the ‚Äúnormal‚Äù heap-based shenanigans. We never call &lt;code&gt;Block_copy&lt;/code&gt;, so this is with as-fast-as-possible variable access and management with few allocations.&lt;/p&gt;
    &lt;p&gt;It‚Äôs very slightly disappointing that: normal C functions with an &lt;code&gt;ARG*&lt;/code&gt; blob; a custom C++ class using a discriminated union and &lt;code&gt;operator()&lt;/code&gt;; any mildly conscientious use of lambdas; and, any other such shenanigans perform better than the very best Apple Blocks has to offer. One has to imagine that all of the ARC management functions made to copy the &lt;code&gt;int^(void)&lt;/code&gt; ‚Äúhat-style‚Äù function pointers, even if they end up not doing much for the data stored on the stack, impacted the results here. But, this is also somewhat good news: because Apple Block hat pointers are cheaply-copiable entities (they are just pointers to a Block object), it means that even if we copy all of the arguments into the closure every function call, that copying is about as cheap as it can get. Obivously, as regular ‚ÄúLambdas‚Äù and ‚ÄúLambas (No Function Helpers)‚Äù demonstrate, being able to just slurp everything up by address/by reference ‚Äì including visible function arguments ‚Äì with &lt;code&gt;[&amp;amp;]&lt;/code&gt; saves us a teensy, tiny bit of time7.&lt;/p&gt;
    &lt;p&gt;The cheapness of &lt;code&gt;int^(void)&lt;/code&gt; hat-pointer function types is likely the biggest saving grace for Apple Blocks in this benchmark. In the one place we need to be careful, we rename the input argument &lt;code&gt;k&lt;/code&gt; to &lt;code&gt;arg_k&lt;/code&gt; and then make a &lt;code&gt;__block&lt;/code&gt; variable to actually refer to a shared &lt;code&gt;int k&lt;/code&gt; (and get the right answer):&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	__block fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;All of the &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;x3&lt;/code&gt; ‚Äì like the bad Lambda case ‚Äì are copied over and over and over again. One could change the name of all the arugments &lt;code&gt;arg_xI&lt;/code&gt; and then have an &lt;code&gt;xI&lt;/code&gt; variable inside that is marked &lt;code&gt;__block&lt;/code&gt;, but that‚Äôs more effort and very unlikely to have any serious impact on the code while possibly degrading performance for the setup of multiple shared variables that all have to also be ARC-reference-counted and be stored inside each and every new &lt;code&gt;b&lt;/code&gt; block that is created.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Brief Aside: Self-Referencing Functions/Closures&lt;/head&gt;
    &lt;p&gt;It‚Äôs also important to note that just writing this:&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;(no &lt;code&gt;__block&lt;/code&gt; on the &lt;code&gt;b&lt;/code&gt; variable) is actually a huge bug. Apple Blocks, like older C++ Lambdas, cannot technically refer to ‚Äúitself‚Äù inside. You have to refer to the ‚Äúself‚Äù by capturing the variable it set to. For those who use C++ and are familiar with the lambdas over there, it‚Äôs like making sure you capture the variable you initialize with the lambda by reference while also making sure it has a concrete type. It can only be escaped by using &lt;code&gt;auto&lt;/code&gt; and Deducing This, or some other combination of referential-use. That is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;auto x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;does not compile, as the type&lt;code&gt;auto&lt;/code&gt;isn‚Äôt figured out yet;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function_ref&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles but due to C++ shenanigans produces a dangling reference to a temporary lambda that dies after the full expression (the initialization);&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles and works with no segfaults because&lt;code&gt;std::function&lt;/code&gt;allocates, and the reference to itself&lt;code&gt;&amp;amp;x&lt;/code&gt;is just fine.&lt;/item&gt;
      &lt;item&gt;and, finally, &lt;code&gt;auto x = [](this const auto&amp;amp; self, int v) { if (v != limit) self(v + 1); return v + 8; }&lt;/code&gt;which compiles and works with no segfaults because the invisible&lt;code&gt;self&lt;/code&gt;parameter is just a reference to the current object.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem with the most recent Apple Blocks snippet just above is that it‚Äôs the equivalent of doing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice that there‚Äôs no &lt;code&gt;&amp;amp;x&lt;/code&gt; in the lambda initializer‚Äôs capture list. It‚Äôs copying an (uninitialized) variable by-value into the lambda. This is what Apple Blocks set into a variable that does not have a &lt;code&gt;__block&lt;/code&gt; specifier, like in our bad code case with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All variations of this on all implementations which allow for self-referencing allow this and compile some form of this. You would imagine some implementations would warn about this, but this is leftover nonsense from allowing a variable to refer to itself in its initialization. The obvious reason this happens in C and C++ is because you can create self-referential structures, but unfortunately neither languages provided a safe way to do this generally. C++23‚Äôs Deducing This does not work inside of regular functions and non-objects, so good luck applying to other places and other extensions. The only extension which does not suffer this problem is GNU Nested Functions, because it creates a function declaration / definition rather than a variable with an initializer. Thus, this code from the benchmarks works:&lt;/p&gt;
    &lt;code&gt;inline static int gnu_nested_functions_a(int k, int xl(void), int x2(void), int x3(void), int x4(void), int x5(void)) {
	int b(void) {
		return gnu_nested_functions_a(--k, b, xl, x2, x3, x4);
	}
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;And it has the semantics one would expect, unlike how Blocks, Lambdas, or others with default by-value copying works.&lt;/p&gt;
    &lt;p&gt;In the general case, this is what the paper &lt;code&gt;__self_func&lt;/code&gt; was going to solve8, but‚Ä¶ that‚Äôs going to need some time for me to convince WG14 that maybe it IS actually a good idea. We can probably just keep writing the buggy code a few dozen more times for the recursion case and keep leaving it error prone, but I‚Äôll try my best to convince them one more time that the above situation is very not-okay.&lt;/p&gt;
    &lt;head rend="h1"&gt;Thinking It Over&lt;/head&gt;
    &lt;p&gt;While the Man-or-Boy test isn‚Äôt exactly the end-all, be-all performance test, due to flexing both (self)-referential data and utilization of local copies with recursion, it is surprisingly suitable for figuring out if a closure design is decent enough in a mid to high-level programming language. It also gives me some confidence that, at the very least, the baseline for performance of statically-known, compile-time understood, non-type erased, callable Closure object will have the best implementation quality and performance tradeoffs for a language like ISO C no matter the compiler implementation.&lt;/p&gt;
    &lt;p&gt;In the future, at some point, I‚Äôll have to write about why that is. It‚Äôs a bit upside-down from the perspective of readers of this blog to first address performance and then later write about the design, but it‚Äôs nice to make sure we‚Äôre not designing ourselves into a bad performance corner at the offset of this whole adventure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learned Insights&lt;/head&gt;
    &lt;p&gt;Surprising nobody, the more information the compiler is allowed to accrue (the Lambda design), the better its ability to make the code fast. What might be slightly more surprising is that a slim, compact layer of type erasure ‚Äì not a bulky set of Virtual Function Calls (C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code design) ‚Äì does not actually cost much at all (Lambdas with &lt;code&gt;std::function_ref&lt;/code&gt;). This points out something else that‚Äôs part of the ISO C proposal for Closures (but not formally in its wording): Wide Function Pointers.&lt;/p&gt;
    &lt;p&gt;The ability to make a thin &lt;code&gt;{ some_function_type* func; void* context; }&lt;/code&gt; type backed by the compiler in C would be extremely powerful. Martin Uecker has a proposal that has received interest and passing approval in the Committee, but it would be nice to move it along in a nice direction. My suggestion is having &lt;code&gt;%&lt;/code&gt; as a modifier, so it can be used easily since wide function pointers are an extremely prevalent concept. Being able to write something like the following would be very easy and helpful.&lt;/p&gt;
    &lt;code&gt;typedef int(compute_fn_t)(int);

int do_computation(int num, compute_fn_t% success_modification);
&lt;/code&gt;
    &lt;p&gt;A wide function pointer type like this would also be traditionally convertible from a number of already-existing extensions, too, where GNU Nested Functions, Apple Blocks, C++-style Lambdas, and more could create the appropriate wide function pointer type to be cheaply used. Additionally, it also works for FFI: things like Go closures already use GCC‚Äôs &lt;code&gt;__builtin_call_with_static_chain&lt;/code&gt; to transport through their Go functions in C. Many other functions from other languages could be cheaply and efficiently bridged with this, without having to come up with hairbrained schemes about where to put a &lt;code&gt;void* userdata&lt;/code&gt; or some kind of implicit context pointer / implicit environment pointer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Extensions?&lt;/head&gt;
    &lt;p&gt;Unfortunately ‚Äì except for the borland closure annotation ‚Äì there‚Äôs too many things that are performance-stinky about both GNU Nested Functions and Apple Blocks. It‚Äôs no wonder GCC is trying to add &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the story of GNU Nested Functions; they might be able to tighten up that performance and make it more competitive with Apple Blocks. But, unfortunately, since it is heap-based, there‚Äôs a real chance that its maximum performance ceiling is only as good as Apple Blocks, and not as good as a C++-style Lambda.&lt;/p&gt;
    &lt;p&gt;Both GNU Nested Functions and Apple Blocks ‚Äì as they are implemented ‚Äì do not really work well in ISO C. GNU Nested Functions because their base design and most prevalent implementation are performance-awful, but also Apple Blocks because of the copying and indirection runtime of Blocks that manage ARC pointers providing a hard upper limit on how good the performance can actually be in complex cases.&lt;/p&gt;
    &lt;p&gt;Regular C code, again, performs middle-of-the-road here. It‚Äôs not the worst of it, but it‚Äôs not the best at all, which means there‚Äôs some room beneath how we could go having the C code run. While it‚Äôs hard to fully trust the Rosetta Code Man-or-Boy code for C as the best, it is a pretty clear example of how a ‚Äúnormal‚Äù C developer would do it and how it‚Äôs not actually able to hit maximum performance for this situation.&lt;/p&gt;
    &lt;p&gt;I wanted to add a version of regular C code that used a dynamic array with &lt;code&gt;static&lt;/code&gt;s to transfer data, or a bunch of &lt;code&gt;thread_local&lt;/code&gt;s, but I could not bring myself to actually care enough to write a complex association scheme from a specific invocation of the recursive function &lt;code&gt;a&lt;/code&gt; and the slot of dynamic data that represented the closure‚Äôs data. I‚Äôm sure there‚Äôs schemes for it and I could think of a few, but at that point it‚Äôs such a violent contortion to get a solution that going that I figured it simply wasn‚Äôt worth the effort. But, as always,&lt;/p&gt;
    &lt;p&gt;pull requests are welcome. üíö&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Banner and Title Photo by Lukas, from Pexels&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;See: https://github.com/soasis/idk/tree/main/benchmarks/closures. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See https://github.com/catchorg/Catch2/blob/devel/docs/benchmarks.md. And try it out. It‚Äôs pretty good, I just haven‚Äôt gotten off my butt to make the swap to it yet. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apple Blocks used to have an implementation in GCC that could be turned on and it used a Blocks Runtime to achieve it. But, I think it was gutted when some NeXT support and Objective-C stuff was wiped out after being unmaintained for some time. There‚Äôs been talk of reintroducing it, but obviously someone has to actually sit down and either redo it from scratch (advantageous because Apple has changed the ABI of Blocks) or try to ressurect / fix the old support for this stuff. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Apple Blocks cannot have the ‚Äúby address‚Äù capturing mechanism it has ‚Äì the&lt;/p&gt;&lt;code&gt;__block&lt;/code&gt;storage class modifier ‚Äì applied to function arguments, for some reason. So, all function arguments are de-facto copied into a Block Expression unless someone saves a tempory inside the body of the function before the Block and then uses&lt;code&gt;__block&lt;/code&gt;on that to make it a by-reference capture. ‚Ü©&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WG14 rejected the paper last meeting, unfortunately, as not motivated enough. Funnily enough, it was immediately after this meeting that I got slammed in the face with this bug. Foresight and ‚Äúbeing prepared‚Äù is just not something even the most diehard C enthusiasts really embodies, unfortunately, and most industry vendors tend to take a more strongly conservative position over a bigger one. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46228597</guid><pubDate>Thu, 11 Dec 2025 07:21:33 +0000</pubDate></item><item><title>A ‚Äúfrozen‚Äù dictionary for Python</title><link>https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/</link><description>&lt;doc fingerprint="9f1980339ca5cbf8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A "frozen" dictionary for Python&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Dictionaries are ubiquitous in Python code; they are the data structure of choice for a wide variety of tasks. But dictionaries are mutable, which makes them problematic for sharing data in concurrent code. Python has added various concurrency features to the language over the last decade or so‚Äîasync, free threading without the global interpreter lock (GIL), and independent subinterpreters‚Äîbut users must work out their own solution for an immutable dictionary that can be safely shared by concurrent code. There are existing modules that could be used, but a recent proposal, PEP 814 ("Add frozendict built-in type"), looks to bring the feature to the language itself.&lt;/p&gt;
    &lt;p&gt;Victor Stinner announced the PEP that he and Donghee Na have authored in a post to the PEPs category of the Python discussion forum on November 13. The idea has come up before, including in PEP 416, which has essentially the same title as 814 and was authored by Stinner back in 2012. It was rejected by Guido van Rossum at the time, in part due to its target: a Python sandbox that never really panned out.&lt;/p&gt;
    &lt;head rend="h4"&gt;frozendict&lt;/head&gt;
    &lt;p&gt;The idea is fairly straightforward: add frozendict as a new immutable type to the language's builtins module. As Stinner put it:&lt;/p&gt;
    &lt;quote&gt;We expect frozendict to be safe by design, as it prevents any unintended modifications. This addition benefits not only CPython's standard library, but also third-party maintainers who can take advantage of a reliable, immutable dictionary type.&lt;/quote&gt;
    &lt;p&gt;While frozendict has a lot in common with the dict built-in type, it is not a subclass of dict; instead, it is a subclass of the base object type. The frozendict() constructor can be used to create one in various ways:&lt;/p&gt;
    &lt;quote&gt;fd = frozendict() # empty fd = frozendict(a=1, b=2) # frozen { 'a' : 1, 'b' : 2 } d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) # same l = [ ( 'a', 1 ), ( 'b', 2 ) ] fd = frozendict(l) # same fd2 = frozendict(fd) # same assert d == fd == fd2 # True&lt;/quote&gt;
    &lt;p&gt;As with dictionaries, the keys for a frozendict must be immutable, thus hashable, but the values may or may not be. For example, a list is a legitimate type for a value in either type of dictionary, but it is mutable, making the dictionary as a whole (frozen or not) mutable. However, if all of the values stored in a frozendict are immutable, it is also immutable, so it can be hashed and used in places where that is required (e.g. dictionary keys, set elements, or entries in a functools.lru_cache).&lt;/p&gt;
    &lt;p&gt;As might be guessed, based on the last line of the example above, frozen dictionaries that are hashable can be compared for equality with other dictionaries of either type. In addition, neither the hash() value nor the equality test depend on the insertion order of the dictionary, though that order is preserved in a frozen dictionary (as it is in the regular variety). So:&lt;/p&gt;
    &lt;quote&gt;d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) d2 = { 'b' : 2, 'a' : 1 } fd2 = frozendict(d2) assert d == d2 == fd == fd2 # frozendict unions work too, from the PEP &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | frozendict(y=1) frozendict({'x': 1, 'y': 1}) &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | dict(y=1) frozendict({'x': 1, 'y': 1})For the unions, a new frozen dictionary is created in both cases; the "|=" union-assignment operator also works by generating a new frozendict for the result.&lt;/quote&gt;
    &lt;p&gt; Iteration over a frozendict works as expected; the type implements the collections.abc.Mapping abstract base class, so .items() returns an iterable of key-value tuples, while .keys() and .values() provide the keys and values of the frozen dictionary. For the most part, a frozendict acts like a dict that cannot change; the specific differences between the two are listed in the PEP. It also contains a lengthy list of places in the standard library where a dict could be switched to a frozendict to "&lt;quote&gt;enhance safety and prevent unintended modifications&lt;/quote&gt;". &lt;/p&gt;
    &lt;head rend="h4"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;The reaction to the PEP was generally positive, with the usual suggestions for tweaks and more substantive additions to the proposal. Stinner kept the discussion focused on the proposal at hand for the most part. One part of the proposal was troubling to some: converting a dict to a frozendict was described as an O(n) shallow copy. Daniel F Moisset thought that it would make sense to have an in-place transformation that could be O(1) instead. He proposed adding a .freeze() method that would essentially just change the type of a dict object to frozendict.&lt;/p&gt;
    &lt;p&gt;However, changing the type of an existing object is fraught with peril, as Brett Cannon described:&lt;/p&gt;
    &lt;quote&gt;But now you have made that dictionary frozen for everyone who holds a reference to it, which means side-effects at a distance in a way that could be unexpected (e.g. context switch in a thread and now suddenly you're going to get an exception trying to mutate what was a dict a microsecond ago but is now frozen). That seems like asking for really nasty debugging issues just to optimize some creation time.&lt;/quote&gt;
    &lt;p&gt; The PEP is not aimed at performance, he continued, but is meant to help "&lt;quote&gt;lessen bugs in concurrent code&lt;/quote&gt;". Moisset noted, that dictionaries can already change in unexpected ways via .clear() or .update(), thus the debugging issues already exist. He recognized that the authors may not want to tackle that as part of the PEP, but wanted to try to ensure that an O(1) transformation was not precluded in the future. &lt;/p&gt;
    &lt;p&gt; Cannon's strong objection is to changing the type of the object directly. Ben Hsing and "Nice Zombies" proposed ways to construct a new frozendict without requiring the shallow copy‚Äîthus O(1)‚Äîby either moving the hash table to a newly created frozendict, while clearing the dictionary, or by using a copy-on-write scheme for the table. As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n), which would be a silly thing to do, but that it sometimes happens "&lt;quote&gt;because it makes people stop complaining&lt;/quote&gt;", he said in a footnote. In light of the discussion, the PEP specifically defers that optimization to a later time, suggesting that it could also be done for other frozen types (tuple and frozenset), perhaps by resurrecting PEP 351 ("The freeze protocol"). &lt;/p&gt;
    &lt;p&gt;On December 1, Stinner announced that the PEP had been submitted to the steering council for pronouncement. Given that Na is on the council, though will presumably recuse himself from deciding on this PEP, he probably has a pretty good sense for how it might be received by the group. So it seems likely that the PEP has a good chance of being approved. The availability of the free-threaded version of the language (i.e. without the GIL) means that more multithreaded Python programs are being created, so having a safe way to share dictionaries between threads will be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Dictionaries&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Python Enhancement Proposals (PEP)/PEP 814&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Dec 5, 2025 9:01 UTC (Fri) by jeeger (subscriber, #104979) [Link] (7 responses) Posted Dec 5, 2025 9:29 UTC (Fri) by taladar (subscriber, #68407) [Link] (6 responses) Posted Dec 5, 2025 9:55 UTC (Fri) by intelfx (subscriber, #130118) [Link] (5 responses) Obviously, yes. What the GP is saying is that functions that are O(1) are also, strictly speaking, O(n), since the big-O notation only defines, informally, an "upper bound" on the algorithmic complexity. (The answer here is that engineers tend to casually use the big-O notation where they really mean Knuth's Œò notation instead.) Posted Dec 7, 2025 12:29 UTC (Sun) by Baughn (subscriber, #124425) [Link] (4 responses) If he'd used uppercase-T instead, then we'd use it. Posted Dec 7, 2025 13:21 UTC (Sun) by excors (subscriber, #95769) [Link] (3 responses) As is often the case, Knuth solved that problem too, by inventing TeX half a century ago. Now we just need LWN to implement server-side KaTeX rendering. Posted Dec 7, 2025 13:57 UTC (Sun) by dskoll (subscriber, #1630) [Link] (2 responses) I solved it in a horrible way. Look up "theta" on Wikipedia, then paste the result: Œò Posted Dec 7, 2025 15:10 UTC (Sun) by adobriyan (subscriber, #30858) [Link] (1 responses) Posted Dec 10, 2025 1:56 UTC (Wed) by raven667 (subscriber, #5198) [Link] Posted Dec 5, 2025 11:45 UTC (Fri) by iabervon (subscriber, #722) [Link] (3 responses) Next, I want a flag to json.loads() that causes it to return hashable values instead of mutable ones (without the caller needing to know how to accomplish that). Posted Dec 6, 2025 0:49 UTC (Sat) by AdamW (subscriber, #48457) [Link] (2 responses) It says frozendicts will be ordered, but hashes and comparisons will not care about the order. So frozendict({"a": "b", "c": "d"}) and frozendict({"c": "d", "a": "b"}) will have the same hash and compare as equal, but they're not really the same? I don't know how I feel about that! Posted Dec 6, 2025 5:02 UTC (Sat) by NYKevin (subscriber, #129325) [Link] Whether this is a problem is debatable, but it is also moot. Non-frozen dicts have behaved this way forever, so making frozendict behave differently would be pretty terrible language design. Posted Dec 6, 2025 5:23 UTC (Sat) by iabervon (subscriber, #722) [Link] The history is that the iterator order used to be unpredictable, so the same object might give different orders when traversed multiple times and objects constructed by adding the items in different order might give the same order when traversed multiple times. However, a more recent implementation of dict started to traverse the items in the order the keys were first added, just because that was more convenient, and then the language changed to guarantee this. Of course, that meant that there was now something you could reliably determine about dicts that wasn't included in the equality rules that had always existed. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;quote&gt; As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n) &lt;/quote&gt; I might be misremembering from my Uni days, but all O(1) algorithms are also O(n), so the statement doesn't make sense. I'd be happy for someone to correct me though. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46229467</guid><pubDate>Thu, 11 Dec 2025 09:51:47 +0000</pubDate></item><item><title>Meta shuts down global accounts linked to abortion advice and queer content</title><link>https://www.theguardian.com/global-development/2025/dec/11/meta-shuts-down-global-accounts-linked-to-abortion-advice-and-queer-content</link><description>&lt;doc fingerprint="b68009bf1a2a8f13"&gt;
  &lt;main&gt;
    &lt;p&gt;Meta has removed or restricted dozens of accounts belonging to abortion access providers, queer groups and reproductive health organisations in the past weeks in what campaigners call one of the ‚Äúbiggest waves of censorship‚Äù on its platforms in years.&lt;/p&gt;
    &lt;p&gt;The takedowns and restrictions began in October and targeted the Facebook, Instagram and WhatsApp accounts of more than 50 organisations worldwide, some serving tens of thousands of people ‚Äì in what appears to be a growing push by Meta to limit reproductive health and queer content across its platforms. Many of these were from Europe and the UK, however the bans also affected groups serving women in Asia, Latin America and the Middle East.&lt;/p&gt;
    &lt;p&gt;Repro Uncensored, an NGO tracking digital censorship against movements focused on gender, health and justice, said that it had tracked 210 incidents of account removals and severe restrictions affecting these groups this year, compared with 81 last year.&lt;/p&gt;
    &lt;p&gt;Meta denied an escalating trend of censorship. ‚ÄúEvery organisation and individual on our platforms is subject to the same set of rules, and any claims of enforcement based on group affiliation or advocacy are baseless,‚Äù it said in a statement, adding that its policies on abortion-related content had not changed.&lt;/p&gt;
    &lt;p&gt;Campaigners say the actions indicate that Meta is taking its Trump-era approach to women‚Äôs health and LGBTQ+ issues global. Earlier this year, it appeared to ‚Äúshadow-ban‚Äù or remove the accounts of organisations on Instagram or Facebook helping Americans to find abortion pills. Shadow-banning is when a social media platform severely restricts the visibility of a user‚Äôs content without telling the user.&lt;/p&gt;
    &lt;p&gt;In this latest purge, it blocked abortion hotlines in countries where abortion is legal, banned queer and sex-positive accounts in Europe, and removed posts with even non-explicit, cartoon depictions of nudity.&lt;/p&gt;
    &lt;p&gt;‚ÄúWithin this last year, especially since the new US presidency, we have seen a definite increase in accounts being taken down ‚Äì not only in the US, but also worldwide as a ripple effect,‚Äù said Martha Dimitratou, executive director of Repro Uncensored.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis has been, to my knowledge, at least one of the biggest waves of censorship we are seeing,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Campaigners have accused Meta of being condescending and unresponsive, with the company offering only vague reasons why certain accounts were taken down ‚Äì and appearing unwilling to engage.&lt;/p&gt;
    &lt;p&gt;In one email shared with the Guardian, a Meta consultant appears to invite a number of reproductive health organisations to a closed-door online briefing about ‚Äúthe challenges that you are facing with Meta‚Äôs content moderation policies‚Äù.&lt;/p&gt;
    &lt;p&gt;The email says the meeting ‚Äúwill not be an opportunity to raise critiques of Meta‚Äôs practices or to offer recommendations for policy changes‚Äù.&lt;/p&gt;
    &lt;p&gt;Dimitratou said such closed-door meetings had happened before, saying they ‚Äúreinforce the power imbalance that allows big tech to decide whose voices are amplified and whose are silenced‚Äù.&lt;/p&gt;
    &lt;p&gt;In another instance, a Meta employee counselled an affected organisation in a personal message to simply move away from the platform entirely and start a mailing list, saying that bans were likely to continue. Meta said it did not send this message.&lt;/p&gt;
    &lt;p&gt;Meta‚Äôs recent takedowns are part of a broader pattern of the company purging accounts, and then ‚Äì at times ‚Äì appearing to backtrack after public pressure, said Carolina Are, a fellow at Northumbria University‚Äôs Centre for Digital Citizens.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt wouldn‚Äôt be as much of a problem if platforms‚Äô appeals actually worked, but they don‚Äôt. And appeals are the basis of any democratic justice system,‚Äù she added.&lt;/p&gt;
    &lt;p&gt;Meta said that it aimed to reduce enforcement mistakes against accounts on its platform, but added that the appeals process for banned accounts had become frustratingly slow.&lt;/p&gt;
    &lt;p&gt;Organisations affected by the bans include Netherlands-registered Women Help Women, a nonprofit offering information about abortion to women worldwide, including in Brazil, the Philippines and Poland. It fields about 150,000 emails from women each year, said its executive director, Kinga Jelinska.&lt;/p&gt;
    &lt;p&gt;Women Help Women has been on Facebook for 11 years, said Jelinska, and while its account had been suspended before, this was the first time it was banned outright. The ban could be ‚Äúlife-threatening‚Äù, she said, pushing some women towards dangerous, less reliable information sources. Little explanation was given for the ban.&lt;/p&gt;
    &lt;p&gt;A message from Meta to the group dated 13 November said its page ‚Äúdoes not follow our Community Standards on prescription drugs‚Äù, adding: ‚ÄúWe know this is disappointing, but we want to keep Facebook safe and welcoming for everyone.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs a very laconic explanation, a feeling of opacity,‚Äù Jelinska said. ‚ÄúThey just removed it. That‚Äôs it. We don‚Äôt even know which post it was about.‚Äù&lt;/p&gt;
    &lt;p&gt;Meta said more than half of the accounts flagged by Repro Uncensored have been reinstated, including Women Help Women which it said was taken down in error. ‚ÄúThe disabled accounts were correctly removed for violating a variety of our policies including our Human Exploitation policy,‚Äù it added.&lt;/p&gt;
    &lt;p&gt;Jacarandas was founded by a group of young feminists when abortion was decriminalised in Colombia in 2022, to advise women and girls on how to get a free, legal abortion. The group‚Äôs executive director, Viviana Monsalve, said its WhatsApp helpline had been blocked then reinstated three times since October. The WhatsApp account is currently banned and Monsalve said they had received little information from Meta about whether this would continue.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe wrote [Meta] an email and said, ‚Äòhey, we are a feminist organisation. We work in abortion. Abortion is allowed in Colombia up to 24 weeks. It‚Äôs allowed to give information about it,‚Äô‚Äù said Monsalve.&lt;/p&gt;
    &lt;p&gt;Without Meta‚Äôs cooperation, Monsalve said it was difficult to plan for the future. ‚ÄúYou are not sure if [a ban] will happen tomorrow or after tomorrow, because they didn‚Äôt answer anything.‚Äù&lt;/p&gt;
    &lt;p&gt;Meta said: ‚ÄúOur policies and enforcement regarding abortion medication-related content have not changed: we allow posts and ads promoting healthcare services like abortion, as well as discussion and debate around them, as long as they follow our policies.‚Äù&lt;/p&gt;
    &lt;p&gt;While groups such as Jacarandas and Women Help Women had their accounts removed outright, other groups said that they increasingly faced Meta restricting their posts and shadow-banning their content.&lt;/p&gt;
    &lt;p&gt;Fatma Ibrahim, the director of the Sex Talk Arabic, a UK-based platform which offers Arabic-language content on sexual and reproductive health, said that the organisation had received a message almost every week from Meta over the past year saying that its page ‚Äúdidn‚Äôt follow the rules‚Äù and would not be suggested to other people, based on posts related to sexuality and sexual health.&lt;/p&gt;
    &lt;p&gt;Two weeks ago, these messages escalated to a warning, in which Meta noted its new policies on nudity and removed a post from the Sex Talk Arabic‚Äôs page. The offending post was an artistic depiction of a naked couple, obscured by hearts.&lt;/p&gt;
    &lt;p&gt;Ibrahim said the warning was ‚Äúcondescending‚Äù, and that Meta‚Äôs moderation was US-centric and lacked context.&lt;/p&gt;
    &lt;p&gt;‚ÄúDespite the profits they make from our region, they don‚Äôt invest enough to understand the social issues women fight against and why we use social media platforms for such fights,‚Äù she said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46230072</guid><pubDate>Thu, 11 Dec 2025 11:26:45 +0000</pubDate></item></channel></rss>