<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 21 Oct 2025 12:20:23 +0000</lastBuildDate><item><title>How to stop Linux threads cleanly</title><link>https://mazzo.li/posts/stopping-linux-threads.html</link><description>&lt;doc fingerprint="a6380056a916bcd8"&gt;
  &lt;main&gt;&lt;p&gt;Let√¢s say you√¢re writing a long running multi-threaded application, on Linux. Maybe it√¢s a database or a server of some sort. Let√¢s also imagine that you√¢re not running on some managed runtime (maybe the JVM, Go, or BEAM), but rather managing threads spawned using the &lt;code&gt;clone&lt;/code&gt; syscall. Think of threads created in C with &lt;code&gt;pthread_create&lt;/code&gt;, or using C++√¢s &lt;code&gt;std::thread&lt;/code&gt;.1&lt;/p&gt;&lt;p&gt;Once you get into the business of starting threads, you√¢re probably also in the business of stopping them. However the former is much easier than the latter. With √¢stopping√¢ I mean stopping the thread while giving it a chance to run some cleanup operations before fully terminating. Or in other words, we want to terminate a thread while ensuring that memory is freed, locks are released, logs are flushed, and so on.2&lt;/p&gt;&lt;p&gt;This task is sadly not as straightforward as it should be, and there definitely isn√¢t a one-size-fits-all solution. This blog post aims to give an overview of the problem space and to highlight some pitfalls in an area with no shortage, and present a little magic trick at the end.&lt;/p&gt;&lt;p&gt;If you can afford it, you can structure each thread as such:&lt;/p&gt;&lt;code&gt;while (true) {
if (stop) { break; }
   // Perform some work completing in a reasonable time
   }&lt;/code&gt;&lt;p&gt;&lt;code&gt;stop&lt;/code&gt; here is a per-thread boolean. When we want to stop a thread, we set &lt;code&gt;stop&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;, and then call &lt;code&gt;pthread_join&lt;/code&gt; or equivalents to ensure that the thread has actually terminated.&lt;/p&gt;&lt;p&gt;Here√¢s a contrived but working example in C++:&lt;/p&gt;&lt;code&gt;#include &amp;lt;thread&amp;gt;
#include &amp;lt;atomic&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

static std::atomic&amp;lt;bool&amp;gt; stop = false;

int main() {
std::thread thr([] {
   // prints every second until stopped
     for (int i = 0; !stop.load(); i++) {
     ("iterated %d times\n", i);
       printf(1);
       sleep}
     ("thread terminating\n");
     printf});
   // waits 5 seconds, then stops thread
   (5);
   sleep.store(true);
   stop.join();
   thr("thread terminated\n");
   printfreturn 0;
   }&lt;/code&gt;&lt;p&gt;Which prints:&lt;/p&gt;&lt;code&gt;iterated 0 times
iterated 1 times
iterated 2 times
iterated 3 times
iterated 4 times
thread terminating
thread terminated&lt;/code&gt;&lt;p&gt;If you can write or refactor your code to work in such time slices, then terminating threads is very easy.&lt;/p&gt;&lt;p&gt;Note that the loop body does not need to be fully non blocking √¢ it just needs to be terminated as quickly as we want our termination to be quick. For instance, if our thread is reading from a socket, we could set &lt;code&gt;SO_TIMEOUT&lt;/code&gt; to be 100 milliseconds so that we know that every iteration of the loop will terminate quickly.3&lt;/p&gt;&lt;p&gt;Quasi-busy loops are all well and good, but they√¢re sometimes not desirable. The most common roadblock is foreign code that we don√¢t control which does not fit this pattern √¢ think of a third-party library doing some blocking network call.&lt;/p&gt;&lt;p&gt;As we√¢ll see later, there√¢s essentially no clean way to stop a thread running code we don√¢t control, but there are other reasons to not want to write all our code with the quasi-busy loop pattern.&lt;/p&gt;&lt;p&gt;If we have many threads even relatively slow timeouts might cause significant scheduling overhead due to spurious wakeups, especially on an already busy system. The timeouts will also make debugging and inspecting the system considerably more annoying (e.g.√Ç imagine what the output of &lt;code&gt;strace&lt;/code&gt; would look like).&lt;/p&gt;&lt;p&gt;So it is worth thinking about how to stop a thread while it is blocked on a syscall. The most straightforward way to do that is through signals.4&lt;/p&gt;&lt;p&gt;Signals are the main way to interrupt execution of a thread without explicit coordination of the interrupted thread, and are therefore very relevant to the topic of this blog post. They√¢re also a bit of a mess. These two facts generate unhappiness.&lt;/p&gt;&lt;p&gt;For a good overview on signals I recommend the surprisingly informative man page, but I√¢ll give a sufficient overview here. If you already know how signals work,5 you can skip to the next section.&lt;/p&gt;&lt;p&gt;Signals can arise because of some hardware exception6 or be initiated by software. The most familiar instance of a software-initiated signal is your shell sending SIGINT to the foreground process when you press &lt;code&gt;ctrl-c&lt;/code&gt;. All signals initiated by software originate from a handful of syscalls √¢ for instance &lt;code&gt;pthread_kill&lt;/code&gt; will send a signal to a thread.7&lt;/p&gt;&lt;p&gt;Hardware initiated signals are generally handled immediately, while software initiated signals are handled when a CPU is about to re-enter user mode after the kernel has done some work.8 In any event, when a signal needs to handled in a given thread:&lt;/p&gt;&lt;p&gt;If the signal has been blocked by the receiving thread, it√¢ll wait to be handled until it is unblocked;&lt;/p&gt;&lt;p&gt;If the signal is not blocked, it might be:&lt;/p&gt;&lt;p&gt;Which signals are blocked is controlled by modifying the signal mask using &lt;code&gt;sigprocmask&lt;/code&gt;/&lt;code&gt;pthread_sigmask&lt;/code&gt;, and which action is taken if the thread is not blocked is controlled by &lt;code&gt;sigaction&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Assuming that the signal is not blocked, paths 2.a and 2.b will be managed entirely by the kernel, while path 2.c will cause the kernel to pass control to a user-space signal handler which will do something with the signal.&lt;/p&gt;&lt;p&gt;Importantly, if some thread is in a syscall (for instance blocked while reading from a socket), and a signal needs to be handled, the syscall will return early with error code &lt;code&gt;EINTR&lt;/code&gt; after the signal handler has run.&lt;/p&gt;&lt;p&gt;The signal handler code is subject to various constraints, but otherwise it can do as it pleases, including deciding to not give back control to the code that was executing before. By default, most signals just cause the program to stop abruptly, possibly with a core dump. In the next few sections we√¢re going to explore various ways to use signals to stop our threads.&lt;/p&gt;&lt;p&gt;Let√¢s first examine a way to stop threads, implemented through signals, which would seem to do exactly what we want: thread cancellation.&lt;/p&gt;&lt;p&gt;The API for thread cancellation is very promising. &lt;code&gt;pthread_cancel(tid)&lt;/code&gt; will √¢cancel√¢ thread &lt;code&gt;tid&lt;/code&gt;. The way &lt;code&gt;pthread_cancel&lt;/code&gt; works boils down to:&lt;/p&gt;&lt;code&gt;tid&lt;/code&gt;;&lt;p&gt;There are additional details, but that√¢s essentially all there is to it. However, trouble lies ahead.&lt;/p&gt;&lt;p&gt;Recall that signals can essentially arise anywhere in your code. So if we have code such as&lt;/p&gt;&lt;code&gt;();
 lock// critical work here
(); unlock&lt;/code&gt;&lt;p&gt;we might get a signal in the critical section. In the case of thread cancellation, our thread might get cancelled while we√¢re holding a lock as above, or with some memory to be freed, or in general with some outstanding resource, and our cleanup code will never run. This is not good.&lt;/p&gt;&lt;p&gt;There are some mitigating circumstances, although none sufficient:&lt;/p&gt;&lt;p&gt;Thread cancellation can be temporarily disabled. So we could disable it any time we are in such a critical section.&lt;/p&gt;&lt;p&gt;However some √¢critical sections√¢ are very long (consider the lifespan of some allocated memory), and moreover we√¢d have to make sure to decorate all relevant code by enabling/disabling cancellation at the right time.&lt;/p&gt;&lt;p&gt;Linux threads include facilities to add/remove global cleanup handlers with &lt;code&gt;pthread_cleanup_push&lt;/code&gt; and &lt;code&gt;pthread_cleanup_pop&lt;/code&gt;. These cleanup handlers are run when a thread is cancelled.&lt;/p&gt;&lt;p&gt;However to ensure safety using these functions one would have to again decorate every critical section with not only with a push/pop, but also temporarily disabling cancellations to avoid races as we setup the cleanup.&lt;/p&gt;&lt;p&gt;Again, this would be very error prone and would slow down our code considerably.&lt;/p&gt;&lt;p&gt;By default the signal sent by thread cancellation is only received at √¢cancellation points√¢, which to a first approximation are the syscalls that might block √¢ see &lt;code&gt;pthreads(7)&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;So really we would only run into trouble if we have such a syscall in the critical sections. But again, we√¢d have to manually ensure that either critical section have no cancellation points, or that they√¢re made safe otherwise (possibly with the two measures described above).&lt;/p&gt;&lt;p&gt;If you√¢re a C++/Rust programmer, you might have sneered at the explicit locking above √¢ you√¢ve got RAII to handle such cases:&lt;/p&gt;&lt;code&gt;{
const std::lock_guard&amp;lt;std::mutex&amp;gt; lock(mutex);
   // critical work here
   // The destructor for `lock` will do the unlocking
   }&lt;/code&gt;&lt;p&gt;You might have also been wondering what happens if a thread cancellation arrives in the RAII-managed critical section here.&lt;/p&gt;&lt;p&gt;The answer is that thread cancellation will trigger a stack unwinding very much like throwing an exception would (in fact it√¢s implemented with a special exception), which means that destructors will be run on cancellation. This mechanism is known as forced unwinding. Great, right?9&lt;/p&gt;&lt;p&gt;Well, since thread cancellation is implemented using exceptions, and thread cancellation can happen in arbitrary places, we√¢re always liable to a cancellation happening in a &lt;code&gt;noexcept&lt;/code&gt; block, which will cause your program to crash via &lt;code&gt;std::terminate&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;So since C++11, and especially since C++14 where destructors are marked as &lt;code&gt;noexcept&lt;/code&gt; by default, thread cancellation is essentially useless in C++.10&lt;/p&gt;&lt;p&gt;However note that even if this mechanism worked in C++, it√¢d still not be safe in many situations. Consider situations like:&lt;/p&gt;&lt;code&gt;{
const std::lock_guard&amp;lt;std::mutex&amp;gt; lock(mutex);
   += x;
   balance_1 -= x;
   balance_2 }&lt;/code&gt;&lt;p&gt;If we get forcefully unwound after &lt;code&gt;balance_1 += x&lt;/code&gt;, our invariants go out of the window. This is why Java√¢s form of forced unwinding, &lt;code&gt;Thread.stop&lt;/code&gt;, was deprecated.11&lt;/p&gt;&lt;p&gt;As a brief aside, the nature of signals (and by extension thread cancellation) implies that it√¢s impossible to cleanly stop code that you don√¢t control. You cannot guarantee that memory isn√¢t leaked, files are closed, global locks are released, and so on.&lt;/p&gt;&lt;p&gt;If you need to interrupt foreign code reliably, it√¢s better to isolate it in its own process. It might still leak temporary files and other such persistent resources, but most relevant state would be cleaned up by the operating system when the process dies.&lt;/p&gt;&lt;p&gt;Hopefully you√¢re now convinced that unrestricted thread cancellation is not a great idea in most circumstances. However we can pick the circumstances explicitly by enabling thread cancellation only at specific times. So our event loop becomes:&lt;/p&gt;&lt;code&gt;(PTHREAD_CANCEL_DISABLE);
 pthread_setcancelstatewhile (true) {
(PTHREAD_CANCEL_ENABLE);
   pthread_setcancelstate// syscall that might block indefinitely, e.g. reading
   // from a socket
   (PTHREAD_CANCEL_DISABLE);
   pthread_setcancelstate// Perform some work completing in a reasonable time
   }&lt;/code&gt;&lt;p&gt;We turn off thread cancellation by default, but turn it back on as we do our blocking syscall.12&lt;/p&gt;&lt;p&gt;Refactoring our code to fit this pattern might seem onerous. However many applications with long lived threads already contain loops with a blocking syscall at the beginning (reading from a socket, sleeping on a timer, and so on), followed by some work that will not block indefinitely.&lt;/p&gt;&lt;p&gt;However once we√¢ve done this, it might be worth getting rid of thread cancellation entirely. Relying on the stack unwinding to free resources would not be portable to alternative libcs, and we√¢d need to be fairly careful if we wanted to perform some explicit cleanup actions outside destructors.&lt;/p&gt;&lt;p&gt;So instead we can work with signals directly. We can pick SIGUSR1 as our √¢stopping√¢ signal, install a handler which sets our stopping variable, and check the variable before doing blocking syscalls.13&lt;/p&gt;&lt;p&gt;Here√¢s a worked out example in C++. The interesting parts of the code are setting up the signal handler:&lt;/p&gt;&lt;code&gt;// thread_local isn't really necessary here with one thread,
// but it would be necessary if we had many threads we wanted
// to kill separately.
static thread_local std::atomic&amp;lt;bool&amp;gt; stop = false;

static void stop_thread_handler(int signum) {
.store(true);
   stop}

int main() {
// install signal handler
   {
   struct sigaction act = {{ 0 }};
     .sa_handler = &amp;amp;stop_thread_handler;
     actif (sigaction(SIGUSR1, &amp;amp;act, nullptr) &amp;lt; 0) {
     ("sigaction");
       die_syscall}
     }
   ...   &lt;/code&gt;&lt;p&gt;And the code checking the flag before running the syscall:&lt;/p&gt;&lt;code&gt;ssize_t recvlen;
if (stop.load()) {
break;
   } else {
= recvfrom(sock, buffer.data(), buffer.size(), 0, nullptr, nullptr);
   recvlen }
if (recvlen &amp;lt; 0 &amp;amp;&amp;amp; errno == EINTR &amp;amp;&amp;amp; stop.load()) {
// we got the signal while running the syscall
   break;
   }&lt;/code&gt;&lt;p&gt;However, the code checking the flag and starting the syscall is racy:&lt;/p&gt;&lt;code&gt;if (stop.load()) {
break;
   } else {
// signal handler runs here, syscall blocks until
   // packet arrives -- no prompt termination!
   = recvfrom(sock, buffer.data(), buffer.size(), 0, nullptr, nullptr);
   recvlen }&lt;/code&gt;&lt;p&gt;There√¢s no easy way to check the flag and run the syscall atomically.14&lt;/p&gt;&lt;p&gt;Another approach to this problem would be to have USR1 blocked normally, and unblock it only when the syscall runs, similarly to what we did with the temporary thread cancellation. If the syscall terminates with &lt;code&gt;EINTR&lt;/code&gt;, we know that we should quit.15&lt;/p&gt;&lt;p&gt;Sadly the race is still there, just between the unblocking and running the syscall:&lt;/p&gt;&lt;code&gt;(SIG_SETMASK, &amp;amp;unblock_usr1); // unblock USR1
 ptread_sigmask// signal handler runs here, syscall blocks until
// packet arrives -- no prompt termination!
ssize_t recvlen = recvfrom(sock, buffer.data(), buffer.size(), 0, nullptr, nullptr);
(SIG_SETMASK, &amp;amp;block_usr1); // block USR1 again ptread_sigmask&lt;/code&gt;&lt;p&gt;However, there often is an easy to atomically change the sigmask and run a syscall:&lt;/p&gt;&lt;code&gt;select&lt;/code&gt;/&lt;code&gt;poll&lt;/code&gt;/&lt;code&gt;epoll_wait&lt;/code&gt; have &lt;code&gt;pselect&lt;/code&gt;/&lt;code&gt;ppoll&lt;/code&gt;/&lt;code&gt;epoll_pwait&lt;/code&gt; variants which take a &lt;code&gt;sigmask&lt;/code&gt; argument;&lt;code&gt;read&lt;/code&gt;/&lt;code&gt;write&lt;/code&gt; and similar syscalls can be replaced by their non-blocking versions and a blocking &lt;code&gt;ppoll&lt;/code&gt;;&lt;code&gt;timerfd&lt;/code&gt; or just &lt;code&gt;ppoll&lt;/code&gt; with no file descriptors but with a timeout;&lt;code&gt;io_uring_enter&lt;/code&gt; supports this use case out of the box.&lt;p&gt;The syscalls above already cover a very large footprint.16&lt;/p&gt;&lt;p&gt;In this style, the receive loop of the program becomes:&lt;/p&gt;&lt;code&gt;struct pollfd pollsock = {
.fd = sock,
   .events = POLLIN,
   };
if (ppoll(&amp;amp;pollsock, 1, nullptr, &amp;amp;usr1_unmasked) &amp;lt; 0) {
if (errno == EINTR) {
   break;
     }
   ("ppoll");
   die_syscall}
ssize_t recvlen = recvfrom(sock, buffer.data(), buffer.size(), 0, nullptr, nullptr);&lt;/code&gt;&lt;p&gt;Sadly, not all syscalls have variants which let us atomically change the sigmask as they execute. &lt;code&gt;futex&lt;/code&gt;, the main syscall used to implement userspace concurrency primitives, is a notable example of a syscall which does not include such a facility.17&lt;/p&gt;&lt;p&gt;In the case of &lt;code&gt;futex&lt;/code&gt; one can interrupt threads through &lt;code&gt;FUTEX_WAKE&lt;/code&gt;, but it turns out we can setup a mechanism to safely check the boolean stop flag atomically with starting any syscall.18&lt;/p&gt;&lt;p&gt;To recap, the problematic code looks like this:&lt;/p&gt;&lt;code&gt;if (stop.load()) {
break;
   } else {
// signal handler runs here, syscall blocks until
   // packet arrives -- no prompt termination!
   = recvfrom(sock, buffer.data(), buffer.size(), 0, nullptr, nullptr);
   recvlen }&lt;/code&gt;&lt;p&gt;If we could know that no signal handler is ran between the flag check and the syscall, then we√¢d be safe.&lt;/p&gt;&lt;p&gt;Linux 4.18 introduced a syscall, &lt;code&gt;rseq&lt;/code&gt; (√¢restartable sequences√¢),19 which lets us achieve this, although with some effort.20 The &lt;code&gt;rseq&lt;/code&gt; machinery works as follows:&lt;/p&gt;&lt;p&gt;You write some code which you want to run atomically with regards to preemption or signals √¢ the critical section.&lt;/p&gt;&lt;p&gt;Before the critical section is entered, we inform the kernel that the critical section is about to run by writing to a bit of memory shared between the kernel and userspace.&lt;/p&gt;&lt;p&gt;This bit of memory contains:&lt;/p&gt;&lt;code&gt;start_ip&lt;/code&gt;, the instruction pointer which marks the begin of the critical section;&lt;code&gt;post_commit_offset&lt;/code&gt;, the length of the critical section;&lt;code&gt;abort_ip&lt;/code&gt;, the instruction pointer to jump to if the kernel needs to preempt the critical section.&lt;p&gt;If the kernel has preempted a thread, or if a signal needs to be delivered to the thread, it checks if the thread is in a &lt;code&gt;rseq&lt;/code&gt; critical section, and if it does sets the program counter for the thread to &lt;code&gt;abort_ip&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The process above forces the critical section to be a single contiguous block (from &lt;code&gt;start_ip&lt;/code&gt; to &lt;code&gt;start_ip+post_commit_offset&lt;/code&gt;) which we must know the address of. These requirements force us to write it in inline assembly.&lt;/p&gt;&lt;p&gt;Note that rather than disabling preemption entirely, &lt;code&gt;rseq&lt;/code&gt; lets us specify some code (the code starting at &lt;code&gt;abort_ip&lt;/code&gt;) to perform some cleanup if the critical section is interrupted. The correct functioning of the critical section therefore often depends on a √¢commit instruction√¢ at the very end of the critical section which makes the changes in the critical section visible.&lt;/p&gt;&lt;p&gt;In our case the √¢commit instruction√¢ is &lt;code&gt;syscall&lt;/code&gt; √¢ the instruction which will invoke the syscall that we√¢re interested in.21&lt;/p&gt;&lt;p&gt;Which leads us to the following x86-64 widget for a 6-argument syscall stub which atomically checks a stop flag and executes a &lt;code&gt;syscall&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;// Returns -1 and sets errno to EINTR if `*stop` was true
// before starting the syscall.
long syscall_or_stop(bool* stop, long n, long a, long b, long c, long d, long e, long f) {
long ret;
   register long rd __asm__("r10") = d;
   register long re __asm__("r8")  = e;
   register long rf __asm__("r9")  = f;
   __asm__ __volatile__ (
   R"(
           # struct rseq_cs {
      #     __u32   version;
      #     __u32   flags;
      #     __u64   start_ip;
      #     __u64   post_commit_offset;
      #     __u64   abort_ip;
      # } __attribute__((aligned(32)));
      .pushsection __rseq_cs, "aw"
      .balign 32
      1:
      .long 0, 0                # version, flags
      .quad 3f, (4f-3f), 2f     # start_ip, post_commit_offset, abort_ip
      .popsection

      .pushsection __rseq_failure, "ax"
      # sneak in the signature before abort section as
      # `ud1 &amp;lt;sig&amp;gt;(%%rip), %%edi`, so that objdump will print it
      .byte 0x0f, 0xb9, 0x3d
      .long 0x53053053
      2:
      # exit with EINTR
      jmp 5f
      .popsection

      # we set rseq-&amp;gt;rseq_cs to our structure above.
      # rseq = thread pointer (that is fs) + __rseq_offset
      # rseq_cs is at offset 8
      leaq 1b(%%rip), %%r12
      movq %%r12, %%fs:8(%[rseq_offset])
      3:
      # critical section start -- check if we should stop
      # and if yes skip the syscall
      testb $255, %[stop]
      jnz 5f
      syscall
      # it's important that syscall is the very last thing we do before
      # exiting the critical section to respect the rseq contract of
      # "no syscalls".
      4:
      jmp 6f

      5:
      movq $-4, %%rax # EINTR

      6:
    )"
: "=a" (ret) // the output goes in rax
     : [stop] "m" (*stop),
     [rseq_offset] "r" (__rseq_offset),
       "a"(n), "D"(a), "S"(b), "d"(c), "r"(rd), "r"(re), "r"(rf)
       : "cc", "memory", "rcx", "r11", "r12"
     );
   if (ret &amp;lt; 0 &amp;amp;&amp;amp; ret &amp;gt; -4096) {
   = -ret;
     errno = -1;
     ret }
   return ret;
   }

// A version of recvfrom which atomically checks
// the flag before running.
static long recvfrom_or_stop(bool* stop, int socket, void* buffer, size_t length) {
return syscall_or_stop(stop, __NR_recvfrom, socket, (long)buffer, length, 0, 0, 0);
   }&lt;/code&gt;&lt;p&gt;We√¢re using glibc√¢s recently added support for &lt;code&gt;rseq&lt;/code&gt;, which provides a &lt;code&gt;__rseq_offset&lt;/code&gt; variable containing the offset where the critical section information lives, relative to the thread pointer. All we need to do in the critical section is check the flag, skip the syscall if it√¢s set, and perform the syscall if it is. If the flag is set we pretend the syscall has failed with &lt;code&gt;EINTR&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;You can find the full code for the previous example using this trick to call &lt;code&gt;recvfrom&lt;/code&gt; here. I√¢m not necessarily advocating the use of this technique, but it√¢s definitely an interesting curiosity.&lt;/p&gt;&lt;p&gt;It√¢s quite frustrating that there√¢s no agreed upon way to interrupt and stack unwind a Linux thread and to protect critical sections from such unwinding. There are no technical obstacles to such facilities existing, but clean teardown is often a neglected part of software.&lt;/p&gt;&lt;p&gt;Haskell is one language where these capabilities do exist in the form of asynchronous exceptions, although one still needs to be careful to protect critical sections appropriately.&lt;/p&gt;&lt;p&gt;Peter Cawley provided input on many details treated in this blog post and read its draft. He also suggested &lt;code&gt;rseq&lt;/code&gt; as a possible solution. Many thanks also go to Niklas Hamb√É¬ºchen, Alexandru S√É¬ßvortov, Alex Sayers, and Alex Appetiti for reading drafts of this blog post.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45589156</guid><pubDate>Wed, 15 Oct 2025 07:28:45 +0000</pubDate></item><item><title>Show HN: I'm making a detective game built on Wikipedia</title><link>https://detective.wiki/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45617948</guid><pubDate>Fri, 17 Oct 2025 15:34:04 +0000</pubDate></item><item><title>Code from MIT's 1986 SICP video lectures</title><link>https://github.com/felipap/sicp-code</link><description>&lt;doc fingerprint="a5619a2afc97f391"&gt;
  &lt;main&gt;
    &lt;p&gt;MIT lectures on Structure and Interpretation of Computer Programs, as taught in 1986 by Gerald Sussman and Harold Abelson, are available online in youtube and MIT OCW's website. The videos are available in 240p/360p: very poor quality for visualizing the code in the slides. In addition to that, the camera won't fixate on the board for long, making it extremely difficult to follow.&lt;/p&gt;
    &lt;p&gt;This project's intent is to make the lectures' code and content readable (unlike the slide below) and available in digital format.&lt;/p&gt;
    &lt;p&gt;I tried to maintain consistency of notation, comments and indentation throughout the lectures. Much of the original indentation on the slides was maintained, except for when I was able to improve readability by changing it. Lecture breaks are also noted.&lt;/p&gt;
    &lt;p&gt;Every &lt;code&gt;SLIDE&lt;/code&gt;, &lt;code&gt;TERMINAL&lt;/code&gt; or &lt;code&gt;BOARD&lt;/code&gt; content is marked with the time of appearance. E.g.&lt;/p&gt;
    &lt;code&gt;;# SLIDE 0:00:00
    ... slide content
;# END SLIDE
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I have added comments and remarks throughout the code to help comprehension.&lt;/item&gt;
      &lt;item&gt;Though all files have extension .scm, their content represent a literal transcription of the slides, some of which isn't code. They're expected to fail to compile in all Scheme interpreters.&lt;/item&gt;
      &lt;item&gt;I have fixed the code on a few, rare occasions. In all of them I've noted the correction using a colon followed by an exclamation mark. Notice that ';!' was also used, once or twice, in the lecture slides.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;it's clearly readable&lt;/item&gt;
      &lt;item&gt;it's not code&lt;/item&gt;
      &lt;item&gt;its content isn't "textable"&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Standardize syntax and style across the transcription files.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45621557</guid><pubDate>Fri, 17 Oct 2025 20:18:54 +0000</pubDate></item><item><title>A laser pointer at 2B FPS [video]</title><link>https://www.youtube.com/watch?v=o4TdHrMi6do</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45632429</guid><pubDate>Sun, 19 Oct 2025 06:42:19 +0000</pubDate></item><item><title>Space Elevator</title><link>https://neal.fun/space-elevator/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45640226</guid><pubDate>Mon, 20 Oct 2025 04:42:08 +0000</pubDate></item><item><title>DeepSeek OCR</title><link>https://github.com/deepseek-ai/DeepSeek-OCR</link><description>&lt;doc fingerprint="d0978f309aa0d982"&gt;
  &lt;main&gt;
    &lt;p&gt;üì• Model Download | üìÑ Paper Link | üìÑ Arxiv Paper Link |&lt;/p&gt;
    &lt;p&gt;Explore the boundaries of visual-text compression.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[2025/10/20]üöÄüöÄüöÄ We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Our environment is cuda11.8+torch2.6.0.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone this repository and navigate to the DeepSeek-OCR folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/deepseek-ai/DeepSeek-OCR.git&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Conda&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;conda create -n deepseek-ocr python=3.12.9 -y
conda activate deepseek-ocr&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Packages&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;download the vllm-0.8.5 whl&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
pip install -r requirements.txt
pip install flash-attn==2.7.3 --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Note: if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers&amp;gt;=4.51.1&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VLLM:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-vllm&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;image: streaming output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_image.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;pdf: concurrency ~2500tokens/s(an A100-40G)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_pdf.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;batch eval for benchmarks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_eval_batch.py&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transformers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;from transformers import AutoModel, AutoTokenizer
import torch
import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
model_name = 'deepseek-ai/DeepSeek-OCR'

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)
model = model.eval().cuda().to(torch.bfloat16)

# prompt = "&amp;lt;image&amp;gt;\nFree OCR. "
prompt = "&amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown. "
image_file = 'your_image.jpg'
output_path = 'your/output/dir'

res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)&lt;/code&gt;
    &lt;p&gt;or you can&lt;/p&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-hf
python run_dpsk_ocr.py&lt;/code&gt;
    &lt;p&gt;The current open-source model supports the following modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native resolution: &lt;list rend="ul"&gt;&lt;item&gt;Tiny: 512√ó512 Ôºà64 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Small: 640√ó640 Ôºà100 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Base: 1024√ó1024 Ôºà256 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;item&gt;Large: 1280√ó1280 Ôºà400 vision tokensÔºâ‚úÖ&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Dynamic resolution &lt;list rend="ul"&gt;&lt;item&gt;Gundam: n√ó640√ó640 + 1√ó1024√ó1024 ‚úÖ&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# document: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown.
# other image: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;OCR this image.
# without layouts: &amp;lt;image&amp;gt;\nFree OCR.
# figures in document: &amp;lt;image&amp;gt;\nParse the figure.
# general: &amp;lt;image&amp;gt;\nDescribe this image in detail.
# rec: &amp;lt;image&amp;gt;\nLocate &amp;lt;|ref|&amp;gt;xxxx&amp;lt;|/ref|&amp;gt; in the image.
# 'ÂÖàÂ§©‰∏ã‰πãÂøßËÄåÂøß'&lt;/code&gt;
    &lt;p&gt;We would like to thank Vary, GOT-OCR2.0, MinerU, PaddleOCR, OneChart, Slow Perception for their valuable models and ideas.&lt;/p&gt;
    &lt;p&gt;We also appreciate the benchmarks: Fox, OminiDocBench.&lt;/p&gt;
    &lt;p&gt;coming soonÔºÅ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45640594</guid><pubDate>Mon, 20 Oct 2025 06:26:33 +0000</pubDate></item><item><title>AWS multiple services outage in us-east-1</title><link>https://health.aws.amazon.com/health/status?ts=20251020</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45640838</guid><pubDate>Mon, 20 Oct 2025 07:22:28 +0000</pubDate></item><item><title>Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system</title><link>https://www.tomshardware.com/tech-industry/semiconductors/alibaba-says-new-pooling-system-cut-nvidia-gpu-use-by-82-percent</link><description>&lt;doc fingerprint="799894bc8f16d6d1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system‚Äî up to 9x increase in output lets 213 GPUs perform like 1,192&lt;/head&gt;
    &lt;p&gt;A paper presented at SOSP 2025 details how token-level scheduling helped one GPU serve multiple LLMs, reducing demand from 1,192 to 213 H20s.&lt;/p&gt;
    &lt;p&gt;Alibaba Cloud claims its new Aegaeon pooling system reduces the number of Nvidia GPUs required to serve large language models by 82% during a multi-month beta test inside its Model Studio marketplace. The result, published in a peer-reviewed paper presented at the 2025 ACM Symposium on Operating Systems (SOSP) in Seoul, suggests that cloud providers may be able to extract significantly more inference capacity from existing silicon, especially in constrained markets like China, where the supply of Nvidia's latest H20s remains limited.&lt;/p&gt;
    &lt;p&gt;Unlike training-time breakthroughs that chase model quality or speed, Aegaeon is an inference-time scheduler designed to maximize GPU utilization across many models with bursty or unpredictable demand. Instead of pinning one accelerator to one model, Aegaeon virtualizes GPU access at the token level, allowing it to schedule tiny slices of work across a shared pool. This means one H20 could serve several different models simultaneously, with system-wide ‚Äúgoodput‚Äù ‚Äî a measure of effective output ‚Äî rising by as much as nine times compared to older serverless systems.&lt;/p&gt;
    &lt;p&gt;The system was tested in production over several months, according to the paper, which lists authors from both Peking University and Alibaba‚Äôs infrastructure division, including CTO Jingren Zhou. During that window, the number of GPUs needed to support dozens of different LLMs ‚Äî ranging in size up to 72 billion parameters ‚Äî fell from 1,192 to just 213.&lt;/p&gt;
    &lt;p&gt;While the paper does not break down which models contributed most to the savings, reporting by the South China Morning Post says the tests were conducted using Nvidia‚Äôs H20, one of the few accelerators still legally available to Chinese buyers under current U.S. export controls.&lt;/p&gt;
    &lt;p&gt;Alibaba says the gains came from two main techniques: Packing multiple models per GPU, and using a token-level autoscaler to dynamically allocate compute as output is generated, rather than reserving resources at the request level. In benchmarks, Aegaeon beat the goodput of ServerlessLLM and MuxServe by margins ranging from 1.5 times to 9 times.&lt;/p&gt;
    &lt;p&gt;Whether those savings translate outside Alibaba‚Äôs stack remains to be seen. Alibaba Cloud‚Äôs paper does not specify the exact network fabric used in the beta test, but we know the company offers its own eRDMA elastic RDMA network and has a record of building highly‚Äëintegrated GPU serving stacks, suggesting the results may depend on an optimized, vertically integrated environment.&lt;/p&gt;
    &lt;p&gt;Regardless, the result is likely to attract interest from other hyperscalers looking to stretch scarce accelerator fleets as inference demand continues to spike.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Luke James is a freelance writer and journalist. Although his background is in legal, he has a personal interest in all things tech, especially hardware and microelectronics, and anything regulatory.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;zsydeepsky&lt;/header&gt;Reply&lt;quote/&gt;There are plenty of H20s currently selling on China's used e-market.pug_s said:Does that mean that there will be used H20's in the Chinese market?&lt;lb/&gt;Just checked, half of them have their price dropped(3-19%).&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45643163</guid><pubDate>Mon, 20 Oct 2025 12:31:22 +0000</pubDate></item><item><title>BERT is just a single text diffusion step</title><link>https://nathan.rs/posts/roberta-diffusion/</link><description>&lt;doc fingerprint="35106edd82bc1c52"&gt;
  &lt;main&gt;
    &lt;p&gt;A while back, Google DeepMind unveiled Gemini Diffusion, an experimental language model that generates text using diffusion. Unlike traditional GPT-style models that generate one word at a time, Gemini Diffusion creates whole blocks of text by refining random noise step-by-step.&lt;/p&gt;
    &lt;p&gt;I read the paper Large Language Diffusion Models and was surprised to find that discrete language diffusion is just a generalization of masked language modeling (MLM), something we‚Äôve been doing since 2018. The first thought I had was, ‚Äúcan we finetune a BERT-like model to do text generation?‚Äù I decided to try a quick proof of concept out of curiosity.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NOTE: After I wrote the article I stumbled upon the paper DiffusionBERT which does essentially the same thing but with more rigorous testing! Check it out if this post interested you.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;A Short History of Transformers#&lt;/head&gt;
    &lt;p&gt;The original Transformer architecture, introduced in 2017, was an encoder-decoder model. In 2018, researchers realized that the encoder and decoder components of the model could be separated (with the advent of BERT and GPT), and two distinct families of models were created:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Encoder-only models (BERT-style, bidirectional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encoder models used masked language modeling (MLM) as a training objective: randomly mask out a subset of tokens of each input and train the encoder to reconstruct the missing tokens (fill in the blanks). The model sees the entire (partially masked) context at once and learns bidirectional representations. This architecture excelled at tasks requiring a full‚Äêsentence (or paragraph) representation (e.g., classification and retrieval).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Decoder-only models (GPT-style, autoregressive)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Decoder models used next‚Äêtoken prediction as a training objective: at each position $t$, predict the token at position $t + 1$ given all tokens up to $t$ as context. Only the left context is used to predict future values (unidirectional). This architecture excelled at generative tasks where you produce text one token at a time, such as open‚Äêended generation, summarization, and translation.&lt;/p&gt;
    &lt;p&gt;Originally, BERT saw immediate use in tasks such as classification, whereas GPT-style models didn‚Äôt become popular until later (due to initial limited capabilities). Eventually, the generation capabilities of autoregressive (decoder) transformers vastly improved. The general training objective of ‚Äúnext token prediction‚Äù means a much larger space of use cases when compared to encoder models.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discrete Language Diffusion Models#&lt;/head&gt;
    &lt;p&gt;Diffusion models were first popularized in image generation. In image generation, diffusion models gradually add Gaussian noise to an image (forward process) and then train a neural network to iteratively denoise it (reverse process). A high‚Äêlevel summary of continuous diffusion with images is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Forward process: Start from a clean image x‚ÇÄ, then add small amounts of (usually Gaussian) noise at each timestep until you end up with near‚Äêpure noise.&lt;/item&gt;
      &lt;item&gt;Reverse process: Train a model (often a U‚ÄêNet) to predict the noise at each timestep, gradually recovering the original image in discrete denoising steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Applying this idea to language means we need a way to add noise to text and then remove it in stages. The simplest way to do this is a masking‚Äêbased noise process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Forward (masking) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;At timestep t = 0, you have a fully uncorrupted text sequence.&lt;/item&gt;
          &lt;item&gt;At each subsequent timestep t &amp;gt; 0, randomly replace a fraction of tokens with a special &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;token according to a pre‚Äêdefined schedule (e.g., gradually increasing the masked proportion from 0% to 100%).&lt;/item&gt;
          &lt;item&gt;By the final timestep T, the entire sequence may be masked (all tokens are &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reverse (denoising) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Train a model (often a standard Transformer encoder) to predict the original token IDs given a partially masked sequence at timestep t.&lt;/item&gt;
          &lt;item&gt;This is akin to performing masked language modeling at varying mask rates: at early timesteps, only a few tokens are masked (easy to predict); at later timesteps, many tokens are masked (harder).&lt;/item&gt;
          &lt;item&gt;By chaining together predictions from high‚Äêmask‚Äêrate back down to zero, you can recover (or generate) a full sequence.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this discrete text diffusion framework, the model learns a likelihood bound on the data distribution by optimizing a sum of denoising losses over all timesteps, rather than a single MLM objective at a fixed mask probability.&lt;/p&gt;
    &lt;p&gt;As we can see, BERT‚Äôs masked language modeling objective is the same training objective as text diffusion, but just for a subset of masking rates. By introducing variable masking rates (from 0 to 1) and a scheduled sequence of denoising steps (inspired by diffusion theory), we can transform BERT‚Äôs masked language modeling objective into a full generative procedure.&lt;/p&gt;
    &lt;head rend="h2"&gt;RoBERTa Diffusion#&lt;/head&gt;
    &lt;p&gt;In 2019, RoBERTa was released. It was essentially just an enhancement of the original BERT model, with better hyperparameters, data training size, and a more simple training objective (MLM only, removed next sentence prediction).&lt;/p&gt;
    &lt;p&gt;Here we use the HuggingFace &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;dataset&lt;/code&gt; libraries to pull in the original RoBERTa weights, tokenizer, and the Trainer class to easily finetune the model on the WikiText dataset.
The main code (full code here) looks like this below:&lt;/p&gt;
    &lt;code&gt;# Load and tokenize dataset and instantiate the model
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")
model = RobertaForMaskedLM.from_pretrained("roberta-base")

# Create the training args and Trainer instance
training_args = TrainingArguments(
    output_dir="finetuned-roberta-diffusion",
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    save_strategy="epoch",
    save_total_limit=1,
    logging_steps=200,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    data_collator=diffusion_collator, # custom implementation
    tokenizer=tokenizer,
)

# Train &amp;amp; save
trainer.train()
trainer.save_model("finetuned-roberta-diffusion")&lt;/code&gt;
    &lt;p&gt;Currently we have 10 diffusion steps, so we randomly sample a percentage $p$ out of &lt;code&gt;mask_probs&lt;/code&gt; (1.0, 0.9, 0.9, &amp;amp;mldr;, 0.1) and mask that percent of the tokens each batch.
The custom &lt;code&gt;diffusion_collator&lt;/code&gt; function (see code here) samples one mask-probability &lt;code&gt;p&lt;/code&gt; from &lt;code&gt;mask_probs&lt;/code&gt; per batch and sets each token to &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; with &lt;code&gt;p&lt;/code&gt; probability.&lt;/p&gt;
    &lt;p&gt;To be able to condition the generation on a ‚Äúprompt‚Äù, we currently never mask the first 16 tokens. That means that during training, each step will always have the first 16 tokens as context for generation.&lt;/p&gt;
    &lt;p&gt;Simplified code for the &lt;code&gt;diffusion_collator&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;  def diffusion_collator(examples):
      batch = tokenizer.pad(examples, return_tensors="pt")

      # Randomly select masking probability for this batch
      mask_prob = random.choice([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])

      # Never mask the first PREFIX_LEN tokens (preserved context)
      maskable_positions = batch.input_ids[:, PREFIX_LEN:]

      # Create random mask for the chosen probability
      mask = torch.rand(maskable_positions.shape) &amp;lt; mask_prob

      # Apply masking
      batch.input_ids[:, PREFIX_LEN:][mask] = tokenizer.mask_token_id
      batch.labels = batch.input_ids.clone()

      return batch&lt;/code&gt;
    &lt;p&gt;For inference, we start with an input which is a tensor of size 256 (since we are generating blocks of 256 tokens). The first 16 positions are the token ids that correspond to the prompt, and the last 240 are just &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens. We iterate through the denoising schedule and each step, we generate a prediction and then remask the sequence again. The process looks like this:&lt;/p&gt;
    &lt;code&gt;Step 0: [PREFIX] &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; ...     (100% masked)
Step 1: [PREFIX] will &amp;lt;mask&amp;gt; over &amp;lt;mask&amp;gt; control ...        (90% masked)
Step 2: [PREFIX] will begin &amp;lt;mask&amp;gt; greater control ...      (80% masked)
...
Step 10: [PREFIX] will begin to assert greater control ...  (0% masked - DONE)&lt;/code&gt;
    &lt;p&gt;Simplified code for generation looks like:&lt;/p&gt;
    &lt;code&gt;# Generate text through iterative denoising
for step, mask_prob in enumerate(mask_probs):
    # Forward pass: predict masked tokens
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = outputs.logits  # shape: (1, MAX_LEN, vocab_size)

    # For each masked position, sample from top-k/top-p filtered distribution
    for pos in range(PREFIX_LEN, MAX_LEN):
        if input_ids[0, pos] == tokenizer.mask_token_id:
            logits = predictions[0, pos, :]
            # Apply top-k and top-p filtering
            filtered_logits = top_k_top_p_filtering(logits, top_k=TOP_K, top_p=TOP_P)
            probs = F.softmax(filtered_logits, dim=-1)
            # Sample token
            sampled_token = torch.multinomial(probs, 1)
            input_ids[0, pos] = sampled_token

    # Re-mask a portion of non-prefix tokens for next iteration
    if mask_prob &amp;gt; 0:
        mask_indices = torch.rand(MAX_LEN - PREFIX_LEN) &amp;lt; mask_prob
        input_ids[0, PREFIX_LEN:][mask_indices] = tokenizer.mask_token_id&lt;/code&gt;
    &lt;p&gt;Here is an example output generation of the fine-tuned model after training on an H200 for 30 minutes (the first line is the initial prompt):&lt;/p&gt;
    &lt;code&gt;Following their victory in the French and Indian War, Britain began to assert
greater...

...dominion over Europe beginning about the early 19th. There conflict took
place on the island, between British and Irish Ireland. British officials 
administered British Ireland, a Celtic empire under the control of the Irish 
nationalist authorities, defined as a dominion of Britain. As the newly Fortic 
states acquired independent and powerful status, many former English colonies
played their part in this new, British @-@ controlled colonial system. Following
this period the Non @-@ Parliamentaryist Party won its influence in Britain in 
1890, led by the support of settlers from the Irish colonies. Looking inwards, 
Sinclair, Lewis questioned, and debated the need to describe " The New Britain "&lt;/code&gt;
    &lt;p&gt;The output looks surprisingly coherent! Most of the quirks present are actually just quirks from the formatting of WikiText (spaces around punctuation &lt;code&gt;"&lt;/code&gt;, turning hyphens &lt;code&gt;-&lt;/code&gt; into &lt;code&gt;@-@&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Below is a comparison between our diffusion model and GPT-2:&lt;/p&gt;
    &lt;p&gt;We see GPT-2‚Äôs output is more coherent and slightly faster (~9 seconds vs ~13) but I‚Äôm pleasantly surprised with how good my simple implementation was. It is a good proof of concept, and with new approaches like AR-Diffusion and Skip-Step Diffusion (and a more optimized implementation), the quality and speed can be drastically improved.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;We‚Äôve seen that masked language models like RoBERTa, originally designed for fill-in-the-blank tasks, can be repurposed into fully generative engines by interpreting variable-rate masking as a discrete diffusion process. By gradually corrupting text with &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens and training the model to iteratively denoise at increasing mask intensities, we effectively turn the standard MLM objective into a step-by-step generation procedure.&lt;/p&gt;
    &lt;p&gt;Even without architectural changes, a fine-tuned RoBERTa can generate coherent looking text after slightly modifying the training objective, validating the idea that BERT-style models are essentially just text diffusion models trained on one masking rate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45644328</guid><pubDate>Mon, 20 Oct 2025 14:31:16 +0000</pubDate></item><item><title>Show HN: I created a cross-platform GUI for the JJ VCS (Git compatible)</title><link>https://judojj.com</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45645120</guid><pubDate>Mon, 20 Oct 2025 15:35:19 +0000</pubDate></item><item><title>Production RAG: what I learned from processing 5M+ documents</title><link>https://blog.abdellatif.io/production-rag-processing-5m-documents</link><description>&lt;doc fingerprint="9bf95f134d9a6771"&gt;
  &lt;main&gt;
    &lt;p&gt;I've spent the last 8 months in the RAG trenches, I want to share what actually worked vs. wasted our time. We built RAG for Usul AI (9M pages) and an unnamed legal AI enterprise (4M pages).&lt;/p&gt;
    &lt;head rend="h2"&gt;Langchain + Llamaindex&lt;/head&gt;
    &lt;p&gt;We started out with youtube tutorials. First Langchain ‚Üí Llamaindex. Got to a working prototype in a couple of days and were optimistic with the progress. We run tests on subset of the data (100 documents) and the results looked great. We spent the next few days running the pipeline on the production dataset and got everything working in a week ‚Äî incredible.&lt;/p&gt;
    &lt;p&gt;Except it wasn't, the results were subpar and only the end users could tell. We spent the following few months rewriting pieces of the system, one at a time, until the performance was at the level we wanted. Here are things we did ranked by ROI.&lt;/p&gt;
    &lt;head rend="h2"&gt;What moved the needle&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query Generation: not all context can be captured by the user's last query. We had an LLM review the thread and generate a number of semantic + keyword queries. We processed all of those queries in parallel, and passed them to a reranker. This made us cover a larger surface area and not be dependent on a computed score for hybrid search.&lt;/item&gt;
      &lt;item&gt;Reranking: the highest value 5 lines of code you'll add. The chunk ranking shifted a lot. More than you'd expect. Reranking can many times make up for a bad setup if you pass in enough chunks. We found the ideal reranker set-up to be 50 chunk input -&amp;gt; 15 output.&lt;/item&gt;
      &lt;item&gt;Chunking Strategy: this takes a lot of effort, you'll probably be spending most of your time on it. We built a custom flow for both enterprises, make sure to understand the data, review the chunks, and check that a) chunks are not getting cut mid-word or sentence b) ~each chunk is a logical unit and captures information on its own&lt;/item&gt;
      &lt;item&gt;Metadata to LLM: we started by passing the chunk text to the LLM, we ran an experiment and found that injecting relevant metadata as well (title, author, etc.) improves context and answers by a lot.&lt;/item&gt;
      &lt;item&gt;Query routing: many users asked questions that can't be answered by RAG (e.g. summarize the article, who wrote this). We created a small router that detects these questions and answers them using an API call + LLM instead of the full-blown RAG set-ups.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our stack&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vector database: Azure -&amp;gt; Pinecone -&amp;gt; Turbopuffer (cheap, supports keyword search natively)&lt;/item&gt;
      &lt;item&gt;Document Extraction: Custom&lt;/item&gt;
      &lt;item&gt;Chunking: Unstructured.io by default, custom for enterprises (heard that Chonkie is good)&lt;/item&gt;
      &lt;item&gt;Embedding: text-embedding-large-3, haven't tested others&lt;/item&gt;
      &lt;item&gt;Reranker: None -&amp;gt; Cohere 3.5 -&amp;gt; Zerank (less known but actually good)&lt;/item&gt;
      &lt;item&gt;LLM: GPT 4.1 -&amp;gt; GPT 5 -&amp;gt; GPT 4.1, covered by Azure credits&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Going Open-source&lt;/head&gt;
    &lt;p&gt;We put all our learning into an open-source project: agentset-ai/agentset under an MIT license. Feel free to reach out if you have any questions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45645349</guid><pubDate>Mon, 20 Oct 2025 15:55:36 +0000</pubDate></item><item><title>x86-64 Playground ‚Äì An online assembly editor and GDB-like debugger</title><link>https://x64.halb.it/</link><description>&lt;doc fingerprint="aefedf1ae8255dda"&gt;
  &lt;main&gt;
    &lt;p&gt;x86-64 Playground is a web app for experimenting and learning x86-64 assembly.&lt;/p&gt;
    &lt;p&gt;The Playground web app provides an online code editor where you can write, compile, and share assembly code for a wide range of popular assemblers such as GNU As, Fasm and Nasm.&lt;/p&gt;
    &lt;p&gt;Unlike traditional onlide editors, this playground allows you to follow the execution of your program step by step, inspecting memory and registers of the running process from a GDB-like interface.&lt;/p&gt;
    &lt;p&gt;You can bring your own programs! Drag and drop into the app any x86-64-Linux static executable to run and debug it in the same sandboxed environment, without having to install anything.&lt;/p&gt;
    &lt;p&gt;The app is for anyone that wants to run amd64 assembly snippets or inspect the inner workings of simple Linux ELF files.&lt;/p&gt;
    &lt;p&gt;It has been designed with the academic world of binary exploitation in mind; The debugger interface offers visualizations similar to the GDB+PwnGDB debugger plugin, and all the controls are labelled with the respective GDB commands.&lt;/p&gt;
    &lt;p&gt;Combined with Compiler Explorer, this app provides a noise-free environment to learn the basics behind the inner workings of a Linux process. When you are ready, it includes the guides and resources necessary to keep experimenting on your own linux environment, with the actual GDB debugger.&lt;/p&gt;
    &lt;p&gt;Have you ever seen a responsive debugger? The app places the mobile experience at the center of its design, and can be embedded in any web page to add interactivity to technical tutorials or documentations.&lt;/p&gt;
    &lt;p&gt;Follow the guide to embed in your website both the asm editor and debugger.&lt;/p&gt;
    &lt;p&gt;The app is open-source, and available on Github. It's powered by the Blink Emulator, which emulates an x86-64-Linux environment entirely client side in your browser. This means that all the code you write, or the excutables you debug are never sent to the server.&lt;/p&gt;
    &lt;p&gt;everything runs in your browser, and once the Web App loads it will work without an internet connection.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45646958</guid><pubDate>Mon, 20 Oct 2025 17:55:18 +0000</pubDate></item><item><title>Claude Code on the web</title><link>https://www.anthropic.com/news/claude-code-on-the-web</link><description>&lt;doc fingerprint="7f0c0efeffb1c2ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Claude Code on the web&lt;/head&gt;
    &lt;p&gt;Today, we're introducing Claude Code on the web, a new way to delegate coding tasks directly from your browser.&lt;/p&gt;
    &lt;p&gt;Now in beta as a research preview, you can assign multiple coding tasks to Claude that run on Anthropic-managed cloud infrastructure, perfect for tackling bug backlogs, routine fixes, or parallel development work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run coding tasks in parallel&lt;/head&gt;
    &lt;p&gt;Claude Code on the web lets you kick off coding sessions without opening your terminal. Connect your GitHub repositories, describe what you need, and Claude handles the implementation.&lt;/p&gt;
    &lt;p&gt;Each session runs in its own isolated environment with real-time progress tracking, and you can actively steer Claude to adjust course as it‚Äôs working through tasks.&lt;/p&gt;
    &lt;p&gt;With Claude Code running in the cloud, you can now run multiple tasks in parallel across different repositories from a single interface and ship faster with automatic PR creation and clear change summaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;Flexible for every workflow&lt;/head&gt;
    &lt;p&gt;The web interface complements your existing Claude Code workflow. Running tasks in the cloud is especially effective for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answering questions about how projects work and how repositories are mapped&lt;/item&gt;
      &lt;item&gt;Bugfixes and routine, well-defined tasks&lt;/item&gt;
      &lt;item&gt;Backend changes, where Claude Code can use test-driven development to verify changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also use Claude Code on mobile. As part of this research preview, we‚Äôre making Claude Code available on our iOS app so developers can explore coding with Claude on the go. It‚Äôs an early preview, and we hope to quickly refine the mobile experience based on your feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security-first cloud execution&lt;/head&gt;
    &lt;p&gt;Every Claude Code task runs in an isolated sandbox environment with network and filesystem restrictions. Git interactions are handled through a secure proxy service that ensures Claude can only access authorized repositories‚Äîhelping keep your code and credentials protected throughout the entire workflow.&lt;/p&gt;
    &lt;p&gt;You can also add custom network configuration to choose what domains Claude Code can connect to from its sandbox. For example, you can allow Claude to download npm packages over the internet so that it can run tests and validate changes.&lt;/p&gt;
    &lt;p&gt;Read our engineering blog and documentation for a deep dive on Claude Code‚Äôs sandboxing approach.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;Claude Code on the web is available now in research preview for Pro and Max users. Visit claude.com/code to connect your first repository and start delegating tasks.&lt;/p&gt;
    &lt;p&gt;Cloud-based sessions share rate limits with all other Claude Code usage. Explore our documentation to learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45647166</guid><pubDate>Mon, 20 Oct 2025 18:12:23 +0000</pubDate></item><item><title>Today is when the Amazon brain drain sent AWS down the spout</title><link>https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/</link><description>&lt;doc fingerprint="3d28b35914248ecf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Today is when the Amazon brain drain finally sent AWS down the spout&lt;/head&gt;
    &lt;head rend="h2"&gt;When your best engineers log off for good, don‚Äôt be surprised when the cloud forgets how DNS works&lt;/head&gt;
    &lt;p&gt;column "It's always DNS" is a long-standing sysadmin saw, and with good reason: a disproportionate number of outages are at their heart DNS issues. And so today, as AWS is still repairing its downed cloud as this article goes to press, it becomes clear that the culprit is once again DNS. But if you or I know this, AWS certainly does.&lt;/p&gt;
    &lt;p&gt;And so, a quiet suspicion starts to circulate: where have the senior AWS engineers who've been to this dance before gone? And the answer increasingly is that they've left the building ‚Äî taking decades of hard-won institutional knowledge about how AWS's systems work at scale right along with them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What happened?&lt;/head&gt;
    &lt;p&gt;AWS reports that on October 20, at 12:11 AM PDT, it began investigating ‚Äúincreased error rates and latencies for multiple AWS services in the US-EAST-1 Region.‚Äù About an hour later, at 1:26 AM, the company confirmed ‚Äúsignificant error rates for requests made to the DynamoDB endpoint‚Äù in that region. By 2:01 AM, engineers had identified DNS resolution of the DynamoDB API endpoint for US-EAST-1 as the likely root cause, which led to cascading failures for most other things in that region. DynamoDB is a "foundational service" upon which a whole mess of other AWS services rely, so the blast radius for an outage touching this thing can be huge.&lt;/p&gt;
    &lt;p&gt;As a result, much of the internet stopped working: banking, gaming, social media, government services, buying things I don't need on Amazon.com itself, etc.&lt;/p&gt;
    &lt;p&gt;AWS has given increasing levels of detail, as is their tradition, when outages strike, and as new information comes to light. Reading through it, one really gets the sense that it took them 75 minutes to go from "things are breaking" to "we've narrowed it down to a single service endpoint, but are still researching," which is something of a bitter pill to swallow. To be clear: I've seen zero signs that this stems from a lack of transparency, and every indication that they legitimately did not know what was breaking for a patently absurd length of time.&lt;/p&gt;
    &lt;p&gt;Note that for those 75 minutes, visitors to the AWS status page (reasonably wondering why their websites and other workloads had just burned down and crashed into the sea) were met with an "all is well!" default response. Ah well, it's not as if AWS had previously called out slow outage notification times as an area for improvement. Multiple times even. We can keep doing this if you'd like.&lt;/p&gt;
    &lt;head rend="h3"&gt;The prophecy&lt;/head&gt;
    &lt;p&gt;AWS is very, very good at infrastructure. You can tell this is a true statement by the fact that a single one of their 38 regions going down (albeit a very important region!) causes this kind of attention, as opposed to it being "just another Monday outage." At AWS's scale, all of their issues are complex; this isn't going to be a simple issue that someone should have caught, just because they've already hit similar issues years ago and ironed out the kinks in their resilience story.&lt;/p&gt;
    &lt;p&gt;Once you reach a certain point of scale, there are no simple problems left. What's more concerning to me is the way it seems AWS has been flailing all day trying to run this one to ground. Suddenly, I'm reminded of something I had tried very hard to forget.&lt;/p&gt;
    &lt;p&gt;At the end of 2023, Justin Garrison left AWS and roasted them on his way out the door. He stated that AWS had seen an increase in Large Scale Events (or LSEs), and predicted significant outages in 2024. It would seem that he discounted the power of inertia, but the pace of senior AWS departures certainly hasn't slowed ‚Äî and now, with an outage like this, one is forced to wonder whether those departures are themselves a contributing factor.&lt;/p&gt;
    &lt;p&gt;You can hire a bunch of very smart people who will explain how DNS works at a deep technical level (or you can hire me, who will incorrect you by explaining that it's a database), but the one thing you can't hire for is the person who remembers that when DNS starts getting wonky, check that seemingly unrelated system in the corner, because it has historically played a contributing role to some outages of yesteryear.&lt;/p&gt;
    &lt;p&gt;When that tribal knowledge departs, you're left having to reinvent an awful lot of in-house expertise that didn't want to participate in your RTO games, or play Layoff Roulette yet again this cycle. This doesn't impact your service reliability ‚Äî until one day it very much does, in spectacular fashion. I suspect that day is today.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AWS outage exposes Achilles heel: central control plane&lt;/item&gt;
      &lt;item&gt;Major AWS outage across US-East region breaks half the internet&lt;/item&gt;
      &lt;item&gt;Amazon spills plan to nuke Washington...with X-Energy mini-reactors&lt;/item&gt;
      &lt;item&gt;Amazon turns James Bond into the Man Without the Golden Gun&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The talent drain evidence&lt;/head&gt;
    &lt;p&gt;This is The Register, a respected journalistic outlet. As a result, I know that if I publish this piece as it stands now, an AWS PR flak will appear as if by magic, waving their hands, insisting that "there is no talent exodus at AWS," a la Baghdad Bob. Therefore, let me forestall that time-wasting enterprise with some data.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is a fact that there have been 27,000+ Amazonians impacted by layoffs between 2022 and 2024, continuing into 2025. It's hard to know how many of these were AWS versus other parts of its Amazon parent, because the company is notoriously tight-lipped about staffing issues.&lt;/item&gt;
      &lt;item&gt;Internal documents reportedly say that Amazon suffers from 69 percent to 81 percent regretted attrition across all employment levels. In other words, "people quitting who we wish didn't."&lt;/item&gt;
      &lt;item&gt;The internet is full of anecdata of senior Amazonians lamenting the hamfisted approach of their Return to Office initiative; experts have weighed in citing similar concerns.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you were one of the early employees who built these systems, the world is your oyster. There's little reason to remain at a company that increasingly demonstrates apparent disdain for your expertise.&lt;/p&gt;
    &lt;head rend="h3"&gt;My take&lt;/head&gt;
    &lt;p&gt;This is a tipping point moment. Increasingly, it seems that the talent who understood the deep failure modes is gone. The new, leaner, presumably less expensive teams lack the institutional knowledge needed to, if not prevent these outages in the first place, significantly reduce the time to detection and recovery. Remember, there was a time when Amazon's "Frugality" leadership principle meant doing more with less, not doing everything with basically nothing. AWS's operational strength was built on redundant, experienced people, and when you cut to the bone, basic things start breaking.&lt;/p&gt;
    &lt;p&gt;I want to be very clear on one last point. This isn't about the technology being old. It's about the people maintaining it being new. If I had to guess what happens next, the market will forgive AWS this time, but the pattern will continue.&lt;/p&gt;
    &lt;p&gt;AWS will almost certainly say this was an "isolated incident," but when you've hollowed out your engineering ranks, every incident becomes more likely. The next outage is already brewing. It's just a matter of which understaffed team trips over which edge case first, because the chickens are coming home to roost. ¬Æ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45649178</guid><pubDate>Mon, 20 Oct 2025 20:50:03 +0000</pubDate></item><item><title>Old Computer Challenge ‚Äì Modern Web for the ZX Spectrum</title><link>https://0x00.cl/blog/2025/occ-2025/</link><description>&lt;doc fingerprint="a76899312b36a4a4"&gt;
  &lt;main&gt;
    &lt;p&gt;2025/10/19&lt;/p&gt;
    &lt;p&gt;Table of contents&lt;/p&gt;
    &lt;p&gt;Last year I participated in the OCC challenge and between work and personal life I totally forgot about it, luckily I saw a post online of someone doing the challenge and remembered about it. I didn‚Äôt want to miss this year challenge as it was fun learning something new last year, so hopefully its not too late.&lt;/p&gt;
    &lt;p&gt;This years challenge it was a DIY, you could create your own OCC challenge. So I thought it‚Äôd be fun to try to recreate a website and navigation for the ZX Spectrum using BASIC and the limited graphics capabilities of the computer.&lt;/p&gt;
    &lt;p&gt;Well, in this case the hardware is the ZX Spectrum, though I must say that I used an emulator (Fuse) to test and run my code.&lt;/p&gt;
    &lt;p&gt;The ZX Spectrum image resolution is 256x192 pixels, so the space is very limited. The colour palette is made up of 8 colors, with a ‚Äúbrighter‚Äù variation.&lt;/p&gt;
    &lt;p&gt;With this in mind I had to design a website for the ZX Spectrum and how you‚Äôd navigate with it.&lt;/p&gt;
    &lt;p&gt;So given the hardware and limitations I thought that I‚Äôd first do Google, since they have a simple design and is I‚Äôd say well known by everyone.&lt;/p&gt;
    &lt;p&gt;The image below, is what Googles homepage would look like.&lt;/p&gt;
    &lt;p&gt;You‚Äôd access their services listed by pressing the first letter of that service, in case there is more than 1 with the same letter you‚Äôd use another letter or simply add a second page of services.&lt;/p&gt;
    &lt;p&gt;In this case you can access Search by pressing ‚ÄúS‚Äù. It will simply prompt for your search and in this case I searched for ‚ÄúHow to run LLM on a microprocessor‚Äù and you‚Äôd navigate through the different results by going pressing ‚Äúw‚Äù (up) or ‚Äús‚Äù (down).&lt;/p&gt;
    &lt;p&gt;Hacker news is text heavy content, so it makes it ‚Äúeasy‚Äù to design it for the ZX Spectrum as its graphics capabilities are kinda limited.&lt;/p&gt;
    &lt;p&gt;As you can see, you‚Äôll be able to see your username and the amount of points you have in the top bar and in this case I decided that navigation would be done ‚Äúless‚Äù interactively and you‚Äôll access posts by simply selecting the number (1 to 6). If you want to go to the second page of posts you can press ‚ÄúM‚Äù and if you want to submit you press ‚ÄúS‚Äù.&lt;/p&gt;
    &lt;p&gt;So if we select the first post by pressing 1, we‚Äôll load more details about the post and the comment section.&lt;/p&gt;
    &lt;p&gt;You‚Äôll see the title, user that posted it, time and the comment section with the amount of comments, in this case 42. The problem with the comment section is that some comment threads can go deep and have different branches as multiple users will reply. So in this case what I thought was to simply navigate the top level comments with ‚ÄúM‚Äù for the next comment and ‚ÄúB‚Äù to go back to the previous top level comment. I guess you could add a ‚Äú[R]eply‚Äù option, to reply to the comment you are seeing on screen and navigating through the comment replies, you‚Äôd press something like ‚ÄúC‚Äù and you‚Äôll enter a page of the child comments of the parent comment. The only problem with this is that because the screen is so tiny and limited, that most of the time you‚Äôll only be able to see 1 comment at a time and when posts have 300 comments, navigating them is going to be hard.&lt;/p&gt;
    &lt;p&gt;And this is what submitting a new link would look like, you‚Äôd simply ‚Äúedit‚Äù the different fields and then ‚ÄúS"ubmit&lt;/p&gt;
    &lt;p&gt;Personally I had fun doing this, I didn‚Äôt have to spend a lot of time researching before doing that as I had already done a lot of that work in last year challenge, so I could focus on what I wanted to do. You can check the source code and try it yourself in my repository https://codeberg.org/0x00cl/Web-ZX&lt;/p&gt;
    &lt;p&gt;Here are videos of the interactions in the ZX Spectrum&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45650792</guid><pubDate>Mon, 20 Oct 2025 23:40:25 +0000</pubDate></item><item><title>Practical Scheme</title><link>https://practical-scheme.net/index.html#docs</link><description>&lt;doc fingerprint="c343dacf9eb15a95"&gt;
  &lt;main&gt;
    &lt;p&gt;This page is a collection of libraries and extensions to use Scheme as a production tool. By "production tools" I mean the tools to process daily chores for systems engineers and programmers---parsing files, generate reports, watching processes, providing small GUI wrappers, and all sorts of those things. Currently I'm using Perl for those purpose, but I'm always longing to use Scheme for them. So I started this page.&lt;/p&gt;
    &lt;p&gt;Most stuffs in this site are done as my private project at home, except the ones explicitly stated otherwise. I upload libraries even in its alpha/beta stage, since I'd like to test and use them at work, too. In a way, my primary interest is to make my life happier. No warranty comes with them, as usual, but it'll be nice if somebody else finds they are useful.&lt;/p&gt;
    &lt;p&gt;If you can read Japanese, visit the Japanese page which contains some translations of Lisp/Scheme related articles.&lt;/p&gt;
    &lt;p&gt;I wrote a Wiki Clone in Scheme (Gauche). Come and try it: WiLiKi.&lt;/p&gt;
    &lt;head rend="h2"&gt;Applications and tools&lt;/head&gt;
    &lt;p&gt;Scheme-related stand alone programs.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Gauche - Current version 0.9.15 (2024/04/24)&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;An R7RS Scheme implementation aimed at a handy script engine. Quick startup, built-in system interface, and native multilingual support are some of the goals.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;WiLiKi - Current version 0.6.2 (2014/11/28)&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;A wiki engine written in Scheme.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;Chaton - Current version&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;A Comet-based Webchat system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;escm - Current version 1.1 (2014/11/28)&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;A filter program which copies the input text to output, with processing embedded Scheme expressions. This program itself is independent from any Scheme implementation; you can use your favorite one. Useful to process text files with a bit of dynamic parts. This page itself is processed by escm to embed information such as the update time of libraries, and synchronize with Japanese version. A complete new version of escm, named aescm, is being developed by TAGA Yoshitaka ( http://sourceforge.net/projects/escm/)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Libraries and Extensions&lt;/head&gt;
    &lt;p&gt;The following libraries and extensions are written for Gauche. See here for libraries written for STk.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Gauche-gl - Download Document Current version 0.6 (2014/08/09)&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;OpenGL binding for Gauche. Supports most of OpenGL 1.0 to 4.1 APIs (including OpenGL Shading Language API), and some of GLU and GLUT API. Requires Gauche 0.9.4 or later.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;Gauche-gtk2 - Download Document Current version 0.6.1 (2022/3/20)&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;GTK2 binding for Gauche.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Documents&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Scheme Cross Reference&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;A cross reference of library procedures of various Scheme implementations. Updated constantly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;Shooting A Moving Target--- An Experience In Developing A Production Tracking Database&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;An application of CommonLisp in practice. (yeah, it's not Scheme... anyway, I put it here).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;Tracking Assets in the Production of 'Final Fantasy : The Spirits Within'&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;A follow-up of the article above, a kind of post-mortem of the production.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;Gluing Things Together - Scheme in the Real-time CG Content Production&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;A paper presented at International Lisp Conference 2002 at San Francisco, October 2002. (there's also a pdf version).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-5"&gt;Efficient floating-point number handling for dynamically typed scripting languages (pdf)&lt;/item&gt;
      &lt;item rend="dd-5"&gt;
        &lt;p&gt;A paper presented at Dynamic Language Symposium 2008.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-6"&gt;Schemer's Way&lt;/item&gt;
      &lt;item rend="dd-6"&gt;
        &lt;p&gt;Trying to explain Scheme's merits to non-Scheme programmers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Other Resources&lt;/head&gt;
    &lt;p&gt;This list no way covers everything, but you can follow links in those links.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Schemers.org&lt;/item&gt;
      &lt;item rend="dd-1"&gt;A good anchor point to collect information of Scheme. You can get R*RS, the language standard. The site is also a center of SRFI's--- Scheme Request For Implementation---which provides common interface of libraries across various implementations.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;SCM&lt;/item&gt;
      &lt;item rend="dd-2"&gt;A compact, fast and portable implementation of Scheme interpreter.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;SLIB&lt;/item&gt;
      &lt;item rend="dd-3"&gt;A large collection of portable Scheme libraries. The contents spans from small utilities complements the standard conformance, to the full-featured relational database.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Programming Languages by Dai Inukai&lt;/item&gt;
      &lt;item rend="dd-4"&gt;Scheme-related documents by Dai Inukai, the author of "Nyuumon Scheme (Scheme Primer)" in Japan. Check this out if you're interested in processing Japanese in Scheme.&lt;/item&gt;
      &lt;item rend="dt-5"&gt;Bigloo&lt;/item&gt;
      &lt;item rend="dd-5"&gt;A scheme system with compiler and integrated development environment. If you're planning to write an enterprise software rather than just a bunch of scripts, look at it.&lt;/item&gt;
      &lt;item rend="dt-6"&gt;Guile&lt;/item&gt;
      &lt;item rend="dd-6"&gt;GNU adopted Scheme for the base of extension language several years ago. The effort became Guile. If you have one of popular Linux distributions, you may already have it.&lt;/item&gt;
      &lt;item rend="dt-7"&gt;scsh&lt;/item&gt;
      &lt;item rend="dd-7"&gt;I haven't used this one much, but looks good if you're looking for a tool to do syste programming.&lt;/item&gt;
      &lt;item rend="dt-8"&gt;The Internet Scheme Repository&lt;/item&gt;
      &lt;item rend="dd-8"&gt;As the name suggests.&lt;/item&gt;
      &lt;item rend="dt-9"&gt;Kawa - the Java-based Scheme System&lt;/item&gt;
      &lt;item rend="dd-9"&gt;A Scheme environment written in Java by Per Bothner. Scheme code is compiled to Java bytecode, hence has the property "write once run everywhere".&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45652859</guid><pubDate>Tue, 21 Oct 2025 05:47:39 +0000</pubDate></item><item><title>Language Support for Marginalia Search</title><link>https://www.marginalia.nu/log/a_126_multilingual/</link><description>&lt;doc fingerprint="f210038521b88b84"&gt;
  &lt;main&gt;
    &lt;p&gt;One of the big ambitions for the search engine this year has been to enable searching in more languages than English, and a pilot project for this has just been completed, allowing experimental support for German, French and Swedish.&lt;/p&gt;
    &lt;p&gt;These changes are now live for testing, but with an extremely small corpus of documents.&lt;/p&gt;
    &lt;p&gt;As the search engine has been up to this point built with English in mind, some anglo-centric assumptions made it into its code. A lot of the research on search engines generally seems to embed similar assumptions.&lt;/p&gt;
    &lt;p&gt;As this is a domain rife with unknown unknowns, the ambition for this pilot was to implement support for just a few additional languages in order to get a feel for how much work would be required to support more languages in general, as well as to assess how much the index grows when this is done.&lt;/p&gt;
    &lt;p&gt;Though it was fully understood upfront that supporting all languages in one go is unrealistic, as some languages are more different than others and require significant additional work. Human language is surprisingly disparate.&lt;/p&gt;
    &lt;p&gt;A language like Japanese, for example, has not only multiple alphabets, but embeds character width in unicode; on top of that the language doesn‚Äôt put spaces between words. As such the language requires special normalization.&lt;/p&gt;
    &lt;p&gt;Latin, on the other hand, has dozens of forms for each word, and the words can often be reordered without significantly changing the meaning of a sentence. On the one hand this makes the grammatical analysis of the language somewhat easier since the words announce their function in the sentence fairly unambiguously, but on the other you probably need to store the text in a lemmatized form, and then strongly de-prioritize word order when matching.&lt;/p&gt;
    &lt;p&gt;Google‚Äôs bungled handling of Russian was supposedly why Yandex was able to eke out a foothold in that market.&lt;/p&gt;
    &lt;head rend="h2"&gt;What needs changing&lt;/head&gt;
    &lt;p&gt;The search engine‚Äôs language processing chain is fairly long, but the most salient parts go something like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Text is extracted from the HTML&lt;/item&gt;
      &lt;item&gt;Language is identified using fasttext&lt;/item&gt;
      &lt;item&gt;Text is broken into sentences&lt;/item&gt;
      &lt;item&gt;Words are lowercased and Unicode is normalized&lt;/item&gt;
      &lt;item&gt;Sentences are stemmed and POS-tagged&lt;/item&gt;
      &lt;item&gt;Sentences, with stemming and POS-tag data is fed into keyword extraction algorithms&lt;list rend="ul"&gt;&lt;item&gt;Keywords are mapped to positions and HTML tags&lt;/item&gt;&lt;item&gt;Important keywords are identified using TF-IDF (using stemmed forms)&lt;/item&gt;&lt;item&gt;Important keywords are identified using grammar patterns (POS-tags)&lt;/item&gt;&lt;item&gt;Important keywords are identified using other heuristics&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Keywords are hashed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Stemming is an imperfect way of getting a base form of a word, though generally such algorithms have a great number of flaws, so that e.g. universe and university seem to be the same word. This is only used in tf-idf calculations.&lt;/p&gt;
    &lt;p&gt;Part-of-Speech (POS) tagging is a grammatical annotation process where the role of each word is as best possible identified. This helps identify named entities, subjects, and so on.&lt;/p&gt;
    &lt;p&gt;Both of these processes needless to say require some awareness of the language being acted upon.&lt;/p&gt;
    &lt;p&gt;These ‚Äúimportant keywords‚Äù are used to assign documents to a special index that helps with recall by ensuring these documents are included in the set that is ranked before the execution timer runs out. This is not strictly necessary, and in some cases such as where POS-tagging is not possible, can be disabled, partially or as a whole.&lt;/p&gt;
    &lt;p&gt;The normalization step is subject to cultural differences that do not translate. In English you‚Äôd probably expect to find the metal band Tr√É¬∂jan, typing ‚Äútrojan‚Äù. In Swedish these are different letters entirely that should not match, the former means ‚Äúthe shirt‚Äù, the latter ‚Äútrojan‚Äù in the Homeric or IT-security sense. Though a Swedish person would likely also say that they should be able to find m√É¬º(e)sli with the keyword ‚Äúmusli‚Äù, but a German-speaker would disagree and say that u and √É¬º are clearly not the same.&lt;/p&gt;
    &lt;p&gt;There also exists a bootstrapping problem, as the statistical model used to calculate TF-IDF is based on documents in the index. Since almost all of the documents in the index up until this point have been in English, term frequencies for the newly added languages are missing. This breaks TF-IDF, as used in identifying important keywords, until a new model can be constructed. Thankfully the BM-25 model used in ranking is robust to this, as it relies on live data from the index itself.&lt;/p&gt;
    &lt;p&gt;The basic approach to parametrize language handling selected was to inject a language definition object, from which language appropriate logic is accessible.&lt;/p&gt;
    &lt;p&gt;This is configurable via XML. Here XML was chosen because it arguably has the best built-in validation support, making it a fantastic use case for a self-contained configuration file like this one, where late validation would be very annoying to deal with.&lt;/p&gt;
    &lt;p&gt;Much of the configuration file consists of various grammatical patterns used to identify important keywords based on the role of a word in a sentence.&lt;/p&gt;
    &lt;code&gt;&amp;lt;ngrams type="noun"&amp;gt;
    &amp;lt;pospattern&amp;gt;VBG&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;RB VBG&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NNP* JJ)&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NN* JJ) NN*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NN* JJ) (NN* JJ) NN*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NN* JJ) (NN* JJ) (NN* JJ) NN*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NNP* JJ) (NNP* IN TO CC) NNP*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NNP* JJ) (NNP* IN TO CC) DT NNP*&amp;lt;/pospattern&amp;gt;
    &amp;lt;pospattern&amp;gt;(NNP* JJ) (NNP* IN TO CC) (NNP* IN TO CC) NNP*&amp;lt;/pospattern&amp;gt;
&amp;lt;/ngrams&amp;gt;
&lt;/code&gt;
    &lt;p&gt;An expression like &lt;code&gt;(NN* JJ) (NN* JJ) NN*&lt;/code&gt; is interpreted as&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Any tag starting with &lt;code&gt;NN&lt;/code&gt;, or the tag&lt;code&gt;JJ&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Any tag starting with &lt;code&gt;NN&lt;/code&gt;, or the tag&lt;code&gt;JJ&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Any tag starting with &lt;code&gt;NN&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Previously these patterns were hard coded, and finding a performant alternative implementation took some effort. A bit mask approach was selected, as it allows for some very basic bit-level concurrency that drastically reduces the number of branches needed.&lt;/p&gt;
    &lt;p&gt;As far as grammatical analysis goes, the approach used by the search engine is pretty medieval, but it does do a fairly good job at what it sets out to do, and as a result, one thing it is generally pretty good at is finding websites about some topic.&lt;/p&gt;
    &lt;p&gt;In some ways the imperfections introduced by the old-fashioned way of approaching language processing is almost helpful in bringing in more relevant results, as they tend to capture more variations of the words related to the topic of the document.&lt;/p&gt;
    &lt;p&gt;There are more places that need minor language dependent behavior changes that are glossed over here, both in the language processing pipeline discussed above, and in the query parser, though in the interest of keeping this update from becoming an overly verbose git diff, these will be glossed over.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tooling&lt;/head&gt;
    &lt;p&gt;To help make sense of this, a test tool was built that runs the language processing pipeline in isolation, and outputs annotated intermediate results for human inspection.&lt;/p&gt;
    &lt;p&gt;Work in this domain poses special problems that all but demand human testing. Machine testing can be good for catching regressions or getting access to some code for easier debugging, but natural language has so many nuances that any test suite is woefully inadequate compared to a pair of human eyeballs.&lt;/p&gt;
    &lt;p&gt;It has already helped refine the algorithms used to identify important keywords in English, which wasn‚Äôt the intent of building the tool, but its immediate consequence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Integration&lt;/head&gt;
    &lt;p&gt;Integrating the new multi-language search data into the system poses some design considerations.&lt;/p&gt;
    &lt;p&gt;One option would be to stick everything in one big index, and then filter results based on language during or after ranking. The strength of this is that it becomes possible to search in any language without specifying it upfront.&lt;/p&gt;
    &lt;p&gt;The drawbacks of the one-index approach is that it grows the index, which makes all queries slower; it also grows the number of keywords in the lexicon, which is something that we generally want to avoid.&lt;/p&gt;
    &lt;p&gt;The way the search engine handles mapping keywords to numeric ids is to use a hash algorithm. Not a hash table, but the output of the hash algorithm itself. This seems absolutely unhinged at first glance, but works remarkably well as long as the lexicon stays small enough.&lt;/p&gt;
    &lt;p&gt;Hash collisions do happen on rare occasions, but they need to happen between words where the words actually appear in the same documents to be a problem, generally leading to the ranking algorithm having to trudge through irrelevant documents and performing worse as a result of wasting its time budget.&lt;/p&gt;
    &lt;p&gt;Massively expanding the lexicon like we would if we were to mingle the documents increases the likelihood there will be an actual problem arising from these rare false positives.&lt;/p&gt;
    &lt;p&gt;If we stick every keyword from every language in the same index, a different problem arises, namely that homophones exist across different languages, meaning that the index lookup needs to wade through irrelevant documents that are trivially unrelated to the query.&lt;/p&gt;
    &lt;p&gt;The words &lt;code&gt;salt&lt;/code&gt; and &lt;code&gt;lag&lt;/code&gt;, if they appear in the same document in English likely selects documents relating to esports, whereas in Swedish they select for documents relating to food preservation.&lt;/p&gt;
    &lt;p&gt;The alternative option is to separate the indexes.&lt;/p&gt;
    &lt;p&gt;The drawback here is that you must specify the language upfront, and querying in all languages becomes very expensive, as it executing multiple queries, though the desired language of the search results are generally known beforehand so this is a relatively small concern that, at best, affects a small number of machine-access use cases.&lt;/p&gt;
    &lt;p&gt;Since it has far fewer problems, and promises to be faster and more accurate, this approach was selected.&lt;/p&gt;
    &lt;p&gt;In practice this was implemented as language-specific keyword-document mappings, that point into a common file containing document lists.&lt;/p&gt;
    &lt;p&gt;Initially the indexes were constructed from a common journal file, which was consumed repeatedly, but this turned out to be slow, and a partitioned approach was selected instead, with one journal per language. This almost completely removes any overhead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outcome&lt;/head&gt;
    &lt;p&gt;The changes discussed above have been implemented, and upon evaluation seems to work reasonably well, though evaluation has somewhat run into a dead end, as the index itself is extremely small for the newly added languages.&lt;/p&gt;
    &lt;p&gt;The experience of small index is devious as it may just mean poor recall, though looking at the documents database for one index partition, this is about 12% of the index, it really is quite small!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;iso&lt;/cell&gt;
        &lt;cell role="head"&gt;document count&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;en&lt;/cell&gt;
        &lt;cell&gt;112,846,397&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;de&lt;/cell&gt;
        &lt;cell&gt;7,623,983&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fr&lt;/cell&gt;
        &lt;cell&gt;4,852,759&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;sv&lt;/cell&gt;
        &lt;cell&gt;1,020,962&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To verify this is not due some silent, catastrophic processing error, the proportions were compared against the number of documents found in the 50 GB document sample used in testing, using a simplified process that only does language identification.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;iso&lt;/cell&gt;
        &lt;cell role="head"&gt;document count&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;en&lt;/cell&gt;
        &lt;cell&gt;11,497,571&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;de&lt;/cell&gt;
        &lt;cell&gt;614,311&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fr&lt;/cell&gt;
        &lt;cell&gt;409,877&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;es&lt;/cell&gt;
        &lt;cell&gt;267,408&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ja&lt;/cell&gt;
        &lt;cell&gt;217,599&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nl&lt;/cell&gt;
        &lt;cell&gt;196,130&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&amp;amp;mldr;&lt;/cell&gt;
        &lt;cell&gt;&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;sv&lt;/cell&gt;
        &lt;cell&gt;67,670&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The proportions aren‚Äôt identical, but in the same general ballpark. The small size of the sample, along with the uneven distribution and apparent rarity of these documents adequately explains the disparity.&lt;/p&gt;
    &lt;p&gt;The lack of documents in languages other than English is likely due to how the index has been grown, by following and adding links from English websites. These occasionally lead to bilingual websites, and on rare occasions to websites completely in a different language, though it seems reasonable most websites that are not at least partially in English sees few or no links from English-language websites.&lt;/p&gt;
    &lt;p&gt;Adding to the problem, up until fairly recently the index wasn‚Äôt really growing very much at all, only through manual submissions.&lt;/p&gt;
    &lt;p&gt;Beyond a certain point, meaningfully growing the index by just following links became difficult.&lt;/p&gt;
    &lt;p&gt;Most known domains are dead, so merely adding more domains to the list of websites to crawl only serves to pollute the database with junk data.&lt;/p&gt;
    &lt;p&gt;In order to get around this, and reach the goal of indexing a billion documents, a new process was built to visit candidate websites to verify that they are in fact real and on-line, before assigning them to an index partition.&lt;/p&gt;
    &lt;p&gt;The process has been running for almost a quarter, and has managed to identify about 800,000 viable new domains in that time window. (This has brought the document total up to 969M documents. So very nearly there now!)&lt;/p&gt;
    &lt;p&gt;Web search is unusual in how often you run into these extremely long running processes that need to cook for months, sometimes up to a year before they really begin to pay off.&lt;/p&gt;
    &lt;p&gt;We‚Äôll have to see whether building this new process was so prescient it ends up being sufficient to identify and add new domains in more languages, as links from the newly processed Swedish, French and German websites have been added to the domain database, or if some sort of manual seeding or targeted selection process is needed.&lt;/p&gt;
    &lt;p&gt;It seems plausible it will at least begin to remedy the data starvation, as the rate of successful domain discovery has shot up significantly since processing links from the documents processed in the newly added languages, and many of the new domains are indeed from &lt;code&gt;.de&lt;/code&gt;, &lt;code&gt;.se&lt;/code&gt;, &lt;code&gt;.fr&lt;/code&gt;, and &lt;code&gt;.ch&lt;/code&gt; domains.&lt;/p&gt;
    &lt;p&gt;For now we‚Äôll have to wait and see how the data-set evolves. It is difficult to further refine the multi-language aspect of the search data with a data-set this small.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45653143</guid><pubDate>Tue, 21 Oct 2025 06:48:53 +0000</pubDate></item><item><title>Pasta/80 is a simple Pascal cross compiler targeting the Z80 microprocessor</title><link>https://github.com/pleumann/pasta80</link><description>&lt;doc fingerprint="a6f5f818a2971b8d"&gt;
  &lt;main&gt;
    &lt;p&gt;PASTA/80 is a simple Pascal cross compiler targeting the Z80 microprocessor. It generates code for these classic and modern machines:&lt;/p&gt;
    &lt;p&gt;The compiler follows the single-pass recursive-descent approach championed by Niklaus Wirth, inventor of Pascal, in his books and lectures. It doesn't have an explicit syntax tree, but instead generates code on the fly during parsing. As a result, the compiler might not always generate the most efficient code possible (it definitely cannot compete with LLVM and doesn't try to), but it's very fast.&lt;/p&gt;
    &lt;p&gt;The supported Pascal dialect is an almost exact clone of the original Turbo Pascal 3.0 for CP/M (see this manual for details). So you have at your disposal the following language elements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All the basic data types (&lt;code&gt;Boolean&lt;/code&gt;,&lt;code&gt;Byte&lt;/code&gt;,&lt;code&gt;Char&lt;/code&gt;,&lt;code&gt;Integer&lt;/code&gt;,&lt;code&gt;Pointer&lt;/code&gt;,&lt;code&gt;Real&lt;/code&gt;and&lt;code&gt;String&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;array of&lt;/code&gt;,&lt;code&gt;record&lt;/code&gt;,&lt;code&gt;set of&lt;/code&gt;, enumerations, subranges and pointers as a way of building new data types.&lt;/item&gt;
      &lt;item&gt;The decision-making elements &lt;code&gt;if..then..else&lt;/code&gt;and&lt;code&gt;case..of&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The loop elements &lt;code&gt;for..do&lt;/code&gt;,&lt;code&gt;while..do&lt;/code&gt;and&lt;code&gt;repeat..until&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;with..do&lt;/code&gt;notation for "opening" records.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;procedure&lt;/code&gt;and&lt;code&gt;function&lt;/code&gt;including value and&lt;code&gt;var&lt;/code&gt;parameters and nesting.&lt;/item&gt;
      &lt;item&gt;The standard procedures for screen input and output (i.e. &lt;code&gt;ReadLn&lt;/code&gt;,&lt;code&gt;WriteLn&lt;/code&gt;etc.).&lt;/item&gt;
      &lt;item&gt;All conversion and utility procedures and functions that Turbo Pascal 3.0 had.&lt;/item&gt;
      &lt;item&gt;The three kinds of disk files, that is untyped (&lt;code&gt;file&lt;/code&gt;), typed (&lt;code&gt;file of&lt;/code&gt;) and&lt;code&gt;Text&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A dynamic heap of up to 32767 bytes with &lt;code&gt;GetMem&lt;/code&gt;,&lt;code&gt;FreeMem&lt;/code&gt;,&lt;code&gt;New&lt;/code&gt;and&lt;code&gt;Dispose&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Inline assembly (via opcodes, not via mnemonics, so this page might be handy).&lt;/item&gt;
      &lt;item&gt;Overlays (in memory, Spectrum 128K and Next only, see below).&lt;/item&gt;
      &lt;item&gt;Some compiler directives: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;$i &amp;lt;file&amp;gt;&lt;/code&gt;for including Pascal source files (including nesting and cycle detection)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$l &amp;lt;file&amp;gt;&lt;/code&gt;for including an assembly file (aka "linking" a library)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$a(+/-)&lt;/code&gt;for enabling or disabling absolute mode (default is on, disable for recursion)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$i(+/-)&lt;/code&gt;for enabling or disabling IO checking (when off, check&lt;code&gt;IOResult&lt;/code&gt;after calls)&lt;/item&gt;&lt;item&gt;&lt;code&gt;$k(+/-)&lt;/code&gt;for enabling or disabling stack overflow checking&lt;/item&gt;&lt;item&gt;&lt;code&gt;$u(+/-)&lt;/code&gt;for enabling or disabling Ctrl-C checking&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The compiler also has some features that were borrowed from or inspired by later versions of Turbo Pascal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C-style &lt;code&gt;//&lt;/code&gt;one-line comments in addition to&lt;code&gt;{..}&lt;/code&gt;and&lt;code&gt;(*..*)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Binary literals (using a &lt;code&gt;%&lt;/code&gt;prefix).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Break&lt;/code&gt;and&lt;code&gt;Continue&lt;/code&gt;for loop control.&lt;/item&gt;
      &lt;item&gt;Querying the keyboard via &lt;code&gt;KeyPressed&lt;/code&gt;and&lt;code&gt;ReadKey&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Color support via &lt;code&gt;TextColor&lt;/code&gt;and&lt;code&gt;TextBackground&lt;/code&gt;with constants for the 8 Spectrum Next colors.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Inc&lt;/code&gt;and&lt;code&gt;Dec&lt;/code&gt;for more efficient increasing and decreasing of variables.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Include&lt;/code&gt;and&lt;code&gt;Exclude&lt;/code&gt;for more efficient handling of sets.&lt;/item&gt;
      &lt;item&gt;A simple &lt;code&gt;Assert&lt;/code&gt;facility that counts passes/fails and shows the failed line number.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since that covers most of the functionality of Turbo Pascal 3 you might ask what is missing. These are the current limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All the remaining compiler directives are not yet supported.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Mark&lt;/code&gt;/&lt;code&gt;Release&lt;/code&gt;are not currently supported.&lt;/item&gt;
      &lt;item&gt;The standard files &lt;code&gt;Input&lt;/code&gt;,&lt;code&gt;Output&lt;/code&gt;,&lt;code&gt;Kbd&lt;/code&gt;,&lt;code&gt;Con&lt;/code&gt;and&lt;code&gt;Lst&lt;/code&gt;are not supported.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Chain&lt;/code&gt;and&lt;code&gt;Execute&lt;/code&gt;are not supported.&lt;/item&gt;
      &lt;item&gt;Add-on libraries from the PC version of Turbo Pascal 3.0 are not yet supported (although there are a few graphics primitives for the ZX targets).&lt;/item&gt;
      &lt;item&gt;The new instructions of the Z80N CPU inside the ZX Spectrum Next are not yet being leveraged.&lt;/item&gt;
      &lt;item&gt;No separate compilation. Everything is compiled from source, always.&lt;/item&gt;
      &lt;item&gt;Binary size is quite large compared to the original.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The runtime library, being partially written in Pascal itself, gets quite large when compiled. I hope to bring this down again by reimplementing more of it in Z80 assembly (or improve the code generator, which, although it has a peephole optimizer, is not generating super-efficient Z80 code).&lt;/p&gt;
    &lt;p&gt;The compiler is itself written in Pascal. You can compile it with Free Pascal (I use version 3.2.2). Just run&lt;/p&gt;
    &lt;code&gt;$ fpc pasta&lt;/code&gt;
    &lt;p&gt;The Pascal compiler generates Z80 assembler code and relies on sjasmplus as a backend for the final translation step to binary. It can also, in &lt;code&gt;--ide&lt;/code&gt; mode (see below), make use of various other external tools. The compiler tries to detect these external tools automatically (from your system's &lt;code&gt;PATH&lt;/code&gt;), but sometimes it's best to create a file &lt;code&gt;.pasta80.cfg&lt;/code&gt; in your home directory specifying necessary paths (there is a sample in &lt;code&gt;misc&lt;/code&gt; that you can adapt).&lt;/p&gt;
    &lt;code&gt;# PASTA/80 config

HOME      = ~/Spectrum/pasta80
ASSEMBLER = ~/Spectrum/sjasmplus/sjasmplus
...
&lt;/code&gt;
    &lt;p&gt;You can check your whole setup by calling the compiler with &lt;code&gt;--config&lt;/code&gt;. It will show the full paths of all internal and external requirements and whether they are fulfilled.&lt;/p&gt;
    &lt;p&gt;To run the compiler just invoke the executable with the name of a Pascal source file to translate.&lt;/p&gt;
    &lt;p&gt;The default target is CP/M. There is an optional parameter that enables some simple peephole optimizations and another one that uses dependency analysis to eliminate unused Pascal procedures and functions:&lt;/p&gt;
    &lt;code&gt;$ pasta hello.pas             # Compiles hello.pas to hello.com
$ pasta hello                 # Source file .pas suffix is optional
$ pasta --opt hello.pas       # Enables peephole optimizations
$ pasta --opt --dep hello.pas # The same plus dependency analysis&lt;/code&gt;
    &lt;p&gt;You can run the resulting &lt;code&gt;.com&lt;/code&gt; files on a real CP/M machine or in a CP/M emulator. I recommend the excellent tnylpo. For programs that use VT52 control codes you have to start tnylpo in full-screen mode:&lt;/p&gt;
    &lt;code&gt;$ tnylpo hello                # Run in line-mode
$ tnylpo -s -t @ hello        # Monochrome full-screen, wait when finished
$ tnylpo -soy,4,0 -t @ hello  # Color full-screen, wait when finished&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;"Hello, World" in line mode&lt;/cell&gt;
        &lt;cell role="head"&gt;"Hello, World" in full-screen&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To generate binaries for the ZX Spectrum 48K, 128K and Next targets, use the &lt;code&gt;--zx48&lt;/code&gt;, &lt;code&gt;--zx128&lt;/code&gt; and &lt;code&gt;--zxnext&lt;/code&gt; parameters, respectively.&lt;/p&gt;
    &lt;code&gt;$ pasta --zx48 hello.pas      # Compiles for ZX Spectrum 48K
$ pasta --zx128 hello.pas     # Compiles for ZX Spectrum 48K
$ pasta --zxnext hello.pas    # Compiles for ZX Spectrum Next&lt;/code&gt;
    &lt;p&gt;The main difference between the three (currently) is that the ZX Spectrum Next target supports file IO (on the SD card), while the other two do not. The remaining routines are mostly the same. Screen output is handled via &lt;code&gt;rst $10&lt;/code&gt; in the ROM. In both cases the binaries are expected to be run from address 0x8000.&lt;/p&gt;
    &lt;p&gt;The default output format for the ZX Spectrum targets is a simple binary file that contains exactly the bytes of the compiled program (plus a +3DOS header when compiling for the Spectrum Next). In addition to that (and for more complex cases involving overlays), the compiler can also generate snapshot files or tape files, the latter including a suitable BASIC loader:&lt;/p&gt;
    &lt;code&gt;$ pasta --zx48 --sna examples/hello.pas   # .sna file
$ pasta --zx48 --tap examples/jacques.pas # .tap file with BASIC loader&lt;/code&gt;
    &lt;p&gt;Being self-contained, snapshots and tapes are a convenient way to distribute your programs and to launch them an emulator, such as Fuse:&lt;/p&gt;
    &lt;code&gt;$ open -a Fuse examples/hello.sna         # Launch .sna file in FUSE (on Mac)
$ open -a Fuse examples/jacques.tap       # Launch .tap file in FUSE (on Mac)&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Hello world in FUSE&lt;/cell&gt;
        &lt;cell role="head"&gt;Frere Jacques in FUSE (yes, with sound!)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When compiling for the Next, another useful format is a runnable directory. It contains exactly the same files that would also be in the .tap file, including a BASIC loader named &lt;code&gt;run.bas&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;$ pasta --zxnext --run examples/pq.pas    # Results in directory named pq.run&lt;/code&gt;
    &lt;p&gt;The directory has the suffix &lt;code&gt;.run&lt;/code&gt;. When attempting to enter such a directory in the Next's file browser, the loader is started automatically (press Symbol Shift + Enter to really see the contents). If you are a Mac user: Yes, it's a bit like an &lt;code&gt;.app&lt;/code&gt; bundle.&lt;/p&gt;
    &lt;p&gt;The Spectrum 128K and Next targets support overlays. This means you can have larger programs than would normally fit into the 64K address space of a Z80 machine. The rules are the same as for Turbo Pascal 3.0:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Overlays can be applied to global procedures and functions only, not to nested ones (though nested ones will be overlayed if the containing ones are, too).&lt;/item&gt;
      &lt;item&gt;Overlays cannot be applied to global variables, that is, you cannot use them for data (at least not without tricks).&lt;/item&gt;
      &lt;item&gt;All consecutive procedures and functions that are marked as &lt;code&gt;overlay&lt;/code&gt;go into the same overlay. Use any declaration inbetween to separate overlays.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the following example, there are three overlays: Overlay 0 contains A and B, overlay 1 contains D, and overlay 2 contains E.&lt;/p&gt;
    &lt;code&gt;overlay procedure A; (* Overlay 0 *)
begin
end;

overlay procedure B; (* Overlay 0 *)
begin
end;

procedure C; (* Not in an overlay *)
begin
end;

overlay procedure D; (* Overlay 1 *)
begin
end;

type
  Dummy = Integer;   (* Separator *)

overlay procedure E; (* Overlay 2 *)
begin
end;&lt;/code&gt;
    &lt;p&gt;In contrast to Turbo Pascal 3.0, overlays are not implemented via disk files. Instead, they use the additional RAM of the Spectrum 128K and Next machines. The uppermost 16K bank (Spectrum 128K) or 8K page (Spectrum Next) will be reserved for overlays. Each overlay can have a maximum size of 8K. The compiler manages everything and generates special "far calls" whenever necessary.&lt;/p&gt;
    &lt;p&gt;To enable overlays, use the &lt;code&gt;--ovr&lt;/code&gt; command line parameter, ideally in conjuncton with the &lt;code&gt;--tap&lt;/code&gt; parameter, as the tape loaders for 128K and Next are fully overlay-aware.&lt;/p&gt;
    &lt;code&gt;$ pasta --zx128 --tap --opt --dep --ovr tests/all.pas # Test suite as 128K tape&lt;/code&gt;
    &lt;p&gt;The compiler prints a report of which overlays go into which RAM banks or pages.&lt;/p&gt;
    &lt;code&gt;----------------------------------------
PASTA/80 Pascal System      Version 0.96
                            ZX 128K, Z80

Copyright (C) 2020-25 by  Joerg Pleumann
----------------------------------------

Compiling...
  tests/all.pas -&amp;gt; tests/all.z80
Assembling...
  tests/all.z80 -&amp;gt; tests/all.tap

Program   : 10781 bytes ($8000-$AA1C)
Heap      :  1507 bytes ($AA1D-$AFFF)
Stack     :  4096 bytes ($B000-$BFFF)

Overlay  0:  7399 bytes ($C000-$DCE6) in bank  0
Overlay  1:  7185 bytes ($E000-$FC10) in bank  0
Overlay  2:  2725 bytes ($C000-$CAA4) in bank  1
Overlay  3:  6293 bytes ($E000-$F894) in bank  1
Overlay  4:  6392 bytes ($C000-$D8F7) in bank  3
Overlay  5:  6527 bytes ($E000-$F97E) in bank  3
&lt;/code&gt;
    &lt;p&gt;Without the &lt;code&gt;--ovr&lt;/code&gt; parameter, overlay markers are simply ignored. This means you can use the same source code for platforms that do support overlays and for those that don't.&lt;/p&gt;
    &lt;p&gt;Caution: Overlays somewhat break the safety of the Pascal language. Be careful when using pointers or &lt;code&gt;var&lt;/code&gt; parameters for passing data between overlays. The memory you refer to may have just been paged out! It might make sense to compile your overlays with &lt;code&gt;{$a-}&lt;/code&gt;, so that all local variables are stored on the stack (which is always visible).&lt;/p&gt;
    &lt;p&gt;There is a folder containing &lt;code&gt;examples&lt;/code&gt; and a folder containing &lt;code&gt;tests&lt;/code&gt; for the compiler. The main test suite &lt;code&gt;all.pas&lt;/code&gt; needs to be compiled with &lt;code&gt;--opt --dep&lt;/code&gt; because of its size. Otherwise it won't fit into 64K. The Spectrum 128K and Next targets can (only) handle it using overlays, the Spectrum 48K target can't. Both the examples and the tests should give you a pretty good overview of what the compiler can do.&lt;/p&gt;
    &lt;p&gt;I also solved all puzzles of Advent of Code 2022 with an earlier version of the compiler and made YouTube videos of the solutions running on the ZX Spectrum Next, in CP/M mode.&lt;/p&gt;
    &lt;p&gt;As a fun little gimmick the compiler can be started like this&lt;/p&gt;
    &lt;code&gt;$ pasta --ide&lt;/code&gt;
    &lt;p&gt;to run it in an interactive mode that has an interface reminiscient of Turbo Pascal 3.0.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Main menu&lt;/cell&gt;
        &lt;cell role="head"&gt;Editor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When started in an ordinary terminal, this mode relies on the editor &lt;code&gt;nano&lt;/code&gt; being present on your system (on MacOS you might want to install the real &lt;code&gt;nano&lt;/code&gt; via a package manager because Apple sells you the much more limited &lt;code&gt;pico&lt;/code&gt; editor as &lt;code&gt;nano&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;You can also run it in a shell within Visual Studio Code, in which case it would automatically use VSC's editor (via the &lt;code&gt;code&lt;/code&gt; command, which, on a Mac, you might have to make available from VCS's settings) and act a bit like a plugin.&lt;/p&gt;
    &lt;p&gt;The following external tools are supported for running compiled programs on the host machine:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tnylpo for CP/M programs (press &amp;lt;R&amp;gt; for line mode, &amp;lt;Shift-R&amp;gt; for full-screen mode).&lt;/item&gt;
      &lt;item&gt;Fuse for programs targeting the ZX Spectrum 48K and 128K machines.&lt;/item&gt;
      &lt;item&gt;CSpect for ZX Spectrum Next programs. &lt;list rend="ul"&gt;&lt;item&gt;Please have hdfmonkey ready for manipulating the SD card image.&lt;/item&gt;&lt;item&gt;If you're on MacOS or Linux, you also need &lt;code&gt;mono&lt;/code&gt;because CSpect is a .NET application.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As mentioned before, everything that is in your &lt;code&gt;PATH&lt;/code&gt; should be detected automatically. There are some exceptions, though, so it makes sense to copy &lt;code&gt;misc/.pasta80.cfg&lt;/code&gt; to your home directory and adapt it. Use the &lt;code&gt;--config&lt;/code&gt; parameter to let PASTA/80 check your setup and get feedback on what is in place and what is missing.&lt;/p&gt;
    &lt;p&gt;The following screenshots show some applications compiled for the CP/M target and running in the &lt;code&gt;tnylpo&lt;/code&gt; emulator.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;2048&lt;/cell&gt;
        &lt;cell role="head"&gt;Game of Life&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Micro Calc&lt;/cell&gt;
        &lt;cell role="head"&gt;Galactic Empire&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These screenshots show some applications compiled for the ZX Spectrum 48K target and running in the FUSE emulator.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;2048&lt;/cell&gt;
        &lt;cell role="head"&gt;Game of Life&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Graphics Demo&lt;/cell&gt;
        &lt;cell role="head"&gt;Equation Solver&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;PASTA/80 Pascal Compiler&lt;/p&gt;
    &lt;p&gt;Copyright (c) 2020-2025 by J√∂rg Pleumann&lt;/p&gt;
    &lt;p&gt;The PASTA/80 compiler is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License (GPL) as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The runtime library (folder&lt;/p&gt;&lt;code&gt;rtl&lt;/code&gt;) comes with a linking exception that makes sure the GPL does not transfer to binaries created using PASTA/80.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The examples (folder&lt;/p&gt;&lt;code&gt;examples&lt;/code&gt;) are considered public domain or whatever comes closest to that in your jurisdiction.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Individual files or folders may use different licenses, so you might want to double check.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.&lt;/p&gt;
    &lt;p&gt;What does this mean for you?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You can use the compiler, free of charge, to build any application, open-source or prioprietary, free or paid, and distribute the generated binary without restriction. You can distribute binaries created with PASTA/80 under a license of your choosing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can modify the compiler according to your needs. If you distribute the compiler or parts of it, binary or source, modified or not, you have to comply with the rules laid out in the GPL (copyright info, source code, ...) unless the linking exception applies.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The math48 library is coypright (c) 1980 by Anders Hejlsberg, used by permission.&lt;/p&gt;
    &lt;p&gt;Some assembly routines adapted from Leventhal/Saville, "Z80 Assembly Subroutines", Osborne/McGraw-Hill 1983.&lt;/p&gt;
    &lt;p&gt;Turbo Pascal is a registered trademark of Code Gear LLC / Embarcadero.&lt;/p&gt;
    &lt;p&gt;Z80 is a registered trademark of Zilog, Inc.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45653330</guid><pubDate>Tue, 21 Oct 2025 07:23:09 +0000</pubDate></item><item><title>Tesla is heading into multi-billion-dollar iceberg of its own making</title><link>https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/</link><description>&lt;doc fingerprint="21ef1c15a9a3b3d0"&gt;
  &lt;main&gt;
    &lt;p&gt;Tesla‚Äôs ‚ÄòFull Self-Driving Supervised‚Äô expansion is back firing as it exposes its shortcomings. Customers left without promised features are growing discontent and demanding to be compensated.&lt;/p&gt;
    &lt;p&gt;It‚Äôs turning into a multi-billion-dollar iceberg of Tesla‚Äôs own making.&lt;/p&gt;
    &lt;p&gt;In 2016, Tesla proudly announced that all its vehicles produced onward are equipped with ‚Äúall the hardware for full self-driving,‚Äù which would be delivered through future software updates.&lt;/p&gt;
    &lt;p&gt;The automaker turned out to be significantly wrong about that.&lt;/p&gt;
    &lt;p&gt;At the time, it was producing its electric vehicles with a hardware suite known as HW2, which it had to upgrade to HW3 because it couldn‚Äôt support self-driving (FSD) capability.&lt;/p&gt;
    &lt;p&gt;HW3 was produced in vehicles from 2019 to 2023 and Tesla switched to HW4 in 2024.&lt;/p&gt;
    &lt;p&gt;At first, CEO Elon Musk claimed that FSD software updates on newer HW4 cars would lag roughly 6 months behind updates to HW3 cars to make sure to deliver the promised self-driving capability to those who have been waiting and paid for the promised capabiltiy a long time ago.&lt;/p&gt;
    &lt;p&gt;That strategy barely lasted a few months. Tesla quickly started releasing new FSD updates to HW4 cars first and it now hasn‚Äôt released a significant update to HW3 cars in close to a year.&lt;/p&gt;
    &lt;p&gt;Tesla only admitted in January 2025 that HW3 won‚Äôt be able to support unsupervised self-driving. Musk claimed that Tesla would retrofit the computers, but there has been no word about it for 10 months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tesla customers are starting to be fed up.&lt;/head&gt;
    &lt;p&gt;The catalyst is Tesla‚Äôs current FSD expansion in international markets. Previously, Tesla‚Äôs FSD was limited to North America, but over the last year, the automaker has been expanding FSD to China and now Australia and New Zealand.&lt;/p&gt;
    &lt;p&gt;However, the expansion is back-firing as HW3 owners are starting to realize that they will never get what they paid for.&lt;/p&gt;
    &lt;p&gt;In Australia and NZ, Tesla only launched FSD on HW4 vehicles with no clear plan for HW3, which the automaker already admitted won‚Äôt support unsupervised self-driving. The automaker appears to have only adapted its latest version of FSD for HW4 to the Australian market.&lt;/p&gt;
    &lt;p&gt;To add to the insult, with the launch of FSD in Australia, Tesla started to offer FSD subcriptions for $149 AUD a month for both HW3 and HW3 cars despite the software not being available for HW3.&lt;/p&gt;
    &lt;p&gt;HW3 owners reached out to Electrek after seeing this in their app:&lt;/p&gt;
    &lt;p&gt;It‚Äôs unclear why would Tesla sell a subcription to something that doesn‚Äôt even exist, but it is not helping build confidence with customers.&lt;/p&gt;
    &lt;p&gt;To try to appease owners, Tesla started sending emails to Australia HW3 owners offering $5,000 discounts on new inventory vehicles when transfering their FSD package:&lt;/p&gt;
    &lt;p&gt;However, this offer is misleading in itself, as it is not actually specific to HW3 owners as the email leads people to believe.&lt;/p&gt;
    &lt;p&gt;A visit on Tesla‚Äôs Australia inventory website shows that Tesla is offering a $5,000 disounct on all inventory vehicles with FSD for any buyer:&lt;/p&gt;
    &lt;p&gt;Therefore, it has nothing to do with ‚Äúloyalty‚Äù.&lt;/p&gt;
    &lt;p&gt;As we recently reported, thousands of Tesla owners have now joined a class action lawsuit in Australia over Tesla misleading customers with its self-driving promises.&lt;/p&gt;
    &lt;p&gt;It adds to similar ongoing lawsuits in the US and China.&lt;/p&gt;
    &lt;p&gt;With hundreds of thousands of FSD customers who paid up to $15,000 for package, Tesla is on the hook for billions of dollars in compensations or retrofits in the best-case scenario.&lt;/p&gt;
    &lt;head rend="h2"&gt;Electrek‚Äôs Take&lt;/head&gt;
    &lt;p&gt;We are seeing more people losing patience and it is only going to get worse.&lt;/p&gt;
    &lt;p&gt;There were a lot of interesting interactions on this post, which is pretty mild in my opinion. And yet, you see the usual Elon lemmings downplaying Tesla not delivering features it promised:&lt;/p&gt;
    &lt;p&gt;I don‚Äôt want to burst anyone‚Äôs bubble, but we need to be realistic here. If you are a HW3 owner and still think that Tesla is going to retrofit your up to 10-years-old car with a computer that is going to make self-driving, you are being delusional.&lt;/p&gt;
    &lt;p&gt;Tesla will have to end up compensating owners and at this point, I have serious doubts that it will do it by itself without being forced through courts.&lt;/p&gt;
    &lt;p&gt;Furthermore, it shouldn‚Äôt be just people who bought FSD. Tesla said that all cars had the hardware capable of self-driving whether people bought the software package or not. If that‚Äôs not true, it affects the resale value of the vehicle regardless of if someone purchased the package.&lt;/p&gt;
    &lt;p&gt;I have a fairly simple solution for Tesla to make it right.&lt;/p&gt;
    &lt;p&gt;Tesla needs to offer all HW3 owners a $5,000 loyalty discount, that goes on top of all other incentive program, when upgrading to a new car.&lt;/p&gt;
    &lt;head rend="h2"&gt;Top comment by Psychokisser&lt;/head&gt;
    &lt;p&gt;I think a lot of us on these sites have felt for years that FSD was little more than low-grade balloon juice. My view was always that Tesla and Elon were in for a world of self-created pain eventually, the only question being if "eventually" was a few months or a few decades away. It seems like we're getting closer to that reckoning.&lt;/p&gt;
    &lt;p&gt;As for HW3 owners who bought FSD, which basically turned out to be an interest free loan to Tesla for years, the automaker needs to offer free FSD transfer and a $10,000 discount on a car upgrade.&lt;/p&gt;
    &lt;p&gt;While this might sound like a lot, I think it‚Äôs in line with the incredible liability that Tesla is facing from all the on going lawsuits.&lt;/p&gt;
    &lt;p&gt;On top of it, it will go a long way to regain the trust of long-time customers, which Tesla swindled by selling them features it simply can‚Äôt deliver.&lt;/p&gt;
    &lt;p&gt;The main reason why I think Tesla doesn‚Äôt want to do that is that it will likely have to do the same thing to HW4 owners in the next few years and that would be the death of the company.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45654635</guid><pubDate>Tue, 21 Oct 2025 11:39:19 +0000</pubDate></item><item><title>StarGrid: A Brand-New Palm OS Strategy Game in 2025</title><link>https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html</link><description>&lt;doc fingerprint="efe6195dcbc22af0"&gt;
  &lt;main&gt;&lt;p&gt;This year my side project of choice was to create a brand new game for Palm OS, it started out as something that I thought I would finish in a month but ended up taking more than half a year in the end.&lt;/p&gt;&lt;p&gt;Let me present you with StarGrid, a space themed strategy game played on a hexagonal grid:&lt;/p&gt;&lt;p&gt;Your objective is to capture all the enemy flags or destroy all of your opponents.&lt;/p&gt;&lt;p&gt;No Palm OS device at hand? No problem, just play it on your browser thanks to the CloudPilot emulator.&lt;/p&gt;Game download and in-browser emulator&lt;p&gt;Allot of 'manual' labor went into this game, no premade game engine, no additional sdk's. Just making it from scratch, trying to solve one technical puzzle after another, but learning so many neat things along the way.&lt;/p&gt;&lt;p&gt;Coding for Palm certainly comes with it's own obstacles:&lt;/p&gt;&lt;p&gt;- Memory is tight so you need to take into account devices that can't even keep the playing field into memory, solution there was to hide the tiles when ships are moving.&lt;/p&gt;&lt;p&gt;- Maximum code size itself is also very limited, requiring you to segment your application into multiple individual parts. Detailed documentation on this was long gone, so I had to scrap some info together from developers that uploaded their 25 year old code to GitHub.&lt;/p&gt;&lt;p&gt;You can follow along the blog posts to see how I got here:&lt;/p&gt;StarGrid: A new game I'm making for Palm OS in 2025 Building the CPU Player for StarGrid Moving out of the vaporware phase - StarGrid's alpha release for PalmOS is here! StarGrid for Palm OS almost ready (and why do my side projects always explode in scope)&lt;p&gt;I won't immediately jump into the next big sideproject, I think I need a breather. I do however have some ideas lined up that I've been wanting to explore for a while now:&lt;/p&gt;&lt;p&gt;- making a top-down racing game (think micromachines)&lt;/p&gt;&lt;p&gt;- create an Outrun or Lotus III-like racing game&lt;/p&gt;&lt;p&gt;- building a ray-tracing game (like wolf3d).&lt;/p&gt;&lt;p&gt;Much more exciting stuff to come.&lt;/p&gt;&lt;p&gt;It's my way of keeping my favorite handheld operating system alive.&lt;/p&gt;&lt;p&gt;For now I hope at least some people will enjoy playing StarGrid and even if it's not their cup of tea, the game is fully open source, so I hope that can contribute to others making games and applications for this not-so-forgotten platform called Palm OS.&lt;/p&gt;StarGrid on GitHub&lt;p&gt;RetroGames, PalmOS, Development, StarGrid&lt;/p&gt;&lt;p&gt;You can get in touch through Mastodon:&lt;/p&gt;@rxpz@social.linux.pizza&lt;p&gt;StarGrid has arrived, a Brand-New Palm OS Strategy Game in 2025! was published on 2025-10-21&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45654660</guid><pubDate>Tue, 21 Oct 2025 11:42:14 +0000</pubDate></item></channel></rss>