<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 02 Dec 2025 16:14:47 +0000</lastBuildDate><item><title>After Windows Update, Password icon invisible, click where it used to be</title><link>https://support.microsoft.com/en-us/topic/august-29-2025-kb5064081-os-build-26100-5074-preview-3f9eb9e1-72ca-4b42-af97-39aace788d93</link><description>&lt;doc fingerprint="1416e8130a2670d4"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;August 29, 2025—KB5064081 (OS Build 26100.5074) Preview&lt;/head&gt;&lt;head class="contentToggle"&gt;Applies To&lt;/head&gt;&lt;p&gt;Release Date:&lt;/p&gt;&lt;p&gt;8/29/2025&lt;/p&gt;&lt;p&gt;Version:&lt;/p&gt;&lt;p&gt;OS Build 26100.5074&lt;/p&gt;&lt;p&gt;Windows Secure Boot certificate expirationWindows Secure Boot certificate expiration and CA updates.&lt;/p&gt;Important: Secure Boot certificates used by most Windows devices are set to expire starting in June 2026. This might affect the ability of certain personal and business devices to boot securely if not updated in time. To avoid disruption, we recommend reviewing the guidance and taking action to update certificates in advance. For details and preparation steps, see&lt;p&gt;To learn about Windows update terminology, see the pages on types of Windows updates and monthly quality update types. For an overview, see the update history page for Windows 11, version 24H2.&lt;/p&gt;&lt;p&gt;Follow @WindowsUpdate to find out when new content is published to the Windows release health dashboard.&lt;/p&gt;&lt;head rend="h2"&gt;Highlights&lt;/head&gt;&lt;p&gt;A gradual rollout distributes a release update over a period of time instead of all at once. This means that users receive the update at different times, and it might not be immediately available to all users.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;[Recall] New! Recall opens to a personalized homepage that puts your recent activity and top-used apps and websites front and center, making it easy to pick up where you left off. After turning on snapshot collection, the homepage highlights key productivity features like Recent Snapshots, which show the latest snapshots to help you quickly resume tasks, and Top Apps and Websites, which display the three apps and websites you’ve used most in the past 24 hours. You can set filters in Settings to control which apps and websites are saved in snapshots. A new navigation bar on the leftmost side of the screen provides quick access to Home, Timeline, Feedback, and Settings.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Click to Do] New! When you launch Click to Do for the first time, you'll see a quick interactive tutorial. It shows how to complete tasks faster by demonstrating actions on both text and images—such as summarizing large blocks of text or removing image backgrounds. To revisit the tutorial later, select More options &amp;gt; Start tutorial.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[General] New! When an app requests access to location, camera, microphone, or other device capabilities, Windows shows a redesigned system dialog box. To emphasize the privacy prompt, the screen dims slightly, and the prompt appears at the center of the screen.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Taskbar]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;New! The larger clock with seconds is now back in the notification center, displayed above the date and calendar. To turn this option on, go to Settings &amp;gt; Time &amp;amp; language &amp;gt; Date &amp;amp; time, and turn on Show time in the Notification Center.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Fixed: If you accidentally click and drag your mouse across the taskbar preview thumbnail, the preview might stop working.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Search on the Taskbar]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;New! When you use Search from the Windows taskbar, a new grid view will help you more quickly and accurately identify the desired image within your search.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;New! Search on the taskbar now provides clearer status information. If your search results are incomplete while your PC is organizing files in the background, Windows shows a notice with a link to check progress. You can dismiss the notice when you're done. There is also a status for files and folders, so you can easily tell whether they’re available online (cloud) or stored on your device.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Lock screen] New! More widget options and support for lock screen widget personalization (previously referred to as “Weather and more”) are rolling out. After initial launch with Windows Insiders in the European Economic Area (EEA), these updates are expanding to all regions. You can add, remove, and rearrange lock screen widgets such as Weather, Watchlist, Sports, Traffic, and more. Any widget that supports the small sizing option can be added. To customize your lock screen widgets, go to Settings &amp;gt; Personalization &amp;gt; Lock screen.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[File Explorer]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;New! Dividers now separate top-level icons in the File Explorer context menu.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;New! When you're signed in with a work or school account (Entra ID), File Explorer will display people icons in the Activity column and the Recommended section at the top of File Explorer Home. Hover over or select a person's icon to open their Microsoft 365 Live Persona Card, which shows who they are and how they're connected to the file.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Fixed: If you try to use the unblock open in Properties for a file, it still shows as blocked when you open Properties the next time.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Windows Hello]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;New! As part of the enhanced passkey features released in September 2023, you’ll see a redesigned Windows Hello interface. These modernized visual updates support fast, clear communication that appear across multiple authentication flows, including the Windows sign-in screen, passkey, Recall, the Microsoft Store, and more. The Windows security credential experience for passkey offers a cleaner, more intuitive interface designed to support fast, secure sign-in. You can now easily switch between authentication options such as passkeys or connected devices.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Fixed: Windows Hello might recognize your face on the login screen, however it would still fail and then prompt you to enter your pin. If you continue experiencing issues, you might need to go to the Facial Recognition section under Settings &amp;gt; Accounts &amp;gt;Sign-in options and select Improve recognition.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Improved: Fingerprint login after standby is now more robust.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Settings]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;New! Windows activation and expiration prompts match the Windows 11 design and appear as system notifications when action is required. There also have been improvements to messaging under Settings &amp;gt; System &amp;gt; Activation.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;New! You can go to Settings &amp;gt; Privacy &amp;amp; security &amp;gt; Text and Image Generation to see which third-party apps have recently used generative AI models provided by Windows. You can also choose which apps are permitted to use them—putting you in charge of your device’s AI experience.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;New! As part of the Copilot+ PC experience, the agent in Settings helps you quickly find and change settings. Initially available on Snapdragon®-powered Copilot+ PCs, agent in Settings now supports AMD- and Intel™-powered Copilot+ PCs. It currently works only when your primary display language is set to English.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Fixed: Settings might crash if you attempt to add a security key under Settings &amp;gt; Account &amp;gt; Sign-in options.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Task Manager] New! Task Manager now uses standard metrics to show CPU workload consistently across all pages, aligning with industry standards and third-party tools. If you prefer the previous view, you can enable a new optional column called CPU Utility in the Details tab to display the earlier CPU usage value shown on the Processes page.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Widgets]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;New! Multiple dashboards are now available in your Widgets Board. This gives you more space for your favorite widgets and helps you stay informed with a feed that connects you to current events. A new navigation bar on the left side makes it easy to switch between your widget’s dashboard and other views like the Discover feed. After initial launch in the EEA, these updates are expanding to all regions.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;New! A new visual experience is available for the Discover feed on the Widgets Board. The layout is more organized, personalized, and engaging. Copilot-curated stories are now included, offering a well-rounded view of each topic with summaries, videos, and images from trusted MSN premium publishers. To customize your feed, go to Widgets &amp;gt; Discover dashboard &amp;gt; Personalization settings.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Windows Backup for Organizations] New! Windows Backup for Organizations is now generally available! Experience seamless device transitions with enterprise-grade backup and restore. Whether you're refreshing your organization’s devices, upgrading to Windows 11, or deploying AI-powered PCs, this solution helps sustain productivity with minimal disruption, ensuring business continuity and organizational resilience.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[PowerShell 2.0] Starting in August 2025, Windows 11, version 24H2, will no longer include Windows PowerShell 2.0. This legacy component was introduced in Windows 7 and officially deprecated in 2017. Most users won’t be affected, as newer versions such as PowerShell 5.1 and PowerShell 7.x remain available and supported. If you use older scripts or tools that depend on PowerShell 2.0, update them to avoid compatibility issues.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Live captions] Fixed: Changing the opacity of live captions in Settings &amp;gt; Accessibility &amp;gt; Captions &amp;gt; Caption Style, has no effect.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Input]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Fixed: Attempting to type Chinese with an IME after copying something with CTRL + C can result in the first character not displaying.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Fixed: An underlying issue related to textinputframework.dll could result in certain apps like Sticky Notes and Notepad crashing.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[dbgcore.dll] Fixed: An underlying issue with dbgcore.dll could result in certain apps, including explorer.exe, crashing.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Kerberos] Fixed: There might be an underlying crash in Kerberos when attempting to access a cloud file share.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Login] Improved: Addressed some underlying cases which could lead to you seeing a blank white screen, or a screen saying, "just a moment", for a few minutes when logging into your PC.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Miracast] Fixed: An issue where, on certain devices, audio would initially play but stop a few seconds after casting to a TV.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Audio] Improved: Addressed an underlying audio service stops responding which could impact the ability to play audio in certain cases.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Cryptographic Provider (known issue)] Fixed: Fixed: This update addresses an issue where you might see an error in Windows Event Viewer with Error ID 57. The event displays the following message: The 'Microsoft Pluton Cryptographic Provider' provider was not loaded because initialization failed.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Improvements&lt;/head&gt;&lt;p&gt;This non-security update includes quality improvements. The following summary outlines key issues addressed by the KB update after you install it. Also, included are available new features. The bold text within the brackets indicates the item or area of the change.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;[Device management] Fixed: This update addresses an issue that prevented some system recovery features from working properly due to a temporary file sharing conflict. This affected certain device management tools and disrupted key functions on some devices.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[File system] Fixed: An issue in Resilient File System (ReFS) where using backup apps with large files could sometimes exhaust system memory.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Input]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Fixed: This update addresses an issue with the Chinese (Simplified) Input Method Editor (IME) where some extended characters appear as empty boxes.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Fixed This update addresses an issue that prevents typing on the touch keyboard when using the Microsoft Changjie, Microsoft Bopomofo, or Microsoft Japanese Input Method Editors (IMEs). The issue occurs after switching to a previous version of the IME.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Performance] Fixed: This update addresses an issue that slows application installation on ARM64 devices. Some installers might take longer to complete.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;[Print] To meet security goals and support new print capabilities, this update transitions Windows printing components from MSVCRT to a modern Universal C Runtime Library.&lt;/p&gt;&lt;p&gt;As a result of this change, print clients running versions of Windows prior to Windows 10, version 2004 and Windows Server, version 2004 (Build number 19041) will intentionally fail to print to remote print servers running Windows 11, versions 24H2 or 25H2, and Windows Server 2025, that have installed this update, or later updates. Attempting to print from an unsupported print client to an updated print server will fail with one of the following errors:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;The printer driver is not installed on this computer. Some printer properties will not be accessible unless you install the print driver.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Windows cannot connect to the printer.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;To work around this issue, either (1) upgrade your print client to Windows 10, version 22H2, or a newer version of Windows; or, (2) configure print clients released prior to Windows 10, version 22H2, to use pre-Windows Server 2025 print servers.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If you installed earlier updates, your device downloads and installs only the new updates contained in this package.&lt;/p&gt;&lt;head rend="h2"&gt;AI Components&lt;/head&gt;&lt;p&gt;This release updates the following AI components:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;p&gt;AI Component&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;Version&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;p&gt;Image Search&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;1.2508.906.0&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;p&gt;Content Extraction&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;1.2508.906.0&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;p&gt;Semantic Analysis&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;1.2508.906.0&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Settings Model&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;1.2508.906.0&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h3"&gt;Windows 11 servicing stack update (KB5064531)- 26100.5074&lt;/head&gt;&lt;p&gt;This update makes quality improvements to the servicing stack, which is the component that installs Windows updates. Servicing stack updates (SSU) ensure that you have a robust and reliable servicing stack so that your devices can receive and install Microsoft updates. To learn more about SSUs, see Simplifying on-premises deployment of servicing stack updates.&lt;/p&gt;&lt;head rend="h2"&gt;Known issues in this update&lt;/head&gt;&lt;p&gt;Symptoms&lt;/p&gt;&lt;p&gt;After installing the August 2025 Windows security update (KB5063878), you might experience delays or uneven audio and video performance when using Network Device Interface (NDI) to stream or transfer feeds between PCs.&lt;/p&gt;&lt;p&gt;This issue affects streaming apps such as OBS Studio (Open Broadcaster Software) and NDI Tools, especially when Display Capture is enabled on the source PC. The problem can even occur under low-bandwidth conditions.&lt;/p&gt;&lt;p&gt;Workaround&lt;/p&gt;&lt;p&gt;This issue is addressed in KB5065426.&lt;/p&gt;&lt;p&gt;Symptoms&lt;/p&gt;&lt;p&gt;A security improvement was included in the August 2025 Windows security update and later updates to enforce the requirement that User Account Control (UAC) prompt for administrator credentials when performing Windows Installer (MSI) repair and related operations. This improvement addressed security vulnerability CVE-2025-50173.&lt;/p&gt;&lt;p&gt;After installing the update, standard users might see a User Account Control (UAC) prompt in several scenarios.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Running MSI repair commands (such as msiexec /fu).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Opening Autodesk apps, including some versions of AutoCAD, Civil 3D and Inventor CAM, or when installing an MSI file after a user signs into the app for the first time.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Installing apps that configure per user.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Running Windows Installer during Active Setup.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Deploying packages through Manager Configuration Manager (ConfigMgr) that rely on user-specific "advertising" configurations.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Enabling Secure Desktop.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If a non-admin user runs an app that initiates an MSI repair operation without displaying UI, it will fail with an error message. For example, installing and running Office Professional Plus 2010 as a standard user will fail with Error 1730 during the configuration process.&lt;/p&gt;&lt;p&gt;Workaround&lt;/p&gt;&lt;p&gt;This issue is addressed in KB5065426.&lt;/p&gt;&lt;p&gt;Symptoms&lt;/p&gt;&lt;p&gt;Some Digital TV and Blu-ray/DVD apps might not play protected content as expected after installing the August 29, 2025, Windows non-security preview update (KB5064081), or later updates.&lt;/p&gt;&lt;p&gt;Apps that use Enhanced Video Renderer with HDCP enforcement or Digital Rights Management (DRM) for digital audio might show copyright protection errors, frequent playback interruptions, unexpected stops, or black screens.&lt;/p&gt;&lt;p&gt;Streaming services are not affected.&lt;/p&gt;&lt;p&gt;Workaround&lt;/p&gt;&lt;p&gt;The non-security September 2025 Windows preview update (KB5065789) and later updates address problems affecting certain applications that use the Enhanced Video Renderer (EVR) with HDCP (High-bandwidth Digital Content Protection) enforcement. The non-security October Windows preview update (KB5067036) includes additional improvements to address problems affecting applications using Digital Rights Media (DRM) for digital audio.&lt;/p&gt;&lt;p&gt;Symptoms&lt;/p&gt;&lt;p&gt;After installing the August 2025 non-security preview update (KB5064081) or later updates, you might notice that the password icon is not visible in the sign-in options on the lock screen. If you hover over the space where the icon should appear, you’ll see that the password button is still available. Select this placeholder to open the password text box and enter your password. After entering your password, you can sign in normally.&lt;/p&gt;&lt;p&gt;Workaround&lt;/p&gt;&lt;p&gt;Microsoft is working to resolve this issue and will provide information when it’s available.&lt;/p&gt;&lt;head rend="h2"&gt;How to get this update&lt;/head&gt;&lt;p&gt;Before you install this update&lt;/p&gt;&lt;p&gt;Microsoft combines the latest servicing stack update (SSU) for your operating system with the latest cumulative update (LCU). For general information about SSUs, see Servicing stack updates and Servicing Stack Updates (SSU): Frequently Asked Questions.&lt;/p&gt;&lt;p&gt;Install this update&lt;/p&gt;&lt;p&gt;To install this update, use one of the following Windows and Microsoft release channels.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;p&gt;Available&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;Next Step&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Open Start &amp;gt; Settings Update &amp;amp; Security &amp;gt; Windows Update. In the Optional updates available area, you will find the link to download and install available updates.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;p&gt;Available&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;Next Step&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;These changes will appear in the next security update to Windows Update for Business.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="6"&gt;&lt;cell&gt;&lt;p&gt;Available&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;Next Step&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Yes 1&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;Before you install this update&lt;/p&gt;&lt;p&gt;To get the standalone package(s) for this update, go to the Microsoft Update Catalog website. This KB contains one or more MSU files that require installation in a specific order.&lt;/p&gt;&lt;p&gt;Install this update&lt;/p&gt;&lt;p&gt;Method 1: Install all MSU files together&lt;/p&gt;&lt;p&gt;Download all MSU files for KB5064081 from Microsoft Update Catalog and place them in the same folder (for example, C:/Packages). Use Deployment Image Servicing and Management (DISM.exe) to install the target update. DISM will use the folder specified in PackagePath to discover and install one or more prerequisite MSU files as needed.&lt;/p&gt;&lt;p&gt;Updating Windows PC&lt;/p&gt;&lt;p&gt;To apply this update to a running Windows PC, run the following command from an elevated Command Prompt:&lt;/p&gt;&lt;p&gt;Or, run the following command from an elevated Windows PowerShell prompt:&lt;/p&gt;&lt;p&gt;Or use Windows Update Standalone Installer to install the target update.&lt;/p&gt;&lt;p&gt;Updating Windows Installation media&lt;/p&gt;&lt;p&gt;To apply this update to Windows Installation media, see Update Windows installation media with Dynamic Update.&lt;/p&gt;&lt;p&gt;Note: When downloading other Dynamic Update packages, ensure they match the same month as this KB. If the SafeOS Dynamic Update or Setup Dynamic Update is not available for the same month as this KB, use the most recently published version of each.&lt;/p&gt;&lt;p&gt;To add this update to a mounted image, run the following command from an elevated Command Prompt:&lt;/p&gt;&lt;p&gt;Or, run the following command from an elevated Windows PowerShell prompt:&lt;/p&gt;&lt;p&gt;Method 2: Install each MSU file individually, in order&lt;/p&gt;&lt;p&gt;Download and install each MSU file individually either using DISM or Windows Update Standalone Installer in the following order:&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;1 This latest cumulative update includes updates for AI components. Even though the AI component updates are included in the update, the AI components are only applicable to Windows Copilot+ PCs and will not install on Windows PC or Windows Server.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;p&gt;Available&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;Next Step&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;p/&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;You can import this update into Windows Server Update Services (WSUS) manually. See the Microsoft Update Catalog for instructions.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;If you want to remove the LCU&lt;/p&gt;&lt;p&gt;To remove the LCU after installing the combined SSU and LCU package, use the DISM/Remove-Package command line option with the LCU package name as the argument. You can find the package name by using this command: DISM /online /get-packages.&lt;/p&gt;&lt;p&gt;Running Windows Update Standalone Installer (wusa.exe) with the /uninstall switch on the combined package will not work because the combined package contains the SSU. You cannot remove the SSU from the system after installation.&lt;/p&gt;&lt;p&gt;File information&lt;/p&gt;&lt;p&gt;For a list of the files provided in this update, download the file information for cumulative update 5064081.&lt;/p&gt;&lt;p&gt;For a list of the files provided in the servicing stack update, download the file information for the SSU (KB5064531) - version 26100.5074.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46116567</guid><pubDate>Tue, 02 Dec 2025 02:12:14 +0000</pubDate></item><item><title>Reverse math shows why hard problems are hard</title><link>https://www.quantamagazine.org/reverse-mathematics-illuminates-why-hard-problems-are-hard-20251201/</link><description>&lt;doc fingerprint="b53048ad00258ef5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;‘Reverse Mathematics’ Illuminates Why Hard Problems Are Hard&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;When it comes to hard problems, computer scientists seem to be stuck. Consider, for example, the notorious problem of finding the shortest round-trip route that passes through every city on a map exactly once. All known methods for solving this “traveling salesperson problem” are painfully slow on maps with many cities, and researchers suspect there’s no way to do better. But nobody knows how to prove it.&lt;/p&gt;
    &lt;p&gt;For over 50 years, researchers in the field of computational complexity theory have sought to turn intuitive statements like “the traveling salesperson problem is hard” into ironclad mathematical theorems, without much success. Increasingly, they’re also seeking rigorous answers to a related and more nebulous question: Why haven’t their proofs succeeded?&lt;/p&gt;
    &lt;p&gt;This work, which treats the process of mathematical proof as an object of mathematical analysis, is part of a famously intimidating field called metamathematics. Metamathematicians often scrutinize the basic assumptions, or axioms, that serve as the starting points for all proofs. They change the axioms they start with, then explore how the changes affect which theorems they can prove. When researchers use metamathematics to study complexity theory, they try to map out what different sets of axioms can and can’t prove about computational difficulty. Doing so, they hope, will help them understand why they’ve come up short in their efforts to prove that problems are hard.&lt;/p&gt;
    &lt;p&gt;In a paper published last year, three researchers took a new approach to this challenge. They inverted the formula that mathematicians have used for millennia: Instead of starting with a standard set of axioms and proving a theorem, they swapped in a theorem for one of the axioms and then proved that axiom. They used this approach, called reverse mathematics, to prove that many distinct theorems in complexity theory are actually exactly equivalent.&lt;/p&gt;
    &lt;p&gt;“I was surprised that they were able to get this much done,” said Marco Carmosino, a complexity theorist at IBM. “People are going to look at this and they’re going to say, ‘This is what got me into metamathematics.’”&lt;/p&gt;
    &lt;head rend="h2"&gt;Pigeon Proofs&lt;/head&gt;
    &lt;p&gt;The story of the reverse-mathematics paper began in the summer of 2022, when Lijie Chen, a complexity theorist now at the University of California, Berkeley, was wrapping up his doctorate. He found himself with a lot of extra time on his hands and decided to devote a few months to reading up on metamathematics.&lt;/p&gt;
    &lt;p&gt;“Because I was graduating, I didn’t have much research to do,” Chen said. “I was figuring I should learn something new.”&lt;/p&gt;
    &lt;p&gt;As he read, Chen began thinking about a branch of complexity theory called communication complexity, which studies the information two or more people must exchange to accomplish certain tasks. One of the simplest problems in communication complexity, called the “equality problem,” is like a collaborative game. Two players start with separate strings of 0s and 1s (or bits). Their goal is to use as little communication as possible to determine whether their strings are the same. The simplest strategy is for one player to just send their full string for the other to check. Is there any way to do better?&lt;/p&gt;
    &lt;p&gt;Complexity theorists proved decades ago that the answer is no. To solve the equality problem, the players need to send, at a minimum, a number of bits equal to the number in the full string. Theorists say that this string length is a “lower bound” on the amount of communication needed.&lt;/p&gt;
    &lt;p&gt;Chen wasn’t focused on the equality problem’s lower bound itself — he was interested in how researchers had proved it. All known proofs depend on a simple theorem called the pigeonhole principle, which states that if you put some number of pigeons into a smaller number of holes, at least one hole must end up holding more than one bird. That may sound self-evident, but it can be a surprisingly powerful tool in complexity theory and beyond.&lt;/p&gt;
    &lt;p&gt;Chen had hit upon a tantalizing hint that the link between the equality problem and the pigeonhole principle might also go the other way. It’s easy to use the pigeonhole principle to prove the equality problem’s lower bound. Could you instead use the lower bound to prove the pigeonhole principle?&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncanny Equality&lt;/head&gt;
    &lt;p&gt;Chen discussed his idea with Jiatu Li, at the time an undergraduate at Tsinghua University with whom Chen had recently collaborated on another paper. To make the connection rigorous, they would have to choose a set of axioms to work with. Metamathematics researchers prefer to use axioms that are more restricted than the typical ones. These weaker axioms make it easier to pin down the precise relationships between different theorems. Chen and Li decided to work with a popular set of axioms called PV1. PV1 is strong enough to prove some important theorems about computational complexity on its own. Add a specific version of the pigeonhole principle as an extra axiom, and you can also prove the equality problem’s lower bound. In December 2022, Li and Chen formally showed that, as Chen had suspected, the proof also works with the two theorems interchanged.&lt;/p&gt;
    &lt;p&gt;The fact that you can prove the equality problem’s lower bound from the pigeonhole principle or vice versa implies that within the logical framework of PV1, the two theorems are exactly equivalent. When Li and Chen discussed the result with Igor Oliveira, a complexity theorist at the University of Warwick, the trio realized that their reverse-mathematics method might also work for theorems in other far-flung areas of complexity theory. Over the following months, they systematically proved equivalences for many other theorems.&lt;/p&gt;
    &lt;p&gt;“At the beginning, we only had two equivalent things,” Chen said. “But now we have a big web of stuff.”&lt;/p&gt;
    &lt;p&gt;The team’s most striking connection related the same version of the pigeonhole principle to one of the first theorems that students encounter in introductory complexity theory courses. This “classic banger of a theorem,” as Carmosino described it, sets a lower bound on the amount of time required for a type of theoretical computer called a single-tape Turing machine to determine whether a string of 0s and 1s is a palindrome (that is, whether it reads the same forward and backward). Li, Chen and Oliveira used reverse mathematics to prove that within PV1, this palindrome lower-bound theorem is equivalent to the pigeonhole principle.&lt;/p&gt;
    &lt;p&gt;“If you told me this, I wouldn’t believe it,” Chen said. “It sounds very ridiculous.”&lt;/p&gt;
    &lt;p&gt;The equivalence between the palindrome lower bound and the pigeonhole principle is surprising because the two theorems are so superficially different. The pigeonhole principle doesn’t inherently have anything to do with computation: It’s a simple statement about counting. The palindrome lower bound, on the other hand, is a statement about a specific model of computation. The new result implies that such seemingly narrow theorems are more general than they appear.&lt;/p&gt;
    &lt;p&gt;“It suggests that these complexity lower bounds we want to understand are much more fundamental,” Oliveira said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncharted Territory&lt;/head&gt;
    &lt;p&gt;This new web of equivalences has also helped illuminate the limits of PV1. Researchers already had reason to believe that the pigeonhole principle can’t be proved from the axioms of PV1 alone, so Li, Chen and Oliveira’s results imply that their other equivalent theorems are also likely unprovable in PV1.&lt;/p&gt;
    &lt;p&gt;“I think it’s beautiful,” said Ján Pich, a complexity theorist at Oxford University who proved a big result about the power of PV1 in 2014. But he cautioned that the reverse mathematics approach may be most useful for revealing new connections between theorems that researchers have already proved. “It doesn’t tell us much, as far as we can say, about the complexity of statements which we do not know how to prove.”&lt;/p&gt;
    &lt;p&gt;Understanding this uncharted territory remains a distant goal for metamathematics researchers. But that hasn’t tempered Li’s enthusiasm for the subject. He started graduate school at the Massachusetts Institute of Technology in 2023, and he recently wrote a 140-page guide to metamathematics for complexity theorists. It’s one example of a broader trend: After decades of relative obscurity, metamathematics is increasingly attracting attention from a wider community of researchers who bring new perspectives to the field.&lt;/p&gt;
    &lt;p&gt;“People are tired of being stuck,” Carmosino said. “It’s time to just step back and work out the foundation.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46116724</guid><pubDate>Tue, 02 Dec 2025 02:35:47 +0000</pubDate></item><item><title>What will enter the public domain in 2026?</title><link>https://publicdomainreview.org/features/entering-the-public-domain/2026/</link><description>&lt;doc fingerprint="e28955d9ed431297"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What Will Enter the Public Domain in 2026?A Festive Countdown&lt;/head&gt;
    &lt;p&gt;At the start of each year, on January 1st, a new crop of works enter the public domain and become free to enjoy, share, and reuse for any purpose. Due to differing copyright laws around the world, there is no one single public domain — and here we focus on three of the most prominent. Newly entering the public domain in 2026 will be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;works by people who died in 1955, for countries with a copyright term of “life plus 70 years” (e.g. UK, Russia, most of EU and South America);&lt;/item&gt;
      &lt;item&gt;works by people who died in 1975, for countries with a term of “life plus 50 years” (e.g. New Zealand, and most of Africa and Asia);&lt;/item&gt;
      &lt;item&gt;films and books (incl. artworks featured) published in 1930 for the United States.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In our advent-style calendar below, find our top pick of what lies in store for 2026. Each day, as we move through December, we’ll open a new window to reveal our highlights! By public domain day on January 1st they will all be unveiled — look out for a special blogpost from us on that day. (And, of course, if you want to dive straight in and explore the vast swathe of new entrants for yourself, just visit the links above).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check out John Mark Ockerbloom’s own “Public Domain Day Countdown” on Mastodon, and summarised in his blogpost.&lt;/item&gt;
      &lt;item&gt;See the selection from Standard eBooks of works entering the US in 2026, all of which they've made available to read for free.&lt;/item&gt;
      &lt;item&gt;Read more about what makes the public domain so important in Communia’s Public Domain Manifesto.&lt;/item&gt;
      &lt;item&gt;Wondering if “bad things happen to works when they enter the public domain”? Wonder no more.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46117112</guid><pubDate>Tue, 02 Dec 2025 03:23:10 +0000</pubDate></item><item><title>Apple Releases Open Weights Video Model</title><link>https://starflow-v.github.io</link><description>&lt;doc fingerprint="7a02bdcf136904a2"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;STARFlow-V is the first normalizing flow-based causal video generator demonstrating that normalizing flows can match video diffusion models in visual quality while offering end-to-end training, exact likelihood estimation, and native multi-task support across T2V/I2V/V2V generation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Method Pipeline&lt;/head&gt;
    &lt;p&gt;Figure: STARFlow-V pipeline. The model processes text prompts and noise through a Deep Autoregressive Block (global temporal reasoning) to produce intermediate latents, which are then refined by Shallow Flow Blocks (local within-frame details). A Learnable Causal Denoiser (trained via Flow-Score Matching) cleans the output. The model is trained end-to-end with two objectives: Maximum Likelihood for the flow and Flow-Score Matching for the denoiser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Contributions&lt;/head&gt;
    &lt;head rend="h4"&gt;Global-Local Architecture for Causal Video Modeling&lt;/head&gt;
    &lt;p&gt;A novel two-level architecture that separates global temporal reasoning from local within-frame details. A deep causal Transformer block processes the video autoregressively in compressed latent space to capture long-range spatiotemporal dependencies, while shallow flow blocks operate independently on each frame to model rich local structures. This design mitigates compounding errors common in pixel-space autoregressive models.&lt;/p&gt;
    &lt;head rend="h4"&gt;Flow-Score Matching Denoising&lt;/head&gt;
    &lt;p&gt;A unified training framework that combines normalizing flow maximum likelihood with flow-score matching for denoising. Instead of using imperfect or non-causal denoisers, we train a lightweight causal neural denoiser alongside the main flow model. This denoiser learns to predict the score (gradient of log-probability) of the model's own distribution, enabling high-quality single-step refinement while preserving causality.&lt;/p&gt;
    &lt;head rend="h4"&gt;Video-Aware Jacobi Iteration&lt;/head&gt;
    &lt;p&gt;Generation (flow inversion) is recast as solving a nonlinear system, enabling block-wise parallel updates of multiple latents simultaneously instead of one-by-one generation. Combined with video-aware initialization that uses temporal information from adjacent frames and pipelined execution between deep and shallow blocks, this achieves significant speedup while maintaining generation quality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Model Details&lt;/head&gt;
    &lt;p&gt;STARFlow-V is trained on 70M text-video pairs and 400M text-image pairs, with a final 7B parameter model that can generate 480p video at 16fps. The model operates in a compressed latent space and leverages the invertible nature of normalizing flows to natively support multiple generation tasks without any architectural changes or retraining.&lt;/p&gt;
    &lt;head rend="h3"&gt;Explore the Results&lt;/head&gt;
    &lt;p&gt;Navigate through the tabs above to see our model's capabilities across different generation tasks. Each category demonstrates specific aspects of STARFlow-V, from standard text-to-video generation to long-form video creation and comparisons with diffusion-based baselines.&lt;/p&gt;
    &lt;head rend="h3"&gt;BibTeX&lt;/head&gt;
    &lt;p&gt;If you find STARFlow-V useful in your research, please consider citing our work:&lt;/p&gt;
    &lt;quote&gt;@article{gu2025starflowv, title={STARFlow-V: End-to-End Video Generative Modeling with Scalable Normalizing Flows}, author={Gu, Jiatao and Shen, Ying and Chen, Tianrong and Dinh, Laurent and Wang, Yuyang and Bautista, Miguel \'Angel and Berthelot, David and Susskind, Josh and Zhai, Shuangfei}, journal={arXiv preprint arXiv:XXXX.XXXXX}, year={2025} }&lt;/quote&gt;
    &lt;head rend="h2"&gt;Text-to-Video Generation&lt;/head&gt;
    &lt;p&gt;Our model generates high-quality videos directly from text descriptions.&lt;/p&gt;
    &lt;head rend="h3"&gt;"a border collie balancing on a fallen log over a shallow stream; locked-off shot with gentle world motion; natural lighting"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a campfire crackling with embers lifting; static shot; night warmth, ultra-realistic, 4K 2"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a cassowary stepping through rainforest shade; locked-off telephoto with soft bokeh; golden-hour warmth, ultra-realistic, 4K."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a chameleon rolling its eyes in different directions; handheld with minimal sway; overcast soft light, ultra-realistic, 4K; soft"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a chef tossing vegetables in a pan; medium shot; stovetop glow, ultra-realistic, 4K."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a chipmunk stuffing seeds into full cheeks; locked-off shot with gentle world motion; blue-hour ambience, ultra-realistic, 4K; l"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a colorful nebula drifting with subtle motion; locked-off shot with gentle world motion; natural lighting, ultra-realistic, 4K;"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a corgi wearing neon-pink sunglasses on a sunlit pier; drone orbit with steady altitude hold; light film grain for realism; gold"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a giant panda nibbling a bamboo shoot; cinematic handheld at eye level; natural lighting, ultra-realistic, 4K."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a heron stepping carefully in marsh shallows; handheld with minimal sway; overcast soft light, ultra-realistic, 4K; soft depth o"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a humanoid robot practicing slow tai chi in a plaza; handheld with minimal sway; blue-hour ambience, ultra-realistic, 4K; occasi"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a kettle venting steam on a stove; static composition with foreground elements drifting; light film grain for realism; window li"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a penguin waddling across wet rocks; gentle push-in from a stable tripod; overcast soft light, ultra-realistic, 4K; soft depth o"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a potter shaping clay on a spinning wheel; low-angle tilt up revealing the scene; occasional lens flare at frame edge; clean stu"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a puffin turning its head with a beak full of fish; gentle push-in from a stable tripod; natural lighting, ultra-realistic, 4K;"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a rooftop garden swaying in wind; smooth dolly-in along ground-level sliders; soft depth of field and creamy bokeh; candlelit gl"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a sailboat drifting on calm water; wide shot; hazy sunlight, ultra-realistic, 4K."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a sheep flock drifting across a grassy hillside; locked-off shot with gentle world motion; golden-hour warmth, ultra-realistic,"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a skier floating through fresh powder; slow gimbal push-in with subtle handheld micro-shake; light film grain for realism; misty"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a small service robot trundling down a neon alley; handheld with minimal sway; blue-hour ambience, ultra-realistic, 4K; natural"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a snail extending its eyestalks after a light mist; gentle push-in from a stable tripod; blue-hour ambience, ultra-realistic, 4K"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a starfish gripping a tidepool rock as water swirls; gentle push-in from a stable tripod; natural lighting, ultra-realistic, 4K;"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a tram sliding past in light rain; handheld follow with natural breathing sway; a faint fingerprint smudge catching light; harsh"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a zebra flicking its tail in warm savanna light; slow pan across the scene; golden-hour warmth, ultra-realistic, 4K; light film"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"aerial shot flying low over rolling sand dunes patterned by the wind."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"an ostrich scanning an open plain; slow gimbal push-in; overcast soft light; ultra-realistic, 4K."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"carbonation rising in a glass of seltzer; shallow parallax orbit at chest height; tiny focus breathing during rack focus; golden"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"cherry blossoms falling along a riverside path; locked-off shot with gentle world motion; natural lighting, ultra-realistic, 4K;"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"close-up shot of a wind chime gently moving and ringing in a light breeze."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"drone shot flying low over a lavender field with rows converging to the horizon."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" forward dolly shot through a narrow alley full of hanging lanterns and street food stalls."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"lavender swaying with bees passing through; gentle push-in from a stable tripod; overcast soft light, ultra-realistic, 4K; soft"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" macro shot of a ladybug crawling along the edge of a green leaf."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" macro shot of ink swirling and mixing in a glass of water against a white background."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" macro shot of raindrops rippling on a calm pond with concentric circles overlapping."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"paper lanterns bobbing in a night festival; over-the-shoulder follow maintaining subject center; soft depth of field and creamy"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" shot of a drone circling a small island surrounded by clear blue water."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" shot of a drone flying over a patch of colorful autumn forest."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" shot of a snow globe being shaken, flakes swirling around a tiny village."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"steam rising from a cup of tea by a window; locked-off shot; soft morning light, ultra-realistic, 4K. 2"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" timelapse of stars streaking across the night sky above a desert landscape."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" underwater shot of koi fish gliding past colorful pebbles in a clear pond."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;" wide shot of waves crashing dramatically against black volcanic rocks at the coast."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"wisteria clusters swinging under a pergola; locked-off shot with gentle world motion; natural lighting, ultra-realistic, 4K; lig"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h2"&gt;Image-to-Video Generation&lt;/head&gt;
    &lt;p&gt;Generate videos from input images while maintaining temporal consistency. Due to the autoregressive nature of our model, we don't need to change the architecture at all—one model handles all tasks seamlessly.&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;Input Image&lt;/p&gt;
    &lt;p&gt;Generated Video&lt;/p&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h2"&gt;Video-to-Video Generation&lt;/head&gt;
    &lt;p&gt;Our model can extend and transform existing videos while maintaining temporal consistency. Due to the autoregressive nature of our model, we don't need to change the architecture at all—one model handles all tasks seamlessly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Add_hand&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Add_horse&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Convert_orange_into_lemon&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Turn_blackberries_into_red_currant&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Detect_sheep&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Detect_book&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Detect_depth&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Detect_hand&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Detect_magnolia_tree&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Inpaint&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Inpaint&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Inpaint&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_flowers_Electric_Blue&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_it_abstract_Bauhaus_style&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_it_concept_art_style&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_it_doodle_style&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_it_gothic_gloomy&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_it_traditional_Chinese_ink_painting_style&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_the_beach_golden_sandy&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_the_jellyfish_maroon_color&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_the_train_metallic_silver_and_rusty&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Make_the_vase_golden&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Outpaint&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Outpaint.&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Outpaint.&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Outpaint.&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h3"&gt;Outpaint.&lt;/head&gt;
    &lt;p&gt;384p • 16fps • 2s&lt;/p&gt;
    &lt;head rend="h2"&gt;Long Video Generation&lt;/head&gt;
    &lt;p&gt;Extended video generation (10s, 15s, 30s) using autoregressive segment-by-segment generation. The tail of each 5s segment is re-encoded as the prefix for the next segment, leveraging the invertibility of normalizing flows.&lt;/p&gt;
    &lt;head rend="h3"&gt;"a black ink drop blooming through clear water in a tumbler; static macro with minimal parallax; tendrils feathering out in slow"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a corgi dog wearing a tie sat by a window"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a corgi dozing in a sunbeam on hardwood floor; slow dolly-in at ankle height; dust motes drifting in the light shaft, shallow de"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a corgi sticking its head out of a car window; tracking from mirror level, horizon bob from suspension; fur whipping in the wind"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a dim street lit only by vending machines; slow dolly-forward at waist height; saturated glow halos, tiny insects swarming in li"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a street waffle being dusted with powdered sugar; tight close-up from plate level; sugar creating tiny puffs on impact, some gra"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"fall leaves spiraling down in a courtyard; upward-looking locked-off shot; branches framing sky, occasional leaf grazing lens; l"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"school of koi swirling just below pond surface; top-down gimbal drift; occasional surface glare flare, ripples distorting bodies"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"subway doors closing on a busy platform; low-angle from floor level; rolling shutter wobble as train accelerates, reflections sl"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 10s&lt;/p&gt;
    &lt;head rend="h3"&gt;"zoom-in corgi face"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 13s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a corgi dog sits in front of a blackboard teaching"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 15s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a corgi dog wearing a tie sitting in front of a blackboard"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 15s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a golden doodle tilting its head at a squeaky toy"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 30s&lt;/p&gt;
    &lt;head rend="h3"&gt;"paper lanterns bobbing in a night festival; over-the-shoulder follow maintaining subject center; soft depth of field and creamy"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 30s&lt;/p&gt;
    &lt;head rend="h3"&gt;"POV from the boat deck looking at a corgi wearing neon-pink sunglasses; wind noise feel, slight horizon bob, water droplets on l"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 30s&lt;/p&gt;
    &lt;head rend="h3"&gt;"This close-up shot of a Victoria crowned pigeon"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 30s&lt;/p&gt;
    &lt;head rend="h2"&gt;Method Comparisons&lt;/head&gt;
    &lt;p&gt;Side-by-side comparisons with baseline Autoregressive diffusion models. All prompts are sampled from VBench (Huang, 2023). Each video shows three methods from left to right: NOVA (https://github.com/baaivision/NOVA), WAN-Causal (finetuned from WAN provided by https://huggingface.co/gdhe17/Self-Forcing/blob/main/checkpoints/ode_init.pt), and STARFlow-V (Ours).&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"A panda drinking coffee in a cafe in Paris, in cyberpunk style"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"A person is playing piano"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"A person is tasting beer"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"a backpack and an umbrella"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"A 3D model of a 1800s victorian house."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"A corgi's head depicted as an explosion of a nebula"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 4s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"A cute happy Corgi playing in park, sunset"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"A shark swimming in clear Caribbean ocean"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"a bird"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"a drone flying over a snowy forest."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 6s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"arch"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"cliff"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"a person drinking coffee in a cafe"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"In a still frame, a stop sign"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"A boat sailing leisurely along the Seine River with the Eiffel Tower in background, in super slow motion"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 3s&lt;/p&gt;
    &lt;p&gt;(top) WAN-Causal&lt;/p&gt;
    &lt;p&gt;(mid) STARFlow-V&lt;/p&gt;
    &lt;p&gt;(bot)&lt;/p&gt;
    &lt;head rend="h3"&gt;"The bund Shanghai, zoom in"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 7s&lt;/p&gt;
    &lt;head rend="h2"&gt;Failure Cases&lt;/head&gt;
    &lt;p&gt;Examples where our model struggles or produces suboptimal results, particularly on complex motion and physical interactions. These limitations stem from: (1) insufficient training due to resource constraints, (2) low-quality training data, and (3) the absence of post-training refinement—we perform only pretraining without supervised fine-tuning (SFT) or reinforcement learning (RL).&lt;/p&gt;
    &lt;head rend="h3"&gt;"a dog shaking off water on a dock; handheld with minimal sway; blue-hour ambience, ultra-realistic, 4K; light film grain."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a goat kid hopping onto a small boulder then back down; handheld with minimal sway; blue-hour ambience, ultra-realistic, 4K; nat"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;""A green powder is being poured into a test tube"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a hamster running steadily in a clear exercise wheel; handheld with minimal sway; golden-hour warmth, ultra-realistic, 4K; light"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a skateboarder kickflipping off a curb; shallow parallax orbit at chest height; slight chromatic aberration at highlights; blue-"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a small octopus exploring a jar with one curious arm; gentle push-in from a stable tripod; golden-hour warmth, ultra-realistic,"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"a trail runner cresting a ridge at dawn; over-the-shoulder follow maintaining subject center; tiny focus breathing during rack f"&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
    &lt;head rend="h3"&gt;"fresh bread being sliced on a wooden board; close-up; kitchen window light, ultra-realistic, 4K."&lt;/head&gt;
    &lt;p&gt;480p • 16fps • 5s&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46117802</guid><pubDate>Tue, 02 Dec 2025 05:10:01 +0000</pubDate></item><item><title>Rootless Pings in Rust</title><link>https://bou.ke/blog/rust-ping/</link><description>&lt;doc fingerprint="41b3411fd915f5af"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Rootless pings in Rust&lt;/head&gt;
    &lt;p&gt;Sending a ping by creating an ICMP socket normally requires root: you can’t create a raw socket to send ICMP packets without it. The &lt;code&gt;ping&lt;/code&gt; command line tool works without root however, how is that possible? It turns out you can create a UDP socket with a protocol flag, which allows you to send the ping rootless. I couldn’t find any simple examples of this online and LLMs are surprisingly bad at this (probably because of the lack of examples). Therefore I posted an example on GitHub in Rust. The gist of it is this:&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Create a UDP socket with ICMP protocol&lt;/head&gt;
    &lt;p&gt;Using the socket2 crate.&lt;/p&gt;
    &lt;code&gt;use socket2::{Domain, Protocol, Socket, Type};
use std::net::UdpSocket;

let socket = Socket::new(Domain::IPV4, Type::DGRAM, Some(Protocol::ICMPV4))?;
let socket: UdpSocket = socket.into();
&lt;/code&gt;
    &lt;head rend="h2"&gt;2. Create and send the ping packet&lt;/head&gt;
    &lt;p&gt;Note that you don’t need to provide an IP header and that Linux and macOS behave differently here: the Linux kernel overrides the identifier and checksum fields, while macOS does use them and the checksum needs to be correct.&lt;/p&gt;
    &lt;code&gt;let sequence: u16 = 1;
let mut packet: Vec&amp;lt;u8&amp;gt; = vec![
	8, // type: echo request
	0, // code: always 0 for echo request
	0, 0, // checksum: calculated by kernel on Linux, required on macOS
	0, 1, // identifier: overwritten by kernel on Linux, not on macOS
	(sequence &amp;gt;&amp;gt; 8) as u8, (sequence &amp;amp; 0xff) as u8,
	b'h', b'e', b'l', b'l', b'o', // payload (can be anything)
];

// Checksum is determined by the kernel on Linux, but it's needed on macOS
let checksum = calculate_checksum(&amp;amp;packet);
packet[2] = (checksum &amp;gt;&amp;gt; 8) as u8;
packet[3] = (checksum &amp;amp; 0xff) as u8;

// Port can be anything, doesn't matter
socket.send_to(&amp;amp;packet, "1.1.1.1:0")?;
&lt;/code&gt;
    &lt;head rend="h2"&gt;3. Receive and interpret the response&lt;/head&gt;
    &lt;p&gt;Here macOS and Linux are different again: macOS includes the IP header in the response, Linux does not.&lt;/p&gt;
    &lt;code&gt;let mut buffer = vec![0u8; 64];
let (size, from_addr) = socket.recv_from(&amp;amp;mut buffer)?;

// On macOS, the IP header is included in the received packet, strip it
#[cfg(target_os = "macos")]
const IP_HEADER_LEN: usize = 20;

// On Linux, the IP header is not included
#[cfg(not(target_os = "macos"))]
const IP_HEADER_LEN: usize = 0;

let data = &amp;amp;buffer[IP_HEADER_LEN..size];
let reply_type = data[0]; // should be 0
let reply_sequence = ((data[6] as u16) &amp;lt;&amp;lt; 8) | (data[7] as u16); // should equal 'sequence'
let payload = &amp;amp;data[8..]; // should be b"hello"
&lt;/code&gt;
    &lt;p&gt;Of course you can implement latency, loss, periodic pings etc. but that’s left as an exercise to the reader.&lt;/p&gt;
    &lt;p&gt;Nov 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46118432</guid><pubDate>Tue, 02 Dec 2025 07:01:03 +0000</pubDate></item><item><title>How Brian Eno Created Ambient 1: Music for Airports (2019)</title><link>https://reverbmachine.com/blog/deconstructing-brian-eno-music-for-airports/</link><description>&lt;doc fingerprint="84e0c9c839be0ff2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How Brian Eno Created Ambient 1: Music for Airports&lt;/head&gt;
    &lt;p&gt;Brian Eno’s Ambient 1: Music for Airports is a landmark album in ambient and electronic music. Although it wasn’t the first ambient album, it was the first album to be explicitly labelled as ‘ambient music’.&lt;/p&gt;
    &lt;p&gt;Music for Airports was released in 1979, though some sources cite 1978 due to its copyright date. It marked a continuation of Eno’s experimentation with the tape machine as a compositional tool, a process he’d begun four years prior with 1975’s Discreet Music.&lt;/p&gt;
    &lt;p&gt;Music for Airports also saw Eno’s further exploration of generative, systems-created music, whereby Eno would focus on creating a system that would generate ambient music, something he continues to explore in the modern age with his range of iOS apps.&lt;/p&gt;
    &lt;p&gt;In this article, I’ll discuss how Music for Airports was created, and I’ll deconstruct and recreate the tracks 2/1 and 1/2. Hopefully, the article will demystify some of Brian Eno’s techniques, and give you some ideas about how to adopt some of his ambient music techniques yourself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Brian Eno &amp;amp; Ambient Music&lt;/head&gt;
    &lt;p&gt;Brian Eno’s experiments with tape loops go as far back as 1973’s (No Pussyfooting), a collaborative album with King Crimson guitarist Robert Fripp. For the recording of (No Pussyfooting), Eno employed an early experiment in sound-on-sound tape looping, where he would run Robert Fripp’s guitar into two tape machines, that were then fed back into each other.&lt;/p&gt;
    &lt;p&gt;Fripp’s guitar melodies were recorded and then bounced back and forth between the two tape machines, creating long, fading delays that would build up to create a dense soundscape. The length of the delay was controlled by the physical distance between the two machines.&lt;/p&gt;
    &lt;p&gt;Brian Eno’s tape experimentations continued with Discreet Music in 1975. The album’s 30-minute long title track was composed by sequencing his EMS Synthi AKS synth and recording it into a similar dual tape machine system, with the simple musical phrases repeating over a long period of time. This system utilised an EQ and delay effect before the tape machines, allowing Eno to subtly change the sounds in real-time.&lt;/p&gt;
    &lt;p&gt;Discreet Music uses two separate loops, one of 63 seconds duration and another of 68 seconds duration. Eno found that using two loops of different lengths created a phasing effect where every repeat would produce different variations as the two loops interlocked in different ways. I wrote a separate article going more in-depth on the recording of Discreet Music, available here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Recording Music for Airports&lt;/head&gt;
    &lt;p&gt;Music for Airports was released in 1979, though Brian Eno started working on it while working on David Bowie’s Low, in 1976. Part of it was recorded at the recording studio of Conny Plank, a legendary Krautrock producer, where he started by recording single notes sung by a trio of female singers, which he would later loop via tape machines. At a 1996 talk, Eno described the recording of Music for Airports:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Music for Airports, at least one of the pieces on there, is structurally very, very simple. There are sung notes, sung by three women and myself. One of the notes repeats every 23 1/2 seconds. It is in fact a long loop running around a series of tubular aluminum chairs in Conny Plank’s studio. The next lowest loop repeats every 25 7/8 seconds or something like that. The third one every 29 15/16 seconds or something. What I mean is they all repeat in cycles that are called incommensurable — they are not likely to come back into sync again.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Eno had previously recorded Before and After Science and Cluster &amp;amp; Eno at Conny Plank’s studio, and would go on to record Devo’s Q. Are We Not Men? A: We Are Devo! there too.&lt;/p&gt;
    &lt;p&gt;To compose the music of Music for Airports, Brian Eno’s experiments focused on using small recordings of music – sustained notes or 3-4 note phrases – and looping them at different rates, determined by the length of tape they are recorded on. The difference in tape lengths between loops would cause them to intersect in interesting ways; on each repeat, new phrases and variations on existing themes would emerge. Eno himself puts it best:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“The particular piece I’m referring to was done by using a whole series of very long tape loops, like fifty, sixty, seventy feet long. There were twenty-two loops. One loop had just one piano note on it. Another one would have two piano notes. Another one would have a group of girls singing one note, sustaining it for ten seconds. There are eight loops of girls’ voices and about fourteen loops of piano.&lt;/p&gt;
      &lt;p&gt;I just set all of these loops running and let them configure in whichever way they wanted to, and in fact the result is very, very nice. The interesting thing is that it doesn’t sound at all mechanical or mathematical as you would imagine. It sounds like some guy is sitting there playing the piano with quite intense feeling. The spacing and dynamics of “his” playing sound very well organized. That was an example of hardly interfering at all.“&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Graphic Score&lt;/head&gt;
    &lt;p&gt;Music for Airports liner notes contain a graphic score designed by Brian Eno himself. Not a trained musician, and unable to read or write sheet music, he instead used graphic symbols to denote each musical phrase or loop. Look closely and you can see individual symbols on each row, each spaced apart differently, reflecting the recording technique used to craft the album.&lt;/p&gt;
    &lt;p&gt;Brian Eno also designed the cover art for Music for Airports, as well the rest of the ambient series: Ambient 2: The Plateaux of Mirror with Harold Budd, Ambient 3: Day of Radiance with Laraaji and Ambient 4: On Land, each of which has map-like covers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deconstructing 1/1&lt;/head&gt;
    &lt;p&gt;The first track on Music for Airports is 1/1, which features a serene sounding piano melody interspersed with ethereal textures. 1/1 has been used in the films 9½ Weeks and The Lovely Bones.&lt;/p&gt;
    &lt;p&gt;The piano in 1/1 was performed by Robert Wyatt, a prog rock musician who started as the drummer in Soft Machine before pursuing a solo career. The piano recording has been run through an echo unit, looped and then slowed down, a process that Eno would have done by manually joining two ends of a reel of tape, and then playing it back on a reel-to-reel machine at half speed. Slowing down a tape machine causes the pitch of the musical content to drop, with half-speed causing a drop of an octave.&lt;/p&gt;
    &lt;p&gt;The piano loop in 1/1 features interplay between a traditional piano and a Rhodes electric piano. Here is the loop, and then the isolated piano and rhodes parts, it may have sounded at the original speed, before being reverb’d and slowed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1/1 Original Speed 00:00&lt;/item&gt;
      &lt;item&gt;1/1 Piano 00:00&lt;/item&gt;
      &lt;item&gt;1/1 Rhodes 00:00&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once slowed down, the texture of the instruments change, becoming bassy and less defined. The echo effect gets smeared and stretched, creating an unreal ambience that is emblematic of the sound of Music for Airports. And this was some 45 years before the popularity of reverb and slowed versions on YouTube were a thing.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1/1 Slowed 00:00&lt;/item&gt;
      &lt;item&gt;1/1 Piano Slowed 00:00&lt;/item&gt;
      &lt;item&gt;1/1 Rhodes Slowed 00:00&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The performance is mostly in the key of D major, with the Rhodes piano holding down D bass notes throughout. However, the final Rhodes phrase contains a C natural note, leading the music into modal D mixolydian territory.&lt;/p&gt;
    &lt;p&gt;Mixolydian is a mode, or scale, that contains the same notes as the major scale with one difference: it has a minor 7th instead of a major 7th. The Mixolydian mode has a more ambiguous sound than major, as it features a major 3rd and a minor 7th. The sound is still major, but with a less ‘sweet’ sound than in D major. The use of the Mixolydian mode is another facet that gives 1/1 it’s restful, relaxing sound; it sounds emotionally ambiguous.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deconstructing 2/1&lt;/head&gt;
    &lt;p&gt;Music for Airports’ second track, 2/1 consists of a choir singing shapeless harmonies. There are no real melodies present, and the voices occasionally form chords, but there is no discernible structure. This song is composed of seven loops, all of different lengths, with each loop playing back a single, sung note. In the graphic score, you can see Brian Eno’s use of rectangles to represent looped tracks, with the spaces between them varying.&lt;/p&gt;
    &lt;p&gt;These loops have been recreated below, along with the approximate times that each loop repeats. Note that these times aren’t perfect, and the timings fluctuate throughout the piece, likely due to the way they were looped (the tapes were wrapped around chair legs). Some things to note here are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The C and high F loops are very close together to each other in duration (20.1 seconds and 19.6 seconds, respectively). They start out with the notes playing separately, and over time the notes gradually get closer together until they play back at the same time, almost as a chord.&lt;/item&gt;
      &lt;item&gt;The D♭ loop has the longest length by a significant margin, and D♭ is also the most dissonant of all the notes in the piece, as it creates a semi-tone clash with C every time the two play together. The longer loop length for this note may have been a conscious decision by Eno, or the composition may have just come out this way through experimentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Click on the play icons below to start each individual loop. The dice button at the bottom randomises the loop times and the reset button reverts the loop times back to the original album times.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deconstructing 1/2&lt;/head&gt;
    &lt;p&gt;For the second track on Music for Airports, 1/2, Brian Eno uses eight short piano phrases recorded to tape to create the sense of a bigger composition, one that is again shapeless and without structure, but one that continually evolves as the piece progresses. Some of the loops are just single notes, and others consist of simple 3 or 4-note snippets, with one loop being an arpeggiated chord.&lt;/p&gt;
    &lt;p&gt;These loops overlap in different places upon each repeat, creating the illusion of new phrases and melodies, which are simply different combinations of the eight snippets. Looped for long periods, like the tracks 11:36 running time, allows for many variations and themes to emerge.&lt;/p&gt;
    &lt;p&gt;Click on the play icons below to start each individual loop. The dice button at the bottom randomises the loop times and the reset button reverts the loop times back to the original album times.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ambient Approach&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;One way to get into this Music for Airports style of composition is to start using tape. If you can’t commit to getting big reel-to-reel machines, you can always start with cheaper cassette players. Some great online resources for cassette tape loops are Amulets, Hainbach, and Gemini Horror.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to work with this style in your DAW, turn off the grid and start creating loops in seconds/milliseconds instead of bars/beats. In Ableton Live you can press ⌘ + 4 (Ctrl-4 on Windows) to turn off the grid, allowing you to create unquantized loops that work in a similar way to the tape loops described in this article. Create several clips of different lengths, set them all to loop simultaneously, and record the results.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To dig deeper into this style of tape loop ambient music, check out William Basinski’s The Disintegration Loops. William Basinski used a similar concept to Brian Eno, only the tapes he used rapidly deteriorated upon playback, causing the musical material to degrade over the length of the recording.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Further Reading&lt;/head&gt;
    &lt;p&gt;Below is a list of interviews with Brian Eno as well as external resources for generative music:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Music for Airports Liner Notes&lt;/item&gt;
      &lt;item&gt;Eric Tamm: Brian Eno His Music And The Vertical Color Of Sound&lt;/item&gt;
      &lt;item&gt;Brian Eno San Francisco 1986 talk&lt;/item&gt;
      &lt;item&gt;Tero Parviainen: How Generative Music Works&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46118722</guid><pubDate>Tue, 02 Dec 2025 07:46:47 +0000</pubDate></item><item><title>Comparing AWS Lambda ARM64 vs. x86_64 Performance Across Runtimes in Late 2025</title><link>https://chrisebert.net/comparing-aws-lambda-arm64-vs-x86_64-performance-across-multiple-runtimes-in-late-2025/</link><description>&lt;doc fingerprint="76b950f540b24094"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Comparing AWS Lambda Arm64 vs x86_64 Performance Across Multiple Runtimes in Late 2025&lt;/head&gt;
    &lt;p&gt;See how AWS Lambda arm64 stacks up against x86_64 in real-world benchmarks across CPU, memory, and I/O workloads using Node.js, Python, and Rust.&lt;/p&gt;
    &lt;p&gt;If you know me at all, you know I’m a big proponent of serverless services when they’re used for the right workloads. I helped launch a new product that increased usage by over 30x so far this year. Choosing serverless was a big part of why that launch went as smoothly as it did. Given how heavily I use Lambda, I have a vested interest in how its performance and architecture evolve.&lt;/p&gt;
    &lt;p&gt;Initially, Amazon Web Services (AWS) Lambda only supported x86_64-based compute. In 2021, AWS added support for arm64-based Graviton processors, which were advertised as offering equal or better performance at a lower price point and with a smaller environmental footprint.&lt;/p&gt;
    &lt;p&gt;Back in October 2023, AWS published a blog post titled "Comparing AWS Lambda Arm vs. x86 Performance, Cost, and Analysis." This post was a great reference at the time, but nearly two years later, I haven’t seen many follow-up benchmarks either on the official AWS blog or from the community. I’ve been wondering how things look in 2025 if you apply a similar methodology, which led me to build a more modern, generic benchmark of my own.&lt;/p&gt;
    &lt;p&gt;Going into this, I expected arm64 to be the most performant architecture and Rust to be the most performant runtime, but I wanted actual data to support my assumptions. So, I built a benchmark that runs Lambda functions on both x86_64 and arm64 architectures across CPU-intensive, memory-intensive, and light workloads, using the actively supported AWS runtimes for Node.js, Rust, and Python. While you should always benchmark and evaluate the performance of your real-world production workloads, generic benchmarks are always interesting for investigating general performance trends.&lt;/p&gt;
    &lt;p&gt;In this post, I’ll first highlight the high-level findings. Then I’ll walk through the benchmark design (workloads, runtimes, and configurations), and finally, I’ll dig into the detailed results. Unlike the AWS benchmark mentioned earlier, this project is fully open source and available on GitHub. You’re welcome to replicate my results, extend the tests, or adapt them to your own workloads. You can find the code in the aws-lambda-performance-benchmarks repository. The results of a recent benchmark run are also published to that repo.&lt;/p&gt;
    &lt;quote&gt;Note: This benchmark includes the officially supported Rust runtime (announced GA on November 14, 2025) and the Python 3.14 runtime (announced GA on November 18, 2025). I’ll talk more about those under Runtimes below.&lt;/quote&gt;
    &lt;head rend="h2"&gt;TLDR: The Winners&lt;/head&gt;
    &lt;p&gt;If you don’t have time to read the entire post, here are the key takeaways. I ran the benchmark several times in the &lt;code&gt;us-east-2&lt;/code&gt; (Ohio) region, and observed similar results across benchmark runs. The results shared in this post come from my most recent run, which tested 42 Lambda functions (7 runtimes × 2 architectures × 3 workloads). After collecting samples, I removed outliers using basic statistical techniques and calculated mean, median, and P50/P90/P95/P99 percentiles.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Performance champion: Rust on arm64 is the most performant and cost-efficient combination overall. There are a few instances where x86_64 Rust slightly beats out arm64 by a thin margin, but with a 20% cost discount, arm still wins at efficiency.&lt;/item&gt;
      &lt;item&gt;Python: Python 3.11 on arm64 slightly outperformed the newer Python runtimes in my tests (which honestly surprised me, but turns out to match other public benchmarks).&lt;/item&gt;
      &lt;item&gt;Node.js: Node.js 22 on arm64 was consistently faster than Node.js 20 on x86_64. There's essentially a “free” ~15-20% speedup just by switching architectures!&lt;/item&gt;
      &lt;item&gt;Cost: Across the board, arm64 delivered roughly 30–40% lower compute costs with equal or better performance than x86_64. Unless you're using a library that isn't compatible with arm64 or have a unique workload, arm is a good default architecture choice for Lambda.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Benchmark Methodology&lt;/head&gt;
    &lt;p&gt;My goal was to create an updated benchmark similar to the 2023 AWS Lambda benchmark blog post. Unfortunately, I was unable to find the original code used for that benchmark, so I created a new, similar benchmark from scratch.&lt;/p&gt;
    &lt;p&gt;The existing AWS benchmark used three types of workloads, which I also adopted:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Light: A workload that is lightweight but realistic.&lt;/item&gt;
      &lt;item&gt;CPU-intensive: A workload that stresses compute utilization.&lt;/item&gt;
      &lt;item&gt;Memory-intensive: A workload that stresses memory utilization.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The original benchmark included both single-threaded and multi-threaded CPU and memory-intensive tests. I chose not to include multi-threaded tests in my benchmark for two reasons. First, building a robust benchmark harness is time-intensive. It took a good amount of my free time to make the single-threaded version of this benchmark before adding even more tests to run and analyze. Second, most of my personal use cases for Lambda aren’t multi-threaded, and the multi-threaded results weren’t particularly interesting to me.&lt;/p&gt;
    &lt;p&gt;It’s worth reminding readers that AWS Lambda allocates CPU power in proportion to the amount of memory configured for a function. This means that to get 1 full vCPU of compute, you must allocate 1,769 MB of memory to a Lambda. For my single-threaded workloads, you’d expect diminishing returns if a Lambda is configured with more memory than this, since a single-threaded Lambda workload cannot use more than one vCPU in these tests. However, for the memory-intensive workload, additional performance can be achieved at an extra cost by allocating more memory.&lt;/p&gt;
    &lt;p&gt;The benchmark includes detailed documentation on its workloads in the Benchmark Design page on GitHub. Here’s a summary of what each workload actually tests:&lt;/p&gt;
    &lt;head rend="h3"&gt;Workloads&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Light: A realistic workload that uses DynamoDB batch write (5 items) followed by batch read (5 items). This test includes AWS SDK overhead, serialization/deserialization, and network I/O latency with minimal compute. Because this test accesses DynamoDB, it introduces some performance variability due to network latency and underlying database performance, but it remains a realistic Lambda scenario.&lt;/item&gt;
      &lt;item&gt;CPU-intensive: Performs 500,000 iterations of SHA-256 cryptographic hashing in a tight loop. This is a pure compute workload with no AWS SDK dependencies, designed to stress CPU performance and measure single-threaded execution speed.&lt;/item&gt;
      &lt;item&gt;Memory-intensive: Allocates and sorts a 100 MB array using native 64-bit types, stressing memory bandwidth and CPU together. I chose 100 MB to ensure lower-memory Lambda configurations could complete successfully. I initially tried allocating larger arrays, but this quickly became too much for Python on the lower-power configurations. The Rust and Node.js runtimes were capable of allocating larger arrays even at the smallest 128 MB Lambda memory allocation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A best effort was made to consistently implement these workloads across runtimes. There are always some variances across languages, but the implementations are very similar.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runtimes&lt;/head&gt;
    &lt;p&gt;The original 2023 blog post tested similar workloads across Node.js, Ruby, and Python. The runtime versions tested at that time are no longer supported by AWS. Since 2023, Rust has grown in popularity, and Ruby usage in Lambda has remained relatively niche compared to Node.js and Python. I chose to replace Ruby with Rust, which AWS officially supported this month. I had also never created a Rust Lambda before and wanted to try it out.&lt;/p&gt;
    &lt;head rend="h4"&gt;Benchmarked AWS Runtimes&lt;/head&gt;
    &lt;p&gt;As of November 2025, the benchmark tested all actively supported AWS runtimes for Node.js, Python, and Rust:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Lambda Runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;Release Date&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Node.js 20&lt;/cell&gt;
        &lt;cell&gt;November 15, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Node.js 22&lt;/cell&gt;
        &lt;cell&gt;November 22, 2024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python 3.11&lt;/cell&gt;
        &lt;cell&gt;July 27, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python 3.12&lt;/cell&gt;
        &lt;cell&gt;December 14, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python 3.13&lt;/cell&gt;
        &lt;cell&gt;November 13, 2024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python 3.14&lt;/cell&gt;
        &lt;cell&gt;November 18, 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;November 14, 2025&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You can always view the latest runtimes supported by AWS on the Lambda runtimes page.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory Configurations&lt;/head&gt;
    &lt;p&gt;I ran each workload across all runtimes included in the benchmark using the following memory configurations on x86_64 and arm64 architectures.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Workload Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory Configurations (MB)&lt;/cell&gt;
        &lt;cell role="head"&gt;Total Configs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Light&lt;/cell&gt;
        &lt;cell&gt;128, 256, 512, 1024, 1769, 2048&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CPU-intensive&lt;/cell&gt;
        &lt;cell&gt;128, 256, 512, 1024, 1769, 2048&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Memory-intensive&lt;/cell&gt;
        &lt;cell&gt;128, 256, 512, 1024, 1769, 2048, 4096, 8192, 10240&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Sampling, Runs, and Cold vs. Warm Starts&lt;/head&gt;
    &lt;p&gt;For each combination of runtime, architecture, workload, and memory configuration, I ran multiple test runs and captured cold start and warm start metrics separately. I was more interested in warm start sampling, but also wanted to collect enough cold start samples so we could compare cold starts across runtimes. At the time I completed these tests, the 3.14 Python Lambda runtime was just released. Since this runtime is newer, there may be a higher cold-start penalty, since most AWS customers haven't had a chance to update their Lambdas to target it yet, which makes the Lambda infrastructure less likely to get a cache hit when loading the runtime for a cold Python 3.14 Lambda. If we ran these tests a few weeks from now, the cold start times for the Python 3.14 runtime could improve.&lt;/p&gt;
    &lt;p&gt;At a high level:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data collected: The Metrics Collection page provides complete details on how metrics are collected using CloudWatch log trailing, without adding any additional overhead to the tests. Init duration, duration, billed duration, and max memory used metrics for each Lambda invocation are collected and stored in DynamoDB. AWS provides helpful documentation on what these metrics mean on the Understanding the Lambda execution environment lifecycle page.&lt;/item&gt;
      &lt;item&gt;Sample size: For each configuration, the benchmark performs 125 cold invocations and 500 warm invocations (625 total invocations per config) across 294 unique test configurations, resulting in 183,750 Lambda invocations per test run. I ran multiple benchmark runs in &lt;code&gt;us-east-2&lt;/code&gt;and saw results that were relatively similar across time of day.&lt;/item&gt;
      &lt;item&gt;Cold start tests: Cold starts were triggered by toggling the Lambda's memory configuration, which invalidates all warm instances without requiring redeployment. For this benchmark, I collected 125 cold samples for each configuration. I borrowed this cold start testing technique from AJ Stuyvenberg's blog posts. AJ's approach dramatically speeds up testing by eliminating the need to wait for natural cold starts. The AWS Lambda Power Tuning Tool takes a similar approach when used to benchmark an individual Lambda.&lt;/item&gt;
      &lt;item&gt;Warm start tests: For each configuration, 500 warm Lambda invocations were collected for this benchmark.&lt;/item&gt;
      &lt;item&gt;Statistics: After collecting raw samples, statistical outliers (min and max&lt;lb/&gt;values) were removed. Then, from the cleaned data, the mean, median, standard deviation, and P50/P90/P95/P99 percentiles for each configuration were calculated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Results Overview&lt;/head&gt;
    &lt;p&gt;The primary comparisons in this post focus on warm invocations, since that's what most production traffic looks like. Here are some key findings from the data:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;arm64 wins on cost in every scenario&lt;/item&gt;
      &lt;item&gt;Rust is dramatically faster than interpreted runtimes &lt;list rend="ul"&gt;&lt;item&gt;8x faster than Node.js, 2x faster than Python&lt;/item&gt;&lt;item&gt;Cold starts favor ARM64 with 13-24% faster initialization&lt;/item&gt;&lt;item&gt;x86 Rust slightly outperformed arm64 Rust for CPU-intensive work at high memory&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Python 3.11 is the fastest Python&lt;list rend="ul"&gt;&lt;item&gt;9-15% faster than 3.12, 3.13, and 3.14&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Node.js 22 beats Node.js 20&lt;list rend="ul"&gt;&lt;item&gt;Node.js 22 beat out Node.js 20 by 8-11% across the board&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;CPU-Intensive Workload Results&lt;/head&gt;
    &lt;p&gt;The CPU-intensive workload (500,000 SHA-256 iterations) provides a clean view of raw compute performance without I/O overhead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Warm Start Performance&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;Rust completes the same workload in a fraction of the time compared to interpreted runtimes:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;arm64 @2048MB&lt;/cell&gt;
        &lt;cell role="head"&gt;x86 @2048MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;163ms&lt;/cell&gt;
        &lt;cell&gt;147ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Python 3.11&lt;/cell&gt;
        &lt;cell&gt;263ms&lt;/cell&gt;
        &lt;cell&gt;341ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Python 3.14&lt;/cell&gt;
        &lt;cell&gt;287ms&lt;/cell&gt;
        &lt;cell&gt;358ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Node.js 22&lt;/cell&gt;
        &lt;cell&gt;1,260ms&lt;/cell&gt;
        &lt;cell&gt;1,384ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Node.js 20&lt;/cell&gt;
        &lt;cell&gt;1,377ms&lt;/cell&gt;
        &lt;cell&gt;1,549ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Rust is 8x faster than Node.js and nearly twice as fast as Python for this compute-heavy workload.&lt;/p&gt;
    &lt;p&gt;One surprising finding: x86 Rust outperformed arm64 Rust by 10% at higher memory configurations. This was unexpected given arm64's general performance parity. For most runtimes, arm64 matches or exceeds x86 performance, but Rust on x86 appears to have an edge for pure compute at scale.&lt;/p&gt;
    &lt;head rend="h3"&gt;Python Version Comparison&lt;/head&gt;
    &lt;p&gt;Python 3.11 consistently outperformed newer versions across all memory configurations. It was 9-15% faster than Python 3.12, 3.13, and 3.14. This surprised me, but matched other Python benchmarks I found online. Python 3.11 had notable performance improvements over previous versions, and subsequent releases have had negligible single-threaded improvements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Node.js Version Comparison&lt;/head&gt;
    &lt;p&gt;Node.js 22 showed consistent improvements over Node.js 20, with execution times 8-11% faster across memory configurations. Combined with the benefits of the arm64 architecture, upgrading from Node.js 20 on x86 to Node.js 22 on arm64 delivers approximately 18% performance improvement at no additional cost.&lt;/p&gt;
    &lt;head rend="h3"&gt;Rust Comparison&lt;/head&gt;
    &lt;p&gt;Rust performed consistently well regardless of the amount of memory allocated to it. It is a highly efficient runtime for CPU-intensive workloads.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cost Analysis&lt;/head&gt;
    &lt;p&gt;Arm64 delivered 7-38% cost savings for CPU-intensive workloads across all runtimes. Even when x86 is slightly faster, arm64's 20% lower price per GB-second wins on total cost.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory-Intensive Workload Results&lt;/head&gt;
    &lt;p&gt;The memory-intensive workload allocates and sorts a 100MB array, stressing both memory bandwidth and CPU.&lt;/p&gt;
    &lt;head rend="h3"&gt;Warm Start Performance&lt;/head&gt;
    &lt;p&gt;This workload showed interesting patterns:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;arm64 @10240MB&lt;/cell&gt;
        &lt;cell role="head"&gt;x86 @10240MB&lt;/cell&gt;
        &lt;cell role="head"&gt;arm64 Advantage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;706ms&lt;/cell&gt;
        &lt;cell&gt;811ms&lt;/cell&gt;
        &lt;cell&gt;+13%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Node.js 20&lt;/cell&gt;
        &lt;cell&gt;1,900ms&lt;/cell&gt;
        &lt;cell&gt;2,623ms&lt;/cell&gt;
        &lt;cell&gt;+28%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Node.js 22&lt;/cell&gt;
        &lt;cell&gt;1,894ms&lt;/cell&gt;
        &lt;cell&gt;2,597ms&lt;/cell&gt;
        &lt;cell&gt;+27%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Python 3.11&lt;/cell&gt;
        &lt;cell&gt;9,178ms&lt;/cell&gt;
        &lt;cell&gt;12,717ms&lt;/cell&gt;
        &lt;cell&gt;+28%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Arm64's advantage grows with memory allocation in these charts. At the maximum 10GB Lambda configuration, arm64 was 27-28% faster than x86 for Node.js workloads.&lt;/p&gt;
    &lt;p&gt;Rust again dominates, completing the memory-intensive workload 2.7x faster than Node.js and 13x faster than Python.&lt;lb/&gt;Python showed unexpectedly high variability in the memory-intensive workload. Python 3.12 and 3.13 on arm64 were actually slower than x86 at several memory configurations, which is not what we'd expect. I suspect this is related to Python's memory management and garbage collection behavior rather than the underlying architecture or an issue with my benchmark. Python 3.11 showed more consistent advantages on arm64. &lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Cost Analysis&lt;/head&gt;
    &lt;p&gt;Arm64 delivered significant cost savings for memory-intensive workloads, up to 42%, at higher memory configurations where arm64's performance advantage is most clear. Node.js and Rust showed the most consistent savings (23-42%), while Python's cost efficiency varied due to the performance anomalies noted above.&lt;/p&gt;
    &lt;head rend="h2"&gt;Light Workload Results&lt;/head&gt;
    &lt;p&gt;The light workload (DynamoDB batch read/write) represents a realistic Lambda scenario in which I/O latency dominates execution time. I'll keep this section brief since the results tell a simple story.&lt;/p&gt;
    &lt;p&gt;For I/O-bound workloads, the runtime differences largely disappear. When network latency is the bottleneck, whether you're running Rust or Python matters far less. All runtimes completed the light workload in 15-80ms at 512MB and above. The main takeaway for light workloads: optimize for cost, not raw performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cold Start Analysis&lt;/head&gt;
    &lt;p&gt;Cold starts are often a critical concern for Lambda users, especially for latency-sensitive applications.&lt;/p&gt;
    &lt;p&gt;Here's a breakdown of the init duration by runtime we observed during these benchmarks:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;arm64 Init (avg)&lt;/cell&gt;
        &lt;cell role="head"&gt;x86 Init (avg)&lt;/cell&gt;
        &lt;cell role="head"&gt;arm64 Advantage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;16ms&lt;/cell&gt;
        &lt;cell&gt;21ms&lt;/cell&gt;
        &lt;cell&gt;+24%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python 3.11&lt;/cell&gt;
        &lt;cell&gt;79ms&lt;/cell&gt;
        &lt;cell&gt;94ms&lt;/cell&gt;
        &lt;cell&gt;+16%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python 3.12&lt;/cell&gt;
        &lt;cell&gt;89ms&lt;/cell&gt;
        &lt;cell&gt;107ms&lt;/cell&gt;
        &lt;cell&gt;+17%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python 3.13&lt;/cell&gt;
        &lt;cell&gt;100ms&lt;/cell&gt;
        &lt;cell&gt;122ms&lt;/cell&gt;
        &lt;cell&gt;+18%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python 3.14&lt;/cell&gt;
        &lt;cell&gt;124ms&lt;/cell&gt;
        &lt;cell&gt;143ms&lt;/cell&gt;
        &lt;cell&gt;+13%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Node.js 20&lt;/cell&gt;
        &lt;cell&gt;134ms&lt;/cell&gt;
        &lt;cell&gt;155ms&lt;/cell&gt;
        &lt;cell&gt;+13%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Node.js 22&lt;/cell&gt;
        &lt;cell&gt;129ms&lt;/cell&gt;
        &lt;cell&gt;150ms&lt;/cell&gt;
        &lt;cell&gt;+14%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Rust cold starts are 5-8x faster than interpreted runtimes. At 16ms on arm64, Rust initialization is nearly imperceptible. This makes Rust an excellent choice for latency-sensitive applications where cold starts matter.&lt;/p&gt;
    &lt;p&gt;Arm64 consistently showed 13-24% faster cold start initialization across all runtimes. This is a meaningful improvement that compounds with cold start frequency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cost Efficiency Deep Dive&lt;/head&gt;
    &lt;p&gt;The cost analysis reveals why arm64 is such a great default:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;arm64 is 20% cheaper per GB-second than x86&lt;/item&gt;
      &lt;item&gt;arm64 performance matches or exceeds x86 in most cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The combined cost and performance efficiency of arm64 make it a strong default architecture for Lambda, unless a library you are using in a Lambda isn't compatible with arm. Even when x86 shows a slight performance edge (like Rust CPU-intensive at high memory), arm64's price advantage typically results in better cost efficiency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance Consistency (P99 Latency)&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;For production workloads, tail latency often matters more than averages. A function that's fast on average but spiky at P99 can still blow your latency budgets.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;The P99 results largely mirror the mean duration patterns. Rust delivered the most consistent performance with P99/mean ratios close to 1.0. Python and Node.js showed slightly more variability (P99 was 5-15% higher than the mean in some configurations), but nothing that would change your runtime selection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;The verdict is clear: arm64 should be your default targeted CPU architecture for Lambda. After multiple benchmark runs analyzing 183,750 Lambda invocations across 294 configurations, the data consistently points in one direction. Unless you have a specific library compatibility issue, arm64 wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why ARM64 Wins&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Benefit&lt;/cell&gt;
        &lt;cell role="head"&gt;Impact&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Performance&lt;/cell&gt;
        &lt;cell&gt;Equal or better in 90%+ of scenarios&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cold starts&lt;/cell&gt;
        &lt;cell&gt;13-24% faster initialization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Cost&lt;/cell&gt;
        &lt;cell&gt;25-40% lower per invocation&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The rare exceptions (like Rust being CPU-intensive at high memory usage) don't outweigh these consistent benefits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runtime Selection Guide&lt;/head&gt;
    &lt;p&gt;You should never rely on generic benchmarks and instead, evaluate Lambdas for your specific workloads. However, if we were to use these generic benchmarks as a guide, this is what they would suggest:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Your Priority&lt;/cell&gt;
        &lt;cell role="head"&gt;Best Choice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Maximum performance&lt;/cell&gt;
        &lt;cell&gt;Rust on arm64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Minimal cold starts&lt;/cell&gt;
        &lt;cell&gt;Rust on arm64 (16ms init)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python workloads&lt;/cell&gt;
        &lt;cell&gt;Python 3.11 on arm64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Node.js workloads&lt;/cell&gt;
        &lt;cell&gt;Node.js 22 on arm64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;I/O-bound workloads&lt;/cell&gt;
        &lt;cell&gt;Any runtime—optimize for cost&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Writing Benchmarks Is Hard&lt;/head&gt;
    &lt;p&gt;This benchmark is relatively simple, with only three workloads to test. However, because each workload runs across multiple architectures and runtimes, even a ‘simple’ benchmark can become time-consuming to build, deploy, and collect results. I’m sure this benchmark isn’t perfect, but it still reveals some useful trends. Going through this process also gave me tremendous respect for the people who regularly build and contribute to benchmarking tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reproduce These Results&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;The complete benchmark code is available at aws-lambda-performance-benchmarks. Run your own tests, or adapt the workloads to match your production patterns. I'd love to hear how your real-world results compare. Switching to arm64 is the easiest performance win you can get.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46119214</guid><pubDate>Tue, 02 Dec 2025 09:11:41 +0000</pubDate></item><item><title>Advent of Compiler Optimisations 2025</title><link>https://xania.org/202511/advent-of-compiler-optimisation</link><description>&lt;doc fingerprint="256ff9515e1f33d2"&gt;
  &lt;main&gt;
    &lt;p&gt;Today I’m announcing a project that’s been in the making for around a year. As my time off draws to a close, I’ve been working on an “Advent of” type project, to be released one a day from the 1st of December until the 25th.&lt;/p&gt;
    &lt;p&gt;This December will be the Advent of Compiler Optimisations: I’ll release one blog post and video each day, each detailing a fun and interesting C or C++ optimisation that your compiler can do. I’ll go into the details of when it applies, how to interpret the assembly, and perhaps as importantly, when it doesn’t apply.&lt;/p&gt;
    &lt;p&gt;I’ll be covering some very low-level, architecture-specific tricks as well as larger, more high-level optimisations. While I mostly cover x86-64, I do touch on 64-bit and 32-bit ARM as well.&lt;/p&gt;
    &lt;p&gt;You can follow along by watching the AoCO2025 tag on this blog, subscribing to me on YouTube, or following the YouTube playlist.&lt;/p&gt;
    &lt;p&gt;It’s been a colossal amount of work, but a lot of fun too. I hope you enjoy learning how amazing compilers are as much as I do!&lt;/p&gt;
    &lt;p&gt;See you on the first of December!&lt;/p&gt;
    &lt;p&gt;Matt Godbolt is a C++ developer living in Chicago. He works for Hudson River Trading on super fun but secret things. He is one half of the Two's Complement podcast. Follow him on Mastodon or Bluesky.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46119500</guid><pubDate>Tue, 02 Dec 2025 09:51:42 +0000</pubDate></item><item><title>Man unexpectedly cured of HIV after stem cell transplant</title><link>https://www.newscientist.com/article/2506595-man-unexpectedly-cured-of-hiv-after-stem-cell-transplant/</link><description>&lt;doc fingerprint="368500099aab4a12"&gt;
  &lt;main&gt;
    &lt;p&gt;A man has become the seventh person to be left HIV-free after receiving a stem cell transplant to treat blood cancer. Significantly, he is also the second of the seven who received stem cells that were not actually resistant to the virus, strengthening the case that HIV-resistant cells may not be necessary for an HIV cure.&lt;/p&gt;
    &lt;p&gt;“Seeing that a cure is possible without this resistance gives us more options for curing HIV,” says Christian Gaebler at the Free University of Berlin.&lt;/p&gt;
    &lt;p&gt;Five people have previously become free of HIV after receiving stem cells from donors who carried a mutation in both copies of a gene encoding a protein called CCR5, which HIV uses to infect immune cells. This led scientists to conclude that having two copies of the mutation, which completely removes CCR5 from immune cells, was crucial for curing HIV. “The belief was that using these HIV-resistant stem cells was essential,” says Gaebler.&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;But last year a sixth person – known as the “Geneva patient” – was declared free of the virus for more than two years after receiving stem cells without the CCR5 mutation, suggesting CCR5 isn’t the whole story – although many scientists think the roughly two-year virus-free period isn’t quite long enough to show they were actually cured, says Gaebler.&lt;/p&gt;
    &lt;p&gt;The latest case strengthens the idea that the Geneva patient has been cured. It involves a man who, in October 2015, received stem cells to treat leukaemia, a type of blood cancer where immune cells grow uncontrollably. The man, who was aged 51 at the time, had HIV. During his treatment, he was given chemotherapy to destroy the vast majority of his immune cells, making room for the donor stem cells to produce a healthy immune system.&lt;/p&gt;
    &lt;p&gt;Free newsletter&lt;/p&gt;
    &lt;head rend="h4"&gt;Sign up to Health Check&lt;/head&gt;
    &lt;p&gt;Expert insight and news on scientific developments in health, nutrition and fitness, every Saturday.&lt;/p&gt;
    &lt;p&gt;Ideally, the man would have received HIV-resistant stem cells, but these weren’t available, so doctors used cells that carried one typical and one mutated copy of the CCR5 gene. At the time, the man was taking a standard HIV therapy called antiretroviral therapy (ART), a combination of drugs that suppress the virus to undetectable levels, meaning it can’t be passed on to other people – and reducing the risk that the donor cells would be infected.&lt;/p&gt;
    &lt;p&gt;But about three years after the transplant, he chose to stop taking ART. “He felt that he’d waited some time after the stem cell transplant, he was in remission for the cancer, and he was always feeling that the transplant would work,” says Gaebler.&lt;/p&gt;
    &lt;p&gt;Shortly after, the team found no signs of the virus in blood samples from the man. He has since remained free of the virus for seven years and three months, enough for him to be considered “cured”. He has had no detectable HIV in his body for the second longest period of the seven people declared free of the virus – with the longest case being HIV-free for about 12 years. “It’s amazing that 10 years ago his chances of dying of cancer were extremely high and now he’s overcome this deadly diagnosis, a persistent viral infection and he’s not taking any medications – he’s healthy,” says Gaebler.&lt;/p&gt;
    &lt;p&gt;The discovery upends our understanding of what’s required for curing HIV via this approach. “We thought you needed to transplant from donors that lack CCR5 – it turns out that you don’t,” says Ravindra Gupta at the University of Cambridge, who wasn’t involved in the study.&lt;/p&gt;
    &lt;p&gt;Scientists have generally thought that such cures relied on any virus lurking in the recipient’s remaining immune cells – following chemotherapy – being unable to infect the donor cells, meaning it can’t replicate. “Essentially, the pool of host cells to infect runs dry,” says Gaebler.&lt;/p&gt;
    &lt;p&gt;But the latest case suggests that, instead, cures can be achieved as long as non-resistant donor cells are able to destroy any of the patient’s remaining original immune cells before the virus can spread to them, speculates Gaebler. Such immune reactions are often driven by differences in the proteins displayed on the two sets of cells. These make the donor cells recognise residual recipient cells as a threat to eliminate, says Gaebler.&lt;/p&gt;
    &lt;p&gt;The findings suggest that a wider pool of stem cell transplants than we thought – including those without two copies of the CCR5 mutation – could potentially cure HIV, says Gaebler.&lt;/p&gt;
    &lt;p&gt;But it is likely that many factors, such as the recipient’s and donor’s genetics, need to align in order for this to work, so that, for instance, the donor’s cells can rapidly destroy the recipient’s. What’s more, in the latest case, the man carried one copy of the CCR5 mutation, which could have altered how his immune cells were spread across the body in a way that made it easier to cure him of the virus, says Gaebler.&lt;/p&gt;
    &lt;p&gt;This means that most people receiving stem cell transplants for HIV and blood cancer should be offered HIV-resistant stem cells where possible, says Gaebler.&lt;/p&gt;
    &lt;p&gt;It’s also important to point out that cancer-free people with HIV won’t benefit from stem cell transplants, as it’s a very risky procedure that can lead to life-threatening infections, says Gaebler. Most people are better off taking ART – often in the form of daily pills – which is a much safer and convenient way to stop HIV from spreading, enabling people to enjoy long and healthy lives, he says. Moreover, a recently available drug called lenacapavir provides nearly complete protection against HIV with just two injections per year.&lt;/p&gt;
    &lt;p&gt;Nonetheless, efforts are being made to cure HIV by genetically editing immune cells, and prevent it using vaccines.&lt;/p&gt;
    &lt;p&gt;Journal reference:&lt;/p&gt;
    &lt;p&gt;Topics:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46119699</guid><pubDate>Tue, 02 Dec 2025 10:20:34 +0000</pubDate></item><item><title>Addressing the adding situation</title><link>https://xania.org/202512/02-adding-integers</link><description>&lt;doc fingerprint="aaebfc0e603b9ac8"&gt;
  &lt;main&gt;
    &lt;p&gt;Written by me, proof-read by an LLM. &lt;lb/&gt;Details at end.&lt;/p&gt;
    &lt;p&gt;Yesterday we saw how compilers zero registers efficiently. Today let’s look at something a tiny bit less trivial (though not by much): adding two integers. What do you think a simple x86 function to add two ints1 would look like? An &lt;code&gt;add&lt;/code&gt;, right? Let’s take a look!&lt;/p&gt;
    &lt;p&gt;Probably not what you were thinking, right? x86 is unusual in mostly having a maximum of two operands per instruction2. There’s no &lt;code&gt;add&lt;/code&gt; instruction to add &lt;code&gt;edi&lt;/code&gt; to &lt;code&gt;esi&lt;/code&gt;, putting the result in &lt;code&gt;eax&lt;/code&gt;. On an ARM machine this would be a simple &lt;code&gt;add r0, r0, r1&lt;/code&gt; or similar, as ARM has a separate destination operand. On x86, things like &lt;code&gt;add&lt;/code&gt; are not &lt;code&gt;result = lhs + rhs&lt;/code&gt; but &lt;code&gt;lhs += rhs&lt;/code&gt;. This can be a limitation, as we don’t get to control which register the result goes into, and we in fact lose the old value of &lt;code&gt;lhs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So how do compilers work around this limitation? The answer lies in an unexpected place - the sophisticated memory addressing system of the x86. Nearly every operand can be a memory reference - there’s no specific “load” or “store”; a &lt;code&gt;mov&lt;/code&gt; can just refer to memory directly. Those memory references are pretty rich: you can refer to memory addressed by a constant, relative to a register, or relative to a register plus an offset (optionally multiplied by 1, 2, 4 or 8). Something like &lt;code&gt;add eax, word ptr [rdi + rsi * 4 + 0x1000]&lt;/code&gt; is still a single instruction3!&lt;/p&gt;
    &lt;p&gt;Sometimes you don’t want to access the memory at one of these complex addresses, you just want to calculate what the address would be. Sort of like C’s “address-of” (&lt;code&gt;&amp;amp;&lt;/code&gt;) operator. That’s what &lt;code&gt;lea&lt;/code&gt; (Load Effective Address) does: it calculates the address without touching memory.&lt;/p&gt;
    &lt;p&gt;Why is this useful for addition? Well, if we’re not actually accessing memory, we can abuse the addressing hardware as a calculator! That complex addressing mode with its register-plus-register-times-scale is really just shifting and adding - so &lt;code&gt;lea&lt;/code&gt; becomes a cheeky way to do three-operand addition4.&lt;/p&gt;
    &lt;p&gt;The compiler writes our simple addition in terms of the address of memory at &lt;code&gt;rdi&lt;/code&gt; offset by &lt;code&gt;rsi&lt;/code&gt;. We get a full add of two registers and we get to specify the destination too. You’ll notice that the operands are referenced as &lt;code&gt;rdi&lt;/code&gt; and &lt;code&gt;rsi&lt;/code&gt; (the 64-bit version) even though we only wanted a 32-bit add: because we are using the memory addressing system it unconditionally calculates a 64-bit address. However, in this case it doesn’t matter; those top bits5 are discarded when the result is written to the 32-bit &lt;code&gt;eax&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;lea&lt;/code&gt; often saves an instruction, is useful if both of the operands are still needed later on in other calculations (as it leaves them unchanged), and can execute on x86’s multiple execution units in the same cycle. Compilers know this though, so you don’t have to worry!&lt;/p&gt;
    &lt;p&gt;See the video that accompanies this post.&lt;/p&gt;
    &lt;p&gt;This post is day 2 of Advent of Compiler Optimisations 2025, a 25-day series exploring how compilers transform our code.&lt;/p&gt;
    &lt;p&gt;This post was written by a human (Matt Godbolt) and reviewed and proof-read by LLMs and humans.&lt;/p&gt;
    &lt;p&gt;Support Compiler Explorer on Patreon or GitHub, or by buying CE products in the Compiler Explorer Shop.&lt;/p&gt;
    &lt;p&gt;The Linux system I’m compiling for here passes parameters in &lt;code&gt;edi&lt;/code&gt; and &lt;code&gt;esi&lt;/code&gt;, and expects the result in &lt;code&gt;eax&lt;/code&gt;. We’ll cover calling conventions later in the series. ↩&lt;/p&gt;
    &lt;p&gt;Though some AVX instructions and some multiplies do allow a separate destination. ↩&lt;/p&gt;
    &lt;p&gt;As someone who grew up with 6502, and then 32-bit ARM, coming to the x86 ISA was quite a shock. The x86 is truly a “Complex Instruction Set Computer”. ↩&lt;/p&gt;
    &lt;p&gt;Three-operand meaning we can specify two source registers and a separate destination, unlike &lt;code&gt;add&lt;/code&gt; which overwrites one of its operands. ↩&lt;/p&gt;
    &lt;p&gt;Those top bits should be zero, as the ABI requires it: the compiler relies on this here. Try editing the example above to pass and return &lt;code&gt;long&lt;/code&gt;s to compare. ↩&lt;/p&gt;
    &lt;p&gt;Matt Godbolt is a C++ developer living in Chicago. He works for Hudson River Trading on super fun but secret things. He is one half of the Two's Complement podcast. Follow him on Mastodon or Bluesky.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46120181</guid><pubDate>Tue, 02 Dec 2025 11:30:29 +0000</pubDate></item><item><title>A series of vignettes from my childhood and early career</title><link>https://www.jasonscheirer.com/weblog/vignettes/</link><description>&lt;doc fingerprint="7b999075bda747ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Series of Vignettes From My Childhood and Early Career&lt;/head&gt;
    &lt;p&gt;A short set of anecdotes, apropos of nothing.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Death of Software Engineering as a Profession&lt;/head&gt;
    &lt;p&gt;When I was younger, I really liked programming! I loved the sense of accomplishment, I loved the problem solving, I loved sharing what I made with the people around me to both amuse and assist.&lt;/p&gt;
    &lt;p&gt;One particularly wise adult (somewhere around 1996) took me aside and said, “You know, you’re lucky you enjoy programming, because you won’t be able to make a living on it in the future. Doing it for love over money is a good idea.”&lt;/p&gt;
    &lt;p&gt;“Coding is over, with Object Oriented programming one person who is much smarter than any of us could hope to be will develop the library just once and we will all use it going forward, forever. Once a problem is solved it never needs solving again.&lt;/p&gt;
    &lt;p&gt;“In 5 years there’s going to be a library of objects, like books on a bookshelf, and every software problem will be solved by business people just snapping the object libraries they need together like LEGOs. They won’t need you at all.”&lt;/p&gt;
    &lt;p&gt;I thought about this advice, and how Software Engineering would be ending by the time I entered school. I realized I had not even thought about my education yet. I was in middle school. Programming was not it, though, I knew that.&lt;/p&gt;
    &lt;p&gt;I’m here nearly 30 years later and software continues to pay my bills, despite everything. Open source exists, there are libraries I can use to piece things together to solve all the time. New problem sets not covered by the garden path come up all the time. Clicking the LEGOs together continues to be a hard task. Every time we fix it at one level of abstraction we operate one level higher and the world keeps turning.&lt;/p&gt;
    &lt;p&gt;Whenever I’m threatened with a good time and someone proclaims “this is it for you” all that happens is my job becomes more annoying. Haven’t gotten the sweet release of extinction quite yet.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Time Computing Changed Forever and Everyone Who Didn’t Move Got Left Behind&lt;/head&gt;
    &lt;p&gt;Around 1993 or so was the advent of the “Multimedia Age.” Multimedia was the buzzword. Software has to be multimedia ready. Education had to teach children to be ready for the multimedia age. If your tool, however inappropriate as it was, did not have multimedia features, you were going to be left behind. You needed a video guide. You needed to be on CD-ROM. This is just the new normal.&lt;/p&gt;
    &lt;p&gt;“Multimedia” just means “sound and video.” We had a high concept term for a very direct, low concept concept.&lt;/p&gt;
    &lt;p&gt;And the multimedia boom fizzled out. It became boring. Nobody is impressed by a video on a website and nobody thinks less of a website that doesn’t use sound and video if it’s not appropriate. You pop a &lt;code&gt;&amp;lt;video /&amp;gt;&lt;/code&gt; tag in your HTML and your job is done. The amazing thing became mundane. The dream of “multimedia” became commonplace and everyone just accepted it as normal. I’m not aware of any industries that collapsed dramatically due to multimedia. Nobody really reskilled. Video editing is still a pretty rare thing to find, and we don’t commonly have sound engineers working on the audio UX of software products.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Death of Software Engineering as a Profession: Again&lt;/head&gt;
    &lt;p&gt;In 2000 a coworker took me aside and showed me his brand-new copy of IntelliJ IDE. “It’s over for us,” he said, “this thing makes it so programmers aren’t strictly necessary, like one person can operate this tool and they can lay the rest of us off.”&lt;/p&gt;
    &lt;p&gt;I was pretty awestruck, he got some amazing autocomplete right in the IDE. Without having to have a separate JavaDocs window open to the side, and without having to manually open the page for the class he needed documentation on, it just was there inline. It gave him feedback before the compile cycle on a bunch of issues that you normally don’t see until build. That was a nice bit of preventative work and seemed to have the potential to keep a developer in flow longer.&lt;/p&gt;
    &lt;p&gt;And then he showed me the killer feature “that’s going to get us all out of a job:” the refactoring tools.&lt;/p&gt;
    &lt;p&gt;He then proceeded to show me the tools, easily moving around code to new files, renaming classes across the codebase, all kinds of manual things that would have taken a person a few days to do on their own. It was magical.&lt;/p&gt;
    &lt;p&gt;After some thought I said, “that’s amazing, but does it write new logic too or does it just move code around?”&lt;/p&gt;
    &lt;p&gt;He didn’t seem fazed by that, and doubled down on the insistence that these powerful tools were our doom. I made a distinction between “useful” code and “filler” code, but apparently what is valued is not the quality and nature of the code but its volume and presence. This tool definitely gave both volume and presence to the tiny human-written nuggets within.&lt;/p&gt;
    &lt;head rend="h1"&gt;That Time I Automated Someone Out of a Job&lt;/head&gt;
    &lt;p&gt;At my first job in High School I was working in an office in a suburban office park with programmers from many different local agencies. One guy I chatted up was a contractor: these people were highly regarded, somewhat feared specialists. The guy in question was working on a multi-year migration of some county health computer system from MUMPS to a more modern relational system. He showed me the main family of problems he was solving to show off how smart he was for solving them; they were largely rote problems of migrating table schemas and records in a pretty uniform way. But there were a lot of them, and he was working hard to meet his deadline!&lt;/p&gt;
    &lt;p&gt;I thought about it, and seeking his approval and validation, set out to help him. To show what I could do. I wrote a Python script that could solve the 85% case (it was mostly string manipulation) and even put a little TkInter dialog around it so he could select the files he wanted to migrate visually. It ran great, but he looked a little afraid when I demonstrated it to him:&lt;/p&gt;
    &lt;p&gt;“You didn’t show this to anyone else, did you?”&lt;/p&gt;
    &lt;p&gt;“Nope.”&lt;/p&gt;
    &lt;p&gt;“Oh thank God.”&lt;/p&gt;
    &lt;p&gt;I take it he used my tool because he had a lot more free time to goof off for the remaining six months of his contract. I don’t think he told anyone else what he had either, but I’m guessing that he had a lot more MUMPS migration contracts lined up when he could finish them in a matter of days.&lt;/p&gt;
    &lt;head rend="h1"&gt;That Time I Automated Myself Out of a Job&lt;/head&gt;
    &lt;p&gt;At the same job, I was paid to maintain a series of government agency web sites. One of my main tasks was to keep a list of mental health providers up-to-date on an HTML page and upload it to the server.&lt;/p&gt;
    &lt;p&gt;This process was pretty mechanical: take Excel sheet from inbox, open in Excel, copy Excel table to HTML table.&lt;/p&gt;
    &lt;p&gt;Within a month I had a fully automated workflow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I used Windows Automation to watch my Outlook inbox&lt;/item&gt;
      &lt;item&gt;When an email came in from the person who sent me the Excels it would download it&lt;/item&gt;
      &lt;item&gt;Open the Excel file in excel using Windows Automation&lt;/item&gt;
      &lt;item&gt;Export it to CSV from Excel (the automation did this, I simply watched a ghost remote control an Excel window that opened and closed itself)&lt;/item&gt;
      &lt;item&gt;Run a Python script that would inject that CSV data as an HTML table into the file&lt;/item&gt;
      &lt;item&gt;Run another Python script that would connect to the FTP server and upload the file. It would randomly pause and issue typos so it looked like the FTP session was being operated by a human at a keyboard so nobody thought anything on my plot.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I lived in fear of being found out, and told no one that the thing I was getting paid to do was no longer being done by me.&lt;/p&gt;
    &lt;p&gt;About 9 months later the department in question hired a full-time web developer for $45k/yr to bring their website in-house. I was costing them about $25/hr, probably skating under $2000/yr for my outsourced services. This was clearly not about money.&lt;/p&gt;
    &lt;p&gt;And what I feared did not happen. When I no longer had that work to sustain me my managers just put me on something else.&lt;/p&gt;
    &lt;p&gt;There’s always more work.&lt;/p&gt;
    &lt;head rend="h1"&gt;We Don’t Engage in Theft&lt;/head&gt;
    &lt;p&gt;In my last years of undergraduate education and my first couple of years out of college I worked on projects that did some sort of Natural Language Processing tasks. For these we required training data, and the more the better.&lt;/p&gt;
    &lt;p&gt;On that, though, we had responsibilities. We had to make sure the data we had also came with some sort of license or implicit permission. You didn’t just steal a pile of PDFs or scoop up a person’s web site and put it in your training set. There were ethical constrains, and legal consequences. You acted above-board when training your AI models.&lt;/p&gt;
    &lt;p&gt;There were times we’d train models on Wikipedia dumps. They were always comparatively amazing results when we trained on good, large data like that. Cogent. Interesting. Even a simple Markov chain on Wikipedia looked smart.&lt;/p&gt;
    &lt;p&gt;When we wrote web crawlers, we wrote them to respect &lt;code&gt;robots.txt&lt;/code&gt;. We kept them on local domains. The &lt;code&gt;user-agent&lt;/code&gt; field of the crawlers included our email address, and if an angry webmaster didn’t like the way we were crawling them we’d fix it. Getting crawled aggressively at once taxed servers and spammed logs so we’d space it out to hours or days. If their &lt;code&gt;robots.txt&lt;/code&gt; was missing or malformed and they still didn’t want us there, we’d block the site from crawling.&lt;/p&gt;
    &lt;p&gt;We made sure we had explicit permission to collect data for our training corpora.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Time Computing Changed Forever and Everyone Who Didn’t Move Got Left Behind: Again&lt;/head&gt;
    &lt;p&gt;The dot com boom was a crazy time. The internet has just become mainstream and there was a new gold rush. Money was there just for the taking, so many VC funded business plans were just “traditional business X, but on the internet!” and the money flowed. How it flowed.&lt;/p&gt;
    &lt;p&gt;Most of these companies, however, didn’t really have a solid business model other than buying some servers and a domain name and “we’ll put this thing on the internet.”&lt;/p&gt;
    &lt;p&gt;Out of this crash came green shoots: Web 2.0, which used the web natively, organically, gave a good web-native experience. Eventually the dream of the internet, the promise of the hype, was made manifest after a lot of people learned a lot of really unnecessary, really painful lessons. They spent less and put their things on the internet because they made sense on the internet of the present, not because the internet was the next big thing.&lt;/p&gt;
    &lt;p&gt;The dream of the widespread, ubiquitous internet came true, and there were very few fatalities. Some businesses died, but it was more glacial than volcanic in time scale. When ubiquitous online services became commonplace it just felt mundane. It didn’t feel forced. It was the opposite of the dot com boom just five years later: the internet is here and we’re here to build a solid business within it in contrast with we should put this solid business on the internet somehow, because it’s coming.&lt;/p&gt;
    &lt;head rend="h1"&gt;Closing&lt;/head&gt;
    &lt;p&gt;This is indeed a set of passive-aggressive jabs on the continuing assault on our senses by the LLM hype lobby.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46120549</guid><pubDate>Tue, 02 Dec 2025 12:28:34 +0000</pubDate></item><item><title>Lazier Binary Decision Diagrams for set-theoretic types</title><link>https://elixir-lang.org/blog/2025/12/02/lazier-bdds-for-set-theoretic-types/</link><description>&lt;doc fingerprint="eccf223ac2a79e54"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Lazier Binary Decision Diagrams (BDDs) for set-theoretic types&lt;/head&gt;
    &lt;p&gt;The Elixir team and the CNRS are working on a set-theoretic type system for Elixir which, simply put, is a type-system powered by unions, intersections, and negations. As part of the implementation of said type systems, we need an efficient way of representing said operations. This article discusses the existing approaches found in theory and practice, as well as the improvements we have introduced as part of Elixir v1.19.&lt;/p&gt;
    &lt;p&gt;This article covers the implementation details of the type system. You don’t need to understand these internals to use the type system, just as you don’t need to know virtual machine bytecodes or compiler passes to use a programming language. Our goal is to document our progress and provide guidance for future maintainers and implementers. Let’s get started.&lt;/p&gt;
    &lt;head rend="h2"&gt;DNFs - Disjunctive Normal Forms&lt;/head&gt;
    &lt;p&gt;A Disjunctive Normal Form (DNF) is a standardized way of expressing logical formulas using only disjunctions (unions) of conjunctions (intersections). In the context of set-theoretic type systems, DNFs provide a canonical representation for union and intersection types, represented respectively as &lt;code&gt;or&lt;/code&gt; and &lt;code&gt;and&lt;/code&gt; in Elixir.&lt;/p&gt;
    &lt;p&gt;In Elixir, we would represent those as lists of lists. Consider a type expression like &lt;code&gt;(A and B) or (C and D)&lt;/code&gt;. This is already in DNF, it’s a union of intersections, and it would be represented as: &lt;code&gt;[[A, B], [C, D]]&lt;/code&gt;. This means performing unions between two DNFs is a simple list concatenation:&lt;/p&gt;
    &lt;code&gt;def union(dnf1, dnf2), do: dnf1 ++ dnf2
&lt;/code&gt;
    &lt;p&gt;However, more complex expressions like &lt;code&gt;A and (B or C)&lt;/code&gt; need to be converted. Using distributive laws, this becomes &lt;code&gt;(A and B) or (A and C)&lt;/code&gt;, which is now in DNF. In other words, the intersection of DNFs is a Cartesian product:&lt;/p&gt;
    &lt;code&gt;def intersection(dnf1, dnf2) do
  for intersections1 &amp;lt;- dnf1,
      intersections2 &amp;lt;- dnf2 do
    intersections1 ++ intersections2
  end
end
&lt;/code&gt;
    &lt;p&gt;The advantage of DNFs is their simple structure. Every type can be represented as unions of intersecting terms, making operations like checking if a type is empty simply a matter of checking if all unions have at least one intersection that is empty:&lt;/p&gt;
    &lt;code&gt;def empty?(dnf) do
  Enum.all?(dnf, fn intersections -&amp;gt;
    Enum.any?(intersections, &amp;amp;empty_component?/1)
  end)
end
&lt;/code&gt;
    &lt;p&gt;On the other hand, the snippets above already help us build an intuition on the drawbacks of DNFs.&lt;/p&gt;
    &lt;p&gt;First, we have seen how intersections are Cartesian products, which can lead to exponential blow ups when performing the intersection of unions. For example, &lt;code&gt;(A₁ or A₂) and (B₁ or B₂) and (C₁ or C₂)&lt;/code&gt; leads to &lt;code&gt;(A₁ and B₁ and C₁) or (A₁ and B₁ and C₂) or (A₁ and B₂ and C₁) or ...&lt;/code&gt;, with 8 distinct unions.&lt;/p&gt;
    &lt;p&gt;Furthermore, if we implement unions as simple list concatenations, those unions can end up with duplicated entries, which exacerbates the exponential blow up when we perform intersections of these unions. This forces us to aggressively remove duplicates in unions, making it more complex and expensive than a concatenation.&lt;/p&gt;
    &lt;p&gt;Despite their limitations, DNFs served us well and were the data structure used as part of Elixir v1.17 and v1.18. However, since Elixir v1.19 introduced type inference of anonymous functions, negations became more prevalent in the type system, making exponential growth more frequent. Let’s understand why.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inferring anonymous functions&lt;/head&gt;
    &lt;p&gt;Imagine the following anonymous function:&lt;/p&gt;
    &lt;code&gt;fn
  %{full_name: full} -&amp;gt; "#{full}"
  %{first_name: first, last_name: last} -&amp;gt; "#{last}, #{first}"
end
&lt;/code&gt;
    &lt;p&gt;We can say the first clause accepts any map with the key &lt;code&gt;full_name&lt;/code&gt;. The second clause accepts any map with the keys &lt;code&gt;first_name&lt;/code&gt; and &lt;code&gt;last_name&lt;/code&gt; which DO NOT have the key &lt;code&gt;full_name&lt;/code&gt; (otherwise they would have matched the first clause). Therefore, the inferred type should be:&lt;/p&gt;
    &lt;code&gt;$ %{full_name: String.Chars.t()} -&amp;gt; String.t()
$ %{first_name: String.Chars.t(), last_name: String.Chars.t()} and not
    %{full_name: String.Chars.t()} -&amp;gt; String.t()
&lt;/code&gt;
    &lt;p&gt;As you can see, in order to express this type, we need a negation (&lt;code&gt;not&lt;/code&gt;). Or, more precisely, a difference since &lt;code&gt;A and not B&lt;/code&gt; is the same as &lt;code&gt;A - B&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Implementing negations/differences in DNFs is relatively straightforward. Instead of lists of lists, we now use lists of two-element tuples, where the first element is a list of positive types, and the second is a list of negative types. For example, previously we said &lt;code&gt;(A and B) or (C and D)&lt;/code&gt; would be represented as &lt;code&gt;[[A, B], [C, D]]&lt;/code&gt;, now it will be represented as:&lt;/p&gt;
    &lt;code&gt;[{[A, B], []}, {[C, D], []}]
&lt;/code&gt;
    &lt;p&gt;While &lt;code&gt;(A and not B) or C or D&lt;/code&gt; is represented as:&lt;/p&gt;
    &lt;code&gt;[{[A], [B]}, {[C], []}, {[D], []}]
&lt;/code&gt;
    &lt;p&gt;The difference between two DNFs is implemented similarly to intersections, except we now need to perform the Cartesian product over the positive and negative parts of each conjunction. And given anonymous functions have differences, inferring the types of anonymous functions are now exponentially expensive, which caused some projects to take minutes to compile. Not good!&lt;/p&gt;
    &lt;head rend="h2"&gt;BDDs - Binary Decision Diagrams&lt;/head&gt;
    &lt;p&gt;Luckily, those exact issues are well documented in literature and are addressed by Binary Decision Diagrams (BDDs), introduced by Alain Frisch (2004) and later recalled and expanded by Giuseppe Castagna (2016).&lt;/p&gt;
    &lt;p&gt;BDDs represent set-theoretic operations as an ordered tree. This requires us to provide an order, any order, across all types. Given all Elixir values have a total order, that’s quite straightforward. Furthermore, by ordering it, we can detect duplicates as we introduce nodes in the tree. The tree can have three distinct node types:&lt;/p&gt;
    &lt;code&gt;type bdd() = :top or :bottom or {type(), constrained :: bdd(), dual :: bdd()}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;:top&lt;/code&gt; represents the top type (where the intersection &lt;code&gt;type and :top&lt;/code&gt; returns &lt;code&gt;type&lt;/code&gt;) and &lt;code&gt;:bottom&lt;/code&gt; represents the bottom type (where the intersection &lt;code&gt;type and :bottom&lt;/code&gt; returns &lt;code&gt;:bottom&lt;/code&gt;). Non-leaf nodes are represented via a three-element tuple, where the first element is the type (what we have been calling &lt;code&gt;A&lt;/code&gt;,  &lt;code&gt;B&lt;/code&gt;… so far), the second element is called in literature the constrained branch, and the third element is the dual branch.&lt;/p&gt;
    &lt;p&gt;In order to compute the actual type of a non-leaf node, we need to compute &lt;code&gt;(type() and constrained()) or (not type() and dual())&lt;/code&gt; (hence the names constrained and dual). Let’s see some examples.&lt;/p&gt;
    &lt;p&gt;The type &lt;code&gt;A&lt;/code&gt; is represented as &lt;code&gt;{A, :top, :bottom}&lt;/code&gt;. This is because, if we compute &lt;code&gt;(A and :top) or (not A and :bottom)&lt;/code&gt;, we get &lt;code&gt;A or :bottom&lt;/code&gt;, which is equivalent to &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The type &lt;code&gt;not A&lt;/code&gt; is represented as &lt;code&gt;{A, :bottom, :top}&lt;/code&gt;, and it gives us &lt;code&gt;(A and :bottom) or (not A and :top)&lt;/code&gt;, which yields &lt;code&gt;:bottom or not A&lt;/code&gt;, which is equivalent to &lt;code&gt;not A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The type &lt;code&gt;A and B&lt;/code&gt;, assuming &lt;code&gt;A &amp;lt; B&lt;/code&gt; according to a total order, is represented as &lt;code&gt;{A, {B, :top, :bottom}, :bottom}&lt;/code&gt;. Expanding it node by node gives us:&lt;/p&gt;
    &lt;code&gt;(A and ((B and :top) or (not B and :bottom))) or (not A and :bottom)
(A and (B or :bottom)) or (not A and :bottom)
(A and B) or :bottom
(A and B)
&lt;/code&gt;
    &lt;p&gt;While the difference &lt;code&gt;A and not B&lt;/code&gt; is represented as &lt;code&gt;{A, {B, :bottom, :top}, :bottom}&lt;/code&gt;, which we also expand node by node:&lt;/p&gt;
    &lt;code&gt;(A and ((B and :bottom) or (not B and :top))) or (not A and :bottom)
(A and (:bottom or not B)) or (not A and :bottom)
(A and not B) or :bottom
(A and not B)
&lt;/code&gt;
    &lt;p&gt;Finally, the union &lt;code&gt;A or B&lt;/code&gt; is implemented as &lt;code&gt;{A, :top, {B, :top, :bottom}}&lt;/code&gt;. Let’s expand it:&lt;/p&gt;
    &lt;code&gt;(A and :top) or (not A and ((B and :top) or (not B and :bottom)))
(A and :top) or (not A and (B or :bottom))
A or (not A and B)
(A or not A) and (A or B)
:top and (A or B)
A or B
&lt;/code&gt;
    &lt;p&gt;In other words, Binary Decision Diagrams allow us to represent unions, intersections, and differences efficiently, removing the exponential blow up. Guillaume Duboc implemented them as part of Elixir v1.19, addressing the bottlenecks introduced as part of the new type system features… but unfortunately BDDs introduced new slow downs.&lt;/p&gt;
    &lt;p&gt;The issue with BDDs comes when applying unions to intersections and differences. Take the following type &lt;code&gt;(A and B) or C&lt;/code&gt;. Since we need to preserve the order &lt;code&gt;A &amp;lt; B &amp;lt; C&lt;/code&gt;, it would be represented as:&lt;/p&gt;
    &lt;code&gt;{A, {B, :top, {C, :top, :bottom}}, {C, :top, :bottom}}
&lt;/code&gt;
    &lt;p&gt;which can be expanded as:&lt;/p&gt;
    &lt;code&gt;(A and ((B and :top) or (not B and ((C and :top) or (not C and :bottom))))) or (not A and ((C and :top) or (not C and :bottom)))
(A and (B or (not B and C))) or (not A and C)
(A and (B or C)) or (not A and C)
(A and B) or (A and C) or (not A and C)
(A and B) or C
&lt;/code&gt;
    &lt;p&gt;As you can see, although the representation is correct, its expansion ends-up generating too many disjunctions. And while we can simplify them back to &lt;code&gt;(A and B) or C&lt;/code&gt; symbolically, doing such simplications in practice are too expensive.&lt;/p&gt;
    &lt;p&gt;In other words, the BDD expansion grows exponentially in size on consecutive unions, which is particularly troublesome because we must expand the BDD every time we check for emptiness or subtyping.&lt;/p&gt;
    &lt;p&gt;At the end of the day, it seems we traded faster intersections/differences for slower unions. Perhaps we can have our cake and eat it too?&lt;/p&gt;
    &lt;head rend="h2"&gt;BDDs with lazy unions (or ternary decision diagrams)&lt;/head&gt;
    &lt;p&gt;Luckily, the issue above was also forecast by Alain Frisch (2004), where he suggests an additional representation, called BDDs with lazy unions.&lt;/p&gt;
    &lt;p&gt;In a nutshell, we introduce a new element, called &lt;code&gt;uncertain&lt;/code&gt;, to each non-leaf node to represent unions:&lt;/p&gt;
    &lt;code&gt;type lazy_bdd() = :top or :bottom or
  {type(), constrained :: bdd(), uncertain :: bdd(), dual :: bdd()}
&lt;/code&gt;
    &lt;p&gt;We’ll refer to the &lt;code&gt;uncertain&lt;/code&gt; as unions going forward.&lt;/p&gt;
    &lt;p&gt;The type of each non-leaf node can be computed by &lt;code&gt;(type() and constrained()) or uncertain() or (not type() and dual())&lt;/code&gt;. Here are some examples:&lt;/p&gt;
    &lt;code&gt;A = {A, :top, :bottom, :bottom}
A and B = {A, {B, :top, :bottom, :bottom}, :bottom, :bottom}
A or B = {A, :top, {B, :top, :bottom, :bottom}, :bottom}
&lt;/code&gt;
    &lt;p&gt;And, going back to &lt;code&gt;(A and B) or C&lt;/code&gt;, it can be represented as:&lt;/p&gt;
    &lt;code&gt;{A, {B, :top, :bottom, :bottom}, {C, :top, :bottom, :bottom}, :bottom}
&lt;/code&gt;
    &lt;p&gt;The duplication of &lt;code&gt;C&lt;/code&gt; is fully removed. With our new representation in hand, the next step is to implement union, intersection, and difference of lazy BDDs, using the formulas found in literature and described below.&lt;/p&gt;
    &lt;p&gt;Assuming that a lazy BDD &lt;code&gt;B&lt;/code&gt; is represented as &lt;code&gt;{a, C, U, D}&lt;/code&gt;, and therefore &lt;code&gt;B1 = {a1, C1, U1, D2}&lt;/code&gt; and &lt;code&gt;B2 = {a2, C2, U2, D2}&lt;/code&gt;, the union of the lazy BDDs &lt;code&gt;B1 or B2&lt;/code&gt; can be computed as:&lt;/p&gt;
    &lt;code&gt;{a1, C1 or C2, U1 or U2, D1 or D2} when a1 == a2
{a1, C1, U1 or B2, D1} when a1 &amp;lt; a2
{a2, C2, B1 or U2, D2} when a1 &amp;gt; a2
&lt;/code&gt;
    &lt;p&gt;The intersection &lt;code&gt;B1 and B2&lt;/code&gt; is:&lt;/p&gt;
    &lt;code&gt;{a1, (C1 or U1) and (C2 or U2), :bottom, (D1 or U1) and (D2 or U2)} when a1 == a2
{a1, C1 and B2, U1 and B2, D1 and B2} when a1 &amp;lt; a2
{a2, B1 and C2, B1 and U2, B1 and D2} when a1 &amp;gt; a2
&lt;/code&gt;
    &lt;p&gt;The difference &lt;code&gt;B1 and not B2&lt;/code&gt; is:&lt;/p&gt;
    &lt;code&gt;{a1, (C1 or U1) and not (C2 or U2), :bottom, (D1 or U1) and not (D2 or U2)} when a1 == a2
{a1, (C1 or U1) and not B2, :bottom, (D1 or U1) and not B2} when a1 &amp;lt; a2
{a2, B1 and not (C2 or U2), :bottom, B1 and not (D2 or U2)} when a1 &amp;gt; a2
&lt;/code&gt;
    &lt;p&gt;Guillaume Duboc first implemented lazy BDDs to represent our function types, addressing some of the bottlenecks introduced alongside BDDs. Afterwards, we attempted to convert all types to use lazy BDDs, hoping they would address the remaining bottlenecks, but that was not the case. There were still some projects that type checked instantaneously in Elixir v1.18 (which used DNFs) but took minutes on v1.19 release candidates, which could only point to large unions still being the root cause. However, weren’t lazy BDDs meant to address the issue with unions?&lt;/p&gt;
    &lt;p&gt;That was the question ringing in Guillaume’s head and in mine after an hours-long conversation, when we decided to call it a day. Unbeknownst to each other, we both continued working on the problem that night and the following morning. Separately, we were both able to spot the issue and converge on the same solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lazier BDDs (for intersections)&lt;/head&gt;
    &lt;p&gt;If you carefully look at the formulas above, you can see that intersections and differences of equal nodes cause a distribution of unions. Here is the intersection:&lt;/p&gt;
    &lt;code&gt;{a1, (C1 or U1) and (C2 or U2), :bottom, (D1 or U1) and (D2 or U2)} when a1 == a2
&lt;/code&gt;
    &lt;p&gt;Notice how U1 and U2 now appear on both constrained and dual parts and the whole union part of the node disappeared, now listed simply as &lt;code&gt;:bottom&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In addition, considering the common case where &lt;code&gt;C1 = C2 = :top&lt;/code&gt; and &lt;code&gt;D1 = D2 = :bottom&lt;/code&gt;, the node above becomes &lt;code&gt;{a1, :top, :bottom, U1 and U2}&lt;/code&gt;, which effectively moves the unions to the dual part. If you play close attention to it, since the uncertain is now &lt;code&gt;:bottom&lt;/code&gt;, we reverted back to the original BDD representation. Any further &lt;code&gt;union&lt;/code&gt; on those nodes will behave exactly as in the non-lazy BDDs, which we know to be problematic.&lt;/p&gt;
    &lt;p&gt;In other words, certain operations on lazy BDDs cause unions to revert to the previous BDD representation. So it seems lazy BDDs are not lazy enough? Could we stop this from happening?&lt;/p&gt;
    &lt;p&gt;Guillaume and I arrived at a new formula using different approaches. Given Guillaume’s approach can also be used to optimize differences, that’s the one I will show below. In particular, we know the intersection of equal nodes is implemented as:&lt;/p&gt;
    &lt;code&gt;{a1, (C1 or U1) and (C2 or U2), :bottom, (D1 or U1) and (D2 or U2)} when a1 == a2
&lt;/code&gt;
    &lt;p&gt;If we distribute the intersection in the constrained part, we get:&lt;/p&gt;
    &lt;code&gt;(C1 and C2) or (C1 and U2) or (U1 and C2) or (U1 and U2)
&lt;/code&gt;
    &lt;p&gt;If we distribute the intersection in the dual part, we get:&lt;/p&gt;
    &lt;code&gt;(D1 and D2) or (D1 and U2) or (U1 and D2) or (U1 and U2)
&lt;/code&gt;
    &lt;p&gt;We can clearly see both parts have &lt;code&gt;U1 and U2&lt;/code&gt;, which we can then move to the union! Leaving us with:&lt;/p&gt;
    &lt;code&gt;{a1,
 (C1 and C2) or (C1 and U2) or (U1 and C2),
 (U1 and U2),
 (D1 and D2) or (D1 and U2) or (U1 and D2)} when a1 == a2
&lt;/code&gt;
    &lt;p&gt;We can then factor out &lt;code&gt;C1&lt;/code&gt; in the constrained and &lt;code&gt;D1&lt;/code&gt; in the dual (or &lt;code&gt;C2&lt;/code&gt; and &lt;code&gt;D2&lt;/code&gt; respectively), resulting in:&lt;/p&gt;
    &lt;code&gt;{a1,
 (C1 and (C2 or U2)) or (U1 and C2),
 (U1 and U2),
 (D1 and (D2 or U2)) or (U1 and D2)} when a1 == a2
&lt;/code&gt;
    &lt;p&gt;While this new formula requires more operations, if we consider the common case &lt;code&gt;C1 = C2 = :top&lt;/code&gt; and &lt;code&gt;D1 = D2 = :bottom&lt;/code&gt;, we now have &lt;code&gt;{a1, :top, U1 and U2, :bottom}&lt;/code&gt;, with the unions perfectly preserved in the middle. We independently implemented this formula and noticed it addressed all remaining bottlenecks!&lt;/p&gt;
    &lt;head rend="h2"&gt;Lazier BDDs (for differences)&lt;/head&gt;
    &lt;p&gt;The issues we outlined above for intersections are even worse for differences. Let’s check the difference formula:&lt;/p&gt;
    &lt;code&gt;{a1, (C1 or U1) and not (C2 or U2), :bottom, (D1 or U1) and not (D2 or U2)} when a1 == a2
{a1, (C1 or U1) and not B2, :bottom, (D1 or U1) and not B2} when a1 &amp;lt; a2
{a2, B1 and not (C2 or U2), :bottom, B1 and not (D2 or U2)} when a1 &amp;gt; a2
&lt;/code&gt;
    &lt;p&gt;As you can see, all operations shuffle the union nodes and return &lt;code&gt;:bottom&lt;/code&gt;. But this time, we know how to improve it! Let’s start with &lt;code&gt;a1 == a2&lt;/code&gt;. If we expand the difference in the constrained part, we get:&lt;/p&gt;
    &lt;code&gt;(C1 and not C2 and not U2) or (U1 and not C2 and not U2)
&lt;/code&gt;
    &lt;p&gt;If we do the same in the dual part, we have:&lt;/p&gt;
    &lt;code&gt;(D1 and not D2 and not U2) or (U1 and not D2 and not U2)
&lt;/code&gt;
    &lt;p&gt;Unfortunately, there are no shared union terms between the constrained and dual parts, unless C2 and D2 are &lt;code&gt;:bottom&lt;/code&gt;. Therefore, instead of fully rewriting the difference of equal nodes, we add the following special case:&lt;/p&gt;
    &lt;code&gt;{a1, C1 and not U2, U1 and not U2, D1 and not U2}
when a1 == a2 and C2 == :bottom and D2 == :bottom
&lt;/code&gt;
    &lt;p&gt;We can apply a similar optimization when &lt;code&gt;a1 &amp;lt; a2&lt;/code&gt;. The current formula:&lt;/p&gt;
    &lt;code&gt;{a1, (C1 or U1) and not B2, :bottom, (D1 or U1) and not B2} when a1 &amp;lt; a2
&lt;/code&gt;
    &lt;p&gt;The constrained part can be written as &lt;code&gt;(C1 and not B2) or (U1 and not B2)&lt;/code&gt; and the dual part as &lt;code&gt;(D1 and not B2) or (U1 and not B2)&lt;/code&gt;. Given &lt;code&gt;(U1 and not B2)&lt;/code&gt; is shared on both parts, we can also convert it to a union, resulting in:&lt;/p&gt;
    &lt;code&gt;{a1, C1 and not B2, U1 and not B2, D1 and not B2} when a1 &amp;lt; a2
&lt;/code&gt;
    &lt;p&gt;Unfortunately, we can’t apply this for &lt;code&gt;a2 &amp;gt; a1&lt;/code&gt;, as differences are asymmetric and do not distribute over unions on the right side. Therefore, the updated formula for difference is:&lt;/p&gt;
    &lt;code&gt;{a1, C1 and not U2, U1 and not U2, D1 and not U2} when a1 == a2 and C2 == :bottom and D2 == :bottom
{a1, (C1 or U1) and not (C2 or U2), :bottom, (D1 or U1) and not (D2 or U2)} when a1 == a2
{a1, C1 and not B2, U1 and not B2, D1 and not B2} when a1 &amp;lt; a2
{a2, B1 and not (C2 or U2), :bottom, B1 and not (D2 or U2)} when a1 &amp;gt; a2
&lt;/code&gt;
    &lt;p&gt;With these new formulas, all new typing features in Elixir v1.19 perform efficiently and most projects now type check faster than in Elixir v1.18. We have also been able to use the rules above to derive additional optimizations for differences, such as when &lt;code&gt;a1 == a2 and U2 == :bottom&lt;/code&gt;, which will be part of future releases. Hooray!&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;As there is an increasing interest in implementing set-theoretic types for other dynamic languages, we hope this article shines a brief light on the journey and advancements made by the research and Elixir teams when it comes to representing set-theoretic types.&lt;/p&gt;
    &lt;p&gt;The type system was made possible thanks to a partnership between CNRS and Remote. The development work is currently sponsored by Fresha and Tidewave.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46120603</guid><pubDate>Tue, 02 Dec 2025 12:37:47 +0000</pubDate></item><item><title>Python Data Science Handbook</title><link>https://jakevdp.github.io/PythonDataScienceHandbook/</link><description>&lt;doc fingerprint="a7599437eedc495a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Python Data Science Handbook&lt;/head&gt;
    &lt;p&gt;Jake VanderPlas&lt;/p&gt;
    &lt;p&gt;This website contains the full text of the Python Data Science Handbook by Jake VanderPlas; the content is available on GitHub in the form of Jupyter notebooks.&lt;/p&gt;
    &lt;p&gt;The text is released under the CC-BY-NC-ND license, and code is released under the MIT license.&lt;/p&gt;
    &lt;p&gt;If you find this content useful, please consider supporting the work by buying the book!&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Preface¶&lt;/head&gt;
    &lt;head rend="h3"&gt;1. IPython: Beyond Normal Python¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Help and Documentation in IPython&lt;/item&gt;
      &lt;item&gt;Keyboard Shortcuts in the IPython Shell&lt;/item&gt;
      &lt;item&gt;IPython Magic Commands&lt;/item&gt;
      &lt;item&gt;Input and Output History&lt;/item&gt;
      &lt;item&gt;IPython and Shell Commands&lt;/item&gt;
      &lt;item&gt;Errors and Debugging&lt;/item&gt;
      &lt;item&gt;Profiling and Timing Code&lt;/item&gt;
      &lt;item&gt;More IPython Resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2. Introduction to NumPy¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Understanding Data Types in Python&lt;/item&gt;
      &lt;item&gt;The Basics of NumPy Arrays&lt;/item&gt;
      &lt;item&gt;Computation on NumPy Arrays: Universal Functions&lt;/item&gt;
      &lt;item&gt;Aggregations: Min, Max, and Everything In Between&lt;/item&gt;
      &lt;item&gt;Computation on Arrays: Broadcasting&lt;/item&gt;
      &lt;item&gt;Comparisons, Masks, and Boolean Logic&lt;/item&gt;
      &lt;item&gt;Fancy Indexing&lt;/item&gt;
      &lt;item&gt;Sorting Arrays&lt;/item&gt;
      &lt;item&gt;Structured Data: NumPy's Structured Arrays&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;3. Data Manipulation with Pandas¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Introducing Pandas Objects&lt;/item&gt;
      &lt;item&gt;Data Indexing and Selection&lt;/item&gt;
      &lt;item&gt;Operating on Data in Pandas&lt;/item&gt;
      &lt;item&gt;Handling Missing Data&lt;/item&gt;
      &lt;item&gt;Hierarchical Indexing&lt;/item&gt;
      &lt;item&gt;Combining Datasets: Concat and Append&lt;/item&gt;
      &lt;item&gt;Combining Datasets: Merge and Join&lt;/item&gt;
      &lt;item&gt;Aggregation and Grouping&lt;/item&gt;
      &lt;item&gt;Pivot Tables&lt;/item&gt;
      &lt;item&gt;Vectorized String Operations&lt;/item&gt;
      &lt;item&gt;Working with Time Series&lt;/item&gt;
      &lt;item&gt;High-Performance Pandas: eval() and query()&lt;/item&gt;
      &lt;item&gt;Further Resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;4. Visualization with Matplotlib¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple Line Plots&lt;/item&gt;
      &lt;item&gt;Simple Scatter Plots&lt;/item&gt;
      &lt;item&gt;Visualizing Errors&lt;/item&gt;
      &lt;item&gt;Density and Contour Plots&lt;/item&gt;
      &lt;item&gt;Histograms, Binnings, and Density&lt;/item&gt;
      &lt;item&gt;Customizing Plot Legends&lt;/item&gt;
      &lt;item&gt;Customizing Colorbars&lt;/item&gt;
      &lt;item&gt;Multiple Subplots&lt;/item&gt;
      &lt;item&gt;Text and Annotation&lt;/item&gt;
      &lt;item&gt;Customizing Ticks&lt;/item&gt;
      &lt;item&gt;Customizing Matplotlib: Configurations and Stylesheets&lt;/item&gt;
      &lt;item&gt;Three-Dimensional Plotting in Matplotlib&lt;/item&gt;
      &lt;item&gt;Geographic Data with Basemap&lt;/item&gt;
      &lt;item&gt;Visualization with Seaborn&lt;/item&gt;
      &lt;item&gt;Further Resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;5. Machine Learning¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What Is Machine Learning?&lt;/item&gt;
      &lt;item&gt;Introducing Scikit-Learn&lt;/item&gt;
      &lt;item&gt;Hyperparameters and Model Validation&lt;/item&gt;
      &lt;item&gt;Feature Engineering&lt;/item&gt;
      &lt;item&gt;In Depth: Naive Bayes Classification&lt;/item&gt;
      &lt;item&gt;In Depth: Linear Regression&lt;/item&gt;
      &lt;item&gt;In-Depth: Support Vector Machines&lt;/item&gt;
      &lt;item&gt;In-Depth: Decision Trees and Random Forests&lt;/item&gt;
      &lt;item&gt;In Depth: Principal Component Analysis&lt;/item&gt;
      &lt;item&gt;In-Depth: Manifold Learning&lt;/item&gt;
      &lt;item&gt;In Depth: k-Means Clustering&lt;/item&gt;
      &lt;item&gt;In Depth: Gaussian Mixture Models&lt;/item&gt;
      &lt;item&gt;In-Depth: Kernel Density Estimation&lt;/item&gt;
      &lt;item&gt;Application: A Face Detection Pipeline&lt;/item&gt;
      &lt;item&gt;Further Machine Learning Resources&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46120611</guid><pubDate>Tue, 02 Dec 2025 12:38:28 +0000</pubDate></item><item><title>Proximity to coworkers increases long-run development, lowers short-term output</title><link>https://pallais.scholars.harvard.edu/publications/power-proximity-coworkers-training-tomorrow-or-productivity-today</link><description>&lt;doc fingerprint="bd199c9eb1166645"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Power of Proximity to Coworkers: Training for Tomorrow or Productivity Today?&lt;/head&gt;
    &lt;head rend="h4"&gt;Publication information:&lt;/head&gt;
    &lt;head rend="h3"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Amidst the rise of remote work, we ask: what are the effects of proximity to coworkers? We find being near coworkers has tradeoffs: proximity increases long-run human capital development at the expense of short-term output. We study software engineers at a Fortune 500 firm, whose main campus has two buildings several blocks apart. When offices were open, engineers working in the same building as all their teammates received 22 percent more online feedback than engineers with distant teammates. After offices closed for COVID-19, this advantage largely disappears. Yet sitting together reduces engineers' programming output, particularly for senior engineers. The tradeoffs from proximity are more acute for women, who both do more mentoring and receive more mentorship when near their coworkers. Proximity impacts career trajectories, dampening short-run pay raises but boosting them in the long run. These results can help to explain national trends: workers in their twenties who often need mentorship and workers over forty who often provide mentorship are more likely to return to the office. However, even if most mentors and mentees go into the office, remote work may reduce interaction: pre-COVID, having just one distant teammate reduced feedback among co-located workers.&lt;/p&gt;
    &lt;p&gt;Amidst the rise of remote work, we ask: what are the effects of proximity to coworkers? We find being near coworkers has tradeoffs: proximity increases long-run human capital development at the expense of short-term output. We study software engineers at a Fortune 500 firm, whose main campus has two buildings several blocks apart. When offices were open, engineers working in the same building as all their teammates received 22 percent more online feedback than engineers with distant teammates. After offices closed for COVID-19, this advantage largely disappears. Yet sitting together reduces engineers' programming output, particularly for senior engineers. The tradeoffs from proximity are more acute for women, who both do more mentoring and receive more mentorship when near their coworkers. Proximity impacts career trajectories, dampening short-run pay raises but boosting them in the long run. These results can help to explain national trends: workers in their twenties who often need mentorship and workers over forty who often provide mentorship are more likely to return to the office. However, even if most mentors and mentees go into the office, remote work may reduce interaction: pre-COVID, having just one distant teammate reduced feedback among co-located workers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Full text&lt;/head&gt;
    &lt;p&gt;Revise and resubmit, Quarterly Journal of Economics&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes&lt;/head&gt;
    &lt;p&gt;Revise and resubmit, Quarterly Journal of Economics&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46121243</guid><pubDate>Tue, 02 Dec 2025 14:01:05 +0000</pubDate></item><item><title>Zig's new plan for asynchronous programs</title><link>https://lwn.net/SubscriberLink/1046084/4c048ee008e1c70e/</link><description>&lt;doc fingerprint="be00d845634f13c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Zig's new plan for asynchronous programs&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;p&gt;The designers of the Zig programming language have been working to find a suitable design for asynchronous code for some time. Zig is a carefully minimalist language, and its initial design for asynchronous I/O did not fit well with its other features. Now, the project has announced (in a Zig SHOWTIME video) a new approach to asynchronous I/O that promises to solve the function coloring problem, and allows writing code that will execute correctly using either synchronous or asynchronous I/O.&lt;/p&gt;
    &lt;p&gt;In many languages (including Python, JavaScript, and Rust), asynchronous code uses special syntax. This can make it difficult to reuse code between synchronous and asynchronous parts of a program, introducing a number of headaches for library authors. Languages that don't make a syntactical distinction (such as Haskell) essentially solve the problem by making everything asynchronous, which typically requires the language's runtime to bake in ideas about how programs are allowed to execute.&lt;/p&gt;
    &lt;p&gt;Neither of those options was deemed suitable for Zig. Its designers wanted to find an approach that did not add too much complexity to the language, that still permitted fine control over asynchronous operations, and that still made it relatively painless to actually write high-performance event-driven I/O. The new approach solves this by hiding asynchronous operations behind a new generic interface, Io.&lt;/p&gt;
    &lt;quote&gt;No AI slop, all substance: subscribe to LWN today&lt;p&gt;LWN has always been about quality over quantity; we need your help to continue publishing in-depth, reader-focused articles about Linux and the free-software community. Please subscribe today to support our work and keep LWN on the air; we are offering a free one-month trial subscription to get you started.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Any function that needs to perform an I/O operation will need to have access to an instance of the interface. Typically, that is provided by passing the instance to the function as a parameter, similar to Zig's Allocator interface for memory allocation. The standard library will include two built-in implementations of the interface: Io.Threaded and Io.Evented. The former uses synchronous operations except where explicitly asked to run things in parallel (with a special function; see below), in which case it uses threads. The latter (which is still a work-in-progress) uses an event loop and asynchronous I/O. Nothing in the design prevents a Zig programmer from implementing their own version, however, so Zig's users retain their fine control over how their programs execute.&lt;/p&gt;
    &lt;p&gt;Loris Cro, one of Zig's community organizers, wrote an explanation of the new behavior to justify the approach. Synchronous code is not much changed, other than using the standard library functions that have moved under Io, he explained. Functions like the example below, which don't involve explicit asynchronicity, will continue to work. This example creates a file, sets the file to close at the end of the function, and then writes a buffer of data to the file. It uses Zig's try keyword to handle errors, and defer to ensure the file is closed. The return type, !void, indicates that it could return an error, but doesn't return any data:&lt;/p&gt;
    &lt;quote&gt;const std = @import("std"); const Io = std.Io; fn saveFile(io: Io, data: []const u8, name: []const u8) !void { const file = try Io.Dir.cwd().createFile(io, name, .{}); defer file.close(io); try file.writeAll(io, data); }&lt;/quote&gt;
    &lt;p&gt;If this function is given an instance of Io.Threaded, it will create the file, write data to it, and then close it using ordinary system calls. If it is given an instance of Io.Evented, it will instead use io_uring, kqueue, or some other asynchronous backend suitable to the target operating system. In doing so, it might pause the current execution and go work on a different asynchronous function. Either way, the operation is guaranteed to be complete by the time writeAll() returns. A library author writing a function that involves I/O doesn't need to care about which of these things the ultimate user of the library chooses to do.&lt;/p&gt;
    &lt;p&gt;On the other hand, suppose that a program wanted to save two files. These operations could profitably be done in parallel. If a library author wanted to enable that, they could use the Io interface's async() function to express that it does not matter which order the two files are saved in:&lt;/p&gt;
    &lt;quote&gt;fn saveData(io: Io, data: []const u8) !void { // Calls saveFile(io, data, "saveA.txt") var a_future = io.async(saveFile, .{io, data, "saveA.txt"}); var b_future = io.async(saveFile, .{io, data, "saveB.txt"}); const a_result = a_future.await(io); const b_result = b_future.await(io); try a_result; try b_result; const out: Io.File = .stdout(); try out.writeAll(io, "save complete"); }&lt;/quote&gt;
    &lt;p&gt;When using an Io.Threaded instance, the async() function doesn't actually do anything asynchronously — it just runs the provided function right away. So, with that version of the interface, the function first saves file A and then file B. With an Io.Evented instance, the operations are actually asynchronous, and the program can save both files at once.&lt;/p&gt;
    &lt;p&gt;The real advantage of this approach is that it turns asynchronous code into a performance optimization. The first version of a program or library can write normal straight-line code. Later, if asynchronicity proves to be useful for performance, the author can come back and write it using asynchronous operations. If the ultimate user of the function has not enabled asynchronous execution, nothing changes. If they have, though, the function becomes faster transparently — nothing about the function signature or how it interacts with the rest of the code base changes.&lt;/p&gt;
    &lt;p&gt;One problem, however, is with programs where two parts are actually required to execute simultaneously for correctness. For example, suppose that a program wants to listen for connections on a port and simultaneously respond to user input. In that scenario, it wouldn't be correct to wait for a connection and only then ask for user input. For that use case, the Io interface provides a separate function, asyncConcurrent() that explicitly asks for the provided function to be run in parallel. Io.Threaded uses a thread in a thread pool to accomplish this. Io.Evented treats it exactly the same as a normal call to async().&lt;/p&gt;
    &lt;quote&gt;const socket = try openServerSocket(io); var server = try io.asyncConcurrent(startAccepting, .{io, socket}); defer server.cancel(io) catch {}; try handleUserInput(io);&lt;/quote&gt;
    &lt;p&gt;If the programmer uses async() where they should have used asyncConcurrent(), that is a bug. Zig's new model does not (and cannot) prevent programmers from writing incorrect code, so there are still some subtleties to keep in mind when adapting existing Zig code to use the new interface.&lt;/p&gt;
    &lt;p&gt; The style of code that results from this design is a bit more verbose than languages that give asynchronous functions special syntax, but Andrew Kelley, creator of the language, said that "&lt;quote&gt;it reads like standard, idiomatic Zig code.&lt;/quote&gt;" In particular, he noted that this approach lets the programmer use all of Zig's typical control-flow primitives, such as try and defer; it doesn't introduce any new language features specific to asynchronous code. &lt;/p&gt;
    &lt;p&gt;To demonstrate this, Kelley gave an example of using the new interface to implement asynchronous DNS resolution. The standard getaddrinfo() function for querying DNS information falls short because, although it makes requests to multiple servers (for IPv4 and IPv6) in parallel, it waits for all of the queries to complete before returning an answer. Kelley's example Zig code returns the first successful answer, canceling the other inflight requests.&lt;/p&gt;
    &lt;p&gt;Asynchronous I/O in Zig is far from done, however. Io.Evented is still experimental, and doesn't have implementations for all supported operating systems yet. A third kind of Io, one that is compatible with WebAssembly, is planned (although, as that issue details, implementing it depends on some other new language features). The original pull request for Io lists 24 planned follow-up items, most of which still need work.&lt;/p&gt;
    &lt;p&gt;Still, the overall design of asynchronous code in Zig appears to be set. Zig has not yet had its 1.0 release, because the community is still experimenting with the correct way to implement many features. Asynchronous I/O was one of the larger remaining priorities (along with native code generation, which was also enabled by default for debug builds on some architectures this year). Zig seems to be steadily working its way toward a finished design — which should decrease the number of times Zig programmers are asked to rewrite their I/O because the interface has changed again.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46121539</guid><pubDate>Tue, 02 Dec 2025 14:31:16 +0000</pubDate></item><item><title>Nixtml: Static website and blog generator written in Nix</title><link>https://github.com/arnarg/nixtml</link><description>&lt;doc fingerprint="174f82137de7eda4"&gt;
  &lt;main&gt;
    &lt;p&gt;A static website generator written in nix. Inspired by hugo.&lt;/p&gt;
    &lt;code&gt;{
  description = "My website generated using nixtml.";

  inputs = {
    nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
    flake-utils.url = "github:numtide/flake-utils";
    nixtml.url = "github:arnarg/nixtml";
  };

  outputs =
    {
      self,
      nixpkgs,
      flake-utils,
      nixtml,
    }:
    (flake-utils.lib.eachDefaultSystem (
      system:
      let
        pkgs = import nixpkgs { inherit system; };
      in
      {
        packages.blog = nixtml.lib.mkWebsite {
          inherit pkgs;

          name = "my-blog";
          baseURL = "https://my-blog.com";

          # Arbitrary metdata to be used in
          # templates.
          metadata = {
            lang = "en";
            title = "My Blog";
            description = "This is my blog";
          };

          # Walk a directory of markdown files
          # and create a page for each of them.
          content.dir = ./content; 

          # Copy an entire directory and symlink
          # in the final website derivation.
          static.dir = ./static;

          # Collections are for paginating content
          # and generating RSS feeds.
          collections.blog = {
            path = "posts";

            # Posts in the collection should be
            # grouped by optional tags in posts'
            # frontmatter.
            taxonomies = [ "tags" ];
          };

          # Import any nixtml modules (good for
          # "themes").
          imports = [ ./theme.nix ];
        };

        # Quickly build and serve website with
        # `nix run .#serve`.
        apps.serve = {
          type = "app";
          program =
            (pkgs.writeShellScript "serve-blog" ''
              ${pkgs.python3}/bin/python -m http.server -d ${self.packages.${system}.blog} 8080
            '').outPath;
        };
      }
    ));
}&lt;/code&gt;
    &lt;p&gt;Templates should be defined in modules under &lt;code&gt;website.layouts&lt;/code&gt;. All templates should be a function to a string (or list of strings, that is automatically coerced to a string).&lt;/p&gt;
    &lt;p&gt;In nixtml's lib there are functions for most commonly used HTML tags which can be used like this:&lt;/p&gt;
    &lt;code&gt;{lib, ...}: let
  inherit (lib.tags)
    html
    head
    body
    div
    ;
  inherit (lib) attrs;
in {
  website.layouts.base =
    { path, content, ... }@context:
    "&amp;lt;!DOCTYPE html&amp;gt;\n"
    +
      html
        [ (attrs.lang metadata.lang) ]
        [
          (head [ ] [ (partials.head context) ])
          (body
            [
              (attrs.classes [
                "font-sans"
                "bg-white"
              ])
            ]
            [
              (div
                [
                  (attrs.classes [ "container" ])
                ]
                [ content ]
              )
            ]
          )
        ];
}&lt;/code&gt;
    &lt;p&gt;The above is equivalent to defining the markup using strings in nix:&lt;/p&gt;
    &lt;code&gt;{lib, ...}: {
  website.layouts.base =
    { path, content, ... }@context: ''
      &amp;lt;!DOCTYPE html&amp;gt;
      &amp;lt;html lang="${metadata.lang}"&amp;gt;
        &amp;lt;head&amp;gt;
          ${partials.head context}
        &amp;lt;/head&amp;gt;
        &amp;lt;body class="font-sant bg-white"&amp;gt;
          &amp;lt;div class="container"&amp;gt;
            ${content}
          &amp;lt;/div&amp;gt;
        &amp;lt;/body&amp;gt;
      &amp;lt;/html&amp;gt;
    '';
}&lt;/code&gt;
    &lt;p&gt;Each template in &lt;code&gt;website.layouts&lt;/code&gt; has a specific purpose.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;website.layouts.base&lt;/code&gt;: Used for the skeleton of each HTML file for the website. It gets passed the result of other rendered templates.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;website.layouts.hom&lt;/code&gt;: Used for&lt;code&gt;./index.md&lt;/code&gt;, if found in&lt;code&gt;website.content.dir&lt;/code&gt;. It gets passed the metadata in the markdown frontmatter as well as the HTML content generated from markdown.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;website.layouts.page&lt;/code&gt;: Used for any other markdown file found in&lt;code&gt;website.content.dir&lt;/code&gt;. It gets passed the metadata in the markdown frontmatter as well as the HTML content generated from markdown.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;website.layouts.collection&lt;/code&gt;: Used for pagination pages for collections.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;website.layouts.taxonomy&lt;/code&gt;: Used for pagination pages for taxonomies in collections.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;website.layouts.partials&lt;/code&gt;: An attribute set of templates (functions to string or list of strings) that can be used to reduce repitition in the other standard templates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By setting &lt;code&gt;website.content.dir&lt;/code&gt; nixtml will traverse that directory, transform any markdown file it finds and output an HTML file in the final website derivation with the same path. For example, &lt;code&gt;${content.dir}/about.md&lt;/code&gt; becomes &lt;code&gt;about/index.html&lt;/code&gt; in the final website derivation.&lt;/p&gt;
    &lt;p&gt;Collections allow you to group, paginate and list related content such as blog posts or portfolio pieces.&lt;/p&gt;
    &lt;p&gt;Create a collection under &lt;code&gt;website.collections.&amp;lt;name&amp;gt;&lt;/code&gt; and point it to a folder inside &lt;code&gt;website.content.dir&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;website.collections.blog = {
  path = "blog/posts";     # ./content/blog/posts/
  pagination.perPage = 5;  # Number of items each listing page shows
  rss.enable = true;       # Generate /blog/index.xml
};&lt;/code&gt;
    &lt;p&gt;nixtml automatically produces listing pages hosting &lt;code&gt;pagination.perPage&lt;/code&gt; items per page (&lt;code&gt;blog/index.html&lt;/code&gt;, &lt;code&gt;blog/page/2/index.html&lt;/code&gt;, …) rendered with the &lt;code&gt;collection&lt;/code&gt; layout template.&lt;/p&gt;
    &lt;p&gt;You may want to allow readers to explore entries by common key–words such as tags, categories or authors. Activate any number of taxonomies with the list key &lt;code&gt;taxonomies&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;website.collections.blog = {
  path = "blog/posts";
  taxonomies = [ "tags" "series" ];
};&lt;/code&gt;
    &lt;p&gt;In every markdown file inside that collection you can now list these terms in the YAML frontmatter:&lt;/p&gt;
    &lt;code&gt;---
title: "My Emacs Setup"
date: 2024-07-15
tags:
  - emacs
  - productivity
series:
  - dotfiles
---
Post body…&lt;/code&gt;
    &lt;p&gt;nixtml will then create pages such as &lt;code&gt;/blog/tags/emacs/index.html&lt;/code&gt;, &lt;code&gt;/blog/tags/emacs/page/2/index.html&lt;/code&gt; and so on using the &lt;code&gt;taxonomy&lt;/code&gt; layout template.&lt;/p&gt;
    &lt;p&gt;Inside collection or taxonomy templates you always receive the same context attribute set:&lt;/p&gt;
    &lt;code&gt;{
  # --- collection &amp;amp; taxonomy -------------
  pageNumber,     # Current page number
  totalPages,     # Total amount of pages
  items,          # List of posts in this page
  hasNext,        # A next page exists (bool)
  hasPrev,        # A previous page exists (bool)
  nextPageURL,    # URL to next page
  prevPageURL,    # URL to previous page
  # --- only taxonomy --------------------
  title,          # The tag or term being shown
}&lt;/code&gt;
    &lt;p&gt;Look at the examples directory to see how to work with nixtml. They can be built with &lt;code&gt;nix build .#examples.simple&lt;/code&gt; and &lt;code&gt;nix build .#examples.blog&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46121799</guid><pubDate>Tue, 02 Dec 2025 14:54:49 +0000</pubDate></item><item><title>Show HN: Marmot – Single-binary data catalog (no Kafka, no Elasticsearch)</title><link>https://github.com/marmotdata/marmot</link><description>&lt;doc fingerprint="3d5bdebb3d9cca6a"&gt;
  &lt;main&gt;
    &lt;p&gt;Discover any data asset across your entire org in seconds&lt;/p&gt;
    &lt;p&gt;Open-source catalog for all your data assets. Search everything - tables, topics, queues, buckets, and more.&lt;/p&gt;
    &lt;p&gt;Marmot is an open-source data catalog designed for teams who want powerful data discovery without enterprise complexity. Built with a focus on simplicity and speed, Marmot helps you catalog assets across your entire data stack - from databases and APIs to message queues and data pipelines.&lt;/p&gt;
    &lt;p&gt;Unlike traditional catalogs that require extensive infrastructure and configuration, Marmot ships as a single binary with an intuitive UI, making it easy to deploy and start cataloging in minutes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deploy in Minutes: Single binary, Docker, or Kubernetes - no complex setup required&lt;/item&gt;
      &lt;item&gt;Powerful Search: Powerful query language with full-text, metadata, and boolean operators&lt;/item&gt;
      &lt;item&gt;Track Lineage: Interactive dependency graphs to understand data flows and impact&lt;/item&gt;
      &lt;item&gt;Flexible Integrations: CLI, REST API, Terraform, and Pulumi - catalog assets your way&lt;/item&gt;
      &lt;item&gt;Lightweight: PostgreSQL-backed with minimal resource requirements&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Find any data asset across your entire organisation in seconds. Combine full-text search with structured queries using metadata filters, boolean logic, and comparison operators.&lt;/p&gt;
    &lt;p&gt;Trace data flows from source to destination with interactive dependency graphs. Understand upstream and downstream dependencies, identify bottlenecks, and analyse impact before making changes.&lt;/p&gt;
    &lt;p&gt;Store rich metadata for any asset type. From tables and topics to APIs and dashboards.&lt;/p&gt;
    &lt;p&gt;Assign ownership, document business context, and create glossaries. Keep your entire team aligned with centralised knowledge about your data assets.&lt;/p&gt;
    &lt;p&gt;New to Marmot? Follow the Quickstart Guide for a guided setup.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Interested in exploring Marmot? Check out the live demo&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;See Local Development for how to get started developing locally.&lt;/p&gt;
    &lt;p&gt;All types of contributions are encouraged and valued!&lt;/p&gt;
    &lt;p&gt;Ways to Contribute:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Report bugs or suggest features via GitHub Issues&lt;/item&gt;
      &lt;item&gt;Improve documentation&lt;/item&gt;
      &lt;item&gt;Build new plugins for data sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before contributing, please check out the Contributing Guide.&lt;/p&gt;
    &lt;p&gt;Marmot is open-source software licensed under the MIT License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46121860</guid><pubDate>Tue, 02 Dec 2025 14:59:40 +0000</pubDate></item><item><title>Mistral 3 family of models released</title><link>https://mistral.ai/news/mistral-3</link><description>&lt;doc fingerprint="2d4daf5d864004ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Mistral 3&lt;/head&gt;
    &lt;p&gt;The next generation of &lt;lb/&gt; open multimodal and multilingual AI&lt;/p&gt;
    &lt;p&gt;Today, we announce Mistral 3, the next generation of Mistral models. Mistral 3 includes three state-of-the-art small, dense models (14B, 8B, and 3B) and Mistral Large 3 – our most capable model to date – a sparse mixture-of-experts trained with 41B active and 675B total parameters. All models are released under the Apache 2.0 license. Open-sourcing our models in a variety of compressed formats empowers the developer community and puts AI in people’s hands through distributed intelligence.&lt;/p&gt;
    &lt;p&gt;The Ministral models represent the best performance-to-cost ratio in their category. At the same time, Mistral Large 3 joins the ranks of frontier instruction-fine-tuned open-source models.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistral Large 3: A state-of-the-art open model&lt;/head&gt;
    &lt;p&gt;Mistral Large 3 is one of the best permissive open weight models in the world, trained from scratch on 3000 of NVIDIA’s H200 GPUs. Mistral Large 3 is Mistral’s first mixture-of-experts model since the seminal Mixtral series, and represents a substantial step forward in pretraining at Mistral. After post-training, the model achieves parity with the best instruction-tuned open-weight models on the market on general prompts, while also demonstrating image understanding and best-in-class performance on multilingual conversations (i.e., non-English/Chinese).&lt;/p&gt;
    &lt;p&gt;Mistral Large 3 debuts at #2 in the OSS non-reasoning models category (#6 amongst OSS models overall) on the LMArena leaderboard.&lt;/p&gt;
    &lt;p&gt;We release both the base and instruction fine-tuned versions of Mistral Large 3 under the Apache 2.0 license, providing a strong foundation for further customization across the enterprise and developer communities. A reasoning version is coming soon!&lt;/p&gt;
    &lt;head rend="h3"&gt;Mistral, NVIDIA, vLLM &amp;amp; Red Hat join forces to deliver faster, more accessible Mistral 3&lt;/head&gt;
    &lt;p&gt;Working in conjunction with vLLM and Red Hat, Mistral Large 3 is very accessible to the open-source community. We’re releasing a checkpoint in NVFP4 format, built with llm-compressor. This optimized checkpoint lets you run Mistral Large 3 efficiently on Blackwell NVL72 systems and on a single 8×A100 or 8×H100 node using vLLM.&lt;/p&gt;
    &lt;p&gt;Delivering advanced open-source AI models requires broad optimization, achieved through a partnership with NVIDIA. All our new Mistral 3 models, from Large 3 to Ministral 3, were trained on NVIDIA Hopper GPUs to tap high-bandwidth HBM3e memory for frontier-scale workloads. NVIDIA’s extreme co-design approach brings hardware, software, and models together. NVIDIA engineers enabled efficient inference support for TensorRT-LLM and SGLang for the complete Mistral 3 family, for efficient low-precision execution.&lt;/p&gt;
    &lt;p&gt;For Large 3’s sparse MoE architecture, NVIDIA integrated state-of-the-art Blackwell attention and MoE kernels, added support for prefill/decode disaggregated serving, and collaborated with Mistral on speculative decoding, enabling developers to efficiently serve long-context, high-throughput workloads on GB200 NVL72 and beyond. On the edge, delivers optimized deployments of the Ministral models on DGX Spark, RTX PCs and laptops, and Jetson devices, giving developers a consistent, high-performance path to run these open models from data center to robot.&lt;/p&gt;
    &lt;p&gt;We are very thankful for the collaboration and want to thank vLLM, Red Hat, and NVIDIA in particular.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ministral 3: State-of-the-art intelligence at the edge&lt;/head&gt;
    &lt;p&gt;For edge and local use cases, we release the Ministral 3 series, available in three model sizes: 3B, 8B, and 14B parameters. Furthermore, for each model size, we release base, instruct, and reasoning variants to the community, each with image understanding capabilities, all under the Apache 2.0 license. When married with the models’ native multimodal and multilingual capabilities, the Ministral 3 family offers a model for all enterprise or developer needs.&lt;/p&gt;
    &lt;p&gt;Furthermore, Ministral 3 achieves the best cost-to-performance ratio of any OSS model. In real-world use cases, both the number of generated tokens and model size matter equally. The Ministral instruct models match or exceed the performance of comparable models while often producing an order of magnitude fewer tokens.&lt;/p&gt;
    &lt;p&gt;For settings where accuracy is the only concern, the Ministral reasoning variants can think longer to produce state-of-the-art accuracy amongst their weight class - for instance 85% on AIME ‘25 with our 14B variant.&lt;/p&gt;
    &lt;head rend="h2"&gt;Available Today&lt;/head&gt;
    &lt;p&gt;Mistral 3 is available today on Mistral AI Studio, Amazon Bedrock, Azure Foundry, Hugging Face (Large 3 &amp;amp; Ministral), Modal, IBM WatsonX, OpenRouter, Fireworks, Unsloth AI, and Together AI. In addition, coming soon on NVIDIA NIM and AWS SageMaker.&lt;/p&gt;
    &lt;head rend="h3"&gt;One more thing… customization with Mistral AI&lt;/head&gt;
    &lt;p&gt;For organizations seeking tailored AI solutions, Mistral AI offers custom model training services to fine-tune or fully adapt our models to your specific needs. Whether optimizing for domain-specific tasks, enhancing performance on proprietary datasets, or deploying models in unique environments, our team collaborates with you to build AI systems that align with your goals. For enterprise-grade deployments, custom training ensures your AI solution delivers maximum impact securely, efficiently, and at scale.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get started with Mistral 3&lt;/head&gt;
    &lt;p&gt;The future of AI is open. Mistral 3 redefines what’s possible with a family of models built for frontier intelligence, multimodal flexibility, and unmatched customization. Whether you’re deploying edge-optimized solutions with Ministral 3 or pushing the boundaries of reasoning with Mistral Large 3, this release puts state-of-the-art AI directly into your hands.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Mistral 3?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Frontier performance, open access: Achieve closed-source-level results with the transparency and control of open-source models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multimodal and multilingual: Build applications that understand text, images, and complex logic across 40+ native languages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scalable efficiency: From 3B to 675B active parameters, choose the model that fits your needs, from edge devices to enterprise workflows.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Agentic and adaptable: Deploy for coding, creative collaboration, document analysis, or tool-use workflows with precision.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Next Steps&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Explore the model documentation:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Technical documentation for customers is available on our AI Governance Hub&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Start building: Ministral 3 and Large 3 on Hugging Face, or deploy via Mistral AI’s platform for instant API access and API pricing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customize for your needs: Need a tailored solution? Contact our team to explore fine-tuning or enterprise-grade training.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Share your projects, questions, or breakthroughs with us: Twitter/X, Discord, or GitHub.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Science has always thrived on openness and shared discovery. As pioneering French scientist and two-time Nobel laureate Marie Skłodowska-Curie once said, “Nothing in life is to be feared, it is only to be understood. Now is the time to understand more, so that we may fear less.”&lt;/p&gt;
    &lt;p&gt;This philosophy drives our mission at Mistral AI. We believe that the future of AI should be built on transparency, accessibility, and collective progress. With this release, we invite the world to explore, build, and innovate with us, unlocking new possibilities in reasoning, efficiency, and real-world applications.&lt;/p&gt;
    &lt;p&gt;Together, let’s turn understanding into action.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46121889</guid><pubDate>Tue, 02 Dec 2025 15:01:53 +0000</pubDate></item><item><title>Is 2026 Next Year?</title><link>https://www.google.com/search?q=is+2026+next+year&amp;oq=is+2026+next+year</link><description>&lt;doc fingerprint="585a2ccc31446f2e"&gt;
  &lt;main&gt;
    &lt;p&gt;Please click here if you are not redirected within a few seconds. If you're having trouble accessing Google Search, please click here , or send feedback .&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46122071</guid><pubDate>Tue, 02 Dec 2025 15:20:11 +0000</pubDate></item><item><title>Fallout 2's Chris Avellone describes his game design philosophy</title><link>https://arstechnica.com/gaming/2025/12/fallout-2-designer-chris-avellone-recalls-his-first-forays-into-game-development/</link><description>&lt;doc fingerprint="9de0420f78e86d1c"&gt;
  &lt;main&gt;
    &lt;p&gt;Chris Avellone wants you to have a good time.&lt;/p&gt;
    &lt;p&gt;People often ask creatives—especially those in careers some dream of entering—”how did you get started?” Video game designers are no exception, and Avellone says that one of the most important keys to his success was one he learned early in his origin story.&lt;/p&gt;
    &lt;p&gt;“Players are selfish,” Avellone said, reflecting on his time designing the seminal computer roleplaying game Planescape: Torment. “The more you can make the experience all about them, the better. So Torment became that. Almost every single thing in the game is about you, the player.”&lt;/p&gt;
    &lt;p&gt;The true mark of a successful game is when players really enjoy themselves, and serving that essential egotism is one of the fundamental laws of game design.&lt;/p&gt;
    &lt;p&gt;It’s a lesson he learned long before he became an internationally renowned game designer, before Fallout 2 and Planescape: Torment were twinkles in the eyes of Avellone and his co-workers at Interplay. Avellone’s first introduction to building fictional worlds came not from the digital realm but from the analog world of pen and paper roleplaying games.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table-top takeaways&lt;/head&gt;
    &lt;p&gt;Avellone discovered Dungeons and Dragons at the tender young age of nine, and it was a formative influence on his creative life and imagination.&lt;/p&gt;
    &lt;p&gt;“Getting exposed to the idea of Dungeons and Dragons early was a wake-up call,” he told me. “‘Oh wow, it’s like make believe with rules!’—like putting challenges on your imagination where not everything was guaranteed to succeed, and that made it more fun. However, what I noticed is that I wasn’t usually altering the systems drastically, it was more using them as a foundation for the content.”&lt;/p&gt;
    &lt;p&gt;At first, Avellone wasn’t interested in engineering the games and stories himself. He wanted a more passive role, but life had different ideas.&lt;/p&gt;
    &lt;p&gt;“I never started out with a desire to be the game master,” Avellone remembered. “I wanted to be one of the players, but once it became clear that nobody else in my friend circle really wanted to be a game master—to be fair, it was a lot of work—I bit the bullet and tried my hand at it. Over time, I discovered I really enjoyed helping tell an interactive story with the players.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46122518</guid><pubDate>Tue, 02 Dec 2025 15:56:39 +0000</pubDate></item></channel></rss>