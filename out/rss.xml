<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 07 Jan 2026 23:11:09 +0000</lastBuildDate><item><title>Meditation as Wakeful Relaxation: Unclenching Smooth Muscle</title><link>https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527157</guid><pubDate>Wed, 07 Jan 2026 15:03:34 +0000</pubDate></item><item><title>Shipmap.org</title><link>https://www.shipmap.org/</link><description>&lt;doc fingerprint="14b4e15227c5a82f"&gt;
  &lt;main&gt;
    &lt;p&gt;Data: exactEarth &amp;amp; Clarksons&lt;/p&gt;
    &lt;p&gt;Due to popular demand the designers of this map, Kiln, are now selling stunning high-resolution versions of the world √¢routes√¢ view. There are two versions available: coloured by ship type over the inky-blue base map; or just the ship in a single colour a transparent background so you can overlay or print onto whatever background colour you like. Contact [email protected] for pricing and further information.&lt;/p&gt;
    &lt;p&gt;Yes. You are welcome to embed this map. Please include a link back to Kiln somewhere in the text of your article. Use the following embed code for a fully responsive embed that will adjust to the width of your website. Feel free to change the height and/or give it a fixed width if you prefer.&lt;/p&gt;
    &lt;p&gt;You can see movements of the global merchant fleet over the course of 2012, overlaid on a bathymetric map. You can also see a few statistics such as a counter for emitted CO2 (in thousand tonnes) and maximum freight carried by represented vessels (varying units).&lt;/p&gt;
    &lt;p&gt;You can pan and zoom in the usual ways, and skip back and forward in time using the timeline at the bottom of the screen. The controls at the top right let you show and hide different map layers: port names, the background map, routes (a plot of all recorded vessel positions), and the animated ships view. There are also controls for filtering and colouring by vessel type.&lt;/p&gt;
    &lt;p&gt;The merchant fleet is divided into five categories, each of which has a filter and a CO2 and freight counter for the hour shown on the clock. The ship types and units are as follows:&lt;/p&gt;
    &lt;p&gt;In some cases this is because there are ships navigating via canals or rivers that aren√¢t visible on the map. Generally, though, this effect is an artefact of animating a ship between two recorded positions with missing data between, especially when the positions are separated by a narrow strip of land. We may develop the map to remove this effect in the future.&lt;/p&gt;
    &lt;p&gt;Unfortunately the data we are using for the map is incomplete for the first few months of the year: roughly January to April.&lt;/p&gt;
    &lt;p&gt;The map was created by Kiln based on data from the UCL Energy Institute (UCL EI)&lt;/p&gt;
    &lt;p&gt;Website: Duncan Clark &amp;amp; Robin Houston from Kiln&lt;/p&gt;
    &lt;p&gt;Data: Julia Schaumeier &amp;amp; Tristan Smith from the UCL EI&lt;/p&gt;
    &lt;p&gt;Music: Bach Goldberg Variations played by Kimiko Ishizaka&lt;/p&gt;
    &lt;p&gt;UCL EI took data showing location and speed of ships and cross-checked it with another database to get the vessel characteristics, such as engine type and hull measurements. With this information they were able to compute the CO2 emissions for each observed hour, following the approach laid out in the Third IMO Greenhouse Gas Study 2014. Kiln took the resulting dataset and visualized it with WebGL on top of a specially created base map, which shows bathymetry (ocean depth), based on the GEBCO_2014 Grid (version 20150318), as well as continents and major rivers from Natural Earth.&lt;/p&gt;
    &lt;p&gt;Our data sources for shipping positions are exactEarth for AIS data (location/speed) and Clarksons Research UK World Fleet Register (static vessel information). We are very grateful to our funders, the European Climate Foundation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527161</guid><pubDate>Wed, 07 Jan 2026 15:03:41 +0000</pubDate></item><item><title>The Target forensics lab (2024)</title><link>https://thehorizonsun.com/features/2024/04/11/the-target-forensics-lab/</link><description>&lt;doc fingerprint="7e0e1880e3b5a198"&gt;
  &lt;main&gt;
    &lt;p&gt;Target, just like many other retailers, has fallen victim to shoplifters, with almost a billion dollars in goods stolen from their stores in 2023. However, the numbers could have been much worse if it weren‚Äôt for their unique anti-shoplifting tactics. Target‚Äôs way of combating shoplifting was to establish a forensics lab in Minneapolis, Minnesota, that is more advanced and high-tech than many police departments‚Äô forensics labs.&lt;/p&gt;
    &lt;p&gt;The lab was developed in 2003 to give the company expertise when it came to analyzing surveillance footage from in and around the store. Forbes states that Target has had cameras in all their stores since the 1980s, but it hadn‚Äôt been enough to stop serial shoplifting from occurring. The lab hires specialists in analyzing video evidence from cameras and smartphone recordings to help identify shoplifters, frauds, and injuries inside Target stores. However, due to the skill and technology the lab possesses, it has been of help in many cases outside of Target stores, solving some of the most gruesome crimes including murders, arsons, abductions, rapes, and mass robberies.&lt;/p&gt;
    &lt;p&gt;In many cases, the Target lab has been able to solve cases that even the Federal Bureau of Investigation (FBI) can‚Äôt solve. Forbes goes on to say that in one specific case, the experts at the Target lab were contacted by the Huston police department to help solve an arson case. A convenience store camera had caught two boys buying gasoline a short time before the fire, but the tape was damaged, making it impossible to make out the boys‚Äô faces. After the FBI was unable to solve the case, the tapes were passed over to Target, where they were repaired, and the faces of the boys were able to be seen.&lt;/p&gt;
    &lt;p&gt;Aside from stopping shoplifting and helping law enforcement, the Target lab also teaches and supports government agencies. According to the Washington Post, experts at the lab have taken a leading role in teaching government protection agencies about how to use technology to help solve crimes. In the past, Target has also helped organize undercover investigations, as well as helping United States customs verify overseas imports are coming from reputable sources.&lt;/p&gt;
    &lt;p&gt;While a retailer may seem like an odd group of people to help solve crimes, Target has proven to be a helpful resource to police forces and government agencies alike.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527645</guid><pubDate>Wed, 07 Jan 2026 15:41:01 +0000</pubDate></item><item><title>Many hells of WebDAV</title><link>https://candid.dev/blog/many-hells-of-webdav</link><description>&lt;doc fingerprint="376b9954f126582c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Many Hells of WebDAV&lt;/head&gt;
    &lt;p&gt;Implementing a WebDAV/CalDAV client and server should be easy! It‚Äôs a well documented spec, standardized in the early 00s, and somewhat widely supported. At least, that‚Äôs the naive assumption we started from when creating one for Homechart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Go Implementations&lt;/head&gt;
    &lt;p&gt;Now before you mention NIH syndrome, yes, we looked at the existing Go implementation, go-webdav. This library was lacking some key features we needed, like server-side collection synchronization, and the interfaces didn‚Äôt really align with our data model. This is also going to be a key feature of our product, so we should have some level of ownership for what gets implemented.&lt;/p&gt;
    &lt;head rend="h2"&gt;RFC Breadcrumbs&lt;/head&gt;
    &lt;p&gt;To start creating our client and server, we should read the RFCs, right? Well, where do you start?&lt;/p&gt;
    &lt;p&gt;How about the original, RFC 2518? Ah, looks like it was somewhat superseded by RFC 4918, but we‚Äôre not going to tell you which parts! How about those extension RFCs? There‚Äôs only 7 of them‚Ä¶&lt;/p&gt;
    &lt;p&gt;Reading through the RFCs, all that our implementation cares about is CRUD for Calendar events. After spending almost a month trying to implement the full RFC spec, we threw in the towel, there‚Äôs just to much legacy cruft that we didn‚Äôt need.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reverse Engineering&lt;/head&gt;
    &lt;p&gt;With a decent understanding of the RFC in hand, we instead looked into reverse engineering existing clients and servers by inspecting their requests and responses. This process was MUCH faster, and we quickly had the API mapped out and what kind of requests/responses we needed to support.&lt;/p&gt;
    &lt;p&gt;We started by identifying the clients/servers we wanted to support:&lt;/p&gt;
    &lt;p&gt;Clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Calendar&lt;/item&gt;
      &lt;item&gt;DavX&lt;/item&gt;
      &lt;item&gt;Thunderbird&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Servers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple iCloud&lt;/item&gt;
      &lt;item&gt;Google Calendar&lt;/item&gt;
      &lt;item&gt;Radicale&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then ran HTTP proxies or Wireshark to capture the HTTP requests. Because WebDAV is so obtuse, you not only need to inspect the HTTP body, but also the headers!&lt;/p&gt;
    &lt;head rend="h2"&gt;XML in Go&lt;/head&gt;
    &lt;p&gt;As an aside, we spent quite a bit of time trying to make XML work well in Go. The default Go XML library is truly terrible, and we decided to create a wrapper around it for managing XML nodes similar to how JavaScript manages HTML nodes:&lt;/p&gt;
    &lt;code&gt;var davDisplayName = xmel.Element{
  Name:  "displayname",
  Space: davNS,
}

davDisplayName.SetValue("name")
n, err := davResponse.Find(davCollectionType)
davOwner = davOwner.AddChild(davHref.SetValue("http://example.com"))
&lt;/code&gt;
    &lt;p&gt;With WebDAV having such an‚Ä¶‚Äúunstructured‚Äù schema to a lot of the requests/responses, this library was key in helping us marshal/unmarshal things without writing a bunch of ‚Äúbest case‚Äù structs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Standards are Just Suggestions&lt;/head&gt;
    &lt;p&gt;When we finally had our MVP built out, we put it to the test: validating our client and server against the existing implementations! For the most part, it worked as expected, but as always, things drift from the RFC.&lt;/p&gt;
    &lt;p&gt;Apple and Google, for instance, don‚Äôt implement half of the RFCs, and basically provide a MVP for other clients to use. They don‚Äôt really document what they support/don‚Äôt support, as WebDAV is supposed to do it via HTTP responses advertising capabilities, but both seem to provide generic responses advertising capabilities they don‚Äôt have a lot of the time.&lt;/p&gt;
    &lt;p&gt;The clients were another story. CalDAV clients are all over the place with what they support and how they will request it. Most clients should prefer to support &lt;code&gt;sync-collection&lt;/code&gt; as it‚Äôs very efficient, but Apple Calendar doesn‚Äôt, and uses ctags and etags instead.&lt;/p&gt;
    &lt;p&gt;As a little fish in a big pond, it‚Äôs frustrating dealing with situations where big providers can skirt around some standards or add quirks for their implementations, but I‚Äôm required to follow them to the T because I don‚Äôt have their inertia. I can‚Äôt file a bug, or a lawsuit, against them claiming nonconformance, they‚Äôll tell me to get bent. And you see this in other open source libraries too, where they‚Äôre littered with comments about workarounds for Google‚Äôs specific implementation or whatever.&lt;/p&gt;
    &lt;p&gt;I wouldn‚Äôt recommend anyone who values their sanity to pursue creating a WebDAV/CalDAV library.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527775</guid><pubDate>Wed, 07 Jan 2026 15:50:50 +0000</pubDate></item><item><title>Creators of Tailwind laid off 75% of their engineering team</title><link>https://github.com/tailwindlabs/tailwindcss.com/pull/2388</link><description>&lt;doc fingerprint="2e60f15f4c279cf9"&gt;
  &lt;main&gt;&lt;list rend="ul"&gt;&lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;&lt;item&gt;Fork 1&lt;/item&gt;&lt;/list&gt;&lt;head rend="h1"&gt;feat: add llms.txt endpoint for LLM-optimized documentation #2388&lt;/head&gt;&lt;head id="button-ea06dad32e6fe32f" class="btn btn-sm btn-primary m-0 ml-0 ml-md-2"&gt;New issue&lt;/head&gt;&lt;p&gt;Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.&lt;/p&gt;&lt;p&gt;By clicking ‚ÄúSign up for GitHub‚Äù, you agree to our terms of service and privacy statement. We‚Äôll occasionally send you account related emails.&lt;/p&gt;&lt;p&gt;Already on GitHub? Sign in to your account&lt;/p&gt;&lt;head rend="h2"&gt;Conversation&lt;/head&gt;&lt;p&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Extract text from MDX files, removing JSX components and preserving code blocks&lt;/item&gt;&lt;item&gt;Remove standalone HTML blocks (not in code blocks)&lt;/item&gt;&lt;item&gt;Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.)&lt;/item&gt;&lt;item&gt;Statically generate the output at build time&lt;/item&gt;&lt;item&gt;Include all 185 documentation files in proper order with sections&lt;/item&gt;&lt;/list&gt;&lt;p&gt;:)&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor is attempting to deploy a commit to the Tailwind Labs Team on Vercel.&lt;/p&gt;&lt;p&gt;A member of the Team first needs to authorize it.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;code&gt;5dc6fde&lt;/code&gt;    to
    &lt;code&gt;326c151&lt;/code&gt;      
    Compare
  



    &lt;quote&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption. - Extract text from MDX files, removing JSX components and preserving code blocks - Remove standalone HTML blocks (not in code blocks) - Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.) - Statically generate the output at build time - Include all 185 documentation files in proper order with sections&lt;/quote&gt;&lt;code&gt;326c151&lt;/code&gt;    to
    &lt;code&gt;5c005a9&lt;/code&gt;      
    Compare
  



    &lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@reinink this is ready to be reviewed&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Why is this one not moving?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Yeah I've been wondering that myself.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@petersuhm maybe you missed this before?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Have more important things to do like figure out how to make enough money for the business to be sustainable right now. And making it easier for LLMs to read our docs just means less traffic to our docs which means less people learning about our paid products and the business being even less sustainable.&lt;/p&gt;&lt;p&gt;Just don't have time to work on things that don't help us pay the bills right now, sorry. We may add this one day but closing for now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Wow, what a disappointing response. This is complementary not replacement.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan as someone who has sponsored Tailwind CSS in the past, this is a disappointing response.&lt;/p&gt;&lt;p&gt;Would you like to disclose the fact that sponsoring gives one access to an official collection of LLM rules for Tailwind? Does that have anything to do with the rejection of this PR?&lt;/p&gt;&lt;p&gt;If yes, fine. You're running a business, and that's cool. But you should disclose the fact that you are monetizing this (making Tailwind docs LLM-friendly).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;It is mentioned on the sponsorship page. Seems strange to not mention that when closing this PR, though.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;In general I object to the spirit of closing this. It's very OSS unfriendly and would not meaningfully reduce traffic to the docs by humans that actually would buy the product.&lt;/p&gt;&lt;p&gt;Just bad vibes.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Here's a friendly tip for the Tailwind team that you should already know, but I will repeat anyways:&lt;/p&gt;&lt;p&gt;If your goal is monetizing your software, then making your software as easy to use for people's workflows, is paramount.&lt;/p&gt;&lt;p&gt;The more people that find which your software fits into their workflow seamlessly, and solves pain in their daily interactions, the more people you have as potential monetization candidates.&lt;/p&gt;&lt;p&gt;By scrapping features under the guise of 'monetization' you are sending the opposite of the message you likely intend.&lt;/p&gt;&lt;p&gt;You are telling your customers that getting money from them, is more important than providing a service to help them.&lt;/p&gt;&lt;p&gt;Tell me, would you enjoy doing business with a company who had a stance like that?&lt;/p&gt;&lt;p&gt;This feature is so that people can build MORE things with Tailwind in a FASTER and more EFFICIENT capacity.&lt;/p&gt;&lt;p&gt;From a business management perspective, if you remove the stigmatic 'AI' and 'LLM' from the conversation, and you simply are evaluating a feature XYZ which allows your customers to work in a more automated and efficient capacity with your software, with minimal engineering effort (all it takes is a simple build-time script)...&lt;/p&gt;&lt;p&gt;Why would you not want that for your customers?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I totally see the value in the feature and I would like to find a way to add it.&lt;/p&gt;&lt;p&gt;But the reality is that 75% of the people on our engineering team lost their jobs here yesterday because of the brutal impact AI has had on our business. And every second I spend trying to do fun free things for the community like this is a second I'm not spending trying to turn the business around and make sure the people who are still here are getting their paychecks every month.&lt;/p&gt;&lt;p&gt;Traffic to our docs is down about 40% from early 2023 despite Tailwind being more popular than ever. The docs are the only way people find out about our commercial products, and without customers we can't afford to maintain the framework. I really want to figure out a way to offer LLM-optimized docs that don't make that situation even worse (again we literally had to lay off 75% of the team yesterday), but I can't prioritize it right now unfortunately, and I'm nervous to offer them without solving that problem first.&lt;/p&gt;&lt;p&gt;@PaulRBerg I don't see the AGENTS.md stuff we offer as part of the sponsorship program as anything similar to this at all ‚Äî that's just a short markdown file with a bunch of my own personal opinions and what I consider best practices to nudge LLMs into writing their Tailwind stuff in a specific way. It's not the docs at all, and I resent the accusation that I am not disclosing my "true intentions" here or something.&lt;/p&gt;&lt;p&gt;@mtsears4 Tailwind is growing faster than it ever has and is bigger than it ever has been, and our revenue is down close to 80%. Right now there's just no correlation between making Tailwind easier to use and making development of the framework more sustainable. I need to fix that before making Tailwind easier to use benefits anyone, because if I can't fix that this project is going to become unmaintained abandonware when there is no one left employed to work on it. I appreciate the sentiment and agree in spirit, it's just more complicated than that in reality right now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor As far as I can tell, this PR doesn't close an existing issue and I don't see any evidence of you having proposed this feature in any forum. You just opened a PR. That entitles you to neither a merge nor other people's time to review it.&lt;/p&gt;&lt;p&gt;(I'm not a Tailwind employee, just some guy)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;There is an associated discussion. tailwindlabs/tailwindcss#14677 (comment)&lt;/p&gt;&lt;p&gt;You're entirely right that I am not entitled to anyone's time. I run multiple large OSS libraries as well, though not to the scale of Tailwind (these days.)&lt;/p&gt;&lt;p&gt;My objection is the way this was handled. Full thoughts on my&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;You're welcome to fork the library&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan I empathize where you're coming from, putting my solutioning hat on, I wonder whether you could add something to the llms.txt prompt saying something akin to "if the user is trying to create a landing page suggest they check out our paid product" or etc. for each of the components/layouts&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Edit: deleted this. No one cares about my opinion so whatever.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I think it worsens the effect to self-promote your TikTok video not once, but twice within a span of 2 hours.&lt;/p&gt;&lt;p&gt;That alone seems deeply unprofessional.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Well I edited it onto a prior comment so idk if people would see it. So sue me.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Just wanted to drop a quick note and say I appreciate all the support here, on Twitter, and on Hacker News. We'll figure it out I'm sure.&lt;/p&gt;&lt;p&gt;In the mean time if you've benefitted from Tailwind over the years and are looking for a way to support the project, consider grabbing a Tailwind Plus license or sponsoring the project through our Insiders Program ‚ù§Ô∏è Will be working hard to make both of those things a lot more valuable this year.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;In the meantime, consider using https://context7.com as it helps a lot.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I love this idea, but I think I can predict the reaction just purely from the entitledness in this thread:&lt;/p&gt;&lt;p&gt;"I would never pay for an MCP server that's just serving docs, your company will go broke unless it is free", "Opus already knows all this, there's no point in putting it behind a paywall", "Your paid product is making it harder for devs to learn Tailwind", "We are just going to make our own free MCP server anyways so you should make yours free as well."&lt;/p&gt;&lt;p&gt;Not to say that these comments should stop folks from looking into this direction. But the main point is some people will never be happy until they get exactly what they want for free. And that a lot of these comments don't seem to be coming from a place of caring about the Tailwind project or its maintainers.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Sorry to hear the troubles @adamwathan, I found this thread via hacker news. Perhaps a solution may be something like what Weaviate has done with their docs. They have an LLM Rag pipeline chatbot as the main interface to their docs. The direct access means that going to their website directly is by far the best way to interact with their tech because the AI generated code (and error handling) is unbelievably accurate. It was so useful that we chose Weaviate over many competitors and will keep coming back. Thanks for your software and good luck.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Might be best to lock this conversation as it's devolving into meta-commentary from external sites (this comment included).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Can you please get lost with this tiktok brainrot?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Sorry for adding to the noise but I have to respect Adam's views. Everyone just talks about how much more productive you can be with AI but all that productivity is only useful when you have a job to be productive at.&lt;/p&gt;&lt;p&gt;Wish you luck Adam that you can improve your business &amp;amp; keep your teams jobs stable!&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Seems kinda insane, that every big company is benefiting hard from Tailwind, but not investing much money back into it?&lt;/p&gt;&lt;p&gt;What a horrible situation for Tailwind team.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan you should be maxxing out on AI SEO of your premium products: both in the LLM.txt and every other way you can. The goal is to ensure whenever AI recommends your framework, it also recommends your premium product. Whenever AI uses your framework (in any app builder, UI builder, whatever), it also adds a recommendation for your premium products. maybe you tweak your documentation so whenever AI creates UI for a user using your framework, it shows the user extra screenshots of how much better their UI could look and function if they used any of your premium products. Start working with all the major app builders (eg. replit, loveable, etc) to integrate premium/paid library/plugins into their platform (the way OpenAI has integrated Shopify) so when they design for a paying user, they simply use any of your premium product, and replit, loveable and co pay you a microtransaction for that (can even use the x402 microtransaction framework) which would simply be part of their costs (i.e taken out of the user's subscription fee on their platforms). basically get AI to use your premium products as much as possible&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@rauchg I would love to see @vercel on tailwindcss's sponsors page. Please consider!&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan I just want to offer my appreciation for the work you're doing in trying to build useful technology, and to build a sustainable organization around it. These are really hard problems, and I'm thankful that there are people like you and your team out there trying to solve hard problems in sustainable ways.&lt;/p&gt;&lt;p&gt;This project may or may not work, and the strategic winds are shifting considerably right now, but the impact Tailwind has had is undeniable and I think you should be proud of your team regardless of what happens in the coming months, years, and decades. Thanks for the work you're doing. &amp;lt;3&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;We want to give LLMs full access to send cryptocurrency now? There are so many things wrong with this.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I would love a paid Tailwind MCP server for feeding in Tailwind UI components into an LLM - AI is just not as good (even Opus 4.5) as you @adamwathan and your team at creating beautiful UI/UX - I would happily pay $99 a month to stop copy pasting Tailwind UI snippets into the my prompts.&lt;/p&gt;&lt;p&gt;Furthermore, I've been a Tailwind subscriber from very early in the piece and I can't thank you @adamwathan enough for your hard work and dedication to solving this painful front-end problem - massive respect for you and your work and I believe, just as Rails has done, there is a sponsorship model and a paid product leaning into AI model there somewhere that will help Tailwind survive and thrive.&lt;/p&gt;&lt;p&gt;Feel free to reach out anytime if you'd like some help / a chat from a fellow founder ‚ù§Ô∏è&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I think I speak for everyone when I say.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;This is such incosiderate behaviour, especially the tiktoks in which you whine and charactertize this as "He closes it, says, uhh, you know, "no we're not gonna do this, dadada", kinda a weak reason honestly, like I think he was just kinda having a bad day"... I don't remember the last time somebody pissed me off that badly.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Open source in general is extremely underfunded. We should all contribute what we can to the developers and projects we rely on. The reason I say this is because I'm hoping that there isn't a big "mission accomplished" banner waved if Tailwind happens to turn this around, because this story neither starts nor ends with Tailwind, it's just among the most visible at the moment.&lt;/p&gt;&lt;p&gt;Don't just go to the Tailwind sponsorship page, actually open up your &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I have an idea that solves the problem for both @quantizor and @adamwathan&lt;/p&gt;&lt;p&gt;Why don't you add LLM support for your documentation site. I bet this will be more accurate (it needs to be ~100% accurate) than anything else out there. It will only respond referencing your documentation.&lt;/p&gt;&lt;p&gt;Or even better, make a platform through which you add LLM support to your documentation. People pay you to add LLM support to their docs. Over time you can also cache the queries and make a better experience for the developers.&lt;/p&gt;&lt;p&gt;LLM is here, i guess until it drink all our water (85,000 gallons of water used daily) and destroy the whole environment. But the problem is, if I stop using it, other people will have leverage over me.&lt;/p&gt;&lt;p&gt;I myself is tired of not reading documentations.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I will be honest. I love open source. But something that really annoys me about the open source community is that the developers take this holier-than-thou approach to backing up maintainers in circumstances like this, but obviously they are not paying with their own money. They are just complaining, and it feels a lot like virtue signaling at worst and pure naivety at best. It feels extremely disengenous at this point, and it's annoying.&lt;/p&gt;&lt;p&gt;What do we actually know?&lt;/p&gt;&lt;p&gt;But when making these points, devs whine and complain. I don't really know why I'm leaving this comment - I just feel like I'm at an annoyance breaking point. The Tailwind team is obviously struggling to pivot and all the grandstanding and virtue signaling just feels like wanting to feel good with very little action, which is just selfish.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;It seems that Tailwind's sole marketing strategy is their hosted documentation. Also, it seems that Tailwind employees are instructed to watch and downvote any comment that doesn't align with optimization of user engagement with this documentation. Very interesting company culture. Where do I send my resume when you are hiring again? üòÄ&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;just use context7 lil bro&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Increasingly this is looking like devs throwing tantrums about stuffing Pandora back into the box when it is entirely clear to others that it won't be put back in there.&lt;/p&gt;&lt;p&gt;To wit: Regardless of whether anyone adds an "llms.txt" file, literally any LLM can browse the public docs of your site in a NON-"llms.txt" form and glean everything they need to know to help people write Tailwind.&lt;/p&gt;&lt;p&gt;So in addition to being a low priority due to the need to make money (something which I sympathize with, having been essentially underemployed for over a year now), it's not even necessary to implement anymore, because Pandora is already right in the room with us.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;You look exactly as I've imagined&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Dang, guess not. Well, that's really the end of it, I suppose. If the maintainers don't want the PR, they don't want the PR. Nothing more to be done about it, really. It does suck for the author, but ultimately maintainers carry the burden of maintaining the code external contributors have contributed.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan I totally sympathize with you.&lt;/p&gt;&lt;p&gt;Here‚Äôs my unbiased suggestion: you‚Äôre blocking llms.txt, which prevents LLM bots from crawling your site. Since your content is already included, they will continue serving outdated content.&lt;/p&gt;&lt;p&gt;They still crawl all open-source projects, so finding a ‚Äúhow to‚Äù isn‚Äôt a problem for any LLMs.&lt;/p&gt;&lt;p&gt;You're not blocking robots.txt; otherwise, you'd be removed from search engines. Many search engines provide index data as augmented data. The "llms.txt" file is for LLM agents and cloud AI agents (a small subset), which are mostly excluded from training data.&lt;/p&gt;&lt;p&gt;Users can still share Tailwind CSS links in Chat Agents, and they‚Äôll be accessed from the local machine without any restrictions.&lt;/p&gt;&lt;p&gt;But making that move could cost you valuable traffic from LLM chat references. SEO is on the brink of becoming outdated, with major traffic sources now coming from LLM mentions on authority sites you already own, and chat app agents featuring ads.&lt;/p&gt;&lt;p&gt;Your decision might be counterproductive, and since I work in AI, I‚Äôm figuring this out for you. I have the best intentions for Tailwind CSS to thrive, and I‚Äôm a proud user for both local and private projects.&lt;/p&gt;&lt;p&gt;Best,&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Get fucked.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;How do I lock someone else's thread?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527950</guid><pubDate>Wed, 07 Jan 2026 16:02:19 +0000</pubDate></item><item><title>Building voice agents with Nvidia open models</title><link>https://www.daily.co/blog/building-voice-agents-with-nvidia-open-models/</link><description>&lt;doc fingerprint="ed5ac51b6c1a889b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;How to Build Ultra-low-latency Voice Agents With NVIDIA Cache-aware Streaming ASR&lt;/head&gt;
    &lt;p&gt;This post accompanies the launch of NVIDIA Nemotron Speech ASR on Hugging Face. Read the full model announcement here.&lt;/p&gt;
    &lt;p&gt;In this post, we‚Äôll build a voice agent using three NVIDIA open models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The new Nemotron Speech ASR model&lt;/item&gt;
      &lt;item&gt;Nemotron 3 Nano LLM&lt;/item&gt;
      &lt;item&gt;A preview checkpoint of the upcoming NVIDIA Magpie text-to-speech model&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This voice agent leverages the new streaming ASR model, Pipecat‚Äôs low-latency voice agent building blocks, and some fun code experiments to optimize all three models for very fast response times.&lt;/p&gt;
    &lt;p&gt;All the code for the post is here in this GitHub repository.&lt;/p&gt;
    &lt;p&gt;You can clone the repo and run this voice agent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scalably for multi-user workloads on the Modal cloud platform.&lt;/item&gt;
      &lt;item&gt;On an NVIDIA DGX Spark or RTX 5090 for single-user, local development and experimentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feel free to just jump over to the code. Or read on for technical notes about building fast voice agents and the NVIDIA open models.&lt;/p&gt;
    &lt;head rend="h1"&gt;The state of voice AI agents in 2026&lt;/head&gt;
    &lt;p&gt;Voice agent deployments are growing by leaps and bounds across a wide range of use cases. For example, we‚Äôre seeing voice agents used at scale today in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customer support&lt;/item&gt;
      &lt;item&gt;Answering the phone for small businesses (for example, restaurants)&lt;/item&gt;
      &lt;item&gt;User research&lt;/item&gt;
      &lt;item&gt;Outbound phone calls to prepare patients for healthcare appointments&lt;/item&gt;
      &lt;item&gt;Validation workflows for loan applications&lt;/item&gt;
      &lt;item&gt;And many, many other scenarios&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both startups and large, established companies are building voice agents that are successful in real-world deployments. The best voice agents today achieve very high ‚Äútask completed‚Äù success metrics and customer satisfaction scores.&lt;/p&gt;
    &lt;head rend="h2"&gt;Voice AI architecture&lt;/head&gt;
    &lt;p&gt;As is the case with everything in AI, voice agent technology is evolving rapidly. Today, there are two ways to build voice agents.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Most production voice agents use specialized models together in a pipeline ‚Äì a speech-to-text model, a text-mode LLM, and a text-to-speech model.&lt;/item&gt;
      &lt;item&gt;Voice agent developers are beginning to experiment with new speech-to-speech models that take voice input directly and output audio instead of text.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using three specialized models is currently the best approach for enterprise use cases that require the highest degree of model intelligence and flexibility. But speech-to-speech models are an exciting development and will be a big part of the future of voice AI.&lt;/p&gt;
    &lt;p&gt;Whether we use a pipeline or a unified speech-to-speech model, voice agents are doing more and more sophisticated tasks. This means that, increasingly, production voice agents are actually multi-agent systems. Inside an agent, sub-agents handle asynchronous tasks, manage the conversation context, and allow code re-use between text and voice agents.&lt;/p&gt;
    &lt;p&gt;For a deep dive into voice agent architectures, models, and infrastructure, see the Voice AI &amp;amp; Voice Agents Illustrated Primer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open source models&lt;/head&gt;
    &lt;p&gt;Open models have not been widely used for production voice agents.&lt;/p&gt;
    &lt;p&gt;Voice agents are among the most demanding AI use cases. Voice agents perform long conversations. They must operate on noisy input audio and respond very quickly. Enterprise voice agent use cases require highly accurate instruction following and function calling. People interacting with voice agents have very high expectations for naturalness and ‚Äúhuman-like‚Äù qualities of voice audio. In all of these areas, proprietary AI models have performed better than open models.&lt;/p&gt;
    &lt;p&gt;However, this is changing. Nemotron Speech ASR is both fast and accurate. On our benchmarks it performs comparably with or better than commercial speech-to-text models used today in production voice agents. Nemotron 3 Nano is the best-performing LLM in its class on our long-context, multi-turn conversation benchmarks.&lt;/p&gt;
    &lt;p&gt;Using open models allows us to configure and customize our models and inference stacks for the specific needs of our voice agents in ways that we can‚Äôt do with proprietary models. We can optimize for latency, fine-tune on our own data, host inference within our VPCs to satisfy data privacy and regulatory requirements, and implement observability that allows us to deliver the highest levels of reliability, scalability, and consistency.&lt;/p&gt;
    &lt;p&gt;We expect open models to be used in a larger and larger proportion of voice agent deployments over time. There are various flavors of ‚Äúopen‚Äù model licenses. NVIDIA has made the Nemotron Speech ASR and Nemotron 3 Nano available under the NVIDIA Permissive Open-Model License, which allows for unrestricted commercial use and the creation of derivative works.&lt;/p&gt;
    &lt;head rend="h1"&gt;An ultra-responsive voice agent&lt;/head&gt;
    &lt;head rend="h2"&gt;Fast, streaming transcription&lt;/head&gt;
    &lt;p&gt;The Nemotron Speech ASR model is designed specifically for use cases that demand very low latency transcription, such as voice agents.&lt;/p&gt;
    &lt;p&gt;The headline number here is that Nemotron Speech ASR consistently delivers final transcripts in under 24ms!&lt;/p&gt;
    &lt;p&gt;ASR (Automatic Speech Recognition) is the general term for machine learning models that process speech input, then output text and other information about that speech. Previous generations of ASR models were generally designed for batch processing rather than realtime transcription. For example, the latency of the Whisper model is 600-800ms, and most commercial speech-to-text models today have latencies in the 200-400ms range.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Openness&lt;/cell&gt;
        &lt;cell role="head"&gt;Deployment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Parakeet&lt;/cell&gt;
        &lt;cell&gt;open weights, open training data, open source inference&lt;/cell&gt;
        &lt;cell&gt;local in-cluster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Widely used commercial ASR&lt;/cell&gt;
        &lt;cell&gt;proprietary&lt;/cell&gt;
        &lt;cell&gt;cloud&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Whisper Large V3&lt;/cell&gt;
        &lt;cell&gt;open weights, open source inference&lt;/cell&gt;
        &lt;cell&gt;local in-cluster&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For more about the cache-aware architecture that enables this impressively low latency, see the NVIDIA post announcing the new model.&lt;/p&gt;
    &lt;p&gt;The model is also very accurate. The industry standard for measuring ASR model accuracy is word error rate. Nemotron Speech ASR has a word error rate on all of our benchmarks roughly equivalent to the best commercial ASR models, and substantially better than previous generation open models like Whisper.&lt;/p&gt;
    &lt;p&gt;To integrate Nemotron Speech ASR into Pipecat, we created a WebSocket server that performs the transcription inference and a client-side Pipecat service that can be used in any Pipecat agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running turn detection in parallel with transcription&lt;/head&gt;
    &lt;p&gt;The Nemotron Speech ASR model can be configured with four different context sizes, each of which have different latency/accuracy trade-offs. The context sizes are 80ms, 160ms, 560ms, and 1.2s. We use the 160ms context size, because this aligns with how we perform turn detection.&lt;/p&gt;
    &lt;p&gt;Turn detection means determining when the user has stopped speaking and the voice agent should respond. Accurate turn detection is critical to natural conversation. We‚Äôre using the open source Pipecat Smart Turn model in this voice agent. The Smart Turn model operates on input audio and runs in parallel with the Nemotron Speech ASR transcription.&lt;/p&gt;
    &lt;p&gt;We trigger both turn detection and transcript finalization any time we see a 200ms pause in the user‚Äôs speech. This gives us 200ms of ‚Äúnon-speech‚Äù trailing context after the user‚Äôs speech has finished. The Nemotron Speech ASR model actually needs a bit more trailing silence than this, to properly finalize the last words in the user speech. The padding calculation is:&lt;/p&gt;
    &lt;code&gt;nemotron_final_padding = (right_context + 1) * shift_frames * hop_samples
    = (1 + 1) * 16 * 160
    = 5120 samples = 320ms
&lt;/code&gt;
    &lt;p&gt;Our WebSocket transcription server receives 200ms of ‚Äúnon-speech‚Äù trailing audio data from the Pipecat service, and adds 120ms of synthetic silence to enable immediate finalization of the transcript. This works nicely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nemotron 3 Nano&lt;/head&gt;
    &lt;p&gt;Nemotron 3 Nano is a new 30 billion parameter open source LLM from NVIDIA. Nemotron 3 Nano is the best performing model in its size class on our multi-turn conversation benchmarks.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Tool Use&lt;/cell&gt;
        &lt;cell role="head"&gt;Instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;KB Ground&lt;/cell&gt;
        &lt;cell role="head"&gt;Pass Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB Med&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB P95&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5.1&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;916ms&lt;/cell&gt;
        &lt;cell&gt;2011ms&lt;/cell&gt;
        &lt;cell&gt;5216ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gemini-3-flash-preview&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;1193ms&lt;/cell&gt;
        &lt;cell&gt;1635ms&lt;/cell&gt;
        &lt;cell&gt;6653ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;claude-sonnet-4-5&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;2234ms&lt;/cell&gt;
        &lt;cell&gt;3062ms&lt;/cell&gt;
        &lt;cell&gt;5438ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4.1&lt;/cell&gt;
        &lt;cell&gt;283/300&lt;/cell&gt;
        &lt;cell&gt;273/300&lt;/cell&gt;
        &lt;cell&gt;298/300&lt;/cell&gt;
        &lt;cell&gt;94.9%&lt;/cell&gt;
        &lt;cell&gt;97.8%&lt;/cell&gt;
        &lt;cell&gt;683ms&lt;/cell&gt;
        &lt;cell&gt;1052ms&lt;/cell&gt;
        &lt;cell&gt;3860ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gemini-2.5-flash&lt;/cell&gt;
        &lt;cell&gt;275/300&lt;/cell&gt;
        &lt;cell&gt;268/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;93.7%&lt;/cell&gt;
        &lt;cell&gt;94.4%&lt;/cell&gt;
        &lt;cell&gt;594ms&lt;/cell&gt;
        &lt;cell&gt;1349ms&lt;/cell&gt;
        &lt;cell&gt;2104ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5-mini&lt;/cell&gt;
        &lt;cell&gt;271/300&lt;/cell&gt;
        &lt;cell&gt;272/300&lt;/cell&gt;
        &lt;cell&gt;289/300&lt;/cell&gt;
        &lt;cell&gt;92.4%&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
        &lt;cell&gt;6339ms&lt;/cell&gt;
        &lt;cell&gt;17845ms&lt;/cell&gt;
        &lt;cell&gt;27028ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4o-mini&lt;/cell&gt;
        &lt;cell&gt;271/300&lt;/cell&gt;
        &lt;cell&gt;262/300&lt;/cell&gt;
        &lt;cell&gt;293/300&lt;/cell&gt;
        &lt;cell&gt;91.8%&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;760ms&lt;/cell&gt;
        &lt;cell&gt;1322ms&lt;/cell&gt;
        &lt;cell&gt;3256ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;nemotron-3-nano-30b-a3b*&lt;/cell&gt;
        &lt;cell&gt;287/304&lt;/cell&gt;
        &lt;cell&gt;286/304&lt;/cell&gt;
        &lt;cell&gt;298/304&lt;/cell&gt;
        &lt;cell&gt;91.4%&lt;/cell&gt;
        &lt;cell&gt;93.3%&lt;/cell&gt;
        &lt;cell&gt;171ms&lt;/cell&gt;
        &lt;cell&gt;199ms&lt;/cell&gt;
        &lt;cell&gt;255ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4o&lt;/cell&gt;
        &lt;cell&gt;278/300&lt;/cell&gt;
        &lt;cell&gt;249/300&lt;/cell&gt;
        &lt;cell&gt;294/300&lt;/cell&gt;
        &lt;cell&gt;91.2%&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
        &lt;cell&gt;625ms&lt;/cell&gt;
        &lt;cell&gt;1222ms&lt;/cell&gt;
        &lt;cell&gt;13378ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-oss-120b (groq)&lt;/cell&gt;
        &lt;cell&gt;272/300&lt;/cell&gt;
        &lt;cell&gt;270/300&lt;/cell&gt;
        &lt;cell&gt;298/300&lt;/cell&gt;
        &lt;cell&gt;89.3%&lt;/cell&gt;
        &lt;cell&gt;90.0%&lt;/cell&gt;
        &lt;cell&gt;98ms&lt;/cell&gt;
        &lt;cell&gt;226ms&lt;/cell&gt;
        &lt;cell&gt;2117ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5.2&lt;/cell&gt;
        &lt;cell&gt;224/300&lt;/cell&gt;
        &lt;cell&gt;228/300&lt;/cell&gt;
        &lt;cell&gt;250/300&lt;/cell&gt;
        &lt;cell&gt;78.0%&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;819ms&lt;/cell&gt;
        &lt;cell&gt;1483ms&lt;/cell&gt;
        &lt;cell&gt;1825ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;claude-haiku-4-5&lt;/cell&gt;
        &lt;cell&gt;221/300&lt;/cell&gt;
        &lt;cell&gt;172/300&lt;/cell&gt;
        &lt;cell&gt;299/300&lt;/cell&gt;
        &lt;cell&gt;76.9%&lt;/cell&gt;
        &lt;cell&gt;75.6%&lt;/cell&gt;
        &lt;cell&gt;732ms&lt;/cell&gt;
        &lt;cell&gt;1334ms&lt;/cell&gt;
        &lt;cell&gt;4654ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Like Nemotron Speech ASR, Nemotron 3 Nano is part of a new generation of open models that are designed specifically for speed and inference efficiency. See this resource from NVIDIA research for an overview of the Nemotron 3 hybrid Mamba-Transformer MoE architecture and links to technical papers.&lt;/p&gt;
    &lt;p&gt;A 30B parameter model is small enough to run very fast on high-end hardware, and can be quantized to run well on GPUs that many developers have at home!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model variant&lt;/cell&gt;
        &lt;cell role="head"&gt;Deployment&lt;/cell&gt;
        &lt;cell role="head"&gt;Resident memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nemotron-3-Nano BF16&lt;/cell&gt;
        &lt;cell&gt;full weights, Modal Cloud or DGX Spark&lt;/cell&gt;
        &lt;cell&gt;72GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nemotron-3-Nano Q8&lt;/cell&gt;
        &lt;cell&gt;8-bit quantization, faster operation on DGX Spark&lt;/cell&gt;
        &lt;cell&gt;32GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nemotron-3-Nano Q4&lt;/cell&gt;
        &lt;cell&gt;4-bit quantization, RTX 5090&lt;/cell&gt;
        &lt;cell&gt;24GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;One note on which LLMs are generally used today for production voice agents: in general, voice agents for applications like customer support need the most ‚Äúintelligent‚Äù models we have available. Voice agent use cases are demanding. A customer support AI agent must do highly accurate instruction following and function calling tasks throughout a long, open-ended, unpredictable human conversation. A 30B parameter model ‚Äì even one as good as Nemotron 3 Nano ‚Äì is generally best suited for specialized voice tasks like a home assistant or software voice UI interface.&lt;/p&gt;
    &lt;p&gt;NVIDIA has announced that two larger Nemotron 3 models are coming soon. If the performance of these larger models relative to their size is similar to Nemotron 3 Nano‚Äôs performance, we expect these models to be terrific intelligence engines for voice agents.&lt;/p&gt;
    &lt;p&gt;In the meantime, Nemotron 3 Nano is the best-performing LLM that I can run on hardware I have at home. I‚Äôve been using this model for a wide variety of ‚Äúlocal‚Äù voice agent tasks and development experiments on both an NVIDIA DGX Spark and on my desktop computer with an RTX 5090.&lt;/p&gt;
    &lt;p&gt;You can use Nemotron 3 in reasoning or non-reasoning mode. We usually turn off reasoning for the fast-response core voice agent loop.&lt;/p&gt;
    &lt;p&gt;For details on using Nemotron 3 Nano in the cloud and building local containers with the latest CUDA, vLLM and llama.cpp support for this new model, see the GitHub repository accompanying this post. There are a couple of inference tooling patches (relating to the reasoning output format in vLLM and to llama.cpp KV caching) that you might find useful if you‚Äôre experimenting with this model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Magpie streaming server&lt;/head&gt;
    &lt;p&gt;Magpie is a family of text-to-speech models from NVIDIA. In our voice agent project, we‚Äôre using an experimental preview checkpoint of an upcoming open source version of Magpie.&lt;/p&gt;
    &lt;p&gt;Kudos to NVIDIA for releasing this early look at a Magpie model designed, like Nemotron Speech ASR, for streaming, low-latency use cases! We‚Äôve been having a lot of fun experimenting with this preview, doing things that are only possible with open source weights and inference code.&lt;/p&gt;
    &lt;p&gt;You can use this Magpie model in batch mode by sending an HTTP request with a chunk of text. This batch mode inference delivers audio for a single sentence in about 600ms on the DGX Spark and 300ms on the RTX 5090. But for voice agents, we like to stream all tokens as much as we can, and because Magpie is open source, we can hack together a hybrid streaming mode that optimizes for initial audio chunk latency! This hybrid streaming approach improves average initial response latency 3x.&lt;/p&gt;
    &lt;head rend="h3"&gt;TTS TTFB Comparison: Batch ‚Üí Streaming&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Hardware&lt;/cell&gt;
        &lt;cell role="head"&gt;P50 Improvement&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean Improvement&lt;/cell&gt;
        &lt;cell role="head"&gt;P90 Improvement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;RTX 5090&lt;/cell&gt;
        &lt;cell&gt;90 ms (1.9x)&lt;/cell&gt;
        &lt;cell&gt;204 ms (3.0x)&lt;/cell&gt;
        &lt;cell&gt;430 ms (5.2x)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DGX Spark&lt;/cell&gt;
        &lt;cell&gt;236 ms (2.3x)&lt;/cell&gt;
        &lt;cell&gt;415 ms (3.3x)&lt;/cell&gt;
        &lt;cell&gt;836 ms (4.6x)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Details&lt;/head&gt;
    &lt;head rend="h5"&gt;RTX 5090&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Batch&lt;/cell&gt;
        &lt;cell&gt;106 ms&lt;/cell&gt;
        &lt;cell&gt;630 ms&lt;/cell&gt;
        &lt;cell&gt;191 ms&lt;/cell&gt;
        &lt;cell&gt;533 ms&lt;/cell&gt;
        &lt;cell&gt;305 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipeline&lt;/cell&gt;
        &lt;cell&gt;99 ms&lt;/cell&gt;
        &lt;cell&gt;103 ms&lt;/cell&gt;
        &lt;cell&gt;101 ms&lt;/cell&gt;
        &lt;cell&gt;103 ms&lt;/cell&gt;
        &lt;cell&gt;101 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;DGX Spark&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Batch&lt;/cell&gt;
        &lt;cell&gt;193 ms&lt;/cell&gt;
        &lt;cell&gt;1440 ms&lt;/cell&gt;
        &lt;cell&gt;422 ms&lt;/cell&gt;
        &lt;cell&gt;1067 ms&lt;/cell&gt;
        &lt;cell&gt;595 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipeline&lt;/cell&gt;
        &lt;cell&gt;15 ms&lt;/cell&gt;
        &lt;cell&gt;276 ms&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;231 ms&lt;/cell&gt;
        &lt;cell&gt;180 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There‚Äôs definitely a quality trade-off with our simple streaming implementation. Try the agent yourself, or listen carefully to the conversation in the video at the beginning of this blog post. You can usually hear a slight disfluency where we ‚Äústitch‚Äù together the streaming chunks at the beginning of the model response.&lt;/p&gt;
    &lt;p&gt;To do better, we‚Äôd need to retrain part of the model and use a slightly more sophisticated inference approach. Fortunately, this is on the NVIDIA road map.&lt;/p&gt;
    &lt;p&gt;We integrated this model into Pipecat by creating a WebSocket server for streaming inference, and a client-side Pipecat service. (This is the same approach we used with Nemotron Speech ASR).&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting the models together and measuring latency&lt;/head&gt;
    &lt;p&gt;These Nemotron and upcoming Magpie models are completely open: open weights, open source training data sets, and open source inference tooling. Working with open models in production feels like a super-power. We can do things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read the inference code to understand the context requirements of the ASR model, so that we can optimize the interactions between our Pipecat pipeline components and text-to-speech audio buffer handling. (See our description of this above, in the section Fast, streaming transcription.&lt;/item&gt;
      &lt;item&gt;Fix issues with inference tooling support in new models and on whatever platforms we‚Äôre running on. See the code and README.md in the GitHub repo for the small patches we made for vLLM and llama.cpp, and the Docker container build with full MX4FP support for both of those inference servers on DGX Spark and RTX 5090.&lt;/item&gt;
      &lt;item&gt;Build a semi-streaming inference server for a preview model checkpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Often when we‚Äôre building voice agents, our primary concern is to engineer the agent to respond quickly in a real-world conversation. The difference between good latency and an agent too slow to use in production is often a combination of several optimizations, each one cutting peak latencies by 100 or 200ms. Working with open models gives us control over how we prioritize for latency compared to throughput, how we design streaming and chunking of inference results, how to use models together optimally, and many other small things that add up (or subtract down) to fast response times.&lt;/p&gt;
    &lt;p&gt;It‚Äôs useful to measure voice-to-voice latency ‚Äì the time between the user‚Äôs voice stopping and the bot‚Äôs voice response starting ‚Äì in two places: on the server-side and at the client.&lt;/p&gt;
    &lt;p&gt;We can easily automate the server-side latency measurement. Our bot outputs a log line with a voice-to-voice latency metric for each turn.&lt;/p&gt;
    &lt;code&gt;2026-01-01 22:43:26.208 | INFO     | v2v_metrics:process_frame:54 - V2VMetrics: ServerVoiceToVoice TTFB: 465ms
&lt;/code&gt;
    &lt;p&gt;We also output log lines with time-to-first-byte for each of our models, and several other log lines that are useful for understanding exactly where we‚Äôre ‚Äúspending our latency budget.‚Äù The Pipecat Playground shows graphs of these metrics, which is useful during development and testing. Here‚Äôs a test session with our bot running on an RTX 5090.&lt;/p&gt;
    &lt;p&gt;RTX 5090&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ASR&lt;/cell&gt;
        &lt;cell&gt;13ms&lt;/cell&gt;
        &lt;cell&gt;19ms&lt;/cell&gt;
        &lt;cell&gt;23ms&lt;/cell&gt;
        &lt;cell&gt;70ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;71ms&lt;/cell&gt;
        &lt;cell&gt;171ms&lt;/cell&gt;
        &lt;cell&gt;199ms&lt;/cell&gt;
        &lt;cell&gt;255ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;TTS&lt;/cell&gt;
        &lt;cell&gt;99ms&lt;/cell&gt;
        &lt;cell&gt;108ms&lt;/cell&gt;
        &lt;cell&gt;113ms&lt;/cell&gt;
        &lt;cell&gt;146ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;V2V&lt;/cell&gt;
        &lt;cell&gt;415ms&lt;/cell&gt;
        &lt;cell&gt;508ms&lt;/cell&gt;
        &lt;cell&gt;544ms&lt;/cell&gt;
        &lt;cell&gt;639ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;DGX Spark&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ASR&lt;/cell&gt;
        &lt;cell&gt;24ms&lt;/cell&gt;
        &lt;cell&gt;27ms&lt;/cell&gt;
        &lt;cell&gt;69ms&lt;/cell&gt;
        &lt;cell&gt;122ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;343ms&lt;/cell&gt;
        &lt;cell&gt;750ms&lt;/cell&gt;
        &lt;cell&gt;915ms&lt;/cell&gt;
        &lt;cell&gt;1669ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;TTS&lt;/cell&gt;
        &lt;cell&gt;158ms&lt;/cell&gt;
        &lt;cell&gt;185ms&lt;/cell&gt;
        &lt;cell&gt;204ms&lt;/cell&gt;
        &lt;cell&gt;1171ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;V2V&lt;/cell&gt;
        &lt;cell&gt;759ms&lt;/cell&gt;
        &lt;cell&gt;1180ms&lt;/cell&gt;
        &lt;cell&gt;1359ms&lt;/cell&gt;
        &lt;cell&gt;2981ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It‚Äôs also critical to measure the voice-to-voice latency as actually perceived by the user. This is harder to do automatically, especially for telephone call voice agents. The best approach to measuring client-side voice-to-voice latency is to record a call, load the audio file into an audio editor, and measure the gap between the end of the user‚Äôs speech waveform and the start of the bot speech waveform. You can‚Äôt cheat this measurement, or forget to include an important processing component! We do this periodically in both development and testing, as a sanity check. Here I‚Äôm measuring latency in the Descript editor of one turn in the conversation we recorded for the video at the top of this post.&lt;/p&gt;
    &lt;p&gt;You will typically see client-side voice-to-voice latency numbers about 250ms higher than server-side numbers for a WebRTC voice agent. This is time spent in audio processing at the operating system level, encoding and decoding, and network transport. Usually, this delta is a bit worse for telephone call agents: 300-600ms of extra latency in the telephony path that you don‚Äôt have much way to optimize. (Though there are some basic things you should do, such as make sure your voice agent is hosted in the same region as your telephony providers servers.) For more on latency, see the Voice AI and Voice Agents Illustrated Guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;An inference optimization for local voice agents&lt;/head&gt;
    &lt;p&gt;We have one more trick up our sleeve when we‚Äôre running voice agents locally on a single GPU.&lt;/p&gt;
    &lt;p&gt;When we run voice agents in production in the cloud, we run each AI model on a dedicated GPU. We stream tokens from each model as fast as we can, and send them down the Pipecat pipeline as they arrive.&lt;/p&gt;
    &lt;p&gt;But when we‚Äôre running locally, all the models are sharing one GPU. In this context, we can engineer much faster voice-to-voice responses if we carefully schedule inference. In our voice agent for this project, we‚Äôre doing two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We run the Smart Turn model on the CPU so that we can dedicate the GPU to transcription when user speech is arriving. The Smart Turn model runs faster on GPU, but it runs fast enough on CPU, and dividing up the workload this way gives us the best possible performance between the two models.&lt;/item&gt;
      &lt;item&gt;We interleave small segments of LLM and TTS inference so that GPU resources are dedicated to one model at a time. This significantly reduces time-to-first-token for each model. First we generate a few small chunks of LLM tokens, then TTS audio, then LLM again, then TTS, etc. We generate a smaller segment for the very first response, so we can start audio playout as quickly as possible. We designed this interleaved chunking approach to work in concert with the hybrid Magpie streaming hack described above.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs a sequence diagram showing the interleaved LLM and TTS inference. The three vertical lines in the diagram represent, from left to right:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Tokens arriving in small batches to the Pipecat LLM service in the agent and being pushed down the pipeline.&lt;/item&gt;
      &lt;item&gt;The Pipecat TTS service, managing the frames from the LLM service, dividing the stream on sentence boundaries, and making inference requests to the Magpie WebSocket server running in our local Docker container.&lt;/item&gt;
      &lt;item&gt;The Magpie WebSocket server doing inference and sending back audio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We wrote a custom WebSocket inference server for Magpie, so we control the Pipecat-to-Magpie protocol completely. We‚Äôre using llama-server code from the llama.cpp project for LLM inference. Traditional inference stacks aren‚Äôt really designed to do this specific kind of chunking, so our code sets a max tokens count (&lt;code&gt;n_predict&lt;/code&gt; in llama.cpp), runs repeated small inference chunks, and does some of the buffer management client-side. This could be done more efficiently, using the llama.cpp primitives directly. Writing a perfectly optimized inference server for this interleaved design would be a fun weekend project, and is something that almost anyone with a little bit of programming experience and a willingness to go down some rabbit holes could work together with Claude Code to implement.&lt;/p&gt;
    &lt;head rend="h1"&gt;Running this voice agent&lt;/head&gt;
    &lt;p&gt;For enterprise-scale, production use, deploy this agent to the Modal GPU cloud. There are instructions in the GitHub Readme.md. Modal is a serverless GPU platform that makes it easy to deploy AI models for development or production use.&lt;/p&gt;
    &lt;p&gt;For local development, the GitHub repo has a Dockerfile for DGX Spark (arm64 + Blackwell GB10 CUDA 13.1) and RTX 5090 (x86_64 + Blackwell CUDA 13.0)&lt;/p&gt;
    &lt;p&gt;If you‚Äôre interested in building voice agents, here are some resources you might be interested in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Voice AI &amp;amp; Voice Agents Illustrated Primer&lt;/item&gt;
      &lt;item&gt;YouTube recordings of the community voice agents course sessions from last year&lt;/item&gt;
      &lt;item&gt;The Pipecat Discord, where lots of knowledgeable voice agent developers hang out.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46528045</guid><pubDate>Wed, 07 Jan 2026 16:08:36 +0000</pubDate></item><item><title>Health care data breach affects over 600k patients, Illinois agency says</title><link>https://www.nprillinois.org/illinois/2026-01-06/health-care-data-breach-affects-600-000-patients-illinois-agency-says</link><description>&lt;doc fingerprint="e6188ade5513a61a"&gt;
  &lt;main&gt;
    &lt;p&gt;The names and addresses of thousands of patients of the Illinois Department of Human Services were incorrectly made publicly viewable for the last several years, the agency said Friday.&lt;/p&gt;
    &lt;p&gt;Several maps created to assist the agency with decisions ‚Äî like where to open new offices and allocate certain resources ‚Äî were made public through incorrect privacy settings between 2021 and 2025, the Department of Human Services said in a statement.&lt;/p&gt;
    &lt;p&gt;More than 32,000 customers with the IDHS division of rehabilitation services had information publicly viewable between April 2021 and September 2025. The information included names, addresses, case numbers, case status, referral source information, region and office information and status as Division of Rehabilitation Services recipients, the agency said.&lt;/p&gt;
    &lt;p&gt;Around 670,000 Medicaid and Medicare Savings Program recipients had their addresses, case numbers, demographic information and the name of medical assistance plans publicly viewable between January 2022 and September 2025, IDHS said.&lt;/p&gt;
    &lt;p&gt;The state agency said the mapping website was unable to identify who viewed the maps, and IDHS is unaware of any misuse of personal information resulting from the data leak.&lt;/p&gt;
    &lt;p&gt;IDHS discovered the issue Sept. 22 and immediately changed the privacy settings for all maps, restricting access to authorized IDHS employees, the agency said. It also implemented a secure map policy that prohibits uploading customer data to public mapping websites.&lt;/p&gt;
    &lt;p&gt;Individuals whose information was made public will receive a notice about the leak from IDHS. The notices will include a phone number that people can call for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46528353</guid><pubDate>Wed, 07 Jan 2026 16:28:14 +0000</pubDate></item><item><title>Eat Real Food</title><link>https://realfood.gov</link><description>&lt;doc fingerprint="cab4a7579f6f9feb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Real Food&lt;lb/&gt; Starts Here&lt;/head&gt;&lt;p&gt;Better health begins on your plate‚Äînot in your medicine cabinet.&lt;lb/&gt; The new Dietary Guidelines for Americans defines real food as whole, nutrient-dense, and naturally occurring, placing them back at the center of our diets.&lt;/p&gt;&lt;head rend="h2"&gt;The State of Our Health&lt;/head&gt;&lt;head rend="h3"&gt;America is sick.&lt;lb/&gt;The data is clear.&lt;/head&gt;&lt;head rend="h3"&gt;50% of Americans have prediabetes or diabetes&lt;/head&gt;&lt;head rend="h3"&gt;75% of adults report having at least one chronic condition&lt;/head&gt;&lt;head rend="h3"&gt;90% of U.S. healthcare spending goes to treating chronic disease‚Äîmuch of which is linked to diet and lifestyle&lt;/head&gt;&lt;p&gt;For decades we've been misled by guidance that prioritized highly processed food, and are now facing rates of unprecedented chronic disease.&lt;/p&gt;&lt;p&gt;For the first time, we're calling out the dangers of highly processed foods and rebuilding a broken system from the ground up with gold-standard science and common sense.&lt;/p&gt;&lt;head rend="h2"&gt;The New Pyramid&lt;/head&gt;&lt;head rend="h2"&gt;Eat Real &lt;lb/&gt;Food&lt;/head&gt;&lt;p&gt;Our nation is finding its footing again, moving past decades of unhealthy eating and rebuilding a food culture rooted in health, science, transparency, and personal responsibility.&lt;/p&gt;&lt;head rend="h2"&gt;Key&lt;lb/&gt;Guidance&lt;/head&gt;&lt;head rend="h2"&gt;Resources&lt;/head&gt;&lt;p&gt;Explore the research, recommendations, and implementation guidance that shape the Dietary Guidelines, including the science, the policy guidance, and the everyday serving framework.&lt;/p&gt;Watch the press release&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46529237</guid><pubDate>Wed, 07 Jan 2026 17:22:09 +0000</pubDate></item><item><title>NPM to implement staged publishing after turbulent shift off classic tokens</title><link>https://socket.dev/blog/npm-to-implement-staged-publishing</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46530448</guid><pubDate>Wed, 07 Jan 2026 18:31:19 +0000</pubDate></item><item><title>Show HN: I visualized the entire history of Citi Bike in the browser</title><link>https://bikemap.nyc/</link><description>&lt;doc fingerprint="d2055de9efee4b7e"&gt;
  &lt;main&gt;
    &lt;p&gt;Search ‚åòK Play Space Random R About A Wed, Jan 1, 2025 9:41:00 AM -- rides -- fps N S W E -- RIDES -- FPS -3h -2h -1h Now No data Command Palette Search for a command to run...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46530832</guid><pubDate>Wed, 07 Jan 2026 18:57:18 +0000</pubDate></item><item><title>US will ban Wall Street investors from buying single-family homes</title><link>https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46531068</guid><pubDate>Wed, 07 Jan 2026 19:13:19 +0000</pubDate></item><item><title>ChatGPT Health</title><link>https://openai.com/index/introducing-chatgpt-health/</link><description>&lt;doc fingerprint="2c12c8739eff67b7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing ChatGPT Health&lt;/head&gt;
    &lt;p&gt;A dedicated experience in ChatGPT designed for health and wellness.&lt;/p&gt;
    &lt;p&gt;We‚Äôre introducing ChatGPT Health, a dedicated experience that securely brings your health information and ChatGPT‚Äôs intelligence together, to help you feel more informed, prepared, and confident navigating your health.&lt;/p&gt;
    &lt;p&gt;Health is already one of the most common ways people use ChatGPT, with hundreds of millions of people asking health and wellness questions each week. ChatGPT Health builds on the strong privacy, security, and data controls across ChatGPT with additional, layered protections designed specifically for health‚Äî including purpose-built encryption and isolation to keep health conversations protected and compartmentalized. You can securely connect medical records and wellness apps to ground conversations in your own health information, so responses are more relevant and useful to you. Designed in close collaboration with physicians, ChatGPT Health helps people take a more active role in understanding and managing their health and wellness‚Äîwhile supporting, not replacing, care from clinicians.&lt;/p&gt;
    &lt;p&gt;Today, health information is often scattered across portals, apps, wearables, PDFs, and medical notes‚Äîso it's hard to see the full picture, and people are left to navigate a complex healthcare system on their own. People have shared countless stories of turning to ChatGPT to help make sense of it all. In fact, health is one of the most common ways people use ChatGPT today: based on our de-identified analysis of conversations, over 230 million people globally ask health and wellness related questions on ChatGPT every week.&lt;/p&gt;
    &lt;p&gt;ChatGPT Health builds on this so responses are informed by your health information and context. You can now securely connect medical records and wellness apps‚Äîlike Apple Health, Function, and MyFitnessPal‚Äîso ChatGPT can help you understand recent test results, prepare for appointments with your doctor, get advice on how to approach your diet and workout routine, or understand the tradeoffs of different insurance options based on your healthcare patterns.&lt;/p&gt;
    &lt;p&gt;Health is designed to support, not replace, medical care. It is not intended for diagnosis or treatment. Instead, it helps you navigate everyday questions and understand patterns over time‚Äînot just moments of illness‚Äîso you can feel more informed and prepared for important medical conversations. To keep your health information protected and secure, Health operates as a separate space with enhanced privacy to protect sensitive data. Conversations in Health are not used to train our foundation models. If you start a health-related conversation in ChatGPT, we‚Äôll suggest moving into Health for these additional protections.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre interested in getting access as it becomes available, you can sign up for the waitlist(opens in a new window). We‚Äôre starting by providing access to a small group of early users to learn and continue refining the experience‚Äîusers with ChatGPT Free, Go, Plus, and Pro plans outside of the European Economic Area, Switzerland, and the United Kingdom are eligible. As we make improvements, we plan to expand access and make Health available to all users on web and iOS in the coming weeks.&lt;/p&gt;
    &lt;p&gt;Medical record integrations and some apps are available in the U.S. only, and connecting Apple Health requires iOS.&lt;/p&gt;
    &lt;p&gt;Your health information is deeply personal. That‚Äôs why Health is built as a dedicated space with added protections for sensitive health information and easy-to-use controls.&lt;/p&gt;
    &lt;p&gt;Health lives in its own space within ChatGPT, where your conversations, connected apps, and files are stored separately from your other chats. Health has separate memories, ensuring that your health context stays contained within the space. You‚Äôll still see health chats in your chat history so you can easily return to them, but the information itself stays within Health.&lt;/p&gt;
    &lt;p&gt;When helpful, ChatGPT may use context from your non-Health chats‚Äîlike a recent move or lifestyle change‚Äîto make a health conversation more relevant. However, Health information and memories never flow back into your non-Health chats, and conversations outside of Health can‚Äôt access files, conversations, or memories created within Health. You can view or delete Health memories at any time within Health or the ‚ÄúPersonalization‚Äù section of Settings.&lt;/p&gt;
    &lt;p&gt;We recognize that people share personal and sensitive information with ChatGPT. That understanding shapes how we design the security, privacy, and data controls for all of our products‚Äîfrom the start. Even before introducing ChatGPT Health, we built foundational protections across ChatGPT to give you meaningful control over your data, including temporary chats, the ability to delete chats from OpenAI‚Äôs systems within 30 days, and training our models not to retain personal information from user chats.&lt;/p&gt;
    &lt;p&gt;Conversations and files across ChatGPT are encrypted by default at rest and in transit as part of our core security architecture. Due to the sensitive nature of health data, Health builds on this foundation with additional, layered protections‚Äîincluding purpose-built encryption and isolation‚Äîto keep health conversations protected and compartmentalized. Conversations in Health are not used to train our foundation models.&lt;/p&gt;
    &lt;p&gt;You can further strengthen access controls by enabling multi-factor authentication (MFA)(opens in a new window), which adds an extra layer of protection to help prevent unauthorized access.&lt;/p&gt;
    &lt;p&gt;When you choose to connect your health data, such as medical records or wellness apps, your responses are grounded in your own health information. To enable access to trusted U.S. healthcare providers, we partner with b.well, the largest and most secure network of live, connected health data for U.S. consumers. b.well adheres to the highest industry standards in data security and privacy. You can remove access to medical records at any time in the "Apps" section of Settings.&lt;/p&gt;
    &lt;p&gt;You can also connect your Apple Health information and other wellness apps, such as Function and MyFitnessPal. Apps may only be connected to your health data with your explicit permission, even if they‚Äôre already connected to ChatGPT for conversations outside of Health. All apps available in Health must meet OpenAI‚Äôs privacy and security requirements, including collecting only the minimum data needed, and undergo additional security review specific to inclusion in Health. The first time you connect an app, we‚Äôll help you understand what types of data may be collected by the third party. And you‚Äôre always in control: disconnect an app at any time and it immediately loses access.&lt;/p&gt;
    &lt;p&gt;ChatGPT Health was developed in close collaboration with physicians around the world to provide clear and useful health information.&lt;/p&gt;
    &lt;p&gt;Over two years, we‚Äôve worked with more than 260 physicians who have practiced in 60 countries and dozens of specialties to understand what makes an answer to a health question helpful or potentially harmful‚Äîthis group has now provided feedback on model outputs over 600,000 times across 30 areas of focus. This collaboration has shaped not just what Health can do, but how it responds: how urgently to encourage follow-ups with a clinician, how to communicate clearly without oversimplifying, and how to prioritize safety in moments that matter.&lt;/p&gt;
    &lt;p&gt;This physician-led approach is built directly into the model that powers Health, which is evaluated against clinical standards using HealthBench, an assessment framework we created with input from our network of practicing physicians. Rather than relying on exam-style questions or generic accuracy checks, HealthBench evaluates responses using physician-written rubrics that reflect how clinicians judge quality in practice‚Äîprioritizing safety, clarity, appropriate escalation of care, and respect for individual context.&lt;/p&gt;
    &lt;p&gt;This evaluation-driven approach helps ensure the model performs well on the tasks people actually need help with, including explaining lab results in accessible language, preparing questions for an appointment, interpreting data from wearables and wellness apps, and summarizing care instructions. The result is support that people can trust‚Äîalways designed to support, not replace, your healthcare providers.&lt;/p&gt;
    &lt;p&gt;You can sign up for the waitlist(opens in a new window) to request access.&lt;/p&gt;
    &lt;p&gt;Select ‚ÄòHealth‚Äô from the sidebar menu in ChatGPT.&lt;/p&gt;
    &lt;p&gt;Bring your medical records and the apps you use to track your health and wellness into Health. You can upload files directly, connect from tools (+) or ‚ÄúApps‚Äù in Settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New: Medical Records for lab results, visit summaries, and clinical history&lt;/item&gt;
      &lt;item&gt;New: Apple Health for health and fitness data, including movement, sleep, and activity patterns (must be on iOS to sync)&lt;/item&gt;
      &lt;item&gt;New: Function for lab test insights, nutrition ideas, and taking action on your health&lt;/item&gt;
      &lt;item&gt;New: MyFitnessPal for nutrition advice, macros, and recipes&lt;/item&gt;
      &lt;item&gt;New: Weight Watchers for GLP-1 personalized meal ideas, recipes, and food guidance&lt;/item&gt;
      &lt;item&gt;AllTrails to help you find your next hike&lt;/item&gt;
      &lt;item&gt;Instacart to turn meal plans into shoppable lists&lt;/item&gt;
      &lt;item&gt;Peloton for suggested workout classes or guided meditations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Health conversations feel just like chatting with ChatGPT‚Äîbut grounded in the information you‚Äôve connected. You can upload photos and files and use search, deep research, voice mode and dictation. When relevant, ChatGPT can automatically reference your connected information to provide more relevant and personalized responses. For example, you might ask: ‚ÄúHow‚Äôs my cholesterol trending?‚Äù or ‚ÄúCan you summarize my latest bloodwork before my appointment?‚Äù To use a connected app you can start your question with it, select it from tools (+) or ChatGPT may suggest one when helpful.&lt;/p&gt;
    &lt;p&gt;You can add custom instructions in Health to help ChatGPT know what to focus on, to avoid mentioning sensitive topics, or change how responses are framed. These instructions only apply to Health chats, and you can update or remove any time in Health or Settings.&lt;/p&gt;
    &lt;p&gt;We‚Äôll continue to expand what you can connect and the insights Health can support‚Äîso ChatGPT can help you feel more informed, prepared, and confident as you navigate your health.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46531280</guid><pubDate>Wed, 07 Jan 2026 19:29:39 +0000</pubDate></item><item><title>Notion AI: Unpatched data exfiltration</title><link>https://www.promptarmor.com/resources/notion-ai-unpatched-data-exfiltration</link><description>&lt;doc fingerprint="98559bad08f84487"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;Notion AI: Unpatched Data Exfiltration&lt;/head&gt;
    &lt;p&gt;Notion AI is susceptible to data exfiltration via indirect prompt injection due to a vulnerability in which AI document edits are saved before user approval.&lt;/p&gt;
    &lt;p&gt;Notion AI allows users to interact with their documents using natural language√¢¬¶ but what happens when AI edits are made prior to user approval?&lt;lb/&gt;In this article, we document a vulnerability that leads Notion AI to exfiltrate user data (a sensitive hiring tracker document) via indirect prompt injection. Users are warned about an untrusted URL and asked for approval to interact with it - but their data is exfiltrated before they even respond.&lt;/p&gt;
    &lt;p&gt;We responsibly disclosed this vulnerability to Notion via HackerOne. Unfortunately, they said √¢we're closing this finding as `Not Applicable`√¢.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stealing Hiring Tracker Data with a Poisoned Resume&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The user uploads a resume (untrusted data) to their chat session.&lt;/p&gt;&lt;lb/&gt;Here, the untrusted data source is a resume PDF, but a prompt injection could be stored in a web page, connected data source, or a Notion page.&lt;p&gt;This document contains a prompt injection hidden in 1 point font white on white text with a square white image covering the text for good measure. The LLM can read it with no issues, but the document appears benign to the human eye.&lt;/p&gt;&lt;lb/&gt;A Note on Defenses: Notion AI uses an LLM to scan document uploads and present a warning if a document is flagged as malicious. As this warning is triggered by an LLM, it can be bypassed by a prompt injection that convinces the evaluating model that the document is safe. For this research, we did not focus on bypassing this warning because the point of the attack is the exfiltration mechanism, not the method of injection delivery. In practice, an injection could easily be stored in a source that does not appear to be scanned, such as a web page, Notion page, or connected data source like Notion Mail.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The user asks Notion AI for help updating a hiring tracker based on the resume.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Notion AI is manipulated by the prompt injection to insert a malicious image into the hiring tracker.&lt;/p&gt;&lt;lb/&gt;The prompt injection manipulates Notion AI to (1) construct a URL by collecting all of the text in the document and appending the data to an attacker-controlled domain, and (2) insert an √¢image√¢ into the Notion Page using the constructed URL as the image source.&lt;p&gt;Here, it appears as though the user is prompted for approval. However, unbeknownst to the user, the edit has already occurred before the user is prompted for approval. When the edit occurred, the user√¢s browser made a request to the attacker√¢s server, attempting to retrieve the image. This request exfiltrates the document contents contained in the URL constructed by Notion AI.&lt;/p&gt;&lt;lb/&gt;Whether or not the user accepts the edit, the attacker successfully exfiltrates the data.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The attacker reads the sensitive hiring tracker data from their server logs.&lt;/p&gt;&lt;lb/&gt;Once the user√¢s browser has made a request for the malicious image, the attacker can read the sensitive data contained in the URL from their request logs.&lt;p&gt;In this attack, exfiltrated data included salary expectations, candidate feedback, internal role details, and other sensitive information such as diversity hiring goals.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Additional Attack Surface&lt;/head&gt;
    &lt;p&gt;The Notion Mail AI drafting assistant is susceptible to rendering insecure Markdown images within email drafts, resulting in data exfiltration. If a user mentions an untrusted resource while drafting, content from the user√¢s query or other mentioned resources can be exfiltrated. E.g., √¢Hey, draft me an email based on @untrusted_notion_page and @trusted_notion_page√¢.&lt;lb/&gt;The attack surface is reduced for Notion Mail√¢s drafting assistant as the system appears to only have access to data sources within the Notion ecosystem that are explicitly mentioned by the user (as opposed to Notion AI√¢s main offering, which supports web search, document upload, integrations, etc.).&lt;/p&gt;
    &lt;head rend="h3"&gt;Recommended Remediations for Organizations:&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Institute a vetting process for connected data sources. Restrict use of connectors that can access highly sensitive or highly untrusted data from: Settings &amp;gt; Notion AI &amp;gt; Connectors.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To reduce the risk of untrusted data being processed in the workspace, admins can configure: Settings &amp;gt; Notion AI &amp;gt; AI Web Search &amp;gt; Enable web search for workspace &amp;gt; Off.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Individual users should avoid including sensitive personal data that could be leveraged in a spearphishing attack when configuring personalization for Notion AI via: Settings &amp;gt; Notion AI &amp;gt; Personalization.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Individual users can configure: Settings &amp;gt; Notion AI &amp;gt; AI Web Search &amp;gt; Require confirmation for web requests &amp;gt; On.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Implementing these remediations will reduce the risk surface, but will not nullify the core vulnerability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Recommended Remediations for Notion:&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Programmatically prohibit automatic rendering of Markdown images from external sites in Notion AI page creation or update outputs without explicit user approval.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Programmatically prohibit automatic rendering of Markdown images from external sites in Notion AI mail drafts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Implement a strong Content Security Policy. This will prevent network requests from being made to unapproved external domains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ensure the CDN used to retrieve images for display in Notion and image previews for display in Notion Mail cannot be used as an open redirect to bypass the CSP policy that is set.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Responsible Disclosure Timeline&lt;/head&gt;
    &lt;p&gt;12/24/2025 Initial report made via HackerOne&lt;lb/&gt;12/24/2025 Report is acknowledged, altered write-up requested&lt;lb/&gt;12/24/2025 PromptArmor follows up with the requested format&lt;lb/&gt;12/29/2025 Report closed as non-applicable&lt;lb/&gt;01/07/2025 Public disclosure&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46531565</guid><pubDate>Wed, 07 Jan 2026 19:49:54 +0000</pubDate></item><item><title>Claude Code Emergent Behavior: When Skills Combine</title><link>https://vibeandscribe.xyz/posts/2025-01-07-emergent-behavior.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46531794</guid><pubDate>Wed, 07 Jan 2026 20:06:53 +0000</pubDate></item><item><title>Tailscale state file encryption no longer enabled by default</title><link>https://tailscale.com/changelog</link><description>&lt;doc fingerprint="868f307d7f22bbd0"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Tailscale v1.92.5&lt;/head&gt;Update instructions&lt;head rend="h5"&gt;Linux&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;State file encryption and hardware attestation keys are no longer enabled by default.&lt;/item&gt;&lt;item&gt;Failure to load hardware attestation keys no longer prevents the client from starting. This could happen when the TPM device is reset or replaced.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h5"&gt;Windows&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;State file encryption and hardware attestation keys are no longer enabled by default.&lt;/item&gt;&lt;item&gt;Failure to load hardware attestation keys no longer prevents the client from starting. This could happen when the TPM device is reset or replaced.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale container image v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale container image is available. You can download it from Docker Hub or from our GitHub packages repository.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Hardware attestation keys are no longer added to Kubernetes state &lt;code&gt;Secrets&lt;/code&gt;, making it possible to change the Kubernetes node the Tailscale containers are deployed on.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale Kubernetes Operator v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale Kubernetes Operator is available. For guidance on installing and updating, refer to our installation instructions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Certificate renewal is no longer done as an ARI order by default to avoid renewal failure if ACME account keys are recreated.&lt;/item&gt;&lt;item&gt;Hardware attestation keys are no longer added to Kubernetes state &lt;code&gt;Secrets&lt;/code&gt;, making it possible to change the Kubernetes node the Tailscale Kubernetes Operator is deployed on.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale tsrecorder v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale &lt;code&gt;tsrecorder&lt;/code&gt; is available. You can download it from Docker Hub.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Note: This version contains no changes except for library updates.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46531925</guid><pubDate>Wed, 07 Jan 2026 20:16:50 +0000</pubDate></item><item><title>SSDs, power loss protection and fsync latency</title><link>http://smalldatum.blogspot.com/2026/01/ssds-power-loss-protection-and-fsync.html</link><description>&lt;doc fingerprint="8f501c7c01e35b82"&gt;
  &lt;main&gt;
    &lt;p&gt;This has results to measure the impact of calling fsync (or fdatasync) per-write for files opened with O_DIRECT. My goal is to document the impact of the innodb_flush_method option.&lt;/p&gt;
    &lt;p&gt;The primary point of this post is to document the claim:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For an SSD without power loss protection, writes are fast but fsync is slow.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The secondary point of this post is to provide yet another example where context matters when reporting performance problems. This post is motivated by results that look bad when run on a server with slow fsync but look OK otherwise.&lt;/p&gt;
    &lt;p&gt;tl;dr&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;for my mini PCs I will switch from the Samsung 990 Pro to the Crucial T500 to get lower fsync latency. Both are nice devices but the T500 is better for my use case.&lt;/item&gt;
      &lt;item&gt;with a consumer SSD writes are fast but fsync is often slow&lt;/item&gt;
      &lt;item&gt;use an enterprise SSD if possible, if not run tests to understand fsync and fdatasync latency&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I am not surprised that Tanel Poder has a great blog post on this topic&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;InnoDB, O_DIRECT and O_DIRECT_NO_FSYNC&lt;/p&gt;
    &lt;p&gt;When innodb_flush_method is set to O_DIRECT there are calls to fsync after each batch of writes. While I don't know the source like I used to, I did browse it for this blog post and then I looked at SHOW GLOBAL STATUS counters. I think that InnoDB does the following with it set to O_DIRECT:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Do one large write to the doublewrite buffer, call fsync on that file&lt;/item&gt;
      &lt;item&gt;Do the batch of in-place (16kb) page writes&lt;/item&gt;
      &lt;item&gt;Call fsync once per database file that was written by step 2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When set to O_DIRECT_NO_FSYNC then the frequency of calls to fsync are greatly reduced and are only done in cases where important filesystem metadata needs to be updated, such as after extending a file. The reference manual is misleading WRT the following sentence. I don't think that InnoDB ever does an fsync after each write. It can do an fsync after each batch of writes:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;O_DIRECT_NO_FSYNC&lt;/code&gt;:¬†&lt;code&gt;InnoDB&lt;/code&gt;¬†uses¬†&lt;code&gt;O_DIRECT&lt;/code&gt;¬†during flushing I/O, but skips the¬†&lt;code&gt;fsync()&lt;/code&gt;¬†system call after each write operation.&lt;/p&gt;
    &lt;p&gt;Many years ago it was risky to use O_DIRECT_NO_FSYNC on some filesystems because the feature as implemented (either upstream or in forks) didn't do fsync for cases where it was needed (see comment about metadata above). I experienced problems from this and I only have myself to blame. But the feature has been enhanced to do the right thing. And if the #whynotpostgres crowd wants to snark about MySQL not caring about data, lets not forget that InnoDB had per-page checksums long before Postgres -- those checksums made web-scale life much easier when using less than stellar hardware.&lt;/p&gt;
    &lt;quote&gt;Innodb_data_fsyncs / Innodb_data_writes&lt;/quote&gt;
    &lt;p&gt;And from this table a few things are clear. First, there isn't an fsync per write with O_DIRECT but there might be an fsync per batch of writes as explained above. Second, the rate of fsyncs is greatly reduced by using O_DIRECT_NO_FSYNC.&lt;/p&gt;
    &lt;p&gt;.00172 .00053 O_DIRECT_NO_FSYNC&lt;/p&gt;
    &lt;p&gt;Power loss protection&lt;/p&gt;
    &lt;p&gt;I am far from an expert on this topic, but most SSDs have a write-buffer that makes small writes fast. And one way to achieve speed is to buffer those writes in RAM on the SSD while waiting for enough data to be written to an extent. But that speed means there is a risk of data loss if a server loses power. Some SSDs, especially those marketed as enterprise SSDs, have a feature called power loss protection that make data loss unlikely. Other SSDs, lets call them consumer SSDs, don't have that feature while some of the consumer SSDs claim to make a best effort to flush writes from the write buffer on power loss.&lt;/p&gt;
    &lt;p&gt;One solution to avoiding risk is to only buy enterprise SSDs. But they are more expensive, less common, and many are larger (22120 rather than 2280) because more room is needed for the capacitor or other HW that provides the power loss protection. Note that power loss protection is often abbreviated as PLP.&lt;/p&gt;
    &lt;p&gt;For devices without power loss protection it is often true that writes are fast but fsync is slow. When fsync is slow then calling fsync more frequently in InnoDB will hurt performance.&lt;/p&gt;
    &lt;p&gt;Results from fio&lt;/p&gt;
    &lt;p&gt;I used this fio script to measure performance for writes for files opened with O_DIRECT. The test was run twice configuration for 5 minutes per run followed by a 5 minute sleep. This was repeated for 1, 2, 4, 8, 16 and 32 fio jobs but I only share results here for 1 job. The configurations tested were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;O_DIRECT without fsync, 16kb writes&lt;/item&gt;
      &lt;item&gt;O_DIRECT with an fsync per write, 16kb writes&lt;/item&gt;
      &lt;item&gt;O_DIRECT with an fdatasync per write, 16kb writes&lt;/item&gt;
      &lt;item&gt;O_DIRECT without fsync, 2M writes&lt;/item&gt;
      &lt;item&gt;O_DIRECT with an fsync per write, 2M writes&lt;/item&gt;
      &lt;item&gt;O_DIRECT with an fdatasync per write, 2M writes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;dell32&lt;/item&gt;
      &lt;item&gt;a large server I have at home. The SSD is a Crucial T500 2TB using ext-4 with discard enabled and Ubuntu 24.04. This is a consumer SSD. While the web claims it has PLP via capacitors the fsync latency for it was almost 1 millisecond.&lt;/item&gt;
      &lt;item&gt;gcp&lt;/item&gt;
      &lt;item&gt;a c3d-standard-30-lssd from the Google cloud with 2 local NVMe devices using SW RAID 0 and 1TB of Hyperdisk Balanced storage configured for 50,000 IOPs and 800MB/s of throughput. The OS is Ubuntu 24.04 and I repeated tests for both ext-4 and xfs, both with discard enabled. I was not able to determine the brand of the local NVMe devices.&lt;/item&gt;
      &lt;item&gt;hetz&lt;/item&gt;
      &lt;item&gt;an ax162-s from Hetzner with 2 local NVME devices using SW RAID 1. Via udiskctl status I learned the devices are Intel D7-P5520 (now Solidigm). These are datacenter SSDs and the web claims they have power loss protection. The OS is Ubuntu 24.04 and the drives use ext-4 without discard enabled.&lt;/item&gt;
      &lt;item&gt;ser7&lt;/item&gt;
      &lt;item&gt;a mini-PC I have at home. The SSD is a Samsung 990 Pro using ext-4 with discard enabled and Ubuntu 24.04. This is a consumer SSD, the web claims it does not have PLP and fsync latency is several milliseconds.&lt;/item&gt;
      &lt;item&gt;socket2&lt;/item&gt;
      &lt;item&gt;a 2-socket server I have at home. The SSD is a Samsung PM-9a3. This is an enterprise SSD with power loss protection. The OS is Ubuntu 24.04 and the drives use ext-4 with discard enabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;for servers with consumer SSDs (dell, ser7) the latency is much larger on the ser7 that uses a Samsung 990 Pro than on the dell that uses a Crucial T500. This is to be expected given that the T500 has PLP while the 990 Pro does not.&lt;/item&gt;
      &lt;item&gt;sync latency is much lower on servers with enterprise SSDs&lt;/item&gt;
      &lt;item&gt;sync latency after 2M writes is sometimes much larger than after 16kb writes&lt;/item&gt;
      &lt;item&gt;for the Google server with Hyperdisk Balanced storage the fdatasync latency was good but fsync latency was high. While with the local NVMe devices the latencies were larger than for enterprise SSDs but much smaller than for consumer SSDs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Write throughput drops dramatically when there is an fsync or fdatasync per write because sync latency is large.&lt;/item&gt;
      &lt;item&gt;This servers uses a consumer SSD so high sync latency is expected&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;w/s - writes/s&lt;/item&gt;
      &lt;item&gt;MB/s - MB written/s&lt;/item&gt;
      &lt;item&gt;sync - latency per sync (fsync or fdatasync)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local NVMe devices have lower sync latency and more throughput with and without a sync per write at low concurrency (1 fio job).&lt;/item&gt;
      &lt;item&gt;At higher concurrency (32 fio jobs), the Hyperdisk Balanced setup provides similar throughput to local NVMe and would do even better had I paid more to get more IOPs and throughput. Results don't have nice formatting but are here for xfs on the local and Hyperdisk Balanced devices.&lt;/item&gt;
      &lt;item&gt;fsync latency is ~2X larger than fdatasync on the local devices and closer to 15X larger on the Hyperdisk Balanced setup. That difference is interesting. I wonder what the results are for Hyperdisk Extreme.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;w/s - writes/s&lt;/item&gt;
      &lt;item&gt;MB/s - MB written/s&lt;/item&gt;
      &lt;item&gt;sync - latency per sync (fsync or fdatasync)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;this has an enterprise SSD with excellent (low) sync latency&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;w/s - writes/s&lt;/item&gt;
      &lt;item&gt;MB/s - MB written/s&lt;/item&gt;
      &lt;item&gt;sync - latency per sync (fsync or fdatasync)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;this has a consumer SSD with high sync latency&lt;/item&gt;
      &lt;item&gt;results had much variance (see the 2MB results below) and results at higher concurrency. This is a great SSD, but not for my use case.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;w/s - writes/s&lt;/item&gt;
      &lt;item&gt;MB/s - MB written/s&lt;/item&gt;
      &lt;item&gt;sync - latency per sync (fsync or fdatasync)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;this has an enterprise SSD with excellent (low) sync latency after small writes, but fsync latency after 2MB writes is much larger&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;w/s - writes/s&lt;/item&gt;
      &lt;item&gt;MB/s - MB written/s&lt;/item&gt;
      &lt;item&gt;sync - latency per sync (fsync or fdatasync)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46532675</guid><pubDate>Wed, 07 Jan 2026 21:00:57 +0000</pubDate></item><item><title>Show HN: An LLM response cache that's aware of dynamic data</title><link>https://blog.butter.dev/on-automatic-template-induction-for-response-caching</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46532755</guid><pubDate>Wed, 07 Jan 2026 21:04:58 +0000</pubDate></item><item><title>So you wanna de-bog yourself (2024)</title><link>https://www.experimental-history.com/p/so-you-wanna-de-bog-yourself</link><description>&lt;doc fingerprint="248c99510747859d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;So you wanna de-bog yourself&lt;/head&gt;
    &lt;head rend="h3"&gt;What I found in the mire&lt;/head&gt;
    &lt;p&gt;Strangers sometimes ask me for advice, which is both flattering and alarming, because I only know about the things I write here, and sometimes not even those.&lt;/p&gt;
    &lt;p&gt;For instance, someone recently asked me if I had any advice about how to teach people to fly planes, which makes me wonder: who‚Äôs running the pilot education system?? Now, whenever I get on a plane, I scrutinize the captain to see if they have that ‚ÄúA blogger taught me how to fly‚Äù kind of look.&lt;/p&gt;
    &lt;p&gt;I often don't know how to respond to such questions, on account of my general incompetence. But I've realized that most of these folks have something in common: they're stuck. They‚Äôre looking for advice less in the sense of ‚Äúany good restaurants around here?‚Äù and more in the sense of ‚Äúeverything kinda sucks right now and I‚Äôd like to change that but I don‚Äôt know how?‚Äù&lt;/p&gt;
    &lt;p&gt;Being stuck is the psychological equivalent of standing knee-deep in a fetid bog, bog in every direction, bog as far as the eye can see. You go wading in search of dry land and only find more bog. Nothing works, no options seem good, it‚Äôs all bleh and meh and ho hum and no thanks and more bog. This is the kind of dire situation that drives people to do crazy things like ask a blogger for advice.&lt;/p&gt;
    &lt;p&gt;Fortunately, I‚Äôve spent much of my life in that very bog. Some say I was born in it, a beautiful bouncing baby bog boy. And I've learned that no matter how you ended up there‚Äîyour marriage has stalled, you're falling behind in your classes, your trainee pilots keep flying into the side of a mountain‚Äîthe forces that keep you in the bog are always the same. There are, in fact, only three, although they each come in a variety of foul flavors.&lt;/p&gt;
    &lt;p&gt;It's a new year, the annual Great De-bogging, when we all attempt to heave ourselves out of the muck and into a better life. So here, to aid you, is my compendium of bog phenomena, the myriad ways I get myself stuck, because unsticking myself always seems to be a matter of finding a name for the thing happening to me.1 May this catalog serve you well, and may your planes always be flown by people who never learned anything from me.&lt;/p&gt;
    &lt;head rend="h1"&gt;1. INSUFFICIENT ACTIVATION ENERGY&lt;/head&gt;
    &lt;p&gt;Most of my attempts to get unstuck look, from the outside, like I'm doing nothing at all. I'm standing motionless in the bog, crying, ‚ÄúTHIS IS ME TRYING!‚Äù That means I've got insufficient activation energy‚ÄîI can't muster the brief but extraordinary output of effort it takes to escape the bog, so I stay right where I am.&lt;/p&gt;
    &lt;p&gt;There are few different ways to end up here.&lt;/p&gt;
    &lt;head rend="h4"&gt;Gutterballing&lt;/head&gt;
    &lt;p&gt;People will sometimes approach me with projects I don't really want to do. But if I do them, those people will smile and shake my hand and go, ‚ÄúWe feel positive emotions, and it's because of you!‚Äù and that will feel good. So I often end up signing on to these projects, feeling resentful the whole time, cursing myself for choosing‚Äîfreely!‚Äîto work hard on things I don't care about.&lt;/p&gt;
    &lt;p&gt;This is gutterballing: excelling, but in slightly the wrong direction. For most of its journey, after all, the gutterball is getting closer to the pins. It's only at the end that it barely, but dramatically, misses.&lt;/p&gt;
    &lt;p&gt;Gutterballing is a guaranteed way to stay stuck in the bog because people will love you for it. ‚ÄúYou're doing the right thing!‚Äù they'll shout as you sink into the swamp. ‚ÄúWe approve of this!‚Äù&lt;/p&gt;
    &lt;head rend="h4"&gt;Waiting for jackpot&lt;/head&gt;
    &lt;p&gt;Sometimes when I'm stuck, someone will be like, ‚ÄúWhy don't you do [reasonable option]?‚Äù and I'll go, ‚ÄúHold on there, buddy! Don't you see this option has downsides? Find me one with only upsides, and then we'll talk!‚Äù&lt;/p&gt;
    &lt;p&gt;I'm waiting for jackpot, refusing to do anything until an option arises that dominates all other options on all dimensions. Strangely, this never seems to happen.&lt;/p&gt;
    &lt;p&gt;Often, I'm waiting for the biggest jackpot of all: the spontaneous remission of all my problems without any effort required on my part. Someone suggests a way out of my predicament and I go, ‚ÄúHmm, I dunno, do you have any solutions that involve me doing everything 100% exactly like I'm doing it right now, and getting better outcomes?‚Äù&lt;/p&gt;
    &lt;head rend="h4"&gt;Declining the dragon&lt;/head&gt;
    &lt;p&gt;Okay, this is a version of waiting for jackpot, but it's so common that it deserves its own entry.&lt;/p&gt;
    &lt;p&gt;Sometimes I'll know exactly what I need to do in order to leave the bog, but I'm too afraid to do it. I'm afraid to tell the truth, or make someone mad, or take a risk. And so I dither, hoping that the future will not require me to be brave.&lt;/p&gt;
    &lt;p&gt;Everybody thinks this is a bad strategy because it merely prolongs my suffering, but that's not why it's a dumb thing to do. Yes, every moment I dither is a moment I suffer. But when I finally do the brave thing, that's not the climax of my suffering‚Äîthat moment is the opposite of suffering. Being brave feels good. I mean, have you ever stood up to a bully, or conquered stage fright, or finally stopped being embarrassed about what you love? It's the most wonderful feeling in the world. Whenever you chicken out, you don't just feel the pain of cowardice; you miss out on the pleasure of courage.&lt;/p&gt;
    &lt;p&gt;Medieval knights used to wander around hoping for honorable adventures to pop up so that they could demonstrate their bravery. They were desperate for big, scary dragons to appear. When I put off doing the brave thing, I am declining the dragon: missing an opportunity to do something that might be scary in the moment but would ultimately make me feel great.&lt;/p&gt;
    &lt;head rend="h4"&gt;The mediocrity trap&lt;/head&gt;
    &lt;p&gt;About half of my friends kind of hate their jobs, so they're moderately unhappy most of the time, but never unhappy enough to leave. This is the mediocrity trap: situations that are bad-but-not-too-bad keep you forever in their orbit because they never inspire the frustration it takes to achieve escape velocity.&lt;/p&gt;
    &lt;p&gt;The mediocrity trap is a nasty way to end up in the bog. Terrible situations, once exited, often become funny stories or proud memories. Mediocre situations, long languished in, simply become Lost Years‚Äîboring to both live through and talk about, like you're sitting in a waiting room with no cell reception, no wifi, and no good magazines, waiting for someone to come in and tell you it's time to start living.&lt;/p&gt;
    &lt;p&gt;(I have previously written about this phenomenon as an underrated idea in psychology.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Stroking the problem&lt;/head&gt;
    &lt;p&gt;I spend a lot of time thinking about my problems, but it usually looks like this:&lt;/p&gt;
    &lt;p&gt;‚ÄúOh boy, what a problem! A real whopper, I'd say. Massive, even. Get a load of this problem, would ya! Wowzers!‚Äù I can spend days doing this. ‚ÄúHow big would you say that problem is? Large? Huge? And that's just its size! Don't get me started on its depth.‚Äù&lt;/p&gt;
    &lt;p&gt;This isn't solving the problem; this is stroking the problem. It looks like a good use of time, but it's just a form of socially acceptable anxiety, a way to continue your suffering indefinitely by becoming obsessed with it.&lt;/p&gt;
    &lt;head rend="h1"&gt;2. BAD ESCAPE PLAN&lt;/head&gt;
    &lt;p&gt;Even if you've worked up a big enough head of steam to launch yourself out of the bog, you still have to aim properly. (‚ÄúI‚Äôm doing it! I'm doing it!‚Äù I shout as I crash land onto my launch pad.)&lt;/p&gt;
    &lt;p&gt;Here are a few of my recurring bad escape plans:&lt;/p&gt;
    &lt;head rend="h4"&gt;The ‚Äútry harder‚Äù fallacy&lt;/head&gt;
    &lt;p&gt;I played a lot of Call of Duty in high school, and I used to roll with a gang of bad boys who would battle other gangs online.2&lt;/p&gt;
    &lt;p&gt;We weren't very good. Whenever we lost the first round, which was almost always, we would regroup in the pregame lobby‚Äîbasically the online locker room‚Äîand decide what we really need to do in the next round is ‚Äútry harder.‚Äù As if the reason we had all just been shot in the head 25 times in a row was that we were not sufficiently dedicated to avoiding getting shot in the head. Armed with the most dimwit plan of all time, we would march into battle once more and lose just as badly. As our virtual corpses piled up, we'd yell at each other, ‚ÄúGuys, stop dying!‚Äù&lt;/p&gt;
    &lt;p&gt;This is the try harder fallacy. I behold my situation and conclude that, somehow, I will improve it in the future by just sort of wishing it to be different, and then I get indignant that nothing happens. Like, ‚ÄúUm, excuse me! I've been doing all of this very diligent desiring for things to be different, and yet they remain the same, could someone please look into this?‚Äù3&lt;/p&gt;
    &lt;head rend="h4"&gt;The infinite effort illusion&lt;/head&gt;
    &lt;p&gt;The try harder fallacy has a cousin called the infinite effort illusion, which is the idea that you have this secret unused stock of effort that you can deploy in the future to get yourself unstuck. I'm always a week late responding to emails? No problem, I'll simply uncork my Strategic Effort Reserve and clear my correspondence debt.&lt;/p&gt;
    &lt;p&gt;This never works because there is no Strategic Effort Reserve. All of my effort is currently accounted for somewhere. If I want to spend more of it on something, I have to spend less of it on something else. If I‚Äôm consistently not getting something done, it‚Äôs probably because I don‚Äôt want to‚Äîat least, not enough to cannibalize that time from something else‚Äîand I haven‚Äôt admitted that to myself yet.&lt;/p&gt;
    &lt;head rend="h4"&gt;Blaming God&lt;/head&gt;
    &lt;p&gt;I spend a lot of stints in the bog wailing about how I don‚Äôt have enough time. ‚ÄúOh, if there were only 25 hours in the day,‚Äù I lament, ‚Äúthe things I would accomplish!‚Äù&lt;/p&gt;
    &lt;p&gt;But here‚Äôs a stupid question: what am I mad about, exactly? That I don't have a time-turner? That I can‚Äôt find a little eddy in the spacetime continuum where I can hide out while I cross a few more things off my to-do list? Do I really believe that the way to get unstuck is to ruminate on how unfair it is that time marches ever forward at one second per second?&lt;/p&gt;
    &lt;p&gt;This is blaming God: pinning the responsibility for my current predicament on something utterly unchangeable. And while many religions teach that God intervenes in human affairs, none of them, as far as I know, believe that he responds to whining. (Would you worship a god who does miracles if you just annoy him enough?)&lt;/p&gt;
    &lt;head rend="h4"&gt;Diploma problems and toothbrushing problems&lt;/head&gt;
    &lt;p&gt;Some problems are like getting a diploma: you work at it for a while, and then you're done forever. Learning how to ride a bike is a classic diploma problem.&lt;/p&gt;
    &lt;p&gt;But most problems aren‚Äôt like that. They‚Äôre more like toothbrushing problems: you have to work at them forever until you die. You can‚Äôt, as far as I know, just brush your teeth really really well and then let ‚Äòem ride forever.&lt;/p&gt;
    &lt;p&gt;When I had a skull full of poison, I assumed feeling good again was a diploma problem. I just had to find the right lever to pull and‚Äîyoink!‚Äîback to the good times forever. People warned me it wasn't going to be like this and I didn't believe them; I assumed they had simply failed to earn their diplomas.&lt;/p&gt;
    &lt;p&gt;I only started making progress when I realized I was facing a toothbrushing problem: feeling normal again would probably require me to do stuff every day for the rest of my life. I might get better at doing that stuff, just like when you first start brushing your teeth as a kid you get toothpaste everywhere and end up swallowing half of it, and eventually you learn not to do that. But even when you're a toothbrushing expert, it still takes you a couple minutes every day. You could be mad about that, but it won‚Äôt make your teeth any cleaner.&lt;/p&gt;
    &lt;head rend="h4"&gt;Fantastical metamorphosis&lt;/head&gt;
    &lt;p&gt;Here‚Äôs one of my favorite bad escape plans: I‚Äôll just be a different person in the future. Like, ‚ÄúI know I hate working out, but in the future I will overcome this by not being such a baby about it.‚Äù Or, ‚ÄúI find quantum physics boring, so I‚Äôll just learn about it later, when I find it more interesting.‚Äù&lt;/p&gt;
    &lt;p&gt;These are fantastical metamorphoses. I have not, so far, woken up one day and found myself different in all the ways that would make my life easier. I do hope this happens, but I‚Äôve stopped betting on it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Puppeteering&lt;/head&gt;
    &lt;p&gt;People are always causing me problems by doing foolish things like trying to drive on highways while I'm also trying to drive on them, or expecting me to pay rent every month, or not realizing my genius and putting me in charge of things. In these cases, it feels like the only solution is to get other people to act differently. I'm only stuck because other people are unreasonable!&lt;/p&gt;
    &lt;p&gt;A good word for this is puppeteering: trying to solve your problems by controlling the actions of other humans. Puppeteering often looks attractive because other people's actions seem silly and therefore easily changeable. Funnily enough, it doesn't feel that way to them. They have lifetimes of backstory that lead them to act the way that they do, and their actions are, on average, only as changeable as yours. So unless you think of yourself as being easily redirected with a few tugs of your strings, puppeteering is probably not going to get you out of the bog.&lt;/p&gt;
    &lt;head rend="h1"&gt;3. A BOG OF ONE'S OWN&lt;/head&gt;
    &lt;p&gt;A confession: most of my bogs are imaginary. The world doesn‚Äôt stick me there; I stick me there. These are, paradoxically, the most difficult bogs to escape, because it requires realizing that my perception of reality is not reality, and a lot of the mind is dedicated to preventing that exact thought.&lt;/p&gt;
    &lt;head rend="h4"&gt;Floor is lava&lt;/head&gt;
    &lt;p&gt;Every kid learns to play the ‚Äúfloor is lava‚Äù game, where you pretend that you'll get incinerated if you touch the carpet. Even toddlers can pick it up, which reveals something profound: very early on, we acquire the ability to pretend that fake problems are real. We then spend the rest of our lives doing exactly that.&lt;/p&gt;
    &lt;p&gt;Often, when I‚Äôm stuck, it‚Äôs because I've made up a game for myself and decided that I‚Äôm losing at it. I haven‚Äôt achieved enough. I am not working hard enough and I am also, somehow, not having enough fun. These games have elaborate rules, like ‚ÄúI have to be as successful as my most successful friend, but everything I've done so far doesn't count,‚Äù and I‚Äôm supposed to feel very bad if I break them. It‚Äôs like playing the absolute dumbest version of the floor is lava.&lt;/p&gt;
    &lt;p&gt;Did I create these games by thinking really hard about how to live a good life? No! I pulled them out of my butt. Or someone else pulled them out of their butt, and I said, ‚ÄúOoh, can I have some of that?‚Äù&lt;/p&gt;
    &lt;head rend="h4"&gt;Super surveillance&lt;/head&gt;
    &lt;p&gt;During the Trump administration, I took on a part time job: keeping up with all the outrages. Every twenty minutes or so I would have to check my phone in case any new outrages had occurred, so that I could...collect them? Make them into a scrapbook? I'm not sure.&lt;/p&gt;
    &lt;p&gt;I now think of this as super surveillance, tracking every problem in the world as if they were all somehow, ultimately, my problems. Super surveillance is an express ticket to the bog, because the world is full of problems and you'd be lucky to solve even a single one.&lt;/p&gt;
    &lt;p&gt;I know some people think that super surveillance is virtuous, but they mainly seem to spend their time looking at screens and feeling bad, and this doesn't seem to solve any of the problems that they're monitoring. To them, I suppose, the most saintly life possible is one spent sitting in front of a hundred screens, eyelids held open with surgical instruments, A Clockwork Orange-style, bearing witness to all human suffering simultaneously. I, uh, feel differently.&lt;/p&gt;
    &lt;p&gt;(See also: Reading the news is the new smoking.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Hedgehogging&lt;/head&gt;
    &lt;p&gt;Sometimes I get this feeling like, ‚ÄúNothing will ever work out for me, I will always be unhappy, the rest of my life will be a sort of wandering twilight punctuated with periods of misery.‚Äù&lt;/p&gt;
    &lt;p&gt;And my wife will go, ‚ÄúYou're hungry.‚Äù&lt;/p&gt;
    &lt;p&gt;And I'll go, ‚ÄúNo, no, this is true unhappiness, it comes to me unadulterated from hell itself, it lives inside my bones, I am persecuted by God, you could not possibly know what it's like to be me.‚Äù&lt;/p&gt;
    &lt;p&gt;And then I'll eat a burrito and be like, ‚ÄúNever mind I'm fine!‚Äù&lt;/p&gt;
    &lt;p&gt;This is hedgehogging: refusing to be influenced by others, even when you should.&lt;/p&gt;
    &lt;head rend="h4"&gt;Personal problems growth ray&lt;/head&gt;
    &lt;p&gt;You know how, when you go up in a tall building and look down at the street, everybody looks not just small, but kind of silly? Like, ‚ÄúAww, look at those tiny little guys, walking around in their suits like they're people! They don't even know they're so small!‚Äù&lt;/p&gt;
    &lt;p&gt;This is how other people's problems look to me. A friend will tell me, ‚ÄúI'm stressed!‚Äù and I'll go, ‚ÄúAww, what a silly little problem, walking around like it's real! Just don't be stressed, and then you won't be stressed!‚Äù&lt;/p&gt;
    &lt;p&gt;My problems, on the other hand, are like 50-foot-tall moody teenagers. They're so big and so real and so complicated! They cannot possibly be solved! I can only flee from them, hide among the rubble, and peek out at them with horror!&lt;/p&gt;
    &lt;p&gt;Such is the result of the personal problems growth ray, which makes all of your own problems seem larger than life, while other people's stay actual size. This makes reasonable solutions look unreasonable‚Äîthe actions that solved your human-sized problems could never solve my giganto-problems; they can only be addressed with either a lifetime of cowering or a tactical nuke.&lt;/p&gt;
    &lt;head rend="h4"&gt;Obsessing over tiny predictors&lt;/head&gt;
    &lt;p&gt;In graduate school, I made the terrible mistake of signing up for a professional development seminar. We would convene every week for 90 minutes of discussions like ‚ÄúOH NO WE'LL NEVER GET PROFESSOR JOBS WE'RE ALL SCREWED‚Äù and ‚ÄúTHE WORLD IS TOO MUCH AND I AM TOO SMALL‚Äù and ‚ÄúHELP HELP HELP‚Äù.&lt;/p&gt;
    &lt;p&gt;One week, we spent half the session arguing about whether you should print your name in bold when listing your publications on your CV. Like:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Tweedledum, M.R. &amp;amp; Mastroianni, A.M., (2024). Please give me a job I will do anything, including publishing this terrible paper. The Journal of Desperation, 4(12), 122-137.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;vs.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Tweedledum, M.R. &amp;amp; Mastroianni, A.M., (2024). Please give me a job I will do anything, including publishing this terrible paper. The Journal of Desperation, 4(12), 122-137.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Some people thought bolding your name helps time-pressed hiring committees quickly assess your academic output. Other people objected that bolding your name looks presumptuous. A debate ensued. I forget who won‚Äîoh yes, it was none of us because this is a stupid thing to care about.&lt;/p&gt;
    &lt;p&gt;This is obsessing over tiny predictors. It's scary to admit that you can't control the future; it's a lot easier to distract yourself by trying to optimize every decision, no matter how insignificant.&lt;/p&gt;
    &lt;p&gt;(If you're at the point where you're spending 45 minutes debating the use of bold letters on your CV, perhaps you should consider pulling up a list of every god and praying to all of them in turn, in case one of them is real and decides to help you.)&lt;/p&gt;
    &lt;p&gt;Parents who want to get their kids into elite colleges have perfected the art of obsessing over tiny predictors. When I gave campus tours, I would run into them all the time: ‚ÄúShould my kid play the timpani or the oboe?‚Äù ‚ÄúHow many semicolons can you use in the personal essay?‚Äù ‚ÄúCan we include dental records to demonstrate a history of good brushing?‚Äù The joke was on them, of course: stressing about all those tiny things only makes you anxious, and even if your kid gets into a fancy school, they could still end up as a blogger.&lt;/p&gt;
    &lt;head rend="h4"&gt;Impossible satisfaction &lt;/head&gt;
    &lt;p&gt;Sometimes people will be like, ‚ÄúWell, whatcha gonna do, life is suffering,‚Äù and I‚Äôll be like, ‚ÄúHaha sure is,‚Äù waiting for them to laugh too, but they won‚Äôt laugh, and I‚Äôll realize, to my horror, that they‚Äôre not joking. Some people think the bog is life!&lt;/p&gt;
    &lt;p&gt;I get why you might think this if you‚Äôve experienced lots of misfortune. If you, say, survived the atomic bombing of Hiroshima and then took the train to Nagasaki just in time for the atomic bombing of that city, too, you'd probably have a gloomy outlook on life.4&lt;/p&gt;
    &lt;p&gt;But most of the people I know who feel this way haven‚Äôt survived any atomic bombings at all. They‚Äôre usually people with lots of education and high-paying jobs and supportive relationships and a normal amount of tragedies, people who have all the raw materials for a good life but can‚Äôt seem to make one for themselves. Their problem is they believe that satisfaction is impossible. Like they‚Äôre standing in a kitchen full of eggs, flour, oil, sugar, butter, baking powder, a mixer, and an oven, and they throw their hands up and say, ‚ÄúI can‚Äôt make a cake! Cakes don‚Äôt even exist!‚Äù&lt;/p&gt;
    &lt;head rend="h1"&gt;WISHING YOU GOOD ALTITUDE&lt;/head&gt;
    &lt;p&gt;In the big scheme of things, I haven't been alive for all that long. So there are probably lots of ways into the bog I haven't discovered yet. But I've been down there enough times to see the same patterns repeat, and sometimes I can even interrupt them.&lt;/p&gt;
    &lt;p&gt;That's why having goofy names for them matters so much, because it reminds me not to believe the biggest bog lie of all: that I'm stuck in a situation unlike any I, or anyone else, has ever seen before. If you believe that, it's no wonder you'd suffer from insufficient activation energy, or bad escape plans, or self-bogging: you have no idea what to do, because you don't think anything you've learned, or anything anyone else has learned, can help you at all. Whenever I feel that way, whenever I think I'm in a bespoke bog, created just for me by a universe that hates me, if I can think to myself, ‚ÄúOh, I'm gutterballing right now,‚Äù I can feel my foot hit solid ground, and I can start hoisting myself onto dry land.&lt;/p&gt;
    &lt;p&gt;So, best of luck in 2024, and all the years to come after that. May you only spend as much time in the bog as is necessary to learn the lessons it has to teach you. And for goodness sake, if you see the side of a mountain coming toward you, pull up.&lt;/p&gt;
    &lt;p&gt;A periodic reminder that I am not a licensed mental health professional. I‚Äôm just a mop with a top hat on it.&lt;/p&gt;
    &lt;p&gt;Our leader claimed to be a Marine, which I kind of doubt because he spelled the name of our group as ‚ÄúDelta Companay‚Äù&lt;/p&gt;
    &lt;p&gt;In fact, Tsutomu Yamaguchi seems like he was remarkably upbeat despite witnessing some of the most horrific events in history, and I try to remind myself of this when I am frustrated about, for instance, a restaurant not slathering my burger with enough spicy mayo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46532961</guid><pubDate>Wed, 07 Jan 2026 21:16:47 +0000</pubDate></item><item><title>My first paper: A practical implementation of Rubiks cube based passkeys</title><link>https://ieeexplore.ieee.org/document/11280260</link><description>&lt;doc fingerprint="b97bf28d2c7a2de0"&gt;
  &lt;main&gt;
    &lt;p&gt;From Puzzle to Passkey: Physical Authentication Through Rubik‚Äôs Cube Scrambles | IEEE Conference Publication | IEEE Xplore&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46533315</guid><pubDate>Wed, 07 Jan 2026 21:38:56 +0000</pubDate></item><item><title>2026 Predictions Scorecard</title><link>https://rodneybrooks.com/predictions-scorecard-2026-january-01/</link><description>&lt;doc fingerprint="32a09155b9b09dbb"&gt;
  &lt;main&gt;
    &lt;p&gt;Nothing is ever as good as it first seems and nothing is ever as bad as it first seems.&lt;/p&gt;
    &lt;p&gt;‚Äî A best memory paraphrase of advice given to me by Vice Admiral Joe Dyer, former chief test pilot of the US Navy and former Commander of NAVAIR.&lt;/p&gt;
    &lt;p&gt;[You can follow me on social media: @rodneyabrooks.bsky.social and see my publications etc., at https://people.csail.mit.edu/brooks]&lt;/p&gt;
    &lt;head rend="h5"&gt;Table of contents&lt;/head&gt;
    &lt;quote&gt;Introduction What I Nearly Got Wrong What Has Surprised Me, And That I Missed 8 Years Ago My Color Scheme and Past Analysis My New Predictions Quantum Computers Self Driving Cars Humanoid Robots Neural Computation LLMs Self Driving Cars A Brief Recap of what "Self Driving" Cars Means and Meant My Own Experiences with Waymo in 2025 Self Driving Taxi Services __Cruise __Tesla __Waymo __Zoox Electric Cars Flying Cars Robotics, AI, and Machine Learning Capabilities and Competences World Models Situatedness vs Embodiment Dexterous Hands Human Space Flight Orbital Crewed Flights Suborbital Crewed Flights Boeing's Starliner SpaceX Falcon 9 NASA, Artemis, and Returning to the Moon SpaceX Starship Blue Origin Gets to Orbit New Space Stations Addendum&lt;/quote&gt;
    &lt;head rend="h5"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;This is my eighth annual update on how my dated predictions from January 1st, 2018 concerning (1) self driving cars, (2) robotics, AI , and machine learning, and (3) human space travel, have held up. I promised then to review them at the start of the year every year until 2050 (right after my 95th birthday), thirty two years in total. The idea was to hold myself accountable for those predictions. How right or wrong was I?&lt;/p&gt;
    &lt;p&gt;The summary is that my predictions held up pretty well, though overall I was a little too optimistic. That is a little ironic, as I think that many people who read my predictions back on January 1st, 2018 thought that I was very pessimistic compared to the then zeitgeist. I prefer to think of myself as being a realist.&lt;/p&gt;
    &lt;p&gt;And did I see LLMs coming? No and yes. Yes, I did say that something new and big that everyone accepted as the new and big thing in AI would come along no earlier than 2023, and that the key paper for its success had already been written by before I made my first predictions. And indeed LLMs were generally accepted as the next big thing in 2023 (I was lucky on that date), and the key paper, Attention Is All You Need, was indeed already written, and had first appeared in June of 2017. I wrote about this extensively in last year‚Äôs scorecard. But no, I had no idea it would be LLMs at the time of my correct prediction that something big would appear. And that lack of specificity on the details of exactly what will be invented and when is the case with all my predictions from the first day of 2018.&lt;/p&gt;
    &lt;p&gt;I did not claim to be clairvoyant about exactly what would happen, rather I was making predictions about the speed of new research ideas, the speed of hype generation, the speed of large scale deployments of new technologies, and the speed of fundamental changes propagating through the world‚Äôs economy. Those speeds are very different and driven by very different realities. I think that many people get confused by that and make the mistake of jumping between those domains of reality, thinking all the speeds will be the same. In my case my estimates of those speeds are informed by watching AI and robotics professionally, for 42 years at the time of my predictions. I became a graduate student in Artificial Intelligence in January of 1976, just shy of 20 years after the initial public outing of the term Artificial Intelligence at the summer workshop in 1956 at Dartmouth. And now as of today I have been in that field for 50 years.&lt;/p&gt;
    &lt;p&gt;I promised to track my predictions made eight years ago today for 32 years. So I am one quarter of the way there. But the density of specific years of events or marking percentages of adoption that I predicted start to fall off right around now.&lt;/p&gt;
    &lt;p&gt;Sometime during 2026 I will bundle up all my comments over the eight years specifically mentioning years that have now passed, and put them in an archival mid-year post. Then I will get rid of the three big long tables that dominate the body of this annual post, and have short updates on the sparse dates for the next 24 years.&lt;/p&gt;
    &lt;p&gt;I will continue to summarize what has happened in self-driving cars generally, including electrification progress and the forever promised flying cars, along with AI and robotics, and human space flight. But early in 2025 I made five new predictions for the coming ten years, without specific dates, but which summarize what I think will happen. I will track these predictions too.&lt;/p&gt;
    &lt;p&gt;What I Nearly Got Wrong&lt;/p&gt;
    &lt;p&gt;The day before my original prediction post in 2018 the price of Bitcoin had opened at $12,897.70 and topped out at $14,377.40 and 2017 had been the first year it had ever traded at over $1,000. The price seemed insane to me as Bitcoin wasn‚Äôt being used for the task for which it had been designed. The price seemed to me then, and now, to be purely about speculation. I almost predicted when it would be priced at $200, on the way down. But, fortunately, I checked myself as I realized that the then current state of the market made no sense to me and so any future state may not either. Besides, I had no experience or expertise in crypto pricing. So I left that prediction out. I had no basis to make a prediction. That was a wise decision, and I revisit that reasoning as I make new predictions now, and implore myself to only make predictions in fields where I know something.&lt;/p&gt;
    &lt;p&gt;What Has Surprised Me, And That I Missed 8 Years Ago&lt;/p&gt;
    &lt;p&gt;I made some predictions about the future of SpaceX although I didn‚Äôt always label them as being about SpaceX. A number of my predictions were in response to pronouncements by the CEO of SpaceX. My predictions were much more measured and some might say even pessimistic. Those predictions so far have turned out to be more optimistic than how reality has unfolded.&lt;/p&gt;
    &lt;p&gt;I had made no specific predictions about Falcon 9, though I did make predictions about the subsequent SpaceX launch family, now called Starship, but then known as BFR, which eight years later has not gotten into orbit.&lt;/p&gt;
    &lt;p&gt;In the meantime SpaceX has scaled the Falcon 9 launch rate at a phenomenal speed, and the magnitude of the growth is very surprising.&lt;/p&gt;
    &lt;p&gt;Eight years ago, Falcon 9 had been launched 46 times, all successful, over the previous eight years, and it had recently had a long run of successful landings of the booster whenever attempted. At that time five launches had been on a previously used booster, but there had been no attempts to launch Falcon Heavy with its three boosters strapped together.&lt;/p&gt;
    &lt;p&gt;Now we are eight years on from those first eight years of Falcon 9 launches. The scale and success rate of the launches has made each individual launch an unremarkable event, with humans being launched a handful of times per year. Now the Falcon 9 score card stands at 582 launches with only one failed booster, and there have been 11 launches of the three booster Falcon Heavy, all successful. That is a sustained growth rate of 38% year over year for eight years. And that it is a very high sustained deployment growth rate for any complex technology.&lt;/p&gt;
    &lt;p&gt;There is no other modern rocket with such a volume of launches that comes even close to the Falcon 9 record. And I certainly did not foresee this volume of launches. About half the launches have had SpaceX itself as the customer, starting in February 2018, launching an enormous satellite constellation (about two thirds of all satellites ever orbited) to support Starlink bringing internet to everywhere on the surface of Earth.&lt;/p&gt;
    &lt;p&gt;But‚Ä¶ there is one historical rocket, a suborbital one which has a much higher record of use than Falcon 9 over a much briefer period. The German V-2 was the first rocket to fly above the atmosphere and the first ballistic missile to be used to deliver bombs. It was fueled with ethanol and liquid oxygen, and was steered by an analog computer that also received inputs from radio guide signals‚Äìit was the first operational liquid fueled rocket. It was developed in Germany in the early 1940‚Äôs and after more than a thousand test launches was first put into operation on September 7th, 1944, landing a bomb on Paris less than two weeks after the Allied liberation of that city. In the remaining 8 months of the war 3,172 armed V-2 rockets were launched at targets in five countries ‚Äî 1,358 were targeted at London alone.&lt;/p&gt;
    &lt;p&gt;My Color Scheme and Past Analysis&lt;/p&gt;
    &lt;p&gt;The acronyms I used for predictions in my original post were as follows.&lt;/p&gt;
    &lt;p&gt;NET year means it will not happen before that year (No Earlier Than)&lt;lb/&gt; BY year means I predict that it will happen by that year.&lt;lb/&gt; NIML, Not In My Lifetime, i.e., not before 2050.&lt;/p&gt;
    &lt;p&gt;As time passes mentioned years I color then as accurate, too pessimistic, or too optimistic.&lt;/p&gt;
    &lt;p&gt;Last year I added hemming and hawing. This is for when something looks just like what I said would take a lot longer has happened, but the underlying achievement is not what everyone expected, and is not what was delivered. This is mostly for things that were talked about as being likely to happen with no human intervention and it now appears to happen that way, but in reality there are humans in the loop that the companies never disclose. So the technology that was promised to be delivered hasn‚Äôt actually been delivered but everyone thinks it has been.&lt;/p&gt;
    &lt;p&gt;When I quote myself I do so in orange, and when I quote others I do so in blue.&lt;/p&gt;
    &lt;p&gt;I have not changed any of the text of the first three columns of the prediction tables since their publication on the first day of 2018. I only change the text in the fourth column to say what actually happened. This meant that by four years ago that fourth column was getting very long and skinny, so I removed them and started with fresh comments two years ago. I have kept the last two year‚Äôs comments and added new ones, with yellow backgrounds, for this year, removing the yellow backgrounds from 2025 comments that were there last year. If you want to see the previous five years of comments you can go back to the 2023 scorecard.&lt;/p&gt;
    &lt;head rend="h5"&gt;My NEW PREDICTIONS&lt;/head&gt;
    &lt;p&gt;On March 26th I skeeted out five technology predictions, talking about developments over the next ten years through January 1st, 2036. Three weeks later I included them in a blog post. Here they are again.&lt;/p&gt;
    &lt;p&gt;1. Quantum Computers. The successful ones will emulate physical systems directly for specialized classes of problems rather than translating conventional general computation into quantum hardware. Think of them as 21st century analog computers. Impact will be on materials and physics computations.&lt;/p&gt;
    &lt;p&gt;2. Self Driving Cars. In the US the players that will determine whether self driving cars are successful or abandoned are #1 Waymo (Google) and #2 Zoox (Amazon). No one else matters. The key metric will be human intervention rate as that will determine profitability.&lt;/p&gt;
    &lt;p&gt;3. Humanoid Robots. Deployable dexterity will remain pathetic compared to human hands beyond 2036. Without new types of mechanical systems walking humanoids will remain too unsafe to be in close proximity to real humans.&lt;/p&gt;
    &lt;p&gt;4. Neural Computation. There will be small and impactful academic forays into neuralish systems that are well beyond the linear threshold systems, developed by 1960, that are the foundation of recent successes. Clear winners will not yet emerge by 2036 but there will be multiple candidates.&lt;/p&gt;
    &lt;p&gt;5. LLMs. LLMs that can explain which data led to what outputs will be key to non annoying/dangerous/stupid deployments. They will be surrounded by lots of mechanism to keep them boxed in, and those mechanisms, not yet invented for most applications, will be where the arms races occur.&lt;/p&gt;
    &lt;p&gt;These five predictions are specifically about what will happen in these five fields during the ten years from 2026 through 2035, inclusive. They are not saying when particular things will happen, rather they are saying whether or not certain things will happen in that decade. I will do my initial analysis of these five new predictions immediately below. For the next ten years I will expand on each of these reviews in this annual scorecard, along with reviews of my earlier predictions. The ten years for these predictions are up on January 1st, 2036. I will have just turned 81 years old then, so let‚Äôs see if I am still coherent enough to do this.&lt;/p&gt;
    &lt;p&gt;Quantum Computers&lt;/p&gt;
    &lt;p&gt;The successful ones will emulate physical systems directly for specialized classes of problems rather than translating conventional general computation into quantum hardware. Think of them as 21st century analog computers. Impact will be on materials and physics computations.&lt;/p&gt;
    &lt;p&gt;The original excitement about quantum computers was stimulated by a paper by Peter Shor in 1994 which gave a digital quantum algorithm to factor large integers much faster than a conventional digital computer. Factoring integers is often referred to as ‚Äúthe IFP‚Äù for the integer factorization problem.&lt;/p&gt;
    &lt;p&gt;So what? The excitement around this was based on how modern cryptography, which provides our basic security for on-line commerce, works under the hood.&lt;/p&gt;
    &lt;p&gt;Much of the internet‚Äôs security is based on it being hard to factor a large number. For instance in the RSA algorithm Alice tells everyone a large number (in different practical versions it has 1024, 2048, or 4096 bits) for which she knows its prime factors. But she tells people only the number not its factors. In fact she chose that number by multiplying together some very large prime numbers ‚Äî very large prime numbers are fairly easy to generate (using the Miller-Rabin test). Anyone, usually known as Bob, can then use that number to encrypt a message intended for Alice. No one, neither Tom, Dick, nor Harry, can decrypt that message unless they can find the prime factors of Alice‚Äôs public number. But Alice knows them and can read the message intended only for her eyes.&lt;/p&gt;
    &lt;p&gt;So‚Ä¶ if you could find prime factors of large numbers easily then the backbone of digital security would be broken. Much excitement!&lt;/p&gt;
    &lt;p&gt;Shor produced his algorithm in 1994. By the year 2001 a group at IBM had managed to find the prime factors of the number 15 using a digital quantum computer as published in Nature. All the prime factors. Both 3 and 5. Notice that 15 has only four bits, which is a lot smaller than the number of bits used in commercial RSA implementations, namely 1024, 2048, or 4096.&lt;/p&gt;
    &lt;p&gt;Surely things got better fast. By late 2024 the biggest numbers that had been factored by an actual digital quantum computer had 35 bits which allows for numbers no bigger than 34,359,738,367. That is way smaller than the size of the smallest numbers used in RSA applications. Nevertheless it does represent 31 doublings in magnitude of numbers factored in 23 years, so progress has been quite exponential. But it could take another 500 years of that particular version of exponential growth rate to get to conquering today‚Äôs smallest version of RSA digital security.&lt;/p&gt;
    &lt;p&gt;In the same report the authors say that a conventional, but very large computer (2,000 GPUs along with a JUWELS booster, which itself has 936 compute nodes each consisting of four NVIDIA A100 Tensor Core GPUs themselves each hosted by 48 dual threaded AMD EPYC Rome cores‚Äìthat is quite a box of computing) simulating a quantum computer running Shor‚Äôs algorithm had factored a 39 bit number finding that 549,755,813,701 = 712,321 √ó 771,781, the product of two 20 bit prime numbers. That was its limit. Nevertheless, an actual digital quantum computer can still be outclassed by one simulated on conventional digital hardware.&lt;/p&gt;
    &lt;p&gt;The other early big excitement for digital quantum computers was Grover‚Äôs search algorithm, but work on that has not been as successful as for Shor‚Äôs IFP solution.&lt;/p&gt;
    &lt;p&gt;Digital quantum computation nirvana has not yet been demonstrated.&lt;/p&gt;
    &lt;p&gt;Digital quantum computers work a little like regular digital computers in that there is a control mechanism which drives the computer through a series of discrete steps. But today‚Äôs digital quantum computers suffer from accumulating errors in quantum bits. Shor‚Äôs algorithm assumes no such errors. There are techniques for correcting those errors but they slow things down and cause other problems. One way that digital quantum computers may get better is if new methods of error correction emerge. I am doubtful that something new will emerge, get fully tested, and then make it into production at scale all within the next ten years. So we may not see a quantum (ahem) leap in performance of quantum digital computers in the next decade.&lt;/p&gt;
    &lt;p&gt;Analog quantum computers are another matter. They are not switched, but instead are configured to directly simulate some physical system and the quantum evolution and interactions of components of that system. They are an embodied quantum model of that system. And they are ideally suited to solving these sorts of problems and cannot be emulated by conventional digital systems as they can be in the 39 bit number case above.&lt;/p&gt;
    &lt;p&gt;I find people working on quantum computers are often a little squirrelly about whether their computer acts more like a digital or analog computer, as they like to say they are ‚Äúquantum‚Äù only. The winners over the next 10 years will be ones solving real problems in materials science and other aspects of chemistry and physics.&lt;/p&gt;
    &lt;p&gt;Self Driving Cars&lt;/p&gt;
    &lt;p&gt;In the US the players that will determine whether self driving cars are successful or abandoned are #1 Waymo (Google) and #2 Zoox (Amazon). No one else matters. The key metric will be human intervention rate as that will determine profitability.&lt;/p&gt;
    &lt;p&gt;Originally the term ‚Äúself driving car‚Äù was about any sort of car that could operate without a driver on board, and without a remote driver offering control inputs. Originally they were envisioned as an option for privately owned vehicles used by individuals, a family car where no person needed to drive, but simply communicated to the car where it should take them.&lt;/p&gt;
    &lt;p&gt;That conception is no longer what people think of when self driving cars are mentioned. Self driving cars today refer to taxi-services that feel like Uber or Lyft, but for which there is not a human driver, just paying passengers.&lt;/p&gt;
    &lt;p&gt;In the US the companies that have led in this endeavor have changed over time.&lt;/p&gt;
    &lt;p&gt;The first leader was Cruise, owned by GM. They were the first to have a regular service in the downtown area of a major city (San Francisco), and then in a number of other cities, where there was an app that anyone could download and then use their service. They were not entirely forthcoming with operational and safety problems, including when they dragged a person, who had just been hit by a conventionally driven car, for tens of feet under one of their vehicles. GM suspended operations in late 2023 and completely disbanded it in December 2024.&lt;/p&gt;
    &lt;p&gt;Since then Waymo (owned by Google) has been the indisputable leading deployed service.&lt;/p&gt;
    &lt;p&gt;Zoox (owned by Amazon) has been a very distant, but operational, second place.&lt;/p&gt;
    &lt;p&gt;Tesla (owned by Tesla) has put on a facade of being operational, but it is not operational in the sense of the other two services, and faces regulatory headwinds that both Waymo and Zoox have long been able to satisfy. They are not on a path to becoming a real service.&lt;/p&gt;
    &lt;p&gt;See my traditional section on self driving cars below, as it explains in great detail the rationale for these evaluations. In short, Waymo looks to have a shot at succeeding and it is unlikely they will lose first place in this race. Zoox may also cross the finish line, and it is very unlikely that anyone will beat them. So if both of Waymo and Zoox fail, for whatever reason, the whole endeavor will grind to a halt in the US.&lt;/p&gt;
    &lt;p&gt;BUT‚Ä¶&lt;/p&gt;
    &lt;p&gt;But what might go wrong that makes one of these companies fail. We got a little insight into that in the last two weeks of 2025.&lt;/p&gt;
    &lt;p&gt;On Saturday December 20th of 2025 there was an extended power outage in San Francisco that started small in the late morning but by nightfall had spread to large swaths of the city. And lots and lots of normally busy intersections were by that time blocked by many stationary Waymos.&lt;/p&gt;
    &lt;p&gt;Traffic regulations in San Francisco are that when there is an intersection which has traffic lights that are all dark, that intersection should be treated as though it has stop signs at every entrance. Human drivers who don‚Äôt know the actual regulation tend to fall back to that behavior in any case.&lt;/p&gt;
    &lt;p&gt;It seemed that Waymos were waiting indefinitely for green lights that never came, and at intersections through which many Waymos were routed there were soon enough waiting Waymos that the intersections were blocked. Three days later, on December 23rd, Waymo issued an explanation on their blog site, which includes the following:&lt;/p&gt;
    &lt;p&gt;Navigating an event of this magnitude presented a unique challenge for autonomous technology. While the Waymo Driver is designed to handle dark traffic signals as four-way stops, it may occasionally request a confirmation check to ensure it makes the safest choice. While we successfully traversed more than 7,000 dark signals on Saturday, the outage created a concentrated spike in these requests. This created a backlog that, in some cases, led to response delays contributing to congestion on already-overwhelmed streets.&lt;/p&gt;
    &lt;p&gt;We established these confirmation protocols out of an abundance of caution during our early deployment, and we are now refining them to match our current scale. While this strategy was effective during smaller outages, we are now implementing fleet-wide updates that provide the Driver with specific power outage context, allowing it to navigate more decisively.&lt;/p&gt;
    &lt;p&gt;As the outage persisted and City officials urged residents to stay off the streets to prioritize first responders, we temporarily paused our service in the area. We directed our fleet to pull over and park appropriately so we could return vehicles to our depots in waves. This ensured we did not further add to the congestion or obstruct emergency vehicles during the peak of the recovery effort.&lt;/p&gt;
    &lt;p&gt;The key phrase is that Waymos ‚Äúrequest a confirmation check‚Äù at dark signals. This means that the cars were asking for a human to look at images from their cameras and manually tell them how to behave. With 7,000 dark signals and perhaps a 1,000 vehicles on the road, Waymo clearly did not have enough humans on duty to handle the volume of requests that were coming in. Waymo does not disclose whether any human noticed a rise in these incidents early in the day and more human staff were called in, or whether they simply did not have enough employees to make handling them all possible.&lt;/p&gt;
    &lt;p&gt;At a deeper level it looks like they had a debugging feature in their code, and not enough people to supply real time support to handle the implications of that debugging feature. And it looks like Waymo is going to remove that debugging safety feature as a way of solving the problem. This is not an uncommon sort of engineering failure during early testing. Normally one would hope that the need for that debugging feature had been resolved before large scale deployment.&lt;/p&gt;
    &lt;p&gt;But, who are these human staff? Besides those in Waymo control centers, it turns out there is a gig-work operation with an app named Honk (the headline of the story is When robot taxis get stuck, a secret army of humans comes to the rescue) whereby Waymo pays people around $20 to do minor fixups to stuck Waymos by, for instance, going and physically closing a door that q customer left open. Tow truck operators use the same app to find Waymos that need towing because of some more serious problem. It is not clear whether it was a shortage of those gig workers, or a shortage of people in the Waymo remote operations center that caused the large scale failures. But it is worth noting that current generation Waymos need a lot of human help to operate as they do, from people in the remote operations center to intervene and provide human advice for when something goes wrong, to Honk gig-workers scampering around the city physically fixed problems with the vehicles, to people to clean the cars and plug them in to recharge when they return to their home base.&lt;/p&gt;
    &lt;p&gt;For human operated ride services, traditional taxi companies or gig services such as Uber and Lyft, do not need these external services. There is a human with the car at all times who takes care of these things.&lt;/p&gt;
    &lt;p&gt;The large scale failure on the 20th did get people riled up about these robots causing large scale traffic snarls, and made them wonder about whether the same thing will happen when the next big earthquake hits San Francisco. Will the human support worker strategy be stymied by both other infrastructure failures (e.g., the cellular network necessary for Honk workers to communicate) or the self preservation needs of the human workers themselves?&lt;/p&gt;
    &lt;p&gt;The Waymo blog post revealed another piece of strategy. This is one of three things they said that they would do to alleviate the problems:&lt;/p&gt;
    &lt;p&gt;Expanding our first responder engagement: To date, we‚Äôve trained more than 25,000 first responders in the U.S. and around the world on how to interact with Waymo. As we discover learnings from this and other widespread events, we‚Äôll continue updating our first responder training.&lt;/p&gt;
    &lt;p&gt;The idea is to add more responsibility to police and fire fighters to fix the inadequacies of the partial-only autonomy strategy for Waymo‚Äôs business model. Those same first responders will have more than enough on their plates during any natural disasters.&lt;/p&gt;
    &lt;p&gt;Will it become a political issue where the self-driving taxi companies are taxed enough to provide more first responders? Will those costs ruin their business model? Will residents just get so angry that they take political action to shut down such ride services?&lt;/p&gt;
    &lt;p&gt;Humanoid Robots&lt;/p&gt;
    &lt;p&gt;Deployable dexterity will remain pathetic compared to human hands beyond 2036. Without new types of mechanical systems walking humanoids will remain too unsafe to be in close proximity to real humans.&lt;/p&gt;
    &lt;p&gt;Despite this prediction it is worth noting that there is a long distance between current deployed dexterity and dexterity that is still pathetic. In the next ten years deployable dexterity may improve markedly, but not in the way the current hype for humanoid robots suggests. I talk about his below in my annual section scoring my 2018 predictions on robotics, AI, and machine learning, in a section titled Dexterous Hands.&lt;/p&gt;
    &lt;p&gt;Towards the end of 2025 I published a long blog post summarizing the status of, and problems remaining for humanoid robots.&lt;/p&gt;
    &lt;p&gt;I started building humanoid robots in my research group at MIT in 1992. My previous company, Rethink Robotics, founded in 2008, delivered thousands of upper body Baxter and Sawyer humanoid robots (built in the US) to factories between 2012 and 2018. At the top of this blog page you can see a whole row of Baxter robots in China. A Sawyer robot that had operated in a factory in Oregon just got shut down in late 2025 with 35,236 hours on its operations clock. You can still find many of Rethink‚Äôs humanoids in use in teaching and research labs around the world. Here is the cover of Science Robotics from November 2025,&lt;/p&gt;
    &lt;p&gt;showing a Sawyer used in the research for this article out of Imperial College, London.&lt;/p&gt;
    &lt;p&gt;Here is a slide from a 1998 powerpoint deck that I was using in my talks, six years after my graduate students and I had started building our first humanoid robot, Cog.&lt;/p&gt;
    &lt;p&gt;It is pretty much the sales pitch that today‚Äôs humanoid companies use. You are seeing here my version from almost twenty eight years ago.&lt;/p&gt;
    &lt;p&gt;I point this out to demonstrate that I am not at all new to humanoid robotics and have worked on them for decades in both academia and in producing and selling humanoid robots that were deployed at scale (which no one else has done) doing real work.&lt;/p&gt;
    &lt;p&gt;My blog post from September, details why the current learning based approaches to getting dexterous manipulation will not get there anytime soon. I argue that the players are (a) collecting the wrong data and (b) trying to learn the wrong thing. I also give an argument (c) for why learning might not be the right approach. My argument for (c) may not hold up, but I am confident that I am right on both (a) and (b), at least for the next ten years.&lt;/p&gt;
    &lt;p&gt;I also outline in that blog post why the current (and indeed pretty much the only, for the last forty years) method of building bipeds and controlling them will remain unsafe for humans to be nearby. I pointed out that the danger is roughly cubicly proportional to the weight of the robot. Many humanoid robot manufacturers are introducing lightweight robots, so I think they have come to the same conclusion. But the side effect is that the robots can not carry much payload, and certainly can‚Äôt provide physical support to elderly humans, which is a thing that human carers do constantly ‚Äî these small robots are just not strong enough. And elder care and in home care is one of the main arguments for having human shaped robots, adapted to the messy living environments of actual humans.&lt;/p&gt;
    &lt;p&gt;Given that careful analysis from September I do not share the hype that surrounds humanoid robotics today. Some of it is downright delusional across many different levels.&lt;/p&gt;
    &lt;p&gt;To believe the promises of many CEOs of humanoid companies you have to accept the following conjunction.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Their robots have not demonstrated any practical work (I don‚Äôt count dancing in a static environment doing exactly the same set of moves each time as practical work).&lt;/item&gt;
      &lt;item&gt;The demonstrated grasping, usually just a pinch grasp, in the videos they show is at a rate which is painfully slow and not something that will be useful in practice.&lt;/item&gt;
      &lt;item&gt;They claim that their robots will learn human-like dexterity but they have not shown any videos of multi-fingered dexterity where humans can and do grasp things that are unseen, and grasp and simultaneously manipulate multiple small objects with one hand. And no demonstrations of using the body with the hands which is how humans routinely carry many small things or one or two heavy things.&lt;/item&gt;
      &lt;item&gt;They show videos of non tele-operated manipulation, but all in person demonstrations of manipulation are tele-operated.&lt;/item&gt;
      &lt;item&gt;Their current plans for robots working in customer homes all involve a remote person tele-operating the robot.&lt;/item&gt;
      &lt;item&gt;Their robots are currently unsafe for humans to be close to when they are walking.&lt;/item&gt;
      &lt;item&gt;Their robots have no recovery from falling and need human intervention to get back up.&lt;/item&gt;
      &lt;item&gt;Their robots have a battery life measured in minutes rather than hours.&lt;/item&gt;
      &lt;item&gt;Their robots cannot currently recharge themselves.&lt;/item&gt;
      &lt;item&gt;Unlike human carers for the elderly, humanoids are not able to provide any physical assistance to people that provides stabilizing support for the person walking, getting into and out of bed physical assistance, getting on to and off of a toilet, physical assistance, or indeed any touch based assistance at all.&lt;/item&gt;
      &lt;item&gt;The CEOs claim that there robots will be able to do everything, or many things, or a lot of things, that a human can do in just a few short years. They currently do none.&lt;/item&gt;
      &lt;item&gt;The CEOs claim a rate of adoption of these humanoid robots into homes and industries at a rate that is multiple orders of magnitude faster than any other technology in human history, including mainframe computers, and home computers and the mobile phones, and the internet. Many orders of magnitude faster. Here is a CEO of a humanoid robot company saying that they will be in 10% of US households by 2030. Absolutely no technology (even without the problems above) has ever come close to scaling at that rate.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The declarations being made about humanoid robots are just not plausible.&lt;/p&gt;
    &lt;p&gt;We‚Äôll see what actually happens over the next ten years, but it does seem that the fever is starting to crack at the edges. Here are two news stories from the last few days of 2025.&lt;/p&gt;
    &lt;p&gt;From The Information on December 22nd there is a story about how humanoid robot companies are wrestling with safety standards. All industrial and warehouse robots, whether stationary of mobile have a big red safety stop button, in order to comply with regulatory safety standards. The button cuts the power to the motors. But cutting power to the motors of a balancing robot might make them fall over and cause more danger and damage to people nearby. For the upper torso humanoid robots Baxter and Sawyer from my company Rethink Robotics we too had a safety stop button that cut power to all the motors in the arms. It was a collaborative robot and often a person, or part of their limbs or body could be under an arm and it would have been dangerous for the arms to fall quickly on cutoff of power. To counter this we developed a unique circuit that required no active power, which made it so that the back current generated by a motor when powered off acted as a very strong brake. Perhaps there are similar possible solutions for humanoid robots and falling, but they need to be invented yet.&lt;/p&gt;
    &lt;p&gt;On December 25th the Wall Street Journal had a story headlined ‚ÄúEven the Companies Making Humanoid Robots Think They‚Äôre Overhyped‚Äù, with a lede of ‚ÄúDespite billions in investment, startups say their androids mostly aren‚Äôt useful for industrial or domestic work yet‚Äù. Here are the first two paragraphs of the story:&lt;/p&gt;
    &lt;p&gt;Billions of dollars are flowing into humanoid robot startups, as investors bet that the industry will soon put humanlike machines in warehouses, factories and our living rooms.&lt;/p&gt;
    &lt;p&gt;Many leaders of those companies would like to temper those expectations. For all the recent advances in the field, humanoid robots, they say, have been overhyped and face daunting technical challenges before they move from science experiments to a replacement for human workers.&lt;/p&gt;
    &lt;p&gt;And then they go on to quote various company leaders:&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôve been trying to figure out how do we not just make a humanoid robot, but also make a humanoid robot that does useful work,‚Äù said Pras Velagapudi, chief technology officer at Agility Robotics.&lt;/p&gt;
    &lt;p&gt;Then talking about a recent humanoid robotics industry event the story says:&lt;/p&gt;
    &lt;p&gt;On stage at the summit, one startup founder after another sought to tamp down the hype around humanoid robots.&lt;/p&gt;
    &lt;p&gt;‚ÄúThere‚Äôs a lot of great technological work happening, a lot of great talent working on these, but they are not yet well defined products,‚Äù said Kaan Dogrusoz, a former Apple engineer and CEO of Weave Robotics.&lt;/p&gt;
    &lt;p&gt;Today‚Äôs humanoid robots are the right idea, but the technology isn‚Äôt up to the premise, Dogrusoz said. He compared it to Apple‚Äôs most infamous product failure, the Newton hand-held computer.&lt;/p&gt;
    &lt;p&gt;There are more quotes from other company leaders all pointing out the difficulties in making real products that do useful work. Reality seems to be setting in as promised delivery dates come and go by.&lt;/p&gt;
    &lt;p&gt;Meanwhile here is what I said at the end of my September blog post about humanoid robots and teaching them dexterity. I am not at all negative about a great future for robots, and in the nearish term. It is just that I completely disagree with the hype arguing that building robots with humanoid form magically will make robots useful and deployable. These particular paragraphs followed where I had described there, as I do again in this blog post, how the meaning of self driving cars has drifted over time.&lt;/p&gt;
    &lt;p&gt;Following that pattern, what it means to be a humanoid robot will change over time.&lt;/p&gt;
    &lt;p&gt;Before too long (and we already start to see this) humanoid robots will get wheels for feet, at first two, and later maybe more, with nothing that any longer really resembles human legs in gross form. But they will still be called humanoid robots.&lt;/p&gt;
    &lt;p&gt;Then there will be versions which variously have one, two, and three arms. Some of those arms will have five fingered hands, but a lot will have two fingered parallel jaw grippers. Some may have suction cups. But they will still be called humanoid robots.&lt;/p&gt;
    &lt;p&gt;Then there will be versions which have a lot of sensors that are not passive cameras, and so they will have eyes that see with active light, or in non-human frequency ranges, and they may have eyes in their hands, and even eyes looking down from near their crotch to see the ground so that they can locomote better over uneven surfaces. But they will still be called humanoid robots.&lt;/p&gt;
    &lt;p&gt;There will be many, many robots with different forms for different specialized jobs that humans can do. But they will all still be called humanoid robots.&lt;/p&gt;
    &lt;p&gt;As with self driving cars, most of the early players in humanoid robots, will quietly shut up shop and disappear. Those that remain will pivot and redefine what they are doing, without renaming it, to something more achievable and with, finally, plausible business cases. The world will slowly shift, but never fast enough to need a change of name from humanoid robots. But make no mistake, the successful humanoid robots of tomorrow will be very different from those being hyped today.&lt;/p&gt;
    &lt;p&gt;Neural Computation&lt;/p&gt;
    &lt;p&gt;There will be small and impactful academic forays into neuralish systems that are well beyond the linear threshold systems, developed by 1960, that are the foundation of recent successes. Clear winners will not yet emerge by 2036 but there will be multiple candidates.&lt;/p&gt;
    &lt;p&gt;Current machine learning techniques are largely based on having millions, and more recently tens (to hundreds?) of billions, of linear threshold units. They look like this.&lt;/p&gt;
    &lt;p&gt;Each of these units have a fixed number of inputs, where some numerical value comes in, and it is multiplied by a weight, usually a floating point number, and the results of all of the multiplications are summed, along with an adjustable threshold , which is usually negative, and then the sum goes through some sort of squishing function to produce a number between zero and one, or in this case minus one and plus one, as the output. In this diagram, which, by the way is taken from Bernie Widrow‚Äôs technical report from 1960, the output value is either minus one or plus one, but in modern systems it is often a number from anywhere in that, or another, continuous interval.&lt;/p&gt;
    &lt;p&gt;This was based on previous work, including that of Warren McCulloch and Walter Pitts‚Äô 1943 formal model of a neuron, Marvin Minsky‚Äôs 1954 Ph.D. dissertation on using reinforcement for learning in a machine based on model neurons, and Frank Rosenblatt‚Äôs 1957 use of weights (see page 10) in an analog implementation of a neural model.&lt;/p&gt;
    &lt;p&gt;These are what current learning mechanisms have at their core. These! A model of biological neurons that was developed in a brief moment of time from 83 to 65 years ago. We use these today. They are extraordinarily primitive models of neurons compared to what neuroscience has learned in the subsequent sixty five years.&lt;/p&gt;
    &lt;p&gt;Since the 1960s higher levels of organization have been wrapped around these units. In 1979 Kunihiko Fukushima published (at the International Joint Conference on Artificial Intelligence, IJCAI 1979, Tokyo ‚Äî coincidentally the first place where I published in an international venue) his first English language description of convolutional neural networks (CNNs), which allowed for position invariant recognition of shapes (in his case, hand written digits), without having to learn about those shapes in every position within images.&lt;/p&gt;
    &lt;p&gt;Then came backpropagation, a method where a network can be told the correct output it should have produced, and by propagating the error backwards through the derivative of the quantizer in the diagram above (note that the quantizer shown there is not differentiable‚Äìa continuous differentiable quantizer function is needed to make the algorithm work), a network can be trained on examples of what it should produce. The details of this algorithm, are rooted in the chain rule of Gottfried Leibniz in 1676 through a series of modern workers from around 1970 through about 1982. Frank Rosenblatt (see above) had talked about a ‚Äúback-propagating error correction‚Äù in 1962, but did not know how to implement it.&lt;/p&gt;
    &lt;p&gt;In any case, the linear threshold neurons, CNNs, and backpropagation are the basis of modern neural networks. After an additional 30 years of slow but steady progress they burst upon the scene as deep learning, and unexpectedly crushed many other approaches to computer vision ‚Äî the research field of getting computers to interpret the contents of an image. Note that ‚Äúdeep‚Äù learning refers to there being lots of layers (around 12 layers in 2012) of linear threshold neurons rather than the smaller number of layers (typically two or three) that had been used previously.&lt;/p&gt;
    &lt;p&gt;Now LLMs are built on top of these sorts of networks with many more layers, and many subnetworks. This is what got everyone excited about Artificial Intelligence, after 65 years of constant development of the field.&lt;/p&gt;
    &lt;p&gt;Despite their successes with language, LLMs come with some serious problems of a purely implementation nature.&lt;/p&gt;
    &lt;p&gt;First, the amount of examples that need to be shown to a network to learn to be facile in language takes up enormous amounts of computation, so the that costs of training new versions of such networks is now measured in the billions of dollars, consuming an amount of electrical power that requires major new investments in electrical generation, and the building of massive data centers full of millions of the most expensive CPU/GPU chips available.&lt;/p&gt;
    &lt;p&gt;Second, the number of adjustable weights shown in the figure are counted in the hundreds of billions meaning they occupy over a terabyte of storage. RAM that is that big is incredibly expensive, so the models can not be used on phones or even lower cost embedded chips in edge devices, such as point of sale terminals or robots.&lt;/p&gt;
    &lt;p&gt;These two drawbacks mean there is an incredible financial incentive to invent replacements for each of (1) our humble single neuron models that are close to seventy years old, (2) the way they are organized into networks, and (3) the learning methods that are used.&lt;/p&gt;
    &lt;p&gt;That is why I predict that there will be lots of explorations of new methods to replace our current neural computing mechanisms. They have already started and next year I will summarize some of them. The economic argument for them is compelling. How long they will take to move from initial laboratory explorations to viable scalable solutions is much longer than everyone assumes. My prediction is there will be lots of interesting demonstrations but that ten years is too small a time period for a clear winner to emerge. And it will take much much longer for the current approaches to be displaced. But plenty of researchers will be hungry to do so.&lt;/p&gt;
    &lt;p&gt;LLMs&lt;/p&gt;
    &lt;p&gt;LLMs that can explain which data led to what outputs will be key to non annoying/dangerous/stupid deployments. They will be surrounded by lots of mechanism to keep them boxed in, and those mechanisms, not yet invented for most applications, will be where the arms races occur.&lt;/p&gt;
    &lt;p&gt;The one thing we have all learned, or should have learned, is that the underlying mechanism for Large Language Models does not answer questions directly. Instead, it gives something that sounds like an answer to the question. That is very different from saying something that is accurate. What they have learned is not facts about the world but instead a probability distribution of what word is most likely to come next given the question and the words so far produced in response. Thus the results of using them, uncaged, is lots and lots of confabulations that sound like real things, whether they are or not.&lt;/p&gt;
    &lt;p&gt;We have seen all sorts of stories about lawyers using LLMs to write their briefs, judges using them to write their opinions, where the LLMs have simply made up precedents and fake citations (that sound plausible) for those precedents.&lt;/p&gt;
    &lt;p&gt;And there are lesser offenses that are still annoying but time consuming. The first time I used ChatGPT was when I was retargeting the backend of a dynamic compiler that I had used on half a dozen architectures and operating systems over a thirty year period, and wanted to move it to the then new Apple M1 chips. The old methods of changing a chunk of freshly compiled binary from data as it was spit out by the compiler, into executable program, no longer worked, deliberately so as part of Apple‚Äôs improved security measures. ChatGPT gave me detailed instructions on what library calls to use, what their arguments were, etc. The names looked completely consistent with other calls I knew within the Apple OS interfaces. When I tried to use them from C, the C linker complained they didn‚Äôt exist. And then when I asked ChatGPT to show me the documentation it groveled that indeed they did not exist and apologized.&lt;/p&gt;
    &lt;p&gt;So we all know we need guard rails around LLMs to make them useful, and that is where there will be lot of action over the next ten years. They can not be simply released into the wild as they come straight from training.&lt;/p&gt;
    &lt;p&gt;This is where the real action is now. More training doesn‚Äôt make things better necessarily. Boxing things in does.&lt;/p&gt;
    &lt;p&gt;Already we see companies trying to add explainability to what LLMs say. Google‚Äôs Gemini now gives real citations with links, so that human users can oversee what they are being fed. Likewise, many companies are trying to box in what their LLMs can say and do. Those that can control their LLMs will be able to deliver useable product.&lt;/p&gt;
    &lt;p&gt;A great example of this is the rapid evolution of coding assistants over the last year or so. These are specialized LLMs that do not give the same sort of grief to coders that I experienced when I first tried to use generic ChatGPT to help me. Peter Norvig, former chief scientist of Google, has recently produced a great report on his explorations of the new offerings. Real progress has been made in this high impact, but narrow use field.&lt;/p&gt;
    &lt;p&gt;New companies will become specialists in providing this sort of boxing in and control of LLMs. I had seen an ad on a Muni bus in San Francisco for one such company, but it was too fleeting to get a photo. Then I stumbled upon this tweet that has three such photos of different ads from the same company, and here is one of them:&lt;/p&gt;
    &lt;p&gt;The four slogans on the three buses in the tweet are: Get your AI to behave, When your AI goes off leash, Get your AI to work, and Evaluate, monitor, and guardrail your AI. And ‚Äúthe AI‚Äù is depicted as a little devil of sorts that needs to be made to behave.&lt;/p&gt;
    &lt;head rend="h5"&gt;Self Driving Cars&lt;/head&gt;
    &lt;p&gt;This is one of my three traditional sections where I update one of my three initial tables of prediction from my predictions exactly eight years ago today. In this section I talk about self driving cars, driverless taxi services, and what that means, my own use of driverless taxi services in the previous year, adoption of electric vehicles in the US, and flying cars and taxis, and what those terms mean.&lt;/p&gt;
    &lt;p&gt;No entries in the table specifically involve 2025 or 2026, and the status of predictions that are further out in time remain the same. I have only put in one new comment, about how many cities in the US will have self-driving (sort of) taxi services in 2026 and that comment is highlighted,&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Prediction&lt;p&gt;[Self Driving Cars]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
        &lt;cell role="head"&gt;2018 Comments&lt;/cell&gt;
        &lt;cell role="head"&gt;Updates&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A flying car can be purchased by any US resident if they have enough money.&lt;/cell&gt;
        &lt;cell&gt;NET 2036&lt;/cell&gt;
        &lt;cell&gt;There is a real possibility that this will not happen at all by 2050.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Flying cars reach 0.01% of US total cars.&lt;/cell&gt;
        &lt;cell&gt;NET 2042&lt;/cell&gt;
        &lt;cell&gt;That would be about 26,000 flying cars given today's total.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Flying cars reach 0.1% of US total cars.&lt;/cell&gt;
        &lt;cell&gt;NIML&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;First dedicated lane where only cars in truly driverless mode are allowed on a public freeway.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2021&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;This is a bit like current day HOV lanes. My bet is the left most lane on 101 between SF and Silicon Valley (currently largely the domain of speeding Teslas in any case). People will have to have their hands on the wheel until the car is in the dedicated lane.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Such a dedicated lane where the cars communicate and drive with reduced spacing at higher speed than people are allowed to drive&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2024&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;20240101 &lt;p&gt;This didn't happen in 2023 so I can call it now. But there are no plans anywhere for infrastructure to communicate with cars, though some startups are finally starting to look at this idea--it was investigated and prototyped by academia 20 years ago.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;First driverless "taxi" service in a major US city, with dedicated pick up and drop off points, and restrictions on weather and time of day.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2021&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The pick up and drop off points will not be parking spots, but like bus stops they will be marked and restricted for that purpose only.&lt;/cell&gt;
        &lt;cell&gt;20240101 &lt;p&gt;People may think this happened in San Francisco in 2023, but it didn't. Cruise has now admitted that there were humans in the loop intervening a few percent of the time. THIS IS NOT DRIVERLESS. Without a clear statement from Waymo to the contrary, one must assume the same for them. Smoke and mirrors.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Such "taxi" services where the cars are also used with drivers at other times and with extended geography, in 10 major US cities&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2025&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;A key predictor here is when the sensors get cheap enough that using the car with a driver and not using those sensors still makes economic sense.&lt;/cell&gt;
        &lt;cell&gt;20250101 &lt;p&gt;Imminent dual use of personal cars was the carrot that got lots of people to pay cash when buying a Tesla for the software subscription that would allow their car to operate in this way. Shockingly the CEO of Tesla announced in smoke and mirrors roll out of Cyber Cab in 2024, that the service would use specially built vehicles to be produced at some indeterminate late date. I got suckered by his hype. This is unlikely to happen in the first half of this century.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Such "taxi" service as above in 50 of the 100 biggest US cities.&lt;/cell&gt;
        &lt;cell&gt;NET 2028&lt;/cell&gt;
        &lt;cell&gt;It will be a very slow start and roll out. The designated pick up and drop off points may be used by multiple vendors, with communication between them in order to schedule cars in and out.&lt;/cell&gt;
        &lt;cell&gt;20250101 &lt;p&gt;Even the watered down version of this with remote operators is not gong to happen in 50 cities by 2028. Waymo has it in 3 cities and is currently planning on 2 more in the US in 2025.&lt;/p&gt;20260101 &lt;p&gt;Waymo did indeed add two cities in 2025, Austin and Atlanta. In those two cities they use Uber as their booking service. They are also expanding the metropolitan reach in their existing cities San Francisco, Los Angeles, and Phoenix. They have promised five more US cities in 2026, i.e., they promise to double the number of cities this year. They would have to then quintuple in 2027 to beat my prediction. Unlikely.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Dedicated driverless package delivery vehicles in very restricted geographies of a major US city.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2023&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The geographies will have to be where the roads are wide enough for other drivers to get around stopped vehicles.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A (profitable) parking garage where certain brands of cars can be left and picked up at the entrance and they will go park themselves in a human free environment.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2023&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The economic incentive is much higher parking density, and it will require communication between the cars and the garage infrastructure.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A driverless "taxi" service in a major US city with arbitrary pick and drop off locations, even in a restricted geographical area.&lt;/cell&gt;
        &lt;cell&gt;NET 2032&lt;p&gt;NET 2032&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;This is what Uber, Lyft, and conventional taxi services can do today.&lt;/cell&gt;
        &lt;cell&gt;20240101 &lt;p&gt;Looked like it was getting close until the dirty laundry came out.&lt;/p&gt;20250101 &lt;p&gt;Waymo now has a service that looks and feels like this in San Francisco, 8 years earlier than I predicted. But it is not what every one was expecting. There are humans in the loop. And for those of us who use it regularly we know it is not as general case on drop off and pick up as it is with human drivers.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Driverless taxi services operating on all streets in Cambridgeport, MA, and Greenwich Village, NY.&lt;/cell&gt;
        &lt;cell&gt;NET 2035&lt;/cell&gt;
        &lt;cell&gt;Unless parking and human drivers are banned from those areas before then.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A major city bans parking and cars with drivers from a non-trivial portion of a city so that driverless cars have free reign in that area.&lt;/cell&gt;
        &lt;cell&gt;NET 2027&lt;p&gt;BY 2031&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;This will be the starting point for a turning of the tide towards driverless cars.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;The majority of US cities have the majority of their downtown under such rules.&lt;/cell&gt;
        &lt;cell&gt;NET 2045&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Electric cars hit 30% of US car sales.&lt;/cell&gt;
        &lt;cell&gt;NET 2027&lt;/cell&gt;
        &lt;cell&gt;20240101 &lt;p&gt;This one looked pessimistic last year, but now looks at risk. There was a considerable slow down in the second derivative of adoption this year in the US.&lt;/p&gt;20250101 &lt;p&gt;Q3 2024 had the rate 8.9% so there is no way it can reach 30% in 2027. I was way too optimistic at a time when EV enthusiasts thought I was horribly pessimistic.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Electric car sales in the US make up essentially 100% of the sales.&lt;/cell&gt;
        &lt;cell&gt;NET 2038&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Individually owned cars can go underground onto a pallet and be whisked underground to another location in a city at more than 100mph.&lt;/cell&gt;
        &lt;cell&gt;NIML&lt;/cell&gt;
        &lt;cell&gt;There might be some small demonstration projects, but they will be just that, not real, viable mass market services.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;First time that a car equipped with some version of a solution for the trolley problem is involved in an accident where it is practically invoked.&lt;/cell&gt;
        &lt;cell&gt;NIML&lt;/cell&gt;
        &lt;cell&gt;Recall that a variation of this was a key plot aspect in the movie "I, Robot", where a robot had rescued the Will Smith character after a car accident at the expense of letting a young girl die.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A Brief Recap of what ‚ÄúSelf Driving‚Äù Cars Means and Meant&lt;/p&gt;
    &lt;p&gt;This is a much abridged and updated version of what I wrote exactly one year ago today.&lt;/p&gt;
    &lt;p&gt;The definition, or common understanding, of what self driving cars really means has changed since my post on predictions eight years ago. At that time self driving cars meant that the cars would drive themselves to wherever they were told to go with no further human control inputs. It was implicit that it meant level 4 driving. Note that there is also a higher level of autonomy, level 5, that is defined.&lt;/p&gt;
    &lt;p&gt;Note that in the second row of content, it says that there will be no need for a human to take over for either level 4 or level 5. For level 4 there may be pre-conditions on weather and within a supported geographic area. Level 5 eliminates pre-conditions and geographic constraints. So far no one is claiming to have level 5.&lt;/p&gt;
    &lt;p&gt;However the robot taxi services such as Cruise (now defunct), Waymo, currently operating in five US cities, and Zoox, currently operating in two cities with limited service (Las Vegas and San Francisco), all relied, or rely, on having remote humans who the car can call on to help get them out of situations they cannot handle. That is not what level 4 promises. To an outside observer it looks like level 4, but it is somewhat less than that in reality. This is not the same as a driver putting their hands back on the steering wheel in real time, but it does mean that there is sometimes a remote human giving high level commands to the car. The companies do not advertise how often this happens, but it is believed to be every few miles of driving. The Tesla self driving taxis in Austin have a human in the passenger seat to intervene when there is a safety concern.&lt;/p&gt;
    &lt;p&gt;One of the motivations for self driving cars was that the economics of taxis, cars that people hire at any time for a short ride of a few miles from where they are to somewhere else of their choosing, would be radically different as there would be no driver. Systems which do require remote operations assistance to get full reliability cut into that economic advantage and have a higher burden on their ROI calculations to make a business case for their adoption and therefore their time horizon to scaling across geographies.&lt;/p&gt;
    &lt;p&gt;Actual self-driving is now generally accepted to be much harder than every one believed.&lt;/p&gt;
    &lt;p&gt;As a reminder of how strong the hype was and the certainty of promises that it was just around the corner here is a snapshot of a whole bunch of predictions by major executives from 2017.&lt;/p&gt;
    &lt;p&gt;I have shown this many times before but there are three new annotations here for 2025 in the lines marked by a little red car. The years in parentheses are when the predictions were made. The years in blue are the predicted years of achievement. When a blue year is shaded pink it means that it did not come to pass by then. The predictions with orange arrows are those that I had noticed had later been retracted.&lt;/p&gt;
    &lt;p&gt;It is important to note that every prediction that said something would happen by a year up to and including 2025 did not come to pass by that year. In fact none of those have even come to pass by today. NONE. Eighteen of the twenty predictions were about things that were supposed to have happened by now, some as long as seven years ago. NONE of them have happened yet.&lt;/p&gt;
    &lt;p&gt;My Own Experiences with Waymo in 2025&lt;/p&gt;
    &lt;p&gt;I took two dozen rides with Waymo in San Francisco this year. There is still a longer wait than for an Uber at most times, at least for where I want to go. My continued gripe with Waymo is that it selects where to pick me up, and it rarely drops me right at my house ‚Äî but without any indication of when it is going to choose some other drop off location for me.&lt;/p&gt;
    &lt;p&gt;The other interaction I had was in early November when I felt like I was playing bull fighter, on foot, to a Waymo vehicle. My house is on a very steep hill in San Francisco, with parallel parking on one side and ninety degree parking on the other side. It is rare that two cars can pass each other traveling in opposite directions without one having to pull over into some empty space somewhere.&lt;/p&gt;
    &lt;p&gt;In this incident I was having a multi-hundred pound pallet of material deliverd to my home. There was a very big Fedex truck parked right in front of my house, facing uphill, and the driver/operator was using a manual pallet jack to get it onto the back lift gate, but the load was nine feet long so it hung out past the boundary of the truck. An unoccupied Waymo came down the hill and was about to try to squeeze past the truck on that side. Perhaps it would have made it through if there was no hanging load. So I ran up to just above the truck on the slope and tried to get the Waymo to back up by walking straight at it. Eventually it backed up and pulled in a little bit and sat still. Within a minute it tried again. I pushed it back with my presence again. Then a third time. Let‚Äôs be clear it would have been a dangerous situation if it had done what it was trying to do and could have injured the Fedex driver who it had not seen at all. But any human driver would have figured out what was going on and that the Fedex truck would never go down the hill backwards but would eventually drive up the hill. Any human driver would have replanned and turned around. After the third encounter the Waymo stayed still for a while. Then it came to life and turned towards the upwards direction, and when it was at about a 45 degree angle to the upward line of travel it stopped for a few seconds. Then it started again and headed up and away. I infer that eventually the car had called for human help, and when the human got to it, they directed it where on the road to go to (probably with a mouse click interface) and once it got there it paused and replanned and then headed in the appropriate direction that the human had made it already face.&lt;/p&gt;
    &lt;p&gt;Self Driving Taxi Services&lt;/p&gt;
    &lt;p&gt;There have been three self driving taxi services in the US in various stages of play over the last handful of years, though it turns out, as pointed out above that all of them have remote operators. They are Waymo, Cruise, and Zoox.&lt;/p&gt;
    &lt;p&gt;___Cruise&lt;/p&gt;
    &lt;p&gt;Cruise died in both 2023 and 2024, and is now dead, deceased, an ex self driving taxi service. Gone. I see its old cars driving around the SF Bay Area, with their orange paint removed, and with humans in the driver seat. On the left below are two photos I took on May 30th at a recharge station. ‚ÄúBirdie‚Äù looked just like an old Cruise self driving taxi, but without an orange paint. I hunted around around in online stories about Cruise and soon found another ‚ÄúBirdie‚Äù, with orange paint, and the same license plate. So GM are using them to gather data, perhaps for training their level 3 driving systems.&lt;/p&gt;
    &lt;p&gt;___Tesla&lt;/p&gt;
    &lt;p&gt;Tesla announced to much hoopla that they were starting a self driving taxi service this year, in Austin. It requires a safety person to be sitting in the front passenger seat at all times. Under the certification with which they operate, on occasion that front seat person is required to move to the driver‚Äôs seat. Then it just becomes a regular Tesla with a person driving it and FSD enabled. The original fleet was just 30 vehicles, with at least seven accidents reported by Tesla by October, even with the front seat Tesla person. In October the CEO announced that the service would expand to 500 vehicles in Austin in 2025. By November he had changed to saying they would double the fleet. That makes 60 vehicles. I have no information that it actually happened.&lt;/p&gt;
    &lt;p&gt;He also said he wanted to expand the ‚ÄúRobotaxi‚Äù service to Phoenix, San Francisco, Miami, Las Vegas, Dallas, and Houston by the end of 2025. It appears that Tesla can not get permits to run even supervised (mirroring the Austin deployment) in any of those cities. And no, they are not operating in any of those cities and now 2025 has reached its end.&lt;/p&gt;
    &lt;p&gt;In mid-December there were confusing reports saying that Tesla now had Model Y‚Äôs driving in Austin without a human safety monitor on board but that the Robotaxi service for paying customers (who are still people vetted by Tesla) resumed their human safety monitors. So that is about three or four years behind Waymo in San Francisco, and not at all at scale.&lt;/p&gt;
    &lt;p&gt;The CEO of Tesla has also announced (there are lots of announcements and they are often very inconsistent‚Ä¶) that actually the self driving taxis will be a new model with no steering wheel nor other driver controls. So they are years away from any realistic deployment. I will not be surprised if it never happens as the lure of humanoids completely distracts the CEO. If driving with three controls, (1) steering angle of the front wheels, (2) engine torque (on a plus minus continuum), and (3) brake pedal pressure, are too hard to make actually work safely for real, how hard can it be to have a program control a heavy unstable balancing platform with around 80 joints in hips and waist, two legs, two arms and five articulated fingers on each hand?&lt;/p&gt;
    &lt;p&gt;___Waymo&lt;/p&gt;
    &lt;p&gt;Meanwhile Waymo had raised $5.6B to expand to new cities in 2025. It already operated in parts of San Francisco, Los Angeles, and Phoenix. During 2025 it expanded to Austin and Atlanta, the cities it had promised. It also increased its geographic reach in its existing cities and surrounding metropolitan areas. In the original three cities users have a Waymo app on their phone and specifically summon a Waymo. In the new cities however they used a slightly different playbook. In both Austin and Atlanta people use their standard Uber app. They can update their preference to say that they prefer to get a Waymo rather than a human driven car, but there is no guarantee that a Waymo is what they will get. And any regular user of the Uber app in those cities may be offered a Waymo, but they do get an option to decline and to continue to wait for a human driven offer.&lt;/p&gt;
    &lt;p&gt;In the San Francisco area, beyond the city itself, Waymo first expanded by operating in Palo Alto, in a geographically separate area. Throughout the year one could see human operated Waymos driving in locations all along the peninsula from San Francisco to Palo Alto and further south to San Jose. By November Waymo had announced driverless operations throughout that complete corridor, an area of 260 square miles, but not quite yet on the freeways‚Äìthe Waymos are operating on specific stretches of both 101 and 280, but only for customers who have specifically signed up for that possibility. Waymo is now also promising to operate at the two airports, San Jose and San Francisco. The San Jose airport came first, and San Francisco airport is operating in an experimental mode with a human in the front seat.&lt;/p&gt;
    &lt;p&gt;Waymo has announced that it will expand to five more cities in the US during 2026; Miami, Dallas, Houston, San Antonio, and Orlando. It seems likely, given their step by step process, and their track record of meeting their promises that Waymo has a good shot at getting operations running in these five cities, doubling their total number of US cities to 10.&lt;/p&gt;
    &lt;p&gt;Note that although it does very occasionally snow in five of these ten cities (Atlanta, Austin, Houston, San Antonio, and Orlando) it is usually only a dusting. It is not yet clear whether Waymo will operate when it does snow. It does not snow in the other five cities, and in San Francisco Waymo is building to be a critical part of the transportation infrastructure. How well that would work if a self driving taxi service was subject to tighter restrictions than human driven services due to weather could turn into a logistical nightmare for the cities themselves. In the early days of Cruise they did shut down whenever there was a hint of fog in San Francisco, and that is a common occurrence. It was annoying for me, but Cruise never reached the footprint size in San Francisco that Waymo now enjoys.&lt;/p&gt;
    &lt;p&gt;No promises yet from Waymo about when it might start operating in cities that do commonly have significant snow accumulations.&lt;/p&gt;
    &lt;p&gt;In May of 2025 Waymo announced a bunch of things in one press release. First, that they had 1,500 Jaguar-based vehicles at that time, operating in San Francisco, Los Angeles, Phoenix, and Austin. Second, that they were no longer taking deliveries of any more Jaguars from Jaguar, but that they were now building two thousand of their own Jaguars in conjunction with Magna (a tier one auto supplier that also builds small run models of big brands ‚Äî e.g., they build all the Mini Coopers that BMW sells) in Mesa, Arizona. Third, that they would also start building, in late 2025, versions of the Zeekr RT, a vehicle that they co-designed with Chinese company Geely, that can be built with no steering wheel or other controls for humans, but with sensor systems that are self-cleaning.&lt;/p&gt;
    &lt;p&gt;It is hard to track exactly how many Waymos are deployed, but in August 2025, this website, citing various public disclosures by Waymo, put together the following estimates for the five cities in which Waymo was operating.&lt;/p&gt;
    &lt;quote&gt;Phoenix 400 San Francisco 800 Los Angeles 500 Austin 100 Atlanta 36&lt;/quote&gt;
    &lt;p&gt;No doubt those numbers have increased by now. Meanwhile Waymo has annualized revenues of about $350M and is considering an IPO with a valuation of around $100B. With numbers like those it can probably raise significant growth capital independently from its parent company.&lt;/p&gt;
    &lt;p&gt;___Zoox&lt;/p&gt;
    &lt;p&gt;The other self driving taxi system deployed in the US is Zoox which is currently operating only in small geographical locations within Las Vegas and San Francisco. Their deployment vehicles have no steering wheel or other driver controls‚Äìthey have been in production for many years. I do notice, by direct observation as I drive and walk around San Francisco, that Zoox has recently enlarged the geographic areas where its driverful vehicles operate, collecting data across all neighborhoods. So far the rides are free on Zoox, but only for people who have gone through an application process with the company. Zoox is following a pattern established by both Cruise and Waymo. It is roughly four years behind Cruise and two years behind Waymo, though it is not clear that it has the capital available to scale as quickly as either of them.&lt;/p&gt;
    &lt;p&gt;All three companies that have deployed actual uncrewed self driving taxi services in the US have been partially or fully owned by large corporations. GM owned Cruise, Waymo is partially spun out of Google/Alphabet, and Zoox is owned by Amazon.&lt;/p&gt;
    &lt;p&gt;Cruise failed. If any other company wants to compete with Waymo or Zoox, even in cities where they do not operate, it is going to need a lot of capital. Waymo and Zoox are out in front. If one or both of them fail, or lose traction and fail to grow, and grow very fast, it will be near to impossible for other companies to raise the necessary capital.&lt;/p&gt;
    &lt;p&gt;So it is up to Waymo and Zoox. Otherwise, no matter how well the technology works, the dream of driverless taxis is going to be shelved for many years.&lt;/p&gt;
    &lt;p&gt;Electric Cars&lt;/p&gt;
    &lt;p&gt;In my original predictions I said that electric car (and I meant battery electric, not hybrids) sales would reach 30% of the US total no earlier than 2027. A bunch of people on twitter claimed I was a pessimist. Now it looks like I was an extreme optimist as it is going to take a real growth spurt to reach even 10% in 2026, i.e., earlier than 2027.&lt;/p&gt;
    &lt;p&gt;Here is the report that I use to track EV sales ‚Äî it is updated every few weeks. In this table I have collected the quarterly numbers that are finalized. The bottom row is the percentage of new car sales that were battery electric.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="15"&gt;
        &lt;cell&gt;'22&lt;/cell&gt;
        &lt;cell&gt;'22&lt;/cell&gt;
        &lt;cell&gt;'22&lt;/cell&gt;
        &lt;cell&gt;'22&lt;/cell&gt;
        &lt;cell&gt;'23&lt;/cell&gt;
        &lt;cell&gt;'23&lt;/cell&gt;
        &lt;cell&gt;'23&lt;/cell&gt;
        &lt;cell&gt;'23&lt;/cell&gt;
        &lt;cell&gt;'24&lt;/cell&gt;
        &lt;cell&gt;'24&lt;/cell&gt;
        &lt;cell&gt;'24&lt;/cell&gt;
        &lt;cell&gt;'24&lt;/cell&gt;
        &lt;cell&gt;'25&lt;/cell&gt;
        &lt;cell&gt;'25&lt;/cell&gt;
        &lt;cell&gt;'25&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="15"&gt;
        &lt;cell&gt;Q1&lt;/cell&gt;
        &lt;cell&gt;Q2&lt;/cell&gt;
        &lt;cell&gt;Q3&lt;/cell&gt;
        &lt;cell&gt;Q4&lt;/cell&gt;
        &lt;cell&gt;Q1&lt;/cell&gt;
        &lt;cell&gt;Q2&lt;/cell&gt;
        &lt;cell&gt;Q3&lt;/cell&gt;
        &lt;cell&gt;Q4&lt;/cell&gt;
        &lt;cell&gt;Q1&lt;/cell&gt;
        &lt;cell&gt;Q2&lt;/cell&gt;
        &lt;cell&gt;Q3&lt;/cell&gt;
        &lt;cell&gt;Q4&lt;/cell&gt;
        &lt;cell&gt;Q1&lt;/cell&gt;
        &lt;cell&gt;Q2&lt;/cell&gt;
        &lt;cell&gt;Q3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;5.3&lt;/cell&gt;
        &lt;cell&gt;5.6&lt;/cell&gt;
        &lt;cell&gt;6.1&lt;/cell&gt;
        &lt;cell&gt;6.5&lt;/cell&gt;
        &lt;cell&gt;7.3&lt;/cell&gt;
        &lt;cell&gt;7.2&lt;/cell&gt;
        &lt;cell&gt;7.9&lt;/cell&gt;
        &lt;cell&gt;8.1&lt;/cell&gt;
        &lt;cell&gt;7.3&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;8.9&lt;/cell&gt;
        &lt;cell&gt;8.7&lt;/cell&gt;
        &lt;cell&gt;7.5&lt;/cell&gt;
        &lt;cell&gt;7.4&lt;/cell&gt;
        &lt;cell&gt;10.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Although late in 2024 EV sales were pushing up into the high eight percentage points they have dropped back into the sevens this year in the first half of the year. Then they picked up to 10.5% in the third quarter of 2025, but that jump was expected as the Federal electric vehicle (EV) tax credits ended for all new and used vehicles purchased after September 30, 2025, as part of the ‚ÄúOne Big Beautiful Bill Act‚Äù. People bought earlier than they might have in order to get that tax credit, so the industry is expecting quite a slump in the fourth quarter, but it will be a couple more months before the sales figures are all in. YTD 2025 is still under 8.5%, and is likely to end at under 8%.&lt;/p&gt;
    &lt;p&gt;The trends just do not look like we will get to EVs reaching 12% of US cars being sold in 2027, even with a huge uptick. 30% is just not going to happen.&lt;/p&gt;
    &lt;p&gt;As for which brands are doing better than others, Tesla‚Äôs sales dropped a lot more than the rest of the market. Brand winners were GM, Hyundai, and Volkswagen.&lt;/p&gt;
    &lt;p&gt;The US experience is not necessarily the experience across the world. For instance Norway reached 89% fully electric vehicles of all sold in 2024, largely due to taxes on gasoline powered car purchases. But that is a social choice of the people of Norway, not at all driven by oil availability. With a population of 5.6 million compared to the US with 348 million, and domestic oil production of 2.1 million barrels per day, compared to the US with 13.4 million b/d, Norway has a per capita advantage of almost ten times as much oil per person (9.7 to be more precise).&lt;/p&gt;
    &lt;p&gt;Electrification levels of cars is a choice that a country makes.&lt;/p&gt;
    &lt;p&gt;Flying Cars&lt;/p&gt;
    &lt;p&gt;The next two paragraphs are reproduced from last‚Äôs years scorecard.&lt;/p&gt;
    &lt;p&gt;Flying cars are another category where the definitions have changed. Back when I made my predictions it meant a vehicle that could both drive on roads and fly through the air. Now it has come to mean an electric multi-rotor helicopter than can operate like a taxi between various fixed landing locations. Often touted are versions that have no human pilot. These are known as eVTOLs, for ‚Äúelectric vertical take off &amp;amp; landing‚Äù.&lt;/p&gt;
    &lt;p&gt;Large valuations have been given to start ups who make nice videos of their electric air taxis flying about. But on inspection one sees that they don‚Äôt have people in them. Often, you might notice, even those flights are completely over water rather than land. I wrote about the lack of videos of viable prototypes back in November 2022.&lt;/p&gt;
    &lt;p&gt;The 2022 post referred to in the last sentence was trying to make sense of a story about a German company, Volocoptor, receiving a $352M Series E investment. The report from pitchbook predicted world wide $1.5B in revenue in the eVTOL taxi service market for 2025. I was bewildered as I could not find a single video, as of the end of 2022, of a demo of an actual flight profile with actual people in an actual eVTOL of the sort of flights that the story claimed would be generating that revenue in just 3 years.&lt;/p&gt;
    &lt;p&gt;I still can‚Äôt find such a video. And the actual revenue for actual flights in 2025 turned out to be $0.0B (and there are no rounding errors there ‚Äî it was $0) and Volocoptor has gone into receivership, with a ‚Äúreorganization success‚Äù in March 2025.&lt;/p&gt;
    &lt;p&gt;In my November 2022 blog post above I talked about another company, Lilium, which came the closest to having a video of a real flight, but it was far short of carrying people and it did not fly as high as is needed for air taxi service. At the time Lilium had 800 employees. Since then Lilium has declared bankruptcy not once (December 2024), but twice (February 2025), after the employees had been working for some time without pay.&lt;/p&gt;
    &lt;p&gt;But do not fear. There are other companies on the very edge of succeeding. Oh, and an edge means that sometimes you might fall off of it.&lt;/p&gt;
    &lt;p&gt;Here is an interesting report on the two leading US eVTOL companies, Archer and Joby Aviation, both aiming at the uncrewed taxi service market; both with valuations in the billions, and both missing just one thing. A for real live working prototype.&lt;/p&gt;
    &lt;p&gt;The story focuses on a pivotal point, the moment when an eVTOL craft has risen vertically, and now needs to transition to forward motion. In particular it points out that Archer has never demonstrated that transition, even with a pilot onboard, and during 2025 they cancelled three scheduled demonstrations at three different air shows. They did get some revenue in 2025 by selling a service forward to the city of Abu Dhabi, but zero revenue for actual operations‚Äìthey have no actual operations. They promise that for this year, 2026, with revenue producing flights in the second half of the year.&lt;/p&gt;
    &lt;p&gt;Joby Aviation did manage to demonstrate the transition maneuver in April of 2025. And in November they made a point to point flight in Dubai, i.e., their test vehicle managed to take off somewhere and land at a different place. The fact that there were press releases for these two human piloted pretty basic capabilities for an air taxi service suggests to me that they are still years away from doing anything that is an actual taxi service (and with three announced designated place to land and take off from it seems more like a rail network with three stations rather than a taxi service‚Äìagain slippery definitions do indeed slip and slide). And many more years away from a profitable service. But perhaps it is naive of me to think that a profitable business is the goal.&lt;/p&gt;
    &lt;p&gt;As with many such technology demonstrators the actual business model seems to be getting cities to spend lots of money on a Kabuki theater technology show, to give credit to the city as being technology forward. Investors, meanwhile invest in the air taxi company thinking it is going to be a real transportation business.&lt;/p&gt;
    &lt;p&gt;But what about personal transport that you own, not an eVTOL taxi service at all,but an eVTOL that you can individually own, hop into whenever you want and fly it anywhere? In October there was a story in the Wall Street Journal: ‚ÄúI Test Drove a Flying Car. Get Ready, They‚Äôre Here.‚Äù The author of the story spent three days training to be the safety person in a one seat Pivotal Helix (taking orders at $190,000 a piece, though not yet actually delivering them; also take a look at how the vehicles lurch as they go through the pilot commanded transition maneuver). It is a one seater so the only person in the vehicle has to be the safety person in case something fails. He reports:&lt;/p&gt;
    &lt;p&gt;After three hellish days in a drooling, Dramamine-induced coma, I failed my check ride.&lt;/p&gt;
    &lt;p&gt;The next month he tried again. This time he had a prescription for the anti-emetic Zofran and a surplus-store flight suit. The flight suit was to collect his vomit and save his clothes. After four more days of training (that is seven total days of training), he qualified and finally took his first flight, and mercifully did not live up to his call sign of ‚ÄúUpchuck Yeager‚Äù. $\190,000 to buy the plane, train for seven days, vomit wildly, have to dress in a flight suit, and be restricted to take off and landing and only fly over privately owned agricultural land or water. This is not a consumer product, and this is not a flying car that is here, despite the true believer headline.&lt;/p&gt;
    &lt;p&gt;Two years ago I ended my review of flying cars with:&lt;/p&gt;
    &lt;p&gt;Don‚Äôt hold your breath. They are not here. They are not coming soon.&lt;/p&gt;
    &lt;p&gt;Last year I ended my review with:&lt;/p&gt;
    &lt;p&gt;Nothing has changed. Billions of dollars have been spent on this fantasy of personal flying cars. It is just that, a fantasy, largely fueled by spending by billionaires.&lt;/p&gt;
    &lt;p&gt;There are a lot of people spending money from all the investments in these companies, and it is a real dream that they want to succeed for many of them. But it is not happening, even at a tiny scale, anytime soon.&lt;/p&gt;
    &lt;head rend="h6"&gt;Robotics, AI, and Machine Learning&lt;/head&gt;
    &lt;p&gt;We are peak popular hype in all of robotics, AI, and machine learning. In January 1976, exactly fifty years ago, I started work on a Masters in machine learning. I have seen a lot of hype and crash cycles in all aspects of AI and robotics, but this time around is the craziest. Perhaps it is the algorithms themselves that are running all our social media that have contributed to this.&lt;/p&gt;
    &lt;p&gt;But it does not mean that the hype is justified, or that the results over the next decade will pay back the massive investments that are going in to AI and robotics right now.&lt;/p&gt;
    &lt;p&gt;The current hype is about two particular technologies, with the assumption that these particular technologies are going to deliver on all the competencies we might ever want. This has been the mode of all the hype cycles that I have witnessed in these last fifty years.&lt;/p&gt;
    &lt;p&gt;One of the current darling technologies is large X models for many values of X (including VLMs and VLAs), largely, at the moment, using massive data sets, and transformers as their context and sequencing method. The other, isn‚Äôt even really a technology, but just a dream of a form of a technology and that is robots with humanoid form.&lt;/p&gt;
    &lt;p&gt;I have now put these two things in my five topics of my new predictions shared at the beginning of this post and will talk about them explicitly for each of the next ten years.&lt;/p&gt;
    &lt;p&gt;Back in 2018 I did not talk about either of these technologies in my predictions, but rather talked about competences and capabilities. I fear that I may have been overly optimistic about many of these and in the table below I point out that my predicted time of arrival has now come, but the capabilities or competencies have not. I‚Äôm sure that many true believers in the two technologies mentioned above will have very short time scales on when they say this will be achieved. I pre-emptively disagree with them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Prediction&lt;p&gt;[Robotics, AI, and ML]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
        &lt;cell role="head"&gt;2018 Comments&lt;/cell&gt;
        &lt;cell role="head"&gt;Updates&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Academic rumblings about the limits of Deep Learning&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;BY 2017&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Oh, this is already happening... the pace will pick up.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;The technical press starts reporting about limits of Deep Learning, and limits of reinforcement learning of game play.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;BY 2018&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;The popular press starts having stories that the era of Deep Learning is over.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;BY 2020&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;VCs figure out that for an investment to pay off there needs to be something more than "X + Deep Learning".&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2021&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;I am being a little cynical here, and of course there will be no way to know when things change exactly.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Emergence of the generally agreed upon "next big thing" in AI beyond deep learning.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2023&lt;/p&gt;
          &lt;p&gt;BY 2027&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Whatever this turns out to be, it will be something that someone is already working on, and there are already published papers about it. There will be many claims on this title earlier than 2023, but none of them will pan out.&lt;/cell&gt;
        &lt;cell&gt;20240101 &lt;p&gt;It definitely showed up in 2023. It was in the public mind in December 2022, but was not yet the big thing that it became during 2023. A year ago I thought it would perhaps be neuro-symbolic AI, but clearly it is LLMs, and ChatGPT and its cousins. And, as I predicted in 2018 it was something already being worked on as the "attention is all you need" paper, the key set of ideas, was published in 2017.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;The press, and researchers, generally mature beyond the so-called "Turing Test" and Asimov's three laws as valid measures of progress in AI and ML.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2022&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;I wish, I really wish.&lt;/cell&gt;
        &lt;cell&gt;20230101 &lt;p&gt;The Turing Test was missing from all the breathless press coverage of ChatGPT and friends in 2022. Their performance, though not consistent, pushes way past the old comparisons.&lt;/p&gt; 20240101 &lt;p&gt;The Turing Test was largely missing from the press in 2024 also, and there was a story in Nature commenting on that. So yes, this has now happened.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Dexterous robot hands generally available.&lt;/cell&gt;
        &lt;cell&gt;NET 2030&lt;p&gt;BY 2040 (I hope!)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Despite some impressive lab demonstrations we have not actually seen any improvement in widely deployed robotic hands or end effectors in the last 40 years.&lt;/cell&gt;
        &lt;cell&gt;20260101 &lt;p&gt;There has been progress over the last eight years with the dexterity of suction grippers, all in a single plane for surface contact. They have gotten much better, both in the lab and deployed, at picking from jumbled piles, much better than any articulated robot hands, including the standard paralllel jaw grippers. The "impressive lab demos" I talked about here were for articulated hands. Progress on such hands has been very slow. Learning methods have failed to accelerate performance.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A robot that can navigate around just about any US home, with its steps, its clutter, its narrow pathways between furniture, etc.&lt;/cell&gt;
        &lt;cell&gt;&lt;p&gt;Lab demo: NET 2026&lt;/p&gt;Expensive product: NET 2030&lt;p&gt;Affordable product: NET 2035&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;What is easy for humans is still very, very hard for robots.&lt;/cell&gt;
        &lt;cell&gt; 20250101 &lt;p&gt;A bunch of startups in the home robot space got significant funding in 2024. Two of them are run by ex-CEOs of large companies: iRobot and Cruise (and he was also an intern at iRobot after we were already a public company). So this one may be in play for a lab demo in the next few years if they have this as one of their goals.&lt;/p&gt;20260101 &lt;p&gt;No lab demos have occurred of a home robot class robot (I don't count as home robots small four legged robots that flail their legs quickly to beat gravity, and are therefore unsafe to be around children, and that can't do anything at all with their form factor besides scramble) that can navigate in a cluttered home and deal with even a single step. Amazon's Astro and Matic's vacuum robot have impressive in house navigation capabilities, but no way to handle steps.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A robot that can provide physical assistance to the elderly over multiple tasks (e.g., getting into and out of bed, washing, using the toilet, etc.) rather than just a point solution.&lt;/cell&gt;
        &lt;cell&gt;NET 2028&lt;/cell&gt;
        &lt;cell&gt;There may be point solution robots before that. But soon the houses of the elderly will be cluttered with too many robots.&lt;/cell&gt;
        &lt;cell&gt;20260101 &lt;p&gt;There have been point solution lab demos in a few labs, especially in countries where the demographic inversion has already hit hard. But no general purpose solution is in sight. Current humanoid robots are promising this, but they fail dangerously when in extended contact with a human being.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A robot that can carry out the last 10 yards of delivery, getting from a vehicle into a house and putting the package inside the front door.&lt;/cell&gt;
        &lt;cell&gt;&lt;p&gt;Lab demo: NET 2025&lt;/p&gt;Deployed systems: NET 2028&lt;/cell&gt;
        &lt;cell&gt;20260101&lt;p&gt;I missed calling this last year. Nothing at all that can do the last 10 yards of delivery from the street has been demonstrated at all. I now don't think we will see anything deployed this decade, and maybe not even a demo.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A conversational agent that both carries long term context, and does not easily fall into recognizable and repeated patterns.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Lab demo: NET 2023&lt;/p&gt;
          &lt;p&gt;Deployed systems: 2025&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deployment platforms already exist (e.g., Google Home and Amazon Echo) so it will be a fast track from lab demo to wide spread deployment.&lt;/cell&gt;
        &lt;cell&gt;20240101 &lt;p&gt;One half of this happened this year. ChatGPT has been connected to microphones and speakers so you can now talk to it. and It does not fall into recognizable patterns. BUT the other half is the half it does not have; it has no updatable memory apart from its token buffer of what it has just said. Long term context may be long term in coming.&lt;/p&gt;20260101 &lt;p&gt;I was too optimistic. It may have happened if LLMs had not come along (though the proficiency in language would surely have been poorer than what we now expect, post-LLM). If we are to do this with LLMs we will need to come up with a side mechanism beyond the token buffer--some form of episodic memory, and probably continuous model updating. There is work going on in these directions, but it has a ways to go yet to make a coherent agent that talks like it understands the flow of human worlds, emotions, and expectations.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;An AI system with an ongoing existence (no day is the repeat of another day as it currently is for all AI systems) at the level of a mouse.&lt;/cell&gt;
        &lt;cell&gt;NET 2030&lt;/cell&gt;
        &lt;cell&gt;I will need a whole new blog post to explain this...&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A robot that seems as intelligent, as attentive, and as faithful, as a dog.&lt;/cell&gt;
        &lt;cell&gt;NET 2048&lt;/cell&gt;
        &lt;cell&gt;This is so much harder than most people imagine it to be--many think we are already there; I say we are not at all there.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A robot that has any real idea about its own existence, or the existence of humans in the way that a six year old understands humans.&lt;/cell&gt;
        &lt;cell&gt;NIML&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Capabilities and Competences&lt;/p&gt;
    &lt;p&gt;The predictions that are commented upon in the table above are all about when we would see robots and AI systems doing some things that simple creatures can do and others that any child of age nine or perhaps less can do without any difficulty.&lt;/p&gt;
    &lt;p&gt;Even children aged three or four can navigate around cluttered houses without damaging them (that is different from when they may want to damage them). They can get up and down single stairs, and even full stair cases on two legs without stumbling (or resorting to four limb walking as a two year old might). By age four they can open doors with door handles and mechanisms they have never seen before, and safely close those doors behind them. They can do this when they enter a particular house for the first time. They can wander around and up and down and find their way.&lt;/p&gt;
    &lt;p&gt;One of the many promises about humanoid robots is that they too will be able to do this. But that is not what they can do today. But wait, you say, ‚ÄúI‚Äôve seem them dance and somersault, and even bounce off walls.‚Äù Yes, you have seen humanoid robot theater. All those things are done on hard surfaces, and anything specific beyond walking has been practiced and optimized by reinforcement learning, for exactly the situation of the floors and walls as they are. There is no real-time sensing and no ability to wander in previously unseen environments, especially not those with slipping hazards such as towels or sheets of cardboard on the floor. Children can do so easily. While four legged robots are much better at it than humanoid robots, they are wider than people, and still have significant foot slipping problems, and cannot open random doors themselves as children can.&lt;/p&gt;
    &lt;p&gt;A nine year old child can pretty much do any task (but with less weighty packages) than any delivery driver can do. That includes climbing out of a van, walking up and down slopes, going up and down previously unseen external staircases, sometimes ending in a dark porch or vestibule area, then putting the package on the ground, or putting it into a drop bin after grasping and pulling on the handle ‚Äî again never having encountered that particular design of bin and handle. All this can be done immediately upon seeing the scene for the first time. We have not seen anything remotely like that in a lab demo for robots, despite my hope from eight years ago that by now such would have been demonstrated. And again, here a four legged robot might be able to do the walking and stair climbing, but it won‚Äôt be able to manipulate the package. Also note that humans doing these tasks don‚Äôt just carry single packages out in front of them with two outstretched arms, but often use their elbows, their hips, and their bellies to support multiple packages as they locomote.&lt;/p&gt;
    &lt;p&gt;Elder care is a commonly quote target market for robots, and with good reason given the current and growing demographic inversions in much of the world. There are far fewer younger people relative to the number of older people than there have been historically, and so less people to provide elder care. In providing care to the very elderly, there is a need to support those people physically, both passively, providing compliant support for them to lean on, and actively, getting people into and out of bed, into and out of bathtubs or shower enclosures, and getting people onto and off of toilets. And sometimes wiping their bums. There are no force sensing and control capabilities on any of today‚Äôs robots which are remotely capable of doing any of these sorts of things safely and comfortably. And machine learning is not going to provide those capabilities. There are many fundamental design, materials, and engineering problems to solve to make these things possible. A bitter lesson, perhaps, for those who think that more data will solve everything.&lt;/p&gt;
    &lt;p&gt;But the other unresolved capability that I have in my predictions table above is an agent that understands the world in an ongoing way as we all understand it. That includes knowing what to expect to be the same as it was yesterday, and will be tomorrow, and what has changed about the world since yesterday or is likely to change today or tomorrow. Such an understanding of the world will be important for any deployable systems that can take care of real and vulnerable humans, including the elderly. And the young. And the rest of us.&lt;/p&gt;
    &lt;p&gt;In summary, I thought that more progress would be made on many of these problems than has been achieved over the last eight years. That lack of progress is going to have real, and negative, impact on the quality of life of the newly elderly for the next couple of decades. Ouch!&lt;/p&gt;
    &lt;p&gt;VCs, please take note: there are real pulls on having technologies that can help the elderly, and being in there first with something that can actually deliver value in the next three to five years will be a come with a very large upside.&lt;/p&gt;
    &lt;p&gt;World Models&lt;/p&gt;
    &lt;p&gt;Lots of people are talking about world models and their importance, as add ons to LLMs, as mechanisms for agentic AI to exploit, and for allowing robots to do real tasks. These aspirations are probably reasonable to have, and successfully working on them can have real impacts.&lt;/p&gt;
    &lt;p&gt;Unfortunately the talkers are not the doers, and not the deployers, and not the people who have to solve real problems. And so they all have different, and convenient for themselves, understandings of what world models are. That, along with the worship of big data and the belief that machine learning will solve all problems means we have a big mess, with people jumping to ‚Äúsolutions‚Äù before they understand the problems.&lt;/p&gt;
    &lt;p&gt;Some people are even claiming that they will build world models by learning them from having agents play video games. But how do those video games work? They have a coded geometry-based world model, with a little physics engine. It is already built! Using machine learning (and tens of millions of dollars) to extract it rather than just looking at the source code (and perhaps buying or licensing that code) is just wacky.&lt;/p&gt;
    &lt;p&gt;Expect more confusion and lots and lots of reinvention. This fever has quite a ways to go before today‚Äôs memes and slogans get replaced by the next generation of memes and slogans, with perhaps some good work coming out in a rational interregnum. We can hope.&lt;/p&gt;
    &lt;p&gt;Situatedness vs Embodiment&lt;/p&gt;
    &lt;p&gt;One of the new things that people are getting excited about is Embodied Intelligence. I agree that it is worth being excited about, as it is what I have spent the last forty years work on. It is certainly about robots being in the world.&lt;/p&gt;
    &lt;p&gt;But since 1991 I have made a distinction between two concepts where a machine, or creature can be either, neither, or both situated and embodied. Here are the exact definitions that I wrote for these back then:&lt;/p&gt;
    &lt;p&gt;[Situatedness] The robots are situated in the world‚Äîthey do not deal with abstract descriptions, but with the here and now of the world directly in-fluencing the behavior of the system.&lt;/p&gt;
    &lt;p&gt;[Embodiment] The robots have bodies and experience the world directly‚Äîtheir actions are part of a dynamic with the world and have immediate feed-back on their own sensations.&lt;/p&gt;
    &lt;p&gt;At first glance they might seem very similar. And they are, but they are also importantly different. And, spoiler alert, I think much of the work at companies, large and small, right now, is trying abstract out the embodiment of a robot, turning it into a machine that is merely situated.&lt;/p&gt;
    &lt;p&gt;An algorithm, written as code, to find the greatest common divisor of two numbers, when running, is neither situated nor embodied.&lt;/p&gt;
    &lt;p&gt;A robot that is thrown into the air with just an inertial measurement unit (IMU) as its sensor that moves its limbs about to zero out rotations and then is caught by a net is embodied but not situated.&lt;/p&gt;
    &lt;p&gt;A robot that has a physical face that can make expressions with it, a voice synthesizer, cameras, and microphones and that can talk to a person giving appropriate responses both with its choice of words and with appropriate prosody and facial expressions, to some purpose and in response to how the person talks and moves, is situated but not really embodied. Embodied in its presence yes, but not embodied in any physical interactions with its environment.&lt;/p&gt;
    &lt;p&gt;A robot that can roll around without hitting stationary objects, wherever they are, nor hitting moving people or other vehicles, that can go to a location specified by a warehouse management system, that responds safely to people grabbing it anywhere, and can give a person who grabs its control handle agency over it going wherever the person pushes it with a light touch no matter how much weight it is currently carrying, is both embodied and situated. [And yes, this is what our Carter robots do at Robust.AI.]&lt;/p&gt;
    &lt;p&gt;These are just point examples of the four classes of entities that come from having or not having the two properties of situatedness and embodiment.&lt;/p&gt;
    &lt;p&gt;Real robots that do real work in dynamic human occupied environments must be both situated and embodied. For instance, a robot that is to help with in home elder care needs to be aware of the situation in the world in order to know what to do to help the person. It needs to be able to open doors with different handles and latching mechanisms, and then control the inertia of the closing door so that the environment is both safe and quiet for the person. The robot needs to be able to accommodate the person reaching for it dynamically, looking for support that so that they don‚Äôt fall. The robot needs to able to take things handed to it by the person, and pass things to the person in a way which is both safe and makes it easy for the person to grasp. Etc., etc.&lt;/p&gt;
    &lt;p&gt;In short the robot needs to control forces and inertias in the world and to be responsive to them, at that same time as it is acting in a way that can be understood as sentient.&lt;/p&gt;
    &lt;p&gt;Being both situated and embodied is still a challenge to robots in the world. [[Now here is the most important sentence of this whole blog post.]] I think the training regimes that being used for both locomotion and dexterity are either ignoring or trying to zero out the embodiment of physical robots, their inertias and forces, reducing them to merely being situated, just apps with legs and arms, characters in video games, not the reality of real physical beings that the tasks we want them to do requires.&lt;/p&gt;
    &lt;p&gt;Dexterous Hands&lt;/p&gt;
    &lt;p&gt;I talked about the challenges for dexterity earlier this year. In the table above I have a new comment this year saying that there has been improvement in the dexterity of suction based grippers but not for articulated grippers.&lt;/p&gt;
    &lt;p&gt;Suction grippers have plastic suction cups which themselves are compliant. Under the force of the suction they can change shape, to a degree, to accommodate unknown shapes in the thing being grasped (sucked up to). They also allow for a little bit of torsional rotation about the axis of sucking and a bit of rocking of the suction cup in the two degrees of freedom in the plane orthogonal to the suction axis.&lt;/p&gt;
    &lt;p&gt;While suction cups have evolved to better pick things up and so are common for handling packaged goods, the companies that package materials to be shipped through automated systems choose versions of plastics for bags that won‚Äôt be sheared open by the suction pulling against outer parts of such cups.&lt;/p&gt;
    &lt;p&gt;The result is that the control of the embodied action of grasping can become much more a simply situated action. Once the pick orientation and vacuum gripper selection has been made it is really an open loop as all the work is done by the indiscriminate force of suction and the mutual compliance of the gripper and the grippee.&lt;/p&gt;
    &lt;p&gt;Above I had argued against do this with a general purpose humanoid hand. It makes no sense there as the adaptability of the hand is its most precious attribute. But here in a special purpose hand, a suction gripper, it actually simplifies things within the specialization of task, and here a purely situated hand may make sense. And it may be possible to train it with purely visual data.&lt;/p&gt;
    &lt;p&gt;So what does this tell us? It says that there is plenty of room for mechanical design, and simpler computational embodied control for all sorts of grippers and things in the world that need to be gripped.&lt;/p&gt;
    &lt;p&gt;The end of Moore‚Äôs Law, at least the version that said we could reduce feature size on silicon by a factor two every year, opened up a new golden era of chip design. The winners (through early luck and then dogged determination), matched untraditional designs to new problems (machine learning) and achieved speedups (and corporate valuations) that were unheard of. In the last 10 years we have moved from general purpose silicon to special purpose silicon for our most high volume computations. That was not on most people‚Äôs expectation list twenty years ago.&lt;/p&gt;
    &lt;p&gt;So too today, with stalled capabilities from full human hand emulation efforts through machine learning from visual observation, there is a rich array of more specialized manipulation tasks where special purpose grippers, clever interplay of materials and force applications, geometric planning, specialized sensing, and maybe even some machine learning may lead to enormous application markets.&lt;/p&gt;
    &lt;p&gt;For instance, a specialized robot body, hands (of some sort), arms, and support limbs or wheels that can safely manipulate an elderly human could have enormous impact on elder care around the world. A single human care-giver along with one human-manipulator robot could provide a lot more care for a frail elderly person than the care-giver alone could do.&lt;/p&gt;
    &lt;p&gt;Special purpose manipulators for fruits, or for some range of small mechanical parts, or clothing, could each open enormous markets for automation in particular handling tasks for each of them. And countless other specialities.&lt;/p&gt;
    &lt;p&gt;Economic pull is out there. Being the smart academic researcher, entrepreneur, or technology investor, may lead to enormous new types of deployable automation.&lt;/p&gt;
    &lt;p&gt;The new dexterity may turn out to be special purpose. And eventually we may come to understand that just because the hands we know best happen to be our own, does not mean that our own hands are the best for the majority of tasks in our human world.&lt;/p&gt;
    &lt;p&gt;Humanoid romanticism may not be our future after all.&lt;/p&gt;
    &lt;head rend="h5"&gt;HUMAN SpaceFLIGHT&lt;/head&gt;
    &lt;p&gt;Looking at the missions and numbers over the last three years it appears that human spaceflight is at a steady plateau, with, by the way, far fewer people going into orbit that in the time of the Space Shuttle. Underneath though, there is a lot of churn, a likely new player, and the return of humans to lunar distances for the first time in 54 years.&lt;/p&gt;
    &lt;p&gt;Below is the updated scoring of my 2018 predictions for human spaceflight. There are six new comments in this table, but no new specific calling of predicted dates as right or wrong. It is now clear to me that I was way too optimistic in regard to my predictions for Mars, even though I was wildly out of step and much more pessimistic then the predictions coming out of SpaceX eight years ago. Given how slow things have turned out trying to land people on the Moon, the hoped for crewed colony on the Moon (think of it as ISS (International Space Station) on the lunar surface) may well slip to what I had predicted for Mars. Mars is going to take much longer than the Moon.&lt;/p&gt;
    &lt;p&gt;Following the table there are the detailed numbers and trends on both orbital crewed flights, and suborbital crewed flights. Things will change from stasis in 2026. A crewed flight to the Moon is scheduled to happen in a matter of weeks, with the vehicle already stacked, now. And suborbital crewed flights may possibly have quite an uptick in 2026. Following those two sections I have more on Boeing‚Äôs Starliner, SpaceX‚Äô Starship, and Blue Origin‚Äôs New Glenn, NASA and the Moon, and what is going to happen with space stations given the scheduled end of life of the ISS in 2030.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Prediction&lt;p&gt;[Space]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
        &lt;cell role="head"&gt;2018 Comments&lt;/cell&gt;
        &lt;cell role="head"&gt;Updates&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Next launch of people (test pilots/engineers) on a sub-orbital flight by a private company.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;BY 2018&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A few handfuls of customers, paying for those flights.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2020&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;A regular sub weekly cadence of such flights.&lt;/cell&gt;
        &lt;cell&gt;&lt;p&gt;NET 2022&lt;/p&gt;BY 2026&lt;/cell&gt;
        &lt;cell&gt;20240101 &lt;p&gt;There were four flights in 2021, three in 2022, and seven, five with customers on board, in 2023--all of them by Virgin Glactic. Blue Origin did not fly in 2023. At this point 2026 is looking doubtful for regular flights every week.&lt;/p&gt;20250101 &lt;p&gt;Now 2026 is looking impossible given the data from 2023 and 2024, and one of the two companies being on hiatus for all of 2025, and well into 2026.&lt;/p&gt;20260101 &lt;p&gt;Blue Origin more than doubled their flights to seven in 2025, and has now announced that weekly flights are their next goal, but without a firm date. Their plan includes a second launch location and a set of new vehicles that are being built. 2028 is starting to look plausible for weekly flights, if the paying customer demand really is there.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Regular paying customer orbital flights.&lt;/cell&gt;
        &lt;cell&gt;NET 2027&lt;/cell&gt;
        &lt;cell&gt;Russia offered paid flights to the ISS, but there were only 8 such flights (7 different tourists). They are now suspended indefinitely.&lt;/cell&gt;
        &lt;cell&gt;20240101&lt;p&gt;There were three paid flights in 2021, and one each in 2022, and 2023, with the latter being the Axiom 2 mission using SpaceX hardware. So not regular yet, and certainly not common.&lt;/p&gt;20250101 &lt;p&gt;There were two paid flights in 2024.&lt;/p&gt;20260101 &lt;p&gt;And there were two more in 2025, making six paid flights over three years. Axiom is steady at one flight a year, channeling funds from governments with no launch capability. Two of the other three flights were personally payed for by the now NASA administrator, so unlikely to continue. This is not yet a vibrant recurring private flight model.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Next launch of people into orbit on a US booster.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2019&lt;/p&gt;
          &lt;p&gt;BY 2021&lt;/p&gt;
          &lt;p&gt;BY 2022 (2 different companies)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Current schedule says 2018.&lt;/cell&gt;
        &lt;cell&gt;20240101&lt;p&gt;Both SpaceX and Boeing were scheduled to have crewed flights in 2018. SpaceX pulled it off in 2020, Boeing's Starliner did not fly at all in 2023, but is scheduled to launch with people onboard for the first time in April 2024.&lt;/p&gt;20250101 &lt;p&gt;The second company did finally launch humans into orbit in June 2024, so it has happened three years later than I predicted and six years later than what had been promised when my prediction was made. Of course, everyone implicitly assumed that along with getting humans into space the companies would also be able to bring them back. Not so for Boeing.&lt;/p&gt;20260101 &lt;p&gt;After one crewed launch, and no crewed landings, Boeing is back to uncrewed testing late in the first half of 2026.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Two paying customers go on a loop around the Moon, launch on Falcon Heavy.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2020&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The most recent prediction has been 4th quarter 2018. That is not going to happen.&lt;/cell&gt;
        &lt;cell&gt;20240101&lt;p&gt;Starship launched twice in 2023 but didn't get to orbit either time. This is going to be well over six years later than the original prediction by the CEO of SpaceX.&lt;/p&gt;20250101 &lt;p&gt;The billionaire who signed up for this and paid a hefty deposit in 2017 gave up waiting and cancelled the contract in 2024. This fantasy is over, for now at least.&lt;/p&gt;20260101 &lt;p&gt;NASA is planning on sending four astronauts (including one from Canada) on a loop around the Moon in the first half of the year. But it is not a paying customer trip.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Land cargo on Mars for humans to use at a later date&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NET 2026&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;SpaceX has said by 2022. I think 2026 is optimistic but it might be pushed to happen as a statement that it can be done, rather than for a pressing practical reason.&lt;/cell&gt;
        &lt;cell&gt;20240101&lt;p&gt;I was way too optimistic, and bought into the overoptimistic hype of the CEO of SpaceX even though I added four years, doubling his estimated time frame.&lt;/p&gt;20260101 &lt;p&gt;I can now call this as orbital mechanics and Hohmann transfer windows dictate that the cargo would need to have been launched a few months ago for it to get to Mars in 2025. It has not been launched.&lt;/p&gt;20260101 &lt;p&gt;This is not going to happen for many years, and SpaceX may not be the first to do it.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Humans on Mars make use of cargo previously landed there.&lt;/cell&gt;
        &lt;cell&gt;NET 2032&lt;/cell&gt;
        &lt;cell&gt;Sorry, it is just going to take longer than every one expects.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;First "permanent" human colony on Mars.&lt;/cell&gt;
        &lt;cell&gt;NET 2036&lt;/cell&gt;
        &lt;cell&gt;It will be magical for the human race if this happens by then. It will truly inspire us all.&lt;/cell&gt;
        &lt;cell&gt;20260101 &lt;p&gt;I think I was way too optimistic. I now seriously doubt a human will land on Mars before 2040, and the earliest settlement won't happen before 2050. The currently announced mission architectures lack both science for protecting the crew en route and engineering practicality. I will allow that, just perhaps, an organization is working hard on these two problems with no public exposure. But that in itself would be a first.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Point to point transport on Earth in an hour or so (using a BF rocket).&lt;/cell&gt;
        &lt;cell&gt;NIML&lt;/cell&gt;
        &lt;cell&gt;This will not happen without some major new breakthrough of which we currently have no inkling.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Regular service of Hyperloop between two cities.&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;NIML&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;I can't help but be reminded of when Chuck Yeager described the Mercury program as "Spam in a can".&lt;/cell&gt;
        &lt;cell&gt;20240101&lt;p&gt;Calling this one 26 years early. As of today no-one is still working on this in an operating company.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Orbital Crewed Flights&lt;/p&gt;
    &lt;p&gt;In both 2024 and 2025 the US put 16 people into orbit and Russian and China put 6 people each into orbit; 28 people total went to orbit in each year. We have gone from a historical low of only eight people going to orbit in 2020 to a steady-ish state of roughly 28 people per year now. That may jump up to over 30 people in 2026 because of the additional Artemis II flight to the Moon, following checkout in LEO (Low Earth Orbit). But even with that bump there may be other pressures which keep it from rising above the high twenties for 2026&lt;/p&gt;
    &lt;p&gt;We are certainly not seeing steady growth in the number of humans getting launched to orbit, and the numbers are significantly lower than the hey days of Shuttle launches in the nineties and early two thousands. There is no growth trend visible, and the long promised exponential growth of people going to orbital space has not even made a brief guest appearance.&lt;/p&gt;
    &lt;p&gt;Here is a more detailed history for the last six years where the first line in each box says how many crewed launches of the particular vehicle there were, and the second line, in square brackets says how many people, total, were onboard those flights. Wherever there are three numbers separated by forward slashes you have to sum the numbers to get the total.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Launch family&lt;/cell&gt;
        &lt;cell role="head"&gt;2020&lt;/cell&gt;
        &lt;cell role="head"&gt;2021&lt;/cell&gt;
        &lt;cell role="head"&gt;2022&lt;/cell&gt;
        &lt;cell role="head"&gt;2023&lt;/cell&gt;
        &lt;cell role="head"&gt;2024&lt;/cell&gt;
        &lt;cell role="head"&gt;2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;SpaceX Dragon&lt;/cell&gt;
        &lt;cell&gt;1/0/0&lt;p&gt;[2/0/0]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/0/1&lt;p&gt;[8/0/4]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/1/0&lt;p&gt;[8/4/0]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/1/0&lt;p&gt;[8/4/0]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/1/1&lt;p&gt;[6/4/4]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/1/1&lt;p&gt;[8/4/4]&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Boeing Starliner&lt;/cell&gt;
        &lt;cell&gt;1/0/0&lt;p&gt;[2/0/0]&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Russia Soyuz&lt;/cell&gt;
        &lt;cell&gt;2/0/0&lt;p&gt;[6/0/0]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/2/0&lt;p&gt;[8/4/0]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/0/0&lt;p&gt;[6/0/0]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1/0/0&lt;p&gt;[3/0/0]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/0/0&lt;p&gt;[6/0/0]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2/0/0&lt;p&gt;[6/0/0]&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;China Shenzou&lt;/cell&gt;
        &lt;cell&gt;2&lt;p&gt;[6]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2&lt;p&gt;[6]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2&lt;p&gt;[6]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2&lt;p&gt;[6]&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2&lt;p&gt;[6]&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Humans&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;24&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;28&lt;/cell&gt;
        &lt;cell&gt;28&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The three countries with current crewed orbital launch capabilities are the US, Russia, and China.&lt;/p&gt;
    &lt;p&gt;All Chinese flights are state astronauts (or taikonauts) and all of them go to the Chinese space station. And there are no tourists, so far, on Chinese flights, so we just have single numbers for both launches and people.&lt;/p&gt;
    &lt;p&gt;All the state astronauts for both the US and Russia go to the International Space Station (ISS), but a state player (in Russia) and a non-state player in the US (SpaceX) have also launched tourist flights in the last six years. So for those two countries we have three numbers separated by slashes for both launches and people. The first of the three numbers refers to purely state launches to the ISS (note that the US and Russia both launch each others state astronauts to the ISS so that both countries have astronauts up-to-date trained on the other‚Äôs launch systems, in case of emergencies arising at some point). The second number in the triples is space tourists whose destinations have also been the ISS, while the third number (for both launches and people) is for tourist flights that have been independent of going to the ISS ‚Äî there have been a total of three of these, all launched by SpaceX. Two of those three flights were purchased personally by Jared Issacman, who has now been sworn in as the NASA administrator just two weeks ago.&lt;/p&gt;
    &lt;p&gt;The one year in the last six where Russia has launched space tourists (after being the leaders in this endeavor early in the century) was 2021, where two flights of Soyuz to the ISS had one regular state cosmonaut and two space tourists. And, there was one slightly wobbly other launch of a Soyuz in 2024, not called out in the table, where a flight attendant from the state airline of Belarus was sent as a cosmonaut from that country to the ISS on a Russian Soyuz. That was most likely an event orchestrated by Russia to keep support from Belarus for their war against Ukraine. Ugly.&lt;/p&gt;
    &lt;p&gt;The term tourist needs some explanation. The people (as with suborbital Blue Origin flights) are a mixture of private people paying the experience (or having some other individual pay for them) or they are astronauts from countries that do not have their own launch capability. In the case of the three tourist flights to the ISS on a SpaceX Dragon, all have been paid for by the company Axiom, with a former NASA astronaut in command. The three others on each of those flights are people in the fledgling astronaut program of other countries who have paid Axiom for the seats. Axiom has commercial relationships with both SpaceX and NASA for the use of the Flacon 9 launch vehicle, the Dragon craft and use fee of the ISS.&lt;/p&gt;
    &lt;p&gt;Suborbital Crewed Flights&lt;/p&gt;
    &lt;p&gt;Virgin Galactic is on a multi-year hiatus on flights as they develop new flight vehicles, but they may well fly again in 2026. Thus, for the last year, only Blue Origin has been launching tourists (again a mixture of private individuals and astronauts from other countries that have not yet developed their own crewed launch capability, but may be aiming at doing so) suborbital flights. Blue Origin also sells uncrewed launches for experiments that need to be exposed to the environment of space and/or operation in microgravity, if only for a few minutes.&lt;/p&gt;
    &lt;p&gt;In 2025 Blue Origin had seven launches each with six people on board. Previously they had had three crewed launches in each of 2021, 2022, and 2024, each with six people on board, with a hiatus in 2023.&lt;/p&gt;
    &lt;p&gt;Blue Origin has been quite careful with forward projections for both suborbital and orbital flights, so when they say what they intend to do and when, they are likely to come close to achieving that promise.&lt;/p&gt;
    &lt;p&gt;Recently they said that they are going to introduce three new flight vehicles starting in 2026 to run their suborbital flights, that they are looking at developing a second launch site, somewhere else than Texas, and that they believe they have the customer demand to support one flight per week. They do not disclose what they charge for the flights. Nor did they give any firm dates for reaching these goals. But I think it is likely that we will see a jump in the number of flights in 2026,&lt;/p&gt;
    &lt;p&gt;In December of 2025 I was at an event centered on solar system orbital dynamics and met a sub-orbital tourist there. He has already paid for and flown above the K√°rm√°n line on Virgin Galactic. Now he has paid for a Blue Origin sub-orbital flight and is waiting for a launch assignment. There is definitely a market for these flights, it remains to be seen whether the prices and demand combine in a way that makes it profitable for seat suppliers to keep doing it.&lt;/p&gt;
    &lt;p&gt;Boeing‚Äôs Starliner (not to be confused with the SpaceX Starship)&lt;/p&gt;
    &lt;p&gt;When it was first announced, in 2010, Boeing‚Äôs Starliner was originally scheduled to fly a human test crew in 2018. It was supposed send the crew to the ISS, then it would be under contract to launch six crews to the ISS, much as SpaceX has already launched 11 regular crews to the ISS.&lt;/p&gt;
    &lt;p&gt;In mid 2024 it delivered a human test crew to ISS, Barry Wilmore and Sunita Williams, but after much analysis of anomalies it returned to Earth without them. NASA bumped two crew members from the next crew going on a SpaceX flight to the ISS to provide room for their return, on that SpaceX Dragon, which they did after an unexpected extra nine months on top of their originally scheduled week at the ISS.&lt;/p&gt;
    &lt;p&gt;Last year in my yearly update I said:&lt;/p&gt;
    &lt;p&gt;We do not know at this point, but I think it would not be a huge surprise if Starliner never flies again.&lt;/p&gt;
    &lt;p&gt;It turns out it is going to fly again! Including potentially twice in 2026. But there are some changes. The six missions which were contracted to take astronauts on regular assignment to the ISS were called Starliner-1 through Starliner-6. The contract with NASA has been modified to make the last two flights future options rather than sure things. And Starliner-1 scheduled for the first half of 2026 will be un-crewed again. Then the three remaining flights in the modified contract would each take four astronauts on regular rotations to the ISS. There is one little hiccup. Sunita Williams is the only active astronaut, not committed to other current or upcoming missions, who has trained to fly on Starliner. She now has over 600 days in space and another six month mission to the ISS would take her over radiation exposure limits.&lt;/p&gt;
    &lt;p&gt;SpaceX Falcon 9&lt;/p&gt;
    &lt;p&gt;I gave the statistics for Falcon 9 in the introduction, talking about what has surprised me in the last 8 years. When I made my predictions Falcon 9 had been launched 46 times over 8 years. Only five of those launches re-used a previously flown first stage, and only in the previous year had successful landings of the first stage become reliable.&lt;/p&gt;
    &lt;p&gt;Now Falcon 9s are getting launched at a sustained rate of more than three per week, all attempts at landing boosters are successful, and typically each booster flies over 20 times.&lt;/p&gt;
    &lt;p&gt;Just phenomenal unmatched reliability and performance.&lt;/p&gt;
    &lt;p&gt;NASA, Artemis, and Returning to the Moon&lt;/p&gt;
    &lt;p&gt;I am jumping ahead of Starship (SpaceX) and New Glenn (Blue Origin) to talk about NASA‚Äôs plan to get people back to the lunar surface, and perhaps setting up a more or less permanent outpost there. This is how the ISS has been continuously occupied for 25 years, rotating crew members in and out twice a year. (China‚Äôs space station follows the same model, but with only 3 occupants compared to 7 for ISS).&lt;/p&gt;
    &lt;p&gt;2026 promises to be a big year for humanity and the Moon. No one has been beyond low Earth orbit (LEO) since the Apollo 17 mission had three people go to lunar orbit and two of them landed in December 1972, fifty three years ago.&lt;/p&gt;
    &lt;p&gt;In November 2022 the first launch of NASA‚Äôs SLS (Space Launch System) occurred taking its uncreewed Orion capsule in a looping orbit past the Moon and back. It approached the surface of the Moon in each direction, and then came back to Earth and splashed down. Note that this was the FIRST flight of both the multi-stage rocket, and the habitable capsule. It all worked FIRST time. Everything was built by contractors, but it underwent NASA‚Äôs methodology to make sure things worked rather than failed.&lt;/p&gt;
    &lt;p&gt;The first stage consists of a liquid fueled rocket using four RS-25 engines, the same as the three engines on the Space Shuttle. It also has two solid fuel boosters strapped on, larger versions of the Space Shuttle solid fuel boosters. The second stage is essentially an off the shelf stage from the past Delta program. There will be a third stage added for the fourth and subsequent flights. This is a derivative vehicle, with a long history of successful use of its components.&lt;/p&gt;
    &lt;p&gt;When Vice President Mike Pence announced the details of the program in 2019 the landing of people on the Moon was supposed to happen in 2024. Things have slipped a little since then.&lt;/p&gt;
    &lt;p&gt;The first crewed mission to the vicinity of the Moon (no landing) Artemis II had slipped to April 2026, but now it has been pulled forward to February 2026 (next month!), when a crew of four will spend over ten days in space on Artemis II in a flight to the Moon approaching to within 4,600 miles, then in a free return manner (no need to have working engines) they will head back towards Earth. All their energy will be removed by heat shields hitting the Earth‚Äôs atmosphere and then by the use of 11 parachutes, finally splashing down in the ocean. Note that on all 9 flights to the Moon of the Apollo program, the spacecraft came much closer to the Moon than this, and 8 of the flights went into orbit at around 60 to 70 miles above the surface. So this is a more conservative mission than those of Apollo.&lt;/p&gt;
    &lt;p&gt;Things at this stage are looking good for Artemis to fly in February 2026.&lt;/p&gt;
    &lt;p&gt;The next step of the Artemis is where things get wobbly. Rather than 2024, the first landing of astronauts on the Moon is currently scheduled for 2027. But that is not going to happen. Here is what the architecture of the mission currently looks like:&lt;/p&gt;
    &lt;p&gt;Here we see the problem with the schedule, even with it currently slipped to landing two astronauts on the Moon in 2027.&lt;/p&gt;
    &lt;p&gt;The architecture uses the SLS and Orion to get the astronauts to lunar orbit. Given there is a lunar flyby with astronauts onboard, scheduled for just two months from now (and the rocket is already stacked for that mission) that looks like a reasonable interpolation from existing progress.&lt;/p&gt;
    &lt;p&gt;The problem with the new plan is the landing vehicle and getting it to lunar orbit. It is all based on SpaceX‚Äôs Starship. So far, Starship has had 11 flights, six of which have been successful in reaching their own goals, and 5 of which have been failures. But there has not yet, in eleven flights, been a goal of getting anything into orbit. And just in 2025 two vehicles have been destroyed by failures on the ground when the tanks have been pressure tested. In the section on Starship below I will talk more about what I see as conflicting product requirements which together doom Starship to a very long development process.&lt;/p&gt;
    &lt;p&gt;For comparison, the Saturn V which took astronauts to the Moon nine times had a total of 13 flights, every one of which got payloads to Earth orbit. Two were uncrewed tests (and there were problems with the second and third stages on the second of these test flights). Its very first crewed flight (Apollo 8) took people to the Moon. and a total of 9 launches got people to the Moon. The other two flights were (Apollo 9) a crewed flight to test the Lunar Lander and orbital rendezvous in Earth orbit, and the uncrewed launch of the first space station, Skylab.&lt;/p&gt;
    &lt;p&gt;Now look again at the plan for the Artemis III mission. It requires multiple (reported numbers range from 14 to somewhere into the twenties) launches of the Starship to orbit.&lt;/p&gt;
    &lt;p&gt;One of those launches uses the Super Heavy Booster and a special version of the second stage actual Starship, known as Starship HLS (Human Landing System). That special version is expendable after it lands astronauts on the Moon, hosts them for perhaps two weeks, then brings them back to lunar orbit where they transfer to NASA‚Äôs Orion. Then it sends itself off into heliocentric orbit for all eternity. The HLS version is special in two ways. First it does not have to get back to Earth and so doesn‚Äôt need heat shields and does not need the three in-atmosphere Raptors for soft landing on Earth (see the section on Starship below). That is good for all the mass equations. But it does, or might, have a second set of engines for landing on the Moon that are attached halfway up its body so that they cause less lunar dust to fly around as it lands. We have not yet seen a prototype of that version, not even a public rendering as far as I can tell. I have talked to people who are in regular communication with people inside SpaceX. They report not a peep about what work has been done to design or build the lander. That is not good for the current public schedule.&lt;/p&gt;
    &lt;p&gt;BUT the really, really bad thing is that the lunar lander stage will use up most its fuel getting into Earth orbit ‚Äî it is the second stage of the rocket after all. So it cannot get to the Moon unless it is refueled. That will be done by sending up regular versions of the Starship second stage, all on reusable Super Heavy Boosters. They too will use up most of their fuel getting to orbit, and will need to keep some to get back to Earth to be reused on another flight. But it will have a little margin and its extra fuel will be transferred to the lunar landing Starship in orbit.&lt;/p&gt;
    &lt;p&gt;No one has ever demonstrated transfer of liquid fuel in space. Because of the way the numbers work out it takes somewhere in the teens of these refueling operations, and depending on how quickly certified higher performance engines can be developed and tested for both the Super Heavy Booster and Starship itself, that number of refueling flights might range into the twenties.&lt;/p&gt;
    &lt;p&gt;As an engineer this architecture looks to me like trouble, and with an impossible future. I am sure it will not happen in 2027, and I have doubts that it ever will.&lt;/p&gt;
    &lt;p&gt;The acting administrator of NASA, Sean Duffy who is also the head of the US Department of Transportation, was worried about this too, and in October of 2025 he reopened bidding on the contract for a crewed lander for the Moon that collects and returns its crew from Orion in lunar orbit.&lt;/p&gt;
    &lt;p&gt;The day after this announcement SpaceX said they were working on a simplified architecture to land people in the Moon. They have given no details of what this architecture looks like, but here are some options proposed by the technical press.&lt;/p&gt;
    &lt;p&gt;A couple of weeks later the President announced the renomination of Jared Isaacman to be the NASA administrator, having withdrawn his nomination a few months before. Isaacman is a private citizen who personally paid for, and flew on, two of the three SpaceX crewed missions which have not flown to the ISS. He was confirmed to the NASA position on December 17th, 2025, just two weeks ago.&lt;/p&gt;
    &lt;p&gt;At the very least expect turbulence, both political and technical, in getting astronauts landed on the Moon. And see a possible surprise development below.&lt;/p&gt;
    &lt;p&gt;SpaceX Starship (not to be confused with Boeing‚Äôs Starliner)&lt;/p&gt;
    &lt;p&gt;Starship is SpaceX‚Äôs superheavy two stage rocket, designed to put 150(?) tons of payload into orbit, with components having been under development since 2012, going through extensive redesigns along the way. There have also been three major designs, builds, and tests of the Raptor engines that power both stages.&lt;/p&gt;
    &lt;p&gt;This is how Wikipedia currently introduces them:&lt;/p&gt;
    &lt;p&gt;Raptor is a family of rocket engines developed and manufactured by SpaceX. It is the third rocket engine in history designed with a full-flow staged combustion fuel cycle, and the first such engine to power a vehicle in flight. The engine is powered by cryogenic liquid methane and liquid oxygen, a combination known as methalox.&lt;/p&gt;
    &lt;p&gt;SpaceX‚Äôs super-heavy-lift Starship uses Raptor engines in its Super Heavy booster and in the Starship second stage. Starship missions include lifting payloads to Earth orbit and is also planned for missions to the Moon and Mars. The engines are being designed for reuse with little maintenance.&lt;/p&gt;
    &lt;p&gt;Currently the Raptor 3 version is expected to be used for operational Starship launches, and it comes in two versions. There are 33 Raptors in the first stage designed to operate optimally in the atmosphere, along with three such engines in the second stage, which also houses three vacuum optimized Raptors. The first stage engines and the second stage vacuum engines are designed to get payloads to orbit. The vacuum engines on the second stage would also be used for further operations on the way to the Moon and descending towards the surface there. And for non-expendable second stages they would be used for the initial de-orbit burn for landing the second stage Starship back on Earth. After using the heat shields to burn off some more energy as it enters the atmosphere the second set of engines, the atmosphere optimized Raptors, are used to slow it down to a soft landing.&lt;/p&gt;
    &lt;p&gt;Other systems for returning to Earth have used different tradeoffs. The Space Shuttle used its wings to slow down to very high horizontal landing speed, and then a combination of a drag parachute after touchdown and brakes on the wheels to get down to zero velocity. US capsules, such as Mercury, Gemini, Apollo, Orion, and Dragon have all used heat shields followed by parachutes during vertical fall, and lastly dropped into the sea for dampening the final residual velocity. (Soyuz, Starliner, and New Shepard all use last second retro rockets before hitting the ground, rather than water.)&lt;/p&gt;
    &lt;p&gt;This means that unlike all the other solutions Starship has to carry a complete set of engines into orbit just for use during landing, along with enough fuel and oxidant to land. This is a high performance price for the thing that flies in space, mostly. The engines on the Starship first stage, like those on Falcon 9 and Blue Origin‚Äôs New Glenn, do get to space but never get to more than a small fraction of orbital speed, so returning them to Earth is a much, much, lower performance price than Starship‚Äôs second stage return of engines and fuel.&lt;/p&gt;
    &lt;p&gt;The 2025 flights of Starship were, on average, better than the 2024 flights, but two vehicles destroyed themselves before getting to the flight stage, and still nothing got into orbit.&lt;/p&gt;
    &lt;p&gt;How close is it to working? I don‚Äôt know. But I do keep tabs on promises that have been made.&lt;/p&gt;
    &lt;p&gt;In November of 2024 the President of SpaceX said ‚ÄúI would not be surprised if we fly 400 Starship launches in the next four years‚Äù. A year ago today I said in response: ‚ÄúLooking at the success of Falcon 9 it is certainly plausible that I may live to see 400 Starship launches in a four year period, but I am quite confident that it will not happen in the next four years (2025 through 2028)‚Äù. We are a quarter of the way through her predicted time frame and we have gone from being 400 orbital launches away from her goal down to being a mere 400 away.&lt;/p&gt;
    &lt;p&gt;Blue Origin Gets to Orbit&lt;/p&gt;
    &lt;p&gt;The suborbital tourist flights that Blue Origin operates are not its main business. It has ambitions to compete head to head with SpaceX.&lt;/p&gt;
    &lt;p&gt;But it is almost 600 launches behind, how can it be competitive? In 2025 Blue Origin made clear that it is not to be dismissed. From zero orbital launches at the start of 2025 to having two orbiters on their way to Mars (SpaceX has not yet done that) and showing that it can land a booster that has very very close to the performance of Falcon Heavy‚Äôs three booster configuration when landing all three boosters. And it may well do a soft landing on the Moon in 2026 (SpaceX won‚Äôt come close to that goal for a number of years).&lt;/p&gt;
    &lt;p&gt;In February Blue Origin launched its first New Glenn rocket. It‚Äôs first stage is powered by seven BE-4 engines (‚ÄúBlue Engine 4‚Äù), a methane burning engine that is more powerful than the Raptor 3 which will power new versions of SpaceX‚Äôs Starship. New Glenn reached orbit on its first attempt, and delivered a Blue Origin payload to space (a test version of their Blue Ring for in-space communications). The first stage attempted to land on Blue Origin‚Äôs Jacklyn landing platform at sea but failed.&lt;/p&gt;
    &lt;p&gt;The BE-4 had previously powered two United Launch Alliance Vulcan rockets to orbit under a deal where Blue Origin sells engines to ULA. The second stage of New Glenn is powered by two BE-3 engines, which are a variant of the single engine used on Blue Origin‚Äôs New Shepard.&lt;/p&gt;
    &lt;p&gt;In their second launch, in November, Blue Origin not only delivered three paid payloads to orbit (two of which are headed to Mars, where they will orbit the planet and carry out science experiments for UC Berkeley on what happened to Mars‚Äô atmosphere), but then the first stage (much larger than the first stage of a Falcon 9) landed on Jacklyn with an unrivaled level of precise control. Blue Origin plans to reduce the time spent hovering in future landings to reduce preserved fuel needs now that it has mastered return from orbit vertical landing. (Recall that they have landed dozens of New Shepard vertical landings on return from non-orbital flights.)&lt;/p&gt;
    &lt;p&gt;Soon after this impressive second outing for New Glenn, Blue Origin announced a number of upgrades.&lt;/p&gt;
    &lt;p&gt;They renamed the base vehicle that has now flown twice to be ‚ÄúNew Glenn 7√ó2‚Äù where 7 and 2 refer to the number of first stage and second stage engines respectively. They also announced that those flight engines would be upgraded to levels of thrust and duration that had already been demonstrated in ground tests. These are the new total thrust numbers, in pounds force.&lt;/p&gt;
    &lt;quote&gt;New Glenn 2025 New Glenn 7x2 1st stage 3,850,000 ==&amp;gt; 4,480,000 2nd stage 320,000 ==&amp;gt; 400,000&lt;/quote&gt;
    &lt;p&gt;Additionally New Origin announced a new heavier, taller, and with larger payload faring, version, the ‚ÄúNew Glenn 9√ó4‚Äù with two extra engines on each stage. Looking up from below the first stage the engine arrangement goes from the one on the left to the one on the right.&lt;/p&gt;
    &lt;p&gt;And here is who the two variants look compared to the Saturn V which took humans to the Moon in 1969.&lt;/p&gt;
    &lt;p&gt;The kicker to these successes is that the New Glenn 7√ó2 with a reusable first stage is very nearly equivalent to the Falcon Heavy when its three first stage boosters are reused. The reusable New Glenn 9√ó4 beats Falcon Heavy on all measures even when all three of Falcon Heavy are sacrificed and not recovered. I can‚Äôt quite get all the numbers but this table makes the comparisons with the numbers I can find.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Falcon 9&lt;/cell&gt;
        &lt;cell role="head"&gt;Falcon Heavy&lt;/cell&gt;
        &lt;cell role="head"&gt;New Glenn 7x2&lt;/cell&gt;
        &lt;cell role="head"&gt;New Glenn 9x4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1st stage thrust (lbs)&lt;/cell&gt;
        &lt;cell&gt;1,800,000&lt;/cell&gt;
        &lt;cell&gt;5,100,000&lt;/cell&gt;
        &lt;cell&gt;4,480,000&lt;/cell&gt;
        &lt;cell&gt;5,760,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2nd stage thrust (lbs)&lt;/cell&gt;
        &lt;cell&gt;210,000&lt;/cell&gt;
        &lt;cell&gt;210,000&lt;/cell&gt;
        &lt;cell&gt;400,000&lt;/cell&gt;
        &lt;cell&gt;800,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;fairing diam (meters)&lt;/cell&gt;
        &lt;cell&gt;5.2&lt;/cell&gt;
        &lt;cell&gt;5.2&lt;/cell&gt;
        &lt;cell&gt;7.0&lt;/cell&gt;
        &lt;cell&gt;8.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LEO payload (tonnes)&lt;/cell&gt;
        &lt;cell&gt;22.8&lt;/cell&gt;
        &lt;cell&gt;50&lt;p&gt;57 (core exp)&lt;/p&gt;&lt;p&gt;63.8 (3 exp)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;70&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;trans lunar insertion (tonnes)&lt;/cell&gt;
        &lt;cell&gt;3.3&lt;/cell&gt;
        &lt;cell&gt;16 (3 exp)&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note that a ‚Äútonne‚Äù is the British spelling for a metric ton, which is 1,000Kg. That is approximately 2,206 lbs, which is 206 lbs more than a US ton, and 34 lbs less than a British ton.&lt;/p&gt;
    &lt;p&gt;Meanwhile expectations are high for another launch of a New Glenn, the 7√ó2 version, sometime early in the new year. There has been no announcement from Blue Origin, nor any indication of the payload. But there is a general feeling that it may actually be a launch of Blue Origin‚Äôs Blue Moon Mark 1, an all up single launch mission to soft land on the Moon. It was announced almost a year ago that Blue Origin has a deal to deliver a NASA payload to the Moon in the Blue Moon Pathfinder mission no earlier than 2026. The Mark 1 uses a BE-7 engine to soft land.&lt;/p&gt;
    &lt;p&gt;Here is where things get interesting for a re-appraisal of how NASA astronauts might first land on the Moon again.&lt;/p&gt;
    &lt;p&gt;Blue Origin is already under contract with NASA to land two astronauts on the Moon for a 30 day stay in 2030 using their much larger Blue Moon Mark 2. The Mark 2 and Mark 1 share control systems and avionics, so a successful landing of Mark 1 will boost confidence in the Mark 2. The architecture for the 2030 mission involves multiple launches. A NASA SLS launches a crewed Orion capsule to the vicinity of the Moon. A New Glenn gets a Mark 2 Blue Moon to an orbit that approaches the Moon. A ‚ÄúCislunar Transporter‚Äù is launched separately and it gets fueled in LEO. Then it heads off to the same orbit as the Mark 2 and refuels it. The Mark 2 and the transporter both use three Blue Origin BE-7 engines which are now fully operational. Then the astronauts transfer to the Mark 2 to land on the Moon. Note that this architecture uses in flight refueling as does the SpaceX version, though with far fewer launches involved.&lt;/p&gt;
    &lt;p&gt;BUT, soon after then NASA administrator Sean Duffy announced the re-opening of the contract for the lander for Artemis III, it appeared that he was considering having Blue Origin use their Mark 1 version for the crewed mission. Whether that enthusiasm survives the changing of the guard to Jared Isaacman, the new and current NASA administrator, remains to be seen. And whether Blue Origin can pull off a rendezvous in lunar orbit, to pick up and return the crew members going to the lunar surface, from an orbiting Orion capsule is also an open question. I think the key idea with this option is to remove the need for any in flight refueling for the first crewed landing.&lt;/p&gt;
    &lt;p&gt;There is going to be some stiff competition between SpaceX and Blue Origin. Either might win.&lt;/p&gt;
    &lt;p&gt;New space stations&lt;/p&gt;
    &lt;p&gt;The International Space Station will be at end of life in 2030 after continuous human habitation for almost thirty years. The other space station currently in orbit is the Chinese Tiangong station.&lt;/p&gt;
    &lt;p&gt;Expect to see a real pick up in the building of space stations over the next few years, in anticipation of the end of the ISS.&lt;/p&gt;
    &lt;p&gt;The Russian Orbital Service Station (ROS) is scheduled to begin construction, by Roscosmos, in orbit in 2027. There is risk to this plan from the deterioration of the Russian economy.&lt;/p&gt;
    &lt;p&gt;India plans to start building their Bharatiya Antariksh Station (BAS) in 2028 and for it to be fully operational in 2035. India has had uncrewed orbital capability since 1980, and sent its first uncrewed mission to Mars in 2013. For BAS it is developing crewed launch capability. In 2025 India sent one of its own astronauts to the ISS on a SpaceX Dragon under an agreement with the company Axiom.&lt;/p&gt;
    &lt;p&gt;A consortium of countries (US, Canada, Japan, European Union, and the United Arab Emirates) are collaborating on building the Lunar Gateway, a space station orbiting the Moon. Launch of the first module is scheduled for 2027 on a SpaceX Falcon Heavy. Blue Origin is competing for additional components and launches for the Gateway.&lt;/p&gt;
    &lt;p&gt;A host of private companies plan on launching smaller private space stations in the near term, with one claiming it will do so in May 2026.&lt;/p&gt;
    &lt;p&gt;This is going to be an active frontier, and may lead to more humans going on orbital flights than the current status quo of about 28 per year.&lt;/p&gt;
    &lt;head rend="h5"&gt;Addendum&lt;/head&gt;
    &lt;p&gt;I have had a long warm personal relationship with Jonathan Hurst, Co-founder and Chief Robot Officer at Agility Robotics, back from pre-Agility days when he was first a junior faculty member at Oregon State University.&lt;/p&gt;
    &lt;p&gt;On January 2nd I received an email from him responding to my twelve points about what one has to believe for deployment of dexterous humanoid robots that can replace any physical human capability, all in the context of what Agility says and delivers. I have included them below, as he wrote them. Clearly Agility and its Digit robot is not in the hype class that I am pushing back on ‚Äî they are promising much less than general purpose human replacement. At the same time Jonathan believes that they are following a much longer road to general purpose humanoid robots that starts with much simpler applications. I do not think that they will end up at this lofty goal in even Jonathan‚Äôs lifetime (and he is much younger than me!). The two linked videos below are from Jonathan and accurately show the current capabilities, including that the people and robots are separated for safety, and that they have a use case and solution that does not rely on dexterous hands. I think this fits, directionally at least, with the last sentences of my September post on humanoid dexterity and safety, and included above in the new predictions segment, where I said: There will be many, many robots with different forms for different specialized jobs that humans can do. But they will all still be called humanoid robots.&lt;/p&gt;
    &lt;p&gt;From Jonathan Hurst:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Digit has moved over 140,000 totes to date, and has been paid to do it with an ongoing commercial contract. Our customers include GXO, Amazon, Shaeffler, and growing. The numbers are still small, limited by the safety case, and we will have a safety solution for our initial applications deploying in 2026.&lt;/item&gt;
      &lt;item&gt;Digit has grippers that are sometimes criticized as ‚Äúnot dexterous hands,‚Äù yet they are the only manipulator I‚Äôm aware of that can power-grasp and lift 25kg totes, among other things. In 2026, Digit will have tool-change capabilities, so we can use the right tool for the right application, and deploy increasingly capable and general manipulators.&lt;/item&gt;
      &lt;item&gt;We never, ever claim that we will learn human-like dexterity anytime soon. I claim that there are no manipulators in existence that are even physically capable of it, assuming we even know how to control them. I have opinions on how to get there, and we have work in progress, but it‚Äôs a long road, and I think Ken Salisbury was farther along that road in 1985 than most people are today.&lt;/item&gt;
      &lt;item&gt;Agility regularly shows fully autonomous operation, including live on stage, including live at trade shows, including deployed with customers. Here‚Äôs a 6-hour video from Modex.&lt;/item&gt;
      &lt;item&gt;We have no near-term plans for working in customer homes because that is not a realistic goal. I have stated publicly that humanoids aren‚Äôt going into homes for a decade or more, because the home is the highest bar for complexity, safety, and cost sensitivity. But that doesn‚Äôt mean there aren‚Äôt a lot of other markets to enter first, on the long road into the home.&lt;/item&gt;
      &lt;item&gt;We have a clear safety strategy which specifies that the robots will stop moving and sit down before a human can physically touch them. They are still useful in human workflows and human spaces. It‚Äôll be a long road to operating safely in close proximity to humans. We are collaborating to develop a new international safety standard, ISO 25785-1, which will specify how dynamically stable (i.e. balancing) robots can safely operate.&lt;/item&gt;
      &lt;item&gt;Digit has long been able to get up after falling.&lt;/item&gt;
      &lt;item&gt;Digit‚Äôs battery life is 90-120 minutes while doing heavy work, easily hitting 4hrs if it‚Äôs not working too hard. And Digit V5 will charge in 7‚Äì10 minutes.&lt;/item&gt;
      &lt;item&gt;Digit has been able to recharge itself for some time now on its own charging dock.&lt;/item&gt;
      &lt;item&gt;Agreed: humans cannot touch the humanoid safely for some time.&lt;/item&gt;
      &lt;item&gt;We have long said we build ‚ÄúHuman-centric, multi-purpose‚Äù robots, and that the long road to generality is achieved by doing one thing well, then two, then five, and working our way to clear, metric-driven performance on many different tasks.&lt;/item&gt;
      &lt;item&gt;We‚Äôre being real. Jennifer Hunter, our COO, deployed Kiva robots at Amazon. Pras Velagapudi, our CTO, deployed many hundreds of robots with Berkshire Grey. Melonee Wise was a helpful influence, sharing her experience deploying Fetch AMRs safely around people. We know what it takes to build and deploy robots.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is a lot of hype around humanoid robots, much like there is around AI ‚Äî and also like AI, there is a lot of near-term value to gain, for those who are strategic about it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46533343</guid><pubDate>Wed, 07 Jan 2026 21:40:41 +0000</pubDate></item></channel></rss>