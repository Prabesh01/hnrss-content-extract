<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 06 Oct 2025 21:08:47 +0000</lastBuildDate><item><title>Sharpie found a way to make pens more cheaply by manufacturing them in the U.S.</title><link>https://www.wsj.com/business/sharpie-us-production-cost-cutting-d9ba2abd</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45480354</guid><pubDate>Sun, 05 Oct 2025 10:01:00 +0000</pubDate></item><item><title>It's just a virus, the E.R. told him – days later, he was dead</title><link>https://www.nytimes.com/2025/10/05/well/sam-terblanche-virus-death-columbia.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45487519</guid><pubDate>Mon, 06 Oct 2025 03:57:26 +0000</pubDate></item><item><title>Gem.coop</title><link>https://gem.coop/</link><description>&lt;doc fingerprint="2b6d011076da77e2"&gt;
  &lt;main&gt;
    &lt;p&gt;Ruby ecosystem.&lt;/p&gt;
    &lt;p&gt;We aim for fast, simple hosting, that is compatible with Bundler but optimized for the next generation. It’s built for the community by the former maintainers and operators of RubyGems.org.&lt;/p&gt;
    &lt;p&gt;All gems published to RubyGems.org are available, updated in real time. Get started right now with a simple change to your Gemfile:&lt;/p&gt;
    &lt;code&gt;-source "https://rubygems.org"
+source "https://gem.coop"
&lt;/code&gt;
    &lt;p&gt;Governance for this project is modeled on Homebrew, with setup assistance from Mike McQuaid, and will be published on or before October 10. Everyone from the Ruby community is welcome to contribute and participate.&lt;/p&gt;
    &lt;p&gt;If you want to follow along with our progress, subscribe to the gem.coop newsletter for monthly updates.&lt;/p&gt;
    &lt;p&gt;Our goal is to provide fast, community-owned, transparent, sustainable, and secure gem hosting for everyone. We’re launching with support for bundling and installing all public gems, and we’re excited to keep improving.&lt;/p&gt;
    &lt;p&gt;— The Gem Cooperative (@deivid-rodriguez, @duckinator, @indirect, @martinemde, @segiddins, @simi)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45487771</guid><pubDate>Mon, 06 Oct 2025 04:59:39 +0000</pubDate></item><item><title>Battering RAM – Low-cost interposer attacks on confidential computing</title><link>https://batteringram.eu/</link><description>&lt;doc fingerprint="be7b4c01fd29b2c1"&gt;
  &lt;main&gt;
    &lt;p&gt;Modern computers use memory modules (DRAM) to store everything in use: from photos and passwords to credit card numbers. Public cloud providers increasingly deploy hardware-level memory encryption to protect this sensitive data. However, we previously showed that malicious memory modules, nicknamed “Bad RAM”, can bypass these protections by deliberately supplying false metadata during processor boot. In response, modern cloud systems now validate memory more strictly at startup.&lt;/p&gt;
    &lt;head rend="h4"&gt;Breaking Memory Encryption with Two-Faced DRAM&lt;/head&gt;
    &lt;p&gt;Battering RAM fully breaks cutting-edge Intel SGX and AMD SEV-SNP confidential computing processor security technologies designed to protect sensitive workloads from compromised hosts, malicious cloud providers, or rogue employees. Our stealthy interposer bypasses both memory encryption and state-of-the-art boot-time defenses, invisible to the operating system. It enables arbitrary plaintext access to SGX-protected memory, and breaks SEV’s attestation feature on fully patched systems. Ultimately, Battering RAM exposes the limits of today’s scalable memory encryption. Intel and AMD have acknowledged our findings, but defending against Battering RAM would require a fundamental redesign of memory encryption itself.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building Battering RAM on a $50 Budget&lt;/head&gt;
    &lt;p&gt;Unlike commercial passive interposers, which are exceedingly expensive and commonly cost over $100,000, we developed a custom-built interposer that uses simple analog switches to actively manipulate signals between the processor and memory, and can be built for less than $50.&lt;/p&gt;
    &lt;p&gt;All schematics and board files for our custom interposer are available as open source in our GitHub repository. The PCB is a standard 4-layer design and can be fabricated at any major PCB fabricator such as JLCPCB, PCBWay, or Eurocircuits.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Part Number&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Qty.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;PCB&lt;/cell&gt;
        &lt;cell&gt;Custom&lt;/cell&gt;
        &lt;cell&gt;$18.49&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DDR4 Connector&lt;/cell&gt;
        &lt;cell&gt;CONN-DDR4-288-SM&lt;/cell&gt;
        &lt;cell&gt;$16.00&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Microcontroller&lt;/cell&gt;
        &lt;cell&gt;Raspberry Pi Pico 1/2&lt;/cell&gt;
        &lt;cell&gt;$4.00&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Switches&lt;/cell&gt;
        &lt;cell&gt;ADG902BRMZ&lt;/cell&gt;
        &lt;cell&gt;$4.04&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Voltage regulator&lt;/cell&gt;
        &lt;cell&gt;LD1117S25TR&lt;/cell&gt;
        &lt;cell&gt;$0.61&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Resistor&lt;/cell&gt;
        &lt;cell&gt;0402, 1kOhm&lt;/cell&gt;
        &lt;cell&gt;&amp;lt;$0.01&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Capacitor&lt;/cell&gt;
        &lt;cell&gt;0603, 100nF&lt;/cell&gt;
        &lt;cell&gt;$0.02&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Capacitor&lt;/cell&gt;
        &lt;cell&gt;1206, 10μF&lt;/cell&gt;
        &lt;cell&gt;$0.18&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total&lt;/cell&gt;
        &lt;cell&gt;$47.62&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Battering RAM in Action&lt;/head&gt;
    &lt;head rend="h3"&gt;Questions and Answers&lt;/head&gt;
    &lt;p&gt;Battering RAM can affect all systems using DDR4 memory, but is especially relevant for "confidential computing" workloads running in public cloud environments.&lt;/p&gt;
    &lt;p&gt;Modern Intel and AMD x86 cloud processors feature built-in access control and memory encryption to keep private data safe, even from the company running the cloud. However, our research shows that these guarantees can be bypassed with a low-cost memory interposer, allowing a rogue cloud infrastructure provider or insider with limited physical access to compromise protected workloads.&lt;/p&gt;
    &lt;p&gt;Confidential computing aims to protect private data even from the cloud provider, using hardware-level access control and memory encryption. Even if someone accesses the memory, they should only see encrypted (garbled) data. Battering RAM uses a low-cost, custom-built memory interposer installed between the processor and memory to tamper with such encrypted memory. It requires only brief one-time physical access, which is realistic in cloud environments, considering, for instance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rogue cloud employees;&lt;/item&gt;
      &lt;item&gt;Datacenter technicians or cleaning personnel;&lt;/item&gt;
      &lt;item&gt;Coercive local law enforcement agencies;&lt;/item&gt;
      &lt;item&gt;Supply chain tampering during shipping or manufacturing of the memory modules.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Intel SGX and AMD SEV-SNP are two leading hardware-based trusted execution environments that enable secure cloud computations without needing to trust the cloud provider. They do this by enforcing strict access control and encrypting memory so that even if someone accesses it, they only see unreadable data.&lt;/p&gt;
    &lt;p&gt;AMD SEV and Intel SGX are widely offered by major cloud providers like like Amazon AWS, Google Cloud, Microsoft Azure, and IBM cloud. They also power privacy features in real-world applications like Signal, WhatsApp, and Chrome, and are used in sectors like healthcare to protect sensitive data.&lt;/p&gt;
    &lt;p&gt;No. While Intel Scalable SGX and AMD SEV-SNP use memory encryption to protect data stored in DRAM, this encryption is static: the same plaintext at the same physical address always maps to the same ciphertext. This defends against passive attacks, such as cold boot attacks, but not against Battering RAM, which can actively corrupt or replay memory contents. Because the encryption is static, replayed data decrypts to the original value, allowing stale data to be reused.&lt;/p&gt;
    &lt;p&gt;Furthermore, Intel's memory encryption engine for DDR4 systems, TME, relies on a single key for the entire memory range. This means encryption is static, not only per address, but also shared across both attacker and victim. By replaying and capturing ciphertexts from attacker-controlled pages, the attacker can recover and inject arbitrary plaintext within the victim’s memory.&lt;/p&gt;
    &lt;p&gt;Hence, Battering RAM exposes the fundamental limits of the scalable memory encryption designs currently used by Intel and AMD, which omit cryptographic freshness checks in favor of larger protected memory sizes.&lt;/p&gt;
    &lt;p&gt;BadRAM similarly exploited physical address aliasing to modify and replay encrypted memory on AMD SEV-SNP systems. However, BadRAM relied on modifying the SPD chip on the DIMM to report a false memory size at boot time, introducing static ghost address lines. In response, Intel and AMD added boot-time checks to detect and block such static aliases.&lt;/p&gt;
    &lt;p&gt;Battering RAM, on the other hand, is capable of introducing memory aliases dynamically at runtime. As a result, Battering RAM can circumvent Intel's and AMD's boot-time alias checks.&lt;/p&gt;
    &lt;p&gt;Concurrent to our work on Battering RAM, an independent research team developed the WireTap attack, which uses a commercial DDR4 DRAM interposer to break Intel Scalable SGX. Both Battering RAM and WireTap stem from a similar attack vector, but the approaches and findings are distinct.&lt;/p&gt;
    &lt;p&gt;The key differences between these two attacks are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cost: commercial DRAM interposers require specialized, high-speed signal analyzers (typically retailing at &amp;gt;$150,000), whereas our custom-built interposer requires only two simple analog switches and some control logic, totalling about $50. Battering RAM, therefore, shows that physical attacks are practical and not limited to resourceful adversaries with a large budget.&lt;/item&gt;
      &lt;item&gt;Technique: Battering RAM and WireTap exploit distinct techniques: memory aliasing vs. ciphertext side-channel analysis. Commercial DRAM interposers passively capture memory traffic, requiring additional ciphertext side-channel inference to recover secrets. In contrast, Battering RAM uses a custom-built interposer that actively redirects address lines to introduce aliases, allowing not just observation but also replay and corruption of ciphertext and culminating in plaintext read/write access on Scalable SGX.&lt;/item&gt;
      &lt;item&gt;Target: Both Battering RAM and WireTap expose the security limitations of current, scalable memory encryption technologies. Battering RAM breaks remote attestation for both Intel Scalable SGX and AMD SEV-SNP, whereas WireTap was only demonstrated on Intel Scalable SGX but may affect AMD DDR4 systems similarly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We found that our interposer can compromise the security of two widely-deployed TEEs, Intel Scalable SGX and AMD SEV-SNP. Both of these technologies employ a memory encryption scheme that is vulnerable to memory replay attacks. Furthermore, Scalable SGX on DDR4 platforms only employs a single memory encryption key for the entire physical memory space. We show this limitation can be exploited to create an arbitrary plaintext primitive. This severely undermines the protections in the presence of a physical adversary.&lt;/p&gt;
    &lt;p&gt;On top of that, our interposer re-enables the previously-mitigated BadRAM attacks. To combat this threat, AMD rolled out firmware-level mitigations that scan for aliases at boot time. As the interposer can enable and disable the interposer at runtime, these checks are easily bypassed. As a result, Battering RAM re-enables previous attacks on AMD SEV-SNP and Intel Client SGX .&lt;/p&gt;
    &lt;p&gt;Arm has also announced a cloud TEE called CCA . Based on the specification, DDR4 systems may also be vulnerable to Battering RAM. However, as no hardware is available yet, we were unable to test our interposer on CCA.&lt;/p&gt;
    &lt;p&gt;The table below summarizes our findings across different TEEs. Each column indicates whether we were able to read, write, or replay ciphertexts, and read/write plaintext in protected memory regions.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;TEE&lt;/cell&gt;
        &lt;cell role="head"&gt;Read&lt;/cell&gt;
        &lt;cell role="head"&gt;Write&lt;/cell&gt;
        &lt;cell role="head"&gt;Replay&lt;/cell&gt;
        &lt;cell role="head"&gt;Plaintext&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Intel Scalable SGX&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;AMD SEV-SNP&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Client SGX&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Intel TDX&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Arm CCA&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;No, our interposer only works on DDR4, which remains widely deployed today; e.g., a recent market study indicates that DDR4 still accounted for around 65% of sold DRAM modules in 2024.&lt;/p&gt;
    &lt;p&gt;DDR5 reorganizes the command/address bus, which removes the possibility of adding simple switches to the address lines. However, the underlying issue is not fixed, as current memory encryption engines still do not provide freshness guarantees. A determined attacker could theoretically still design more advanced interposers to perform similar attacks on DDR5.&lt;/p&gt;
    &lt;p&gt;Yes, our GitHub repository contains the hardware schematics and board files for the custom DDR4 interposer, firmware for the microcontroller, and proof-of-concept code for all attacks described in our paper. The interposer can be built for under $50, and the bill of materials is listed above.&lt;/p&gt;
    &lt;p&gt;We disclosed our findings to both Intel and AMD in February 2025. Both vendors have acknowledged our findings, but noted that physical attacks on DRAM are out of scope for their current products. To better reflect this position, Intel deposited the whitepaper on Scalable SGX, previously removed from the Intel website, permanently on arXiv.&lt;/p&gt;
    &lt;p&gt;Following an embargo period until September 30, 2025, both vendors have issued a public security advisory: Intel advisory | AMD advisory&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt; Confidential computing is here, but is not invincible. &lt;p&gt;Despite strong adoption by major CPU vendors and cloud providers, current technologies have critical physical-layer limitations that remain underexamined.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Reevaluate your threat models. &lt;p&gt;Encrypted memory is not inherently secure against physical tampering, and firmware-based mitigations alone are insufficient in threat scenarios involving limited physical access, such as malicious insiders or supply-chain compromises.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Advanced physical attacks are accessible at low cost. &lt;p&gt;Our open-source $50 custom device costs only a fraction of commercial DRAM interposers (upwards of $100,000) and is capable of breaking multi-million-dollar cloud security technologies from Intel and AMD.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45488713</guid><pubDate>Mon, 06 Oct 2025 07:47:17 +0000</pubDate></item><item><title>Nobel Prize in Physiology or Medicine 2025</title><link>https://www.nobelprize.org/prizes/medicine/2025/press-release/</link><description>&lt;doc fingerprint="9384913f192e615e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Press release&lt;/head&gt;
    &lt;p&gt;English&lt;lb/&gt;English (pdf)&lt;lb/&gt;Swedish&lt;lb/&gt;Swedish (pdf)&lt;/p&gt;
    &lt;p&gt;6 October 2025&lt;/p&gt;
    &lt;p&gt;The Nobel Assembly at Karolinska Institutet has decided to award the 2025 Nobel Prize in Physiology or Medicine to:&lt;/p&gt;
    &lt;p&gt;Mary E. Brunkow&lt;lb/&gt;Institute for Systems Biology,&lt;lb/&gt;Seattle, USA&lt;/p&gt;
    &lt;p&gt;Fred Ramsdell&lt;lb/&gt;Sonoma Biotherapeutics,&lt;lb/&gt;San Francisco, USA&lt;/p&gt;
    &lt;p&gt;Shimon Sakaguchi&lt;lb/&gt;Osaka University,&lt;lb/&gt;Osaka, Japan&lt;/p&gt;
    &lt;p&gt;“for their discoveries concerning peripheral immune tolerance”&lt;/p&gt;
    &lt;head rend="h2"&gt;They discovered how the immune system is kept in check&lt;/head&gt;
    &lt;p&gt;The body’s powerful immune system must be regulated, or it may attack our own organs. Mary E. Brunkow, Fred Ramsdell and Shimon Sakaguchi are awarded the Nobel Prize in Physiology or Medicine 2025 for their groundbreaking discoveries concerning peripheral immune tolerance that prevents the immune system from harming the body.&lt;/p&gt;
    &lt;p&gt;Every day, our immune system protects us from thousands of different microbes trying to invade our bodies. These all have different appearances, and many have developed similarities with human cells as a form of camouflage. So how does the immune system determine what it should attack and what it should defend?&lt;/p&gt;
    &lt;p&gt;Mary Brunkow, Fred Ramsdell and Shimon Sakaguchi are awarded the Nobel Prize in Physiology or Medicine 2025 for their fundamental discoveries relating to peripheral immune tolerance. The laureates identified the immune system’s security guards, regulatory T cells, which prevent immune cells from attacking our own body.&lt;/p&gt;
    &lt;p&gt;“Their discoveries have been decisive for our understanding of how the immune system functions and why we do not all develop serious autoimmune diseases,” says Olle Kämpe, chair of the Nobel Committee.&lt;/p&gt;
    &lt;p&gt;Shimon Sakaguchi was swimming against the tide in 1995, when he made the first key discovery. At the time, many researchers were convinced that immune tolerance only developed due to potentially harmful immune cells being eliminated in the thymus, through a process called central tolerance. Sakaguchi showed that the immune system is more complex and discovered a previously unknown class of immune cells, which protect the body from autoimmune diseases.&lt;/p&gt;
    &lt;p&gt;Mary Brunkow and Fred Ramsdell made the other key discovery in 2001, when they presented the explanation for why a specific mouse strain was particularly vulnerable to autoimmune diseases. They had discovered that the mice have a mutation in a gene that they named Foxp3. They also showed that mutations in the human equivalent of this gene cause a serious autoimmune disease, IPEX.&lt;/p&gt;
    &lt;p&gt;Two years after this, Shimon Sakaguchi was able to link these discoveries. He proved that the Foxp3 gene governs the development of the cells he identified in 1995. These cells, now known as regulatory T cells, monitor other immune cells and ensure that our immune system tolerates our own tissues.&lt;/p&gt;
    &lt;p&gt;The laureates’ discoveries launched the field of peripheral tolerance, spurring the development of medical treatments for cancer and autoimmune diseases. This may also lead to more successful transplantations. Several of these treatments are now undergoing clinical trials.&lt;/p&gt;
    &lt;head rend="h2"&gt;Illustrations&lt;/head&gt;
    &lt;p&gt;The illustrations are free to use for non-commercial purposes. Attribute “© The Nobel Committee for Physiology or Medicine. Ill. Mattias Karlén”&lt;/p&gt;
    &lt;p&gt;Illustration: The Nobel Prize in Physiology or Medicine 2025&lt;lb/&gt;Illustration: How T cells discover a virus&lt;lb/&gt;Illustration: How harmful T cells are eliminated&lt;lb/&gt;Illustration: The experiment that inspired Sakaguchi&lt;lb/&gt;Illustration: Sakaguchi defines a new class of T cells&lt;lb/&gt;Illustration: Brunkow and Ramsdell find the scurfy mutation&lt;lb/&gt;Illustration: How regulatory T cells protect us&lt;/p&gt;
    &lt;head rend="h2"&gt;Read more about this year’s prize&lt;/head&gt;
    &lt;p&gt;Popular science background: They understood how the immune system is kept in check (pdf)&lt;lb/&gt;Scientific background to the Nobel Prize in Physiology or Medicine 2025 (pdf)&lt;/p&gt;
    &lt;p&gt;Mary E. Brunkow, born 1961. Ph.D. from Princeton University, Princeton, USA. Senior Program Manager at the Institute for Systems Biology, Seattle, USA.&lt;/p&gt;
    &lt;p&gt;Fred Ramsdell, born 1960. Ph.D. 1987 from the University of California, Los Angeles, USA. Scientific Advisor, Sonoma Biotherapeutics, San Francisco, USA.&lt;/p&gt;
    &lt;p&gt;Shimon Sakaguchi, born 1951. M.D. 1976 and Ph.D. 1983 from Kyoto University, Japan. Distinguished Professor at the Immunology Frontier Research Center, Osaka University, Japan.&lt;/p&gt;
    &lt;p&gt;Prize amount: 11 million Swedish kronor, to be shared equally between the laureates.&lt;lb/&gt;Press contact: Pernilla Witte, +46 8 524 86 107, [email protected] or Thomas Perlmann, [email protected], Secretary-General, The Nobel Assembly at Karolinska Institutet.&lt;/p&gt;
    &lt;p&gt;Illustrations: © The Nobel Committee for Physiology or Medicine.&lt;/p&gt;
    &lt;p&gt;The Nobel Assembly, consisting of 50 professors at Karolinska Institutet, awards the Nobel Prize in Physiology or Medicine. Its Nobel Committee evaluates the nominations. Since 1901 the Nobel Prize has been awarded to scientists who have made the most important discoveries for the benefit of humankind.&lt;/p&gt;
    &lt;p&gt;Nobel Prize® is the registered trademark of the Nobel Foundation&lt;/p&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6–13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45489533</guid><pubDate>Mon, 06 Oct 2025 09:41:16 +0000</pubDate></item><item><title>Show HN: I've build a platform for writing technical/scientific documents</title><link>https://www.monsterwriter.com</link><description>&lt;doc fingerprint="e4a541928d31a8b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The best way to write a thesis or notes&lt;/head&gt;
    &lt;p&gt;Elevate your thesis effortlessly with the ultimate paper writing experience!&lt;/p&gt;
    &lt;p&gt;MonsterWriter assists students write exceptional academic papers by providing customized layouts that meet university requirements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Revolutionize Your Thesis Workflow&lt;/head&gt;
    &lt;p&gt;Say goodbye to thesis writing struggles and hello to unmatched productivity and success!&lt;/p&gt;
    &lt;p&gt;MonsterWriter has helped students like you turn in outstanding thesis papers, and now it's your turn to join them!&lt;/p&gt;
    &lt;head rend="h3"&gt;Focus on writing, not on formatting&lt;/head&gt;
    &lt;p&gt;By using the app, students can more effectively manage their time and avoid last-minute cramming&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Better time management&lt;/item&gt;
      &lt;item&gt;Improved quality of work&lt;/item&gt;
      &lt;item&gt;Easy access&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Add citations automatically&lt;/head&gt;
    &lt;p&gt;The details of your citations will be added automatically by MonsterWriter when you enter a website link, the ISBN, or the DOI code of a quote you recently used in your paper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Don't stress about citations styles&lt;/item&gt;
      &lt;item&gt;Beautifully organized References&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Write your way&lt;/head&gt;
    &lt;p&gt;Express yourself with rich text and content&lt;/p&gt;
    &lt;p&gt;MonsterWriter provides features for complex content. Equations, footnotes, bibliography, table of contents, captions, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pricing&lt;/head&gt;
    &lt;p&gt;Help support further development and unlock new features&lt;/p&gt;
    &lt;head rend="h3"&gt;Don't let thesis writing stress you out anymore.&lt;/head&gt;
    &lt;p&gt;Download MonsterWriter and get on the path to success!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45489999</guid><pubDate>Mon, 06 Oct 2025 10:58:14 +0000</pubDate></item><item><title>Modern messaging: Running your own XMPP server</title><link>https://www.codedge.de/posts/modern-messaging-running-your-own-xmpp-server</link><description>&lt;doc fingerprint="ddf8a45a3902bcc6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Modern messaging: Running your own XMPP server&lt;/head&gt;
    &lt;p&gt;Since a years we know, or might suspect, our chats are listend on, our uploaded files are sold for advertising or what purpose ever and the chance our social messengers leak our private data is incredibly high. It is about time to work against this.&lt;/p&gt;
    &lt;p&gt;Since 3 years the European Commission works on a plan to automatically monitor all chat, email and messenger conversations.12 If this is going to pass, and I strongly hope it will not, the European Union is moving into a direction we know from states suppressing freedom of speech.&lt;/p&gt;
    &lt;p&gt;I went for setting up my own XMPP server, as this does not have any big resource requirements and still support clustering (for high-availabilty purposes), encryption via OMEMO, file sharing and has support for platforms and operating systems. Also the ecosystem with clients and multiple use cases evolved over the years to provide rock-solid software and solutions for multi-user chats or event audio and video calls.&lt;/p&gt;
    &lt;p&gt;Info&lt;/p&gt;
    &lt;p&gt;All steps and settings are bundled in a repository containing Ansible roles: https://codeberg.org/codedge/chat&lt;/p&gt;
    &lt;p&gt;All code snippets written below work in either Debian os Raspberry Pi OS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setting up your own XMPP server&lt;/head&gt;
    &lt;p&gt;The connection from your client to the XMPP server is encrypted and we need certificates for our server. First thing to do is setting up our domains and point it to the IP - both IPv4 and IPv6 is supported and we can specify both later in our configuration.&lt;/p&gt;
    &lt;p&gt;I assume the server is going to be run under &lt;code&gt;xmpp.example.com&lt;/code&gt; and you all the following domains have been set up.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;xmpp.example.com&lt;/cell&gt;
        &lt;cell&gt;your main xmpp server address&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;conference.xmpp.example.com&lt;/cell&gt;
        &lt;cell&gt;needed for MUC (Multi User Chat)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;proxy.xmpp.example.com&lt;/cell&gt;
        &lt;cell&gt;needed for SOCKS5 proxy support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;pubsub.xmpp.example.com&lt;/cell&gt;
        &lt;cell&gt;needed for publish/subscribe support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;upload.xmpp.example.com&lt;/cell&gt;
        &lt;cell&gt;needed for file uploads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;stun.xmpp.example.com&lt;/cell&gt;
        &lt;cell&gt;needed for audio&amp;amp;video calling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;turn.xmpp.example.com&lt;/cell&gt;
        &lt;cell&gt;needed for audio&amp;amp;video calling&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fill in the IPv6 addresses accordingly.&lt;/p&gt;
    &lt;p&gt;ejabberd is a robust server software, that is included in most Linux distributions.&lt;/p&gt;
    &lt;p&gt;Install from Process One repository&lt;lb/&gt;I discovered ProcessOne, the company behind ejabberd, also provides a Debian repository.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Install from Github&lt;lb/&gt;To get the most recent one, I use the packages offered in their code repository. Installing version 25.07 just download the asset from the release:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Make sure the fowolling ports are opened in your firewall, taken from ejabberd firewall settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;5222: Jabber/XMPP client connections, plain or STARTTLS&lt;/item&gt;
      &lt;item&gt;5223: Jabber client connections, using the old SSL method&lt;/item&gt;
      &lt;item&gt;5269: Jabber/XMPP incoming server connections&lt;/item&gt;
      &lt;item&gt;5280/5443: HTTP/HTTPS for Web Admin and many more&lt;/item&gt;
      &lt;item&gt;7777: SOCKS5 file transfer proxy&lt;/item&gt;
      &lt;item&gt;3478/5349: STUN+TURN/STUNS+TURNS service&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Port &lt;code&gt;1883&lt;/code&gt;, used for MQTT, is also mentioned in the ejabberd docs, but we do not use this in our setup. So this port stays closed.&lt;/p&gt;
    &lt;p&gt;Depending how you installed ejabberd the config file is either at &lt;code&gt;/etc/ejabberd/conf/ejabberd.yml&lt;/code&gt;
or &lt;code&gt;/opt/ejabberd/conf/ejabberd.yml&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;General configuration&lt;/head&gt;
    &lt;p&gt;The configuration is a balance of 70:30 between having a privacy-focused setup for your users and meeting most of the suggestions of the XMPP complicance test. That means, settings that protect the provacy of the users are higher rated despite not passing the test.&lt;/p&gt;
    &lt;p&gt;Therefore notable privacy and security settings are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;XMPP over HTTP is disabled (mod_bosh)&lt;/item&gt;
      &lt;item&gt;Discover then a user last accessed a server is disabled (mod_last)&lt;/item&gt;
      &lt;item&gt;Delete uploaded files on a regular base (see upload config)&lt;/item&gt;
      &lt;item&gt;Register account via a web page is disabled (mod_register_web)&lt;/item&gt;
      &lt;item&gt;In-band registration can be enabled, default off, captcha secured (mod_register, see registration config)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Info&lt;/p&gt;
    &lt;p&gt;The configuration file is in YAML format. Keep an eye for indentation.&lt;/p&gt;
    &lt;p&gt;Let’s start digging into the configuration.&lt;/p&gt;
    &lt;p&gt;Set the domain of your server&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Set the database type&lt;lb/&gt;Instead of using the default &lt;code&gt;mnesia&lt;/code&gt; type, we opt for &lt;code&gt;sql&lt;/code&gt;, better said &lt;code&gt;sqlite&lt;/code&gt;.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Generate DH params&lt;lb/&gt;Generate a fresh set of params for the DH key exchange. In your terminal run&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;and link the new file in the ejabberd configuration.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Ensure TLS for server-to-server connections&lt;lb/&gt;Use TLS for server-to-server (s2s) connections.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The listners&lt;lb/&gt;The listeners aka &lt;code&gt;request_handlers&lt;/code&gt; inside the config especially for &lt;code&gt;/admin&lt;/code&gt;, &lt;code&gt;/captcha&lt;/code&gt;, &lt;code&gt;/upload&lt;/code&gt; and &lt;code&gt;/ws&lt;/code&gt; are important.
All of them listen on port &lt;code&gt;5443&lt;/code&gt;. Only one request handler is attached to port &lt;code&gt;5280&lt;/code&gt;, the &lt;code&gt;/.well-known/acme-challenge&lt;/code&gt;.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;ACLs &amp;amp; Access rules&lt;/head&gt;
    &lt;p&gt;For adminstration of ejabberd we need a user with admin rights and properly set up ACLs and access rules. There is a separat section for ACLs inside the config in which we set up an admin user name &lt;code&gt;root&lt;/code&gt;. The name of the user
is important for later, when we actually create this user.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The &lt;code&gt;access_rules&lt;/code&gt; should already be set up, just to confirm that you have a correct entry for the &lt;code&gt;configure&lt;/code&gt; action.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Now the new &lt;code&gt;root&lt;/code&gt; user needs to be create by running this command on the console.
Watch out to put in the correct domain.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Another user can be registered with the same command.&lt;lb/&gt;We set &lt;code&gt;root&lt;/code&gt; as the admin user in the config previously. That is how ejabberd knows which user has admin permissions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enable file uploads&lt;/head&gt;
    &lt;p&gt;Enabling file uploads is done with &lt;code&gt;mod_http_upload&lt;/code&gt;.
First, create a folder where the uploads should be stored.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Now update the ejabberd configuration like this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The allowed file upload size is defined in the &lt;code&gt;max_size&lt;/code&gt; param and is set to 10MB.&lt;/p&gt;
    &lt;p&gt;Make sure, to delete uploaded files in a reasonable amount of time via cronjob. This is an example of a cronjob, that deletes files that are older than 1 week.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Registration&lt;/head&gt;
    &lt;p&gt;Registration in ejabberd is done via &lt;code&gt;mod_register&lt;/code&gt;
and can be enabled with these entries in the config file:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;If you want to enable registration for your server make sure you enable a captcha for it. Otherwise you will get a lot of spam and fake registrations.&lt;/p&gt;
    &lt;p&gt;ejabberd provides a working captcha script, that you can copy to your server and link in your configuration. You will need &lt;code&gt;imaggemagick&lt;/code&gt; and &lt;code&gt;gstools&lt;/code&gt; installed
on you system. In the &lt;code&gt;ejabberd.yml&lt;/code&gt; config file&lt;/p&gt;
    &lt;head rend="h2"&gt;Add TLS&lt;/head&gt;
    &lt;p&gt;ejabberd can provision TLS certificates on its own. No need to install certbot. To not expose ejabberd directly to the internet, &lt;code&gt;nginx&lt;/code&gt; is put in front of the XMPP server. Instead of using nginx, every other web server (caddy, &amp;amp;mldr;)
or proxy can be used as well.&lt;/p&gt;
    &lt;p&gt;Here is a sample config for nginx:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Alternative connection methods&lt;/head&gt;
    &lt;p&gt;The nginx vhosts offers files, &lt;code&gt;host-meta&lt;/code&gt; and &lt;code&gt;host-meta.json&lt;/code&gt;, for indicating which other connection methods (BOSH, WS) your server offers. The details can be read in XEP-0156 extension.
Opposite to the examples in the XEP, there is no BOSH, but only a websocket connection our server offers. The BOSH part is removed from the config file.&lt;/p&gt;
    &lt;p&gt;host-meta&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;host-meta.json&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Put that file in a folder your nginx serves. Have a look at the path and URL it is expected to be, see &lt;code&gt;.well-known&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Choose your client&lt;/head&gt;
    &lt;p&gt;Clients I can recommend are Profanity, an easy to use command-line client, and Monal for MacOS and iOS. A good overview of client can be found on the offical XMPP website.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Citizen-led initiative collecting information about Chat Controle https://fightchatcontrol.eu ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explanation by Patrick Breyer, former member of the European Parliament https://www.patrick-breyer.de/en/posts/chat-control/ ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45490439</guid><pubDate>Mon, 06 Oct 2025 12:02:45 +0000</pubDate></item><item><title>Show HN: Kent Dybvig's Scheme Machine in 400 Lines of C (Heap-Memory Model)</title><link>https://gist.github.com/swatson555/8cc36d8d022d7e5cc44a5edb2c4f7d0b</link><description>&lt;doc fingerprint="c5d093b93a485002"&gt;
  &lt;main&gt;
    &lt;p&gt; Created &lt;relative-time&gt;February 17, 2023 12:42&lt;/relative-time&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;tool-tip&gt;Save swatson555/8cc36d8d022d7e5cc44a5edb2c4f7d0b to your computer and use it in GitHub Desktop.&lt;/tool-tip&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Heap based scheme machine. &lt;/p&gt;
    &lt;p&gt; This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;/* Heap based virtual machine described in section 3.4 of Three Implementation Models for Scheme, Dybvig&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;*/&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;stdio.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;stdlib.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;string.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;ctype.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;#include &amp;lt;assert.h&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char token[128][32];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int lexer(char* input) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int ii = 0; // input index&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int ti = 0; // token index&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while(input[ii] != '\0')&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;switch(input[ii]) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Ignore whitespace and newlines&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case ' ':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case '\n':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ii;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Turn a left parenthesis into a token.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case '(':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][0] = '(';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][1] = '\0';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ii;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Turn a right parenthesis into a token.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case ')':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][0] = ')';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][1] = '\0';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ii;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Turn an apostrophe into a token.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;case '\'':&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][0] = '\'';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][1] = '\0';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ii;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Anything else is a symbol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;default:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;for(int i = 0;; ++i) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if(input[ii] != ' ' &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;input[ii] != ')' &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;input[ii] != '(' &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;input[ii] != '\n' &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;input[ii] != '\0') {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][i] = input[ii++];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;token[ti][i] = '\0';&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;++ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;break;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return ti;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int curtok;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* nexttok() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return token[curtok++];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* peektok() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return token[curtok];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;typedef struct Pair {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;} Pair;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;typedef struct Text {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;struct Text* cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;} Text;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair text[1280];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* textptr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int istext(void* x) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return x &amp;gt;= (void*)&amp;amp;text &amp;amp;&amp;amp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;x &amp;lt; (void*)&amp;amp;text[1280];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* cons(void* x, void* y) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;assert(istext(textptr));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;textptr-&amp;gt;car = x;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;textptr-&amp;gt;cdr = y;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return textptr++;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read(char* ln);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read_exp();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read_list();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read(char* ln) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// Initialize the lexer and list memory.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;curtok = 0;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;textptr = text;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;lexer(ln);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return read_exp();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read_exp() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* tok = nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (tok[0] == '(' &amp;amp;&amp;amp; peektok()[0] == ')') {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return NULL;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (tok[0] == '\'')&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("quote", cons(read_exp(), NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (tok[0] == '(')&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return read_list();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return tok;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* read_list() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char* tok = peektok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if(tok[0] == ')') {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return NULL;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if(tok[0] == '.') {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tok = read_exp();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;nexttok();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return tok;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* fst = read_exp();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* snd = read_list();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons(fst, snd);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print(void* exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_exp(void* exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_list(Pair* list);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_cons(Pair* pair);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print(void* exp) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("\n");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_exp(void* exp) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (istext(exp)) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* pair = exp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if(!istext(pair-&amp;gt;cdr) &amp;amp;&amp;amp; pair-&amp;gt;cdr != NULL) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("(");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_cons(exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(")");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("(");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_list(exp);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("%s", exp ? (char*)exp : "()");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_list(Pair* list) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (list-&amp;gt;cdr == NULL) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(list-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(")");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if(!istext(list-&amp;gt;cdr) &amp;amp;&amp;amp; list-&amp;gt;cdr != NULL) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_cons(list);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(")");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(list-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(" ");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_list(list-&amp;gt;cdr);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void print_cons(Pair* pair) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(pair-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf(" . ");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print_exp(pair-&amp;gt;cdr);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* compile(void* exp, void* next) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (istext(exp)) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* p = exp;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(p-&amp;gt;car, "quote") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("constant", cons(p-&amp;gt;cdr-&amp;gt;car, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(p-&amp;gt;car, "lambda") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("close", cons(p-&amp;gt;cdr-&amp;gt;car, cons(compile(p-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car, cons("return", NULL)), cons(next, NULL))));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(p-&amp;gt;car, "if") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return compile(p-&amp;gt;cdr-&amp;gt;car, cons("test", cons(compile(p-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car, next),&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;cons(compile(p-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car, next),&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NULL))));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(p-&amp;gt;car, "set!") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return compile(p-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car, cons("assign", cons(p-&amp;gt;cdr-&amp;gt;car, cons(next, NULL))));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(p-&amp;gt;car, "call/cc") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* c = cons("conti", cons(cons("argument", cons(compile(p-&amp;gt;cdr-&amp;gt;car, cons("apply", NULL)), NULL)), NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* n = next;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(n-&amp;gt;car, "return") == 0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return c;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("frame", cons(next, cons(c, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* args = (Pair*)p-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* c = compile(p-&amp;gt;car, cons("apply", NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while (args) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;c = compile(args-&amp;gt;car, cons("argument", cons(c, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;args = args-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* n = next;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(n-&amp;gt;car, "return") == 0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return c;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("frame", cons(next, cons(c, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if(isdigit(*((char*)exp))) { // a number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("constant", cons(exp, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if(strcmp(exp, "#t") == 0) { // a boolean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("constant", cons(exp, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if(strcmp(exp, "#f") == 0) { // a boolean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("constant", cons(exp, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else { // a symbol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons("refer", cons(exp, cons(next, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* get(void* env, char* var) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* e = env;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while(env) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* cur = e-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* vars = cur-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pair* vals = cur-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while (vars &amp;amp;&amp;amp; vals) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(vars-&amp;gt;car, var) == 0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return vals-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;vars = vars-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;vals = vals-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;e = e-&amp;gt;cdr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fprintf(stderr, "No definition in environment for %s.\n", var);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;assert(0);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void set(void* env, char* var, char* val) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* ref = get(env, var);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ref = val;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* extend(void* env, void* vars, void* vals) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons(cons(vars, vals), env);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* callframe(void* next, void* env, void* rib, void* stack) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons(next, cons(env, cons(rib, cons(stack, NULL))));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* closure(void* body, void* env, void* vars) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return cons(body, cons(env, cons(vars, NULL)));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* continuation(void* stack) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return closure(cons("nuate", cons(stack, cons("v", NULL))), NULL, cons("v", NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* accum;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* next;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* env;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* rib;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* stack;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void virtmach() {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* n = next;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;if (strcmp(n-&amp;gt;car, "halt") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "refer") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = get(env, n-&amp;gt;cdr-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "constant") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "close") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* vars = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* body = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* x = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = closure(body, env, vars);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = x;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "test") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* consequent = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* alternate = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = strcmp(accum, "#f") == 0 ? alternate : consequent;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "assign") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;set(env, n-&amp;gt;cdr-&amp;gt;car, accum);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "conti") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = continuation(stack);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "nuate") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stack = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;accum = get(env, n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = cons("return", NULL);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "frame") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stack = callframe(n-&amp;gt;cdr-&amp;gt;car, env, rib, stack);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rib = NULL;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "argument") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rib = cons(accum, rib);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = n-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "apply") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* a = accum;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* body = a-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* clos = a-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;void* vars = a-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;env = extend(env, vars, rib);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rib = NULL;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = body;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else if (strcmp(n-&amp;gt;car, "return") == 0) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text* s = stack;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = s-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;env = s-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rib = s-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stack = s-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;cdr-&amp;gt;car;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;else {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;fprintf(stderr, "Unhandled operation.\n");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;assert(0);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;int main(int argc, char** argv) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;// note! repl implies there's a top-level but there isn't...&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("Lisp REPL\n\n");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("&amp;gt;&amp;gt; ");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;char buffer[256];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;while (fgets(buffer, 256, stdin)) {&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;next = compile(read(buffer), cons("halt", NULL));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;virtmach();&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;print(accum);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;printf("&amp;gt;&amp;gt; ");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;return 0;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;}&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45491609</guid><pubDate>Mon, 06 Oct 2025 14:06:29 +0000</pubDate></item><item><title>Mise: Monorepo Tasks</title><link>https://github.com/jdx/mise/discussions/6564</link><description>&lt;doc fingerprint="7c542d728fd7a4ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Monorepo Tasks #6564&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;We're excited to announce Monorepo Tasks, a powerful new feature that brings first-class monorepo support to mise tasks! 🚀&lt;/p&gt;
          &lt;head&gt;What is it?&lt;/head&gt;
          &lt;p&gt;Monorepo Tasks allows you to manage tasks across multiple projects in a single repository, with each project maintaining its own tools, environment variables, and tasks. Think of it as bringing the power of tools like Bazel or Turborepo to mise's task system, but with mise's signature simplicity.&lt;/p&gt;
          &lt;head&gt;Key Features&lt;/head&gt;
          &lt;head&gt;🎯 Unified Task Namespace&lt;/head&gt;
          &lt;p&gt;All tasks across your monorepo are automatically discovered and prefixed with their location:&lt;/p&gt;
          &lt;code&gt;mise //projects/frontend:build
mise //projects/backend:test
mise //services/api:deploy&lt;/code&gt;
          &lt;head&gt;🌳 Smart Tool &amp;amp; Environment Inheritance&lt;/head&gt;
          &lt;p&gt;Define common tools at the root, override them where needed:&lt;/p&gt;
          &lt;code&gt;# Root mise.toml
[tools]
node = "20"      # Inherited everywhere
python = "3.12"

# projects/legacy-app/mise.toml
[tools]
node = "14"      # Override just for this project
# python still inherited!&lt;/code&gt;
          &lt;head&gt;🎭 Powerful Wildcard Patterns&lt;/head&gt;
          &lt;p&gt;Run tasks across multiple projects with ease:&lt;/p&gt;
          &lt;code&gt;# Run tests in ALL projects
mise //...:test

# Run all build tasks under services/
mise //services/...:build

# Run ALL tasks in frontend (wildcards need quotes)
mise '//projects/frontend:*'

# Run all test:* tasks everywhere
mise '//...:test:*'&lt;/code&gt;
          &lt;head&gt;✨ Consistent Execution Anywhere&lt;/head&gt;
          &lt;p&gt;Run tasks from anywhere in the monorepo - they always execute with the correct context, tools, and environment from their config_root.&lt;/p&gt;
          &lt;head&gt;🔒 Automatic Trust Propagation&lt;/head&gt;
          &lt;p&gt;Trust your monorepo root once, and all descendant configs are automatically trusted.&lt;/p&gt;
          &lt;head&gt;Quick Start&lt;/head&gt;
          &lt;p&gt;1. Enable the feature in your root &lt;/p&gt;
          &lt;code&gt;experimental_monorepo_root = true

[tools]
node = "20"
python = "3.12"&lt;/code&gt;
          &lt;p&gt;2. Set the experimental flag:&lt;/p&gt;
          &lt;code&gt;export MISE_EXPERIMENTAL=1&lt;/code&gt;
          &lt;p&gt;3. Add tasks to your projects:&lt;/p&gt;
          &lt;code&gt;# projects/frontend/mise.toml
[tasks.build]
run = "npm run build"

[tasks.test]
run = "npm test"&lt;/code&gt;
          &lt;p&gt;4. Run tasks from anywhere:&lt;/p&gt;
          &lt;code&gt;mise //projects/frontend:build
mise //...:test  # Run tests in all projects!&lt;/code&gt;
          &lt;head&gt;Example Monorepo Structure&lt;/head&gt;
          &lt;p&gt;Run all service builds: &lt;/p&gt;
          &lt;head&gt;Why This Matters&lt;/head&gt;
          &lt;p&gt;Managing monorepos is hard. Coordinating tools, tasks, and environments across dozens of projects is even harder. With Monorepo Tasks, you get:&lt;/p&gt;
          &lt;head&gt;How Does This Compare to Other Tools?&lt;/head&gt;
          &lt;p&gt;The monorepo ecosystem is rich with excellent tools, each with different strengths. Here's how mise's Monorepo Tasks compares:&lt;/p&gt;
          &lt;head&gt;Simple Task Runners&lt;/head&gt;
          &lt;p&gt;Taskfile and Just are fantastic for single-project task automation. They're lightweight and easy to set up, but they weren't designed with monorepos in mind. While you can have multiple Taskfiles/Justfiles in a repo, they don't provide unified task discovery, cross-project wildcards, or automatic tool/environment inheritance across projects.&lt;/p&gt;
          &lt;p&gt;mise's advantage: Automatic task discovery across the entire monorepo with a unified namespace and powerful wildcard patterns.&lt;/p&gt;
          &lt;head&gt;JavaScript-Focused Tools&lt;/head&gt;
          &lt;p&gt;Nx, Turborepo, and Lerna are powerful tools specifically designed for JavaScript/TypeScript monorepos.&lt;/p&gt;
          &lt;p&gt;mise's advantage: Language-agnostic support. While these tools excel in JS/TS ecosystems, mise works equally well with Rust, Go, Python, Ruby, or any mix of languages. You also get unified tool version management (not just tasks) and environment variables across your entire stack.&lt;/p&gt;
          &lt;head&gt;Large-Scale Build Systems&lt;/head&gt;
          &lt;p&gt;Bazel (Google) and Buck2 (Meta) are industrial-strength build systems designed for massive, multi-language monorepos at companies with thousands of engineers.&lt;/p&gt;
          &lt;p&gt;Both are extremely powerful but come with significant complexity:&lt;/p&gt;
          &lt;p&gt;mise's advantage: Simplicity through non-hermetic builds. mise doesn't try to control your entire build environment in isolation - instead, it manages tools and tasks in a flexible, practical way. This "non-hermetic" approach means you can use mise without restructuring your entire codebase or learning a new language. You get powerful monorepo task management with simple TOML configuration - enough power for most teams without the enterprise-level complexity that hermetic builds require.&lt;/p&gt;
          &lt;head&gt;Other Notable Tools&lt;/head&gt;
          &lt;p&gt;Rush (Microsoft) offers strict dependency management and build orchestration for JavaScript monorepos, with a focus on safety and convention adherence.&lt;/p&gt;
          &lt;p&gt;Moon is a newer Rust-based build system that aims to be developer-friendly while supporting multiple languages.&lt;/p&gt;
          &lt;head&gt;The mise Sweet Spot&lt;/head&gt;
          &lt;p&gt;mise's Monorepo Tasks aims to hit the sweet spot between simplicity and power:&lt;/p&gt;
          &lt;p&gt;When to choose mise:&lt;/p&gt;
          &lt;p&gt;When to consider alternatives:&lt;/p&gt;
          &lt;p&gt;The best tool is the one that fits your team's needs. mise's Monorepo Tasks is designed for teams who want powerful monorepo management without the complexity overhead, especially when working across multiple languages.&lt;/p&gt;
          &lt;head&gt;Try It Out!&lt;/head&gt;
          &lt;p&gt;This feature is experimental, which means:&lt;/p&gt;
          &lt;p&gt;Read the full documentation: Monorepo Tasks Guide&lt;/p&gt;
          &lt;head&gt;We Want Your Feedback!&lt;/head&gt;
          &lt;p&gt;Please try it out and let us know:&lt;/p&gt;
          &lt;p&gt;Share your experience in the comments below! 👇&lt;/p&gt;
          &lt;p&gt;Special shoutout to the mise community for the feedback and ideas that led to this feature. Happy building! 🛠️&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 2 comments 7 replies&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Does this support &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Excited to see this! We're currently using turbo in a mixed Rust/wasm/TS/Python/Go repo, and it's been a bit of a mixed bag (admittedly, I don't know how much of that is because we're unwilling to invest effort into modelling task inputs/outputs correctly in turbo).&lt;/p&gt;
          &lt;p&gt;Compounding the issue is that what we really want a whole bunch of things out of it:&lt;/p&gt;
          &lt;p&gt;Absent these, I don't really see us adopting this anytime soon unfortunately.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45491621</guid><pubDate>Mon, 06 Oct 2025 14:07:46 +0000</pubDate></item><item><title>Launch HN: Grapevine (YC S19) – A company GPT that actually works</title><link>https://getgrapevine.ai/</link><description>&lt;doc fingerprint="a74e481eafb1f0bb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Stop wasting time searching.&lt;/head&gt;
    &lt;head rend="h1"&gt;Stop wasting time searching.&lt;/head&gt;
    &lt;head rend="h1"&gt;Stop wasting time searching.&lt;/head&gt;
    &lt;p&gt;One AI agent that searches across your docs, code, and communicationâso you donât have to.&lt;/p&gt;
    &lt;p&gt;Thread&lt;/p&gt;
    &lt;p&gt;Aleks&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive ðªðª&lt;/p&gt;
    &lt;p&gt;1 reply&lt;/p&gt;
    &lt;p&gt;Grapevine&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I can help with that, and you can get started for free!&lt;/p&gt;
    &lt;p&gt;Thread&lt;/p&gt;
    &lt;p&gt;Aleks&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive ðªðª&lt;/p&gt;
    &lt;p&gt;1 reply&lt;/p&gt;
    &lt;p&gt;Grapevine&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I can help with that, and you can get started for free!&lt;/p&gt;
    &lt;p&gt;Thread&lt;/p&gt;
    &lt;p&gt;Aleks&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I wish there were a company GPT that ACTUALLY worked + wasn't exorbitantly expensive ðªðª&lt;/p&gt;
    &lt;p&gt;1 reply&lt;/p&gt;
    &lt;p&gt;Grapevine&lt;/p&gt;
    &lt;p&gt;12:12 PM&lt;/p&gt;
    &lt;p&gt;I can help with that, and you can get started for free!&lt;/p&gt;
    &lt;head rend="h2"&gt;What if ChatGPT was aware of my company's context?&lt;/head&gt;
    &lt;head rend="h3"&gt;We've all wondered it at some point.&lt;/head&gt;
    &lt;head rend="h3"&gt;What if AI already understood your companyâso you could skip the busywork, the repetitive asks, the frustration?&lt;/head&gt;
    &lt;head rend="h3"&gt;It could take care of the many chores that exist in work today, making our days a little less annoying and little more fun.&lt;/head&gt;
    &lt;head rend="h3"&gt;Other products we've tried haven't quite worked. Some of us have tried to build it ourselves.&lt;/head&gt;
    &lt;head rend="h3"&gt;That's why we built Grapevine. And it finally works.&lt;/head&gt;
    &lt;p&gt;#team-infra&lt;/p&gt;
    &lt;p&gt;For commonly asked cross-team questions&lt;/p&gt;
    &lt;p&gt;Johnny&lt;/p&gt;
    &lt;p&gt;Jul 28th at 4:23 PM&lt;/p&gt;
    &lt;p&gt;Hey Infra team, Iâd like to create a new S3 bucket...&lt;/p&gt;
    &lt;p&gt;Actual Slack thread&lt;/p&gt;
    &lt;p&gt;3 sources&lt;/p&gt;
    &lt;p&gt;Real examples of us using Grapevine internally, over the last 2 months&lt;/p&gt;
    &lt;p&gt;&amp;gt;85%&lt;/p&gt;
    &lt;p&gt;&amp;gt;85%&lt;/p&gt;
    &lt;p&gt;&amp;gt;85%&lt;/p&gt;
    &lt;p&gt;of answers are helpful &amp;amp; accurate&lt;/p&gt;
    &lt;p&gt;of answers are helpful &amp;amp; accurate&lt;/p&gt;
    &lt;p&gt;*from hundreds of real questions from beta customers&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started in under 30 minutes&lt;/head&gt;
    &lt;p&gt;10 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Connect your data and setup Slack bot&lt;/head&gt;
    &lt;p&gt;30 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Start asking questions&lt;/head&gt;
    &lt;p&gt;2 days&lt;/p&gt;
    &lt;head rend="h6"&gt;Tackle queries with full historical context&lt;/head&gt;
    &lt;p&gt;Over time&lt;/p&gt;
    &lt;head rend="h6"&gt;Grapevine keeps learning and getting sharper&lt;/head&gt;
    &lt;head rend="h2"&gt;Get started in under 30 minutes&lt;/head&gt;
    &lt;p&gt;10 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Connect your data and setup Slack bot&lt;/head&gt;
    &lt;p&gt;30 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Start asking questions&lt;/head&gt;
    &lt;p&gt;2 days&lt;/p&gt;
    &lt;head rend="h6"&gt;Tackle queries with full historical context&lt;/head&gt;
    &lt;p&gt;Over time&lt;/p&gt;
    &lt;head rend="h6"&gt;Grapevine keeps learning and getting sharper&lt;/head&gt;
    &lt;head rend="h2"&gt;Get started in under 30 minutes&lt;/head&gt;
    &lt;p&gt;10 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Connect your data and setup Slack bot&lt;/head&gt;
    &lt;p&gt;30 min&lt;/p&gt;
    &lt;head rend="h6"&gt;Start asking questions&lt;/head&gt;
    &lt;p&gt;2 days&lt;/p&gt;
    &lt;head rend="h6"&gt;Tackle queries with full historical context&lt;/head&gt;
    &lt;p&gt;Over time&lt;/p&gt;
    &lt;head rend="h6"&gt;Grapevine keeps learning and getting sharper&lt;/head&gt;
    &lt;head rend="h3"&gt;Always Secure&lt;/head&gt;
    &lt;head rend="h6"&gt;Encrypted at rest&lt;/head&gt;
    &lt;p&gt;Data is encrypted using industry-standard AES-256, protecting your companyâs knowledge even when itâs not in use.&lt;/p&gt;
    &lt;head rend="h6"&gt;Encrypted at rest&lt;/head&gt;
    &lt;p&gt;Data is encrypted using industry-standard AES-256, protecting your companyâs knowledge even when itâs not in use.&lt;/p&gt;
    &lt;head rend="h6"&gt;Encrypted at rest&lt;/head&gt;
    &lt;p&gt;All data is encrypted using industry-standard AES-256&lt;/p&gt;
    &lt;head rend="h6"&gt;Isolated databases&lt;/head&gt;
    &lt;p&gt;Every customerâs data is siloed and stored separately to ensure maximum privacy and security.&lt;/p&gt;
    &lt;head rend="h6"&gt;Isolated databases&lt;/head&gt;
    &lt;p&gt;Every customerâs data is siloed and stored separately to ensure maximum privacy and security.&lt;/p&gt;
    &lt;head rend="h6"&gt;Isolated databases&lt;/head&gt;
    &lt;p&gt;Every customerâs data is siloed and stored separately&lt;/p&gt;
    &lt;head rend="h6"&gt;SOC II Compliant&lt;/head&gt;
    &lt;p&gt;Built to Gather's SOC II Type 2 standards, with regularly scheduled security audits and more details upon request.&lt;/p&gt;
    &lt;head rend="h6"&gt;SOC II Compliant&lt;/head&gt;
    &lt;p&gt;Built to Gather's SOC II Type 2 standards, with regularly scheduled security audits and more details upon request.&lt;/p&gt;
    &lt;head rend="h6"&gt;SOC II Compliant&lt;/head&gt;
    &lt;p&gt;Every customerâs data is siloed and stored separately&lt;/p&gt;
    &lt;p&gt;Grapevine will not train models on your data&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started today.&lt;/head&gt;
    &lt;head rend="h2"&gt;What if ChatGPT was aware of my company's context?&lt;/head&gt;
    &lt;p&gt;We've all wondered this at some point. And we finally built a version of this that works. But don't take our word for itâtry it today!&lt;/p&gt;
    &lt;p&gt;#team-infra&lt;/p&gt;
    &lt;p&gt;For commonly asked cross-team questions&lt;/p&gt;
    &lt;p&gt;Johnny&lt;/p&gt;
    &lt;p&gt;Jul 28th at 4:23 PM&lt;/p&gt;
    &lt;p&gt;Hey Infra team, Iâd like to create a new S3 bucket...&lt;/p&gt;
    &lt;p&gt;Actual Slack thread&lt;/p&gt;
    &lt;p&gt;3 sources&lt;/p&gt;
    &lt;p&gt;Real examples of us using Grapevine internally, over the last 2 months&lt;/p&gt;
    &lt;p&gt;#team-infra&lt;/p&gt;
    &lt;p&gt;For commonly asked cross-team questions&lt;/p&gt;
    &lt;p&gt;Johnny&lt;/p&gt;
    &lt;p&gt;Jul 28th at 4:23 PM&lt;/p&gt;
    &lt;p&gt;Hey Infra team, Iâd like to create a new S3 bucket...&lt;/p&gt;
    &lt;p&gt;Actual Slack thread&lt;/p&gt;
    &lt;p&gt;3 sources&lt;/p&gt;
    &lt;p&gt;Real examples of us using Grapevine internally, over the last 2 months&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45492564</guid><pubDate>Mon, 06 Oct 2025 15:39:59 +0000</pubDate></item><item><title>OpenZL: An open source format-aware compression framework</title><link>https://engineering.fb.com/2025/10/06/developer-tools/openzl-open-source-format-aware-compression-framework/</link><description>&lt;doc fingerprint="b69d42b82801cb99"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenZL is a new open source data compression framework that offers lossless compression for structured data.&lt;/item&gt;
      &lt;item&gt;OpenZL is designed to offer the performance of a format-specific compressor with the easy maintenance of a single executable binary.&lt;/item&gt;
      &lt;item&gt;You can get started with OpenZL today by visiting our Quick Start guide and the OpenZL GitHub repository.&lt;/item&gt;
      &lt;item&gt;Learn more about the theory behind OpenZL in this whitepaper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Today, we are excited to announce the public release of OpenZL, a new data compression framework. OpenZL offers lossless compression for structured data, with performance comparable to specialized compressors. It accomplishes this by applying a configurable sequence of transforms to the input, revealing hidden order in the data, which can then be more easily compressed. Despite applying distinct transformation permutations for every file type, all OpenZL files can be decompressed using the same universal OpenZL decompressor.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Decade of Lessons&lt;/head&gt;
    &lt;p&gt;When Zstandard was announced, it came with a simple pitch: It promised the same or better compression ratio of prior default but at the much increased speed required by datacenter workloads. By pairing strong entropy coding with a design that fully utilized modern CPU capabilities, Zstandard offered a substantial improvement that justified its presence in datacenters.&lt;/p&gt;
    &lt;p&gt;However, while it was improved over time, remaining within the Zstandard framework offers diminishing returns. So we started looking for the next great leap in data compression.&lt;/p&gt;
    &lt;p&gt;In this quest, one pattern kept repeating: Using generic methods on structured data leaves compression gains on the table. Data isn’t just byte soup. It can be columnar, encode enums, be restricted to specific ranges, or carry highly repetitive fields. More importantly, it has predictable shapes. A bespoke compressor that leans into that structure can beat general-purpose tools on both ratio and speed. But there’s a catch — every bespoke scheme means another compressor and decompressor to create, ship, audit, patch, and trust.&lt;/p&gt;
    &lt;p&gt;OpenZL is our answer to the tension between the performance of format-specific compressors and the maintenance simplicity of a single executable binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Make the Structure Explicit&lt;/head&gt;
    &lt;p&gt;General compressors rely on a one-size fits all processing strategy, or alternatively spend a lot of their cycles guessing which techniques to use. OpenZL saves those cycles by making the structure an explicit input parameter. Compression can then focus on a sequence of reversible steps that surface patterns before coding.&lt;/p&gt;
    &lt;p&gt;As a user, you provide OpenZL with the data shape (via a preset or a thin format description). Then the trainer, an offline optimization component, builds an effective compression config that can be re-employed for similar data. During encoding that config resolves into a concrete decode recipe that’s embedded into the frame. The universal decoder will directly execute that recipe, without any out-of-band information.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Example Compression Using OpenZL&lt;/head&gt;
    &lt;p&gt;As an example, let’s compress sao, which is part of the Silesia Compression Corpus. This file follows a well-defined format featuring an array of records, each one describing a star. Providing this information to OpenZL is enough to give it an edge over generic lossless compressors, which only see bytes.&lt;/p&gt;
    &lt;p&gt;Comparison on a M1 cpu, using clang-17&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compressor&lt;/cell&gt;
        &lt;cell&gt;zstd -3&lt;/cell&gt;
        &lt;cell&gt;xz -9&lt;/cell&gt;
        &lt;cell&gt;OpenZL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compressed Size&lt;/cell&gt;
        &lt;cell&gt;5,531,935 B&lt;/cell&gt;
        &lt;cell&gt;4,414,351 B&lt;/cell&gt;
        &lt;cell&gt;3,516,649 B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compression Ratio&lt;/cell&gt;
        &lt;cell&gt;x1.31&lt;/cell&gt;
        &lt;cell&gt;x1.64&lt;/cell&gt;
        &lt;cell&gt;x2.06&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compression Speed&lt;/cell&gt;
        &lt;cell&gt;220 MB/s&lt;/cell&gt;
        &lt;cell&gt;3.5 MB/s&lt;/cell&gt;
        &lt;cell&gt;340 MB/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Decompression Speed&lt;/cell&gt;
        &lt;cell&gt;850 MB/s&lt;/cell&gt;
        &lt;cell&gt;45 MB/s&lt;/cell&gt;
        &lt;cell&gt;1200 MB/s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Crucially, OpenZL produces a higher compression ratio while preserving or even improving speed, which is critical for data center processing pipelines.&lt;/p&gt;
    &lt;p&gt;For illustration, this result is achieved using the following simple graph:&lt;/p&gt;
    &lt;head rend="h3"&gt;A Brief Explanation&lt;/head&gt;
    &lt;p&gt;So what is happening in this example?&lt;/p&gt;
    &lt;p&gt;We start by separating the header from the rest, a large table of structures. Then each field gets extracted into its own stream: the array of structures becomes a structure of arrays. After that point, we expect that each stream contains homogeneous data of the same type and semantic meaning. We can now focus on finding an optimal compression strategy for each one.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SRA0 is a position on the X axis. Due to the way the table is generated, the index is mostly sorted, inviting the use of delta to reduce the range of values represented. This mechanically makes the resulting stream easier to compress.&lt;/item&gt;
      &lt;item&gt;SDEC0 is a position on the Y axis. It’s not as well sorted as the X axis, but we can at least exploit the fact that it’s bounded between a minimum and a maximum. This makes the higher bytes more predictable, which can be exploited for better compression with the transpose operation.&lt;/item&gt;
      &lt;item&gt;The other fields (IS, MAG, XRPM, XDPM) share a common property: their cardinality is much lower than their quantities, and there is no relation between 2 consecutive values. This makes them a good target for tokenize, which will convert the stream into a dictionary and an index list.&lt;/item&gt;
      &lt;item&gt;The resulting dictionaries and index lists are very different. They benefit from completely different compression strategies. So they are sent to dedicated processing graphs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The graph continues beyond these steps. But at some point, we can also stop making decisions. The main work is to group data into homogeneous streams. After that, one can count on openzl to take care of the rest.&lt;/p&gt;
    &lt;p&gt;To go even further, we would like to generate compression strategies that are specifically fine-tuned for each stream. This is where the offline trainer stage comes into play.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generate a Compressor Automatically&lt;/head&gt;
    &lt;p&gt;It’s possible to take full control of the compression process, but it’s also not required. A faster strategy is to just describe your data and let the system learn a compression config.&lt;/p&gt;
    &lt;p&gt;Describe the input: With the Simple Data Description Language (SDDL), you sketch how the bytes map to fields — rows, columns, enums, nested records. SDDL is for parsing only; it just tells OpenZL the shape of your data. Alternatively, you can write your own parser function directly using one of the supported languages, and register it with OpenZL to delegate the logic.&lt;/p&gt;
    &lt;p&gt;Learn the config: Starting from a preset, a parser function or an SDDL description, the trainer runs a budgeted search over transform choices and parameters to produce a Plan. It can provide a full set of speed/ratio tradeoffs, or directly target the best configuration respecting some speed constraints. Internally it uses a cluster finder (to group fields that behave alike) and a graph explorer (to try candidate subgraphs and keep score).&lt;/p&gt;
    &lt;p&gt;Resolve at encode-time: While compressing, the encoder turns the Plan into a concrete recipe — the Resolved Graph. If the Plan has control points, it picks the branch that fits the data and records that choice into the frame.&lt;/p&gt;
    &lt;p&gt;Decode without coordination: Each frame chunk carries its own resolved graph. The single decoder checks it, enforces limits, and runs the steps in order. When a plan improves, you just roll out the new plan, no new decompressor needed. Old data keeps decoding; new data get improved gains.&lt;/p&gt;
    &lt;p&gt;In practice the loop is straightforward: describe (SDDL) → train (produce a plan) → compress (emit frames with resolved graphs) → decode anywhere with the same binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Embracing Changes: Re-Training and In-Flight Control&lt;/head&gt;
    &lt;p&gt;In the real world, data evolves constantly, in both structure and content. A compressor built for one version of a schema would have a short lifetime.&lt;/p&gt;
    &lt;p&gt;Thankfully, with the flexibility offered by compression plans, we can react swiftly to data changes. At Meta, this is the core mission of Managed Compression, originally created to automate dictionary compression with Zstandard, and presented in an earlier blog on how we improved compression at with Zstandard.&lt;/p&gt;
    &lt;p&gt;OpenZL offers a training process that updates compression plans to maintain or improve compression performance, based on provided data samples. Now the synergy with Managed Compression is apparent: Each registered use case is monitored, sampled, periodically re-trained, and receives new configs when they prove beneficial. The decompression side continues to decode both old and new data without any change.&lt;/p&gt;
    &lt;p&gt;Runtime Adaptation: A compression config can include control points that read lightweight statistics at compression time (e.g., string repetition stats, run-length, histogram skew, delta variance) and choose the best branch of the Plan to go to next. Many technologies can be used, and textbook classifiers qualify. Control points handle bursts, outliers, and seasonal shifts without brute-force exploration: exploration is bounded, in order to maintain speed expectations. Taken branches are then recorded into the frame, and the decoder just executes the recorded path.&lt;/p&gt;
    &lt;p&gt;This gives the best of both worlds: dynamic behavior at compression time to handle variations and exceptions — without turning compression into an unbounded search problem — and with zero complexity added to the decoder.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Advantages of the Universal Decoder&lt;/head&gt;
    &lt;p&gt;OpenZL is capable of compressing a vast array of data formats, and they can all be decompressed with a single decompressor binary. Even when the compression configuration changes, the decoder does not. This may sound like operational minutiae, but it’s critical to OpenZL’s deployment success.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One audited surface: Security and correctness reviews focus on a single binary with consistent invariants, fuzzing, and hardening; there’s no myriad of per-format tools that can drift apart.&lt;/item&gt;
      &lt;item&gt;Fleet-wide improvements: A decoder update (security or performance — SIMD kernels, memory bounds, scheduling) benefits every compressed file, even those that predate the change.&lt;/item&gt;
      &lt;item&gt;Operational clarity: Same binary, same CLI, same metrics and dashboards across datasets; patching and rollout are uneventful by design.&lt;/item&gt;
      &lt;item&gt;Continuous training: With one decoder and many compression plans, we can keep improving while the system is live. Train a plan offline, try it on a small slice, then roll it out like any other config change. Backward compatibility is built-in — old frames still decode while new frames get better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, it’s possible to afford domain-specific compression without fragmenting the ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results With OpenZL&lt;/head&gt;
    &lt;p&gt;When OpenZL is able to understand and parse the file format, it is able to offer large improvements in compression ratio, while still providing fast compression and decompression speed. However, this is no magic bullet. When OpenZL doesn’t understand the input file format, it simply falls back to zstd.&lt;/p&gt;
    &lt;p&gt;OpenZL, through its offline training capabilities, is also able to offer a wide range of configurations in the tradeoff space of compression ratio, compression speed, and decompression speed. Unlike traditional compressors, which offer configuration by setting a compression level, OpenZL offers configuration by serializing the compressor graph. This allows an immense amount of flexibility to select diverse tradeoffs.&lt;/p&gt;
    &lt;p&gt;These results are based on datasets we’ve developed for our whitepaper. The datasets were chosen because they are highly structured and in a format that OpenZL supports. Every figure below is produced with scripts in the OpenZL repository so they can be reproduced, and the input data and logs from our runs have been uploaded to GitHub.&lt;/p&gt;
    &lt;p&gt;Note that data points connected by a line are pareto-optimal. All such points have the property that there is no point in the same dataset which beats them in both metrics.&lt;/p&gt;
    &lt;head rend="h3"&gt;When It’s Not Useful&lt;/head&gt;
    &lt;p&gt;OpenZL relies on a description of some structure to leverage its set of transforms. When there is no structure, there is no advantage. This is typically the case in pure text documents, such as enwik or dickens. In these cases, OpenZL falls back to zstd, offering essentially the same level of performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started With OpenZL&lt;/head&gt;
    &lt;p&gt;OpenZL’s selection of codecs is well-suited to compressing vector, tabular, or tree-structured data, and can be expected to perform well with numeric, string, or binary data. Common examples include timeseries datasets, ML tensors, and database tables. Keep in mind that we are bound by the limits of information theory, so the input needs to have some order that can be uncovered. As time goes on, we plan to incorporate additional codecs, as described in the next section.&lt;/p&gt;
    &lt;p&gt;If your data fits one of the above categories, then give it a try! Visit the OpenZL site and our Quick Start guide to get started.&lt;/p&gt;
    &lt;p&gt;If you want to dive into the code, check out the GitHub repository for source, documentation, and examples. We welcome contributions and feedback from the community!&lt;/p&gt;
    &lt;head rend="h2"&gt;Where We’re Going&lt;/head&gt;
    &lt;p&gt;OpenZL’s general direction is set: make it easier to expose structures, and exploit it with automated compression plans for evolving data.&lt;/p&gt;
    &lt;p&gt;Next up: We’re extending the transform library for time-series and grid-shaped data, improving performance of codecs, and enabling the trainer to find better compression plans faster. We also are actively working to extend SDDL to describe nested data formats more flexibly. Finally, the automated compressor explorer is getting better at proposing safe, testable changes to a compression plan within a specified budget.&lt;/p&gt;
    &lt;p&gt;Where the community can help: If you have a format or a dataset with obvious structure, try compressing it with an OpenZL prebuilt Plan. If it’s promising, try generating a new plan with the trainer or customizing it with our documentation to improve it. If it’s a format that the public might want, send it to us in a PR.&lt;/p&gt;
    &lt;p&gt;You can also contribute to the OpenZL core. If you have a knack for optimizing C/C++, help us speed up the engine or add transforms to cover new data formats. If your super power is reliability, the project would surely benefit from more validation rules and resource caps. And if you care about benchmarks, add your dataset to the harness so others can reproduce your results.&lt;/p&gt;
    &lt;p&gt;How to engage: Open an issue on the GitHub issue board. If you have a use-case for which you would expect OpenZL to do better, provide a few small samples, so that we can analyze them together. You may also contribute to codec optimizations, and propose new graphs, parsers or control points. All these topics do not impact the universality of the decoder.&lt;/p&gt;
    &lt;p&gt;We believe OpenZL opens up a new universe of possibilities to the data compression field, and we’re excited to see what the open source community will do with it!&lt;/p&gt;
    &lt;p&gt;To learn more about Meta Open Source, visit our website, subscribe to our YouTube channel, or follow us on Facebook, Threads, X, Bluesky and LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45492803</guid><pubDate>Mon, 06 Oct 2025 16:01:58 +0000</pubDate></item><item><title>One to two Starlink satellites are falling back to Earth each day</title><link>https://earthsky.org/human-world/1-to-2-starlink-satellites-falling-back-to-earth-each-day/</link><description>&lt;doc fingerprint="f6121e362ac0ec89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;1 to 2 Starlink satellites are falling back to Earth each day&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; Watch from multiple locations as a Starlink satellite reenters Earth’s atmosphere, burning up over California, on September 25, 2025. Currently, there are 1 to 2 Starlink satellites falling back to Earth each day. Soon there will be more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Starlink satellites are falling&lt;/head&gt;
    &lt;p&gt;It might not be long before you look up and see a fiery, slow-moving object streaking across your night sky and, clearly, breaking into pieces. That’s if you haven’t seen such a thing already. There are currently one to two Starlink satellites falling back to Earth every day, according to retired Harvard astrophysicist Jonathan McDowell. His acclaimed website Jonathan’s Space Report is widely regarded as the definitive source on spacecraft that go up … and come down. When we asked him about the deluge of Starlink satellite breakups that have recently been flooding social media, he pointed us to his graph showing Starlink reentries over time.&lt;/p&gt;
    &lt;p&gt;There are more than 8,000 Starlink satellites overhead at this moment. They’re a product of the space transportation company SpaceX. And that number is growing. Plus there are other companies and countries also deploying more and more satellites, adding to the number of satellites in Earth orbit. Many of these are in low-Earth orbits, which extend up to an altitude of 1,200 miles (2,000 km) above our planet. And the lifespan of low-Earth orbit satellites, such as Starlink, is only about 5 to 7 years. Soon, McDowell told us, there will be up to 5 satellite reentries per day. He said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;With all constellations deployed, we expect about 30,000 low-Earth orbit satellites (Starlink, Amazon Kuiper, others) and perhaps another 20,000 satellites at 1,000 km [620 miles] from the Chinese systems. For the low-orbit satellites we expect a 5-year replacement cycle, and that translates to 5 reentries a day. It’s not clear if the Chinese will orbit-lower theirs or just accelerate us to chain-reaction Kessler syndrome.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Kessler syndrome is a scenario in which the density of objects in low-Earth orbit is high enough that collisions between objects cause a cascade, with each collision generating space debris that increases the likelihood of further collisions. Read more about the Kessler syndrome here.&lt;/p&gt;
    &lt;p&gt;Watch our interviews with Jonathan McDowell:&lt;/p&gt;
    &lt;p&gt;Space junk and other human-made space hazards&lt;/p&gt;
    &lt;p&gt;The truth about Near-Earth Collisions&lt;/p&gt;
    &lt;p&gt;Catch a falling SpaceX Starlink&lt;/p&gt;
    &lt;head rend="h3"&gt;How to tell the difference between space junk and meteors&lt;/head&gt;
    &lt;p&gt;In many of the images online showing the fiery disintegration of something in our atmosphere, a photographer is asking:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;What did I just see?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So, is there a quick way to tell the difference between a meteor and space junk burning up overhead? McDowell explained:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The easy ‘meteor vs. space junk’ discriminator is speed. A meteor from solar orbit, even a big fireball, lasts only a few seconds and is gone, whizzz. Space junk goes more like airplane angular speed (really faster than a plane, but higher so it cancels out) and may be overhead for a couple of minutes.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Watch a Russian rocket reenter Earth’s atmosphere over southeastern Australia in May 2020.&lt;/p&gt;
    &lt;head rend="h3"&gt;How solar activity affects Starlink satellites and others&lt;/head&gt;
    &lt;p&gt;As SpaceX launches more and more Starlink satellites, more of them also come falling back to Earth. But they don’t all fall out of orbit for the same reason. Besides the fact that some of them are at the end of their lifespans, there are other reasons satellites can reenter.&lt;/p&gt;
    &lt;p&gt;For example, high solar activity can shorten the lifespan of satellites, and we’re just past a solar maximum and still in the period of high solar activity. Solar storms heat Earth’s upper atmosphere, causing it to “puff up.” The result is an increase in atmospheric drag: low-Earth orbit satellites like Starlink (and ISS, and Earth-observing satellites) find themselves flying through thicker air than usual. That extra air density creates aerodynamic drag, which slows the satellites down and causes them to lose altitude.&lt;/p&gt;
    &lt;p&gt;Operators might be able to boost some satellites back up. But, if they can’t, the satellites can reenter the atmosphere prematurely. That’s what happened in early 2022, when a solar storm doomed 40 recently launched Starlink satellites.&lt;/p&gt;
    &lt;head rend="h3"&gt;Also, malfunctions can occur&lt;/head&gt;
    &lt;p&gt;Solar activity isn’t the only thing that brings satellites down. Malfunctions can occur. For example, on July 12, 2024, a Falcon 9 rocket failed during the second stage, leaving 20 Starlink satellites in the “wrong” orbit. In that case, McDowell said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;All but two of the satellites reentered on the day of launch, and the last one reentered on July 20, eight days after launch.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Recent Starlink satellites reentries shared on social media&lt;/head&gt;
    &lt;p&gt;Here’s a sample of some of the recent Starlink satellite reentries that people have witnessed. Keep in mind that many of the reentries are not being witnessed. That’s because some 70% of Earth’s surface is water, so in many places the reentries happen overhead where no one is around to see them. Also, reentries that happen in the middle of the night or during bright daylight are less likely to be witnessed.&lt;/p&gt;
    &lt;p&gt;September 25, 2025: A Starlink satellite lit up the sky over the Bay Area in California.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;WATCH: Starlink debris seen over the skies of Sacramento County, California.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;A reentry observed over California at 0245 UTC Sep 26 (7.45pm PDT Thu Sep 25) is consistent with Starlink 1586.&lt;/p&gt;
      &lt;p&gt;— Jonathan McDowell (@planet4589.bsky.social) 2025-09-26T23:51:48.535Z&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;September 22, 2025: A Starlink satellite burned up over Saskatchewan, Canada.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Here's the official Global Meteor Network camera video from Lucky Lake, Saskatchewan! This video is courtesy University of Western Ontario and Defence R&amp;amp;D Canada.I counted 13 pieces in the video, how many do we think made it to the ground and are sitting on canola stubble east of Saskatoon?&lt;/p&gt;
      &lt;p&gt;— Prof. Sam Lawler (@sundogplanets.mastodon.social.ap.brid.gy) 2025-09-26T15:52:56.000Z&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Starlink 1066 reentered over Canada at about 0600 UTC Sep 23.&lt;/p&gt;
      &lt;p&gt;— Jonathan McDowell (@planet4589.bsky.social) 2025-09-23T23:39:15.840Z&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;September 21, 2025: This Starlink reentry created a stir when it burned up over Texas.&lt;/p&gt;
    &lt;quote&gt;
      &lt;section&gt;@hearts4hoovestherapy&lt;/section&gt;
      &lt;p&gt;Well that was cool! #Hearts4Hooves #fyppppppppppppppppppppppp #goviral #foryoupage #fyp?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Starlink 1636, launched in Aug 2020, reentered over Texas last night (Sep 22 0130 UTC / Sep 21 8.30pm CDT) and was observed widely.&lt;/p&gt;
      &lt;p&gt;— Jonathan McDowell (@planet4589.bsky.social) 2025-09-23T00:27:13.160Z&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;What effect do the disintegrating satellites have on Earth’s atmosphere?&lt;/head&gt;
    &lt;p&gt;In 2023, the National Oceanic and Atmospheric Administration (NOAA) shared a scientific investigation of Earth’s stratosphere. The stratosphere is the layer of atmosphere more than 7 miles (11 km) above Earth’s surface, where jet planes fly and the ozone layer exists. NOAA said the stratosphere:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;… contains an unexpected quantity of particles with a variety of exotic metals. The scientists believe the particles come from satellites and spent rocket boosters as they are vaporized by the intense heat of reentry.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The researchers found particles containing the rare elements niobium and hafnium. They also found a significant number of particles contained copper, lithium and aluminum at concentrations far exceeding the abundance found in space dust. The use of these elements in heat-resistant, high-performance alloys pointed at the spaceflight industry as the culprit.&lt;/p&gt;
    &lt;p&gt;These tiny particles could absorb and reflect the sun’s rays. They could also serve as surfaces for ozone-destroying chemical reactions. And these particles could change our planet’s atmosphere in ways we still don’t fully understand. Research in this area is ongoing.&lt;/p&gt;
    &lt;p&gt;Bottom line: There are now one to two Starlink satellites falling back to Earth each day, burning up in the atmosphere with consequences not fully understood.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45493143</guid><pubDate>Mon, 06 Oct 2025 16:32:30 +0000</pubDate></item><item><title>Ladybird passes the Apple 90% threshold on web-platform-tests</title><link>https://twitter.com/awesomekling/status/1974781722953953601</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45493358</guid><pubDate>Mon, 06 Oct 2025 16:52:58 +0000</pubDate></item><item><title>OpenAI DevDay 2025: Opening keynote [video]</title><link>https://www.youtube.com/watch?v=hS1YqcewH0c</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45493432</guid><pubDate>Mon, 06 Oct 2025 17:00:05 +0000</pubDate></item><item><title>UpCodes (YC S17) is hiring remote engineers across the Americas</title><link>https://up.codes/careers?utm_source=HN</link><description>&lt;doc fingerprint="f1efd1e76f34fe34"&gt;
  &lt;main&gt;
    &lt;p&gt;Try for Free&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45493453</guid><pubDate>Mon, 06 Oct 2025 17:01:31 +0000</pubDate></item><item><title>Beginnings: The Dempster Dumpster</title><link>https://www.classicrefusetrucks.com/albums/DE/DE01.html</link><description>&lt;doc fingerprint="4ed1d93eafb3af08"&gt;
  &lt;main&gt;
    &lt;p&gt;&amp;amp;nbsp&amp;amp;nbsp&amp;amp;nbsp George Roby Dempster* was born in 1887 to Scottish &amp;amp; Irish immigrants. By age sixteen he had a job running a locomotive (while not in school), having previously worked various railroad jobs in Virgina. As a young adult, Dempster later worked as an equipment operator on the Panama Canal project, and in the early 1930's he formed a construction company with his brothers back home in Knoxville. It was during this time that Dempster developed a novel device for the lifting and transporting of portable storage containers which would be of great significance not only to the refuse collection industry, but to the betterment of sanitation practices in general.&lt;/p&gt;
    &lt;p&gt;&amp;amp;nbsp&amp;amp;nbsp&amp;amp;nbsp The first Dempster Dumpster lifts were employed by the family company on their construction sites, and consisted of a hydraulic hoist mounted to motor truck whereby open top buckets could be engaged, lifted and transported. The device also allowed for emptying of the shallow buckets by simply tipping them as they were held in the raised position by the hoist. First patented in February 1935, the device began to attract the attention of rival operators and before long Dempster Brothers Incorporated was in the truck equipment business.&lt;/p&gt;
    &lt;p&gt;Model 200 LF showing hookup, lift, carry and dump sequence&lt;/p&gt;
    &lt;p&gt;&amp;amp;nbsp&amp;amp;nbsp&amp;amp;nbsp Though Dempster probably didn't originally envision his invention as refuse collector, it turned out to be a perfectly timed solution to what was becoming a critical sanitation problem in many cities. On the collection front, refuse packer trucks were first beginning to gain favor by the late 1930's, but refuse storage methods left much to be desired. Particularly troublesome were businesses and apartments, which generated large concentrations of waste in densely populated areas. This was further exacerbated by a trend toward disposable packaging used by all manner of consumer products, which greatly increased the volume of refuse. Rows of overflowing trash cans were not an uncommon eyesore, a blight on many otherwise modern cities. Another method, was the refuse vault, which had to be shoveled out by the collectors, was not only unsanitary but a tedious waste of manpower.&lt;/p&gt;
    &lt;p&gt;Model H Dumpster system: 6-yard container is raised by lifting against pins welded on each side of bin, power provided by a hydraulic reeving hoist&lt;/p&gt;
    &lt;p&gt;&amp;amp;nbsp&amp;amp;nbsp&amp;amp;nbsp With the availability of larger and now fully enclosed containers with hinged bottom dumping, the Dumpster system first began to attract the attention of sanitation officials. Not surprisingly, it was Dempster's hometown of Knoxville that became America's first "Dumpster City" in 1937, with the purchase of a single Dumpster truck and eighteen containers of two cubic yard capacity each. Not only did the system generate favorable public response, the city also cut collection costs by more than half compared to the old method of open dump trucks. One driver could now pick up, empty and return each container without the need for any additional men. The containers sealed out insects and vermin, and sealed in refuse from wind and weather. This method is termed the "short haul" system. Because small capacity containers are used, it was practical for situations where the disposal point was relatively close to the collection area. At this time, many American municipalities still had dumps located within or near the city limits.&lt;/p&gt;
    &lt;p&gt;To dump, an arrester hook suspends top section of container while hoist is lowered allowing bottom-hinged floor to swing open.&lt;/p&gt;
    &lt;p&gt;&amp;amp;nbsp&amp;amp;nbsp&amp;amp;nbsp There would be many more "Dumpster Cities" in the future. Though it did not happen overnight, the Dempster Dumpster changed refuse handling practice on such a scale that to this day, the term "Dumpster" is commonly used to describe any large refuse storage container, a brand recognition factor that would be the envy of any company. Perhaps this is due to the fact that the "Dumpster" was a piece of equipment that the public had extremely intimate contact with. For example, a Gar Wood Load-Packer might pass briefly by your house two days a week, but a Dumpster was on the street almost all the time, emblazoned with the trademarked name for every user to see.&lt;/p&gt;
    &lt;p&gt;&amp;amp;nbsp&amp;amp;nbsp&amp;amp;nbsp Heavy-duty Dumpster lift hoists were introduced which featured direct-lift hydraulic rams with pivot arms and chains. Dubbed model series 'LF' and 'LFW' (Load Forward) they not only eliminated the cables and pulleys of the old reeving hoist, but also boosted the dead lift capacity to handle bigger containers. The Load Forward hoist would become Dempster's most popular model. Production continued during World War II, with Dempster supplying units to the armed forces as well as other necessary war material.&lt;/p&gt;
    &lt;p&gt;Larger LF (Load Forward) series were capable of lifting more weight than the model H, and the carried filled dumpsters in a forward position over the rear axle. They were well suited as site dumpers in rock quarries. This model 300 LF is carrying a load of stone in a 3-yard bottom-hinged dumpster&lt;/p&gt;
    &lt;p&gt;&amp;amp;nbsp&amp;amp;nbsp&amp;amp;nbsp During the 1940s, Dempster obviously saw great promise in the refuse collection body market and introduced the Dumpster Kolector,, a massive ten-cubic yard end-dumping container fitted with a single trailer axle. It was designed to be towed behind a light truck, and when filled was merely hauled away by a Dumpster hoist truck which could also deliver an empty unit. This system eliminated down time for the crew, and the same Dumpster hoist could service commercial/apartment containers throughout the route. This "satellite" system would find more widespread use in the 1960's with modern front loaders, and eventually spawned "trains" of multiple trailers pulled by a single truck.&lt;/p&gt;
    &lt;p&gt;Dumpster Kolector trailer towed on route by pickup, carried to dump by LFW hoist truck&lt;/p&gt;
    &lt;p&gt;&amp;amp;nbsp&amp;amp;nbsp&amp;amp;nbsp Dempster's Dumpster system was steadily improved in the years following the war, and was an unqualified success. He built a quality product and advertised it heavily. The system was the solution to the dire need of municipalities for sanitary refuse storage, a need not initially addressed by the "big three" rear loader manufacturers, and it spread like wildfire across the country. Their only major competitor was cross-town rival Brooks Brothers, but that firm's Load-Lugger system was better suited to industrial and construction waste and never quite matched Dempster's aggressive marketing and broad selection of containers.&lt;/p&gt;
    &lt;p&gt;Light-duty model 150 "B" from the 1930s. This was similar to the "H", but used a single direct lift hydraulic cylinder instead of reeving hoist. Load was carried rearward in position shown here.&lt;/p&gt;
    &lt;p&gt;At the opposite end of the spectrum from the 150 "B" was this rugged 400 "LF" or Load-Forward model. A pair of massive cylinders did the heavy lifting&lt;/p&gt;
    &lt;p&gt;A horizontally mounted cylinder was used to slide the load forward on to the chassis&lt;/p&gt;
    &lt;p&gt;The Model "BG" (below grade) used a double-spool drum winch which was capable of lifting and lowering containers great distances&lt;/p&gt;
    &lt;p&gt;The Model "BG" raises Dumpster from a deep work pit&lt;/p&gt;
    &lt;p&gt;Dempster 6-cubic yard garbage container from 1940...&lt;/p&gt;
    &lt;p&gt;...raised to the transport position...&lt;/p&gt;
    &lt;p&gt;...and in dumping position.&lt;/p&gt;
    &lt;p&gt;Dempster pioneered the containerization of commercial and residential waste.&lt;/p&gt;
    &lt;p&gt;City of Plainview, Texas: Model LF dumping an 8-yard refuse Dumpster. This is a "sump bottom" type of container, preferred for use in rubbish/garbage collection with liquids present&lt;/p&gt;
    &lt;p&gt;This 1950 model LFW hoist riding atop a REO truck chassis belonging to the City of Baltimore, Bureau of Sanitation, is preparing to lift a Dumpster container. Note the attractive lettering and pinstriping on the hoist, as well as on the ten yard Dumpster container. Idle containers served as a "billboard" for sanitation departments which took great pride in their modern equipment.&lt;/p&gt;
    &lt;p&gt;Dempster Dumpsters in Baltimore, 1950 film (courtesy of Periscope Films)&lt;/p&gt;
    &lt;p&gt;GMC tandem cabover with the big LFW-503C&lt;/p&gt;
    &lt;p&gt;Type DTLF was similar to the LFW, but carried the container in a level position&lt;/p&gt;
    &lt;p&gt;1955: Dempster Dumpster at work in Pampa, Texas (video courtesy of Refuse Truck Media)&lt;/p&gt;
    &lt;p&gt;1957: Dempster Dumpster LFW-253C (video courtesy of Refuse Truck Media)&lt;/p&gt;
    &lt;p&gt;REFERENCES&lt;/p&gt;
    &lt;p&gt;Fountain Citians Who Made A Difference: George R. Dempster&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45493713</guid><pubDate>Mon, 06 Oct 2025 17:22:48 +0000</pubDate></item><item><title>OpenAI ChatKit</title><link>https://github.com/openai/chatkit-js</link><description>&lt;doc fingerprint="22998170f99ebfba"&gt;
  &lt;main&gt;
    &lt;p&gt;ChatKit is a batteries-included framework for building high-quality, AI-powered chat experiences. It’s designed for developers who want to add advanced conversational intelligence to their apps fast—with minimal setup and no reinventing the wheel. ChatKit delivers a complete, production-ready chat interface out of the box.&lt;/p&gt;
    &lt;p&gt;Key features include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deep UI customization so that ChatKit feels like a first-class part of your app&lt;/item&gt;
      &lt;item&gt;Built-in response streaming for interactive, natural conversations&lt;/item&gt;
      &lt;item&gt;Tool and workflow integration for visualizing agentic actions and chain-of-thought reasoning&lt;/item&gt;
      &lt;item&gt;Rich interactive widgets rendered directly inside the chat&lt;/item&gt;
      &lt;item&gt;Attachment handling with support for file and image uploads&lt;/item&gt;
      &lt;item&gt;Thread and message management for organizing complex conversations&lt;/item&gt;
      &lt;item&gt;Source annotations and entity tagging for transparency and references&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Simply drop the ChatKit component into your app, configure a few options, and you're good to go.&lt;/p&gt;
    &lt;p&gt;ChatKit is a framework-agnostic, drop-in chat solution. You don’t need to build custom UIs, manage low-level chat state, or patch together various features yourself. Just add the ChatKit component, give it a client token, and customize the chat experience as needed, no extra work needed.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Generate a client token on your server.&lt;/p&gt;
        &lt;quote&gt;from fastapi import FastAPI from pydantic import BaseModel from openai import OpenAI import os app = FastAPI() openai = OpenAI(api_key=os.environ["OPENAI_API_KEY"]) @app.post("/api/chatkit/session") def create_chatkit_session(): session = openai.chatkit.sessions.create({ # ... }) return { client_secret: session.client_secret }&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install the React bindings&lt;/p&gt;
        &lt;quote&gt;npm install @openai/chatkit-react&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add the ChatKit JS script to your page&lt;/p&gt;
        &lt;quote&gt;&amp;lt;script src="https://cdn.platform.openai.com/deployments/chatkit/chatkit.js" async &amp;gt;&amp;lt;/script&amp;gt;&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Render ChatKit&lt;/p&gt;
        &lt;quote&gt;import { ChatKit, useChatKit } from '@openai/chatkit-react'; export function MyChat() { const { control } = useChatKit({ api: { async getClientSecret(existing) { if (existing) { // implement session refresh } const res = await fetch('/api/chatkit/session', { method: 'POST', headers: { 'Content-Type': 'application/json', }, }); const { client_secret } = await res.json(); return client_secret; }, }, }); return &amp;lt;ChatKit control={control} className="h-[600px] w-[320px]" /&amp;gt;; }&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45493718</guid><pubDate>Mon, 06 Oct 2025 17:23:20 +0000</pubDate></item><item><title>Apps SDK</title><link>https://developers.openai.com/apps-sdk/</link><description>&lt;doc fingerprint="9dca97fe33d1af9f"&gt;
  &lt;main&gt;
    &lt;p&gt;Our framework to build apps for ChatGPT.&lt;/p&gt;
    &lt;p&gt;Design components and conversational flows that feel native to ChatGPT.&lt;/p&gt;
    &lt;p&gt;Build apps that meet our quality, safety, and policy standards.&lt;/p&gt;
    &lt;p&gt;Identify and prioritize Apps SDK use cases.&lt;/p&gt;
    &lt;p&gt;Create and configure an MCP server.&lt;/p&gt;
    &lt;p&gt;Learn how to deploy your MCP server&lt;/p&gt;
    &lt;p&gt;Improve discovery and behavior with rich metadata.&lt;/p&gt;
    &lt;p&gt;Security and privacy considerations for Apps SDK.&lt;/p&gt;
    &lt;p&gt;Troubleshoot issues in Apps SDK apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45494558</guid><pubDate>Mon, 06 Oct 2025 18:27:33 +0000</pubDate></item><item><title>My Life in Ambigrammia</title><link>https://www.theatlantic.com/ideas/archive/2025/10/ambigrams-words-double-meanings-art/684404/</link><description>&lt;doc fingerprint="762310184fa340b4"&gt;
  &lt;main&gt;
    &lt;p&gt;In August of 1950, when I was just a little tyke and my sister Laura but a babe in arms, our family set out in our 1947 Kaiser from Princeton, New Jersey, for parts West. We were moving out to California so that my dad could take up a new position in physics at the almost-unknown institution named Leland Stanford Jr. University. En route, we passed through many states and innumerable gas stations. I loved the smell of gasoline when we filled up, and was fascinated by the logos of the many different brands of gas. One day, as we were passing through Ohio, my dad pointed at the sign of the Standard Oil of Ohio station where we had stopped:&lt;/p&gt;
    &lt;p&gt;He offhandedly commented that if you twisted your head around, you could read it upside down. He even said it out loud: “Oy-hose.” What a silly-sounding, meaningless word! I practically split my sides. “OIHOS” was the funniest thing my 5-year-old self had ever heard. It was also the first ambigram I had ever seen.&lt;/p&gt;
    &lt;p&gt;Well, actually, it wasn’t a true ambigram, but it was a close cousin to one. Let me explain. By ambigram, I mean a piece of writing expressly designed to squeeze in more than one reading. The etymology combines Latin’s ambi, meaning “two-sided,” with Greek’s gram, meaning “piece of writing”—and thus, if you look at an ambigram one way, it says one thing, and if you look at it another way, it says another thing (or possibly the same thing)—and deliberately so. A true ambigram is intentionally designed so as to have that Janus-like property. Since it’s unlikely that the creator of the SOHIO logo had the nonword OIHOS in mind as a rotated reading, I hesitate to call it a true ambigram, although, as my dad keenly observed, it had two pronounceable readings.&lt;/p&gt;
    &lt;p&gt;When you engage in “ambigrammia” (the act or art of producing an ambigram), you are not so much creating something new as discovering something old—or rather, something timeless, something that already (sort of) existed, something that could have been found by someone else, at least in principle. Ambigrammia is thus neither fish nor fowl, in that it floats somewhere between creation and discovery.&lt;/p&gt;
    &lt;p&gt;Let me spell this out a bit. Some ambigrams, when you see them, make you think, Oh, that was such an obvious find. A triviality! Anybody would have seen that possibility a mile away. Those are discovery-type ambigrams. Other ambigrams, though, make you wonder, How on earth did anyone ever dream this up? What kind of a mind could have created this? Those are creation-type ambigrams. And then there are ones that lie in between those two extremes.&lt;/p&gt;
    &lt;p&gt;Over the years, my passion for ambigrammia has given me many insights into creativity and what I call “discoverativity” (a proneness to making discoveries) and how they are linked. Aside from those two “-ivities,” ambigrammia also involves “explorativity” (a passion for probing unfamiliar terrain), “manipulativity” (a bent for tweaking things), and “projectivity” (the ability to imagine how others see things).&lt;/p&gt;
    &lt;p&gt;Once in a while I write Ambigrammia with a capital A, using it as a proper noun, almost as if it were a place, a realm, a territory, a world—for indeed, Ambigrammia is a microcosm inhabited by, well, Ambigrammists, of course. It’s time that I exhibited some more ambigrams to make all of this more concrete.&lt;/p&gt;
    &lt;p&gt;Like SOHIO, this design has a second reading if you rotate it by 180 degrees. Do so, and you’ll see that it still says the same thing. It’s not that the word ambigram is a palindrome, of course, nor that it is intrinsically symmetrical. Rather, it was forced to be symmetrical by the act of distorting its letters.&lt;/p&gt;
    &lt;p&gt;Here’s another example:&lt;/p&gt;
    &lt;p&gt;This one doesn’t have rotational symmetry; it has mirror symmetry. Reflect it, and it will look identical. Once again, the word doesn’t naturally have this property; it was forced to be symmetrical by that sadistic letter-abuser, Douglas R. Hofstadter.&lt;/p&gt;
    &lt;p&gt;My first ambigrams were drawn in the mid-1960s (but that name for them only came along 20 years later). I was following in the footsteps of my friend Peter Jones, but neither of us tried to make our ambigrams graceful; for us, they were just awkward-looking jokes with letters, and we only did a few dozen each and then ran out of gas. Below is Peter’s rendering of the university that we were both attending at that time:&lt;/p&gt;
    &lt;p&gt;It’s pretty gawky, verging on the illegible, but this kind of thing amused us to no end.&lt;/p&gt;
    &lt;p&gt;In the mid-1970s, I met Scott Kim, who had independently come up with the same idea as Peter had (but roughly 10 years later), and his ambigrams (or “inversions,” as Scott called them) were incredibly graceful. A simple example is shown below.&lt;/p&gt;
    &lt;p&gt;I was amazed and even intimidated by the beauty of Scott’s creations (or discoveries), and it took me several years to overcome my shock and to start drawing my own artistic ambigrams. (It was in late 1983 that I coined the word ambigram.)&lt;/p&gt;
    &lt;p&gt;Usually, an ambigram takes me about an hour from the moment of tackling the challenge with crude pencil sketches until the final artistic product has been rendered in felt-tip pen, in full color. Here’s a typical example, based on the name of my daughter. Producing it took about an hour altogether.&lt;/p&gt;
    &lt;p&gt;But some ambigrams come far more quickly, and many come far more slowly. Sometimes it takes a week or two before I find a satisfying solution to a difficult challenge! Altogether, I’ve designed about 5,000 ambigrams, and I’ve probably devoted 10,000 hours of my life to ambigrammia.&lt;/p&gt;
    &lt;p&gt;I very much enjoy the ambigram below, which I designed on the name of a great Russian composer, as it’s a visual pun—namely, the three orange circles can be seen as representing his suite entitled The Love for Three Oranges.&lt;/p&gt;
    &lt;p&gt;In case you couldn’t make it out, the composer in question is Sergei Prokofiev. And I’ve made ambigrams on the names of dozens of other composers as well.&lt;/p&gt;
    &lt;p&gt;Below is a 90-degree-rotation ambigram in honor of a great Baroque composer, known particularly for his fugues.&lt;/p&gt;
    &lt;p&gt;When I first met Scott Kim, he introduced me to the recherché musical notion of “canon by retrograde inversion,” which means that if you turn the score around by 180 degrees (thus reversing time while also turning upward melodic jumps into downward ones, and downward ones into upward ones), it remains unchanged—thus it is a musical ambigram. Scott showed me such a canon for two violins that is attributed to Mozart, and I, thereby inspired, wrote my own musical ambigram for piano, of which the first line is given below (so if you rotate it by 180 degrees, you get its last line).&lt;/p&gt;
    &lt;p&gt;A few years ago, I yearned to draw a map of my natal town. I don’t recall the spark that lit the fire, but it launched me on an epic trek, riding five horses at the same time! Actually, the horses were merely burros, and to be honest, the burros were just boroughs. But no matter! Off I set on an epic trek upon five boroughs at once. And good grief—before I could even finish saying “The Bronx is up and the Battery’s down!,” my map was already complete! And it had all been drawn in capital letters, to boot! And all using 180-degree rotations! Will wonders never cease?&lt;/p&gt;
    &lt;p&gt;Occasionally, when I show people ambigrammatical stunts like the five boroughs, they ask me, with eyes full of wonder, “Can you do this for any name?” I usually reply, “Well, it all depends on what you mean by ‘do this.’” I think what people generally mean by their question is: “Can you take any old word or name and make it turn into itself via 180-degree rotation?”&lt;/p&gt;
    &lt;p&gt;They don’t take into account such crucial questions as how legible the end result might be and who the intended audience is, nor the possibility of my resorting to other symmetries, such as wall reflections, lake reflections, quarter turns (clockwise or counterclockwise), and so forth—nor does it occur to them that I even have the option of reframing the challenge itself (meaning that, instead of doing “Elizabeth,” I might try to do “ELIZABETH” or “Betty” or “BETTY” or “Liz” or “LIZ” or “Lizzie” or “LIZZIE,” depending on Elizabeth’s range of nicknames). Like legibility and audience, such dodgings or tweakings of the challenge are out of sight, out of mind for them.&lt;/p&gt;
    &lt;p&gt;People tend to believe that, whatever curveball they might throw at me, I can hit it smack out of the park. I certainly cannot always do that, but maybe it’s just as well for me and other ambigrammists that people have such a belief; it makes what we do seem more like a set of magically inexplicable tricks than like hard work.&lt;/p&gt;
    &lt;p&gt;The first year I taught a class on ambigrammia was 1985; since then I’ve done it several times more. In a recent ambigrams class, the first assignment I gave involved just single words, such as the students’ first names, but after that, I wanted them to tackle sets of words, so I asked them for suggestions, and one student suggested the 12 canonical birthstones (one for each month of the year). This was a very difficult assignment, and not all the students were able to complete it, but we all had fun tackling it.&lt;/p&gt;
    &lt;p&gt;Since I was more advanced in ambigrammia than my students were, I threw in an extra element to the challenge when I myself tackled it: I aimed for doing all the birthstones as 180-degree rotations, and on top of that, in capital letters. Such a feat was not a priori doable, but after a lot of blood, sweat, and tears, I managed to come up with a 12-member “unigram” (meaning a design consisting of numerous ambigrams realized under the same set of constraints) that satisfied me.&lt;/p&gt;
    &lt;p&gt;One of my best unigrams is a set of mirror-reflection ambigrams on the seven colors of the rainbow (associated with the mnemonic name ROY G BIV). The rainbow is displayed below, with, at the bottom, a mirror-reflection version of my signature, DOUG, which, by some miracle, is simultaneously a mirror-image reflection of the year in which it was made (2006).&lt;/p&gt;
    &lt;p&gt;This “DOUG/2006” ambigram, aside from being mirror-symmetric, is also an “oscillation ambigram.” It’s called that because, like the famous Necker cube, it oscillates back and forth in the viewer’s mind between two readings without any need for rotation or reflection. Such ambigrams, being very difficult to carry off, are very rare. One of the best oscillations I’ve produced is shown below.&lt;/p&gt;
    &lt;p&gt;It describes the dual nature of light—both wave and particle simultaneously. The letters in WAVE are wide capitals, while the letters in Particle are narrow and lowercase (except that the P is a capital).&lt;/p&gt;
    &lt;p&gt;Why, you might wonder (and “you” includes me), have I devoted such a large portion of my life to the obscure and esoteric art form of ambigrammia? Is ambigrammia just a hobby or pastime for me? No, it’s far more than that. It’s what I would call a passionate binge—one among dozens scattered down the decades of my life, ever in pursuit of some elusive form of beauty. Indeed, when I look back, I see my life as a relentless search for beauty—a quest that I recently took to calling “My Wild Grace Chase.” (In fact, I’m working on a book by that title right now.)&lt;/p&gt;
    &lt;p&gt;What is it that makes creating ambigrams so compelling for me? It’s not just their double-readability or their sometime symmetry. The hope of coming up with a design with those alluring traits is merely a launching pad that sets me off on a quest. What fires me up during the quest are those exciting moments of discovery, those sudden jolts of insight—and later, when it’s done and polished, the repeated savoring of unanticipated small pieces of visual magic that I found along the way. I often gaze and wonder, How did I ever think that up? Yes, strangely enough, my peak achievements in ambigrammia continue to gratify me, their onetime maker, with their surprising internal harmonies.&lt;/p&gt;
    &lt;p&gt;My joy at creating a new ambigram and then savoring it many times over reminds me a bit of my reactions, when I was a teenager, to hearing the fugues in Bach’s Well-Tempered Clavier the very first time, and then over and over again. It wasn’t the intellectual side of these highly complex pieces that thrilled me; it was the powerful emotions that they evoked. But the fact that these pieces were fugues—compositions having subtle and intricate contrapuntal structures, compositions having voices interweaving in a way that I had never imagined—was inseparable from the feelings of awe and reverence that coursed up and down my spine as I listened to them. I think that something comparable, though diluted, could be said about the pleasure that ambigrams give me, both when I make them and when I look at them. Each one started out as a mystery shrouded in total fog—but I persevered, had some bad luck and some good luck, eventually found a hidden pathway, and finally, with hard work, wound up with a polished gem that retains charm for me long after the fact.&lt;/p&gt;
    &lt;p&gt;When facing a fresh new ambigram challenge, I can never anticipate what I’ll wind up doing with it. Will the end product be a 180-degree rotation, a wall reflection, a lake reflection, a quarter turn, or something else? Will it be in capitals, smalls, cursive, or some mixture thereof ? I don’t know in advance. Will it be curvy or angular, jagged or jaunty, ornate or minimal, swashbuckling or spare? No idea! And sometimes it takes me many hours—even days—to discover the secret that, all the time, was lurking hidden inside it. Because it was always potentially findable, it’s probably better to call it a discovery rather than a creation.&lt;/p&gt;
    &lt;p&gt;What gives me the greatest pleasure is the fact that here is a brand-new miniature piece of art, born from the yoking-together of a challenge and a constraint (or set of multiple constraints). What a joyful experience it is to be able to convert a novel challenge into a tiny visual gem! In the end, it’s not the fact that I’ve discovered or created a doubly readable piece of writing that transports me. In the end, it’s the beauty—the fact that I’ve somehow come up with a totally unforeseen embodiment of beauty—that matters.&lt;/p&gt;
    &lt;p&gt;I have never been inclined to let machines do my thinking for me, which is why I have no interest in seeing AI applied to ambigrammia. I have my doubts as to whether current AI systems could come up with great ambigrams, but whether they could or could not do so, I would be loath to use such an approach. I have a reverence for the creative/discoverative human mind and want to use my own mind as much as I can in all facets of life. This may strike some readers as an old-fashioned attitude, but that’s me all over.&lt;/p&gt;
    &lt;p&gt;A few months ago, in talking with my friend Joshua Cynamon, I explained that I’ve recently felt plagued by waves of troubled confusion over putting so much energy and care into a book of doubly readable pieces of calligraphy—just frilly little colorful baubles. What a crazy thing to spend one’s later years on, especially in the frightening times that we’re living through. I said to Josh, “I’m working on this book during a period when many of my dearest friends are growing older and confronting great sadness, when democracy seems teetering on the brink of destruction, when the very concept of truth seems to be going down the drain, when artificial intelligence may soon overtake human intelligence, when global warming is threatening the survival of all sentient life on Earth. At this scary moment in history, doesn’t it seem weird and self-indulgent of me to be working on something so frivolous and flighty as a collection of ambigrams?”&lt;/p&gt;
    &lt;p&gt;Josh replied, “I can well imagine your doubts, Doug, but actually I feel that it’s very sensible for you to be working so hard on your ambigrams book—in fact, it’s a crucial thing for you to be doing—because bringing beauty into the world is so important to you, and to us all, most especially in dark times.”&lt;/p&gt;
    &lt;p&gt;Josh’s thoughtful reaction helped me to feel once again that my artistic project was worthwhile, and it brought back a memory of a remark that my daughter Monica made to me late one night over the phone, just as the results of the 2016 presidential election were becoming clear. She asked me, “How can this nightmare be happening?” I said, “I don’t know. I don’t know.” She asked, “What is humanity coming to?” All I could think of to say was, “The world is unpredictable. Nobody can know where things will go.” And then, out of the blue, Monica said, “In the end, only art can save the world.” It was such a powerful and idealistic statement of faith that I’ve never forgotten it, and I dearly hope that she is right.&lt;/p&gt;
    &lt;p&gt;This essay is an adapted excerpt from Hofstadter’s brand-new book, Ambigrammia: Between Creation and Discovery (Yale University Press, 2025).&lt;/p&gt;
    &lt;p&gt;When you buy a book using a link on this page, we receive a commission. Thank you for supporting The Atlantic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45495711</guid><pubDate>Mon, 06 Oct 2025 20:07:25 +0000</pubDate></item><item><title>Valorant's 128-Tick Servers</title><link>https://technology.riotgames.com/news/valorants-128-tick-servers</link><description>&lt;doc fingerprint="77fad04f25224d93"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VALORANT's 128-Tick Servers&lt;/head&gt;
    &lt;p&gt;Hi, I’m Brent “Brentmeister” Randall and I’m an engineer on the Gameplay Integrity team for VALORANT. My team is responsible for VALORANT’s build system, automation framework, game client performance, and server performance. In this article, I’ll be focusing on that last topic - I’ll be telling the technical story behind our search for optimal server performance.&lt;/p&gt;
    &lt;p&gt;From very early on in development, we knew that VALORANT would have very strict server performance requirements. I hope I can give you some insight into why that is, and how we accomplished our ambitious goals. When we started, a server frame took 50ms, and by the end we reached sub-2ms per frame - all by looking at code optimization, hardware tweaks, and OS tunings.&lt;/p&gt;
    &lt;p&gt;Let’s go on a journey together.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Importance of Netcode&lt;/head&gt;
    &lt;p&gt;All online shooters (even VALORANT) have some amount of peeker’s advantage. We’ve done a video blog on netcode and peeker’s advantage, and a previous Tech Blog article on the same.&lt;/p&gt;
    &lt;p&gt;To provide a short summary - in VALORANT, a key part of the gameplay is taking strategic positions and holding them. Holding positions can become impossible if other players can run around a corner and kill the defender before the defender can react due to latency. That latency is partly based on the network and partly based on the server tick rate. To give defenders the time they need to react to aggressors, we determined that VALORANT would require 128-tick servers. If you’re interested in how we came to that conclusion, our tech blog post on peeker’s advantage covers it in detail.&lt;/p&gt;
    &lt;head rend="h1"&gt;Code Optimization&lt;/head&gt;
    &lt;p&gt;When we think about hosting servers at a 128-tick rate, our biggest constraint is CPU resources. We need to be able to process an entire frame within 7.8125ms, but if we do that, a single game would take up an entire CPU core!&lt;/p&gt;
    &lt;p&gt;This diagram demonstrates how many games we can run per core:&lt;/p&gt;
    &lt;p&gt;Utilizing 1 full core per game would make it prohibitively expensive to host our game servers at scale, considering we knew we wanted to offer 128-tick for free and not as a premium service players had to pay for. After crunching the numbers, we knew we needed to do better than 3 games per core (gpc). To put this in perspective, we generally run 36 core hosts, so each physical game server needed to host 108 games or 1080 players. Even 3 gpc was a massive investment, but Riot leadership understood and supported our ambitious server performance goals.&lt;/p&gt;
    &lt;p&gt;Let’s take that 7.8125ms, divide it by 3 gpc, and we end up with 2.6ms. But wait - we also need to reserve 10% for overhead of the OS, scheduling, and other software running on the host. After these calculations, we end up with a target budget of just 2.34ms per frame. When we looked at VALORANT’s initial data, we were at 50ms; we had a long way to go. This was going to be an effort that needed to involve the entire development team.&lt;/p&gt;
    &lt;head rend="h2"&gt;Breaking The Problem Down&lt;/head&gt;
    &lt;p&gt;“Make the server 20x faster” isn’t a very tractable problem, so we applied the single best tool in software engineering: Break a big intimidating problem down into smaller solvable problems. We needed to figure out where those 50ms were being spent so that we could start shaving it down. We sat down with the VALORANT technical leads and discussed what the big areas of CPU cost likely were and came up with a list of categories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;replication&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FoW&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;network&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;animation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;gameplay&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;movement&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;equippable&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;character&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;physics&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;other&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Armed with this list, we built a system that allowed programmers to easily mark up game code and categorize them appropriately. Every line of code that executes is scoped to one of the above buckets using a macro system, and we added a concept of subsystems for finer grained analysis of larger systems. We called the system ValSubsystemTelemetry.&lt;/p&gt;
    &lt;p&gt;As the game runs, these scopes let us track how much time we’re spending in each category.&lt;/p&gt;
    &lt;head rend="h2"&gt;Leveraging Riot Tech: Analytics Platform&lt;/head&gt;
    &lt;p&gt;So now we’ve got a library and we’re generating all this data. What do we do with it?&lt;/p&gt;
    &lt;p&gt;Part of working for a larger studio like Riot means we’re able to leverage existing tools and tech that other teams develop and support. In this case, for example, a central team at Riot had developed a technology called the Analytics Platform. This tool allows programmers at Riot to publish data to our big data warehouse and then build visualizations around it.&lt;/p&gt;
    &lt;p&gt;Here are some of the ways we visualize performance data on VALORANT:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Average server frame time per round&lt;/p&gt;
    &lt;p&gt;Average processing time of each VALSubsystem&lt;/p&gt;
    &lt;p&gt;Without data like this, it’s easy for non-performant code or content changes to make it into the game undetected. We might go weeks before these sorts of changes stack up to a breaking point where developers or players notice things slowing down.&lt;/p&gt;
    &lt;p&gt;Digging back through weeks of changelists to find culprits is costly work, but it’s a much easier task when you’ve been tracking performance data all along. In the second image above, for example, we can see that there’s a problem between change numbers 445887 and 446832 that caused replication (the orange line) to take longer. This type of visualization allows us to look through a much smaller set of changes and quickly assign an engineer to resolve the issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Budgets for Subsystems&lt;/head&gt;
    &lt;p&gt;Now that we had the visualizations set up for data verifications, we were able to set budgets for each subsystem and follow up on any discrepancies. The VALORANT tech leads got together again and discussed what reasonable budgets would be for each of the systems. This was largely informed by where the systems were at that time, and what opportunities for optimization existed in systems according to the experts. From there, each team and expert had a goal in mind and we could work in parallel to get the performance to a shippable state.&lt;/p&gt;
    &lt;p&gt;Here we can see some of the early data organized by the categories mentioned above:&lt;/p&gt;
    &lt;p&gt;Let’s take a look at two specific sections to demonstrate how we whittled down performance costs. First we’ll focus on replication because it was the system that needed the biggest change. Then we’ll take a look at animation, because the changes we had to make are highly representative of the types of solutions we made across all categories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Replication&lt;/head&gt;
    &lt;p&gt;Property Replication, which we often just call replication for short, is a system in Unreal Engine 4 (UE4) that allows for network synchronization of state between server and clients. It’s a great system for quickly prototyping new characters/abilities/features that require networking with clients. A developer can simply markup a variable as “replicated” and it will automatically be synced between the server and clients.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it’s also pretty slow. It requires scanning through every variable marked as replicated every frame, then comparing it to each of the 10 clients’ last known states, and then packaging any deltas to send to the client. This is effectively random access across memory and is really cache-intensive, slow work. Regardless of state changes, the variables are still checked. I consider “polling” systems like this a performance anti-pattern.&lt;/p&gt;
    &lt;p&gt;The fix is to utilize another UE4 networking tool: Remote Procedure Calls (RPCs). RPCs allow the server to call functions over the network that execute on one or more of the clients. Using RPCs on state changing gameplay events limit the performance cost to the frame on which the state change occurs. This “push” model is far more performant. The downside is that designers and engineers have to think more carefully about placement of RPCs and handling cases like reconnect. However, we found in many cases changing from a replicated variable to an RPC offered a 100x to 10000x performance improvement!&lt;/p&gt;
    &lt;p&gt;As an example, consider player health. One way to network player health would be to mark player health as replicated. Each frame, the game server would check if the value has changed and if so notify the correct clients. With an RPC, you would likely send a “ShotHit” event from the server with the damage value. Clients would stay in sync by applying that damage to the player's health themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;Animation&lt;/head&gt;
    &lt;p&gt;Animation was a huge cost for us on the server side. To properly figure out if a shot hit or not, we need to run the same animations on the server that players see on their clients. Hit registration in VALORANT works by saving player positions and animation state in a historical buffer. When the server receives a shot packet from the client, it rewinds the player positions and animation state using the historical buffer to calculate if the shot hit. Initially we were computing animation and filling this buffer every frame. However, after careful testing and comparisons we found that we could animate every 4th frame. In the event of a rewind we could lerp between the saved animations. This effectively cut animation costs down by 75%.&lt;/p&gt;
    &lt;p&gt;Another important realization was that amortized server performance is the most important type of performance at scale. Imagine a VALORANT server running about 150 games. At any given time, ~50 of those games are going to be in the buy phase. Players will be purchasing equipment safely behind their spawn barriers and no shots can hurt them. We realized we don’t even need to do any server-side animation during the buy phase, we could just turn it off. So that’s exactly what we did - if you look at the server view, players are just in the idle pose during the buy phase. This helped reduce costs of the animation system over the course of a round by another 33%!&lt;/p&gt;
    &lt;p&gt;The red wireframe shows the server’s non-animated hitbox vs the client’s blue.&lt;/p&gt;
    &lt;head rend="h1"&gt;Real World Performance&lt;/head&gt;
    &lt;p&gt;Now you’ve got a taste of how we optimized the code - but performance is more than code. It’s also the platform you’re running on. So let’s discuss something that was causing huge issues with performance - the OS and hardware.&lt;/p&gt;
    &lt;p&gt;To properly test how our game was going to perform in the real world, we needed to fashion a load test. We had to know how the server would perform with 100+ instances all running on the same CPU. Building the load test was critical for successfully launching VALORANT. It allowed us to predict exactly how many cores we would need per player, and allowed us to solve a number of issues that only appeared at high load. Turns out, it’s not as simple as the 7.8ms / 3 games per core that I mentioned before.&lt;/p&gt;
    &lt;p&gt;(Editor's note: You can read more about load lesting for VALORANT here!)&lt;/p&gt;
    &lt;p&gt;First, let’s take a look at this chart. It graphs frame time on the Y-axis and number of instances on the X-axis.&lt;/p&gt;
    &lt;p&gt;So with only a single instance running, we hit a glorious 1.5ms... but once we’ve got 168 instances running, we’re hovering around 5.7ms.&lt;/p&gt;
    &lt;p&gt;Oh no - what’s going on here? To understand why this happened and how we resolved it, we’ll have to first take a look at modern CPU architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU Architecture&lt;/head&gt;
    &lt;p&gt;Take a look at the image above, and you’ll notice several important things. Each core has its own L1/L2 caches, but the larger L3 cache is shared between cores. With only one server running on the host, it gets to hog the L3 cache all to itself, which results in fewer misses - meaning the CPU core spends less time waiting on a memory request. That's why our low load scenarios had our servers running blazing fast, but they start to slow down as the cache grows more and more contended with each instance. We did some measurements with a team of cloud computing experts at Intel to make sure we weren’t hitting thermal limits or other factors, and narrowed it down simply to cache performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Collaborating with Intel&lt;/head&gt;
    &lt;p&gt;Luckily, Intel had a few tricks up their sleeves from their platform measuring and analysis tools. We were still running on the older Intel Xeon E5 processors, which made use of an inclusive cache. Basically, inclusive means that each cache line present in the L2 cache must be present in the L3 cache as well. If a line gets evicted from the L3 cache, it gets evicted from the L2 cache as well! That means that even though each L2 cache was separate, it was possible cores were thrashing one another’s L2 cache by causing evictions in L3 cache.&lt;/p&gt;
    &lt;p&gt;Totally unfair, don’t you think? With the Intel Xeon Scalable processors, Intel moved to a non-inclusive cache, completely eliminating this problem. Moving to the more modern Xeon Scalable processors showed major performance gains for our server application. We still see the effects of L3 contention but we saw roughly a 30% increase in performance, even using similar clock speeds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Non-Uniform Memory Access&lt;/head&gt;
    &lt;p&gt;We wanted to push our memory performance even further. First, you’ll need to understand another aspect of modern CPU architecture called Non-Uniform Memory Access (NUMA). On server architectures, you often run dual (or more) socket CPUs. Each of these sockets has direct access to a portion of the system’s RAM, and shares data through an interconnect. This allows for increased memory bandwidth (2x more connections), but the interconnect can become a bottleneck. Revisiting the diagram above, you can see a simplified layout of a NUMA architecture with two sockets. If only the operating system could make sure to allocate memory and CPU resources to keep interconnect traffic down...&lt;/p&gt;
    &lt;p&gt;Well, it turns out that many modern OS are NUMA-aware and can do this. On Linux, for example, one way to do this is to use numactl when starting a process. On VALORANT, we start game server instances back and forth between nodes like this:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;numactl --cpunodebind={gameid % 2} --membind={gameid % 2} ShooterGameServer&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Making maximum use of the architecture with such a small change led to a performance boost of around 5%. We turned our memory access from about 50% NUMA local to 97-99% NUMA local! In addition to the 5%, performance was much more consistent between server instances.&lt;/p&gt;
    &lt;head rend="h3"&gt;OS Scheduler&lt;/head&gt;
    &lt;p&gt;During our time monitoring the game server host, we saw an interesting pattern where cores would hover at around 90-96% usage but never reach 100% - even when the host was loaded to 2x the number of games it should be able to host. We suspected the OS scheduler was the cause, so we used Intel’s PMU profiling tool, Adaptive Optimization, along with the perf utility on Linux to dig into scheduler events. Utilizing Linux also meant we had the chance to review the source code for the scheduler as well.&lt;/p&gt;
    &lt;p&gt;Through our investigations, we learned that modern Linux uses the Completely Fair Scheduler (CFS). The scheduler is really clever and has a number of optimizations. One optimization is that it tries to keep processes on the same core, preventing them from migrating to run on other cores. One reason it may do this could be to allow processes to reuse still hot cache lines. Another reason might be to prevent unnecessarily waking up idle cores to do small amounts of work. The migration cost would basically keep the process waiting on a busy core for a length of time before considering allowing it to migrate it to an available core. The default value for this in our Linux distro was .5ms.&lt;/p&gt;
    &lt;p&gt;In VALORANT’s case, .5ms is a meaningful chunk of our 2.34ms budget. You could process nearly a 1/4th of a frame in that time! There’s 0% chance that any of the game server’s memory is still going to be hot in cache. While an individual game server idles in between frames, the other game servers are utilizing the cache to its fullest extent. By lowering the migration cost setting to 0, we guarantee that the scheduler immediately migrates a game server that needs to run to any available core on the system. Doing this lets us make much better use of CPU resources on the system and granted another 4% performance boost. Additionally, we saw the amount of time individual cores spent idle drop to nearly 0% under load.&lt;/p&gt;
    &lt;head rend="h3"&gt;C-States&lt;/head&gt;
    &lt;p&gt;Another area where we found straightforward wins was in controlling the C-State that we allowed the CPU to enter. When a multi-core CPU runs, it allows cores to enter different power states. Under reduced load, cores will often enter lower states to conserve power. However, once load increases, it takes time for those cores to swap to higher power states. In highly cyclical workloads - like a bunch of game servers processing frames then sleeping - the CPU ends up swapping power states frequently. Each swap has a latency that negatively affects performance. By limiting our process to the higher C-States (C0, C1 and C1E), we were able to host another 1-3% games stably. It particularly stabilized performance of 60-90% loaded servers where the reduced workload was allowing many cores to frequently idle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hyperthreading&lt;/head&gt;
    &lt;p&gt;Hyperthreading is a CPU architectural technique where a single physical core can host two simultaneous threads. With hyperthreading, certain parts of the core are shared (like caches), and certain parts (like different compute units) are duplicated.&lt;/p&gt;
    &lt;p&gt;It ultimately depends on the specific CPU you’re looking at. Early on in development, we had 25ms frame times and we found that turning off hyperthreading yielded 20ms frame times. Performance increased across the board by 25%! However, our friends at Intel were skeptical. Given our load, we could potentially squeeze out more out of the hardware by making use of the virtual cores that hyperthreading offers. When we flipped hyperthreading back on, we saw performance increase by 25%.&lt;/p&gt;
    &lt;p&gt;How did this happen? Along the way we had reduced server frame time to well below our 7.8125ms target. We migrated to the Intel Xeon Scalable processors architecture which improved cache and hyperthreading performance. We tweaked the scheduler to make better use of available cores. We disabled C-States below C1E for better core latency and many other optimizations.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Importance of Measuring&lt;/head&gt;
    &lt;p&gt;The lesson here is that each application's performance profile and considerations are different. Even the same application a year later can have drastically different performance needs. The only way to be sure is to create a reproducible test and measure.&lt;/p&gt;
    &lt;p&gt;If you just make a list of “performance tweaks” you might learn about in, say, a game dev blog post on the internet, and execute them without considering your application’s specific needs and considerations, you might hurt performance more than you help it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other Performance Optimizations&lt;/head&gt;
    &lt;head rend="h3"&gt;Clocksource&lt;/head&gt;
    &lt;p&gt;Games tend to frequently mark the passing of time. This is generally done by making system calls to the OS via the clocksource. An OS running in hypervisor environments (like AWS) might use virtualized clock sources that are less performant. On our AWS nodes, we were initially using the Xen clocksource provided by the Xen hypervisor. We changed to the tsc clocksource which is provided by the CPU instruction rdtsc. For our game servers, we were able to get about a 1-3% boost in performance by moving our clocksource.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ghost Story: It Only Happens on Prod&lt;/head&gt;
    &lt;p&gt;As we neared our ship date we noticed that one of our game server load test hosts performed worse than the others. It was identical hardware and the only difference was that it was running our full deployment stack. Load tests run on this box generated double to triple the number of hitches or slow frames than ones running on hardware that I provisioned by hand. We investigated several angles - could it be the AWS hypervisor, configuration differences, or even just bad hardware? Nothing was panning out.&lt;/p&gt;
    &lt;p&gt;Eventually we decided to look again at the Linux scheduler with perf sched to just see if we could find some differences on how the processes were running. We found out that every 5 seconds like clockwork, 72 processes would start called scheduler_1 … scheduler_72. One for each virtual core. These would start and immediately kick any running game server off the cores. On highly loaded game servers, this caused a cascading number of hitches. It turned out that Mesos, which we were using for our deployments, utilized Telegraf for metrics, which made DNS requests every 5 seconds, which were hijacked by dcos_net, an Erlang application.&lt;/p&gt;
    &lt;p&gt;Erlang has a configurable for how many threads its scheduler is allowed to spawn. The default is one thread per core on the system, hence the 72 processes. Once we set this to a more reasonable default like 4, the problem disappeared overnight. The lesson here is that it’s vitally important to measure performance in a configuration that matches your production environment!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Ultimately, it’s easy to miss the forest for the trees. Even in this quick fly-by through some of our performance efforts, the technical minutiae can stack up quickly. It’s easy to get lost in tiny details, tweaks, and oddities.&lt;/p&gt;
    &lt;p&gt;You’ve really got to continually revisit and reinforce the holistic performance goals you have in mind. Make sure you align the entire team around your goals so that you can enlist the right help from your team at the right time.&lt;/p&gt;
    &lt;p&gt;Code optimization is a big portion of performance, but you need to be able to break down your application performance into discrete chunks. And don’t forget about optimizing the environment (hardware and operating system) to host your application in the most efficient way.&lt;/p&gt;
    &lt;p&gt;Above all measure, measure, measure. VALORANT’s measurements ultimately allowed us to launch while predicting our server hardware needs within 1%. This resulted in a smooth launch experience and free 128-tick servers for our players.&lt;/p&gt;
    &lt;p&gt;I’d like to end with a special thanks to the Intel team who worked with us to investigate the hardware and OS tunings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kim McLaughlin&lt;/item&gt;
      &lt;item&gt;Harshad S Sane&lt;/item&gt;
      &lt;item&gt;Dory Simaan&lt;/item&gt;
      &lt;item&gt;Prabha Viswanathan&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Their insight and contributions were invaluable for helping us meet our performance goals.&lt;/p&gt;
    &lt;p&gt;Thanks for reading! If you have any questions or comments, please post them below.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45496146</guid><pubDate>Mon, 06 Oct 2025 20:47:25 +0000</pubDate></item></channel></rss>