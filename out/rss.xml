<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 18 Sep 2025 03:45:09 +0000</lastBuildDate><item><title>Apple Photos app corrupts images</title><link>https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/</link><description>&lt;doc fingerprint="7241bf3571248d9f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Apple Photos App Corrupts Images&lt;/head&gt;Sep 17, 2025 @ 8:59 am&lt;p&gt;The Apple Photos app sometimes corrupts images when importing from my camera. I just wanted to make a blog post about it in case anyone else runs into the problem. I‚Äôve seen other references to this online, but most of the people gave up trying to fix it, and none of them went as far as I did to debug the issue.&lt;/p&gt;&lt;p&gt;I‚Äôll try to describe the problem, and the things I‚Äôve tried to do to fix it. But also note that I‚Äôve (sort of) given up on the Photos app too. Since I can‚Äôt trust it to import photos from my camera, I switched to a different workflow.&lt;/p&gt;&lt;p&gt;Here is a screenshot of a corrupted image in the Photos app:&lt;/p&gt;&lt;head rend="h2"&gt;How I used to import images&lt;/head&gt;&lt;p&gt;I‚Äôve got an OM System OM-1 camera. I used to shoot in RAW + jpg, then when I would import to Photos app, I would check the ‚Äúdelete photos after import‚Äù checkbox in order to empty the SD card. Turns out ‚Äúdelete after import‚Äù was a huge mistake.&lt;/p&gt;&lt;head rend="h2"&gt;Getting corrupted images&lt;/head&gt;&lt;p&gt;I‚Äôm pretty sure I‚Äôd been getting corrupted images for a while, but it would only be 1 or 2 images out of thousands, so I thought nothing of it (it was probably my fault anyway, right?)&lt;/p&gt;&lt;p&gt;But the problem really got me upset when last year I went to a family member‚Äôs wedding and took tons of photos. Apple Photos combines RAW + jpg photos so you don‚Äôt have a bunch of duplicates, and when you view the images in the photos app, it just shows you the jpg version by default. After I imported all of the wedding photos I noticed some of them were corrupted. Upon closer inspection, I found that it sometimes had corrupted the jpg, sometimes corrupted the RAW file, and sometimes both. Since I had been checking the ‚Äúdelete after import‚Äù box, I didn‚Äôt know if the images on the SD card were corrupted before importing or not. After all, the files had been deleted so there was no way to check.&lt;/p&gt;&lt;p&gt;I estimate I completely lost about 30% of the images I took that day.&lt;/p&gt;&lt;p&gt;Losing so many photos really rattled me, but I wanted to figure out the problem so I didn‚Äôt lose images in the future.&lt;/p&gt;&lt;head rend="h2"&gt;Narrowing down the problem&lt;/head&gt;&lt;p&gt;I was worried this was somehow a hardware problem. Copying files seems so basic, I didn‚Äôt think there was any way a massively deployed app like Photos could fuck it up (especially since its main job is managing photo files). So, to narrow down the issue I changed out all of the hardware. Here are all the things I did:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Switched USB-C cables&lt;/item&gt;&lt;item&gt;Bought a new SD card direct from the manufacturer (to eliminate the possibility of buying a bootleg SD card)&lt;/item&gt;&lt;item&gt;Switched to only shooting in RAW (if importing messes up 30% of my images, but I cut the number of images I import by half, then that should be fewer corrupted images right? lol)&lt;/item&gt;&lt;item&gt;Bought a new laptop&lt;/item&gt;&lt;item&gt;Bought a new camera: the OM System OM-1 MKii&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I did each of these steps over time, as to only change one variable at a time, and still the image corruption persisted. I didn‚Äôt really want to buy a new camera, the MKii is not really a big improvement over the OM-1, but we had a family trip coming up and the idea that pressing the shutter button on the camera might not actually record the image didn‚Äôt sit well with me.&lt;/p&gt;&lt;head rend="h2"&gt;Finally a smoking gun&lt;/head&gt;&lt;p&gt;Since I had replaced literally all of the hardware involved, I knew it must be a software problem. I stopped checking the ‚Äúdelete after import‚Äù button, and started reviewing all of the photos after import. After verifying none of them were corrupt, then I would format the SD card. I did this for months without finding any corrupt files. At this point I figured it was somehow a race condition or something when copying the photo files and deleting them at the same time.&lt;/p&gt;&lt;p&gt;However, after I got home from RailsConf and imported my photos, I found one corrupt image (the one above). I was able to verify that the image was not corrupt on the SD card, so the camera was working fine (meaning I probably didn‚Äôt need to buy a new camera body at all).&lt;/p&gt;&lt;p&gt;I tried deleting the corrupt file and re-importing the original to see if it was something about that particular image, but it re-imported just fine. In other words, it seems like the Photos app will corrupt files randomly.&lt;/p&gt;&lt;p&gt;I don‚Äôt know if this is a problem that is specific to OM System cameras, and I‚Äôm not particularly interested in investing in a new camera system just to find out.&lt;/p&gt;&lt;p&gt;If I compare the corrupted image with the non-corrupted image, the file sizes are exactly the same, but the bytes are different:&lt;/p&gt;&lt;p&gt;Checksums:&lt;/p&gt;&lt;code&gt;aaron@tc ~/Downloads&amp;gt; md5sum P7110136-from-camera.ORF Exports/P7110136.ORF 
17ce895fd809a43bad1fe8832c811848  P7110136-from-camera.ORF
828a33005f6b71aea16d9c2f2991a997  Exports/P7110136.ORF
&lt;/code&gt;&lt;p&gt;File sizes:&lt;/p&gt;&lt;code&gt;aaron@tc ~/Downloads&amp;gt; ls -al P7110136-from-camera.ORF Exports/P7110136.ORF
-rw-------@ 1 aaron  staff  18673943 Jul 12 04:38 Exports/P7110136.ORF
-rwx------  1 aaron  staff  18673943 Jul 17 09:29 P7110136-from-camera.ORF*
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;P7110136-from-camera.ORF&lt;/code&gt; is the non-corrupted file, and &lt;code&gt;Exports/P7110136.ORF&lt;/code&gt; is the corrupted file from Photos app.
Here‚Äôs a screenshot of the preview of the non-corrupted photo:&lt;/p&gt;&lt;p&gt;Here is the binary diff between the files. I ran both files through &lt;code&gt;xxd&lt;/code&gt; then diffed them.&lt;/p&gt;&lt;head rend="h2"&gt;My new workflow&lt;/head&gt;&lt;p&gt;I‚Äôm not going to put any more effort into debugging this problem, but I wanted to blog about it in case anyone else is seeing the issue. I take a lot of photos, and to be frank, most of them are not very good. I don‚Äôt want to look through a bunch of bad photos every time I look at my library, so culling photos is important. Culling photos in the Photos app is way too cumbersome, so I‚Äôve switched to using Darktable.&lt;/p&gt;&lt;p&gt;My current process is:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Import images to Darktable&lt;/item&gt;&lt;item&gt;Delete the ones I don‚Äôt like&lt;/item&gt;&lt;item&gt;Process ones I do like&lt;/item&gt;&lt;item&gt;Export both the jpg and the original raw file&lt;/item&gt;&lt;item&gt;Import those to the Photos app so they‚Äôre easy to view and share&lt;/item&gt;&lt;item&gt;Periodically format my SD card&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I‚Äôve not seen any file corruption when importing to Darktable, so I am convinced this is a problem with the Photos app. But now, since all of my images land in Darktable before making their way to the Photos app, I don‚Äôt really care anymore. The bad news is that I‚Äôve spent a lot of time and money trying to debug this. I guess the good news is that now I have redundant hardware!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45274277</guid><pubDate>Wed, 17 Sep 2025 11:07:44 +0000</pubDate></item><item><title>Tau¬≤ benchmark: How a prompt rewrite boosted GPT-5-mini by 22%</title><link>https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/</link><description>&lt;doc fingerprint="3e0f6dd72893063a"&gt;
  &lt;main&gt;
    &lt;p&gt;Now on the front page of Hacker News ‚Äî join the discussion.&lt;/p&gt;
    &lt;p&gt;In a recent post, we introduced the Tau¬≤ benchmark, a framework for benchmaring LLMs. Today we‚Äôre sharing a surprising discovery we made while using it: a simple prompt rewrite boosted a small model‚Äôs success rate by over 20%. This post is a deep-dive on how we found and fixed this performance bottleneck by making subtle changes to agent policies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking LLMs with Tau¬≤&lt;/head&gt;
    &lt;p&gt;On the recent OpenAI Summer Update, we have seen that GPT-5 model has made significant strides in agentic tasks. To validate these claims, they‚Äôve turned to the Tau¬≤ benchmark, which simulates real-world agent interactions across various domains like telecom, retail, and airlines.&lt;/p&gt;
    &lt;p&gt;Before moving any further, we have to establish that GPT-5 showed significant improvement only in one benchmark domain - which is Telecom. The other ones have been somehow overlooked during model presentation - therefore we won‚Äôt bother about them either (üòâ).&lt;/p&gt;
    &lt;p&gt;In agentic interactions, accuracy is non-negotiable, but model speed is equally vital for user experience. Therefore, it makes sense to consider alternatives to flagship models, such as the recently introduced GPT-5-mini.&lt;/p&gt;
    &lt;p&gt;GPT-5-mini offers significant advantages: it‚Äôs roughly twice as fast in latency and noticeably more efficient in throughput. While delivering 85‚Äì95% of the full GPT-5‚Äôs performance, it is also five times cheaper.&lt;/p&gt;
    &lt;p&gt;Therefore, we ran an experiment to explore two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How well GPT-5-mini performs on this benchmark.&lt;/item&gt;
      &lt;item&gt;Whether we can improve its results by making subtle changes to the domain, such as modifying agent policies or task descriptions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Baseline: Expect GPT-5-mini to Fail 45% of the Time&lt;/head&gt;
    &lt;p&gt;Firstly, we‚Äôre going to establish the benchmark for the GPT-5-mini model. As the telecom benchmark contains over 100 tests, we‚Äôll use their subset. Luckily, the telecom_small task set comes in handy with just 20 test scenarios.&lt;/p&gt;
    &lt;p&gt;Running the benchmark with:&lt;/p&gt;
    &lt;code&gt;tau2 run \
    --domain telecom \
    --agent-llm gpt-5-mini \
    --user-llm gpt-5-mini \
    --num-trials 2 --task-set-name telecom_small&lt;/code&gt;
    &lt;p&gt;Our results are:&lt;/p&gt;
    &lt;p&gt;We ended up running 40 simulations:&lt;/p&gt;
    &lt;p&gt;The initial success rate was low: just 55%. The GPT-5-mini with its limited reasoning capabilities doesn‚Äôt even get close to flagship GPT-5.&lt;/p&gt;
    &lt;p&gt;There‚Äôs an additional interesting metric this benchmark has introduced, which is pass^k. This measures how well an agent can perform when it‚Äôs challenged with the same task k times. I like to think of it as the reliability of the AI Agent.&lt;lb/&gt; Another intriguing aspect of this benchmark are tasks which failed for all given trials - which could imply that the AI Agent is simply not capable of handling at all. This can happen due to multiple factors - reasoning might be too difficult, user ask could not be specific enough, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hack: Using Claude to Rewrite Prompts for GPT-5-mini&lt;/head&gt;
    &lt;p&gt;When hacking the AI Agent for &lt;code&gt;GPT-5-mini&lt;/code&gt; model we aimed to answer three questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can I improve the overall success rate when using this limited model?&lt;/item&gt;
      &lt;item&gt;Are we able to ‚Äúunlock‚Äù more tasks which agents are capable of handling?&lt;/item&gt;
      &lt;item&gt;Does agent reliability improve?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beauty of generative AI is that we could offload lots of work and allow ourselves a little laziness. Therefore, we asked Claude to analyze the AI agent policies in the telecom domain, which are the building blocks for the agent prompt. From there, we asked specifically to assume that these policies are going to feed an AI Agent prompt, which will be using faster, non-flagship LLM. Output documents (&lt;code&gt;main_policy.md&lt;/code&gt; and &lt;code&gt;tech_support_manual.md&lt;/code&gt;) will have to be rephrased so that they are easier to follow for the agent and result in improving its behaviour.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the ‚Äúexecutive summary‚Äù of improvements it made:&lt;/p&gt;
    &lt;code&gt;I've created two AI-optimized versions of the telecom domain documentation:

Key Improvements Made:

1. Structure &amp;amp; Flow
   - Decision Trees: Clear branching logic with ‚îú‚îÄ‚îÄ and ‚îî‚îÄ‚îÄ notation
   - Sequential Steps: Numbered, ordered procedures instead of scattered explanations
   - Prerequisites: Explicit dependency checks before proceeding

2. AI Agent Optimizations
   - Tool Call Clarity: Exact function names and parameters
   - Binary Decisions: Clear yes/no conditions instead of ambiguous language
   - Error Handling: Specific failure conditions and next steps
   - Verification Steps: "Recheck" instructions after each fix

3. Cognitive Load Reduction
   - Reference Tables: Quick lookup for tools and purposes
   - Pattern Recognition: Common issue combinations and their solutions
   - Critical Reminders: Common AI mistakes section to prevent errors

4. Actionable Language
   - Removed verbose explanations mixed with instructions
   - Consolidated multiple documents' logic into single workflows
   - Used imperative commands: "Check X", "If Y then Z"
   - Added immediate verification steps&lt;/code&gt;
    &lt;p&gt;We‚Äôve moved from lengthy policy declarations to clear, directive instructions. Instead of wordy statements, the new style uses step-by-step logic, conditions, and patterns. For prompts, this works more like a checklist ‚Äî ‚ÄúCheck X ‚Üí If Y, then Z‚Äù ‚Äî rather than vague, descriptive policies.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Result: 22% Accuracy Boost and 50% Fewer Unsolvable Tasks&lt;/head&gt;
    &lt;p&gt;Let‚Äôs review what our improved AI agent results look like:&lt;/p&gt;
    &lt;p&gt;The new prompts led to a significant performance boost. Pass^k metrics surged:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;k=1 from 0.55 to 0.675 (a 22.73% improvement) ‚Üí In plain terms, GPT-5-mini now succeeds on 67.5% of tasks instead of 55%.&lt;/item&gt;
      &lt;item&gt;k=2 from 0.4 to 0.5 (a 25% improvement) ‚Üí Meaning retries became more effective too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For context, flagship GPT-5 scores ~97% on this benchmark, o3 comes in at 58%, and GPT-4.1 at 34%. With our optimized prompts, GPT-5-mini not only jumped well above its own baseline but also outperformed o3, landing much closer to GPT-5 than before.&lt;/p&gt;
    &lt;p&gt;The side-by-side comparison shows exactly where the gains came from. On the left side of the screen you‚Äôll see the ‚Äústock‚Äù AI agent results, on the right - our AI agent improved for GPT-5-mini.&lt;/p&gt;
    &lt;p&gt;The screenshot above outlines that with our updated prompts and policies, we managed to ‚Äúunlock‚Äù some of the tests which were previously always failing due to GPT-5-mini‚Äôs limited capabilities. Now there are only 3 tasks, which the agent didn‚Äôt manage to solve at all within the given 2 trials - compared to 6.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Takeaways for Your Own Models&lt;/head&gt;
    &lt;p&gt;This experiment shows that thoughtful prompt design can meaningfully boost the performance of smaller models like GPT-5-mini. By restructuring policies into clear, step-by-step instructions, we not only improved success rates but also ‚Äúunlocked‚Äù tasks that previously seemed unsolvable for the model.&lt;/p&gt;
    &lt;p&gt;The key was in simplifying language, reducing ambiguity, and breaking down reasoning into explicit, actionable steps. Smaller models struggle with long-winded or fuzzy policies, but thrive when given structured flows, binary decisions, and lightweight verification steps.&lt;/p&gt;
    &lt;p&gt;The takeaway is clear: using a frontier model to automatically optimize prompts can unlock major improvements for smaller LLMs. With strategic optimization, lightweight models can deliver decent results at a fraction of the cost ‚Äî making them a compelling alternative when efficiency and affordability matter as much as accuracy.&lt;/p&gt;
    &lt;p&gt;If you found this helpful, let us know! Prompt engineering is still an open playground, and we‚Äôre excited to see what creative approaches others are exploring in this space.&lt;/p&gt;
    &lt;p&gt;Discuss it on LinkedIn, X or Hacker News.&lt;/p&gt;
    &lt;p&gt;UPDATE: Since publishing this post and hitting the front page of HN, some readers expressed interest in seeing the actual before and after policies (which are building block for the agent prompt). Initially I thought these would be too lengthy for the article and no one would care, but since there‚Äôs interest, I‚Äôm happy to share them in this Pull Request.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45275354</guid><pubDate>Wed, 17 Sep 2025 13:03:24 +0000</pubDate></item><item><title>UUIDv47: Store UUIDv7 in DB, emit UUIDv4 outside (SipHash-masked timestamp)</title><link>https://github.com/stateless-me/uuidv47</link><description>&lt;doc fingerprint="2c041edee4ad24ea"&gt;
  &lt;main&gt;
    &lt;p&gt;uuidv47 lets you store sortable UUIDv7 in your database while emitting a UUIDv4-looking fa√ßade at your API boundary. It does this by XOR-masking only the UUIDv7 timestamp field with a keyed SipHash-2-4 stream tied to the UUID‚Äôs own random bits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Header-only C (C89) ¬∑ zero deps&lt;/item&gt;
      &lt;item&gt;Deterministic, invertible mapping (exact round-trip)&lt;/item&gt;
      &lt;item&gt;RFC-compatible version/variant bits (v7 in DB, v4 on the wire)&lt;/item&gt;
      &lt;item&gt;Key-recovery resistant (SipHash-2-4, 128-bit key)&lt;/item&gt;
      &lt;item&gt;Full tests provided&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why&lt;/item&gt;
      &lt;item&gt;Quick start&lt;/item&gt;
      &lt;item&gt;Public API&lt;/item&gt;
      &lt;item&gt;Specification &lt;list rend="ul"&gt;&lt;item&gt;UUIDv7 bit layout&lt;/item&gt;&lt;item&gt;Fa√ßade mapping (v7 ‚Üî v4)&lt;/item&gt;&lt;item&gt;SipHash message derived from random&lt;/item&gt;&lt;item&gt;Invertibility&lt;/item&gt;&lt;item&gt;Collision analysis&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Security model&lt;/item&gt;
      &lt;item&gt;Build, test, coverage&lt;/item&gt;
      &lt;item&gt;Integration tips&lt;/item&gt;
      &lt;item&gt;Performance notes&lt;/item&gt;
      &lt;item&gt;FAQ&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DB-friendly: UUIDv7 is time-ordered ‚Üí better index locality &amp;amp; pagination.&lt;/item&gt;
      &lt;item&gt;Externally neutral: The fa√ßade hides timing patterns and looks like v4 to clients/systems.&lt;/item&gt;
      &lt;item&gt;Secret safety: Uses a PRF (SipHash-2-4). Non-crypto hashes are not suitable when the key must not leak.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include "uuidv47.h"

int main(void){
  const char* s = "00000000-0000-7000-8000-000000000000";
  uuid128_t v7;
  if (!uuid_parse(s, &amp;amp;v7)) return 1;
  uuidv47_key_t key = { .k0 = 0x0123456789abcdefULL, .k1 = 0xfedcba9876543210ULL };
  uuid128_t facade = uuidv47_encode_v4facade(v7, key);
  uuid128_t back = uuidv47_decode_v4facade(facade, key);

  char a[37], b[37], c[37];
  uuid_format(&amp;amp;v7, a);
  uuid_format(&amp;amp;facade, b);
  uuid_format(&amp;amp;back, c);
  printf("v7 (DB) : %s\n", a);
  printf("v4 (API): %s\n", b);
  printf("back    : %s\n", c);
}&lt;/code&gt;
    &lt;p&gt;Build &amp;amp; run with the provided Makefile: make test make coverage sudo make install&lt;/p&gt;
    &lt;code&gt;typedef struct { uint8_t  b[16]; } uuid128_t;
typedef struct { uint64_t k0, k1; } uuidv47_key_t;

uuid128_t uuidv47_encode_v4facade(uuid128_t v7, uuidv47_key_t key);
uuid128_t uuidv47_decode_v4facade(uuid128_t v4_facade, uuidv47_key_t key);
int  uuid_version(const uuid128_t* u);
void set_version(uuid128_t* u, int ver);
void set_variant_rfc4122(uuid128_t* u);
bool uuid_parse (const char* str, uuid128_t* out);
void uuid_format(const uuid128_t* u, char out[37]);&lt;/code&gt;
    &lt;p&gt;UUIDv7 bit layout:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ts_ms_be: 48-bit big-endian timestamp&lt;/item&gt;
      &lt;item&gt;ver: high nibble of byte 6 = 0x7 (v7) or 0x4 (fa√ßade)&lt;/item&gt;
      &lt;item&gt;rand_a: 12 random bits&lt;/item&gt;
      &lt;item&gt;var: RFC variant (0b10)&lt;/item&gt;
      &lt;item&gt;rand_b: 62 random bits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fa√ßade mapping:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encode: ts48 ^ mask48(R), set version=4&lt;/item&gt;
      &lt;item&gt;Decode: encTS ^ mask48(R), set version=7&lt;/item&gt;
      &lt;item&gt;Random bits unchanged&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SipHash input: 10 bytes from random field: msg[0] = (byte6 &amp;amp; 0x0F) msg[1] = byte7 msg[2] = (byte8 &amp;amp; 0x3F) msg[3..9] = bytes9..15&lt;/p&gt;
    &lt;p&gt;Invertibility: XOR mask is reversible with known key.&lt;/p&gt;
    &lt;p&gt;Collision analysis: Injective mapping. Only risk is duplicate randoms per ms.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Goal: Secret key unrecoverable even with chosen inputs.&lt;/item&gt;
      &lt;item&gt;Achieved: SipHash-2-4 is a keyed PRF.&lt;/item&gt;
      &lt;item&gt;Keys: 128-bit. Derive via HKDF.&lt;/item&gt;
      &lt;item&gt;Rotation: store small key ID outside UUID.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;make test
make coverage
make debug
sudo make install
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Do encode/decode at API boundary.&lt;/item&gt;
      &lt;item&gt;For Postgres, write tiny C extension.&lt;/item&gt;
      &lt;item&gt;For sharding, hash v4 fa√ßade with xxh3 or SipHash.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SipHash-2-4 on 10-byte message is extremely fast. No allocations.&lt;/p&gt;
    &lt;p&gt;Q: Why not xxHash with a secret? A: Not a PRF; secret can leak. Use SipHash.&lt;/p&gt;
    &lt;p&gt;Q: Is fa√ßade indistinguishable from v4? A: Yes, variable bits uniform, version/variant set to v4.&lt;/p&gt;
    &lt;p&gt;MIT, Copyright (c) 2025 Stateless Limited&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45275973</guid><pubDate>Wed, 17 Sep 2025 14:02:29 +0000</pubDate></item><item><title>YouTube addresses lower view counts which seem to be caused by ad blockers</title><link>https://9to5google.com/2025/09/16/youtube-lower-view-counts-ad-blockers/</link><description>&lt;doc fingerprint="7c588c5fb891325b"&gt;
  &lt;main&gt;
    &lt;p&gt;Over the past month or so, many YouTubers have been reporting major drops to their video view counts. Theories have run wild, but there‚Äôs one explanation involving ad blockers that makes the most sense, but YouTube isn‚Äôt confirming anything directly.&lt;/p&gt;
    &lt;p&gt;Since mid-August, many YouTubers have noticed their view counts are considerably lower than they were before, in some cases with very drastic drops. The reason for the drop, though, has been shrouded in mystery for many creators.&lt;/p&gt;
    &lt;p&gt;The most likely explanation seems to be that YouTube is not counting views properly for users with an ad blocker enabled, another step in the platform‚Äôs continued war on ad blockers. This was first realized by Josh Strife Hayes, who noticed that view counts on TV, phones, and tablets have been steady, while views on computers have dropped by around 50% since the mid-August trend started. TechLinked, a channel in the Linus Tech Tips family, confirmed similar numbers within its statistics.&lt;/p&gt;
    &lt;p&gt;This aligns with one of the possible explanations that YouTube itself hinted at in an acknowledgement of lower view counts.&lt;/p&gt;
    &lt;p&gt;Google says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Viewers Using Ad Blockers &amp;amp; Other Content Blocking Tools: Ad blockers and other extensions can impact the accuracy of reported view counts. Channels whose audiences include a higher proportion of users utilizing such tools may see more fluctuations in traffic related to updates to these tools.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Top comment by Napalmxxx2&lt;/head&gt;
    &lt;p&gt;We wouldn't need ad blockers if we weren't force fed irrelevant to us ads, unskippable ads, and the worst offender; ads that are LONGER than the video being watched.&lt;/p&gt;
    &lt;p&gt;We wouldn't feel the necessity if we weren't intruded on by a monopoly out for its own self interest.&lt;/p&gt;
    &lt;p&gt;The rest of the post addresses prior speculation that YouTube‚Äôs new AI-powered age verification tools were to blame ‚Äì which YouTube adamantly says is not the case ‚Äì while also offering other possible explanations such as ‚Äúseasonal viewing habits‚Äù and competition on the platform.&lt;/p&gt;
    &lt;p&gt;YouTube says ‚Äúthere is no systemic issue that is impacting creators‚Äù regarding lower view counts.&lt;/p&gt;
    &lt;p&gt;This ad blocker situation does seem the most likely explanation, though. In a prior video, Linus Tech Tips had noted that while view counts were down, ad revenue was not. If computer views are the only ones down, it stands to reason that viewers using an ad blocker are not being counted correctly, especially if ad revenue isn‚Äôt taking a hit from the lower view counts. YouTube‚Äôs hint that ad blockers ‚Äúcan impact the accuracy of reported view counts‚Äù certainly suggests this is possible, even if it‚Äôs not firm confirmation.&lt;/p&gt;
    &lt;head rend="h2"&gt;More on YouTube:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;YouTube for Android TV, Google TV will now let you test new features in beta&lt;/item&gt;
      &lt;item&gt;YouTube recommending awful videos? Here‚Äôs how to fix that&lt;/item&gt;
      &lt;item&gt;YouTube rolls out new profanity guidelines for creators&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Follow Ben: Twitter/X, Threads, Bluesky, and Instagram&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45276262</guid><pubDate>Wed, 17 Sep 2025 14:29:10 +0000</pubDate></item><item><title>Launch HN: RunRL (YC X25) ‚Äì Reinforcement learning as a service</title><link>https://runrl.com</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45277704</guid><pubDate>Wed, 17 Sep 2025 16:13:00 +0000</pubDate></item><item><title>Ton Roosendaal to step down as Blender chairman and CEO</title><link>https://www.cgchannel.com/2025/09/ton-roosendaal-to-step-down-as-blender-chairman-and-ceo/</link><description>&lt;doc fingerprint="e995505b9976c37f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ton Roosendaal to step down as Blender chairman and CEO&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; Ton Roosendaal is to stop down as chairman and Blender CEO on 1 January 2026. The news was announced during today‚Äôs keynote at the annual Blender Conference.&lt;/p&gt;
    &lt;p&gt;Roosendaal ‚Äì the original author of the open-source 3D software, and its public figurehead for the past three decades ‚Äì will pass on his roles to current Blender COO Francesco Siddi.&lt;/p&gt;
    &lt;p&gt;Roosendaal himself will move to the newly established Blender Foundation supervisory board.&lt;/p&gt;
    &lt;p&gt;Other new Blender Foundation board positions will also include Sergey Sharybin (Head of Development), Dalai Felinto (Head of Product) and Fiona Cohen (Head of Operations).&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôve been preparing for this since 2019,‚Äù said Roosendaal, ‚ÄúI am very proud to have such a wonderfully talented young team around me to bring our free and open source project into the next decade.‚Äù&lt;/p&gt;
    &lt;p&gt;We aim to update this story with a brief retrospective of Ton‚Äôs time as Blender CEO and the growth of Blender during that time, so check back for updates.&lt;/p&gt;
    &lt;p&gt;Read the official announcement that Ton Roosendaal is stepping down as Blender CEO&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45278279</guid><pubDate>Wed, 17 Sep 2025 16:49:37 +0000</pubDate></item><item><title>Event Horizon Labs (YC W24) Is Hiring</title><link>https://www.ycombinator.com/companies/event-horizon-labs/jobs/U6oyyKZ-founding-engineer-at-event-horizon-labs</link><description>&lt;doc fingerprint="a34a90db072aed8a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;At Event Horizon Labs, we‚Äôre building an autonomous system that turns raw compute and data directly into alpha, executing without the friction or bias of human emotion.&lt;/p&gt;
      &lt;head rend="h3"&gt;What You‚Äôll Build&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;High-Concurrency Pipelines&lt;lb/&gt; Architect infrastructure that seamlessly handles thousands of concurrent requests, ensuring consistent performance even at peak load.&lt;/item&gt;
        &lt;item&gt;Robust, Fault-Tolerant Systems&lt;lb/&gt; Build reliable frameworks that gracefully manage the complexity of trading ‚Äî where resilience and uptime are paramount.&lt;/item&gt;
        &lt;item&gt;Foundational Technology&lt;lb/&gt; As part of our founding team, you‚Äôll have the autonomy to shape core infrastructure decisions and lay the groundwork for future engineers to build upon.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What We‚Äôre Looking For&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Top-Tier Systems Design Expertise&lt;lb/&gt; Deep experience with distributed systems, networking, and the art of optimizing database performance at scale.&lt;/item&gt;
        &lt;item&gt;Containerization &amp;amp; Cloud Mastery&lt;lb/&gt; Strong proficiency in Docker, Kubernetes, and modern cloud environments to deploy, monitor, and scale AI infrastructure globally.&lt;/item&gt;
        &lt;item&gt;Proven Record of Scaling &amp;amp; Performance&lt;lb/&gt; Demonstrated ability to push databases and systems to their limits‚Äîexperience with high concurrency, complex query optimization, and near real-time data processing.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Why EHL?&lt;/head&gt;
      &lt;p&gt;We‚Äôre a small, high-caliber group of engineers and researchers obsessed with pushing the boundaries of AI systems. If you‚Äôve been honing your skills at the top echelons of programming, research, or system design‚Äîand want to tackle some of the hardest problems in AI infrastructure‚Äîthis is the team where you can truly stand out.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45278424</guid><pubDate>Wed, 17 Sep 2025 17:00:05 +0000</pubDate></item><item><title>Drought in Iraq reveals tombs created 2,300 years ago</title><link>https://www.smithsonianmag.com/smart-news/severe-droughts-in-iraq-reveals-dozens-of-ancient-tombs-created-2300-years-ago-180987347/</link><description>&lt;doc fingerprint="c772991916f40645"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Severe Drought in Iraq Reveals Dozens of Ancient Tombs Created 2,300 Years Ago&lt;/head&gt;
    &lt;head rend="h2"&gt;The tombs, which likely date to Iraq‚Äôs Hellenistic period, were discovered along the Mosul Dam reservoir&lt;/head&gt;
    &lt;p&gt;Archaeologists have discovered 40 tombs in Iraq after severe droughts lowered water levels in the country‚Äôs largest reservoir, Agence France-Presse (AFP) reports.&lt;/p&gt;
    &lt;p&gt;The tombs were discovered along the Mosul Dam reservoir in northern Iraq‚Äôs Duhok Province. They are thought to be more than 2,300 years old, likely from the Hellenistic period, when Iraq was under the rule of the Seleucid Empire.&lt;/p&gt;
    &lt;head rend="h4"&gt;Quick fact: The reach of the Seleucid Empire&lt;/head&gt;
    &lt;p&gt;At its peak, the empire stretched from what is now Turkey to the edges of India.&lt;/p&gt;
    &lt;p&gt;A team of archaeologists had surveyed the same area in 2023, also when water levels were low. However, they only spotted parts of the tombs.&lt;/p&gt;
    &lt;p&gt;This time, conditions were different. Iraq is in the midst of one of its driest years on record in nearly a century, according to Al Jazeera. Severe water shortages are threatening the country‚Äôs health and safety as water reserves have dwindled to just 8 percent of their full capacity.&lt;/p&gt;
    &lt;p&gt;When water levels in the Mosul Dam reservoir dropped ‚Äúto their lowest,‚Äù archaeologists were able to return and access the tombs, Bekas Brefkany, leader of the site‚Äôs archaeological work, tells AFP.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhile the receding waters have had negative impacts, for us as archaeologists, they have also provided an opportunity to rediscover areas that had long been hidden underwater,‚Äù Brefkany explains to the Art Newspaper‚Äôs Hadani Ditmars.&lt;/p&gt;
    &lt;p&gt;Brefkany and his team are racing to excavate the tombs before the dam is flooded again. What they dig up will be transferred to the Duhok Museum for study and preservation. As Brefkany tells the Art Newspaper, he hopes that researchers will eventually be able to ‚Äúshed more light on causes of death, family ties and the broader social context of these burials.‚Äù&lt;/p&gt;
    &lt;p&gt;Iraq has contended with numerous droughts in recent years. They have brought regions of the country to the brink of humanitarian disaster, which experts warn will worsen if the government doesn‚Äôt intervene, per Al Jazeera. According to the United Nations, Iraq is one of the countries ‚Äúmost vulnerable to the impact of climate change‚Äù due to rising temperatures, frequent sand and dust storms, and water scarcity.&lt;/p&gt;
    &lt;p&gt;But the droughts have also led to archaeological discoveries across the country. In 2022, ruins of a 3,400-year-old city were found in a dried-up section of the Mosul Dam reservoir along the Tigris River. As water levels continue to drop, the archaeologists expect to unearth more artifacts, Nazim Zibari, an archaeologist at the site, tells France 24.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis work that we are doing now is very difficult, complex and it takes time,‚Äù Zibari says.&lt;/p&gt;
    &lt;p&gt;Earlier this year, archaeologists found a rare stone relief among the ruins of ancient Assyrian metropolis Nineveh in Mosul, a city in northern Iraq. The artifact depicts important deities alongside Ashurbanipal, the last great king of the Assyrian Empire.&lt;/p&gt;
    &lt;p&gt;Editors‚Äô note, September 17, 2025: This article has been edited to correct a quote that should have been attributed to France 24.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45278581</guid><pubDate>Wed, 17 Sep 2025 17:12:15 +0000</pubDate></item><item><title>Tinycolor supply chain attack post-mortem</title><link>https://sigh.dev/posts/ctrl-tinycolor-post-mortem/</link><description>&lt;doc fingerprint="155065c78032ffc7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;@ctrl/tinycolor Supply Chain Attack Post-mortem&lt;/head&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;A malicious GitHub Actions workflow was pushed to a shared repo and exfiltrated a npm token with broad publish rights. The attacker then used that token to publish malicious versions of 20 packages, including &lt;code&gt;@ctrl/tinycolor&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;My GitHub account, the @ctrl/tinycolor repository were not directly compromised. There was no phishing involved, and no malicious packages were installed on my machine and I already use pnpm to avoid unapproved postinstall scripts. There was no pull request involved because a repo admin does not need a pull request to add new github actions.&lt;/p&gt;
    &lt;p&gt;GitHub/npm security responded quickly, unpublishing the malicious versions. I followed by releasing clean versions to flush caches, as advised.&lt;/p&gt;
    &lt;p&gt;For broader context, see Socket‚Äôs write-up or StepSecurity‚Äôs analysis. For community discussion, see this Hacker News post, which spent 24 hours on the front page. I‚Äôm also finding this wiz.io post helpful.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I Found Out&lt;/head&gt;
    &lt;p&gt;On September 15 around 4:30 PM PT, Wes Todd DM‚Äôd me on Bluesky and looped me into the OpenJS Foundation Slack. By that point, Wes had already alerted GitHub/npm security, who were compiling lists of affected packages and rapidly unpublishing compromised versions.&lt;/p&gt;
    &lt;p&gt;Early guidance (attributed to Daniel Pereira) was to look for suspicious &lt;code&gt;Shai-Hulud&lt;/code&gt; repos or branches. I wasn‚Äôt able to find any of these repos or branches on my own personal repos. The mystery was: how was I impacted at all?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Shai-Hulud was the Fremen term for the sandworm of Arrakis. - dune wiki&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;What Actually Happened&lt;/head&gt;
    &lt;p&gt;A while ago, I collaborated on angulartics2, a shared repository where multiple people still had admin rights. That repo still contained a GitHub Actions secret ‚Äî a npm token with broad publish rights. This collaborator had access to projects with other people which I believe explains some of the other 40 initial packages that were affected.&lt;/p&gt;
    &lt;p&gt;A new Shai-Hulud branch was force pushed to angulartics2 with a malicious github action workflow by a collaborator. The workflow ran immediately on push (did not need review since the collaborator is an admin) and stole the npm token. With the stolen token, the attacker published malicious versions of 20 packages. Many of which are not widely used, however the @ctrl/tinycolor package is downloaded about 2 million times a week.&lt;/p&gt;
    &lt;p&gt;GitHub and npm security teams moved quickly to unpublish the malicious versions. I then re-published fresh, verified versions of the packages I maintain to flush caches and restore trust.&lt;/p&gt;
    &lt;head rend="h2"&gt;Impact&lt;/head&gt;
    &lt;p&gt;Malicious versions of several packages ‚Äî including @ctrl/tinycolor ‚Äî were briefly available on npm before removal. Installing those compromised versions would have triggered a postinstall payload, which is documented in detail by StepSecurity.&lt;/p&gt;
    &lt;p&gt;What should you do if you‚Äôve installed a compromised version of a package? see StepSecurity‚Äôs immediate actions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Publishing Setup &amp;amp; Interim Plan&lt;/head&gt;
    &lt;p&gt;I currently use semantic-release with GitHub Actions to handle publishing. The automation is convenient and predictable. I also have npm provenance enabled on many packages, which provides attestations of how they were built. Unfortunately, provenance didn‚Äôt prevent this attack because the attacker had a valid token.&lt;/p&gt;
    &lt;p&gt;My goal is to move to npm‚Äôs Trusted Publishing (OIDC) to eliminate static tokens altogether. However, semantic-release integration is still in progress: npm/cli#8525.&lt;/p&gt;
    &lt;p&gt;For the forseeable future, @ctrl/tinycolor requires 2FA for publishing, and all tokens have been revoked. Not expecting to merge any new changes anytime soon.&lt;/p&gt;
    &lt;p&gt;For smaller packages, I‚Äôll continue using semantic-release but under stricter controls: no new contributors will be added, and each repo will use a granular npm token limited to publish-only rights for that specific package.&lt;/p&gt;
    &lt;p&gt;Local 2FA based publishing isn‚Äôt sustainable, so I‚Äôm watching OIDC/Trusted Publishing closely and will adopt it as soon as it fits the workflow.&lt;/p&gt;
    &lt;p&gt;I plan to continue using pnpm that prevents unapproved postinstall scripts from being run and I‚Äôll look into adding pnpm‚Äôs new minimumReleaseAge setting.&lt;/p&gt;
    &lt;head rend="h2"&gt;Publishing Wishlist&lt;/head&gt;
    &lt;p&gt;If I could wave a magic wand and design my ideal setup, npm would allow me to require Trusted Publishing (OIDC) with a single toggle for all of my packages. That same toggle would block any release missing provenance, enforcing security at the account level. I‚Äôd also want first-class semantic-release support with OIDC and provenance so no static tokens are ever needed.&lt;/p&gt;
    &lt;p&gt;On top of that, I‚Äôd like a secure, human-approved publishing option directly in the GitHub UI: a protected workflow_dispatch flow that uses github 2FA approval to satisfy 2FA, without requiring me to publish from my laptop.&lt;/p&gt;
    &lt;p&gt;GitHub Environments ‚Äî or equivalent workflow protections ‚Äî should be available without a Pro subscription, or else integrated directly into Trusted Publishing so that security doesn‚Äôt depend on the pricing tier.&lt;/p&gt;
    &lt;p&gt;It would be really nice if NPM also had a more visible mark on the package details page to indicate if the package had a postinstall script. Also, once the packages are pulled its not clear what versions were removed and why.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thanks&lt;/head&gt;
    &lt;p&gt;Thanks to Wes Todd, the OpenJS Foundation, and the GitHub/npm security teams for their rapid and coordinated response. Everyone was incredibly fast, helpful, and knowledgeable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45278657</guid><pubDate>Wed, 17 Sep 2025 17:18:38 +0000</pubDate></item><item><title>DeepSeek writes less secure code for groups China disfavors?</title><link>https://www.washingtonpost.com/technology/2025/09/16/deepseek-ai-security/</link><description>&lt;doc fingerprint="bfec9dba84e66b14"&gt;
  &lt;main&gt;
    &lt;p&gt;The Chinese artificial intelligence engine DeepSeek often refuses to help programmers or gives them code with major security flaws when they say they are working for the banned spiritual movement Falun Gong or others considered sensitive by the Chinese government, new research shows.&lt;/p&gt;
    &lt;p&gt;By Joseph Menn&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1OpinionGeorge F. WillA seamlessly unserious president&lt;/item&gt;
      &lt;item&gt;2Rachel RoubeinandMariana AlfaroFired CDC leader Susan Monarez told senators about pressure from RFK Jr.&lt;/item&gt;
      &lt;item&gt;3Rachel Roubein,Dan DiamondandMariana AlfaroTakeaways from fired CDC director‚Äôs Senate testimony&lt;/item&gt;
      &lt;item&gt;4Anahad O‚ÄôConnorWe asked dietitians for healthy high-protein meals. Here‚Äôs their list.&lt;/item&gt;
      &lt;item&gt;5Alyssa FowersandRachel LermanWhat the Fed rate cut means for your home, car and credit card loans&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45278740</guid><pubDate>Wed, 17 Sep 2025 17:24:14 +0000</pubDate></item><item><title>DeepMind and OpenAI win gold at ICPC</title><link>https://codeforces.com/blog/entry/146536</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45279357</guid><pubDate>Wed, 17 Sep 2025 18:15:16 +0000</pubDate></item><item><title>WASM 3.0 Completed</title><link>https://webassembly.org/news/2025-09-17-wasm-3.0/</link><description>&lt;doc fingerprint="f22a885b44e613e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Published on September 17, 2025 by Andreas Rossberg.&lt;/p&gt;
    &lt;p&gt;Three years ago, version 2.0 of the Wasm standard was (essentially) finished, which brought a number of new features, such as vector instructions, bulk memory operations, multiple return values, and simple reference types.&lt;/p&gt;
    &lt;p&gt;In the meantime, the Wasm W3C Community Group and Working Group have not been lazy. Today, we are happy to announce the release of Wasm 3.0 as the new ‚Äúlive‚Äù standard.&lt;/p&gt;
    &lt;p&gt;This is a substantially larger update: several big features, some of which have been in the making for six or eight years, finally made it over the finishing line.&lt;/p&gt;
    &lt;p&gt;64-bit address space. Memories and tables can now be declared to use &lt;code&gt;i64&lt;/code&gt; as their address type instead of just &lt;code&gt;i32&lt;/code&gt;. That expands the available address space of Wasm applications from 4 gigabytes to (theoretically) 16 exabytes, to the extent that physical hardware allows. While the web will necessarily keep enforcing certain limits ‚Äî on the web, a 64-bit memory is limited to 16 gigabytes ‚Äî the new flexibility is especially interesting for non-web ecosystems using Wasm, as they can support much, much larger applications and data sets now.&lt;/p&gt;
    &lt;p&gt;Multiple memories. Contrary to popular belief, Wasm applications were always able to use multiple memory objects ‚Äî and hence multiple address spaces ‚Äî simultaneously. However, previously that was only possible by declaring and accessing each of them in separate modules. This gap has been closed, a single module can now declare (define or import) multiple memories and directly access them, including directly copying data between them. This finally allows tools like wasm-merge, which perform ‚Äústatic linking‚Äù on two or more Wasm modules by merging them into one, to work for all Wasm modules. It also paves the way for new uses of separate address spaces, e.g., for security (separating private data), for buffering, or for instrumentation.&lt;/p&gt;
    &lt;p&gt;Garbage collection. In addition to expanding the capabilities of raw linear memories, Wasm also adds support for a new (and separate) form of storage that is automatically managed by the Wasm runtime via a garbage collector. Staying true to the spirit of Wasm as a low-level language, Wasm GC is low-level as well: a compiler targeting Wasm can declare the memory layout of its runtime data structures in terms of struct and array types, plus unboxed tagged integers, whose allocation and lifetime is then handled by Wasm. But that‚Äôs it. Everything else, such as engineering suitable representations for source-language values, including implementation details like method tables, remains the responsibility of compilers targeting Wasm. There are no built-in object systems, nor closures or other higher-level constructs ‚Äî which would inevitably be heavily biased towards specific languages. Instead, Wasm only provides the basic building blocks for representing such constructs and focuses purely on the memory management aspect.&lt;/p&gt;
    &lt;p&gt;Typed references. The GC extension is built upon a substantial extension to the Wasm type system, which now supports much richer forms of references. Reference types can now describe the exact shape of the referenced heap value, avoiding additional runtime checks that would otherwise be needed to ensure safety. This more expressive typing mechanism, including subtyping and type recursion, is also available for function references, making it possible to perform safe indirect function calls without any runtime type or bounds check, through the new &lt;code&gt;call_ref&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;Tail calls. Tail calls are a variant of function calls that immediately exit the current function, and thereby avoid taking up additional stack space. Tail calls are an important mechanism that is used in various language implementations both in user-visible ways (e.g., in functional languages) and for internal techniques (e.g., to implement stubs). Wasm tail calls are fully general and work for callees both selected statically (by function index) and dynamically (by reference or table).&lt;/p&gt;
    &lt;p&gt;Exception handling. Exceptions provide a way to locally abort execution, and are a common feature in modern programming languages. Previously, there was no efficient way to compile exception handling to Wasm, and existing compilers typically resorted to convoluted ways of implementing them by escaping to the host language, e.g., JavaScript. This was neither portable nor efficient. Wasm 3.0 hence provides native exception handling within Wasm. Exceptions are defined by declaring exception tags with associated payload data. As one would expect, an exception can be thrown, and selectively be caught by a surrounding handler, based on its tag. Exception handlers are a new form of block instruction that includes a dispatch list of tag/label pairs or catch-all labels to define where to jump when an exception occurs.&lt;/p&gt;
    &lt;p&gt;Relaxed vector instructions. Wasm 2.0 added a large set of vector (SIMD) instructions, but due to differences in hardware, some of these instructions have to do extra work on some platforms to achieve the specified semantics. In order to squeeze out maximum performance, Wasm 3.0 introduces ‚Äúrelaxed‚Äù variants of these instructions that are allowed to have implementation-dependent behavior in certain edge cases. This behavior must be selected from a pre-specified set of legal choices.&lt;/p&gt;
    &lt;p&gt;Deterministic profile. To make up for the added semantic fuzziness of relaxed vector instructions, and in order to support settings that demand or need deterministic execution semantics (such as blockchains, or replayable systems), the Wasm standard now specifies a deterministic default behavior for every instruction with otherwise non-deterministic results ‚Äî currently, this includes floating-point operators and their generated NaN values and the aforementioned relaxed vector instructions. Between platforms choosing to implement this deterministic execution profile, Wasm thereby is fully deterministic, reproducible, and portable.&lt;/p&gt;
    &lt;p&gt;Custom annotation syntax. Finally, the Wasm text format has been enriched with generic syntax for placing annotations in Wasm source code. Analogous to custom sections in the binary format, these annotations are not assigned any meaning by the Wasm standard itself, and can be chosen to be ignored by implementations. However, they provide a way to represent the information stored in custom sections in human-readable and writable form, and concrete annotations can be specified by downstream standards.&lt;/p&gt;
    &lt;p&gt;In addition to these core features, embeddings of Wasm into JavaScript benefit from a new extension to the JS API:&lt;/p&gt;
    &lt;p&gt;With these new features, Wasm has much better support for compiling high-level programming languages. Enabled by this, we have seen various new languages popping up to target Wasm, such as Java, OCaml, Scala, Kotlin, Scheme, or Dart, all of which use the new GC feature.&lt;/p&gt;
    &lt;p&gt;On top of all these goodies, Wasm 3.0 also is the first version of the standard that has been produced with the new SpecTec tool chain. We believe that this makes for an even more reliable specification.&lt;/p&gt;
    &lt;p&gt;Wasm 3.0 is already shipping in most major web browsers, and support in stand-alone engines like Wasmtime is on track to completion as well. The Wasm feature status page tracks support across engines.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45279384</guid><pubDate>Wed, 17 Sep 2025 18:16:53 +0000</pubDate></item><item><title>Optimizing ClickHouse for Intel's 280 core processors</title><link>https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu</link><description>&lt;doc fingerprint="fb5f1c762d32084a"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;This is a guest post from Jiebin Sun, Zhiguo Zhou, Wangyang Guo and Tianyou Li, performance optimization engineers at Intel Shanghai.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Intel's latest processor generations are pushing the number of cores in a server to unprecedented levels - from 128 P-cores per socket in Granite Rapids to 288 E-cores per socket in Sierra Forest, with future roadmaps targeting 200+ cores per socket. These numbers multiply on multi-socket systems, such servers may consist of 400 and more cores. The paradigm of "more, not faster cores" is driven by physical limitations. Since the end of Dennard scaling in the mid-2000s, power density concerns made it increasingly difficult to push single-thread performance further.&lt;/p&gt;&lt;p&gt;For analytical databases like ClickHouse, ultra-high core counts represent a huge opportunity and a complex challenge at the same time. While more cores theoretically mean more power to process tasks in parallel, most databases struggle to utilize the available hardware fully. Bottlenecks for parallel processing like lock contention, cache coherence, non-uniform memory access (NUMA), memory bandwidth, and coordination overhead become significantly worse as the core count increases.&lt;/p&gt;&lt;head rend="h2"&gt;Optimizing for ultra-high core counts #&lt;/head&gt;&lt;p&gt;Over the past three years, I dedicated a part of my professional life to understand and optimize ClickHouse's scalability on Intel Xeon ultra-high core count processors. My work focused on using various profiling and analysis tools - including perf, emon, and Intel VTune - to analyze all 43 ClickBench queries on ultra-high core count servers systematically, identifying bottlenecks, and optimizing the ClickHouse accordingly.&lt;/p&gt;&lt;p&gt;The results have been exciting: individual optimizations routinely deliver speedups of multiple times for individual queries, in some cases up to 10x. The geometric mean of all 43 ClickBench queries consistently improved between 2% and 10% per optimization. The results demonstrate that ClickHouse can be made scale very well on ultra-high core count systems.&lt;/p&gt;&lt;head rend="h2"&gt;The core scaling challenge #&lt;/head&gt;&lt;p&gt;Beyond single-thread performance, several key challenges must be addressed to optimize performance in ultra-high core count systems.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Cache coherence overhead: Bouncing cache lines costs CPU cycles.&lt;/item&gt;&lt;item&gt;Lock contention: Amdahl's Law becomes brutal for serialized code sections as little as 1% of the overall code.&lt;/item&gt;&lt;item&gt;Memory bandwidth: Utilizing the memory bandwidth effectively is a persistent challenge for data-intensive systems. Proper memory reuse, management and caching becomes critical.&lt;/item&gt;&lt;item&gt;Thread coordination: The cost of synchronizing threads grows super-linearly with the number of threads.&lt;/item&gt;&lt;item&gt;NUMA effects: The memory latency and bandwidth on multi-socket systems differs for local or remote memory.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This blog post summarizes our optimizations for ClickHouse on ultra-high core count servers. All of them were merged into the main codeline and they now help to speed up queries in ClickHouse deployments around the globe.&lt;/p&gt;&lt;p&gt;Hardware setup: Our work was conducted on Intel's latest generation platforms, including 2 x 80 vCPUs Ice Lake (ICX), 2 x 128 vCPUs Sapphire Rapids (SPR), 1 x 288 vCPUs Sierra Forest (SRF), and 2 x 240 vCPUs Granite Rapids (GNR). SMT (Hyper-threading) was enabled, except on SRF which doesn't support SMT, and high-memory-bandwidth configurations.&lt;/p&gt;&lt;p&gt;Software setup: We used perf, Intel VTune, pipeline visualization, and other custom profiling infrastructure.&lt;/p&gt;&lt;head rend="h2"&gt;The five optimization areas #&lt;/head&gt;&lt;p&gt;Through a systematic analysis of ClickHouse's performance on ultra-high core count systems, I identified five areas with a high potential for optimization. Each area addresses a different aspect of scalability, and together they form a comprehensive approach to unlocking the full potential of ultra-high core count systems.&lt;/p&gt;&lt;p&gt;My journey began with the most fundamental challenge: lock contention.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 1: Lock contention #&lt;/head&gt;&lt;p&gt;According to queue theory, if N threads compete for the same lock, the cycles grows quadratically (N^2). For example, if we go from 8 to 80 cores, lock wait times increase by (80/8)¬≤ = 100x. Furthermore, cache coherence traffic for the mutex itself grows linearly with the core count, and the overhead for context switching compounds the problem. In such settings, every mutex becomes a potential scalability obstacle, and seemingly innocent synchronization patterns can bring entire systems to their knee.&lt;/p&gt;&lt;p&gt;The key insight is that lock contention isn't just about removing locks - it's about rethinking more fundamentally how threads coordinate and share state. This requires a multi-pronged approach: reducing the duration of critical sections, replacing exclusive locks (mutexes) with more granular synchronization primitives, and in some cases, eliminating shared state entirely.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 1.1: Query condition cache (PR #80247) #&lt;/head&gt;&lt;p&gt;After resolving jemalloc page faults (an optimization detailed below), a new hotspot appeared in &lt;code&gt;native_queued_spin_lock_slowpath&lt;/code&gt; which consumed 76% of the CPU time. This function was called from &lt;code&gt;QueryConditionCache::write&lt;/code&gt; on 2√ó240 vCPU systems.&lt;/p&gt;&lt;p&gt;What is the query condition cache?&lt;/p&gt;&lt;p&gt;ClickHouse‚Äôs query condition cache stores the results of WHERE filters, enabling the database to skip irrelevant data. In each SELECT query, multiple threads check if cache entries must be updated based on different criteria:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;the hash of the filter condition (as cache key)&lt;/item&gt;&lt;item&gt;the read mark ranges&lt;/item&gt;&lt;item&gt;whether the currently read part has a final mark&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The query condition cache is read-heavy, i.e. there are far more reads than writes, but the original implementation used exclusive locking for all operations.&lt;/p&gt;&lt;p&gt;Reducing critical paths in read-heavy workloads&lt;/p&gt;&lt;p&gt;This optimization demonstrates the importance of reducing the time spent holding locks, especially write locks in read-heavy code.&lt;/p&gt;&lt;p&gt;With 240 threads within a single query, the original code created a perfect storm:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Unnecessary write locks: All threads acquired exclusive locks, even when they only read cache entries.&lt;/item&gt;&lt;item&gt;Long critical sections: Expensive updates of cache entries were performed inside exclusive locks.&lt;/item&gt;&lt;item&gt;Redundant work: Multiple threads updated the same cache entries potentially multiple times.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Our optimization uses double-checked locking with atomic operations to resolve these bottlenecks:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The code now first checks with atomic reads (no locking), respectively under a shared lock if an update is needed at all (fast path).&lt;/item&gt;&lt;item&gt;Next, the code checks immediately after acquiring an exclusive lock (slow path) if an update is actually required - another thread may have performed the same update in the meantime.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Implementation&lt;/p&gt;&lt;p&gt;Based on PR #80247, the optimization introduces a fast path which checks if an update is needed before acquiring the expensive write lock.&lt;/p&gt;&lt;code&gt;/// Original code
void updateCache(mark_ranges, has_final_mark)
{
    acquire_exclusive_lock(cache_mutex);  /// 240 threads wait here!

    /// Always update marks, even if already in desired state
    for (const auto &amp;amp; range : mark_ranges)
        set_marks_to_false(range.begin, range.end);

    if (has_final_mark):
        set_final_mark_to_false();

    release_lock(cache_mutex);
}
&lt;/code&gt;&lt;code&gt;
/// Optimized code
void updateCache(mark_ranges, has_final_mark)
{
    /// Fast path: Check if update is needed with a cheap shared lock
    acquire_shared_lock(cache_mutex);  /// Multiple threads can read simultaneously

    need_update = false;
    for (const auto &amp;amp; range : mark_ranges)
    {
        if (any_marks_are_true(range.begin, range.end))
        {
            need_update = true;
            break;
        }
    }

    if (has_final_mark &amp;amp;&amp;amp; final_mark_is_true())
        need_update = true;

    release_shared_lock(cache_mutex);

    if (!need_update)
        return;  /// Early out - no expensive lock needed!

    /// Slow path: Actually need to update, acquire exclusive lock
    acquire_exclusive_lock(cache_mutex);

    /// Double-check: verify update is still needed after acquiring lock
    need_update = false;
    for (const auto &amp;amp; range : mark_ranges)
    {
        if (any_marks_are_true(range.begin, range.end))
        {
            need_update = true;
            break;
        }
    }

    if (has_final_mark &amp;amp;&amp;amp; final_mark_is_true())
        need_update = true;

    if (need_update)
    {
        // Perform the actual updates only if still needed
        for (const auto &amp;amp; range : mark_ranges)
            set_marks_to_false(range.begin, range.end);

        if (has_final_mark)
            set_final_mark_to_false();
    }

    release_lock(cache_mutex);
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;The optimized code delivered impressive performance improvements:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;CPU cycles spend for &lt;code&gt;native_queued_spin_lock_slowpath&lt;/code&gt;reduced from 76% to 1%&lt;/item&gt;&lt;item&gt;The QPS of ClickBench queries Q10 and Q11 improved by 85% and 89%&lt;/item&gt;&lt;item&gt;The geometric mean of all ClickBench queries improved by 8.1%&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Optimization 1.2: Thread-Local Timer ID (PR #48778) #&lt;/head&gt;&lt;p&gt;ClickHouse's query profiler was frequently creating and deleting a global timer_id variable, causing lock contention during query profiling.&lt;/p&gt;&lt;p&gt;Query profiler timer usage&lt;/p&gt;&lt;p&gt;ClickHouse's query profiler uses POSIX timers to sample thread stacks in periodic intervals for performance analysis. The original implementation:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;created and deleted timer_id frequently during profiling, and&lt;/item&gt;&lt;item&gt;required global synchronization for all operations that read or write the timer.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Usage of shared data structures that needed protection with locks caused significant overhead.&lt;/p&gt;&lt;p&gt;Eliminating global state with thread-local storage&lt;/p&gt;&lt;p&gt;Here, we eliminated lock contention by thread-local storage, removing the need for shared state. Now, each thread has its own timer_id. This avoids shared state and the overhead of thread synchronization. To update a timer, it is no longer required to acquire locks.&lt;/p&gt;&lt;p&gt;Technical solution&lt;/p&gt;&lt;code&gt;/// Original code
class QueryProfiler
{
    static global_mutex timer_management_lock

    void startProfiling()
    {
        timer_id = create_new_timer();  /// Expensive system call

        acquire_exclusive_lock(timer_management_lock);  /// Global lock!
        update_shared_timer_state(timer_id);  /// Modify shared state
        release_lock(timer_management_lock);
    }

    void stopProfiling()
    {
        acquire_exclusive_lock(timer_management_lock);
        cleanup_shared_timer_state(timer_id);
        release_lock(timer_management_lock);

        delete_timer(timer_id);
    }
}
&lt;/code&gt;&lt;code&gt;/// Optimized code
class QueryProfiler
{
    static thread_local timer_id per_thread_timer;
    static thread_local boolean timer_initialized;

    void startProfiling()
    {
        if (!timer_initialized)
        {
            per_thread_timer = create_new_timer();  /// Once per thread
            timer_initialized = true;
        }

        /// Reuse existing timer - no locks, no system calls!
        enable_timer(per_thread_timer);
    }

    void stopProfiling()
    {
        /// Just disable timer - no deletion, no locks!
        disable_timer(per_thread_timer);
    }
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;The new implementation has the following advantages:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It eliminated timer-related lock contention hotspots from profiling traces&lt;/item&gt;&lt;item&gt;It reduced timer create/delete system calls through reuse&lt;/item&gt;&lt;item&gt;It makes profiling on ultra-high core count servers more scalable.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Thread-local storage can eliminate lock contention by removing the need for shared state. Global synchronization becomes unnecessary if threads maintain their own state.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 2: Memory management #&lt;/head&gt;&lt;p&gt;Memory optimization on ultra-high core count systems differs a lot from single-threaded memory management. Memory allocators themselves become contention points, memory bandwidth is divided across more cores, and allocation patterns that work fine on small systems can create cascading performance problems at scale. It is crucial to be mindful of how much memory is allocated and how memory is used.&lt;/p&gt;&lt;p&gt;This class of optimizations involves the allocator‚Äôs behavior, reducing pressure on memory bandwidth, and sometimes completely rethinking algorithms to eliminate memory-intensive operations altogether.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 2.1: Jemalloc Memory Reuse Optimization (PR #80245) #&lt;/head&gt;&lt;p&gt;This optimization is motivated by high page fault rates and excessive resident memory usage which we observed for certain aggregation queries on ultra-high core count systems.&lt;/p&gt;&lt;p&gt;Understanding two-level hash tables in ClickHouse&lt;/p&gt;&lt;p&gt;Aggregation in ClickHouse uses different hash tables, depending on the data type, data distribution and data size. Large aggregation states are maintained in ephemeral hash tables.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The 1st level consists of 256 static buckets, each pointing to a 2nd level hash table.&lt;/item&gt;&lt;item&gt;2nd level hash tables grow independently of each other.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Memory reuse for two-level hash tables&lt;/p&gt;&lt;p&gt;At the end of an aggregation query, all hash tables used by the query are deallocated. In particular, the 256 sub-hash tables are deallocated and their memory is merged into larger free memory blocks.&lt;/p&gt;&lt;p&gt;jemalloc (as ClickHouse‚Äôs memory allocator) unfortunately prevented the reuse of merged memory blocks for future smaller allocations. This is because by default, only memory from blocks up to 64x larger than the requested size can be reused. This issue in jemalloc is very subtle but critical on ultra-high core count systems.&lt;/p&gt;&lt;p&gt;Based on jemalloc issue #2842, we noticed a fundamental problem with jemalloc‚Äôs memory reuse for the irregularly-sized allocations typical in two-level hash tables:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Extent management issue: When large allocations are freed, jemalloc fails to efficiently track and reuse these memory extents.&lt;/item&gt;&lt;item&gt;Size class fragmentation: Memory gets trapped in size classes that don't match future allocation patterns.&lt;/item&gt;&lt;item&gt;Metadata overhead: Excessive metadata structures prevent efficient memory coalescing.&lt;/item&gt;&lt;item&gt;Page fault amplification: New allocations trigger page faults instead of reusing existing committed pages.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We identified jemalloc's &lt;code&gt;lg_extent_max_active_fit&lt;/code&gt; parameter as the root cause - it was too restrictive for ClickHouse's allocation patterns.&lt;/p&gt;&lt;p&gt;We contributed the fix to jemalloc PR #2842, but jemalloc didn‚Äôt have new stable releases for an extended period. Fortunately, we could resolve this issue through jemalloc's configuration parameters at compilation time.&lt;/p&gt;&lt;p&gt;Based on ClickHouse PR #80245, the fix involved tuning jemalloc's configuration parameters:&lt;/p&gt;&lt;code&gt;/// Original jemalloc configuration
JEMALLOC_CONFIG_MALLOC_CONF = "oversize_threshold:0,muzzy_decay_ms:0,dirty_decay_ms:5000"
/// lg_extent_max_active_fit defaults to 6, meaning memory can be reused from extents up to 64x larger than the requested allocation size
&lt;/code&gt;&lt;code&gt;/// Optimized jemalloc configuration
JEMALLOC_CONFIG_MALLOC_CONF = "oversize_threshold:0,muzzy_decay_ms:0,dirty_decay_ms:5000,lg_extent_max_active_fit:8"
/// lg_extent_max_active_fit is set to 8.
/// This allows memory reuse from extents up to 256x larger
/// than the requested allocation size (2^8 = 256x vs default 2^6 = 64x).
/// The 256x limit matches ClickHouse's two-level hash table structure (256 buckets).
/// This enables efficient reuse of merged hash table memory blocks.
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;The optimization improved&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;the performance of ClickBench query Q35 by 96.1%,&lt;/item&gt;&lt;item&gt;memory usage (VmRSS, resident memory) and page faults reduced for the same query went down by 45.4% and 71%, respectively.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The behavior of the memory allocator can have a dramatic impact on ultra-high core count systems.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 2.2: AST Query Rewriting for Memory Reduction (PR #57853) #&lt;/head&gt;&lt;p&gt;ClickBench query Q29 was memory-bound and bottlenecked in excessive memory accesses caused by redundant computations of the form &lt;code&gt;sum(column + literal)&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Understanding the memory bottleneck&lt;/p&gt;&lt;p&gt;ClickBench query Q29 contains multiple sum expressions with literals:&lt;/p&gt;&lt;code&gt;SELECT SUM(ResolutionWidth), SUM(ResolutionWidth + 1), SUM(ResolutionWidth + 2), 
       SUM(ResolutionWidth + 3), SUM(ResolutionWidth + 4), SUM(ResolutionWidth + 5), 
       SUM(ResolutionWidth + 6), SUM(ResolutionWidth + 7), SUM(ResolutionWidth + 8), 
       SUM(ResolutionWidth + 9), SUM(ResolutionWidth + 10), SUM(ResolutionWidth + 11), 
       SUM(ResolutionWidth + 12), SUM(ResolutionWidth + 13), SUM(ResolutionWidth + 14), 
       SUM(ResolutionWidth + 15), SUM(ResolutionWidth + 16), SUM(ResolutionWidth + 17), 
       SUM(ResolutionWidth + 18), SUM(ResolutionWidth + 19), SUM(ResolutionWidth + 20),
       -- ... continues up to SUM(ResolutionWidth + 89)
FROM hits;
&lt;/code&gt;&lt;p&gt;The original query execution&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Loaded column ‚ÄúResolutionWidth‚Äù from storage once,&lt;/item&gt;&lt;item&gt;Compute expressions - 90 times, creating 90 temporary columns (one per expression),&lt;/item&gt;&lt;item&gt;Sum values performing 90 separate aggregation operations on each computed column.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Creating 90 temporary columns and running 90 redundant aggregations obviously created massive memory pressure.&lt;/p&gt;&lt;p&gt;Frontend query optimization for memory efficiency&lt;/p&gt;&lt;p&gt;This optimization demonstrates how better optimizer rules can reduce memory pressure by eliminating redundant computations. The key insight is that many analytical queries contain patterns that can be algebraically simplified.&lt;/p&gt;&lt;p&gt;The optimization recognizes that &lt;code&gt;sum(column + literal)&lt;/code&gt; can be rewritten to &lt;code&gt;sum(column) + count(column) * literal&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ClickBench query Q29 sped up by 11.5x on a 2√ó80 vCPU system.&lt;/item&gt;&lt;item&gt;The geometric mean of all ClickBench queries saw a 5.3% improvement overall.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;More intelligent query plans can be more effective than optimizing execution itself. Avoiding work is better than doing work efficiently.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 3: Increase parallelism #&lt;/head&gt;&lt;p&gt;Fast aggregation is a core promise of any analytical database. From a database perspective, aggregating data in parallel threads is only one part of the equation. It is equally important to merge the local results in parallel.&lt;/p&gt;&lt;p&gt;ClickHouse's aggregation operator has two phases: In the first phase, each thread processes its portion of the data in parallel, creating a local and partial result. In the second phase, all partial results must be merged. If the merge phase is not properly parallelized, it becomes a bottleneck. More threads can actually make this issue worse by creating more partial results to merge.&lt;/p&gt;&lt;p&gt;Solving this issue requires careful algorithm design, smart data structure choices, and a deep understanding how hash tables behave under different load patterns. The goal is to eliminate the serial merge phase and enable linear scaling even for the most complex aggregation queries.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 3.1: Hash Table Conversion (PR #50748) #&lt;/head&gt;&lt;p&gt;ClickBench query Q5 showed a severe performance degradation as the core count increased from 80 to 112 threads. Our pipeline analysis revealed serial processing in the hash table conversion.&lt;/p&gt;&lt;p&gt;Understanding hash tables in ClickHouse&lt;/p&gt;&lt;p&gt;ClickHouse uses two types of hash tables for hash aggregation:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Single-level hash tables: This is a flat hash table that is suitable (= faster) for smaller datasets.&lt;/item&gt;&lt;item&gt;Two-level hash tables: This is a hierarchical hash table with 256 buckets. Two-level hash tables are more amendable to large datasets.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The database chooses the right hash table type based on the size of the processed data: Once a single-level hash table reaches a certain threshold during aggregation, it is automatically converted to a two-level hash table. The code to merge hash tables of different types was serialized.&lt;/p&gt;&lt;p&gt;The serial bottleneck&lt;/p&gt;&lt;p&gt;When merging hash tables from different threads,&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;single-level hash tables were serially merged in a pair-wise manner, e.g. ht1 / ht2 ‚Üí result, then result / ht3, etc.&lt;/item&gt;&lt;item&gt;two-level hash tables are merged one-by-one as well but the merge is parallelized across buckets.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In the case of mixed single/two-level hash tables, the single-level hash tables had to be converted to two-level hash tables first (this was a serial process). Once the was done, the resulting two-level hash tables could be merged in parallel.&lt;/p&gt;&lt;p&gt;With Q5, increasing the number of threads from 80 to 112 meant that each thread processes less data. With 80 threads, all hash tables were two-level. With 112 threads, the aggregation ended up with the mixed scenario: some hash tables remained single-level while others became two-level. This caused serialization - all single-level hash tables had to be converted to two-level before parallel merging could take place.&lt;/p&gt;&lt;p&gt;To diagnose the issue, pipeline visualization was a crucial tool. The telltale sign was that the merge phase duration increased with thread count - this is the opposite of what should happen.&lt;/p&gt;&lt;p&gt;Performance degradation with increased core count&lt;/p&gt;Pipeline visualization (max_threads=80) - the merge phase is reasonable Pipeline visualization (max_threads=112) - the merge phase takes 3.2x longer&lt;p&gt;Our optimization parallelizes the conversion phase: instead of converting all single-level hash tables to two-level hash tables one by one (serially), we now convert them in parallel. As each hash table can be converted independently, this eliminates the serial bottleneck.&lt;/p&gt;&lt;code&gt;/// Original code
void mergeHashTable(left_table, right_table)
{
    if (left_table.is_single_level() &amp;amp;&amp;amp; right_table.is_two_level())    
        left_table.convert_to_two_level();  /// Serial conversion blocks threads

    /// Now merge
    merge_sets(left_table, right_table);
}
&lt;/code&gt;&lt;code&gt;/// Optimized code
void mergeHashTableParallel(all_tables)
{
    /// Phase 1: Parallel conversion
    parallel_tasks = [];
    for (const auto &amp;amp; table : all_tables)
    {
        if (table.is_single_level())
        {
            /// Parallel conversion!
            task = create_parallel_task(table.convert_to_two_level());
            parallel_tasks.add(task);
        }
    }

    /// Wait for all conversions to complete
    wait_for_all_tasks(parallel_tasks);

    /// Phase 2: Now all sets are two-level, merge efficiently.
    for (const auto &amp;amp; pair : all_tables)
        merge_sets(pair.left_table, pair.right_table);
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;The performance did not improve only for Q5 - the optimization enabled linear scaling for any aggregation-heavy query on ultra-high core count systems.&lt;/p&gt;&lt;p&gt;Performance improvement after parallel conversion - Q5 achieves 264% improvement&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ClickBench query Q5 improved by a 264% on a 2√ó112 vCPU system,&lt;/item&gt;&lt;item&gt;24 queries achieved &amp;gt;5% improvement,&lt;/item&gt;&lt;item&gt;the overall geometric mean improved by 7.4%&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The optimization demonstrates that scalability isn't just about making things more parallel - it's about eliminating serial sections that grow with parallelism. Sometimes you need to restructure algorithms on a more deep level, not just add more threads.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 3.2: Single-Level Hash Table Merging (PR #52973) #&lt;/head&gt;&lt;p&gt;We noticed that the performance was also subpar when all hash tables were single-level.&lt;/p&gt;&lt;p&gt;Extending parallel merge to single-level cases&lt;/p&gt;&lt;p&gt;Building on PR #50748, this optimization recognizes that the benefits of parallel merging are not limited to mixed hash tables. Even when all hash tables are single-level, parallel merging can improve performance if the total data size is large enough.&lt;/p&gt;&lt;p&gt;The challenge was to determine when single-level hash tables should be merged in parallel parallel:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;If datasets are too small, parallelization introduces extra overhead.&lt;/item&gt;&lt;item&gt;If datasets are too large, parallelization may not be beneficial enough.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Based on the implementation in PR #52973, the optimization added parallel merges to all single-level cases:&lt;/p&gt;&lt;code&gt;/// Before: Only parallelize mixed-level merges
void parallelizeMergePrepare(hash_tables)
{
    single_level_count = 0;

    for (const auto &amp;amp; hash_table : hash_tables)
        if hash_table.is_single_level():
            single_level_count++;

    /// Only convert if mixed levels (some single, some two-level)
    if single_level_count &amp;gt; 0 and single_level_count &amp;lt; hash_tables.size():
        convert_to_two_level_parallel(hash_tables);
}
&lt;/code&gt;&lt;code&gt;/// Optimized code
void parallelizeMergePrepare(hash_tables):
{
    single_level_count = 0;
    all_single_hash_size = 0;

    for (const auto &amp;amp; hash_table : hash_tables)
        if (hash_table.is_single_level())
            single_level_count++

    /// Calculate total size if all hash tables are single-level
    if (single_level_count == hash_tables.size())
        for (const auto &amp;amp; hash_table : hash_tables)
            all_single_hash_size += hash_table.size();

    /// Convert if mixed levels OR if all single-level with average size &amp;gt; THRESHOLD
    if (single_level_count &amp;gt; 0 and single_level_count &amp;lt; hash_tables.size())
        ||
       (all_single_hash_size / hash_tables.size() &amp;gt; THRESHOLD)
        convert_to_two_level_parallel(hash_tables);
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Performance for single-level merge scenarios improved by 235%&lt;/item&gt;&lt;item&gt;The optimal threshold was determined through systematic testing&lt;/item&gt;&lt;item&gt;There were no regressions on small datasets&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Optimization 3.3: Parallel Merge with Key Support (PR #68441) #&lt;/head&gt;&lt;p&gt;GROUP BY operations with large hash tables were merged serially.&lt;/p&gt;&lt;p&gt;Extending parallelization to keyed aggregations&lt;/p&gt;&lt;p&gt;The previous two optimizations (3.1 and 3.2) addressed merges without key - simple hash table operations like &lt;code&gt;COUNT(DISTINCT)&lt;/code&gt;. We applied the same optimization to merges with key where hash tables contain both keys and aggregated values that must be combined, e.g. general &lt;code&gt;GROUP BY&lt;/code&gt; semantics.&lt;/p&gt;&lt;p&gt;Performance Impact:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ClickBench query Q8 improved by 10.3%, Q9 by 7.6%&lt;/item&gt;&lt;item&gt;There were no regressions in other queries&lt;/item&gt;&lt;item&gt;CPU utilization during the merge phase improved&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Parallel merging can be extended to complex aggregation scenarios with careful attention to cancellation and error handling.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 4: Algorithm optimization #&lt;/head&gt;&lt;p&gt;Harnessing the full potential of SIMD instructions is notoriously difficult. Compilers are conservative about vectorization, and database workloads often have complex control flows that inhibit auto-vectorization.&lt;/p&gt;&lt;p&gt;Effective usage of SIMD instructions in databases requires thinking beyond traditional vectorization. Besides processing N data items simultaneously instead of one, one can also utilize parallel SIMD comparisons for smart pruning strategies which lead to less work done overall. This idea is particularly powerful for string operations. These are at the same time frequently used in practice and computationally expensive.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 4.1: Two-character SIMD string search (PR #46289) #&lt;/head&gt;&lt;p&gt;String search (e.g. plain substring search or LIKE pattern search) is a bottleneck in a lot of queries, for example in ClickBench query Q20.&lt;/p&gt;&lt;p&gt;Understanding string search in analytical queries&lt;/p&gt;&lt;p&gt;Clickbench query 20 evaluates a LIKE pattern on millions of URLs, making fast string search crucial.&lt;/p&gt;&lt;code&gt;SELECT COUNT(*) FROM hits WHERE URL LIKE '%google%'
&lt;/code&gt;&lt;p&gt;Reducing false positives with two-character filtering&lt;/p&gt;&lt;p&gt;PR #46289 is based on the insight that SIMD instructions can be used in a smart way beyond brute-force parallelization. The original code already leveraged SIMD instructions but it only considered the search pattern‚Äôs first character, leading to expensive false positives. We rewrite the code to check the second character as well. This improved selectivity dramatically while adding only a negligible amount of new SIMD operations.&lt;/p&gt;&lt;code&gt;/// Original code
class StringSearcher
{
    first_needle_character = needle[0];
    first_needle_character_vec = broadcast_to_simd_vector(first_needle_character);

    void search()
    {
        for (position in haystack; step by 16 bytes)
        {
            haystack_chunk = load_16_bytes(haystack + position);
            first_matches = simd_compare_equal(haystack_chunk, first_needle_character_vec);
            match_mask = extract_match_positions(first_matches);

            for (const auto &amp;amp; match : match_mask)
                /// High false positive rate - many expensive verifications
                if (full_string_match(haystack + match_pos, needle))
                    return match_pos;
        }
    }
}
&lt;/code&gt;&lt;code&gt;// Optimized code
class StringSearcher
{
    first_needle_character = needle[0];
    second_needle_character = needle[1];  /// Second character
    first_needle_character_vec = broadcast_to_simd_vector(first_needle_character);
    second_needle_character_vec = broadcast_to_simd_vector(second_needle_character);

    void search()
    {
        for (position : haystack, step by 16 bytes)
        {
            haystack_chunk1 = load_16_bytes(haystack + position);
            haystack_chunk2 = load_16_bytes(haystack + position + 1);

            /// Compare both characters simultaneously
            first_matches = simd_compare_equal(haystack_chunk1, first_needle_character_vec);
            second_matches = simd_compare_equal(haystack_chunk2, second_needle_character_vec);
            combined_matches = simd_and(first_matches, second_matches);

            match_mask = extract_match_positions(combined_matches);

            for (const auto &amp;amp; match : match_mask)
                // Dramatically fewer false positives - fewer expensive verifications
                if full_string_match(haystack + match_pos, needle):
                    return match_pos;
        }
    }
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;Two-character SIMD filtering improved performance significantly:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ClickBench query Q20 sped up by 35%&lt;/item&gt;&lt;item&gt;Other queries which perform substring matching saw an overall improvement of ~10%&lt;/item&gt;&lt;item&gt;The geometric mean of all queries improved by 4.1%&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The performance improvements are a result of fewer false positives, better cache locality and more efficient branch prediction.&lt;/p&gt;&lt;p&gt;Two-character SIMD filtering demonstrates that effective SIMD optimization isn't just about processing more data per instruction - it's about using SIMD's parallel comparison capabilities to improve the algorithmic efficiency. The two-character approach shows how a small number of additional SIMD operations can in some cases yield massive performance gains.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 5: False Sharing #&lt;/head&gt;&lt;p&gt;False sharing occurs when multiple threads access variables in the same cache. The CPU's cache coherence protocol works at cache line granularity, meaning that any cache line modifications - including modifications of two different variables - are treated as conflicts which require expensive synchronization between cores. On a 2 x 240 vCPUs system, false sharing can turn simple counter increments into system-wide performance disasters.&lt;/p&gt;&lt;p&gt;Eliminating false sharing requires how CPU cache coherence is implemented at the hardware level. It's not enough to optimize algorithms - to avoid false sharing, one must also optimize the memory layout to make sure that frequently-accessed data structures don't accidentally interfere with each other through cache line conflicts. This involves for example a strategic data layout and use of alignment and padding.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 5.1: Profile Event Counter Alignment (PR #82697) #&lt;/head&gt;&lt;p&gt;ClickBench query Q3 showed 36.6% of CPU cycles spent in &lt;code&gt;ProfileEvents::increment&lt;/code&gt; on a 2√ó240 vCPU system. Performance profiling revealed a severe cache line contention.&lt;/p&gt;&lt;p&gt;ProfileEvents counters at scale&lt;/p&gt;&lt;p&gt;Profile event counters refer to ClickHouse's internal eventing system - profile events track all internal operations, from detailed query execution steps to memory allocations. In a typical analytical query, these counters are incremented millions of times across all threads. The original implementation organized multiple counters in the same memory region without considering cache line boundaries.&lt;/p&gt;&lt;p&gt;This creates three challenges:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Cache line physics: Modern Intel processors use 64-byte cache lines. When any byte in a cache line is modified, the entire line must be invalidated in the other cores' caches.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;False sharing amplification: With 240 threads, each counter update triggers a cache line invalidation across potentially dozens of cores. What should be independent operations become serialized through the cache coherence protocol.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Exponential degradation: As the number of cores increases, the probability of a simultaneous access to the same cache line grows exponentially, compounding the impact of cache misses.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Using perf, I discovered that &lt;code&gt;ProfileEvents::increment&lt;/code&gt; was generating massive cache coherence traffic. The smoking gun was the cache line utilization report that showed eight different counters packed into a single cache line. We also added new capabilities to Linux‚Äôs perf c2c tool and worked with the community to help developers more easily identify false sharing issues like this.&lt;/p&gt;&lt;p&gt;Proper cache line alignment ensures that each counter gets its own 64-byte cache line. This transforms false sharing (bad) into true sharing (manageable). When a thread updates its counter, now only a single cache line wil be affected.&lt;/p&gt;&lt;p&gt;Based on our implementation in PR #82697, the fix improved the cache line alignment for the profile event counters:&lt;/p&gt;&lt;code&gt;// Before: Counters packed without alignment
struct ProfileEvents:
    atomic_value counters[NUM_EVENTS]  // Multiple counters per cache line
    // 8 counters sharing single 64-byte cache lines

// After: Cache line aligned counters  
struct ProfileEvents:
    struct alignas(64) AlignedCounter:
        atomic_value value
        // Padding automatically added to reach 64 bytes
    
    AlignedCounter counters[NUM_EVENTS]  // Each counter gets own cache line
    // Now each counter has exclusive cache line ownership
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;This optimization pattern applies to any frequently updated shared and compact data structure. The lesson is that the memory layout becomes critical at scale - what works fine on eight cores can be excruciatingly slow on 240 cores.&lt;/p&gt;After optimization: ProfileEvents::increment drops to 8.5% (from 36.6%)&lt;p&gt;As a result of our optimization, ClickBench query Q3 saw a 27.4% improvement on ultra-high core count systems. The performance gain increases with the number of cores because the cache coherence overhead grows super-linearly. This optimization therefore doesn't merely fix a bottleneck - it changes the scalability curve.&lt;/p&gt;ClickBench Q3: 27.4% improvement, with larger gains on higher core count systems&lt;head rend="h2"&gt;Building a foundation that scales #&lt;/head&gt;&lt;p&gt;In this post I covered optimizations for five performance bottlenecks:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Lock contention - The coordination overhead grows exponentially with core count.&lt;/item&gt;&lt;item&gt;Memory optimization - The memory bandwidth per core decreases as the core count increases.&lt;/item&gt;&lt;item&gt;Increased parallelism - Serial phases become the dominant bottleneck.&lt;/item&gt;&lt;item&gt;SIMD optimization - Smarter algorithms like two-character filtering beyond brute-force vectorization can improve performance significantly.&lt;/item&gt;&lt;item&gt;False sharing - False sharing is caused by the granularity of cache line size.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The bottlenecks and optimizations presented here are not just about ClickHouse - they represent a fundamental shift in how we must approach database optimization in the ultra-high core count era. As processors continue to evolve toward higher core counts, these techniques will become essential for any system that needs to scale.&lt;/p&gt;&lt;p&gt;Our optimizations enable ClickHouse to achieve close-to-linear scalability as the core count increases. This enables ClickHouse to thrive as an analytics database in a future world where Intel and other hardware manufacturers push the core count into the thousands.&lt;/p&gt;&lt;head rend="h2"&gt;References and Resources #&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Source Code: All optimizations available in ClickHouse main branch&lt;/item&gt;&lt;item&gt;Slide Deck: 2025 Shanghai Meetup Presentation&lt;/item&gt;&lt;item&gt;Pull Requests: Individual PRs linked throughout this post with detailed performance analysis&lt;/item&gt;&lt;item&gt;Intel Intrinsics Guide: Intel¬Æ Intrinsics Guide&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Acknowledgments #&lt;/head&gt;&lt;p&gt;Special thanks to the ClickHouse community for rigorous code review and performance validation. These optimizations represent collaborative effort between Intel and ClickHouse teams to unlock the full potential of modern ultra-high core count processors.&lt;/p&gt;&lt;p&gt;For questions about implementation details or performance reproduction, please refer to the individual PR discussions linked throughout this post.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45279792</guid><pubDate>Wed, 17 Sep 2025 18:46:03 +0000</pubDate></item><item><title>Gluon: a GPU programming language based on the same compiler stack as Triton</title><link>https://github.com/triton-lang/triton/blob/main/python/tutorials/gluon/01-intro.py</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45280592</guid><pubDate>Wed, 17 Sep 2025 19:50:11 +0000</pubDate></item><item><title>A postmortem of three recent issues</title><link>https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues</link><description>&lt;doc fingerprint="8d3083968c38d9d6"&gt;
  &lt;main&gt;
    &lt;p&gt;Between August and early September, three infrastructure bugs intermittently degraded Claude's response quality. We've now resolved these issues and want to explain what happened.&lt;/p&gt;
    &lt;p&gt;In early August, a number of users began reporting degraded responses from Claude. These initial reports were difficult to distinguish from normal variation in user feedback. By late August, the increasing frequency and persistence of these reports prompted us to open an investigation that led us to uncover three separate infrastructure bugs.&lt;/p&gt;
    &lt;p&gt;To state it plainly: We never reduce model quality due to demand, time of day, or server load. The problems our users reported were due to infrastructure bugs alone.&lt;/p&gt;
    &lt;p&gt;We recognize users expect consistent quality from Claude, and we maintain an extremely high bar for ensuring infrastructure changes don't affect model outputs. In these recent incidents, we didn't meet that bar. The following postmortem explains what went wrong, why detection and resolution took longer than we would have wanted, and what we're changing to prevent similar future incidents.&lt;/p&gt;
    &lt;p&gt;We don't typically share this level of technical detail about our infrastructure, but the scope and complexity of these issues justified a more comprehensive explanation.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we serve Claude at scale&lt;/head&gt;
    &lt;p&gt;We serve Claude to millions of users via our first-party API, Amazon Bedrock, and Google Cloud's Vertex AI. We deploy Claude across multiple hardware platforms, namely AWS Trainium, NVIDIA GPUs, and Google TPUs. This approach provides the capacity and geographic distribution necessary to serve users worldwide.&lt;/p&gt;
    &lt;p&gt;Each hardware platform has different characteristics and requires specific optimizations. Despite these variations, we have strict equivalence standards for model implementations. Our aim is that users should get the same quality responses regardless of which platform serves their request. This complexity means that any infrastructure change requires careful validation across all platforms and configurations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline of events&lt;/head&gt;
    &lt;p&gt;The overlapping nature of these bugs made diagnosis particularly challenging. The first bug was introduced on August 5, affecting approximately 0.8% of requests made to Sonnet 4. Two more bugs arose from deployments on August 25 and 26.&lt;/p&gt;
    &lt;p&gt;Although initial impacts were limited, a load balancing change on August 29 started to increase affected traffic. This caused many more users to experience issues while others continued to see normal performance, creating confusing and contradictory reports.&lt;/p&gt;
    &lt;head rend="h2"&gt;Three overlapping issues&lt;/head&gt;
    &lt;p&gt;Below we describe the three bugs that caused the degradation, when they occurred, and how we resolved them:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Context window routing error&lt;/head&gt;
    &lt;p&gt;On August 5, some Sonnet 4 requests were misrouted to servers configured for the upcoming 1M token context window. This bug initially affected 0.8% of requests. On August 29, a routine load balancing change unintentionally increased the number of short-context requests routed to the 1M context servers. At the worst impacted hour on August 31, 16% of Sonnet 4 requests were affected.&lt;/p&gt;
    &lt;p&gt;Approximately 30% of Claude Code users who made requests during this period had at least one message routed to the wrong server type, resulting in degraded responses. On Amazon Bedrock, misrouted traffic peaked at 0.18% of all Sonnet 4 requests from August 12. Incorrect routing affected less than 0.0004% of requests on Google Cloud's Vertex AI between August 27 and September 16.&lt;/p&gt;
    &lt;p&gt;However, some users were affected more severely, as our routing is "sticky". This meant that once a request was served by the incorrect server, subsequent follow-ups were likely to be served by the same incorrect server.&lt;/p&gt;
    &lt;p&gt;Resolution: We fixed the routing logic to ensure short- and long-context requests were directed to the correct server pools. We deployed the fix on September 4. A rollout to our first-party platforms and Google Cloud‚Äôs Vertex was completed by September 16. The fix is in the process of being rolled out on Bedrock.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Output corruption&lt;/head&gt;
    &lt;p&gt;On August 25, we deployed a misconfiguration to the Claude API TPU servers that caused an error during token generation. An issue caused by a runtime performance optimization occasionally assigned a high probability to tokens that should rarely be produced given the context, for example producing Thai or Chinese characters in response to English prompts, or producing obvious syntax errors in code. A small subset of users that asked a question in English might have seen "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ" in the middle of the response, for example.&lt;/p&gt;
    &lt;p&gt;This corruption affected requests made to Opus 4.1 and Opus 4 on August 25-28, and requests to Sonnet 4 August 25‚ÄìSeptember 2. Third-party platforms were not affected by this issue.&lt;/p&gt;
    &lt;p&gt;Resolution: We identified the issue and rolled back the change on September 2. We've added detection tests for unexpected character outputs to our deployment process.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Approximate top-k XLA:TPU miscompilation&lt;/head&gt;
    &lt;p&gt;On August 25, we deployed code to improve how Claude selects tokens during text generation. This change inadvertently triggered a latent bug in the XLA:TPU[1] compiler, which has been confirmed to affect requests to Claude Haiku 3.5.&lt;/p&gt;
    &lt;p&gt;We also believe this could have impacted a subset of Sonnet 4 and Opus 3 on the Claude API. Third-party platforms were not affected by this issue.&lt;/p&gt;
    &lt;p&gt;Resolution: We first observed the bug affecting Haiku 3.5 and rolled it back on September 4. We later noticed user reports of problems with Opus 3 that were compatible with this bug, and rolled it back on September 12. After extensive investigation we were unable to reproduce this bug on Sonnet 4 but decided to also roll it back out of an abundance of caution.&lt;/p&gt;
    &lt;p&gt;Simultaneously, we have (a) been working with the XLA:TPU team on a fix for the compiler bug and (b) rolled out a fix to use exact top-k with enhanced precision. For details, see the deep dive below.&lt;/p&gt;
    &lt;head rend="h2"&gt;A closer look at the XLA compiler bug&lt;/head&gt;
    &lt;p&gt;To illustrate the complexity of these issues, here's how the XLA compiler bug manifested and why it proved particularly challenging to diagnose.&lt;/p&gt;
    &lt;p&gt;When Claude generates text, it calculates probabilities for each possible next word, then randomly chooses a sample from this probability distribution. We use "top-p sampling" to avoid nonsensical outputs‚Äîonly considering words whose cumulative probability reaches a threshold (typically 0.99 or 0.999). On TPUs, our models run across multiple chips, with probability calculations happening in different locations. To sort these probabilities, we need to coordinate data between chips, which is complex.[2]&lt;/p&gt;
    &lt;p&gt;In December 2024, we discovered our TPU implementation would occasionally drop the most probable token when temperature was zero. We deployed a workaround to fix this case.&lt;/p&gt;
    &lt;p&gt;The root cause involved mixed precision arithmetic. Our models compute next-token probabilities in bf16 (16-bit floating point). However, the vector processor is fp32-native, so the TPU compiler (XLA) can optimize runtime by converting some operations to fp32 (32-bit). This optimization pass is guarded by the &lt;code&gt;xla_allow_excess_precision&lt;/code&gt; flag which defaults to true.&lt;/p&gt;
    &lt;p&gt;This caused a mismatch: operations that should have agreed on the highest probability token were running at different precision levels. The precision mismatch meant they didn't agree on which token had the highest probability. This caused the highest probability token to sometimes disappear from consideration entirely.&lt;/p&gt;
    &lt;p&gt;On August 26, we deployed a rewrite of our sampling code to fix the precision issues and improve how we handled probabilities at the limit that reach the top-p threshold. But in fixing these problems, we exposed a trickier one.&lt;/p&gt;
    &lt;p&gt;Our fix removed the December workaround because we believed we'd solved the root cause. This led to a deeper bug in the approximate top-k operation‚Äîa performance optimization that quickly finds the highest probability tokens.[3] This approximation sometimes returned completely wrong results, but only for certain batch sizes and model configurations. The December workaround had been inadvertently masking this problem.&lt;/p&gt;
    &lt;p&gt;The bug's behavior was frustratingly inconsistent. It changed depending on unrelated factors such as what operations ran before or after it, and whether debugging tools were enabled. The same prompt might work perfectly on one request and fail on the next.&lt;/p&gt;
    &lt;p&gt;While investigating, we also discovered that the exact top-k operation no longer had the prohibitive performance penalty it once did. We switched from approximate to exact top-k and standardized some additional operations on fp32 precision.[4] Model quality is non-negotiable, so we accepted the minor efficiency impact.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why detection was difficult&lt;/head&gt;
    &lt;p&gt;Our validation process ordinarily relies on benchmarks alongside safety evaluations and performance metrics. Engineering teams perform spot checks and deploy to small "canary" groups first.&lt;/p&gt;
    &lt;p&gt;These issues exposed critical gaps that we should have identified earlier. The evaluations we ran simply didn't capture the degradation users were reporting, in part because Claude often recovers well from isolated mistakes. Our own privacy practices also created challenges in investigating reports. Our internal privacy and security controls limit how and when engineers can access user interactions with Claude, in particular when those interactions are not reported to us as feedback. This protects user privacy but prevents engineers from examining the problematic interactions needed to identify or reproduce bugs.&lt;/p&gt;
    &lt;p&gt;Each bug produced different symptoms on different platforms at different rates. This created a confusing mix of reports that didn't point to any single cause. It looked like random, inconsistent degradation.&lt;/p&gt;
    &lt;p&gt;More fundamentally, we relied too heavily on noisy evaluations. Although we were aware of an increase in reports online, we lacked a clear way to connect these to each of our recent changes. When negative reports spiked on August 29, we didn't immediately make the connection to an otherwise standard load balancing change.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we're changing&lt;/head&gt;
    &lt;p&gt;As we continue to improve our infrastructure, we're also improving the way we evaluate and prevent bugs like those discussed above across all platforms where we serve Claude. Here's what we're changing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More sensitive evaluations: To help discover the root cause of any given issue, we‚Äôve developed evaluations that can more reliably differentiate between working and broken implementations. We‚Äôll keep improving these evaluations to keep a closer eye on model quality.&lt;/item&gt;
      &lt;item&gt;Quality evaluations in more places: Although we run regular evaluations on our systems, we will run them continuously on true production systems to catch issues such as the context window load balancing error.&lt;/item&gt;
      &lt;item&gt;Faster debugging tooling: We'll develop infrastructure and tooling to better debug community-sourced feedback without sacrificing user privacy. Additionally, some bespoke tools developed here will be used to reduce the remediation time in future similar incidents, if those should occur.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Evals and monitoring are important. But these incidents have shown that we also need continuous signal from users when responses from Claude aren't up to the usual standard. Reports of specific changes observed, examples of unexpected behavior encountered, and patterns across different use cases all helped us isolate the issues.&lt;/p&gt;
    &lt;p&gt;It remains particularly helpful for users to continue to send us their feedback directly. You can use the &lt;code&gt;/bug&lt;/code&gt; command in Claude Code or you can use the "thumbs down" button in the Claude apps to do so. Developers and researchers often create new and interesting ways to evaluate model quality that complement our internal testing. If you'd like to share yours, reach out to feedback@anthropic.com.&lt;/p&gt;
    &lt;p&gt;We remain grateful to our community for these contributions.&lt;/p&gt;
    &lt;p&gt;[1] XLA:TPU is the optimizing compiler that translates XLA High Level Optimizing language‚Äîoften written using JAX‚Äîto TPU machine instructions.&lt;/p&gt;
    &lt;p&gt;[2] Our models are too large for single chips and are partitioned across tens of chips or more, making our sorting operation a distributed sort. TPUs (just like GPUs and Trainium) also have different performance characteristics than CPUs, requiring different implementation techniques using vectorized operations instead of serial algorithms.&lt;/p&gt;
    &lt;p&gt;[3] We had been using this approximate operation because it yielded substantial performance improvements. The approximation works by accepting potential inaccuracies in the lowest probability tokens, which shouldn't affect quality‚Äîexcept when the bug caused it to drop the highest probability token instead.&lt;/p&gt;
    &lt;p&gt;[4] Note that the now-correct top-k implementation may result in slight differences in the inclusion of tokens near the top-p threshold, and in rare cases users may benefit from re-tuning their choice of top-p.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45281139</guid><pubDate>Wed, 17 Sep 2025 20:41:07 +0000</pubDate></item><item><title>One Token to rule them all ‚Äì Obtaining Global Admin in every Entra ID tenant</title><link>https://dirkjanm.io/obtaining-global-admin-in-every-entra-id-tenant-with-actor-tokens/</link><description>&lt;doc fingerprint="3d63029d0fa3c0af"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;One Token to rule them all - obtaining Global Admin in every Entra ID tenant via Actor tokens&lt;/head&gt;
    &lt;p&gt;While preparing for my Black Hat and DEF CON talks in July of this year, I found the most impactful Entra ID vulnerability that I will probably ever find. This vulnerability could have allowed me to compromise every Entra ID tenant in the world (except probably those in national cloud deployments1). If you are an Entra ID admin reading this, yes that means complete access to your tenant. The vulnerability consisted of two components: undocumented impersonation tokens, called ‚ÄúActor tokens‚Äù, that Microsoft uses in their backend for service-to-service (S2S) communication. Additionally, there was a critical flaw in the (legacy) Azure AD Graph API that failed to properly validate the originating tenant, allowing these tokens to be used for cross-tenant access.&lt;/p&gt;
    &lt;p&gt;Effectively this means that with a token I requested in my lab tenant I could authenticate as any user, including Global Admins, in any other tenant. Because of the nature of these Actor tokens, they are not subject to security policies like Conditional Access, which means there was no setting that could have mitigated this for specific hardened tenants. Since the Azure AD Graph API is an older API for managing the core Azure AD / Entra ID service, access to this API could have been used to make any modification in the tenant that Global Admins can do, including taking over or creating new identities and granting them any permission in the tenant. With these compromised identities the access could also be extended to Microsoft 365 and Azure.&lt;/p&gt;
    &lt;p&gt;I reported this vulnerability the same day to the Microsoft Security Response Center (MSRC). Microsoft fixed this vulnerability on their side within days of the report being submitted and has rolled out further mitigations that block applications from requesting these Actor tokens for the Azure AD Graph API. Microsoft also issued CVE-2025-55241 for this vulnerability.&lt;/p&gt;
    &lt;head rend="h1"&gt;Impact&lt;/head&gt;
    &lt;p&gt;These tokens allowed full access to the Azure AD Graph API in any tenant. Requesting Actor tokens does not generate logs. Even if it did they would be generated in my tenant instead of in the victim tenant, which means there is no record of the existence of these tokens.&lt;/p&gt;
    &lt;p&gt;Furthermore, the Azure AD Graph API does not have API level logging. Its successor, the Microsoft Graph, does have this logging, but for the Azure AD Graph this telemetry source is still in a very limited preview and I‚Äôm not aware of any tenant that currently has this available. Since there is no API level logging, it means the following Entra ID data could be accessed without any traces:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;User information including all their personal details stored in Entra ID.&lt;/item&gt;
      &lt;item&gt;Group and role information.&lt;/item&gt;
      &lt;item&gt;Tenant settings and (Conditional Access) policies.&lt;/item&gt;
      &lt;item&gt;Applications, Service Principals, and any application permission assignment.&lt;/item&gt;
      &lt;item&gt;Device information and BitLocker keys synced to Entra ID.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This information could be accessed by impersonating a regular user in the victim tenant. If you want to know the full impact, my tool roadrecon uses the same API, if you run it then everything you find in the GUI of the tool could have been accessed and modified by an attacker abusing this flaw.&lt;/p&gt;
    &lt;p&gt;If a Global Admin was impersonated, it would also be possible to modify any of the above objects and settings. This would result in full tenant compromise with access to any service that uses Entra ID for authentication, such as SharePoint Online and Exchange Online. It would also provide full access to any resource hosted in Azure, since these resources are controlled from the tenant level and Global Admins can grant themselves rights on Azure subscriptions. Modifying objects in the tenant does (usually) result in audit logs being generated. That means that while theoretically all data in Microsoft 365 could have been compromised, doing anything other than reading the directory information would leave audit logs that could alert defenders, though without knowledge of the specific artifacts that modifications with these Actor tokens generate, it would appear as if a legitimate Global Admin performed the actions.&lt;/p&gt;
    &lt;p&gt;Based on Microsoft‚Äôs internal telemetry, they did not detect any abuse of this vulnerability. If you want to search for possible abuse artifacts in your own environment, a KQL detection is included at the end of this post.&lt;/p&gt;
    &lt;head rend="h1"&gt;Technical details&lt;/head&gt;
    &lt;head rend="h2"&gt;Actor tokens&lt;/head&gt;
    &lt;p&gt;Actor tokens are tokens that are issued by the ‚ÄúAccess Control Service‚Äù. I don‚Äôt know the exact origins of this service, but it appears to be a legacy service that is used for authentication with SharePoint applications and also seems to be used by Microsoft internally. I came across this service while investigating hybrid Exchange setups. These hybrid setups used to provision a certificate credential on the Exchange Online Service Principal (SP) in the tenant, with which it can perform authentication. These hybrid attacks were the topic of some talks I did this summer, the slides are on the talks page. In this case the hybrid part is not relevant, as in my lab I could also have added a credential on the Exchange Online SP without the complete hybrid setup. Exchange is not the only app which can do this, but since I found this in Exchange we will keep talking about these tokens in the context of Exchange.&lt;/p&gt;
    &lt;p&gt;Exchange will request Actor tokens when it wants to communicate with other services on behalf of a user. The Actor token allows it to ‚Äúact‚Äù as another user in the tenant when talking to Exchange Online, SharePoint and as it turns out the Azure AD Graph. The Actor token (a JSON Web Token / JWT) looks as follows when decoded:&lt;/p&gt;
    &lt;code&gt;{
    "alg": "RS256",
    "kid": "_jNwjeSnvTTK8XEdr5QUPkBRLLo",
    "typ": "JWT",
    "x5t": "_jNwjeSnvTTK8XEdr5QUPkBRLLo"
}
{
    "aud": "00000002-0000-0000-c000-000000000000/graph.windows.net@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "exp": 1752593816,
    "iat": 1752507116,
    "identityprovider": "00000001-0000-0000-c000-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "iss": "00000001-0000-0000-c000-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nameid": "00000002-0000-0ff1-ce00-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nbf": 1752507116,
    "oid": "a761cbb2-fbb6-4c80-aa50-504962316eb2",
    "rh": "1.AXQAj_KHYn9PIkOWUahpfY_hvAIAAAAAAAAAwAAAAAAAAACtAQB0AA.",
    "sub": "a761cbb2-fbb6-4c80-aa50-504962316eb2",
    "trustedfordelegation": "true",
    "xms_spcu": "true"
}.[signature from Entra ID]
&lt;/code&gt;
    &lt;p&gt;There are a few fields here that differ from regular Entra ID access tokens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;aud&lt;/code&gt;field contains the GUID of the Azure AD Graph API, as well as the URL&lt;code&gt;graph.windows.net&lt;/code&gt;and the tenant it was issued to&lt;code&gt;6287f28f-4f7f-4322-9651-a8697d8fe1bc&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The expiry is exactly 24 hours after the token was issued.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;iss&lt;/code&gt;contains the GUID of the Entra ID token service itself, called ‚ÄúAzure ESTS Service‚Äù, and again the tenant GUID where it was issued.&lt;/item&gt;
      &lt;item&gt;The token contains the claim &lt;code&gt;trustedfordelegation&lt;/code&gt;, which is&lt;code&gt;True&lt;/code&gt;in this case, meaning we can use this token to impersonate other identities. Many Microsoft apps could request such tokens. Non-Microsoft apps requesting an Actor token would receive a token with this field set to&lt;code&gt;False&lt;/code&gt;instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When using this Actor token, Exchange would embed this in an unsigned JWT that is then sent to the resource provider, in this case the Azure AD graph. In the rest of the blog I call these impersonation tokens since they are used to impersonate users.&lt;/p&gt;
    &lt;code&gt;{
    "alg": "none",
    "typ": "JWT"
}
{
    "actortoken": "eyJ0eXAiOiJKV1Qi&amp;lt;snip&amp;gt;TxeLkNB8v2rWWMLGpaAaFJlhA",
    "aud": "00000002-0000-0000-c000-000000000000/graph.windows.net@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "exp": 1756926566,
    "iat": 1756926266,
    "iss": "00000002-0000-0ff1-ce00-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nameid": "10032001E2CBE43B",
    "nbf": 1756926266,
    "nii": "urn:federation:MicrosoftOnline",
    "sip": "doesnt@matter.com",
    "smtp": "doesnt@matter.com",
    "upn": "doesnt@matter.com"
}.[no signature]
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;sip&lt;/code&gt;, &lt;code&gt;smtp&lt;/code&gt;, &lt;code&gt;upn&lt;/code&gt; fields are used when accessing resources in Exchange online or SharePoint, but are ignored when talking to the Azure AD Graph, which only cares about the &lt;code&gt;nameid&lt;/code&gt;. This &lt;code&gt;nameid&lt;/code&gt; originates from an attribute of the user that is called the &lt;code&gt;netId&lt;/code&gt; on the Azure AD Graph. You will also see it reflected in tokens issued to users, in the &lt;code&gt;puid&lt;/code&gt; claim, which stands for Passport UID. I believe these identifiers are an artifact from the original codebase which Microsoft used for its Microsoft Accounts (consumer accounts or MSA). They are still used in Entra ID, for example to map guest users to the original identity in their home tenant.&lt;/p&gt;
    &lt;p&gt;As I mentioned before, these impersonation tokens are not signed. That means that once Exchange has an Actor token, it can use the one Actor token to impersonate anyone against the target service it was requested for, for 24 hours. In my personal opinion, this whole Actor token design is something that never should have existed. It lacks almost every security control that you would want:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There are no logs when Actor tokens are issued.&lt;/item&gt;
      &lt;item&gt;Since these services can craft the unsigned impersonation tokens without talking to Entra ID, there are also no logs when they are created or used.&lt;/item&gt;
      &lt;item&gt;They cannot be revoked within their 24 hours validity.&lt;/item&gt;
      &lt;item&gt;They completely bypass any restrictions configured in Conditional Access.&lt;/item&gt;
      &lt;item&gt;We have to rely on logging from the resource provider to even know these tokens were used in the tenant.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Microsoft uses these tokens to talk to other services in their backend, something that Microsoft calls service-to-service (S2S) communication. If one of these tokens leaks, it can be used to access all the data in an entire tenant without any useful telemetry or mitigation. In July of this year, Microsoft did publish a blog about removing these insecure legacy practices from their environment, but they do not provide any transparency about how many services still use these tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;The fatal flaw leading to cross-tenant compromise&lt;/head&gt;
    &lt;p&gt;As I was refining my slide deck and polished up my proof-of-concept code for requesting and generating these tokens, I tested more variants of using these tokens, changing various fields to see if the tokens still worked with the modified information. As one of the tests I changed the tenant ID of the impersonation token to a different tenant in which none of my test accounts existed. The Actor tokens tenant ID was my &lt;code&gt;iminyour.cloud&lt;/code&gt; tenant, with tenant ID &lt;code&gt;6287f28f-4f7f-4322-9651-a8697d8fe1bc&lt;/code&gt; and the unsigned JWT generated had the tenant ID &lt;code&gt;b9fb93c1-c0c8-4580-99f3-d1b540cada32&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I sent this token to &lt;code&gt;graph.windows.net&lt;/code&gt; using my CLI tool &lt;code&gt;roadtx&lt;/code&gt;, expecting a generic access denied since I had a tenant ID mismatch. However, I was instead greeted by a curious error message:&lt;/p&gt;
    &lt;p&gt;Note that these are the actual screenshots I made during my research, which is why the formatting may not work as well in this blog&lt;/p&gt;
    &lt;p&gt;The error message suggested that while my token was valid, the identity could not be found in the tenant. Somehow the API seemed to accept my token even with the mismatching tenant. I quickly looked up the &lt;code&gt;netId&lt;/code&gt; of a user that did exist in the target tenant, crafted a token and the Azure AD Graph happily returned the data I requested. I tested this in a few more test tenants I had access to, to make sure I was not crazy, but I could indeed access data in other tenants, as long as I knew their tenant ID (which is public information) and the &lt;code&gt;netId&lt;/code&gt; of a user in that tenant.&lt;/p&gt;
    &lt;p&gt;To demonstrate the vulnerability, here I am using a Guest user in the target tenant to query the &lt;code&gt;netId&lt;/code&gt; of a Global Admin. Then I impersonate the Global Admin using the same Actor token, and can perform any action in the tenant as that Global Admin over the Azure AD Graph.&lt;/p&gt;
    &lt;p&gt;First I craft an impersonation token for a Guest user in my victim tenant:&lt;/p&gt;
    &lt;p&gt;I use this token to query the &lt;code&gt;netId&lt;/code&gt; of a Global Admin:&lt;/p&gt;
    &lt;p&gt;Then I create an impersonation token for this Global Admin (the UPN is kept the same since it is not validated by the API):&lt;/p&gt;
    &lt;p&gt;And finally this token is used to access the tenant as the Global Admin, listing the users, something the guest user was not able to do:&lt;/p&gt;
    &lt;p&gt;I can even run roadrecon with this impersonation token, which queries all Azure AD Graph API endpoints to enumerate the available information in the tenant.&lt;/p&gt;
    &lt;p&gt;None of these actions would generate any logs in the victim tenant.&lt;/p&gt;
    &lt;head rend="h1"&gt;Practical abuse&lt;/head&gt;
    &lt;p&gt;With this vulnerability it would be possible to compromise any Entra ID tenant. Starting with an Actor token from an attacker controlled tenant, the following steps would lead to full control over the victim tenant:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the tenant ID for the victim tenant, this can be done using public APIs based on the domain name.&lt;/item&gt;
      &lt;item&gt;Find a valid &lt;code&gt;netId&lt;/code&gt;of a regular user in the tenant. Methods for this will be discussed below.&lt;/item&gt;
      &lt;item&gt;Craft an impersonation token with the Actor token from the attacker tenant, using the tenant ID and &lt;code&gt;netId&lt;/code&gt;of the user in the victim tenant.&lt;/item&gt;
      &lt;item&gt;List all Global Admins in the tenant and their &lt;code&gt;netId&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Craft an impersonation token for the Global Admin account.&lt;/item&gt;
      &lt;item&gt;Perform any read or write action over the Azure AD Graph API.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If an attacker makes any modifications in the tenant in step 6, that would be the only event in this chain that generates any telemetry in the victim tenant. An attacker could for example create new user accounts, grant these Global Admin privileges and then sign in interactively to any Entra ID, Microsoft 365 or third party application that integrates with the victim tenant. Alternatively they could add credentials on existing applications, grant these apps API permissions and use that to exfiltrate emails or files from Microsoft 365, a technique that is popular among threat actors. An attacker could also add credentials to Microsoft Service Principals in the victim tenant, several of which can request Actor tokens that allow impersonation against SharePoint or Exchange. For my DEF CON and Black Hat talks I made a demo video about using these Actor tokens to obtain Global Admin access. The video uses Actor tokens within a tenant, but the same technique could have been applied to any other tenant by abusing this vulnerability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding netIds&lt;/head&gt;
    &lt;p&gt;Since tenant IDs can be resolved when the domain name of a tenant is known, the only identifier that is not immediately available to the attacker is a valid &lt;code&gt;netId&lt;/code&gt; for a user in that specific tenant. As I mentioned above, these IDs are added to Entra ID access tokens as the &lt;code&gt;puid&lt;/code&gt; claim. Any token found online, in screenshots, examples or logs, even those that are long expired or with an obfuscated signature, would provide an attacker with enough information to breach the tenant. Threat actors that still have old tokens for any tenant from previous breaches can immediately access those tenants again as long as the victim account still exists.&lt;/p&gt;
    &lt;p&gt;The above is probably not a very common occurrence. What is a more realistic attack is simply brute-forcing the &lt;code&gt;netId&lt;/code&gt;. Unlike object IDs, which are randomly generated, netIds are actually incremental. Looking at the differences in netIds between my tenant and those of some tenants I analyzed, I found the difference between a newly created user in my tenant and their newest user to be in the range of 100.000 to 100 million. Simply brute forcing the &lt;code&gt;netId&lt;/code&gt; could be accomplished in minutes to hours for any target tenant, and the more user exist in a tenant the easier it is to find a match. Since this does not generate any logs it isn‚Äôt a noisy attack either. Because of the possibility to brute force these netIds I would say this vulnerability could have been used to take over any tenant without any prerequisites. There is however a third technique which is even more effective (and more fun from a technical level).&lt;/p&gt;
    &lt;head rend="h2"&gt;Compromising tenants by hopping over B2B trusts&lt;/head&gt;
    &lt;p&gt;I previously mentioned that a users &lt;code&gt;netId&lt;/code&gt; is used to establish links between a user account in multiple tenants. This is something that I researched a few years ago when I gave a talk at Black Hat USA 22 about external identities. The below screenshot is taken from one of my slides, which illustrates this:&lt;/p&gt;
    &lt;p&gt;The way this works is as follows. Suppose we have tenant A and tenant B. A user in tenant B is invited into tenant A. In the new guest account that is created in tenant A, their &lt;code&gt;netId&lt;/code&gt; is stored on the &lt;code&gt;alternativeSecurityIds&lt;/code&gt; attribute. That means that an attacker wanting to abuse this bug can simply read that attribute in tenant A, put it in an impersonation token for tenant B and then impersonate the victim in their home tenant. It should be noted that this works against the direction of invite. Any user in any tenant where you accept an invite will be able to read your &lt;code&gt;netId&lt;/code&gt;, and with this bug could have impersonated you in your home tenant. In your home tenant you have a full user account, which can enumerate other users. This is not a bug or risk with B2B trusts, but is simply an unintended consequence of the B2B design mechanism. A guest account in someone else‚Äôs tenant would also be sufficient with the default Entra ID guest settings because the default settings allow users to query the &lt;code&gt;netId&lt;/code&gt; of a user as long as the UPN is known.&lt;/p&gt;
    &lt;p&gt;To abuse this, a threat actor could perform the following steps, given that they have access to at least one tenant with a guest user:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query the guest users and their &lt;code&gt;alternativeSecurityIds&lt;/code&gt;attribute which gives the&lt;code&gt;netId&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Query the tenant ID of the guest users home tenant based on the domain name in their UPN.&lt;/item&gt;
      &lt;item&gt;Create an impersonation token, impersonating the victim in their home tenant.&lt;/item&gt;
      &lt;item&gt;Optionally list Global Admins and impersonate those to compromise the entire tenant.&lt;/item&gt;
      &lt;item&gt;Repeat step 1 for each tenant that was compromised.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The steps above can be done in 2 API calls per tenant, which do not generate any logs. Most tenants will have guest users from multiple distinct other tenants. This means the number of tenants you compromise with this scales exponentially and the information needed to compromise the majority of all tenants worldwide could have been gathered within minutes using a single Actor token. After at least 1 user is known per victim tenant, the attacker can selectively perform post-compromise actions in these tenants by impersonating Global Admins.&lt;/p&gt;
    &lt;p&gt;Looking at the list of guest users in the tenants of some of my clients, this technique would be extremely powerful. I also observed that one of the first tenants you will likely compromise is Microsoft‚Äôs own tenant, since Microsoft consultants often get invited to customer tenants. Many MSPs and Microsoft Partners will have a guest account in the Microsoft tenant, so from the Microsoft tenant a compromise of most major service provider tenants is one step away.&lt;/p&gt;
    &lt;p&gt;Needless to say, as much as I would have liked to test this technique in practice to see how fast this would spread out, I only tested the individual steps in my own tenants and did not access any data I‚Äôm not authorized to.&lt;/p&gt;
    &lt;head rend="h1"&gt;Detection&lt;/head&gt;
    &lt;p&gt;While querying data over the Azure AD Graph does not leave any logs, modifying data does (usually) generate audit logs. If modifications are done with Actor tokens, these logs look a bit curious.&lt;/p&gt;
    &lt;p&gt;Since Actor tokens involve both the app and the user being impersonated, it seems Entra ID gets confused about who actually made the change, and it will log the UPN of the impersonated Global Admin, but the display name of Exchange. Luckily for defenders this creates a nice giveaway when Actor tokens are used in the tenant. After some testing and filtering with some fellow researchers that work on the blue side (thanks to Fabian Bader and Olaf Hartong) we came up with the following detection query:&lt;/p&gt;
    &lt;code&gt;AuditLogs
| where not(OperationName has "group")
| where not(OperationName == "Set directory feature on tenant")
| where InitiatedBy has "user"
| where InitiatedBy.user.displayName has_any ( "Office 365 Exchange Online", "Skype for Business Online", "Dataverse", "Office 365 SharePoint Online", "Microsoft Dynamics ERP")
&lt;/code&gt;
    &lt;p&gt;The exclusion for group operations is there because some of these products do actually use Actor tokens to perform operations on your behalf. For example creating specific groups via the Exchange Online PowerShell module will make Exchange use an Actor token on your behalf and create the group in Entra ID.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This blog discussed a critical token validation failure in the Azure AD Graph API. While the vulnerability itself was a bad oversight in the token handling, the whole concept of Actor tokens is a protocol that was designed to behave with all the properties mentioned in the paragraphs above. If it weren‚Äôt for the complete lack of security measures in these tokens, I don‚Äôt think such a big impact with such limited telemetry would have been possible.&lt;/p&gt;
    &lt;p&gt;Thanks to the people at MSRC who immediately picked up the vulnerability report, searched for potential variants in other resources, and to the engineers who followed up with fixes for the Azure AD Graph and blocked Actor tokens for the Azure AD Graph API requested with credentials stored on Service Principals, essentially restricting the usage of these Actor tokens to only Microsoft internal services.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure timeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;July 14, 2025 - reported issue to MSRC.&lt;/item&gt;
      &lt;item&gt;July 14, 2025 - MSRC case opened.&lt;/item&gt;
      &lt;item&gt;July 15, 2025 - reported further details on the impact.&lt;/item&gt;
      &lt;item&gt;July 15, 2025 - MSRC requested to halt further testing of this vulnerability.&lt;/item&gt;
      &lt;item&gt;July 17, 2025 - Microsoft pushed a fix for the issue globally into production.&lt;/item&gt;
      &lt;item&gt;July 23, 2025 - Issue confirmed as resolved by MSRC.&lt;/item&gt;
      &lt;item&gt;August 6, 2025 - Further mitigations pushed out preventing Actor tokens being issued for the Azure AD Graph with SP credentials.&lt;/item&gt;
      &lt;item&gt;September 4, 2025 - CVE-2025-55241 issued.&lt;/item&gt;
      &lt;item&gt;September 17, 2025 - Release of this blogpost.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;I do not have access to any tenants in a national cloud deployment, so I was not able to test whether the vulnerability existed there. Since national cloud deployments use their own token signing keys, it is unlikely that it would have been possible to execute this attack from a tenant in the public cloud to one of these national clouds. I do consider it likely that this attack would have worked across tenants in the same national cloud deployments, but that is speculation. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45282497</guid><pubDate>Wed, 17 Sep 2025 23:03:21 +0000</pubDate></item><item><title>Stepping Down as Libxml2 Maintainer</title><link>https://discourse.gnome.org/t/stepping-down-as-libxml2-maintainer/31398</link><description>&lt;doc fingerprint="1f9f5cdbc00696ce"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;nwellnhof
(Nick Wellnhofer)
1&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I‚Äôm stepping down as maintainer of libxml2 which means that this project is more or less unmaintained for now.&lt;/p&gt;
        &lt;p&gt;I will fix regressions in the 2.15 release until the end of 2025.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 17 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Thank you for your hard work!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;mcatanzaro
(Michael Catanzaro)
3&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Yes, thank you for maintaining libxml2 for such a long time!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;imcsk8
(Iv√°n Chavero)
4&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Hello, since I‚Äôve stepped in as libxslt maintainer I‚Äôve been studying both libxslt and libxml2 codebases. I have the time to maintain the library I just need to get familiar with the latest changes you introduced like:&lt;/p&gt;
        &lt;p&gt;I haven‚Äôt find how to manage both output and input buffers. I found functions like: xmlOutputBufferCreateIO but by the places in which I‚Äôve found them is not clear on how to use them.&lt;/p&gt;
        &lt;p&gt;Should I send you an email with my questions or do you prefer other means of communication?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 6 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;sri
(sri)
5&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Thank you Nick for maintaining the key libraries of the internet and used in millions of products globaly. Best of luck to you.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;I am one of those millions of people that use this library on the behalf of us Thank you very much!!&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45283196</guid><pubDate>Thu, 18 Sep 2025 00:17:20 +0000</pubDate></item><item><title>Meta Ray-Ban Display</title><link>https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45283306</guid><pubDate>Thu, 18 Sep 2025 00:30:44 +0000</pubDate></item><item><title>Hypervisor 101 in Rust</title><link>https://tandasat.github.io/Hypervisor-101-in-Rust/</link><description>&lt;doc fingerprint="8eb71d9bf88743ae"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome to Hypervisor 101 in Rust&lt;/head&gt;
    &lt;p&gt;This is a day long course to quickly learn the inner working of hypervisors and techniques to write them for high-performance fuzzing.&lt;/p&gt;
    &lt;p&gt;This course covers foundation of hardware-assisted virtualization technologies, such as VMCS/VMCB, guest-host world switches, EPT/NPT, as well as useful features and techniques such as exception interception for virtual machine introspection for fuzzing.&lt;/p&gt;
    &lt;p&gt;The class is made up of lectures using the materials within this directory and hands-on exercises with source code under the &lt;code&gt;Hypervisor-101-in-Rust/hypervisor&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;This lecture materials are written for the &lt;code&gt;gcc2023&lt;/code&gt; branch, which notionally have incomplete code for step-by-step exercises. Check out the starting point of the branch as below to go over hands-on exercises before you start.&lt;/p&gt;
    &lt;code&gt;git checkout b17a59dd634a7b0c2b9a6d493fc9b0ff22dcfce5
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45283731</guid><pubDate>Thu, 18 Sep 2025 01:18:58 +0000</pubDate></item><item><title>Show HN: The text disappears when you screenshot it</title><link>https://unscreenshottable.vercel.app/?text=Hello</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45284311</guid><pubDate>Thu, 18 Sep 2025 02:18:45 +0000</pubDate></item></channel></rss>