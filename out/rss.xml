<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 14 Dec 2025 14:38:24 +0000</lastBuildDate><item><title>I tried Gleam for Advent of Code</title><link>https://blog.tymscar.com/posts/gleamaoc2025/</link><description>&lt;doc fingerprint="4758503509883eb8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Tried Gleam for Advent of Code, and I Get the Hype&lt;/head&gt;
    &lt;p&gt;I do Advent of Code every year.&lt;/p&gt;
    &lt;p&gt;For the last seven years, including this one, I have managed to get all the stars. I do not say that to brag. I say it because it explains why I keep coming back.&lt;/p&gt;
    &lt;p&gt;It is one of the few tech traditions I never get bored of, even after doing it for a long time. I like the time pressure. I like the community vibe. I like that every December I can pick one language and go all in.&lt;/p&gt;
    &lt;p&gt;This year, I picked Gleam.&lt;/p&gt;
    &lt;head rend="h2"&gt;A much shorter year#&lt;/head&gt;
    &lt;p&gt;Advent of Code is usually 25 days. This year Eric decided to do 12 days instead.&lt;/p&gt;
    &lt;p&gt;So instead of 50 parts, it was 24.&lt;/p&gt;
    &lt;p&gt;That sounds like a relaxed year. It was not, but not in a bad way.&lt;/p&gt;
    &lt;p&gt;The easier days were harder than the easy days in past years, but they were also really engaging and fun to work through. The hard days were hard, especially the last three, but they were still the good kind of hard. They were problems I actually wanted to wrestle with.&lt;/p&gt;
    &lt;p&gt;It also changes the pacing in a funny way. In a normal year, by day 10 you have a pretty comfy toolbox. This year it felt like the puzzles were already demanding that toolbox while I was still building it.&lt;/p&gt;
    &lt;p&gt;That turned out to be a perfect setup for learning a new language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Gleam felt like a good AoC language#&lt;/head&gt;
    &lt;p&gt;Gleam is easy to like quickly.&lt;/p&gt;
    &lt;p&gt;The syntax is clean. The compiler is helpful, and the error messages are super duper good. Rust good.&lt;/p&gt;
    &lt;p&gt;Most importantly, the language strongly nudges you into a style that fits Advent of Code really well. Parse some text. Transform it a few times. Fold. Repeat.&lt;/p&gt;
    &lt;p&gt;Also, pipes. Pipes everywhere. I love pipes.&lt;/p&gt;
    &lt;p&gt;One thing I did not expect was how good the editor experience would be. The LSP worked much better than I expected. It basically worked perfectly the whole time. I used the Gleam extension for IntelliJ and it was great.&lt;/p&gt;
    &lt;p&gt;https://plugins.jetbrains.com/plugin/25254-gleam-language&lt;/p&gt;
    &lt;p&gt;I also just like FP.&lt;/p&gt;
    &lt;p&gt;FP is not always easier, but it is often easier. When it clicks, you stop writing instructions and you start describing the solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;The first Gleam superpower: &lt;code&gt;echo&lt;/code&gt;#&lt;/head&gt;
    &lt;p&gt;The first thing I fell in love with was &lt;code&gt;echo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It is basically a print statement that does not make you earn it. You can &lt;code&gt;echo&lt;/code&gt; any value. You do not have to format anything. You do not have to build a string. You can just drop it into a pipeline and keep going.&lt;/p&gt;
    &lt;p&gt;This is the kind of thing I mean:&lt;/p&gt;
    &lt;code&gt;list.range(0, 5)
|&amp;gt; echo
|&amp;gt; list.map(int.to_string)
|&amp;gt; echo
&lt;/code&gt;
    &lt;p&gt;You can quickly inspect values at multiple points without breaking the flow.&lt;/p&gt;
    &lt;p&gt;I did miss string interpolation, especially early on. &lt;code&gt;echo&lt;/code&gt; made up for a lot of that.&lt;/p&gt;
    &lt;p&gt;It mostly hit when I needed to generate text, not when I needed to inspect values. The day where I generated an LP file for &lt;code&gt;glpsol&lt;/code&gt; is the best example. It is not hard code, but it is a lot of string building. Without interpolation it turns into a bit of a mess of &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;s.&lt;/p&gt;
    &lt;p&gt;This is a small excerpt from my LP generator:&lt;/p&gt;
    &lt;code&gt;"Minimize\n"
&amp;lt;&amp;gt; "  total: "
&amp;lt;&amp;gt; buttons
|&amp;gt; string.join(" + ")
&amp;lt;&amp;gt; "\n\nSubject To\n"
&lt;/code&gt;
    &lt;p&gt;It works. It is just the kind of code where you really feel missing interpolation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Options everywhere, and why that matters for grid puzzles#&lt;/head&gt;
    &lt;p&gt;A lot of AoC is grids.&lt;/p&gt;
    &lt;p&gt;Grids are where you normally either crash into out of bounds bugs, or you litter your code with bounds checks you do not care about.&lt;/p&gt;
    &lt;p&gt;In my day 4 solution I used a dict as a grid. The key ergonomic part is that &lt;code&gt;dict.get&lt;/code&gt; gives you an option-like result, which makes neighbour checking safe by default.&lt;/p&gt;
    &lt;p&gt;This is the neighbour function from my solution:&lt;/p&gt;
    &lt;code&gt;fn get_neighbours(grid: Grid(Object), pos: Position) -&amp;gt; List(Object) {
  [
    #(pos.0 - 1, pos.1 - 1),
    #(pos.0 - 1, pos.1),
    #(pos.0 - 1, pos.1 + 1),
    #(pos.0, pos.1 - 1),
    #(pos.0, pos.1 + 1),
    #(pos.0 + 1, pos.1 - 1),
    #(pos.0 + 1, pos.1),
    #(pos.0 + 1, pos.1 + 1),
  ]
  |&amp;gt; list.filter_map(fn(neighbour_pos) { grid |&amp;gt; dict.get(neighbour_pos) })
}
&lt;/code&gt;
    &lt;p&gt;That last line is the whole point.&lt;/p&gt;
    &lt;p&gt;No bounds checks. No sentinel values. Out of bounds just disappears.&lt;/p&gt;
    &lt;head rend="h2"&gt;The list toolbox is genuinely good#&lt;/head&gt;
    &lt;p&gt;I expected to write parsers and helpers, and I did. What I did not expect was how often Gleam already had the exact list function I needed.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;list.transpose&lt;/code&gt; saved a whole day#&lt;/head&gt;
    &lt;p&gt;Day 6 part 1 was basically a transpose problem in disguise.&lt;/p&gt;
    &lt;p&gt;I read the input, chunked it into rows, transposed it, and suddenly the rest of the puzzle became obvious.&lt;/p&gt;
    &lt;code&gt;input
|&amp;gt; list.transpose
|&amp;gt; list.map(fn(line) { line |&amp;gt; calculate_instruction })
|&amp;gt; bigi.sum
&lt;/code&gt;
    &lt;p&gt;In a lot of languages you end up writing your own transpose yet again. In Gleam it is already there.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;list.combination_pairs&lt;/code&gt; is a cheat code#&lt;/head&gt;
    &lt;p&gt;Another example is &lt;code&gt;list.combination_pairs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In day 8 I needed all pairs of 3D points. In an imperative language you would probably write nested loops and then question your off by one logic.&lt;/p&gt;
    &lt;p&gt;In Gleam it is a one liner:&lt;/p&gt;
    &lt;code&gt;boxes
|&amp;gt; list.combination_pairs
&lt;/code&gt;
    &lt;p&gt;Sometimes FP is not about being clever. It is about having the right function name.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;fold_until&lt;/code&gt; is my favorite thing I found#&lt;/head&gt;
    &lt;p&gt;If I had to pick one feature that made me want to keep writing Gleam after AoC, it is &lt;code&gt;fold_until&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Early exit without hacks is fantastic in puzzles.&lt;/p&gt;
    &lt;p&gt;In day 8 part 2 I kept merging sets until the first set in the list contained all boxes. When that happens, I stop.&lt;/p&gt;
    &lt;p&gt;The core shape looks like this:&lt;/p&gt;
    &lt;code&gt;|&amp;gt; list.fold_until(initial, fn(acc, pair) {
  case done_yet {
    True -&amp;gt; Stop(new_acc)
    False -&amp;gt; Continue(new_acc)
  }
})
&lt;/code&gt;
    &lt;p&gt;It is small, explicit, and it reads like intent.&lt;/p&gt;
    &lt;p&gt;I also used &lt;code&gt;fold_until&lt;/code&gt; in day 10 part 1 to find the smallest combination size that works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Gleam fought me a bit#&lt;/head&gt;
    &lt;p&gt;Even though I enjoyed Gleam a lot, I did hit a few recurring friction points.&lt;/p&gt;
    &lt;p&gt;None of these are deal breakers. They are just the kind of things you notice when you do 24 parts in a row.&lt;/p&gt;
    &lt;head rend="h3"&gt;File IO is not in the standard library#&lt;/head&gt;
    &lt;p&gt;This one surprised me on day 1.&lt;/p&gt;
    &lt;p&gt;For AoC you read a file every day. In this repo I used &lt;code&gt;simplifile&lt;/code&gt; everywhere because you need something. It is fine, I just did not expect basic file IO to be outside the standard library.&lt;/p&gt;
    &lt;head rend="h3"&gt;Regex is a dependency too#&lt;/head&gt;
    &lt;p&gt;Day 2 part 2 pushed me into regex and I had to add &lt;code&gt;gleam_regexp&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is the style I used, building a regex from a substring:&lt;/p&gt;
    &lt;code&gt;let assert Ok(re) = regexp.from_string("^(" &amp;lt;&amp;gt; substring &amp;lt;&amp;gt; ")+$")
regexp.check(re, val)
&lt;/code&gt;
    &lt;p&gt;Again, totally fine. It just surprised me.&lt;/p&gt;
    &lt;head rend="h3"&gt;List pattern matching limitations#&lt;/head&gt;
    &lt;p&gt;You can do &lt;code&gt;[first, ..rest]&lt;/code&gt; and you can do &lt;code&gt;[first, second]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;But you cannot do &lt;code&gt;[first, ..middle, last]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It is not the end of the world, but it would have made some parsing cleaner.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comparisons are explicit#&lt;/head&gt;
    &lt;p&gt;In Gleam a lot of comparisons are not booleans. You get an &lt;code&gt;order&lt;/code&gt; value.&lt;/p&gt;
    &lt;p&gt;This is great for sorting. It is also very explicit. It can be a bit verbose when you just want an &lt;code&gt;&amp;lt;=&lt;/code&gt; check.&lt;/p&gt;
    &lt;p&gt;In day 5 I ended up writing patterns like this:&lt;/p&gt;
    &lt;code&gt;case cmp_start, cmp_end {
  order.Lt, _ -&amp;gt; False
  _, order.Gt -&amp;gt; False
  _, _ -&amp;gt; True
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Big integers, and targeting JavaScript#&lt;/head&gt;
    &lt;p&gt;I used &lt;code&gt;bigi&lt;/code&gt; a few times this year.&lt;/p&gt;
    &lt;p&gt;On the Erlang VM, integers are arbitrary precision, so you usually do not care about overflow. That is one of the nicest things about the BEAM.&lt;/p&gt;
    &lt;p&gt;If you want your Gleam code to also target JavaScript, you do care. JavaScript has limits, and suddenly using &lt;code&gt;bigi&lt;/code&gt; becomes necessary for some puzzles.&lt;/p&gt;
    &lt;p&gt;I wish that was just part of &lt;code&gt;Int&lt;/code&gt;, with a single consistent story across targets.&lt;/p&gt;
    &lt;head rend="h2"&gt;The most satisfying part: XOR as bitmasks#&lt;/head&gt;
    &lt;p&gt;Day 10 part 1 was my favorite part of the whole event.&lt;/p&gt;
    &lt;p&gt;The moment I saw the toggling behavior, it clicked as XOR. Represent the lights as a number. Represent each button as a bitmask. Find the smallest combination of bitmasks that XOR to the target.&lt;/p&gt;
    &lt;p&gt;This is the fold from my solution:&lt;/p&gt;
    &lt;code&gt;combination
|&amp;gt; list.fold(0, fn(acc, comb) {
  int.bitwise_exclusive_or(acc, comb)
})
&lt;/code&gt;
    &lt;p&gt;It felt clean, it felt fast, and it felt like the representation did most of the work.&lt;/p&gt;
    &lt;head rend="h2"&gt;The least satisfying part: shelling out to &lt;code&gt;glpsol&lt;/code&gt;#&lt;/head&gt;
    &lt;p&gt;Day 10 part 2 was the opposite feeling.&lt;/p&gt;
    &lt;p&gt;I knew brute force was out. It was clearly a system of linear equations.&lt;/p&gt;
    &lt;p&gt;In previous years I would reach for Z3, but there are no Z3 bindings for Gleam. I tried to stay in Gleam, and I ended up generating an LP file and shelling out to &lt;code&gt;glpsol&lt;/code&gt; using &lt;code&gt;shellout&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It worked, and honestly the LP format is beautiful.&lt;/p&gt;
    &lt;p&gt;Here is the call:&lt;/p&gt;
    &lt;code&gt;let _ =
  shellout.command(
    "glpsol",
    ["--lp", "temp.lp", "-w", "temp_sol.txt"],
    ".",
    [],
  )
&lt;/code&gt;
    &lt;p&gt;It is a hack, but it is a pragmatic hack, and that is also part of Advent of Code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memoization keys that actually model the problem#&lt;/head&gt;
    &lt;p&gt;Day 11 part 2 is where I was happy I was writing Gleam.&lt;/p&gt;
    &lt;p&gt;The important detail was that the memo key is not just the node. It is the node plus your state.&lt;/p&gt;
    &lt;p&gt;In my case the key was:&lt;/p&gt;
    &lt;code&gt;#(neighbour, new_seen_dac, new_seen_fft)
&lt;/code&gt;
    &lt;p&gt;Once I got the memo threading right, it ran instantly.&lt;/p&gt;
    &lt;head rend="h2"&gt;The finale, and the troll heuristic#&lt;/head&gt;
    &lt;p&gt;The last day was the only puzzle I did not fully enjoy.&lt;/p&gt;
    &lt;p&gt;Not because it was bad. It just felt like it relied on assumptions about the input, and I am one of those people that does not love doing that.&lt;/p&gt;
    &lt;p&gt;I overthought it for a bit, then I learned it was more of a troll problem. The “do the areas of the pieces, when fully interlocked, fit on the board” heuristic was enough.&lt;/p&gt;
    &lt;p&gt;In my solution it is literally this:&lt;/p&gt;
    &lt;code&gt;heuristic_area &amp;lt;= max_area
&lt;/code&gt;
    &lt;p&gt;Sometimes you build a beautiful mental model and then the right answer is a single inequality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing thoughts#&lt;/head&gt;
    &lt;p&gt;I am very happy I picked Gleam this year.&lt;/p&gt;
    &lt;p&gt;It has sharp edges, mostly around where the standard library draws the line and a few language constraints that show up in puzzle code. But it also has real strengths.&lt;/p&gt;
    &lt;p&gt;Pipelines feel good. Options and Results make unsafe problems feel safe. The list toolbox is better than I expected. &lt;code&gt;fold_until&lt;/code&gt; is incredible. Once you stop trying to write loops and you let it be functional, the solutions start to feel clearer.&lt;/p&gt;
    &lt;p&gt;I cannot wait to try Gleam in a real project. I have been thinking about using it to write a webserver, and I am genuinely excited to give it a go.&lt;/p&gt;
    &lt;p&gt;And of course, I cannot wait for next year’s Advent of Code.&lt;/p&gt;
    &lt;p&gt;If you want to look at the source for all 12 days, it is here:&lt;/p&gt;
    &lt;p&gt;https://github.com/tymscar/Advent-Of-Code/tree/master/2025/gleam/aoc/src&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46255991</guid><pubDate>Sat, 13 Dec 2025 17:00:14 +0000</pubDate></item><item><title>EasyPost (YC S13) Is Hiring</title><link>https://www.easypost.com/careers</link><description>&lt;doc fingerprint="e60e91b34b3de70c"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance shipping made easy&lt;/p&gt;
    &lt;head rend="h3"&gt;Dear Job Seekers,&lt;/head&gt;
    &lt;p&gt;We want to ensure your safety and protect you from potential scams. Recently, there have been fraudulent recruitment initiatives online that impersonate our company. These scams aim to deceive unsuspecting applicants by offering nonexistent positions and requesting personal information or upfront fees.&lt;/p&gt;
    &lt;p&gt;Remember that our company does not endorse any job postings outside our official channels. If you encounter a suspicious offer, report it through the job platform on which you found it or report email as spam.&lt;/p&gt;
    &lt;p&gt;If you need to check on the validity of an email from EasyPost, feel free to reach out directly to recruiting@easypost.com&lt;/p&gt;
    &lt;p&gt;For more information on this scam, please see this FTC Consumer Alert.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future of you&lt;/head&gt;
    &lt;p&gt;As industry experts, we’re working not only to help our customers make sense of the industry, but to define where it’s headed. We are looking for candidates who are approachable, dynamic, inventive, intelligent, and reliable to join our team in unpacking the future of shipping.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future of shipping&lt;/head&gt;
    &lt;p&gt;How can modern, flexible technology improve the customer experience of shipping? What if every business was able to offer same-day shipping? How much waste would be removed from the environment if all our shipments were consolidated into one delivery per week? At EasyPost, we’re figuring out the answer to these questions and more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Life at EasyPost&lt;/head&gt;
    &lt;head rend="h4"&gt;Adaptive&lt;/head&gt;
    &lt;p&gt;Embrace new challenges to grow your skill set.&lt;/p&gt;
    &lt;head rend="h4"&gt;Simple&lt;/head&gt;
    &lt;p&gt;Create efficient solutions that are easy to execute.&lt;/p&gt;
    &lt;head rend="h4"&gt;Inclusive&lt;/head&gt;
    &lt;p&gt;Share new ideas and work collaboratively across teams.&lt;/p&gt;
    &lt;head rend="h2"&gt;Team and technology&lt;/head&gt;
    &lt;p&gt;We’re a fun group of passionate entrepreneurs who built our own revolutionary software designed to make shipping simple. EasyPost started as an Engineering first company and we are proud to have a pragmatic approach to software development. Our team has a wealth of diverse experience and different backgrounds ranging from startups to large technology companies.Be part of a leading technology company:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CI/CD inspired workflows – we deploy dozens of times a day&lt;/item&gt;
      &lt;item&gt;Small services over monoliths – we’ve deployed hundreds of services&lt;/item&gt;
      &lt;item&gt;Strong engineering tooling and developer support&lt;/item&gt;
      &lt;item&gt;Transparency and participation around architecture and technology decisions&lt;/item&gt;
      &lt;item&gt;Culture of blamelessness and improving today from yesterday’s shortcomings&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46256002</guid><pubDate>Sat, 13 Dec 2025 17:01:25 +0000</pubDate></item><item><title>The Rise of Computer Games, Part I: Adventure</title><link>https://technicshistory.com/2025/12/13/the-rise-of-computer-games-part-i-adventure/</link><description>&lt;doc fingerprint="a702114962a38afd"&gt;
  &lt;main&gt;
    &lt;p&gt;Author’s note: I originally intended for this post to cover adventure games, computer role-playing games, wargames and other simulations, a brief look at the home video game market, and finally the rise of hybrids that fused home video game systems with personal computers. In the grand scheme of the story about personal computers that I am trying to tell, it probably does not make sense to lavish nearly 7,000 words on early adventure games alone, but it’s a topic of personal interest to me and the tale grew in the telling.&lt;/p&gt;
    &lt;p&gt;Play was central to the formation of personal computer culture. For the early hobbyists who were fascinated by the guts of the machine, the computer was a plaything in and of itself. Many of those who joined the hobby in 1975 or 1976 did so because of games: they had experience with the extensive BASIC game culture that circulated in the time-sharing systems of universities, high schools, and even corporations, and wanted to keep playing at home.&lt;/p&gt;
    &lt;p&gt;Even after the rise of commercial personal computer software, when the first truly useful applications began appearing, games remained by far the most popular software category (counting by number of titles produced and number of units sold, although not by dollar value). One 1980 catalog of Apple II software, for example, lists 265 titles, of which roughly two-thirds are games, from Ack-Ack (an anti-aircraft target shooter)to Wipe Off (a Breakout clone). The rest of the catalog comprises demos, educational programs, and a smattering of business software. Whatever they might say about the practical value of the personal computer, buyers had an evident hunger for games.[1]&lt;/p&gt;
    &lt;head rend="h3"&gt;The Early Games and Their Market&lt;/head&gt;
    &lt;p&gt;Computer owners got their hands on games in one of three ways. In the early years, the most common means would be simply copying a paper or cassette tape from a friend or colleague, whether with the permission of the original author or not. In the early years, most hobbyists treated game software as a commons to be freely shared, just as it had been in the time-sharing culture through cooperatives like DECUS. This peer-to-peer copying would never entirely go away, despite the commercialization of game software and various schemes by publishers to try to prevent it.&lt;/p&gt;
    &lt;p&gt;Many magazines and books also published “type-ins,” complete computer programs (almost always written in BASIC) intended to be manually entered at the keyboard (and then saved to tape or disk), and these, too, were most often games. Dave Ahl’s BASIC Computer Games (first published in 1973 by Digital Equipment Corporation), a collection of over 100 type-ins, reputedly sold one million copies by 1979. Though type-in publication continued through the 1980s, the inherent limits on the length of such programs (only the most dedicated would tackle a type-in that was more than a few hundred lines long) and their reliance on the universality of BASIC (rather than more performant compiled languages) meant that their significance waned as the sophistication of the game market increased. They could serve as fun demos or educational tools for learning to code, but could not compare to similar games available commercially.[2]&lt;/p&gt;
    &lt;p&gt;Finally, of course, there were the commercial titles offered by software publishers. The game business began in the same way as the personal computer hardware business: with hobby-entrepreneurs selling their creations to fellow hobbyists. In July 1976, for example, D.E. Hipps of Miami, Florida offered a Star Trek game written for MicroSoft’s Altair BASIC for $10 (no one at this stage of the industry paid any attention to niceties such as licensing agreements for the use of the Star Trek name). No common standard data storage standard existed; hobbyists employed a mix of paper teletype tapes, cassette storage, and (for the most extravagant) floppy disks. So Hipps opted to distribute his game as printed source code: a type-in! SCELBI (creators of one of the early, pre-Altair hobby computers), offered another Star Trek variant called Galaxy in the same form. By the late 1970s, the convergence of the industry on a small number of popular storage standards (with CP/M dominant) resolved this problem, and most games were distributed in plastic baggies containing instructions and a cassette or floppy disk.[3]&lt;/p&gt;
    &lt;p&gt;It didn’t take long for other entrepreneurs to see a business opportunity in making it easier for software authors to publish their games. It took some time for clear business models and market verticals to emerge. No categorial distinction existed between publishers of games and publishers of utility and business software prior to 1980: Personal Software’s first big hit was MicroChess, followed by VisiCalc, followed by (as we’ll soon see) Zork. Programma International’s founder began as a hoarder of Apple II software, much of it acquired from copies unauthorized by the original author, then turned legitimate to sell those authors’ software instead. Softape tried selling bundles of software by subscription, and then started its own newsletter for subscribers, Softalk.&lt;/p&gt;
    &lt;p&gt;Some magazines went the other way around: Softside magazine (located the next town over from BYTE’s Peterborough, New Hampshire headquarters) created The Software Exchange (TSE), while Dave Ahl’s Creative Computing set up a label called Sensational Software. Type-ins printed in the magazines became a gateway drug to more convenient (and often more complex and interesting) software available for sale on cassette or diskette.[4]&lt;/p&gt;
    &lt;p&gt;Figure 21: Creative Computing heavily advertised the Sensational Software brand in the pages of the magazine, as in this July 1980 example describing some of their most popular hits and offering a free catalog of their full offering of 400 titles.&lt;/p&gt;
    &lt;p&gt;The early personal computer game culture imitated what came before it. The boundary between mini- and microcomputer culture was permeated by thousands who used time-sharing systems at work or school and then went home to a hobby computer. Prior to 1977, a game written for a personal computer was almost invariably based on a game drawn from the other side of that boundary.&lt;/p&gt;
    &lt;p&gt;Barring a few exceptions (such as the PLATO system available at some universities), users interacted with such computer systems through teletypes or video teletypes that alternated sending and receiving text. So, the resulting games were turn-based, purely textual, and relied on strategy and calculation (or pure luck) to win, not timing and reaction speed. These textual games suited the early hobbyists perfectly, since almost all of their computers also had text-only interfaces, whether mechanical teletypes or video displays like the TV Typewriter.&lt;/p&gt;
    &lt;p&gt;Other than simple quizzes, demos, and guessing games, popular titles included simulations such as Hammurabi, Civil War and Lunar Lander; statistical recreations of sports contests (baseball, basketball, golf, etc.); or classic games or puzzles from the physical world, like checkers, Yahtzee, and the towers of Hanoi. By far the most popular by far, however, judging by the number of variations published and references in hobby magazines, were descendants of Mike Mayfield’s 1971 Star Trek, a strategic game of galactic war against the Klingons.[5]&lt;/p&gt;
    &lt;p&gt;Figure 22:&lt;/p&gt;
    &lt;p&gt;Some early personal computers, however, had video controllers with built-in memory, which allowed for more sophisticated interfaces than the simple back-and-forth exchanges of a teletype. Processor Technology, whose VDM-1 display interface could paint characters at arbitrary points on the screen, sold real-time textual games by Steve Dompier like Trek-80 (released in 1976, despite the name). Its interface (including a galactic sector map made of text characters and readouts of the Enterprise’s weapon and shield status) updated in real-time in response to player and (simulated) enemy actions, rather than scrolling by one turn at a time. Cromemco, maker of the Dazzler, an Altair-compatible graphics board, offered the only personal computer games to use pixel graphics prior to the Apple II, starting with a version of the seminal Spacewar in early 1977. They followed with a suite of similar games such as Tankwar and Dogfight.[6]&lt;/p&gt;
    &lt;p&gt;After 1977, when computers with graphical displays became more widely available (especially the full-color Apple II), computer games tapped a new vein of inspiration (and imitation): arcade games. Originally commercialized by Atari and its imitators as standalone arcade cabinets in the early 1970s, then moving into homes by the mid-1970s, these games were typically real-time and focused on action. Relatively cheap and easy-to-make, and relatively disposable to the user (few took more than a few minutes to play a complete game), computer action games proliferated by the hundreds and thousands, many of them direct or near clones of pre-existing arcade or home video games.&lt;/p&gt;
    &lt;p&gt;By 1980, however, there were major innovations that set personal computer games apart from other game media. In-depth simulations, expansive adventures that took hours to solve, and dungeon crawls teeming with a variety of monsters, treasures, and traps provided immersive experiences that the action-oriented video game consoles, did not, and (given their limited memory and storage capacity) could not provide. Once combined with full-color, bitmapped graphics, these games also surpassed anything previously available on their time-sharing predecessors. The era of imitation was definitively over.&lt;/p&gt;
    &lt;head rend="h3"&gt;Adventure&lt;/head&gt;
    &lt;p&gt;For several years of my childhood, for reasons that I no longer recall, our family’s Apple IIe computer, equipped with a green-and-black monochrome monitor, resided in my bedroom. Though much of my autobiographical memory is quite hazy, I can clearly remember each of the Apple II games we owned, stored on 5 ¼-inch-square floppy disks: Syzygy (a space shooter in the vein of Asteroids), One on One: Dr. J vs. Larry Bird, Winter Games, and Arcticfox (a sci-fi tank simulator with wireframe graphics).&lt;/p&gt;
    &lt;p&gt;But the game that truly captured my imagination, the game whose opening sequence and imagery remain etched (monochromatically) in my mind, was King’s Quest II: Romancing the Throne, a 1985 title by Sierra On-Line. The forty-nine-screen, hand-drawn fairy tale kingdom that you explore in the game (via your avatar, King Graham) felt like a vast world of endless possibility compared to the cramped half-court of One on One, the endlessly repeating monotony of a biathlon course in Winter Games, or the sterile polygonal landscape of Arcticfox’s Antarctica. That open-ended feeling was enhanced by the lure of hidden secrets just out of reach, and a freeform text interface that accepted English commands like “THROW APPLE” (though only a tiny subset of the commands you could imagine would actually work). Despite its limitations and many, many frustrations (at age seven or eight, with no hint book and no Internet walkthroughs, I certainly never came close to completing it), it made me feel that I was truly experiencing an adventure.&lt;/p&gt;
    &lt;p&gt;The adventure game genre originated in a freely shared, text-driven game created in the time-sharing world. The game, which I will call Adventure (it is variously called Colossal Cave Adventure, Colossal Cave, Adventure, or simply ADVENT, after the game’s PDP-10 file name)challenged players to find five treasures within a cave complex by navigating a maze, solving puzzles, and defeating a band of axe-wielding dwarves Its author was Will Crowther, a programmer at Bolt, Beranek and Newman (BBN), where he had written core infrastructural software for ARPANET, the first nationwide computer network.&lt;/p&gt;
    &lt;p&gt;In 1975, Crowther went through a painful divorce. He had always enjoyed playing games with his school age daughters, so he began crafting a game on the company’s DEC PDP-10 to help him stay connected with them. Crowther copied the physical structure of Adventure’s cave directly from a portion of the Mammoth complex in Kentucky. (He had met his wife through caving, and they had explored Mammoth together, so the game was also, in a sense, a means of staying connected to his former, married life.) It is probable (though not certain) that Crowther also drew some inspiration from a popular 1973 time-sharing game called Hunt the Wumpus, which required users to use textual clues to find and kill a Wumpus hidden in a system of caves without falling into a pit. But the conceptual structure of Adventure (delving into the earth to find treasure and magical artifacts in the face of devious obstacles and armed foes) came from a new game of pencil, paper, and imagination that Crowther was playing with some of his BBN friends, called Dungeons and Dragons.[7]&lt;/p&gt;
    &lt;p&gt;In Crowther’s words:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…the caving had stopped, because that had become awkward, so I decided I would fool around and write a program that was a re-creation in fantasy of my caving, and also would be a game for the kids, and perhaps had some aspects of the Dungeons and Dragons that I had been playing.[8]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Just as in the later King’s Quest II, the player used simple verb-noun commands (such as “TAKE LAMP”) to interact with the world, but lacking a graphical screen with a visible avatar, he or she also used text commands to move about the world, from one room of the cave to the next (e.g., “SOUTH” or “EAST”). Crowther showed the game off to his D&amp;amp;D buddies and his daughters, then took a new job in California, and forgot about it.[9]&lt;/p&gt;
    &lt;p&gt;Time-sharing games had once propagated gradually from computer to computer via collectives like the Digital Equipment Computer Users’ Society or colleagues and friends mailing paper tapes to one another. But BBN was on the ARPANET, and Crowther had put his game on a public directory in the BBN computer. From there, someone copied it across the network to a computer in a Stanford lab, where a graduate student, Don Woods, found it in early 1977.&lt;/p&gt;
    &lt;p&gt;Fascinated by Crowther’s game, Woods contacted him for the FORTRAN source code, and set about expanding it. He increased the scope by adding more rooms, more puzzles, more foes, and more ways to interact with the world; but he also added the ability to save your progress and a point-tracking system with a final objective: to find fifteen treasures and return them to the starting location. Woods’ larger, more polished version of Adventure spread rapidly across the time-sharing world, and became an obsession for some, keeping them at the office past midnight in search of that last treasure. (One of Woods’ additions was a setting to allow admins to disable the game during working hours.)[10]&lt;/p&gt;
    &lt;head rend="h3"&gt;Adventureland&lt;/head&gt;
    &lt;p&gt;A frizzy-haired Florida man, Scott Adams, was the first to commercialize a version of Adventure for the personal computer. He had first fallen in love with computers on a time-sharing terminal at his Miami high school in the late 1960s. He went on to earn a computer science degree and by the late 1970s, was working as a programmer at telecom manufacturer Stromberg-Carlson. On the side he had become an avid home computer hobbyist, purchasing a Sphere computer in 1975 and then a TRS-80 in 1977. Shortly thereafter he discovered Adventure on the company time-sharing system and, like many before and after, could not quit playing until he had beaten it.&lt;/p&gt;
    &lt;p&gt;Adams decided that it would be an interesting challenge to build something similar for the TRS-80. It would have to be much smaller to fit in the sixteen kilobytes of memory he had available. The Crowther-Woods Adventure contained 140 distinct locations and ran to eighty kilobytes of (uncompiled) FORTRAN and fifty-four kilobytes of data for the game text. Adams’ Adventureland was considerably smaller, with fewer than thirty-five locations—not necessarily to the detriment of gameplay; for example, the cutting lopped off most of Adventure’s huge and torturous mazes.[11]&lt;/p&gt;
    &lt;p&gt;Adams’ local TRS-80 buddies were impressed enough with his game that he decided to sell it through both The TRS-80 Software Exchange and Creative Computing, who offered it on cassette for $24.95 and $14.95, respectively, in their January 1979 magazine issues. He followed up with a whole series of games, starting with Pirate Adventure, and ported the games from the TRS-80 to other popular computer platforms. His wife Alexis joined the venture as a business manager and game designer, co-authoring Mystery Fun House and Voodoo Castle.[12]&lt;/p&gt;
    &lt;p&gt;The adventure game genre is often criticized for absurd and unfair puzzles, which can be guessed at only through trial-and-error, and tedious mazes or other navigational obfuscations. These early games from circa 1980 are among the worst offenders. In Adventureland, for example, a “very thin” black bear blocks your way, and the only way to get past it is to “yell” at it. Feeding this apparently hungry bear honey will prevent you from completing the game, because the honey is one of the treasures you must collect. You could easily get to state in these games where you have lost the game without knowing it.[13]&lt;/p&gt;
    &lt;p&gt;But these criticisms are retrospective: the contemporary press and the buying public lapped up the Adams’ adventures and all of their imitators. We have to remember that the appeal of this genre lay in getting immersed (one might say “lost”) in the game for hours every evening, clawing your way forward towards ultimate triumph for weeks, or even months, on end. In a market full of arcade-like games that offered the convenient but shallow fun of a bag of potato chips, adventure games provided a rich and fulfilling meal for the imagination. As one lover of the genre put it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Adventure is the product of imagination appealing to imagination. It is not just the puzzle, or the theme, or the nonplayer characters and their personalities. It is a verbal tapestry of interwoven phrases that whisk you away to magical kingdoms of the mind. The computer becomes a tool of reaching that conveys you where it will. You go along eagerly, breathlessly, awaiting what comes next.[14]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The catch was that this delicacy was consumable only once: a solved adventure game was no more interesting to revisit than a solved crossword puzzle. So, they had to provide a challenge: no one wanted to pay $24.95 for a game on the way home from work and then breeze through it before bedtime. A game that was very fair would also risk being seen as a waste of money. Despite improvements in design in future years that would banish some of the worst practices of the genre, adventure games remained trapped on the horns of this dilemma.[15]&lt;/p&gt;
    &lt;head rend="h3"&gt;Zork&lt;/head&gt;
    &lt;p&gt;The Adams’ “Adventure” line made them wealthy enough to build a faux-castle outside Orlando, and kicked off one of the most popular computer game genres of the 1980s. By late 1980, half-a-dozen other companies were putting out personal computer adventure games, from The Programmer’s Guild to Mad Hatter Software, as well as a version of Crowther-Woods Adventure put out by Microsoft. But they are overshadowed in the historical record by a competitor that subsequently dominated both sales of and critical attention to text adventure games. It began at MIT. In the spring of 1977, the Crowther-Woods Adventure arrived over the ARPANET at the PDP-10 at the Laboratory for Computer Science (LCS), and sank its claws into its employees. Impressed by what Crowther and Woods had done, but convinced that it could be made even better, a group of LCS staff set out in May 1977 to one-up Adventure.[16]&lt;/p&gt;
    &lt;p&gt;Dave Lebling, who had already worked on several games (including Maze, the first first-person shooter game), kicked off the project. Lebling played Dungeons and Dragons in the same Cambridge D&amp;amp;D group as Crowther had (though not at the same time), and based the game’s combat system on the tabletop game. Then Marc Blank, Tim Anderson, and Bruce Daniels filled in most of the core structure of the program. They gave it the place-holder name of Zork (a term used as an inside-joke expletive at LCS, as in “why won’t this zorking thing work”), which ended up sticking permanently. The game reached its completed state in early 1979, by which point it greatly exceeded the original Adventure in scale, with 191 rooms and 211 items, occupying a full megabyte of memory.[17]&lt;/p&gt;
    &lt;p&gt;Coded in a LISP-descendant called MUDDLE or MDL, Zork had an elegant design that encapsulated all the information about the possible interactions of each room and item in a single block of code and data, making it much easier to extend than Adventure. It also had a much richer text interface: both Adventure and AdventureLand accepted only “verb noun” commands, but Zork also allowed for conjunctions and prepositions (for example, “TAKE SWORD AND LANTERN FROM SACK”).Though aping the basic tropes of Adventure (a small overland area leading to an underground treasure-hunt), its more complex architecture allowed for a richer and more clever set of puzzles.[18]&lt;/p&gt;
    &lt;p&gt;In the spring of 1979, several key staff members of LCS were poised to leave MIT. Their supervisor, Al Vezza, proposed to keep the band together by forming a company. Incorporated in June as Infocom, its new employees and shareholders included Lebling, Blank, and Anderson.&lt;/p&gt;
    &lt;p&gt;While the various partners mulled what exactly to do with their new business, Blank and a fellow LCS alum, Joel Berez, figured out how to cram Zork onto a microcomputer: they cut the number of rooms and items in the game in half and removed all the features of MDL not needed for the game, creating an interpreter for a simpler language they called Zork Implementation Language (ZIL). The resulting program occupied just seventy-seven kilobytes. To get this to fit into a microcomputer memory half that size, they had one last trick: a virtual memory system built into the interpreter, to swap chunks of the program on and off the disk as needed (typical floppy disk capacities at the time were over 100 kilobytes, and continued to grow). This meant that Zork could only run off of a floppy drive (whose rapidly spinning disk could sync to a new data location in a fraction of a second and supply data at fifteen kilobytes per second), never a cassette (which took a minute or more to fully unwind or rewind and supplied data at 300 bits per second). Or, to put it another way, the growing market prevalence of affordable floppy drives made larger personal computer adventure games feasible: it took about twenty minutes to load a Scott Adams adventure game from tape.[19]&lt;/p&gt;
    &lt;p&gt;In late 1979, Blank and Berez convinced a reluctant Vezza (who wanted to get into business software) to make a microcomputer Zork Infocom’s first product. They initially published through Personal Software, co-owned by MIT’s own Dan Fylstra, which had just recently released VisiCalc. But after VisiCalc’s smash success, Fylstra no longer wanted to deal in mere games, so Infocom became its own publisher for subsequent games—including Zork II and III, built from the remaining unused material from the original PDP-10 Zork.&lt;/p&gt;
    &lt;p&gt;Zork became available in December 1980 and sold 10,000 units in 1981, mostly on the Apple II, despite an eye-watering price of $39.95, at a time when most games cost fifteen to twenty-five dollars. Then, astonishingly, in an industry typically characterized by ephemerality and obsolescence, sales continued to grow, year after year. They peaked in 1984 with over 150,000 copies sold. No doubt Zork’s self-referential humor, its restrained but clever marketing, and the high quality of the game itself (certainly the most well-crafted adventure game to date) all helped to sell the game.[20]&lt;/p&gt;
    &lt;p&gt;But many sales also must have arisen from the startling impression given by sitting down in a store (or at a friend’s house) to interact with this remarkable piece of software. Bob Liddil, reviewing Zork for BYTE magazine, pointed to the fluency of the parser as the element that first pulled him in:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I was eager to test Zork’s biggest selling point, intelligent input (ie: its ability to accept free-form instructions). I typed “OPEN THE BAG AND GET THE LUNCH,” in reference to a brown paper sack inside the house. The computer complied. There was water and food, so I typed “EAT THE LUNCH AND DRINK THE WATER,” to which the computer responded with gratitude for satisfying its hunger and thirst. I was hooked.[21]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The game seemed to understand the user and to have an appropriate answer (or a witty retort) ready for everything they might try, from expletives (“FUCK &amp;gt; SUCH LANGUAGE IN A HIGH-CLASS ESTABLISHMENT LIKE THIS!”) to attempts to outwit the command system (“FIND HANDS &amp;gt; WITHIN SIX FEET OF YOUR HEAD, ASSUMING YOU HAVEN’T LEFT THAT SOMEWHERE.”), to questions about the imaginary world in which the game is played (“WHAT IS A ZORKMID? &amp;gt; THE ZORKMID IS THE UNIT OF CURRENCY OF THE GREAT UNDERGROUND EMPIRE.”) Along with VisiCalc and WordStar, Zork functioned not just as a piece of software that did something, but also as an existence proof (for the owner and for skeptical friends and family) that the microcomputer could be more than merely a toy version of a real computer.[22]&lt;/p&gt;
    &lt;p&gt;Zork sales finally fell off in the mid-1980s, not because new text adventure games had surpassed it (Infocom continued to rule that particular roost, and Zork remained their flagship), but because of the steady improvement in personal computer graphics and the corresponding ascendancy of graphical games over textual ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mystery House&lt;/head&gt;
    &lt;p&gt;The first graphical adventure game actually appeared several months before Zork: On-Line Systems’ Mystery House, created by Ken and Roberta Williams. Unlike Scott Adams and most of the early personal computer hobbyists, Ken Williams got into computers for money, not love. Raised in greater Los Angeles in an unhappy home, he was a driven and impatient young man, and graduated high school at just sixteen. Roberta Heuer, a dreamy young woman whom Williams met through a double date, was impressed enough by his intelligence and ambition to give in to his insistence that they marry in 1972, while they were both still teenagers.&lt;/p&gt;
    &lt;p&gt;With the expectation of children to come, Ken abandoned his physics program at Cal Poly Pomona for a more immediately lucrative career in data processing. His father-in-law helped him get a loan to attend Control Data Corporation’s training school (the Control Data Institute), and from there he went on to a series of positions working on “big iron” batch-processing systems, constantly bouncing from job to job and home to home in search of better opportunities and a fatter pay check. He and Roberta wanted a bigger house and more creature comforts, but most of all they dreamed of an early retirement to a life out-of-doors, far from the city.[23]&lt;/p&gt;
    &lt;p&gt;The Williamses took no notice of microcomputers until Ken and one of his co-workers, Robert Leff, concocted a way to make money off of them: selling fellow programmers a microcomputer implementation of FORTRAN, one of the most popular data processing languages. Not only could this venture make him and Roberta still richer (always a key consideration), it could free them to finally move away from the traffic and grind of Los Angeles and to live out their dream of rural life. Initially Ken planned to write FORTRAN for the TRS-80, but he redirected his energies to the more capable Apple II after he and Roberta got themselves one for a mutual Christmas present.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Roberta had gotten hooked on adventure games. Ken had an electromechanical teletype terminal in their home for one of his consulting jobs, and connected it to a computer with the Crowther-Woods Adventure available to play. He showed the game off to Roberta. For Ken it was a curiosity, but for Roberta it became an obsession: she would not quit until she had beaten the game, weeks later. Ken brought home a borrowed TRS-80 and cassette tapes for the Scott Adams adventure series, and she flew through those, too. Soon she had an idea for a game of her own: instead of a treasure hunt, it would be a murder mystery; a mix of Clue and Ten Little Indians set in a creepy old Victorian house.&lt;/p&gt;
    &lt;p&gt;She insisted that Ken help her create it, and, after putting her off several times, he finally relented. Roberta wanted to add pictures of each room as a way to make this new game better than what came before, taking advantage of the Apple II’s 280×192 pixel high-resolution graphics mode. Because storing dozens of bitmapped images on a floppy disk would be impossible, Ken bought a VersaWriter accessory, a tablet with a movable arm that let Roberta capture the (x, y) position of each line endpoint in her pictures and store them into the computer. He wrote code to re-create the pictures from these coordinates by drawing the lines at runtime.[24]&lt;/p&gt;
    &lt;p&gt;Like Crowther and Adams, Ken split the data tables apart from the code that interpreted them. This allowed Roberta to work out all of the information about the rooms in the game, the items they contain, and the actions the player can perform, without needing to write any code. This division of labor between programming and design, quite novel to computer game software, came about from the accident of Roberta’s limited technical skills (she had worked briefly as COBOL programmer, at Ken’s insistence) and Ken’s lack of interest in the game: he was still focused on launching Apple FORTRAN.[25]&lt;/p&gt;
    &lt;p&gt;Then, while visiting local computer stores to pitch his computer language, Ken demoed an early version of Roberta’s game and everyone in the store gathered around to see it. The owners asked when they could have copies to sell. Ken realized he was backing the wrong horse: it was Roberta’s side project that would make them rich, not FORTRAN. Moreover, rather than give up a cut to a publisher like Programma International, they would take all the revenue for themselves, by publishing the game through the company name he had already registered for his never-to-be-released FORTRAN, On-Line Systems. On top of that, they could make even more money by distributing games into the stores they were already visiting on behalf of other software authors, like Scott Adams’ Florida-based Adventure International. Eventually unable to manage both publishing and distribution, he convinced his former colleague and erstwhile FORTRAN partner, Robert Leff, to buy out the distribution business, which grew into the industry behemoth Softsel.[26]&lt;/p&gt;
    &lt;p&gt;After a month of development on nights and weekends (Ken’s pace was manic: in his memoir he writes that he always strove to be a “Triple-A” player, and his brother called him a “chronic workaholic”), the Williams’ started selling Mystery House in May 1980. It required forty-eight kilobytes of memory, but with chip prices falling continuously, this was not so stringent a requirement as it had been even a year before.&lt;/p&gt;
    &lt;p&gt;The game’s simplistic “mystery” ends with the player gunning down the de facto murderer: the only living character to be found in a houseful of victims. The puzzles are among the more poorly clued and arbitrary to be found in a genre full of such frustrations. But for adventure-starved gamers of the time it was enchanting: not only could they witness the virtual world which they were navigating, it actually changed in response to their actions (picking up an object, like the taunting note that kicked off the murders, would remove it from the scene). Roberta’s drawings, crude and child-like as they certainly are, gave the game a visual appeal that drew in new buyers, and more than justified its price of $24.95.[27]&lt;/p&gt;
    &lt;p&gt;That summer Ken and Roberta were pulling in $30,000 a month and shopping for a house far from Los Angeles, in Coarsegold, California, nestled in the foothills of the Sierra Nevada near Yosemite National Park. On-Line Systems became Sierra On-Line. A few months later a second “High-Res Adventure” followed, The Wizard and the Princess, which added visually-stunning color to Mystery House’s line drawings: Ken used dithering techniques to make the six colors available in high-res mode appear like twenty-one. Roberta’s King’s Quest series, which I encountered on my Apple II, did not begin until 1984. It became Sierra’s best seller: by 1987, the first three installments of the series had sold a combined 500,000 copies, at least according to Sierra’s own marketing.[28]&lt;/p&gt;
    &lt;p&gt;It stands out, in a story populated almost entirely with male characters, that two of the earliest adventure game designers (Alexis Adams and Roberta Williams) were women. The scope of Alexis’ contributions aren’t entirely clear, but Roberta was arguably the most successful adventure game designer of all time. There was an appeal in the adventure game genre, which had more in common with a mystery novel or a logic puzzle than an arcade game and typically eschewed violence (the summary execution of Mystery House’s killer notwithstanding), that attracted some women to an otherwise almost entirely masculine industry.[29]&lt;/p&gt;
    &lt;p&gt;In a world where multiple discovery and parallel invention are the norm, it is also remarkable that all of the games we have discussed (and indeed all the computer adventure games ever made) can trace their ancestry to the Crowther-Woods Adventure. In the meantime, though, many other computer game authors had drawn inspiration from Dungeons and Dragons, spawning an entirely different genre of computer games, more in tune with D&amp;amp;D’s wargaming roots.&lt;/p&gt;
    &lt;p&gt;[1] Programma International, “Spring 1980 Catalog” (Spring 1980), 3-5 (https://ia903201.us.archive.org/12/items/Programma_Catalog_Spring_1980_for_APPLE_II/Programma_Catalog_Spring_1980_for_APPLE_II.pdf).&lt;/p&gt;
    &lt;p&gt;[2] J.J. Anderson, “Dave tells Ahl—The History of Creative Computing,”Creative Computing (November 1984), 72.&lt;/p&gt;
    &lt;p&gt;[3] “A Star Trek Product,” BYTE (July 1976), 92; “Scelbi Software,” BYTE (July 1976), 17.&lt;/p&gt;
    &lt;p&gt;[4] Alexander Smith, They Create Worlds: The Story of the People and Companies That Shaped the Video Game Industry, Vol. I: 1971-1982 (Boca Raton: CRC Press, 2020), 366-368; Jimmy Maher “Adventureland, Part 2,” The Digital Antiquarian (June 24, 2011) (https://www.filfre.net/2011/06/adventureland-part-2); David H. Ahl, “The First Decade of Personal Computing,” Creative Computing (November 1984), 30.&lt;/p&gt;
    &lt;p&gt;[5] Smith, They Create Worlds, 266-267; David H. Ahl, ed., 101 BASIC Computer Games: Microcomputer Edition (New York: Workman Publishing, 1977).&lt;/p&gt;
    &lt;p&gt;[6] The Wargaming Scribe, “The beginning of home computer gaming: the VDM-1 and the SOL-20” (August 16, 2023) (https://zeitgame.net/archives/10450); “Cromemco Dazzler Games” (Mountain View: Cromemco, 1977); Steve North, “Two Space Games (With Graphics!) For Your Home Computer,” Creative Computing (July/August 1977) 43-44; “Spacewar Available for the Cromemco Dazzler,” Cromemco News (January 1977).&lt;/p&gt;
    &lt;p&gt;[7] Smith, They Create Worlds, 383-384; Katie Hafner, “Will Crowther Interview,” (March 1994), 1-5 (https://archive.org/details/WillCrowtherInterview/mode/1up). I read into&lt;/p&gt;
    &lt;p&gt;[8] Quoted in Dale Peterson, Genesis II: Creation and Recreation with Computers (Reston: Prentice-Hall, 1983), 188.&lt;/p&gt;
    &lt;p&gt;[9] Smith, They Create Worlds, 384-385; Dennis G. Jerz, “Somewhere Nearby is Colossal Cave,” (2007), 83 (https://jerz.setonhill.edu/resources/preprint/SNiCC.pdf&lt;/p&gt;
    &lt;p&gt;[10] Smith, They Create Worlds, 384-385; Jerz, “Somewhere Nearby is Colossal Cave,” 13; Jimmy Maher, “The Completed Adventure, Part 1” The Digital Antiquarian (June 2, 2011) (https://www.filfre.net/2011/06/the-completed-adventure-part-1/); Tracy Kidder, The Soul of A New Machine (New York: Little, Brown, 2000 [1981]), 86-89.&lt;/p&gt;
    &lt;p&gt;[11] IF Archive Adventure zip (https://unbox.ifarchive.org/?url=/if-archive/games/source/adv350-pdp10.tar.gz);”AdventureLand map (https://www.solutionarchive.com/file/id%2C3/); Maher, “AdventureLand, Part 1,” The Digital Antiquarian (June 22, 2011) (https://www.filfre.net/2011/06/adventureland-part-1).&lt;/p&gt;
    &lt;p&gt;[12] Robert Levering, Michael Katz, and Milton Moskowitz, The Computer Entrepreneurs: Who’s Making It Big and How in America’s Upstart Industry (New York: NAL Books, 1984), 114-118; Smith, They Create Worlds, 388.&lt;/p&gt;
    &lt;p&gt;[13] Jimmy Maher, “Adventureland, Part 1,” The Digital Antiquarian (June 22, 2011) (https://www.filfre.net/2011/06/adventureland-part-1).&lt;/p&gt;
    &lt;p&gt;[14] Bob Liddil, “On the Road to Adventure,” BYTE (December 1980), 170.&lt;/p&gt;
    &lt;p&gt;[15] The 1990 LucasArts adventure, Loom, for example, though it is an artistic masterpiece, was criticized by reviewers for being too short and too easy. Scorpia, “Scorpion’s View: ‘Conquests of Camelot’ and ‘Loom’,” Computer Gaming World (July-August 1990), 51, 63, Simply making the games larger, with more puzzles was technically infeasible in the early years (we have already seen that Adventureland had to be much smaller than Adventure to fit on a microcomputer); later, as the costs of game production went up, it became financially infeasible. There is an expert dissection of the sins of one early adventure game in Jimmy Maher, “The Wizard and the Princess, Part 2,” The Digital Antiquarian (October 21, 2011) (https://www.filfre.net/2011/10/the-wizard-and-the-princess-part-2).&lt;/p&gt;
    &lt;p&gt;[16] Bob Liddil, “On the Road to Adventure,” BYTE (December 1980), 162.&lt;/p&gt;
    &lt;p&gt;[17] Jimmy Maher, “The Roots of Infocom,” Digital Antiquarian (January 1, 2012) (https://www.filfre.net/2012/01/the-roots-of-infocom); Jimmy Maher, “Zork on the PDP-10,” Digital Antiquarian (January 3, 2012) (https://www.filfre.net/2012/01/zork-on-the-pdp-10); Stephen Granade and Philip Jong, “David Lebling Interview,” Brass Lantern (undated, ca. 2000) (http://brasslantern.org/community/interviews/lebling.html); Nick Montfort, Twisty Little Passage: An Approach to Interactive Fiction (Cambridge: MIT Press, 2003), 86. Eric Roberts, Crowther and Lebling’s dungeon master, ran a variant of D&amp;amp;D he called Mirkwood Tales. Jon Peterson, Playing at the World (San Diego: Unreason Press, 2012), 617-618, 622.&lt;/p&gt;
    &lt;p&gt;[18] P. David Lebling, “Zork and the Future of Computerized Fantasy Simulations,” BYTE (December 1980), 172-182.&lt;/p&gt;
    &lt;p&gt;[19] Jimmy Maher, “ZIL and the Z-Machine,” The Digital Antiquarian (https://www.filfre.net/2012/01/zil-and-the-z-machine); Maya Posch, “Zork And The Z-Machine: Bringing The Mainframe To 8-bit Home Computers,” Hackaday (May 22, 2019) (https://hackaday.com/2019/05/22/zork-and-the-z-machine-bringing-the-mainframe-to-8-bit-home-computers); Scott Adams “Pirate’s Adventure,” BYTE (December 1980), 212. Virtual memory was a well-established technique in minicomputer and mainframe operating systems, but no widely used personal computer OS offered virtual memory until the release of Windows 3.0 in 1990.&lt;/p&gt;
    &lt;p&gt;[20] “InfoCom Shipments By Title and Year” (https://www.flickr.com/photos/textfiles/2419969220); Bob Liddil, “Zork, The Great Underground Empire,” Byte (February 1981), 262.&lt;/p&gt;
    &lt;p&gt;[21] Liddil, “Zork, The Great Underground Empire,” 262.&lt;/p&gt;
    &lt;p&gt;[22] Jimmy Maher, “Parser Games,” Digital Antiquarian (January 16, 2012) (https://www.filfre.net/2012/01/parser-games).&lt;/p&gt;
    &lt;p&gt;[23] Levy, Hackers, 293-297, 302-303; Ken Williams, Not All Fairy Tales Have Happy Endings: The Rise and Fall of Sierra On-Line (Ken Williams, 2020), 12-24, 22-24; Jimmy Maher, “Ken and Roberta,” The Digital Antiquarian (October 2, 2011) (https://www.filfre.net/2011/10/ken-and-roberta).&lt;/p&gt;
    &lt;p&gt;[24] Williams, Not All Fairy Tales, 55-56, 66-68, 88; Levy, Hackers, 303-304; Ken Wiliams, “Introduction to The Roberta Williams Anthology” (1996) (https://wiki.sierrahelp.com/index.php/Introduction_to_The_Roberta_Williams_Anthology). The account in the previous paragraphs is interpolated from the above sources, which are partially contradictory. All differ about who got the Apple II and why. Levy never mentions the TRS-80 or any adventure games besides Adventure, and has Roberta finishing that game after the time the Apple II was purchased, implying she never played any other adventure games before deciding to write Mystery House: the timeline would simply be too tight. I believe this is wrong, and either an intentional elision or a false interpolation by Levy. It is unlikely that the Williamses would later entirely hallucinate having brought home and played the whole series of Scott Adams games. The accounts also differ on whose idea it was to add pictures to the game. I’m inclined to believe it was Roberta, to whom the game idea and all the passion for it belonged.&lt;/p&gt;
    &lt;p&gt;[25] Williams, Not All Fairy Tales, 69-73.&lt;/p&gt;
    &lt;p&gt;[26] Levy, Hackers, 308-310; Williams, Not All Fairy Tales, 73; Ken Williams, “A Message From the President,” Sierra News Magazine (Summer 1990), 35].&lt;/p&gt;
    &lt;p&gt;[27] John Williams, “Sierra’s First Ten Years,” Sierra News Magazine (Summer 1990), 6; Williams, Not All Fairy Tales, 79; Jimmy Maher, “Mystery House, Part 2,” Digital Antiquarian (October 9, 2011) (https://www.filfre.net/2011/10/mystery-house-part-2); “Game 57: Mystery House,” Data-Driven Gamer (April 22, 2019) (https://datadrivengamer.blogspot.com/2019/04/game-57-mystery-house.html).&lt;/p&gt;
    &lt;p&gt;[28] Jimmy Maher, “The Wizard and the Princess, Part 1,” Digital Antiquarian (October 20, 2011) (https://www.filfre.net/2011/10/the-wizard-and-the-princess-part-1); “On-Line Systems Presents: Hi-Res Adventure,” Softline (September 1981), 16; Levy, Hackers, 310-311; “Sales Data,” King’s Quest Omnipedia (https://kingsquest.fandom.com/wiki/Sales_data#King’s_Quest_Original).&lt;/p&gt;
    &lt;p&gt;[29] In later years, Sierra On-Line would employ several more women as designers—Lori Cole (the Quest for Glory series), Christy Marx (Conquests of Camelot and Conquests of the Longbow), and Jane Jensen (the Gabriel Knight series), while Amy Briggs created Plundered Hearts at Infocom. It is hard to get any reliable numbers on the audience for adventure games: in 1989, Sierra estimated that 35-40% of the players of King’s Quest IV were women, which surely was well above average for a computer game. Patricia Cignarella, “Girls Just Want To Have Fun,” Sierra News Magazine (Autumn 1989), 25.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46257599</guid><pubDate>Sat, 13 Dec 2025 20:19:10 +0000</pubDate></item><item><title>I fed 24 years of my blog posts to a Markov model</title><link>https://susam.net/fed-24-years-of-posts-to-markov-model.html</link><description>&lt;doc fingerprint="251a939f24803ea2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Fed 24 Years of My Blog Posts to a Markov Model&lt;/head&gt;
    &lt;p&gt;Yesterday I shared a little program called Mark V. Shaney Junior at github.com/susam/mvs. It is a minimal implementation of a Markov text generator inspired by the legendary Mark V. Shaney program from the 1980s. If you don't know about Mark V. Shaney, read more about it on the Wikipedia article Mark V. Shaney. In this post, I will discuss this program, explain how it works and share some of the results it produces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Recreational Programming&lt;/head&gt;
    &lt;p&gt;The program I shared yesterday has only about 30 lines of Python and favours simplicity over efficiency. Even if you have never worked with Markov models before, I am quite confident that it will take you less than 20 minutes to understand the whole program and make complete sense of it. I also offer an explanation further below in this post.&lt;/p&gt;
    &lt;p&gt;As a hobby, I often engage in exploratory programming where I write computer programs not to solve a specific problem but simply to explore a particular idea or topic for the sole purpose of recreation. I must have written small programs to explore Markov chains for various kinds of state spaces over a dozen times by now. Every time, I just pick my last experimental code and edit it to encode the new state space I am exploring. That's usually my general approach to such one-off programs. I have hundreds of tiny little experimental programs lying on my disk at any given time.&lt;/p&gt;
    &lt;p&gt; Once in a while, I get the itch to take one of those exploratory programs, give it some finishing touches, wrap it up in a nice Git repo along with a &lt;code&gt;README.md&lt;/code&gt;, &lt;code&gt;CHANGES.md&lt;/code&gt;
  and the whole shebang and share it on
  github.com/susam and
  codeberg.org/susam.  The
  Mark V. Shaney Junior program that I shared yesterday happened to be
  one such exercise.
&lt;/p&gt;
    &lt;head rend="h2"&gt;Gibberish&lt;/head&gt;
    &lt;p&gt;If you scroll down the README of this project, you'll find some nice examples of the gibberish produced by this program. The first few examples there are the result of training the model on A Christmas Carol by Charles Dickens, one of my favourite authors. It is often said that Dickens never used fewer words when more would suffice. So I thought there couldn't be a better piece of text when it comes to testing out my tiny Markov model. I'll not reproduce the generated text examples here for the sake of brevity. If you are interested to take a look, just head over to the Gibberish section of the README.&lt;/p&gt;
    &lt;p&gt;Soon after sharing the project, I wondered what kind of gibberish it would produce if I fed all 24 years of my blog posts and pages into the program. Well, here's one of the results:&lt;/p&gt;
    &lt;quote&gt;$ ./mvs &amp;lt; susam.txt while a query replace operation is approved by the user. The above variable defines the build job. It can be incredibly useful while working on assembly language and machine code. In fact, all internal resources like the result to refine the search prompt changes from bck-i-search: to fwd-i-search:. Now type C-SPC (i.e. ctrl+space) to set a mark causes Emacs to use 32-bit registers like EBP, ESP, etc. Thus the behaviour is undefined. Such code may behave differently when compiled with the readily available GNU tools like the shape of 8. Flipping "P" horizontally makes it a proper quine: cat $0&lt;/quote&gt;
    &lt;p&gt;This is the text that comes out after the program consumes over 200 posts consisting of about 200,000 words. My blog also has a comments section with over 500 comments consisting of about 40,000 words. All comments were excluded while training the model. Here is another output example:&lt;/p&gt;
    &lt;quote&gt;$ ./mvs &amp;lt; susam.txt enjoy asking "what happens if" and then type M-x zap-up-to-char RET b. The buffer for this specific video, the actual fare for 8.3 km and 11 are all written from scratch. No prior knowledge is expected to slow down in future. For now, I will add a statement like x = 0.999..., the inner corner square as discussed in the code segment into the REPL window. Unlike Slimv, Vlime can work with and debug executable files, it can be more convenient. M-x: Execute Extended Command The key sequence M-q invokes the command cat and type TAB to indent the current&lt;/quote&gt;
    &lt;p&gt;Here is a particularly incoherent but amusing one:&lt;/p&gt;
    &lt;quote&gt;$ ./mvs &amp;lt; susam.txt Then open a new Lisp source file and the exact answer could harm students' self-esteem. Scientists have arbitrarily assumed that an integral domain. However, the string and comment text. To demonstrate how a build job can trigger itself, pass input to standard output or standard error), Eshell automatically runs the following command in Vim and Emacs will copy the message length limit of 512 characters, etc. For example, while learning to play the game between normal mode to move the point is on an old dictionary lying around our house and that is moving to the small and supportive community&lt;/quote&gt;
    &lt;p&gt;No, I have never said anywhere that opening a Lisp source file could harm anyone's self-esteem. The text generator has picked up the 'Lisp source file' phrase from my Lisp in Vim post and the 'self-esteem' bit from the From Perl to Pi post.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Markov Property&lt;/head&gt;
    &lt;p&gt;By default, this program looks at trigrams (all sequences of three adjacent words) and creates a map where the first two words of the trigram are inserted as the key and the third word is appended to its list value. This map is the model. In this way, the model captures each pair of adjacent words along with the words that immediately follow each pair. The text generator first chooses a key (a pair of words) at random and selects a word that follows. If there are multiple followers, it picks one uniformly at random. It then repeats this process with the most recent pair of words, consisting of one word from the previous pair and the word that was just picked. It continues to do this until it can no longer find a follower or a fixed word limit (100 by default) is reached. That is pretty much the whole algorithm. There isn't much more to it. It is as simple as it gets. For that reason, I often describe a simple Markov model like this as the 'hello, world' for language models.&lt;/p&gt;
    &lt;p&gt;If the same trigram occurs multiple times in the training data, the model records the follower word (the third word) multiple times in the list associated with the key (the first two words). This representation can be optimised, of course, by keeping frequencies of the follower words rather than duplicating them in the list, but that is left as an exercise to the reader. In any case, when the text generator chooses a follower for a given pair of words, a follower that occurs more frequently after that pair has a higher probability of being chosen. In effect, the next word is sampled based only on the previous two words and not on the full history of the generated text. This memoryless dependence on the current state is what makes the generator Markov. Formally, for a discrete-time stochastic process, the Markov property can be expressed as \[ P(X_{n+1} \mid X_n, X_{n-1}, \ldots, X_1) = P(X_{n+1} \mid X_n). \] where \( X_n \) represents the \( n \)th state. In our case, each state \( X_n \) is a pair of words \( (w_{n-1}, w_{n}) \) but the state space could just as well consist of other objects, such as a pair of characters, pixel values or musical notes. The sequence of states \( (X_1, X_2, \dots) \) visited by the program forms a Markov chain.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some More Gibberish&lt;/head&gt;
    &lt;p&gt;In 2025, given the overwhelming popularity of large language models (LLMs), Markov models like this look unimpressive. Unlike LLMs, a simple Markov model cannot capture global structure or long-range dependencies within the text. It relies entirely on local word transition statistics. Also, these days, one hardly needs a Markov model to generate gibberish; social media provides an ample supply. Nevertheless, I think the simplicity of its design and implementation serves as a good entry point into language models.&lt;/p&gt;
    &lt;p&gt;In my implementation, the number of words in the key of the map can be set via command line arguments. By default, it is 2 as described above. This value is also known as the order of the model. So by default the order is 2. If we increase it to, say, 3 or 4, the generated text becomes a little more coherent. Here is one such example:&lt;/p&gt;
    &lt;quote&gt;$ ./mvs 4 &amp;lt; susam.txt It is also possible to search for channels by channel names. For example, on Libera Chat, to search for all channels with 'python' in its name, enter the IRC command: /msg alis list python. Although I have used Libera Chat in the examples above, there are plenty of infinite fields, so they must all be integral domains too. Consider the field of rational numbers Q. Another quick way to arrive at this fact is to observe that when one knight is placed on a type D square, only two positions for the second knight such that the two knights attack&lt;/quote&gt;
    &lt;p&gt;Except for a couple of abrupt and meaningless transitions, the text is mostly coherent. We need to be careful about not increasing the order too much. In fact, if we increase the order of the model to 5, the generated text becomes very dry and factual because it begins to quote large portions of the blog posts verbatim. Not much fun can be had with that.&lt;/p&gt;
    &lt;p&gt;Before I end this post, let me present one final example where I ask it to generate text from an initial prompt:&lt;/p&gt;
    &lt;quote&gt;$ ./mvs 2 100 'Finally we' &amp;lt; susam.txt Finally we divide this number by a feed aggregrator for Emacs-related blogs. The following complete key sequences describe the effects of previous evaluations shall have taken a simple and small to contain bad content. This provides an interactive byte-compiled Lisp function in MATLAB and GNU bash 5.1.4 on Debian is easily reproducible in Windows XP. Older versions might be able to run that server for me it played a significant burden on me as soon as possible. C-u F: Visit the marked files or directories in the sense that it was already initiated and we were to complete the proof.&lt;/quote&gt;
    &lt;p&gt;Apparently, this is how I would sound if I ever took up speaking gibberish!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46257607</guid><pubDate>Sat, 13 Dec 2025 20:19:53 +0000</pubDate></item><item><title>Recovering Anthony Bourdain's Li.st's</title><link>https://sandyuraz.com/blogs/bourdain/</link><description>&lt;doc fingerprint="3b6d9815a09e1a8c"&gt;
  &lt;main&gt;
    &lt;p&gt;🍯 At least 2 days ago&lt;/p&gt;
    &lt;p&gt;Loved reading through GReg TeChnoLogY Anthony Bourdain’s Lost Li.st’s and seeing the list of lost Anthony Bourdain li.st’s made me think on whether at least some of them we can recover.&lt;/p&gt;
    &lt;p&gt;Having worked in security and crawling space for majority of my career—I don’t have the access nor permission to use the proprietary storages—I thought we might be able to find something from publicly available crawl archives.&lt;/p&gt;
    &lt;p&gt;All of the code and examples lead to the source git repository. This article has also been discussed on hackernews. Also, a week before I published this, mirandom had the same idea as me and published their findings—go check them out.&lt;/p&gt;
    &lt;p&gt;If Internet Archive had the partial list that Greg published, what about the Common Crawl? Reading through their documentation, it seems straightforward enough to get prefix index for Tony’s lists and grep for any sub-paths.&lt;/p&gt;
    &lt;p&gt; Putting something up with help of Claude to prove my theory, we have &lt;code&gt;commoncrawl_search.py&lt;/code&gt; that makes a single index request to a specific dataset and if any hits discovered, retrieve them from the public s3 bucket—since they are small straight-up HTML documents, seems even more feasible than I had initially thought.
&lt;/p&gt;
    &lt;p&gt; Simply have a python version around 3.14.2 and install the dependencies from &lt;code&gt;requirements.txt&lt;/code&gt;. Run the below and we are in business. Now, below, you’ll find the command I ran and then some manual archeological effort to prettify the findings.
&lt;/p&gt;
    &lt;code&gt;python commoncrawl_search.py "https://li.st/Bourdain*" --all --download&lt;/code&gt;
    &lt;p&gt; Any and all emphasis, missing punctuation, cool grammar is all by Anthony Bourdain. The only modifications I have made is to the layout, to represent &lt;code&gt;li.st&lt;/code&gt; as closely as possible with no changes to the content.
&lt;/p&gt;
    &lt;p&gt; From Greg’s page, let’s go and try each entry one by one, I’ll put the table of what I wasn’t able to find in Common Crawl, but I would assume exists elsewhere—I’d be happy to take another look. And no, none of this above has been written by AI, only the code since I don’t really care about &lt;code&gt;warcio&lt;/code&gt; encoding or writing the same python requests method for the Nth time. Enjoy!
&lt;/p&gt;
    &lt;p&gt;Cocaine&lt;/p&gt;
    &lt;p&gt;True Detective&lt;/p&gt;
    &lt;p&gt;Scripps Howard&lt;/p&gt;
    &lt;p&gt;Dinners where it takes the waiter longer to describe my food than it takes me to eat it.&lt;/p&gt;
    &lt;p&gt;Beer nerds&lt;/p&gt;
    &lt;p&gt;I admit it: my life doesn’t suck. Some recent views I’ve enjoyed&lt;/p&gt;
    &lt;p&gt;Montana at sunset : There’s pheasant cooking behind the camera somewhere. To the best of my recollection some very nice bourbon. And it IS a big sky .&lt;/p&gt;
    &lt;p&gt;Puerto Rico: Thank you Jose Andres for inviting me to this beautiful beach!&lt;/p&gt;
    &lt;p&gt;Naxos: drinking ouzo and looking at this. Not a bad day at the office .&lt;/p&gt;
    &lt;p&gt;LA: My chosen final resting place . Exact coordinates .&lt;/p&gt;
    &lt;p&gt;Istanbul: raki and grilled lamb and this ..&lt;/p&gt;
    &lt;p&gt;Borneo: The air is thick with hints of durian, sambal, coconut..&lt;/p&gt;
    &lt;p&gt;Chicago: up early to go train #Redzovic&lt;/p&gt;
    &lt;p&gt;The Wire&lt;/p&gt;
    &lt;p&gt;Tinker, Tailor, Soldier, Spy (and its sequel : Smiley’s People)&lt;/p&gt;
    &lt;p&gt;Edge of Darkness (with Bob Peck and Joe Don Baker )&lt;/p&gt;
    &lt;p&gt;Dreamcasting across time with the living and the dead, this untitled, yet to be written masterwork of cinema, shot, no doubt, by Christopher Doyle, lives only in my imagination.&lt;/p&gt;
    &lt;p&gt;This guy&lt;/p&gt;
    &lt;p&gt;And this guy&lt;/p&gt;
    &lt;p&gt;All great films need:&lt;/p&gt;
    &lt;p&gt;The Oscar goes to..&lt;/p&gt;
    &lt;p&gt;And&lt;/p&gt;
    &lt;p&gt;If you bought these vinyls from an emaciated looking dude with an eager, somewhat distracted expression on his face somewhere on upper Broadway sometime in the mid 80’s, that was me . I’d like them back. In a sentimental mood.&lt;/p&gt;
    &lt;p&gt;material things I feel a strange, possibly unnatural attraction to and will buy (if I can) if I stumble across them in my travels. I am not a paid spokesperson for any of this stuff .&lt;/p&gt;
    &lt;p&gt;Vintage Persol sunglasses : This is pretty obvious. I wear them a lot. I collect them when I can. Even my production team have taken to wearing them.&lt;/p&gt;
    &lt;p&gt;19th century trepanning instruments: I don’t know what explains my fascination with these devices, designed to drill drain-sized holes into the skull often for purposes of relieving "pressure" or "bad humours". But I can’t get enough of them. Tip: don’t get a prolonged headache around me and ask if I have anything for it. I do.&lt;/p&gt;
    &lt;p&gt;Montagnard bracelets: I only have one of these but the few that find their way onto the market have so much history. Often given to the indigenous mountain people ’s Special Forces advisors during the very early days of America’s involvement in Vietnam .&lt;/p&gt;
    &lt;p&gt;Jiu Jitsi Gi’s: Yeah. When it comes to high end BJJ wear, I am a total whore. You know those people who collect limited edition Nikes ? I’m like that but with Shoyoroll . In my defense, I don’t keep them in plastic bags in a display case. I wear that shit.&lt;/p&gt;
    &lt;p&gt;Voiture: You know those old school, silver plated (or solid silver) blimp like carts they roll out into the dining room to carve and serve your roast? No. Probably not. So few places do that anymore. House of Prime Rib does it. Danny Bowein does it at Mission Chinese. I don’t have one of these. And I likely never will. But I can dream.&lt;/p&gt;
    &lt;p&gt;Kramer knives: I don’t own one. I can’t afford one . And I’d likely have to wait for years even if I could afford one. There’s a long waiting list for these individually hand crafted beauties. But I want one. Badly. http://www.kramerknives.com/gallery/&lt;/p&gt;
    &lt;p&gt;R. CRUMB : All of it. The collected works. These Taschen volumes to start. I wanted to draw brilliant, beautiful, filthy comix like Crumb until I was 13 or 14 and it became clear that I just didn’t have that kind of talent. As a responsible father of an 8 year old girl, I just can’t have this stuff in the house. Too dark, hateful, twisted. Sigh...&lt;/p&gt;
    &lt;p&gt;THE MAGNIFICENT AMBERSONS : THE UNCUT, ORIGINAL ORSON WELLES VERSION: It doesn’t exist. Which is why I want it. The Holy Grail for film nerds, Welles’ follow up to CITIZEN KANE shoulda, coulda been an even greater masterpiece . But the studio butchered it and re-shot a bullshit ending. I want the original. I also want a magical pony.&lt;/p&gt;
    &lt;p&gt;I like good spy novels. I prefer them to be realistic . I prefer them to be written by real spies. If the main character carries a gun, I’m already losing interest. Spy novels should be about betrayal.&lt;/p&gt;
    &lt;p&gt; Ashenden–Somerset Maugham&lt;lb/&gt;Somerset wrote this bleak, darkly funny, deeply cynical novel in the early part of the 20th century. It was apparently close enough to the reality of his espionage career that MI6 insisted on major excisions. Remarkably ahead of its time in its atmosphere of futility and betrayal. &lt;/p&gt;
    &lt;p&gt; The Man Who Lost the War–WT Tyler&lt;lb/&gt;WT Tyler is a pseudonym for a former "foreign service" officer who could really really write. This one takes place in post-war Berlin and elsewhere and was, in my opinion, wildly under appreciated. See also his Ants of God. &lt;/p&gt;
    &lt;p&gt; The Human Factor–Graham Greene&lt;lb/&gt;Was Greene thinking of his old colleague Kim Philby when he wrote this? Maybe. Probably. See also Our Man In Havana. &lt;/p&gt;
    &lt;p&gt; The Tears of Autumn -Charles McCarry&lt;lb/&gt;A clever take on the JFK assassination with a Vietnamese angle. See also The Miernik Dossier and The Last Supper &lt;/p&gt;
    &lt;p&gt; Agents of Innocence–David Ignatius&lt;lb/&gt;Ignatius is a journalist not a spook, but this one, set in Beirut, hewed all too closely to still not officially acknowledged events. Great stuff. &lt;/p&gt;
    &lt;p&gt;I wake up in a lot of hotels, so I am fiercely loyal to the ones I love. A hotel where I know immediately wher I am when I open my eyes in the morning is a rare joy. Here are some of my favorites&lt;/p&gt;
    &lt;p&gt;CHATEAU MARMONT ( LA) : if I have to die in a hotel room, let it be here. I will work in LA just to stay at the Chateau.&lt;/p&gt;
    &lt;p&gt;CHILTERN FIREHOUSE (London): Same owner as the Chateau. An amazing Victorian firehouse turned hotel. Pretty much perfection&lt;/p&gt;
    &lt;p&gt;THE RALEIGH (Miami): The pool. The pool!&lt;/p&gt;
    &lt;p&gt;LE CONTINENTAL (Saigon): For the history.&lt;/p&gt;
    &lt;p&gt;HOTEL OLOFSSON (Port au Prince): Sagging, creaky and leaky but awesome .&lt;/p&gt;
    &lt;p&gt;PARK HYATT (Tokyo): Because I’m a film geek.&lt;/p&gt;
    &lt;p&gt;EDGEWATER INN (Seattle): kind of a lumber theme going on...ships slide right by your window. And the Led Zep "Mudshark incident".&lt;/p&gt;
    &lt;p&gt;THE METROPOLE (Hanoi): there’s a theme developing: if Graham Greene stayed at a hotel, chances are I will too.&lt;/p&gt;
    &lt;p&gt;GRAND HOTEL D'ANGKOR (Siem Reap): I’m a sucker for grand, colonial era hotels in Asia.&lt;/p&gt;
    &lt;p&gt;THE MURRAY (Livingston,Montana): You want the Peckinpah suite&lt;/p&gt;
    &lt;p&gt;from my phone&lt;/p&gt;
    &lt;p&gt;Bun Bo Hue&lt;/p&gt;
    &lt;p&gt;Kuching Laksa&lt;/p&gt;
    &lt;p&gt;Pot au Feu&lt;/p&gt;
    &lt;p&gt;Jamon&lt;/p&gt;
    &lt;p&gt;Linguine&lt;/p&gt;
    &lt;p&gt;Meat&lt;/p&gt;
    &lt;p&gt;Dessert&lt;/p&gt;
    &lt;p&gt;Light Lunch&lt;/p&gt;
    &lt;p&gt;Meat on a Stick&lt;/p&gt;
    &lt;p&gt;Oily Little Fish&lt;/p&gt;
    &lt;p&gt;Snack&lt;/p&gt;
    &lt;p&gt;Soup&lt;/p&gt;
    &lt;p&gt;Homage&lt;/p&gt;
    &lt;p&gt;Not TOO random&lt;/p&gt;
    &lt;p&gt;Madeline&lt;/p&gt;
    &lt;p&gt;Beirut&lt;/p&gt;
    &lt;p&gt;Musubi&lt;/p&gt;
    &lt;p&gt;BudaeJiggae&lt;/p&gt;
    &lt;p&gt;Dinner&lt;/p&gt;
    &lt;p&gt;Bootsy Collins&lt;/p&gt;
    &lt;p&gt;Bill Murray&lt;/p&gt;
    &lt;p&gt;Spaghetti a la bottarga . I would really, really like some of this. Al dente, lots of chili flakes&lt;/p&gt;
    &lt;p&gt;A big, greasy double cheeseburger. No lettuce. No tomato. Potato bun.&lt;/p&gt;
    &lt;p&gt;A street fair sausage and pepper hero would be nice. Though shitting like a mink is an inevitable and near immediate outcome&lt;/p&gt;
    &lt;p&gt;Some uni. Fuck it. I’ll smear it on an English muffin at this point.&lt;/p&gt;
    &lt;p&gt;I wonder if that cheese is still good?&lt;/p&gt;
    &lt;p&gt;In which my Greek idyll is Suddenly invaded by professional nudists&lt;/p&gt;
    &lt;p&gt;Endemic FUPA. Apparently a prerequisite for joining this outfit.&lt;/p&gt;
    &lt;p&gt;Pistachio dick&lt;/p&gt;
    &lt;p&gt;70’s bush&lt;/p&gt;
    &lt;p&gt;T-shirt and no pants. Leading one to the obvious question : why bother?&lt;/p&gt;
    &lt;p&gt;Popeye’s Mac and Cheese&lt;/p&gt;
    &lt;p&gt;The cheesy crust on the side of the bowl of Onion Soup Gratinee&lt;/p&gt;
    &lt;p&gt;Macaroons . Not macarons . Macaroons&lt;/p&gt;
    &lt;p&gt;Captain Crunch&lt;/p&gt;
    &lt;p&gt;Double Double Animal Style&lt;/p&gt;
    &lt;p&gt;Spam Musubi&lt;/p&gt;
    &lt;p&gt;Aerosmith&lt;/p&gt;
    &lt;p&gt;Before he died, Warren Zevon dropped this wisdom bomb: "Enjoy every sandwich". These are a few locals I’ve particularly enjoyed:&lt;/p&gt;
    &lt;p&gt;PASTRAMI QUEEN: (1125 Lexington Ave. ) Pastrami Sandwich. Also the turkey with Russian dressing is not bad. Also the brisket.&lt;/p&gt;
    &lt;p&gt;EISENBERG'S SANDWICH SHOP: ( 174 5th Ave.) Tuna salad on white with lettuce. I’d suggest drinking a lime Rickey or an Arnold Palmer with that.&lt;/p&gt;
    &lt;p&gt;THE JOHN DORY OYSTER BAR: (1196 Broadway) the Carta di Musica with Bottarga and Chili is amazing. Is it a sandwich? Yes. Yes it is.&lt;/p&gt;
    &lt;p&gt;RANDOM STREET FAIRS: (Anywhere tube socks and stale spices are sold. ) New York street fairs suck. The same dreary vendors, same bad food. But those nasty sausage and pepper hero sandwiches are a siren song, luring me, always towards the rocks. Shitting like a mink almost immediately after is guaranteed but who cares?&lt;/p&gt;
    &lt;p&gt;BARNEY GREENGRASS : ( 541 Amsterdam Ave.) Chopped Liver on rye. The best chopped liver in NYC.&lt;/p&gt;
    &lt;p&gt;A work in progress&lt;/p&gt;
    &lt;p&gt;SIBERIA in any of its iterations. The one on the subway being the best&lt;/p&gt;
    &lt;p&gt;LADY ANNES FULL MOON SALOON a bar so nasty I’d bring out of town visitors there just to scare them&lt;/p&gt;
    &lt;p&gt;THE LION'S HEAD old school newspaper hang out&lt;/p&gt;
    &lt;p&gt;KELLY'S on 43rd and Lex. Notable for 25 cent drafts and regularly and reliably serving me when I was 15&lt;/p&gt;
    &lt;p&gt;THE TERMINAL BAR legendary dive across from port authority&lt;/p&gt;
    &lt;p&gt;BILLY'S TOPLESS (later, Billy’s Stopless) an atmospheric, working class place, perfect for late afternoon drinking where nobody hustled you for money and everybody knew everybody. Great all-hair metal jukebox . Naked breasts were not really the point.&lt;/p&gt;
    &lt;p&gt;THE BAR AT HAWAII KAI. tucked away in a giant tiki themed nightclub in Times Square with a midget doorman and a floor show. Best place to drop acid EVER.&lt;/p&gt;
    &lt;p&gt;THE NURSERY after hours bar decorated like a pediatrician’s office. Only the nursery rhyme characters were punk rockers of the day.&lt;/p&gt;
    &lt;p&gt;It was surprising to see that only one page was not recoverable from the common crawl.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;David Bowie Related&lt;/cell&gt;
        &lt;cell&gt;1/14/2016&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I’ve enjoyed this little project tremendously—a little archeology project. Can we declare victory for at least this endeavor? Hopefully, we would be able to find images, but that’s a little tougher, since that era’s cloudfront is fully gone.&lt;/p&gt;
    &lt;p&gt;What else can we work on restoring and setting up some sort of a public archive to store them? I made this a git repository for the sole purpose so that anyone interested can contribute their interest and passion for these kinds of projects.&lt;/p&gt;
    &lt;p&gt;Thank you and until next time! ◼︎&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46258163</guid><pubDate>Sat, 13 Dec 2025 21:18:01 +0000</pubDate></item><item><title>An off-grid, flat-packable washing machine</title><link>https://www.positive.news/society/flat-pack-washing-machine-spins-a-fairer-future/</link><description>&lt;doc fingerprint="ec0349190ebcc67a"&gt;
  &lt;main&gt;
    &lt;p&gt;A former Dyson engineer is rolling out a revolution for household chores in deprived communities after inventing an off-grid, flat-packable washing machine&lt;/p&gt;
    &lt;p&gt;Some five billion people in remote and developing regions still wash their clothes by hand. It’s a task that unfairly burdens women and young girls, who can spend up to 20 hours a week on the chore.&lt;/p&gt;
    &lt;p&gt;Enter Navjot Sawhney, who founded the UK-based social enterprise The Washing Machine Project (TWMP) to tackle this, and has now shipped almost 500 of his hand-crank Divya machines to 13 countries, including Mexico, Ghana, Iraq and the US.&lt;/p&gt;
    &lt;p&gt;The Divya washing machine, made up of an outer drum and an inner one which rotates, operates a 30-minute wash cycle where it completes a 5kg load needing only a few minutes of manual turning.&lt;/p&gt;
    &lt;p&gt;It works like this: after loading the clothes, detergent and water, and letting it sit for 10-15 minutes, users can close the lid and turn the handle for two minutes, repeating this twice more after ten minutes of letting the clothes sit in between spins. And voila — the machine can then be drained using the tap at the front.&lt;/p&gt;
    &lt;p&gt;This saves up to 75% of time for its user, and halves water consumption. “The machine takes a task that is exhausting and time-consuming and transforms it into something simple, easier to manage, and time saving,” said Sawhney.&lt;/p&gt;
    &lt;p&gt;The Divya project’s development didn’t end with its invention. “We went back to the drawing board and really listened to the people we were designing for, for the context in which they lived. That research changed everything,” said Laura Tuck, the organisation’s R&amp;amp;D Lead.&lt;/p&gt;
    &lt;p&gt;One crucial consideration was making sure Divyas were fit for the locations where they would be used. For example, in Uganda, machines were delivered to a small island on Lake Victoria using a fishing boat. Repairs or replacements could not get there easily, so the TWMP team needed to rethink how the originally complex gear-system machine could work in these conditions. The solution? Designing a product that was simpler, more intuitive, and repairable locally using the skills and infrastructure available.&lt;/p&gt;
    &lt;p&gt;Guided by feedback from real users during workshops and focus groups, TWMP improved the machine’s durability, physical strain, and usability, with the team introducing robust metal frames, simplified workflows, and improved seals and taps.&lt;/p&gt;
    &lt;p&gt;The innovation has already impacted the lives of almost 50,000 people – and Sawhney is just getting started.&lt;/p&gt;
    &lt;p&gt;TWMP hopes to reach 1,000,000 people by 2030, but says it cannot do this alone; it is building a network of partners including NGOs, UN agencies, and local communities, including the Whirlpool Foundation, the charity wing of the US-based home appliance firm.&lt;/p&gt;
    &lt;p&gt;Localised production will begin in early 2026, manufacturing a new generation of machines in India, closer to those who use them. The project is also piloting ‘Hubs’, where machines can be assembled and distributed, but also offering training, workshops, and educational activities, extending the impact of the time saved by Divya machines.&lt;/p&gt;
    &lt;p&gt;It is also seeking policy engagement to embed laundry access in wider strategies around water, sanitation, hygiene, and gender equality.&lt;/p&gt;
    &lt;p&gt;Images: The Washing Machine Project&lt;/p&gt;
    &lt;head rend="h3"&gt;Be part of the solution&lt;/head&gt;
    &lt;p&gt;At Positive News, we’re not chasing clicks or profits for media moguls – we’re here to serve you and have a positive social impact. We can’t do this unless enough people like you choose to support our journalism.&lt;/p&gt;
    &lt;p&gt;Give once from just £1, or join 1,700+ others who contribute an average of £3 or more per month. Together, we can build a healthier form of media – one that focuses on solutions, progress and possibilities, and empowers people to create positive change.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46258906</guid><pubDate>Sat, 13 Dec 2025 22:38:08 +0000</pubDate></item><item><title>Linux Sandboxes and Fil-C</title><link>https://fil-c.org/seccomp</link><description>&lt;doc fingerprint="ea639991fc2930e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linux Sandboxes And Fil-C&lt;/head&gt;
    &lt;p&gt;Memory safety and sandboxing are two different things. It's reasonable to think of them as orthogonal: you could have memory safety but not be sandboxed, or you could be sandboxed but not memory safe.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Example of memory safe but not sandboxed: a pure Java program that opens files on the filesystem for reading and writing and accepts filenames from the user. The OS will allow this program to overwrite any file that the user has access to. This program can be quite dangerous even if it is memory safe. Worse, imagine that the program didn't have any code to open files for reading and writing, but also had no sandbox to prevent those syscalls from working. If there was a bug in the memory safety enforcement of this program (say, because of a bug in the Java implementation), then an attacker could cause this program to overwrite any file if they succeeded at achieving code execution via weird state.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Example of sandboxed but not memory safe: a program written in assembly that starts by requesting that the OS revoke all of its capabilities beyond just pure compute. If the program did want to open a file or write to it, then the kernel will kill the process, based on the earlier request to have this capability revoked. This program could have lots of memory safety bugs (because it's written in assembly), but even if it did, then the attacker cannot make this program overwrite any file unless they find some way to bypass the sandbox.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In practice, sandboxes have holes by design. A typical sandbox allows the program to send and receive messages to broker processes that have higher privileges. So, an attacker may first use a memory safety bug to make the sandboxed process send malicious messages, and then use those malicious messages to break into the brokers.&lt;/p&gt;
    &lt;p&gt;The best kind of defense is to have both a sandbox and memory safety. This document describes how to combine sandboxing and Fil-C's memory safety by explaining what it takes to port OpenSSH's seccomp-based Linux sandbox code to Fil-C.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Fil-C is a memory safe implementation of C and C++ and this site has a lot of documentation about it. Unlike most memory safe languages, Fil-C enforces safety down to where your code meets Linux syscalls and the Fil-C runtime is robust enough that it's possible to use it in low-level system components like &lt;code&gt;init&lt;/code&gt; and &lt;code&gt;udevd&lt;/code&gt;. Lots of programs work in Fil-C, including OpenSSH, which makes use of seccomp-BPF sandboxing.&lt;/p&gt;
    &lt;p&gt;This document focuses on how OpenSSH uses seccomp and other technologies on Linux to build a sandbox around its unprivileged &lt;code&gt;sshd-session&lt;/code&gt; process. Let's review what tools Linux gives us that OpenSSH uses:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;chroot&lt;/code&gt;to restrict the process's view of the filesystem.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running the process with the&lt;/p&gt;&lt;code&gt;sshd&lt;/code&gt;user and group, and giving that user/group no privileges.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setrlimit&lt;/code&gt;to prevent opening files, starting processes, or writing to files.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;seccomp-BPF syscall filter to reduce the attack surface by allowlisting only the set of syscalls that are legitimate for the unprivileged process. Syscalls not in the allowlist will crash the process with&lt;/p&gt;&lt;code&gt;SIGSYS&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Chromium developers and the Mozilla developers both have excellent notes about how to do sandboxing on Linux using seccomp. Seccomp-BPF is a well-documented kernel feature that can be used as part of a larger sandboxing story.&lt;/p&gt;
    &lt;p&gt;Fil-C makes it easy to use &lt;code&gt;chroot&lt;/code&gt; and different users and groups. The syscalls that are used for that part of the sandbox are trivially allowed by Fil-C and no special care is required to use them.&lt;/p&gt;
    &lt;p&gt;Both &lt;code&gt;setrlimit&lt;/code&gt; and seccomp-BPF require special care because the Fil-C runtime starts threads, allocates memory, and performs synchronization. This document describes what you need to know to make effective use of those sandboxing technologies in Fil-C. First, I describe how to build a sandbox that prevents thread creation without breaking Fil-C's use of threads. Then, I describe what tweaks I had to make to OpenSSH's seccomp filter. Finally, I describe how the Fil-C runtime implements the syscalls used to install seccomp filters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preventing Thread Creation Without Breaking The Fil-C Runtime&lt;/head&gt;
    &lt;p&gt;The Fil-C runtime uses multiple background threads for garbage collection and has the ability to automatically shut those threads down when they are not in use. If the program wakes up and starts allocating memory again, then those threads are automatically restarted.&lt;/p&gt;
    &lt;p&gt;Starting threads violates the "no new processes" rule that OpenSSH's &lt;code&gt;setrlimit&lt;/code&gt; sandbox tries to achieve (since threads are just lightweight processes on Linux). It also relies on syscalls like &lt;code&gt;clone3&lt;/code&gt; that are not part of OpenSSH's seccomp filter allowlist.&lt;/p&gt;
    &lt;p&gt;It would be a regression to the sandbox to allow process creation just because the Fil-C runtime relies on it. Instead, I added a new API to &lt;code&gt;&amp;lt;stdfil.h&amp;gt;&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;void zlock_runtime_threads(void);
&lt;/code&gt;
    &lt;p&gt;This forces the runtime to immediately create whatever threads it needs, and to disable shutting them down on demand. Then, I added a call to &lt;code&gt;zlock_runtime_threads()&lt;/code&gt; in OpenSSH's &lt;code&gt;ssh_sandbox_child&lt;/code&gt; function before either the &lt;code&gt;setrlimit&lt;/code&gt; or seccomp-BPF sandbox calls happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tweaks To The OpenSSH Sandbox&lt;/head&gt;
    &lt;p&gt;Because the use of &lt;code&gt;zlock_runtime_threads()&lt;/code&gt; prevents subsequent thread creation from happening, most of the OpenSSH sandbox just works. I did not have to change how OpenSSH uses &lt;code&gt;setrlimit&lt;/code&gt;. I did change the following about the seccomp filter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Failure results in&lt;/p&gt;&lt;code&gt;SECCOMP_RET_KILL_PROCESS&lt;/code&gt;rather than&lt;code&gt;SECCOMP_RET_KILL&lt;/code&gt;. This ensures that Fil-C's background threads are also killed if a sandbox violation occurs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MAP_NORESERVE&lt;/code&gt;is added to the&lt;code&gt;mmap&lt;/code&gt;allowlist, since the Fil-C allocator uses it. This is not a meaningful regression to the filter, since&lt;code&gt;MAP_NORESERVE&lt;/code&gt;is not a meaningful capability for an attacker to have.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sched_yield&lt;/code&gt;is allowed. This is not a dangerous syscall (it's semantically a no-op). The Fil-C runtime uses it as part of its lock implementation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nothing else had to change, since the filter already allowed all of the &lt;code&gt;futex&lt;/code&gt; syscalls that Fil-C uses for synchronization.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Fil-C Implements &lt;code&gt;prctl&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The OpenSSH seccomp filter is installed using two &lt;code&gt;prctl&lt;/code&gt; calls. First, we &lt;code&gt;PR_SET_NO_NEW_PRIVS&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) == -1) {
        debug("%s: prctl(PR_SET_NO_NEW_PRIVS): %s",
            __func__, strerror(errno));
        nnp_failed = 1;
}
&lt;/code&gt;
    &lt;p&gt;This prevents additional privileges from being acquired via &lt;code&gt;execve&lt;/code&gt;. It's required that unprivileged processes that install seccomp filters first set the &lt;code&gt;no_new_privs&lt;/code&gt; bit.&lt;/p&gt;
    &lt;p&gt;Next, we &lt;code&gt;PR_SET_SECCOMP, SECCOMP_MODE_FILTER&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;if (prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, &amp;amp;preauth_program) == -1)
        debug("%s: prctl(PR_SET_SECCOMP): %s",
            __func__, strerror(errno));
else if (nnp_failed)
        fatal("%s: SECCOMP_MODE_FILTER activated but "
            "PR_SET_NO_NEW_PRIVS failed", __func__);
&lt;/code&gt;
    &lt;p&gt;This installs the seccomp filter in &lt;code&gt;preauth_program&lt;/code&gt;. Note that this will fail in the kernel if the &lt;code&gt;no_new_privs&lt;/code&gt; bit is not set, so the fact that OpenSSH reports a fatal error if the filter is installed without &lt;code&gt;no_new_privs&lt;/code&gt; is just healthy paranoia on the part of the OpenSSH authors.&lt;/p&gt;
    &lt;p&gt;The trouble with both syscalls is that they affect the calling thread, not all threads in the process. Without special care, Fil-C runtime's background threads would not have the &lt;code&gt;no_new_privs&lt;/code&gt; bit set and would not have the filter installed. This would mean that if an attacker busted through Fil-C's memory safety protections (in the unlikely event that they found a bug in Fil-C itself!), then they could use those other threads to execute syscalls that bypass the filter!&lt;/p&gt;
    &lt;p&gt;To prevent even this unlikely escape, the Fil-C runtime's wrapper for &lt;code&gt;prctl&lt;/code&gt; implements &lt;code&gt;PR_SET_NO_NEW_PRIVS&lt;/code&gt; and &lt;code&gt;PR_SET_SECCOMP&lt;/code&gt; by handshaking all runtime threads using this internal API:&lt;/p&gt;
    &lt;code&gt;/* Calls the callback from every runtime thread. */
PAS_API void filc_runtime_threads_handshake(void (*callback)(void* arg), void* arg);
&lt;/code&gt;
    &lt;p&gt;The callback performs the requested &lt;code&gt;prctl&lt;/code&gt; from each runtime thread. This ensures that the &lt;code&gt;no_new_privs&lt;/code&gt; bit and the filter are installed on all threads in the Fil-C process.&lt;/p&gt;
    &lt;p&gt;Additionally, because of ambiguity about what to do if the process has multiple user threads, these two &lt;code&gt;prctl&lt;/code&gt; commands will trigger a Fil-C safety error if the program has multiple user threads.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The best kind of protection if you're serious about security is to combine memory safety with sandboxing. This document shows how to achieve this using Fil-C and the sandbox technologies available on Linux, all without regressing the level of protection that those sandboxes enforce or the memory safety guarantees of Fil-C.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46259064</guid><pubDate>Sat, 13 Dec 2025 22:58:29 +0000</pubDate></item><item><title>Closures as Win32 Window Procedures</title><link>https://nullprogram.com/blog/2025/12/12/</link><description>&lt;doc fingerprint="a6c91f3926ae8748"&gt;
  &lt;main&gt;
    &lt;p&gt; nullprogram.com/blog/2025/12/12/ &lt;/p&gt;
    &lt;p&gt;Back in 2017 I wrote about a technique for creating closures in C using JIT-compiled wrapper. It’s neat, though rarely necessary in real programs, so I don’t think about it often. I applied it to &lt;code&gt;qsort&lt;/code&gt;,
which sadly accepts no context pointer. More practical would be
working around insufficient custom allocator interfaces, to
create allocation functions at run-time bound to a particular allocation
region. I’ve learned a lot since I last wrote about this subject, and a
recent article had me thinking about it again, and how I could do
better than before. In this article I will enhance Win32 window procedure
callbacks with a fifth argument, allowing us to more directly pass extra
context. I’m using w64devkit on x64, but the everything here should
work out-of-the-box with any x64 toolchain that speaks GNU assembly.&lt;/p&gt;
    &lt;p&gt;A window procedure has this prototype:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;LRESULT Wndproc(
  HWND hWnd,
  UINT Msg,
  WPARAM wParam,
  LPARAM lParam,
);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To create a window we must first register a class with &lt;code&gt;RegisterClass&lt;/code&gt;,
which accepts a set of properties describing a window class, including a
pointer to one of these functions.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    MyState *state = ...;

    RegisterClassA(&amp;amp;(WNDCLASSA){
        // ...
        .lpfnWndProc   = my_wndproc,
        .lpszClassName = "my_class",
        // ...
    });

    HWND hwnd = CreateWindowExA("my_class", ..., state);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The thread drives a message pump with events from the operating system, dispatching them to this procedure, which then manipulates the program state in response:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    for (MSG msg; GetMessageW(&amp;amp;msg, 0, 0, 0);) {
        TranslateMessage(&amp;amp;msg);
        DispatchMessageW(&amp;amp;msg);  // calls the window procedure
    }
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;All four &lt;code&gt;WNDPROC&lt;/code&gt; parameters are determined by Win32. There is no context
pointer argument. So how does this procedure access the program state? We
generally have two options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Global variables. Yucky but easy. Frequently seen in tutorials.&lt;/item&gt;
      &lt;item&gt;A &lt;code&gt;GWLP_USERDATA&lt;/code&gt; pointer attached to the window.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The second option takes some setup. Win32 passes the last &lt;code&gt;CreateWindowEx&lt;/code&gt;
argument to the window procedure when the window created, via &lt;code&gt;WM_CREATE&lt;/code&gt;.
The procedure attaches the pointer to its window as &lt;code&gt;GWLP_USERDATA&lt;/code&gt;. This
pointer is passed indirectly, through a &lt;code&gt;CREATESTRUCT&lt;/code&gt;. So ultimately it
looks like this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    case WM_CREATE:
        CREATESTRUCT *cs = (CREATESTRUCT *)lParam;
        void *arg = (struct state *)cs-&amp;gt;lpCreateParams;
        SetWindowLongPtr(hwnd, GWLP_USERDATA, (LONG_PTR)arg);
        // ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;In future messages we can retrieve it with &lt;code&gt;GetWindowLongPtr&lt;/code&gt;. Every time
I go through this I wish there was a better way. What if there was a fifth
window procedure parameter though which we could pass a context?&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef LRESULT Wndproc5(HWND, UINT, WPARAM, LPARAM, void *);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;We’ll build just this as a trampoline. The x64 calling convention passes the first four arguments in registers, and the rest are pushed on the stack, including this new parameter. Our trampoline cannot just stuff the extra parameter in the register, but will actually have to build a stack frame. Slightly more complicated, but barely so.&lt;/p&gt;
    &lt;head rend="h3"&gt;Allocating executable memory&lt;/head&gt;
    &lt;p&gt;In previous articles, and in the programs where I’ve applied techniques like this, I’ve allocated executable memory with &lt;code&gt;VirtualAlloc&lt;/code&gt; (or &lt;code&gt;mmap&lt;/code&gt;
elsewhere). This introduces a small challenge for solving the problem
generally: Allocations may be arbitrarily far from our code and data, out
of reach of relative addressing. If they’re further than 2G apart, we need
to encode absolute addresses, and in the simple case would just assume
they’re always too far apart.&lt;/p&gt;
    &lt;p&gt;These days I’ve more experience with executable formats, and allocation, and I immediately see a better solution: Request a block of writable, executable memory from the loader, then allocate our trampolines from it. Other than being executable, this memory isn’t special, and allocation works the usual way, using functions unaware it’s executable. By allocating through the loader, this memory will be part of our loaded image, guaranteed to be close to our other code and data, allowing our JIT compiler to assume a small code model.&lt;/p&gt;
    &lt;p&gt;There are a number of ways to do this, and here’s one way to do it with GNU-styled toolchains targeting COFF:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;        .section .exebuf,"bwx"
        .globl exebuf
exebuf:	.space 1&amp;lt;&amp;lt;21
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;This assembly program defines a new section named &lt;code&gt;.exebuf&lt;/code&gt; containing 2M
of writable (&lt;code&gt;"w"&lt;/code&gt;), executable (&lt;code&gt;"x"&lt;/code&gt;) memory, allocated at run time just
like &lt;code&gt;.bss&lt;/code&gt; (&lt;code&gt;"b"&lt;/code&gt;). We’ll treat this like an arena out of which we can
allocate all trampolines we’ll probably ever need. With careful use of
&lt;code&gt;.pushsection&lt;/code&gt; this could be basic inline assembly, but I’ve left it as a
separate source. On the C side I retrieve this like so:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef struct {
    char *beg;
    char *end;
} Arena;

Arena get_exebuf()
{
    extern char exebuf[1&amp;lt;&amp;lt;21];
    Arena r = {exebuf, exebuf+sizeof(exebuf)};
    return r;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Unfortunately I have to repeat myself on the size. There are different ways to deal with this, but this is simple enough for now. I would have loved to define the array in C with the GCC &lt;code&gt;section&lt;/code&gt; attribute,
but as is usually the case with this attribute, it’s not up to the task,
lacking the ability to set section flags. Besides, by not relying on the
attribute, any C compiler could compile this source, and we only need a
GNU-style toolchain to create the tiny COFF object containing &lt;code&gt;exebuf&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;While we’re at it, a reminder of some other basic definitions we’ll need:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;#define S(s)            (Str){s, sizeof(s)-1}
#define new(a, n, t)    (t *)alloc(a, n, sizeof(t), _Alignof(t))

typedef struct {
    char     *data;
    ptrdiff_t len;
} Str;

Str clone(Arena *a, Str s)
{
    Str r = s;
    r.data = new(a, r.len, char);
    memcpy(r.data, s.data, (size_t)r.len);
    return r;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Which have been discussed at length in previous articles.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trampoline compiler&lt;/head&gt;
    &lt;p&gt;From here the plan is to create a function that accepts a &lt;code&gt;Wndproc5&lt;/code&gt; and a
context pointer to bind, and returns a classic &lt;code&gt;WNDPROC&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;WNDPROC make_wndproc(Arena *, Wndproc5, void *arg);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Our window procedure now gets a fifth argument with the program state:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;LRESULT my_wndproc(HWND, UINT, WPARAM, LPARAM, void *arg)
{
    MyState *state = arg;
    // ...
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;When registering the class we wrap it in a trampoline compatible with &lt;code&gt;RegisterClass&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    RegisterClassA(&amp;amp;(WNDCLASSA){
        // ...
        .lpfnWndProc   = make_wndproc(a, my_wndproc, state),
        .lpszClassName = "my_class",
        // ...
    });
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;All windows using this class will readily have access to this state object through their fifth parameter. It turns out setting up &lt;code&gt;exebuf&lt;/code&gt; was the
more complicated part, and &lt;code&gt;make_wndproc&lt;/code&gt; is quite simple!&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;WNDPROC make_wndproc(Arena *a, Wndproc5 proc, void *arg)
{
    Str thunk = S(
        "\x48\x83\xec\x28"      // sub   $40, %rsp
        "\x48\xb8........"      // movq  $arg, %rax
        "\x48\x89\x44\x24\x20"  // mov   %rax, 32(%rsp)
        "\xe8...."              // call  proc
        "\x48\x83\xc4\x28"      // add   $40, %rsp
        "\xc3"                  // ret
    );
    Str r   = clone(a, thunk);
    int rel = (int)((uintptr_t)proc - (uintptr_t)(r.data + 24));
    memcpy(r.data+ 6, &amp;amp;arg, sizeof(arg));
    memcpy(r.data+20, &amp;amp;rel, sizeof(rel));
    return (WNDPROC)r.data;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The assembly allocates a new stack frame, with callee shadow space, and with room for the new argument, which also happens to re-align the stack. It stores the new argument for the &lt;code&gt;Wndproc5&lt;/code&gt; just above the shadow space.
Then calls into the &lt;code&gt;Wndproc5&lt;/code&gt; without touching other parameters. There
are two “patches” to fill out, which I’ve initially filled with dots: the
context pointer itself, and a 32-bit signed relative address for the call.
It’s going to be very near the callee. The only thing I don’t like about
this function is that I’ve manually worked out the patch offsets.&lt;/p&gt;
    &lt;p&gt;It’s probably not useful, but it’s easy to update the context pointer at any time if hold onto the trampoline pointer:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;void set_wndproc_arg(WNDPROC p, void *arg)
{
    memcpy((char *)p+6, &amp;amp;arg, sizeof(arg));
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;So, for instance:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    MyState *state[2] = ...;  // multiple states
    WNDPROC proc = make_wndproc(a, my_wndproc, state[0]);
    // ...
    set_wndproc_arg(proc, state[1]);  // switch states
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Though I expect the most common case is just creating multiple procedures:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    WNDPROC procs[] = {
        make_wndproc(a, my_wndproc, state[0]),
        make_wndproc(a, my_wndproc, state[1]),
    };
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To my slight surprise these trampolines still work with an active Control Flow Guard system policy. Trampolines do not have stack unwind entries, and I thought Windows might refuse to pass control to them.&lt;/p&gt;
    &lt;p&gt;Here’s a complete, runnable example if you’d like to try it yourself: &lt;code&gt;main.c&lt;/code&gt; and &lt;code&gt;exebuf.s&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Better cases&lt;/head&gt;
    &lt;p&gt;This is more work than going through &lt;code&gt;GWLP_USERDATA&lt;/code&gt;, and real programs
have a small, fixed number of window procedures — typically one — so this
isn’t the best example, but I wanted to illustrate with a real interface.
Again, perhaps the best real use is a library with a weak custom allocator
interface:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef struct {
    void *(*malloc)(size_t);   // no context pointer!
    void  (*free)(void *);     // "
} Allocator;

void *arena_malloc(size_t, Arena *);

// ...

    Allocator perm_allocator = {
        .malloc = make_trampoline(exearena, arena_malloc, perm);
        .free   = noop_free,
    };
    Allocator scratch_allocator = {
        .malloc = make_trampoline(exearena, arena_malloc, scratch);
        .free   = noop_free,
    };
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Something to keep in my back pocket for the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46259334</guid><pubDate>Sat, 13 Dec 2025 23:39:48 +0000</pubDate></item><item><title>An Implementation of J (1992)</title><link>https://www.jsoftware.com/ioj/ioj.htm</link><description>&lt;doc fingerprint="8f2792390b4c60ef"&gt;
  &lt;main&gt;&lt;p&gt; An Implementation of J&lt;lb/&gt; Roger K.W. Hui &lt;/p&gt;&lt;p&gt;Preface&lt;/p&gt;J is a dialect of APL freely available on a wide variety of machines. It is the latest in the line of development known as "dictionary APL". The spelling scheme uses the ASCII alphabet. The underlying concepts, such as arrays, verbs, adverbs, and rank, are extensions and generalizations of ideas in APL\360. Anomalies have been removed. The result is at once simpler and more powerful than previous dialects.&lt;p&gt;Ex ungue leonem.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;0. Introduction&lt;/cell&gt;&lt;cell&gt;6. Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;6.1 Numeric Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1. Interpreting a Sentence&lt;/cell&gt;&lt;cell&gt;6.2 Boxed Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.1 Word Formation&lt;/cell&gt;&lt;cell&gt;6.3 Formatted Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.2 Parsing&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.3 Trains&lt;/cell&gt;&lt;cell&gt;7. Comparatives&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.4 Name Resolution&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Appendices&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2. Nouns&lt;/cell&gt;&lt;cell&gt;A. Incunabulum&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.1 Arrays&lt;/cell&gt;&lt;cell&gt;B. Special Code&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.2 Types&lt;/cell&gt;&lt;cell&gt;C. Test Scripts&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.3 Memory Management&lt;/cell&gt;&lt;cell&gt;D. Program Files&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.4 Global Variables&lt;/cell&gt;&lt;cell&gt;E. Foreign Conjunction&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;F. System Summary&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3. Verbs&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.1 Anatomy of a Verb&lt;/cell&gt;&lt;cell&gt;Bibliography&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.2 Rank&lt;/cell&gt;&lt;cell&gt;Glossary and Index&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.3 Atomic (Scalar) Verbs&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.4 Obverses, Identities, and Variants&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.5 Error Handling&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;4. Adverbs and Conjunctions&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5. Representation&lt;/cell&gt;&lt;cell&gt;5.1 Atomic Representation&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5.2 Boxed Representation&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5.3 Tree Representation&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;5.4 Linear Representation&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46259702</guid><pubDate>Sun, 14 Dec 2025 00:34:56 +0000</pubDate></item><item><title>If a Meta AI model can read a brain-wide signal, why wouldn't the brain?</title><link>https://1393.xyz/writing/if-a-meta-ai-model-can-read-a-brain-wide-signal-why-wouldnt-the-brain</link><description>&lt;doc fingerprint="396fd0cf3833952a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If a Meta AI model can read a brain-wide signal, why wouldn’t the brain?&lt;/head&gt;
    &lt;p&gt;Did you know migratory birds and sea turtles are able to navigate using the Earth's magnetic field? It's called magnetoreception. Basically, being able to navigate was evolutionarily advantageous, so life evolved ways to feel the Earth's magnetic field. A LOT of ways. Like a shocking amount of ways. Here's a few examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Magnetotactic bacteria – magnetite chains as built-in compass needles.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Land plants – growth, germination, tropisms modulated by weak magnetic fields.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Honey bee – abdomen magnetite and magnetic compass in foragers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;American cockroach – behavior disrupted by specific RF fields, consistent with a magnetic sense.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fruit fly – cryptochrome-dependent magnetic compass under blue light.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Monarch butterfly – time-compensated magnetic compass for migration.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Loggerhead sea turtle – hatchlings orient in coil-manipulated magnetic fields.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Common carp – spontaneous north–south body alignment in ponds.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sharks and rays – ampullae of Lorenzini detect electric and magnetic fields for navigation and prey detection.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tadpoles – magnetically driven orientation tied to visual system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Box turtle – homing disrupted when local geomagnetic field is altered.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Domestic chicken – chicks trained to find social reward using a magnetic compass.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Homing pigeon – altered magnetic fields at the head deflect homing bearings.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Blind mole-rat – subterranean mammal with light-independent magnetic compass and map.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cattle and deer – grazing/herd bodies align roughly north–south at global scale.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Domestic dogs – defecation posture tracks geomagnetic north–south under quiet field conditions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Humans – alpha-band EEG shows robust, orientation-specific responses to Earth-strength field rotations.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It would seem evolution adores detecting magnetic fields. And it makes sense! A literal "sense of direction" is quite useful in staying alive - nearly all life benefits from it, including us.&lt;/p&gt;
    &lt;p&gt;We don't totally understand how our magnetoreception works yet, but we know that it does. In 2019, some Caltech researchers put some people in a room shielded from the Earth's magnetic field, with a big magnetic field generator in it. They hooked them up to an EEG, and watched what happened in their brains as they manipulated the magnetic field. The result: some of those people showed a response to the magnetic fields on the EEG!&lt;/p&gt;
    &lt;p&gt;That gets my noggin joggin. Our brain responds to magnetic field changes, but we aren't aware of it? What if it affects our mood? Would you believe me if I told you lunar gravity influences the Earth's magnetosphere? Perhaps I was too dismissive of astrology.&lt;/p&gt;
    &lt;head rend="h3"&gt;But seriously&lt;/head&gt;
    &lt;p&gt;Biomagnetism is "the phenomenon of magnetic fields produced by living organisms". Hold up. Produced by? I made another list for you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Weakly electric fish – electric organs generate pulsed currents whose surrounding magnetic fields (nanotesla scale) have been recorded directly near the fish.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Earthworms – single action potentials in the giant axon system produce biomagnetic fields detectable with magnetic resonance spectroscopy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Crayfish – the giant axon’s action currents generate ~10⁻¹⁰–10⁻⁹ T fields measured directly with toroidal pickup coils.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Frogs – action potentials in the sciatic nerve produce pico– to 10⁻¹⁰ T magnetic fields, recorded non-invasively with SQUIDs and optical magnetometers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Guinea pigs – isolated guinea-pig hearts generate magnetocardiograms; their heartbeat fields (tens of picotesla) are recorded with optically pumped magnetometers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cats – neuromagnetic fields from the auditory cortex of domestic cats are measured with magnetoencephalography.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Monkeys – cortical responses to tactile and auditory stimuli in macaque monkeys are mapped by measuring their brain’s magnetic fields with MEG.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rabbits – SQUID magnetometry outside the skull of anesthetized rabbits detects magnetic signatures of spreading cortical depolarization.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Humans – multiple biomagnetic signals are routinely measured: heart fields (magnetocardiography), brain fields (magnetoencephalography), skeletal-muscle fields (magnetomyography) and peripheral-nerve fields (magnetoneurography).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh, right. We run on electricity, so we generate magnetic fields. Makes sense. But wait. We can detect magnetic fields AND we produce them? That's...hmm. Interesting. Let's read about magnetoencephalography (insane word).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Magnetoencephalography (MEG) is a functional neuroimaging technique for mapping brain activity by recording magnetic fields produced by electrical currents occurring naturally in the brain, using very sensitive magnetometers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That's not the interesting thing about MEG. The interesting thing about MEG is that researchers at Meta, using MEG, were able to decode the brains magnetic fields into actual images and words. Who else forgot we successfully read people's minds in 2023? I know I did.&lt;/p&gt;
    &lt;p&gt;Here's how it worked: they trained models on public MEG datasets, and then used those models to decode the thoughts of study participants.&lt;/p&gt;
    &lt;p&gt;In their words:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Overall, our results show that MEG can be used to decipher, with millisecond precision, the rise of complex representations generated in the brain.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I read this back when it happened, but then I read it again the other day and it struck me that it's actually an insane line of research. Like, we can train a model to understand the information that underlies human brain waves. They were able to translate brain activity, from the magnetic field alone, into images and text.&lt;/p&gt;
    &lt;p&gt;So our brain's magnetic field is like a constant (with millisecond precision) readout of the current state of our brain? That's...HUH?&lt;/p&gt;
    &lt;p&gt;So, okay. Research has established that humans can sense magnetic fields, and Meta was able to show that our own brain's magnetic field represents a high-enough-fidelity-to-read-data analog representation of our current "state" of mind. To such an extent that we've been able to decode it across many different brains, despite barely understanding it.&lt;/p&gt;
    &lt;p&gt;So, couldn't our own brain be reading it's own field? I mean, what are the chances evolution wouldn't take advantage of an available wireless summary of the global state of the brain? Wouldn't that answer the binding problem?&lt;/p&gt;
    &lt;head rend="h2"&gt;They're minerals, Marie!&lt;/head&gt;
    &lt;p&gt;The question becomes how does the brain read the field? Well, let's go back to magnetoreception for a minute. How does that work again?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Magnetite biomineralization is a genetically-controlled biochemical process through which organisms make perfect ferrimagnetic crystals, usually of single magnetic domain size. This process is an ancient one, having evolved about 2 billion years ago in the magnetotactic bacteria, and presumably was incorporated in the genome of higher organisms, including humans.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Our brains manufacture ferrimagnetic crystals. Table stakes. Established science.&lt;/p&gt;
    &lt;p&gt;But let's not get too excited. The Earth's magnetic field is quite strong. In fact it's 50 to 500 MILLION times stronger than the brain's magnetic field. So these magnets could detect the Earth's magnetic field for sure, but could they detect the brain's own much weaker fields?&lt;/p&gt;
    &lt;p&gt;I'm here to tell you: hell yeah they could. Those ferrimagnetic crystals created by our brains tend to be in a tight size range that could resonate with and potentially modulate the specific spectrum of known neural oscillations (A.K.A. brain waves). This "stochastic resonance", which is mathematically possible in the brain, would allow for these tiny crystals to locally overcome the Earth's magnetic field.&lt;/p&gt;
    &lt;p&gt;So now we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The brain's magnetic field can be read to extract the actual high-fidelity thoughts a person is having&lt;/item&gt;
      &lt;item&gt;The brain creates magnetic crystals that just so happen to be the perfect size to interact with it's own magnetic field&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feels like we're getting somewhere. Let's pull the thread.&lt;/p&gt;
    &lt;head rend="h2"&gt;Everything is computer&lt;/head&gt;
    &lt;p&gt;If those crystals can read the brain's magnetic field, they can also certainly write to it. The brain's hardwiring/neurochemistry could manipulate the crystals much more easily than the magnetic field could.&lt;/p&gt;
    &lt;p&gt;So, using those biological magnets, could the brain...self-tune itself?&lt;/p&gt;
    &lt;p&gt;Let me zoom out. Here's how I'm thinking about it: the magnetic field seems to represent a current "state" of your thoughts. Nature loves analog systems. What if the brain used it's own magnetic field as a sort of analog compression of all of the underlying information? It definitionally represents the the sum total of all of the electrical activity in the neurons in the brain. It's as low-latency as it gets, limited only by the speed of light. We know our thoughts are encoded within it. Why not?&lt;/p&gt;
    &lt;p&gt;Let's imagine how this might work. We have this global state vehicle, we can read it with magnetic crystals, but how do we complete the loop? There would need to be some global write system, right?&lt;/p&gt;
    &lt;head rend="h3"&gt;The blue spot&lt;/head&gt;
    &lt;p&gt;The locus coeruleus (LC) (latin for "blue spot") is a tiny but obscenely important little part of our brain. It synthesizes "norepinephrine, which is a chemical that changes how alert, focused, and "plastic" (malleable) your brain is. I'll quote the Wiki:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The projections of this nucleus reach far and wide. For example, they innervate the spinal cord, the brain stem, cerebellum, hypothalamus, the hippocampus, the thalamic relay nuclei, the amygdala, the basal telencephalon, and the cortex. The norepinephrine from the LC has an excitatory effect on most of the brain, mediating arousal and priming the brain's neurons to be activated by stimuli.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The page continues to list all of the functions the LC-NA system is known to influence:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arousal and sleep-wake cycle&lt;/item&gt;
      &lt;item&gt;Attention and memory&lt;/item&gt;
      &lt;item&gt;Behavioral and cognitive flexibility, creativity, personality, behavioral inhibition, and stress&lt;/item&gt;
      &lt;item&gt;Cognitive control&lt;/item&gt;
      &lt;item&gt;Decision making and utility maximization&lt;/item&gt;
      &lt;item&gt;Emotions&lt;/item&gt;
      &lt;item&gt;Neuroplasticity&lt;/item&gt;
      &lt;item&gt;Posture and balance&lt;/item&gt;
      &lt;item&gt;Global model failure where predictions about the world are strongly violated&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That sure sounds like a global write system.&lt;/p&gt;
    &lt;p&gt;Did I mention it's located at the center of the brain?&lt;/p&gt;
    &lt;head rend="h3"&gt;Compacting...&lt;/head&gt;
    &lt;p&gt;Let's summarize again.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The brain's magnetic field can be read to extract the actual high-fidelity thoughts a person is having&lt;/item&gt;
      &lt;item&gt;The brain creates magnetic crystals that just so happen to be the perfect size to interact with it's own magnetic field&lt;/item&gt;
      &lt;item&gt;The brain has a single system that can, in response to stimuli, release a chemical that changes how the rest of the brain responds to stimuli&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So let's make this a causal loop.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The brain creates a structured magnetic field pattern&lt;/item&gt;
      &lt;item&gt;Magnetic crystals, in reaction to that pattern, trigger neurons to send signals (some to the LC-NA system)&lt;/item&gt;
      &lt;item&gt;Based on the data the LC receives (danger, reward, big decision), it fires a burst of norepinephrine&lt;/item&gt;
      &lt;item&gt;That norephinephrine globally changes brain neurochemistry, which changes how magnetic crystals react to the magnetic field&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The brain could then use that process to heavily optimize itself locally based on global state.&lt;/p&gt;
    &lt;head rend="h2"&gt;The easier-than-expected problem of consciousness&lt;/head&gt;
    &lt;p&gt;So hold on, if the "brain computer" uses a lossy summary of all neuron activity to make decisions, doesn't that kind of sound familiar? Isn't a "lossy summary of all neuron activity" kind of equivalent to...what it feels like to be conscious?&lt;/p&gt;
    &lt;p&gt;Like, basically the brain is a computer, but imagine it has to compress all of this data into one dimension. "Feeling like" something is a compression artifact - it's a lossy representation of all of the underlying data. So, we're computers, but it feels like this to be a computer because of the way the data is compressed.&lt;/p&gt;
    &lt;p&gt;"What it feels like" to be conscious is the inevitable end result of...extremely optimized data compression. Incredible. Evolution is an unmatched engineer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Air pollution may be more serious than we realize&lt;/head&gt;
    &lt;p&gt;The magnetic crystals created by our brains are not the only ones we've found in there. Among the perfectly shaped and sized biogenic crystals are pollution-derived magnetic particles of many shapes and sizes that are prolific in urban, airborne particulate matter. We breathe this in through the nose, and it enters the brain directly through the olfactory nerve.&lt;/p&gt;
    &lt;p&gt;Now, remember how the natural magnetic crystals are a very specific size and shape that allows them to resonate with the brain's magnetic field and ultimately overcome the Earth's magnetic field. Above a certain volume, pollution-derived particles would significantly change the math in that system, and could very easily impact the ability for the brain's magnetic field to interact with it's own natural magnetic crystals.&lt;/p&gt;
    &lt;p&gt;So, there should be evidence of a system breakdown among people who breathe enough sufficiently polluted air. We would see a breakdown of ability to learn - significant issues with memory. It would build up over time and it's progression would be slow at first. The locus coeruleus in particular would become less active as the brain lost the ability to send signals to "self-tune" itself.&lt;/p&gt;
    &lt;p&gt;Air pollution has been significantly correlated with Alzheimer's disease. They've even recently demonstrated evidence of causation.&lt;/p&gt;
    &lt;p&gt;And critically, where have we (inexplicably thus far) seen some of the earliest signs of Alzheimer's pathology?&lt;/p&gt;
    &lt;p&gt;The locus coeruleus.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you found this interesting, I put together a whitepaper with a lot more digging on the Alzheimer's theory.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46260106</guid><pubDate>Sun, 14 Dec 2025 01:43:33 +0000</pubDate></item><item><title>Lean theorem prover mathlib</title><link>https://github.com/leanprover-community/mathlib4</link><description>&lt;doc fingerprint="adf4ba731ce775e1"&gt;
  &lt;main&gt;
    &lt;p&gt;Mathlib is a user maintained library for the Lean theorem prover. It contains both programming infrastructure and mathematics, as well as tactics that use the former and allow to develop the latter.&lt;/p&gt;
    &lt;p&gt;You can find detailed instructions to install Lean, mathlib, and supporting tools on our website. Alternatively, click on one of the buttons below to open a GitHub Codespace or a Gitpod workspace containing the project.&lt;/p&gt;
    &lt;p&gt;Please refer to https://github.com/leanprover-community/mathlib4/wiki/Using-mathlib4-as-a-dependency&lt;/p&gt;
    &lt;p&gt;Got everything installed? Why not start with the tutorial project?&lt;/p&gt;
    &lt;p&gt;For more pointers, see Learning Lean.&lt;/p&gt;
    &lt;p&gt;Besides the installation guides above and Lean's general documentation, the documentation of mathlib consists of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The mathlib4 docs: documentation generated automatically from the source &lt;code&gt;.lean&lt;/code&gt;files.&lt;/item&gt;
      &lt;item&gt;A description of currently covered theories, as well as an overview for mathematicians.&lt;/item&gt;
      &lt;item&gt;Some extra Lean documentation not specific to mathlib (see "Miscellaneous topics")&lt;/item&gt;
      &lt;item&gt;Documentation for people who would like to contribute to mathlib&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Much of the discussion surrounding mathlib occurs in a Zulip chat room, and you are welcome to join, or read along without signing up. Questions from users at all levels of expertise are welcome! We also provide an archive of the public discussions, which is useful for quick reference.&lt;/p&gt;
    &lt;p&gt;The complete documentation for contributing to &lt;code&gt;mathlib&lt;/code&gt; is located
on the community guide contribute to mathlib&lt;/p&gt;
    &lt;p&gt;You may want to subscribe to the &lt;code&gt;mathlib4&lt;/code&gt; channel on Zulip to introduce yourself and your plan to the community.
Often you can find community members willing to help you get started and advise you on the fit and
feasibility of your project.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;To obtain precompiled&lt;/p&gt;&lt;code&gt;olean&lt;/code&gt;files, run&lt;code&gt;lake exe cache get&lt;/code&gt;. (Skipping this step means the next step will be very slow.)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;To build&lt;/p&gt;&lt;code&gt;mathlib4&lt;/code&gt;run&lt;code&gt;lake build&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;To build and run all tests, run&lt;/p&gt;&lt;code&gt;lake test&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can use&lt;/p&gt;&lt;code&gt;lake build Mathlib.Import.Path&lt;/code&gt;to build a particular file, e.g.&lt;code&gt;lake build Mathlib.Algebra.Group.Defs&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you added a new file, run the following command to update&lt;/p&gt;
        &lt;code&gt;Mathlib.lean&lt;/code&gt;
        &lt;quote&gt;lake exe mk_all&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mathlib has the following guidelines and conventions that must be followed&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The style guide&lt;/item&gt;
      &lt;item&gt;A guide on the naming convention&lt;/item&gt;
      &lt;item&gt;The documentation style&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can run &lt;code&gt;lake exe cache get&lt;/code&gt; to download cached build files that are computed by &lt;code&gt;mathlib4&lt;/code&gt;'s automated workflow.&lt;/p&gt;
    &lt;p&gt;If something goes mysteriously wrong, you can try one of &lt;code&gt;lake clean&lt;/code&gt; or &lt;code&gt;rm -rf .lake&lt;/code&gt; before trying &lt;code&gt;lake exe cache get&lt;/code&gt; again.
In some circumstances you might try &lt;code&gt;lake exe cache get!&lt;/code&gt;
which re-downloads cached build files even if they are available locally.&lt;/p&gt;
    &lt;p&gt;Call &lt;code&gt;lake exe cache&lt;/code&gt; to see its help menu.&lt;/p&gt;
    &lt;p&gt;The mathlib4_docs repository is responsible for generating and publishing the mathlib4 docs.&lt;/p&gt;
    &lt;p&gt;That repo can be used to build the docs locally:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/leanprover-community/mathlib4_docs.git
cd mathlib4_docs
cp ../mathlib4/lean-toolchain .
lake exe cache get
lake build Mathlib:docs&lt;/code&gt;
    &lt;p&gt;The last step may take a while (&amp;gt;20 minutes). The HTML files can then be found in &lt;code&gt;.lake/build/doc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For users familiar with Lean 3 who want to get up to speed in Lean 4 and migrate their existing Lean 3 code we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A survival guide for Lean 3 users&lt;/item&gt;
      &lt;item&gt;Instructions to run &lt;code&gt;mathport&lt;/code&gt;on a project other than mathlib.&lt;code&gt;mathport&lt;/code&gt;is the tool the community used to port the entirety of&lt;code&gt;mathlib&lt;/code&gt;from Lean 3 to Lean 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are a mathlib contributor and want to update dependencies, use &lt;code&gt;lake update&lt;/code&gt;,
or &lt;code&gt;lake update batteries aesop&lt;/code&gt; (or similar) to update a subset of the dependencies.
This will update the &lt;code&gt;lake-manifest.json&lt;/code&gt; file correctly.
You will need to make a PR after committing the changes to this file.&lt;/p&gt;
    &lt;p&gt;Please do not run &lt;code&gt;lake update -Kdoc=on&lt;/code&gt; as previously advised, as the documentation related
dependencies should only be included when CI is building documentation.&lt;/p&gt;
    &lt;p&gt;For a list containing more detailed information, see https://leanprover-community.github.io/teams/maintainers.html&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Anne Baanen (@Vierkantor): algebra, number theory, tactics&lt;/item&gt;
      &lt;item&gt;Matthew Robert Ballard (@mattrobball): algebra, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Riccardo Brasca (@riccardobrasca): algebra, number theory, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Kevin Buzzard (@kbuzzard): algebra, number theory, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Mario Carneiro (@digama0): lean formalization, tactics, type theory, proof engineering&lt;/item&gt;
      &lt;item&gt;Bryan Gin-ge Chen (@bryangingechen): documentation, infrastructure&lt;/item&gt;
      &lt;item&gt;Johan Commelin (@jcommelin): algebra, number theory, category theory, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Anatole Dedecker (@ADedecker): topology, functional analysis, calculus&lt;/item&gt;
      &lt;item&gt;Rémy Degenne (@RemyDegenne): probability, measure theory, analysis&lt;/item&gt;
      &lt;item&gt;Floris van Doorn (@fpvandoorn): measure theory, model theory, tactics&lt;/item&gt;
      &lt;item&gt;Frédéric Dupuis (@dupuisf): linear algebra, functional analysis&lt;/item&gt;
      &lt;item&gt;Sébastien Gouëzel (@sgouezel): topology, calculus, geometry, analysis, measure theory&lt;/item&gt;
      &lt;item&gt;Markus Himmel (@TwoFX): category theory&lt;/item&gt;
      &lt;item&gt;Yury G. Kudryashov (@urkud): analysis, topology, measure theory&lt;/item&gt;
      &lt;item&gt;Robert Y. Lewis (@robertylewis): tactics, documentation&lt;/item&gt;
      &lt;item&gt;Jireh Loreaux (@j-loreaux): analysis, topology, operator algebras&lt;/item&gt;
      &lt;item&gt;Heather Macbeth (@hrmacbeth): geometry, analysis&lt;/item&gt;
      &lt;item&gt;Patrick Massot (@patrickmassot): documentation, topology, geometry&lt;/item&gt;
      &lt;item&gt;Bhavik Mehta (@b-mehta): category theory, combinatorics&lt;/item&gt;
      &lt;item&gt;Kyle Miller (@kmill): combinatorics, tactics, metaprogramming&lt;/item&gt;
      &lt;item&gt;Kim Morrison (@kim-em): category theory, tactics&lt;/item&gt;
      &lt;item&gt;Oliver Nash (@ocfnash): algebra, geometry, topology&lt;/item&gt;
      &lt;item&gt;Joël Riou (@joelriou): category theory, homology, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Michael Rothgang (@grunweg): differential geometry, analysis, topology, linters&lt;/item&gt;
      &lt;item&gt;Damiano Testa (@adomani): algebra, algebraic geometry, number theory, tactics, linter&lt;/item&gt;
      &lt;item&gt;Adam Topaz (@adamtopaz): algebra, category theory, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Eric Wieser (@eric-wieser): algebra, infrastructure&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jeremy Avigad (@avigad): analysis&lt;/item&gt;
      &lt;item&gt;Reid Barton (@rwbarton): category theory, topology&lt;/item&gt;
      &lt;item&gt;Gabriel Ebner (@gebner): tactics, infrastructure, core, formal languages&lt;/item&gt;
      &lt;item&gt;Johannes Hölzl (@johoelzl): measure theory, topology&lt;/item&gt;
      &lt;item&gt;Simon Hudon (@cipher1024): tactics&lt;/item&gt;
      &lt;item&gt;Chris Hughes (@ChrisHughes24): algebra&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46260128</guid><pubDate>Sun, 14 Dec 2025 01:49:11 +0000</pubDate></item><item><title>Compiler Engineering in Practice</title><link>https://chisophugis.github.io/2025/12/08/compiler-engineering-in-practice-part-1-what-is-a-compiler.html</link><description>&lt;doc fingerprint="4c3f098e613990c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compiler Engineering in Practice - Part 1: What is a Compiler?&lt;/head&gt;
    &lt;p&gt;“Compiler Engineering in Practice” is a blog series intended to pass on wisdom that seemingly every seasoned compiler developer knows, but is not systematically written down in any textbook or online resource. Some (but not much) prior experience with compilers is needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a compiler?&lt;/head&gt;
    &lt;p&gt;The first and most important question is “what is a compiler?”. In short, a compiler is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a translator that translates between two different languages, where those languages represent a description of a computation, and&lt;/item&gt;
      &lt;item&gt;the behavior of the computation in the output language must “match” the behavior of the computation in the input language (more on this below).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, an input language can be C, and the output can be x86 assembly. By this definition, an assembler is also a compiler (albeit a simple one), in that it reads x86 textual assembly and outputs x86 binary machine code, which are two different languages. The &lt;code&gt;python&lt;/code&gt; program that executes Python code contains a compiler – one that reads Python source code and outputs Python interpreter bytecode.&lt;/p&gt;
    &lt;p&gt;This brings me to my first important point about practical compiler engineering – it’s not some mystical art. Compilers, operating systems, and databases are usually considered some kind of special corner of computer science / software engineering for being complex, and indeed, there are some corners of compilers that are a black art. But taking a step back, a compiler is simply a program that reads a file and writes a file. From a development perspective, it’s not that different from &lt;code&gt;cat&lt;/code&gt; or &lt;code&gt;grep&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Why does this matter? Because it means that compilers are easy to debug if you build them right. There are no time-dependent interrupts like an operating system, async external events like a web browser, or large enough scale that hardware has to be considered unreliable like a database. It’s just a command line program (or can be reduced to one if engineered right), such that nearly all bugs are reproducible and debuggable in isolation from the comfort of your workstation. No connecting to a flaky dev board, no extensive mocking of various interfaces.&lt;/p&gt;
    &lt;p&gt;You might say – wait a minute – if I’m running on my company’s AI hardware, I may need to connect to a dev board. Yes, but if you do things right, you will rarely need to do that when debugging the compiler proper. Which brings me to…&lt;/p&gt;
    &lt;head rend="h2"&gt;Reliability&lt;/head&gt;
    &lt;p&gt;Compilers are like operating systems and databases in that the bar for reliability is extremely high. One cannot build a practical compiler haphazardly. Why? Because of miscompiles.&lt;/p&gt;
    &lt;p&gt;Miscompiles are when the compiler produces an output file in the output language that does not “match” the specification of its computation in the input language. To avoid a miscompile, the output program must behave identically to the input program, as far as can be observed by the outside world, such as network requests, values printed to the console, values written to files, etc.&lt;/p&gt;
    &lt;p&gt;For integer programs, bit-exact results are required, though there are some nuances regarding undefined behavior, as described in John Regehr’s “laws of physics of compilers”. For floating point programs, the expectation of bit-exact results is usually too strict. Transformations on large floating point computations (like AI programs) need some flexibility to produce slightly different outputs in order to allow efficient execution. There is no widely-agreed-upon formal definition of this, though there are reasonable ways to check for it in practice (“atol/rtol” go a long way).&lt;/p&gt;
    &lt;head rend="h3"&gt;How bad is a miscompile?&lt;/head&gt;
    &lt;p&gt;Miscompiles can have massive consequences for customers. A miscompile of a database can cause data loss. A miscompile of an operating system can cause a security vulnerability. A miscompile of an AI program can cause bad medical advice. The stakes are extremely high, and debugging a miscompile when it happens “in the wild” can easily take 3+ months (and it can take months for a customer to even realize that their issue is caused by a miscompile).&lt;/p&gt;
    &lt;p&gt;If that weren’t enough, there’s a self-serving reason to avoid miscompiles – if you have too many of them, your development velocity on your compiler will grind to a halt. Miscompiles can easily take 100x or 1000x of the time to debug vs a bug that makes itself known during the actual execution of the compiler (rather than the execution of the program that was output by the compiler). That’s why most aspects of practical compiler development revolve around ensuring that if something goes wrong, that it halts the compiler before a faulty output program is produced.&lt;/p&gt;
    &lt;p&gt;A miscompile is a fundamental failure of the compiler’s contract with its user. Every miscompile should be accompanied by a deep look in the mirror and self-reflection about what went wrong to allow it to sneak through, and what preventative measures can (and should immediately) be taken to ensure that this particular failure mode never happens again.&lt;/p&gt;
    &lt;p&gt;Especially in the AI space, there are lots of compilers that play fast and loose with this, and as a result get burned. The best compiler engineers tend to be highly pedantic and somewhat paranoid about what can go wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why compilers are hard – the IR data structure&lt;/head&gt;
    &lt;p&gt;Compilers do have an essential complexity that makes them “hard”, and this again comes from the whole business of making sure that the input program and the output of the compiler have the same behavior. To understand this, we have to discuss how a compiler represents the meaning of the input program and how it preserves that meaning when producing the output program. This notion of “meaning” is sometimes called the program semantics.&lt;/p&gt;
    &lt;p&gt;The primary data structure in a compiler is usually some form of graph data structure that represents the compiler’s understanding of “what computation this program is supposed to do”. Hence, it represents the computation that the compiler needs to preserve all the way to the output program. This data structure is usually called an IR (intermediate representation). The primary way that compilers work is by taking an IR that represents the input program, and applying a series of small transformations all of which have been individually verified to not change the meaning of the program (i.e. not miscompile). In doing so, we decompose one large translation problem into many smaller ones, making it manageable.&lt;/p&gt;
    &lt;p&gt;I think it’s fair to say that compiler IR’s are the single most complex monolithic data structure in all of software engineering, in the sense that interpreting what can and cannot be validly done with the data structure is complex. To be clear, compiler IR’s are not usually very complex in the implementation sense like a “lock-free list” that uses subtle atomic operations to present a simple insert/delete/etc. interface.&lt;/p&gt;
    &lt;p&gt;Unlike a lock-free list, compiler IR’s usually have a very complex interface, even if they have a very simple internal implementation. Even specifying declaratively or in natural language what are the allowed transformations on the data structure is usually extremely difficult (you’ll see things like “memory models” or “abstract machines” that people spend years or decades trying to define properly).&lt;/p&gt;
    &lt;head rend="h3"&gt;A very complex schema&lt;/head&gt;
    &lt;p&gt;Firstly, the nodes in the graph usually have a complex schema. For example, a simple “integer multiply operation” (a node in the graph) is only allowed to have certain integer types as operands (incoming edges). And there may easily be thousands of kinds of operations at varying abstraction levels in any practical compiler, each with their own unique requirements. For example, a simple C &lt;code&gt;*&lt;/code&gt; (multiplication) operator will go through the following evolution in Clang:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It first becomes Clang’s &lt;code&gt;BinaryOperator&lt;/code&gt;node, which takes two “expressions” as operands (which may be mutable uint32_t values, for example).&lt;/item&gt;
      &lt;item&gt;It will then be converted to an LLVM IR &lt;code&gt;mul&lt;/code&gt;operation, which takes as operands an&lt;code&gt;llvm::Value&lt;/code&gt;, which represents an immutable value of the&lt;code&gt;i32&lt;/code&gt;type, say.&lt;/item&gt;
      &lt;item&gt;It will then be converted to a GlobalISel &lt;code&gt;G_MUL&lt;/code&gt;operation, whose operands represent not only an 32-bit integer, but also begin to capture notions like which “register bank” the value should eventually live in.&lt;/item&gt;
      &lt;item&gt;It will then be turned into a target-specific MIR node like &lt;code&gt;IMUL32rri&lt;/code&gt;or&lt;code&gt;IMUL32rr&lt;/code&gt;selecting among a variety of physical x86 instructions which can implement a multiplication. At this level, operands may represent physical, mutable hardware registers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From a compiler developer’s perspective, all these “multiply operations” are deeply different from each other because of the different information captured at each abstraction level (again, compiler developers are usually very pedantic). Failing to adequately differentiate between abstraction levels is a common disease among poorly written compilers.&lt;/p&gt;
    &lt;p&gt;At every level, precise attention to detail is needed – for example, if the multiplication is expected to overflow mod 2^32 in the source program, and we accidentally convert it to overflow mod 2^64 (such as by using a 64-bit register), then we have introduced a miscompile. Each operation has its own unique set of constraints and properties like these which apply when transforming the program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complex interactions between operations&lt;/head&gt;
    &lt;p&gt;Additionally, how these operations in the IR graph relate to each other can be very complex, especially when mutable variables and control flow are involved. For example, you may realize that an operation always executes, but we may be able to move it around to hide it under an &lt;code&gt;if&lt;/code&gt; condition to optimize the program. Consider the program:&lt;/p&gt;
    &lt;code&gt;x = y + z;
...
if (condition) {
    print(x); // The only time that `x` is referenced.
}
&lt;/code&gt;
    &lt;p&gt;Is it safe to convert this to&lt;/p&gt;
    &lt;code&gt;...
if (condition) {
    print(y + z);
}
&lt;/code&gt;
    &lt;p&gt;? Well, it depends on what’s hidden in that &lt;code&gt;...&lt;/code&gt;. For example, if the program is:&lt;/p&gt;
    &lt;code&gt;x = y + z;
...
y += 5;
...
if (condition) {
    print(x);
}
&lt;/code&gt;
    &lt;p&gt;Then it’s not legal, since by the time we get to the &lt;code&gt;if&lt;/code&gt;, the value of &lt;code&gt;y&lt;/code&gt; will have changed and we’ll print the wrong value. One of the primary considerations when designing compiler IR’s is how to make the transformations as simple and obviously correct as possible (more on that in another blog post).&lt;/p&gt;
    &lt;p&gt;Usually production compilers will deal with IR graphs from thousands to millions of nodes. Understandably then, the compounding effect of the IR complexity is front and center in all compiler design discussions. A single invalid transformation can result in a miscompile.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compilers are just software&lt;/head&gt;
    &lt;p&gt;Practical compilers are often live for years or decades and span millions of lines of code, so the entire suite of software engineering wisdom applies to them – good API design, testing, reusability, etc. though usually with additional compiler-specific twists.&lt;/p&gt;
    &lt;p&gt;For example, while API design is very important for most programs’ code (as it is for compilers’), compilers also have an additional dimension of “IR design”. As described above, the IR can be very complex to understand and transform, and designing it right can greatly mitigate this. (more on this in a future blog post)&lt;/p&gt;
    &lt;p&gt;Similarly, since compilers are usually decomposed into the successive application of multiple “passes” (self-contained IR transformations), there are a variety of testing and debugging strategies specific to compilers. (more on this in a future blog post).&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion and acknowledgements&lt;/head&gt;
    &lt;p&gt;I hope you have found this post helpful. I have a few more sketched out that should be coming soon. Please let me know on my LinkedIn if you have any feedback or topics you’d like to suggest. Big thanks to Bjarke Roune for his recent blog post that inspired me to finally get this series off the ground. Also to Dan Gohman for his blog post on canonicalization from years back. There’s too few such blog posts giving the big picture of practical compiler development. Please send me any other ones you know about on LinkedIn.&lt;/p&gt;
    &lt;p&gt;Stay tuned for future parts of this series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modern Compilers in the Age of AI&lt;/item&gt;
      &lt;item&gt;Organizing a Compiler&lt;/item&gt;
      &lt;item&gt;Testing, Code Review, and Robustness&lt;/item&gt;
      &lt;item&gt;The Compiler Lifecycle&lt;/item&gt;
      &lt;item&gt;…&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46261452</guid><pubDate>Sun, 14 Dec 2025 07:45:15 +0000</pubDate></item><item><title>Shai-Hulud compromised a dev machine and raided GitHub org access: a post-mortem</title><link>https://trigger.dev/blog/shai-hulud-postmortem</link><description>&lt;doc fingerprint="c417348bcef3474a"&gt;
  &lt;main&gt;
    &lt;p&gt;On November 25th, 2025, we were on a routine Slack huddle debugging a production issue when we noticed something strange: a PR in one of our internal repos was suddenly closed, showed zero changes, and had a single commit from... Linus Torvalds?&lt;/p&gt;
    &lt;p&gt;The commit message was just "init."&lt;/p&gt;
    &lt;p&gt;Within seconds, our #git Slack channel exploded with notifications. Dozens of force-pushes. PRs closing across multiple repositories. All attributed to one of our engineers.&lt;/p&gt;
    &lt;p&gt;We had been compromised by Shai-Hulud 2.0, a sophisticated npm supply chain worm that compromised over 500 packages, affected 25,000+ repositories, and spread across the JavaScript ecosystem. We weren't alone: PostHog, Zapier, AsyncAPI, Postman, and ENS were among those hit.&lt;/p&gt;
    &lt;p&gt;This is the complete story of what happened, how we responded, and what we've changed to prevent this from happening again.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;No Trigger.dev packages were ever compromised. The&lt;/p&gt;&lt;code&gt;@trigger.dev/*&lt;/code&gt;packages and&lt;code&gt;trigger.dev&lt;/code&gt;CLI were never infected with Shai-Hulud malware. This incident involved one of our engineers installing a compromised package on their development machine, which led to credential theft and unauthorized access to our GitHub organization. Our published packages remained safe throughout.&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Attack Timeline&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, 04:11&lt;/cell&gt;
        &lt;cell&gt;Malicious packages go live&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, ~20:27&lt;/cell&gt;
        &lt;cell&gt;Engineer compromised&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, 22:36&lt;/cell&gt;
        &lt;cell&gt;First attacker activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 02:56-05:32&lt;/cell&gt;
        &lt;cell&gt;Overnight reconnaissance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 09:08-15:08&lt;/cell&gt;
        &lt;cell&gt;Legitimate engineer work (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 09:10-09:17&lt;/cell&gt;
        &lt;cell&gt;Attacker monitors engineer activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 15:17-15:27&lt;/cell&gt;
        &lt;cell&gt;Final recon&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 15:27-15:37&lt;/cell&gt;
        &lt;cell&gt;Destructive attack&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, ~15:32&lt;/cell&gt;
        &lt;cell&gt;Detection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, ~15:36&lt;/cell&gt;
        &lt;cell&gt;Access revoked&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 16:35&lt;/cell&gt;
        &lt;cell&gt;AWS session blocked&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 22:35&lt;/cell&gt;
        &lt;cell&gt;All branches restored&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nov 26, 20:16&lt;/cell&gt;
        &lt;cell&gt;GitHub App key rotated&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;The compromise&lt;/head&gt;
    &lt;p&gt;On the evening of November 24th, around 20:27 UTC (9:27 PM local time in Germany), one of our engineers was experimenting with a new project. They ran a command that triggered &lt;code&gt;pnpm install&lt;/code&gt;. At that moment, somewhere in the dependency tree, a malicious package executed.&lt;/p&gt;
    &lt;p&gt;We don't know exactly which package delivered the payload. The engineer was experimenting at the time and may have deleted the project directory as part of cleanup. By the time we investigated, we couldn't trace back to the specific package. The engineer checked their shell history and they'd only run install commands in our main trigger repo, cloud repo, and one experimental project.&lt;/p&gt;
    &lt;p&gt;This is one of the frustrating realities of these attacks: once the malware runs, identifying the source becomes extremely difficult. The package doesn't announce itself. The &lt;code&gt;pnpm install&lt;/code&gt; completes successfully. Everything looks normal.&lt;/p&gt;
    &lt;p&gt;What we do know is that the Shai-Hulud malware ran a &lt;code&gt;preinstall&lt;/code&gt; script that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Downloaded and executed TruffleHog, a legitimate security tool repurposed for credential theft&lt;/item&gt;
      &lt;item&gt;Scanned the engineer's machine for secrets: GitHub tokens, AWS credentials, npm tokens, environment variables&lt;/item&gt;
      &lt;item&gt;Exfiltrated everything it found&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the engineer later recovered files from their compromised laptop (booted in recovery mode), they found the telltale signs:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;.trufflehog-cache&lt;/code&gt; directory and &lt;code&gt;trufflehog_3.91.1_darwin_amd64.tar.gz&lt;/code&gt; file found on the compromised machine. The &lt;code&gt;extract&lt;/code&gt; directory was empty, likely cleaned up by the malware to cover its tracks.&lt;/p&gt;
    &lt;head rend="h2"&gt;17 hours of reconnaissance&lt;/head&gt;
    &lt;p&gt;The attacker had access to our engineer's GitHub account for 17 hours before doing anything visible. According to our GitHub audit logs, they operated methodically.&lt;/p&gt;
    &lt;p&gt;Just over two hours after the initial compromise, the attacker validated their stolen credentials and began mass cloning:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
        &lt;cell role="head"&gt;Activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:36:50&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;First attacker access, mass cloning begins&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:36-22:39&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;73 repositories cloned&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:48-22:50&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~70 more repositories cloned (second wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:55-22:56&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~90 repositories cloned (third wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:59-23:04&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~70 repositories cloned (fourth wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;23:32:59&lt;/cell&gt;
        &lt;cell&gt;India&lt;/cell&gt;
        &lt;cell&gt;Attacker switches to India-based infrastructure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;23:32-23:37&lt;/cell&gt;
        &lt;cell&gt;India&lt;/cell&gt;
        &lt;cell&gt;73 repositories cloned&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;23:34-23:35&lt;/cell&gt;
        &lt;cell&gt;US + India&lt;/cell&gt;
        &lt;cell&gt;Simultaneous cloning from both locations&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The simultaneous activity from US and India confirmed we were dealing with a single attacker using multiple VPNs or servers, not separate actors.&lt;/p&gt;
    &lt;p&gt;While our engineer slept in Germany, the attacker continued their reconnaissance. More cloning at 02:56-02:59 UTC (middle of the night in Germany), sporadic activity until 05:32 UTC. Total repos cloned: 669 (527 from US infrastructure, 142 from India).&lt;/p&gt;
    &lt;p&gt;Here's where it gets unsettling. Our engineer woke up and started their normal workday:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Actor&lt;/cell&gt;
        &lt;cell role="head"&gt;Activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;09:08:27&lt;/cell&gt;
        &lt;cell&gt;Engineer&lt;/cell&gt;
        &lt;cell&gt;Triggers workflow on cloud repo (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;09:10-09:17&lt;/cell&gt;
        &lt;cell&gt;Attacker&lt;/cell&gt;
        &lt;cell&gt;Git fetches from US, watching the engineer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;09:08-15:08&lt;/cell&gt;
        &lt;cell&gt;Engineer&lt;/cell&gt;
        &lt;cell&gt;Normal PR reviews, CI workflows (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The attacker was monitoring our engineer's activity while they worked, unaware they were compromised.&lt;/p&gt;
    &lt;p&gt;During this period, the attacker created repositories with random string names to store stolen credentials, a known Shai-Hulud pattern:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/xfjqb74uysxcni5ztn&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/ls4uzkvwnt0qckjq27&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/uxa7vo9og0rzts362c&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They also created three repos marked with "Sha1-Hulud: The Second Coming" as a calling card. These repositories were empty by the time we examined them, but based on the documented Shai-Hulud behavior, they likely contained triple base64-encoded credentials.&lt;/p&gt;
    &lt;head rend="h2"&gt;10 minutes of destruction&lt;/head&gt;
    &lt;p&gt;At 15:27 UTC on November 25th, the attacker switched from reconnaissance to destruction.&lt;/p&gt;
    &lt;p&gt;The attack began on our &lt;code&gt;cloud&lt;/code&gt; repo from India-based infrastructure:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Repo&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:35&lt;/cell&gt;
        &lt;cell&gt;First force-push&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;Attack begins&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:37&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;PR #300 closed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:44&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:27:50&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev&lt;/cell&gt;
        &lt;cell&gt;PR #2707 closed&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The attack continued on our main repository:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:28:13&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2706 (release PR)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:30:51&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2451&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:31:10&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2382&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:31:16&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push to trigger.dev&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:31:31&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2482&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;At 15:32:43-46 UTC, 12 PRs on jsonhero-web were closed in 3 seconds. Clearly automated. PRs #47, #169, #176, #181, #189, #190, #194, #197, #204, #206, #208 all closed within a 3-second window.&lt;/p&gt;
    &lt;p&gt;Our critical infrastructure repository was targeted next:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:41&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/infra PR #233&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:45&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push (India)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:48&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/infra PR #309&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:35:49&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push (India)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The final PR was closed on json-infer-types at 15:37:13 UTC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Detection and response&lt;/head&gt;
    &lt;p&gt;We got a lucky break. One of our team members was monitoring Slack when the flood of notifications started:&lt;/p&gt;
    &lt;p&gt;Our #git Slack channel during the attack. A wall of force-pushes, all with commit message "init."&lt;/p&gt;
    &lt;p&gt;Every malicious commit was authored as:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;Author: Linus Torvalds &amp;lt;[email protected]&amp;gt;Message: init&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;An attacked branch: a single "init" commit attributed to Linus Torvalds, thousands of commits behind main.&lt;/p&gt;
    &lt;p&gt;We haven't found reports of other Shai-Hulud victims seeing this same "Linus Torvalds" vandalism pattern. The worm's documented behavior focuses on credential exfiltration and npm package propagation, not repository destruction. This destructive phase may have been unique to our attacker, or perhaps a manual follow-up action after the automated worm had done its credential harvesting.&lt;/p&gt;
    &lt;p&gt;Within 4 minutes of detection we identified the compromised account, removed them from the GitHub organization, and the attack stopped immediately.&lt;/p&gt;
    &lt;p&gt;Our internal Slack during those first minutes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Urmmm guys? what's going on?"&lt;/p&gt;
      &lt;p&gt;"add me to the call @here"&lt;/p&gt;
      &lt;p&gt;"Nick could you double check Infisical for any machine identities"&lt;/p&gt;
      &lt;p&gt;"can someone also check whether there are any reports of compromised packages in our CLI deps?"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Within the hour:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:36&lt;/cell&gt;
        &lt;cell&gt;Removed from GitHub organization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:40&lt;/cell&gt;
        &lt;cell&gt;Removed from Infisical (secrets manager)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:45&lt;/cell&gt;
        &lt;cell&gt;Removed from AWS IAM Identity Center&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~16:00&lt;/cell&gt;
        &lt;cell&gt;Removed from Vercel and Cloudflare&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16:35&lt;/cell&gt;
        &lt;cell&gt;AWS SSO sessions blocked via deny policy (sessions can't be revoked)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;16:45&lt;/cell&gt;
        &lt;cell&gt;IAM user console login deleted&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;The damage&lt;/head&gt;
    &lt;p&gt;Repository clone actions: 669 (public and private), including infrastructure code, internal documentation, and engineering plans.&lt;/p&gt;
    &lt;p&gt;Branches force-pushed: 199 across 16 repositories&lt;/p&gt;
    &lt;p&gt;Pull requests closed: 42&lt;/p&gt;
    &lt;p&gt;Protected branch rejections: 4. Some of our repositories have main branch protection enabled, but we had not enabled it for all repositories at the time of the incident.&lt;/p&gt;
    &lt;p&gt;npm packages were not compromised. This is the difference between "our repos got vandalized" and "our packages got compromised."&lt;/p&gt;
    &lt;p&gt;Our engineer didn't have an npm publishing token on their machine, and even if they did we had already required 2FA for publishing to npm. Without that, Shai-Hulud would have published malicious versions of &lt;code&gt;@trigger.dev/sdk&lt;/code&gt;, &lt;code&gt;@trigger.dev/core&lt;/code&gt;, and others, potentially affecting thousands of downstream users.&lt;/p&gt;
    &lt;p&gt;Production databases or any AWS resources were not accessed. Our AWS CloudTrail audit showed only read operations from the compromised account:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Event Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Count&lt;/cell&gt;
        &lt;cell role="head"&gt;Service&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ListManagedNotificationEvents&lt;/cell&gt;
        &lt;cell&gt;~40&lt;/cell&gt;
        &lt;cell&gt;notifications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DescribeClusters&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;ECS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DescribeTasks&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;ECS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DescribeMetricFilters&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;CloudWatch&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These were confirmed to be legitimate operations by our engineer.&lt;/p&gt;
    &lt;p&gt;One nice surprise: AWS actually sent us a proactive alert about Shai-Hulud. They detected the malware's characteristic behavior (ListSecrets, GetSecretValue, BatchGetSecretValue API calls) on an old test account that hadn't been used in months, so we just deleted it. But kudos to AWS for the proactive detection and notification.&lt;/p&gt;
    &lt;head rend="h2"&gt;The recovery&lt;/head&gt;
    &lt;p&gt;GitHub doesn't have server-side reflog. When someone force-pushes, that history is gone from GitHub's servers.&lt;/p&gt;
    &lt;p&gt;But we found ways to recover.&lt;/p&gt;
    &lt;p&gt;Push events are retained for 90 days via the GitHub Events API. We wrote a script that fetched pre-attack commit SHAs:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;# Find pre-attack commit SHA from eventsgh api repos/$REPO/events --paginate | \  jq -r '.[] | select(.type=="PushEvent") |  select(.payload.ref=="refs/heads/'$BRANCH'") |  .payload.before' | head -1&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Public repository forks still contained original commits. We used these to verify and restore branches.&lt;/p&gt;
    &lt;p&gt;Developers who hadn't run &lt;code&gt;git fetch --prune&lt;/code&gt; (all of us?) still had old SHAs in their local reflog.&lt;/p&gt;
    &lt;p&gt;Within 7 hours, all 199 branches were restored.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub app private key exposure&lt;/head&gt;
    &lt;p&gt;During the investigation, our engineer was going through files recovered from the compromised laptop and discovered something concerning: the private key for our GitHub App was in the trash folder.&lt;/p&gt;
    &lt;p&gt;When you create a private key in the GitHub App settings, GitHub automatically downloads it. The engineer had created a key at some point, and while the active file had been deleted, it was still in the trash, potentially accessible to TruffleHog.&lt;/p&gt;
    &lt;p&gt;Our GitHub App has the following permissions on customer repositories:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Permission&lt;/cell&gt;
        &lt;cell role="head"&gt;Access Level&lt;/cell&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;contents&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could read/write repository contents&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;pull_requests&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could read/create/modify PRs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;deployments&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could create/trigger deployments&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;checks&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could create/modify check runs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;commit_statuses&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could mark commits as passing/failing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;metadata&lt;/cell&gt;
        &lt;cell&gt;read&lt;/cell&gt;
        &lt;cell&gt;Could read repository metadata&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To generate valid access tokens, an attacker would need both the private key (potentially compromised) and the installation ID for a specific customer (stored in our database which was not compromised, not on the compromised machine).&lt;/p&gt;
    &lt;p&gt;We immediately rotated the key:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 26, 18:51&lt;/cell&gt;
        &lt;cell&gt;Private key discovered in trash folder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 26, 19:54&lt;/cell&gt;
        &lt;cell&gt;New key deployed to test environment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nov 26, 20:16&lt;/cell&gt;
        &lt;cell&gt;New key deployed to production&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We found no evidence of unauthorized access to any customer repositories. The attacker would have needed installation IDs from our database to generate tokens, and our database was not compromised as previously mentioned.&lt;/p&gt;
    &lt;p&gt;However, we cannot completely rule out the possibility. An attacker with the private key could theoretically have called the GitHub API to enumerate all installations. We've contacted GitHub Support to request additional access logs. We've also analyzed the webhook payloads to our GitHub app, looking for suspicious push or PR activity from connected installations &amp;amp; repositories. We haven't found any evidence of unauthorized activity in these webhook payloads.&lt;/p&gt;
    &lt;p&gt;We've sent out an email to potentially effected customers to notify them of the incident with detailed instructions on how to check if they were affected. Please check your email for more details if you've used our GitHub app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical deep-dive: how Shai-Hulud works&lt;/head&gt;
    &lt;p&gt;For those interested in the technical details, here's what we learned about the malware from Socket's analysis and our own investigation.&lt;/p&gt;
    &lt;p&gt;When npm runs the &lt;code&gt;preinstall&lt;/code&gt; script, it executes &lt;code&gt;setup_bun.js&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detects OS/architecture&lt;/item&gt;
      &lt;item&gt;Downloads or locates the Bun runtime&lt;/item&gt;
      &lt;item&gt;Caches Bun in &lt;code&gt;~/.cache&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Spawns a detached Bun process running &lt;code&gt;bun_environment.js&lt;/code&gt;with output suppressed&lt;/item&gt;
      &lt;item&gt;Returns immediately so &lt;code&gt;npm install&lt;/code&gt;completes successfully with no warnings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The malware runs in the background while you think everything is fine.&lt;/p&gt;
    &lt;p&gt;The payload uses TruffleHog to scan &lt;code&gt;$HOME&lt;/code&gt; for GitHub tokens (from env vars, gh CLI config, git credential helpers), AWS/GCP/Azure credentials, npm tokens from &lt;code&gt;.npmrc&lt;/code&gt;, environment variables containing anything that looks like a secret, and GitHub Actions secrets (if running in CI).&lt;/p&gt;
    &lt;p&gt;Stolen credentials are uploaded to a newly-created GitHub repo with a random name. The data is triple base64-encoded to evade GitHub's secret scanning.&lt;/p&gt;
    &lt;p&gt;Files created:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;contents.json&lt;/code&gt;(system info and GitHub credentials)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;environment.json&lt;/code&gt;(all environment variables)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cloud.json&lt;/code&gt;(cloud provider credentials)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;truffleSecrets.json&lt;/code&gt;(filesystem secrets from TruffleHog)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;actionsSecrets.json&lt;/code&gt;(GitHub Actions secrets if any)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If an npm publishing token is found, the malware validates the token against the npm registry, fetches packages maintained by that account, downloads each package, patches it with the malware, bumps the version, and re-publishes, infecting more packages.&lt;/p&gt;
    &lt;p&gt;This is how the worm spread through the npm ecosystem, starting from PostHog's compromised CI on November 24th at 4:11 AM UTC. Our engineer was infected roughly 16 hours after the malicious packages went live.&lt;/p&gt;
    &lt;p&gt;If no credentials are found to exfiltrate or propagate, the malware attempts to delete the victim's entire home directory. Scorched earth.&lt;/p&gt;
    &lt;p&gt;File artifacts to look for: &lt;code&gt;setup_bun.js&lt;/code&gt;, &lt;code&gt;bun_environment.js&lt;/code&gt;, &lt;code&gt;cloud.json&lt;/code&gt;, &lt;code&gt;contents.json&lt;/code&gt;, &lt;code&gt;environment.json&lt;/code&gt;, &lt;code&gt;truffleSecrets.json&lt;/code&gt;, &lt;code&gt;actionsSecrets.json&lt;/code&gt;, &lt;code&gt;.trufflehog-cache/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;Malware file hashes (SHA1):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;:&lt;code&gt;d60ec97eea19fffb4809bc35b91033b52490ca11&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;:&lt;code&gt;3d7570d14d34b0ba137d502f042b27b0f37a59fa&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setup_bun.js&lt;/code&gt;:&lt;code&gt;d1829b4708126dcc7bea7437c04d1f10eacd4a16&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We've published a detection script that checks for Shai-Hulud indicators.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we've changed&lt;/head&gt;
    &lt;p&gt;We disabled npm scripts globally:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;npm config set ignore-scripts true --location=global&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;This prevents &lt;code&gt;preinstall&lt;/code&gt;, &lt;code&gt;postinstall&lt;/code&gt;, and other lifecycle scripts from running. It's aggressive and some packages will break, but it's the only reliable protection against this class of attack.&lt;/p&gt;
    &lt;p&gt;We upgraded to pnpm 10. This was significant effort (had to migrate through pnpm 9 first), but pnpm 10 brings critical security improvements. Scripts are ignored by default. You can explicitly whitelist packages that need to run scripts via &lt;code&gt;pnpm.onlyBuiltDependencies&lt;/code&gt;. And the &lt;code&gt;minimumReleaseAge&lt;/code&gt; setting prevents installing packages published recently.&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;# pnpm-workspace.yamlminimumReleaseAge: 4320 # 3 days in minutespreferOffline: true&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;To whitelist packages that legitimately need build scripts:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm approve-builds&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;This prompts you to select which packages to allow (like &lt;code&gt;esbuild&lt;/code&gt;, &lt;code&gt;prisma&lt;/code&gt;, &lt;code&gt;sharp&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;For your global pnpm config:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm config set minimumReleaseAge 4320pnpm config set --json minimumReleaseAgeExclude '["@trigger.dev/*", "trigger.dev"]'&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;We switched npm publishing to OIDC. No more long-lived npm tokens anywhere. Publishing now uses npm's trusted publishers with GitHub Actions OIDC. Even if an attacker compromises a developer machine, they can't publish packages because there are no credentials to steal. Publishing only happens through CI with short-lived, scoped tokens.&lt;/p&gt;
    &lt;p&gt;We enabled branch protection on all repositories. Not just critical repos or just OSS repos. Every repository with meaningful code now has branch protection enabled.&lt;/p&gt;
    &lt;p&gt;We've adopted Granted for AWS SSO. Granted encrypts SSO session tokens on the client side, unlike the AWS CLI which stores them in plaintext.&lt;/p&gt;
    &lt;p&gt;Based on PostHog's analysis of how they were initially compromised (via &lt;code&gt;pull_request_target&lt;/code&gt;), we've reviewed our GitHub Actions workflows. We now require approval for external contributor workflow runs on all our repositories (previous policy was only for public repositories).&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons for other teams&lt;/head&gt;
    &lt;p&gt;The ability for packages to run arbitrary code during installation is the attack surface. Until npm fundamentally changes, add this to your &lt;code&gt;~/.npmrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;ignore-scripts=true&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Yes, some things will break. Whitelist them explicitly. The inconvenience is worth it.&lt;/p&gt;
    &lt;p&gt;pnpm 10 ignores scripts by default and lets you set a minimum age for packages:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm config set minimumReleaseAge 4320  # 3 days&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Newly published packages can't be installed for 3 days, giving time for malicious packages to be detected.&lt;/p&gt;
    &lt;p&gt;Branch protection takes 30 seconds to enable. It prevents attackers from pushing to a main branch, potentially executing malicious GitHub action workflows.&lt;/p&gt;
    &lt;p&gt;Long-lived npm tokens on developer machines are a liability. Use trusted publishers with OIDC instead.&lt;/p&gt;
    &lt;p&gt;If you don't need a credential on your local machine, don't have it there. Publishing should happen through CI only.&lt;/p&gt;
    &lt;p&gt;Our #git Slack channel is noisy. That noise saved us.&lt;/p&gt;
    &lt;head rend="h2"&gt;A note on the human side&lt;/head&gt;
    &lt;p&gt;One of the hardest parts of this incident was that it happened to a person.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Sorry for all the trouble guys, terrible experience"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Our compromised engineer felt terrible, even though they did absolutely nothing wrong. It could have happened to any team member.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;npm install&lt;/code&gt; is not negligence. Installing dependencies is not a security failure. The security failure is in an ecosystem that allows packages to run arbitrary code silently.&lt;/p&gt;
    &lt;p&gt;They also discovered that the attacker had made their GitHub account star hundreds of random repositories during the compromise. Someone even emailed us: "hey you starred my repo but I think it was because you were hacked, maybe remove the star?"&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from compromise to first attacker activity&lt;/cell&gt;
        &lt;cell&gt;~2 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time attacker had access before destructive action&lt;/cell&gt;
        &lt;cell&gt;~17 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Duration of destructive attack&lt;/cell&gt;
        &lt;cell&gt;~10 minutes (15:27-15:37 UTC)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from first malicious push to detection&lt;/cell&gt;
        &lt;cell&gt;~5 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from detection to access revocation&lt;/cell&gt;
        &lt;cell&gt;~4 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time to full branch recovery&lt;/cell&gt;
        &lt;cell&gt;~7 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Repository clone actions by attacker&lt;/cell&gt;
        &lt;cell&gt;669&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Repositories force-pushed&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Branches affected&lt;/cell&gt;
        &lt;cell&gt;199&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pull requests closed&lt;/cell&gt;
        &lt;cell&gt;42&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Protected branch rejections&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Resources&lt;/head&gt;
    &lt;p&gt;About the Attack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Socket.dev: Shai-Hulud Strikes Again V2 - Technical deep-dive into the malware&lt;/item&gt;
      &lt;item&gt;PostHog Post-Mortem - Another company's experience with Shai-Hulud&lt;/item&gt;
      &lt;item&gt;Wiz Blog: Shai-Hulud 2.0 Supply Chain Attack&lt;/item&gt;
      &lt;item&gt;The Hacker News Coverage&lt;/item&gt;
      &lt;item&gt;Endor Labs Analysis&lt;/item&gt;
      &lt;item&gt;HelixGuard Advisory (referenced in AWS alert)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mitigation Resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;npm Trusted Publishers - OIDC-based publishing&lt;/item&gt;
      &lt;item&gt;pnpm onlyBuiltDependencies - Whitelist packages allowed to run scripts&lt;/item&gt;
      &lt;item&gt;pnpm minimumReleaseAge - Delay installation of new packages&lt;/item&gt;
      &lt;item&gt;Granted - AWS SSO credential management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have questions about this incident? Reach out on Twitter/X or Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262021</guid><pubDate>Sun, 14 Dec 2025 10:07:00 +0000</pubDate></item><item><title>Efficient Basic Coding for the ZX Spectrum</title><link>https://blog.jafma.net/2020/02/24/efficient-basic-coding-for-the-zx-spectrum/</link><description>&lt;doc fingerprint="bb19c69cc4bc9fbe"&gt;
  &lt;main&gt;
    &lt;p&gt;[Click here to read this in English ]&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Éste es el primero de una serie de artículos que explican los fundamentos de la (in)eficiencia de los programas en BASIC puro para el ZX Spectrum:&lt;/p&gt;
      &lt;p&gt;I. Sobre los números de línea&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;El intérprete de lenguaje Sinclair BASIC incluido en la ROM del ZX Spectrum es, en muchos aspectos, una maravilla del software, concretamente de la programación en ensamblador, y daría para hablar durante mucho tiempo. En esta serie queremos destacar los puntos más importantes a tener en cuenta para que los programas escritos en ese lenguaje sean lo más eficientes posibles, en primer lugar en tiempo de ejecución, pero también en espacio ocupado en memoria.&lt;/p&gt;
    &lt;p&gt;En esta primera entrega de la serie trataremos de las líneas de dichos programas; más allá de la necesidad de numerarlas, algo que no se hace desde hace décadas en ningún lenguaje de programación, está el propio hecho de la eficiencia del intérprete a la hora de manejarlas.&lt;/p&gt;
    &lt;p&gt;Antes de meternos en el meollo, conviene resumir los límites que existen en esta máquina relativos a las líneas de programa:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Las líneas de programa, una vez éste queda almacenado en la memoria listo para su ejecución, ocupan 2 bytes (por cierto, almacenados en formato big-endian, el único caso de este formato en el ZX). Esto podría llevar a pensar que tenemos disponibles desde la línea 0 a la 65535 (el máximo número que puede almacenarse en 2 bytes), pero no es exactamente así. A la hora de editar manualmente un programa sólo se nos permite numerar las líneas desde 1 a 9999. Si el programa es manipulado fuera del editor (se puede hacer con &lt;code&gt;POKE&lt;/code&gt;), es posible tener la línea 0, y ésta aparecer al listarlo, pero no será editable. De la misma manera (manipulando el programa con&lt;code&gt;POKE&lt;/code&gt;) se pueden numerar líneas por encima de la 9999; sin embargo, esto causará problemas en ejecución: muchas sentencias del lenguaje que admiten un número de línea como parámetro, como&lt;code&gt;GO TO&lt;/code&gt;o&lt;code&gt;RESTORE&lt;/code&gt;, dan error si la línea es mayor de 32767; la pila de llamadas dejará de funcionar correctamente si se hace un&lt;code&gt;GO SUB&lt;/code&gt;a una línea mayor de 15871 (3DFF en hexadecimal); el intérprete reserva el número de línea 65534 para indicar que está ejecutando código escrito en el buffer de edición (y no en el listado del programa); por último, listar programas por pantalla tampoco funciona bien con líneas mayores de 9999, y en cuanto las editemos manualmente volverán a quedar con sólo 4 dígitos decimales.&lt;/item&gt;
      &lt;item&gt;La longitud en bytes de cada línea de programa se almacena justo después del número de línea, ocupando 2 bytes (esta vez en little-endian). Esta longitud no incluye ni el número de línea ni la longitud en sí misma. Por tanto, podríamos esperar poder tener líneas de un máximo de 65535 bytes en su contenido principal (menos 1, porque siempre tiene que haber un 0x0D al final para indicar el fin de línea); asimismo, las líneas más cortas ocuparán en memoria 2+2+1+1 = 6 bytes: serían aquéllas que contienen una sola sentencia que no tiene parámetros, p.ej., &lt;code&gt;10 CLEAR&lt;/code&gt;. Una rutina muy importante en la ROM del Spectrum, la encargada de buscar la siguiente línea o la siguiente variable saltándose la actual (llamada&lt;code&gt;NEXT-ONE&lt;/code&gt;y situada en la dirección 0x19B8) funciona perfectamente con rangos de tamaño de línea entre 0 y 65535, pero en ejecución el intérprete dejará de interpretar una línea en cuanto se encuentre un 0x0D al comienzo de una sentencia (si la línea es más larga, por ejemplo porque se haya extendido mediante manipulaciones externas, ignorará el resto, por lo que puede ser usado ese espacio para almacenar datos dentro del programa). Más importante aún: dará error al tratar de ejecutar más de 127 sentencias en una misma línea, es decir, una línea en ejecución sólo puede tener, en la práctica, desde 1 hasta 127 sentencias.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Una vez resumidos los datos básicos sobre las líneas y los números de línea, nos centraremos en una característica muy concreta del intérprete de BASIC que resulta fundamental para conseguir incrementar su eficiencia en la ejecución de programas:&lt;/p&gt;
    &lt;p&gt;El intérprete no usa una tabla indexada de líneas de programa&lt;/p&gt;
    &lt;p&gt;Los programas BASIC del ZX se pre-procesan nada más teclearlos (tras teclear líneas completas en el caso del ZX Spectrum +2 y superiores), lo que ahorra espacio en ROM al evitar el analizador léxico que haría falta posteriormente. En ese pre-proceso no sólo se resumen palabras clave de varias letras en un sólo byte, es decir, se tokeniza (qué palabro más feo), sino que se aprovecha para insertar en los lugares más convenientes para la ejecución algunos elementos pre-calculados: un ejemplo es el propio tamaño en memoria de cada línea, como se ha explicado antes, pero también se almacenan silenciosamente los valores numéricos de los literales escritos en el texto (justo tras dichos literales), y se reservan huecos para recoger los argumentos de las funciones de usuario (justo tras los nombres de los correspondientes parámetros en la sentencia &lt;code&gt;DEF FN&lt;/code&gt;), por ejemplo. &lt;/p&gt;
    &lt;p&gt;Lo que nunca, nunca se hace es reservar memoria para almacenar una tabla con las direcciones en memoria de cada línea de programa. Es decir, una tabla que permita saber, a partir de un número de línea y con complejidad computacional constante (tardando siempre lo mismo independientemente del número de línea, lo que formalmente se escribe O(1)), el lugar de memoria donde comienza el contenido tokenizado de dicha línea, para poder acceder rápidamente a las sentencias correspondientes y ejecutarlas.&lt;/p&gt;
    &lt;p&gt;Esto tiene una consecuencia importante para el intérprete: cualquier sentencia del lenguaje que admita como parámetro una línea (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, etc.) implica, durante su ejecución, buscar activamente el comienzo de dicha línea a lo largo de toda la memoria donde reside el programa. Desde el punto de vista de la complejidad computacional, esto no es constante, sino lineal (o sea, peor): O(n), siendo n el número de líneas de programa; en otras palabras: tarda más cuanto más lejos esté la línea que se busca del comienzo del programa. El intérprete implementa esa búsqueda con un puntero (o sea, una dirección de memoria) que empieza apuntando a donde reside la primera línea en memoria; mientras no sea ésta la línea que se busca, o la inmediatamente posterior a la que se busca si se busca una que no existe, suma al puntero el tamaño que ocupa el contenido de la línea en memoria, obteniendo un nuevo puntero que apunta al lugar de memoria donde reside la siguiente línea, y repite el proceso.&lt;/p&gt;
    &lt;p&gt;Un importante resultado de esta implementación del intérprete es que toda sentencia que implique un salto a una línea de programa (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt;, &lt;code&gt;FN&lt;/code&gt;) incrementará su tiempo de cómputo linealmente con el número de líneas que haya antes de la de destino. Esto se puede comprobar con un programa que mide el tiempo para distintas líneas de destino, como el que puede descargarse aquí. Tras ejecutarlo (¡cuidado!: tarda más de 17 horas en terminar debido al nivel de precisión con el que queremos estimar los tiempos) obtenemos los siguientes resultados:&lt;/p&gt;
    &lt;p&gt;Como se observa, los saltos incrementan su tiempo en 71 microsegundos por cada línea más que haya antes de la de destino; eso supone unos 7 milisegundos cuando hay 100 líneas antes, lo que puede ser mucho si el salto se repite a menudo (por ejemplo, si lo hace un bucle &lt;code&gt;FOR&lt;/code&gt;–&lt;code&gt;NEXT&lt;/code&gt;). El programa anterior toma 10000 medidas de tiempo para calcular la media mostrada finalmente en la gráfica, por lo que el Teorema del Límite Central  indica que los resultados expuestos arriba tienen una incertidumbre  pequeña, del orden de 115.5 microsegundos si consideramos como fuente de  incertidumbre original más importante los 20 milisegundos producidos como máximo por la discretización del tiempo de la variable del sistema &lt;code&gt;FRAMES&lt;/code&gt; (el hecho de tomar tantos datos hace, por el mismo teorema, que la distribución de la estimación sea simétrica y no tenga bias, por lo que la media mostrada en la figura será prácticamente la verdadera, a pesar de dicha incertidumbre). También se observan en la gráfica los 5.6 milisegundos de media que se tarda en ejecutar todo lo que no es el salto en el programa de prueba.&lt;/p&gt;
    &lt;p&gt;Por tanto, aquí va la primera regla de eficiencia para mejorar el tiempo de cómputo:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Si quieres que cierta parte de tu programa BASIC se ejecute más rápido, y esa parte contiene el destino de bucles (&lt;/p&gt;&lt;code&gt;GO TO&lt;/code&gt;,&lt;code&gt;NEXT&lt;/code&gt;) o es llamada muy frecuentemente por otras (&lt;code&gt;GO SUB&lt;/code&gt;o&lt;code&gt;DEF FN&lt;/code&gt;), deberías moverla al principio del programa, o lo más cerca del principio que puedas; de esa manera, el intérprete tardará sensiblemente menos en encontrar las líneas a las que hay que saltar.&lt;/quote&gt;
    &lt;p&gt;Para ayudar en la tarea de identificar estos problemas, el intérprete de BASIC incluido en la herramienta ZX-Basicus puede producir un perfil de la frecuencia de ejecución de cada sentencia de un programa (opción &lt;code&gt;--profile&lt;/code&gt;); si la lista ordenada de frecuencias que recopila no va en orden creciente de número de línea, significa que algunas líneas de las más frecuentemente llamadas podrían estar mal situadas.&lt;/p&gt;
    &lt;p&gt;Existe un truco en BASIC para hacer que el intérprete no tenga que buscar desde el principio del programa para encontrar una línea, sino que empiece la búsqueda en otro lugar (más cercano a lo que busque). Consiste en cambiar el contenido de la variable del sistema &lt;code&gt;PROG&lt;/code&gt;, que está en la dirección 23635 y ocupa 2 bytes, por la dirección de memoria donde resida la primera línea que queramos que el intérprete use para sus búsquedas (eso hará que el intérprete ignore la existencia de todas las anteriores, así que ¡éstas dejarán de ser accesibles!). En general no hay modo fácil de saber en qué dirección de memoria reside una línea, pero la variable del sistema &lt;code&gt;NXTLIN&lt;/code&gt; (dirección 23637, 2 bytes) guarda en todo momento la dirección de la línea siguiente a la que estamos (la herramienta de análisis de ZX-Basicus también puede ser útil, pues produce un listado con la localización de cada elemento del programa BASIC en memoria si éste se ha guardado en un fichero &lt;code&gt;.tap&lt;/code&gt;). Por tanto, para, por ejemplo, hacer que un bucle vaya más rápido, se puede hacer &lt;code&gt;POKE&lt;/code&gt; a los dos bytes de &lt;code&gt;PROG&lt;/code&gt; con el valor que tengan los de &lt;code&gt;NXTLIN&lt;/code&gt; cuando estemos en la línea anterior a la del bucle; desde ese momento, la primera línea del bucle irá tan rápida como si fuera la primera de todo el programa. Eso sí, ¡es importante recuperar el valor original de &lt;code&gt;PROG&lt;/code&gt; si queremos volver a ejecutar alguna vez el resto!&lt;/p&gt;
    &lt;p&gt;El problema de la búsqueda secuencial de líneas que hace la ROM del ZX tiene un efecto particular en el caso de las funciones de usuario (&lt;code&gt;DEF FN&lt;/code&gt;): dado que están pensadas para ser llamadas desde diversos puntos del programa, deberían ir al principio del mismo si esas llamadas van a ser frecuentes, pues cada vez que sean llamadas el intérprete tiene que buscarlas. (Una alternativa, preferida por muchos programadores, es no utilizar &lt;code&gt;DEF FN&lt;/code&gt;, dado el mayor coste de su ejecución respecto a insertar la expresión directamente donde se necesite.) El perfil de frecuencias de uso producido por el intérprete de ZX-Basicus también informa sobre el número de veces que se ha llamado a cada función de usuario con &lt;code&gt;FN&lt;/code&gt;, y la utilidad de transformación tiene una opción (&lt;code&gt;--delunusedfn&lt;/code&gt;) que borra automáticamente todas las sentencias &lt;code&gt;DEF FN&lt;/code&gt; no utilizadas en el código.&lt;/p&gt;
    &lt;p&gt;Es importante hacer notar aquí que el intérprete de BASIC no sólo tiene un comportamiento lineal (O(n)) a la hora de buscar líneas de programa, sino también al buscar sentencias. Es decir: si el programa pretende saltar a una sentencia distinta de la primera de una línea, el intérprete tendrá que buscar dicha sentencia recorriendo todas las anteriores. En Sinclair BASIC existen instrucciones de salto a sentencias distintas de la primera de una línea: &lt;code&gt;NEXT&lt;/code&gt; y &lt;code&gt;RETURN&lt;/code&gt;, que por tanto sufren del problema de las búsquedas lineales. Es conveniente situar el retorno de la llamada o el principio del bucle al principio de la línea, para que el intérprete no tenga que buscar la sentencia concreta dentro de la misma, yendo sentencia a sentencia hasta encontrarla.&lt;/p&gt;
    &lt;p&gt;No existen instrucciones para saltar a sentencias (distintas de la primera) explícitamente dadas por el usuario, pero esto se puede lograr engañando al intérprete con un truco, que podríamos llamar el “GOTO con POKE”, cuya existencia me ha señalado Rafael Velasco al verlo usado en algún programa escrito en una sola línea de BASIC. Este truco se basa en dos variables del sistema: &lt;code&gt;NEWPPC&lt;/code&gt; (dirección 23618 de memoria, 2 bytes) y &lt;code&gt;NSPPC&lt;/code&gt; (dirección 23620, 1 byte). En caso de que una sentencia del programa haga un salto (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;RETURN&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt; …), se rellenan con la línea (en &lt;code&gt;NEWPPC&lt;/code&gt;) y la sentencia (en &lt;code&gt;NSPPC&lt;/code&gt;) a donde hay que saltar, mientras que si no hace un salto, sólo se rellena &lt;code&gt;NSPPC&lt;/code&gt; con 255. Antes de ejecutar la siguiente sentencia, el intérprete consulta &lt;code&gt;NSPPC&lt;/code&gt;, y, si su bit nº 7 no es 1, salta a donde indiquen estas dos variables, mientras que si es 1, sigue ejecutando la siguiente sentencia del programa. El truco del “GOTO con POKE” consiste en manipular estas variables con &lt;code&gt;POKE&lt;/code&gt;, primero en &lt;code&gt;NEWPPC&lt;/code&gt; y luego en &lt;code&gt;NSPPC&lt;/code&gt;, de forma que, justo tras ejecutar el &lt;code&gt;POKE&lt;/code&gt; de &lt;code&gt;NSPPC&lt;/code&gt;, el intérprete se cree que tiene que hacer un salto a donde indican. De esta manera podemos ir a cualquier punto del programa, línea y sentencia incluidas.&lt;/p&gt;
    &lt;p&gt;Recuperando el hilo principal de esta entrada, las sentencias del lenguaje Sinclair BASIC afectadas por el problema de los números de línea / número de sentencia son:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;GO TO&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;GO SUB&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FN&lt;/code&gt;(requiere buscar la línea del correspondiente&lt;code&gt;DEF FN&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RETURN&lt;/code&gt;(debe retornar a un número de línea almacenado en la pila de direcciones de retorno)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NEXT&lt;/code&gt;(debe ir a la línea correspondiente al&lt;code&gt;FOR&lt;/code&gt;de su variable)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RESTORE&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RUN&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LIST&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LLIST&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Como las cuatro últimas no suelen usarse más que esporádicamente (las tres últimas prácticamente nunca dentro de un programa), la identificación de las zonas de código que deben moverse al principio debería enfocarse en bucles, rutinas y funciones de usuario (&lt;code&gt;FN&lt;/code&gt;). &lt;/p&gt;
    &lt;p&gt;Así, los &lt;code&gt;RETURN&lt;/code&gt; deberían hacerse hacia lugares próximos al comienzo del programa, es decir, los &lt;code&gt;GO SUB&lt;/code&gt;  correspondientes deberían estar allí (al principio del programa), y, si puede ser, en la primera sentencia de sus respectivas líneas para que no haya que buscar dentro de la línea la sentencia en cuestión, búsqueda que también se hace linealmente. &lt;/p&gt;
    &lt;p&gt;Los bucles &lt;code&gt;FOR&lt;/code&gt; pueden sustituirse por réplicas consecutivas del cuerpo en caso de que éstas no sean muy numerosas (esto se llama “desenrrollado de bucles”),  lo cual queda muy feo y ocupa más memoria de programa pero evita el coste  adicional de ejecución del salto &lt;code&gt;NEXT&lt;/code&gt; (y el de creación de variable en el &lt;code&gt;FOR&lt;/code&gt;).  &lt;/p&gt;
    &lt;p&gt;En pocas palabras: el código que llama mucho a otro código, es llamado mucho por otro código, o tiene muchos bucles internos debería ir al principio de un programa BASIC y en las primeras sentencias de dichas líneas.&lt;/p&gt;
    &lt;p&gt;Quiero aprovechar para mencionar en este punto que, aunque es de lo más común, en muchos casos sería recomendable no usar expresiones para las referencias a líneas, al menos en las primeras etapas de la escritura de un programa (es decir, no escribir “saltos paramétricos” como &lt;code&gt;GO TO 2*n+100&lt;/code&gt;, &lt;code&gt;GO SUB x*1000&lt;/code&gt;, etc., sino solamente con literales numéricos, como &lt;code&gt;GOTO 100&lt;/code&gt;, &lt;code&gt;GO SUB 2000&lt;/code&gt;). El uso de los saltos paramétricos hace el mantenimiento del programa un verdadero infierno, e impide su análisis automático. De todas formas, hay que admitir que usar expresiones como argumento de &lt;code&gt;GO TO&lt;/code&gt; / &lt;code&gt;GO SUB&lt;/code&gt; puede ser más rápido que escribir sentencias &lt;code&gt;IF&lt;/code&gt; para lograr el mismo objetivo. &lt;/p&gt;
    &lt;p&gt;Todo el asunto de los números de línea tiene una segunda consecuencia:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Para acelerar lo más posible todo el programa deberías escribir líneas lo más largas posible. Así, la búsqueda de una línea particular será más rápida, ya que habrá que recorrer menos líneas hasta llegar a ella (ir de una línea a la siguiente durante la búsqueda que hace el intérprete de la ROM cuesta el mismo tiempo independientemente de su longitud).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ZX-Basicus tiene una transformación disponible con la opción &lt;code&gt;--mergelines&lt;/code&gt; que hace esto automáticamente: aumenta el tamaño de las líneas siempre que esto respete el flujo del programa original. &lt;/p&gt;
    &lt;p&gt;Nótese que el usar menos líneas pero más largas ahorra también espacio en memoria, ya que no hay que almacenar números, longitudes ni marcas de fin de esas líneas. Por contra, con líneas largas es más costoso encontrar una sentencia a la que haya que retornar con un &lt;code&gt;RETURN&lt;/code&gt; o volver con un &lt;code&gt;NEXT&lt;/code&gt;, así como buscar una función de usuario (&lt;code&gt;DEF FN&lt;/code&gt;) que no esté al principio de su línea, por lo que hay que tener también eso en cuenta y llegar a una solución de compromiso.&lt;/p&gt;
    &lt;p&gt;Aún hay una tercera consecuencia de esta limitación del intérprete de BASIC de la ROM:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Las sentencias no ejecutables (&lt;/p&gt;&lt;code&gt;REM&lt;/code&gt;y sentencias vacías) que ocupan una sola línea deberían eliminarse siempre que se pueda, pues incrementan el tiempo de búsqueda, o bien ponerlas al final del todo. Asimismo, las sentencias&lt;code&gt;DATA&lt;/code&gt;, que normalmente no se usan más de una vez durante la ejecución del programa, deberían estar al final del programa.&lt;/quote&gt;
    &lt;p&gt;ZX-Basicus también ayuda en esto: permite eliminar automáticamente comentarios &lt;code&gt;REM&lt;/code&gt; (opción &lt;code&gt;--delrem&lt;/code&gt;) y sentencias vacías (opción &lt;code&gt;--delempty&lt;/code&gt;). La primera opción permite preservar algunos comentarios sin ser eliminados: los que comiencen por algún carácter que nosotros decidamos, pues siempre es interesante no dejar el código totalmente indocumentado. &lt;/p&gt;
    &lt;p&gt;En cualquier caso, quizás la opción más importante del optimizador de código de que dispone ZX-Basicus es &lt;code&gt;--move&lt;/code&gt;, que da la posibilidad de mover trozos de código de un lugar a otro con menos esfuerzo que a mano. Con ella se puede cambiar de sitio una sección completa del programa; la utilidad se encarga de renumerar el resultado automáticamente. Hay que tener en cuenta, sin embargo, que esta utilidad (como cualquier otra existente) no puede renumerar ni trabajar con números de línea calculados mediante expresiones, por lo que todas las referencias a líneas de programa deberían estar escritas como literales, tal y como se ha recomendado antes.&lt;/p&gt;
    &lt;p&gt;.oOo.&lt;/p&gt;
    &lt;p&gt;[Click here to read this in Spanish ]&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is the first in a series of posts that explain the foundations of the (in)efficiency of pure BASIC programs written for the ZX Spectrum:&lt;/p&gt;
      &lt;p&gt;I. On line numbers&lt;/p&gt;
      &lt;p&gt;II. On variables&lt;/p&gt;
      &lt;p&gt;III. On expressions&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Sinclair BASIC interpreter that the ZX Spectrum included in ROM was, in so many aspects, a wonder of software, particularly in assembly programming.&lt;/p&gt;
    &lt;p&gt;In this series of posts we will visit the main issues that allow our BASIC programs to execute efficiently, mainly considering time, but also memory consumption.&lt;/p&gt;
    &lt;p&gt;In this first post we are concerned in particular with the lines in a program; beyond the need for numbering them explicitly, something that does not exist in any programming language since decades, we are interested in the efficciency of the BASIC interpreter when managing lines and their numbers.&lt;/p&gt;
    &lt;p&gt;Before going to the point, we summarize here some limits that the ZX Spectrum has related to program lines:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Program line numbers, once the program is stored in memory and ready to be executed, take 2 bytes (by the way, they are stored in big-endian format, the only case of that in the ZX). This could lead to line numbers in the range 0 to 65535 (maximum value that can be stored into 2 bytes), but unfortunately that cannot be done easily. When editing a program manually, only lines from 1 to 9999 are allowed. If the program is manipulated outside the editor (which can be done with &lt;code&gt;POKE&lt;/code&gt;), it is possible to have a line numbered as 0, and that line will appear in the listing of the program, but it will no longer be editable. In the same way (using&lt;code&gt;POKE&lt;/code&gt;) you can have lines above 9999, but this causes trouble: many statements that admit a line number as a parameter, such as&lt;code&gt;GOTO&lt;/code&gt;or&lt;code&gt;RESTORE&lt;/code&gt;, produce an error if that line is greater than 32767; the call stack stop working correctly if we do a&lt;code&gt;GO SUB&lt;/code&gt;to a line greater than 15871 (3DFF in hexadecimal); the interpreter reserves the line number 65534 to indicate that it is executing code from the edition buffer (and not from the program listing); also, listing the program on the screen does not work well with lines greater than 9999, and right at the moment we edit these lines manually, they will be set to line numbers with just 4 digits.&lt;/item&gt;
      &lt;item&gt;The length of each program line (in bytes) is stored after the line number, and occupies 2 bytes (this time in little-endian). This length does not take into account the 2 bytes of the line number or the 2 bytes of itself. We could think that each line can have up to 65535 bytes (a 0x0D byte has to always be at the end to mark the end of the line), and that the shortest line takes 2+2+1+1 = 6 bytes of memory if it contains just one statement without parameters, e.g., &lt;code&gt;10 CLEAR&lt;/code&gt;. A very important ROM routine, the one in charge of finding the line or variable that is after the current one, skipping the latter (called&lt;code&gt;NEXT-ONE&lt;/code&gt;and located at 0x19B8) works perfectly well with line lengths in the range 0 to 65535. However, during execution, the interpreter stops its work on a line as soon as it finds 0x0D in the beginning of a statement (if the line is longer because it has been externally manipulated, it will ignore the rest, thus the remaining space can be used for storing -hidden- data within the program), and more importantly: the interpreter yields an error if trying to execute more than 127 statements in a given line. Consequently, a line in execution can only have from 1 to 127 statements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once we have summarized these data, we will focus on a very specific feature of the BASIC interpreter of the ZX Spectrum, one that is crucial for the efficiency of running BASIC programs:&lt;/p&gt;
    &lt;p&gt;There is no table of program addresses indexed with line numbers&lt;/p&gt;
    &lt;p&gt;BASIC programs were pre-processed right after typing them (after typing whole lines in the case of ZX Spectrum +2 and up), which saved space in ROM by not implementing a lexical analyzer. In that pre-processing, multi-character keywords were summarized into one-byte tokens, but many other things happened too: number literals were coded in binary form and hidden near the source numbers, line lengths were stored at the beginning of each line, placeholders were prepared for the parameters of user functions (&lt;code&gt;DEF FN&lt;/code&gt;) in order to store arguments when they are called, etc.&lt;/p&gt;
    &lt;p&gt;Unfortunately, there is one thing that was not done before executing the program: to build a table that, for each line number, provides in constant time (computational complexity O(1)) the memory address where that line is stored.&lt;/p&gt;
    &lt;p&gt;This has an important effect in the interpreter execution: every time it finds a statement in the program that has a line number as a parameter, (e.g., &lt;code&gt;GOTO&lt;/code&gt;, &lt;code&gt;GOSUB&lt;/code&gt;, etc.), the interpreter must search the entire program memory, line by line, until finding the place in memory where the referred line resides. This has a computational complexity of O(n), being n the number of lines in the program, i.e., it is linearly more costly to find the last lines in the program than the earlier ones. The interpreter works like this: it starts with a memory address that points to the beginning of the program, reads the line number that is there, if it is the one searched for, or the one immediatly after it, ends, otherwise reads the line length, add that length to the pointer, and repeats the process.&lt;/p&gt;
    &lt;p&gt;The result of this interpreter inner workings is that any statement that involves a jump to a line in the program (&lt;code&gt;GOTO&lt;/code&gt;, &lt;code&gt;GOSUB&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt;, &lt;code&gt;FN&lt;/code&gt;) will increase its execution time linearly with the number of lines that exist before the one of destination. That can be checked out with a BASIC program that measures that time for different destinations, such as the one you can download here. After executing it (care!: it takes more than 17 hours to achieve the precision we require in the estimations) we got this:&lt;/p&gt;
    &lt;p&gt;As you can see, the execution time in a jump increases in 71 microseconds per line of the program that we add before the destination line; that amounts to about 7 milliseconds if you have 100 lines before the destination, which can be a lot if the jump is part of a loop that repeats a lot of times. Our testing program takes 10000 measurements to get the final average time, thus the Central Limit Theorem suggests that the results in the figure above have a small amount of uncertainty, of around 115.5 microseconds if we consider as the main source of original uncertainty the [0,20] milliseconds produced by the time discretization of the &lt;code&gt;FRAMES&lt;/code&gt; system variable (this uncertainty does not affect the fact that, due to the same theorem and the large number of measurements, the average estimates will be distributed symmetrically and unbiasedly, i.e., they are practically equal to the real ones). You can also observe in the graph above that the parts of the loops in the testing program that are not the jump itself consume 5.6 milliseconds on average.&lt;/p&gt;
    &lt;p&gt;The first consequence of this is the first rule for writing efficient programs in pure Sinclair BASIC for the ZX Spectrum:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Those parts of the program that require a faster execution should be placed at the beginning (smaller line numbers). The same should be done for parts that contain loops or routines that are frequently called.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ZX-Basicus has an optimizing tool that can help in this aspect. For instance, it can execute a BASIC program in the PC and collect a profile with the frequency of execution of each statement (using the &lt;code&gt;--profile&lt;/code&gt; option). In this way, you can identify those parts of the code that would require to be re-located earlier in the listing.&lt;/p&gt;
    &lt;p&gt;There is a BASIC trick to cheat the interpreter and make it to search for a line starting in a place different from the start of the program. It consists in changing the value of the system variable &lt;code&gt;PROG&lt;/code&gt;, which is located at the memory address 23635 and occupies 2 bytes, to the memory address of the first line we wish the interpreter to use for its line search (therefore ignoring all the previous ones). In general, it is not easy to get the memory address of a line, but you can consult the system variable &lt;code&gt;NXTLIN&lt;/code&gt; (at 23637, 2 bytes), which stores the address of the next line to be executed (the analysis tool of ZX-Basicus also provides this kind of information with the location in memory of every element in the BASIC program if it is stored in a &lt;code&gt;.tap&lt;/code&gt; file). You can make, for example, a loop faster: do &lt;code&gt;POKE&lt;/code&gt; in the two bytes of &lt;code&gt;PROG&lt;/code&gt; with the value stored in &lt;code&gt;NXTLIN&lt;/code&gt;, and do that right at the line previous to the one of the loop; the result is that the loop will be as fast as though it was in first line of the program. However, do not forget to restore the original value of &lt;code&gt;PROG&lt;/code&gt; in order to access previous parts of that program!&lt;/p&gt;
    &lt;p&gt;User functions definitions (&lt;code&gt;DEF FN&lt;/code&gt;) are specially sensitive to the problem of searching line numbers. They are devised for being called repeteadly, therefore, they should also be at the beginning of the program. However, many programmers choose not to use them because of their high execution cost (which includes finding the line where they are defined, evaluating arguments, placing their values in the placeholders, and evaluating the expression of their bodies). The profile produced by ZX-Basicus also reports the number of calls to user functions (&lt;code&gt;FN&lt;/code&gt;), and it provides an option (&lt;code&gt;--delunusedfn&lt;/code&gt;) that automatically delete all &lt;code&gt;DEF FN&lt;/code&gt; that are not called in the program.&lt;/p&gt;
    &lt;p&gt;It is important to note that the BASIC interpreter has a linear (O(n)) behaviour not only when searching for lines, but also when searching for statements within a line. If the program tries to jump to a statement different from the first one in a line, the interpreter will search for that statement by skipping all the previous ones. In Sinclair BASIC we have instructions that may jump to statements different from the first ones in their lines: &lt;code&gt;NEXT&lt;/code&gt; and &lt;code&gt;RETURN&lt;/code&gt;, that, consequently, suffer from the problem of the linear searches. It is better to place the return of the call or the start of the loop at the beginning of a line to prevent the interpreter to conduct a linear search (statement by statement) to find them.&lt;/p&gt;
    &lt;p&gt;There are no instructions in the language to jump to statements that are explicitly given by the user, but that can be achieved by cheating the interpreter with a trick, that we could call “GOTO-with-POKE”, whose has been brought to my attention by Rafael Velasco, that saw it in a BASIC program entirely written in a single line. It is based on two system variables: &lt;code&gt;NEWPPC&lt;/code&gt; (address 23618, 2 bytes) and &lt;code&gt;NSPPC&lt;/code&gt; (address 23620, 1 byte). When a program statement makes a jump (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;RETURN&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt; …), the target line is stored into &lt;code&gt;NEWPPC&lt;/code&gt; and the target statement into &lt;code&gt;NSPPC&lt;/code&gt;; if the statement does not make a jump, &lt;code&gt;NSPPC&lt;/code&gt; is filled with 255; before executing the next statement, the interpret reads &lt;code&gt;NSPPC&lt;/code&gt; and, if the bit 7 of this variables is not 1, jumps to the place defined by &lt;code&gt;NEWPPC&lt;/code&gt;:&lt;code&gt;NSPPC&lt;/code&gt;, but if that bit is 1 it just goes on with the next statement. The “GOTO-with-POKE” trick consists in &lt;code&gt;POKE&lt;/code&gt;ing those variables, firstly &lt;code&gt;NEWPPC&lt;/code&gt;, then &lt;code&gt;NSPPC&lt;/code&gt;; right after the last &lt;code&gt;POKE&lt;/code&gt;, the interpreter believes there is a jump to do. In this way, we can go to any line and statement in our program.&lt;/p&gt;
    &lt;p&gt;Recovering the main thread of this post, the statements of the Sinclair BASIC language that involve to search lines in the program are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;GO TO&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;GO SUB&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FN&lt;/code&gt;(since&lt;code&gt;DEF FN&lt;/code&gt;must be searched for)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RETURN&lt;/code&gt;(it returns to a certain number of line and statement)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NEXT&lt;/code&gt;(it jumps to the corresponding&lt;code&gt;FOR&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RESTORE&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RUN&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LIST&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LLIST&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since the last four are used sporadically (the last three are very rare inside a program), the identification of parts of the program to be placed at the beginning for gaining in efficiency should focus on loops, routines and user functions. &lt;code&gt;RETURN&lt;/code&gt; statements should be used to return to places close to the beginning too, if they are frequently used, i.e., the corresponding &lt;code&gt;GO SUB&lt;/code&gt;  should be placed at the beginning, and, if possible, at the beginning  of their lines in order to reduce the cost of searching them within those lines. Also, in cases where they can not be re-placed, &lt;code&gt;FOR&lt;/code&gt; loops can be unrolled  (repeating their bodies as many times as iterations they have) to avoid  the jumps and the maintainance of the iteration variable. In summary: the code that calls a lot of routines, or is called frequently, or has many internal loops, should be placed at the beginning of the program. &lt;/p&gt;
    &lt;p&gt;I also recommend to only use literal numbers in the parameters of the statements that need a line (e.g., &lt;code&gt;GOTO 100&lt;/code&gt;, &lt;code&gt;GO SUB 2000&lt;/code&gt;), at least in the first stages of the writing of a program; do not use expressions at that time (“parametrical jumps”, e.g., &lt;code&gt;GO TO 2*n+100&lt;/code&gt;, &lt;code&gt;GO SUB x*1000&lt;/code&gt;, etc.), since that makes the maintainance and analysis of the program really difficult. I have to admit, though, that  using expressions as arguments in &lt;code&gt;GO TO&lt;/code&gt; / &lt;code&gt;GO SUB&lt;/code&gt; usually runs faster than writing &lt;code&gt;IF&lt;/code&gt; statements to achieve the same functionality. &lt;/p&gt;
    &lt;p&gt;The second consequence of the interpreter lacking an efficient line number table is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lines should be long (the maximum length is 127 statements in a line for the ROM interpreter not to issue an error). In that way, the search for a particular one will be more efficient, since traversing the lines has the same cost independently on their lengths (it only depends on the number of lines).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In this aspect, ZX-Basicus has an option (&lt;code&gt;--mergelines&lt;/code&gt;) that automatically merges contiguous lines, as long as that does not changes the program execution flow, in order to obtain the least number of lines.&lt;/p&gt;
    &lt;p&gt;Notice that having less but longer lines also saves memory space, since there are less line numbers and lengths (and end-line markers) to store. However, having longer lines makes less efficient the search for some statement within them (as in the case of &lt;code&gt;FOR&lt;/code&gt;…&lt;code&gt;NEXT&lt;/code&gt;, or &lt;code&gt;GO SUB&lt;/code&gt;, or &lt;code&gt;DEF FN&lt;/code&gt;). A suitable trade-off must be reached.&lt;/p&gt;
    &lt;p&gt;Finally, the third consequence of not having a line number table is:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Non-executable statements (&lt;/p&gt;&lt;code&gt;REM&lt;/code&gt;and empty statements) that fill entire lines should be eliminated or placed at the end, since they increase the search time for no reason. Also,&lt;code&gt;DATA&lt;/code&gt;statements, that are commonly used only once during the program execution, are excellent candidates to be placed at the end of the program.&lt;/quote&gt;
    &lt;p&gt;In this, ZX-Basicus has also some help for the programmer: it can delete automatically empty statements (&lt;code&gt;--delempty&lt;/code&gt;) and &lt;code&gt;REM&lt;/code&gt; (&lt;code&gt;--delrem&lt;/code&gt;); it can preserve some of the latter for keeping minimum documentation, though.&lt;/p&gt;
    &lt;p&gt;All in all, there is a fundamental tool in ZX-Basicus that is related to this post: option &lt;code&gt;--move&lt;/code&gt; re-locates portions of code, renumbering automatically all the line references (it can also serve to renumber the whole program, but that has no relation to speed-ups). Only take into account that it cannot work with line references that are not literal numbers (expressions, variables, etc.). &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262480</guid><pubDate>Sun, 14 Dec 2025 12:04:44 +0000</pubDate></item><item><title>Europeans' health data sold to US firm run by ex-Israeli spies</title><link>https://www.ftm.eu/articles/europe-health-data-us-firm-israel-spies</link><description>&lt;doc fingerprint="bfd091199664b185"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Europeans’ health data sold to U.S. firm run by ex-Israeli spies&lt;/head&gt;&lt;p&gt;The European messaging service Zivver – which is used for confidential communication by governments and hospitals in the EU and the U.K. – has been sold to Kiteworks, an American company with strong links to Israeli intelligence. Experts have expressed deep concerns over the deal.&lt;/p&gt;&lt;head rend="h3"&gt;This story in 1 minute&lt;/head&gt;&lt;p&gt;What’s the news?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;With the sale of Amsterdam-based data security company Zivver, sensitive information about European citizens is now in the hands of Kiteworks.&lt;/item&gt;&lt;item&gt;The CEO of the American tech company is a former cyber specialist from an elite unit of the Israeli army, as are several other members of its top management.&lt;/item&gt;&lt;item&gt;Various institutions in Europe and the U.K. – from hospitals to courts and immigration services – use Zivver to send confidential documents. While Zivver says these documents are encrypted, an investigation by Follow the Money shows that the company is able to read their contents.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Why does this matter?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Cybersecurity and intelligence experts told Follow the Money that the takeover should either have been prevented or properly assessed in advance.&lt;/item&gt;&lt;item&gt;Zivver processes information that could be extremely valuable to third parties, such as criminals or foreign intelligence services.&lt;/item&gt;&lt;item&gt;That information is now subject to invasive U.S. law, and overseen by a company with well-documented links to Israeli intelligence.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How was this investigated?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Follow the Money investigated the acquisition of Zivver and the management of Kiteworks, and spoke to experts in intelligence services and cyber security.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This article is part of an ongoing series.&lt;/p&gt;The EU Files&lt;p&gt;When the American data security company Kiteworks bought out its Dutch industry peer Zivver in June, CEO Jonathan Yaron described it as “a proud moment for all of us”.&lt;/p&gt;&lt;p&gt;The purchase was “a significant milestone in Kiteworks’ continued mission to safeguard sensitive data across all communication channels”, he added in a LinkedIn post.&lt;/p&gt;&lt;p&gt;But what Yaron did not mention was that this acquisition – coming at a politically charged moment between the U.S. and the EU – put highly sensitive, personal data belonging to European and British citizens directly into American hands.&lt;/p&gt;&lt;p&gt;Zivver is used by institutions including hospitals, health insurers, government services and immigration authorities in countries including the Netherlands, Germany, Belgium and the U.K.&lt;/p&gt;&lt;p&gt;Neither did Yaron mention that much of Kiteworks’ top management – himself included – are former members of an elite Israeli Defence Force unit that specialised in eavesdropping and breaking encrypted communications.&lt;/p&gt;&lt;p&gt;Our journalism is only possible thanks to the trust of our paying members. Not a member yet? Sign up now&lt;/p&gt;&lt;p&gt;In addition to this, an investigation by Follow the Money shows that data processed by Zivver is less secure than the service leads its customers to believe. Research found that emails and documents sent by Zivver can be read by the company itself. This was later confirmed by Zivver to Follow the Money.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;“All of the red flags should have been raised during this acquisition”&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Zivver maintained, however, that it does not have access to the encryption keys used by customers, and therefore cannot hand over data to U.S. authorities.&lt;/p&gt;&lt;p&gt;This is despite independent researchers confirming that the data was – for a brief period – accessible to the company. If U.S. officials wanted access to such communication, Zivver would be legally obligated to provide it.&lt;/p&gt;&lt;p&gt;Cybersecurity experts now point to serious security concerns, and ask why this sale seems to have gone through without scrutiny from European authorities.&lt;/p&gt;&lt;p&gt;“All of the red flags should have been raised during this acquisition,” said intelligence expert Hugo Vijver, a former long-term officer in AIVD, the Dutch security service.&lt;/p&gt;&lt;head rend="h2"&gt;Classified documents&lt;/head&gt;&lt;p&gt;Amsterdam-based Zivver – which was founded in 2015 by Wouter Klinkhamer and Rick Goud – provides systems for the encrypted exchange of information via email, chat and video, among other means.&lt;/p&gt;&lt;p&gt;Dutch courts, for example, work with Zivver to send classified documents, and solicitors use the service to send confidential information to the courts.&lt;/p&gt;&lt;p&gt;Other government agencies in the Netherlands – including the immigration service – also use Zivver. So do vital infrastructure operators such as the Port of Rotterdam and The Hague Airport.&lt;/p&gt;&lt;p&gt;In the U.K., a number of NHS hospitals and local councils use the company. In Belgium and Germany it is used in major hospitals.&lt;/p&gt;&lt;p&gt;The information that Zivver secures for its customers is therefore confidential and sensitive by nature.&lt;/p&gt;&lt;p&gt;When approached by Follow the Money, a number of governmental agencies said the company’s Dutch origins were a big factor in their decision to use Zivver.&lt;/p&gt;&lt;p&gt;Additionally, the fact that the data transferred via Zivver was stored on servers in Europe also played a role in their decisions.&lt;/p&gt;&lt;p&gt;Now that Zivver has been acquired by a company in the United States, that data is subject to far-reaching American laws. This means that the U.S. government can request access to this information if it wishes, regardless of where the data is stored.&lt;/p&gt;&lt;head rend="h2"&gt;The Trump effect&lt;/head&gt;&lt;p&gt;These laws are not new, but they have become even more draconian since U.S. President Donald Trump's return to office, according to experts.&lt;/p&gt;&lt;p&gt;Bert Hubert, a former regulator of the Dutch intelligence services, warned: “America is deteriorating so rapidly, both legally and democratically, that it would be very naive to hand over your courts and hospitals to their services.”&lt;/p&gt;&lt;p&gt;“Trump recently called on Big Tech to ignore European legislation. And that is what they are going to do. We have no control over it,” he added.&lt;/p&gt;&lt;p&gt;In Europe, Hubert said: “We communicate almost exclusively via American platforms. And that means that the U.S. can read our communications and disrupt our entire society if they decide that they no longer like us.”&lt;/p&gt;&lt;p&gt;Zivver had offered an alternative – a European platform governed by EU law.&lt;/p&gt;&lt;p&gt;“We are now throwing that away. If you want to share something confidential with a court or government, consider using a typewriter. That's about all we have left,” Hubert said.&lt;/p&gt;&lt;head rend="h2"&gt;Israeli ties&lt;/head&gt;&lt;p&gt;Beyond American jurisdiction, Kiteworks’ management raises another layer of concern: its links to Israeli intelligence.&lt;/p&gt;&lt;p&gt;Several of the company’s top executives, including CEO Yaron, are veterans of Unit 8200, the elite cyber unit of the Israel Defence Force (IDF). The unit is renowned for its code-breaking abilities and feared for its surveillance operations.&lt;/p&gt;&lt;quote&gt;In Israel, there is a revolving door between the army, lobby, business and politics&lt;/quote&gt;&lt;p&gt;Unit 8200 has been linked to major cyber operations, including the Stuxnet attack on Iranian nuclear facilities in 2007. More recently, it was accused of orchestrating the detonation of thousands of pagers in Lebanon, an incident the United Nations said violated international law and killed at least two children.&lt;/p&gt;&lt;p&gt;The unit employs thousands of young recruits identified for their digital skills. It is able to intercept global telephone and internet traffic.&lt;/p&gt;&lt;p&gt;International media have reported that Unit 8200 intercepts and stores an average of one million Palestinian phone calls every hour, data that ends up on Microsoft servers in Europe.&lt;/p&gt;&lt;p&gt;Some veterans themselves have also objected to the work of the unit. In 2014, dozens of reservists signed a letter to Israeli leaders saying they no longer wanted to participate in surveillance of the occupied territories.&lt;/p&gt;&lt;p&gt;“The lines of communication between the Israeli defence apparatus and the business community have traditionally been very short,” said Dutch intelligence expert Vijver. “In Israel, there is a revolving door between the army, lobby, business and politics.”&lt;/p&gt;&lt;head rend="h2"&gt;Veterans at the helm&lt;/head&gt;&lt;p&gt;That revolving door is clearly visible in big U.S. tech companies – and Kiteworks is no exception.&lt;/p&gt;&lt;p&gt;Aside from Yaron, both Chief Business Officer Yaron Galant and Chief Product Officer Amit Toren served in Unit 8200, according to publicly available information.&lt;/p&gt;&lt;p&gt;They played a direct role in negotiating the acquisition of Zivver. Their background was known to Zivver’s directors Goud and Klinkhamer at the time.&lt;/p&gt;&lt;p&gt;Other senior figures also have military intelligence backgrounds. Product director Ron Margalit worked in Unit 8200 before serving in the office of Israeli Prime Minister Benjamin Netanyahu. Mergers and acquisitions director Uri Kedem is a former Israeli naval captain.&lt;/p&gt;&lt;p&gt;Kiteworks is not unique in this respect.&lt;/p&gt;&lt;p&gt;Increasing numbers of U.S. cybersecurity firms now employ former Israeli intelligence officers. This trend, experts say, creates vulnerabilities that are rarely discussed.&lt;/p&gt;&lt;p&gt;An independent researcher quoted by U.S. Drop Site News said: “Not all of these veterans will send classified data to Tel Aviv. But the fact that so many former spies work for these companies does create a serious vulnerability: no other country has such access to the American tech sector.”&lt;/p&gt;&lt;p&gt;Or, as the ex-intelligence regulator Hubert put it: “Gaining access to communication flows is part of Israel’s long-term strategy. A company like Zivver fits perfectly into that strategy.”&lt;/p&gt;&lt;p&gt;The information handled by Zivver – confidential communications between governments, hospitals and citizens – is a potential goldmine for intelligence services.&lt;/p&gt;&lt;p&gt;According to intelligence expert Vijver, access to this kind of material makes it easier to pressure individuals into cooperating with intelligence agencies. Once an intelligence service has access to medical, financial and personal data, it can more easily pressure people into spying for it, he said.&lt;/p&gt;&lt;p&gt;But the gain for intelligence services lies not just in sensitive information, said Hubert: “Any data that allows an agency to tie telephone numbers, addresses or payment data to an individual is of great interest to them.”&lt;/p&gt;&lt;p&gt;He added: “It is exactly this type of data that is abundantly present in communications between civilians, governments and care institutions. In other words, the information that flows through a company like Zivver is extremely valuable for intelligence services.”&lt;/p&gt;&lt;head rend="h2"&gt;A flawed security model?&lt;/head&gt;&lt;p&gt;These geopolitical concerns become more pronounced when combined with technical worries about Zivver’s encryption.&lt;/p&gt;&lt;p&gt;For years, Zivver presented itself as a European alternative that guaranteed privacy. Its marketing materials claimed that messages were encrypted on the sender’s device and that the company had “zero access” to content.&lt;/p&gt;&lt;p&gt;But an investigation by two cybersecurity experts at a Dutch government agency, at the request of Follow the Money, undermines this claim.&lt;/p&gt;&lt;p&gt;The experts, who participated in the investigation on condition of anonymity, explored what happened when that government agency logged into Zivver’s web application to send information.&lt;/p&gt;&lt;p&gt;Tests showed that when government users sent messages through Zivver’s web application, the content – including attachments – was uploaded to Zivver’s servers as readable text before being encrypted. The same process applied to email addresses of senders and recipients.&lt;/p&gt;&lt;p&gt;“In these specific cases, Zivver processed the messages in readable form,” said independent cybersecurity researcher Matthijs Koot, who verified the findings.&lt;/p&gt;&lt;p&gt;“Even if only briefly, technically speaking it is possible that Zivver was able to view these messages,” he said.&lt;/p&gt;&lt;p&gt;He added: “Whether a message is encrypted at a later stage makes little difference. It may help against hackers, but it no longer matters in terms of protection against Zivver.”&lt;/p&gt;&lt;p&gt;Despite these findings, Zivver continues to insist on its website and in promotional material elsewhere – including on the U.K. government’s Digital Marketplace – that “contents of secure messages are inaccessible to Zivver and third parties”.&lt;/p&gt;&lt;p&gt;So far, no evidence has surfaced that Zivver misused its technical access. But now that the company is owned by Kiteworks, experts see a heightened risk.&lt;/p&gt;&lt;p&gt;Former intelligence officer Vijver puts it bluntly: “Given the links between Zivver, Kiteworks and Unit 8200, I believe there is zero chance that no data is going to Israel. To think otherwise is completely naive.”&lt;/p&gt;&lt;head rend="h2"&gt;A missed safeguard&lt;/head&gt;&lt;p&gt;The sale of Zivver could technically have been blocked or investigated under Dutch law. According to the Security Assessment of Investments, Mergers and Acquisitions Act, such sensitive takeovers are supposed to be reviewed by a specialised agency.&lt;/p&gt;&lt;p&gt;But the Dutch interior ministry declared that Zivver was not part of the country’s “critical infrastructure,” meaning that no review was carried out.&lt;/p&gt;&lt;p&gt;That, in Hubert’s view, was “a huge blunder”.&lt;/p&gt;&lt;p&gt;“It’s bad enough that a company that plays such an important role in government communications is falling into American hands, but the fact that there are all kinds of Israeli spies there is very serious,” he said.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;“The takeover is taking place in an unsafe world full of geopolitical tensions”&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Experts say the Zivver case highlights Europe’s lack of strategic control over its digital infrastructure.&lt;/p&gt;&lt;p&gt;Mariëtte van Huijstee of the Netherlands-based Rathenau Institute said: “I doubt whether the security of sensitive emails and files … should be left to the private sector. And if you think that is acceptable, should we leave it to non-European parties over whom we have no control?”&lt;/p&gt;&lt;p&gt;“We need to think much more strategically about our digital infrastructure and regulate these kinds of issues much better, for example by designating encryption services as vital infrastructure,” she added.&lt;/p&gt;&lt;p&gt;Zivver, for its part, claimed that security will improve under Kiteworks. Zivver’s full responses to Follow the Money’s questions can be read here and here.&lt;/p&gt;&lt;p&gt;But Van Huijstee was not convinced.&lt;/p&gt;&lt;p&gt;“Kiteworks employs people who come from a service that specialises in decrypting files,” she said.&lt;/p&gt;&lt;p&gt;“The takeover is taking place in an unsafe world full of geopolitical tensions, and we are dealing with data that is very valuable. In such a case, trust is not enough and more control is needed.”&lt;/p&gt;&lt;head rend="h3"&gt;Related articles&lt;/head&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;head rend="h3"&gt;Collection&lt;/head&gt;&lt;head rend="h3"&gt;Authors&lt;/head&gt;&lt;head rend="h3"&gt;Sebastiaan Brommersma&lt;/head&gt;&lt;p&gt;Former IP lawyer. Currently writing about digital technology, from big tech and big data to AI and cybercrime.&lt;/p&gt;&lt;head rend="h3"&gt;Siem Eikelenboom&lt;/head&gt;&lt;p&gt;Experienced investigative journalist. Was part of the international team that investigated the Panama Papers.&lt;/p&gt;Send a news tip&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262524</guid><pubDate>Sun, 14 Dec 2025 12:15:09 +0000</pubDate></item><item><title>The Gorman Paradox: Where Are All the AI-Generated Apps?</title><link>https://codemanship.wordpress.com/2025/12/14/the-gorman-paradox-where-are-all-the-ai-generated-apps/</link><description>&lt;doc fingerprint="7b1708caec5e440f"&gt;
  &lt;main&gt;
    &lt;p&gt;In 1950, while discussing the recent wave of flying saucer reports over lunch with colleagues at Los Alamos National Laboratory in New Mexico, physicist Enrico Fermi asked a simple question.&lt;/p&gt;
    &lt;p&gt;There are hundreds of billions of stars in our Milky Way galaxy, and – presumed at the time – a significant percentage have Earth-like habitable planets orbiting them. The galaxy is billions of years old, and the odds are high that there should be other technological civilisations out there. But we see no convincing sign of them.&lt;/p&gt;
    &lt;p&gt;So, where is everybody?&lt;/p&gt;
    &lt;p&gt;This question is now know as the Fermi Paradox.&lt;/p&gt;
    &lt;p&gt;In the last couple of years, I’ve been seeing another paradox. Many people claim that working software can now be produced for pennies on the pound, in a fraction of the time that it takes humans. Some go so far as to claim that we’re in the age of commoditised software, throwaway software, and hail the end of the software industry as we know it.&lt;/p&gt;
    &lt;p&gt;Why buy a CRM solution or a ERM system when “AI” can generate one for you in hours or even minutes? Why sign up for a SaaS platform when Cursor can spit one out just as good in the blink of an eye?&lt;/p&gt;
    &lt;p&gt;But when we look beyond the noise – beyond these sensational flying saucer reports – we see nothing of the sort. No AI-generated Spotify or Salesforce or SAP. No LLM-generated games bothering the charts. No noticeable uptick in new products being added to the app stores.&lt;/p&gt;
    &lt;p&gt;So, where is everybody?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262545</guid><pubDate>Sun, 14 Dec 2025 12:18:59 +0000</pubDate></item><item><title>Baumol's Cost Disease</title><link>https://en.wikipedia.org/wiki/Baumol_effect</link><description>&lt;doc fingerprint="ef48805ab3ed0642"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Baumol effect&lt;/head&gt;&lt;p&gt;In economics, the Baumol effect, also known as Baumol's cost disease, first described by William J. Baumol and William G. Bowen in the 1960s, is the tendency for wages in jobs that have experienced little or no increase in labor productivity to rise in response to rising wages in other jobs that did experience high productivity growth.[1][2] In turn, these sectors of the economy become more expensive over time, because the input costs increase while productivity does not. Typically, this affects services more than manufactured goods, and in particular health, education, arts and culture.[3]&lt;/p&gt;&lt;p&gt;This effect is an example of cross elasticity of demand. The rise of wages in jobs without productivity gains results from the need to compete for workers with jobs that have experienced productivity gains and so can naturally pay higher wages. For instance, if the retail sector pays its managers low wages, those managers may decide to quit and get jobs in the automobile sector, where wages are higher because of higher labor productivity. Thus, retail managers' salaries increase not due to labor productivity increases in the retail sector, but due to productivity and corresponding wage increases in other industries.&lt;/p&gt;&lt;p&gt;The Baumol effect explains a number of important economic developments:[3]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The share of total employment in sectors with high productivity growth decreases, while that of low productivity sectors increases.[4]&lt;/item&gt;&lt;item&gt;Economic growth slows down, due to the smaller proportion of high growth sectors in the whole economy.[4]&lt;/item&gt;&lt;item&gt;Government spending is disproportionately affected by the Baumol effect, because of its focus on services like health, education and law enforcement.[3][5]&lt;/item&gt;&lt;item&gt;Increasing costs in labor-intensive service industries, or below average cost decreases, are not necessarily a result of inefficiency.[3]&lt;/item&gt;&lt;item&gt;Due to income inequality, services whose prices rise faster than incomes can become unaffordable to many workers. This happens despite overall economic growth, and has been exacerbated by the rise in inequality in recent decades.[4]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Baumol referred to the difference in productivity growth between economic sectors as unbalanced growth. Sectors can be differentiated by productivity growth as progressive or non-progressive. The resulting transition to a post-industrial society, i.e. an economy where most workers are employed in the tertiary sector, is called tertiarization.&lt;/p&gt;&lt;head rend="h2"&gt;Description&lt;/head&gt;[edit]&lt;p&gt;Increases in labor productivity tend to result in higher wages.[6][7] Productivity growth is not uniform across the economy, however. Some sectors experience high productivity growth, while others experience little or negative productivity growth.[8] Yet wages have tended to rise not only in sectors with high productivity growth, but also in those with little to no productivity growth.&lt;/p&gt;&lt;p&gt;The American economists William J. Baumol and William G. Bowen proposed that wages in sectors with stagnant productivity rise out of the need to compete for workers with sectors that experience higher productivity growth, which can afford to raise wages without raising prices. With higher labor costs, but little increase in productivity, sectors with low productivity growth see their costs of production rise. As summarized by Baumol in a 1967 paper:[9]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;If productivity per man hour rises cumulatively in one sector relative to its rate of growth elsewhere in the economy, while wages rise commensurately in all areas, then relative costs in the nonprogressive sectors must inevitably rise, and these costs will rise cumulatively and without limit...Thus, the very progress of the technologically progressive sectors inevitably adds to the costs of the technologically unchanging sectors of the economy, unless somehow the labor markets in these areas can be sealed off and wages held absolutely constant, a most unlikely possibility.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h2"&gt;Origins&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Jean Fourastié: unbalanced growth in economic sectors&lt;/head&gt;[edit]&lt;p&gt;Studying various price series over time, Jean Fourastié noticed the unequal technological progress in different industries.[10]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;But what is essential is that very large sectors of economic activity have remained practically unaffected by technological progress. For example, the men's barber does not cut more clients' hair in 1948 than in 1900; entire professions have not changed their working methods from 1900 to 1930. ... (1949: 27).&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;He predicted that this would lead to a gradual increase in the share of services in the economy, and the resulting post-industrial society:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;... the absolute volume of secondary production continues to grow; but from a certain state of economic development, the value of these growing productions diminishes in relation to the total volume of national production. Thus, tertiary values invade economic life; that is why it can be said that the civilization of technical progress will be a tertiary civilization. (1949: 59)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In a 2003 article, Baumol noted: "For the origins of the analysis, see Fourastié (1963)."[11][12]&lt;/p&gt;&lt;head rend="h3"&gt;Baumol and Bowen: rising wages despite productivity stagnation&lt;/head&gt;[edit]&lt;p&gt;The original study on the Baumol effect was conducted for the performing arts sector.[1] American economists Baumol and Bowen in 1965 said "the output per man-hour of the violinist playing a Schubert quartet in a standard concert hall is relatively fixed." In other words, they said the productivity of classical music performance had not increased. However, the real wages of musicians had increased substantially since the 19th century. Gambling and Andrews pointed out in 1984 that productivity does go up with the size of the performance halls.[13] Furthermore Greenfield pointed out in 1995 that far more people hear the performance due to advances in amplification, recording and broadcasting, so productivity has increased many-fold.[14][15]&lt;/p&gt;&lt;head rend="h2"&gt;Effects&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Price and output&lt;/head&gt;[edit]&lt;p&gt;Firms may respond to increases in labor costs induced by the Baumol effect in a variety of ways, including:[16]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Cost and price disease: Prices in stagnant industries tend to grow faster than average&lt;/item&gt;&lt;item&gt;Stagnant output: Real output in low-productivity-growth industries tends to grow more slowly relative to the overall economy&lt;/item&gt;&lt;item&gt;Employment effects: Firms in stagnant industries may reduce employment, decrease hours, or increase non-monetary compensation&lt;/item&gt;&lt;/list&gt;&lt;p&gt;An important implication of Baumol effect is that it should be expected that, in a world with technological progress, the costs of manufactured goods will tend to fall (as productivity in manufacturing continually increases) while the costs of labor-intensive services like education, legal services, and health care (where productivity growth is persistently slow) will tend to rise (see chart).[a][19]&lt;/p&gt;&lt;p&gt;A 2008 study by American economist William Nordhaus showed as much, concluding that "Baumol-type diseases" in technologically stagnant sectors have led to "rising relative prices and declining relative real outputs."[16] In the realm of prices, Nordhaus showed that in the United States from 1948–2001 "productivity trends are associated almost percentage-point for percentage-point with price decline." Industries with low productivity growth thus saw their relative prices increase, leading Nordhaus to conclude: "The hypothesis of a cost-price disease due to slow productivity growth is strongly supported by the historical data. Industries with relatively lower productivity growth show a percentage-point for percentage-point higher growth in relative prices." A similar conclusion held for real output: "The real output/stagnation hypothesis is strongly confirmed. Technologically stagnant industries have shown slower growth in real output than have the technologically dynamic ones. A one percentage-point higher productivity growth was associated with a three-quarters percentage-point higher real output growth."&lt;/p&gt;&lt;head rend="h3"&gt;Affordability and inequality&lt;/head&gt;[edit]&lt;p&gt;While the Baumol effect suggests that costs in low-productivity-growth industries will continually rise, Baumol argues the "stagnant-sector services will never become unaffordable to society. This is because the economy's constantly growing productivity simultaneously increases the population's overall purchasing power."[20] To see this, consider an economy with a real national income of $100 billion with healthcare spending amounting to $20 billion (20% of national income), leaving $80 billion for other purchases. Say that, over 50 years, due to productivity growth real national income doubles to $200 billion (an annual growth rate of about 1.4%). In this case, even if healthcare spending were to rise by 500% to $120 billion, there would still be $80 billion left over for other purchases—exactly the same amount as 50 years prior. In this scenario, healthcare now accounts for 60% of national income, compared to 20% fifty years prior, and yet the amount of income left to purchase other goods remains unchanged. Further, if healthcare costs were to account for anything less than 60% of national income, there would be more income left over for other purchases (for instance, if healthcare costs were to rise from 20% of national income to 40% of national income, there would be $120 billion left over for other purchases—40% more than 50 years prior). So it can be seen that even if productivity growth were to lead to substantial healthcare cost increases as a result of Baumol's cost disease, the wealth increase brought on by that productivity growth would still leave society able to purchase more goods than before.&lt;/p&gt;&lt;p&gt;While this is true for society in the aggregate, it is not the case for all workers as individuals. Baumol noted that the increase in costs "disproportionally affects the poor."[4] Although a person's income may increase over time, and the affordability of manufactured goods may increase too, the price increases in industries subject to the Baumol effect can be larger than the increase in many workers' wages (see chart above, note average wages). These services become less affordable, especially to low income earners, despite the overall economic growth. This effect is exacerbated by the increase in income inequality observed in recent decades.[4]&lt;/p&gt;&lt;head rend="h3"&gt;Government spending&lt;/head&gt;[edit]&lt;p&gt;The Baumol effect has major implications for government spending. Since most government spending goes towards services that are subject to the cost disease—law enforcement, education, healthcare etc.—the cost to the government of providing these services will rise as time goes on.[5][21]&lt;/p&gt;&lt;head rend="h3"&gt;Labor force distribution&lt;/head&gt;[edit]&lt;p&gt;One implication of the Baumol effect is a shift in the distribution of the labor force from high-productivity industries to low-productivity industries.[9] In other words, the effect predicts that the share of the workforce employed in low-productivity industries will rise over time.&lt;/p&gt;&lt;p&gt;The reasoning behind this can be seen through a thought experiment offered by Baumol in his book The Cost Disease:[22]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Let us assume for simplicity that the share of the economy's total output that comes from the progressive sector [industries with high productivity growth], as measured in physical units rather than money, does not change. Because the economy has only two sectors, progressive and stagnant [industries with low productivity growth], whose production together accounts for all of its output, it follows that the stagnant sector also must maintain a constant share of the total.&lt;/p&gt;&lt;p&gt;This has significant implications for the distribution of the economy's labor force. By definition, labor productivity grows significantly faster in the progressive sector than in the stagnant sector, so to keep a constant proportion between the two sectors' output, more and more labor has to move from the progressive sector into the stagnant sector.[b]&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;As predicted by the Baumol effect, the proportion of the United States labor force employed in stagnant industries has grown substantially since the 1960s. In particular, the United States has morphed from a manufacturing economy into a service economy (see chart).[23] However, how much of this is due to the Baumol effect rather than other causes is disputed.[24][25] In a 2010 study, the economist Talan B. İşcan devised a model from which he concluded that both Baumol and Engel effects played significant roles in the rising share of employment in services in the United States (though he noted that "considerable gaps between the calibrated model and the actual data remain").[26] An older 1968 study by economist Victor Fuchs likewise concluded that the Baumol effect played a major role in the shift to services, although he determined that demand shifts like those proposed in Engel's law played only a minor role.[27] The economists Robert Rowthorn and Ramana Ramaswamy also concluded that relatively faster growth of productivity in manufacturing played a role in the shift to services.[28] The economist Tom Elfring, however, argued in a 1989 paper that the Baumol effect has played a secondary role to growth in demand for services since the 1970s.[29] Alternative theories for the shift to services include demand-side theories (the Baumol effect is broadly a supply-side explanation) like the three-sector model devised by Allan Fisher[30] and Colin Clark[31] in the 1930s, which posit that services satisfy higher needs than goods and so as income grows a higher share of income will be used for the purchase of services;[25] changes in the inter-industry division of labor, favoring specialized service activities;[25] outsourcing to countries with lower labor costs;[32] increasing participation of women in the labor force;[33] and trade specialization.[34]&lt;/p&gt;&lt;p&gt;The Baumol effect has also been used to describe the reallocation of labor out of agriculture (in the United States, in 1930 21.5% of the workforce was employed in agriculture and agriculture made up 7.7% of GDP; by 2000, only 1.9% of the workforce was employed in agriculture and agriculture made up only 0.7% of GDP[35]).[36] In a 2009 study, the economists Benjamin N. Dennis and Talan B. İşcan concluded that after the 1950s relatively faster productivity growth in agriculture was the key driver behind the continuing shift in employment from agriculture to non-farm goods (prior to the 1950s, they determined that Engel's law explained almost all labor reallocation out of agriculture).[37]&lt;/p&gt;&lt;head rend="h3"&gt;Economic growth and aggregate productivity&lt;/head&gt;[edit]&lt;p&gt;In his original paper on the cost disease, Baumol argued that in the long run the cost disease implies a reduction in aggregate productivity growth and correspondingly a reduction in economic growth.[9] This follows straightforwardly from the labor distribution effects of the cost disease. As a larger and larger share of the workforce moves from high-productivity-growth industries to low-productivity-growth industries, it is natural to expect that the overall rate of productivity growth will slow. Since economic growth is driven in large part by productivity growth, economic growth would also slow.&lt;/p&gt;&lt;p&gt;The economist Nicholas Oulton, however, argued in a 2001 paper that Baumol effect may counterintuitively result in an increase in aggregate productivity growth.[38] This could occur if many services produce intermediate inputs for the manufacturing sector, i.e. if a significant number of services are business services.[c] In this case, even though the slow-growth service sector is increasing in size, because these services further boost the productivity growth of the shrinking manufacturing sector overall productivity growth may actually increase. Relatedly, the economist Maurizio Pugno described how many stagnant services, like education and healthcare, contribute to human capital formation, which enhances growth and thus "oppos[es] the negative Baumol effect on growth."[39]&lt;/p&gt;&lt;p&gt;The economist Hiroaki Sasaki, however, disputed Oulton's argument in a 2007 paper.[40] Sasaki constructed an economic model that takes into account the use of services as intermediate inputs in high-productivity-growth industries and still concluded that a shift in labor force distribution from higher-productivity-growth manufacturing to lower-productivity-growth services decreases the rate of economic growth in the long run. Likewise, the economists Jochen Hartwig and Hagen Krämer concluded in a 2019 paper that, while Outlon's theory is "logically consistent", it is "not in line with the data", which shows a lowering of aggregate productivity growth.[41]&lt;/p&gt;&lt;head rend="h2"&gt;Sectors&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Education&lt;/head&gt;[edit]&lt;p&gt;The Baumol effect has been applied to the education sector,[42][43][44] including by Baumol himself.[45][46] By most measures, productivity growth in the education sector over the last several decades has been low or even negative;[47][48] the average student-teacher ratio in American universities, for instance, was sixteen to one in 2011, just as it was in 1981.[44] Yet, over this period, tuition costs have risen substantially.[49] It has been proposed that this is at least partially explained by the Baumol effect: even though there has been little or even negative productivity growth in the education sector, because of productivity increases across other sectors of the economy universities today would not be able to attract professors with 1980s-level salaries, so they are forced to raise wages to maintain their workforce. To afford the increased labor costs, universities raise tuition fees (i.e. they increase prices).[50]&lt;/p&gt;&lt;p&gt;Evidence on the role of the Baumol effect in rising education costs has been mixed. Economists Robert B. Archibald and David H. Feldman, both of the College of William &amp;amp; Mary, argued in a 2006 study, for instance, that the Baumol effect is the dominant driver behind increasing higher education costs.[51] Other studies, however, have found a lesser role for the Baumol effect. In a 2014 study, the economists Robert E. Martin and Carter Hill devised a model that determined that the Baumol effect explained only 23%–32% of the rise in higher education costs.[52] The economists Gary Rhoades and Joanna Frye went further in a 2015 study and argued that the Baumol effect could not explain rising tuition costs at all, as "relative academic labor costs have gone down as tuition has gone up."[53] The cost disease may also have only limited effects on primary and secondary education: a 2016 study on per-pupil public education spending by Manabu Nose, an economist at the International Monetary Fund, found that "the contribution of Baumol's effect was much smaller than implied by theory"; Nose argued that it was instead rising wage premiums paid for teachers in excess of market wages that were the dominant reason for increasing costs, particularly in developing countries.[54]&lt;/p&gt;&lt;p&gt;Alternative explanations for rising higher education costs include Bowen's revenue theory of cost,[52][55] reduced public subsidies for education,[56][57][58] administrative bloat,[56][59] the commercialization of higher education,[60] increased demand for higher education,[61] the easy availability of federal student loans,[62][63] difficulty comparing prices of different universities,[64] technological change,[43] and lack of a central mechanism to control price increases.[57]&lt;/p&gt;&lt;head rend="h3"&gt;Healthcare&lt;/head&gt;[edit]&lt;p&gt;The Baumol effect has been applied to the rising cost of healthcare,[46] as the healthcare industry has long had low productivity growth.[65][66] Empirical studies have largely confirmed the large role of the Baumol effect in the rising cost of healthcare in the United States,[67][68][69][70][71] although there is some disagreement.[72] Likewise, a 2021 study determined that "Baumol's cost disease ha[s] a significant positive impact on health expenditure growth" in China.[73] However, a paper by economists Bradley Rossen and Akhter Faroque on healthcare costs in Canada found that "the cost disease ... is a relatively minor contributor [in the growth of health-care spending in Canada], while technical progress in health care and growth in per capita incomes are by far the biggest contributors."[74]&lt;/p&gt;&lt;p&gt;Despite substantial technological innovation and capital investment, the healthcare industry has struggled to significantly increase productivity. As summarized by the economists Alberto Marino, David Morgan, Luca Lorenzoni, and Chris James:[75]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Technological advancements, capital investments and economies of scale do not make for a cumulative rise in output that is on par with progressive sectors of the economy ... [A]utomation and better technology generally do not allow for large productivity increases. A health professional is difficult to substitute, in particular by using new technologies, which may actually also bring an increase in volume (e.g. faster diagnostic tests). Increases in volume likely brought about by new technology will also drive up expenditure, since new health professionals will have to be hired to treat everyone. Moreover, new technologies require more specialised training for say [sic] doctors, driving wages up further since more years of experience are required.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Service industry&lt;/head&gt;[edit]&lt;p&gt;Baumol's cost disease is often used to describe consequences of the lack of growth in productivity in the quaternary sector of the economy and public services, such as public hospitals and state colleges.[42] Labor-intensive sectors that rely heavily on non-routine human interaction or activities, such as health care, education, or the performing arts, have had less growth in productivity over time. As with the string quartet example, it takes nurses the same amount of time to change a bandage or college professors the same amount of time to mark an essay today as it did in 1966.[76] In contrast, goods-producing industries, such as the car manufacturing sector and other activities that involve routine tasks, workers are continually becoming more productive by technological innovations to their tools and equipment.&lt;/p&gt;&lt;p&gt;The reported productivity gains of the service industry in the late 1990s are largely attributable to total factor productivity.[77] Providers decreased the cost of ancillary labor through outsourcing or technology. Examples include offshoring data entry and bookkeeping for health care providers and replacing manually-marked essays in educational assessment with multiple choice tests that can be automatically marked.&lt;/p&gt;&lt;head rend="h2"&gt;Technical description&lt;/head&gt;[edit]&lt;p&gt;In the 1967 paper Macroeconomics of Unbalanced Growth: The Anatomy of Urban Crisis, Baumol introduced a simple two-sector model to demonstrate the cost disease.[9] To do so, he imagined an economy consisting of only two sectors: sector one, which has constant productivity (that is, the number of goods workers can produce per man hour does not change as time goes on), and sector two, which sees productivity grow at a constant compounded rate (that is, the number of goods workers can produce per man hour grows at a rate , where is time). To simplify, he assumed that the quantity of goods produced by these two sectors (the "output" of each of the two sectors) is directly proportional to the quantity of labor employed (that is, doubling the number of workers doubles the output, tripling the number of workers triples the output, and so on) and that output depends only upon labor productivity and the quantity of labor. Since there is no increase in labor productivity in sector one, the output of sector one at time (denoted ) is:&lt;/p&gt;&lt;p&gt;where is the quantity of labor employed in sector one and is a constant that can be thought of as the amount of output each worker can produce at time . This equation simply says that the amount of output sector one produces equals the number of workers in sector one multiplied by the number of goods each worker can produce. Since productivity does not increase, the number of goods each worker produces remains and output remains constant through time for a given number of workers.&lt;/p&gt;&lt;p&gt;Since the labor productivity of sector two increases at a constant compounded rate , the output of sector two at time (denoted ) is:&lt;/p&gt;&lt;p&gt;where is the quantity of labor employed in sector two and is a constant that can be thought of as the amount of output each worker can produce at time . Since productivity grows at a constant compounded rate , the number of goods each worker produces at time equals , and the output of sector two grows at a rate proportional to productivity growth.&lt;/p&gt;&lt;p&gt;To more clearly demonstrate how wages and costs change through time, wages in both sectors are originally set at the same value . It is then assumed that wages rise in direct proportion to productivity (i.e., a doubling of productivity results in a doubling of wages, a tripling of productivity results in a tripling of wages, and so on). This means that the wages of the two sectors at time determined solely by productivity are:&lt;/p&gt;&lt;p&gt;(since productivity remains unchanged), and&lt;/p&gt;&lt;p&gt;(since productivity increases at a rate ).&lt;/p&gt;&lt;p&gt;These values, however, assume that workers do not move between the two sectors. If workers are equally capable of working in either sector, and they choose which sector to work in based upon which offers a higher wage, then they will always choose to work in the sector that offers the higher wage. This means that if sector one were to keep wages fixed at , then as wages in sector two grow with productivity workers in sector one would quit and seek jobs in sector two. Firms in sector one are thus forced to raise wages to attract workers. More precisely, in this model the only way firms in either sector can attract workers is to offer the same wage as firms in the other sector—if one sector were to offer lower wages, then all workers would work in the other sector.&lt;/p&gt;&lt;p&gt;So to maintain their workforces, wages in the two sectors must equal each other: . And since it is sector two that sees its wage naturally rise with productivity, while sector one's does not naturally rise, it must be the case that:&lt;/p&gt;&lt;p&gt;.&lt;/p&gt;&lt;p&gt;This typifies the labor aspect of the Baumol effect: as productivity growth in one sector of the economy drives up that sector's wages, firms in sectors without productivity growth must also raise wages to compete for workers.[d]&lt;/p&gt;&lt;p&gt;From this simple model, the consequences on the costs per unit output in the two sectors can be derived. Since the only factor of production within this model is labor, each sector's total cost is the wage paid to workers multiplied by the total number of workers. The cost per unit output is the total cost divided by the amount of output, so with representing the unit cost of goods in sector one at time and representing the unit cost of goods in sector two at time :&lt;/p&gt;&lt;p&gt;Plugging in the values for and from above:&lt;/p&gt;&lt;p&gt;It can be seen that in the sector with growing labor productivity (sector two), the cost per unit output is constant since both wages and output rise at the same rate. However, in the sector with stagnant labor productivity (sector one), the cost per unit output rises exponentially since wages rise exponentially faster than output.&lt;/p&gt;&lt;p&gt;This demonstrates the cost aspect of the Baumol effect (the "cost disease"). While costs in sectors with productivity growth do not increase, in sectors with little to no productivity growth costs necessarily rise due to the rising prevailing wage. Furthermore, if the productivity growth differential persists (that is, the low-productivity-growth sectors continue to see low productivity growth into the future while high-productivity-growth sectors continue to see high productivity growth), then costs in low-productivity-growth sectors will rise cumulatively and without limit.&lt;/p&gt;&lt;p&gt;Baumol's model can also be used to demonstrate the effect on the distribution of labor. Assume that, despite the change in the relative costs and prices of the two industries, the magnitude of the relative outputs of the two sectors are maintained. A situation similar to this could occur, for instance, "with the aid of government subsidy, or if demand for the product in question were sufficiently price inelastic or income elastic." The output ratio and its relation to the labor ratio, ignoring constants and , is then given by:&lt;/p&gt;&lt;p&gt;Letting (i.e. is the total labor supply), it follows that:&lt;/p&gt;&lt;p&gt;It can be seen that as approaches infinity, the quantity of labor in the non-progressive sector approaches the total labor supply while the quantity of labor in the progressive sector approaches zero. Hence, "if the ratio of the outputs of the two sectors is held constant, more and more of the total labor force must be transferred to the non-progressive sector and the amount of labor in the other sector will tend to approach zero."&lt;/p&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;head rend="h2"&gt;Notes&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Note that low productivity growth does not afflict all services. Telecommunications, for example, has seen substantial productivity growth.[17] Service industries are simply more likely than manufactured goods industries to be immune to productivity growth, for the reasons that they are often less able to be standardized and that the quality of services is more likely to be closely linked to the amount of labor provided.[18]&lt;/item&gt;&lt;item&gt;^ A technical description of this effect can be found in Technical description&lt;/item&gt;&lt;item&gt;^ In the two-sector model Baumol devised in his original paper (see Technical description), services are produced only for final consumption.&lt;/item&gt;&lt;item&gt;^ Note that this is an assumption of the model. As Baumol states in the original paper, "We suppose wages are equal in the two sectors and are fixed at dollars per unit of labor, where itself grows in accord with the productivity of sector 2, our 'progressive' sector." The preceding two paragraphs simply demonstrate the logic of that assumption.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b Baumol, W. J.; Bowen, W. G. (1965). "On the Performing Arts: The Anatomy of Their Economic Problems". The American Economic Review. 55 (1/2): 495–502. JSTOR 1816292.&lt;/item&gt;&lt;item&gt;^ Baumol, William J.; Bowen, William G. (1966). Performing Arts, The Economic Dilemma: A Study of Problems Common to Theater, Opera, Music, and Dance. Cambridge, Mass.: M.I.T. Press. ISBN 0262520117.&lt;/item&gt;&lt;item&gt;^ a b c d Lee, Timothy B. (May 4, 2017). "William Baumol, whose famous economic theory explains the modern world, has died". Vox. Archived from the original on January 31, 2022. Retrieved March 1, 2022.&lt;/item&gt;&lt;item&gt;^ a b c d e Hartwig, Jochen; Krämer, Hagen M. (December 2023). "Revisiting Baumol's Disease: Structural Change, Productivity Slowdown and Income Inequality". Intereconomics. 58 (6): 320–325. doi:10.2478/ie-2023-0066. hdl:10419/281398.&lt;/item&gt;&lt;item&gt;^ a b Baumol 2012, p. 27.&lt;/item&gt;&lt;item&gt;^ Anderson, Richard G. (2007). "How Well Do Wages Follow Productivity Growth?" (PDF). Federal Reserve Bank of St. Louis. Archived (PDF) from the original on December 3, 2021. Retrieved March 1, 2022.&lt;/item&gt;&lt;item&gt;^ Feldstein, Martin (July 2008). "Did wages reflect growth in productivity?". Journal of Policy Modeling. 30 (4): 591–594. doi:10.1016/j.jpolmod.2008.04.003.&lt;/item&gt;&lt;item&gt;^ Baily, Martin Neil; Bosworth, Barry; Doshi, Siddhi (January 2020). "Productivity comparisons: Lessons from Japan, the United States, and Germany" (PDF). Brookings Institution. p. 14. Archived (PDF) from the original on December 18, 2021. Retrieved March 1, 2022.&lt;/item&gt;&lt;item&gt;^ a b c d Baumol, William J. (June 1967). "Macroeconomics of Unbalanced Growth: The Anatomy of Urban Crisis" (PDF). The American Economic Review. 57 (3): 415–426. JSTOR 1812111. Archived (PDF) from the original on February 26, 2022.&lt;/item&gt;&lt;item&gt;^ Brown, Edmund A. (1951). "Review of Le Grand espoir du XXe Siècle: Progrès technique, Progrès Èconomique, Progrès Social.; La Civilisation de 1960., Jean Fourastié". Political Science Quarterly. 66 (4): 603–606. doi:10.2307/2145452. JSTOR 2145452.&lt;/item&gt;&lt;item&gt;^ Baumol, William J. (2003). "The Cost Disease of the Personal Services". The Encyclopedia of Public Choice. pp. 456–460. doi:10.1007/978-0-306-47828-4_70. ISBN 978-0-7923-8607-0. This is a reprint of Fourastié 1949.&lt;/item&gt;&lt;item&gt;^ Alcouffe, A.; Le Bris, D. (2020). "Technical Progress and Structural Change in Jean Fourastié's Theory of Development". History of Political Economy. 52 (1): 101–133.&lt;/item&gt;&lt;item&gt;^ Gambling, Trevor; Andrews, Gordon (1984). "Does Baumol's Disease Exist? Some Findings from the Royal Shakespeare Company". Journal of Cultural Economics. 8 (2): 73–92. ISSN 0885-2545 – via JSTOR.&lt;/item&gt;&lt;item&gt;^ Greenfield, Harry (March 1, 2005). "Letter: curing 'Baumol's Disease'". The Service Industries Journal. 25 (2): 289–290. doi:10.1080/0264206042000305466. ISSN 0264-2069 – via Semantic Scholar.&lt;/item&gt;&lt;item&gt;^ Akehurst, Gary. "What Do We Really Know About Services?". Service Business. 2 (1): 1–15. Archived from the original on September 7, 2024 – via Pennsylvania State University.&lt;/item&gt;&lt;item&gt;^ a b Nordhaus, William D (January 27, 2008). "Baumol's Diseases: A Macroeconomic Perspective" (PDF). The B.E. Journal of Macroeconomics. 8 (1). doi:10.2202/1935-1690.1382. S2CID 153319511.&lt;/item&gt;&lt;item&gt;^ Modica, Nathan F.; Chansky, Brian (May 2019). "Productivity trends in the wired and wireless telecommunications industries" (PDF). Bureau of Labor Statistics. Archived (PDF) from the original on January 14, 2022. Retrieved March 24, 2022.&lt;/item&gt;&lt;item&gt;^ Baumol 2012, pp. 22–24.&lt;/item&gt;&lt;item&gt;^ Lee, Timothy B. (May 4, 2017). "William Baumol, whose famous economic theory explains the modern world, has died". Vox. Archived from the original on January 31, 2022. Retrieved March 1, 2022.&lt;/item&gt;&lt;item&gt;^ Baumol 2012, p. xx.&lt;/item&gt;&lt;item&gt;^ Baumol, William J. (1993). "Health care, education and the cost disease: A looming crisis for public choice". The Next Twenty-five Years of Public Choice. pp. 17–28. doi:10.1007/978-94-017-3402-8_3. ISBN 978-94-017-3404-2.&lt;/item&gt;&lt;item&gt;^ Baumol 2012, p. 80.&lt;/item&gt;&lt;item&gt;^ Short, Doug (September 5, 2011). "Charting The Incredible Shift From Manufacturing To Services In America". Business Insider. Archived from the original on April 21, 2021. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ Urquhart, Michael (April 1984). "The employment shift to services: where did it come from?" (PDF). Monthly Labor Review. 107 (4): 15–22. Archived from the original (PDF) on January 30, 2022 – via Bureau of Labor Statistics. &lt;quote&gt;Suggested explanations for the faster growth of services employment include changes in the demand for goods and services as a result of rising incomes and relative price movements, slower productivity growth in services, the increasing participation of women in the labor force since World War II, and the growing importance of the public and nonprofit sector in general. But no consensus exists on the relative importance of the above factors in developing an adequate explanation of the sectoral shifts in employment.&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;^ a b c Schettkat, Ronald; Yocarini, Lara (2003). The Shift to Services: A Review of the Literature (Report). hdl:10419/20200. S2CID 154141056. SSRN 487282.&lt;/item&gt;&lt;item&gt;^ Iscan, Talan (January 30, 2010). "How Much Can Engel's Law and Baumol's Disease Explain the Rise of Service Employment in the United States?". The B.E. Journal of Macroeconomics. 10 (1). doi:10.2202/1935-1690.2001. S2CID 154824000.&lt;/item&gt;&lt;item&gt;^ Fuchs, Victor (1968). The Service Economy. National Bureau of Economic Research. ISBN 978-0870144769.&lt;/item&gt;&lt;item&gt;^ Rowthorn, Robert; Ramaswamy, Ramana (1999). "Growth, Trade, and Deindustrialization". IMF Staff Papers. 46 (1): 18–41. doi:10.2307/3867633. JSTOR 3867633.&lt;/item&gt;&lt;item&gt;^ Elfring, Tom (July 1989). "The Main Features and Underlying Causes of the Shift to Services". The Service Industries Journal. 9 (3): 337–356. doi:10.1080/02642068900000040.&lt;/item&gt;&lt;item&gt;^ Fisher, Allan G. B. (1935). The Clash of Progress and Security. Macmillan. ISBN 9780678001585. &lt;code&gt;{{cite book}}&lt;/code&gt;: ISBN / Date incompatibility (help)[page needed]&lt;/item&gt;&lt;item&gt;^ Clark, Colin (1940). The Conditions of Economic Progress. Macmillan. ISBN 9780598475732. &lt;code&gt;{{cite book}}&lt;/code&gt;: ISBN / Date incompatibility (help)[page needed]&lt;/item&gt;&lt;item&gt;^ Scharpf, F. W. (1990). "Structures of Postindustrial Society or Does Mass Unemployment Disappear in the Service and Information Economy". In Appelbaum, E. (ed.). Labor Market Adjustments to Structural Change and Technological Progress. New York: Praeger. pp. 17–36. ISBN 978-0-275-93376-0.&lt;/item&gt;&lt;item&gt;^ Urquhart, Michael (April 1984). "The employment shift to services: where did it come from?" (PDF). Monthly Labor Review. 107 (4): 15–22. Archived from the original (PDF) on January 30, 2022.&lt;/item&gt;&lt;item&gt;^ Rowthorn, R. E.; Wells, J.R. (1987). De-Industrialization and Foreign Trade. Cambridge University Press. ISBN 978-0521269476.&lt;/item&gt;&lt;item&gt;^ Dimitri, Carolyn; Effland, Anne; Conklin, Neilson (June 2005). "The 20th Century Transformation of U.S. Agriculture and Farm Policy" (PDF). United States Department of Agriculture. Archived from the original (PDF) on March 24, 2022. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ Baumol 2012, pp. 117–119.&lt;/item&gt;&lt;item&gt;^ Dennis, Benjamin N.; İşcan, Talan B. (April 2009). "Engel versus Baumol: Accounting for structural change using two centuries of U.S. data". Explorations in Economic History. 46 (2): 186–202. doi:10.1016/j.eeh.2008.11.003.&lt;/item&gt;&lt;item&gt;^ Oulton, N. (October 2001). "Must the growth rate decline? Baumol's unbalanced growth revisited". Oxford Economic Papers. 53 (4): 605–627. doi:10.1093/oep/53.4.605.&lt;/item&gt;&lt;item&gt;^ Pugno, Maurizio (January 2006). "The service paradox and endogenous economic growth" (PDF). Structural Change and Economic Dynamics. 17 (1): 99–115. doi:10.1016/j.strueco.2005.02.003. hdl:11572/43500.&lt;/item&gt;&lt;item&gt;^ Sasaki, Hiroaki (December 2007). "The rise of service employment and its impact on aggregate productivity growth". Structural Change and Economic Dynamics. 18 (4): 438–459. doi:10.1016/j.strueco.2007.06.003.&lt;/item&gt;&lt;item&gt;^ Hartwig, Jochen; Krämer, Hagen (December 2019). "The 'Growth Disease' at 50 – Baumol after Oulton". Structural Change and Economic Dynamics. 51: 463–471. doi:10.1016/j.strueco.2019.02.006. hdl:10419/170670. S2CID 115407478.&lt;/item&gt;&lt;item&gt;^ a b Helland, Eric; Alexander Tabarrok (2019). "Why Are the Prices So Damn High? Health, Education, and the Baumol Effect" (PDF). Mercatus Center.&lt;/item&gt;&lt;item&gt;^ a b Archibald, Robert B.; Feldman, David H. (2014). Why Does College Cost So Much?. Oxford University Press. ISBN 978-0190214104.&lt;/item&gt;&lt;item&gt;^ a b Surowiecki, James (November 13, 2011). "Debt by Degrees". The New Yorker.&lt;/item&gt;&lt;item&gt;^ Baumol, William J. (June 1967). "Macroeconomics of Unbalanced Growth: The Anatomy of Urban Crisis" (PDF). The American Economic Review. 57 (3): 415–426. JSTOR 1812111. Archived (PDF) from the original on February 26, 2022. &lt;quote&gt;The relatively constant productivity of college teaching ... suggests that, as productivity in the remainder of the economy continues to increase, costs of running the educational organizations will mount correspondingly, so that whatever the magnitude of the funds they need today, we can be reasonably certain that they will require more tomorrow, and even more on the day after that.&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;^ a b Baumol 2012, pp. 3–32.&lt;/item&gt;&lt;item&gt;^ "Labor Productivity and Costs: Elementary and Secondary Schools". Bureau of Labor Statistics. February 23, 2018. Archived from the original on January 4, 2022. Retrieved February 27, 2022.&lt;/item&gt;&lt;item&gt;^ Garrett, Thomas A; Poole, William (January 1, 2006). "Stop Paying More for Less: Ways to Boost Productivity in Higher Education". Federal Reserve Bank of St. Louis. Archived from the original on January 11, 2022. Retrieved February 27, 2022.&lt;/item&gt;&lt;item&gt;^ Carnevale, Anthony P.; Gulish, Artem; Campbell, Kathryn Peltier (2021). "If Not Now, When? The Urgent Need for an All-One-System Approach to Youth Policy" (PDF). McCourt School of Public Policy: Center on Education and the Workforce. Georgetown University. p. 13. Archived (PDF) from the original on October 20, 2021. Retrieved February 27, 2022.&lt;/item&gt;&lt;item&gt;^ Bundick, Brent; Pollard, Emily (2019). "The Rise and Fall of College Tuition Inflation" (PDF). Federal Reserve Bank of Kansas City. Archived (PDF) from the original on December 17, 2021. Retrieved February 27, 2022.&lt;/item&gt;&lt;item&gt;^ Archibald, Robert B.; Feldman, David H. (May 2008). "Explaining Increases in Higher Education Costs" (PDF). The Journal of Higher Education. 79 (3): 268–295. doi:10.1080/00221546.2008.11772099. S2CID 158250944.&lt;/item&gt;&lt;item&gt;^ a b Martin, Robert E.; Hill, R. Carter (2012). Measuring Baumol and Bowen Effects in Public Research Universities (Report). S2CID 153016802. SSRN 2153122.&lt;/item&gt;&lt;item&gt;^ Rhoades, Gary; Frye, Joanna (April 2015). "College tuition increases and faculty labor costs: A counterintuitive disconnect" (PDF). University of Arizona. Archived (PDF) from the original on October 2, 2021. Retrieved February 27, 2021.&lt;/item&gt;&lt;item&gt;^ Nose, Manabu (June 2017). "Estimation of drivers of public education expenditure: Baumol's effect revisited". International Tax and Public Finance. 24 (3): 512–535. doi:10.1007/s10797-016-9410-7. S2CID 155747172.&lt;/item&gt;&lt;item&gt;^ Matthews, Dylan (September 2, 2013). "The Tuition is Too Damn High, Part VI – Why there's no reason for big universities to rein in spending". The Washington Post. Archived from the original on November 13, 2020. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ a b "Why Tuition Costs Are Rising So Quickly". Challenge. 45 (4): 88–108. July 2002. doi:10.1080/05775132.2002.11034164.&lt;/item&gt;&lt;item&gt;^ a b Ripley, Amanda (September 11, 2018). "Why Is College in America So Expensive?". The Atlantic. Archived from the original on March 24, 2022. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Baum, Sandy; McPherson, Michael; Braga, Breno; Minton, Sarah (February 28, 2018). "Tuition and State Appropriations". Urban Institute. Retrieved August 12, 2024.&lt;/item&gt;&lt;item&gt;^ Johnson, J. David (2020). "Administrative Bloat in Higher Education" (PDF). Cambridge Scholars. Archived (PDF) from the original on October 13, 2021. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Renehan, Stewart (August 2015). "Rising Tuition in Higher Education: Should we be Concerned?". Visions for the Liberal Arts. 1 (1) – via CORE.&lt;/item&gt;&lt;item&gt;^ Hoffower, Hillary (June 26, 2019). "College is more expensive than it's ever been, and the 5 reasons why suggest it's only going to get worse". Business Insider. Archived from the original on March 23, 2022. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Bennet, William J. (February 18, 1987). "Our Greedy Colleges". The New York Times. Archived from the original on January 27, 2022. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Robinson, Jenna A. (December 2017). "The Bennett Hypothesis Turns 30" (PDF). James G. Martin Center for Academic Renewal. Archived (PDF) from the original on January 28, 2021. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Akers, Beth (August 27, 2020). "A New Approach for Curbing College Tuition Inflation". Manhattan Institute for Policy Research. Archived from the original on November 29, 2021. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Sheiner, Louise; Malinovskaya, Anna (May 2016). "Measuring Productivity in Healthcare: An Analysis of the Literature" (PDF). Brookings Institution. Archived (PDF) from the original on March 24, 2022. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ "Labor Productivity and Costs: Private Community Hospitals". Bureau of Labor Statistics. June 11, 2021. Archived from the original on January 4, 2022. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ Colombier, Carsten (November 2017). "Drivers of Health-Care Expenditure: What Role Does Baumol's Cost Disease Play?". Social Science Quarterly. 98 (5): 1603–1621. doi:10.1111/ssqu.12384.&lt;/item&gt;&lt;item&gt;^ Hartwig, Jochen (May 2008). "What drives health care expenditure?—Baumol's model of 'unbalanced growth' revisited". Journal of Health Economics. 27 (3): 603–623. doi:10.1016/j.jhealeco.2007.05.006. hdl:10419/50843. PMID 18164773. S2CID 219356527. SSRN 910879.&lt;/item&gt;&lt;item&gt;^ Hartwig, Jochen (January 2011). "Can Baumol's model of unbalanced growth contribute to explaining the secular rise in health care expenditure? An alternative test". Applied Economics. 43 (2): 173–184. doi:10.1080/00036840802400470. hdl:10419/50451. S2CID 154307473.&lt;/item&gt;&lt;item&gt;^ Bates, Laurie J.; Santerre, Rexford E. (March 2013). "Does the U.S. health care sector suffer from Baumol's cost disease? Evidence from the 50 states". Journal of Health Economics. 32 (2): 386–391. doi:10.1016/j.jhealeco.2012.12.003. PMID 23348051.&lt;/item&gt;&lt;item&gt;^ Pomp, Marc; Vujić, Sunčica (December 2008). "Rising health spending, new medical technology and the Baumol effect" (PDF). Bureau for Economic Policy Analysis. Archived (PDF) from the original on January 22, 2022. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ Atanda, Akinwande; Menclova, Andrea Kutinova; Reed, W. Robert (May 2018). "Is health care infected by Baumol's cost disease? Test of a new model". Health Economics. 27 (5): 832–849. doi:10.1002/hec.3641. PMID 29423941. S2CID 46855963.&lt;/item&gt;&lt;item&gt;^ Wang, Linan; Chen, Yuqian (2021). "Determinants of China's health expenditure growth: based on Baumol's cost disease theory". International Journal for Equity in Health. 20 (1): 213. doi:10.1186/s12939-021-01550-y. PMC 8474747. PMID 34565389. S2CID 233774285.&lt;/item&gt;&lt;item&gt;^ Rossen, Bradley; Faroque, Akhter (May 2016). "Diagnosing the Causes of Rising Health-Care Expenditure in Canada: Does Baumol's Cost Disease Loom Large?". American Journal of Health Economics. 2 (2): 184–212. doi:10.1162/AJHE_a_00041. S2CID 57569390.&lt;/item&gt;&lt;item&gt;^ Marino, Alberto; Morgan, David; Lorenzoni, Luca; James, Chris (June 2017). Future trends in health care expenditure: A modelling framework for cross-country forecasts (Report). OECD Health Working Papers. doi:10.1787/247995bb-en. ProQuest 1915769062.&lt;/item&gt;&lt;item&gt;^ Surowiecki, James (July 7, 2003). "What Ails Us". The New Yorker. Archived from the original on May 9, 2021.&lt;/item&gt;&lt;item&gt;^ Bosworth, Barry P.; Triplett, Jack E. (September 1, 2003). "Productivity Measurement Issues in Services Industries: 'Baumol's Disease' Has been Cured". Brookings Institution.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Sources&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Baumol, William J. (2012). The Cost Disease: Why Computers Get Cheaper and Health Care Doesn't. Yale University Press. ISBN 978-0-300-19815-7.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;External links&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Charles Hugh Smith (December 11, 2010). "America's Economic Malady: A Bad Case of 'Baumol's Disease'". DailyFinance.com. Archived from the original on December 17, 2010. Retrieved December 14, 2010.&lt;/item&gt;&lt;item&gt;Sparviero, Sergio; Preston, Paschal (September 2010). "Creativity and the positive reading of Baumol cost disease". The Service Industries Journal. 30 (11): 1903–1917. doi:10.1080/02642060802627541. S2CID 154148314. SSRN 1531602.&lt;/item&gt;&lt;item&gt;Preston, Paschal; Sparviero, Sergio (November 30, 2009). "Creative Inputs as the Cause of Baumol's Cost Disease: The Example of Media Services". Journal of Media Economics. 22 (4): 239–252. doi:10.1080/08997760903375910. S2CID 154146664. SSRN 1531555.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262592</guid><pubDate>Sun, 14 Dec 2025 12:34:35 +0000</pubDate></item><item><title>Kimi K2 1T model runs on 2 512GB M3 Ultras</title><link>https://twitter.com/awnihannun/status/1943723599971443134</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262734</guid><pubDate>Sun, 14 Dec 2025 13:04:34 +0000</pubDate></item><item><title>AI and the ironies of automation – Part 2</title><link>https://www.ufried.com/blog/ironies_of_ai_2/</link><description>&lt;doc fingerprint="b6d01b1d01739db2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI and the ironies of automation - Part 2&lt;/head&gt;
    &lt;p&gt;Some (well-known) consequences of AI automating work&lt;/p&gt;
    &lt;head rend="h1"&gt;AI and the ironies of automation - Part 2&lt;/head&gt;
    &lt;p&gt;In the previous post, we discussed several observations, Lisanne Bainbridge made in her much-noticed paper “The ironies of automation”, she published in 1983 and what they mean for the current “white-collar” work automation attempts leveraging LLMs and AI agents based on LLMs, still requiring humans in the loop. We stopped at the end of the first chapter, “Introduction”, of the paper.&lt;/p&gt;
    &lt;p&gt;In this post, we will continue with the second chapter, “Approaches to solutions”, and see what we can learn there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing apples and oranges?&lt;/head&gt;
    &lt;p&gt;However, before we start: Some of the observations and recommendations made in the paper must be taken with a grain of salt when applying them to the AI-based automation attempts of today. When monitoring an industrial production plant, it is often a matter of seconds until a human operator must act if something goes wrong to avoid severe or even catastrophic accidents.&lt;/p&gt;
    &lt;p&gt;Therefore, it is of the highest importance to design industrial control stations in a way that a human operator can recognize deviations and malfunctions as easily as possible and immediately trigger countermeasures. A lot of work is put into the design of all the displays and controls, like, e.g., the well-known emergency stop switch in a screaming red color that is big enough to be punched with a flat hand, fist or alike within a fraction of a second if needed.&lt;/p&gt;
    &lt;p&gt;When it comes to AI-based solutions automating white-collar work, we usually do not face such critical conditions. However, this is not a reason to dismiss the observations and recommendations in the paper easily because, e.g.:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Most companies are efficiency-obsessed. Hence, they also expect AI solutions to increase “productivity”, i.e., efficiency, to a superhuman level. If a human is meant to monitor the output of the AI and intervene if needed, this requires that the human needs to comprehend what the AI solution produced at superhuman speed – otherwise we are down to human speed. This presents a quandary that can only be solved if we enable the human to comprehend the AI output at superhuman speed (compared to producing the same output by traditional means).&lt;/item&gt;
      &lt;item&gt;Most companies have a tradition of nurturing a culture of urgency and scarcity, resulting in a lot of pressure towards and stress for the employees. Stress is known to trigger the fight-or-flight mode (an ancient survival mechanism built into us to cope with dangerous situations) which massively reduces the normal cognitive capacity of a human. While this mechanism supports humans in making very quick decisions and taking quick actions (essential in dangerous situations), it deprives them of the ability to conduct any deeper analysis (not being essential in dangerous situations). If deeper analysis is required to make a decision, this may take a lot longer than without stress – if possible at all. This means we need to enable humans to conduct deeper analysis under stress as well or to provide the information in a way that eliminates the need for deeper analysis (which is not always possible).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we let this sink in (plus a few other aspects, I did not write down here but you most likely will add in your mind), we quickly come to the conclusion that also in our AI-related automation context humans are often expected to make quick decisions and act based on them, often under conditions that make it hard (if not impossible) to conduct any in-depth analysis.&lt;/p&gt;
    &lt;p&gt;If we then also take into account, that depending on the situation a wrong result produced by an AI solution which eluded the human operator may have severe consequences in the worst case (e.g., assume a major security incident due to a missed wrongdoing of the AI solution), the situation is not that far away anymore from the situation in an industrial plant’s control station.&lt;/p&gt;
    &lt;p&gt;Summarizing, we surely need to add the necessary grain of salt, i.e., ask ourselves how strict the timing constraints in our specific setting are to avoid comparing apples and oranges in the worst case. However, in general we need to consider the whole range of possible settings which will – probably more often than we think – include that humans need to make decisions in a very short time under stressful conditions (which makes things more precarious).&lt;/p&gt;
    &lt;head rend="h2"&gt;The worst UI possible&lt;/head&gt;
    &lt;p&gt;This brings us immediately to Lisanne Bainbridge’s first recommendation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In any situation where a low probability event must be noticed quickly then the operator must be given artificial assistance, if necessary even alarms on alarms.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, the system must support the human operator as well as possible in detecting a problem, especially if it tends to occur rarely. It is a consequence of the “monitoring fatigue” problem we discussed in the previous post.&lt;/p&gt;
    &lt;p&gt;Due to the learnings people have made, a lot of effort has been put into the design of the displays, the controls and also the alerting mechanisms of industrial production control stations, making sure the human operators can make their jobs as good, as stress-free and as reliable as possible.&lt;/p&gt;
    &lt;p&gt;Enter AI agents.&lt;/p&gt;
    &lt;p&gt;The usual idea is that a single human controls a fleet of AI agents that are designed to do some kind of job, e.g., writing code. Sometimes, most agents are generic “workers”, orchestrated by some kind of supervisor that delegates parts of the work to the worker agents. Sometimes, the different agents are “specialists”, each for a certain aspect of the job to be done, that collaborate using some kind of choreography (or are also orchestrated by a supervisor). While the generic workers are easier to set up, the specialized workers usually produce more accurate results.&lt;/p&gt;
    &lt;p&gt;Because these AI-based agents sometimes produce errors, a human – in our example a software developer – needs to supervise the AI agent fleet and ideally intervenes before the AI agents do something they should not do. Therefore, the AI agents typically create a plan of what they intend to do first (which as a side effect also increases the likelihood that they do not drift off). Then, the human verifies the plan and approves it if it is correct, and the AI agents execute the plan. If the plan is not correct, the human rejects it and sends the agents back to replanning, providing information about what needs to be altered.&lt;/p&gt;
    &lt;p&gt;Let us take Lisanne Bainbridge’s recommendation and compare it to this approach that is currently “best practice” to control an AI agent fleet.&lt;/p&gt;
    &lt;p&gt;Unless we tell them to act differently, LLMs and also AI agents based on them are quite chatty. Additionally, they tend to communicate with an air of utter conviction. Thus, they present to you this highly detailed, multi-step plan of what they intend to do, including lots of explanations, in this perfectly convinced tone. Often, these plans are more than 50 or 100 lines of text, sometimes even several hundred lines.&lt;/p&gt;
    &lt;p&gt;Most of the time, the plans are fine. However, sometimes the AI agents mess things up. They make wrong conclusions, or they forget what they are told to do and drift off – not very often, but it happens. Sometimes the problem is obvious at first sight. But more often, it is neatly hidden somewhere behind line 123: “… and because 2 is bigger than 3, it is clear, we need to &amp;lt; do something critical &amp;gt;”. But because it is so much text the agents flood you with all the time and because the error is hidden so well behind this wall of conviction, we miss it – and the AI agent does something critical wrong.&lt;/p&gt;
    &lt;p&gt;We cannot blame the person for missing the error in the plan. The problem is that this is probably the worst UI and UX possible for anyone who is responsible for avoiding errors in a system that rarely produces errors.&lt;/p&gt;
    &lt;p&gt;But LLM-based agents make errors all the time, you may say. Well, not all the time. Sometimes they do. And the better the instructions and the setup of the interacting agents, the fewer errors they produce. Additionally, we can expect more specialized and refined agents in the future that become increasingly better in their respective areas of expertise. Still, most likely they will never become completely error-free because of the underlying technology that cannot guarantee consistent correctness.&lt;/p&gt;
    &lt;p&gt;This is the setting we need to ponder if we talk about the user interface for a human observer: a setting where the agent fleet only rarely makes errors but we still need a human monitoring and intervening if things should go wrong. It is not yet clear how such an interface should look like, but most definitely not as it looks now. Probably we could harvest some good insights from our UX/UI design colleagues for industrial production plant control stations. We would need only to ask them …&lt;/p&gt;
    &lt;head rend="h2"&gt;The training paradox&lt;/head&gt;
    &lt;p&gt;Lisanne Bainbridge then makes several recommendations regarding the required training of the human operator. This again is a rich section, and I can only recommend reading it on your own because it contains several subtle yet important hints that are hard to bring across without citing the whole chapter. Here, I will highlight only a few aspects. She starts with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[Some points made in the previous section] make it clear that it can be important to maintain manual skills.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Then she talks about letting the human operator take over control regularly, i.e., do the job instead of the machine as a very effective training option. Actually, without doing hands-on work regularly, the skills of a human expert deteriorate surprisingly fast.&lt;/p&gt;
    &lt;p&gt;But if taking over the work regularly is not an option, e.g., because we want continuous superhuman productivity leveraging AI agents (no matter if it makes sense or not), we still need to make sure that the human operator can take over if needed. In such a setting, training must take place in some other way, usually using some kind of simulator.&lt;/p&gt;
    &lt;p&gt;However, there is a problem with simulators, especially if human intervention is only needed (and wanted) if things do not work as expected:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are problems with the use of any simulator to train for extreme situations. Unknown faults cannot be simulated, and system behaviour may not be known for faults which can be predicted but have not been experienced.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The consequence of this issue is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This means that training must be concerned with general strategies rather than specific responses […]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is inadequate to expect the operator to react to unfamiliar events solely by consulting operating procedures. These cannot cover all the possibilities, so the operator is expected to monitor them and fill in the gaps.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Which leaves us with the irony:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;However, it is ironic to train operators in following instructions and then put them in the system to provide intelligence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is a problem we will need to face with AI agents and their supervising humans in the future, too. The supervising experts are meant to intervene whenever things become messy, whenever the AI agents get stuck, often in unforeseen ways. These are not regular tasks. Often, these are also not the issues we expect an AI agent to run into and thus can provide training for. These are extraordinary situations, the ones we do not expect – and the more refined and specialized the AI agents will become in the future, the more often the issues that require human intervention will be of this kind.&lt;/p&gt;
    &lt;p&gt;The question is twofold:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;How can we train human operators at all to be able to intervene skillfully in exceptional, usually hard to solve situations?&lt;/item&gt;
      &lt;item&gt;How can we train a human operator so that their skills remain sharp over time and they remain able to address an exceptional situation quickly and resourcefully?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The questions seem to hint at a sort of paradox, and an answer to both questions is all but obvious. At the moment, we still have enough experienced subject matter experts that the questions may feel of lower importance. But if we only start to address the questions when they become pressing, they will be even harder – if not impossible – to solve.&lt;/p&gt;
    &lt;p&gt;To end this consideration with the words of Lisanne Bainbridge:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Perhaps the final irony is that it is the most successful automated systems, with rare need for manual intervention, which may need the greatest investment in human operator training.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, we cannot simply take a few available human experts and make them supervise agents that took over their work without any further investments in the humans. Instead, we need to train them continuously, and the better the agents become, the more expensive the training of the supervisors will become. I highly doubt that decision makers who primarily think about saving money when it comes to AI agents are aware of this irony.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interlude&lt;/head&gt;
    &lt;p&gt;As I wrote in the beginning of first part of this blog series, “The ironies of automation” is a very rich and dense paper. We are still only at the end of the second chapter “Approaches to solutions” which is two and a half pages into the paper and there is still a whole third chapter called “Human-computer collaboration” which takes up another page until we get to the conclusion.&lt;/p&gt;
    &lt;p&gt;While this third chapter also contains a lot of valuable advice that goes well beyond our focus here, I will leave it to you to read it on your own. As I indicated at the beginning, this paper is more than worth the time spent on it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The leadership dilemma&lt;/head&gt;
    &lt;p&gt;However, before finishing this little blog series, I would like to mention a new kind of dilemma that Lisanne Bainbridge did not discuss in her paper because the situation was a bit different with industrial production plant automation than with AI-agent-based automation. But as this topic fits nicely into the just-finished training paradox section, I decided to add it here.&lt;/p&gt;
    &lt;p&gt;The issue is that just monitoring an AI agent fleet doing its work and intervening if things go wrong usually is not sufficient, at least not yet. All the things discussed before apply, but there is more to interacting with AI agents because we cannot simply be reactive with AI agents. We cannot simply watch them doing their work and only intervene if things go wrong. Instead, we additionally need to be proactive with them: We need to direct them.&lt;/p&gt;
    &lt;p&gt;We need to tell the AI agents what to do, what not to do, which chunks to pick and so on. This is basically a leadership role. While you do not lead humans, the kind of work is quite similar: You are responsible for the result; you are allowed to set directions and constraints, but you do not immediately control the work. You only control it through communicating with the agents and trying to direct them in the right direction with orders, with feedback, with changed orders, with setting different constraints, etcetera.&lt;/p&gt;
    &lt;p&gt;This is a skill set most people do not have naturally. Usually, they need to develop it over time. Typically, before people are put in a leadership role directing humans, they will get a lot of leadership training teaching them the skills and tools needed to lead successfully. For most people, this is essential because if they come from the receiving end of orders (in the most general sense of “orders”), typically they are not used to setting direction and constraints. This tends to be a completely new skill they need to learn.&lt;/p&gt;
    &lt;p&gt;This does not apply only to leading humans but also to leading AI agents. While AI agents are not humans, and thus leadership will be different in detail, the basic skills and tools needed are the same. This is, BTW, one of the reasons why the people who praise agentic AI on LinkedIn and the like are very often managers who lead (human) teams. For them, leading an AI agent fleet feels very natural because it is very close to the work they do every day. However, for the people currently doing the work, leading an AI agent fleet usually does not feel natural at all.&lt;/p&gt;
    &lt;p&gt;However, I have not yet seen anyone receiving any kind of leadership training before being left alone with a fleet of AI agents, and I still see little discussion about the issue. “If it does not work properly, you need better prompts” is the usual response if someone struggles with directing agents successfully.&lt;/p&gt;
    &lt;p&gt;Sorry, but it is not that easy. The issue is much bigger than just optimizing a few prompts. The issue is that people have to change their approach completely to get any piece of work done. Instead of doing it directly, they need to learn how to get it done indirectly. They need to learn how to direct a group of AI agents effectively, how to lead them.&lt;/p&gt;
    &lt;p&gt;This also adds to the training irony of the previous topic. Maybe the AI agent fleets will become good enough in the future that we can omit the proactive part of the work and only need to focus on the reactive part of the work, the monitor-and-intervene part. But until then, we need to teach human supervisors of AI agent fleets how to lead them effectively.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving on&lt;/head&gt;
    &lt;p&gt;We discussed several ironies and paradoxes from Lisanne Bainbridge’s “The ironies of automation” and how they also apply to agentic AI. We looked at the unlearning and recall dilemma and what it means for the next generation of human supervisors. We discussed monitoring fatigue and the status issue. We looked at the UX and UI deficiencies of current AI agents and the training paradox. And we finally looked at the leadership dilemma, which Lisanne Bainbridge did not discuss in her paper but which complements the training paradox.&lt;/p&gt;
    &lt;p&gt;I would like to conclude with the conclusion of Lisanne Bainbridge:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[…] humans working without time-pressure can be impressive problem solvers. The difficulty remains that they are less effective when under time pressure. I hope this paper has made clear both the irony that one is not by automating necessarily removing the difficulties, and also the possibility that resolving them will require even greater technological ingenuity than does classic automation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I could not agree more.&lt;/p&gt;
    &lt;p&gt;I think over time we will become clear on how much “The ironies of automation” also applies to automation done with AI agents and that we cannot ignore the insights known for more than 40 years meanwhile. I am also really curious how the solutions to the ironies and paradoxes will look like.&lt;/p&gt;
    &lt;p&gt;Until then, I hope I gave you a bit of food for thought to ponder. If you should have some good ideas regarding the ironies and how to address them, please do not hesitate to share them with the community. We learn best by sharing and discussing, and maybe your contribution will be a step towards solving the issues discussed …&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262816</guid><pubDate>Sun, 14 Dec 2025 13:19:15 +0000</pubDate></item><item><title>Apple Maps claims it's 29,905 miles away</title><link>https://mathstodon.xyz/@dpiponi/115651419771418748</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262950</guid><pubDate>Sun, 14 Dec 2025 13:45:41 +0000</pubDate></item></channel></rss>