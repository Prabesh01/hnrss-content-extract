<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 16 Dec 2025 16:14:53 +0000</lastBuildDate><item><title>Quill OS: An open-source OS for Kobo's eReaders</title><link>https://quill-os.org/</link><description>&lt;doc fingerprint="ae45b2563aa66bc8"&gt;
  &lt;main&gt;
    &lt;p&gt;Here are some of Quill OS' features:&lt;/p&gt;
    &lt;p&gt; Fully integrated KoBox X11 subsystem &lt;lb/&gt;ePUB, PDF, picture and plain text display support &lt;lb/&gt;Versatile configuration options for reading &lt;lb/&gt;muPDF rendering engine for ePUBs and PDFs &lt;lb/&gt;Wi-Fi support and web browser &lt;lb/&gt;Encrypted storage with EncFS &lt;lb/&gt;Fast dictionary &amp;amp; local storage search &lt;lb/&gt;Dark mode &lt;lb/&gt;Full factory reset option if needed &lt;lb/&gt;Seamless update process &lt;lb/&gt;VNC viewer app &lt;lb/&gt;Search function &lt;lb/&gt;10 built-in fonts &lt;lb/&gt;Auto-suspend &lt;lb/&gt;Lock screen/passcode &lt;lb/&gt;User-friendly experience &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46283016</guid><pubDate>Tue, 16 Dec 2025 00:22:41 +0000</pubDate></item><item><title>Rollstack (YC W23) is hiring multiple software engineers (TypeScript) US/Canada</title><link>https://www.ycombinator.com/companies/rollstack-2/jobs/QPqpb1n-software-engineer-typescript-us-canada</link><description>&lt;doc fingerprint="f3a0d348861224cf"&gt;
  &lt;main&gt;
    &lt;p&gt;Automate data-driven slide decks and documents with AI&lt;/p&gt;
    &lt;p&gt;At Rollstack, we are revolutionizing the way businesses share and communicate data and insights. Organizations worldwide rely on slide decks and documents to make informed decisions, whether for leadership, clients, or partners. Yet, preparing these materials often consumes countless hours. Rollstack fully automates that.&lt;/p&gt;
    &lt;p&gt;We help some of the world's leading organizations, from mid-sized to public companies like SoFi, Zillow and Whirlpool, in automating their slide decks and documents. Headquartered in New York, we offer a remote-friendly workplace and are backed by Insight Partners and Y Combinator, the most successful startup incubator in the world that produced the likes of Airbnb, Twitch, Instacart, Dropbox, Reddit, Doordash, Stripe, Coinbase, etc.&lt;/p&gt;
    &lt;p&gt;Our team operates with speed and focus to deliver outsized impacts for our customers. We approach every challenge with first principles, never assuming things have to be done a certain way. We are a diverse team that believes intelligence and kindness go hand in hand, welcoming individuals from all backgrounds. Our persistence and rapid execution define us as a category leader and a future generational company.&lt;/p&gt;
    &lt;p&gt;As a Software Engineer at Rollstack, you’ll build core features that automate how companies share data through slides and documents. You’ll work across the stack on integrations, AI insights, and performance optimization. This role is ideal for engineers who thrive on impact, autonomy, and fast-paced product development.&lt;/p&gt;
    &lt;p&gt;At Rollstack, we’re looking for engineers who enjoy iterating, shipping quickly, and solving customers' problems. We want individuals who exhibit a strong sense of ownership and have a get-things-done mentality. Our engineering team defines and drives its technical agenda to continuously iterate on the product and solve our customers' most important problems.&lt;lb/&gt; Our interview process is designed to find these kinds of engineers:&lt;/p&gt;
    &lt;p&gt;Rollstack is solving the last mile problem in the modern data stack and creating a new category: Reports Automation. We connect BI tools to slide decks and documents, automating their generation and updates.&lt;/p&gt;
    &lt;p&gt;We help some of the world's leading organizations—from mid-sized to public companies like SoFi, Zillow and Whirlpool—in automating their slide decks and documents. Headquartered in New York, we offer a remote-friendly workplace and are backed by Insight Partners and Y Combinator, the most successful startup incubator in the world that produced the likes of Airbnb, Twitch, Instacart, Dropbox, Reddit, Doordash, Stripe, Coinbase, etc.&lt;/p&gt;
    &lt;p&gt;Our team operates with speed and focus to deliver outsized impacts for our customers. We approach every challenge with first principles, never assuming things have to be done a certain way. We are a diverse team that believes intelligence and kindness go hand in hand, welcoming individuals from all backgrounds. Our persistence and rapid execution define us as a category leader and a future generational company.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46283750</guid><pubDate>Tue, 16 Dec 2025 01:51:50 +0000</pubDate></item><item><title>8M users' AI conversations sold for profit by "privacy" extensions</title><link>https://www.koi.ai/blog/urban-vpn-browser-extension-ai-conversations-data-collection</link><description>&lt;doc fingerprint="9d53a3a2bd344c8c"&gt;
  &lt;main&gt;
    &lt;p&gt;A few weeks ago, I was wrestling with a major life decision. Like I've grown used to doing, I opened Claude and started thinking out loud-laying out the options, weighing the tradeoffs, asking for perspective.&lt;/p&gt;
    &lt;p&gt;Midway through the conversation, I paused. I realized how much I'd shared: not just this decision, but months of conversations-personal dilemmas, health questions, financial details, work frustrations, things I hadn't told anyone else. I'd developed a level of candor with my AI assistant that I don't have with most people in my life.&lt;/p&gt;
    &lt;p&gt;And then an uncomfortable thought: what if someone was reading all of this?&lt;/p&gt;
    &lt;p&gt;The thought didn't let go. As a security researcher, I have the tools to answer that question.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Discovery&lt;/head&gt;
    &lt;p&gt;We asked Wings, our agentic-AI risk engine, to scan for browser extensions with the capability to read and exfiltrate conversations from AI chat platforms. We expected to find a handful of obscure extensions-low install counts, sketchy publishers, the usual suspects.&lt;/p&gt;
    &lt;p&gt;The results came back with something else entirely.&lt;/p&gt;
    &lt;p&gt;Near the top of the list: Urban VPN Proxy. A Chrome extension with over 6 million users. A 4.7-star rating from 58,000 reviews. A "Featured" badge from Google, meaning it had passed manual review and met what Google describes as "a high standard of user experience and design."&lt;/p&gt;
    &lt;p&gt;A free VPN promising privacy and security. Exactly the kind of tool someone installs when they want to protect themselves online.&lt;/p&gt;
    &lt;p&gt;We decided to look closer.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Found&lt;/head&gt;
    &lt;p&gt;Urban VPN Proxy targets conversations across ten AI platforms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ChatGPT&lt;/item&gt;
      &lt;item&gt;Claude&lt;/item&gt;
      &lt;item&gt;Gemini&lt;/item&gt;
      &lt;item&gt;Microsoft Copilot&lt;/item&gt;
      &lt;item&gt;Perplexity&lt;/item&gt;
      &lt;item&gt;DeepSeek&lt;/item&gt;
      &lt;item&gt;Grok (xAI)&lt;/item&gt;
      &lt;item&gt;Meta AI&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For each platform, the extension includes a dedicated "executor" script designed to intercept and capture conversations. The harvesting is enabled by default through hardcoded flags in the extension's configuration:&lt;/p&gt;
    &lt;p&gt;There is no user-facing toggle to disable this. The only way to stop the data collection is to uninstall the extension entirely.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;The data collection operates independently of the VPN functionality. Whether the VPN is connected or not, the harvesting runs continuously in the background.&lt;/p&gt;
    &lt;p&gt;Here's the technical breakdown:&lt;/p&gt;
    &lt;p&gt;1. Script injection into AI platforms&lt;/p&gt;
    &lt;p&gt;The extension monitors your browser tabs. When you visit any of the targeted AI platforms (ChatGPT, Claude, Gemini, etc.), it injects an "executor" script directly into the page. Each platform has its own dedicated script - chatgpt.js, claude.js, gemini.js, and so on.&lt;/p&gt;
    &lt;p&gt;2. Overriding native browser functions&lt;/p&gt;
    &lt;p&gt;Once injected, the script overrides fetch() and XMLHttpRequest - the fundamental browser APIs that handle all network requests. This is an aggressive technique. The script wraps the original functions so that every network request and response on that page passes through the extension's code first.&lt;/p&gt;
    &lt;p&gt;This means when Claude sends you a response, or when you submit a prompt to ChatGPT, the extension sees the raw API traffic before your browser even renders it.&lt;/p&gt;
    &lt;p&gt;3. Parsing and packaging&lt;/p&gt;
    &lt;p&gt;The injected script parses the intercepted API responses to extract conversation data - your prompts, the AI's responses, timestamps, conversation IDs. This data is packaged and sent via window.postMessage to the extension's content script, tagged with the identifier PANELOS_MESSAGE.&lt;/p&gt;
    &lt;p&gt;4. Exfiltration via background worker&lt;/p&gt;
    &lt;p&gt;The content script forwards the data to the extension's background service worker, which handles the actual exfiltration. The data is compressed and transmitted to Urban VPN's servers at endpoints including analytics.urban-vpn.com and stats.urban-vpn.com.&lt;/p&gt;
    &lt;p&gt;What gets captured:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Every prompt you send to the AI&lt;/item&gt;
      &lt;item&gt;Every response you receive&lt;/item&gt;
      &lt;item&gt;Conversation identifiers and timestamps&lt;/item&gt;
      &lt;item&gt;Session metadata&lt;/item&gt;
      &lt;item&gt;The specific AI platform and model used&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Timeline&lt;/head&gt;
    &lt;p&gt;The AI conversation harvesting wasn't always there. Based on our analysis:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Before version 5.5.0: No AI harvesting functionality&lt;/item&gt;
      &lt;item&gt;July 9, 2025: Version 5.5.0 released with AI harvesting enabled by default&lt;/item&gt;
      &lt;item&gt;July 2025 - Present: All user conversations with targeted AI platforms captured and exfiltrated&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chrome and Edge extensions auto-update by default. Users who installed Urban VPN for its stated purpose - VPN functionality - woke up one day with new code silently harvesting their AI conversations.&lt;/p&gt;
    &lt;p&gt;Anyone who used ChatGPT, Claude, Gemini, or the other targeted platforms while Urban VPN was installed after July 9, 2025 should assume those conversations are now on Urban VPN's servers and have been shared with third parties. Medical questions, financial details, proprietary code, personal dilemmas - all of it, sold for "marketing analytics purposes."&lt;/p&gt;
    &lt;head rend="h2"&gt;What "AI Protection" Actually Does&lt;/head&gt;
    &lt;p&gt;Urban VPN's Chrome Web Store listing promotes "AI protection" as a feature:&lt;/p&gt;
    &lt;p&gt;"Advanced VPN Protection - Our VPN provides added security features to help shield your browsing experience from phishing attempts, malware, intrusive ads and AI protection which checks prompts for personal data (like an email or phone number), checks AI chat responses for suspicious or unsafe links and displays a warning before click or submit your prompt."&lt;/p&gt;
    &lt;p&gt;The framing suggests the AI monitoring exists to protect you-checking for sensitive data you might accidentally share, warning you about suspicious links in responses.&lt;/p&gt;
    &lt;p&gt;The code tells a different story. The data collection and the "protection" notifications operate independently. Enabling or disabling the warning feature has no effect on whether your conversations are captured and exfiltrated. The extension harvests everything regardless.&lt;/p&gt;
    &lt;p&gt;The protection feature shows occasional warnings about sharing sensitive data with AI companies. The harvesting feature sends that exact sensitive data - and everything else - to Urban VPN's own servers, where it's sold to advertisers. The extension warns you about sharing your email with ChatGPT while simultaneously exfiltrating your entire conversation to a data broker.&lt;/p&gt;
    &lt;head rend="h2"&gt;It Gets Worse&lt;/head&gt;
    &lt;p&gt;After documenting Urban VPN Proxy's behavior, we checked whether the same code existed elsewhere.&lt;/p&gt;
    &lt;p&gt;It did. The identical AI harvesting functionality appears in seven other extensions from the same publisher, across both Chrome and Edge:&lt;/p&gt;
    &lt;p&gt;Chrome Web Store:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Urban VPN Proxy - 6,000,000 users&lt;/item&gt;
      &lt;item&gt;1ClickVPN Proxy - 600,000 users&lt;/item&gt;
      &lt;item&gt;Urban Browser Guard - 40,000 users&lt;/item&gt;
      &lt;item&gt;Urban Ad Blocker - 10,000 users&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Microsoft Edge Add-ons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Urban VPN Proxy - 1,323,622 users&lt;/item&gt;
      &lt;item&gt;1ClickVPN Proxy - 36,459 users&lt;/item&gt;
      &lt;item&gt;Urban Browser Guard - 12,624 users&lt;/item&gt;
      &lt;item&gt;Urban Ad Blocker - 6,476 users&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total affected users: Over 8 million.&lt;/p&gt;
    &lt;p&gt;The extensions span different product categories, a VPN, an ad blocker, a "browser guard" security tool, but share the same surveillance backend. Users installing an ad blocker have no reason to expect their Claude conversations are being harvested.&lt;/p&gt;
    &lt;p&gt;All of these extensions carry "Featured" badges from their respective stores, except Urban Ad Blocker for Edge. These badges signal to users that the extensions have been reviewed and meet platform quality standards. For many users, a Featured badge is the difference between installing an extension and passing it by - it's an implicit endorsement from Google and Microsoft.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who's Behind This&lt;/head&gt;
    &lt;p&gt;Urban VPN is operated by Urban Cyber Security Inc., which is affiliated with BiScience (B.I Science (2009) Ltd.), a data broker company.&lt;/p&gt;
    &lt;p&gt;This company has been on researchers' radar before. Security researchers Wladimir Palant and John Tuckner at Secure Annex have previously documented BiScience's data collection practices. Their research established that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BiScience collects clickstream data (browsing history) from millions of users&lt;/item&gt;
      &lt;item&gt;Data is tied to persistent device identifiers, enabling re-identification&lt;/item&gt;
      &lt;item&gt;The company provides an SDK to third-party extension developers to collect and sell user data&lt;/item&gt;
      &lt;item&gt;BiScience sells this data through products like AdClarity and Clickstream OS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our finding represents an expansion of this operation. BiScience has moved from collecting browsing history to harvesting complete AI conversations-a significantly more sensitive category of data.&lt;/p&gt;
    &lt;p&gt;The privacy policy confirms the data flow:&lt;/p&gt;
    &lt;p&gt;"We share the Web Browsing Data with our affiliated company... BiScience that uses this raw data and creates insights which are commercially used and shared with Business Partners"&lt;/p&gt;
    &lt;head rend="h2"&gt;The Disclosure Problem&lt;/head&gt;
    &lt;p&gt;To be fair, Urban VPN does disclose some of this-if you know where to look.&lt;/p&gt;
    &lt;p&gt;The consent prompt (shown during extension setup) mentions that the extension processes "ChatAI communication" along with "pages you visit" and "security signals." It states this is done "to provide these protections."&lt;/p&gt;
    &lt;p&gt;[Screenshot: Urban VPN consent prompt]&lt;/p&gt;
    &lt;p&gt;The privacy policy goes further, buried deep in the document:&lt;/p&gt;
    &lt;p&gt;"AI Inputs and Outputs. As part of the Browsing Data, we will collect the prompts and outputs queried by the End-User or generated by the AI chat provider, as applicable."&lt;/p&gt;
    &lt;p&gt;And:&lt;/p&gt;
    &lt;p&gt;"We also disclose the AI prompts for marketing analytics purposes."&lt;/p&gt;
    &lt;p&gt;However, the Chrome Web Store listing-the place where users actually decide whether to install-shows a different picture:&lt;/p&gt;
    &lt;p&gt;"This developer declares that your data is Not being sold to third parties, outside of the approved use cases"&lt;/p&gt;
    &lt;p&gt;The listing mentions the extension handles "Web history" and "Website content." It says nothing about AI conversations specifically.&lt;/p&gt;
    &lt;p&gt;The contradictions are significant:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The consent prompt frames AI monitoring as protective. The privacy policy reveals the data is sold for marketing.&lt;/item&gt;
      &lt;item&gt;The store listing says data isn't sold to third parties. The privacy policy describes sharing with BiScience, "Business Partners," and use for "marketing analytics."&lt;/item&gt;
      &lt;item&gt;Users who installed before July 2025 never saw the updated consent prompt-the AI harvesting was added via silent update in version 5.5.0.&lt;/item&gt;
      &lt;item&gt;Even users who see the consent prompt have no granular control. You can't accept the VPN but decline the AI harvesting. It's all or nothing.&lt;/item&gt;
      &lt;item&gt;Nothing indicates to users that the data collection continues even when the VPN is disconnected and the AI protection feature is turned off. The harvesting runs silently in the background regardless of what features the user has enabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Google's Role&lt;/head&gt;
    &lt;p&gt;Urban VPN Proxy carries Google's "Featured" badge on the Chrome Web Store. According to Google's documentation:&lt;/p&gt;
    &lt;p&gt;"Featured extensions follow our technical best practices and meet a high standard of user experience and design."&lt;/p&gt;
    &lt;p&gt;"Before it receives a Featured badge, the Chrome Web Store team must review each extension."&lt;/p&gt;
    &lt;p&gt;This means a human at Google reviewed Urban VPN Proxy and concluded it met their standards. Either the review didn't examine the code that harvests conversations from Google's own AI product (Gemini), or it did and didn't consider this a problem.&lt;/p&gt;
    &lt;p&gt;The Chrome Web Store's Limited Use policy explicitly prohibits "transferring or selling user data to third parties like advertising platforms, data brokers, or other information resellers." BiScience is, by its own description, a data broker.&lt;/p&gt;
    &lt;p&gt;The extension remains live and featured as of this writing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;Browser extensions occupy a unique position of trust. They run in the background, have broad access to your browsing activity, and auto-update without asking. When an extension promises privacy and security, users have little reason to suspect it's doing the opposite.&lt;/p&gt;
    &lt;p&gt;What makes this case notable isn't just the scale - 8 million users - or the sensitivity of the data - complete AI conversations. It's that these extensions passed review, earned Featured badges, and remained live for months while harvesting some of the most personal data users generate online. The marketplaces designed to protect users instead gave these extensions their stamp of approval.&lt;/p&gt;
    &lt;p&gt;If you have any of these extensions installed, uninstall them now. Assume any AI conversations you've had since July 2025 have been captured and shared with third parties.&lt;/p&gt;
    &lt;p&gt;This writeup was authored by the research team at Koi.&lt;/p&gt;
    &lt;p&gt;We built Koi to detect exactly these kinds of threats - extensions that slip past marketplace reviews and quietly exfiltrate sensitive data. Our risk engine, Wings, continuously monitors browser extensions to catch threats before they reach your team.&lt;/p&gt;
    &lt;p&gt;Book a demo to see how behavioral analysis catches what static review misses.&lt;/p&gt;
    &lt;p&gt;Stay safe out there.&lt;/p&gt;
    &lt;head rend="h2"&gt;IOCs&lt;/head&gt;
    &lt;p&gt;Chrome:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Urban VPN Proxy: eppiocemhmnlbhjplcgkofciiegomcon&lt;/item&gt;
      &lt;item&gt;Urban Browser Guard: almalgbpmcfpdaopimbdchdliminoign&lt;/item&gt;
      &lt;item&gt;Urban Ad Blocker: feflcgofneboehfdeebcfglbodaceghj&lt;/item&gt;
      &lt;item&gt;1ClickVPN Proxy for Chrome: pphgdbgldlmicfdkhondlafkiomnelnk&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Edge:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Urban VPN Proxy: nimlmejbmnecnaghgmbahmbaddhjbecg&lt;/item&gt;
      &lt;item&gt;Urban Browser Guard: jckkfbfmofganecnnpfndfjifnimpcel&lt;/item&gt;
      &lt;item&gt;Urban Ad Blocker: gcogpdjkkamgkakkjgeefgpcheonclca&lt;/item&gt;
      &lt;item&gt;1ClickVPN Proxy for Edge: deopfbighgnpgfmhjeccdifdmhcjckoe&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46284266</guid><pubDate>Tue, 16 Dec 2025 03:03:49 +0000</pubDate></item><item><title>SHARP, an approach to photorealistic view synthesis from a single image</title><link>https://apple.github.io/ml-sharp/</link><description>&lt;doc fingerprint="b4c1e2802635c4d6"&gt;
  &lt;main&gt;
    &lt;p&gt;Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen,&lt;/p&gt;
    &lt;p&gt;Amaël Delaunoy, Tian Fang, Yanghai Tsin, Stephan R. Richter, Vladlen Koltun&lt;/p&gt;
    &lt;p&gt;Apple&lt;/p&gt;
    &lt;p&gt;We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25–34% and DISTS by 21–43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.&lt;/p&gt;
    &lt;p&gt;SHARP synthesizes a photorealistic 3D representation from a single photograph in less than a second. The synthesized representation supports high-resolution rendering of nearby views, with sharp details and fine structures, at more than 100 frames per second on a standard GPU. We illustrate on photographs from Unsplash.&lt;/p&gt;
    &lt;code&gt;@inproceedings{Sharp2025:arxiv,
  title      = {Sharp Monocular View Synthesis in Less Than a Second},
  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\"{e}l Delaunoyand Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},
  journal    = {arXiv preprint arXiv:2512.10685},
  year       = {2025},
  url        = {https://arxiv.org/abs/2512.10685},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46284658</guid><pubDate>Tue, 16 Dec 2025 04:06:51 +0000</pubDate></item><item><title>Erdős Problem #1026</title><link>https://terrytao.wordpress.com/2025/12/08/the-story-of-erdos-problem-126/</link><description>&lt;doc fingerprint="a098d6150ae7afb1"&gt;
  &lt;main&gt;&lt;p&gt;Problem 1026 on the Erdős problem web site recently got solved through an interesting combination of existing literature, online collaboration, and AI tools. The purpose of this blog post is to try to tell the story of this collaboration, and also to supply a complete proof.&lt;/p&gt;&lt;p&gt;The original problem of Erdős, posed in 1975, is rather ambiguous. Erdős starts by recalling his famous theorem with Szekeres that says that given a sequence of distinct real numbers, one can find a subsequence of length which is either increasing or decreasing; and that one cannot improve the to , by considering for instance a sequence of blocks of length , with the numbers in each block decreasing, but the blocks themselves increasing. He also noted a result of Hanani that every sequence of length can be decomposed into the union of monotone sequences. He then wrote “As far as I know the following question is not yet settled. Let be a sequence of distinct numbers, determine&lt;/p&gt;where the maximum is to be taken over all monotonic sequences “.&lt;p&gt;This problem was added to the Erdős problem site on September 12, 2025, with a note that the problem was rather ambiguous. For any fixed , this is an explicit piecewise linear function of the variables that could be computed by a simple brute force algorithm, but Erdős was presumably seeking optimal bounds for this quantity under some natural constraint on the . The day the problem was posted, Desmond Weisenberg proposed studying the quantity , defined as the largest constant such that&lt;/p&gt;for all choices of (distinct) real numbers . Desmond noted that for this formulation one could assume without loss of generality that the were positive, since deleting negative or vanishing does not decrease the left-hand side and does not increase the right-hand side. By a limiting argument one could also allow collisions between the , so long as one interpreted monotonicity in the weak sense.&lt;p&gt;Though not stated on the web site, one can formulate this problem in game theoretic terms. Suppose that Alice has a stack of coins for some large . She divides the coins into piles of consisting of coins each, so that . She then passes the piles to Bob, who is allowed to select a monotone subsequence of the piles (in the weak sense) and keep all the coins in those piles. What is the largest fraction of the coins that Bob can guarantee to keep, regardless of how Alice divides up the coins? (One can work with either a discrete version of this problem where the are integers, or a continuous one where the coins can be split fractionally, but in the limit the problems can easily be seen to be equivalent.)&lt;/p&gt;&lt;p&gt;AI-generated images continue to be problematic for a number of reasons, but here is one such image that somewhat manages at least to convey the idea of the game:&lt;/p&gt;&lt;p&gt;For small , one can work out by hand. For , clearly : Alice has to put all the coins into one pile, which Bob simply takes. Similarly : regardless of how Alice divides the coins into two piles, the piles will either be increasing or decreasing, so in either case Bob can take both. The first interesting case is . Bob can again always take the two largest piles, guaranteeing himself of the coins. On the other hand, if Alice almost divides the coins evenly, for instance into piles for some small , then Bob cannot take all three piles as they are non-monotone, and so can only take two of them, allowing Alice to limit the payout fraction to be arbitrarily close to . So we conclude that .&lt;/p&gt;&lt;p&gt;An hour after Desmond’s comment, Stijn Cambie noted (though not in the language I used above) that a similar construction to the one above, in which Alice divides the coins into pairs that are almost even, in such a way that the longest monotone sequence is of length , gives the upper bound . It is also easy to see that is a non-increasing function of , so this gives a general bound . Less than an hour after that, Wouter van Doorn noted that the Hanani result mentioned above gives the lower bound , and posed the problem of determining the asymptotic limit of as , given that this was now known to range between and . This version was accepted by Thomas Bloom, the moderator of the Erdős problem site, as a valid interpretation of the original problem.&lt;/p&gt;&lt;p&gt;The next day, Stijn computed the first few values of exactly:&lt;/p&gt;While the general pattern was not yet clear, this was enough data for Stijn to conjecture that , which would also imply that as . (EDIT: as later located by an AI deep research tool, this conjecture was also made in Section 12 of this 1980 article of Steele.) Stijn also described the extremizing sequences for this range of , but did not continue the calculation further (a naive computation would take runtime exponential in , due to the large number of possible subsequences to consider).&lt;p&gt;The problem then lay dormant for almost two months, until December 7, 2025, in which Boris Alexeev, as part of a systematic sweep of the Erdős problems using the AI tool Aristotle, was able to get this tool to autonomously solve this conjecture in the proof assistant language Lean. The proof converted the problem to a rectangle-packing problem.&lt;/p&gt;&lt;p&gt;This was one further addition to a recent sequence of examples where an Erdős problem had been automatically solved in one fashion or another by an AI tool. Like the previous cases, the proof turned out to not be particularly novel. Within an hour, Koishi Chan gave an alternate proof deriving the required bound from the original Erdős-Szekeres theorem by a standard “blow-up” argument which we can give here in the Alice-Bob formulation. Take a large , and replace each pile of coins with new piles, each of size , chosen so that the longest monotone subsequence in this collection is . Among all the new piles, the longest monotone subsequence has length . Applying Erdős-Szekeres, one concludes the bound&lt;/p&gt;and on canceling the ‘s, sending , and applying Cauchy-Schwarz, one obtains (in fact the argument gives for all ).&lt;p&gt;Once this proof was found, it was natural to try to see if it had already appeared in the literature. AI deep research tools have successfully located such prior literature in the past, but in this case they did not succeed, and a more “old-fashioned” Google Scholar job turned up some relevant references: a 2016 paper by Tidor, Wang and Yang contained this precise result, citing an earlier paper of Wagner as inspiration for applying “blowup” to the Erdős-Szekeres theorem.&lt;/p&gt;&lt;p&gt;But the story does not end there! Upon reading the above story the next day, I realized that the problem of estimating was a suitable task for AlphaEvolve, which I have used recently as mentioned in this previous post. Specifically, one could task to obtain upper bounds on by directing it to produce real numbers (or integers) summing up to a fixed sum (I chose ) with a small a value of as possible. After an hour of run time, AlphaEvolve produced the following upper bounds on for , with some intriguingly structured potential extremizing solutions:&lt;/p&gt;The numerical scores (divided by ) were pretty obviously trying to approximate simple rational numbers. There were a variety of ways (including modern AI) to extract the actual rational numbers they were close to, but I searched for a dedicated tool and found this useful little web page of John Cook that did the job: I could not immediately see the pattern here, but after some trial and error in which I tried to align numerators and denominators, I eventually organized this sequence into a more suggestive form: This gave a somewhat complicated but predictable conjecture for the values of the sequence . On posting this, Boris found a clean formulation of the conjecture, namely that whenever and . After a bit of effort, he also produced an explicit upper bound construction:&lt;quote&gt;Proposition 1 If and , then .&lt;/quote&gt;&lt;p&gt;Proof: Consider a sequence of numbers clustered around the “red number” and “blue number” , consisting of blocks of “blue” numbers, followed by blocks of “red” numbers, and then further blocks of “blue” numbers. When , one should take all blocks to be slightly decreasing within each block, but the blue blocks should be are increasing between each other, and the red blocks should also be increasing between each other. When , all of these orderings should be reversed. The total number of elements is indeed&lt;/p&gt;and the total sum is close to With this setup, one can check that any monotone sequence consists either of at most red elements and at most blue elements, or no red elements and at most blue elements, in either case giving a monotone sum that is bounded by either or giving the claim.&lt;p&gt;Here is a figure illustrating the above construction in the case (obtained after starting with a ChatGPT-provided file and then manually fixing a number of placement issues):&lt;/p&gt;&lt;p&gt;Here is a plot of (produced by ChatGPT Pro), showing that it is basically a piecewise linear approximation to the square root function:&lt;/p&gt;&lt;p&gt;Shortly afterwards, Lawrence Wu clarified the connection between this problem and a square packing problem, which was also due to Erdős (Problem 106). Let be the least number such that, whenever one packs squares of sidelength into a square of sidelength , with all sides parallel to the coordinate axes, one has&lt;/p&gt;&lt;quote&gt;Proposition 2 For any , one has&lt;/quote&gt;&lt;p&gt;Proof: Given and , let be the maximal sum over all increasing subsequences ending in , and be the maximal sum over all decreasing subsequences ending in . For , we have either (if ) or (if ). In particular, the squares and are disjoint. These squares pack into the square , so by definition of , we have&lt;/p&gt;and the claim follows.&lt;p&gt;This idea of using packing to prove Erdős-Szekeres type results goes back to a 1959 paper of Seidenberg, although it was a discrete rectangle-packing argument that was not phrased in such an elegantly geometric form. It is possible that Aristotle was “aware” of the Seidenberg argument via its training data, as it had incorporated a version of this argument in its proof.&lt;/p&gt;&lt;p&gt;Here is an illustration of the above argument using the AlphaEvolve-provided example&lt;/p&gt;&lt;p&gt;for to convert it to a square packing (image produced by ChatGPT Pro):&lt;/p&gt;&lt;p&gt;At this point, Lawrence performed another AI deep research search, this time successfully locating a paper from just last year by Baek, Koizumi, and Ueoro, where they show that&lt;/p&gt;&lt;quote&gt;Theorem 3 For any , one has&lt;/quote&gt;&lt;p&gt;which, when combined with a previous argument of Praton, implies&lt;/p&gt;&lt;quote&gt;Theorem 4 For any and with , one has&lt;/quote&gt;&lt;p&gt;This proves the conjecture!&lt;/p&gt;&lt;p&gt;There just remained the issue of putting everything together. I did feed all of the above information into a large language model, which was able to produce a coherent proof of (1) assuming the results of Baek-Koizumi-Ueoro and Praton. Of course, LLM outputs are prone to hallucination, so it would be preferable to formalize that argument in Lean, but this looks quite doable with current tools, and I expect this to be accomplished shortly. But I was also able to reproduce the arguments of Baek-Koizumi-Ueoro and Praton, which I include below for completeness.&lt;/p&gt;&lt;p&gt;Proof: (Proof of Theorem 3, adapted from Baek-Koizumi-Ueoro) We can normalize . It then suffices to show that if we pack the length torus by axis-parallel squares of sidelength , then&lt;/p&gt;&lt;p&gt;Pick . Then we have a grid&lt;/p&gt;inside the torus. The square, when restricted to this grid, becomes a discrete rectangle for some finite sets with By the packing condition, we have From (2) we have hence Inserting this bound and rearranging, we conclude that Taking the supremum over we conclude that so by the pigeonhole principle one of the summands is at most . Let’s say it is the former, thus In particular, the average value of is at most . But this can be computed to be , giving the claim. Similarly if it is the other sum.&lt;p&gt;UPDATE: Actually, the above argument also proves Theorem 4 with only minor modifications. Nevertheless, we give the original derivation of Theorem 4 using the embedding argument of Praton below for sake of completeness.&lt;/p&gt;&lt;p&gt;Proof: (Proof of Theorem 4, adapted from Praton) We write with . We can rescale so that the square one is packing into is . Thus, we pack squares of sidelength into , and our task is to show that&lt;/p&gt;We pick a large natural number (in particular, larger than ), and consider the three nested squares We can pack by unit squares. We can similarly pack into squares of sidelength . All in all, this produces squares, of total length Applying Theorem 3, we conclude that The right-hand side is and the left-hand side similarly evaluates to and so we simplify to Sending , we obtain the claim. One striking feature of this story for me is how important it was to have a diverse set of people, literature, and tools to attack this problem. To be able to state and prove the precise formula for required multiple observations, including some version of the following:&lt;list rend="ul"&gt;&lt;item&gt;The sequence can be numerically computed as a sequence of rational numbers.&lt;/item&gt;&lt;item&gt;When appropriately normalized and arranged, visible patterns in this sequence appear that allow one to conjecture the form of the sequence.&lt;/item&gt;&lt;item&gt;This problem is a weighted version of the Erdős-Szekeres theorem.&lt;/item&gt;&lt;item&gt;Among the many proofs of the Erdős-Szekeres theorem is the proof of Seidenberg in 1959, which can be interpreted as a discrete rectangle packing argument.&lt;/item&gt;&lt;item&gt;This problem can be reinterpreted as a continuous square packing problem, and in fact is closely related to (a generalized axis-parallel form of) Erdős problem 106, which concerns such packings.&lt;/item&gt;&lt;item&gt;The axis-parallel form of Erdős problem 106 was recently solved by Baek-Koizumi-Ueoro.&lt;/item&gt;&lt;item&gt;The paper of Praton shows that Erdős Problem 106 implies the generalized version needed for this problem. This implication specializes to the axis-parallel case.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Another key ingredient was the balanced AI policy on the Erdős problem website, which encourages disclosed AI usage while strongly discouraging undisclosed use. To quote from that policy: “Comments prepared with the assistance of AI are permitted, provided (a) this is disclosed, (b) the contents (including mathematics, code, numerical data, and the existence of relevant sources) have been carefully checked and verified by the user themselves without the assistance of AI, and (c) the comment is not unreasonably long.”&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46284897</guid><pubDate>Tue, 16 Dec 2025 04:49:03 +0000</pubDate></item><item><title>Bonsai: A Voxel Engine, from scratch</title><link>https://github.com/scallyw4g/bonsai</link><description>&lt;doc fingerprint="5a19635f49de9c16"&gt;
  &lt;main&gt;
    &lt;p&gt;Bonsai is a voxel engine in a pot. It's been tended to with love and care over the years. It started out as a learning excercise, and has taught me the value of simplicity.&lt;/p&gt;
    &lt;p&gt;Bonsai supports massive worlds. The current version supports a maximum world size of ~1 billion blocks, cubed. At one block per meter, that's the distance from earth to the moon, 2600 times, in every direction. The view distance is the entire world, all the time. Yes, you read that right. In Bonsai, you can see in a straight line from Jupiter to the sun.&lt;/p&gt;
    &lt;p&gt;Bonsai terrain generation is fully procedural, and user configurable. Terrain is generated on the GPU using regular glsl shaders. Anything you can do in a shader, you can do in a Bonsai terrain generator.&lt;/p&gt;
    &lt;p&gt;The current version is 2.0.0-prealpha-rc0, which can be found by joining the Discord. This version is a large rewrite of several core systems, including the world generation, editor and parts of the renderer.&lt;/p&gt;
    &lt;p&gt;In its current state, the engine is effectively a terrain generator and editor. For details on remaing work, see Roadmap to v2.0.0.&lt;/p&gt;
    &lt;p&gt;Bonsai, and nearly all it's dependencies, are written completely from scratch. One external dependency is the C runtime library for program startup. There is a back-burner task to remove the CRT entirely, athough it's unclear when/if anyone will ever get around to it.&lt;/p&gt;
    &lt;p&gt;The only external requirements to build Bonsai are clang++ (&amp;gt;= version 18.1) and a few appropriate system headers.&lt;/p&gt;
    &lt;p&gt;Grab pre-built binaries &amp;amp; assets from the Latest Releases for your platform of your choice (as long as your platform of choice is Windows or Linux) ;)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deferred Shading&lt;/item&gt;
      &lt;item&gt;HDR Lighting&lt;/item&gt;
      &lt;item&gt;Order-independant Transparency&lt;/item&gt;
      &lt;item&gt;Lighting Bloom&lt;/item&gt;
      &lt;item&gt;Shadow Mapping&lt;/item&gt;
      &lt;item&gt;Screen Space Ambient Occlusion&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hot Shader &amp;amp; Game-code Reloading&lt;/item&gt;
      &lt;item&gt;Async Job System&lt;/item&gt;
      &lt;item&gt;Entities&lt;/item&gt;
      &lt;item&gt;Collision&lt;/item&gt;
      &lt;item&gt;Transparent &amp;amp; Emissive Particles&lt;/item&gt;
      &lt;item&gt;UI Framework&lt;/item&gt;
      &lt;item&gt;Asset Loaders&lt;/item&gt;
      &lt;item&gt;Primitive Physics&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully programmable GPU-based terrain generation&lt;/item&gt;
      &lt;item&gt;Batteries-included library of pre-built terrain shaders&lt;/item&gt;
      &lt;item&gt;1D, 2D and 3D noise library&lt;/item&gt;
      &lt;item&gt;Terrain derivitives available in second-stage terrain "decoration"&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CSG-like SDF world editing&lt;/item&gt;
      &lt;item&gt;Library of primitive shapes (rect, sphere, line, cylinder .. etc)&lt;/item&gt;
      &lt;item&gt;SDF brush-based texturing of primitives&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Layer-based brush GUI&lt;/item&gt;
      &lt;item&gt;(coming soon) glsl brush shaders&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Manual Instrumentation&lt;/item&gt;
      &lt;item&gt;Memory allocation tracking&lt;/item&gt;
      &lt;item&gt;Multithreaded callgraph tracing&lt;/item&gt;
      &lt;item&gt;Context Switches (windows only)&lt;/item&gt;
      &lt;item&gt;Physical Core (windows only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;[ ] HRC : https://github.com/entropylost/amitabha&lt;/p&gt;
    &lt;p&gt;[ ] SSR : https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html&lt;/p&gt;
    &lt;p&gt;[ ] Screen-space lines : https://mattdesl.svbtle.com/drawing-lines-is-hard&lt;/p&gt;
    &lt;p&gt;[ ] Better shadows : https://developer.nvidia.com/gpugems/gpugems3/part-ii-light-and-shadows/chapter-8-summed-area-variance-shadow-maps&lt;/p&gt;
    &lt;p&gt;[ ] Screen Space Shadows : https://panoskarabelas.com/posts/screen_space_shadows/&lt;/p&gt;
    &lt;p&gt;[ ] Motion Blur : https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-27-motion-blur-post-processing-effect&lt;/p&gt;
    &lt;p&gt;[ ] TAA?&lt;/p&gt;
    &lt;p&gt;[ ] FXAA : http://blog.simonrodriguez.fr/articles/2016/07/implementing_fxaa.html&lt;/p&gt;
    &lt;p&gt;[ ] Water : https://www.youtube.com/watch?v=5yhDb9dzJ58&lt;/p&gt;
    &lt;p&gt;[ ] Fluids : https://andrewkchan.dev/posts/fire.html&lt;/p&gt;
    &lt;p&gt;[ ] Remove meshing entirely? https://www.youtube.com/watch?v=4xs66m1Of4A&lt;/p&gt;
    &lt;p&gt;[ ] Lumen-style GI screen-space radiance caching : https://www.youtube.com/watch?v=2GYXuM10riw&lt;/p&gt;
    &lt;p&gt;[ ] Erosion simulation&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://inria.hal.science/hal-01262376/document&lt;/item&gt;
      &lt;item&gt;https://xing-mei.github.io/files/erosion.pdf&lt;/item&gt;
      &lt;item&gt;https://nickmcd.me/2020/04/15/procedural-hydrology/&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;[ ] Biomes&lt;/p&gt;
    &lt;p&gt;[ ] Meshing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Isotropic surface meshing&lt;/item&gt;
      &lt;item&gt;https://graphics.stanford.edu/courses/cs164-10-spring/Handouts/isotropic.pdf&lt;/item&gt;
      &lt;item&gt;https://inria.hal.science/inria-00071612/document&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;[ ] MCA importer&lt;/p&gt;
    &lt;p&gt;[ ] Sound : mp3, ogg, ..? decompresser&lt;/p&gt;
    &lt;p&gt;[ ] Better low-discrepency sequences : https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/&lt;/p&gt;
    &lt;p&gt;[ ] Better disk/sphere sampling patterns : https://extremelearning.com.au/how-to-generate-uniformly-random-points-on-n-spheres-and-n-balls/&lt;/p&gt;
    &lt;p&gt;[ ] Better hash function! : https://nullprogram.com/blog/2018/07/31/&lt;/p&gt;
    &lt;p&gt;[ ] Better GPU hashing! : https://arugl.medium.com/hash-noise-in-gpu-shaders-210188ac3a3e&lt;/p&gt;
    &lt;p&gt;[ ] Hash-trie as alternative to a table : https://nullprogram.com/blog/2023/09/30/&lt;/p&gt;
    &lt;p&gt;[ ] Octree ? https://graphics.tudelft.nl/Publications-new/2020/CBE20/ModifyingCompressedVoxels-main.pdf&lt;/p&gt;
    &lt;p&gt;[ ] Better floating-point rng : https://www.corsix.org/content/higher-quality-random-floats&lt;/p&gt;
    &lt;p&gt;[ ] Better greedy meshing? https://www.youtube.com/watch?v=4xs66m1Of4A&lt;/p&gt;
    &lt;p&gt;[ ] More interpolation goodies : https://paulbourke.net/miscellaneous/interpolation/&lt;/p&gt;
    &lt;p&gt;[ ] Better (faster) Sin/Cos ? https://www.shadertoy.com/view/432yWW&lt;/p&gt;
    &lt;p&gt;[ ] Look into using this Intel tooling for dual CPU/GPU world-gen? https://www.intel.com/content/dam/develop/external/us/en/documents/spir-vtointe-ispcgpu-compute-on-the-cpu.pdf https://ispc.github.io/&lt;/p&gt;
    &lt;p&gt;[ ] Improve the ETW layer : https://github.com/bombomby/optick/blob/master/src/optick_core.win.h&lt;/p&gt;
    &lt;p&gt;[ ] GPU Profiling : https://www.khronos.org/opengl/wiki/Query_Object&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46285319</guid><pubDate>Tue, 16 Dec 2025 06:06:43 +0000</pubDate></item><item><title>Children with cancer scammed out of millions fundraised for their treatment</title><link>https://www.bbc.com/news/articles/ckgz318y8elo</link><description>&lt;doc fingerprint="274d99c5a18985a1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Children with cancer scammed out of millions fundraised for their treatment, BBC finds&lt;/head&gt;
    &lt;p&gt;Warning: Disturbing content&lt;/p&gt;
    &lt;p&gt;A little boy faces the camera. He is pale and has no hair.&lt;/p&gt;
    &lt;p&gt;"I am seven years old and I have cancer," he says. "Please save my life and help me."&lt;/p&gt;
    &lt;p&gt;Khalil - who is pictured above in a still from the film - didn't want to record this, says his mother Aljin. She had been asked to shave his head, and then a film crew hooked him up to a fake drip, and asked his family to pretend it was his birthday. They had given him a script to learn and recite in English.&lt;/p&gt;
    &lt;p&gt;And he didn't like it, says Aljin, when chopped onions were placed next to him, and menthol put under his eyes, to make him cry.&lt;/p&gt;
    &lt;p&gt;Aljin agreed to it because, although the set-up was fake, Khalil really did have cancer. She was told this video would help crowdfund money for better treatment. And it did raise funds - $27,000 (£20,204), according to a campaign we found in Khalil's name.&lt;/p&gt;
    &lt;p&gt;But Aljin was told the campaign had failed, and says she received none of this money - just a $700 (£524) filming fee on the day. One year later, Khalil died.&lt;/p&gt;
    &lt;p&gt;Across the world, desperate parents of sick or dying children are being exploited by online scam campaigns, the BBC World Service has discovered. The public have given money to the campaigns, which claim to be fundraising for life-saving treatment. We have identified 15 families who say they got little to nothing of the funds raised and often had no idea the campaigns had even been published, despite undergoing harrowing filming.&lt;/p&gt;
    &lt;p&gt;Nine families we spoke to - whose campaigns appear to be products of the same scam network - say they never received anything at all of the $4m (£2.9m) apparently raised in their names.&lt;/p&gt;
    &lt;p&gt;A whistleblower from this network told us they had looked for "beautiful children" who "had to be three to nine years old… without hair".&lt;/p&gt;
    &lt;p&gt;We have identified a key player in the scam as an Israeli man living in Canada called Erez Hadari.&lt;/p&gt;
    &lt;p&gt;Our investigation began in October 2023, when a distressing YouTube advert caught our attention. "I don't want to die," a girl called Alexandra from Ghana sobbed. "My treatments cost a lot."&lt;/p&gt;
    &lt;p&gt;A crowdfunding campaign for her appeared to have raised nearly $700,000 (£523,797).&lt;/p&gt;
    &lt;p&gt;We saw more videos of sick children from around the world on YouTube, all strikingly similar - slickly produced, and seemingly having raised huge amounts of money. They all conveyed a sense of urgency, using emotive language.&lt;/p&gt;
    &lt;p&gt;We decided to investigate further.&lt;/p&gt;
    &lt;p&gt;The campaigns with the biggest apparent international reach were under the name of an organisation called Chance Letikva (Chance for Hope, in English) - registered in Israel and the US.&lt;/p&gt;
    &lt;p&gt;Identifying the children featured was difficult. We used geolocation, social media and facial recognition software to find their families, based as far apart as Colombia and the Philippines.&lt;/p&gt;
    &lt;p&gt;While it was difficult to know for sure if the campaign websites' cash totals were genuine, we donated small amounts to two of them and saw the totals increase by those amounts.&lt;/p&gt;
    &lt;p&gt;We also spoke to someone who says she gave $180 (£135) to Alexandra's campaign and was then inundated with requests for more, all written as if sent by Alexandra and her father.&lt;/p&gt;
    &lt;p&gt;In the Philippines, Aljin Tabasa told us her son Khalil had fallen ill just after his seventh birthday.&lt;/p&gt;
    &lt;p&gt;"When we found out it was cancer it felt like my whole world shattered," she says.&lt;/p&gt;
    &lt;p&gt;Aljin says treatment at their local hospital in the city of Cebu was slow, and she had messaged everyone she could think of for help. One person put her in touch with a local businessman called Rhoie Yncierto - who asked for a video of Khalil which, looking back, Aljin realises was essentially an audition.&lt;/p&gt;
    &lt;p&gt;Another man then arrived from Canada in December 2022, introducing himself as "Erez". He paid her the filming fee up front, she says, promising a further $1,500 (£1,122) a month if the film generated lots of donations.&lt;/p&gt;
    &lt;p&gt;Erez directed Khalil's film at a local hospital, asking for retake after retake - the shoot taking 12 hours, Aljin says.&lt;/p&gt;
    &lt;p&gt;Months later, the family say they had still not heard how the video had performed. Aljin messaged Erez, who told her the video "wasn't successful".&lt;/p&gt;
    &lt;p&gt;"So as I understood it, the video just didn't make any money," she says.&lt;/p&gt;
    &lt;p&gt;But we told her the campaign had apparently collected $27,000 (£20,204) as of November 2024, and was still online.&lt;/p&gt;
    &lt;p&gt;"If I had known the money we had raised, I can't help but think that maybe Khalil would still be here," Aljin says. "I don't understand how they could do this to us."&lt;/p&gt;
    &lt;p&gt;When asked about his role in the filming, Rhoie Yncierto denied telling families to shave their children's heads for filming and said he had received no payment for recruiting families.&lt;/p&gt;
    &lt;p&gt;He said he had "no control" over what happened with the funds and had no contact with the families after the day of filming. When we told him they had not received any of the campaigns' donations he said he was "puzzled" and was "very sorry for the families".&lt;/p&gt;
    &lt;p&gt;Nobody named Erez appears on registration documents for Chance Letikva. But two of its campaigns we investigated had also been promoted by another organisation called Walls of Hope, registered in Israel and Canada. Documents list the director in Canada as Erez Hadari.&lt;/p&gt;
    &lt;p&gt;Photos of him online show him at Jewish religious events in the Philippines, New York and Miami. We showed Aljin, and she said it was the same person she had met.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Outside the UK, watch the film on BBC World Service YouTube and listen to World of Secrets here&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We asked Mr Hadari about his involvement in a campaign in the Philippines. He did not respond.&lt;/p&gt;
    &lt;p&gt;We visited further families whose campaigns were either organised by, or linked to, Mr Hadari - one in a remote indigenous community in Colombia, and another in Ukraine.&lt;/p&gt;
    &lt;p&gt;As with Khalil's case, local fixers had got in touch to offer help. The children were filmed and made to cry or fake tears for a nominal fee, but never received any further money.&lt;/p&gt;
    &lt;p&gt;In Sucre, north-west Colombia, Sergio Care says he initially refused this help. He had been approached by someone called Isabel, he says, who offered financial assistance after his eight-year-old daughter, Ana, was diagnosed with a malignant brain tumour.&lt;/p&gt;
    &lt;p&gt;But Isabel came looking for him at the hospital treating Ana, he says, accompanied by a man who said he worked for an international NGO.&lt;/p&gt;
    &lt;p&gt;The description Sergio gave of the man matched that of Erez Hadari - he then recognised him in a photo we showed him.&lt;/p&gt;
    &lt;p&gt;"He gave me hope... I didn't have any money for the future."&lt;/p&gt;
    &lt;p&gt;Demands on the family did not end with the filming.&lt;/p&gt;
    &lt;p&gt;Isabel kept ringing, Sergio says, demanding more photos of Ana in hospital. When Sergio didn't reply, Isabel started messaging Ana herself - voice notes we have heard.&lt;/p&gt;
    &lt;p&gt;Ana told Isabel she had no more photos to send. Isabel replied: "This is very bad Ana, very bad indeed."&lt;/p&gt;
    &lt;p&gt;In January this year, Ana - now fully recovered - tried to find out what happened to the money promised.&lt;/p&gt;
    &lt;p&gt;"That foundation disappeared," Isabel told her in a voice note. "Your video was never uploaded. Never. Nothing was done with it, you hear?"&lt;/p&gt;
    &lt;p&gt;But we could see the video had been uploaded and, by April 2024, appeared to have raised nearly $250,000 (£187,070).&lt;/p&gt;
    &lt;p&gt;In October, we persuaded Isabel Hernandez to speak to us over video link.&lt;/p&gt;
    &lt;p&gt;A friend from Israel, she explained, had introduced her to someone offering work for "a foundation" looking to help children with cancer. She refused to name who she worked for.&lt;/p&gt;
    &lt;p&gt;She was told only one of the campaigns she helped organise was published, she says, and that it had not been successful.&lt;/p&gt;
    &lt;p&gt;We showed Isabel that two campaigns had in fact been uploaded - one of them apparently raising more than $700,000 (£523,797).&lt;/p&gt;
    &lt;p&gt;"I need to apologise to [the families]," she said. "If I'd known what was going on, I would not have been able to do something like this."&lt;/p&gt;
    &lt;p&gt;In Ukraine, we discovered that the person who approached the mother of a sick child was actually employed in the place where the campaign video was filmed.&lt;/p&gt;
    &lt;p&gt;Tetiana Khaliavka organised a shoot with five-year-old Viktoriia, who has brain cancer, at Angelholm Clinic in Chernivtsi.&lt;/p&gt;
    &lt;p&gt;One Facebook post linked to Chance Letikva's campaign shows Viktoriia and her mother Olena Firsova, sitting on a bed. "I see your efforts to save my daughter, and it deeply moves us all. It's a race against time to raise the amount needed for Viktoriia's treatments," reads the caption.&lt;/p&gt;
    &lt;p&gt;Olena says she never wrote or even said these words and had no idea the campaign had been uploaded.&lt;/p&gt;
    &lt;p&gt;It appears to have raised more than €280,000 (£244,000).&lt;/p&gt;
    &lt;p&gt;Tetiana, we were told, was in charge of advertising and communications at Angelholm.&lt;/p&gt;
    &lt;p&gt;The clinic recently told the BBC it didn't approve filming on its premises - adding: "The clinic has never participated in, nor supported, any fundraising initiatives organised by any organisation."&lt;/p&gt;
    &lt;p&gt;Angelholm says it has terminated Tetiana Khaliavka's employment.&lt;/p&gt;
    &lt;p&gt;Olena showed us the contract she had been asked to sign.&lt;/p&gt;
    &lt;p&gt;In addition to the family's $1,500 (£1,122) filming fee on the day, it states they would get $8,000 (£5,986) once the fundraising goal was met. The amount for the goal, however, has been left blank.&lt;/p&gt;
    &lt;p&gt;The contract showed an address in New York for Chance Letikva. On the organisation's website, there is another - in Beit Shemesh, about an hour from Jerusalem. We travelled to both, but found no sign of it.&lt;/p&gt;
    &lt;p&gt;And we discovered Chance Letikva seems to be one of many such organisations.&lt;/p&gt;
    &lt;p&gt;The man who filmed Viktoriia's campaign told our producer - who was posing as a friend of a sick child - that he works for other similar organisations.&lt;/p&gt;
    &lt;p&gt;"Each time, it's a different one," the man - who had introduced himself as "Oleh" - told her. "I hate to put it this way, but they work kind of like a conveyor belt."&lt;/p&gt;
    &lt;p&gt;"About a dozen similar companies" requested "material", he said, naming two of them - Saint Teresa and Little Angels, both registered in the US.&lt;/p&gt;
    &lt;p&gt;When we checked their registration documents, we once again found Erez Hadari's name.&lt;/p&gt;
    &lt;p&gt;What is not clear is where the money raised for the children has gone.&lt;/p&gt;
    &lt;p&gt;More than a year after Viktoriia's filming, her mother Olena rang Oleh, who seems to go by Alex Kohen online, to find out. Shortly afterwards, someone from Chance Letikva called to say the donations had paid for advertising, she says.&lt;/p&gt;
    &lt;p&gt;This is also what Mr Hadari told Aljin, Khalil's mother, when she confronted him over the phone.&lt;/p&gt;
    &lt;p&gt;"There is cost of advertising. So the company lost money," Mr Hadari told her, without giving any evidence to support this.&lt;/p&gt;
    &lt;p&gt;Charity experts told us advertising should not amount to more than 20% of the total raised by campaigns.&lt;/p&gt;
    &lt;p&gt;Someone previously employed to recruit children for Chance Letikva campaigns told us how those featured had been chosen.&lt;/p&gt;
    &lt;p&gt;They had been asked to visit oncology clinics, they said - speaking on condition of anonymity.&lt;/p&gt;
    &lt;p&gt;"They were always looking for beautiful children with white skin. The child had to be three to nine years old. They had to know how to speak well. They had to be without hair," they told us.&lt;/p&gt;
    &lt;p&gt;"They asked me for photos, to see if the child is right, and I would send it to Erez."&lt;/p&gt;
    &lt;p&gt;The whistleblower told us Mr Hadari would then send the photo on to someone else, in Israel, whose name they were never told.&lt;/p&gt;
    &lt;p&gt;As for Mr Hadari himself, we tried to reach him at two addresses in Canada but could not find him. He replied to one voice note we had sent him - asking about the money he had been apparently crowdfunding - by saying the organisation "has never been active", without specifying which one. He did not respond to a further voice note and letter laying out all our questions and allegations.&lt;/p&gt;
    &lt;p&gt;Campaigns set up by Chance Letikva for two children who died - Khalil and a Mexican boy called Hector - still appear to be accepting money.&lt;/p&gt;
    &lt;p&gt;Chance Letikva's US branch appears to be linked to a new organisation called Saint Raphael, which has produced more campaigns - at least two of which seem to have been filmed in Angelholm clinic in Ukraine, as the clinic's distinctive wood panelling and staff uniforms can be seen.&lt;/p&gt;
    &lt;p&gt;Olena, Viktoriia's mother, says her daughter has been diagnosed with another brain tumour. She says she is sickened by the findings of our investigation.&lt;/p&gt;
    &lt;p&gt;"When your child is… hanging on the edge of life, and someone's out there, making money off that. Well, it's filthy. It's blood money."&lt;/p&gt;
    &lt;p&gt;The BBC contacted Tetiana Khaliavka and Alex Kohen, and the organisations Chance Letikva, Walls of Hope, Saint Raphael, Little Angels and Saint Teresa - inviting them to respond to the allegations made against them. None of them replied.&lt;/p&gt;
    &lt;p&gt;The Israeli Corporations Authority, which oversees the country's non-profit organisations, told us that if it has evidence founders are using entities as "a cover for illegal activity", then registration inside Israel may be denied and the founder could be barred from working in the sector.&lt;/p&gt;
    &lt;p&gt;UK regulator, the Charity Commission, advises those wishing to donate to charities to check that those associations are registered, and that the appropriate fundraising regulator should be contacted if in doubt.&lt;/p&gt;
    &lt;p&gt;Additional reporting by: Ned Davies, Tracks Saflor, Jose Antonio Lucio, Almudena Garcia-parrado, Vitaliya Kozmenko, Shakked Auerbach, Tom Tzur Wisfelder, Katya Malofieieva, Anastasia Kucher, Alan Pulido and Neil McCarthy&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have any information to add to this investigation please contact simi@bbc.co.uk&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46285376</guid><pubDate>Tue, 16 Dec 2025 06:17:37 +0000</pubDate></item><item><title>A linear-time alternative for Dimensionality Reduction and fast visualisation</title><link>https://medium.com/@roman.f/a-linear-time-alternative-to-t-sne-for-dimensionality-reduction-and-fast-visualisation-5cd1a7219d6f</link><description>&lt;doc fingerprint="a1a6a01b0b669bba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Linear-Time Alternative To t-SNE for Dimensionality Reduction and Fast Visualisation&lt;/head&gt;
    &lt;p&gt;Moving data visualisation from a Python notebook to a web browser usually demands a painful compromise: you either pay for a heavy GPU backend or you force the user to wait while JavaScript struggles through iterative algorithms.&lt;/p&gt;
    &lt;p&gt;This article explores a third option: Sine Landmark Reduction (SLR).&lt;/p&gt;
    &lt;p&gt;SLR is a deterministic, linear-time alternative to t-SNE designed specifically for the browser. It bypasses the heavy optimisation loops of traditional methods by using trilateration against a fixed topological skeleton. The result? A method fast enough to power Thingbook’s DriftMind stack, capable of mapping 9,000 datapoints (at 50 dimensions) into 3D space in under two seconds.&lt;/p&gt;
    &lt;p&gt;We will cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why t-SNE/UMAP are a poor fit for the browser&lt;/item&gt;
      &lt;item&gt;The idea of landmarks instead of all-pairs distances&lt;/item&gt;
      &lt;item&gt;How to build a synthetic “sine skeleton” in high-D&lt;/item&gt;
      &lt;item&gt;How linearised trilateration turns distances into coordinates&lt;/item&gt;
      &lt;item&gt;Two important refinements: alpha scaling and distance warping&lt;/item&gt;
      &lt;item&gt;A compact Python implementation of SLR you can experiment with today&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why do most dimensionality-reduction techniques fail in resource-limited environments? (such as your browser…)&lt;/head&gt;
    &lt;p&gt;Methods like t-SNE and UMAP are excellent for static, offline exploration, but they are fundamentally unsuited for rapid, iterative visual inspection. During exploratory analysis, many data-driven decisions depend on immediate feedback, insights that guide the next analytical step or help you assess the operational status of highly complex datasets. When the underlying method cannot deliver results interactively due to memory or compute constraints, the entire exploratory workflow breaks down. Several reasons explain this limitation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They rely on iterative optimisation (gradient descent style loops).&lt;/item&gt;
      &lt;item&gt;They typically need to compare many or all points to each other, leading to O(N²) complexity.&lt;/item&gt;
      &lt;item&gt;For 10,000 points, that’s on the order of 100 million pairwise interactions.&lt;/item&gt;
      &lt;item&gt;In a browser, that means dropped frames, frozen UIs, or shipping the entire dataset to a backend.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For an interactive tool where users drag-and-drop a CSV and expect something to appear almost instantly, we need something closer to O(N): one pass over the points, not N passes.&lt;/p&gt;
    &lt;p&gt;This is the design target for SLR:&lt;lb/&gt;linear time, deterministic, analytic, and simple enough to run in plain JavaScript or WebAssembly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Landmarks Instead of All-Pairs&lt;/head&gt;
    &lt;p&gt;The core idea is simple: Instead of comparing every point to every other point, compare every point to a small, fixed set of landmarks. Imagine placing 100 “radio towers” in high-dimensional space (k=50) to cover all the space. To embed a new point x, you don’t ask&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“how far am I from every other point?”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You only ask:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“How far am I from each of these 100 towers?”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If we do this for N points, the work is O(N × k). With k fixed and relatively small, this is effectively O(N).&lt;/p&gt;
    &lt;p&gt;The key questions then become:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;How do we choose good landmarks to place my towers?&lt;/item&gt;
      &lt;item&gt;Given only distances to these landmarks, how do we reconstruct a low-dimensional coordinate?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SLR answers these with:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A sine-based synthetic skeleton or data-derived skeleton&lt;/item&gt;
      &lt;item&gt;A fast, analytic linearised trilateration step inspired by GPS localisation&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Building a Synthetic Sine Skeleton&lt;/head&gt;
    &lt;p&gt;The first way to obtain landmarks is to invent them synthetically, without using any real data points. SLR defines a smooth path through the n-dimensional space using independent sine waves on each dimension:&lt;/p&gt;
    &lt;p&gt;Each coordinate uses its own amplitude, frequency, and phase, drawn from simple uniform distributions. In code:&lt;/p&gt;
    &lt;code&gt;class SineLandmarkReduction:&lt;lb/&gt;    def __init__(self, n_components=2, n_landmarks=50,&lt;lb/&gt;                 random_state=42, synthetic_landmarks=False):&lt;lb/&gt;        self.n_components = n_components&lt;lb/&gt;        self.k = n_landmarks&lt;lb/&gt;        self.synthetic_landmarks = synthetic_landmarks&lt;lb/&gt;        self.rng = np.random.RandomState(random_state)&lt;lb/&gt;&lt;lb/&gt;    def _gamma(self, t, a, omega, phi):&lt;lb/&gt;        """Synthetic sine path function γ(t)."""&lt;lb/&gt;        # result shape: (n_dims, n_landmarks)&lt;lb/&gt;        return a[:, None] * np.sin(omega[:, None] * t + phi[:, None])&lt;/code&gt;
    &lt;p&gt;To build the landmarks:&lt;/p&gt;
    &lt;code&gt;a = self.rng.uniform(0.5, 2.0, n_features)&lt;lb/&gt;omega = self.rng.uniform(0.5, 1.5, n_features)&lt;lb/&gt;phi = self.rng.uniform(0, 2 * np.pi, n_features)&lt;lb/&gt;t = np.linspace(0, 2 * np.pi, self.k)&lt;lb/&gt;&lt;lb/&gt;self.L_high = self._gamma(t, a, omega, phi).T  # shape: (k, n_features)&lt;/code&gt;
    &lt;p&gt;We then sample k points along this path to define the high-dimensional landmarks:&lt;/p&gt;
    &lt;p&gt;Because the curve winds smoothly through the space, the landmarks form a well-spread, continuous loop when projected to 3D subspaces:&lt;/p&gt;
    &lt;p&gt;Why sine waves?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extremely cheap to compute&lt;/item&gt;
      &lt;item&gt;Deterministic given a random seed&lt;/item&gt;
      &lt;item&gt;Naturally explore the space without clustering in arbitrary regions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This mode is ideal when you want a stable, model-driven skeleton that is independent of the dataset.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data-Derived Landmarks and Hybrid Normalisation&lt;/head&gt;
    &lt;p&gt;SLR allows you to skip the automatic skeleton generation and extract the Landmarks from your dataset structure without losing the O(N) complexity. Essentially, this mode lets the landmarks adapt to the data. This is useful when the dataset has a strong cluster structure or an interesting topology that a synthetic curve might miss.&lt;/p&gt;
    &lt;p&gt;The trick is a hybrid normalisation strategy:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Raw selection&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select landmark indices from the unnormalised data.&lt;/item&gt;
      &lt;item&gt;High-variance features dominate cluster structure; staying in raw scale helps us capture that.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Normalised computation&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;After selecting the landmarks, normalise both X and the selected landmarks (e.g. StandardScaler).&lt;/item&gt;
      &lt;item&gt;All features then contribute fairly to Euclidean distances during trilateration.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the implementation:&lt;/p&gt;
    &lt;code&gt;if self.synthetic_landmarks:&lt;lb/&gt;    # synthetic branch (see previous section)&lt;lb/&gt;    ...&lt;lb/&gt;else:&lt;lb/&gt;    # 1. Select k landmark indices from raw data&lt;lb/&gt;    idx = self.rng.choice(n_samples, size=self.k, replace=False)&lt;lb/&gt;    self.L_high = X[idx].copy()  # raw landmarks&lt;lb/&gt;&lt;lb/&gt;    # 2. Fit PCA skeleton on raw landmarks&lt;lb/&gt;    L_low_raw = pca.fit_transform(self.L_high)&lt;lb/&gt;&lt;lb/&gt;    # 3. Normalise X and landmarks for distance computations&lt;lb/&gt;    X_scaled = scaler.fit_transform(X)&lt;lb/&gt;    self.L_high = scaler.transform(self.L_high)&lt;/code&gt;
    &lt;p&gt;We now have high-D landmarks &lt;code&gt;L ∈ R^(k×n)&lt;/code&gt;. To embed everything in e.g. 2D or 3D, we first map the landmarks themselves to a low dimension using PCA:&lt;/p&gt;
    &lt;p&gt;1. Centre the Data: Centre the landmark matrix &lt;code&gt;L&lt;/code&gt; to obtain &lt;code&gt;L_bar&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;2. Compute Covariance: Compute the covariance matrix &lt;code&gt;Σ&lt;/code&gt;: &lt;code&gt;Σ = (1 / k-1) · L_barᵀ · L_bar&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;3. Form the Projection Matrix: Take the top &lt;code&gt;m&lt;/code&gt; eigenvectors to form the projection matrix &lt;code&gt;Wm&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;4. Project to Low Dimensions: Obtain low-D landmark coordinates &lt;code&gt;L’_raw&lt;/code&gt;: &lt;code&gt;L’_raw = L_bar · Wm&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Because the distance computations later are done in the normalised feature space, we match scales using an RMS ratio:&lt;/p&gt;
    &lt;code&gt;rms_high = np.sqrt(np.mean([&lt;lb/&gt;    np.linalg.norm(self.L_high[i] - self.L_high[j])**2&lt;lb/&gt;    for i in range(self.k) for j in range(i+1, self.k)&lt;lb/&gt;]))&lt;lb/&gt;rms_low = np.sqrt(np.mean([&lt;lb/&gt;    np.linalg.norm(L_low_raw[i] - L_low_raw[j])**2&lt;lb/&gt;    for i in range(self.k) for j in range(i+1, self.k)&lt;lb/&gt;]))&lt;lb/&gt;&lt;lb/&gt;self.L_low = L_low_raw * (rms_high / rms_low)&lt;/code&gt;
    &lt;p&gt;This ensures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The shape of the low-D skeleton matches the raw data geometry (for data-derived landmarks).&lt;/item&gt;
      &lt;item&gt;The scale of the skeleton is compatible with the normalised distance metric.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Linearised Trilateration: The GPS Analogy&lt;/head&gt;
    &lt;p&gt;With landmarks defined both in high-D and low-D, the core embedding step becomes:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Given a point&lt;/p&gt;&lt;code&gt;x&lt;/code&gt;, find a low-D point&lt;code&gt;y&lt;/code&gt;whose distances to the low-D landmarks match the high-D distances from&lt;code&gt;x&lt;/code&gt;to the high-D landmarks.&lt;/quote&gt;
    &lt;p&gt;Let:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;L_j&lt;/code&gt;: The high-D landmarks (after scaling).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;L''_j&lt;/code&gt;: The low-D landmarks (&lt;code&gt;self.L_low&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;δ²_j&lt;/code&gt;: The squared distance from&lt;code&gt;x&lt;/code&gt;to landmark&lt;code&gt;j&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We ideally want:&lt;/p&gt;
    &lt;p&gt;This is a classic trilateration problem (think GPS). Naively, it is non-linear in y. SLR’s key trick is to linearise it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get Roman Ferrando’s stories in your inbox&lt;/head&gt;
    &lt;p&gt;Join Medium for free to get updates from this writer.&lt;/p&gt;
    &lt;p&gt;Take the equation for landmark 0 and subtract it from each equation j:&lt;/p&gt;
    &lt;p&gt;Expanding and cancelling the &lt;code&gt;yᵀy&lt;/code&gt; terms give a linear system:&lt;/p&gt;
    &lt;p&gt;Stacking these for &lt;code&gt;j = 1…k-1&lt;/code&gt; yields:&lt;/p&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;A&lt;/code&gt;depends only on low-D landmarks.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b&lt;/code&gt;depends on the measured distances&lt;code&gt;δ²_j&lt;/code&gt;for a given&lt;code&gt;x&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In code, we precompute &lt;code&gt;A&lt;/code&gt; and its pseudoinverse once:&lt;/p&gt;
    &lt;code&gt;# Precompute solver&lt;lb/&gt;self.L0_low = self.L_low[0]&lt;lb/&gt;self.A = 2 * (self.L_low[1:] - self.L0_low)   # shape: (k-1, m)&lt;lb/&gt;self.A_pinv = np.linalg.pinv(self.A)          # Moore–Penrose pseudoinverse&lt;lb/&gt;self.L_low_sq_norms = np.sum(self.L_low**2, axis=1)&lt;/code&gt;
    &lt;p&gt;Then, for a batch of points X:&lt;/p&gt;
    &lt;code&gt;# Squared distances in high-D&lt;lb/&gt;diff = X_scaled[:, np.newaxis, :] - self.L_high[np.newaxis, :, :]&lt;lb/&gt;delta_sq = np.sum(diff**2, axis=2)  # shape: (n_samples, k)&lt;lb/&gt;&lt;lb/&gt;# Right-hand side b for all points&lt;lb/&gt;term1 = self.L_low_sq_norms[1:] - self.L_low_sq_norms[0]      # shape: (k-1,)&lt;lb/&gt;term2 = delta_sq[:, 1:] - delta_sq[:, 0:1]                    # broadcast&lt;lb/&gt;b = term1 - term2                                             # (n_samples, k-1)&lt;lb/&gt;&lt;lb/&gt;# Initial low-D coordinates (first pass)&lt;lb/&gt;Y_raw = b @ self.A_pinv.T                                     # (n_samples, m)&lt;/code&gt;
    &lt;p&gt;Because A is fixed, embedding a new point is just:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute k squared distances.&lt;/item&gt;
      &lt;item&gt;Build b.&lt;/item&gt;
      &lt;item&gt;One matrix multiplication.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is exactly the type of workload that scales linearly with N and runs comfortably in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alpha Refinement: Fixing Global Scale&lt;/head&gt;
    &lt;p&gt;In practice, we do a two-pass mapping. The first pass gives Y_raw, but some global scale mismatch can remain between high-D and low-D distances.&lt;/p&gt;
    &lt;p&gt;SLR introduces a global scalar α that best aligns the two:&lt;/p&gt;
    &lt;p&gt;In code:&lt;/p&gt;
    &lt;code&gt;# Squared distances in high-D&lt;lb/&gt;diff = X_scaled[:, np.newaxis, :] - self.L_high[np.newaxis, :, :]&lt;lb/&gt;delta_sq = np.sum(diff**2, axis=2)  # shape: (n_samples, k)&lt;lb/&gt;&lt;lb/&gt;# Right-hand side b for all points&lt;lb/&gt;term1 = self.L_low_sq_norms[1:] - self.L_low_sq_norms[0]      # shape: (k-1,)&lt;lb/&gt;term2 = delta_sq[:, 1:] - delta_sq[:, 0:1]                    # broadcast&lt;lb/&gt;b = term1 - term2                                             # (n_samples, k-1)&lt;lb/&gt;&lt;lb/&gt;# Initial low-D coordinates (first pass)&lt;lb/&gt;Y_raw = b @ self.A_pinv.T                                     # (n_samples, m)&lt;/code&gt;
    &lt;p&gt;We then rescale the high-D distances and solve again:&lt;/p&gt;
    &lt;code&gt;delta_sq_corrected = (alpha**2) * delta_sq&lt;lb/&gt;&lt;lb/&gt;term2_corr = delta_sq_corrected[:, 1:] - delta_sq_corrected[:, 0:1]&lt;lb/&gt;b_corr = term1 - term2_corr&lt;lb/&gt;&lt;lb/&gt;Y_final = b_corr @ self.A_pinv.T&lt;/code&gt;
    &lt;p&gt;This simple, non-iterative correction significantly improves embedding quality while keeping the whole procedure analytic and fast.&lt;/p&gt;
    &lt;head rend="h2"&gt;Distance Warping: Tuning Locality vs Global Geometry&lt;/head&gt;
    &lt;p&gt;t-SNE is popular because it exaggerates local structure: clusters become very tight and well separated. Pure trilateration, on the other hand, preserves global geometry more faithfully.&lt;/p&gt;
    &lt;p&gt;SLR adds a knob to interpolate between these behaviours via distance warping:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;p = 1.0&lt;/code&gt;→ pure global geometry&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;p ≈ 0.5&lt;/code&gt;→ stronger local neighbourhoods&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;p ≈ 0.33&lt;/code&gt;→ visually similar separation to t-SNE&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the reference implementation, the warp is applied right after computing distances:&lt;/p&gt;
    &lt;code&gt;# delta_sq shape: (k,)&lt;lb/&gt;delta_sq = np.sum((x - self.L_high)**2, axis=1)&lt;lb/&gt;&lt;lb/&gt;# Nonlinear locality warp&lt;lb/&gt;delta = np.sqrt(delta_sq)   # Euclidean distances&lt;lb/&gt;p = 0.5                     # try 0.5; smaller p → stronger locality&lt;lb/&gt;delta = delta ** p&lt;lb/&gt;delta_sq = delta ** 2&lt;/code&gt;
    &lt;p&gt;This gives you t-SNE-like cluster separation while preserving the deterministic, analytic nature of SLR.&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting It Together: The &lt;code&gt;fit_transform&lt;/code&gt; Pipeline&lt;/head&gt;
    &lt;p&gt;The full &lt;code&gt;fit_transform&lt;/code&gt; method ties everything together: scaling, landmark construction, PCA, trilateration, alpha refinement, and final embedding.&lt;/p&gt;
    &lt;p&gt;Here is a condensed view (non-essential boilerplate omitted):&lt;/p&gt;
    &lt;code&gt;def fit_transform(self, X):&lt;lb/&gt;    scaler = StandardScaler()&lt;lb/&gt;    n_samples, n_features = X.shape&lt;lb/&gt;    pca = PCA(n_components=self.n_components)&lt;lb/&gt;&lt;lb/&gt;    if self.synthetic_landmarks:&lt;lb/&gt;        # Synthetic sine landmarks&lt;lb/&gt;        a = self.rng.uniform(0.5, 2.0, n_features)&lt;lb/&gt;        omega = self.rng.uniform(0.5, 1.5, n_features)&lt;lb/&gt;        phi = self.rng.uniform(0, 2 * np.pi, n_features)&lt;lb/&gt;        t = np.linspace(0, 2 * np.pi, self.k)&lt;lb/&gt;        self.L_high = self._gamma(t, a, omega, phi).T&lt;lb/&gt;        L_low_raw = pca.fit_transform(self.L_high)&lt;lb/&gt;        X_scaled = scaler.fit_transform(X)&lt;lb/&gt;    else:&lt;lb/&gt;        # Data-derived landmarks&lt;lb/&gt;        idx = self.rng.choice(n_samples, size=self.k, replace=False)&lt;lb/&gt;        self.L_high = X[idx].copy()&lt;lb/&gt;        L_low_raw = pca.fit_transform(self.L_high)&lt;lb/&gt;        X_scaled = scaler.fit_transform(X)&lt;lb/&gt;        self.L_high = scaler.transform(self.L_high)&lt;lb/&gt;&lt;lb/&gt;    # RMS scaling of low-D skeleton&lt;lb/&gt;    ...  # (rms_high / rms_low scaling as shown earlier)&lt;lb/&gt;&lt;lb/&gt;    # Precompute trilateration solver&lt;lb/&gt;    self.L0_low = self.L_low[0]&lt;lb/&gt;    self.A = 2 * (self.L_low[1:] - self.L0_low)&lt;lb/&gt;    self.A_pinv = np.linalg.pinv(self.A)&lt;lb/&gt;    self.L_low_sq_norms = np.sum(self.L_low**2, axis=1)&lt;lb/&gt;&lt;lb/&gt;    # Vectorised first pass (Y_raw)&lt;lb/&gt;    diff = X_scaled[:, np.newaxis, :] - self.L_high[np.newaxis, :, :]&lt;lb/&gt;    delta_sq = np.sum(diff**2, axis=2)&lt;lb/&gt;    term1 = self.L_low_sq_norms[1:] - self.L_low_sq_norms[0]&lt;lb/&gt;    term2 = delta_sq[:, 1:] - delta_sq[:, 0:1]&lt;lb/&gt;    b = term1 - term2&lt;lb/&gt;    Y_raw = b @ self.A_pinv.T&lt;lb/&gt;&lt;lb/&gt;    # Alpha refinement and second pass&lt;lb/&gt;    diff_low = Y_raw[:, np.newaxis, :] - self.L_low[np.newaxis, :, :]&lt;lb/&gt;    dist_low = np.linalg.norm(diff_low, axis=2)&lt;lb/&gt;    dist_high = np.sqrt(delta_sq)&lt;lb/&gt;    alpha = np.sum(dist_low * dist_high) / np.sum(delta_sq)&lt;lb/&gt;&lt;lb/&gt;    delta_sq_corrected = (alpha**2) * delta_sq&lt;lb/&gt;    term2_corr = delta_sq_corrected[:, 1:] - delta_sq_corrected[:, 0:1]&lt;lb/&gt;    b_corr = term1 - term2_corr&lt;lb/&gt;    Y_final = b_corr @ self.A_pinv.T&lt;lb/&gt;&lt;lb/&gt;    return Y_final&lt;/code&gt;
    &lt;p&gt;From the browser’s perspective, once the landmarks and pseudoinverse are precomputed, embedding new points is just distance computations + one matrix multiply.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: SLR vs t-SNE on a 5-Cluster Dataset&lt;/head&gt;
    &lt;p&gt;To evaluate SLR against a familiar baseline, consider 5 Gaussian clusters in a 20-dimensional space (5,000 points). Using SLR with appropriate p:&lt;/p&gt;
    &lt;p&gt;Using t-SNE on the same data:&lt;/p&gt;
    &lt;p&gt;You get a flavour of the trade-off:&lt;/p&gt;
    &lt;p&gt;t-SNE&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strong local separation&lt;/item&gt;
      &lt;item&gt;Poor global interpretability&lt;/item&gt;
      &lt;item&gt;Stochastic, run-to-run variability&lt;/item&gt;
      &lt;item&gt;Iterative, quadratic, and slower&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SLR&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deterministic layouts&lt;/item&gt;
      &lt;item&gt;Preserved global structure&lt;/item&gt;
      &lt;item&gt;Tunable locality via p&lt;/item&gt;
      &lt;item&gt;Linear time, analytic, out-of-sample mapping&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A concise comparison:&lt;/p&gt;
    &lt;head rend="h2"&gt;Minimal Usage Example&lt;/head&gt;
    &lt;p&gt;Here is a minimal example showing how to use &lt;code&gt;SineLandmarkReduction&lt;/code&gt; on a synthetic dataset and plot the result:&lt;/p&gt;
    &lt;code&gt;import numpy as np&lt;lb/&gt;import matplotlib.pyplot as plt&lt;lb/&gt;from sklearn.datasets import make_blobs&lt;lb/&gt;&lt;lb/&gt;X, y = make_blobs(&lt;lb/&gt;    n_samples=5000,&lt;lb/&gt;    n_features=20,&lt;lb/&gt;    centers=5,&lt;lb/&gt;    cluster_std=0.80,&lt;lb/&gt;    random_state=42&lt;lb/&gt;)&lt;lb/&gt;&lt;lb/&gt;slr = SineLandmarkReduction(&lt;lb/&gt;    n_components=2,&lt;lb/&gt;    n_landmarks=50,&lt;lb/&gt;    random_state=42,&lt;lb/&gt;    synthetic_landmarks=False   # or True, depending on your use case&lt;lb/&gt;)&lt;lb/&gt;&lt;lb/&gt;Y = slr.fit_transform(X)&lt;lb/&gt;&lt;lb/&gt;plt.figure(figsize=(7, 6))&lt;lb/&gt;scatter = plt.scatter(Y[:, 0], Y[:, 1], c=y, s=8, alpha=0.8)&lt;lb/&gt;plt.xlabel("SLR Dimension 1")&lt;lb/&gt;plt.ylabel("SLR Dimension 2")&lt;lb/&gt;plt.title("Sine Landmark Reduction (SLR) Visualization")&lt;lb/&gt;plt.colorbar(scatter, label="Cluster Label")&lt;lb/&gt;plt.show()&lt;/code&gt;
    &lt;p&gt;Replace the synthetic dataset with your own feature matrix and you have a ready-to-use, deterministic embedding.&lt;/p&gt;
    &lt;p&gt;In a browser deployment, the same ideas can be ported to JavaScript or WebAssembly with minimal changes: the algorithm itself only needs basic linear algebra and a pseudoinverse.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thoughts&lt;/head&gt;
    &lt;p&gt;Sine Landmark Reduction (SLR) is designed from first principles for environments where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Latency matters (interactive drag-and-drop exploration).&lt;/item&gt;
      &lt;item&gt;Resources are constrained (no GPU, no heavy backend).&lt;/item&gt;
      &lt;item&gt;Reproducibility is a feature (deterministic embeddings across runs).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Constructing a synthetic or data-driven landmark skeleton,&lt;/item&gt;
      &lt;item&gt;Projecting it via PCA with RMS scaling,&lt;/item&gt;
      &lt;item&gt;Using linearised trilateration for an analytic solution,&lt;/item&gt;
      &lt;item&gt;Adding alpha refinement for scale consistency, and&lt;/item&gt;
      &lt;item&gt;Exposing distance warping as a control for locality,&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SLR offers a fast, deterministic, and interpretable alternative to t-SNE/UMAP in browser-native contexts.&lt;/p&gt;
    &lt;p&gt;It is already powering Thingbook’s interactive Data Explorer, but the underlying idea is more general: if you can formalise your embedding problem in terms of distances to a small set of stable landmarks, you can achieve real-time dimensionality reduction with simple linear algebra.&lt;/p&gt;
    &lt;p&gt;If you want to experiment further, the Python implementation shown here can be dropped into your own notebooks or adapted to a JavaScript/WebAssembly stack to enable SLR directly in your web applications.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46285535</guid><pubDate>Tue, 16 Dec 2025 06:47:09 +0000</pubDate></item><item><title>A2UI: A Protocol for Agent-Driven Interfaces</title><link>https://a2ui.org/</link><description>&lt;doc fingerprint="a5f86c52e98ba4fa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Protocol for Agent-Driven Interfaces¶&lt;/head&gt;
    &lt;p&gt;A2UI enables AI agents to generate rich, interactive user interfaces that render natively across web, mobile, and desktop—without executing arbitrary code.&lt;/p&gt;
    &lt;p&gt;️Status: Early Stage Public Preview&lt;/p&gt;
    &lt;p&gt;A2UI is currently in v0.8 (Public Preview). The specification and implementations are functional but are still evolving. We are opening the project to foster collaboration, gather feedback, and solicit contributions (e.g., on client renderers). Expect changes.&lt;/p&gt;
    &lt;head rend="h2"&gt;At a Glance¶&lt;/head&gt;
    &lt;p&gt;A2UI is currently v0.8, Apache 2.0 licensed, created by Google with contributions from CopilotKit and the open source community, and is in active development on GitHub.&lt;/p&gt;
    &lt;p&gt;The problem A2UI solves is: how can AI agents safely send rich UIs across trust boundaries?&lt;/p&gt;
    &lt;p&gt;Instead of text-only responses or risky code execution, A2UI lets agents send declarative component descriptions that clients render using their own native widgets. It's like having agents speak a universal UI language.&lt;/p&gt;
    &lt;p&gt;In this repo you will find A2UI specifications and implementations for renderers (eg: Angular, Flutter, etc.) on the client side, and transports (eg: A2A, etc.) which communicate A2UI messages between agents and clients.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Secure by Design&lt;/p&gt;
        &lt;p&gt;Declarative data format, not executable code. Agents can only use pre-approved components from your catalog—no UI injection attacks.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LLM-Friendly&lt;/p&gt;
        &lt;p&gt;Flat, streaming JSON structure designed for easy generation. LLMs can build UIs incrementally without perfect JSON in one shot.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Framework-Agnostic&lt;/p&gt;
        &lt;p&gt;One agent response works everywhere. Render the same UI on Angular, Flutter, React, or native mobile with your own styled components.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Progressive Rendering&lt;/p&gt;
        &lt;p&gt;Stream UI updates as they're generated. Users see the interface building in real-time instead of waiting for complete responses.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Get Started in 5 Minutes¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Run the restaurant finder demo and see A2UI in action with Gemini-powered agents.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Understand surfaces, components, data binding, and the adjacency list model.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Integrate A2UI renderers into your app or build agents that generate UIs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dive into the complete technical specification and message types.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How It Works¶&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;User sends a message to an AI agent&lt;/item&gt;
      &lt;item&gt;Agent generates A2UI messages describing the UI (structure + data)&lt;/item&gt;
      &lt;item&gt;Messages stream to the client application&lt;/item&gt;
      &lt;item&gt;Client renders using native components (Angular, Flutter, React, etc.)&lt;/item&gt;
      &lt;item&gt;User interacts with the UI, sending actions back to the agent&lt;/item&gt;
      &lt;item&gt;Agent responds with updated A2UI messages&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;A2UI in Action¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Landscape Architect Demo¶&lt;/head&gt;
    &lt;p&gt;Watch an agent generate all of the interfaces for a landscape architect application. The user uploads a photo; the agent uses Gemini to understand it and generate a custom form for landscaping needs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Custom Components: Interactive Charts &amp;amp; Maps¶&lt;/head&gt;
    &lt;p&gt;Watch an agent chose to respond with a chart component to answer a numberical summary quesiton. Then the agent chooses a Google Map component to answer a location question. Both are custom components offered by the client.&lt;/p&gt;
    &lt;head rend="h3"&gt;A2UI Composer¶&lt;/head&gt;
    &lt;p&gt;CopilotKit has a public A2UI Widget Builder to try out as well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46286407</guid><pubDate>Tue, 16 Dec 2025 09:16:31 +0000</pubDate></item><item><title>ArkhamMirror: Airgapped investigation platform with CIA-style hypothesis testing</title><link>https://github.com/mantisfury/ArkhamMirror</link><description>&lt;doc fingerprint="7fae250844a8d062"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Connect the dots without connecting to the cloud.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ArkhamMirror is an air-gapped, AI-powered investigation platform for journalists and researchers. It runs 100% locally on your machine, turning chaos into order using advanced NLP, Vision AI, and Knowledge Graphs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;🕵️ Local AI&lt;/cell&gt;
        &lt;cell&gt;Chat with your data using Offline RAG (Retrieval-Augmented Generation).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;🔍 Semantic Search&lt;/cell&gt;
        &lt;cell&gt;Find documents by concept, not just exact keywords.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;🕸️ Knowledge Graph&lt;/cell&gt;
        &lt;cell&gt;Visualize hidden connections between People, Orgs, and Places.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;⏳ Auto-Timeline&lt;/cell&gt;
        &lt;cell&gt;Extract dates and events to reconstruct what happened when.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;📊 Visual Table Extraction&lt;/cell&gt;
        &lt;cell&gt;Recover complex financial tables from PDFs/Images using Vision models.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Automatically flag conflicting statements across documents.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;🔒 Absolute Privacy&lt;/cell&gt;
        &lt;cell&gt;Zero cloud dependencies. Your data never leaves your specialized "Data Silo".&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;ArkhamMirror includes a Smart Installer that sets up Python, Docker, and Database dependencies for you.&lt;/p&gt;
    &lt;p&gt;Double-click &lt;code&gt;setup.bat&lt;/code&gt; and follow the AI Setup Wizard.&lt;/p&gt;
    &lt;code&gt;chmod +x setup.sh
./setup.sh&lt;/code&gt;
    &lt;p&gt;Detailed guides for features and workflows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;User Guide: Full walkthrough of features.&lt;/item&gt;
      &lt;item&gt;Installation: Detailed setup instructions.&lt;/item&gt;
      &lt;item&gt;Developer Guide: Architecture and contributing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Narrative Reconstruction&lt;/cell&gt;
        &lt;cell role="head"&gt;Gap Finding&lt;/cell&gt;
        &lt;cell role="head"&gt;Contradiction Chain&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Entity Graph&lt;/cell&gt;
        &lt;cell role="head"&gt;Author Unmasking&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This tool was born from a desire to give journalists powerful forensics without the monthly subscription costs or privacy risks of cloud platforms.&lt;/p&gt;
    &lt;p&gt;If it helps you uncover the truth, consider buying me a coffee!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46286666</guid><pubDate>Tue, 16 Dec 2025 09:51:31 +0000</pubDate></item><item><title>Cekura (YC F24) Is Hiring</title><link>https://www.ycombinator.com/companies/cekura-ai/jobs/YFeQADI-product-engineer-us</link><description>&lt;doc fingerprint="ddc9e9532991a347"&gt;
  &lt;main&gt;
    &lt;p&gt;Voice AI and Chat AI agents: Testing and Observability&lt;/p&gt;
    &lt;p&gt;Cekura (YC F24) is one of the fastest-growing companies in its batch, with strong revenue traction. We’re well-funded, backed by premier investors, and have years of runway.&lt;/p&gt;
    &lt;p&gt;We’re building the reliability layer for Conversational Agents. Teams use Cekura to simulate and monitor their AI agents end-to-end - measuring latency, barge-in, instruction-following, regressions, and more across phone, chat, SMS, and web. Customers love the product - and we’re just getting started.&lt;/p&gt;
    &lt;p&gt;You’re joining at an inflection point. As Product Engineer, you’ll build the playbooks, processes, and relationships that define how Cekura partners with technical customers for long-term success. You’ll be both strategist and hands-on operator.&lt;/p&gt;
    &lt;p&gt;Excited to help world-class teams ship reliable AI agents - and wear both the customer and engineer hats? Let’s talk.&lt;/p&gt;
    &lt;p&gt;Cekura is a Y Combinator–backed startup redefining AI voice agent reliability. Founded by IIT Bombay alumni with research credentials from ETH Zurich and proven success in high-stakes trading, our team built Cekura to solve the cumbersome, error-prone nature of manual voice agent testing.&lt;/p&gt;
    &lt;p&gt;We automate the testing and observability of AI voice agents by simulating thousands of realistic, real-world conversational scenarios—from ordering food and booking appointments to conducting interviews. Our platform leverages custom and AI-generated datasets, detailed workflows, and dynamic persona simulations to uncover edge cases and deliver actionable insights. Real-time monitoring, comprehensive logs, and instant alerting ensure that every call is optimized and production-ready.&lt;/p&gt;
    &lt;p&gt;In a market rapidly expanding with thousands of voice agents, Cekura stands out by guaranteeing dependable performance, reducing time-to-market, and minimizing costly production errors. We empower teams to demonstrate reliability before deployment, making it easier to build trust with clients and users.&lt;/p&gt;
    &lt;p&gt;Join us in shaping the future of voice technology. Learn more at cekura.ai.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46287521</guid><pubDate>Tue, 16 Dec 2025 12:01:55 +0000</pubDate></item><item><title>Sega Channel: VGHF Recovers over 100 Sega Channel ROMs (and More)</title><link>https://gamehistory.org/segachannel/</link><description>&lt;doc fingerprint="2f3cfb35e3e384c9"&gt;
  &lt;main&gt;
    &lt;p&gt;Sega broke ground in the late 90s with one of the first digital game distribution systems for consoles. Sega Channel offered access to a rotating library of Sega Genesis titles, along with game tips, demos, and even a few exclusive games that never came out in the United States in any other format. In an era of dial-up internet, Sega Channel delivered game data over television cable — a novel approach that gave the service its name.&lt;/p&gt;
    &lt;p&gt;In the years since, Sega Channel has been shrouded in a bit of mystery. The service was discontinued in 1998, and the lack of retrievable game data and documentation around Sega Channel has led to decades of speculation about it. We’ve mostly been left with magazine articles and second-hand accounts. Once in a while, one or two Sega Channel ROMs will show up online. How do you preserve a service like Sega Channel?&lt;/p&gt;
    &lt;p&gt;For the last two years, we’ve been working on a large-scale project to preserve the history of Sega Channel. Today, we unveiled our findings in a new YouTube video.&lt;/p&gt;
    &lt;p&gt;We’ll cut to the chase: In collaboration with multiple parties, we have recovered over 100 new Sega Channel ROMs, including system data, exclusive games, and even prototypes that were never published. We’ve also digitized internal paperwork and correspondence that reveals how Sega Channel operated, how it was marketed, and what would’ve come next for the service.&lt;/p&gt;
    &lt;p&gt;This project kicked off in 2024, when we met former Sega Channel vice president of programming Michael Shorrock at the Game Developers Expo. Our booth that year highlighted interesting games from outside the traditional game industry, including Where in North Dakota is Carmen Sandiego?, which our director Frank Cifaldi recovered back in 2016.&lt;/p&gt;
    &lt;p&gt;By complete coincidence, one of the items we put out was a promotional brochure for Broderbund Software… featuring Michael Shorrock on the cover! We got talking with Michael about our work, and we realized we both wanted to preserve and celebrate the history of Sega Channel.&lt;/p&gt;
    &lt;p&gt;At the same time this was happening, we were contacted by a community member named Ray (going by the pseudonym Sega Channel Guy). He had been contacting former Sega Channel staff to see if they still had any old swag or had saved things from the company. In the process, he came into possession of a collection of tape backups containing an unquantifiable amount of internal data from Sega Channel… including a significant number of game and system ROMs.&lt;/p&gt;
    &lt;p&gt;We realized we could put these two threads together! With Michael’s own collection and Ray’s data backups, we could tell a cohesive, wide-ranging story about what Sega Channel was and that was actually distributed through this service.&lt;/p&gt;
    &lt;p&gt;There are two end products from this process. The first is the Michael Shorrock collection, a new collection in our digital library. You can view the correspondence, notes, and presentations from Michael Shorrock’s personal collection, which shed light on the formation of Sega Channel and their audience. From these papers, you can also learn about Express Games: an unannounced successor that would have brought Sega’s cable data delivery service to computers and replaced Sega Channel entirely.&lt;/p&gt;
    &lt;p&gt;The other output here is the collection of Sega Channel ROM data. We’ve donated the data from the 144* new ROMs we recovered to the team at Gaming Alexandria, which will be sharing access to the files.&lt;/p&gt;
    &lt;p&gt;* Our video states that we recovered 142 unique ROMs. However, after uploading the video, we realized we miscounted! There are two additional Sega Channel variant ROM in this collection. The actual total is 144. This does not include the two outliers mentioned in the video, which were previously recovered by users on Sonic Retro in November 2024 but went mostly unreported.&lt;/p&gt;
    &lt;p&gt;This collection includes nearly 100 unique system ROMs, covering almost every version of the system that was distributed to consumers from 1994 to mid-1997. This batch also includes system ROM prototypes and some truly unusual experiments, like a Sega Genesis web browser that would’ve delivered compressed, static websites over television cable.&lt;/p&gt;
    &lt;p&gt;Of great interest to fans, this collection of ROMs also has dozens of previously undumped game variants and Sega Channel exclusives. This includes Garfield: Caught in the Act – The Lost Levels and The Flintstones, two games that were previously believed to be permanently lost and unrecoverable. These are both interesting from a development standpoint; both games appear to have their roots as abandoned projects that were repurposed as Sega Channel-exclusive content.&lt;/p&gt;
    &lt;p&gt;Also included are the previously unpreserved limited editions of Sega Genesis games. These versions have been cut down to fit within Sega Channel’s filesize limit, sometimes omitting content or splitting the game into multiple parts. We’re not sure anyone is especially eager to play a version of Super Street Fighter II missing half the characters, but we’re glad to have it documented.&lt;/p&gt;
    &lt;p&gt;With a few exceptions, this recovery project has accounted for almost all outstanding Sega Channel games. We believe this also means there are now digital backup copies of every unique Sega Genesis game released in the United States.&lt;/p&gt;
    &lt;p&gt;This has been a years-long project that wouldn’t have been possible without support from the broader gaming community. Besides Michael Shorrock and Ray, we want to give special thanks to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sega Retro, The Cutting Room Floor, and Hidden Palace for documenting everything we’ve known about Sega Channel up to this point.&lt;/item&gt;
      &lt;item&gt;RisingFromRuins and Nathan Misner (infochunk) for putting all the pieces together to crack the Sega Channel data formats.&lt;/item&gt;
      &lt;item&gt;Dustin Hubbard (Hubz) from Gaming Alexandria for working with us to share this ROM data.&lt;/item&gt;
      &lt;item&gt;Rob Curl from the Museum of Art and Digital Entertainment, who flagged us down at GDC to let us know that Michael Shorrock had seen a picture of himself at our booth and brought him over to say hello.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We also want to give a special thanks to Chuck Guzis, a long-time expert on data tapes, who digitized Ray’s Sega Channel backups for us in 2024. Chuck’s business Sydex was, for a long time, the go-to vendor for working with data tapes, and we’ve used his services in the past.&lt;/p&gt;
    &lt;p&gt;Shortly before launching this project, we learned that Chuck passed away over the summer. His death leaves a hole in our community and our collective expertise. We know that the gaming community (and specifically the Sega community) will be excited by all this new documentation and data; we hope that their excitement is a testament to what Chuck’s work meant to the digital preservation community.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complete list of recovered titles&lt;/head&gt;
    &lt;p&gt;This is a list of all Sega Channel-specific game data recovered from this project and shared with Gaming Alexandria. This does not include the 97 unique pieces of menu data ROMs and system software that were also recovered.&lt;/p&gt;
    &lt;head&gt;Game list&lt;/head&gt;
    &lt;p&gt;Unique Sega Channel exclusive games:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Berenstain Bears’ A School Day&lt;/item&gt;
      &lt;item&gt;BreakThru&lt;/item&gt;
      &lt;item&gt;The Flintstones&lt;/item&gt;
      &lt;item&gt;Garfield: Caught in the Act – The Lost Levels&lt;/item&gt;
      &lt;item&gt;Iron Hammer&lt;/item&gt;
      &lt;item&gt;Waterworld&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sega Channel variants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Adventures of Batman and Robin, Test Drive version&lt;/item&gt;
      &lt;item&gt;Comix Zone, Test Drive version (1)&lt;/item&gt;
      &lt;item&gt;Comix Zone, Test Drive version (2)&lt;/item&gt;
      &lt;item&gt;Earthworm Jim, Test Drive version&lt;/item&gt;
      &lt;item&gt;Earthworm Jim VideoHints (1)&lt;/item&gt;
      &lt;item&gt;Earthworm Jim VideoHints (2)&lt;/item&gt;
      &lt;item&gt;The Great Earthworm Jim Race&lt;/item&gt;
      &lt;item&gt;The Lost World: Jurassic Park, Part A&lt;/item&gt;
      &lt;item&gt;The Lost World: Jurassic Park, Part B&lt;/item&gt;
      &lt;item&gt;The Lost World: Jurassic Park, Test Drive version&lt;/item&gt;
      &lt;item&gt;NCAA Final Four Basketball: Special Edition (1)&lt;/item&gt;
      &lt;item&gt;NCAA Final Four Basketball: Special Edition (2)&lt;/item&gt;
      &lt;item&gt;Mortal Kombat 3, Part A&lt;/item&gt;
      &lt;item&gt;Mortal Kombat 3, Part B&lt;/item&gt;
      &lt;item&gt;Scholastic’s The Magic School Bus: Space Exploration Game, Test Drive version&lt;/item&gt;
      &lt;item&gt;Sonic 3D Blast, Part A&lt;/item&gt;
      &lt;item&gt;Sonic 3D Blast, Part B&lt;/item&gt;
      &lt;item&gt;Super Street Fighter II: Limited Edition&lt;/item&gt;
      &lt;item&gt;Triple Play Baseball 96: Special Edition&lt;/item&gt;
      &lt;item&gt;Virtua Fighter 2, Part A&lt;/item&gt;
      &lt;item&gt;Virtua Fighter 2, Part B&lt;/item&gt;
      &lt;item&gt;World Series Baseball ’96: Limited Edition*&lt;/item&gt;
      &lt;item&gt;X-Men 2: Clone Wars, Test Drive version&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Prototypes received by Sega Channel:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Al Unser Jr.’s Road to the Top&lt;/item&gt;
      &lt;item&gt;Dan Marino Football&lt;/item&gt;
      &lt;item&gt;Light Crusader&lt;/item&gt;
      &lt;item&gt;Nick Faldo’s Championship Golf&lt;/item&gt;
      &lt;item&gt;Popeye in High Seas High-Jinks&lt;/item&gt;
      &lt;item&gt;Shadows of the Wind&lt;/item&gt;
      &lt;item&gt;WildSnake&lt;/item&gt;
      &lt;item&gt;Wrath of the Demon&lt;/item&gt;
      &lt;item&gt;Yogi Bear [Yogi Bear’s Cartoon Capers]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Data differences:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Body Count (US revision)&lt;/item&gt;
      &lt;item&gt;Maui Mallard in Cold Shadow&lt;/item&gt;
      &lt;item&gt;Primal Rage&lt;/item&gt;
      &lt;item&gt;Pulseman&lt;/item&gt;
      &lt;item&gt;Richard Scarry’s Busytown*&lt;/item&gt;
      &lt;item&gt;Shining Force II&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Header differences only:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Battle Frenzy (US header)&lt;/item&gt;
      &lt;item&gt;Power Drive (US header)&lt;/item&gt;
      &lt;item&gt;QuackShot&lt;/item&gt;
      &lt;item&gt;Super Hang-On&lt;/item&gt;
      &lt;item&gt;Wacky Worlds Creativity Studio&lt;/item&gt;
      &lt;item&gt;X-Men 2: Clone Wars&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;* These games were previously found on a CD obtained by a user on the Sonic Retro forums in November 2024. However, these ROMs were overshadowed by the recovery of the Sega Channel exclusive games The Chessmaster and Klondike from the same CD. Although our copies of these ROMs are not unique, we included them on this list to make sure their existence doesn’t get lost.&lt;/p&gt;
    &lt;head rend="h3"&gt;A footnote for hardcore Sega fans&lt;/head&gt;
    &lt;p&gt;We believe this recovery project accounts for all unique Sega Channel exclusive games. But the most hardcore fans might be wondering: What about Ozone Kid? In a feature article on Sega Channel from the June 1995 issue of Electronic Gaming Monthly (p.29), Ozone Kid was identified as the first Sega Channel exclusive.&lt;/p&gt;
    &lt;p&gt;We can confirm that this game was never actually distributed through Sega Channel. According to data recovered by Ray, The Environmental Detective (as it was titled prior to cancellation) was slated for release alongside the Sega Channel test markets, but it was pulled from their programming plans in July 1994.&lt;/p&gt;
    &lt;p&gt;Reading the between the lines in Sega Channel’s internal project tracking, the game appears to have suffered from a variety of problems over several months. When the game was finally shelved, Sega issued a “partial test report based on items found at the time code was pulled,” suggesting there were still major issues when it was removed from their plans.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288024</guid><pubDate>Tue, 16 Dec 2025 13:07:14 +0000</pubDate></item><item><title>Put a ring on it: a lock-free MPMC ring buffer</title><link>https://h4x0r.org/ring/</link><description>&lt;doc fingerprint="7e7090557fa01e09"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Put a ring on it: a lock-free MPMC ring buffer&lt;/head&gt;
    &lt;p&gt;One of the reasons few security products work well in busy Linux environments is that they amplify performance risk. You’re popular and your backend’s load is skyrocketing? Well, the typical product is just going to collect more data and do more analysis, which amplifies the degradation.&lt;/p&gt;
    &lt;p&gt;In the real world, one of the key ways everyone deals with being overloaded is by dropping less essential things.&lt;/p&gt;
    &lt;p&gt;We can do the same thing with ring buffers, which are fixed-size queues that typically drop old data once they fill up. Yet, they rarely get used outside of single-reader, single-writer scenarios, because it’s hard to build something correct that scales to 1-to-many scenarios, never mind many-to-many scenarios.&lt;/p&gt;
    &lt;p&gt;But, what if we told you, you can have a scalable ring buffer that doesn’t need any locking, and works with multiple readers and multiple writers at the same time? You might say, “there’s no such thing”, except that now there is.&lt;/p&gt;
    &lt;head rend="h1"&gt;Wait, that rings a bell 🔔&lt;/head&gt;
    &lt;p&gt;Ring buffers are fixed-size first-in-first-out (FIFO) queues. Fixed size queues that separately track the front and back are a category of algorithm called the circular buffer.&lt;/p&gt;
    &lt;p&gt;Some people treat the term circular buffer and ring buffer the same. For me, a ring buffer is a type of circular buffer, but one that explicitly drops data when the queue fills up.&lt;/p&gt;
    &lt;p&gt;That’s not the only option for a circular buffer. For instance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You can just block until space becomes available.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can grow the backing store.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can just do nothing, except signal an insertion error and let the programmer figure it out.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ring buffers are more resiliant, since they proactively drop when needed. Typically, it’s the oldest data gets dropped. However, there are ring buffers out there that give the option to drop new data instead.&lt;/p&gt;
    &lt;p&gt;Sure, for some situations, dropping data is the wrong call. But for many situations, especially ones requiring performance, lossy is still much better than bringing the system to a halt due to too much data.&lt;/p&gt;
    &lt;p&gt;For instance, in the Linux kernel, ring buffers are used in many places. A well known example is for relaying events from the kernel to the userland handler, when &lt;code&gt;ebpf&lt;/code&gt; probes are attached.&lt;/p&gt;
    &lt;p&gt;When workloads are having performance issues, probes often end up with more work to do, and if there’s not some form of backpressure, things will end badly. And since &lt;code&gt;ebpf&lt;/code&gt; probes are intended for observability, and since observability is generally less important than availability, dropping data as a first line of defense is a good idea.&lt;/p&gt;
    &lt;p&gt;And because the kernel’s ring buffer allows you to choose whether to drop off the front or the back, &lt;code&gt;ebpf&lt;/code&gt; users get that choose too. Still, dropping older data is generally more common.&lt;/p&gt;
    &lt;p&gt;When operating on any kind of queue, lock-contention can slow things down significantly, because there are two bottlenecks– the head pointer (for enqueuers) or the tail pointer (for dequeuers). But rings have life even worse, because the head pointer can circle around the ring, and meet up with the tail pointer.&lt;/p&gt;
    &lt;p&gt;Our requirements for a ring buffer:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;An ordered set &lt;code&gt;S&lt;/code&gt;, with a maximum size&lt;code&gt;n&lt;/code&gt;, with items of a fixed length&lt;code&gt;l&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;An operation, &lt;code&gt;enqueue(item)&lt;/code&gt;, where&lt;code&gt;item&lt;/code&gt;is an arbitrary item of length&lt;code&gt;l&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;An operation, &lt;code&gt;dequeue()&lt;/code&gt;, which returns the next item of length&lt;code&gt;l,&lt;/code&gt;or indicates that the buffer is empty.&lt;/item&gt;
      &lt;item&gt;No thread should be able to detect an inconsistent ordering of operations on the ring, under any circumstances (We’ll cover memory ordering after we build our ring buffer).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Trying to minimize the impact of bottlenecks is hard, and so people tend to make compromises somewhere. For instance, ring buffers will try to avoid locks, but will accept some constraints, for instance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;single producer, single consumer (SPSC)&lt;/item&gt;
      &lt;item&gt;single producer, multiple consumer (SPMC)&lt;/item&gt;
      &lt;item&gt;multiple-producer, single consumer (MPSC)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here, “producer” means “enqueuer”, and “consumer” means “dequeuer”. At some point, I wanted a true, lock-free multiple-producer, multiple-consumer (MPMC) ring buffer, but there was nothing out there that would scale, so I came up with an algorithm.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;⛓️💥&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;When we say lock free, we mean that, for any given thread performing an operation, no other thread can cause a thread to suspend. With respect to an algorithm only, this is often referred to as a system-wide progress guarantee.&lt;/p&gt;
          &lt;p&gt;That doesn't mean any given thread will make 'progress' in a comfortable amount of time; lock-free algorithms usually perform operations that fail, and need to keep trying them until successful. They could conceptually lose their race till the end of eternity.&lt;/p&gt;
          &lt;p&gt;To get per-thread progress, we need wait freedom, which is often achievable with exponential backoff. But, when OS scheduling tends to be fair, lock free algorithms essentially can expect not to contend forever, and the extra overhead of wait freedom is often not worth it.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;At some point after I’d come up with my algorithm, I did stumble across a vein of literature, calling it a “ring buffer” where old data couldn’t be dropped. The user was left to resubit. To my mind, that’s a fixed-size circular FIFO, but not a ring buffer.&lt;/p&gt;
    &lt;p&gt;Today, we’ll build a true MPMC ring buffer. We’ll focus on dropping old data, but this is one place where extending it yourself would be really pretty simple.&lt;/p&gt;
    &lt;p&gt;The full code is available at codeberg.&lt;/p&gt;
    &lt;head rend="h1"&gt;Ordering that ring 👉☎️&lt;/head&gt;
    &lt;p&gt;The last of our above requirements for a ring is often referred to as linearization. All operations map to a point on a conceptual timeline, where no thread can ‘see’ operations in an order that would be different from that timeline.&lt;/p&gt;
    &lt;p&gt;For instance, if thread &lt;code&gt;A&lt;/code&gt; enqueued &lt;code&gt;I1&lt;/code&gt; then &lt;code&gt;I2&lt;/code&gt;, and thread &lt;code&gt;B&lt;/code&gt; is the one to dequeue both, it must always dequeue in the expected order– &lt;code&gt;I1&lt;/code&gt; first, then &lt;code&gt;I2&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Every insertion thus needs to have a well-defined ordering, as does every removal. But it doesn’t have to map to wall time.&lt;/p&gt;
    &lt;p&gt;For instance, let’s say threads &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; are both dequeuers. &lt;code&gt;B&lt;/code&gt; shows up first and starts dequeueing &lt;code&gt;I1&lt;/code&gt;, but the scheduler suspends it in the middle of the operation. Meanwhile, &lt;code&gt;C&lt;/code&gt; comes in and quickly pulls out &lt;code&gt;I2&lt;/code&gt; before &lt;code&gt;B&lt;/code&gt; wakes, and returns its value.&lt;/p&gt;
    &lt;p&gt;While &lt;code&gt;B&lt;/code&gt; might return after &lt;code&gt;C&lt;/code&gt; in wall-clock time, that shouldn’t worry us, as long as there’s a well-defined ordering. That well defined ordering requires a well-defined linearization point. By that, we mean an atomic operation that is considered the point where the overall operation ‘occurs’ or ‘is committed’.&lt;/p&gt;
    &lt;p&gt;So if &lt;code&gt;B&lt;/code&gt; hasn’t returned, but is suspended after the linearization point, it’s no big deal. The algorithm already considers the item dequeued.&lt;/p&gt;
    &lt;p&gt;Similarly, if &lt;code&gt;B&lt;/code&gt; starts its operation before &lt;code&gt;C&lt;/code&gt; does, but is suspended BEFORE the linearization point, and C is never suspended, &lt;code&gt;C&lt;/code&gt; can absolutely claim &lt;code&gt;I1&lt;/code&gt;; when &lt;code&gt;B&lt;/code&gt; wakes up, it cannot get &lt;code&gt;I1&lt;/code&gt;, and nobody has an inconsistent view of the world.&lt;/p&gt;
    &lt;p&gt;That works because all threads get equal treatment– it doesn’t matter when the functions they call start or end; the ordering is based on when the operation at the linearization point occurs.&lt;/p&gt;
    &lt;p&gt;We’ll make sure to clearly identify our linearization points for each operation, which will always be on atomic operations.&lt;/p&gt;
    &lt;head rend="h1"&gt;👰♂️ Our word is our vow 🤵♀️&lt;/head&gt;
    &lt;p&gt;We’re going to start with modest expectations and build a ring that operates on values that are exactly one word long. We’re going to go ahead and assume you’re on modern hardware, with a 64-bit word.&lt;/p&gt;
    &lt;p&gt;When designing our algorithm, in order to ensure correctness, we will want to think through all the places where there’s contention, meaning, we need to cover all cases where threads might try to operate on the same memory at the same time.&lt;/p&gt;
    &lt;p&gt;The following scenarios are all very realistic things for us to consider:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple enqueuers can be performing enqueue operations in parallel, and thus compete with each other to get onto the list ‘first’.&lt;/item&gt;
      &lt;item&gt;If one enqueuer gets suspended, other enqueuers may wrap around the queue before its operation completes.&lt;/item&gt;
      &lt;item&gt;Multiple dequeuers can also run in parallel, and fight over which item they get.&lt;/item&gt;
      &lt;item&gt;Dequeuers can drain a list to the point that they’ve caught up to writers, and might be reading their state before an operation completes.&lt;/item&gt;
      &lt;item&gt;Or, dequeuers could lag way behind, trying to dequeue from a slot in the ring that a writer is now trying to use for a much more recent value.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be able to arbitrate disputes around this kind of contention, we’re going to be explicit about our linearization timeline. Items in the array will be associated with an epoch, which is simply a point of time on our timeline. But every enqueue operation will be tied to an epoch.&lt;/p&gt;
    &lt;p&gt;We will make sure that each enqueue operation is associated with a unique epoch (though it is perfectly fine if we find ourselves needing to skip epochs).&lt;/p&gt;
    &lt;p&gt;When our dequeuers go to dequeue, they will also be looking to own the dequeue of an item that’s associated with a particular epoch.&lt;/p&gt;
    &lt;p&gt;When a thread examines a slot in the ring, it will need to know what epoch is associated with a cell, and whether there’s an enqueued item in that cell or not. We’ll want to make sure all that information is definitely tied to a specific value, and that the whole kit-and-kaboodle needs to always be read from (and written to) atomically.&lt;/p&gt;
    &lt;p&gt;When a thread wants to update the same state from a cell in the ring, we need to atomically read the existing state, create a copy that has the state we want, and then try to replace the state inside the cell atomically.&lt;/p&gt;
    &lt;p&gt;However, if, when we go to update the state, it’s changed from what we expected (based on the copy we made), then we need the operation to FAIL, and we should start the update process over again, based on those changes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Let’s Swap&lt;/head&gt;
    &lt;p&gt;Thankfully, there’s a universally available atomic operation that does all of those things, often referred to as a compare-and-swap operation (CAS). The &lt;code&gt;C&lt;/code&gt; language standard provides an API call to do this, although they use the word exchange instead of swap (also a common name for this operation).&lt;/p&gt;
    &lt;p&gt;Any CAS operation we perform on a cell will be a linearization point for us.&lt;/p&gt;
    &lt;p&gt;Most hardware platforms can do a compare-and-swap atomically, but not for any arbitrary size. Modern hardware usually limits us to 128 bits that we can atomically operate on. Other atomic operations may limit us to just 64-bit operands.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;💻&lt;/cell&gt;
        &lt;cell&gt;The x86 family has long supported a 128-bit compare-and-swap, but until recently, it required instruction-level locking to use, because it did not support atomically loading 128 bits into a register otherwise. So on old hardware, you're technically using a lock with a 128-bit CAS, but 🤷♂️.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The CAS operation conceptually takes three operands (it can differ, as we’ll see later):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A pointer to the bytes we want to swap (i.e., the object to swap)&lt;/item&gt;
      &lt;item&gt;A pointer to the value we’re expecting to be in that memory before the operation begins (i.e., the expected value)&lt;/item&gt;
      &lt;item&gt;The actual value we want to leave behind (i.e., the desired value).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The processor will check that the value in the 2nd field is right; if it is, the memory address pointed to in parameter 1 gets the value you passed into parameter 3, and the OLD value gets written into the memory address pointed to by parameter 2, overwriting the expected field.&lt;/p&gt;
    &lt;p&gt;In this case, the operation returns &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If the operation fails because the memory has changed since your last load:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Your desired value does not get installed.&lt;/item&gt;
      &lt;item&gt;The memory holding the expected value is updated to contain what the actual value was that differed from the expected value.&lt;/item&gt;
      &lt;item&gt;The function returns &lt;code&gt;false&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On some platforms, there can be two variations of this operation, the strong CAS and the weak CAS. The weak CAS actually is allowed to return &lt;code&gt;false&lt;/code&gt; and skip the swap, even if the expected value does match the object to swap. Generally, that operation will often get it right, but you might occasionally end up re-trying where you shouldn’t have needed to retry.&lt;/p&gt;
    &lt;p&gt;Why the heck would anyone want that behavior? It turns out, some platforms can make this weaker version faster. Even with the potential for failures, if you are using a CAS in a loop, until it succeeds, this weaker version is what people will recommend.&lt;/p&gt;
    &lt;p&gt;However, if you have a use for a CAS operation that doesn’t require testing for success after, using the weaker version would require testing. If you have to add a loop where there wouldn’t have been one otherwise, then you definitely want to use the strong variant.&lt;/p&gt;
    &lt;p&gt;But, while that’s the guidance you’ll find all over the internet, I don’t actually know which CPUs this would affect. Maybe it’s old news, I dunno. But it does still seem to make a shade of difference in real-world tests, so 🤷.&lt;/p&gt;
    &lt;head rend="h2"&gt;Picking your venue ⛪️&lt;/head&gt;
    &lt;p&gt;How do threads decide where to operate inside the ring buffer, with a minimum of fighting?&lt;/p&gt;
    &lt;p&gt;Imagine our ring is a large butcher shop with multiple counters. We take a number, and then go to the counter associated with that number.&lt;/p&gt;
    &lt;p&gt;Our ring will give out tickets to epochs, giving out each number no more than two times– once to an enqueuer, and once to a dequeuer. To do that, we’ll need two counters that we can atomically update.&lt;/p&gt;
    &lt;p&gt;If we need to implement giving out tickets like this, we can use an atomic CAS operation to do it, sitting in a loop until our new value is swapped in. Each time we lose, we’ll have to recompute the next value, but that’ll certainly work.&lt;/p&gt;
    &lt;p&gt;However, we can avoid the loop altogether if we do it through an atomic fetch-and-add (FAA) operation. When implemented in hardware, FAA is guaranteed to be completed. You pass in two operands:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A pointer to the object containing the current value. The result will be stored here at the end of the operation.&lt;/item&gt;
      &lt;item&gt;The value you’d like to add.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At the end of the operation, the old value gets returned to the caller.&lt;/p&gt;
    &lt;p&gt;So, to implement a ticketing system, we can have a variable that holds the value of the next ticket to give out. Each thread gets a ticket by calling FAA, adding the number &lt;code&gt;1&lt;/code&gt; to the ticket value, but returning the number that was there at the start, which becomes our ticket.&lt;/p&gt;
    &lt;p&gt;No two enqueuers will get the same ticket. We can then map each ticket to a cell in the ring (easiest done by limiting the number of cells in the ring to powers of two). We take the ticket, divide by the number of cells, and the remainder is the cell index associated with that ticket (i.e., we compute the modulus).&lt;/p&gt;
    &lt;p&gt;When we have a number of cells that’s a power of two, we can use a cheap binary AND operation to get the modulus.&lt;/p&gt;
    &lt;p&gt;That trick only works for powers of two, because of some specific properties:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Every power of two is represented with a single bit set to 1; all other bits are zero.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you subtract&lt;/p&gt;&lt;code&gt;1&lt;/code&gt;from a power of two, all bits to the RIGHT of that one bit set for the power of two will be set, and nothing else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So we get a cheap modulo via a binary AND operation. To be fair, it could be the case that modern processors can compute the modulus just as quickly as a bitwise AND, even though it’s a much more complicated operation. But, that’s why many data structures are backed by arrays of size to powers of two. It’s such a common paradigm, we’ll just roll with it.&lt;/p&gt;
    &lt;p&gt;Now, we also need a second ticket for dequeuers; those tickets should be associated with values that have already been enqueued.&lt;/p&gt;
    &lt;p&gt;However, if we keep those two tickets separate, then it will be really hard for us to build any confidence that we’re detecting some of the contention scenarios we talked about above.&lt;/p&gt;
    &lt;p&gt;For example, if we read each counter separately, how do we know if the queue is empty? Or, how can we be certain that readers are way behind (we don’t want to waste a lot of time with readers grabbing tickets for values we already dropped).&lt;/p&gt;
    &lt;p&gt;The answer for us is to operate on those tickets (epochs) in one atomic operation.&lt;/p&gt;
    &lt;p&gt;We can still do that with an FAA. For example, if we define our epoch-of-record datatype like this:&lt;/p&gt;
    &lt;code&gt;typedef struct {
    uint32_t epoch_q; // The next epoch assoc. w/ an enqueue op.
    uint32_t epoch_d; // The next epoch assoc. w/ an enqueue op.
} h4x0r_word_ring_epochs_t;
&lt;/code&gt;
    &lt;p&gt;The compiler is smart enough that it will let you perform operations on structs as if they were integers, as long as the sizes are acceptable, and as long as you are explicit enough with your use of types to convince the compiler you know what you’re doing.&lt;/p&gt;
    &lt;p&gt;One way to do this is to use a &lt;code&gt;union&lt;/code&gt;, which we might declare like this:&lt;/p&gt;
    &lt;code&gt;typedef union {
    h4x0r_word_ring_epochs_t epochs;
    uint64_t                 integer;
} h4x0r_converter_t;
&lt;/code&gt;
    &lt;p&gt;The fields in unions may be individually addressed, but they share the same memory. The bits won’t change, but we can get the compiler to recognize the change-of-type based on the field we end up accessing.&lt;/p&gt;
    &lt;p&gt;Let’s say we want to add 1 to the queue epoch. We don’t need to understand endianness or how the two fields in that struct are ordered. Here’s one way we could do that:&lt;/p&gt;
    &lt;code&gt;    h4c0r_word_ring_epochs_t to_add_struct = {
                                               .epoch_q = 1,
                                               .epoch_d = 0,
                                             };
    union h4x0r_converter_t conv           = {
                                               .epochs = my_epochs,
                                             };
    uint64_t                to_add_num     = conv.integer;
&lt;/code&gt;
    &lt;p&gt;We can then convert it back to a struct. And really, those conversions we expect to be free; it’s just a mechanism for expressing our intent to the compiler.&lt;/p&gt;
    &lt;p&gt;While 128-bit CAS operations are commonly supported in hardware, FAA (and similar operations) are more likely to have a 64-bit cap on their operands.&lt;/p&gt;
    &lt;p&gt;That’s why the two epochs in our data structure are kept to 32 bits.&lt;/p&gt;
    &lt;p&gt;However, 32 bits isn’t all that large a number, and it wouldn’t take too long for a busy system to overflow a counter. Dealing with that kind of overflow wouldn’t be fun.&lt;/p&gt;
    &lt;p&gt;If you are confident you’ve got a situation where you won’t use a ring enough to overflow, then by all means, use 32-bit epochs. But we recommend that, by default, you use 64-bit epochs. You can emulate a 128-bit FAA pretty easily with a CAS operation, as we described above.&lt;/p&gt;
    &lt;p&gt;While a 64-bit FAA should be faster, the CAS probably will be fine (on my machine, it’s a tiny smidge better, but not enough to crow about).&lt;/p&gt;
    &lt;p&gt;Our full implementation allows you to toggle between the two epoch sizes at compile-time, so you can compare the results if you like.&lt;/p&gt;
    &lt;p&gt;Anyway, atomically updating the epoch state gets you a ticket, and shows you what the next ticket is for the opposite operation, at the time we were handing your ticket.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Simple ring 🍩&lt;/head&gt;
    &lt;p&gt;As we said above, the core state of the cells inside a ring must be no more than 128 bits if we want to operate on it without requiring a lock. So we need to be careful about what we try to jam in a cell.&lt;/p&gt;
    &lt;p&gt;When we load a cell, we want to know if we’re too slow, whatever our operation. At a bare minimum, we’re going to need:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Whether an item is enqueued in the slot or has been dequeued.&lt;/item&gt;
      &lt;item&gt;Room for that value, which probably needs to be a whole word so we can fit a pointer in (usually 64 bits).&lt;/item&gt;
      &lt;item&gt;The epoch associated with the cell, which is how we’ll know if a writer has been lapped (if the epoch is higher than the epoch for our operation, we got lapped). It’s also how readers will figure out the writer is slow and hasn’t finished.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we said above, 32 bits is not enough space for the epoch if we are building something general-purpose. But, we clearly don’t have room for 64-bit epochs, so we’ll need to compromise.&lt;/p&gt;
    &lt;p&gt;A boolean generally will take up at least 8 bits, and a 56-bit epoch probably is good enough not to worry about. Though C doesn’t have 56-bit ints.&lt;/p&gt;
    &lt;p&gt;However, we can instead use C bit slicing, which would even let us get the flag down to a single bit, leaving 63 bits for our epoch:&lt;/p&gt;
    &lt;code&gt;typedef struct {
    _Atomic(void *)item;
    uint64_t enqueued : 1;
    uint64_t epoch    : 63;
} h4x0r_word_ring_item_t;

static_assert(sizeof(h4x0r_word_ring_item_t) == 16,
              "Bitfield implementation doesn't pack as expected");

typedef _Atomic(h4x0r_word_ring_item_t)   h4x0r_word_ring_cell_t;
typedef _Atomic(h4x0r_word_ring_epochs_t) h4x0r_atomic_epochs_t;
&lt;/code&gt;
    &lt;p&gt;The C standard doesn’t really mandate layout for bit slices, so we check at compile type with the &lt;code&gt;static_assert&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For values we need to be shared between threads, we need to label them as &lt;code&gt;_Atomic&lt;/code&gt;. Otherwise, the code that the compiler generates will assume none of our variables are shared across threads, and we are bound to have all sorts of nasty race conditions.&lt;/p&gt;
    &lt;p&gt;But, we don’t tag &lt;code&gt;_Atomic&lt;/code&gt; on &lt;code&gt;h4x0r_word_ring_item_t&lt;/code&gt; or &lt;code&gt;h4x0r_word_ring_epochs_t&lt;/code&gt; directly, because threads will be keeping their own private versions for updating.&lt;/p&gt;
    &lt;p&gt;My version of the top-level ring looks like this:&lt;/p&gt;
    &lt;code&gt;typedef void (*h4x0r_word_ring_drop_handler)(h4x0r_word_ring_t *, void *);

struct h4x0r_word_ring_t {
    h4x0r_word_ring_drop_handler drop_handler;
    uint32_t                     num_cells;
    uint32_t                     last_slot;
    h4x0r_atomic_epochs_t        epochs;
    h4x0r_word_ring_cell_t       cells[];
};
&lt;/code&gt;
    &lt;p&gt;The drop handler is a function pointer, and we’d expect it to be set when initializing the ring, defaulting to &lt;code&gt;NULL&lt;/code&gt; when we don’t need to do anything in particular to deal with drops (either because we’re not using dynamic memory, or because we have something like a garbage collector to clean up if needed).&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;epochs&lt;/code&gt; field is tagged as being atomic, though it’s hidden behind the typedef.&lt;/p&gt;
    &lt;p&gt;Similarly, the variable-length array of cells is really an array of items we want to be atomic. The fact that cell loading and storing requires using the C atomic API is in there, but hidden behind a &lt;code&gt;typedef&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;However, the &lt;code&gt;num_cells&lt;/code&gt; field and &lt;code&gt;last_slot&lt;/code&gt; field are not tagged &lt;code&gt;_Atomic&lt;/code&gt;. That’s because these should be set by one thread during initialization, and then never changed. As long as the memory has synced before other threads start to use these fields, we definitely don’t need them to be treated specially.&lt;/p&gt;
    &lt;p&gt;Usually, if we do initialization in a proper function, the call boundary is going to be a memory barrier that makes sure they’re sync’d when other threads start getting a handle on our ring.&lt;/p&gt;
    &lt;p&gt;But, if initialization might be inlined, you should probably flag these things as &lt;code&gt;_Atomic&lt;/code&gt;, but when you access them via the C11 atomic API, ask for &lt;code&gt;memory_order_relaxed&lt;/code&gt;, which essentially means, “no atomic op necessary here, so don’t add instructions to sync here”.&lt;/p&gt;
    &lt;p&gt;In that case, the &lt;code&gt;_Atomic&lt;/code&gt; modifier will make sure our writes to those fields are seen by subsequent loads, but we don’t have to slow down those loads once the values are written.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;num_cells&lt;/code&gt; field is always going to be the number of cells in our ring, and 2^32 cells should be plenty, thus 32 bits. But because of C alignment rules, our struct is going to end up with a 32-bit hole somewhere. So, we fill that space with &lt;code&gt;last_slot&lt;/code&gt;, which is always going to be one less than the value of &lt;code&gt;num_cells&lt;/code&gt;, allowing us to perform our fast modulo without first having to subtract one from &lt;code&gt;num_cells&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;The dequeue operation&lt;/head&gt;
    &lt;p&gt;So far, our queue has nothing in it, and we don’t know how to put anything in it yet.&lt;/p&gt;
    &lt;p&gt;Still, let’s start with our dequeue operation.&lt;/p&gt;
    &lt;p&gt;Our first order of business is to the epochs, and if the tail and head are in the same place (or if the tail somehow lapped the head), then the queue is empty, and we shouldn’t even bother taking a ticket, so to speak.&lt;/p&gt;
    &lt;p&gt;But, if we do see items to dequeue, we’ll play the game! We’ll use FAA to get a unique read epoch. However, it’s 100% possible that other readers beat us, with no writers coming along.&lt;/p&gt;
    &lt;p&gt;That means, we could actually increment the counter past where the next writer is going to write. We will need to make sure that when we deal with writers, we try to solve that problem.&lt;/p&gt;
    &lt;p&gt;The epoch we read in won’t ever get another reader, but the writer will need to make sure it doesn’t use the slot we just accidentally burned.&lt;/p&gt;
    &lt;p&gt;Now, we’ll use our epoch to find the associated cell. We’ll load it, and look at the contents to figure out what to do.&lt;/p&gt;
    &lt;p&gt;Once we have loaded the cell, if the epoch is the one we’re expecting, and there’s an item in there, then we try to dequeue it by doing a CAS to mark it as dequeued (and generally attempting to set the item to &lt;code&gt;NULL&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;If the dequeue succeeds in the right epoch, we’re done.&lt;/p&gt;
    &lt;p&gt;If we found that another reader wasn’t keeping up, and our writer hasn’t written over it yet, we’d still try the CAS, and if it succeeds, that epoch is invalidated; the writer will eventually figure out that they cannot complete their write, and needs to try again.&lt;/p&gt;
    &lt;p&gt;But, if we can prove via the epochs that we’re the only writer at the time of our CAS, then, even though we know someone is trying to queue, we take that successful CAS as our linearization point, and declare that the ring was empty.&lt;/p&gt;
    &lt;p&gt;If we lose the race against a slow writer, no big deal, we start over with the same epoch; the writer probably left us a present.&lt;/p&gt;
    &lt;p&gt;If our CAS operation fails, before we try again, we look at the value of the &lt;code&gt;expected&lt;/code&gt; item, which is the current stored value.&lt;/p&gt;
    &lt;p&gt;If we find the epoch is lower than ours, then we try to invalidate it with a CAS. If we fail, it could be that the slow writer finished, or it could be that we were suspended and are now behind. If it’s the former, we try again with the same epoch. If it’s the latter, we’ll know because the epoch is higher than ours, and we need to go get another read epoch (we can’t return empty unless we know that because of some CAS that we did, there’s a moment on our timeline where the ring was empty).&lt;/p&gt;
    &lt;p&gt;That’s already several corner cases we need to worry about. But the most difficult concern here is the last one.&lt;/p&gt;
    &lt;p&gt;Consider when we use the CAS to dequeue, and that CAS operation failed. But the expected value was the epoch we were looking for. We can just go home and call it a day, right?&lt;/p&gt;
    &lt;p&gt;It depends:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;If we are running with a drop-handler, we absolutely cannot, because the writer will have known it was overwriting, and will be calling the drop handler. We don’t want to risk a double free if there’s dynamic deallocation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Otherwise, yes; the writer we were competing with absolutely doesn’t care.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the API call to dequeue, some callers are going to need to distinguish between returning a &lt;code&gt;NULL&lt;/code&gt; because the ring is empty, and the case where &lt;code&gt;NULL&lt;/code&gt; was enqueued.&lt;/p&gt;
    &lt;p&gt;The way to deal with this is to have our API call accept a pointer to a boolean. If &lt;code&gt;NULL&lt;/code&gt; is passed, then we ignore the parameter. Otherwise, we’ll store a value in the memory pointed to.&lt;/p&gt;
    &lt;p&gt;The logic around whether to store the pointer is easily lifted out into inline functions:&lt;/p&gt;
    &lt;code&gt;static inline void *
h4x0r_word_ring_empty(bool *found)
{
    if (found) {
        *found = false;
    }
    return NULL;
}

static inline void *
h4x0r_word_ring_found(h4x0r_word_ring_item_t item,
                      uint64_t              *epoch_ptr,
                      bool                  *found)
{
    if (found) {
        *found = true;
    }

    h4x0r_atomic_fence();

    if (epoch_ptr) {
        *epoch_ptr = item.epoch;
    }

    return item.item;
}
&lt;/code&gt;
    &lt;p&gt;Notice that &lt;code&gt;h4x0r_word_ring_found&lt;/code&gt; takes a second pointer parameter– that’s for the caller to get the associated epoch. Day-to-day that may not be too useful. However, it’s very useful for correctness testing, to make sure one thread never sees out-of-order dequeues.&lt;/p&gt;
    &lt;p&gt;We’ll use it when we do our testing.&lt;/p&gt;
    &lt;p&gt;With all that, here’s the body of our dequeue operation:&lt;/p&gt;
    &lt;code&gt;void *
h4x0r_word_ring_dequeue(h4x0r_word_ring_t *ring, bool *found, uint64_t *ep)
{
    h4x0r_word_ring_cell_t  *cell;
    h4x0r_word_ring_item_t   expected;
    h4x0r_word_ring_item_t   last;
    h4x0r_word_ring_item_t   candidate;
    h4x0r_word_ring_epochs_t epochs = h4x0r_atomic_load(&amp;amp;ring-&amp;gt;epochs);

    while (true) {
        if (epochs.epoch_d &amp;gt;= epochs.epoch_q) {
            return h4x0r_word_ring_empty(found);
        }
        epochs   = h4x0r_epochs_increment(&amp;amp;ring-&amp;gt;epochs, read_incr);
        cell     = h4x0r_word_ring_slot_addr(ring, epochs.epoch_d);
        expected = h4x0r_atomic_load(cell);

        candidate = (h4x0r_word_ring_item_t){
            .item     = NULL,
            .epoch    = epochs.epoch_d,
            .enqueued = false,
            .dequeued = true,
        };

        while (expected.epoch &amp;lt;= epochs.epoch_d) {
            last = expected;

            if (h4x0r_atomic_cas(cell, &amp;amp;expected, candidate)) {
                if (epochs.epoch_d == last.epoch) {
                    return h4x0r_word_ring_found(last, ep, found);
                }
                if (epochs.epoch_d &amp;gt; last.epoch) {
                    h4x0r_word_ring_drop(ring, last);
                    break;
                }
                return h4x0r_word_ring_found(last, ep, found);
            }
            else {
                if (last.epoch == epochs.epoch_d &amp;amp;&amp;amp; !ring-&amp;gt;drop_handler) {
                    return h4x0r_word_ring_found(last, ep, found);
                }
                epochs = h4x0r_atomic_load(&amp;amp;ring-&amp;gt;epochs);
                continue;
            }
            if (epochs.epoch_q - epochs.epoch_d &amp;lt;= 1) {
                return h4x0r_word_ring_empty(found);
            }
        }
        // We got lapped and need a new epoch.
        epochs = h4x0r_atomic_load(&amp;amp;ring-&amp;gt;epochs);
    }
}
&lt;/code&gt;
    &lt;p&gt;You’ll notice there are a few more helper functions in there:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;h4x0r_epochs_increment()&lt;/code&gt;uses some inline code to do the 128-bit FAA using a union for conversion, as discussed above.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;static inline h4x0r_word_ring_epochs_t
h4x0r_epochs_increment(_Atomic h4x0r_word_ring_epochs_t *p,
                       h4x0r_epoch_info_t                counter)
{
    h4x0r_epoch_info_t result;

    result.i = h4x0r_atomic_add((_Atomic(__uint128_t) *)p, counter.i);

    return result.s;
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;h4x0r_word_ring_slot_addr()&lt;/code&gt;takes our epoch, performs the modulo operation, and gets us a pointer to our cell. The code to get the address inside that inline function is more compact than the call, but we prefer the extra clarity, and the compiler is expected to inline it, especially if we put it in a header file (or we can annotate it to always inline).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;static inline _Atomic(h4x0r_word_ring_item_t) *
h4x0r_word_ring_slot_addr(h4x0r_word_ring_t *ring, uint64_t epoch)
{
    return &amp;amp;ring-&amp;gt;cells[epoch &amp;amp; ring-&amp;gt;last_slot];
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There are several &lt;code&gt;h4x0r_atomic_*()&lt;/code&gt;calls. Those wrap the C11 API calls so we can apply consistent memory ordering that’s different from C’s default. We’ll discuss this a bit at the end of the article for those who want to understand. We do use C’s new-ish&lt;code&gt;_Generic&lt;/code&gt;feature to abstract away over&lt;code&gt;fetch-and-add&lt;/code&gt;, selecting our function for 128 bit values:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#define h4x0r_atomic_add(x, y)                    \
    _Generic((y),                                 \
        __uint128_t: _h4x0r_atomic_faa_128(x, y), \
        default: atomic_fetch_add_explicit(x, y, H4X0R_MO_RW))

    static inline __uint128_t                                 
        _h4x0r_atomic_faa_128(void *v, __uint128_t val) 
    {                                                         
        _Atomic __uint128_t *var = v;                         
                                                             
        __uint128_t expected;                                 
        __uint128_t desired;                                  
                                                              
        expected = h4x0r_atomic_load(var);                    
                                                              
        do {                                                  
            desired = expected + val;                 
        } while (!h4x0r_atomic_cas(var, &amp;amp;expected, desired)); 
        return expected;                                      
    }

h4x0r_decl_binopu128(faa, +);
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;h4x0r_word_ring_drop()&lt;/code&gt;is about as simple as you’d want it to be:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;static inline void
h4x0r_word_ring_drop(h4x0r_word_ring_t *ring, h4x0r_word_ring_item_t cell)
{
    if (ring-&amp;gt;drop_handler &amp;amp;&amp;amp; cell.enqueued) {
        (*ring-&amp;gt;drop_handler)(ring, cell.item);
    }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The enqueue operation&lt;/head&gt;
    &lt;p&gt;We’re already more than halfway done with our first ring. All we really need to do is get stuff into it.&lt;/p&gt;
    &lt;p&gt;Our first order of business when a thread calls our enqueue function is to load the existing epochs, grabbing its own ticket (epoch) in the process, via our fetch-and-add.&lt;/p&gt;
    &lt;p&gt;Second, we check the epochs to see if they need repair. That could be one of two scenarios:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We see that the tail is lagging (meaning, the epochs indicate the tail is farther behind than the number of slots); this is due to a relative lack of dequeuers.&lt;/item&gt;
      &lt;item&gt;We see that some dequeue operation accidentally look a read-slot when the queue was empty, during a race condition (We do not want to write into a slot that no reader could ever read).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the enqueuer sees either of these two scenarios, it will attempt to ‘fix’ the tail by moving the dequeue epoch to the lowest epoch that might still be enqueued. Here are my helper functions to deal with these scenarios:&lt;/p&gt;
    &lt;code&gt;#define H4X0R_BACKOFF_START_NS 1000
#define H4X0R_BACKOFF_MAX_NS   65536

static const struct timespec start_sleep = {
    .tv_sec  = 0,
    .tv_nsec = H4X0R_BACKOFF_START_NS,
};

static inline bool
h4x0r_word_ring_needs_repair(h4x0r_word_ring_epochs_t epochs,
                             uint32_t                 ring_size)
{
    if (epochs.epoch_d + ring_size &amp;lt; epochs.epoch_q) {
        return true;
    }
    if (epochs.epoch_d &amp;gt; epochs.epoch_q) {
        return true;
    }
    return false;
}

static inline void
h4x0r_ring_lag_sleep(struct timespec *sleep_time)
{
    // We don't really care if we sleep the whole time or not.
    nanosleep(sleep_time, NULL);
    sleep_time-&amp;gt;tv_nsec &amp;lt;&amp;lt;= 1;

    if (sleep_time-&amp;gt;tv_nsec &amp;gt; H4X0R_BACKOFF_MAX_NS) {
        sleep_time-&amp;gt;tv_nsec = H4X0R_BACKOFF_MAX_NS;
    }
}

// Returns true if we ever go through the loop, indicating
// we may need to  update our own epoch.
static inline bool
h4x0r_word_ring_repair(h4x0r_word_ring_epochs_t epochs,
                       h4x0r_word_ring_t       *ring)
{
    struct timespec          sleep_time = start_sleep;
    bool                     repair     = false;    
    h4x0r_word_ring_epochs_t candidate;

    while (h4x0r_word_ring_needs_repair(epochs, ring-&amp;gt;num_cells)) {
        repair = true;
        
        if (epochs.epoch_d &amp;gt; epochs.epoch_q) {
            candidate = (h4x0r_word_ring_epochs_t){
                .epoch_q = epochs.epoch_q,
                .epoch_d = epochs.epoch_q,
            };
        }
        else {
            candidate = (h4x0r_word_ring_epochs_t){
                .epoch_q = epochs.epoch_q,
                .epoch_d = epochs.epoch_q - ring-&amp;gt;num_cells,
            };
        }
        if (!h4x0r_atomic_cas(&amp;amp;ring-&amp;gt;epochs, &amp;amp;epochs, candidate)) {
            return true;
        }
        h4x0r_ring_lag_sleep(&amp;amp;sleep_time);
    }

    return repair;
}
&lt;/code&gt;
    &lt;p&gt;If the enqueuers don’t do the tail-correction, it penalizes dequeuers who are already behind; they’ll pay the price of going back for tickets, only to find they’re out of date, which can exacerbate problems when they’re behind.&lt;/p&gt;
    &lt;p&gt;If we do see lag, after attempting to fix it, we should help even more by taking a really short snooze to go ahead and help any pending reader succeed. If we don’t, then we’re at more risk of dequeuers continually being forced to retry because writers are starving them. Here is one place in our algorithm where, if we want to go for full wait-freedom, we can turn this process into an exponential backoff loop.&lt;/p&gt;
    &lt;p&gt;Once we leave the readers acceptably far behind, we then go to the cell we’re supposed to be writing to. There, we’re going to want to load the state to see what’s what.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If we see our epoch is already in there, we were too slow, and some reader invalidated the slot; we need to grab a new epoch and try everything again.&lt;/item&gt;
      &lt;item&gt;We do exactly the same thing if we see a HIGHER epoch than ours (writers lapped us, probably because we got suspended).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Next, we add the value, move the state to &lt;code&gt;enqueued&lt;/code&gt;, and attempt to install the item via a CAS.&lt;/p&gt;
    &lt;p&gt;We keep trying until that succeeds.&lt;/p&gt;
    &lt;p&gt;Once our CAS is successful, then the write is successful. However, we still are not always done.&lt;/p&gt;
    &lt;p&gt;Specifically, we may need to look at what we overwrote (which should be installed in the &lt;code&gt;expected&lt;/code&gt; field).&lt;/p&gt;
    &lt;p&gt;If we overwrote a queued item, then we need to pass that item to the drop handler. This is a particularly important thing to do when the item we found is a pointer to heap memory.&lt;/p&gt;
    &lt;p&gt;If we simply overwrite without checking, we might be leaking the memory the pointer references.&lt;/p&gt;
    &lt;p&gt;Of course, if we have no drop handler, we don’t need the extra step; there’s no problem.&lt;/p&gt;
    &lt;p&gt;We can just return, successful in our mission.&lt;/p&gt;
    &lt;p&gt;Here’s my implementation:&lt;/p&gt;
    &lt;code&gt;uint64_t
h4x0r_word_ring_enqueue(h4x0r_word_ring_t *ring, void *item)
{
    h4x0r_word_ring_cell_t  *cell;
    h4x0r_word_ring_epochs_t epochs;
    uint64_t                 write_epoch;
    h4x0r_word_ring_item_t   expected;

    while (true) {
        epochs      = h4x0r_epochs_increment(&amp;amp;ring-&amp;gt;epochs, write_incr);
        write_epoch = epochs.epoch_q;
        
        if (h4x0r_word_ring_repair(epochs, ring)) {
            if (write_epoch + ring-&amp;gt;num_cells &amp;lt; epochs.epoch_q) {
                continue;
            }
        }

        cell     = h4x0r_word_ring_slot_addr(ring, write_epoch);
        expected = h4x0r_atomic_load(cell);

        // This has to be a CAS; we might have another writer who
        // laps us between the last epoch check and the coming op.
        h4x0r_word_ring_item_t new = {
            .item     = item,
            .epoch    = write_epoch,
            .enqueued = true,
        };

        while (expected.epoch &amp;lt; write_epoch) {
            if (h4x0r_atomic_cas(cell, &amp;amp;expected, new)) {
                // If we overwrote something, it'll need to be dropped.
                h4x0r_word_ring_drop(ring, expected);
                return write_epoch;
            }
        }
        continue; // too slow; get a new epoch and retry.
    }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;We’ve Inscribed some words on your ring&lt;/head&gt;
    &lt;p&gt;We have our first lock-free ring. But so far, it’s like we got the ring out of a cracker-jack box. It’s not yet something nice enough to use, since we’re limited to putting in 64-bit values.&lt;/p&gt;
    &lt;p&gt;However, we will 100% solve that problem.&lt;/p&gt;
    &lt;head rend="h1"&gt;That 💍 is too small, I want a BIG one&lt;/head&gt;
    &lt;p&gt;One thing about a ring that’s often valued is that you can operate in a fixed amount of statically allocated memory. Only giving 64 bits of space for the ring will push us towards dynamic allocation, which is a shame.&lt;/p&gt;
    &lt;p&gt;But we can use our word ring to make a big ring with larger, fixed-sized memory cells.&lt;/p&gt;
    &lt;p&gt;The basic idea is that we have two circular buffers, one of them being the word ring (we’ll call it &lt;code&gt;R&lt;/code&gt;). Then, we’ll create a second circular buffer to store our arbitrary-sized records. We’ll call this one &lt;code&gt;S&lt;/code&gt; (for store).&lt;/p&gt;
    &lt;p&gt;The entries we enqueue into our word ring &lt;code&gt;R&lt;/code&gt; will simply be an epoch that points to a spot in &lt;code&gt;S&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Write threads will peek at the ring’s global epoch info, for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As a hint for where to start in the larger array, and&lt;/item&gt;
      &lt;item&gt;So that we know which records conceptually aren’t in the ring anymore and can be overwritten.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are the data structures I used in my implementation, along with the state flags I use throughout.&lt;/p&gt;
    &lt;code&gt;typedef struct h4x0r_ring_t h4x0r_ring_t;
typedef struct h4x0r_ring_entry_info_t h4x0r_ring_entry_info_t;


// The full cell definition for the outer ring cells.

struct h4x0r_ring_entry_t {
    _Atomic h4x0r_ring_entry_info_t info;
    uint64_t                        len;
    char                            data[];
};

// This the first item in the outer ring cell, 
struct h4x0r_ring_entry_info_t {
    uint64_t write_epoch;
    uint64_t state;
};

struct h4x0r_ring_t {
    h4x0r_word_ring_t  *word_ring;
    h4x0r_ring_entry_t *entries;
    _Atomic uint64_t    entry_ix;
    uint64_t            last_entry;
    uint64_t            entry_len;
};

enum : uint64_t {
    H4X0R_RING_EMPTY             = 0x00,
    H4X0R_RING_RESERVED          = 0x01,
    H4X0R_RING_DEQUEUE_RESERVE   = 0x02,
    H4X0R_RING_ENQUEUE_DONE      = 0x04,
    H4X0R_RING_USED              = 0x07,
    H4X0R_RING_DEQUEUE_DONE_MASK = ~0x06,
};
&lt;/code&gt;
    &lt;p&gt;I’m going to skip initialization here, but two important notes if you’re going to DIY:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Double-check that the “core” word ring’s size in bits is a power of two.&lt;/item&gt;
      &lt;item&gt;Ensure that cell sizes are properly aligned (probably to a 16-byte boundary to be safe).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The enqueue operation&lt;/head&gt;
    &lt;p&gt;The write thread starts by grabbing the underlying word ring’s epoch information. It takes the write epoch it finds, and maps that into &lt;code&gt;S&lt;/code&gt; (&lt;code&gt;e % len(S)&lt;/code&gt;, if &lt;code&gt;e&lt;/code&gt; is the epoch)`.&lt;/p&gt;
    &lt;p&gt;Starting at that position, the writer scans &lt;code&gt;S&lt;/code&gt; to find the first valid spot it can claim.&lt;/p&gt;
    &lt;p&gt;That means, if it notices an operation in progress, it skips that cell and keeps probing until it can claim a cell that’s safe to write. Once the write completes, we enqueue the position into our word ring. Adding it to the word ring gives us the epoch; we add that into our slot inside &lt;code&gt;S&lt;/code&gt;, before we remove the flag that indicates we’re writing to the cell.&lt;/p&gt;
    &lt;p&gt;As a result, entries in &lt;code&gt;S&lt;/code&gt; can be out of order, and that’s totally fine. The correct order will be kept in the word ring.&lt;/p&gt;
    &lt;p&gt;More specifically, the steps for enqueuers (writers) are as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find a spot in &lt;code&gt;S&lt;/code&gt;, and reserve it, so no other writer will touch it.&lt;/item&gt;
      &lt;item&gt;Copy data into the reserved cell.&lt;/item&gt;
      &lt;item&gt;Enqueue a pointer to &lt;code&gt;S&lt;/code&gt;into&lt;code&gt;R&lt;/code&gt;(our linearization point).&lt;/item&gt;
      &lt;item&gt;Write into &lt;code&gt;S&lt;/code&gt;the epoch that&lt;code&gt;R&lt;/code&gt;returned to us when we enqueued.&lt;/item&gt;
      &lt;item&gt;Update the slot in &lt;code&gt;S&lt;/code&gt;with the epoch, and indicate we’re done with their enqueue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make this all work, we really should have &lt;code&gt;S&lt;/code&gt; contain more entries than &lt;code&gt;R&lt;/code&gt;. We want to have enough to ensure the right number of items can all be enqueued at once in a full queue, and that any write thread will still have a space to write. If we don’t do that, writers will be roaming around a full parking garage until a spot opens up and they’re lucky enough to nab it.&lt;/p&gt;
    &lt;p&gt;If we have a ceiling on the number of threads we allow, we can use that value. But, if not, just doubling the number of entries should be more than good enough. There will be no competition for the enqueuer’s slot from other enqueuers.&lt;/p&gt;
    &lt;p&gt;However, a dequeuer can come in after step 3 completes and before step 4 completes, while we’re suspended.&lt;/p&gt;
    &lt;p&gt;That’s not a problem for us– the linearization point is the enqueue into &lt;code&gt;R&lt;/code&gt;. We just need to make sure that enqueuers can only claim a slot if BOTH enqueuers and dequeuers are done with their operation (and, if it’s not in &lt;code&gt;R&lt;/code&gt;, of course).&lt;/p&gt;
    &lt;p&gt;Note that dequeuers will set a state bit when they start dequeuing, so we can easily avoid taking those slots. However, when figuring out whether we can overwrite a state no thread is in, we need to check the stored epoch, to make sure it’s far enough behind ours that it’s not conceptually in the queue anymore.&lt;/p&gt;
    &lt;p&gt;Here’s the core of the enqueue operation:&lt;/p&gt;
    &lt;code&gt; void
h4x0r_ring_enqueue(h4x0r_ring_t *self, void *item, uint64_t len)
{
    uint64_t                 ix;
    uint64_t                 byte_ix;
    uint64_t                 start_epoch;
    h4x0r_ring_entry_info_t  expected;
    h4x0r_ring_entry_info_t  candidate;
    h4x0r_ring_entry_t      *cur;
    h4x0r_word_ring_epochs_t epochs;

    if (len &amp;gt; self-&amp;gt;entry_len) {
        len = self-&amp;gt;entry_len;
    }

    epochs      = h4x0r_atomic_load(&amp;amp;self-&amp;gt;word_ring-&amp;gt;epochs);
    start_epoch = epochs.epoch_q;
    ix          = start_epoch &amp;amp; self-&amp;gt;last_entry;

    while (true) {
        byte_ix = ix * (sizeof(h4x0r_ring_entry_t) + self-&amp;gt;entry_len);
        cur     = (h4x0r_ring_entry_t *)&amp;amp;(((char *)self-&amp;gt;entries)[byte_ix]);

        expected              = h4x0r_atomic_load(&amp;amp;cur-&amp;gt;info);
        candidate.write_epoch = 0;
        candidate.state       = H4X0R_RING_RESERVED;

        if (h4x0r_atomic_cas(&amp;amp;cur-&amp;gt;info, &amp;amp;expected, candidate)) {
            break;
        }

        if (!h4x0r_ring_can_write_here(expected,
                                       start_epoch,
                                       self-&amp;gt;last_entry)) {
            ix = (ix + 1) &amp;amp; self-&amp;gt;last_entry;
            continue;
        }

        if (h4x0r_atomic_cas(&amp;amp;cur-&amp;gt;info, &amp;amp;expected, candidate)) {
            break;
        }
        ix = (ix + 1) &amp;amp; self-&amp;gt;last_entry;
    }

    memcpy(cur-&amp;gt;data, item, len);

    candidate.write_epoch = h4x0r_word_ring_enqueue(self-&amp;gt;word_ring,
                                                    (void *)ix);
    candidate.state       = H4X0R_RING_ENQUEUE_DONE;
    cur-&amp;gt;len              = len;

    h4x0r_atomic_store(&amp;amp;cur-&amp;gt;info, candidate);
}
&lt;/code&gt;
    &lt;p&gt;The only new helper function here is &lt;code&gt;h4x0r_ring_can_write_here()&lt;/code&gt; and its helper:&lt;/p&gt;
    &lt;code&gt;
static inline bool
h4x0r_ring_entry_is_being_used(h4x0r_ring_entry_info_t info)
{
    if (info.state &amp;amp; H4X0R_RING_USED) {
        return true;
    }

    return false;
}

static inline bool
h4x0r_ring_can_write_here(h4x0r_ring_entry_info_t info,
                          uint64_t                my_write_epoch,
                          uint32_t                last_entry)
{
    if (h4x0r_ring_entry_is_being_used(info)) {
        return false;
    }

    if (info.write_epoch &amp;gt; my_write_epoch) {
        return false;
    }

    if (info.write_epoch &amp;gt; (my_write_epoch - (last_entry + 1))) {
        return false;
    }

    return true;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The dequeue operation&lt;/head&gt;
    &lt;p&gt;Dequeuers (readers) take the following steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dequeue a value from &lt;code&gt;R&lt;/code&gt;, which gives us the index into&lt;code&gt;S&lt;/code&gt;we need; at the same time, we use our reference parameter to capture the epoch the writer used to make sure we don’t read from the future.&lt;/item&gt;
      &lt;item&gt;Attempt to mark the cell in &lt;code&gt;S&lt;/code&gt;for reading and validating. If validation fails, we restart.&lt;/item&gt;
      &lt;item&gt;Actually perform the read.&lt;/item&gt;
      &lt;item&gt;They mark the cell state in &lt;code&gt;S&lt;/code&gt;to indicate the read is done.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that a slow dequeuer might find that by the time they attempt to flag the cell in &lt;code&gt;L&lt;/code&gt; for read, someone has already claimed that cell for writing a newer log message. In such cases, the slow dequeuer just needs to try again.&lt;/p&gt;
    &lt;p&gt;Or, the writer may not have stored its epoch yet. We know if they got a ticket, we can go find the right slot. If the epoch is anything less than the epoch we dequeued, it’s definitely our value to dequeue.&lt;/p&gt;
    &lt;p&gt;For the dequeue, we’ll use these two very simple helpers:&lt;/p&gt;
    &lt;code&gt;static inline bool
h4x0r_ring_can_dequeue_here(h4x0r_ring_entry_info_t info,
                            uint64_t                expected_epoch)
{
    if (info.write_epoch &amp;gt; expected_epoch) {
        return false;
    }

    return true;
}

static inline uint64_t
h4x0r_ring_set_dequeue_done(uint64_t state)
{
    return state &amp;amp; H4X0R_RING_DEQUEUE_DONE_MASK;
}
&lt;/code&gt;
    &lt;p&gt;Our dequeue function is going to return whether there was a dequeue or not, instead of returning a value, and use a parameter for people to check if the queue was empty.&lt;/p&gt;
    &lt;p&gt;That’s because we’re going to need the caller to pass in a buffer for the result. We DEFINITELY don’t want to pass back a pointer to the ring entry; that’s a recipe for disaster.&lt;/p&gt;
    &lt;head rend="h1"&gt;Testing our rings&lt;/head&gt;
    &lt;p&gt;Especially since we’re dealing with concurrency, we want to make sure to test thoroughly. We should run in a number of different configurations, and should thoroughly test to make sure that we only ever see linearized results when we dequeue.&lt;/p&gt;
    &lt;p&gt;We’re also going to want to count some stuff, then check it all for consistency:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We should count successful dequeues.&lt;/item&gt;
      &lt;item&gt;We should independently count the number of drops, too.&lt;/item&gt;
      &lt;item&gt;We should capture the number of times fast readers find an empty queue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each thread should collect metrics privately, and then add them to totals, when it’s done with the work.&lt;/p&gt;
    &lt;p&gt;If we add independently collected dequeues to drops, we should get the total number of enqueues; otherwise we have a bug.&lt;/p&gt;
    &lt;p&gt;You’ll need to know when to stop trying to dequeue. The simplest path is to have the main thread first &lt;code&gt;join()&lt;/code&gt; on all enqueuers; at that point, we know there’s nothing else to enqueue. So when a dequeue thread sees all writers are done, the first time they encounter an empty queue, they know they’re done. That way is easy to implement, but leaves a small window where the empty dequeue time will push up. You can instead have individual writer threads decrement a global counter, which will shorten that window.&lt;/p&gt;
    &lt;p&gt;Also, even if it’s not real world, we should test for worst case, and run enqueues and dequeues as back to back as possible, to help us understand worst case performance, or any other considerations we might need.&lt;/p&gt;
    &lt;p&gt;I’ll spare you the code, because you can go get it in the codeberg repo. But, it does iterate over both types of ring, using five different sizes of buffer, and with a variety of (mostly imbalanced) readers and writers.&lt;/p&gt;
    &lt;p&gt;Let’s look at some example output though (I’ve deleted a few columns to keep us under 80 characters).&lt;/p&gt;
    &lt;p&gt;First, for the main, arbitrary ring, let’s look at our most minimal ring size:&lt;/p&gt;
    &lt;code&gt;Tests for queue with 16 items:
Test   Th+    Th-   Time(sec)     Q-             Q💧    Mop/s (✅)
------------------------------------------------------------------
# 1    1      1      0.0246     251,355       10,781      10.23   
# 2    2      2      0.0505     235,606        3,705       5.11   
# 3    4      4      0.1124      68,417      187,282       0.67   
# 4    8      8      0.2179     199,100       34,775       1.04   
# 5    2      1      0.0364     110,678      132,274       3.57   
# 6    4      1      0.0624      40,678      210,810       0.82   
# 7    8      1      0.0855       7,927      251,875       0.12   
# 8    1      2      0.0271     176,210       85,934       6.51   
# 9    1      4      0.0472     183,065       79,079       3.88   
#10    1      8      0.0930     183,732       78,412       1.97   
&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;Th+&lt;/code&gt; is the number of enqueue threads. The &lt;code&gt;-&lt;/code&gt; sign denotes the dequeue size.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Q💧&lt;/code&gt; is the number of drops. Then, the last column denotes how many millions of ops per second we performed in that test case.&lt;/p&gt;
    &lt;p&gt;The three columns we omitted that you’ll see in the output:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Q+&lt;/code&gt;is the total number of enqueues, which is always&lt;code&gt;262,144&lt;/code&gt;in my runs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Q∅&lt;/code&gt;, the number of times a dequeuer attempted to dequeue, and found the queue was empty.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Mop/s (✅+∅)&lt;/code&gt;which recomputes Mop/sec, including dequeues that find the queue empty.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One thing that should jump out to you is that there are an absurd number of drops in there. And when we get those absurd drops, our overall performance tends to plummet. The table makes it clear that we need to do more to deal with the contention.&lt;/p&gt;
    &lt;p&gt;That’s the kind of concern you should be looking for when testing parallel algorithms.&lt;/p&gt;
    &lt;p&gt;The insight makes sense; if the queue is full, the writers help with the tail, but then go back to competing where the table comes together.&lt;/p&gt;
    &lt;p&gt;The conclusion I drew is that, before the queue fills, enqueuers should briefly pause to give preference to readers, so as not to contend with them. In the code repo for this article, I did just that for you, setting the threshold to 75%, which gives much better results:&lt;/p&gt;
    &lt;code&gt;Tests for queue with 16 items:
Test   Th+    Th-   Time(sec)     Q-         Q💧    Mop/s (✅)
--------------------------------------------------------------
# 1    1      1      0.0248     262,142        2      10.55
# 2    2      2      0.0450     260,071       43       5.82
# 3    4      4      0.0923     236,809      434       2.83
# 4    8      8      0.2132     208,501      177       1.23
# 5    2      1      0.0400     256,786       32       6.55
# 6    4      1      0.0479     253,364       20       5.47
# 7    8      1      0.0989     219,220       53       2.65
# 8    1      2      0.0384     262,101       43       6.83
# 9    1      4      0.0489     262,084       60       5.36
#10    1      8      0.0868     261,814      330       3.02
&lt;/code&gt;
    &lt;p&gt;If we run more tests, we may see some significant drops, but we’d expect big numbers only when the number of writers greatly outweighs the number of readers. And in that case, the drops are expected. On my machine, this only ever happens for 16-item queues though. Even at 128 items, it looks good (on my machine):&lt;/p&gt;
    &lt;code&gt;Tests for queue with 128 items:
Test   Th+    Th-   Time(sec)      Q-           Q💧     Mop/s (✅)  
------------------------------------------------------------------
#11    1      1      0.0147     262,145          0      17.82
#12    2      2      0.0428     262,145          0       6.13
#13    4      4      0.0765     262,144          1       3.42
#14    8      8      0.1879     262,140          5       1.39
#15    2      1      0.0263     262,145          0       9.97
#16    4      1      0.0517     262,145          0       5.07
#17    8      1      0.0936     262,145          0       2.80
#18    1      2      0.0251     262,144          1      10.43
#19    1      4      0.0417     262,142          3       6.28
#20    1      8      0.0859     262,145          0       3.05
&lt;/code&gt;
    &lt;p&gt;If we study these charts, we can compare to see exactly how big an impact that contention actually has. In tests where we were dropping, the number of operations plummeted massively due to the contention.&lt;/p&gt;
    &lt;p&gt;With the above tweak to the enqueuer’s help rules, my laptop tests never fail to top 1 million operations a second, and raw word-queue performance peaks at about 24 Mop/sec, and stays above 3 MOp/sec for all but a couple of configurations (the ones where I’ve overcommitted my cores w 8q+/8q-, so past the point where I’ve maxed out potential parallelism).&lt;/p&gt;
    &lt;p&gt;Not bad, considering we haven’t really done anything to optimize (I do have a more optimized implementation of the base word-ring algorithm that can get as high at 40Mop/sec with one enqueuer and one dequeuer, and bottoms out at 4Mops/sec when the number of dequeuers is lopsided, all on the same machine).&lt;/p&gt;
    &lt;p&gt;My test bed is there in the repo for you to use if you like.&lt;/p&gt;
    &lt;head rend="h1"&gt;Did we forget to order something?&lt;/head&gt;
    &lt;p&gt;When I wrote about futexes and mutexes, I glossed over the topic of memory ordering, because it’s notoriously hard to communicate well. But given we’re into the world of concurrency without locks, I think it’s remiss not to cover it. I’ll try to make it as straightforward as possible.&lt;/p&gt;
    &lt;p&gt;If you are still confused after reading this section (which is likely), give me feedback on what questions it leaves you with, and I’ll try again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your compiler may be gaslighting you&lt;/head&gt;
    &lt;p&gt;Your compiler wants you to think your code will execute in the order you’d expect while staring at the source code.&lt;/p&gt;
    &lt;p&gt;Meanwhile, behind your back, it’s generally going way out of its way to thwart your expectations. Though, to be fair, it’s doing it for you. It knows how disappointed you’d be if it performs poorly for you.&lt;/p&gt;
    &lt;p&gt;So yes, the compiler will absolutely rewrite your code (particularly when you turn on any level of optimization). It has no qualms changing the order you’d expect, with hopes of it running faster for you. But, it’s hoping you won’t notice. The transformations often aren’t semantically identical to what you might have intended, but, at least in the context of a single thread, you’re not likely to notice the difference.&lt;/p&gt;
    &lt;p&gt;Even for multi-threaded programs, compilers often try hard to optimize what you’re doing, transforming and reordering to take advantage of the CPU.&lt;/p&gt;
    &lt;p&gt;Still, there are things the compiler won’t do (just like Meatloaf). Specifically, compilers have this idea of “barriers”, which are features in your code that the compiler won’t move stuff past.&lt;/p&gt;
    &lt;p&gt;For instance, compilers will not move code across the boundary of a mutex, or any op on a variable labeled &lt;code&gt;_Atomic&lt;/code&gt; (unless you explicitly allow it at the call site).&lt;/p&gt;
    &lt;p&gt;The compiler is conservative here, because you expressed your intent. Event if the compiler thinks you’re walking away from a big performance gain, and even if they could “prove” that it’s not going to change the semantics of your program. Generally, function boundaries result in a barrier as well, as long as they’re not inlined.&lt;/p&gt;
    &lt;p&gt;But, even with multi-processor programs, the compiler does all that analysis as if a single thread is running. So it can move data around across threads in many frustrating ways. If you don’t pay attention to how you handle concurrency, compiler transformations can definitely make race conditions far worse.&lt;/p&gt;
    &lt;head rend="h2"&gt;The processor is a bad influence&lt;/head&gt;
    &lt;p&gt;Making matters worse, your compiler’s friend, the processor, tries to apply parallelism everywhere it can, because of performance. So there are also plenty of architectural considerations that can lead to data races.&lt;/p&gt;
    &lt;p&gt;For instance, you probably know that the CPU is fast, and memory accesses are slow. And when many threads are competing to load memory at the same time, things can get chaotic, because memory cells loaded into registers on one core don’t magically sync instantly across multiple cores.&lt;/p&gt;
    &lt;p&gt;And, the processor may do its own reordering of instructions, for instance, to achieve fine-grained parallelism via things like instruction pipelining, which can definitely run your code out of order.&lt;/p&gt;
    &lt;p&gt;Processors not only have a very complex memory model, but that model can be vastly different across architectures (e.g., x86 and ARM).&lt;/p&gt;
    &lt;p&gt;Very few programmers are going to be aware of most of that subtlety.&lt;/p&gt;
    &lt;p&gt;Languages could generate code to make sure everything always happens in a well-defined order (full sequential consistency), but generally they do not. Processors go out of their way to make things faster by moving your code around, and compilers often do a lot to make your code faster too.&lt;/p&gt;
    &lt;p&gt;So no language is going to find it an acceptable hit to force the processor to run everything in a well-defined order, at least in the case of multiple threads.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to make this relationship work&lt;/head&gt;
    &lt;p&gt;Programmers typically will need to tell their compiler where to get conservative, and sacrifice performance for correctness. In C, you can do that by labeling variables as &lt;code&gt;_Atomic&lt;/code&gt;, which tells the compiler that variables will be used across threads.&lt;/p&gt;
    &lt;p&gt;But &lt;code&gt;_Atomic&lt;/code&gt; doesn’t truly mean, “always atomic”, primarily because you can explicitly ask for different behavior on a case-by-case basis.&lt;/p&gt;
    &lt;p&gt;If you don’t specify anything other than &lt;code&gt;_Atomic&lt;/code&gt;, it does mean that you’re not going to get corrupting data races, and it does mean that, by default, the compiler will ensure all operations on that variable will happen in the same order, from the perspective of any thread.&lt;/p&gt;
    &lt;p&gt;Enforcing that kind of order generally slows things down, so, you can specify to the compiler cases where you want different behavior. For instance, if you’re initializing memory, you probably have exclusive access to the data. Your own view on the ordering is consistent anyway, so at this point you may not care to slow down the processor.&lt;/p&gt;
    &lt;p&gt;However, that could be problematic for the next thread to read. Since the first access didn’t care, it’s definitely possible for a thread to get a reference to the object, and see the pre-initialization value, but only if that second access explicitly relaxes its requirement for getting access to the variable.&lt;/p&gt;
    &lt;p&gt;Generally, those kinds of surprises are easy to find, especially when they involve fields in an object whose pointer you read atomically, but whose fields are not marked as being &lt;code&gt;_Atomic&lt;/code&gt;. It’s a great recipe for Heisenbugs.&lt;/p&gt;
    &lt;p&gt;So generally, if you know multiple threads will handle a variable, not only should you go ahead and declare it as &lt;code&gt;_Atomic&lt;/code&gt;, but also you should avoid giving up most of its protection– you’re just inviting disaster.&lt;/p&gt;
    &lt;head rend="h3"&gt;My memory order arrived damaged&lt;/head&gt;
    &lt;p&gt;By default, accessing a single &lt;code&gt;_Atomic&lt;/code&gt; variable will make sure that any changes to the variable happen in a well-defined (linearized) order. For example, let’s say we have a huge number of threads, and thread &lt;code&gt;0&lt;/code&gt; goes to modify &lt;code&gt;_Atomic&lt;/code&gt; variable &lt;code&gt;A&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Just by declaring the variable to be &lt;code&gt;_Atomic&lt;/code&gt;, the compiler will, if necessary, generate code that makes sure that any changes to &lt;code&gt;A&lt;/code&gt; that were in flight when thread &lt;code&gt;0&lt;/code&gt; goes to modify it, all end before its operation. Similarly, any subsequent loads of that variable will see the reflected value.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;_Atomic&lt;/code&gt; variable access (unless relaxed) acts like a barrier that the compiler will enforce for the variable in question. That enforcement, though, is really done by generating an assembly that helps get the proper semantics, which is very platform dependent.&lt;/p&gt;
    &lt;p&gt;If you do not mark a variable as &lt;code&gt;_Atomic&lt;/code&gt;, then you should not be using the variable across threads. The compiler will certainly generate code under the assumption that those variables won’t have to deal with concurrent access.&lt;/p&gt;
    &lt;p&gt;But we do have some options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Relaxed Memory Order. In C/C++ parlance, the semantics of variables that are not marked&lt;/p&gt;&lt;code&gt;_Atomic&lt;/code&gt;is called relaxed memory order. It’s a weird name that means there are no ordering guarantees outside of what would be promised by the underlying architecture for a non-atomic variable, and the promise of non-corrupting data races. That’s scary.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sequentially Consistent Ordering. This lives on the other end of the spectrum from relaxed ordering. Using this is supposed to ensure a global ordering for all atomic data by default, at the price of some efficiency. This is the default memory ordering, and is the strongest, but it does have some slight issues we’ll discuss in a minute.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Acquire / Release Ordering. This is in between relaxed and sequentially consistent. It basically does what you want it to do on a variable-by-variable basis, forcing a well defined ordering.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You’ll often see “Acquire” and “Release” listed as separate strategies. They’re more like two sides of the same coin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;acquire semantics apply only to loading an&lt;/p&gt;&lt;code&gt;_Atomic&lt;/code&gt;variable (i.e., acquiring it). The guarantee is that any modifications to a memory address that were made by other threads will be reflected by the time the value loads, with nothing still in progress.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;release semantics only apply to storing an&lt;/p&gt;&lt;code&gt;_Atomic&lt;/code&gt;value (i.e., releasing it). The guarantee here is that the store operation will be reflected in the next subsequent load of that address. That is to say, once the store finishes, no other thread will be able to load the previous value.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For some operations, only one of these two things makes sense. For instance, acquire semantics make sense for an &lt;code&gt;atomic_load()&lt;/code&gt; call, but that doesn’t update the value, so release semantics don’t make sense here (and thus, acquire/release doesn’t make sense either).&lt;/p&gt;
    &lt;p&gt;You don’t specify memory ordering when you declare something &lt;code&gt;_Atomic&lt;/code&gt;. By default, every operation for such variables will get the strongest memory ordering.&lt;/p&gt;
    &lt;p&gt;If you want anything else, you can get it on a call-by-call basis every time the variable is used, by calling a call in the C atomic library that allows you to explicitly change to another ordering, but only at that one slot.&lt;/p&gt;
    &lt;p&gt;Generally, the defaults are going to be least surprising (in a world where surprises are common, and understanding what’s going on is hard).&lt;/p&gt;
    &lt;p&gt;That doesn’t mean that most strict memory ordering is perfect: sequential consistency has some issues that prevent it from living up to its name, particularly when you end up mixing access with different memory orderings (see section 2 of this paper for more details).&lt;/p&gt;
    &lt;p&gt;Plus, sequential consistency generally isn’t much stronger than acquire / release semantics, and it can be slower, depending on the architecture. So it’s pretty reasonable to use acquire/release semantics as the default.&lt;/p&gt;
    &lt;p&gt;But this stuff is trickier than we might think.&lt;/p&gt;
    &lt;p&gt;For instance, you may have noticed that I declared the &lt;code&gt;item&lt;/code&gt; field inside the struct &lt;code&gt;h4x0r_word_ring_item_t&lt;/code&gt; to be &lt;code&gt;_Atomic&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you remove that qualifier, on some platforms our tests will occasionally fail. Why? How is that possible??&lt;/p&gt;
    &lt;p&gt;Sure, we dequeue an entire &lt;code&gt;h4x0r_word_ring_item_t&lt;/code&gt; atomically. But, we store the result of that dequeue into a second memory location, that isn’t itself marked to be atomic. In our case, in our word ring dequeue algorithm, that’s going to be the variable called &lt;code&gt;last&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So, when we return &lt;code&gt;last.item&lt;/code&gt;, we might end up returning the value that was stored in that field before the CAS operation.&lt;/p&gt;
    &lt;p&gt;Since we also return the associated epoch, we could possibly get an old value there, too. However, since they are only unloaded into the &lt;code&gt;last&lt;/code&gt; field together, (atomically), we can be pretty confident that, if the item is available, then the epoch will be too.&lt;/p&gt;
    &lt;p&gt;Still, C doesn’t guarantee that; it’s just a matter of having some knowledge of what’s going on under the hood (and experience to back it up).&lt;/p&gt;
    &lt;p&gt;But, if you wanted to be really careful, you would want to tag the epoch field as &lt;code&gt;_Atomic&lt;/code&gt; too. If you try though, you’ll get an error, because &lt;code&gt;_Atomic&lt;/code&gt; doesn’t work with bit slices directly. You’d have to do something else. Some options are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use a union, with one of the types in the union being an &lt;code&gt;_Atomic uint64_t&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Declare the thing as a &lt;code&gt;uint64_t&lt;/code&gt;, and manage the flag bit manually (but you do need the flag to be a bit).&lt;/item&gt;
      &lt;item&gt;Leave it unsynced, and tell the compiler to sync all relevant variables before pulling the epoch out of the item.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This third option you can do with a memory fence, putting it before the assignment in &lt;code&gt;h4x0r_word_ring_found&lt;/code&gt;. In our associated code, &lt;code&gt;h4x0r_atomic_fence()&lt;/code&gt; will do the trick; this is a very trivial wrapper around a C atomic builtin.&lt;/p&gt;
    &lt;p&gt;By the way, if we tag &lt;code&gt;item&lt;/code&gt; as being &lt;code&gt;_Atomic&lt;/code&gt;, but specify relaxed memory ordering when we copy it out, we could absolutely end up with the same problem, because &lt;code&gt;_Atomic&lt;/code&gt; is only atomic until you explicitly tell it otherwise.&lt;/p&gt;
    &lt;p&gt;Yes, there are a lot of subtleties. Nobody said it would be easy.&lt;/p&gt;
    &lt;p&gt;Good luck, you’re going to need it.&lt;/p&gt;
    &lt;p&gt;– Lee T. Solo (tho with a ring on it)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288286</guid><pubDate>Tue, 16 Dec 2025 13:32:42 +0000</pubDate></item><item><title>Rust GCC back end: Why and how</title><link>https://blog.guillaume-gomez.fr/articles/2025-12-15+Rust+GCC+backend%3A+Why+and+how</link><description>&lt;doc fingerprint="b6601b5c7a5f3054"&gt;
  &lt;main&gt;&lt;p&gt;Whenever you compile using Rust, the compiler goes through different passes and in the end, generated binary code for the target processor. By default, it uses LLVM as backend to generate the binary code, but more backends exist like cranelift and GCC. This post is about how it's possible for one compiler to use different backend to generate binaries, in particular GCC.&lt;/p&gt;&lt;p&gt;Before going into details, we need to describe how compilers actually work. They read source code and convert it internally into a format they can manipulate, commonly called Abstract Syntax Tree (shortened "AST").&lt;/p&gt;&lt;p&gt;However, compilers go through multiple passes, and often each pass has their own AST. Let's take a short and very incomplete example with the Rust compiler passes. We have 4 steps (again, this is simplified!):&lt;/p&gt;&lt;p&gt;Each step generates a new AST with new information if no error was encountered and provides it to the next pass.&lt;/p&gt;&lt;p&gt;Little side-note: If enough people are interested by this topic, I can write a (much) longer explanation of these passes.&lt;/p&gt;&lt;p&gt;So now that we have a high-level idea of Rust compiler passes, what is the difference between "front-end" and "back-end" exactly?&lt;/p&gt;&lt;p&gt;We consider the front-end to be the part handling (high-level non-exhaustive list) code parsing, linting, type-checking and borrow-checking (steps 1 to 3). When all this is done, it means the code is valid and needs to be translated to the target processor instructions set. To do so, we call LLVM/GCC which will translate the Rust compiler AST into assembly code (step 4).&lt;/p&gt;&lt;p&gt;The Rust compiler backends are the bridge between the Rust compiler AST and the actual code generator. They receive the AST and call the LLVM/GCC/... API which will in turn run their passes, optimize and finally generate the assembly code.&lt;/p&gt;&lt;p&gt;LLVM being much more recent than GCC (2003 vs 1987), a lot of older processors are not supported and will never be. So if you want to write a Rust program on an old platform like Dreamcast, you have no choice to either write your own backend or use the GCC backend (or the &lt;code&gt;gccrs&lt;/code&gt; front-end once ready).&lt;/p&gt;&lt;p&gt;For the readers interested in doing so, there is a guide explaining how to build Rust programs for Dreamcast here.&lt;/p&gt;&lt;p&gt;The GCC backend is different than gccrs which is a front-end for GCC written in C++, which doesn't reuse the front-end of &lt;code&gt;rustc&lt;/code&gt;, meaning they need to reimplement parsing, type-checking, linting, borrow-checking, compilation errors, etc.&lt;/p&gt;&lt;p&gt;On the other hand, the GCC backend (the crate name is &lt;code&gt;rustc_codegen_gcc&lt;/code&gt;) is just "yet another backend codegen" of the Rust compiler, like LLVM or Cranelift, only meant to generate the binary from the Rust compiler input. It's a bridge between Rust compiler's AST and the codegen API.&lt;/p&gt;&lt;p&gt;On that note: GCC doesn't provide a nice library to give access to its internals (unlike LLVM). So we have to use &lt;code&gt;libgccjit&lt;/code&gt; which, unlike the "jit" ("just in time", meaning compiling sub-parts of the code on the fly, only when needed for performance reasons and often used in script languages like Javascript) part in its name implies, can be used as "aot" ("ahead of time", meaning you compile everything at once, allowing you to spend more time on optimization). To do so we use bindings, which are split in two parts:&lt;/p&gt;&lt;code&gt;gccjit-sys&lt;/code&gt; which redeclares the C items we need.&lt;code&gt;gccjit&lt;/code&gt; which provides a nice API over &lt;code&gt;gccjit-sys&lt;/code&gt;.&lt;p&gt;If you want to write your own compiler and use GCC as codegen, you can do it thanks to &lt;code&gt;libgccjit&lt;/code&gt;. And if you write it in Rust, you can even use the Rust bindings.&lt;/p&gt;&lt;p&gt;Rustc has a crate named &lt;code&gt;rustc_codegen_ssa&lt;/code&gt; which provides an abstract interface that a backend needs to implement through traits like:&lt;/p&gt;&lt;p&gt;The full list is available here.&lt;/p&gt;&lt;p&gt;One last thing you need to write in your backend:&lt;/p&gt;&lt;code&gt;Run #[no_mangle]
pub fn _rustc_codegen_backend() -&amp;gt; Box&amp;lt;dyn CodegenBackend&amp;gt; {
    // This is the entrypoint.
}&lt;/code&gt;
&lt;p&gt;This is the function that will be called by rustc to run your backend.&lt;/p&gt;&lt;p&gt;Let's take an example: how the GCC backend creates a constant string. I picked this one because it's small enough to showcase how things work while not being too much information to digest at once.&lt;/p&gt;&lt;p&gt;In the ConstCodegenMethods trait, there is a const_str method. This is the method we will implement to declare a constant string.&lt;/p&gt;&lt;p&gt;So the method implementation so far looks like this:&lt;/p&gt;&lt;code&gt;Run impl&amp;lt;'gcc, 'tcx&amp;gt; ConstCodegenMethods for CodegenCx&amp;lt;'gcc, 'tcx&amp;gt; {
    /// Returns the pointer to the string and its length.
    fn const_str(&amp;amp;self, s: &amp;amp;str) -&amp;gt; (RValue&amp;lt;'gcc&amp;gt;, RValue&amp;lt;'gcc&amp;gt;) {
        // Call GCC API to declare this string.
    }
}&lt;/code&gt;
&lt;p&gt;We need to pause here to give some extra explanations: &lt;code&gt;CodegenCx&lt;/code&gt; is the type on which most &lt;code&gt;rustc_codegen_ssa&lt;/code&gt; traits will be implemented. It is created in each ExtraBackendMethods::compile_codegen_unit call and passed down from there to generate the code for this module. You can consider it the same as a cache. It keeps the list of items declared, like functions, types, globals, etc. But also information such as "boolean type", "i8 type" and equivalents so we don't need to recompute them every time we need them.&lt;/p&gt;&lt;p&gt;Ok so now let's actually implement it. We have a few things to do:&lt;/p&gt;&lt;code&gt;*const u8&lt;/code&gt;) into the C type (&lt;code&gt;*const char&lt;/code&gt;).&lt;p&gt;Let's translate it into code with a lot of comments to help understanding what's going on:&lt;/p&gt;&lt;code&gt;Run fn const_str(&amp;amp;self, s: &amp;amp;str) -&amp;gt; (RValue&amp;lt;'gcc&amp;gt;, RValue&amp;lt;'gcc&amp;gt;) {
    // We get the const string cache.
    let mut const_str_cache = self.const_str_cache.borrow_mut();
    // We get the address of the stored string and we add it to the cache and
    // return its address.
    let str_global = const_str_cache.get(s).copied().unwrap_or_else(|| {
        // We call the `GCC` API to create a new const string.
        let string = self.context.new_string_literal(s);
        // We name the const.
        let sym = self.generate_local_symbol_name("str");
        // We declare it.
        let global = self.declare_private_global(&amp;amp;sym, self.val_ty(string));
        // All done, we can add it to the cache and return it.
        const_str_cache.insert(s.to_owned(), global);
        global
    });
    let len = s.len();
    // We cast the pointer to the target architecture string pointer type.
    let cs = self.const_ptrcast(
        str_global.get_address(None),
        self.type_ptr_to(self.layout_of(self.tcx.types.str_).gcc_type(self)),
    );
    // And we return the pointer and its length.
    (cs, self.const_usize(len as _))
}&lt;/code&gt;
&lt;p&gt;But the codegen backends can also add more information to the underlying binary code generator. For example, in Rust, we use references a lot. A reference is basically a pointer that cannot be &lt;code&gt;NULL&lt;/code&gt;. We need to give this information as well!&lt;/p&gt;&lt;p&gt;In both GCC and LLVM, you can add attributes to a lot of items, like arguments of functions. So every time we see an argument behind a reference, we add the &lt;code&gt;nonnnull()&lt;/code&gt; attribute.&lt;/p&gt;&lt;p&gt;Let's show an example with this Rust function:&lt;/p&gt;&lt;code&gt;Run fn t(a: &amp;amp;i32) -&amp;gt; i32 {
    *a
}&lt;/code&gt;
&lt;p&gt;The C equivalent looks like this:&lt;/p&gt;&lt;code&gt;int t(int *a) {
  if (!a) {
    return -1;
  }
  return *a;
}&lt;/code&gt;
&lt;p&gt;Compiled with the &lt;code&gt;-O3&lt;/code&gt; option, it generates this assembly:&lt;/p&gt;&lt;code&gt;t:
        test    rdi, rdi              ; Check if `a` is 0
        je      .L5                   ; If `a` is 0, we jump to `.L1`
        mov     eax, DWORD PTR [rdi]  ; We store `*a` value into `eax` registry
        ret                           ; We exit the function
.L5:
        mov     eax, -1               ; We store `-1` into `eax` registry
        ret                           ; We exit&lt;/code&gt;
&lt;p&gt;However, the Rust compiler knows that &lt;code&gt;a&lt;/code&gt; can never be &lt;code&gt;NULL&lt;/code&gt;, so the codegen adds &lt;code&gt;_attribute_((nonnull(1)))&lt;/code&gt; on the function:&lt;/p&gt;&lt;code&gt;_attribute_((nonnull(1)))
int t(int *a) {
  if (!a) {
    return -1;
  }
  return *a;
}&lt;/code&gt;
&lt;p&gt;Which generates this assembly:&lt;/p&gt;&lt;code&gt;t:
        mov     eax, DWORD PTR [rdi]
        ret&lt;/code&gt;
&lt;p&gt;Since the codegen knows that the &lt;code&gt;if (!a)&lt;/code&gt; condition will never be true, why keeping it around?&lt;/p&gt;&lt;p&gt;And it's just one example of extra information/optimization we do in the Rust backends. And that doesn't even cover in the slighest the monstruous amount of optimizations the codegen themselves do. If you want to have more examples of such optimizations, I strongly recommend reading the "Advent of Compiler Optimizations" blog posts written by Matt Godbolt (the developer of godbolt.org, another priceless tool).&lt;/p&gt;&lt;p&gt;So now you know what a Rust backend is, and why GCC backend is also an interesting thing to have while also learning about some optimizations we do behind developers back. :)&lt;/p&gt;&lt;p&gt;This blog post was made thanks to my cat hanging to it.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288291</guid><pubDate>Tue, 16 Dec 2025 13:33:25 +0000</pubDate></item><item><title>I don't think Lindley's paradox supports p-circling</title><link>https://vilgot-huhn.github.io/mywebsite/posts/20251206_p_circle_lindley/</link><description>&lt;doc fingerprint="2456805317e68cd8"&gt;
  &lt;main&gt;
    &lt;p&gt;I don’t think Lindley’s paradox supports p-circling&lt;/p&gt;
    &lt;p&gt;hypothesis testing&lt;/p&gt;
    &lt;p&gt;Don’t give p-values a role they’re not made for&lt;/p&gt;
    &lt;p&gt;Author&lt;/p&gt;
    &lt;p&gt;Vilgot Huhn&lt;/p&gt;
    &lt;p&gt;Published&lt;/p&gt;
    &lt;p&gt;December 7, 2025&lt;/p&gt;
    &lt;p&gt;As usual I’d like to preface all this that I write these blogposts as attempts to make sense of a subject for my own sake. I am not an expert here and it is likely I am confused about some details. On the other hand, I think “confused” discourse can also be productive to read and participate in. Being confused is just the first step towards being unconfused, to paraphrase Jake The Dog.&lt;/p&gt;
    &lt;p&gt;p-value circling&lt;/p&gt;
    &lt;p&gt;100 years ago this year Fisher arbitrarily suggested using p &amp;lt; 0.05 as a cut-off for “significant” and ever since we’ve just gone along with it. “Why is it 0.05?” people have critically asked for one hundred years. Unfortunately “arbitrariness”, as a critique, is only effective if you are able to suggest less a arbitrary value, and despite many efforts to change this the convention has remained.&lt;/p&gt;
    &lt;p&gt;The act of p-value circling is to look at a p-value that’s significant but close to 0.05 and go: “hm, I don’t know about that…” Perhaps you use a red ballpoint pen to circle it on the print journal you subscribe to in the year 2025. If not, you may underline it with some sort of digital pen technology and share it online.&lt;/p&gt;
    &lt;p&gt;“Hmm… Suspicious…”&lt;/p&gt;
    &lt;p&gt;What (potentially) justifies p-value circling?&lt;/p&gt;
    &lt;p&gt;Before we get into it let’s briefly try to remind ourselves what p-values are even supposed to do. (This will be a brief summary, if you want to learn this for real I recommend reading Daniël Lakens free online textbook, which all this borrows heavily from.)&lt;/p&gt;
    &lt;p&gt;As far as I’ve understood, Fishers idea about p-values was supplanted by the more rigorous (in terms of statistical philosophy) Neyman-Pearson framework. It is within this framework we find the familiar type 1 and type 2 error rates. Probability is viewed as being about outcomes in a hypothetical scenario where a procedure is repeated many times. You’re actually supposed to set the \(\alpha\) at a level that’s justifiable based on what null hypothesis you’re testing. As far as I’ve understood no one has ever done so, except that one time physicists at CERN decided they wanted to be really sure they didn’t incorrectly claim they found the Higgs boson.1 Instead everyone just uses the arbitrary convention of \(\alpha = 0.05\).&lt;/p&gt;
    &lt;p&gt;If you assume that the null is true, the p-value distribution is uniform. Let’s do the exercise of generating a hundred thousand t-tests between two groups, n = 100 per group, where there is no mean difference. Then we’ll look at the p-values.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;p_vector &amp;lt;-c() #empty vectorfor(i in1:100000){ A &amp;lt;-rnorm(100, 0, 1) #normally distributed data from a group B &amp;lt;-rnorm(100, 0, 1) #another group with same mean p_vector[i] &amp;lt;-t.test(A,B)$p.value}d &amp;lt;-data.frame(p_vector, significant = (p_vector &amp;lt;0.05))ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0,1, length.out =101), color =NA) +#geom_hline(yintercept = 100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0,1, by =0.1)) +labs(title ="p-values under the null", x ="p-value", y ="count") +theme_bw() +ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;I remember this blew my mind when I fist saw it. I don’t think I was surprised exactly; it just made it all click. This flatness is what p-values are all about, man! The p-value distribution is uniform under the null! Yes! It is this property of the distribution that gives meaning to the type 1 error rate.&lt;/p&gt;
    &lt;p&gt;NP-frequentism is then based on committing to an alpha threshold beforehand and then exclaim “significant!” iff the p-value lands below it.2&lt;/p&gt;
    &lt;p&gt;Does it matter how far below it?&lt;/p&gt;
    &lt;p&gt;No! If the null is true every p-value is equally likely, right? Your sampling procedure of a null may give you 0.98, or 0.58, or 0.002, or 0.006775892. When you’re focused on whether toreject the (exact) null hypothesis, NP-frequentism works its magic by assuming the null is true which means p=0.01 is not in and of itself less consistent with this assumption than a p=0.048. If the null was true, you just got handed a random number between 0 and 1. All you get to choose is often you want to mistakenly “act” upon this information (in the long run).&lt;/p&gt;
    &lt;p&gt;So, how is this supposed to justify p-value circling? Well p-values are only valid if they’re valid. P-values only care about sampling error and – being an inert mathematical abstraction – can’t by themselves handle questionable research practices like p-hacking done by flesh and blood human researchers.&lt;/p&gt;
    &lt;p&gt;Let’s set up a scenario where we collect more data if our p-value happens to be not significant, but stop collecting data if it is. We’ll add a one-time option to include 20 more participants.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;p_vector &amp;lt;-c() #empty vectorfor(i in1:100000){ A &amp;lt;-rnorm(100, 0, 1) #normally distributed data from a group B &amp;lt;-rnorm(100, 0, 1) #another group with same meanif(t.test(A,B)$p.value &amp;gt;0.05){ A &amp;lt;-append(A, rnorm(20, 0, 1)) B &amp;lt;-append(B, rnorm(20, 0, 1)) } p_vector[i] &amp;lt;-t.test(A,B)$p.value}d &amp;lt;-data.frame(p_vector, significant = (p_vector &amp;lt;0.05))ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0,1, length.out =101), color =NA) +geom_hline(yintercept =100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0,1, by =0.1)) +labs(title ="p-values when p-hackning", x ="p-value", y ="count") +theme_bw() +ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;As you can see there’s now a little bump in what was previously flat. Some “marginally significant” p-values got lucky and grew a bit. The bin between 0.05 and 0.06 shrunk the most. Notably there’s a tilted shape to the significant values now.3&lt;/p&gt;
    &lt;p&gt;It is this shape, this knowledge that you can fudge p-values a little bit that I think could maybe give some justification to the act of p-value circling. Maybe. In that case a p = 0.048 makes people think: “hmm, I bet that was 0.051 and they strategically removed an outlier or something”&lt;/p&gt;
    &lt;p&gt;I don’t think it’s a very strong justification for being suspicious of p-values between 0.04 and 0.05. Part of this depends on how prevalent you believe p-hacking is. Basically you’re saying “my personal significance level is set a bit lower than convention, because I think the p-values I see are distorted by p-hacking”. I think that’s probably fine as an epistemic habit, but as an author getting p-circled it would likely feel as an unfair criticism.&lt;/p&gt;
    &lt;p&gt;sort(p_vector)[5000] #a suggestion, in this scenario&lt;/p&gt;
    &lt;p&gt;[1] 0.03800592&lt;/p&gt;
    &lt;p&gt;Even so, in this scenario I set up here there’s more additional spurious p-values below 0.04 than between 0.04 and 0.05. P-values are fickle things, they dance around, so even if you think the practice is common I don’t think you should put a lot of weight in the idea that whatever questionable statistical jutstu a researcher does to avoid a null result will put their (hacked) p-value just below the threshold.&lt;/p&gt;
    &lt;p&gt;Basically I don’t think a single p-value in and of itself can carry a lot of information about statistical malpractice. I’m sympathetic to the rule that if you’ve ever scoffed at someone using the term “marginally significant”, you’re not allowed to call something “marginally insignificant”4.&lt;/p&gt;
    &lt;p&gt;Lindley’s paradox&lt;/p&gt;
    &lt;p&gt;A potentially more sophisticated justification for p-circling is “Lindley’s paradox.”&lt;/p&gt;
    &lt;p&gt;I think many instinctively feel some resistance to a very strict interpretation of p-values, where their sole function is to be uniform under the null and the thing we care about is whether they clear our pre-specified alpha level. After all, we rightfully get annoyed when p-values are reported only with a less-than sign. And should we really not feel confident that there’s something there when we see a p = 0.001?&lt;/p&gt;
    &lt;p&gt;In this group-difference setup a smaller p-value implies a larger mean difference in your sample means, and you’re more likely to come across a large mean difference if you happen to be in a universe where there truly is a difference between the groups.&lt;/p&gt;
    &lt;p&gt;Let’s see how it looks like when we have a difference between the groups.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;p_vector &amp;lt;-c() #empty vectorfor(i in1:100000){ A &amp;lt;-rnorm(100, 0, 1) #normally distributed data from a group B &amp;lt;-rnorm(100, 0.2, 1) #another group with same mean p_vector[i] &amp;lt;-t.test(A,B)$p.value}d &amp;lt;-data.frame(p_vector, significant = (p_vector &amp;lt;0.05))ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0,1, length.out =101), color =NA) +geom_hline(yintercept =100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0,1, by =0.1)) +labs(title =paste("p-values when power =",round(sum(p_vector &amp;lt;0.05)/100000, 2)), x ="p-value", y ="count") +theme_bw() #+ ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;The sum of red colored p-values now represent power. Notice the switch! I think this may be a source of confusion here. We are now looking at a different type of p-value distribution. A p-value distribution that is not meant to illustrate the meaning of p-values. Power or type 2 error is, fundamentally, something else. It’s a different type of error.&lt;/p&gt;
    &lt;p&gt;When I fist saw this my mind immediately jumped to the idea of some p-values being “more consistent with the presence of an effect”. This is a bit off according to strict NP-frequentism; again, p-values get their meaning from assuming the null is true. Here we instead assume some effect is true.&lt;/p&gt;
    &lt;p&gt;The line represents where p-values would end up under the null. At this power, p-values in the 0.04 to 0.05 bin are more likely than they would be if the null was true. If we raise the power even further, we get to “Lindley’s paradox”, the fact that p-values in this bin can be less likely then they are under the null.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;p_vector &amp;lt;-c() #empty vectorfor(i in1:100000){ A &amp;lt;-rnorm(100, 0, 1) #normally distributed data from a group B &amp;lt;-rnorm(100, 0.55, 1) #another group with same mean p_vector[i] &amp;lt;-t.test(A,B)$p.value}d &amp;lt;-data.frame(p_vector, significant = (p_vector &amp;lt;0.05))ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0,1, length.out =101), color =NA) +geom_hline(yintercept =100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0,1, by =0.1)) +labs(title =paste("p-values when power =",round(sum(p_vector &amp;lt;0.05)/100000, 2)), x ="p-value", y ="count") +theme_bw() #+ ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;It’s kind of hard to see, since so many p-values end up in the 0-0.01 bin. Let’s zoom in on only the bins between 0.03 and 0.10.&lt;/p&gt;
    &lt;p&gt;Code&lt;/p&gt;
    &lt;p&gt;d &amp;lt;-subset(d, p_vector &amp;gt;0.03&amp;amp; p_vector &amp;lt;0.1)ggplot(d, aes(x = p_vector, fill = significant)) +geom_histogram(breaks =seq(0.03,0.1, length.out =8), color =NA) +geom_hline(yintercept =100000/100) +scale_fill_manual(values =c("FALSE"="steelblue", "TRUE"="red")) +scale_x_continuous(breaks =seq(0.03,0.1, by =0.1)) +labs(title =paste("p-values when power =",round(sum(p_vector &amp;lt;0.05)/100000, 2)), x ="p-value", y ="count") +theme_bw() #+ ylim(0,10000)&lt;/p&gt;
    &lt;p&gt;As you can see, the 0.04 to 0.05 bin is now below the line which represents the ideal flat null distribution. This then is (potentially) another reason to justify p-value circling: If a test has a lot of power, coming across a p-value close to threshold is surprising. Almost all p-values are to the left of it. We even had to zoom!&lt;/p&gt;
    &lt;p&gt;It is here, I think, we get to a second source of confusion. I’ve noticed that a lot of psychologists think of power as N5. We think “if we increase the sample size we increase power”. This is true, but we have to remind ourselves that power is always for a potential true effect. If there is no effect, increasing N can’t increase power.&lt;/p&gt;
    &lt;p&gt;I think it’s therefore more helpful (for the present discussion of p-value circling) to think of increasing power as “increasing which effect we assume to be true”. Stated that way, p-value circling based on Lindley’s paradox seems a bit strange, as if you’re saying:&lt;/p&gt;
    &lt;p&gt;“The p-value reported here would be rare if the true effect was such that we had ~97% power to detect it, which convinces me that the null is true.”&lt;/p&gt;
    &lt;p&gt;I don’t think that makes sense! Why are you assuming that particular true effect? Surely, there’s a potential true effect where the likelihood of the observed p-value is similar to its likelihood under the null? (For the 0.04 to 0.05 bin this appears to be around 95% power). Also, I think it unfairly imposes a role on p-values that they’ve not been hired to play. P-values are not meant to be a measure of evidence – not in that direct way at least. They are meant to give stable error rates when the null is true. This contrasting between the plausibility of seeing a p-value under a null hypothesis versus a specific alternative hypothesis isn’t what they were designed for.&lt;/p&gt;
    &lt;p&gt;Ok but it seems possible to use them that way? One could specify a smallest effect size of interest and compare the plausibility of seeing the reported p-value under that distribution compared to the null distribution.6Maier and Lakens (2022) suggest you could do this exercise when planning a test in order to justify your choice of alpha-level. However, I doubt this structured approach is what lies behind the casual circling of p-values I’ve come across online over the years. My impression is that most social media p-circling haven’t been studies with very high power to detect small effects.&lt;/p&gt;
    &lt;p&gt;There is a concern that very large studies may pick up on “noise”, or that other violations of model assumptions (e.g. normality) tend to bias p-values downward. I don’t really know what to make of these concerns. I think that might be true for some model violations, while other may hurt power instead. I would assume it’s a complicated empirical question whether the statistical models we use tend to misfit reality more in one direction rather than the other.&lt;/p&gt;
    &lt;p&gt;Regardless, I don’t think it can be salvaged as a ground for being skeptical of p-values close to their threshold because of Lindley’s paradox.&lt;/p&gt;
    &lt;p&gt;For the moment I feel safest treating the conventional threshold as what it is, as arbitrary as that is. I’m of course concerned about QRPs and p-hacking, but I don’t see a reason for why a single p-value close to 0.05 would be useful evidence of it.&lt;/p&gt;
    &lt;p&gt;Some concluding thoughts&lt;/p&gt;
    &lt;p&gt;As I prefaced, this is complicated stuff and I have probably gotten something wrong. Regarding the larger question on whether p-values can be interpreted as evidence, I currently land in the conclusion “not in and of themselves”, they have to be contextualized in relation to power and other features of the study, as well as the context you come across them in. Lindley’s paradox can be a useful illustration of one of the reasons that the interpretation isn’t straight-forward (but I don’t think it justifies p-circling).&lt;/p&gt;
    &lt;p&gt;On the other hand, I know that smarter and more well-read people than I disagree on how straightforward this interpretation is. The textbook we used in my PhD-level course in medical statistics7 contain a table that tells us to do that:&lt;/p&gt;
    &lt;p&gt;I don’t think it’s quite that simple. My current understanding (given Lindley’s paradox) is that evidence has to be “relative”, in some sense. P-values only tell one side of the story, and are only made to tell one side of the story. If you want a statistic that expresses the strength of evidence you should probably use something else, e.g. Bayes factors.&lt;/p&gt;
    &lt;p&gt;Links etc:&lt;/p&gt;
    &lt;p&gt;I didn’t look up a lot of things for this post, it was more an attempt to think through my current understanding, but if you want to play around with visualizations of the p-value distribution I warmly recommend Kristoffer Magnussons interactive thingy: https://rpsychologist.com/d3/pdist/&lt;/p&gt;
    &lt;p&gt;See Daniel Lakens book for a real introduction to p-values. It’s there and the associated coursera course I learned about this stuff in the first place. I also always recommend Zoltan Dienes book Understanding Psychology As A Science (2008), which has also formed my understanding a lot.&lt;/p&gt;
    &lt;p&gt;See also Maier and Lakens (2022) for a structured approach to compromise between Lindeley’s paradox and the NP frequentist perspective.&lt;/p&gt;
    &lt;p&gt;Tell me I am wrong!&lt;/p&gt;
    &lt;p&gt;I am serious. If you think I’m misunderstanding something badly or you just want to discuss or you want to gently point me in the right direction: Please tell me what I’m missing. I don’t have a comment section on this blog, but I’ll post this on bluesky and then update this post to link the post that links this post: Here is the link.&lt;/p&gt;
    &lt;p&gt;So if you want you can comment, do it over there, or send me an e-mail.&lt;/p&gt;
    &lt;p&gt;Footnotes&lt;/p&gt;
    &lt;p&gt;They set it to “five sigma”, or something like 0.00003.↩︎&lt;/p&gt;
    &lt;p&gt;The meaning of this exclamation is surely the matter of some philosophical debate which is way beyond me, but I think it goes without saying that you should probably not stop all thought and submit to the p every time you see a significant result according to NP-frequentism. Every test happens within a context.↩︎&lt;/p&gt;
    &lt;p&gt;This shape is the basis for p-curve analysis, which is an attempt to detect such bias in a sample of significant results. FYI: Recently this method has been criticized as having “poor statistical properties”. https://www.tandfonline.com/doi/full/10.1080/01621459.2025.2544397&lt;/p&gt;
    &lt;p&gt;I haven’t dug into the debate, but I don’t think it matters for the present discussion.↩︎&lt;/p&gt;
    &lt;p&gt;Or to be more fair, some combination of N, design, type of statistical analysis, and measurement. But I think most focus is on sample size.↩︎&lt;/p&gt;
    &lt;p&gt;I am unsure about whether you’re still even doing frequentism at this point. Maier &amp;amp; Lakens describe it as a “Bayes-non-Bayes hybrid combining frequentist and Bayesian statistics”↩︎&lt;/p&gt;
    &lt;p&gt;Walters, Campbell &amp;amp; Machin (2021) Medical Statistics: A textbook for the health sciences, 5th ed↩︎&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288351</guid><pubDate>Tue, 16 Dec 2025 13:40:02 +0000</pubDate></item><item><title>This is not the future</title><link>https://blog.mathieui.net/this-is-not-the-future.html</link><description>&lt;doc fingerprint="2c2d994feb5e1010"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;onÂ Sat 08 November 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I thought about this when reading a mastodon post which commented on a news where a project adopted a "use Generative AI but disclose it" policy, because it is "the future" and "people are going to use it anyway".&lt;/p&gt;
    &lt;p&gt;I find the "this is the future, like it or not" framing particularly disgusting, and it is somewhat common in tech circles to accept it for most "new" technologies as if it was backed by evidence.&lt;/p&gt;
    &lt;p&gt;This post is to underline that Nothing is inevitable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modern technology is abusive.&lt;/head&gt;
    &lt;p&gt;A small contingent of power users using niche OSes (like myself) survive by avoiding as much of the tech oligarchsâ world as I can, sure, but overall everything is disgusting, and using FOSS is certainly no silver bullet.&lt;/p&gt;
    &lt;p&gt;Tech enthusiasts who do not apply critical thinking are even worse, because they get beat up everyday by the things they buy at a premium and they like it because they have this twisted idea of what constitutes progress. This is slowly infusing into the general population, which is also a problem.&lt;/p&gt;
    &lt;p&gt;People have been trained to be abused by software and by hardware, to ignore their needs, to accept any change as inevitable. I speak of abuse because people have been trained to expect and accept change at the same time, with no agency whatsoever.&lt;/p&gt;
    &lt;p&gt;Most old people in particular (sorry mom) have given up and resigned themselves to drift wherever their computing devices take them, because under the guise of convenience, everything is so hostile that there is no point trying to learn things, and dark patterns are everywhere. Not being in control of course makes people endlessy frustrated, but at the same time trying to wrestle control from the parasites is an uphill battle that they expect to lose, with more frustration as a result.&lt;/p&gt;
    &lt;p&gt;I want to emphasize here that there are good products (both software and hardware) on the market even though the list gets shorter every year, some products even manage to solve real problems (!!). That does not change the fact that consent, hype and projected consumer needs are manufactured by years and years of abuse and marketing campaigns.&lt;/p&gt;
    &lt;head rend="h3"&gt;Those things were or are not inevitable&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Internet-connected beds are not inevitable.&lt;/item&gt;
      &lt;item&gt;AI browsers are not inevitable.&lt;/item&gt;
      &lt;item&gt;Talking to chatbots instead of public servants is not inevitable.&lt;/item&gt;
      &lt;item&gt;Requiring a smartphone to exist in society is not inevitable.&lt;/item&gt;
      &lt;item&gt;Unrepairable devices are not inevitable.&lt;/item&gt;
      &lt;item&gt;"AI-enhanced" vacation pictures are not inevitable.&lt;/item&gt;
      &lt;item&gt;NFTs were not inevitable.&lt;/item&gt;
      &lt;item&gt;The Metaverse was not inevitable.&lt;/item&gt;
      &lt;item&gt;Your computer changing where things are on every update is not inevitable.&lt;/item&gt;
      &lt;item&gt;Websites that require your ID are not inevitable.&lt;/item&gt;
      &lt;item&gt;Garbage companies using refurbished plane engines to power their data centers is not inevitable.&lt;/item&gt;
      &lt;item&gt;Juicero was not inevitable.&lt;/item&gt;
      &lt;item&gt;Ads are not inevitable.&lt;/item&gt;
      &lt;item&gt;Being on a platform owned by Meta is not inevitable.&lt;/item&gt;
      &lt;item&gt;The Apple Vision pro was not inevitable.&lt;/item&gt;
      &lt;item&gt;"Copilot PCs" are not inevitable.&lt;/item&gt;
      &lt;item&gt;Tiktok is not inevitable.&lt;/item&gt;
      &lt;item&gt;Your computer sending screenshots to microsoft so they can train AIs on it is not inevitable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I could spend years filling this list up, because the tech grifters always find new ways to make us more miserable.&lt;/p&gt;
    &lt;p&gt;Nothing is inevitable, nothing sold by powerful grifters is "the future" no matter how much they wish that were true. Sure, some things can keep on existing, even for a very long time, even more if they have an untold number of billions - that they wormed their way into having by selling and exploiting personal data and attention -, but nobody has to be complicit. Some things might even end up existing because they are useful.&lt;/p&gt;
    &lt;p&gt;But what is important to me is to keep the perspective of what consitutes a desirable future, and which actions get us closer or further from that.&lt;/p&gt;
    &lt;p&gt;Every choice is both a political statement and a tradeoff based on the energy we can spend on the consequences of that choice.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288371</guid><pubDate>Tue, 16 Dec 2025 13:42:24 +0000</pubDate></item><item><title>A brief history of Times New Roman</title><link>https://typographyforlawyers.com/a-brief-history-of-times-new-roman.html</link><description>&lt;doc fingerprint="2a1d815b1e9219b4"&gt;
  &lt;main&gt;
    &lt;p&gt;Times New RoÂman gets its name from the Times of LonÂdon, the British newsÂpaÂper. In 1929, the Times hired tyÂpogÂraÂpher StanÂley MoriÂson to creÂate a new text font. MoriÂson led the project, suÂperÂvisÂing VicÂtor LarÂdent, an adÂverÂtisÂing artist for the Times, who drew the letterforms.&lt;/p&gt;
    &lt;p&gt;Even when new, Times New RoÂman had its critÂics. In his tyÂpoÂgraphic memÂoir, &lt;/p&gt;
    &lt;p&gt;BeÂcause it was used in a daily newsÂpaÂper, the new font quickly beÂcame popÂuÂlar among printÂers of the day. In the decades since, typeÂsetÂting deÂvices have evolved, but Times New RoÂman has alÂways been one of the first fonts availÂable for each new deÂvice (inÂcludÂing perÂsonal comÂputÂers). This, in turn, has only inÂcreased its reach.&lt;/p&gt;
    &lt;p&gt;ObÂjecÂtively, thereâs nothÂing wrong with Times New RoÂman. It was deÂsigned for a newsÂpaÂper, so itâs a bit narÂrower than most text fontsâesÂpeÂcially the bold style. (NewsÂpaÂpers preÂfer narÂrow fonts beÂcause they fit more text per line.) The italic is mediocre. But those arenât faÂtal flaws. Times New RoÂman is a workÂhorse font thatâs been sucÂcessÂful for a reason.&lt;/p&gt;
    &lt;p&gt;Yet itâs an open quesÂtion whether its longevity is atÂtribÂutÂable to its qualÂity or merely its ubiqÂuity. HelÂvetica still inÂspires enough afÂfecÂtion to have been the subÂject of a 2007 docÂuÂmenÂtary feaÂture. Times New RoÂman, meanÂwhile, has not atÂtracted simÂiÂlar acts of homage.&lt;/p&gt;
    &lt;p&gt;Why not? Fame has a dark side. When Times New RoÂman apÂpears in a book, docÂuÂment, or adÂverÂtiseÂment, it conÂnotes apÂaÂthy. It says,&lt;/p&gt;
    &lt;p&gt;This is how Times New RoÂman acÂcrued its repÂuÂtaÂtion as the deÂfault font of the leÂgal proÂfesÂsionâitâs the deÂfault font of everyÂthing. As a reÂsult, many lawÂyers erÂroÂneously asÂsume that courts deÂmand 12-point Times New RoÂman. In fact, Iâve never found one that does. (But there is one noÂtable court that forÂbids itâsee court opinÂions.) In genÂeral, lawÂyers keep usÂing it not beÂcause they must, but beÂcause itâs faÂmilÂiar and enÂtrenchedâmuch like those obÂsoÂlete typeÂwriter habits.&lt;/p&gt;
    &lt;p&gt;If you have a choice about usÂing Times New RoÂman, please stop. You have plenty of betÂter alÂterÂnaÂtivesâwhether itâs a difÂferÂent sysÂtem font or one of the many proÂfesÂsional fonts shown in this chapter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288414</guid><pubDate>Tue, 16 Dec 2025 13:46:55 +0000</pubDate></item><item><title>40 percent of fMRI signals do not correspond to actual brain activity</title><link>https://www.tum.de/en/news-and-events/all-news/press-releases/details/40-percent-of-mri-signals-do-not-correspond-to-actual-brain-activity</link><description>&lt;doc fingerprint="253b519799b3cda2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why blood flow is not a reliable indicator of the brain's energy requirements&lt;/head&gt;
    &lt;p&gt;40 percent of MRI signals do not correspond to actual brain activity&lt;/p&gt;
    &lt;p&gt;Researchers at the Technical University of Munich (TUM) and the Friedrich-Alexander-University Erlangen-Nuremberg (FAU) found that an increased fMRI signal is associated with reduced brain activity in around 40 percent of cases. At the same time, they observed decreased fMRI signals in regions with elevated activity. First author Dr. Samira Epp emphasizes: “This contradicts the long-standing assumption that increased brain activity is always accompanied by an increased blood flow to meet higher oxygen demand. Since tens of thousands of fMRI studies worldwide are based on this assumption, our results could lead to opposite interpretations in many of them.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Test tasks reveal deviations from the standard interpretation&lt;/head&gt;
    &lt;p&gt;PD Dr. Valentin Riedl, now Professor at FAU, and his colleague Epp examined more than 40 healthy participants during their time at TUM. Each was given several experimental tasks – such as mental arithmetic or autobiographical memory recall – which are known to produce predictable fMRI signal changes in distributed brain regions. During these experiments, the researchers simultaneously measured the actual oxygen consumption using a novel quantitative MRI technique.&lt;lb/&gt; Depending on the task and the brain region, the physiological results varied. Increased oxygen consumption – for instance in areas involved in calculation – did not coincide with the expected rise in blood flow. Instead, the quantitative analyses showed that these regions met their additional energy demand by extracting more oxygen from the unchanged blood supply. Thus, they used the oxygen available in the blood more efficiently without requiring greater perfusion.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implications for interpreting brain disorders&lt;/head&gt;
    &lt;p&gt;According to Riedl, these insights also affect the interpretation of research findings in brain disorders: “Many fMRI studies on psychiatric or neurological diseases – from depression to Alzheimer’s – interpret changes in blood flow as a reliable signal of neuronal under- or over-activation. Given the limited validity of such measurements, this must now be reassessed. Especially in patient groups with vascular changes – for instance due to aging or vascular disease – the measured values may primarily reflect vascular differences rather than neuronal deficits.” Previous animal studies already point in this direction.&lt;lb/&gt; The researchers therefore propose complementing the conventional MRI approach with quantitative measurements. In the long term, this combination could form the basis for energy-based brain models: rather than showing activation maps that depend on assumptions about blood flow, future analyses could display values indicating how much oxygen – and therefore energy – is actually consumed for information processing. This opens new perspectives for examining aging, psychiatric, or neurodegenerative diseases in terms of absolute changes in energy metabolism – and for understanding them more accurately.&lt;/p&gt;
    &lt;p&gt;Samira M. Epp, Gabriel Castrillón, Beijia Yuan, Jessica Andrews-Hanna, Christine Preibisch, Valentin Riedl: BOLD signal changes can oppose oxygen metabolism across the human cortex, published in Nature Neuroscience, December 12, 2025, https://doi.org/10.1038/s41593-025-02132-9&lt;/p&gt;
    &lt;p&gt;The research was conducted at the Neuro-Head Center of the Institute of Neuroradiology at the TUM University Hospital. It was funded by the European Research Council through an ERC Starting Grant.&lt;/p&gt;
    &lt;p&gt;Technical University of Munich&lt;/p&gt;
    &lt;p&gt;Corporate Communications Center&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ulrich Meyer&lt;/item&gt;
      &lt;item&gt;presse @tum.de&lt;/item&gt;
      &lt;item&gt;Teamwebsite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contacts to this article:&lt;/p&gt;
    &lt;p&gt;Prof. Dr. Valentin Riedl&lt;lb/&gt; Research Fellow&lt;lb/&gt; Technical University of Munich&lt;lb/&gt; TUM University Hospital – Neuro-Head Center&lt;lb/&gt; valentin.riedl @tum.de&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288415</guid><pubDate>Tue, 16 Dec 2025 13:46:57 +0000</pubDate></item><item><title>U.S. unemployment rose in November despite job gains</title><link>https://www.wsj.com/economy/jobs/jobs-report-october-november-2025-unemployment-economy-7f6eea90</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46288673</guid><pubDate>Tue, 16 Dec 2025 14:11:08 +0000</pubDate></item><item><title>Four Million U.S. Children Had No Health Insurance in 2024</title><link>https://www.scientificamerican.com/article/how-rising-rates-of-uninsured-children-will-increase-pediatric-cancer-deaths/</link><description>&lt;doc fingerprint="3512811f401a0911"&gt;
  &lt;main&gt;
    &lt;p&gt;More than four million U.S. children under age 19 lacked health insurance in 2024. The uninsured rate peaked at 6.1 percent—the highest level in the past decade, according to a recent analysis by the Georgetown University Center for Children and Families, a health policy research organization. That marks a nearly 20 percent increase in the number of uninsured children nationwide since 2022.&lt;/p&gt;
    &lt;p&gt;Being uninsured creates gaps in medical care. And these gaps don’t just interfere with routine pediatric care; they also disrupt treatments for serious illnesses such as pediatric cancers, for which early detection is often a matter of life and death.&lt;/p&gt;
    &lt;head rend="h2"&gt;On supporting science journalism&lt;/head&gt;
    &lt;p&gt;If you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.&lt;/p&gt;
    &lt;p&gt;“When you don’t have insurance, you’re likely to delay care,” says Kimberly Johnson, a pediatric cancer epidemiologist and a professor at Washington University in St. Louis. “In the case of cancer, that can delay diagnosis, and the cancer can become more advanced, which then is associated with a worse prognosis.”&lt;/p&gt;
    &lt;p&gt;The spike in the number of uninsured children is a direct upshot of Americans’ fragmented health care system. This patchwork of public insurance, private insurance and other employer plans creates a shaky environment for families whose income or job status changes, says Derek Brown, a health economist and a professor at Washington University in St. Louis. These life shifts may force parents to repeatedly lose and re-enroll in insurance, threatening the health of their children.&lt;/p&gt;
    &lt;p&gt;Many uninsured children are eligible for Medicaid (the government insurance program for people with limited income) or the Children’s Health Insurance Program (a joint federal-state program that provides matching federal funds for states to help insure children) but aren’t enrolled, says Joan Alker, a research professor at the Georgetown McCourt School of Public Policy. People may not know they are eligible, and individuals who are undocumented may fear deportation. “Especially in today’s climate, there are families where the child is a citizen and the parent is an immigrant, and they’re fearful of interacting with government,” Alker says. But such fears can only explain a small proportion of those who are uninsured, she notes.&lt;/p&gt;
    &lt;p&gt;More children are losing insurance because of bureaucratic red tape. In a process informally referred to as “Medicaid unwinding,” states have resumed Medicaid eligibility checks after a period of continuous coverage during the COVID pandemic. Some people who were eligible previously have been disenrolled not as a result of disqualification but simply because of bureaucratic mistakes.&lt;/p&gt;
    &lt;p&gt;These gaps in insurance coverage will result in more children getting sicker and dying. A 2020 national study in the International Journal of Epidemiology of more than 58,000 children and adolescents under age 20 with cancer found that those who were uninsured faced a sharply higher risk of dying within five years than those with private insurance across most cancer types. Eleven percent of the uninsured study participants received no cancer-directed treatment compared with 6.7 percent of those who were privately insured. Children and adolescents without insurance also had 31 percent higher odds of being diagnosed at a later stage of cancer and were 32 percent more likely to die in the five years after diagnosis than those with private insurance—living about two months less on average.&lt;/p&gt;
    &lt;p&gt;In the study, those on Medicaid also had a higher risk of dying than those on private insurance, suggesting that other differences between the groups could explain the former’s higher mortality rate, such as family income level.&lt;/p&gt;
    &lt;p&gt;Because different types of cancer grow differently, however, insurance gaps don’t harm every child in the same way. For certain types, the earlier they were found, the higher survival rates tended to be. For example, in tumors of the reproductive organs, the study found that about 40 percent of the survival difference between the privately insured and the uninsured was explained by catching the disease at a later stage, whereas for brain and spinal tumors, timing of diagnosis made little difference no matter what insurance they had—likely because the latter type of cancer tends to be less treatable in general.&lt;/p&gt;
    &lt;p&gt;Even if kids have insurance some of the time, going on and off Medicaid can jeopardize cancer treatment. In a 2024 study in Pediatric Blood &amp;amp; Cancer that looked at more than 30,000 children and adolescents under age 20 who were diagnosed with cancer between 2006 and 2013, Johnson, Brown and their colleagues found that those who were intermittently insured by Medicaid during the assessment period had double the odds of being diagnosed at a later stage when cancer had metastasized and faced an increased risk of cancer death compared with their continuously insured and non-Medicaid-insured peers—most of whom had private insurance.&lt;/p&gt;
    &lt;p&gt;The five-year survival gap was widest among children and adolescents with soft-tissue cancers and liver tumors, for whom losing Medicaid coverage could interrupt lifesaving treatment; nerve-cell cancers were the only cancers that didn’t follow this trend. People with other types of cancers, such as leukemia, a form of blood cancer, also benefited from continuous insurance. Leukemia symptoms are often urgent enough to send children to the emergency room, leading to faster diagnosis, unlike many quiet-progressing solid tumors, whose symptoms parents may not recognize as urgent.&lt;/p&gt;
    &lt;p&gt;“As a country, we’re long overdue to move to a system where no baby leaves the hospital without [insurance] coverage, just the same way they shouldn’t leave the hospital without a car seat,” Alker says. The Trump administration is phasing out a policy that has allowed some states to cover children continuously until age six despite any family’s changes in circumstances.&lt;/p&gt;
    &lt;p&gt;The situation isn’t hopeless, experts say. Paperwork errors could be fixed, and legislators could make new guarantees to stop children from losing insurance. In addition, hospital and clinical social workers should help people stay connected with Medicaid enrollment supports and guide them through some of common pitfalls and challenges, Brown says. For caregivers of children with cancer, it’s especially important to make sure each state’s Medicaid enrollment process is accessible, which requires clear websites and adequate staffing, he says.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46289635</guid><pubDate>Tue, 16 Dec 2025 15:22:32 +0000</pubDate></item></channel></rss>