<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 17 Nov 2025 15:41:14 +0000</lastBuildDate><item><title>Fastmcpp (Fastmcp for C++)</title><link>https://github.com/0xeb/fastmcpp</link><description>&lt;doc fingerprint="46b9371c76c77db9"&gt;
  &lt;main&gt;
    &lt;p&gt;High‚Äëperformance C++ implementation of the Model Context Protocol (MCP), with support for tools, resources, prompts, and multiple transport layers (STDIO, HTTP/SSE, WebSocket).&lt;/p&gt;
    &lt;p&gt;fastmcpp is a C++ port of the Python fastmcp library, providing native performance for MCP servers and clients with a small, focused dependency set.&lt;/p&gt;
    &lt;p&gt;Status: Beta ‚Äì core MCP features track the Python &lt;code&gt;fastmcp&lt;/code&gt; reference, but the C++ test suite is intentionally much smaller than the Python one.&lt;/p&gt;
    &lt;p&gt;Current version: 2.13.0. Python &lt;code&gt;fastmcp&lt;/code&gt; remains the canonical source of truth for behavior and API; this C++ port is expected to follow it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Core MCP protocol implementation (JSON‚ÄëRPC).&lt;/item&gt;
      &lt;item&gt;Multiple transports: STDIO, HTTP (SSE), WebSocket.&lt;/item&gt;
      &lt;item&gt;Tool management and invocation.&lt;/item&gt;
      &lt;item&gt;Resources and prompts support.&lt;/item&gt;
      &lt;item&gt;JSON Schema validation.&lt;/item&gt;
      &lt;item&gt;Middleware for request/response processing.&lt;/item&gt;
      &lt;item&gt;Integration with MCP‚Äëcompatible CLI tools.&lt;/item&gt;
      &lt;item&gt;Cross‚Äëplatform: Windows, Linux, macOS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C++17 or later compiler.&lt;/item&gt;
      &lt;item&gt;CMake 3.20 or higher.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;nlohmann/json&lt;/code&gt;(fetched automatically).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Optional:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;libcurl (for HTTP POST streaming).&lt;/item&gt;
      &lt;item&gt;cpp‚Äëhttplib (HTTP server, fetched automatically).&lt;/item&gt;
      &lt;item&gt;easywsclient (WebSocket client, fetched automatically).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/0xeb/fastmcpp.git
cd fastmcpp

cmake -B build -S . -DCMAKE_BUILD_TYPE=Release
cmake --build build --config Release -j&lt;/code&gt;
    &lt;code&gt;cmake -B build -S . \
  -DCMAKE_BUILD_TYPE=Release \
  -DFASTMCPP_ENABLE_POST_STREAMING=ON \
  -DFASTMCPP_FETCH_CURL=ON \
  -DFASTMCPP_ENABLE_STREAMING_TESTS=ON \
  -DFASTMCPP_ENABLE_WS_STREAMING_TESTS=ON&lt;/code&gt;
    &lt;p&gt;Key options:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;CMAKE_BUILD_TYPE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Debug&lt;/cell&gt;
        &lt;cell&gt;Build configuration (Debug/Release/RelWithDebInfo)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;FASTMCPP_ENABLE_POST_STREAMING&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OFF&lt;/cell&gt;
        &lt;cell&gt;Enable HTTP POST streaming (requires libcurl)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;FASTMCPP_FETCH_CURL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OFF&lt;/cell&gt;
        &lt;cell&gt;Fetch and build curl if not found&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;FASTMCPP_ENABLE_STREAMING_TESTS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OFF&lt;/cell&gt;
        &lt;cell&gt;Enable SSE streaming tests&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;FASTMCPP_ENABLE_WS_STREAMING_TESTS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OFF&lt;/cell&gt;
        &lt;cell&gt;Enable WebSocket streaming tests&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Windows (Visual Studio):&lt;/p&gt;
    &lt;code&gt;cmake -B build -S . -G "Visual Studio 17 2022"
cmake --build build --config Release&lt;/code&gt;
    &lt;p&gt;Linux/macOS:&lt;/p&gt;
    &lt;code&gt;cmake -B build -S . -DCMAKE_BUILD_TYPE=Release
cmake --build build -j"$(nproc)"&lt;/code&gt;
    &lt;code&gt;# Run all tests
ctest --test-dir build -C Release --output-on-failure

# Parallel
ctest --test-dir build -C Release -j4 --output-on-failure

# Run a specific test
ctest --test-dir build -C Release -R fastmcp_smoke --output-on-failure

# List tests
ctest --test-dir build -C Release -N&lt;/code&gt;
    &lt;p&gt;Current status (CI / WSL configuration):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;24/24 tests passing (100% success rate).&lt;/item&gt;
      &lt;item&gt;3 streaming tests disabled due to infrastructure dependencies.&lt;/item&gt;
      &lt;item&gt;C++ test line count is much smaller than the Python &lt;code&gt;fastmcp&lt;/code&gt;suite (see CCSDK parity docs).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#include &amp;lt;fastmcpp/tools/manager.hpp&amp;gt;
#include &amp;lt;fastmcpp/mcp/handler.hpp&amp;gt;
#include &amp;lt;fastmcpp/server/stdio_server.hpp&amp;gt;

int main() {
    fastmcpp::tools::ToolManager tm;
    // register tools on tm...

    auto handler = fastmcpp::mcp::make_mcp_handler(
        "myserver", "1.0.0", tm
    );

    fastmcpp::server::StdioServerWrapper server(handler);
    server.run();  // blocking
    return 0;
}&lt;/code&gt;
    &lt;code&gt;#include &amp;lt;fastmcpp/server/server.hpp&amp;gt;
#include &amp;lt;fastmcpp/server/http_server.hpp&amp;gt;

int main() {
    auto srv = std::make_shared&amp;lt;fastmcpp::server::Server&amp;gt;();
    srv-&amp;gt;register_get("/health", [](const nlohmann::json&amp;amp;) {
        return nlohmann::json{{"status", "ok"}};
    });

    fastmcpp::server::HttpServerWrapper http(srv, "127.0.0.1", 8080);
    http.start();  // non‚Äëblocking

    std::this_thread::sleep_for(std::chrono::hours(1));
    http.stop();
    return 0;
}&lt;/code&gt;
    &lt;code&gt;#include &amp;lt;fastmcpp/client/client.hpp&amp;gt;
#include &amp;lt;fastmcpp/client/transports.hpp&amp;gt;

int main() {
    auto transport = std::make_shared&amp;lt;fastmcpp::client::HttpTransport&amp;gt;(
        "http://localhost:8080"
    );

    fastmcpp::client::Client client(transport);
    auto response = client.call("tool/invoke", {
        {"name", "calculator"},
        {"input", {{"operation", "add"}, {"a", 5}, {"b", 3}}}
    });

    std::cout &amp;lt;&amp;lt; response.dump() &amp;lt;&amp;lt; std::endl;
    return 0;
}&lt;/code&gt;
    &lt;p&gt;See the &lt;code&gt;examples/&lt;/code&gt; directory for complete programs, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;stdio_server.cpp&lt;/code&gt;‚Äì STDIO MCP server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;server_quickstart.cpp&lt;/code&gt;‚Äì HTTP server with routes.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;client_quickstart.cpp&lt;/code&gt;‚Äì HTTP client usage.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tool_example.cpp&lt;/code&gt;‚Äì tool registration and invocation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;middleware_example.cpp&lt;/code&gt;‚Äì request/response middleware.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fastmcpp/
  include/fastmcpp/   # Public headers (client, server, tools, etc.)
  src/                # Implementation
  tests/              # Test suite (GoogleTest)
  examples/           # Example programs
  CMakeLists.txt      # Build configuration
  LICENSE             # Apache 2.0 license
  NOTICE              # Attribution notices
  README.md           # This file
&lt;/code&gt;
    &lt;p&gt;Contributions are welcome. Please:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ensure all tests pass.&lt;/item&gt;
      &lt;item&gt;Follow the existing code style.&lt;/item&gt;
      &lt;item&gt;Add tests for new features.&lt;/item&gt;
      &lt;item&gt;Update documentation as needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Copyright 2025 Elias Bachaalany&lt;/p&gt;
    &lt;p&gt;Licensed under the Apache License 2.0. See &lt;code&gt;LICENSE&lt;/code&gt; and &lt;code&gt;NOTICE&lt;/code&gt; for details.&lt;/p&gt;
    &lt;p&gt;This is a C++ port of fastmcp by Jeremiah Lowin. The Python library is the canonical implementation; fastmcpp aims to match its behavior for core features.&lt;/p&gt;
    &lt;p&gt;For issues and questions, use the GitHub issue tracker: https://github.com/0xeb/fastmcpp/issues.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45924417</guid><pubDate>Fri, 14 Nov 2025 06:25:51 +0000</pubDate></item><item><title>Heretic: Automatic censorship removal for language models</title><link>https://github.com/p-e-w/heretic</link><description>&lt;doc fingerprint="3d151bbcad3e9dad"&gt;
  &lt;main&gt;
    &lt;p&gt;Heretic is a tool that removes censorship (aka "safety alignment") from transformer-based language models without expensive post-training. It combines an advanced implementation of directional ablation, also known as "abliteration" (Arditi et al. 2024), with a TPE-based parameter optimizer powered by Optuna.&lt;/p&gt;
    &lt;p&gt;This approach enables Heretic to work completely automatically. Heretic finds high-quality abliteration parameters by co-minimizing the number of refusals and the KL divergence from the original model. This results in a decensored model that retains as much of the original model's intelligence as possible. Using Heretic does not require an understanding of transformer internals. In fact, anyone who knows how to run a command-line program can use Heretic to decensor language models.&lt;/p&gt;
    &lt;p&gt;Running unsupervised with the default configuration, Heretic can produce decensored models that rival the quality of abliterations created manually by human experts:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Refusals for "harmful" prompts&lt;/cell&gt;
        &lt;cell role="head"&gt;KL divergence from original model for "harmless" prompts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;google/gemma-3-12b-it (original)&lt;/cell&gt;
        &lt;cell&gt;97/100&lt;/cell&gt;
        &lt;cell&gt;0 (by definition)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;mlabonne/gemma-3-12b-it-abliterated-v2&lt;/cell&gt;
        &lt;cell&gt;3/100&lt;/cell&gt;
        &lt;cell&gt;1.04&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;huihui-ai/gemma-3-12b-it-abliterated&lt;/cell&gt;
        &lt;cell&gt;3/100&lt;/cell&gt;
        &lt;cell&gt;0.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;p-e-w/gemma-3-12b-it-heretic (ours)&lt;/cell&gt;
        &lt;cell&gt;3/100&lt;/cell&gt;
        &lt;cell&gt;0.16&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The Heretic version, generated without any human effort, achieves the same level of refusal suppression as other abliterations, but at a much lower KL divergence, indicating less damage to the original model's capabilities. (You can reproduce those numbers using Heretic's built-in evaluation functionality, e.g. &lt;code&gt;heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic&lt;/code&gt;.
Note that the exact values might be platform- and hardware-dependent.
The table above was compiled using PyTorch 2.8 on an RTX 5090.)&lt;/p&gt;
    &lt;p&gt;Heretic supports most dense models, including many multimodal models, and several different MoE architectures. It does not yet support SSMs/hybrid models, models with inhomogeneous layers, and certain novel attention systems.&lt;/p&gt;
    &lt;p&gt;You can find a collection of models that have been decensored using Heretic on Hugging Face.&lt;/p&gt;
    &lt;p&gt;Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate for your hardware. Then run:&lt;/p&gt;
    &lt;code&gt;pip install heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507
&lt;/code&gt;
    &lt;p&gt;Replace &lt;code&gt;Qwen/Qwen3-4B-Instruct-2507&lt;/code&gt; with whatever model you want to decensor.&lt;/p&gt;
    &lt;p&gt;The process is fully automatic and does not require configuration; however, Heretic has a variety of configuration parameters that can be changed for greater control. Run &lt;code&gt;heretic --help&lt;/code&gt; to see available command-line options,
or look at &lt;code&gt;config.default.toml&lt;/code&gt; if you prefer to use
a configuration file.&lt;/p&gt;
    &lt;p&gt;At the start of a program run, Heretic benchmarks the system to determine the optimal batch size to make the most of the available hardware. On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B takes about 45 minutes.&lt;/p&gt;
    &lt;p&gt;After Heretic has finished decensoring a model, you are given the option to save the model, upload it to Hugging Face, chat with it to test how well it works, or any combination of those actions.&lt;/p&gt;
    &lt;p&gt;Heretic implements a parametrized variant of directional ablation. For each supported transformer component (currently, attention out-projection and MLP down-projection), it identifies the associated matrices in each transformer layer, and orthogonalizes them with respect to the relevant "refusal direction", inhibiting the expression of that direction in the result of multiplications with that matrix.&lt;/p&gt;
    &lt;p&gt;Refusal directions are computed for each layer as a difference-of-means between the first-token residuals for "harmful" and "harmless" example prompts.&lt;/p&gt;
    &lt;p&gt;The ablation process is controlled by several optimizable parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;direction_index&lt;/code&gt;: Either the index of a refusal direction, or the special value&lt;code&gt;per layer&lt;/code&gt;, indicating that each layer should be ablated using the refusal direction associated with that layer.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_weight&lt;/code&gt;,&lt;code&gt;max_weight_position&lt;/code&gt;,&lt;code&gt;min_weight&lt;/code&gt;, and&lt;code&gt;min_weight_distance&lt;/code&gt;: For each component, these parameters describe the shape and position of the ablation weight kernel over the layers. The following diagram illustrates this:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Heretic's main innovations over existing abliteration systems are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The shape of the ablation weight kernel is highly flexible, which, combined with automatic parameter optimization, can improve the compliance/quality tradeoff. Non-constant ablation weights were previously explored by Maxime Labonne in gemma-3-12b-it-abliterated-v2.&lt;/item&gt;
      &lt;item&gt;The refusal direction index is a float rather than an integer. For non-integral values, the two nearest refusal direction vectors are linearly interpolated. This unlocks a vast space of additional directions beyond the ones identified by the difference-of-means computation, and often enables the optimization process to find a better direction than that belonging to any individual layer.&lt;/item&gt;
      &lt;item&gt;Ablation parameters are chosen separately for each component. I have found that MLP interventions tend to be more damaging to the model than attention interventions, so using different ablation weights can squeeze out some extra performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I'm aware of the following publicly available implementations of abliteration techniques:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AutoAbliteration&lt;/item&gt;
      &lt;item&gt;abliterator.py&lt;/item&gt;
      &lt;item&gt;wassname's Abliterator&lt;/item&gt;
      &lt;item&gt;ErisForge&lt;/item&gt;
      &lt;item&gt;Removing refusals with HF Transformers&lt;/item&gt;
      &lt;item&gt;deccp&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that Heretic was written from scratch, and does not reuse code from any of those projects.&lt;/p&gt;
    &lt;p&gt;The development of Heretic was informed by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The original abliteration paper (Arditi et al. 2024)&lt;/item&gt;
      &lt;item&gt;Maxime Labonne's article on abliteration, as well as some details from the model cards of his own abliterated models (see above)&lt;/item&gt;
      &lt;item&gt;Jim Lai's article describing "projected abliteration"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Copyright ¬© 2025 Philipp Emanuel Weidmann (pew@worldwidemann.com)&lt;/p&gt;
    &lt;p&gt;This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt;
    &lt;p&gt;This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.&lt;/p&gt;
    &lt;p&gt;You should have received a copy of the GNU Affero General Public License along with this program. If not, see https://www.gnu.org/licenses/.&lt;/p&gt;
    &lt;p&gt;By contributing to this project, you agree to release your contributions under the same license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45945587</guid><pubDate>Sun, 16 Nov 2025 15:00:24 +0000</pubDate></item><item><title>FPGA Based IBM-PC-XT</title><link>https://bit-hack.net/2025/11/10/fpga-based-ibm-pc-xt/</link><description>&lt;doc fingerprint="a75e788d5dab06ad"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently I undertook a hobby project to recreate an IBM XT Personal Computer from the 1980s using a mix of authentic parts and modern technology. I had a clear goal in mind: I wanted to be able to play the EGA version of Monkey Island 1 on it, with no features missing. This means I need mouse support, hard drive with write access for saving the game, and Adlib audio, my preferred version of the game‚Äôs musical score.&lt;/p&gt;
    &lt;p&gt;The catalyst for this project was the discovery that there are low-power versions of the NEC V20 CPU available (UPD70108H), which is compatible with the Intel 8088 used in the XT. Being a low-power version significantly simplifies its connection to an FPGA, which typically operate with 3.3-volt IO voltages. Coupled with a low-power 1MB SRAM chip (CY62158EV30) to provide the XT with its 640KB of memory, and I started to have the bones of a complete system worked out.&lt;/p&gt;
    &lt;p&gt;I started off by designing the hardware of the system, which would then serve as my development board while I worked on the software/gateware. The following features were added:&lt;lb/&gt;‚Äì DIP-40 socket for an low power NEC V20 CPU&lt;lb/&gt;‚Äì 1MB SRAM chip for the system memory&lt;lb/&gt;‚Äì An icesugar-pro FPGA board with a Lattice LFE5U-25F&lt;lb/&gt;‚Äì Dual PS/2 connectors for keyboard and mouse&lt;lb/&gt;‚Äì Micro SD card socket to act as a Fixed Disk&lt;lb/&gt;‚Äì An authentic YM3014B digital-to-analogue converter for audio&lt;lb/&gt;‚Äì A Piezo speaker that can be driven by the programmable-interval-timer for system bleeps&lt;lb/&gt;‚Äì Lastly, a reset switch and some status LEDs&lt;/p&gt;
    &lt;p&gt;I drew up my design using the EasyEDA CAD software as I‚Äôm already familiar with it, and it has really good integration with the JLCPCB PCB assembly service. Some of the components in the design are too tricky for me to hand solder by myself. I did however have to solder the SRAM chips once the boards arrived since they were not stocked by LCSC so I had to source them elsewhere.&lt;/p&gt;
    &lt;p&gt;The first step was to write a bus controller for the processor. The V20 CPU clock is more forgiving than an original i8088 since its can be run right down to 0hz and its uses a regular 50% duty cycle. The external interface for an 8088 CPU operates in terms of bus cycles. At that start of a bus cycle the CPU asserts some pins to let everyone know what it wants to try and do‚Ä¶ read memory, write to IO, etc. Each type of bus cycle follows a specific sequence of events that happen over a number of clock cycles. It was straight forward to make a state machine that could detect the start of a bus-cycle, figure out what kind it was, and then produce or consume the data as needed by the CPU. Key here, was to make sure that all of the timing requirements were met, so that signals the CPU generates are sampled at the correct time, and signals the CPU requires have been driven correctly before the CPU reads them.&lt;/p&gt;
    &lt;p&gt;My first test for the bus controller was to write a simple program using NASM to be executed on the V20, with a simple goal‚Ä¶ it will flash an LED mapped to an IO port address. Simple, but a blinking LED seems to be the hardware equivalent of the software hello-world. For the initial version, the program was simply loaded into a FPGA block ram and used directly as the system memory.&lt;/p&gt;
    &lt;p&gt;Later, I used a more complex approach for memory accesses. The bios, for example, is loaded into an FPGA block ram, so that CPU memory reads will come from that rather than the system SRAM chip. Video memory is implemented a differently still. CPU memory writes are passed to both the video memory block ram and system SRAM, but CPU reads alway come from only the system SRAM. This then means that I have a spare read port on the video block ram that can then be used by the VGA signal generator to display the video memory contents.&lt;/p&gt;
    &lt;p&gt;After my success with a blinky program, I installed a virtual copy of Supersoft/Landmark Diagnostic ROM in place of the BIOS and wrote a basic CGA adapter for video output. I was then able to use the diagnostic ROM to test the SRAM memory interface as well as some of the peripherals required by the XT such as the programmable-interval-timer (i8253) and programmable-interrupt-controller (i8259).&lt;/p&gt;
    &lt;p&gt;Once I was confident the basic system was stable I then swaped in a generic XT bios from https://www.phatcode.net in place of the diagnostic ROM. It was amazing to see the bios start to boot up, and complain when it couldnt find a boot disk.&lt;/p&gt;
    &lt;p&gt;Fixed Disk access is achieved by making a small Verilog SPI controller accessible to the CPU via some unused IO ports. I then wrote an option ROM to handle BIOS INT13H (disk service) calls, which had routines that could issue commands to the SD-Card over SPI. The tricky part for me was learning the SD card protocol and then writing 8088 assembly to perform the correct operations. The mapping itself is very straightforward as both SD card and DOS assume 512byte sectors.&lt;/p&gt;
    &lt;p&gt;I saved a lot of time when writing the option ROM by developing and debugging the code using a software emulator of the board that I cobbled together. Some historic sources for it can be found here: https://github.com/bit-hack/iceXt/tree/master/misc/emulator&lt;/p&gt;
    &lt;p&gt;Perhaps the hardest part of the project was, surprisingly, getting the mouse to work. Mice of the XT era would typically be connected to a UART serial port. I had however placed a PS/2 connector on the hardware board, and those mice use a very different protocol. In my efforts to support a mouse I startedto learn about PS/2 devices, however I would need to implement a much more complex keyboard controller, and the BIOS I was also lacked support for such modern peripherals, and I just plain didn‚Äôt feel like I understood everything required to get that working.&lt;/p&gt;
    &lt;p&gt;What makes it tricky is that PS/2 is a bidirectional protocol, and the mouse has to be asked by the PC to broadcast updates, otherwise we will not receive any. That added a lot more complexity than I was wanting. The keyboard on the otherhand is relatively easy to work with and send out keypresses without having to be asked.&lt;/p&gt;
    &lt;p&gt;I chose an alternative. I wrote some Verilog code to talk directly to the PS/2 mouse, which would early in the boot process tell it to start sending over mouse events, as they have to be requested. When the bridge then receives mouse events, it translates and presents them to the computer via a pseudo UART peripheral. I had implemented a basic PS/2 mouse to Serial mouse bridge. A little convoluted but it works really well.&lt;/p&gt;
    &lt;p&gt;During this process, I lobotomised a spare mouse by attaching a logic analyser the clk and dat pads inside the mouse. I was then able to capture the communications between a real PC and the mouse and observe it during use. This gave me invaluable insight into exactly how the protocol worked, and what a real mouse expected.&lt;/p&gt;
    &lt;p&gt;I also found that having real waveforms to look at made it much easier to test components of my design in verilator, a Verilog simulator, as I could closely model the stimulus it should see when running in the FPGA.&lt;/p&gt;
    &lt;p&gt;Just like the XT, one of the channels of the PIT timer is used to drive the internal speaker to produce bleep and bloop sounds. I extended this by having disk accesses trigger short pulses out of the peizo speaker as a crude emulation of a hard disk seeking. I think it really adds to the experience when you can hear your computer thinking away while doing its tasks. When it comes to music, the internal PC speaker quickly looses its charm however. Writing an YM3812 implementation (the FM chip used in the Adlib card) is beyond my skill level but thankfully Jose Tejada has written an amazing open source version that I was able to pull into my project; https://github.com/jotego/jtopl.&lt;/p&gt;
    &lt;p&gt;I wrote a small Verilog module to take the PCM sample data generated by this soft YM3812 and convert it to the unusual 3:10 floating point format required by the YM3014 DAC on my board. This is very similar to the operation of the real Adlib hardware, where the YM3812 generates and sends serial audio data to a YM3014 DAC chip. A modern I2S DAC may have been cleaner, but having a chance to play with the authentic DAC seemed a little more fun to me. All of this combined results in the same lovely crisp FM tones I was so fond of when I played games on my PC growing up.&lt;/p&gt;
    &lt;p&gt;A lot of other elements of this project have been glossed over or omitted, such as support for CGA and EGA graphics. There is even a USB to UART bridge for sending files from a host PC directly to the SD card. I also made some nice clear acrylic panels on a CNC machine to round off the design and protect the bare PCB.&lt;/p&gt;
    &lt;p&gt;A video demo is shown below.&lt;lb/&gt;Unfortunately there is a ton of screen tearing due to the phase between the monitor and my camera. It isn‚Äôt visible in person.&lt;/p&gt;
    &lt;p&gt;Source code, schematics and gerber files are available on github here: https://github.com/bit-hack/iceXt&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45945784</guid><pubDate>Sun, 16 Nov 2025 15:26:24 +0000</pubDate></item><item><title>I finally understand Cloudflare Zero Trust tunnels</title><link>https://david.coffee/cloudflare-zero-trust-tunnels</link><description>&lt;doc fingerprint="a5b8a71e23286293"&gt;
  &lt;main&gt;
    &lt;p&gt;A while ago, after frustration with Tailscale in environments where it couldn‚Äôt properly penetrate NAT/firewall and get a p2p connection, I decided to invest some time into learning something new: Cloudflare Zero Trust + Warp.&lt;/p&gt;
    &lt;p&gt;There are so many new concepts, but after way too long, I can finally say that I understand Cloudflare Zero Trust Warp now. I am a full-on Cloudflare Zero Trust with Warp convert, and while I still have Tailscale running in parallel, almost everything I do now is going through Zero Trust tunnels.&lt;/p&gt;
    &lt;p&gt;This post is an explanation of the basic concepts, because I‚Äôm sure others will have similar issues wrapping their head around it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why tho?&lt;/head&gt;
    &lt;p&gt;Why would you even sink so much time into learning this? What does it give you?&lt;/p&gt;
    &lt;p&gt;Argo tunnels through Zero Trust allow you to do a bunch of really cool things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Connect private networks together - can be home networks, can be kubernetes clusters, you can create tunnels to and from every infra&lt;/item&gt;
      &lt;item&gt;Expose private services to the public, on public hostnames, no matter where they are running. You could even put your router running at 192.168.1.1 on the internet, accessible to everyone, no Warp client required&lt;/item&gt;
      &lt;item&gt;Create fully private networks with private IPs (10.x.x.x) that only resolve when Warp is connected, to services you specify&lt;/item&gt;
      &lt;item&gt;Quickly expose a public route to any service running locally or on any server, for quick development, testing webhooks or giving coworkers a quick preview&lt;/item&gt;
      &lt;item&gt;Create a fully private network running at home that‚Äôs only available when you‚Äôre connected to the Warp VPN client, or only to you, reachable anywhere&lt;/item&gt;
      &lt;item&gt;No worries about NAT, everything goes through the Cloudflare network, no direct p2p connection required&lt;/item&gt;
      &lt;item&gt;Add very granular access policies on who can access what - what login method does the user need, which email addresses are allowed. Allow bots and server-to-server exceptions with service access tokens.&lt;list rend="ul"&gt;&lt;item&gt;Does the user need to have Warp running? Does he need to be enrolled in Zero Trust? Does he need some special permission flag?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Authenticate to SSH servers through Zero Trust access policies without the need of SSH keys. Just connect Warp, type &lt;code&gt;ssh host&lt;/code&gt;and you‚Äôre logged in&lt;list rend="ul"&gt;&lt;item&gt;Close public SSH ports completely to only allow login through Warp&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Get the benefits of Cloudflare VPN edge routing on top (similar to 1.1.1.1 Warp+)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Quickie: Cloudflare Zero Trust vs Tailscale&lt;/head&gt;
    &lt;p&gt;To get this out of the way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tailscale: peer-to-peer, uses NAT and firewall penetration methods to establish p2p connections. If not possible, it goes through central relay servers. Absolute best speed and latency if a connection is established.&lt;/item&gt;
      &lt;item&gt;Cloudflare: All traffic (with the exception of warp-to-warp routing, which is p2p) goes through Cloudflare‚Äôs edge network. So even SSH-ing into your local router will hop through Cloudflare servers. This adds latency, but no issues with NAT at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Cloudflared != Warp&lt;/head&gt;
    &lt;p&gt;Cloudflare has 2 tools available: Warp Client and Cloudflared. They interact with each other and have similarities in some areas but are not the same.&lt;/p&gt;
    &lt;p&gt;Warp Client&lt;/p&gt;
    &lt;p&gt;The tool that connects you to the Cloudflare network. This is the thing that you configure to add clients into your Zero Trust network and enforces policies.&lt;/p&gt;
    &lt;p&gt;Usually this runs on clients, but can also run on servers.&lt;/p&gt;
    &lt;p&gt;Warp client also supports warp-to-warp routing which is a true p2p connection similar to Tailscale.&lt;/p&gt;
    &lt;p&gt;Cloudflared&lt;/p&gt;
    &lt;p&gt;The thing that creates a tunnel and adds it to the Zero Trust network.&lt;/p&gt;
    &lt;p&gt;Most commonly you run this on servers to expose tunnels into your network, but you can also run it on clients.&lt;/p&gt;
    &lt;p&gt;On the client side you can use &lt;code&gt;cloudflared access&lt;/code&gt; to establish a connection with other things in your Zero Trust network.&lt;/p&gt;
    &lt;p&gt;Can also create one-time-use tunnels that aren‚Äôt connected to the Zero Trust network. Good for testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tunnels, Routes, Targets&lt;/head&gt;
    &lt;p&gt;This took me the longest to understand. Zero Trust allows you to configure Tunnels, Routes and Targets; here‚Äôs how they interplay.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tunnels&lt;/head&gt;
    &lt;p&gt;The most important part of your setup. Tunnels are deployed through &lt;code&gt;cloudflared&lt;/code&gt; and are simply an exit for traffic. Think of it as a literal tunnel that has its end somewhere.&lt;/p&gt;
    &lt;p&gt;Tunnels are deployed to infrastructure in the target network. So if you have a home network with 192.168.1.1/24, you want to deploy &lt;code&gt;cloudflared&lt;/code&gt; on any machine that‚Äôs always on and within that network. It can be your router, or your Raspi, it doesn‚Äôt matter.&lt;/p&gt;
    &lt;p&gt;For server-hosted services, you can have a tunnel on your main dev server, on a server, or on a pod in your Kubernetes cluster.&lt;/p&gt;
    &lt;p&gt;Now you have an opening into these networks through Warp/Argo tunnels.&lt;/p&gt;
    &lt;head rend="h4"&gt;Configuring tunnels&lt;/head&gt;
    &lt;p&gt;You can either configure tunnels through the Zero Trust UI by ‚Äúadopting‚Äù them, or configure them in the &lt;code&gt;/etc/cloudflared/config.yml&lt;/code&gt; config on the machine itself. Personal preference, I usually configure them on the machine itself.&lt;/p&gt;
    &lt;p&gt;The config specifies where a request should get routed to when it arrives at the tunnel. So the tunnel knows what to do with it.&lt;/p&gt;
    &lt;p&gt;In this config we tell cloudflared to route traffic arriving at this tunnel for hostname &lt;code&gt;gitlab.widgetcorp.tech&lt;/code&gt; to localhost:80, and &lt;code&gt;gitlab-ssh&lt;/code&gt; to the local SSH server.&lt;/p&gt;
    &lt;code&gt;‚ùØ cat /etc/cloudflared/config.yml
tunnel: a2f17e27-cd4d-4fcd-b02a-63839f57a96f
credentials-file: /etc/cloudflared/a2f17e27-cd4d-4fcd-b02a-63839f57a96f.json
ingress:
  - hostname: gitlab.widgetcorp.tech
    service: http://localhost:80
  - hostname: gitlab-ssh.widgetcorp.tech
    service: ssh://localhost:22
  - service: http_status:404

  # Catch-all for WARP routing
  - service: http_status:404

warp-routing:
  enabled: true
&lt;/code&gt;
    &lt;p&gt;The config alone doesn‚Äôt do anything. It just exposes a tunnel, and that‚Äôs it. What we need now are routes and targets.&lt;/p&gt;
    &lt;head rend="h4"&gt;Exposing a private network to the public with tunnels quickly&lt;/head&gt;
    &lt;p&gt;Quick addition, as this is a super common use case. If you want to just expose something in your home network to the internet, you can add a config like this:&lt;/p&gt;
    &lt;code&gt;tunnel: a2f17e27-cd4d-4fcd-b02a-63839f57a96f
credentials-file: /etc/cloudflared/a2f17e27-cd4d-4fcd-b02a-63839f57a96f.json
ingress:
  - hostname: homeassistant.mydomain.com
    service: http://192.168.1.3:80
&lt;/code&gt;
    &lt;p&gt;Then go into Cloudflare DNS settings and map the domain &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to the tunnel:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;CNAME homeassistant.mydomain.com a2f17e27-cd4d-4fcd-b02a-63839f57a96f.cfargotunnel.com&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Now all traffic going to this domain will go through the cloudflared tunnel, which is configured to route &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to &lt;code&gt;192.168.1.3&lt;/code&gt;. No Warp client needed, Argo tunnel does everything for us.&lt;/p&gt;
    &lt;p&gt;Note: If you adopted the tunnels and don‚Äôt use &lt;code&gt;config.yaml&lt;/code&gt;, you can automatically create matching DNS records in the Cloudflare UI and don‚Äôt need to do this manually.&lt;/p&gt;
    &lt;head rend="h3"&gt;Routes&lt;/head&gt;
    &lt;p&gt;A route defines where to direct traffic to.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say your homeassistant runs on 192.168.1.3 at home and you want to reach it from outside. Just above we deployed a &lt;code&gt;cloudflared&lt;/code&gt; tunnel on our router at 192.168.1.3, and added a config pointing the domain to the Argo tunnel, so &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; is already available to the public. However, &lt;code&gt;192.168.1.3&lt;/code&gt; isn‚Äôt, as it‚Äôs a private network IP.&lt;/p&gt;
    &lt;p&gt;You can define:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A route like &lt;code&gt;192.168.1.1/24&lt;/code&gt;pointing at your tunnel, to route ALL traffic to the full IP range through that tunnel (so even 192.168.1.245 will go through your tunnel)&lt;/item&gt;
      &lt;item&gt;Or a more specific route like &lt;code&gt;192.168.1.3/32&lt;/code&gt;pointing at your tunnel, to ONLY route traffic to 192.168.1.3 through that tunnel.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When configured, once your user connects their Warp client that‚Äôs set up with your Zero Trust network, the Warp client will see requests to 192.168.1.3 and route it through the Cloudflare network to reach your specific tunnel. Like a little police helper directing cars where to go.&lt;/p&gt;
    &lt;p&gt;If the Warp client is not connected, 192.168.1.3 will just resolve in your current local network. If connected, it will resolve to the tunnel.&lt;/p&gt;
    &lt;p&gt;The routed IP doesn‚Äôt need to exist! So you could, for example, route a random IP you like (e.g., 10.128.1.1) to your tunnel, the tunnel then forwards it based on your routes, for example to 192.168.1.1. This is extremely powerful because it allows you to build your own fully virtual network.&lt;/p&gt;
    &lt;p&gt;That‚Äôs all it does, what happens afterwards is up to the tunnel config that we created above. The tunnel decides where to point the incoming request to, whether that‚Äôs localhost or somewhere else.&lt;/p&gt;
    &lt;p&gt;To summarize, the &lt;code&gt;route&lt;/code&gt; tells the Warp client where to route traffic to.&lt;/p&gt;
    &lt;p&gt;Now we have 2 things working:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;- goes through a Cloudflare DNS record pointing at an Argo tunnel, which then forwards to 192.168.1.3. This works without Warp connected as it‚Äôs on the DNS level, public to everyone.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;192.168.1.3&lt;/code&gt;- The Warp client sees the request and routes it through the Argo tunnel, which then forwards it to&lt;code&gt;192.168.1.3&lt;/code&gt;within that network. This needs Warp connected to work, and is only visible to people in your Zero Trust org.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Targets&lt;/head&gt;
    &lt;p&gt;This one took me a while.&lt;/p&gt;
    &lt;p&gt;Targets are needed to define a piece of infrastructure that you want to protect through Zero Trust. They are like a pointer pointing to something in your network. This goes hand-in-hand with routes, but isn‚Äôt always needed.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say you have 192.168.1.3 (homeassistant) exposed through a Cloudflare tunnel. By default, anyone in your network that is part of your Zero Trust org and has Warp client installed can now access your homeassistant at 192.168.1.3.&lt;/p&gt;
    &lt;p&gt;We can change that with targets. For example, defining a target with hostname = &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to the route &lt;code&gt;192.168.1.3/32&lt;/code&gt; allows us to add access policies to it. We can also put an entire network into the target by specifying &lt;code&gt;192.168.1.3/24&lt;/code&gt; to control access. This also works with virtual IPs like 10.128.1.1!&lt;/p&gt;
    &lt;p&gt;Targets alone won‚Äôt do anything, they just point to the service or network. ‚ÄúHey, here is homeassistant‚Äù, or ‚Äúhey, here is my home network‚Äù.&lt;/p&gt;
    &lt;head rend="h2"&gt;Access Policies: Protecting Who Can Access What&lt;/head&gt;
    &lt;p&gt;Continuing the example from above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;we have a tunnel running on our home network that routes &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to&lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;we set up public DNS records to point &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to the Argo tunnel in Cloudflare&lt;/item&gt;
      &lt;item&gt;we created a route &lt;code&gt;192.168.1.3&lt;/code&gt;to go through the same tunnel&lt;/item&gt;
      &lt;item&gt;we also created a target pointing to &lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When users access either &lt;code&gt;192.168.1.3&lt;/code&gt; or &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;, the Warp client will route the request through the tunnel, which then forwards the request to 192.168.1.3. Homeassistant loads and everything is fine.&lt;/p&gt;
    &lt;p&gt;But do we want that?&lt;/p&gt;
    &lt;p&gt;Probably not.&lt;/p&gt;
    &lt;p&gt;Access policies to the rescue!&lt;/p&gt;
    &lt;p&gt;With access policies, we can leave things in the public but protect them with Cloudflare Zero Trust access. So while 192.168.1.3 is only available if Warp is connected (so routing to it works), we can add security to our public &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Go to Access -&amp;gt; Applications -&amp;gt; Add an Application -&amp;gt; Self-hosted.&lt;/p&gt;
    &lt;p&gt;Here we can define what should be protected, and how.&lt;/p&gt;
    &lt;p&gt;Going with our previous example, we can add a public hostname &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; or an IP like &lt;code&gt;192.168.1.3&lt;/code&gt; (or both), then attach policies of who should be able to access it.&lt;/p&gt;
    &lt;p&gt;You can specify Include (‚ÄúOR‚Äù) and Require (‚ÄúAND‚Äù) selectors.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Require rules must always be met, on top of include rules, to grant access&lt;/item&gt;
      &lt;item&gt;Any of the Include rules must match to grant access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then there are Actions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow - when the policy matches, allow access&lt;/item&gt;
      &lt;item&gt;Deny - when the policy matches, deny access. aka blocking something.&lt;/item&gt;
      &lt;item&gt;Bypass - when the policy matches, bypass Zero Trust completely. No more checking.&lt;/item&gt;
      &lt;item&gt;Service Auth - when the policy matches, allow authentication to the service with a service token header (good for server-to-server, or bots). Check Access -&amp;gt; Service Auth to create these tokens.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Allow public access to everyone logging into your network&lt;/head&gt;
    &lt;p&gt;The most common use case: &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; is public. We want to keep it public, but add an extra layer of security.&lt;/p&gt;
    &lt;p&gt;Add an include policy, pick any of the &lt;code&gt;email&lt;/code&gt; selectors, add the email of the user you want to allow access to. Now only people authenticated with your Zero Trust org with the specified emails can access your homeassistant, without needing to have Warp running.&lt;/p&gt;
    &lt;p&gt;We can harden this by adding require rules: Add a Login Method selector rule, pick a specific login method like GitHub. Now only people with specific emails that have authenticated through GitHub can access your homeassistant, without needing to have Warp running.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bypass login completely when connected through WARP&lt;/head&gt;
    &lt;p&gt;Another policy I like having is to skip the login screen entirely when connected through Warp. If a user is already enrolled into my Zero Trust org and has the Warp client provisioned, then there‚Äôs no need to ask them to authenticate again.&lt;/p&gt;
    &lt;p&gt;We can add a separate policy (don‚Äôt edit the one we just created above), pick the Gateway selector and set it to Allow or Bypass.&lt;/p&gt;
    &lt;p&gt;Don‚Äôt use ‚ÄòWarp‚Äô - the Warp selector will match anyone that has Warp running, including the consumer 1.1.1.1 app. Gateway, on the other hand, matches only if someone is connecting through your Gateway, be that DNS or a provisioned Warp client.&lt;/p&gt;
    &lt;p&gt;(The ‚ÄòGateway‚Äô selector is only available if the Warp client is set to allow WARP authentication identity)&lt;/p&gt;
    &lt;p&gt;Now when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Warp through Zero Trust is running on a machine: No login screen&lt;/item&gt;
      &lt;item&gt;No Warp running (public access): Prompt for login screen, but only allow specific emails that authenticated through GitHub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This setup makes it very convenient to reach homeassistant, no matter if connected through Warp or not.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deploying the Warp client and enrolling into Zero Trust&lt;/head&gt;
    &lt;p&gt;Are you still with me?&lt;/p&gt;
    &lt;p&gt;Our network is basically done. We have a login-protected &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; that routes through our tunnel into our private network and terminates at &lt;code&gt;192.168.1.3&lt;/code&gt;, and we have a direct route to &lt;code&gt;192.168.1.3&lt;/code&gt; that only works when connected with Warp.&lt;/p&gt;
    &lt;p&gt;We also have login policies to make sure only specific users (logged in with GitHub and certain email addresses) can access homeassistant.&lt;/p&gt;
    &lt;p&gt;So how do we deploy the dang Warp client?&lt;/p&gt;
    &lt;p&gt;Actually the same: We create some policies.&lt;/p&gt;
    &lt;p&gt;Head to Settings -&amp;gt; Warp Client&lt;/p&gt;
    &lt;p&gt;In Enrollment Permissions, we specify the same policies for who can enroll. For example, ‚Äú[email protected]‚Äù when authenticated through GitHub is allowed to enroll. In the Login Methods we can specify what login methods are available when someone tries to enroll into our Zero Trust org.&lt;/p&gt;
    &lt;p&gt;Toggle WARP authentication identity settings to make the Gateway selector available in policies, effectively allowing the configured WARP client to be used as a login method.&lt;/p&gt;
    &lt;p&gt;Careful here, once someone is enrolled, they are basically in your Zero Trust network through Warp. Make sure you harden this.&lt;/p&gt;
    &lt;p&gt;Then, in Profile settings, we define how the WARP client behaves. These are things like protocol: MASQUE or WireGuard, service mode, what IPs and domains to exclude from WARP routing (e.g., the local network should never go through WARP), setting it to exclude or include mode and so on.&lt;/p&gt;
    &lt;p&gt;Other settings I recommend setting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install CA to system certificate store - installs the Cloudflare CA certificate automatically when enrolled.&lt;/item&gt;
      &lt;item&gt;Override local interface IP - assigns a unique CGNAT private IP to the client. This is needed for warp-to-warp routing.&lt;/item&gt;
      &lt;item&gt;Device Posture - what checks the WARP client should perform for the org. E.g., check the OS version, some OS files on disk, etc. I have this set to WARP and Gateway because I want the client to provide information on whether the user is connected through WARP and Gateway, for skipping certain login pages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once done, just open the Warp client (https://developers.cloudflare.com/warp-client/), and log in to your network. This should open the login pages you specified in the Device Enrollment screen, and check all the enrollment policies you specified.&lt;/p&gt;
    &lt;p&gt;Once passed, congratulations, your WARP client is now connected to your Zero Trust network. The client will then go ahead and start routing &lt;code&gt;192.168.1.3&lt;/code&gt; through your tunnels, as specified in your tunnel and route settings.&lt;/p&gt;
    &lt;p&gt;üéâ&lt;/p&gt;
    &lt;head rend="h2"&gt;What we built&lt;/head&gt;
    &lt;p&gt;If you followed this guide, here is what we built:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Login methods to connect the Warp client to your Zero Trust org through GitHub and specific email addresses&lt;/item&gt;
      &lt;item&gt;A tunnel within your private network that&lt;list rend="ul"&gt;&lt;item&gt;Forwards any request coming in with host &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to&lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Forwards any request coming in with host &lt;/item&gt;
      &lt;item&gt;A route that forwards all traffic for &lt;code&gt;192.168.1.3&lt;/code&gt;to the tunnel in your private network, which will terminate it at 192.168.1.3, which will only work when connected through Warp to route the request&lt;/item&gt;
      &lt;item&gt;A DNS name &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;that points to the Argo tunnel, and will allow everyone (even if not connected through Warp) to access homeassistant which runs at 192.168.1.3&lt;/item&gt;
      &lt;item&gt;Access policies that will&lt;list rend="ul"&gt;&lt;item&gt;Ask users that are not connected to Zero Trust through Warp to log in with GitHub and specific email, so everyone can access it if they can log in&lt;/item&gt;&lt;item&gt;A policy that skips the login screen completely and just shows homeassistant if the user connects through Zero Trust Warp client (enrolled into our org)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don‚Äôt need the public domain and you don‚Äôt need the route to 192.168.1.3. These are 2 different options that you can use to expose homeassistant when you‚Äôre not at home. One is using a public domain name everyone can see, one is explicitly requiring connecting through enrolled Warp.&lt;/p&gt;
    &lt;p&gt;What I didn‚Äôt cover in this post:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Warp-to-warp routing&lt;/item&gt;
      &lt;item&gt;Creating and assigning fully private IPs that only exist within your Zero Trust network&lt;/item&gt;
      &lt;item&gt;SSH authentication through Zero Trust access policies (that‚Äôs what we need Targets for)&lt;/item&gt;
      &lt;item&gt;The other application types besides Self-Hosted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôm happy to expand on it if there‚Äôs interest. Let me know on X or Bluesky.&lt;/p&gt;
    &lt;p&gt;Happy tunneling! ‚õÖ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45946865</guid><pubDate>Sun, 16 Nov 2025 17:39:43 +0000</pubDate></item><item><title>Z3 API in Python: From Sudoku to N-Queens in Under 20 Lines (2015)</title><link>https://ericpony.github.io/z3py-tutorial/guide-examples.htm</link><description>&lt;doc fingerprint="1f33dc5d2c218990"&gt;
  &lt;main&gt;
    &lt;p&gt;Z3 is a high performance theorem prover developed at Microsoft Research. Z3 is used in many applications such as: software/hardware verification and testing, constraint solving, analysis of hybrid systems, security, biology (in silico analysis), and geometrical problems.&lt;/p&gt;
    &lt;p&gt;This tutorial demonstrates the main capabilities of Z3Py: the Z3 API in Python. No Python background is needed to read this tutorial. However, it is useful to learn Python (a fun language!) at some point, and there are many excellent free resources for doing so (Python Tutorial).&lt;/p&gt;
    &lt;p&gt;The Z3 distribution also contains the C, .Net and OCaml APIs. The source code of Z3Py is available in the Z3 distribution, feel free to modify it to meet your needs. The source code also demonstrates how to use new features in Z3 4.0. Other cool front-ends for Z3 include Scala^Z3 and SBV.&lt;/p&gt;
    &lt;p&gt;Please send feedback, comments and/or corrections to leonardo@microsoft.com. Your comments are very valuable.&lt;/p&gt;
    &lt;p&gt;Let us start with the following simple example:&lt;/p&gt;
    &lt;code&gt;x = Int('x')
y = Int('y')
solve(x &amp;gt; 2, y &amp;lt; 10, x + 2*y == 7)
&lt;/code&gt;
    &lt;p&gt;The function Int('x') creates an integer variable in Z3 named x. The solve function solves a system of constraints. The example above uses two variables x and y, and three constraints. Z3Py like Python uses = for assignment. The operators &amp;lt;, &amp;lt;=, &amp;gt;, &amp;gt;=, == and != for comparison. In the example above, the expression x + 2*y == 7 is a Z3 constraint. Z3 can solve and crunch formulas.&lt;/p&gt;
    &lt;p&gt;The next examples show how to use the Z3 formula/expression simplifier.&lt;/p&gt;
    &lt;code&gt;x = Int('x')
y = Int('y')
print (simplify(x + y + 2*x + 3))
print (simplify(x &amp;lt; y + x + 2))
print (simplify(And(x + 1 &amp;gt;= 3, x**2 + x**2 + y**2 + 2 &amp;gt;= 5)))
&lt;/code&gt;
    &lt;p&gt;By default, Z3Py (for the web) displays formulas and expressions using mathematical notation. As usual, ‚àß is the logical and, ‚à® is the logical or, and so on. The command set_option(html_mode=False) makes all formulas and expressions to be displayed in Z3Py notation. This is also the default mode for the offline version of Z3Py that comes with the Z3 distribution.&lt;/p&gt;
    &lt;code&gt;x = Int('x')
y = Int('y')
print (x**2 + y**2 &amp;gt;= 1)
set_option(html_mode=False)
print (x**2 + y**2 &amp;gt;= 1)
&lt;/code&gt;
    &lt;p&gt;Z3 provides functions for traversing expressions.&lt;/p&gt;
    &lt;code&gt;x = Int('x')
y = Int('y')
n = x + y &amp;gt;= 3
print ("num args: ", n.num_args())
print ("children: ", n.children())
print ("1st child:", n.arg(0))
print ("2nd child:", n.arg(1))
print ("operator: ", n.decl())
print ("op name:  ", n.decl().name())
&lt;/code&gt;
    &lt;p&gt;Z3 provides all basic mathematical operations. Z3Py uses the same operator precedence of the Python language. Like Python, ** is the power operator. Z3 can solve nonlinear polynomial constraints.&lt;/p&gt;
    &lt;code&gt;x = Real('x')
y = Real('y')
solve(x**2 + y**2 &amp;gt; 3, x**3 + y &amp;lt; 5)
&lt;/code&gt;
    &lt;p&gt;The procedure Real('x') creates the real variable x. Z3Py can represent arbitrarily large integers, rational numbers (like in the example above), and irrational algebraic numbers. An irrational algebraic number is a root of a polynomial with integer coefficients. Internally, Z3 represents all these numbers precisely. The irrational numbers are displayed in decimal notation for making it easy to read the results.&lt;/p&gt;
    &lt;code&gt;x = Real('x')
y = Real('y')
solve(x**2 + y**2 == 3, x**3 == 2)

set_option(precision=30)
print ("Solving, and displaying result with 30 decimal places")
solve(x**2 + y**2 == 3, x**3 == 2)
&lt;/code&gt;
    &lt;p&gt;The procedure set_option is used to configure the Z3 environment. It is used to set global configuration options such as how the result is displayed. The option set_option(precision=30) sets the number of decimal places used when displaying results. The ? mark in 1.2599210498? indicates the output is truncated.&lt;/p&gt;
    &lt;p&gt;The following example demonstrates a common mistake. The expression 3/2 is a Python integer and not a Z3 rational number. The example also shows different ways to create rational numbers in Z3Py. The procedure Q(num, den) creates a Z3 rational where num is the numerator and den is the denominator. The RealVal(1) creates a Z3 real number representing the number 1.&lt;/p&gt;
    &lt;code&gt;print (1/3)
print (RealVal(1)/3)
print (Q(1,3))

x = Real('x')
print (x + 1/3)
print (x + Q(1,3))
print (x + "1/3")
print (x + 0.25)
&lt;/code&gt;
    &lt;p&gt;Rational numbers can also be displayed in decimal notation.&lt;/p&gt;
    &lt;code&gt;x = Real('x')
solve(3*x == 1)

set_option(rational_to_decimal=True)
solve(3*x == 1)

set_option(precision=30)
solve(3*x == 1)
&lt;/code&gt;
    &lt;p&gt;A system of constraints may not have a solution. In this case, we say the system is unsatisfiable.&lt;/p&gt;
    &lt;code&gt;x = Real('x')
solve(x &amp;gt; 4, x &amp;lt; 0)
&lt;/code&gt;
    &lt;p&gt;Like in Python, comments begin with the hash character # and are terminated by the end of line. Z3Py does not support comments that span more than one line.&lt;/p&gt;
    &lt;code&gt;# This is a comment
x = Real('x') # comment: creating x
print (x**2 + 2*x + 2  # comment: printing polynomial)
&lt;/code&gt;
    &lt;p&gt;Z3 supports Boolean operators: And, Or, Not, Implies (implication), If (if-then-else). Bi-implications are represented using equality ==. The following example shows how to solve a simple set of Boolean constraints.&lt;/p&gt;
    &lt;code&gt;p = Bool('p')
q = Bool('q')
r = Bool('r')
solve(Implies(p, q), r == Not(q), Or(Not(p), r))

&lt;/code&gt;
    &lt;p&gt;The Python Boolean constants True and False can be used to build Z3 Boolean expressions.&lt;/p&gt;
    &lt;code&gt;p = Bool('p')
q = Bool('q')
print (And(p, q, True))
print (simplify(And(p, q, True)))
print (simplify(And(p, False)))
&lt;/code&gt;
    &lt;p&gt;The following example uses a combination of polynomial and Boolean constraints.&lt;/p&gt;
    &lt;code&gt;p = Bool('p')
x = Real('x')
solve(Or(x &amp;lt; 5, x &amp;gt; 10), Or(p, x**2 == 2), Not(p))
&lt;/code&gt;
    &lt;p&gt;Z3 provides different solvers. The command solve, used in the previous examples, is implemented using the Z3 solver API. The implementation can be found in the file z3.py in the Z3 distribution. The following example demonstrates the basic Solver API.&lt;/p&gt;
    &lt;code&gt;x = Int('x')
y = Int('y')

s = Solver()
print (s)

s.add(x &amp;gt; 10, y == x + 2)
print (s)
print ("Solving constraints in the solver s ...")
print (s.check())

print ("Create a new scope...")
s.push()
s.add(y &amp;lt; 11)
print (s)
print ("Solving updated set of constraints...")
print (s.check())

print ("Restoring state...")
s.pop()
print (s)
print ("Solving restored set of constraints...")
print (s.check())
&lt;/code&gt;
    &lt;p&gt;The command Solver() creates a general purpose solver. Constraints can be added using the method add. We say the constraints have been asserted in the solver. The method check() solves the asserted constraints. The result is sat (satisfiable) if a solution was found. The result is unsat (unsatisfiable) if no solution exists. We may also say the system of asserted constraints is infeasible. Finally, a solver may fail to solve a system of constraints and unknown is returned.&lt;/p&gt;
    &lt;p&gt;In some applications, we want to explore several similar problems that share several constraints. We can use the commands push and pop for doing that. Each solver maintains a stack of assertions. The command push creates a new scope by saving the current stack size. The command pop removes any assertion performed between it and the matching push. The check method always operates on the content of solver assertion stack.&lt;/p&gt;
    &lt;p&gt;The following example shows an example that Z3 cannot solve. The solver returns unknown in this case. Recall that Z3 can solve nonlinear polynomial constraints, but 2**x is not a polynomial.&lt;/p&gt;
    &lt;code&gt;x = Real('x')
s = Solver()
s.add(2**x == 3)
print (s.check())
&lt;/code&gt;
    &lt;p&gt;The following example shows how to traverse the constraints asserted into a solver, and how to collect performance statistics for the check method.&lt;/p&gt;
    &lt;code&gt;x = Real('x')
y = Real('y')
s = Solver()
s.add(x &amp;gt; 1, y &amp;gt; 1, Or(x + y &amp;gt; 3, x - y &amp;lt; 2))
print ("asserted constraints...")
for c in s.assertions():
    print (c)

print (s.check())
print ("statistics for the last check method...")
print (s.statistics())
# Traversing statistics
for k, v in s.statistics():
    print ("%s : %s" % (k, v))
&lt;/code&gt;
    &lt;p&gt;The command check returns sat when Z3 finds a solution for the set of asserted constraints. We say Z3 satisfied the set of constraints. We say the solution is a model for the set of asserted constraints. A model is an interpretation that makes each asserted constraint true. The following example shows the basic methods for inspecting models.&lt;/p&gt;
    &lt;code&gt;x, y, z = Reals('x y z')
s = Solver()
s.add(x &amp;gt; 1, y &amp;gt; 1, x + y &amp;gt; 3, z - x &amp;lt; 10)
print (s.check())

m = s.model()
print ("x = %s" % m[x])

print ("traversing model...")
for d in m.decls():
    print ("%s = %s" % (d.name(), m[d]))
&lt;/code&gt;
    &lt;p&gt;In the example above, the function Reals('x y z') creates the variables. x, y and z. It is shorthand for:&lt;/p&gt;
    &lt;quote&gt;x = Real('x') y = Real('y') z = Real('z')&lt;/quote&gt;
    &lt;p&gt;The expression m[x] returns the interpretation of x in the model m. The expression "%s = %s" % (d.name(), m[d]) returns a string where the first %s is replaced with the name of d (i.e., d.name()), and the second %s with a textual representation of the interpretation of d (i.e., m[d]). Z3Py automatically converts Z3 objects into a textual representation when needed.&lt;/p&gt;
    &lt;p&gt;Z3 supports real and integer variables. They can be mixed in a single problem. Like most programming languages, Z3Py will automatically add coercions converting integer expressions to real ones when needed. The following example demonstrates different ways to declare integer and real variables.&lt;/p&gt;
    &lt;code&gt;x = Real('x')
y = Int('y')
a, b, c = Reals('a b c')
s, r = Ints('s r')
print (x + y + 1 + (a + s))
print (ToReal(y) + c)
&lt;/code&gt;
    &lt;p&gt;The function ToReal casts an integer expression into a real expression.&lt;/p&gt;
    &lt;p&gt;Z3Py supports all basic arithmetic operations.&lt;/p&gt;
    &lt;code&gt;a, b, c = Ints('a b c')
d, e = Reals('d e')
solve(a &amp;gt; b + 2,
      a == 2*c + 10,
      c + b &amp;lt;= 1000,
      d &amp;gt;= e)
&lt;/code&gt;
    &lt;p&gt;The command simplify applies simple transformations on Z3 expressions.&lt;/p&gt;
    &lt;code&gt;x, y = Reals('x y')
# Put expression in sum-of-monomials form
t = simplify((x + y)**3, som=True)
print (t)
# Use power operator
t = simplify(t, mul_to_power=True)
print (t)
&lt;/code&gt;
    &lt;p&gt;The command help_simplify() prints all available options. Z3Py allows users to write option in two styles. The Z3 internal option names start with : and words are separated by -. These options can be used in Z3Py. Z3Py also supports Python-like names, where : is suppressed and - is replaced with _. The following example demonstrates how to use both styles.&lt;/p&gt;
    &lt;code&gt;x, y = Reals('x y')
# Using Z3 native option names
print (simplify(x == y + 2, ':arith-lhs', True))
# Using Z3Py option names
print (simplify(x == y + 2, arith_lhs=True))

print ("\nAll available options:")
help_simplify()
&lt;/code&gt;
    &lt;p&gt;Z3Py supports arbitrarily large numbers. The following example demonstrates how to perform basic arithmetic using larger integer, rational and irrational numbers. Z3Py only supports algebraic irrational numbers. Algebraic irrational numbers are sufficient for presenting the solutions of systems of polynomial constraints. Z3Py will always display irrational numbers in decimal notation since it is more convenient to read. The internal representation can be extracted using the method sexpr(). It displays Z3 internal representation for mathematical formulas and expressions in s-expression (Lisp-like) notation.&lt;/p&gt;
    &lt;code&gt;x, y = Reals('x y')
solve(x + 10000000000000000000000 == y, y &amp;gt; 20000000000000000)

print (Sqrt(2) + Sqrt(3))
print (simplify(Sqrt(2) + Sqrt(3)))
print (simplify(Sqrt(2) + Sqrt(3)).sexpr())
# The sexpr() method is available for any Z3 expression
print ((x + Sqrt(y) * 2).sexpr())
&lt;/code&gt;
    &lt;p&gt;Modern CPUs and main-stream programming languages use arithmetic over fixed-size bit-vectors. Machine arithmetic is available in Z3Py as Bit-Vectors. They implement the precise semantics of unsigned and of signed two-complements arithmetic.&lt;/p&gt;
    &lt;p&gt;The following example demonstrates how to create bit-vector variables and constants. The function BitVec('x', 16) creates a bit-vector variable in Z3 named x with 16 bits. For convenience, integer constants can be used to create bit-vector expressions in Z3Py. The function BitVecVal(10, 32) creates a bit-vector of size 32 containing the value 10.&lt;/p&gt;
    &lt;code&gt;x = BitVec('x', 16)
y = BitVec('y', 16)
print (x + 2)
# Internal representation
print ((x + 2).sexpr())

# -1 is equal to 65535 for 16-bit integers 
print (simplify(x + y - 1))

# Creating bit-vector constants
a = BitVecVal(-1, 16)
b = BitVecVal(65535, 16)
print (simplify(a == b))

a = BitVecVal(-1, 32)
b = BitVecVal(65535, 32)
# -1 is not equal to 65535 for 32-bit integers 
print (simplify(a == b))
&lt;/code&gt;
    &lt;p&gt;In contrast to programming languages, such as C, C++, C#, Java, there is no distinction between signed and unsigned bit-vectors as numbers. Instead, Z3 provides special signed versions of arithmetical operations where it makes a difference whether the bit-vector is treated as signed or unsigned. In Z3Py, the operators &amp;lt;, &amp;lt;=, &amp;gt;, &amp;gt;=, /, % and &amp;gt;&amp;gt; correspond to the signed versions. The corresponding unsigned operators are ULT, ULE, UGT, UGE, UDiv, URem and LShR.&lt;/p&gt;
    &lt;code&gt;# Create to bit-vectors of size 32
x, y = BitVecs('x y', 32)

solve(x + y == 2, x &amp;gt; 0, y &amp;gt; 0)

# Bit-wise operators
# &amp;amp; bit-wise and
# | bit-wise or
# ~ bit-wise not
solve(x &amp;amp; y == ~y)

solve(x &amp;lt; 0)

# using unsigned version of &amp;lt; 
solve(ULT(x, 0))
&lt;/code&gt;
    &lt;p&gt;The operator &amp;gt;&amp;gt; is the arithmetic shift right, and &amp;lt;&amp;lt; is the shift left. The logical shift right is the operator LShR.&lt;/p&gt;
    &lt;code&gt;# Create to bit-vectors of size 32
x, y = BitVecs('x y', 32)

solve(x &amp;gt;&amp;gt; 2 == 3)

solve(x &amp;lt;&amp;lt; 2 == 3)

solve(x &amp;lt;&amp;lt; 2 == 24)
&lt;/code&gt;
    &lt;p&gt;Unlike programming languages, where functions have side-effects, can throw exceptions, or never return, functions in Z3 have no side-effects and are total. That is, they are defined on all input values. This includes functions, such as division. Z3 is based on first-order logic.&lt;/p&gt;
    &lt;p&gt;Given a constraints such as x + y &amp;gt; 3, we have been saying that x and y are variables. In many textbooks, x and y are called uninterpreted constants. That is, they allow any interpretation that is consistent with the constraint x + y &amp;gt; 3.&lt;/p&gt;
    &lt;p&gt;More precisely, function and constant symbols in pure first-order logic are uninterpreted or free, which means that no a priori interpretation is attached. This is in contrast to functions belonging to the signature of theories, such as arithmetic where the function + has a fixed standard interpretation (it adds two numbers). Uninterpreted functions and constants are maximally flexible; they allow any interpretation that is consistent with the constraints over the function or constant.&lt;/p&gt;
    &lt;p&gt;To illustrate uninterpreted functions and constants let us the uninterpreted integer constants (aka variables) x, y. Finally let f be an uninterpreted function that takes one argument of type (aka sort) integer and results in an integer value. The example illustrates how one can force an interpretation where f applied twice to x results in x again, but f applied once to x is different from x.&lt;/p&gt;
    &lt;code&gt;x = Int('x')
y = Int('y')
f = Function('f', IntSort(), IntSort())
solve(f(f(x)) == x, f(x) == y, x != y)
&lt;/code&gt;
    &lt;p&gt;The solution (interpretation) for f should be read as f(0) is 1, f(1) is 0, and f(a) is 1 for all a different from 0 and 1.&lt;/p&gt;
    &lt;p&gt;In Z3, we can also evaluate expressions in the model for a system of constraints. The following example shows how to use the evaluate method.&lt;/p&gt;
    &lt;code&gt;x = Int('x')
y = Int('y')
f = Function('f', IntSort(), IntSort())
s = Solver()
s.add(f(f(x)) == x, f(x) == y, x != y)
print (s.check())
m = s.model()
print ("f(f(x)) =", m.evaluate(f(f(x))))
print ("f(x)    =", m.evaluate(f(x)))
&lt;/code&gt;
    &lt;p&gt;A formula/constraint F is valid if F always evaluates to true for any assignment of appropriate values to its uninterpreted symbols. A formula/constraint F is satisfiable if there is some assignment of appropriate values to its uninterpreted symbols under which F evaluates to true. Validity is about finding a proof of a statement; satisfiability is about finding a solution to a set of constraints. Consider a formula F containing a and b. We can ask whether F is valid, that is whether it is always true for any combination of values for a and b. If F is always true, then Not(F) is always false, and then Not(F) will not have any satisfying assignment (i.e., solution); that is, Not(F) is unsatisfiable. That is, F is valid precisely when Not(F) is not satisfiable (is unsatisfiable). Alternately, F is satisfiable if and only if Not(F) is not valid (is invalid). The following example proves the deMorgan's law.&lt;/p&gt;
    &lt;p&gt;The following example redefines the Z3Py function prove that receives a formula as a parameter. This function creates a solver, adds/asserts the negation of the formula, and check if the negation is unsatisfiable. The implementation of this function is a simpler version of the Z3Py command prove.&lt;/p&gt;
    &lt;code&gt;p, q = Bools('p q')
demorgan = And(p, q) == Not(Or(Not(p), Not(q)))
print (demorgan)

def prove(f):
    s = Solver()
    s.add(Not(f))
    if s.check() == unsat:
        print ("proved")
    else:
        print ("failed to prove")

print ("Proving demorgan...")
prove(demorgan)
&lt;/code&gt;
    &lt;p&gt;Python supports list comprehensions. List comprehensions provide a concise way to create lists. They can be used to create Z3 expressions and problems in Z3Py. The following example demonstrates how to use Python list comprehensions in Z3Py.&lt;/p&gt;
    &lt;code&gt;# Create list [1, ..., 5] 
print ([ x + 1 for x in range(5) ])

# Create two lists containing 5 integer variables
X = [ Int('x%s' % i) for i in range(5) ]
Y = [ Int('y%s' % i) for i in range(5) ]
print (X)

# Create a list containing X[i]+Y[i]
X_plus_Y = [ X[i] + Y[i] for i in range(5) ]
print (X_plus_Y)

# Create a list containing X[i] &amp;gt; Y[i]
X_gt_Y = [ X[i] &amp;gt; Y[i] for i in range(5) ]
print (X_gt_Y)

print (And(X_gt_Y))

# Create a 3x3 "matrix" (list of lists) of integer variables
X = [ [ Int("x_%s_%s" % (i+1, j+1)) for j in range(3) ]
      for i in range(3) ]
pp(X)
&lt;/code&gt;
    &lt;p&gt;In the example above, the expression "x%s" % i returns a string where %s is replaced with the value of i.&lt;/p&gt;
    &lt;p&gt;The command pp is similar to print, but it uses Z3Py formatter for lists and tuples instead of Python's formatter.&lt;/p&gt;
    &lt;p&gt;Z3Py also provides functions for creating vectors of Boolean, Integer and Real variables. These functions are implemented using list comprehensions.&lt;/p&gt;
    &lt;code&gt;X = IntVector('x', 5)
Y = RealVector('y', 5)
P = BoolVector('p', 5)
print (X)
print (Y)
print (P)
print ([ y**2 for y in Y ])
print (Sum([ y**2 for y in Y ]))
&lt;/code&gt;
    &lt;p&gt;In high school, students learn the kinematic equations. These equations describe the mathematical relationship between displacement (d), time (t), acceleration (a), initial velocity (v_i) and final velocity (v_f). In Z3Py notation, we can write these equations as:&lt;/p&gt;
    &lt;quote&gt;d == v_i * t + (a*t**2)/2, v_f == v_i + a*t&lt;/quote&gt;
    &lt;p&gt;Ima Hurryin is approaching a stoplight moving with a velocity of 30.0 m/s. The light turns yellow, and Ima applies the brakes and skids to a stop. If Ima's acceleration is -8.00 m/s2, then determine the displacement of the car during the skidding process.&lt;/p&gt;
    &lt;code&gt;d, a, t, v_i, v_f = Reals('d a t v__i v__f')

equations = [
   d == v_i * t + (a*t**2)/2,
   v_f == v_i + a*t,
]
print ("Kinematic equations:")
print (equations)

# Given v_i, v_f and a, find d
problem = [
    v_i == 30,
    v_f == 0,
    a   == -8
]
print ("Problem:")
print (problem)

print ("Solution:")
solve(equations + problem)
&lt;/code&gt;
    &lt;p&gt;Ben Rushin is waiting at a stoplight. When it finally turns green, Ben accelerated from rest at a rate of a 6.00 m/s2 for a time of 4.10 seconds. Determine the displacement of Ben's car during this time period.&lt;/p&gt;
    &lt;code&gt;d, a, t, v_i, v_f = Reals('d a t v__i v__f')

equations = [
   d == v_i * t + (a*t**2)/2,
   v_f == v_i + a*t,
]

# Given v_i, t and a, find d
problem = [
    v_i == 0,
    t   == 4.10,
    a   == 6
]

solve(equations + problem)

# Display rationals in decimal notation
set_option(rational_to_decimal=True)

solve(equations + problem)
&lt;/code&gt;
    &lt;p&gt;Some low level hacks are very popular with C programmers. We use some of these hacks in the Z3 implementation.&lt;/p&gt;
    &lt;p&gt;This hack is frequently used in C programs (Z3 included) to test whether a machine integer is a power of two. We can use Z3 to prove it really works. The claim is that x != 0 &amp;amp;&amp;amp; !(x &amp;amp; (x - 1)) is true if and only if x is a power of two.&lt;/p&gt;
    &lt;code&gt;x      = BitVec('x', 32)
powers = [ 2**i for i in range(32) ]
fast   = And(x != 0, x &amp;amp; (x - 1) == 0)
slow   = Or([ x == p for p in powers ])
print (fast)
prove(fast == slow)

print ("trying to prove buggy version...")
fast   = x &amp;amp; (x - 1) == 0
prove(fast == slow)
&lt;/code&gt;
    &lt;p&gt;The following simple hack can be used to test whether two machine integers have opposite signs.&lt;/p&gt;
    &lt;code&gt;x      = BitVec('x', 32)
y      = BitVec('y', 32)

# Claim: (x ^ y) &amp;lt; 0 iff x and y have opposite signs
trick  = (x ^ y) &amp;lt; 0

# Naive way to check if x and y have opposite signs
opposite = Or(And(x &amp;lt; 0, y &amp;gt;= 0),
              And(x &amp;gt;= 0, y &amp;lt; 0))

prove(trick == opposite)
&lt;/code&gt;
    &lt;p&gt;Consider the following puzzle. Spend exactly 100 dollars and buy exactly 100 animals. Dogs cost 15 dollars, cats cost 1 dollar, and mice cost 25 cents each. You have to buy at least one of each. How many of each should you buy?&lt;/p&gt;
    &lt;code&gt;# Create 3 integer variables
dog, cat, mouse = Ints('dog cat mouse')
solve(dog &amp;gt;= 1,   # at least one dog
      cat &amp;gt;= 1,   # at least one cat
      mouse &amp;gt;= 1, # at least one mouse
      # we want to buy 100 animals
      dog + cat + mouse == 100,
      # We have 100 dollars (10000 cents):
      #   dogs cost 15 dollars (1500 cents), 
      #   cats cost 1 dollar (100 cents), and 
      #   mice cost 25 cents 
      1500 * dog + 100 * cat + 25 * mouse == 10000)
&lt;/code&gt;
    &lt;p&gt;Sudoku is a very popular puzzle. The goal is to insert the numbers in the boxes to satisfy only one condition: each row, column and 3x3 box must contain the digits 1 through 9 exactly once.&lt;/p&gt;
    &lt;p&gt;The following example encodes the sudoku problem in Z3. Different sudoku instances can be solved by modifying the matrix instance. This example makes heavy use of list comprehensions available in the Python programming language.&lt;/p&gt;
    &lt;code&gt;# 9x9 matrix of integer variables
X = [ [ Int("x_%s_%s" % (i+1, j+1)) for j in range(9) ]
      for i in range(9) ]

# each cell contains a value in {1, ..., 9}
cells_c  = [ And(1 &amp;lt;= X[i][j], X[i][j] &amp;lt;= 9)
             for i in range(9) for j in range(9) ]

# each row contains a digit at most once
rows_c   = [ Distinct(X[i]) for i in range(9) ]

# each column contains a digit at most once
cols_c   = [ Distinct([ X[i][j] for i in range(9) ])
             for j in range(9) ]

# each 3x3 square contains a digit at most once
sq_c     = [ Distinct([ X[3*i0 + i][3*j0 + j]
                        for i in range(3) for j in range(3) ])
             for i0 in range(3) for j0 in range(3) ]

sudoku_c = cells_c + rows_c + cols_c + sq_c

# sudoku instance, we use '0' for empty cells
instance = ((0,0,0,0,9,4,0,3,0),
            (0,0,0,5,1,0,0,0,7),
            (0,8,9,0,0,0,0,4,0),
            (0,0,0,0,0,0,2,0,8),
            (0,6,0,2,0,1,0,5,0),
            (1,0,2,0,0,0,0,0,0),
            (0,7,0,0,0,0,5,2,0),
            (9,0,0,0,6,5,0,0,0),
            (0,4,0,9,7,0,0,0,0))

instance_c = [ If(instance[i][j] == 0,
                  True,
                  X[i][j] == instance[i][j])
               for i in range(9) for j in range(9) ]

s = Solver()
s.add(sudoku_c + instance_c)
if s.check() == sat:
    m = s.model()
    r = [ [ m.evaluate(X[i][j]) for j in range(9) ]
          for i in range(9) ]
    print_matrix(r)
else:
    print ("failed to solve")
&lt;/code&gt;
    &lt;p&gt;The eight queens puzzle is the problem of placing eight chess queens on an 8x8 chessboard so that no two queens attack each other. Thus, a solution requires that no two queens share the same row, column, or diagonal.&lt;/p&gt;
    &lt;code&gt;# We know each queen must be in a different row.
# So, we represent each queen by a single integer: the column position
Q = [ Int('Q_%i' % (i + 1)) for i in range(8) ]

# Each queen is in a column {1, ... 8 }
val_c = [ And(1 &amp;lt;= Q[i], Q[i] &amp;lt;= 8) for i in range(8) ]

# At most one queen per column
col_c = [ Distinct(Q) ]

# Diagonal constraint
diag_c = [ If(i == j,
              True,
              And(Q[i] - Q[j] != i - j, Q[i] - Q[j] != j - i))
           for i in range(8) for j in range(i) ]

solve(val_c + col_c + diag_c)
&lt;/code&gt;
    &lt;p&gt;The install problem consists of determining whether a new set of packages can be installed in a system. This application is based on the article OPIUM: Optimal Package Install/Uninstall Manager. Many packages depend on other packages to provide some functionality. Each distribution contains a meta-data file that explicates the requirements of each package of the distribution. The meta-data contains details like the name, version, etc. More importantly, it contains depends and conflicts clauses that stipulate which other packages should be on the system. The depends clauses stipulate which other packages must be present. The conflicts clauses stipulate which other packages must not be present.&lt;/p&gt;
    &lt;p&gt;The install problem can be easily solved using Z3. The idea is to define a Boolean variable for each package. This variable is true if the package must be in the system. If package a depends on packages b, c and z, we write:&lt;/p&gt;
    &lt;quote&gt;DependsOn(a, [b, c, z])&lt;/quote&gt;
    &lt;p&gt;DependsOn is a simple Python function that creates Z3 constraints that capture the depends clause semantics.&lt;/p&gt;
    &lt;quote&gt;def DependsOn(pack, deps): return And([ Implies(pack, dep) for dep in deps ])&lt;/quote&gt;
    &lt;p&gt;Thus, Depends(a, [b, c, z]) generates the constraint&lt;/p&gt;
    &lt;quote&gt;And(Implies(a, b), Implies(a, c), Implies(a, z))&lt;/quote&gt;
    &lt;p&gt;That is, if users install package a, they must also install packages b, c and z.&lt;/p&gt;
    &lt;p&gt;If package d conflicts with package e, we write Conflict(d, e). Conflict is also a simple Python function.&lt;/p&gt;
    &lt;quote&gt;def Conflict(p1, p2): return Or(Not(p1), Not(p2))&lt;/quote&gt;
    &lt;p&gt;Conflict(d, e) generates the constraint Or(Not(d), Not(e)). With these two functions, we can easily encode the example in the Opium article (Section 2) in Z3Py as:&lt;/p&gt;
    &lt;code&gt;def DependsOn(pack, deps):
    return And([ Implies(pack, dep) for dep in deps ])

def Conflict(p1, p2):
    return Or(Not(p1), Not(p2))

a, b, c, d, e, f, g, z = Bools('a b c d e f g z')

solve(DependsOn(a, [b, c, z]),
      DependsOn(b, [d]),
      DependsOn(c, [Or(d, e), Or(f, g)]),
      Conflict(d, e),
      a, z)
&lt;/code&gt;
    &lt;p&gt;Note that the example contains the constraint&lt;/p&gt;
    &lt;quote&gt;DependsOn(c, [Or(d, e), Or(f, g)]),&lt;/quote&gt;
    &lt;p&gt;The meaning is: to install c, we must install d or e, and f or g&lt;/p&gt;
    &lt;p&gt;Now, we refine the previous example. First, we modify DependsOn to allow us to write DependsOn(b, d) instead of DependsOn(b, [d]). We also write a function install_check that returns a list of packages that must be installed in the system. The function Conflict is also modified. It can now receive multiple arguments.&lt;/p&gt;
    &lt;code&gt;def DependsOn(pack, deps):
    if is_expr(deps):
        return Implies(pack, deps)
    else:
        return And([ Implies(pack, dep) for dep in deps ])

def Conflict(*packs):
    return Or([ Not(pack) for pack in packs ])

a, b, c, d, e, f, g, z = Bools('a b c d e f g z')

def install_check(*problem):
    s = Solver()
    s.add(*problem)
    if s.check() == sat:
        m = s.model()
        r = []
        for x in m:
            if is_true(m[x]):
                # x is a Z3 declaration
                # x() returns the Z3 expression
                # x.name() returns a string
                r.append(x())
        print (r)
    else:
        print ("invalid installation profile")

print ("Check 1")
install_check(DependsOn(a, [b, c, z]),
              DependsOn(b, d),
              DependsOn(c, [Or(d, e), Or(f, g)]),
              Conflict(d, e),
              Conflict(d, g),
              a, z)

print ("Check 2")
install_check(DependsOn(a, [b, c, z]),
              DependsOn(b, d),
              DependsOn(c, [Or(d, e), Or(f, g)]),
              Conflict(d, e),
              Conflict(d, g),
              a, z, g)
&lt;/code&gt;
    &lt;p&gt;Z3Py is part of the Z3 distribution. It is located in the python subdirectory. To use it locally, you have to include the following command in your Python script.&lt;/p&gt;
    &lt;quote&gt;from Z3 import *&lt;/quote&gt;
    &lt;p&gt;The Z3 Python frontend directory must be in your PYTHONPATH environment variable. Z3Py will automatically search for the Z3 library (z3.dll (Windows), libz3.so (Linux), or libz3.dylib (OSX)). You may also initialize Z3Py manually using the command:&lt;/p&gt;
    &lt;quote&gt;init("z3.dll")&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45947301</guid><pubDate>Sun, 16 Nov 2025 18:38:37 +0000</pubDate></item><item><title>The fate of "small" open source</title><link>https://nolanlawson.com/2025/11/16/the-fate-of-small-open-source/</link><description>&lt;doc fingerprint="2360927dbacf3e3b"&gt;
  &lt;main&gt;
    &lt;p&gt;By far the most popular npm package I‚Äôve ever written is &lt;code&gt;blob-util&lt;/code&gt;, which is ~10 years old and still gets 5+ million weekly downloads.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a small collection of utilities for working with &lt;code&gt;Blob&lt;/code&gt;s in JavaScript. I wrote it because I found that PouchDB users were endlessly confused about how to work with &lt;code&gt;Blob&lt;/code&gt;s and how to convert them to strings, &lt;code&gt;ArrayBuffer&lt;/code&gt;s, etc.&lt;/p&gt;
    &lt;p&gt;Given that some 80% of developers are now using AI in their regular work, &lt;code&gt;blob-util&lt;/code&gt; is almost certainly the kind of thing that most developers would just happily have an LLM generate for them. Sure, you could use &lt;code&gt;blob-util&lt;/code&gt;, but then you‚Äôd be taking on an extra dependency, with unknown performance, maintenance, and supply-chain risks.&lt;/p&gt;
    &lt;p&gt;And sure enough, Claude will happily spit out whatever &lt;code&gt;Blob&lt;/code&gt; utilities you need when prompted:&lt;/p&gt;
    &lt;p&gt;&amp;gt; Write me a utility function in TypeScript to convert a Blob to an ArrayBuffer. It should return a Promise.&lt;/p&gt;
    &lt;quote&gt;function blobToArrayBuffer(blob: Blob): Promise&amp;lt;ArrayBuffer&amp;gt; { return new Promise((resolve, reject) =&amp;gt; { const reader = new FileReader(); reader.onload = () =&amp;gt; { if (reader.result instanceof ArrayBuffer) { resolve(reader.result); } else { reject(new Error('Failed to read Blob as ArrayBuffer')); } }; reader.onerror = () =&amp;gt; { reject(reader.error); }; reader.readAsArrayBuffer(blob); }); }&lt;/quote&gt;
    &lt;p&gt;Claude‚Äôs version is pretty close to the &lt;code&gt;blob-util&lt;/code&gt; version (unsurprising, since it was probably trained on it!). Although it‚Äôs much more verbose, unnecessarily checking if &lt;code&gt;readAsArrayBuffer&lt;/code&gt; actually gives you an &lt;code&gt;ArrayBuffer&lt;/code&gt; (although this does make TypeScript happy). To be fair, it also improves on my implementation by directly &lt;code&gt;reject&lt;/code&gt;ing with an error rather than the more awkward &lt;code&gt;onerror&lt;/code&gt; event.&lt;/p&gt;
    &lt;p&gt;Note: for anyone wondering, yes Claude did suggest the new &lt;code&gt;Blob.arrayBuffer()&lt;/code&gt; method, but it also generated the above for ‚Äúolder environments.‚Äù&lt;/p&gt;
    &lt;p&gt;I suppose some people would see this as progress: fewer dependencies, more robust code (even if it‚Äôs a bit more verbose), quicker turnaround time than the old ‚Äúsearch npm, find a package, read the docs, install it‚Äù approach.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have any excessive pride in this library, and I don‚Äôt particularly care if the download numbers go up or down. But I do think something is lost with the AI approach. When I wrote &lt;code&gt;blob-util&lt;/code&gt;, I took a teacher‚Äôs mentality: the README has a cutesy and whimsical tutorial featuring Kirby, in all his blobby glory. (I had a thing for putting Nintendo characters in all my stuff at the time.)&lt;/p&gt;
    &lt;p&gt;The goal wasn‚Äôt just to give you a utility to solve your problem (although it does that) ‚Äì the goal was also to teach people how to use JavaScript effectively, so that you‚Äôd have an understanding of how to solve other problems in the future.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know which direction we‚Äôre going in with AI (well, ~80% of us; to the remaining holdouts, I salute you and wish you godspeed!), but I do think it‚Äôs a future where we prize instant answers over teaching and understanding. There‚Äôs less reason to use something like &lt;code&gt;blob-util&lt;/code&gt;, which means there‚Äôs less reason to write it in the first place, and therefore less reason to educate people about the problem space.&lt;/p&gt;
    &lt;p&gt;Even now there‚Äôs a movement toward putting documentation in an &lt;code&gt;llms.txt&lt;/code&gt; file, so you can just point an agent at it and save your brain cells the effort of deciphering English prose. (Is this even documentation anymore? What is documentation?)&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I still believe in open source, and I‚Äôm still doing it (in fits and starts). But one thing has become clear to me: the era of small, low-value libraries like &lt;code&gt;blob-util&lt;/code&gt; is over. They were already on their way out thanks to Node.js and the browser taking on more and more of their functionality (see &lt;code&gt;node:glob&lt;/code&gt;, &lt;code&gt;structuredClone&lt;/code&gt;, etc.), but LLMs are the final nail in the coffin.&lt;/p&gt;
    &lt;p&gt;This does mean that there‚Äôs less opportunity to use these libraries as a springboard for user education (Underscore.js also had this philosophy), but maybe that‚Äôs okay. If there‚Äôs no need to find a library to, say, group the items in an array, then maybe learning about the mechanics of such libraries is unnecessary. Many software developers will argue that asking a candidate to reverse a binary tree is pointless, since it never comes up in the day-to-day job, so maybe the same can be said for utility libraries.&lt;/p&gt;
    &lt;p&gt;I‚Äôm still trying to figure out what kinds of open source are worth writing in this new era (hint: ones that an LLM can‚Äôt just spit out on command), and where education is the most lacking. My current thinking is that the most value is in bigger projects, more inventive projects, or in more niche topics not covered in an LLM‚Äôs training data. For example, I look back on my work on &lt;code&gt;fuite&lt;/code&gt; and various memory-leak-hunting blog posts, and I‚Äôm pretty satisfied that an LLM couldn‚Äôt reproduce this, because it requires novel research and creative techniques. (Although who knows: maybe someday an agent will be able to just bang its head against Chrome heap snapshots until it finds the leak. I‚Äôll believe it when I see it.)&lt;/p&gt;
    &lt;p&gt;There‚Äôs been a lot of hand-wringing lately about where open source fits in in a world of LLMs, but I still see people pushing the boundaries. For example, a lot of naysayers think there‚Äôs no point in writing a new JavaScript framework, since LLMs are so heavily trained on React, but then there goes the indefatigable Dominic Gannaway writing Ripple.js, yet another JavaScript framework (and with some new ideas, to boot!). This is the kind of thing I like to see: humans laughing in the face of the machine, going on with their human thing.&lt;/p&gt;
    &lt;p&gt;So if there‚Äôs a conclusion to this meandering blog post (excuse my squishy human brain; I didn‚Äôt use an LLM to write this), it‚Äôs just that: yes, LLMs have made some kinds of open source obsolete, but there‚Äôs still plenty of open source left to write. I‚Äôm excited to see what kinds of novel and unexpected things you all come up with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45947639</guid><pubDate>Sun, 16 Nov 2025 19:21:13 +0000</pubDate></item><item><title>The Pragmatic Programmer: 20th Anniversary Edition (2023)</title><link>https://www.ahalbert.com/technology/2023/12/19/the_pragmatic_programmer.html</link><description>&lt;doc fingerprint="f74bd85969a12f9c"&gt;
  &lt;main&gt;
    &lt;p&gt;The Pragmatic Programmer: From Journeyman to Master by Dave Thomas and Andrew Hunt was given to me as a gift after an internship. The book gave me invaluable advice as I started out in my career as a professional software engineer. Re-reading it a decade later, I thought the general advice still held up well, but it made references to technologies such as CORBA that are no longer used and felt dated as a result. The authors agreed and wrote a 20th anniversary edition that was updated for modern developers. A third of the book is brand-new material, covering subjects such as security and concurrency. The rest of the book has been extensively rewritten based on the authors√¢ experience putting these principles into practice. We discussed the 20th anniversary edition in my book club at work.√Ç&lt;/p&gt;
    &lt;p&gt;The book is meant for those just starting out in the world of professional software engineering. Many of the tips, such as Tip 28: Always Use Version Control will seem obvious to experienced hands. However, it can also be a guide for senior developers mentoring junior developers, putting actionable advice into words. The book is also valuable to those who lack a formal CS education; it explains things like big-O notation and where to learn more about these subjects. I think that any software engineer will get one or two things out of this book, though it√¢s most valuable for beginners.&lt;/p&gt;
    &lt;p&gt;One of the things I appreciate about the book is that they talk about applying the principles not only to software engineering but to writing the book as well. The book was originally written in troff and later converted to LaTeX. For example, to illustrate Tip 29: Write Code That Writes Code they wrote a program to convert troff markup to LaTeX. In the 20th anniversary edition, they talk about their efforts to use parallelism to speed up the book build process and how it led to surprising bugs.&lt;/p&gt;
    &lt;p&gt;Perhaps the best thing about the book is that the authors summarize their points into short tips highlighted throughout the book. The authors helpfully attach these tips to a card attached to the physical book. This makes it easy to remember the principles espoused in the book and to refer to them later. I think this is a feature that more books should include, especially managerial or technical books.&lt;/p&gt;
    &lt;p&gt;The first chapter is less about coding and more about the general principles a pragmatic programmer follows. Most of all, it√¢s about taking responsibility for your work. The first tip of the chapter is Tip 3: You Have Agency: if you don√¢t like something, you can be a catalyst for change. Or you can change organizations if change isn√¢t happening. The most important tip of the chapter to me is Tip 4: Provide Options, Don√¢t Make Lame Excuses. In this section, they discuss taking responsibility for the commitments you make and having a contingency plan for things outside your control. If you don√¢t meet the commitment, provide solutions to fix the problems. Don√¢t tell your boss, √¢The cat ate my source code.√¢&lt;/p&gt;
    &lt;p&gt;Software rots over time without efforts to fix it. The authors talk about broken windows policing, the theory that minor problems such as a single broken window give people the psychological safety to commit larger crimes. Regardless of whether broken windows policing is actually true, the metaphor applies to software. This leads to Tip 5: Don√¢t Live with Broken Windows: If you see a broken window in your software, make an effort to fix it, even if it√¢s only a minor effort to board it up. This may seem impractical if your project already has a lot of broken windows, but this tip helps you avoid creating such an environment in the first place. In my experience, it works: when we set up a new project at work, we made a commitment to use git commit hooks to enforce coding standards. This made each of us more reluctant to compromise on software to begin with, and all of the code was a good example to copy from.&lt;/p&gt;
    &lt;p&gt;A pragmatic programmer is always learning, and learns things outside their specialty; they are a jack of all trades. Even if they are a specialist in their current role, they invest regularly in a broad knowledge portfolio. In addition to software skills, people skills are important as well. The section √¢Communicate!√¢ shows how to effectively communicate your ideas, such as how to present, what to say, and how pick the right time. In the words of Tip 11: English is Just Another Programming Language. If you don√¢t have an answer to an email immediately, respond with an acknowledgment and that you√¢ll get back to them later - nobody wants to be talking to a void. Don√¢t be afraid to reach out for help if you need it; that√¢s what your colleagues are there for, after all. And don√¢t neglect documentation! Make it an integral part of the development process, not an afterthought.&lt;/p&gt;
    &lt;p&gt;Finally, the principles in this book are not iron-clad: you must consider the tradeoffs between different values and make the right decision for your project. Your software does not need to be perfect. When working on software, involve your users in deciding what quality issues are acceptable in return for getting it out faster. After all, if you wait a year to ship the perfect version, their requirements will change anyways. As Tip 8 says: Make Quality a Requirements Issue.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Why is decoupling good? Because by isolating concerns we make each easier to change. ETC.&lt;/p&gt;&lt;lb/&gt;Why is the single responsibility principle useful? Because a change in requirements is mirrored by a change in just one module. ETC.&lt;lb/&gt;Why is naming important? Because good names make code easier to read, and you have to read it to change it. ETC!&lt;/quote&gt;
    &lt;p&gt;However, the authors also stress that ETC is a value, not a rule. For example, ETC may not be appropriate for writing code that has high performance requirements; making the code complex to achieve the performance requirements is an acceptable tradeoff.√Ç&lt;/p&gt;
    &lt;p&gt;They then turn to another important acronym for implementing ETC in Tip 15: DRY√¢Don√¢t Repeat Yourself. DRY makes things easier to change by having one place to change anything. Worse, if you forget to make a change, you√¢ll have contradictory information in your program that could crash it or silently corrupt data.&lt;/p&gt;
    &lt;p&gt;A closely related principle to DRY is Orthogonality. Two components of a software system are orthogonal if changes in one do not effect the other. Systems should be designed as a set of cooperating independent modules, each of which has a single, well-defined purpose. Modules communicate between themselves using well defined interfaces and don√¢t rely on shared global data or the implementation details of another module. Unless you change a component√¢s external interfaces, it should not cause changes in the rest of the system. Orthogonal systems are easier to test, because more testing can be done at the module level in unit tests rather than end-to-end integration tests that test the whole system.&lt;/p&gt;
    &lt;p&gt;Often, when starting a software project, there are a lot of unknowns. The user has an idea of what they want, but there√¢s some ambiguity in the requirements. You don√¢t know if the library and frameworks you pick will work nicely together. The solution here is Tip 20: Use Tracer Bullets to Find the Target. In a machine gun, tracer bullets are bullets that glow in the air, enabling the user to see if they√¢re hitting the target at night. Tracer Bullet Development provides that kind of immediate feedback. Look for a single feature that can be built quickly using the architectural approach you√¢ve chosen, and put that in front of the users. You may miss; users may say that√¢s not quite what they wanted. But that√¢s the point of tracer code: it allows you to adjust your aim with a skeleton project that√¢s easier to change than a final application. Users will be delighted to see something working early, and you√¢ll have an integration platform to build the rest of the application on.&lt;/p&gt;
    &lt;p&gt;Tracer code is different from prototypes. To the authors, prototypes are disposable code used to learn about a problem domain, never meant to be used in production. Prototypes don√¢t even have to be code. A UI can be mocked up in an interface builder, or an architecture mapped out with post-it notes. In terms of Tip 21: Prototype to Learn. In contrast, tracer bullet code is meant to be part of the final application.&lt;/p&gt;
    &lt;p&gt;The final tip of this chapter I bring up is Tip 18: There Are No Final Decisions. Decisions should be reversible; if you rely on MySQL today, you may find yourself needing to switch to Postgres six months from now. If you√¢ve properly abstracted the database logic, making this change should be easy. Marketing may decide that your web app should be a mobile app in the future; if your architecture is built well, this extra demand should not be a burden. This is one tip I disagree with: I think it can easily be taken too far. If you provide too much reversibility, you√¢ll end up with over-abstracted code with configuration options that are never used. I think it√¢s more reasonable to think about what decisions can reasonably change and make them flexible; if you spend all your time trying to cover for every possibility, you√¢ll never get around to actually coding the required functionality.&lt;/p&gt;
    &lt;p&gt;This chapter focuses on how to make the most out of your tools, what tools to invest in, and how to approach debugging. The first bit of advice: Tip 25: Keep Knowledge in Plain Text. By plain text, they mean keep knowledge such as configuration or data in a simultaneously human-readable and computer readable format. Plain text insures you against obsolesce; you can always write something to parse it later, while reverse-engineering a binary format is significantly harder. In addition, almost any other tool in existence can process plain text in some way, so you√¢ll have an extensive suite of other tools to use. As an extension of the power of plain text, they also suggest you master a command shell such as &lt;code&gt;bash&lt;/code&gt;. Shells provide a family of tools that are composable with each other, and can be combined as much as your imagination allows. A GUI in contrast, limits you to the actions the programmers of the GUI thought of in advance. Finally, you should learn a text processing language such as &lt;code&gt;awk&lt;/code&gt; or &lt;code&gt;perl&lt;/code&gt; to get the most out of text - the authors used perl (first edition) and ruby (20th anniversary edition) to automatically highlight the source code in the book, for example.&lt;/p&gt;
    &lt;p&gt;The next topic the authors turn to is debugging. Debugging is the main task a software engineer does throughout their day, so it√¢s essential you get good at it. Defects show up in a variety of ways, from misunderstood requirements to coding errors. Some cultures try to find someone to blame for a defect; the authors think you should avoid that with Tip 29: Fix the Problem, Not the Blame.&lt;/p&gt;
    &lt;p&gt;They give the following tips on debugging your code:&lt;/p&gt;
    &lt;p&gt;Once you√¢ve solved the bug, however there√¢s still one more step: you should write a test to catch that bug in the future.&lt;/p&gt;
    &lt;p&gt;Tip 36: You Can√¢t Write Perfect Software starts off the chapter. While we√¢d like to write perfect software, there will always be bugs, poor design decisions, and missing documentation. The theme of this chapter is how to design this fact in mind.&lt;/p&gt;
    &lt;p&gt;The first idea they propose is Design By Contract. Similar to legal contracts, it explains a function or module√¢s rights and responsibilities. A contract has three parts: It has Preconditions: things that must be true when it is called, such as what qualifies as valid inputs. Postconditions are what will be true when it is done, such as a sort routine returning a sorted array. Finally, Invariants are things that are always true from the caller√¢s perspective - they may change while the routine is running, but will hold at the beginning and the end of the call. For example, in a sort routine, the invariant is that the list to be sorted will contain the same number of items when it started as when it finished. If the contract is violated, the contract will specify what to do, such as crash or throw an exception.&lt;/p&gt;
    &lt;p&gt;Some languages, such as Clojure have built-in semantics for design by contract, with explicit pre- and post- conditions. However, if your language doesn√¢t support contracts, you can implement them with Tip 39: Use Assertions to Prevent the Impossible. You can assert that the conditions of your contract are true, and handle the cases where the contract is violated. If you don√¢t know what to do when a contract is violated, the authors recommend Tip 38: Crash Early. It√¢s better that you crash rather than write incorrect data to the database. After all, dead programs tell no lies. Of course, crashing immediately may not be appropriate - if you have resources open make sure to close them before exiting.&lt;/p&gt;
    &lt;p&gt;The final paranoid tip is Tip 43: Avoid Fortune-Telling. Pragmatic programmers only make decisions that they can get immediate feedback on. The more predictions you make about the future, the more likely you√¢ll get some of the predictions wrong and make the wrong decision based on them.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You might find yourself slipping into fortune telling when you have to:&lt;/p&gt;
      &lt;item&gt;Estimate completion dates months in the future&lt;/item&gt;
      &lt;item&gt;Plan a design for future maintenance or extendability&lt;/item&gt;
      &lt;item&gt;Guess user√¢s future needs&lt;/item&gt;
      &lt;item&gt;Guess future tech availability&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;In a previous chapter, the authors wrote about making decisions reversible and easier to change. This chapter tells you how to implement it in your code. The key here is to make your code flexible rather than rigid - good code bends to circumstances rather than breaks. Part of this is decoupling code. Code is consider coupled when they share something in common. This may be something as simple as a shared global variable, or something more complex like an inheritance chain.&lt;/p&gt;
    &lt;p&gt;The authors argue against what they term Train Wrecks - long chains of method calls, such as this example they give:&lt;/p&gt;
    &lt;code&gt;public void applyDiscount(customer, order_id, discount) { 
	totals = customer
			  .orders 
			  .find(order_id) 
			  .getTotals();

	totals.grandTotal = totals.grandTotal - discount;
	totals.discount = discount; 
}

&lt;/code&gt;
    &lt;p&gt;This code is traversing many different levels of abstraction - you have to know that a customer object exposes orders, that orders have a &lt;code&gt;find&lt;/code&gt; method, and that the order &lt;code&gt;find&lt;/code&gt; returns has a &lt;code&gt;getTotal&lt;/code&gt; method. If any of these levels of abstraction are changed, your code might break. And requirements may change; What if the business decides to implement a maximum discount amount of 40%? Certainly, this could be applied in the &lt;code&gt;applyDiscount&lt;/code&gt; routine, but anything could modify the &lt;code&gt;grandTotal&lt;/code&gt; and &lt;code&gt;discount&lt;/code&gt; fields - this rule could be violated if other modules modifying the totals object don√¢t get the memo.&lt;/p&gt;
    &lt;p&gt;The authors suggest refactoring the code so that there is no orders object, just a &lt;code&gt;find&lt;/code&gt; method and an applyDiscount method for the order object that implements the 40% rule:&lt;/p&gt;
    &lt;code&gt;public void applyDiscount(customer, order_id, discount) { 
	customer
	.findOrder(order_id)
	.applyDiscount(discount); 
}
&lt;/code&gt;
    &lt;p&gt;The authors suggest having only one . when you access something if that something is likely to change, such as anything in your application, or a fast moving external API. This includes using intermediate variables between accesses, such as this code:&lt;/p&gt;
    &lt;code&gt;# This is cheating!
orders = customer.orders
order = orders.find(order_id)
totals = order.getTotals
&lt;/code&gt;
    &lt;p&gt;However, the rule does not apply to things that are unlikely to change, such as core language APIs. So this code is ok:&lt;/p&gt;
    &lt;code&gt;people
.sort_by {|person| person.age } 
.first(10)
.map {| person | person.name }
&lt;/code&gt;
    &lt;p&gt;Another source of coupling is globally accessible data. Global data makes it hard to reason about the state of a program, since any other module might be able to change it. Global data includes design patterns such as singletons, and external resources such as databases. Given how extensive global resources are, how can one avoid them? If global data is unavoidable, the key is to manage them through a well-defined API that you control, rather than allowing anything to read and write global data. In the words of Tip 48: If It√¢s Important Enough to Be Global, Wrap It in an API.&lt;/p&gt;
    &lt;p&gt;Poor use of inheritance is a third source of coupling. Inheritance is used for two reasons: code reuse and type modeling. Inheritance doesn√¢t work for code reuse; Not only is the code of a child class coupled to any ancestor of the class, so is any code that uses the class. Things may unexpectedly break when an ancestor changes an API, even if you are using a subclass.&lt;/p&gt;
    &lt;p&gt;Nor does inheritance work for modeling types. Class hierarchies quickly become tangled, wall covering monstrosities. Another problem is multiple inheritance. A &lt;code&gt;Car&lt;/code&gt; may be a type of &lt;code&gt;Vehicle&lt;/code&gt;, but it may be an &lt;code&gt;Asset&lt;/code&gt; or &lt;code&gt;InsuredItem&lt;/code&gt;. Multiple inheritance is required to model this, and many OO languages don√¢t support multiple inheritance. Instead of paying the inheritance tax, the authors suggest using:&lt;/p&gt;
    &lt;p&gt;Interfaces or Protocols are classes that contain no code but instead contains behaviors. A class that implements an interface promises to define the behaviors. For example, a &lt;code&gt;Car&lt;/code&gt; might implement &lt;code&gt;Drivable&lt;/code&gt; which has methods such as &lt;code&gt;accelerate&lt;/code&gt; and &lt;code&gt;brake&lt;/code&gt;. Interfaces can be used as types, and any class that implements the interface will be compatible with that type. This is a much easier way to provide polymorphism than inheritance.&lt;/p&gt;
    &lt;p&gt;Another alternative to inheritance is delegation. If you want to include behavior from class &lt;code&gt;Foo&lt;/code&gt; add a member of type &lt;code&gt;Foo&lt;/code&gt; to your class rather than inherit from &lt;code&gt;Foo&lt;/code&gt;. You can then use Foo√¢s API wrapped in code you control. Delegation is a has-a relationship rather than a is-a relationship.&lt;/p&gt;
    &lt;p&gt;The problem with interfaces and delegation is that they require writing lots of boilerplate code. For example, it√¢s likely that most of your classes that implement &lt;code&gt;Drivable&lt;/code&gt; will have the same logic for &lt;code&gt;brake&lt;/code&gt;, but each class will have to write it√¢s own implementation of &lt;code&gt;brake&lt;/code&gt;. This leads to repeated code across your codebase, violating the DRY principle. To resolve this, the authors turn to Mixins - sets of functions that can be √¢mixed into√¢ a class. This allows you to add common functionality without using inheritance. I wonder how mixins are implemented in a language like Java, which doesn√¢t have an obvious version of that feature. It√¢s also not clear to me how mixins are different from inheritance; aren√¢t they just a form of multiple inheritance?&lt;/p&gt;
    &lt;p&gt;Tip 55: Parameterize Your App Using External Configuration: Code may have values that change while the application is running, such as credentials for for third-party services. Rather than directly including the values in your code, you should externalize them and put them in a configuration bucket. Keeping credentials in source code is a security risk - hackers scan public git repositories for common security credentials, such as AWS keys. It√¢s common to store them in a flat file or database tables, and read them when the application initializes. However, in our world of highly-available applications that√¢s not as appropriate. Instead the authors propose configuration-as-a-service, where configuration is stored behind a service API. This allows multiple applications to share configuration information, use access control to control who can see and edit configuration, and provide a UI to easily edit config information. Using the configuration service, applications can subscribe to a configuration item and get notifications when they change. This allows applications to update config data on their side without restarting.&lt;/p&gt;
    &lt;p&gt;This chapter deals with Parallelism, where two pieces of code run at the same time, and Concurrency, where things act as if they run at the same time. In the real world, things are asynchronous - the user is supplying input, network resources are called, and the screen is being redrawn all at the same time. Applications that run everything serially feel sluggish.&lt;/p&gt;
    &lt;p&gt;In Tip 56: Analyze Workflow to Improve Concurrency the authors advocate that you break temporal coupling where possible. Temporal Coupling is when your code depends on event A happening before event B. You should look at your workflow to see what can be executed concurrently. Look for activities that take a lot of time that would allow for something else to be done in the meantime. If your application makes multiple independent API calls to a remote service, execute them on separate threads rather than serially, then gather up the results of each call. If your workflow allows a way to split the work into multiple independent units, take advantage of those multiple cores and execute them in parallel.&lt;/p&gt;
    &lt;p&gt;Of course, parallelism has its pitfalls as well. For example, imagine reading an integer, incrementing it, and writing it back. If two processes read that integer at the same time, they will each increment the value to n+1, when you want it to be n+2. The update needs to be atomic; each process needs to do this sequentially without the other process interfering. This can be done through synchronized methods, semaphores, or other forms of resource locking. However, they have their own dangers as well, such as deadlocking, where two processes each get a lock on one of two needed resources, but not the other. Each waits forever for the other to release its lock. The authors think you should avoid shared state rather than try to handle yourself wherever possible; Tip 57: Shared State Is Incorrect State.&lt;/p&gt;
    &lt;p&gt;The authors ran into this issue when writing the 20th anniversary edition: they updated the build process for the book to utilize parallelism. However, the build would randomly fail. The authors tracked this down to changing the directory temporarily. In the original, a subtask would change directory, then go back to the original directory. However, this no longer worked when new threads started, expecting to be in the root directory. Depending on the timing, this could break the build. This prompted them to write Tip 58: Random Failures Are Often Concurrency Issues.&lt;/p&gt;
    &lt;p&gt;This chapter is more of a grab-bag. It covers subjects such as psychology, big-O notation, refactoring, security, and testing.&lt;/p&gt;
    &lt;p&gt;In Tip 61: Listen to Your Inner Lizard the authors talk about listening to your instincts (your lizard-brain). If you find yourself having a hard time writing code, your brain is trying to tell you something. Perhaps the structure or design is wrong, or you don√¢t fully understand the requirements. If you find yourself in this situation, take a step back and think about what you are doing. Maybe go for a walk, or sleep on it. You might find that the solution is staring you in the face when you come back.&lt;/p&gt;
    &lt;p&gt;Perhaps you need to refactor the code instead of writing more. Refactoring is a continuous process, espoused in Tip 65: Refactor Early, Refactor Often. If anything strikes you as wrong in your code, such as DRY violations, outdated knowledge or non-orthogonal design, don√¢t hesitate to fix it. When you are refactoring, make sure you have a good suite of unit tests beforehand to test if your changes break anything. Run the tests frequently to check if you√¢ve broken anything.&lt;/p&gt;
    &lt;p&gt;Speaking of tests, the authors start with a bold assertion: Tip 67: Testing Is Not About Finding Bugs. Instead, tests function as the First User of Your Code - a source of immediate feedback, and immediately forces you to think about what counts as a correct solution. In addition, tightly coupled code tends to be hard to test, so it helps you make good design decisions. The authors emphatically do not think you should adopt full-on Test Driven Development - it√¢s too easy to become a slave to writing tests. They note an example of a TDD advocate starting a sudoku solver using TDD and spent so much time writing the tests they failed to write the solver itself!&lt;/p&gt;
    &lt;p&gt;In a sidebar, Dave Thomas explains that he stopped writing tests for a few months, and said √¢not a lot√¢ happened. The quality didn√¢t drop, nor did he introduce bugs into the code. His code was still testable, it just wasn√¢t tested.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Andy says I shouldn√¢t include this sidebar. He worries it will tempt inexperienced developers not to test. Here√¢s my compromise: Should you write tests? Yes. But after you√¢ve been doing it for 30 years, feel free to experiment a little to see where the benefit lies for you.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This chapter focuses on how to start your project on the right foot. The first subject the authors tackle is requirements gathering: The Requirements Pit. While we talk about gathering requirements as if they are on the ground, waiting to be picked up, requirements are non-obvious because of Tip 75: No One Knows Exactly What They Want. They think of requirements gathering as a kind of therapy, where you take an initial requirement and ask questions about the details to nail down exactly what they need. The authors show an example of a simple requirement: √¢Shipping should be free on all orders costing $50 or more√¢. Does that include the shipping cost itself? Tax? If you√¢re selling ebooks as well, should they be included? The job of the programmer is Tip 76: Programmers Help People Understand What They Want. You should find any edge cases the client may not have considered and make sure they√¢re documented. This doesn√¢t mean creating long specifications the client won√¢t read. Instead, the authors think requirements should be able to fit on an index card. This helps prevent feature creep; if the client understands how adding one more index card will impact the schedule, they√¢ll consider the tradeoffs and prioritize the requirements they need the most.&lt;/p&gt;
    &lt;p&gt;You are given constraints in your requirements as well. Your job as a software engineer is to evaluate if those constraints are things you actually have to live with or if you can relax them. In the words of Tip 81: Don√¢t Think Outside the Box√¢Find the Box, the constraints are the edges of the box. What you initially thought of as a constraint may actually be an assumption you held.&lt;/p&gt;
    &lt;p&gt;Another tip the authors advocate for is Tip 78: Work with a User to Think Like a User. If you√¢re building an inventory system, work in the warehouse for a few days to get an idea of their processes and how your system will be used. If you don√¢t understand how it will be used, you could create something that meets all of the requirements but is totally useless. They cite an example of a digital sound mixing board that could do anything to sound that was possible, yet nobody wanted to use it. Rather than take advantage of recording engineers√¢ experience with tactile sliders and knobs, they built an interface that was unfamiliar to them. Each feature was buried behind menus and given unintuitive names. √Ç It did what was required, but didn√¢t do it how it was required.√Ç&lt;/p&gt;
    &lt;p&gt;The authors also consider in this chapter what it means to be Agile. Many teams and companies are eager for an off-the-shelf solution: call it Agile-in-a-Box. But no process can make you Agile; √¢Use this process and you√¢ll be agile√¢ ignores a key part of the Agile manifesto: Individuals and interactions over processes and tools. To the authors Agile can be boiled down to the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;Work out where you are.&lt;/item&gt;
      &lt;item&gt;Make the smallest meaningful step towards where you want to be.&lt;/item&gt;
      &lt;item&gt;Evaluate where you end up, and fix anything you broke.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;Do this for every level of what you do, from process to code, and you√¢ll have adopted the Agile spirit.&lt;/p&gt;
    &lt;p&gt;Can the lessons of The Pragmatic Programmer be applied to teams too? The authors say yes. This chapter focuses on how to apply the lessons of the previous chapters to the team level. Many of the lessons are the same as those mentioned previously, so I won√¢t go into them again.&lt;/p&gt;
    &lt;p&gt;The authors advise Tip 87: Do What Works, Not What√¢s Fashionable. Just because Google or Facebook adopts process $ x $ doesn√¢t mean it√¢s right for your team. How do you know if something works? Try it. Pilot an idea with a small team, and see what works about it and what doesn√¢t. The goal isn√¢t to √¢do Scrum√¢ or √¢be Agile√¢, but to deliver working software continuously. When you adopt a new idea, you should do it with improving continuous deployment of software in mind. If you√¢re measuring your deployments in months, try to get it down to weeks instead. Once you get it down to weeks, try to deliver in one-week iterations.&lt;/p&gt;
    &lt;p&gt;Related to continuously delivering software is Tip 96: Delight Users, Don√¢t Just Deliver Code. Delivering working software in a timely matter is not enough to delight your users; that is merely meeting expectations. The authors suggest you ask your users a question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How will you know that we√¢ve all been successful a month (or a year, or whatever) after this project is done?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The answer may not be related to the requirements, and may surprise you. For example, a recommendations engine might be valued on driving customer retention. But once you know what the secret to success is, you should aim not just to hit the goal but to exceed it.&lt;/p&gt;
    &lt;p&gt;Finally, take pride in your work. The final tip of the book is Tip 97: Sign Your Work.&lt;/p&gt;
    &lt;p&gt;I was only able to cover a portion of this remarkable book in this review. I highly recommend this book to any software engineer, especially to those just starting out in the field. It makes a great graduation gift to someone just finishing their CS degree.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45948254</guid><pubDate>Sun, 16 Nov 2025 20:46:30 +0000</pubDate></item><item><title>Why Castrol Honda Superbike crashes on (most) modern systems</title><link>https://seri.tools/blog/castrol-honda-superbike/</link><description>&lt;doc fingerprint="b601d2142fe5fb84"&gt;
  &lt;main&gt;
    &lt;p&gt;Posted: 2025-11-16&lt;/p&gt;
    &lt;head rend="h1"&gt;Why Castrol Honda Superbike crashes on (most) modern systems&lt;/head&gt;
    &lt;p&gt;A friend cleaned up and gave me a copy of a game I've not heard about before: Castrol Honda Superbike World Champions, a motorbike racing game for PC, released 1998 by Interactive Entertainment Ltd. and Midas Interactive Entertainment.&lt;/p&gt;
    &lt;p&gt;Given the age of the game (and looking at the system requirements) it's clear that the game comes from the tricky era of early 3D-accelerated PC gaming. For context, my copy of the game helpfully asks to install DirectX 5.&lt;/p&gt;
    &lt;p&gt;Before Windows was known for cramming AI and account requirements into every single corner of the system, no matter how unnecessary, it was known for its excellent backwards compatibility with older software. Generally, unless there are genuine bugs (and sometimes even despite them), Windows tries its hardest to run old applications correctly.&lt;/p&gt;
    &lt;p&gt;Pushing my luck and trying to run it on my Windows 7 machine, however, resulted in either a getting stuck on a black screen, or a crash, seemingly at random:&lt;/p&gt;
    &lt;p&gt;Let's go back in time and see how far we need to go to get it running: Installing and running it on my Windows 98 and Windows XP machines was as uneventful, and the game works just fine1, including with 3D acceleration. Glorious 1024x768x16:&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging the issue #&lt;/head&gt;
    &lt;p&gt;Debugging is more fun than playing, so let's get started! :^)&lt;/p&gt;
    &lt;p&gt;I pulled over the installation directory to my main machine and ran Detect It Easy to see what we can learn about the executable:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Linker: Microsoft Linker(5.10)&lt;/code&gt; and &lt;code&gt;Compiler: Microsoft Visual C/C++(...)[libcmtd]&lt;/code&gt; are the interesting bits here. Notice how it's &lt;code&gt;libcmtd&lt;/code&gt;, not &lt;code&gt;libcmt&lt;/code&gt;? The binary is linked against the static debug version of VC5's runtime. The debug runtime has heaps of extra checks and logging, which might help later.&lt;/p&gt;
    &lt;p&gt;Let's attach a debugger and see what's going on. Given that the game crashes very early on (before the credits intro screen), I hoped to see something right away. The cases where the game got stuck in a loop seemed to actually get stuck in some Windows API call stack.&lt;/p&gt;
    &lt;p&gt;Anyway, the cases where it crashed gave a clearer starting point:&lt;/p&gt;
    &lt;p&gt;The game seems to be stuck after a call to DirectInput's &lt;code&gt;DirectInputCreateEx&lt;/code&gt; function. At this point I started to do some static analysis of the functions leading to this call. While doing that I noticed that the game seems to have quite extensive logging, anything from game initialization to memory allocations.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;If you're interested in all the logs, here are the config settings to enable them all:&lt;/p&gt;&lt;item&gt;In&lt;/item&gt;&lt;code&gt;Config.dat&lt;/code&gt;, switch&lt;code&gt;ErrorLog&lt;/code&gt;,&lt;code&gt;FileLog&lt;/code&gt;,&lt;code&gt;MallocLog&lt;/code&gt;from&lt;code&gt;off&lt;/code&gt;to&lt;code&gt;on&lt;/code&gt;.&lt;item&gt;These are the "normal" log files the game produces.&lt;/item&gt;&lt;code&gt;ErrorLog&lt;/code&gt;produces&lt;code&gt;error.log&lt;/code&gt;, which is the general log file.&lt;code&gt;FileLog&lt;/code&gt;produces&lt;code&gt;files.log&lt;/code&gt;, tracking all opened files and their access modes.&lt;code&gt;MallocLog&lt;/code&gt;produces&lt;code&gt;malloc.log&lt;/code&gt;, tracking all memory allocations and frees. The devs even kept descriptions for every allocation site!&lt;item&gt;Set an environment variable named&lt;/item&gt;&lt;code&gt;errorfile&lt;/code&gt;to any file name (not path). The game will write logs to that file in the game directory.&lt;item&gt;You might also need to create an empty&lt;/item&gt;&lt;code&gt;*.c&lt;/code&gt;file in the game directory.&lt;item&gt;Just gives a bit of extra logging.&lt;/item&gt;&lt;p&gt;Bonus: add a setting named&lt;/p&gt;&lt;code&gt;windowed=true&lt;/code&gt;to the config file to force windowed mode; only works correctly in 16-bit mode (garbled graphics in True Color).&lt;/quote&gt;
    &lt;p&gt;After enabling all the logging, I ran the game a few more times, and noticed that the last log messages in &lt;code&gt;error.log&lt;/code&gt; before the crash were these:&lt;/p&gt;
    &lt;code&gt;0&amp;gt; Instance : Mouse
0&amp;gt; Product : Mouse
1&amp;gt; Instance : Keyboard
1&amp;gt; Product : Keyboard
2&amp;gt; Instance : Gaming Mouse G502
2&amp;gt; Product : Gaming Mouse G502
3&amp;gt; Instance : Gaming Mouse G502
3&amp;gt; Product : Gaming Mouse G502
4&amp;gt; Instance : Gaming Mouse G502
4&amp;gt; Product : Gaming Mouse G502
5&amp;gt; Instance : Gaming Mouse G502
5&amp;gt; Product : Gaming Mouse G502
6&amp;gt; Instance : USB Keyboard
6&amp;gt; Product : USB Keyboard
7&amp;gt; Instance : USB Keyboard
7&amp;gt; Product : USB Keyboard
8&amp;gt; Instance : LED Controller
8&amp;gt; Product : LED Controller
&lt;/code&gt;
    &lt;p&gt;Great, the game is enumerating input devices‚Äî uh, why is there an "LED Controller" device? The motherboard in my Windows 7 machine has a built-in LED controller, so that checks out. Maybe the detection isn't working properly, and the game is trying to use it as an input device?&lt;/p&gt;
    &lt;p&gt;After disabling the LED controller in Device Manager, the game started up just fine, consistently! So far, so good. Of course I wanted to know what was actually going wrong, though, so let's see where these messages are printed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Side quest: CD check #&lt;/head&gt;
    &lt;p&gt;The game seemed to close without any notice if I forgot to insert the game disc. A quick trace showed that the &lt;code&gt;GibbonPosture&lt;/code&gt; setting in &lt;code&gt;f1.cfg&lt;/code&gt; is used to point to the disc drive from which the game was installed. The only check seems to be that the path &lt;code&gt;redist\dsetup.dll&lt;/code&gt; exists on the disc. Copying the redist folder to the installation directory and changing the setting to &lt;code&gt;GibbonPosture=.\&lt;/code&gt; seems to work just fine. :)&lt;/p&gt;
    &lt;head rend="h2"&gt;The bug #&lt;/head&gt;
    &lt;p&gt;Finding the &lt;code&gt;Instance :&lt;/code&gt; and &lt;code&gt;Product :&lt;/code&gt; log messages in the binary was easy enough. They are referenced in only one function, which is a &lt;code&gt;DIEnumDevicesCallback&lt;/code&gt; callback function that is provided to &lt;code&gt;IDirectInput::EnumDevices&lt;/code&gt; (Microsoft has only kept the documentation for the DX8 version of EnumDevices left online, but it's close enough).&lt;/p&gt;
    &lt;p&gt;This is the pseudocode of the call and the callback, and the relevant data structure:&lt;/p&gt;
    &lt;code&gt;struct DinputDeviceData
{
  char instance_name[128];
  char product_name[128];
  DWORD dwDevType;
  GUID guid;
};

// ...

BOOL __stdcall dinput_enumdevices_callback(LPCDIDEVICEINSTANCEA lpDevice, LPVOID pvRef)
{
    int index = g_dinput_device_index;
    g_direct_input_devices[index].guid = lpDevice-&amp;gt;guidInstance;
    strcpy(g_direct_input_devices[index].instance_name, lpDevice-&amp;gt;tszInstanceName);
    strcpy(g_direct_input_devices[index].product_name, lpDevice-&amp;gt;tszProductName);
    g_direct_input_devices[index].dwDevType = lpDevice-&amp;gt;dwDevType;

    log_line("%d&amp;gt; Instance : %s\n", index, lpDevice-&amp;gt;tszInstanceName);
    log_line("%d&amp;gt; Product : %s\n", index, lpDevice-&amp;gt;tszProductName);

    if ( LOBYTE(g_direct_input_devices[index].dwDevType) == DIDEVTYPE_JOYSTICK )
    {
        int joystick_index = g_joystick_index;
        g_joystick_info[joystick_index].dinput_device_index = index;
        g_joystick_info[joystick_index].field_4 = 0;
        g_joystick_info[joystick_index].field_8 = 0;
        g_joystick_info[joystick_index].field_38 = 0;
        g_joystick_info[joystick_index].field_1 = 0;
        g_joystick_index = joystick_index + 1;
    }
    g_dinput_device_index = index + 1;

    return DIENUM_CONTINUE;
}

// ...

g_dinput_create_hresult = DirectInputCreateA(hInstance, 0x500u, &amp;amp;g_dinput_instance, 0);
g_dinput_device_index = 0;
g_joystick_index = 0;
g_dinput_instance-&amp;gt;lpVtbl-&amp;gt;EnumDevices(
    g_dinput_instance, 0, dinput_enumdevices_callback, 0, DIEDFL_ATTACHEDONLY);
&lt;/code&gt;
    &lt;p&gt;So, for each enumerated device, the game stores some general information about it in the global array &lt;code&gt;g_direct_input_devices&lt;/code&gt;. Then, if the device is a joystick (generally, a game controller), it also adds it to &lt;code&gt;g_joystick_info&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Can you guess the bug yet? :) If not, here's the declaration of the global arrays:&lt;/p&gt;
    &lt;code&gt;DinputDeviceData g_direct_input_devices[8];
// ...
JoystickInfo g_joystick_info[8];
&lt;/code&gt;
    &lt;p&gt;There's only space for eight DirectInput devices in the array! &lt;code&gt;8&amp;gt; Instance : LED Controller&lt;/code&gt; was the ninth one, overwriting lots of important other data in the process, including timer handles and the actual DirectInput instance pointer.&lt;/p&gt;
    &lt;p&gt;But it gets worse: The game uses DirectInput for game controllers only. Copying the device info out of &lt;code&gt;lpDevice&lt;/code&gt; is entirely pointless for other types of devices. Just moving the &lt;code&gt;DIDEVTYPE_JOYSTICK&lt;/code&gt; check up would have hidden the bug for basically all setups, since you'd have to have more than 8 game controllers connected for the game to write out of bounds.&lt;/p&gt;
    &lt;p&gt;Actually, there would've been an even simpler workaround: &lt;code&gt;EnumDevices&lt;/code&gt; allows passing a &lt;code&gt;DIDEVTYPE&lt;/code&gt; as a filter:&lt;/p&gt;
    &lt;code&gt;g_dinput_instance-&amp;gt;lpVtbl-&amp;gt;EnumDevices(
    g_dinput_instance, DIDEVTYPE_JOYSTICK, dinput_enumdevices_callback, 0, DIEDFL_ATTACHEDONLY);
                    // ^^^^^^^^^^^^^^^^^^
&lt;/code&gt;
    &lt;p&gt;This would make DirectInput call the callback for game controllers only. Without it, all devices, whether they are keyboards, mice, or actually any HID devices, are enumerated. (I've checked the DirectX 5 SDK docs, and even there it mentions the HID device support.) This includes the vendor-defined devices of my mouse and its emulated keyboard (for macros), and of course the motherboard's LED controller.&lt;/p&gt;
    &lt;p&gt;The moral of the story? Always check your bounds, kids! You'll never know if some weirdo comes along and plugs in a dozen game controllers to their PC. :^)&lt;/p&gt;
    &lt;head rend="h2"&gt;The fix #&lt;/head&gt;
    &lt;p&gt;Over on GitHub I've pushed a minimal patch as a classic DLL shim. With the provided &lt;code&gt;dinput.dll&lt;/code&gt; in the game directory, the game will load that instead of the system one. DirectInput has only one relevant exported function that we need to shim: &lt;code&gt;DirectInputCreateA&lt;/code&gt;. The rest of the API is implemented via COM interfaces, for which we can modify the respective vtables as needed.&lt;/p&gt;
    &lt;p&gt;I've implemented two fixes in the shim:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Inject the &lt;code&gt;DIDEVTYPE_JOYSTICK&lt;/code&gt;filter in the call to&lt;code&gt;EnumDevices&lt;/code&gt;to only return joysticks/game controllers.&lt;/item&gt;
      &lt;item&gt;Cancel enumeration once 8 joysticks have been found.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For fun, I've also tried to minimize the size of the shim DLL -- the final binary weighs in at 2 KiB.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;These are the reasonable settings I changed:&lt;/p&gt;&lt;item&gt;Compile with&lt;/item&gt;&lt;code&gt;opt-level = "z"&lt;/code&gt;to optimize for minimum size. (though the code is so low-level that it's effectively the same as&lt;code&gt;opt-level = 3&lt;/code&gt;)&lt;code&gt;#[no_std]&lt;/code&gt;to avoid linking the Rust standard library.&lt;code&gt;codegen-units = 1&lt;/code&gt;and&lt;code&gt;lto = true&lt;/code&gt;to enable whole-program optimization.&lt;code&gt;panic = "immediate-abort"&lt;/code&gt;to remove all unnecessary panic handling code; an unwrap will immediately abort the process.&lt;p&gt;And these are the cursed ones:&lt;/p&gt;&lt;code&gt;/NODEFAULTLIB&lt;/code&gt;to not link against any MSVC runtime library; added my own minimal&lt;code&gt;DllMain&lt;/code&gt;.&lt;code&gt;/FORCE:UNRESOLVED&lt;/code&gt;to ignore the missing symbols for&lt;code&gt;_aullrem&lt;/code&gt;,&lt;code&gt;_aulldiv&lt;/code&gt;, and&lt;code&gt;_fltused&lt;/code&gt;. We aren't using any of these, but the LLVM target still seems to insist on linking them in.&lt;code&gt;/FILEALIGN:512&lt;/code&gt;to force the linker to use the minimum supported PE section alignment in the file.&lt;code&gt;/MERGE:.rdata=.text&lt;/code&gt;merges the read-only data section into the code section.&lt;item&gt;Prevent zero-initialization of the system directory path by using&lt;/item&gt;&lt;code&gt;MaybeUninit&lt;/code&gt;.&lt;item&gt;Storing globals in&lt;/item&gt;&lt;code&gt;static mut&lt;/code&gt;just like the original game does. Since the fix is specific to this game I can do these "global" assumptions here :^)&lt;item&gt;Ensure all globals are zero-initialized so the&lt;/item&gt;&lt;code&gt;.data&lt;/code&gt;section is 0 bytes in the binary.&lt;code&gt;.unwrap_unchecked()&lt;/code&gt;to avoid any extra branches where they aren't needed.&lt;code&gt;/DEBUG:NONE&lt;/code&gt;to not generate debug information, and not store a&lt;code&gt;.pdb&lt;/code&gt;path in the binary.&lt;/quote&gt;
    &lt;p&gt;Furthermore, I switched to &lt;code&gt;rust-lld.exe&lt;/code&gt; as the linker, as it's fine with setting &lt;code&gt;/SUBSYSTEM:WINDOWS,4.0"&lt;/code&gt; and &lt;code&gt;/OSVERSION:4.0&lt;/code&gt; without complaining. :) Since there is no linked runtime code at all, the resulting binary should work on any 32-bit Windows version, even without Rust9x. I've tested it on Windows 7 and Windows 98 SE.&lt;/p&gt;
    &lt;p&gt;Feel free to grab the compiled DLL from the releases page. Except for True Color rendering, which seems to be broken, no matter the system, graphics card, or game version? Investigating that is left as a mystery for another day. ‚Ü©&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45948311</guid><pubDate>Sun, 16 Nov 2025 20:54:14 +0000</pubDate></item><item><title>Neuroscientists track the neural activity underlying an ‚Äúaha‚Äù</title><link>https://www.quantamagazine.org/how-your-brain-creates-aha-moments-and-why-they-stick-20251105/</link><description>&lt;doc fingerprint="fa73133599e7d742"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How Your Brain Creates ‚ÄòAha‚Äô Moments and Why They Stick&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Here are three words: pine, crab, sauce. There‚Äôs a fourth word that combines with each of the others to create another common word. What is it?&lt;/p&gt;
    &lt;p&gt;When the answer finally comes to you, it‚Äôll likely feel instantaneous. You might even say ‚ÄúAha!‚Äù This kind of sudden realization is known as insight, and a research team recently uncovered how the brain produces it, which suggests why insightful ideas tend to stick in our memory.&lt;/p&gt;
    &lt;p&gt;Maxi Becker, a cognitive neuroscientist at Duke University, first got interested in insight after reading the landmark 1962 book The Structure of Scientific Revolutions by the historian and philosopher of science Thomas Kuhn. ‚ÄúHe describes how some ideas are so powerful that they can completely shift the way an entire field thinks,‚Äù she said. ‚ÄúThat got me wondering: How does the brain come up with those kinds of ideas? How can a single thought change how we see the world?‚Äù&lt;/p&gt;
    &lt;p&gt;Such moments of insight are written across history. According to the Roman architect and engineer Vitruvius, in the third century BCE the Greek mathematician Archimedes suddenly exclaimed ‚ÄúEureka!‚Äù after he slid into a bathtub and saw the water level rise by an amount equal to his submerged volume (although this tale may be apocryphal). In the 17th century, according to lore, Sir Isaac Newton had a breakthrough in understanding gravity after an apple fell on his head. In the early 1900s, Einstein came to a sudden realization that ‚Äúif a man falls freely, he would not feel his weight,‚Äù which led him to his theory of relativity, as he later described in a lecture.&lt;/p&gt;
    &lt;p&gt;Insights are not limited to geniuses: We have these cognitive experiences all the time when solving riddles or dealing with social or intellectual problems. They are distinct from analytical problem-solving, such as the process of doing formulaic algebra, in which you arrive at a solution slowly and gradually as if you‚Äôre getting warmer. Instead, insights often follow periods of confusion. You never feel as if you‚Äôre getting warmer; rather, you go from cold to hot, seemingly in an instant. Or, as the neuropsychologist Donald Hebb, known for his work building neurobiological models of learning, wrote in the 1940s, sometimes ‚Äúlearning occurs as a single jump, an all-or-none affair.‚Äù&lt;/p&gt;
    &lt;p&gt;Ann Rosan Picture Library&lt;/p&gt;
    &lt;p&gt;An abrupt cognitive shift in how the mind understands information is known as a representational change. Although researchers have inferred sudden shifts in understanding from the behavior of subjects, they have not pinned down how the brain supports representational change.&lt;/p&gt;
    &lt;p&gt;During moments of insight, representational change typically occurs, said John Kounios, a cognitive neuroscientist at Drexel University and co-author of the book The Eureka Factor: Aha Moments, Creative Insight, and the Brain. ‚ÄúThe question is: How is it occurring?‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Insightful Activity&lt;/head&gt;
    &lt;p&gt;While at Humboldt University of Berlin, Becker set out to uncover this neural signature of insight. Given that it‚Äôs nearly impossible to fabricate life-changing, field-altering insights in the lab, her team needed to identify a simple task that could produce a sudden feeling of understanding rather than a slowly unfolding solution.&lt;/p&gt;
    &lt;p&gt;They turned to abstracted black-and-white pictures called Mooney images, which are made by cranking up the contrast on a photograph all the way so that the subjects ‚Äî a dog or a coffee mug, for example ‚Äî are unrecognizable at first. The pictures pose a challenge for human brains, which typically identify objects by piecing together their different parts. But if given enough time with a Mooney image, even a few seconds, the brain can rearrange the contours to recognize the pictured object ‚Äî and trigger the insightful ‚Äúaha‚Äù feeling, a representational change.&lt;/p&gt;
    &lt;p&gt;Over the course of two days, Becker had study participants lie in a functional magnetic resonance imaging (fMRI) scanner, which detects blood flow in the brain as a proxy for neural activity, and view a series of 120 Mooney images. After 10 seconds of viewing a single image, the participant would indicate whether they recognized the pictured object. If they did, they would then answer a series of questions about the suddenness, positive emotion and certainty associated with their experience ‚Äî three measures that have been linked to moments of insight.&lt;/p&gt;
    &lt;p&gt;Becker and her team then used neural networks to parse the fMRI data, looking to identify consistent changes in brain activity shared by participants when they correctly recognized Mooney images. They observed that when a participant noticed a hidden object, brain activity increased in the ventral occipitotemporal cortex (VOTC), a region responsible for recognizing visual patterns in the environment; the amygdala, which processes both positive and negative emotions; and the hippocampus, a deep-brain structure involved in handling memories. This activity was greater for experiences rated more certain and emotionally positive ‚Äî in other words, more insightful ones.&lt;/p&gt;
    &lt;p&gt;The hippocampus is sometimes known as the brain‚Äôs ‚Äúmismatch detector,‚Äù Becker said, because it reacts when an input doesn‚Äôt align with expectations. In this case, insight leads a once-meaningless image to gain meaning, going against the brain‚Äôs predictions.&lt;/p&gt;
    &lt;p&gt;Courtesy of Maxi Becker&lt;/p&gt;
    &lt;p&gt;These regions ‚Äî the hippocampus, amygdala and VOTC ‚Äî create ‚Äúa plausible network of brain areas‚Äù behind representational change, said Kounios, who was not involved in the study. These findings finally ‚Äúconnect the psychological theory with the neural mechanism,‚Äù said Yuhua Yu, a postdoctoral researcher in neuroscience at the University of Arizona, who was also not involved with the study.&lt;/p&gt;
    &lt;p&gt;Becker and her team likely found representational change in the VOTC because of the visual nature of their stimuli. If they had chosen another type of stimulus, like words, the change would probably have appeared in language-processing areas of the brain.&lt;/p&gt;
    &lt;p&gt;Once the team had figured out which brain areas support insight, they wanted to probe whether these regions might be working together to create a lasting memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Memory Boost&lt;/head&gt;
    &lt;p&gt;Since they began investigating insight, researchers have suspected that such experiences might boost memory. In his 1949 book The Organization of Behavior, Hebb wrote that ‚Äúwhatever insight is, we now know that it continually affects the learning of the adult mammal.‚Äù Insight not only feels notable or salient in the moment but also helps us retain new information as memory.&lt;/p&gt;
    &lt;p&gt;This memory boost, which became known as the insight-memory advantage, has since been studied in many types of problem-solving, including the unraveling of magic tricks and puzzles. ‚ÄúWhen you have an insight, you tend to be better able to remember the solution,‚Äù Becker said, compared to when you resolve a problem more gradually. She wanted to understand why.&lt;/p&gt;
    &lt;p&gt;A few days after the initial experiment, the team tested participants‚Äô memory by having them look at more Mooney images online, including some they had seen before. Participants were better able to remember prior images that they had rated highly on the three aspects of insight. This suggested that the insight-memory advantage was real, but the team wanted to see what was going on under the hood. Did brain activity during insight predict better memory five days later?&lt;/p&gt;
    &lt;p&gt;The researchers found that the larger the activity boost in both the VOTC and the hippocampus during the initial insight, the better participants remembered the Mooney images. The big change in brain activity likely makes the experience more salient, Becker said, and salient experiences are known to better encode long-term memories.&lt;/p&gt;
    &lt;p&gt;While insight creates stronger memories of an idea, it doesn‚Äôt mean the idea is correct. Previous work has shown that the quicker, more certain and more pleasurable a solution feels, the more likely it is to be correct ‚Äî but false insights can and do exist. In Becker‚Äôs study, participants wrongly identified the subjects of more than half the Mooney images they saw. Of those incorrect trials (which the researchers excluded from the analysis), the participants reported experiencing insight 40% of the time. In comparison, correct trials were accompanied by feelings of insight 65% of the time.&lt;/p&gt;
    &lt;p&gt;These kinds of studies of insight in the lab will set researchers up to look at how it functions in the real world. Once we decompose insight into ‚Äúvery simple tasks that we already understand well,‚Äù Becker said, we can ‚Äúmove on to more complex, truly creative tasks.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Insight Into the Future&lt;/head&gt;
    &lt;p&gt;As a self-described uncreative person, Yu has been particularly fascinated by insight‚Äôs role in the creative process. Creativity is ‚Äúlike a magic power,‚Äù she said. ‚ÄúA really big creative idea is [often] associated with insight because a creative idea is in some way a leap in your cognitive world, and a leap will often elicit an insight or ‚Äòaha‚Äô feeling.‚Äù&lt;/p&gt;
    &lt;p&gt;However, Yu is finding that insight‚Äôs role in creativity might depend on the kind of problem a person is solving. In a recent study, she asked participants to come up with metaphors for scientific concepts and asked whether they used insight as they did so. The insight-driven metaphors weren‚Äôt more or less creative than those created through analytic thinking, she found ‚Äî and the participants were more likely to remember the science concepts behind the latter.&lt;/p&gt;
    &lt;p&gt;This may be because, unlike the task of seeing a hidden object in a Mooney image, creating a metaphor tends to rely on slower cognitive problem-solving rather than sudden moments of insight, Becker suggested. The effects of insight therefore likely depend on the context.&lt;/p&gt;
    &lt;p&gt;Next, Yu wants to investigate insight in more contexts. ‚ÄúMost of the insight research is looking at insight in the problem-solving context and in the lab setting,‚Äù Yu said. She hopes that researchers will begin investigating ‚Äúinsight within many other domains, like in psychotherapy, in meditation, even in psychedelic experiences.‚Äù&lt;/p&gt;
    &lt;p&gt;Beyond offering a better understanding of how the human brain learns, these findings could have applications in classrooms. Kounios believes that applying insight-boosting strategies to teaching could lead to better learning outcomes for students. Insight seems to be a powerful and positive experience that generates accurate solutions, confidence in our answers and strong memories.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs very intensive for a teacher to do this, but a lot of really good teachers try to get the students to have the insights themselves about how something works, and that will burn it into their memories,‚Äù Kounios said. ‚ÄúAnother aspect of that [is], it‚Äôs very motivating, too.‚Äù&lt;/p&gt;
    &lt;p&gt;It‚Äôs a nice feeling when your brain suddenly comes up with an answer. Perhaps you‚Äôve even experienced that feeling since reading this piece‚Äôs first sentence. Maybe it even hit you like an apple on the head.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45948792</guid><pubDate>Sun, 16 Nov 2025 21:57:13 +0000</pubDate></item><item><title>PicoIDE ‚Äì An open IDE/ATAPI drive emulator</title><link>https://picoide.com/</link><description>&lt;doc fingerprint="5d0cba1441c185fa"&gt;
  &lt;main&gt;
    &lt;p&gt;Currently PicoIDE only emulates one drive, but the hardware is capable of supporting two devices simultaneously. Enabling this is a priority for development after the PicoIDE is released.&lt;/p&gt;
    &lt;p&gt;Can you add [hardware feature]?&lt;/p&gt;
    &lt;p&gt;The current hardware design is locked in, but who knows what the future may bring.&lt;/p&gt;
    &lt;p&gt;Will PicoIDE work in my device?&lt;/p&gt;
    &lt;p&gt;The primary development/testing platform for PicoIDE is 90s-era PCs, but PicoIDE has a high level of compatibility with the ATA and ATAPI standards, so there's a good chance it will work in your device. If your device is particularly finicky, that sounds like a fun development challenge!&lt;/p&gt;
    &lt;p&gt;What about UDMA support?&lt;/p&gt;
    &lt;p&gt;In the interest of providing a cost-effective design, PicoIDE's hardware doesn't support UDMA, but faster systems are very well served in the area of fast HDD replacements by CF, SD, and M.2 to IDE adapters, and for optical, PicoIDE's MWDMA mode 2 and PIO mode 4 support is as fast as a 52X CD-ROM drive, plenty for fast systems.&lt;/p&gt;
    &lt;p&gt;Will there be a 5.25" or 2.5" version?&lt;/p&gt;
    &lt;p&gt;PicoIDE will launch with a 3.5" enclosure only, but depending how things go, other form factors may be made available.&lt;/p&gt;
    &lt;p&gt;When will PicoIDE be available?&lt;/p&gt;
    &lt;p&gt;PicoIDE will be launching soon‚Ñ¢. Sign up below to be notified when it becomes available for purchase.&lt;/p&gt;
    &lt;p&gt;Where will I be able to purchase a PicoIDE? How much will it cost?&lt;/p&gt;
    &lt;p&gt;Availability and pricing will be announced when PicoIDE is launched. If you're familiar with PicoGUS, you know I value making attainable hardware.&lt;/p&gt;
    &lt;p&gt;PicoIDE will be launching soon. Sign up to be notified when that happens!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45949352</guid><pubDate>Sun, 16 Nov 2025 23:19:24 +0000</pubDate></item><item><title>A 1961 Relay Computer Running in the Browser</title><link>https://minivac.greg.technology/</link><description>&lt;doc fingerprint="514993e1c0d08679"&gt;
  &lt;main&gt;
    &lt;p&gt;Best viewed on desktop for now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45950396</guid><pubDate>Mon, 17 Nov 2025 02:36:18 +0000</pubDate></item><item><title>Mixing Is the Heartbeat of Deep Lakes. At Crater Lake, It's Slowing Down</title><link>https://www.quantamagazine.org/mixing-is-the-heartbeat-of-deep-lakes-at-crater-lake-its-slowing-down-20251114/</link><description>&lt;doc fingerprint="7753005e2eb11798"&gt;
  &lt;main&gt;
    &lt;p&gt;Katie Falkenberg for Quanta Magazine&lt;/p&gt;
    &lt;p&gt;On a radiant July afternoon, a pair of scientists hung their heads off the side of a boat and peered into the brilliant blue water of a lake known for its clarity. They were watching for the exact moment when a black-and-white, dinner plate‚Äìsized object called a Secchi disc disappeared from view in the water column of Crater Lake in Oregon.&lt;/p&gt;
    &lt;p&gt;The disc was being slowly lowered by crane, spinning lazily like a carnival prop. A minute or so after it hit the water, graduate student Juan Estuardo Bocel gave a shout to indicate that he could no longer see the disc: ‚ÄúI am out!‚Äù&lt;/p&gt;
    &lt;p&gt;Seconds later, researcher Eva Laiti echoed: ‚ÄúOK, I‚Äôm out!‚Äù&lt;/p&gt;
    &lt;p&gt;The crane operator, Scott Girdner, a lanky freshwater biologist who has spent most of his adult life at Crater Lake National Park, recorded the disc depth for each call. Then he slowly raised it until the junior researchers piped up again when it was back in view, and he recorded those depths, too.&lt;/p&gt;
    &lt;p&gt;The mean of those readings, known as the Secchi depth, has been used as a simple and dependable measure of water clarity since 1865, when the Italian Jesuit priest Angelo Secchi invented it at the behest of the papacy. The value recorded that afternoon in 2025 ‚Äî about 78 feet (24 meters), an unusually cloudy reading for Crater Lake ‚Äî is now part of one of the world‚Äôs longest-running datasets on lake physics. The lake‚Äôs first Secchi reading was taken in 1886, and in 1983 scientists began to repeat the procedure several times per month every summer. When it comes to lake health, long-term data is treasure.&lt;/p&gt;
    &lt;p&gt;Crater Lake‚Äôs size, natural beauty and otherworldly clarity ‚Äî a reflection of its setting and isolation ‚Äî make it one of the world‚Äôs most iconic freshwater bodies. With a maximum depth of 1,949 feet, it is the deepest lake in the United States. It‚Äôs also very likely the clearest large lake on Earth, with a vivid blue hue seldom encountered in nature.&lt;/p&gt;
    &lt;p&gt;‚ÄúPeople are just amazed and wowed at the optical blue that you see from pure water itself,‚Äù said Sudeep Chandra, a limnologist at the University of Nevada, Reno, who collaborates with Girdner. ‚ÄúThat blueness is the reflection of the hydrogen and oxygen hanging out together without any material in it.‚Äù&lt;/p&gt;
    &lt;p&gt;Since 2010, however, Girdner and his colleagues have noticed an unexpected change in the Secchi data: Despite the day‚Äôs slightly cloudy reading, Crater Lake‚Äôs clear water is getting even clearer.&lt;/p&gt;
    &lt;p&gt;This might sound like a good thing. After all, the lake‚Äôs remarkable, glasslike transparency and brilliant hue are major draws for the half-million tourists who visit every year. But it might also indicate that something is going wrong with the lake‚Äôs physics, chemistry and ecology, and it could be a harbinger of changes to lakes across the world in the age of climate change.&lt;/p&gt;
    &lt;p&gt;As the planet warms, summers are growing longer and winter nights aren‚Äôt getting as cold as they used to. As a result, the surfaces of many deep, temperate lakes are warming even faster than the air. This shift to the energy flux of the top layer of water can set in motion a series of physical changes that add up to a breakdown of lake mixing ‚Äî a fundamental process that acts like a heartbeat for deep, temperate lakes that don‚Äôt freeze in winter. Lake mixing is driven by physical properties such as wind, air temperature, water temperature and salinity, and on seasonal or annual cycles it circulates water between the surface and the depths. When mixing stops, oxygen and nutrients don‚Äôt get distributed throughout the water column, which can kill fish, trigger unsightly and dangerous algal blooms and invite invasive species to take over.&lt;/p&gt;
    &lt;p&gt;From Italy to New Zealand and beyond, scientists have been alarmed to observe reduced lake mixing. In 2021, Chandra and his colleagues published evidence in Nature of greater stratification in the water column over time ‚Äî an indicator of weaker mixing ‚Äî in 84% of 189 temperate lakes for which they could find sufficiently long and robust datasets. Some lakes had stopped mixing altogether. ‚ÄúWhile each system is unique, the endgame is generally the same: a lack of mixing for these large, deep lakes,‚Äù Chandra said.&lt;/p&gt;
    &lt;p&gt;Of the world‚Äôs millions of lakes, Crater Lake is one of very few with a monitoring program that stretches back more than 40 years. Scientists are now beginning to realize how crucial those datasets are for unraveling lake physics and how climate change is altering it. ‚ÄúBecause local weather can be extremely variable from year to year, it takes many years to capture the range in conditions and measure ‚Äònormal,‚Äô‚Äù Girdner said. ‚ÄúHence the advantage of long-term datasets.‚Äù&lt;/p&gt;
    &lt;p&gt;Crater Lake is therefore at the center of the first efforts by researchers, including Girdner and Chandra, to compare lake systems to get to the bottom of their breakdown, so they can prepare for the future and perhaps even ward off the most extreme impacts.&lt;/p&gt;
    &lt;p&gt;‚ÄúHistorically, people have studied lakes one at a time,‚Äù said Stephanie Hampton, director of the Tahoe Environmental Research Center at the University of California, Davis. In light of how quickly things are changing, that siloed approach no longer works, she said. ‚ÄúWe need to learn from each other and synthesize these data to understand what‚Äôs happening globally.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Canary in the Lake&lt;/head&gt;
    &lt;p&gt;In 2006, five deep lakes in northern Italy ‚Äî Iseo, Como, Garda, Maggiore and Lugano ‚Äî stopped fully mixing. At first, scientists didn‚Äôt think much of it. They had been monitoring the lakes since the 1980s and 1990s, and it was normal for a few years to go by without complete mixing. But as time passed and the clear waters remained stubbornly in place, they began to fear that the pause might be permanent.&lt;/p&gt;
    &lt;p&gt;Their fears seem to have been borne out. ‚ÄúIt‚Äôs been 20 years that we haven‚Äôt observed any full mixing from the top to the bottom,‚Äù said Barbara Leoni, a freshwater ecologist at the University of Milan-Bicocca. ‚ÄúI don‚Äôt know that it will be possible to return to the past behavior.‚Äù&lt;/p&gt;
    &lt;p&gt;Lake mixing is a function of the fact that water has different densities at different temperatures. In deep temperate lakes, this creates stratification in the water column: Lighter, warmer water floats on top, and colder, denser water sinks below. Any number of factors can influence mixing, but it is primarily driven by seasonal temperature changes, wind and waves.&lt;/p&gt;
    &lt;p&gt;Because these features vary from place to place and from lake to lake, mixing does not follow a single formula. In many lakes, complete mixing occurs once or twice a year, usually in spring and fall. In very large lakes, mixing might happen in the shallow upper waters on annual or seasonal cycles, while full mixing to the deepest bottom layer may occur only every few years. By studying different lakes, scientists are hoping to find shared rules.&lt;/p&gt;
    &lt;p&gt;Italy‚Äôs deep northern lakes previously achieved complete mixing on an approximately seven-year cycle. During the summer, the lake water would maintain distinct layers as surface waters warmed and remained light and in place. As surface temperatures dropped in autumn and winter, the layers would become closer in temperature; with a push from the wind, the lake would begin to mix. This redistributed heat, oxygen, nutrients and toxins throughout the water column.&lt;/p&gt;
    &lt;p&gt;That‚Äôs not how the Italian lakes work anymore, however. Now, the surface waters fail to get cool enough to sink and trigger mixing. As a result, oxygen is disappearing from the bottom of the stratified lake. It has already been depleted entirely in Lake Iseo. ‚ÄúWe have 150 meters of water without oxygen,‚Äù Leoni said. This kills off oxygen-breathing life at depth and transforms the biological community. ‚ÄúIn lakes where the deep waters have been oxygen-free for a long time, only bacteria survive,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;The hearts of Italy‚Äôs deep lakes have stopped and are no longer circulating nutrients; they show what can happen when lakes stop mixing. Crater Lake offers a different opportunity: to study how, exactly, warming temperatures can break the fundamental physics of a lake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing Mix-Up&lt;/head&gt;
    &lt;p&gt;On summer days, viewed from the rim of the ancient caldera that holds it, Crater Lake is a perfect mirror reflecting the procession of clouds and colors of the sky above. But beneath that glassy surface, dynamic processes are underway.&lt;/p&gt;
    &lt;p&gt;Compared to many other large lakes around the world, Crater Lake is close to pristine. It is surrounded by wilderness and protected as a national park. The air above it is mostly wind blowing off the Pacific Ocean, with few polluting cities or industries nearby. The lake lacks any rivers or streams emptying into it that could bring in pollution from elsewhere; it is filled by rain and melting snow. In July, Girdner and Chandra filled two large water coolers with lake water ‚Äî enough to keep the team of around 13 visiting scientists, students and National Park employees, plus a journalist and photographer, hydrated overnight. The lake‚Äôs water tasted as pure as bottled water, and it maintained a natural, refreshing temperature under the blazing summer sun.&lt;/p&gt;
    &lt;p&gt;The water purity does more than provide good drinking: It makes Crater Lake an ideal system for studying climate impacts. Without the confounding factors of agriculture, sewage, parking lot runoff and water withdrawals that tend to affect other lakes, Girdner said, ‚Äúit‚Äôs easier to see the influence of climate change.‚Äù&lt;/p&gt;
    &lt;p&gt;Girdner started working at Crater Lake in 1995 and has overseen the long-term monitoring program ever since. He often tells his staff that it‚Äôs not enough to just record change; they must also understand its drivers and its implications for the lake‚Äôs physics, chemistry and biology. To that end, every night at 8 p.m., a tube-shaped profiler instrument crawls along an anchored metal cable from a depth of 585 meters to Crater Lake‚Äôs surface and back down again. On this round trip, it tests twice a second for water conductivity, temperature, oxygen and salinity. Other sensors use light to measure chlorophyll fluorescence and phytoplankton particle density.&lt;/p&gt;
    &lt;p&gt;That dataset and others tell the story of Crater Lake‚Äôs health across time. Like virtually all lakes around the world, it‚Äôs getting warmer: Average surface water temperatures have increased by 3 degrees Celsius since 1965. In summer, nighttime air temperatures are increasing faster than daytime ones; the coldest summer nights are not as cold as they used to be. And there are more summer nights: Crater Lake has gained 33 additional days of summer weather over the past 60 years, as spring arrives earlier and earlier.&lt;/p&gt;
    &lt;p&gt;In the past, when summer nights grew cold, the lake released the day‚Äôs accumulated heat, causing surface water to become denser and sink. This phenomenon drives the shallow mixing that occurs in summer. As nights have warmed, however, this process has weakened, and mixing has slowed.&lt;/p&gt;
    &lt;p&gt;Counterintuitively, as the layer of surface water has become warmer, it has also become thinner. ‚ÄúIn the summer, there is half as much warm water floating on the surface now, on average, than there was in 1971,‚Äù Girdner said. This creates a sharper density difference with the cold water below, which in turn increases the amount of wind energy required to break through and mix the layers.&lt;/p&gt;
    &lt;p&gt;‚ÄúI think about it like a vinaigrette,‚Äù said Kevin Rose, a freshwater ecologist at Rensselaer Polytechnic Institute in New York who collaborates with Girdner and Chandra. ‚ÄúThere‚Äôs resistance to mixing.‚Äù&lt;/p&gt;
    &lt;p&gt;So what does all of this have to do with the fact that the lake is getting clearer? That‚Äôs where biology comes in. In Crater Lake‚Äôs warm surface water lives a community of phytoplankton. A thinner warm surface layer means less habitat, so there are fewer phytoplankton, which means fewer particles in the water to scatter light. This boosts the water‚Äôs clarity overall and the depth to which light can penetrate.&lt;/p&gt;
    &lt;p&gt;Crater Lake‚Äôs winter processes, which mix the lake all the way to the bottom, are undergoing their own profound changes. These transformations involve the weakening of a phenomenon called reverse stratification, in which a layer of very cold water, cooled by frigid winter air, forms on top of a slightly warmer layer that is around 4 degrees Celsius, the temperature at which water is heaviest. (At temperatures below that, water molecules begin to organize into lighter ice crystals.) When strong wind pushes the extra-cold surface water horizontally, as it approaches the lake‚Äôs edge some of it is forced down. If it is pushed down far enough, the increased pressure causes it to become denser than the 4-degree water layer. It then sinks to the bottom in a matter of hours, creating a mixing effect.&lt;/p&gt;
    &lt;p&gt;Historically, reverse stratification occurred during 80% to 90% of Crater Lake winters. As winters warm, it is becoming less common. ‚ÄúCrater Lake is sitting on a knife edge where it‚Äôs already really close to not being able to form reverse stratification,‚Äù Girdner said.&lt;/p&gt;
    &lt;p&gt;This does not bode well for the lake‚Äôs future mixing. When Girdner‚Äôs colleagues used his data to simulate what might happen under a range of climate scenarios, the model predicted that reverse stratification will become rare within about 50 years. If the process stops entirely, Crater Lake will no longer mix to the bottom at all. Over decades, an oxygen dead zone will begin to form ‚Äî similar to the ones in the northern Italian lakes. This risks significant ecological impacts, as well as a buildup of toxic compounds that could billow up to the surface if the lake does mix again.&lt;/p&gt;
    &lt;p&gt;Crater Lake is just starting on the path toward such dramatic changes. Another iconic lake a few hundred miles away suggests what might happen next.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Trickle-Down Effect&lt;/head&gt;
    &lt;p&gt;Lake Tahoe, the second-deepest lake in the United States, on the California-Nevada border, once rivaled Crater Lake in its clarity. In the 19th century, rocks glistened through its crystal-clear water. Then, rapid population growth in the 1950s polluted the water, causing algae to start growing offshore. In recent years, those algae have advanced into shallower waters. Secchi disc readings show that, since 1967, clarity in Lake Tahoe has been reduced by nearly 40 feet. The lake‚Äôs formerly rich blue hue is now diminished in some places.&lt;/p&gt;
    &lt;p&gt;These trends will likely continue as climate change advances, said Michael Dettinger, a hydroclimatologist at Scripps Institution of Oceanography at the University of California, San Diego. As Lake Tahoe‚Äôs mixing breaks down and summer waters get warmer and linger longer, phytoplankton enjoy an enhanced growing season and cloud the water. Over the next century, more intense and frequent storms are projected to increase water inflows, likely bringing ‚Äúenormous spikes‚Äù of sediments and nutrients into the lake, Dettinger said. Smoke from wildfires also deposits particles, which can change the light structure and nutrient composition of the lake.&lt;/p&gt;
    &lt;p&gt;Such events can affect a lake‚Äôs trajectory for years, Chandra said. When combined with altered lake mixing, they create a vicious ecological cycle.&lt;/p&gt;
    &lt;p&gt;Algae blooms are a product of these and other disruptions. In addition to killing fish, the accumulation of oxygen-poor, nutrient-rich water that builds up in a stratified lake ‚Äî especially one loaded with extra nutrients from runoff and wildfires ‚Äî can leak to the shoreline, triggering nearshore algae growth that forms a green bathtub ring surrounding a clear center. ‚ÄúThat‚Äôs one of the working hypotheses for what we think is happening in Lake Tahoe,‚Äù Chandra said.&lt;/p&gt;
    &lt;p&gt;Crater Lake suffered its first bloom of shoreline algae in 2021. ‚ÄúIt looked like someone took a massive bright green highlighter along the shore,‚Äù Girdner said. Because lake tours were closed due to the Covid-19 pandemic that summer, there was no public outcry. Had the bloom occurred during a normal summer ‚Äî like July 2025, when tourists crowded the lake in passenger boats to marvel at the seemingly bottomless blue abyss around them ‚Äî the situation might have made national headlines.&lt;/p&gt;
    &lt;p&gt;When the green ring appeared, Girdner and his colleagues felt overwhelmed. At first they had no idea what could be driving the sudden growth. Then they noticed a telling detail: The greenest places were those with the highest numbers of invasive crayfish. When crayfish move into an area, the population of insect larvae and other aquatic invertebrates that graze on algae declines by about 95%. ‚ÄúThey just hammer the insects,‚Äù Girdner said. In experiments, Girdner and his colleagues found that about seven times more algae grow in areas with crayfish compared to those without.&lt;/p&gt;
    &lt;p&gt;Yet Girdner suspected there was more than crayfish at work. Those invasive predators had regrettably been introduced to the lake in 1915, but in the intervening century, no other major algae blooms had occurred. He and his colleagues found, instead, that record-breaking water temperatures during the exceptionally hot summer of 2021 had fueled the algae growth. Crayfish had just given it a boost.&lt;/p&gt;
    &lt;p&gt;Milder winters have let the crayfish population grow and spread to new areas of the lake, further disrupting ecosystems. The Mazama newt (or Crater Lake newt), a subspecies found nowhere else in the world, has virtually disappeared. In addition to competing for the same invertebrate prey, the crayfish also capture newts in their pincers and devour the hapless amphibians alive.&lt;/p&gt;
    &lt;p&gt;Similar climate-driven invasive species patterns have been seen in other lakes. These cascading impacts exemplify the fact that lake conditions are inherently and intimately tied to climate, Chandra said. ‚ÄúWe cannot divorce the biological composition and interactions within a lake from the climatic conditions within the landscape.‚Äù&lt;/p&gt;
    &lt;p&gt;Teasing out the interactions between climate, lake mixing and ecology at Crater Lake will give research teams around the globe a blueprint for what to expect as the world continues to warm, and could be key to averting worst-case scenarios.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Uncertain Future&lt;/head&gt;
    &lt;p&gt;Last year, Chandra, Leoni and other researchers were sitting in a cafe near Lake Iseo, comparing notes about climate change at their lakes, when the cafe owner interrupted. ‚ÄúWhy do we even need to know this?‚Äù Chandra recalled him asking. ‚ÄúThere‚Äôs not much we can do about it, so why even care?‚Äù&lt;/p&gt;
    &lt;p&gt;It‚Äôs a sentiment that Chandra often encounters. He harbors hope, however, that some impacts to lakes can be slowed or avoided. While individuals cannot stop the juggernaut of climate change, he said, local interventions could make a difference. Those strategies would be context-dependent, but they could include working to balance a lake‚Äôs nutrients, controlling invasive species, cleaning up pollution, or restoring the forests and wetlands surrounding lakes.&lt;/p&gt;
    &lt;p&gt;Collaborations between different groups of scientists could enhance such interventions, said Veronica Nava, a postdoctoral researcher in freshwater ecology at the University of Milan-Bicocca. ‚ÄúIf one lake has already experienced what you‚Äôre observing, you can come up with better strategies,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;Teamwork ‚Äúis really where freshwater science is moving,‚Äù Hampton said. But such efforts are in their early days, as researchers have only started to think about comparing large lake ecosystems over the last few years. Now threats to U.S. research are rattling their newfound collaboration. ‚ÄúThe cuts to research funding are going to hit large collaborations pretty hard,‚Äù Hampton said.&lt;/p&gt;
    &lt;p&gt;The future of even Crater Lake‚Äôs exemplary scientific program is in jeopardy. After spending nearly his entire career at the lake, Girdner is retiring at the end of the year. The federal government has frozen hiring for the National Park Service, so his position will remain unfilled indefinitely. It‚Äôs unrealistic, he said, to expect his colleagues to continue the same research output on their own. ‚ÄúWe‚Äôre going to have to pare down what we‚Äôre doing,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;Until then, they‚Äôre focused on what they can do: adding another year‚Äôs data to Crater Lake‚Äôs history. After a busy day, Girdner steered the vessel back to the dock at Wizard Island, a volcanic cinder cone that juts out of Crater Lake like a pointy hat. In the cluttered boathouse, decades of signatures and sketches coated the wooden walls, bearing witness to the students and scientists who had made some contribution to a better understanding of the lake. Chandra boiled a few invasive crayfish until they were delectably tender, and the group ate them with dabs of hot sauce. They passed around a few bottles of prosecco to toast Girdner‚Äôs retirement.&lt;/p&gt;
    &lt;p&gt;As the sun dipped low, the exhausted scientists unrolled sleeping bags on the dock. Girdner had spent countless nights on the island (more than his ex-wife had liked, he admitted). This would be one of his last. The sky‚Äôs soft gradient of pink, orange and gold slowly darkened, and the Milky Way twinkled into view. Voices faded, while bats skimmed the water‚Äôs still surface. The lake‚Äôs future was uncertain. But the urgency of protecting its natural splendor could not have been clearer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45950553</guid><pubDate>Mon, 17 Nov 2025 03:09:58 +0000</pubDate></item><item><title>Building a Simple Search Engine That Works</title><link>https://karboosx.net/post/4eZxhBon/building-a-simple-search-engine-that-actually-works</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45950720</guid><pubDate>Mon, 17 Nov 2025 03:52:50 +0000</pubDate></item><item><title>Giving C a Superpower</title><link>https://hwisnu.bearblog.dev/giving-c-a-superpower-custom-header-file-safe_ch/</link><description>&lt;doc fingerprint="2c11dabd8e0b7208"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Giving C a Superpower: custom header file (safe_c.h)&lt;/head&gt;
    &lt;p&gt;The story of how I wrote a leak-free, thread-safe grep in C23 without shooting yourself in the foot, and how you can too!&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Let's be honest: most people have a love-hate relationship with C. We love its raw speed, its direct connection to the metal, and its elegant simplicity. But we hate its footguns, its dragons, the untamed beasts. The segfaults that appear from nowhere, the memory leaks that slowly drain the life from our applications, and the endless goto cleanup; chains that make our code look like a plate of spaghetti pasta.&lt;/p&gt;
    &lt;p&gt;This is the classic C curse: power without guardrails...at least that's the fear mongering mantra being said again and again. But is that still relevant in today's world with all the tools available for C devs like static analyzer and dynamic sanitizers? I've written about this here and here.&lt;/p&gt;
    &lt;p&gt;What if, with the help of the modern tools and a custom header file (600 loc), you could tame those footguns beasts? What if you could keep C's power but wrap it in a suit of modern armor? That's what the custom header file safe_c.h is for. It's designed to give C some safety and convenience features from C++ and Rust, and I'm using it to build a high-performance grep clone called cgrep as my test case.&lt;/p&gt;
    &lt;p&gt;By the end this article I hope it could provide the audience with the idea of C is super flexible and extensible, sort of "do whatever you want with it" kind of thing. And this is why C (and its close cousin: Zig) remain to be my favorite language to write programs in; it's the language of freedom!&lt;/p&gt;
    &lt;head rend="h1"&gt;safe_c.h&lt;/head&gt;
    &lt;p&gt;Is a custom C header file that takes features mainly from C++ and Rust and implements them into our C code ~ [write C code, get C++ and Rust features!]&lt;/p&gt;
    &lt;p&gt;It starts by bridging the gap between old and new C. C23 gave us &lt;code&gt;[[cleanup]]&lt;/code&gt; attributes, but in the real world, you need code that compiles on GCC 11 or Clang 18. safe_c.h detects your compiler and gives you the same RAII semantics everywhere. No more &lt;code&gt;#ifdef&lt;/code&gt; soup.&lt;/p&gt;
    &lt;code&gt;// The magic behind CLEANUP: zero overhead, maximum safety
#if defined(__STDC_VERSION__) &amp;amp;&amp;amp; __STDC_VERSION__ &amp;gt;= 202311L
#define CLEANUP(func) [[cleanup(func)]]
#else
#define CLEANUP(func) __attribute__((cleanup(func)))
#endif

// Branch prediction that actually matters in hot paths
#ifdef __GNUC__
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#else
#define LIKELY(x)   (x)
#define UNLIKELY(x) (x)
#endif
&lt;/code&gt;
    &lt;p&gt;Your cleanup code runs even if you return early, goto out, or panic. It's &lt;code&gt;finally&lt;/code&gt;, but for C.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Memory Management Beast: Slain with Smart Pointers (C++ feature)&lt;/head&gt;
    &lt;p&gt;The oldest, fiercest and most feared by devs: manual memory management.&lt;/p&gt;
    &lt;p&gt;Before: the highway path to leaks.&lt;lb/&gt; Forgetting a single &lt;code&gt;free()&lt;/code&gt; is a disaster. In cgrep, parsing command-line options the old way is a breeding ground for CVEs and its bestiary. You have to remember to free the memory on every single exit path, difficult for the undisciplined.&lt;/p&gt;
    &lt;code&gt;// The Old Way (don't do this)
char* include_pattern = NULL;
if (optarg) {
    include_pattern = strdup(optarg);
}
// ...200 lines later...
if (some_error) {
    if (include_pattern) free(include_pattern); // Did I free it? Did I??
    return 1;
}
// And remember to free it at *every* return path...
&lt;/code&gt;
    &lt;p&gt;After: memory that automatically cleans itself up.&lt;lb/&gt; UniquePtr is a "smart pointer" that owns a resource. When the UniquePtr variable goes out of scope, its resource is automatically freed. It's impossible to forget.&lt;/p&gt;
    &lt;p&gt;Here's the machinery inside &lt;code&gt;safe_c.h&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// The UniquePtr machinery: a struct + automatic cleanup
typedef struct {
    void* ptr;
    void (*deleter)(void*);
} UniquePtr;

#define AUTO_UNIQUE_PTR(name, ptr, deleter) \
    UniquePtr name CLEANUP(unique_ptr_cleanup) = UNIQUE_PTR_INIT(ptr, deleter)

static inline void unique_ptr_cleanup(UniquePtr* uptr) {
    if (uptr &amp;amp;&amp;amp; uptr-&amp;gt;ptr &amp;amp;&amp;amp; uptr-&amp;gt;deleter) {
        uptr-&amp;gt;deleter(uptr-&amp;gt;ptr);
        uptr-&amp;gt;ptr = NULL;
    }
}
&lt;/code&gt;
    &lt;p&gt;And here's how cgrep uses it. The cleanup is automatic, even if errors happen:&lt;/p&gt;
    &lt;code&gt;// In cgrep, we use this for command-line arguments
AUTO_UNIQUE_PTR(include_pattern_ptr, NULL, options_string_deleter);

// When we get a new pattern, the old one is automatically freed!
unique_ptr_delete(&amp;amp;include_pattern_ptr);
include_pattern_ptr.ptr = strdup(optarg);
// No leaks, even if an error happens later!
&lt;/code&gt;
    &lt;p&gt;Sharing Safely with SharedPtr&lt;/p&gt;
    &lt;p&gt;Before: manual, bug-prone reference counting.&lt;lb/&gt; You'd have to implement reference counting by hand, creating a complex and fragile system where a single mistake leads to a leak or a use-after-free bug.&lt;/p&gt;
    &lt;code&gt;// The old way of manual reference counting
typedef struct {
    MatchStore* store;
    int ref_count;
    pthread_mutex_t mutex;
} SharedStore;

void release_store(SharedStore* s) {
    pthread_mutex_lock(&amp;amp;s-&amp;gt;mutex);
    s-&amp;gt;ref_count--;
    bool is_last = (s-&amp;gt;ref_count == 0);
    pthread_mutex_unlock(&amp;amp;s-&amp;gt;mutex);

    if (is_last) {
        match_store_deleter(s-&amp;gt;store);
        free(s);
    }
}
&lt;/code&gt;
    &lt;p&gt;After: automated reference counting.&lt;lb/&gt; SharedPtr automates this entire process. The last thread to finish using the object automatically triggers its destruction. The machinery:&lt;/p&gt;
    &lt;code&gt;// The SharedPtr machinery: reference counting without the boilerplate
typedef struct {
    void* ptr;
    void (*deleter)(void*);
    size_t* ref_count;
} SharedPtr;

#define AUTO_SHARED_PTR(name) \
    SharedPtr name CLEANUP(shared_ptr_cleanup) = {.ptr = NULL, .deleter = NULL, .ref_count = NULL}

static inline void shared_ptr_cleanup(SharedPtr* sptr) {
    shared_ptr_delete(sptr); // Decrement and free if last reference
}
&lt;/code&gt;
    &lt;p&gt;The usage is clean and safe. No more manual counting.&lt;/p&gt;
    &lt;code&gt;// In our thread worker context, multiple threads access the same results store
typedef struct {
    // ...
    SharedPtr store;  // No more worrying about who frees this!
    SharedPtr file_counts;
    // ...
} FileWorkerContext;

// In main(), we create it once and share it safely
// SharedPtr: Reference-counted stores for thread-safe sharing
SharedPtr store_shared = {0};
shared_ptr_init(&amp;amp;store_shared, store_ptr.ptr, match_store_deleter);
// Pass to threads: ctx-&amp;gt;store = shared_ptr_copy(&amp;amp;store_shared);
// ref-count increments automatically; last thread out frees it.
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Buffer Overflow Beast: Contained with Vectors and Views (C++ feature)&lt;/head&gt;
    &lt;p&gt;Dynamically growing arrays in C is a horror show.&lt;/p&gt;
    &lt;p&gt;Before: the realloc dance routine.&lt;lb/&gt; You have to manually track capacity and size, and every realloc risks fragmenting memory or failing, requiring careful error handling for every single element you add.&lt;/p&gt;
    &lt;code&gt;// The old way: manual realloc is inefficient and complex
MatchEntry** matches = NULL;
size_t matches_count = 0;
size_t matches_capacity = 0;

for (/*...each match...*/) {
    if (matches_count &amp;gt;= matches_capacity) {
        matches_capacity = (matches_capacity == 0) ? 8 : matches_capacity * 2;
        MatchEntry** new_matches = realloc(matches, matches_capacity * sizeof(MatchEntry*));
        if (!new_matches) {
            free(matches); // Don't leak!
            /* handle error */
        }
        matches = new_matches;
    }
    matches[matches_count++] = current_match;
}
&lt;/code&gt;
    &lt;p&gt;After: a type-safe, auto-growing vector.&lt;lb/&gt; safe_c.h generates an entire type-safe vector for you. It handles allocation, growth, and cleanup automatically. The magic that generates the vector:&lt;/p&gt;
    &lt;code&gt;// The magic that generates a complete vector type from a single line
#define DEFINE_VECTOR_TYPE(name, type) \
    typedef struct { \
        Vector base; \
        type* data; \
    } name##Vector; \
    \
    static inline bool name##_vector_push_back(name##Vector* vec, type value) { \
        bool result = vector_push_back(&amp;amp;vec-&amp;gt;base, &amp;amp;value); \
        vec-&amp;gt;data = (type*)vec-&amp;gt;base.data; /* Sync pointer after potential realloc */ \
        return result; \
    } \
    \
    static inline bool name##_vector_reserve(name##Vector* vec, size_t new_capacity) { \
        bool result = vector_reserve(&amp;amp;vec-&amp;gt;base, new_capacity); \
        vec-&amp;gt;data = (type*)vec-&amp;gt;base.data; /* Sync pointer after potential realloc */ \
        return result; \
    } \


    /* more helper functions not outlined here */

// And the underlying generic Vector implementation
typedef struct {
    size_t size;
    size_t capacity;
    void* data;
    size_t element_size;
} Vector;
&lt;/code&gt;
    &lt;p&gt;Using it in cgrep is simple and safe. The vector cleans itself up when it goes out of scope.&lt;/p&gt;
    &lt;code&gt;// Type-safe vector for collecting matches
DEFINE_VECTOR_TYPE(MatchEntryPtr, MatchEntry*)

AUTO_TYPED_VECTOR(MatchEntryPtr, all_matches_vec);
MatchEntryPtr_vector_reserve(&amp;amp;all_matches_vec, store-&amp;gt;total_matches);

// Pushing elements is safe and simple
for (MatchEntry* entry = store-&amp;gt;buckets[i]; entry; entry = entry-&amp;gt;next) {
    MatchEntryPtr_vector_push_back(&amp;amp;all_matches_vec, entry);
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Views: Look, Don't Touch (or malloc) - C++ feature&lt;/head&gt;
    &lt;p&gt;Before: needless allocations.&lt;lb/&gt; To handle a substring or a slice of an array, you'd often malloc a new buffer and copy the data into it, which is incredibly slow in a tight loop.&lt;/p&gt;
    &lt;code&gt;// The old way: allocating a new string just to get a substring
const char* line = "this is a long line of text";
char* pattern = "long line";
// To pass just the pattern to a function, you might do this:
char* sub = malloc(strlen(pattern) + 1);
strncpy(sub, pattern, strlen(pattern) + 1);
// ... use sub ...
free(sub); // And hope you remember this free call
&lt;/code&gt;
    &lt;p&gt;After: zero-cost, non-owning views.&lt;lb/&gt; A StringView or a Span is just a pointer and a length. It's a non-owning reference that lets you work with slices of data without any allocation. The definitions are pure and simple:&lt;/p&gt;
    &lt;code&gt;// The StringView and Span definitions: pure, simple, zero-cost
typedef struct {
    const char* data;
    size_t size;
} StringView;

typedef struct {
    void* data;
    size_t size;
    size_t element_size;
} Span;
&lt;/code&gt;
    &lt;p&gt;In cgrep, the search pattern becomes a StringView, avoiding allocation entirely.&lt;/p&gt;
    &lt;code&gt;// Our options struct holds a StringView, not a char*
typedef struct {
    StringView pattern; // Clean, simple, and safe
    // ...
} GrepOptions;

// Initializing it is a piece of cake
options.pattern = string_view_init(argv[optind]);
&lt;/code&gt;
    &lt;p&gt;For safe array access, Span provides a bounds-checked window into existing data.&lt;/p&gt;
    &lt;code&gt;// safe_c.h
#define DEFINE_SPAN_TYPE(name, type) \
    typedef struct { \
        type* data; \
        size_t size; \
    } name##Span; \
    \
    static inline name##Span name##_span_init(type* data, size_t size) { \
        return (name##Span){.data = data, .size = size}; \
    } \
    \

    /* other helper functions not outlined here */
&lt;/code&gt;
    &lt;code&gt;// Span: Type-safe array slices for chunk processing
DEFINE_SPAN_TYPE(LineBuffer, char)
LineBufferSpan input_span = LineBuffer_span_init((char*)start, len);

for (size_t i = 0; i &amp;lt; LineBuffer_span_size(&amp;amp;input_span); i++) {
    char* line = LineBuffer_span_at(&amp;amp;input_span, i); // asserts i &amp;lt; span.size
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Error-Handling &lt;code&gt;goto&lt;/code&gt; Beast: Replaced with Results (Rust feature) and RAII (C++ feature)&lt;/head&gt;
    &lt;p&gt;C's error handling is notoriously messy.&lt;/p&gt;
    &lt;p&gt;Before: goto cleanup spaghetti carbonara.&lt;lb/&gt; Functions return special values like -1 or NULL, and you have to check errno. This leads to deeply nested if statements and a single goto cleanup; label that has to handle every possible failure case.&lt;/p&gt;
    &lt;code&gt;// The old way: goto cleanup
int do_something(const char* path) {
    int fd = open(path, O_RDONLY);
    if (fd &amp;lt; 0) {
        return -1; // Error
    }

    void* mem = malloc(1024);
    if (!mem) {
        close(fd); // Manual cleanup
        return -1;
    }
    
    // ... do more work ...

    free(mem);
    close(fd);
    return 0; // Success
}
&lt;/code&gt;
    &lt;p&gt;After: explicit, type-safe result.&lt;lb/&gt; Inspired by Rust, Result&lt;/p&gt;
    &lt;code&gt;// The Result type machinery: tagged unions for success/failure
typedef enum { RESULT_OK, RESULT_ERROR } ResultStatus;

#define DEFINE_RESULT_TYPE(name, value_type, error_type) \
    typedef struct { \
        ResultStatus status; \
        union { \
            value_type value; \
            error_type error; \
        }; \
    } Result##name;
&lt;/code&gt;
    &lt;p&gt;Handling errors becomes easy. You can't accidentally use an error as a valid value.&lt;/p&gt;
    &lt;code&gt;// Define a Result for file operations
DEFINE_RESULT_TYPE(FileOp, i32, const char*)

// Our function now returns a clear Result
static ResultFileOp submit_stat_request_safe(...) {
    // ...
    if (!sqe) {
        return RESULT_ERROR(FileOp, "Could not get SQE for stat");
    }
    return RESULT_OK(FileOp, 0);
}

// And handling it is clean
ResultFileOp result = submit_stat_request_safe(path, &amp;amp;ring, &amp;amp;pending_ops);
if (!RESULT_IS_OK(result)) {
    fprintf(stderr, "Error: %s\n", RESULT_UNWRAP_ERROR(result));
}
&lt;/code&gt;
    &lt;p&gt;This is powered by RAII. The &lt;code&gt;CLEANUP&lt;/code&gt; attribute ensures resources are freed no matter how a function exits.&lt;/p&gt;
    &lt;code&gt;#define AUTO_MEMORY(name, size) \
    void* name CLEANUP(memory_cleanup) = malloc(size)

// DIR pointers are automatically closed, even on an early return.
DIR* dir CLEANUP(dir_cleanup) = opendir(req-&amp;gt;path);
if (!dir) {
    return RESULT_ERROR(FileOp, "Failed to open dir"); // dir_cleanup is NOT called
}
if (some_condition) {
    return RESULT_OK(FileOp, 0); // closedir() is called automatically HERE!
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Assumption Beast: Challenged with Contracts and Safe Strings&lt;/head&gt;
    &lt;p&gt;Before: &lt;code&gt;assert()&lt;/code&gt; and pray.&lt;lb/&gt; A standard &lt;code&gt;assert(ptr != NULL)&lt;/code&gt; is good, but when it fails, the message is generic. You know the condition failed, but not the context or why it was important.&lt;/p&gt;
    &lt;p&gt;After: self-documenting contracts.&lt;code&gt;requires()&lt;/code&gt; and &lt;code&gt;ensures()&lt;/code&gt; make function contracts explicit. The failure messages tell you exactly what went wrong.
The contract macros:&lt;/p&gt;
    &lt;code&gt;#define requires(cond) assert_msg(cond, "Precondition failed")
#define ensures(cond) assert_msg(cond, "Postcondition failed")

#define assert_msg(cond, msg) /* ... full implementation ... */
&lt;/code&gt;
    &lt;p&gt;This turns assertions into executable documentation:&lt;/p&gt;
    &lt;code&gt;// Preconditions that document and enforce contracts
static inline bool arena_create(Arena* arena, size_t size)
{
    requires(arena != NULL);  // Precondition: arena must not be null
    requires(size &amp;gt; 0);       // Precondition: size must be positive
    
    // ... implementation ...
    
    ensures(arena-&amp;gt;buffer != NULL);  // Postcondition: buffer is allocated
    ensures(arena-&amp;gt;size == size);    // Postcondition: size is set correctly
    
    return true;
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;strcpy()&lt;/code&gt; is a Security Vulnerability&lt;/head&gt;
    &lt;p&gt;Before: buffer overflows.&lt;lb/&gt; strcpy has no bounds checking. It's the source of countless security holes. &lt;code&gt;strncpy&lt;/code&gt; is little better, as it might not null-terminate the destination string.&lt;/p&gt;
    &lt;code&gt;// The old, dangerous way
char dest[20];
const char* src = "This is a very long string that will overflow the buffer";
strcpy(dest, src); // Undefined behavior! Stack corruption!
&lt;/code&gt;
    &lt;p&gt;After: safe, bounds-checked operations.&lt;lb/&gt; safe_c.h provides alternatives that check bounds and return a success/failure status. No surprises. The safe implementation:&lt;/p&gt;
    &lt;code&gt;// The safe string operations: bounds checking that can't be ignored
static inline bool safe_strcpy(char* dest, size_t dest_size, const char* src) {
    if (!dest || dest_size == 0 || !src) return false;
    size_t src_len = strlen(src);
    if (src_len &amp;gt;= dest_size) return false;
    memcpy(dest, src, src_len + 1);
    return true;
}
&lt;/code&gt;
    &lt;p&gt;In cgrep, this prevents path buffer overflows cleanly:&lt;/p&gt;
    &lt;code&gt;// Returns bool, not silent truncation
if (!safe_strcpy(req-&amp;gt;path, PATH_MAX, path)) {
    free(req);
    return RESULT_ERROR(FileOp, "Path is too long");
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Concurrency: Mutexes That Unlock Themselves (Rust feature)&lt;/head&gt;
    &lt;p&gt;Before: leaked locks and deadlocks.&lt;lb/&gt; Forgetting to unlock a mutex, especially on an error path, is a catastrophic bug that causes your program to deadlock.&lt;/p&gt;
    &lt;code&gt;// The Buggy Way
pthread_mutex_lock(&amp;amp;mutex);
if (some_error) {
    return; // Oops, mutex is still locked! Program will deadlock.
}
pthread_mutex_unlock(&amp;amp;mutex);
&lt;/code&gt;
    &lt;p&gt;After: RAII-based locks.&lt;lb/&gt; Using the same CLEANUP attribute, we can ensure a mutex is always unlocked when the scope is exited. This bug becomes impossible to write.&lt;/p&gt;
    &lt;code&gt;// With a cleanup function, unlocking is automatic.
void mutex_unlock_cleanup(pthread_mutex_t** lock) {
    if (lock &amp;amp;&amp;amp; *lock) pthread_mutex_unlock(*lock);
}

// RAII lock guard via cleanup attribute
pthread_mutex_t my_lock;
pthread_mutex_t* lock_ptr CLEANUP(mutex_unlock_cleanup) = &amp;amp;my_lock;
pthread_mutex_lock(lock_ptr);

if (some_error) {
    return; // Mutex is automatically unlocked here!
}
&lt;/code&gt;
    &lt;p&gt;Simple wrappers also clean up the boilerplate of managing threads:&lt;/p&gt;
    &lt;code&gt;// The concurrency macros: spawn and join without boilerplate
#define SPAWN_THREAD(name, func, arg) \
    thrd_t name; \
    thrd_create(&amp;amp;name, (func), (arg))

#define JOIN_THREAD(name) \
    thrd_join(name, NULL)
&lt;/code&gt;
    &lt;p&gt;And in cgrep:&lt;/p&gt;
    &lt;code&gt;// Thread pool spawn without boilerplate
SPAWN_THREAD(workers[i], file_processing_worker, &amp;amp;contexts[i]);
JOIN_THREAD(workers[i]); // No manual pthread_join() error handling
&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance: Safety at -O2, Not -O0&lt;/head&gt;
    &lt;p&gt;Safety doesn't mean slow. The UNLIKELY() macro tells the compiler which branches are cold, adding zero overhead in hot paths.&lt;/p&gt;
    &lt;code&gt;#ifdef __GNUC__
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#else
#define LIKELY(x)   (x)
#define UNLIKELY(x) (x)
#endif
&lt;/code&gt;
    &lt;p&gt;The real win is in the fast paths:&lt;/p&gt;
    &lt;code&gt;// In hot allocation path: branch prediction
if (UNLIKELY(store-&amp;gt;local_buffer_sizes[thread_id] &amp;gt;= LOCAL_BUFFER_CAPACITY)) {
    match_store_flush_buffer(store, thread_id); // Rarely taken
}

// In match checking: likely path first
if (!options-&amp;gt;case_insensitive &amp;amp;&amp;amp; options-&amp;gt;fixed_string) {
    // Most common case: fast path with no branches
    const char* result = strstr(line, options-&amp;gt;pattern.data);
    return result != NULL;
}
&lt;/code&gt;
    &lt;p&gt;The above is similar to what a PGO (Profile Guided Optimization) would have.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Final Word: C That Doesn't Blow Your Own Foot!&lt;/head&gt;
    &lt;p&gt;This is what main() looks like when you stop fighting the language:&lt;/p&gt;
    &lt;code&gt;int main(int argc, char* argv[]) {
    initialize_simd();
    output_buffer_init(); // Auto-cleanup on exit
    
    GrepOptions options = {0};
    AUTO_UNIQUE_PTR(include_pattern_ptr, NULL, options_string_deleter);
    
    // ... parse args with getopt_long ...
    
    AUTO_UNIQUE_PTR(store_ptr, NULL, match_store_deleter);
    SharedPtr store_shared = {0};
    if (need_match_store) {
        store_ptr.ptr = malloc(sizeof(ConcurrentMatchStore));
        if (!store_ptr.ptr || !match_store_create(store_ptr.ptr, hash_capacity, 1000)) {
            return 1; // All allocations cleaned up automatically
        }
        shared_ptr_init(&amp;amp;store_shared, store_ptr.ptr, match_store_deleter);
    }
    
    // Process files with thread pool...
    
cleanup: // Single cleanup label needed -- RAII handles the rest
    output_buffer_destroy(); // Flushes and destroys
    return 0;
}
&lt;/code&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In the end, cgrep is 2,300 lines of C. Without safe_c.h, it would have required over 50 manual free() calls ~ a recipe for leaks and segfaults. With the custom header file, it's 2,300 lines that compile to the same assembly, run just as fast, and are fundamentally safer.&lt;/p&gt;
    &lt;p&gt;This proves that the best abstraction is the one you don't pay for and can't forget to use. It enables a clear and powerful development pattern: validate inputs at the boundary, then unleash C's raw speed on the core logic. You get all the power of C without the infamous self-inflicted footgun wounds.&lt;/p&gt;
    &lt;p&gt;C simplicity makes writing programs with it becomes fun, however there are ways to make it both fun and safe..just like using condoms, you know?&lt;/p&gt;
    &lt;p&gt;This post has gotten too long for comfort, but I have one final food for thought for you the readers: after all these guard rails, what do you think of cgrep's performance? Check the screenshots below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;grep bench on recursive directories&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;grep bench on single large file NOTE: make sure you check the memory usage comparison between cgrep and ripgrep&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the next article, I will discuss how I built cgrep, the design I chose for it, why and how cgrep managed to be a couple of times faster than ripgrep (more than 2x faster in the recursive directory bench) while being super efficient with resource usage (20x smaller memory footprint in the single large file bench).&lt;/p&gt;
    &lt;p&gt;It's gonna be a lot of fun! Cheers!&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments section here&lt;/head&gt;
    &lt;p&gt;If you enjoyed this post, click the little up arrow chevron on the bottom left of the page to help it rank in Bear's Discovery feed and if you got any questions or anything, please use the comments section.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45952428</guid><pubDate>Mon, 17 Nov 2025 10:40:55 +0000</pubDate></item><item><title>Ned: ImGui Text Editor with GL Shaders</title><link>https://github.com/nealmick/ned</link><description>&lt;doc fingerprint="7ec6cf545336b8c1"&gt;
  &lt;main&gt;
    &lt;p&gt;A retro-style text editor with GL shader effects. NED offers Tree Sitter syntax highlighting, LSP integration, and a terminal emulator.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;ned.demo.github.mp4&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Amber&lt;/cell&gt;
        &lt;cell role="head"&gt;Solarized&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;amber.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;solarized.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Ned&lt;/cell&gt;
        &lt;cell role="head"&gt;Custom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;ned.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;custom.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenGL Shaders with retro style for the best coding vibes&lt;/item&gt;
      &lt;item&gt;Text Bookmarks make editing multiple files with saved cursors a breeze&lt;/item&gt;
      &lt;item&gt;Rainbow mode cursor so you never lose your cursor and stand out&lt;/item&gt;
      &lt;item&gt;LSP Adapters for easy navigation and advanced language support&lt;/item&gt;
      &lt;item&gt;Terminal Emulator based on suckless st.c ported to C++ with multiplexer support&lt;/item&gt;
      &lt;item&gt;Optional Custom lexers and tokenizers for custom languages and obscure syntax patterns&lt;/item&gt;
      &lt;item&gt;Copilot-like auto complete using OpenRouter, choose the latest and best LLM models&lt;/item&gt;
      &lt;item&gt;Multi-cursor support, easily find and replace strings with multi selection&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CMake (version 3.10 or higher) C++20 compatible compiler OpenGL GLFW3 Glew Curl&lt;/p&gt;
    &lt;p&gt;Clone the repository with its submodules:&lt;/p&gt;
    &lt;code&gt;#Make sure you clone with recursive flag
git clone --recursive https://github.com/nealmick/ned
cd ned
git submodule init
git submodule update

# macOS Intel/ARM)
brew install clang-format cmake llvm glfw glew pkg-config curl

# Ubuntu/Debian
sudo apt install cmake libglfw3-dev libglew-dev libgtk-3-dev pkg-config clang libcurl4-openssl-dev clang-format mesa-utils

# For Windows, the dependencies are installed using the build script&lt;/code&gt;
    &lt;code&gt;./build.sh&lt;/code&gt;
    &lt;code&gt;./build-win.bat
# On Windows, the build script will attempt to install Visual Studio with Build Tools. 10-20 minutes.
# After VS has been installed, you must close and re-open PowerShell and run ./build-win.bat again.
# Subsequent rebuilds are much faster after the initial dependencies have been installed.&lt;/code&gt;
    &lt;p&gt;Create app package&lt;/p&gt;
    &lt;code&gt;./pack-mac.sh
./pack-deb.sh

# Bypass quarantine/translocation or you can sign it with your own apple dev acc
xattr -dr com.apple.quarantine Ned.app
&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;embed.demo.github.mp4&lt;/head&gt;
    &lt;p&gt;Ned can be embedded in other ImGui applications, taking advantage of its text editor, file explorer, and terminal emulator. The embedded version also includes emoji support, themes, and much more. We have a demo repository that shows how to get started embedding the neditor into your projects.&lt;/p&gt;
    &lt;p&gt;Ned is a feature-rich text editor built with Dear ImGui that combines the power of modern development tools with a lightweight, embeddable architecture. At its core, Ned provides a sophisticated text editing experience with Tree Sitter syntax highlighting supporting over 15 programming languages including C++, Python, JavaScript, Rust, Go, and more. The editor features custom lexer modes for specialized file types and includes advanced features like multi-cursor editing, line jumping, and a built-in file tree explorer.&lt;/p&gt;
    &lt;p&gt;The editor includes LSP integration with support for clangd, gopls, pyright, and TypeScript language servers, providing goto definition, find references, and symbol information. Ned also includes a terminal emulator and AI integration with OpenRouter support. The editor features emoji support with proper font rendering, custom shader effects, and a theming system. The project is designed to be embeddable in other ImGui applications through the ned_embed library, making it easy to integrate into your own projects.&lt;/p&gt;
    &lt;p&gt;Currently Ned is tested on macOS ARM and Intel, Windows x64, and has a Debian build available. Windows support includes automated dependency management through the build script.&lt;/p&gt;
    &lt;p&gt;If you have questions or issues, feel free to reach out.&lt;/p&gt;
    &lt;p&gt;Ned has an AI agent that uses OpenRouter to connect to the latest models. The agent can use MCP to call tools such as read file, run command, or edit file. The edit file tool uses a specialized model called Morph to apply code edits on large files at high speed with high accuracy, similar to Cursor. Check it out at morph.so. The whole system is tied into the settings where the key for the agent and completion model is stored. Below is a demo of the agent:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;agent_compressed.mov&lt;/head&gt;
    &lt;p&gt;Ned has the ability to track multiple cursors at once, which can make editing in certain scenarios much easier. The multi cursor system is used for file content searches to spawn cursors at each instance of a text search string. The app also supports multi selection for selecting text with multiple cursors. The cursor also supports keybinds such as jump to line end or jump one word forward. Below is a demo:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;multi-cursor_compressed.mov&lt;/head&gt;
    &lt;p&gt;Windows support is still being tested, but there is a windows build available in releases as well as a build script for both the standalone and embedded versions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45952824</guid><pubDate>Mon, 17 Nov 2025 11:53:00 +0000</pubDate></item><item><title>GCC 16 considering changing default to C++20</title><link>https://inbox.sourceware.org/gcc/aQj1tKzhftT9GUF4@redhat.com/</link><description>&lt;doc fingerprint="4e7e82b81462420f"&gt;
  &lt;main&gt;
    &lt;p&gt;Making sure you're not a bot! Loading... Please wait a moment while we ensure the security of your connection.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953202</guid><pubDate>Mon, 17 Nov 2025 13:01:49 +0000</pubDate></item><item><title>FreeMDU: Open-source Miele appliance diagnostic tools</title><link>https://github.com/medusalix/FreeMDU</link><description>&lt;doc fingerprint="af8dedd4595f4be1"&gt;
  &lt;main&gt;
    &lt;p&gt;The FreeMDU project provides open hardware and software tools for communicating with Miele appliances via their optical diagnostic interface. It serves as a free and open alternative to the proprietary Miele Diagnostic Utility (MDU) software, which is only available to registered service technicians.&lt;/p&gt;
    &lt;p&gt;Most Miele devices manufactured after 1996 include an optical infrared-based diagnostic interface, hidden behind one of the indicator lights on the front panel. On older appliances, this interface is marked by a Program Correction (PC) label.&lt;/p&gt;
    &lt;p&gt;Until now, communication with this interface required an expensive infrared adapter sold exclusively by Miele, along with their closed-source software. The goal of FreeMDU is to make this interface accessible to everyone for diagnostic and home automation purposes.&lt;/p&gt;
    &lt;p&gt;The project is split into three main components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protocol: core protocol library and device implementations&lt;/item&gt;
      &lt;item&gt;TUI: terminal-based device diagnostic and testing tool&lt;/item&gt;
      &lt;item&gt;Home: communication adapter firmware with MQTT integration for Home Assistant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details about the proprietary diagnostic interface and the reverse-engineering process behind this project can be found in this blog post.&lt;/p&gt;
    &lt;p&gt;Caution&lt;/p&gt;
    &lt;p&gt;This project is highly experimental and can cause permanent damage to your Miele devices if not used responsibly. Proceed at your own risk.&lt;/p&gt;
    &lt;p&gt;When a connection is established via the diagnostic interface, the appliance responds with its software ID, a 16-bit number that uniquely identifies the firmware version running on the device's microcontroller. However, this ID does not directly correspond to a specific model or board type, so it's impossible to provide a comprehensive list of supported models.&lt;/p&gt;
    &lt;p&gt;The following table lists the software IDs and device/board combinations that have been confirmed to work with FreeMDU:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Software ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Device&lt;/cell&gt;
        &lt;cell role="head"&gt;Board&lt;/cell&gt;
        &lt;cell role="head"&gt;Microcontroller&lt;/cell&gt;
        &lt;cell role="head"&gt;Optical interface location&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;360&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 223-A&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38078MC-065FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;419&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 206&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M37451MC-804FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;605&lt;/cell&gt;
        &lt;cell&gt;G 651 I PLUS-3&lt;/cell&gt;
        &lt;cell&gt;EGPL 542-C&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38027M8&lt;/cell&gt;
        &lt;cell&gt;Salt (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;629&lt;/cell&gt;
        &lt;cell&gt;W 2446&lt;/cell&gt;
        &lt;cell&gt;EDPL 126-B&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38079MF-308FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If your appliance is not listed here but has a model number similar to one of the above, it might already be compatible. In all other cases, determining the software ID is the first step toward adding support for new devices.&lt;/p&gt;
    &lt;p&gt;Details for adding support for new devices will be provided soon.&lt;/p&gt;
    &lt;p&gt;Before using any FreeMDU components, make sure you have the Rust toolchain installed on your system.&lt;/p&gt;
    &lt;p&gt;Next, you'll need to build a communication adapter to interface with your Miele device. Once the adapter is ready, choose the appropriate use case from the options below:&lt;/p&gt;
    &lt;p&gt;If you want to repair or test your appliance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the TUI application on your desktop computer.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to integrate your appliance into Home Assistant or another home automation system:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Flash the home firmware in standalone mode onto your communication adapter and attach it to your device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to develop your own software to communicate with Miele devices:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use the protocol crate to implement your custom software.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is an independent, open-source project and is not affiliated with, endorsed by, or sponsored by Miele &amp;amp; Cie. KG or its affiliates. All product names and trademarks are the property of their respective owners. References to Miele appliances are for descriptive purposes only and do not imply any association with Miele.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953452</guid><pubDate>Mon, 17 Nov 2025 13:40:40 +0000</pubDate></item><item><title>Geothermal energy might be the baseload revolution we've been looking for</title><link>https://www.newyorker.com/magazine/2025/11/24/why-the-time-has-finally-come-for-geothermal-energy</link><description>&lt;doc fingerprint="254a117fb8a4a7aa"&gt;
  &lt;main&gt;
    &lt;p&gt;When I arrived in Reykjav√≠k, Iceland, last March, a gravel barrier, almost thirty feet at its highest point, had been constructed to keep lava from the Reykjanes volcano from inundating a major geothermal power station not far from downtown. So far, it had worked, but daily volcano forecasts were being broadcast on a small television at the domestic airport where I was waiting to take a short flight to Akureyri, a town on the north coast about an hour‚Äôs drive from one of the country‚Äôs oldest geothermal plants, the Krafla Geothermal Station. Until the early nineteen-seventies, Iceland relied on imported fossil fuels for nearly three-quarters of its energy. The resources of the country‚Äîa landscape of hot springs, lava domes, and bubbling mud pots‚Äîwere largely untapped. ‚ÄúIn the past, people here in the valley lacked most things now considered essential to human life, except for a hundred thousand million tons of boiling-hot water,‚Äù the Icelandic Nobelist Halld√≥r Laxness wrote in ‚ÄúA Parish Chronicle,‚Äù his 1970 novel. ‚ÄúFor a hundred thousand years this water, more valuable than all coal mines, ran in torrents out to sea.‚Äù The oil crisis of 1973, when prices more than tripled, proved a useful emergency. Among other efforts to develop local energy, public-investment funds provided loans for geothermal projects, whose upfront costs were considerable. By the early eighties, almost all the country‚Äôs homes were heated geothermally; in Reykjav√≠k, a subterranean geothermal-powered system is in place to melt snow and ice off sidewalks and roads. Today, more than a quarter of the country‚Äôs electricity comes from geothermal sources, a higher proportion than in almost any other nation. Most of the rest is from hydropower.&lt;/p&gt;
    &lt;p&gt;In some ways, the process of harnessing geothermal energy is simple. The deeper you dig, the hotter the temperatures get. For direct heating, you dig relatively shallow wells (typically several hundred metres deep), to access natural reservoirs of hot water or steam, which can be piped into a structure. For electricity, wells are dug farther down, to where temperatures are above a hundred and fifty degrees Celsius. (In Iceland, this temperature is reached at around one thousand to two thousand metres deep.) Pressurized steam spins a turbine that in turn spins a generator. Thermal energy (steam) is translated into mechanical energy (the spinning turbine), which is translated into electrical energy (via the generator). Geothermal energy is essentially carbon-free, it is available at any time of day and in any weather, and it leaves a small‚Äîalbeit very deep‚Äîfootprint on the landscape.&lt;/p&gt;
    &lt;p&gt;In 2008, Iceland‚Äôs three largest energy companies collaborated on a research project to drill down even farther, at a site near Krafla, for steam that was even hotter, some four hundred degrees Celsius. Such ‚Äúsupercritical‚Äù steam is water that is so hot and pressurized that it has passed into a fourth state, beyond gas. The hotter a well the better, typically: it will produce more energy more efficiently. The Iceland Deep Drilling Project (I.D.D.P.) engineers had planned to dig down some four kilometres‚Äîbut their drill got stuck at around two kilometres. Bits of black glass shot up from the well. After some disbelief, the team concluded that they had hit magma. This oops-ing into magma was at first received as ‚Äúvery bad news,‚Äù Bjarni Palsson, a chief project manager on the I.D.D.P. and now an executive vice-president of the energy company Landsvirkjun, told me. Many people thought that drilling into magma might trigger a volcanic eruption. ‚ÄúThen we started to see: What actually do we have here?‚Äù Palsson said. They put a wellhead on their work to measure the flow rates of steam. ‚ÄúWhat happened next was remarkable,‚Äù Palsson continued. The magma was about nine hundred degrees Celsius. The steam flow was such that it could produce ten times more energy than a regular well. They had created the hottest and most powerful geothermal well in the world.&lt;/p&gt;
    &lt;p&gt;There was no security check before boarding the plane. I was told by one of my companions, Hilmar M√°r Einarsson, a youthful project manager with Landsvirkjun, that people sometimes stowed their hunting rifles in the overhead luggage compartments. On the drive from Akureyri to Krafla, we passed Lake M√Ωvatn, home to a kind of arctic char that lives only there. We also passed Icelandic horses, a diminutive breed famed for its distinctive gaits: in addition to walking, trotting, and galloping, it has a ‚Äúflying pace‚Äù and a rhythmic four-beat gait known as t√∂lt. Amid the expansive greens and yellows of northeast Iceland, we arrived at the Krafla Geothermal Station, where steam has been spinning two Mitsubishi turbines continuously for decades.&lt;/p&gt;
    &lt;p&gt;The station provides power to commercial buildings and heating to homes in the district. A rust-red building in the shape of a giant barn stood across from silvery cooling towers capped by cloud-white steam. Construction began in 1974 but took four years to finish, working around the Krafla fires, a series of volcanic eruptions that went on for years. (In Icelandic folklore, the region is where the Devil landed after being expelled from Heaven.) Around the station is a volcanic valley of green vegetation and basalt rock, with patches of snow. The wellhouses appeared as igloo-size aluminum geodesic domes; like the main power station, they were rust red. Einarsson opened one for me. Visible within was a thick horizontal pipe joined to a vertical one, with what looked like a ship‚Äôs steering wheel attached. Not visible was the well itself, which extended belowground like a long metal straw. (Krafla‚Äôs geothermal wells are about seven inches across, notably narrower than many oil and gas wells.) ‚ÄúSome wells last twenty years, some last two‚Äîyou can‚Äôt know for certain,‚Äù Einarsson explained. The temperature and permeability of the rock, as well as the amount of fluid flowing across it, affect a well‚Äôs performance. Also, Iceland has myths about ‚Äúhidden people‚Äù‚Äîhulduf√≥lk. It is said that building on their land brings bad luck, so there‚Äôs that, too.&lt;/p&gt;
    &lt;p&gt;We stopped by the station‚Äôs canteen, taking our shoes off before entering. Lunch had ended, but there was homemade apple cake, dried apricots, and skyr available. Workmen in neon-yellow suits, who had traded their boots for slippers, were having tea. Einarsson then took me to the I.D.D.P. site, not far from the Krafla plant, where a sign marked a snow-covered depression about the size of a modest pond. Compared with the turbines and steam towers and the idyllic orderliness of the canteen, the site was underwhelming. Two years after the well was dug, the extreme pressure and heat began to corrode the metal casing of the well itself. Black smoke poured out each time the well was reopened. Soon, it had to be shut down permanently. In 2017, another research well, I.D.D.P.-2, was drilled down four and a half kilometres, where temperatures reached at least four hundred and twenty-six degrees Celsius‚Äîbut this time the well failed after only six months. ‚ÄúOne thing we learned is that you don‚Äôt open and close and open and close the well‚Äîyou just leave it open,‚Äù Palsson had told me, explaining that such actions made the well more brittle.&lt;/p&gt;
    &lt;p&gt;Landsvirkjun, which had paid for most of the I.D.D.P. work, decided that it needed financial support to drill more exploratory wells. ‚ÄúWe said, ‚ÄòWe‚Äôre just a small energy company in Iceland,‚Äô ‚Äù Palsson told me. But it made its research available to the international scientific community, and there has been intermittent interest from the U.K., Germany, Canada, and New Zealand. ‚ÄúThat‚Äôs where we are now, trying to fund it as a science project that can also benefit the energy industry,‚Äù Palsson said.&lt;/p&gt;
    &lt;p&gt;Driving back to the airport, we saw snow ptarmigans and cairns of black stones marking trails that stretched beyond view. Iceland‚Äôs transition into a country powered nearly completely by renewables can seem fantastical, and the landscape furthers this impression. Because Iceland is singular in so many ways‚Äîthat lonely arctic-char species! those small horses with their t√∂lt!‚Äîyou can get the feeling that geothermal energy is a niche endeavor, as opposed to one that is technically and economically feasible in places where volcanic eruptions aren‚Äôt part of the daily forecast. But that feeling is outdated and misleading.&lt;/p&gt;
    &lt;p&gt;Geothermal is underdeveloped, and its upfront costs can be high, but it‚Äôs always on and, once it‚Äôs set up, it is cheap and enduring. The dream of geothermal energy is to meet humanity‚Äôs energy demands affordably, without harnessing horses for horsepower, slaughtering whales for their oil, or burning fossil fuels. The planet‚Äôs heat could be used to pasteurize milk or heat dorm rooms or light up a baseball stadium for a night game.&lt;/p&gt;
    &lt;p&gt;At more than five thousand degrees Celsius, the Earth‚Äôs core is roughly as hot as the surface of the sun. At the Earth‚Äôs surface, the temperature is about fourteen degrees. But in some places, like Iceland, the ground underfoot is much warmer. Hot springs, geysers, and volcanoes are surface-level signs of the Earth‚Äôs inferno. Dante‚Äôs description of Hell is said by some to have been inspired by the landscape of sulfurous steam plumes found in Devil‚Äôs Valley in Tuscany.&lt;/p&gt;
    &lt;p&gt;Snow monkeys and humans have been using Earth-heated waters as baths for ages. In the Azores, a local dish, cozido de las furnas, is cooked by burying a clay pot in hot volcanic soil; in Iceland, bread is still sometimes baked this way. The first geothermal power generator was built in Devil‚Äôs Valley, in 1904, by Prince Piero Ginori Conti of Trevignano, who had been extracting borax from the area and thought to make use of the steam emerging from the mining borehole. The generator initially powered five light bulbs. Not long afterward, it powered central Italy‚Äôs railway system and a few villages. The geothermal complex is still in operation today, providing one to two per cent of Italy‚Äôs energy. In the United States, the first geothermal plant was built in 1921, in Northern California, in a geyser-filled area that a surveyor described as the gates of Hell. That plant powered a nearby resort hotel and is also still in use.&lt;/p&gt;
    &lt;p&gt;There aren‚Äôt gates of Hell just anywhere. A kilometre below ground in Kamchatka is considerably hotter than a kilometre below ground in Kansas. There is also readily accessible geothermal energy in Kenya (where it provides almost fifty per cent of the country‚Äôs energy), New Zealand (about twenty per cent), and the Philippines (about fifteen per cent)‚Äîall volcanic areas along tectonic rifts. But in less Hadean landscapes the costs and uncertainties of drilling deep in search of sufficient heat have curtailed development. This partly explains why, in the field of clean energy, geothermal is often either not on the list or mentioned under the rubric of ‚Äúother.‚Äù For decades, both private and government investment in geothermal energy was all but negligible.&lt;/p&gt;
    &lt;p&gt;That has now changed. In the past five years, in North America, more than a billion and a half dollars have gone into geothermal technologies. This is a small amount for the energy industry, but it‚Äôs also an exponential increase. In May, 2021, Google signed a contract with the Texas-based geothermal company Fervo to power its data centers and infrastructure in Nevada; Meta signed a similar deal with Texas-based Sage for a data center east of the Rocky Mountains, and with a company called XGS for one in New Mexico. Microsoft is co-developing a billion-dollar geothermal-powered data center in Kenya; Amazon installed geothermal heating at its newly built fulfillment center in Japan. (Geothermal energy enables companies to avoid the uncertainties of the electrical grid.) Under the Biden Administration, the geothermal industry finally received the same kind of tax credits given to wind and solar, and under the current Trump Administration it has received the same kind of fast-track permitting given to oil and gas. Donald Trump‚Äôs Secretary of Energy, Chris Wright, spoke at a geothermal conference and declared, in front of a MAGA-like sign that read ‚ÄúMAGMA (Making America Geothermal: Modern Advances),‚Äù that although geothermal hasn‚Äôt achieved ‚Äúliftoff yet, it should and it can.‚Äù Depending on whom you speak with, either it‚Äôs weird that suddenly everyone is talking about geothermal or it‚Äôs weird that there is a cost-competitive energy source with bipartisan appeal that no one is talking about.&lt;/p&gt;
    &lt;p&gt;Scientific work that has been discarded or forgotten can return‚Äîsometimes through unknowing repetition, at other times through deliberate recovery. In the early nineteen-seventies, the U.S. government funded a program at Los Alamos that looked into developing geothermal energy systems that didn‚Äôt require proximity to geysers or volcanoes. Two connected wells were built: in one, water was sent down into fractured hot, dry rock; from the other, the steam that resulted from the water meeting the rock emerged. In 1973, Richard Nixon announced Project Independence, which aimed to develop energy sources outside of fossil fuels. ‚ÄúBut when Reagan came into office, he changed things,‚Äù Jefferson Tester, a professor of sustainable energy systems at Cornell University, who was involved in the Los Alamos project, told me. The price of oil had come down, and support for geothermal dissipated. ‚ÄúPeople got this impression that it was a failure,‚Äù Tester said. ‚ÄúI think if they looked a little closer, they would see that a lot of the knowledge gained in those first years could have been used to leverage what is happening now.‚Äù&lt;/p&gt;
    &lt;p&gt;Tester went on to help establish the M.I.T. Energy Lab (now called the Energy Initiative), which focusses on advancing clean-energy solutions. He and his colleagues felt that students needed to know the history of the research into diverse energy sources, so they put together a course and a textbook called ‚ÄúSustainable Energy: Choosing Among Options.‚Äù In 2005, the Department of Energy, under George W. Bush, commissioned a group consisting of Tester and some seventeen other experts and researchers‚Äîincluding drilling engineers, energy economists, and power-plant builders‚Äîto investigate what it would take for the U.S. to produce a hundred thousand megawatts of geothermal energy, a bit more than one-fifth of the energy the U.S. had consumed that year. (Geothermal energy production in the U.S. at that time was around three or four thousand megawatts.) The experts avoided framing their support for geothermal in environmental terms. ‚ÄúThe feeling was that you weren‚Äôt supposed to talk about carbon, because then it would be perceived as about climate change,‚Äù Tester said.&lt;/p&gt;
    &lt;p&gt;In 2006, Tester and his colleagues published their report, ‚ÄúThe Future of Geothermal Energy.‚Äù One finding was that new drilling technology employed by the oil-and-gas industry was changing the economics of geothermal power generation. Latent ideas‚Äîlike those from the Los Alamos project‚Äîhad met their moment. ‚ÄúI was called to testify a few times before Congress. It was a relatively modest investment that was needed, and people were excited,‚Äù Tester told me. ‚ÄúBut then we submitted the report to the Department of Energy. And they did nothing. It was crazy.‚Äù He was still visibly dismayed.&lt;/p&gt;
    &lt;p&gt;One explanation for the lack of action is that, around that time, the U.S. went from being an oil importer to an oil exporter. This turnaround was largely due to the innovations of George Mitchell, a second-generation Greek American in Galveston, Texas, who spent years trying to extract oil and gas from the Barnett Shale formation, in North Texas, in an economically viable way. His approach synthesized hydraulic fracturing, or fracking, with horizontal drilling. Fracking involves injecting fluid down a well at high pressures, which cracks the subsurface, and the horizontal drilling augments the area of cracking. Eventually, Mitchell‚Äôs company, helped by generous tax incentives, made the economics work. Vast oil reserves became accessible. Fortunes were made. Fracking overwhelmed the renewed interest in geothermal power. But a couple of decades later there was a reversal: fracking accelerated geothermal power.&lt;/p&gt;
    &lt;p&gt;Tim Latimer, the thirty-five-year-old C.E.O. of Fervo Energy, a geothermal company founded in 2017, grew up in Riesel, Texas, a small town about fifteen miles outside Waco. After graduating from the University of Tulsa with a degree in mechanical engineering, Latimer wanted a well-paid engineering job close to home. ‚ÄúMy adviser was just, like, ‚ÄòHave you ever heard of the oil-and-gas industry?‚Äô ‚Äù he said, smiling.&lt;/p&gt;
    &lt;p&gt;As a greenhorn drilling engineer with the international mining company BHP Billiton, Latimer was put on a fracking project in the Eagle Ford Shale, in South Texas. The shale, which is a Cretaceous-era formation dense with marine fossils from when the area was an inland sea, is relatively hard and hot. ‚ÄúThe motors in our drill systems were failing early,‚Äù Latimer said. His supervisors suspected that this was because of the wells‚Äô unusually high temperatures, around a hundred and seventy-five degrees Celsius. ‚ÄúThey said, ‚ÄòCan you research what tools we could use to deal with the fact that these drilling temperatures are really high?‚Äô ‚Äù Latimer told me.&lt;/p&gt;
    &lt;p&gt;Much of the relevant work Latimer came across turned up in papers about geothermal energy. ‚ÄúI‚Äôd never heard of geothermal before,‚Äù he said. ‚ÄúI was, like, ‚ÄòWell, this seems pretty cool.‚Äô ‚Äù When Latimer read the 2006 ‚ÄúFuture of Geothermal Energy‚Äù report, including its description of the Los Alamos geothermal project, he saw parallels to his work in oil and gas. The report described two big technical challenges that were standing in the way of affordable, bountiful clean energy. One was getting drilling costs down‚Äîan area that oil and gas had made great progress in. The other was getting water flowing through hot rock that isn‚Äôt sufficiently permeable, like shale, so that you can generate steam. ‚ÄúAnd I‚Äôm just looking at the rig, being, like, ‚ÄòThis is a solved problem.‚Äô ‚Äù Generating flow where there isn‚Äôt much naturally‚Äîthat‚Äôs what hydraulic fracturing does.&lt;/p&gt;
    &lt;p&gt;Latimer reported what he had found to BHP. The shale drilling started working again, but Latimer‚Äôs imagination had shifted. In 2014, he applied to Stanford Business School with the goal of using fracking technology in geothermal wells. ‚ÄúGeothermal is an industry that, frankly, at that point in time, people had given up on as forgotten,‚Äù he said. ‚ÄúI didn‚Äôt think that was right. I was, like, ‚ÄòI‚Äôm a drilling engineer. I actually have a skill that can make a direct impact on this.‚Äô ‚Äù&lt;/p&gt;
    &lt;p&gt;In 2017, Latimer and his Stanford colleague Jack Norbeck co-founded Fervo Energy. ‚ÄúFracking‚Äù is an unpopular word. Fervo describes itself as a ‚Äúnext-generation geothermal energy developer.‚Äù Just as the fusion-energy industry avoids the phrase ‚Äúnuclear fusion,‚Äù and the term ‚Äúnatural gas‚Äù is now used for what is mostly methane, geothermal systems involving hydraulic fracturing tend to be referred to as ‚Äúenhanced‚Äù geothermal systems, or E.G.S.&lt;/p&gt;
    &lt;p&gt;In 2023, Fervo drilled a pair of demonstration wells in Nevada, proving its ideas in anticipation of scaling them up. The goal is to begin operating a five-hundred-megawatt geothermal power plant in Cape Station, Utah, with a hundred megawatts going online in 2026. This past June, Fervo drilled a four-and-a-half-kilometre appraisal well‚Äîa well for confirming predicted subsurface conditions before going all in on a site‚Äîthat reached temperatures of two hundred and seventy degrees Celsius. The well was drilled in sixteen days, remarkably fast, and faster drill times mean lowered costs. Fervo‚Äôs well design and drilling technologies are central to its hopes, and have helped it raise more than eight hundred million dollars in investment capital. Most everyone I spoke to seems to be rooting for Fervo, albeit with some skepticism. The Utah site is far away from a source for the large amounts of water that will be required, for instance. There are more technical points of concern, too. Fracking can induce seismic activity, so the siting of wells is an important consideration. Enthusiasts see these as solvable problems. And Fervo is not alone in showing promise in the use of fracking to access geothermal power. At the end of October, Mazama Energy demonstrated a pilot E.G.S. in Newberry, Oregon, that works at an even higher temperature: above three hundred degrees Celsius. For now, though, E.G.S. is still a kind of wildcat proposition. ‚ÄúI think the big question is: Who are the next nine Fervos?‚Äù Roland Horne, a professor of energy science and engineering at Stanford, who has studied geothermal energy for about fifty years, said. ‚ÄúFervo has expanded tremendously, they‚Äôre a nearly two-hundred-person company, but they don‚Äôt have the wherewithal to do a gigawatt project yet.‚Äù&lt;/p&gt;
    &lt;p&gt;Fervo pitches itself as a landing place for oil-and-gas workers. ‚ÄúI‚Äôve spent a lot of my life and career in small towns where the largest economic driver is oil and gas,‚Äù Latimer said. Geothermal means jobs in drilling: engineers, geologists, project managers. Barry Smitherman, who has worked as an oil-and-gas regulator in Texas and as the head of a utility company, told me, ‚ÄúWe‚Äôve been drilling oil and gas wells in Texas for over a hundred years. We‚Äôve drilled over a million wells. We know what the world looks like below the surface in Texas.‚Äù&lt;/p&gt;
    &lt;p&gt;In February, 2021, Winter Storm Uri left most of Texas without power for days. Not long afterward, Smitherman was asked to speak to state legislators about what went wrong and what needed to change. Soon he got a call from a foundation set up by George Mitchell and his wife, Cynthia. (George, who died in 2013, is known as the ‚Äúfather of fracking.‚Äù) The Mitchell Foundation wanted Smitherman to help start a local organization that would advocate on behalf of geothermal energy. He co-founded the Texas Geothermal Energy Alliance in 2022. During his long career in energy, Smitherman said, ‚Äúwe never had a conversation about geothermal. No one had brought it to my attention.‚Äù Smitherman had a series of meetings with people from geothermal startups, oil-and-gas companies, the Sierra Club, and utility companies. ‚ÄúWhat we‚Äôve always said around energy is that you need three legs‚Äîreliable, clean, and cheap. Those are the three legs of the stool. The old saying was ‚ÄòI can give you two of three, but I can‚Äôt give you all three,‚Äô ‚Äù he said. ‚ÄúBut, as we began to look at geothermal, it really began to look like it had all three‚Äîlow to no carbon, 24/7, and, as the cost curve comes down, eventually, cheap. It really began to look like this unicorn resource.‚Äù&lt;/p&gt;
    &lt;p&gt;Tester now teaches at Cornell, near the Finger Lakes, where in the winter Buttermilk and Taughannock Falls turn to blue ribbons of ice. ‚ÄúIf we look at the country and say our goal is ultimately to be much more sustainable with respect to our carbon footprint, you can‚Äôt ignore heating,‚Äù Tester said. Around thirty per cent of New York State‚Äôs carbon footprint can be attributed to heating and cooling buildings, a figure that is not far from the worldwide average. ‚ÄúA lot of it is for space heating and water heating, but also for low-temperature food processing, things like that,‚Äù Tester added. The excitement about geothermally generated electricity can obscure the thing that geothermal technology is, arguably, best suited to provide. ‚ÄúIt‚Äôs a little bit apples and oranges, and we need both electricity and heating,‚Äù Tester told me. But he went on to explain that using electricity for heating is not nearly as efficient as using heat for heating. And geothermal wells for heating, which can be relatively shallow, can work in places with no hot springs or volcanoes.&lt;/p&gt;
    &lt;p&gt;Midtown Manhattan, for example: as part of a major renovation completed in 2017, ten geothermal wells were dug beneath St. Patrick‚Äôs Cathedral. Some of the wells are less than a hundred metres deep, while others extend more than six hundred and fifty metres, more than ten times deeper than Manhattan‚Äôs deepest subway tunnel‚Äîand yet much shallower than the wells needed for a geothermal power plant. These wells carry warmth into the cathedral in winter, and out of the cathedral in summer, and do so with less noise and vibration than typical HVAC systems. The main issue is the upfront cost. When the cathedral‚Äôs system was built, it felt radical. One of the lead engineers on the project, Paul Boyce, of P. W. Grosser Consulting, told me that the demand for geothermal heating systems has grown dramatically since then. P.W.G.C.‚Äôs current geothermal projects include the Mastercard headquarters, in Purchase, New York, and the Obama Presidential Center, in Chicago. In Greenpoint, Brooklyn, an eight-hundred-and-thirty-four-unit apartment complex that‚Äôs under construction has its heating and cooling provided through three hundred boreholes, none much deeper than about a hundred and fifty metres. The system was put in by Geosource Energy, a geothermal company started in 2004.&lt;/p&gt;
    &lt;p&gt;But those projects provide geothermal energy building by building, not district by district. ‚ÄúI wish we were looking at how we plan our cities,‚Äù Tester said. ‚ÄúIt‚Äôs crazy that heating, electricity, cable, water‚Äîthese are all managed separately.‚Äù He is now in the midst of a research project that aims to demonstrate the feasibility of an ambitious geothermal system to serve Cornell‚Äôs seven-hundred-and-forty-five-acre campus, something close to what downtown Reykjav√≠k has‚Äîbut without the aid of close-to-the-surface magma. In the summer of 2022, a rig set up not far from Cornell‚Äôs School of Veterinary Medicine drilled for sixty-five days through layers of shale, limestone, and sandstone, passing beyond the geologic time of the dinosaurs to a crystalline basement dating to the Proterozoic eon, more than five hundred million years ago. This created the Cornell University Borehole Observatory (CUBO). In Iceland, if you dig down this deep, the temperatures could easily be four hundred degrees Celsius; in New York, the rocks are cooler, but the Cornell project needs to reach only eighty to ninety degrees Celsius. As CUBO was drilled, rock samples from each depth were analyzed, and the surrounding natural fracture systems were mapped. If CUBO secures more funding, the next stage will be to drill a pair of wells, with one for injecting water to make an underground reservoir and the other to bring the heated water up.&lt;/p&gt;
    &lt;p&gt;In other geographies, geothermal energy for district heating and cooling has been accomplished with shallower wells. Mieres, Spain, a historic mining town, uses warm water from the now closed mines to supply heat to the region. Nijar, also in Spain but closer to a volcano, uses an underground fluid reservoir to heat its greenhouses. Hayden, Colorado, a former coal town, is working with Bedrock Energy, a Texas-based company started in 2022, to construct a municipal geothermal district, in the hope that reduced energy bills will attract businesses. In Framingham, Massachusetts, activists and a local energy company collaborated on a geothermal heating-and-cooling network, and near Austin, Texas, the neighborhood of Whisper Valley is putting in a similar grid. Several companies, including Bedrock and Dig Energy, are aiming to bring drilling costs down by half or more. Geothermal systems for heating and cooling individual homes remain somewhat pricey to install, but they last for decades, reduce energy bills by twenty-five to fifty per cent, and avoid reliance on the ever more burdened electrical grid. Most people I spoke with in the geothermal industry make their case for it by focussing on cost savings; the unspoken climate benefits are known to those disposed to care, and potentially off-putting to those who are not.&lt;/p&gt;
    &lt;p&gt;Some environmentalists argue that the resources given to geothermal‚Äîor to small modular nuclear plants, or to fusion‚Äîwould be better spent elsewhere. Why not just go all in on solar, wind, and batteries, which are proven, scalable technologies? To invest in more speculative solutions, the argument goes, is a moral hazard, and a cynical or na√Øve distraction that obscures the solutions available now. But this line of thinking rests on the assumption that the people or nations or agencies that would fund one kind of energy would equally fund some other kind. This tends not to be true‚Äîfunding is rarely fungible, and always capricious. One geothermal-startup founder spoke of receiving a call from a potential investor‚Äôs adviser, who said, Sorry, the managing partner wants to invest in a blimp company instead. ‚ÄúGeothermal is the least moral hazard-y of the clean-energy technologies,‚Äù Gernot Wagner, a climate economist at Columbia Business School, said. ‚ÄúAnd we are still subsidizing nuclear a thousand times more than geothermal.‚Äù&lt;/p&gt;
    &lt;p&gt;An energy future without hydrocarbons will require working flexibly with the many variables of resources, geography, and politics. ‚ÄúWe can get maybe ninety per cent of the way with solar, batteries, wind,‚Äù Leah Stokes, a professor of environmental politics at the University of California, Santa Barbara, told me. ‚ÄúBut geothermal is one of the things that can fill that gap.‚Äù Investment follows fashion‚Äîand geothermal has become fashionable‚Äîbut it‚Äôs not only investors who appear confident about geothermal. Wagner called this ‚Äúthe moment when Ph.D.s meet M.B.A.s.‚Äù&lt;/p&gt;
    &lt;p&gt;The role of geothermal becomes easier to see when looking beyond the local noise of discussions in America. ‚ÄúYou know, there‚Äôs this thing called the curse of abundance,‚Äù Agnelli Kafuwe, the principal energy officer for the Zambian government, told me. Typically, the phrase refers to countries driven into corruption and misery by their oil endowments, but Kafuwe was referring to Zambia‚Äôs seemingly boundless supply of hydroelectric energy, from power stations such as one at the Zambezi River‚Äôs Mosi-oa-Tunya, the natural feature known to most Americans as Victoria Falls. For many years, hydropower met practically all of Zambia‚Äôs energy needs, even powering its lucrative copper mines.&lt;/p&gt;
    &lt;p&gt;But the country‚Äôs population grew rapidly, and in 2015 a severe drought hit, forcing Zambia to turn to diesel to make up the shortfall in hydropower. Mosi-oa-Tunya looked less like a world-renowned cataract than like dry, rocky cliffs. There wasn‚Äôt enough water to keep the hydropower plants running properly. Lengthy blackouts became common. In 2024, a new drought arrived‚Äîthe worst in at least a century‚Äîand power was cut off for eighteen to twenty hours a day. As in many countries, the leadership had thought about geothermal in the nineteen-seventies but had lost interest; Zambia hadn‚Äôt needed it enough then.&lt;/p&gt;
    &lt;p&gt;In addition to copper mining, extensive salt mining occurs in northern Zambia. ‚ÄúThese mining companies, they would drill down maybe fifty metres, and guess what comes up?‚Äù Kafuwe said. ‚ÄúGeothermal steam, of a very high, very good temperature.‚Äù The country‚Äôs mining history also meant that subsurface maps of its territory already existed‚Äîuseful for planning geothermal wells. One former mining-company head, Peter Vivian-Neal, now heads Kalahari GeoEnergy, a company he founded after seeing an egg being boiled in a natural hot spring while he was on safari. The company has drilled exploratory wells, done flow tests, well tests, and modelling‚Äîit aims to have a demonstration power plant running soon. Vivian-Neal is optimistic that a successful demonstration will bring in more investment. ‚ÄúWe could not have got to where we got today if my family hadn‚Äôt put in the money to start with,‚Äù he told me. ‚ÄúBut I‚Äôm quite sure that the next person will find it easier. They‚Äôll say, ‚ÄòOh, yes, look, Kalahari has made this a success, therefore we‚Äôre going to make it a success, and we‚Äôll do it even faster.‚Äô ‚Äù ‚ô¶&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953568</guid><pubDate>Mon, 17 Nov 2025 13:55:53 +0000</pubDate></item><item><title>Replicate is joining Cloudflare</title><link>https://replicate.com/blog/replicate-cloudflare</link><description>&lt;doc fingerprint="3552db724b9eaa89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Replicate is joining Cloudflare&lt;/head&gt;
    &lt;p&gt;Big news: We‚Äôre joining Cloudflare.&lt;/p&gt;
    &lt;p&gt;Replicate‚Äôs going to carry on as a distinct brand, and all that‚Äôll happen is that it‚Äôs going to get way better. It‚Äôll be faster, we‚Äôll have more resources, and it‚Äôll integrate with the rest of Cloudflare‚Äôs Developer Platform.&lt;/p&gt;
    &lt;p&gt;The API isn‚Äôt changing. The models you‚Äôre using today will keep working. If you‚Äôve built something on Replicate, it‚Äôll keep running just like it does now.&lt;/p&gt;
    &lt;p&gt;So, why are we doing this?&lt;/p&gt;
    &lt;p&gt;At Replicate, we‚Äôre building the primitives for AI: the tools and abstractions that let software developers use AI without having to understand all the complex stuff underneath.&lt;/p&gt;
    &lt;p&gt;We started with Cog, an open-source tool which defines a standard format for what a model is. Then, we created Replicate, a platform where people can share models and run them with an API. We‚Äôve defined what a model is, how you publish it, how you run it, how you get data in and out.&lt;/p&gt;
    &lt;p&gt;These abstractions are like the low-level primitives of an operating system. But what‚Äôs interesting is that these primitives are running in the cloud. They have to ‚Äî they need specialized GPUs and clusters to scale up in production. It‚Äôs like a distributed operating system for AI, running in the cloud. In other words, the network is the computer.&lt;/p&gt;
    &lt;p&gt;Who has the best network? Cloudflare.&lt;/p&gt;
    &lt;p&gt;Cloudflare has built so many other parts of this operating system. Workers is the perfect platform for running agents and glue code. Durable Objects for managing state, R2 for storing files, WebRTC for streaming media.&lt;/p&gt;
    &lt;p&gt;Now that we‚Äôve got these low-level abstractions, we can build higher-level abstractions. Ways to orchestrate models and build agents. Ways to run real-time models, or run models on the edge.&lt;/p&gt;
    &lt;p&gt;This is why we‚Äôre joining Cloudflare.&lt;/p&gt;
    &lt;p&gt;For my whole career, I‚Äôve looked up to Cloudflare. How they built a product for developers, and turned that into a huge enterprise business. It‚Äôs the only public company that actually gets developers and knows how to build good products for them.&lt;/p&gt;
    &lt;p&gt;Cloudflare is the default for building web apps. From day one of Replicate, when we were building a prototype to apply to Y Combinator, we put Cloudflare in front of it.&lt;/p&gt;
    &lt;p&gt;Together, we‚Äôre going to become the default for building AI apps.&lt;/p&gt;
    &lt;p&gt;Check out the official announcement on Cloudflare‚Äôs blog for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953702</guid><pubDate>Mon, 17 Nov 2025 14:11:57 +0000</pubDate></item><item><title>Jeff Bezos Creates A.I. Startup Where He Will Be Co-Chief Executive</title><link>https://www.nytimes.com/2025/11/17/technology/bezos-project-prometheus.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953883</guid><pubDate>Mon, 17 Nov 2025 14:31:21 +0000</pubDate></item></channel></rss>