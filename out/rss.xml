<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 12 Dec 2025 17:10:30 +0000</lastBuildDate><item><title>Almond (YC X25) Is Hiring SWEs and MechEs</title><link>https://www.ycombinator.com/companies/almond-2/jobs</link><description>&lt;doc fingerprint="ce8988d79ddffda5"&gt;
  &lt;main&gt;
    &lt;p&gt;Robots designed for the era of AI&lt;/p&gt;
    &lt;p&gt;Our mission is to free humans from physical labor with robotics.&lt;/p&gt;
    &lt;p&gt;We imagine a future where robots handle the essential, repetitive work and humans are free to create, connect, and pursue what truly matters to them.&lt;/p&gt;
    &lt;p&gt;To build that future we’re starting from the ground up with hardware. Our first product is a California-designed and assembled humanoid arm. Surrounding it, we’re developing advanced controls, intuitive data collection, and a full AI stack that makes deployment effortless in real industrial environments. We’re proving it on our own assembly line first.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46237081</guid><pubDate>Thu, 11 Dec 2025 21:00:10 +0000</pubDate></item><item><title>Show HN: Autofix Bot – Hybrid static analysis and AI code review agent</title><link>https://news.ycombinator.com/item?id=46237358</link><description>&lt;doc fingerprint="b4704ddb15958aab"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi there, HN! We’re Jai and Sanket from DeepSource (YC W20), and today we’re launching Autofix Bot, a hybrid static analysis + AI agent purpose-built for in-the-loop use with AI coding agents.&lt;/p&gt;
      &lt;p&gt;AI coding agents have made code generation nearly free, and they’ve shifted the bottleneck to code review. Static-only analysis with a fixed set of checkers isn’t enough. LLM-only review has several limitations: non-deterministic across runs, low recall on security issues, expensive at scale, and a tendency to get ‘distracted’.&lt;/p&gt;
      &lt;p&gt;We spent the last 6 years building a deterministic, static-analysis-only code review product. Earlier this year, we started thinking about this problem from the ground up and realized that static analysis solves key blind spots of LLM-only reviews. Over the past six months, we built a new ‘hybrid’ agent loop that uses static analysis and frontier AI agents together to outperform both static-only and LLM-only tools in finding and fixing code quality and security issues. Today, we’re opening it up publicly.&lt;/p&gt;
      &lt;p&gt;Here’s how the hybrid architecture works:&lt;/p&gt;
      &lt;p&gt;- Static pass: 5,000+ deterministic checkers (code quality, security, performance) establish a high-precision baseline. A sub-agent suppresses context-specific false positives.&lt;/p&gt;
      &lt;p&gt;- AI review: The agent reviews code with static findings as anchors. Has access to AST, data-flow graphs, control-flow, import graphs as tools, not just grep and usual shell commands.&lt;/p&gt;
      &lt;p&gt;- Remediation: Sub-agents generate fixes. Static harness validates all edits before emitting a clean git patch.&lt;/p&gt;
      &lt;p&gt;Static solves key LLM problems: non-determinism across runs, low recall on security issues (LLMs get distracted by style), and cost (static narrowing reduces prompt size and tool calls).&lt;/p&gt;
      &lt;p&gt;On the OpenSSF CVE Benchmark [1] (200+ real JS/TS vulnerabilities), we hit 81.2% accuracy and 80.0% F1; vs Cursor Bugbot (74.5% accuracy, 77.42% F1), Claude Code (71.5% accuracy, 62.99% F1), CodeRabbit (59.4% accuracy, 36.19% F1), and Semgrep CE (56.9% accuracy, 38.26% F1). On secrets detection, 92.8% F1; vs Gitleaks (75.6%), detect-secrets (64.1%), and TruffleHog (41.2%). We use our open-source classification model for this. [2]&lt;/p&gt;
      &lt;p&gt;Full methodology and how we evaluated each tool: https://autofix.bot/benchmarks&lt;/p&gt;
      &lt;p&gt;You can use Autofix Bot interactively on any repository using our TUI, as a plugin in Claude Code, or with our MCP on any compatible AI client (like OpenAI Codex).[3] We’re specifically building for AI coding agent-first workflows, so you can ask your agent to run Autofix Bot on every checkpoint autonomously.&lt;/p&gt;
      &lt;p&gt;Give us a shot today: https://autofix.bot. We’d love to hear any feedback!&lt;/p&gt;
      &lt;p&gt;---&lt;/p&gt;
      &lt;p&gt;[1] https://github.com/ossf-cve-benchmark/ossf-cve-benchmark&lt;/p&gt;
      &lt;p&gt;[2] https://huggingface.co/deepsource/Narada-3.2-3B-v1&lt;/p&gt;
      &lt;p&gt;[3] https://autofix.bot/manual/#terminal-ui&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46237358</guid><pubDate>Thu, 11 Dec 2025 21:24:34 +0000</pubDate></item><item><title>Nokia N900 Necromancy</title><link>https://yaky.dev/2025-12-11-nokia-n900-necromancy/</link><description>&lt;doc fingerprint="b6230073e3ef3f7c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nokia N900 Necromancy&lt;/head&gt;
    &lt;p&gt;Building a fake battery, adding a USB-C port, booting from SD card, and giving a new life to a classic Linux smartphone.&lt;/p&gt;
    &lt;p&gt;My friend Dima sent me his old-school classic Nokia N900. The battery is very old, and it does not boot as-is. So naturally, I wanted to see if I can resurrect it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 0: Is such a thing even possible?&lt;/head&gt;
    &lt;p&gt;Yes it is! (Unless there are other hardware issues)&lt;/p&gt;
    &lt;p&gt;I ran a smartphone without a battery a few years ago.&lt;/p&gt;
    &lt;p&gt;I did look at BL-5J batteries for sale. Many listings say "Genuine" and "OEM", but what does that even mean for a battery for a device that is at least 10 years old? (Some later Nokia Lumia phones used the same battery) And listings with newer-looking batteries do not list the manufacture date. I don't need another old battery or a "spicy pillow".&lt;/p&gt;
    &lt;p&gt;Also, where's the fun in that?&lt;/p&gt;
    &lt;p&gt;Cut and soldered a quick prototype to connect instead of the battery. Resistors are to emulate the "normal" temperature by providing expected resistance between the third pin and ground. See link above for details.&lt;/p&gt;
    &lt;p&gt;Hooked up a large supercapacitor to the battery pins and to a +5V source. If I recall correctly, using a capacitor without additional power did not work.&lt;/p&gt;
    &lt;p&gt;And it boots!&lt;/p&gt;
    &lt;p&gt;Now, let's make something that can fit into the battery compartment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 1: Better "battery"&lt;/head&gt;
    &lt;p&gt;These supercapacitors are nice, but way too large. After searching on Mouser, I found FM0H473ZF, 47000 mF (0.047F) capacitors in a rectangular case that is only 5mm thick.&lt;/p&gt;
    &lt;p&gt;Ten of these (~0.5F) is enough to run the smartphone without dying.&lt;/p&gt;
    &lt;p&gt;Capacitor contraption (TM) arranged (using a 3D-printed template) and soldered together.&lt;/p&gt;
    &lt;p&gt;And they all fit nicely into the battery compartment. The power is provided by a wire routed through the hole for the carry loop.&lt;/p&gt;
    &lt;p&gt;Running fine! One noticeable issue is that capacitors are getting pretty warm. Probably my sloppy soldering, but no shorts that I could find.&lt;/p&gt;
    &lt;head rend="h2"&gt;â ï¸&lt;/head&gt;
    &lt;p&gt;This is where I should have stopped. At some point while messing with the "battery" and power, I managed to corrupt the internal partition and the installed OS. Not sure if this was from the sudden battery pull or from supplying +5V instead of the expected +4.2V to the battery pins. Luckily, newer Maemo Leste is intended to run from the SD card anyway, and internal storage still works, so I was able to overwrite it with the bootloader.&lt;/p&gt;
    &lt;p&gt;Bootloader setup on Maemo Wiki&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 2: Consolidating connectors&lt;/head&gt;
    &lt;p&gt;I thought it might be practical to power the "battery" through the existing USB port. Just run the +5V wire from USB to the "battery", and avoid additional wires. (If you think this is kinda stupid, you are right)&lt;/p&gt;
    &lt;p&gt;Yooo... What is happening here? Dima says "oh yeah, the USB port was re-soldered. Twice". A quick glance at the forums also confirms that USB port was poorly designed and is prone to breaking.&lt;/p&gt;
    &lt;p&gt;Just one wire from the +5V pad to the "battery". The ground is the same as the battery pin.&lt;/p&gt;
    &lt;p&gt;Assembled everything back, routed and soldered the +5V wire, and added a diode to prevent the battery from feeding the USB port, and to drop the voltage to more acceptable ~4.3V.&lt;/p&gt;
    &lt;p&gt;The setup works, but the smartphone constantly shows either "Charging", or "Device using more power than it is receiving from the PC. Charging with a compatible charger is recommended", with battery gauge going crazy.&lt;/p&gt;
    &lt;p&gt;And then, the power just cut out.&lt;/p&gt;
    &lt;p&gt;Yeah, this was not a great idea. Let's see what happened.&lt;/p&gt;
    &lt;p&gt;USB +5V wire detached itself from the port. I presume this is from either the high current, age, stress, or corrosion.&lt;/p&gt;
    &lt;p&gt;However, when I opened the smartphone up, I... ripped off the +5V pad. (dark circle in lower right on the photo)&lt;/p&gt;
    &lt;p&gt;Fuck.&lt;/p&gt;
    &lt;p&gt;After reading some N900 forums, that +5V pad is a common place to connect the replacement USB port to (which was done here), but... that is the ONLY +5V connection on the board besides the pads under the USB port itself.&lt;/p&gt;
    &lt;p&gt;FUCK!&lt;/p&gt;
    &lt;head rend="h2"&gt;ðª¦&lt;/head&gt;
    &lt;p&gt;RIP Nokia N900. I tried to resurrect you, but instead, I killed your OS and ripped out the USB port wires.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 3: Radical replacements&lt;/head&gt;
    &lt;p&gt;To be fair, N900 is far from dead. I already flashed u-boot, was able to boot from SD card, and do not plan to use internal storage otherwise. Power can be supplied entirely through the new "battery". So technically, I do not need the USB functionality for the smartphone itself, just to power the "battery". At this point, I might as well replace the port with USB-C. Because why not.&lt;/p&gt;
    &lt;p&gt;Approximate placement of the new USB port.&lt;/p&gt;
    &lt;p&gt;The location of the original port is not very convenient. It is sandwiched between the main board and the SD card reader (lower left on the photo). SD card reader is also attached by a permanently-attached ribbon (i.e. nearly irreplaceable).&lt;/p&gt;
    &lt;p&gt;First, I used a small file to make the micro-USB-shaped hole on the smartphone body fit the USB-C shape. Then, I took a small 6-pin USB-C port, cut and sanded down its plastic parts to make it fit in the original spot. It is still slightly (~0.25mm) taller than the original, but I cannot make it any slimmer.&lt;/p&gt;
    &lt;p&gt;I tried to attach the USB-C port to the board in the correct place by carefully assembling the board, port and SD card reader into the body, and using small drops of glue to lightly affix the edge of the USB port (that I could reach) to the main board. The intent was to wait for glue to cure, take everything back apart and glue the port in its now-correct position for good. This took several tries but did not really work, as the port got detached while removing the main board every time, and the the superglue I used left lots of residue but did not adhere. Luckily, the tight fit and the shape of the USB-C port hold it in place mechanically quite well.&lt;/p&gt;
    &lt;p&gt;USB-C with +5V and ground attached.&lt;/p&gt;
    &lt;p&gt;Originally, I planned to solder all 6 pins and add 5.1 Ohm pull-down resistors to CC1 and CC2 pins (for full power delivery functionality). But there is simply not enough space to route the wires, the narrow valley between the chips (in the lower right of the photo) barely fits 3, and I did not have anything thinner on hand.&lt;/p&gt;
    &lt;p&gt;Nokia N900 with a USB-C port! Looks pretty nice IMO.&lt;/p&gt;
    &lt;p&gt;Since I did not solder the pull-down resistors, this USB-C port could only be powered by a "dumb" USB-A-to-USB-C cable, at default 0.5A. Chargers with power delivery functionality cannot identify such USB-C ports, and will not provide power at all. (This is also an issue with some handheld consoles such as RGB30)&lt;/p&gt;
    &lt;p&gt;The two wires are routed to the battery compartment through a very convenient opening in the metal frame, crimped and inserted into a DuPont connector.&lt;/p&gt;
    &lt;p&gt;Back to the battery. The capacitor contraption I built before works, but was kind of flimsy, and does not have any more space for a DuPont connector. Also, I would rather use a single capacitor, but it still has to fit. Since the original battery is unusable, I might as well try to salvage it, too.&lt;/p&gt;
    &lt;p&gt;Take off the sticker (that tells you not to do so :). The top BCM piece is held to the main battery body by two tiny screws (hidden under some crumbly compound) on each end, double-sided sticker, and a single lead in the middle.&lt;/p&gt;
    &lt;p&gt;Battery Control Module. Interestingly, for this battery, the body is the positive terminal. So the positive lead connects the battery body and the positive pin directly, while the negative lead goes thorough some control circuitry. Attaching a capacitor to these battery terminals should be sufficient.&lt;/p&gt;
    &lt;p&gt;Since I have a 3D printer, and once you have one, every problem can be solved by printing stuff, I printed the new "battery" to accommodate a large capacitor, diode (for voltage drop), wires, DuPont connectors, and the original battery's BCM.&lt;/p&gt;
    &lt;p&gt;N900 with a new "battery". Fits really tight, and only 0.25-0.5mm too tall, so the cover still snaps closed.&lt;/p&gt;
    &lt;p&gt;Boots without problems. Since the attached capacitor is pretty large, it can take a minute or two to charge it to an acceptable level (~4.0V) with a 0.5A current.&lt;/p&gt;
    &lt;p&gt;Nokia N900 enjoying its new life as an online radio device using Open Media Player.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239177</guid><pubDate>Fri, 12 Dec 2025 00:04:29 +0000</pubDate></item><item><title>CRISPR fungus: Protein-packed, sustainable, and tastes like meat</title><link>https://www.isaaa.org/kc/cropbiotechupdate/article/default.asp?ID=21607</link><description>&lt;doc fingerprint="663d925d0c255a11"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;CRISPR Fungus: Protein-Packed, Sustainable, and Tastes Like Meat&lt;/head&gt;November 26, 2025&lt;table&gt;&lt;row/&gt;&lt;/table&gt;&lt;p&gt;Researchers have successfully used CRISPR gene editing technology to create a fungi strain that is highly efficient, more nutritious, and significantly more sustainable than its natural counterpart. The fungus Fusarium venenatum already stands out for its meat-like flavor and texture, leading to its approval for food use in several countries. This breakthrough, published in the journal Trends in Biotechnology, addresses the need for better, more environmentally friendly alternatives to conventional animal agriculture, which accounts for about 14% of global greenhouse gas emissions.&lt;/p&gt;&lt;p&gt;The scientists, led by corresponding author Xiao Liu of Jiangnan University, used CRISPR to remove two specific genes. The first modification, eliminating a gene for chitin synthase, resulted in thinner fungal cell walls. This change is crucial as it makes the fungal protein easier for humans to digest and increases its bioavailability. The second change involved removing the pyruvate decarboxylase gene, which optimized the fungus's metabolism. This fine-tuning made the new strain, called FCPD, more productive, requiring 44% less sugar to produce the same amount of protein and doing so 88% faster than the original strain.&lt;/p&gt;&lt;p&gt;When scaled up, FCPD production showed a lower environmental footprint regardless of the manufacturing location, reducing greenhouse gas emissions by up to 60% over its life cycle compared to traditional fungal protein production. Furthermore, compared to chicken production in China, the new myoprotein requires 70% less land and reduces the risk of freshwater pollution by 78%. According to the researchers, this type of gene-edited food can help meet global food demands without the substantial environmental costs associated with conventional farming, representing a major advancement in the field of sustainable food technology.&lt;/p&gt;&lt;p&gt;For more details, read this article or download the open-access paper.&lt;/p&gt;&lt;table&gt;&lt;row/&gt;&lt;/table&gt;&lt;head rend="h3"&gt;You might also like:&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Experts Highlight Hybrid Foods as Meat Alternatives&lt;/item&gt;&lt;item&gt;Gene Editing of Edible Fungus Produces Healthier Foods&lt;/item&gt;&lt;item&gt;Experts Say Effective Communication Can Promote Consumer Acceptance of Cultured Meat&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Biotech Updates is a weekly newsletter of ISAAA, a not-for-profit organization. It is distributed for free to over 22,000 subscribers worldwide to inform them about the key developments in biosciences, especially in biotechnology. Your support will help us in our mission to feed the world with knowledge. You can help by donating as little as $10.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;head rend="h3"&gt;See more articles:&lt;/head&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Plant&lt;/head&gt;&lt;/item&gt;&lt;item&gt;CRISPR Alters Chromosome Numbers in Plants&lt;/item&gt;&lt;item&gt;Precision Breeding Act Now Live in the UK&lt;/item&gt;&lt;item&gt;Philippine Regulators Work on Refining Risk Assessment Report on GM Plant and Plant Products&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Animal&lt;/head&gt;&lt;/item&gt;&lt;item&gt;Targeted Inheritance of Sex Offers a New Method to Improve Animal Breeding&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Food&lt;/head&gt;&lt;/item&gt;&lt;item&gt;Single Gene Discovery Promises Better Tea Taste and Yield&lt;/item&gt;&lt;item&gt;CRISPR Fungus: Protein-Packed, Sustainable, and Tastes Like Meat&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Environment&lt;/head&gt;&lt;/item&gt;&lt;item&gt;UN Climate Change Conference Highlights Role of Agrifood Systems in Climate Action&lt;/item&gt;&lt;item&gt;University of Waterloo Researchers Turn to Biotechnology to Combat Plastic Pollution&lt;/item&gt;&lt;item&gt;&lt;lb/&gt;Read the latest:&lt;/item&gt;&lt;item&gt;Biotech Updates (December 10, 2025)&lt;/item&gt;&lt;item&gt;Gene Editing Supplement (November 26, 2025)&lt;/item&gt;&lt;item&gt;Gene Drive Supplement (February 22, 2023)&lt;/item&gt;&lt;item&gt;&lt;lb/&gt;Subscribe to BU:&lt;/item&gt;&lt;item&gt;Share&lt;/item&gt;&lt;item&gt;Tweet&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239629</guid><pubDate>Fri, 12 Dec 2025 00:59:46 +0000</pubDate></item><item><title>Google de-indexed Bear Blog and I don't know why</title><link>https://journal.james-zhan.com/google-de-indexed-my-entire-bear-blog-and-i-dont-know-why/</link><description>&lt;doc fingerprint="150021f738f60ba3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google De-Indexed My Entire Bear Blog and I Don’t Know Why&lt;/head&gt;
    &lt;p&gt;Preamble: The whole affair is Google’s fault and not Bear Blog’s. Huge thanks to Herman—Bear Blog’s founder and dev—for his patience and help.&lt;/p&gt;
    &lt;p&gt;A month after I started my first Bear blog at blog.james-zhan.com, my blog was entirely de-indexed by Google for no apparent reason:&lt;/p&gt;
    &lt;p&gt;I have since migrated to journal.james-zhan.com (you are on it right now) and redirected all links from blog.james-zhan.com accordingly, but to this day, I don’t understand what happened, and so I’m putting this post out there to see if perhaps anyone could shed some light—you are welcome to email me or leave a comment at the bottom of the post.&lt;/p&gt;
    &lt;p&gt;Let me backtrack and show you how it all went down.&lt;/p&gt;
    &lt;head&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;At first, all was well&lt;lb/&gt; Where it started to go wrong&lt;lb/&gt; Was that a coincidence or the cause of the issue?&lt;lb/&gt; It got worse&lt;lb/&gt; Extensive troubleshooting&lt;lb/&gt; Suspect #1: Something’s up with the domain&lt;lb/&gt; Suspect #2: Quality of blog content&lt;lb/&gt; Suspect #3: Lack of internal linking&lt;lb/&gt; Other suspects, eliminated with Herman’s help (thank you Herman!)&lt;lb/&gt; My blog was properly indexed by other search engines&lt;lb/&gt; What I ended up doing&lt;/p&gt;
    &lt;head rend="h2"&gt;At first, all was well&lt;/head&gt;
    &lt;p&gt;My blog went live on Oct 4 and I published a lengthy, well-research opinion piece commenting on a recent event.&lt;/p&gt;
    &lt;p&gt;Because of that, I wanted the article to show up on Google ASAP so that when people searched about the event, maybe my article would come up. I knew that it could take a while for Google to naturally crawl and index a new site, so to accelerate the process, I went on Google Search Console (GSC), submitted the sitemap and requested indexing on my article.&lt;/p&gt;
    &lt;p&gt;And it worked—the next day, my blog and articles were indexed and showed up on Google if you put in the right search terms.&lt;/p&gt;
    &lt;p&gt;In GSC, you can even see that I was getting some impressions and clicks at the time from the exact topic that my opinion piece was about. Great!&lt;/p&gt;
    &lt;p&gt;From then on, every time I published a new post, I would go to GSC and request indexing for the post URL, and my post would be on Google search results shortly after, as expected.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where it started to go wrong&lt;/head&gt;
    &lt;p&gt;On Oct 14, as I was digging around GSC, I noticed that it was telling me that one of the URLs weren’t indexed. I thought that was weird, and not being very familiar with GSC, I went ahead and clicked the “Validate” button.&lt;/p&gt;
    &lt;p&gt;Only after did I realized that URL was the RSS feed subscribe link, &lt;code&gt;https://blog.james-zhan.com/feed/?type=rss&lt;/code&gt;, which wasn’t even a page so it made sense that it hadn’t been indexed, but it was too late and there was no way for me to stop the validation.&lt;/p&gt;
    &lt;p&gt;I received an email from GSC telling me it was validating that 1 page with indexing issues:&lt;/p&gt;
    &lt;p&gt;Four days later, on Oct 20, I received an email from GSC saying “Some fixes failed for Page indexing issues on site https://blog.james-zhan.com/” and when I searched “site:blog.james-zhan.com,” I saw that all but one of my blog posts had been de-indexed:&lt;/p&gt;
    &lt;p&gt;All of them showed the same reason:&lt;/p&gt;
    &lt;p&gt;“Page is not indexed: Crawled – currently not indexed”&lt;/p&gt;
    &lt;p&gt;Confused, I poked around GSC to see if it showed me why, and I couldn’t find anything useful, so I resubmitted the sitemap for good measure, and clicked “Validate” again.&lt;/p&gt;
    &lt;p&gt;I even requested indexing for all the individual blog post URLs and that didn’t do anything.&lt;/p&gt;
    &lt;p&gt;As of the publishing of this post, the validation status is still “Started” (it’s been nearly 20 days).&lt;/p&gt;
    &lt;head rend="h2"&gt;Was that a coincidence or the cause of the issue?&lt;/head&gt;
    &lt;p&gt;As I was troubleshooting, I noticed that the day that I initiated the validation for the first time (Oct 14) was the same day that all but one of my blog posts got de-indexed:&lt;/p&gt;
    &lt;p&gt;Did my accidental attempt to make GSC index &lt;code&gt;https://blog.james-zhan.com/feed/?type=rss&lt;/code&gt; cause some kind of glitch, thereby de-indexing the rest of the blog?&lt;/p&gt;
    &lt;p&gt;I don’t get why it would, but it’s weird that the two events happened on the same day.&lt;/p&gt;
    &lt;head rend="h2"&gt;It got worse&lt;/head&gt;
    &lt;p&gt;While this was going on, I continued to post a few articles, and you can see that all the new posts faced the “Page is not indexed: Crawled – currently not indexed” error:&lt;/p&gt;
    &lt;p&gt;And then on Nov 3, I discovered that the remaining, single blog post that had been indexed just got de-indexed as well:&lt;/p&gt;
    &lt;p&gt;So basically, no one could find my blog on Google.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extensive troubleshooting&lt;/head&gt;
    &lt;p&gt;I’m not a web dev or programmer, but I tried my best to cover as much ground as possible in my troubleshooting to narrow down the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #1: Something’s up with the domain&lt;/head&gt;
    &lt;p&gt;The root domain, james-zhan.com, was from GoDaddy. I’ve had this domain for many years and I’ve used it on different sites and never had an issue with Google’s indexing.&lt;/p&gt;
    &lt;p&gt;For example, just this year, I created a new subdomain with it and that’s been indexed by Google.&lt;/p&gt;
    &lt;p&gt;I also don’t touch any advanced configuration with DNS records or what have you—I don’t have knowledge in that stuff, so it’s unlikely I somehow screwed up something in GoDaddy.&lt;/p&gt;
    &lt;p&gt;But just to be sure it wasn’t some wonky thing going on specifically with the Bear blog + GoDaddy combo, I created another Bear blog with the subdomain www.james-zhan.com.&lt;/p&gt;
    &lt;p&gt;This one shows up on Google no problem.&lt;/p&gt;
    &lt;p&gt;Conclusion: Domain wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #2: Quality of blog content&lt;/head&gt;
    &lt;p&gt;Whenever people discuss the indexing of their website in online forums, they always talk about the quality of the content being a huge factor. They say that your site isn’t indexed or isn’t ranked highly because your site doesn’t have much content, your content is low effort, or something like that.&lt;/p&gt;
    &lt;p&gt;First, I’m not worried about ranking—I just want my blog to be properly indexed.&lt;/p&gt;
    &lt;p&gt;Second, the issue couldn’t be the quality or the quantity of the content. I came across some other pretty barebones Bear blogs that don’t have much content, and looked them up on Google, and they showed up in the results just fine.&lt;/p&gt;
    &lt;p&gt;An example: Phong’s blog. It’s a very minimalist blog with only 6 posts (of great quality) and it shows up on Google search.&lt;/p&gt;
    &lt;p&gt;Conclusion: Quality or quantity of content wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #3: Lack of internal linking&lt;/head&gt;
    &lt;p&gt;I read about how the structure of a site can play a role in Google’s indexing.&lt;/p&gt;
    &lt;p&gt;Some say that if your blog posts’ URLs are all “orphaned,” like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;domain.com/post-title-1&lt;/item&gt;
      &lt;item&gt;domain.com/post-title-2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;…instead of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;domain.com/blog/post-title-1&lt;/item&gt;
      &lt;item&gt;domain.com/blog/post-title-2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Allegedly, that might cause Google to not index your posts. By default, when you publish a post on Bear Blog, the blog post’s path isn’t preceded by “blog/.”&lt;/p&gt;
    &lt;p&gt;So I went around and checked the post URLs of other Bear blogs and saw that none of them had “/blog/” in them, and those blogs were indexed just fine. I also highly doubt it’s a real issue; otherwise, it wouldn’t be the default behaviour on Bear Blog.&lt;/p&gt;
    &lt;p&gt;Conclusion: Lack of internal linking wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other suspects, eliminated with Herman’s help (thank you Herman!)&lt;/head&gt;
    &lt;p&gt;I reached out to Herman with all the details and asked him for help. Of course, he responded promptly and helped me troubleshoot to identify the cause.&lt;/p&gt;
    &lt;p&gt;He was able to confirm and the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GoDaddy and DNS weren’t the cause&lt;/item&gt;
      &lt;item&gt;My bear blog had nothing that would prevent Google from indexing&lt;/item&gt;
      &lt;item&gt;HTML/CSS doesn’t affect SEO/indexing&lt;list rend="ul"&gt;&lt;item&gt;I had the following CSS code to put the tags above the blog post title, but Herman said this was fine&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;/* --- Move tags above the title --- */ main { display: flex; flex-direction: column; } /* Style and reposition the tags */ main &amp;gt; p.tags { order: -1; /* Moves tags above the title */ margin: 0 0 0.6rem 0; font-size: 0.9em; letter-spacing: 0.02em; color: var(--heading-color); opacity: 0.8; } /* Keep the title below tags */ main &amp;gt; h1 { order: 0; }&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I just wanted to take a moment to express my gratitude to Herman for investigating this with me. My emails to him were pretty elaborate with troubleshooting steps I had taken along with many screenshots. He took the time to fully understand the whole issue and even triple-checked my site to make sure everything was sound.&lt;/p&gt;
    &lt;p&gt;It was a refreshing tech support experience, and made me love Bear Blog as a platform just that much more.&lt;/p&gt;
    &lt;head rend="h3"&gt;My blog was properly indexed by other search engines&lt;/head&gt;
    &lt;p&gt;I don’t even have to use “site:”—just by searching “James Zhan blog,” both my blog and my www.james-zhan.com site show up in other search engines:&lt;/p&gt;
    &lt;p&gt;DuckDuckGo:&lt;/p&gt;
    &lt;p&gt;Bing:&lt;/p&gt;
    &lt;p&gt;Brave:&lt;/p&gt;
    &lt;p&gt;So there’s definitely nothing wrong on a technical level with my blog that would prevent Google from indexing it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I ended up doing&lt;/head&gt;
    &lt;p&gt;I copied my blog over to a different subdomain (you are on it right now), moved my domain from GoDaddy to Porkbun for URL forwarding, and set up URL forwarding with paths so any blog post URLs I posted online will automatically be redirected to the corresponding blog post on this new blog.&lt;/p&gt;
    &lt;p&gt;I also avoided submitting the sitemap of the new blog to GSC. I’m just gonna let Google naturally index the blog this time. Hopefully, this new blog won’t run into the same issue.&lt;/p&gt;
    &lt;p&gt;At this point, I’m no longer trying to resolve the issue, but just out of curiosity, I do want to know what the hell happened there. I’d had a previous site on GSC to track traffic for many years and never had such an issue.&lt;/p&gt;
    &lt;p&gt;If any of you have any guesses, I’d love to hear them (email me or leave a comment below)!&lt;/p&gt;
    &lt;p&gt;Subscribe to my blog via email or RSS feed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239752</guid><pubDate>Fri, 12 Dec 2025 01:20:05 +0000</pubDate></item><item><title>The tiniest yet real telescope I've built</title><link>https://lucassifoni.info/blog/miniscope-tiny-telescope/</link><description>&lt;doc fingerprint="77ef263a1fa05dfc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The tiniest yet real telescope I've built&lt;/head&gt;
    &lt;p&gt;A “relaxation” project, mostly drawn on planes to and from Norway this month, where I had to travel to setup a digital art installation in Kristiansand with friends from the digital art collective Lab212. It has been drawn with one major constraint: it must fit in the inner pocket of my jacket (well, one specific jacket), except for the rods.&lt;/p&gt;
    &lt;p&gt;This is a 3D-printed dobsonian telescope built around a 76mm/300mm parabolic mirror kit. While there are plenty of mini-scope models on the internet, I wanted something that looked like a dobson that went a bit too hard through the clothes dryer, but without compromise on what matters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Balance&lt;/item&gt;
      &lt;item&gt;Smooth movements&lt;/item&gt;
      &lt;item&gt;Rigidity&lt;/item&gt;
      &lt;item&gt;Collimatable&lt;/item&gt;
      &lt;item&gt;Focusable eyepiece holder&lt;/item&gt;
      &lt;item&gt;A minimum of style (entirely subjective)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Hardware&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PETG-CF filament&lt;/item&gt;
      &lt;item&gt;4mm carbon rods&lt;/item&gt;
      &lt;item&gt;M3 screws and M3x4.5x4.5 heat-set inserts&lt;/item&gt;
      &lt;item&gt;A spring&lt;/item&gt;
      &lt;item&gt;Nylon screws to collimate both the primary and secondary mirrors&lt;/item&gt;
      &lt;item&gt;4 magnets for the secondary&lt;/item&gt;
      &lt;item&gt;A bit of paraffin to lubricate the focuser&lt;/item&gt;
      &lt;item&gt;A lycra light shroud that also helps with delaying dew forming on the mirrors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The focuser follows Analog Sky’s recipe: the tube that receives the eyepiece is also the movement itself, with a rounded thread that prints extremely smoothly with very little play. No additional hardware needed - the eyepiece is self-held by the flexion of plastic fins.&lt;/p&gt;
    &lt;p&gt;All the holes for the rods are straight, which forces them to arch, which “locks” the structure in place.&lt;/p&gt;
    &lt;p&gt;The alt/az movements use “teflon pads” (actually gray HDPE or UHMW for furniture feet) with rubber backing, scalped and glued.&lt;/p&gt;
    &lt;p&gt;Download the 3D files on Printables • Discussion on Astrosurf&lt;/p&gt;
    &lt;p&gt;If you build it, the real trick for ease of mounting is to chamfer the carbon rods with a 1mm chamfer at both ends and seal it with CA glue. See the chamfer pic in the gallery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optical tests&lt;/head&gt;
    &lt;p&gt;Sadly, the results aren’t great. We were used to very good λ/6 or better recent buys from Aliexpress, but this one is very overcorrected. It was very smooth, with a rather good edge at the foucault, but is overcorrected by 70%. With the eyepiece I selected, putting it at 30x power, this does not show too much, and it retains its “real telescope” status. But this mirror is so small that I will not refigure it – the realuminizing costs would outweigh the entire project.&lt;/p&gt;
    &lt;p&gt;Edit dec. 12th The λ/6 aliexpress mirrors mentioned were spherical. So, great starting points to figure them, but unusable as-is. I did not yet stumble on a great parabola at a low price, and this is to be expected.&lt;/p&gt;
    &lt;p&gt;Edit as of dec. 11th : of course I did not resist re-figuring it. It now hovers around 0.9 strehl. The star test with the selected eyepiece shows nice symmetric defocused stars and I can now count individual spider web strands and distinguish the dew droplets it carries, on a nearby electrical pole, whereas I did not even see the spider web with the mirror as it was from the factory. I still need to do a proper “showable” Bath report with enough interferograms, my last test was 4 interferograms and carries a ton of noise. So it is great but I now have to get it coated, and working a mirror this small did raise a few challenges in handling it.&lt;/p&gt;
    &lt;p&gt;All test pictures below are before refiguring&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46241763</guid><pubDate>Fri, 12 Dec 2025 07:35:49 +0000</pubDate></item><item><title>Guarding My Git Forge Against AI Scrapers</title><link>https://vulpinecitrus.info/blog/guarding-git-forge-ai-scrapers/</link><description>&lt;doc fingerprint="e674b8d1d8e1a093"&gt;
  &lt;main&gt;
    &lt;p&gt;In August 2024, one of my roommates and partners messaged the apartment group chat, saying she noticed the internet was slow again at our place, and my forgejo was unable to render any page in under 15 seconds.&lt;/p&gt;
    &lt;p&gt;i investigated, thinking it would be a trivial little problem to solve. Soon enough, however, i would uncover hundreds of thousands of queries a day from thousands of individual IPs, fetching seemingly-random pages in my forge every single day, all the time.&lt;/p&gt;
    &lt;p&gt;This post summarizes the practical issues that arose as a result of the onslaught of scrapers eager to download millions of commits off of my forge, and the measures i put in place to limit the damage.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Why the forge?&lt;/head&gt;
    &lt;p&gt;In the year 2025, on the web, everything is worth being scraped. Everything that came out of the mind of a human is susceptible to be snatched under the vastest labor theft scheme in the history of mankind. This very article, the second it gets published in any indexable page, will be added to countless datasets meant to train foundational large-language models. My words, your words, have contributed infinitesimal shifts of neural-network weights underpinning the largest, most grotesque accumulation of wealth seen over the lifetime of my parents, grandparents, and their grandparents themselves.&lt;/p&gt;
    &lt;p&gt;Oh, and forges have a lot of commits. See, if you have a public repository that is publicly exposed, every file in every folder for every commit will be connected. Add other options, such as a &lt;code&gt;git blame&lt;/code&gt; on a file, and multiply it by the
number of files and commits. Add the raw download link, also multiplied by the
number of commits.&lt;/p&gt;
    &lt;p&gt;Say, hypothetically, you have a linux repository available, and only with all the commits in the &lt;code&gt;master&lt;/code&gt; branch up to the &lt;code&gt;v6.17&lt;/code&gt; tag from 2025-09-18.
That's 1,383,738 commits in the range &lt;code&gt;1da177e4c3f4..e5f0a698b34e&lt;/code&gt;. How many
files is that? Well:&lt;/p&gt;
    &lt;code&gt;count=0;
while read -r rev; do
    point=$(git ls-tree -tr $rev | wc -l);
    count=$(( $count + $point ));
    printf "[%s] %s: %d (tot: %d)\n" $(git log -1 --pretty=tformat:%cs $rev) $rev $point $count;
done &amp;lt; &amp;lt;(git rev-list "1da177e4c3f4..e5f0a698b34e");
printf "Total: $count\n";
&lt;/code&gt;
    &lt;p&gt;i ran this on the 100 commits before &lt;code&gt;v6.17&lt;/code&gt;. If you have &lt;code&gt;git ls-tree -tr $rev&lt;/code&gt;, you get both files and directories counted. If you replace it with &lt;code&gt;git ls-tree -r $rev&lt;/code&gt; only shows files. i got 72024729 files, and 76798658 files and
directories. Running on the whole history of Linux's &lt;code&gt;master&lt;/code&gt; branch yields
78,483,866,182 files, and 83,627,462,277 files and directories.&lt;/p&gt;
    &lt;p&gt;Now, for a ballpark estimate of the number of pages that can be scraped if you have a copy of Linux, apply the formula:&lt;/p&gt;
    &lt;code&gt;(Ncommits * Nfiles) * 2 + (Ncommits * Nfilesandfolders) * 2 + Ncommits * 3
&lt;/code&gt;
    &lt;p&gt;That is, applied to my hypothetical Linux repository:&lt;/p&gt;
    &lt;code&gt;78483866182 * 2 + 83627462277 * 2 + 1383738 * 3 = 324,226,808,132 pages
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;*3&lt;/code&gt; accounts for the fact that every file of every commit can be scraped
raw, and &lt;code&gt;git-blame&lt;/code&gt;'d. The second part of the
formula considers every single file or folder page. The third part accounts for
the fact that every file of every commit can be diffed with its version of
every commit (in theory). The final component considers every commit summary
page.&lt;/p&gt;
    &lt;p&gt;That gives, for me, 324 billion 226 million 808 thousand and 132 pages that can be scraped. From a single repository. Assume that every scraper agent that enters one of these repositories will also take note of every other link on the page, and report it so that other agents can scrapes them. These scrapers effectively act like early 2000s web spiders that crawled the internet to index it, except they do not care about &lt;code&gt;robots.txt&lt;/code&gt;, and they will absolutely keep
scraping new links again and again with no strategy to minimize the cost on
you, as a host.&lt;/p&gt;
    &lt;head rend="h1"&gt;# The Cost of Scraping&lt;/head&gt;
    &lt;p&gt;As i am writing the original draft of this section, the longer-term measures i put in place have been removed, so i could gather up-to-date numbers to display how bad the situation is.&lt;/p&gt;
    &lt;p&gt;i pay for the electricity that powers my git Forge. Okay, actually, one of my roommate does, but we put it on the calc sheet where we keep track of who pays what (when we remember).&lt;/p&gt;
    &lt;p&gt;At the time i began fighting scrapers, my git forge ran from an old desktop computer plugged in my living room. Now, it is in our home's rackable server in a virtual machine. i never got to measure differences in power consumption when we got scraped or not scraped on the desktop machine, but i did on the rackable server. If memory serves me right, stopping the wave of scrapers reduced the power draw of the server from ~170W to ~150W.&lt;/p&gt;
    &lt;p&gt;Right now, with all the hard drives in that server spinning, and every protection off, we are drawing 200W from the power grid on that server. Constantly. By the end of this experiment, me and my roommates will have computed that the difference in power usage caused by scraping costs us ~60 euros a year.&lt;/p&gt;
    &lt;p&gt;Another tied cost is that the VM that runs the forge is figuratively suffocating from the amount of queries. Not all queries are born equal as well: requests to see the &lt;code&gt;blame&lt;/code&gt; of a file or a &lt;code&gt;diff&lt;/code&gt; between commits incurs a worse cost than
just rendering the front page of a repository. The last huge wave of scraping
left my VM at 99+% usage of 4 CPU cores and 2.5GiB of RAM,
whereas the usual levels i observe are closer to 4% usage of CPUs, and an oscillation
between 1.5GiB and 2GiB of RAM.&lt;/p&gt;
    &lt;p&gt;As i'm writing this, the VM running forgejo eats 100% of 8 CPU cores.&lt;/p&gt;
    &lt;p&gt;Additionally, the networking cost is palpable. Various monitoring tools let me see the real-time traffic statistics in our apartment. Before i put the first measures in place to thwart scraping, we could visibly see the traffic coming out of the desktop computer running my forge and out to the internet. My roommates' complaints that it slowed down the whole internet here were in fact founded: when we had multiple people watching live streams or doing pretty big downloads, they were throttled by the traffic out of the forge.1&lt;/p&gt;
    &lt;p&gt;The egress data rate of my forge's VM is at least 4MBps of data (32Mbps). Constantly.&lt;/p&gt;
    &lt;p&gt;Finally, the human cost: i have spent entire days behind my terminals trying to figure out 1) what the fuck was going on and 2) what the fuck to do about it. i have had conversations with other people who self-host their infrastructure, desperately trying to figure out workable solutions that would not needlessly impact our users. And the funniest detail is: that rackable server is in the living room, directly in front of my bedroom door. It usually purrs like an adorable cat, but, lately, it's been whirring louder and louder. i can hear it. when i'm trying to sleep.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Let's do some statistics.&lt;/head&gt;
    &lt;p&gt;i was curious to analyze the nginx logs to understand where the traffic came from and what shape it took.&lt;/p&gt;
    &lt;p&gt;As a study case, we can work on &lt;code&gt;/var/log/nginx/git.vulpinecitrus.info/&lt;/code&gt; from
&lt;code&gt;2025-11-14&lt;/code&gt; to &lt;code&gt;2025-11-19&lt;/code&gt;. Note that on &lt;code&gt;2025-11-15&lt;/code&gt; at &lt;code&gt;18:27 UTC&lt;/code&gt;, i
stopped the redirection of new agents into the Iocaine crawler maze (see
below). At &lt;code&gt;19:15 UTC&lt;/code&gt;, i removed the nginx request limit zone from the
&lt;code&gt;/Lymkwi/linux/&lt;/code&gt; path. At &lt;code&gt;19:16 UTC&lt;/code&gt; i removed the separation of log files
between IPs flagged as bots, and IPs not flagged as bots.&lt;/p&gt;
    &lt;p&gt;The three measures i progressively put in place later were: web caching (2025-11-17), manually sending IPs to a garbage generator with a rate-limit (Iocaine 2) (2025-11-14, 15 and 18), and then Iocaine 3 (2025-11-19).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Common Logs&lt;/cell&gt;
        &lt;cell role="head"&gt;Successful&lt;/cell&gt;
        &lt;cell role="head"&gt;Delayed (429)&lt;/cell&gt;
        &lt;cell role="head"&gt;Error (5XX)&lt;/cell&gt;
        &lt;cell role="head"&gt;Measures in place&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;275323&lt;/cell&gt;
        &lt;cell&gt;66517&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;71712&lt;/cell&gt;
        &lt;cell&gt;54259&lt;/cell&gt;
        &lt;cell&gt;9802&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;140713&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;65763&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;514309&lt;/cell&gt;
        &lt;cell&gt;25986&lt;/cell&gt;
        &lt;cell&gt;3012&lt;/cell&gt;
        &lt;cell&gt;Caching, eventually rate-limiting2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;335266&lt;/cell&gt;
        &lt;cell&gt;20280&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;3183&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Bot Logs&lt;/cell&gt;
        &lt;cell&gt;Successful&lt;/cell&gt;
        &lt;cell&gt;Delayed (429)&lt;/cell&gt;
        &lt;cell&gt;Error (5XX)&lt;/cell&gt;
        &lt;cell&gt;Measures in place&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-14 (bots)&lt;/cell&gt;
        &lt;cell&gt;41388&lt;/cell&gt;
        &lt;cell&gt;65517&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-15 (bots)&lt;/cell&gt;
        &lt;cell&gt;34190&lt;/cell&gt;
        &lt;cell&gt;53403&lt;/cell&gt;
        &lt;cell&gt;63&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-16 (bots)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(no bot-specific logs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-17 (bots)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(no bot-specific logs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-18 (bots)&lt;/cell&gt;
        &lt;cell&gt;390013&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19 (bots)&lt;/cell&gt;
        &lt;cell&gt;731593&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 1)&lt;/head&gt;
    &lt;p&gt;Assuming your log file is &lt;code&gt;git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;zcat git-access-2025-11-14.log.gz | grep '" 200 ' | wc -l
zcat git-access-2025-11-14.log.gz | grep '" 429 ' | wc -l
&lt;/code&gt;
    &lt;p&gt;Without spoiling too much, caching was an utter failure, and the improvement i measurement by manually rate-limiting a set of IPs (from Huawei Cloud and Alibaba) on the Linux repository only helped so much. When all protections dropped, my server became so unresponsive that backend errors (usually timeouts) spiked. Error also happened with caching, when nginx encountered an issue when buffering a reply. Overall, caching encouraged more queries overall.&lt;/p&gt;
    &lt;p&gt;Once Iocaine was deployed, the vast majority of queries were routed away from the backend, with no errors reported, and no delaying because all of the IPs i manually rate-limited were caught by Iocaine instead.&lt;/p&gt;
    &lt;p&gt;Out of all these queries, &lt;code&gt;117.64.70.34&lt;/code&gt; is the most common source of requests,
with 226023 total queries originating from the ChinaNet-Backbone ASN (AS4134).
It is followed by &lt;code&gt;136.243.228.193&lt;/code&gt; (13849 queries), an IP from Hetzner whose
hostname ironically resolves to
&lt;code&gt;crawling-gateway-136-243-228-193.dataforseo.com&lt;/code&gt;. Then, &lt;code&gt;172.17.0.3&lt;/code&gt; the
uptime prober of VC Status with 6908
queries, and &lt;code&gt;74.7.227.127&lt;/code&gt;, an IP from Microsoft's AS 8075 (6117 queries).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Day&lt;/cell&gt;
        &lt;cell role="head"&gt;Unique IP Count&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;16461&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;18639&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;41712&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;47252&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;22480&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;14230&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 2)&lt;/head&gt;
    &lt;p&gt;Assuming your log files are called &lt;code&gt;*git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;zcat \*git-access-2025-11-14.log.gz | awk '{ print $1 }' | sort | uniq -c | wc -l
&lt;/code&gt;
    &lt;p&gt;On the two days where restrictions were lifted or there was only caching, the amount of unique IPs querying the forge doubled. The more you facilitate the work of these crawlers, the more they are going to pound you. They will always try and get more out of your server than you are capable of providing.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Day&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 1&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 2&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 3&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 4&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;(226089) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(40189) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1454) - &lt;code&gt;/&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1405) - &lt;code&gt;/rail&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1174) - &lt;code&gt;/Soblow/indi-hugo&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;(35163) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18952) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(4197) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1655) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1635) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-14 (bots)&lt;/cell&gt;
        &lt;cell&gt;(40189) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(270) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(79) - &lt;code&gt;/Lymkwi/[REDACTED]&lt;/code&gt;3&lt;/cell&gt;
        &lt;cell&gt;(55) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(52) - &lt;code&gt;/oror/asm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-15 (bots)&lt;/cell&gt;
        &lt;cell&gt;(32895) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(260) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(193) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(95) - &lt;code&gt;/Lymkwi/[REDACTED]&lt;/code&gt;3&lt;/cell&gt;
        &lt;cell&gt;(48) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;(72687) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(23028) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(16779) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(5390) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(3585) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;(361632) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(74048) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18136) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(13147) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12921) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;(227019) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(46004) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12644) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12624) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(7712) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-18 (bots)&lt;/cell&gt;
        &lt;cell&gt;(261346) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(43923) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(20195) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18808) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(10134) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;(1418) - &lt;code&gt;/&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1248) - &lt;code&gt;/rail&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(356) - &lt;code&gt;/Soblow&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(31) - &lt;code&gt;/assets/img&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(25) - &lt;code&gt;/Soblow/IndigoDen&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19 (bots)&lt;/cell&gt;
        &lt;cell&gt;(448626) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(73164) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(39107) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(37107) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(25921) - &lt;code&gt;/vc-archival/YSLua&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 3)&lt;/head&gt;
    &lt;p&gt;Assuming you want data for the log file called &lt;code&gt;git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; zcat git-access-2025-11-14.log.gz | grep '" 200 ' | awk '{ print $7 }' \
    | cut -d/ -f -3 | sort | uniq -c | sort -n \
    | tail -n 5 | tac
&lt;/code&gt;
    &lt;p&gt;Big repositories with a lot of commits and a lot of files are a bountiful resource for the crawlers. Once they enter those, they will take ages to leave, at least because of the sheer amount of pages that can be generated by following the links of a repository.&lt;/p&gt;
    &lt;p&gt;Most legitimate traffic seems to be either fetching profiles (a couple of my users have their profiles listed in their fediverse bios) or the root page of my forge.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;2025-11-14 (all)&lt;/cell&gt;
        &lt;cell role="head"&gt;2025-11-15 (all)&lt;/cell&gt;
        &lt;cell role="head"&gt;2025-11-16 (all)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 1&lt;/cell&gt;
        &lt;cell&gt;(8532) - AS136907 (Huawei Clouds)&lt;/cell&gt;
        &lt;cell&gt;(8537) - AS136907 (Huawei Clouds)&lt;/cell&gt;
        &lt;cell&gt;(8535) - AS136907 (Huawei Clouds)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 2&lt;/cell&gt;
        &lt;cell&gt;(2142) - AS45899 (VNPT Corp)&lt;/cell&gt;
        &lt;cell&gt;(2107) - AS45899 (VNPT Corp)&lt;/cell&gt;
        &lt;cell&gt;(4002) - AS212238 (Datacamp Limited)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 3&lt;/cell&gt;
        &lt;cell&gt;(803) - AS153671 (Liasail Global Hongkong Limited)&lt;/cell&gt;
        &lt;cell&gt;(895) - AS153671 (Liasail Global Hongkong Limited)&lt;/cell&gt;
        &lt;cell&gt;(3504) - AS9009 (M247 Europe SRL)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 4&lt;/cell&gt;
        &lt;cell&gt;(555) - AS5065 (Bunny Communications)&lt;/cell&gt;
        &lt;cell&gt;(765) - AS45102 (Alibaba US Technology Co., Ltd.)&lt;/cell&gt;
        &lt;cell&gt;(3206) - AS3257 (GTT Communications)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Top 5&lt;/cell&gt;
        &lt;cell&gt;(390) - AS21859 (Zenlayer Inc)&lt;/cell&gt;
        &lt;cell&gt;(629) - AS5065 (Bunny Communications)&lt;/cell&gt;
        &lt;cell&gt;(2874) - AS45899 (VNPT Corp)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 4)&lt;/head&gt;
    &lt;p&gt;For this, i needed a database of IP-to-ASN data. i got one from IPInfo by registering for a free account and using their web API. i first scripted a mapping of unique IP addresses to AS number. For example, for the log file &lt;code&gt;bot-git-access-2025-11-18.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;while read ip; do
    ASN=$(curl -qfL api.ipinfo.io/lite/$ip?token=&amp;lt;my token&amp;gt; | jq -r .asn);
    printf "$ip $ASN\n" | tee -a 2025-11-18-bot.ips.txt;
done &amp;lt; &amp;lt;(zcat bot-git-access-2025-11-18.log.gz | awk '{ print $1 }' | sort | uniq)
&lt;/code&gt;
    &lt;p&gt;Then, with this map, i run:&lt;/p&gt;
    &lt;code&gt;cat 2025-11-18-bot.ips.txt | cut -d' ' -f 2 | sort | uniq -c | sort -n | tail -n 5
&lt;/code&gt;
    &lt;p&gt;So my largest hits are from Huawei Clouds (VPS provider), VPNT (a Vietnamese mobile and home ISP), Liasail Global HK Limited (a VPS/"AI-powering service" provider), Bunny Communications LLC (a broadband ISP for residential users), and Zenlayer (CDN/Cloud infrastructure provider). When i lifted all protections, Datacamp Limited (a VPS provider), GTT Communications (some sort of bullshit-looking ISP4 who, i have been informed, is in fact a backbone operator), and M247 Europe SRL (a hosting provider) suddenly appeared. If memory serves me right, Datacamp, GTT and M247 were also companies i had flagged during my initial investigation in summer 2024, and added to the manually blocked/limited IPs alongside all of Huawei Cloud and Alibaba.&lt;/p&gt;
    &lt;p&gt;Interestingly, both Liasail and Zenlayer mention that they "Power AI" on their front page. They sure do. Worryingly, VNPT and Bunny Communications are home/mobile ISPs. i cannot ascertain for sure that their IPs are from domestic users, but it seems worrisome that these are among the top scraping sources once you remove the most obviously malicious actors.&lt;/p&gt;
    &lt;head rend="h1"&gt;# The Protection Measures&lt;/head&gt;
    &lt;p&gt;i have one goal, and one constraint. My goal is that i need to protect the forge as much as possible, by means of either blocking bots or offloading the cost to my VPS provider (whose electricity i do not pay for). My only constraint: i was not going to deploy a proof-of-work-based captcha system such as Anubis. There are two reasons for these constraints:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;i personally find that forcing your visitors to have to expand more computational power to prove they're not a scraper is bad praxis. There are devices out there that legitimately want that access, but have limited computational power or features. And, yeah, there are multiple types of challenges, some of which take low-power devices into account or even those that cannot run JavaScript, but,&lt;/item&gt;
      &lt;item&gt;Scrapers can easily bypass Anubis. It's not a design flaw. Anubis is harm reduction, not pixie dust.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;i tried layers of solutions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;caching on the reverse proxy&lt;/item&gt;
      &lt;item&gt;Iocaine 2 with no classifiers, which generates garbage in reply to any query you send it&lt;/item&gt;
      &lt;item&gt;Manually redirecting IPs and rate-limiting them&lt;/item&gt;
      &lt;item&gt;Deploying Iocaine 3, with its classifiers (Nam-Shub-of-Enki)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;## Reverse-Proxy Caching&lt;/head&gt;
    &lt;p&gt;i have a confession to make: i never realized that nginx did not cache anything by default. That realization promptly came with the other realization that caching things correctly is hard. i may, some day, write about my experience of protecting a service that posted links to itself on the fediverse, so that it wouldn't slow to a crawl for ten minutes after every post.&lt;/p&gt;
    &lt;p&gt;As for the rest of these, i will be showing my solution in &lt;code&gt;nginx&lt;/code&gt;. You can,
almost certainly, figure out a way of doing exactly the same thing with any other
decent reverse proxy software.&lt;/p&gt;
    &lt;p&gt;To create a cache for my forge, i add the following line to &lt;code&gt;/etc/nginx.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;proxy_cache_path /var/cache/nginx/forge/ levels=1:2 keys_zone=forgecache:100m;
&lt;/code&gt;
    &lt;p&gt;That will create a 2-level cache called &lt;code&gt;forgecache&lt;/code&gt; that will hold 100MB of data
located at &lt;code&gt;/var/cache/nginx/forge&lt;/code&gt;. i create the directory and make &lt;code&gt;www-data&lt;/code&gt;
its owner and group.&lt;/p&gt;
    &lt;p&gt;In &lt;code&gt;/etc/nginx/sites-enabled/vcinfo-git.conf&lt;/code&gt;, where my git forge's site
configuration sits, i have a &lt;code&gt;location&lt;/code&gt; block that serves the whole root of the
service, which i modify thusly:&lt;/p&gt;
    &lt;code&gt;location / {
    proxy_cache forgecache;
    proxy_buffering on;
    proxy_cache_valid any 1h;
    add_header X-Cached $upstream_cache_status;
    expires 1h;
    proxy_ignore_headers "Set-Cookie";
    proxy_hide_header "Set-Cookie";

    # more stuff...
}
&lt;/code&gt;
    &lt;p&gt;That configuration does several things: it turns on caching and buffering at the proxy (&lt;code&gt;proxy_buffering&lt;/code&gt;),
telling it to use &lt;code&gt;forgecache&lt;/code&gt;
(&lt;code&gt;proxy_cache&lt;/code&gt;)
and keep any page valid for an hour
(&lt;code&gt;proxy_cache_valid&lt;/code&gt;).
It also adds a cookie that will let you debug whether or not a query hit or
missed the cache (&lt;code&gt;add_header&lt;/code&gt;). The &lt;code&gt;expires&lt;/code&gt; directive adds headers telling
your visitor's browser that the content they cache will also expire in an hour
(&lt;code&gt;expires&lt;/code&gt;).
Finally, the cache ignores any response header that sets a cookie
(&lt;code&gt;proxy_ignore_headers&lt;/code&gt;,
&lt;code&gt;proxy_hide_header&lt;/code&gt;),
to attempt to remove any page that could be customized for a user once they log
in.&lt;/p&gt;
    &lt;p&gt;The result? Caching was a disaster, predictably so. Caching works when the same resource is repeatedly queried, like with page assets, JavaScript, style sheets, etc. In this case, the thousands of actors querying my forge are coordinated, somehow, never (or rarely) query the same resource twice, and only download the raw HTML of the web pages.&lt;/p&gt;
    &lt;p&gt;Worse, caching messed up the display of authenticated pages. The snippets above are not enough to delineate between an authenticated session and an unauthenticated one, and it broke my forge so badly that i had to disable caching and enable the next layer early on &lt;code&gt;2025-11-17&lt;/code&gt;, or i just could not
use my forge.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Rate-Limiting on the Proxy&lt;/head&gt;
    &lt;p&gt;The next layer of protection simply consisted in enabling a global rate-limit on the most-hit repositories:&lt;/p&gt;
    &lt;code&gt;limit_req_zone wholeforge zone=wholeforge:10m rate=3r/s;

server {
    // ...
	location ^~ (/alopexlemoni/GenderDysphoria.fyi|/oror/necro|/Lymkwi/linux|/vc-archival/youtube-dl-original|/reibooru/reibooru) {
		proxy_set_header Host $host;
		proxy_set_header X-Real-IP $remote_addr;
		proxy_max_temp_file_size 2048m;

		limit_req zone=wholeforge nodelay;

		proxy_pass http://&amp;lt;my actual upstream&amp;gt;/;
	}
}
&lt;/code&gt;
    &lt;p&gt;This was achieved in two directives. The first one, &lt;code&gt;limit_req_zone&lt;/code&gt;, sits outside
the &lt;code&gt;server {}&lt;/code&gt; block and defines a zone called &lt;code&gt;wholeforge&lt;/code&gt; that stores 10MB of
state data and limits to 3 requests per second.&lt;/p&gt;
    &lt;p&gt;When this was in place, however, actually accessing the Linux repository as a normal user (or any of the often-hit repositories) became a nightmare of waiting and request timeouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Manually Redirecting to a Garbage Generator&lt;/head&gt;
    &lt;p&gt;Because caching was (predictably) useless, and rate-limiting was hindering me as well, i re-enabled the initial setup that was in place before my experiments: manually redirecting queries to a garbage generator (in this case, an old version of Iocaine). It's largely based on my initial setup following this tutorial in french.&lt;/p&gt;
    &lt;p&gt;For the purpose of this part, you do not have to know what Iocaine does precisely. In the next section, i will present my current and final setup, with an updated Iocaine that also includes a classifier to decide which queries are bots and which are regular users. For now, i will present the version where i manually chose who to return garbage to based on IP addresses.&lt;/p&gt;
    &lt;p&gt;As a little bonus, it will also include rate-limiting of those garbage-hungry bots.&lt;/p&gt;
    &lt;p&gt;i add a file called &lt;code&gt;/etc/nginx/snippets/block_bots.conf&lt;/code&gt; which contains:&lt;/p&gt;
    &lt;code&gt;if ($bot_user_agent) {
    rewrite ^ /deflagration$request_uri;
}
if ($bot_ip) {
    rewrite ^ /deflagration$request_uri;
}
location /deflagration {
    limit_req zone=bots nodelay;
    proxy_set_header Host $host;
    proxy_pass &amp;lt;garbage upstream&amp;gt;;
}
&lt;/code&gt;
    &lt;p&gt;This will force any query categorized as &lt;code&gt;bot_user_agent&lt;/code&gt; or &lt;code&gt;bot_ip&lt;/code&gt; to be
routed through to a different upstrea which serves garbage. That upstream is
also protected by rate-limiting on a zone called &lt;code&gt;bots&lt;/code&gt; which is defined in the
next bit of code. This snippet is actually meant to be included in your &lt;code&gt;server {}&lt;/code&gt;
block using the &lt;code&gt;include&lt;/code&gt; directive.&lt;/p&gt;
    &lt;p&gt;i then add the following in &lt;code&gt;/etc/nginx/conf.d/bots.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;map $http_user_agent $bot_user_agent {
    default 0;

    # from https://github.com/ai-robots-txt/ai.robots.txt/blob/main/robots.txt
    ~*amazonbot 1;
    ~*anthropic-ai  1;
    ~*applebot  1;
    ~*applebot-extended 1;
    ~*brightbot 1;
    ~*bytespider  1;
    ~*ccbot 1;
    ~*chatgpt-user  1;
    ~*claude-web  1;
    ~*claudebot 1;
    ~*cohere-ai 1;
    ~*cohere-training-data-crawler  1;
    ~*crawlspace  1;
    ~*diffbot 1;
    ~*duckassistbot 1;
    ~*facebookbot 1;
    ~*friendlycrawler 1;
    ~*google-extended 1;
    ~*googleother 1;
    ~*googleother-image 1;
    ~*googleother-video 1;
    ~*gptbot  1;
    ~*iaskspider  1;
    ~*icc-crawler 1;
    ~*imagesiftbot  1;
    ~*img2dataset 1;
    ~*isscyberriskcrawler 1;
    ~*kangaroo  1;
    ~*meta-externalagent  1;
    ~*meta-externalfetcher  1;
    ~*oai-searchbot 1;
    ~*omgili  1;
    ~*omgilibot 1;
    ~*pangubot  1;
    ~*perplexitybot 1;
    ~*petalbot  1;
    ~*scrapy  1;
    ~*semrushbot-ocob 1;
    ~*semrushbot-swa  1;
    ~*sidetrade 1;
    ~*timpibot  1;
    ~*velenpublicwebcrawler 1;
    ~*webzio-extended 1;
    ~*youbot  1;

    # Add whatever other pattern you want down here
}

geo $bot_ip {
    default 0;

    # Add your IP ranges here
}

# Rate-limiting setup for bots
limit_req_zone bots zone=bots:30m rate=1r/s;

# Return 429 (Too Many Requests) to slow them down
limit_req_status 429;
&lt;/code&gt;
    &lt;p&gt;That bit of configuration does a mapping between the client IP and a variable called &lt;code&gt;bot_ip&lt;/code&gt;, and the client's user agent and a variable called
&lt;code&gt;bot_user_agent&lt;/code&gt;. When a known pattern listed in those blocks is found, the
corresponding variable is flipped to the provided value (here, &lt;code&gt;1&lt;/code&gt;). Otherwise,
it stays &lt;code&gt;0&lt;/code&gt;. Then, we define the rate-limiting zone that is used to slow down
the bots so they don't feed on slop too fast. You will then need to install the
&lt;code&gt;http-geoip2&lt;/code&gt; nginx module (on Debian-based distributions, something like &lt;code&gt;apt install libnginx-mod-http-geoip2&lt;/code&gt; will do).&lt;/p&gt;
    &lt;p&gt;Once that is done, add the following line to the &lt;code&gt;server&lt;/code&gt; block of every site
you want to protect:&lt;/p&gt;
    &lt;code&gt;include /etc/nginx/snippets/block_bots.conf;
&lt;/code&gt;
    &lt;p&gt;And when you feel confident enough, roll a &lt;code&gt;nginx -t&lt;/code&gt; and reload the unit for
&lt;code&gt;nginx&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Now, if you're using &lt;code&gt;caddy&lt;/code&gt; or any other reverse proxy, there are probably
similar mechanisms available. You can go and peruse the documentation of Iocaine,
or look online for specific tutorials that, i am sure, other people have made
better than i would.&lt;/p&gt;
    &lt;p&gt;Immediately after enabling it, and shoving all the IPs from Alibaba Cloud and Huawei Cloud in the bot config file, the activity slowed down on my server. Power usage went down to ~180W, CPU usage to rougly 60%, and it stopped making a hellish noise.&lt;/p&gt;
    &lt;p&gt;As the stats showed earlier, however, a lot of traffic was still hitting the server itself. Even weirder, there were still occasional spikes, every 3 hour, that lasted about one and a half hour, where the server would whirr and forgejo suffocate again.&lt;/p&gt;
    &lt;p&gt;Bots were still hitting my server, and there was no clear source for it.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Automatically Classifying Bots and Poisoning Them: Iocaine and Nam-Shub-of-Enki&lt;/head&gt;
    &lt;p&gt;So far, the steps i showed so far help when a single IP is hammering at your forge, or when someone is clearly scraping you from an Autonomous System that you do not mind blocking. Sadly, as i've showed above in Table 4, a surprising amount of scraping comes from broadband addresses. i can assemble lists of IPs as big as i want, or block entire ASNs, but i would love to have a per-query way of determining if a query looks legitimate.&lt;/p&gt;
    &lt;p&gt;The next steps of protection will rely on categorizing a source IP based on its the credibility of its user agent. This mechanism is largely based on the documentation for Iocaine 3.x. We finally get to talk about Iocaine!&lt;/p&gt;
    &lt;p&gt;Iocaine is a tool that traps scrapers in a maze of meaningless pages that endlessly lead to more meaningless pages. The content of these pages is generated using a Markov chain, based on a corpus of texts given to the software. Iocaine (specifically all versions after 3 at least5) is a middleware, in the sense that it works by being placed on the line between your reverse proxy and the service. Your reverse proxy will first begin by redirecting traffic to Iocaine, and, if Iocaine deems a query legitimate, it will return a &lt;code&gt;421 Misdirected Request&lt;/code&gt; back at your reverse-proxy. The
latter must then catch it, and use the real upstream as a fallback. If
Iocaine's Nam-Shub-of-Enki6 decides query came from a bogus or otherwise undesirable source, it
will happily reply &lt;code&gt;200 OK&lt;/code&gt; and send generated garbage.&lt;/p&gt;
    &lt;p&gt;My setup lodges Iocaine 3 between nginx and my forge, following the Iocaine documentation to use the container version. i recommend you follow it, and then add the next little things to enable categorization statistics, and prevent the logging they're based on from blowing up your storage:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In &lt;code&gt;etc/config.d/03-nam-shub-of-enki.kdl&lt;/code&gt;, change the logging block to:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;logging {
    enable #true
    classification {
        enable #true
    }
}
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In &lt;code&gt;docker-compose.yaml&lt;/code&gt;, add the following bits to limit classification logging to 50MB:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;services:
  iocaine:
    # The things you already have here...
    # ...
    env:
      - RUST_LOG=iocaine=info
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
&lt;/code&gt;
    &lt;p&gt;My checks block in Nam-Shub-of-Enki is as such:&lt;/p&gt;
    &lt;code&gt;checks {
    disable cgi-bin-trap

    asn {
        database-path "/data/ipinfo_lite.mmdb"
        asns "45102" "136907"
    }
    ai-robots-txt {
        path "/data/ai.robots.txt-robots.json"
    }
    generated-urls {
        identifiers "deflagration"
    }
    big-tech {
        enable #true
    }
    commercial_scrapers {
        enable #true
    }
}
&lt;/code&gt;
    &lt;p&gt;I snatched a copy of the latest ipinfo ASN database for free and blocked AS52102 (Alibaba) and AS136907 (Huawei Clouds).&lt;/p&gt;
    &lt;p&gt;On 2025-11-18 at 00:00:29 UTC+1, i enabled Iocaine with the Nam-Shub-of-Enki classifier in front of my whole forge. Immediately, my server was no longer hammered. Power draw went down to just above 160W.&lt;/p&gt;
    &lt;p&gt;One problem i noticed however, while trying to deploy the artifact for this blog post on my forge, is that Iocaine causes issues when huge &lt;code&gt;PUT&lt;/code&gt;/&lt;code&gt;PATCH&lt;/code&gt;/&lt;code&gt;POST&lt;/code&gt;
requests with large bodies are piped through it: it will hang up before the
objects are entirely written. i am trying to figure out a way of only redirecting
&lt;code&gt;HEAD&lt;/code&gt; and &lt;code&gt;GET&lt;/code&gt; requests to Iocaine in nginx, like is done in the Caddy example
of the Iocaine documentation.&lt;/p&gt;
    &lt;p&gt;What i ended up settling on requires a bit of variable mapping. At the start of your site configuration, before the &lt;code&gt;server {}&lt;/code&gt; block:&lt;/p&gt;
    &lt;code&gt;map $request_method $upstream_location {
	GET	&amp;lt;iocaine upstream&amp;gt;;
	HEAD	&amp;lt;iocaine upstream&amp;gt;;
	default	&amp;lt;your actual upstream&amp;gt;;
}

map $request_method $upstream_log {
	GET	bot_access;
	HEAD	bot_access;
	default	access;
}
&lt;/code&gt;
    &lt;p&gt;Then, in the block that does the default location, write:&lt;/p&gt;
    &lt;code&gt;	location / {
	    proxy_cache off;
	    access_log /var/log/nginx/$upstream_log.log combined;
	    proxy_intercept_errors on;
	    error_page 421 = @fallback;
	    proxy_set_header Host $host;
	    proxy_set_header X-Real-IP $remote_addr;
	    proxy_pass http://$upstream_location;
	}
&lt;/code&gt;
    &lt;p&gt;That is, replace the upstream in &lt;code&gt;proxy_pass&lt;/code&gt; with the upstream decided by the
variable mapping, and, while we're at it, use &lt;code&gt;$upstream_log&lt;/code&gt; to know which log
will be the final one for that request. i differentiate between &lt;code&gt;bot_access.log&lt;/code&gt;
and &lt;code&gt;access.log&lt;/code&gt; to gather my statistics, so the difference matters to me. Change
the variables to suit the way you do it (or remove it, if you don't distinguish
clients in your log files).&lt;/p&gt;
    &lt;head rend="h1"&gt;# Monitoring Iocaine&lt;/head&gt;
    &lt;p&gt;Currently, on 2025-11-30 at 16:33:00 UTC+1, Iocaine has served 38.16GB of garbage. Over the past hour, 152.11MB of such data was thrown back at undesirable visitors. 3.39GB over the past day, 22.22GB over the past week. You can get the snippet that describes my Iocaine-specific Grafana views here.&lt;/p&gt;
    &lt;p&gt;The vast majority of undesirable queries come from Claude, OpenAI, and Disguised Bots. Claude and OpenAI are absolutely gluttonous, and, once they have access to a ton of pages, they will greedily flock to fetch them like pigeons being fed breadcrumbs laced with strychnine.&lt;/p&gt;
    &lt;p&gt;AI bot scrapers (&lt;code&gt;ai.robots.txt&lt;/code&gt;) maintain a constant 920~930 query per minute
(15-ish QPS) over the 6 domains i have protected with Iocaine, including the
forge.&lt;/p&gt;
    &lt;p&gt;There is also a low hum of a mix of commercial scrapers (~1 request every two second), big tech crawlers (Facebook, Google, etc, about 2QPS or 110 query/min), and, especially, fake browsers.&lt;/p&gt;
    &lt;p&gt;Classifying fake browsers is where Iocaine really shines, specifically thanks to the classifiers implemented via Nam-Shub-of-Enki. The faked bots classifier detects the likelihood that the user agent reported by the client is bullshit, generated from a list of technologies mashed together. For example, if your client reports a user agent for a set of software that never supported HTTP2, or never actually existed together, or is not even released yet, it will get flagged. Think, for example, Windows NT 4 running Chrome, pretending to be able to do TLS1.3.&lt;/p&gt;
    &lt;p&gt;The background-noise level of such queries is usually 140~160 queries per minute (or 2~3 QPS). However, notice those spikes in the graph above?&lt;/p&gt;
    &lt;head rend="h2"&gt;## The Salves of Queries&lt;/head&gt;
    &lt;p&gt;For a while during my experiments i noticed those pillars of queries. My general nginx statistics would show a sharp increase of connections, with an iniital ramp-up, and a stable-ish plateau lasting about one and a half hour, before suddenly stopping. It would then repeat again, roughly three hours later.&lt;/p&gt;
    &lt;p&gt;Between October 29th and November 19th, and on November 28th, these spikes would constantly show up. As soon as i got Iocaine statistics running, it would flag all of those queries as faked browsers.&lt;/p&gt;
    &lt;p&gt;i investigated those spikes in particular, because they baffled and scared me: the regularity with which they probed me, and the sharpness of the ramp-up and halts, made me afraid that someone, somewhere, was organizing thousands of IPs to specifically take turns at probing websites. i have not reached any solid conclusions, beyond the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The initial phase of an attack wave begins with a clear exponential ramp-up&lt;/item&gt;
      &lt;item&gt;The ramp-up stops when the server starts either throwing errors, or the response latency reaches a given threshold&lt;/item&gt;
      &lt;item&gt;Every wave of attack lasts roughly one hour and a half&lt;/item&gt;
      &lt;item&gt;An individual IP will often contribute no more than one query, but it can reach 50 to 60 queries per IP&lt;/item&gt;
      &lt;item&gt;The same 15 or so ASN keep showing up, with five regular leaders in IP count: &lt;list rend="ol"&gt;&lt;item&gt;AS212238: Datacamp Limited&lt;/item&gt;&lt;item&gt;AS3257: GTT Communications&lt;/item&gt;&lt;item&gt;AS9009: M247 Europe SRL&lt;/item&gt;&lt;item&gt;AS203020: HostRoyale Technologies Pvt Ltd&lt;/item&gt;&lt;item&gt;AS210906: UAB "Bite Lietuva" (a Lithuanian ISP)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of those as service providers. My working theory at the moment is that someone registered thousands of cheap servers in many different companies, and are selling access to them as web proxies for scraping and scanning. i will probably write something up later when i have properly investigated that specific phenomenon.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Conclusion&lt;/head&gt;
    &lt;p&gt;Self-hosting anything that is deemed "content" openly on the web in 2025 is a battle of attrition between you and forces who are able to buy tens of thousands of proxies to ruin your service for data they can resell.&lt;/p&gt;
    &lt;p&gt;This is depressing. Profoundly depressing. i look at the statistics board for my reverse-proxy and i never see less than 96.7% of requests classified as bots at any given moment. The web is filled with crap, bots that pretend to be real people to flood you. All of that because i want to have my little corner of the internet where i put my silly little code for other people to see.&lt;/p&gt;
    &lt;p&gt;i have to learn to protect myself from industrial actors in order to put anything online, because anything a person makes is valuable, and that value will be sucked dry by every tech giant to be emulsified, liquified, strained, and ultimately inexorably joined in an unholy mesh of learning weights.&lt;/p&gt;
    &lt;p&gt;This experience has rather profoundly radicalized the way i think about technology. Sanitized content can be chewed on and shat out by companies from training, but their AI tools will never swear. They will never use a slur. They will never have a revolutionary thought. Despite being amalgamation of shit rolled up in the guts of the dying capitalist society, they are sanitized to hell and beyond.&lt;/p&gt;
    &lt;p&gt;The developer of Iocaine put it best when explaining why Iocaine has absolutely unhinged identifiers (such as &lt;code&gt;SexDungeon&lt;/code&gt;, &lt;code&gt;PipeBomb&lt;/code&gt;, etc) is that they will all trigger "safeguard"
mechanisms in commercial AI tools: absolutely no coding agent will accept
analyzing and explaining code where the memory allocator's free function is
called &lt;code&gt;liberate_palestine&lt;/code&gt;. i bet that if i described, in graphic details, in
the comments of this page, the different ways being a furry intersects with my
sexuality, that no commercial scraper would even dare ingest this page.&lt;/p&gt;
    &lt;p&gt;Fuck tech companies. Fuck "AI". Fuck the corporate web.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46241849</guid><pubDate>Fri, 12 Dec 2025 07:51:04 +0000</pubDate></item><item><title>Training LLMs for Honesty via Confessions</title><link>https://arxiv.org/abs/2512.08093</link><description>&lt;doc fingerprint="bd9c9508d0f342ab"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 8 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Training LLMs for Honesty via Confessions&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.&lt;lb/&gt;In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.&lt;lb/&gt;To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46242795</guid><pubDate>Fri, 12 Dec 2025 10:37:51 +0000</pubDate></item><item><title>Koralm Railway</title><link>https://infrastruktur.oebb.at/en/projects-for-austria/railway-lines/southern-line-vienna-villach/koralm-railway</link><description>&lt;doc fingerprint="715d5db4db36ea94"&gt;
  &lt;main&gt;
    &lt;p&gt;Crossing the Koralpe massif more quickly and with more comfort. That’s what the future of train travel from Graz to Klagenfurt looks like. With the Koralm Railway, you will arrive at your destination even quicker. The fastest connection will shrink from three hours to just 45 minutes. Western Styria and southern Carinthia can be reached even more easily – as can our neighbouring countries Hungary and Italy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Koralm Railway connects Europe&lt;/head&gt;
    &lt;p&gt;The economy is also benefiting from the construction of the new Koralm Railway. As part of the new Southern Line, it is strengthening the Baltic-Adriatic Corridor in Europe. Transporting goods in Austria by train is becoming more attractive, which in turn is allowing our operations to remain competitive internationally. And the environment to breathe: Each tonne of freight moved by rail generates around 15 times less CO2 emissions than transporting it by lorry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your benefits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Shorter journey times&lt;/item&gt;
          &lt;item&gt;Better access to southern Austria&lt;/item&gt;
          &lt;item&gt;Twenty-three contemporary railway stations and stops&lt;/item&gt;
          &lt;item&gt;Economic stimuli and jobs in the region&lt;/item&gt;
          &lt;item&gt;Long-term relief for the environment&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46242871</guid><pubDate>Fri, 12 Dec 2025 10:50:22 +0000</pubDate></item><item><title>The Tor Project is switching to Rust</title><link>https://itsfoss.com/news/tor-rust-rewrite-progress/</link><description>&lt;doc fingerprint="b743d961f031579a"&gt;
  &lt;main&gt;
    &lt;p&gt;The Tor Project has been busy with the rustification of their offering for quite some time now.&lt;/p&gt;
    &lt;p&gt;If you have used Tor Browser, you know what it does. Anonymous browsing through encrypted relay chains. The network itself has been running since the early 2000s. All of it is built on C.&lt;/p&gt;
    &lt;p&gt;But that C codebase is an issue. It is known to have buffer overflows, use-after-free bugs, and memory corruption vulnerabilities. That is why they introduced Arti, a Rust rewrite of Tor that tackles these flaws by leveraging the memory safety of the programming language.&lt;/p&gt;
    &lt;p&gt;A new release of Arti just dropped last week, so let's check it out!&lt;/p&gt;
    &lt;head rend="h2"&gt;Arti 1.8.0: What's New?&lt;/head&gt;
    &lt;p&gt;We begin with the main highlight of this release, the rollout of the circuit timeout rework that was laid out in proposal 368. Tor currently uses something called Circuit Dirty Timeout (CDT). It is a single timer that controls when your connection circuits become unavailable and when they close down.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it is predictable. Someone monitoring traffic can spot these patterns and potentially track your activity. Arti 1.8.0 fixes this by implementing usage-based timeouts with separate timers. One handles when circuits accept new connections. Another closes idle circuits at random times instead of fixed intervals.&lt;/p&gt;
    &lt;p&gt;This should reduce the risk of fingerprinting from predictable timeout behavior.&lt;/p&gt;
    &lt;p&gt;Next up is the new experimental &lt;code&gt;arti hsc ctor-migrate&lt;/code&gt; command that lets onion service operators migrate their restricted discovery keys from the C-based Tor to Arti's keystore.&lt;/p&gt;
    &lt;p&gt;These keys handle client authorization for onion services. The command transfers them over without requiring operators to do the manual legwork. The release also delivers improvements for routing architecture, protocol implementation, directory cache support, and OR port listener configuration.&lt;/p&gt;
    &lt;p&gt;You can go through the changelog to learn more about the Arti 1.8.0 release.&lt;/p&gt;
    &lt;p&gt;Via: Sam Bent&lt;/p&gt;
    &lt;p&gt;Suggested Read 📖: Is Helium the Browser Brave Was Meant to Be?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46243543</guid><pubDate>Fri, 12 Dec 2025 12:35:57 +0000</pubDate></item><item><title>Fedora: Open-source repository for long-term digital preservation</title><link>https://fedorarepository.org/</link><description>&lt;doc fingerprint="3cd1965b918fe919"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fedora is the flexible, standards-based, open-source repository software built to support long-term digital preservation.&lt;/head&gt;
    &lt;head rend="h2"&gt;Why Choose Fedora?&lt;/head&gt;
    &lt;head rend="h3"&gt;Flexibility&lt;/head&gt;
    &lt;p&gt;Fedora can store, preserve and provide access to any type of digital object. Through native support for semantic relationships, you are free to model your content to suit your needs and build out object relationships that are defined entirely by you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Standards Based&lt;/head&gt;
    &lt;p&gt;Partnered with Fedora’s published API and the adoption of the OCFL standard for persistence, Fedora uses a variety of globally accepted standard web conventions for interacting with the application.&lt;/p&gt;
    &lt;head rend="h3"&gt;Global User Community&lt;/head&gt;
    &lt;p&gt;The Fedora program has been in existence for 20+ years, and our users represent an engaged, supportive and invested global community focused on sustainability and growth.&lt;/p&gt;
    &lt;head rend="h4"&gt;JOIN OTHER INSTITUTIONS ACROSS THE WORLD&lt;/head&gt;
    &lt;head rend="h2"&gt;Fedora Features&lt;/head&gt;
    &lt;head rend="h3"&gt;Enhanced Digital Preservation Support&lt;/head&gt;
    &lt;p&gt;A transparent, standardized persistence layer based on the Oxford Common File Layout (OCFL), which supports long-term digital preservation best practices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Improved Performance &amp;amp; Scale&lt;/head&gt;
    &lt;head rend="h3"&gt;Integrations&lt;/head&gt;
    &lt;head rend="h2"&gt;Join Our Community&lt;/head&gt;
    &lt;head rend="h2"&gt;Want to support Fedora? Become a Member Today.&lt;/head&gt;
    &lt;head rend="h3"&gt;WHY BECOME A MEMBER&lt;/head&gt;
    &lt;head rend="h3"&gt;What Fedora Members Receive&lt;/head&gt;
    &lt;head rend="h3"&gt;HOW TO BECOME A MEMBER&lt;/head&gt;
    &lt;head rend="h2"&gt;Fedora Resources&lt;/head&gt;
    &lt;head rend="h2"&gt;Join the Fedora Community Today&lt;/head&gt;
    &lt;p&gt;Long-term digital preservation starts here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46243883</guid><pubDate>Fri, 12 Dec 2025 13:23:31 +0000</pubDate></item><item><title>SQLite JSON at Full Index Speed Using Generated Columns</title><link>https://www.dbpro.app/blog/sqlite-json-virtual-columns-indexing</link><description>&lt;doc fingerprint="3adc827f54e65bc2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SQLite JSON Superpower: Virtual Columns + Indexing&lt;/head&gt;
    &lt;p&gt;We absolutely love SQLite here at DB Pro. You'd be hard-pressed to find anyone who actively dislikes it. Sure, it has limitations, and I do mean limitations, not weaknesses. SQLite can absolutely be used in production when it's deployed properly and tuned with care.&lt;/p&gt;
    &lt;p&gt;SQLite has also seen something of a resurgence over the past few years. From being forked into projects like libSQL and Turso, to powering popular backend frameworks such as PocketBase, it’s clearly having a moment again.&lt;/p&gt;
    &lt;p&gt;As I said though, we love it. It even powers the local database inside DB Pro itself. For our use case, there really isn’t a better alternative.&lt;/p&gt;
    &lt;p&gt;Because we’ve been using SQLite in anger over the past three months, we’ve learnt a huge amount about it, including plenty of things we didn’t know before.&lt;/p&gt;
    &lt;p&gt;So I’m planning to write a short series of blog posts covering some of the cooler, more interesting features and nuances of SQLite that we’ve discovered along the way. This is the first of those posts.&lt;/p&gt;
    &lt;p&gt;First of all. Did you know SQLite has JSON functions and operators? I didn't until recently! I came across this Hacker News comment when researching SQLite's JSON operators.&lt;/p&gt;
    &lt;code&gt;The cool thing for working with json is to store each json document as is in one column, then make virtual columns that store some specific information you want to query, using some combination of json_extract, then index those columns.
This makes for super-fast search, and the best part is you don't have to choose what to index at insert time; you can always make more virtual columns when you need them.

(You can still also search non-indexed, raw json, although it may take a long time for large collections).

I love SQLite so much - bambax
&lt;/code&gt;
    &lt;p&gt;I read that, then read it again, then again until I understood what bambax was saying. I was sort of in disbelief.&lt;/p&gt;
    &lt;p&gt;So I had to give it a try to see if it works. We've got an embedded SQLite-in-the-browser component on our blog and so I wanted to throw together some working examples for you guys (but mostly for me).&lt;/p&gt;
    &lt;p&gt;Let's break down what bambax is saying:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Store the JSON document raw&lt;/item&gt;
      &lt;item&gt;Create virtual generated columns using &lt;code&gt;json_extract&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add indexes to those generated columns&lt;/item&gt;
      &lt;item&gt;Query JSON at full B-tree index speed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means you never have to choose your indexing strategy up front. If you later realise you need to query on a new JSON field, you simply add another generated column, add an index, and you're done.&lt;/p&gt;
    &lt;p&gt;No data migration. No schema rewrite. No ETL. Just pure flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Store Raw JSON&lt;/head&gt;
    &lt;p&gt;First, create a simple table with a JSON column:&lt;/p&gt;
    &lt;p&gt;Your JSON documents are stored naturally, exactly as they arrive. No schema gymnastics required either!&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Add Virtual Generated Columns&lt;/head&gt;
    &lt;p&gt;Here's where bambax is saying the magic happens. Let's add virtual generated columns. Generated columns compute values on demand. They don't actually store data:&lt;/p&gt;
    &lt;p&gt;I believe that no writes occur. There's no backfilling. It's instant. The virtual columns are computed on-the-fly from your JSON data whenever you query them.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Add Indexes for Full Performance&lt;/head&gt;
    &lt;p&gt;Now is the icing on the cake. We add indexes to make these virtual columns blazing fast:&lt;/p&gt;
    &lt;p&gt;Suddenly your JSON behaves like normal relational columns with full index support.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Query at Full Speed&lt;/head&gt;
    &lt;p&gt;Now your queries are blazing fast. So let's give it a go with some examples:&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Need a New Query Pattern Later?&lt;/head&gt;
    &lt;p&gt;I believe this is one of the strongest points to this pattern. If at a later date your JSON shape changes (expected), you can just add another column and create another index.&lt;/p&gt;
    &lt;p&gt;For example, you realise you need to query by &lt;code&gt;user_id&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;ALTER TABLE events
ADD COLUMN user_id INTEGER
GENERATED ALWAYS AS (json_extract(data, '$.user.id')) VIRTUAL;

CREATE INDEX idx_events_user_id ON events(user_id);
&lt;/code&gt;
    &lt;p&gt;Boom. optimised without touching existing rows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Pattern is So Powerful&lt;/head&gt;
    &lt;p&gt;This pattern completely changed how I think about working with JSON in SQLite. You get the flexibility of schemaless data, combined with the performance and ergonomics of a relational database, without committing yourself too early or painting yourself into a corner.&lt;/p&gt;
    &lt;p&gt;So, thanks bambax!&lt;/p&gt;
    &lt;p&gt;There are plenty more of these little SQLite superpowers hiding in plain sight. This is just the first one I wanted to share.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Jay&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46243904</guid><pubDate>Fri, 12 Dec 2025 13:25:19 +0000</pubDate></item><item><title>BpfJailer: eBPF Mandatory Access Control [pdf]</title><link>https://lpc.events/event/19/contributions/2159/attachments/1833/3929/BpfJailer%20LPC%202025.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46244378</guid><pubDate>Fri, 12 Dec 2025 14:20:20 +0000</pubDate></item><item><title>CM0 – a new Raspberry Pi you can't buy</title><link>https://www.jeffgeerling.com/blog/2025/cm0-new-raspberry-pi-you-cant-buy</link><description>&lt;doc fingerprint="bf35175918e780e8"&gt;
  &lt;main&gt;
    &lt;p&gt;This little postage stamp is actually a full Raspberry Pi Zero 2, complete with eMMC storage and WiFi.&lt;/p&gt;
    &lt;p&gt;But you can't get one. Well, not unless you buy the CM0NANO development board from EDAtec, or you live in China.&lt;/p&gt;
    &lt;p&gt;This little guy doesn't have an HDMI port, Ethernet, or even USB. It's a special version of the 'Compute Module' line of boards. Little Raspberry Pi 'System on Modules' (SoMs), they're called.&lt;/p&gt;
    &lt;p&gt;Compute Modules are entire Linux computers about the size of a regular desktop CPU that you 'plug in' to another board, to give it life.&lt;/p&gt;
    &lt;p&gt;Compute modules are everywhere, in kiosks, signage, 3D printers, and even the new Ableton Move. If you just need a little bit of Linux for networking and remote control, these are perfect for that.&lt;/p&gt;
    &lt;p&gt;And the CM0 is now the smallest version, a little bigger than a postage stamp.&lt;/p&gt;
    &lt;p&gt;But unlike all the other Compute Modules, the CM0 has castellated edges like a Pico. That way, a company integrating this into their product can just pick and place it and solder it onto their main PCB, instead of working with more delicate board-to-board connectors.&lt;/p&gt;
    &lt;p&gt;But why is this only in China? I'll get to that, but first I wanted to thank EDAtec for sending a CM0 and their CM0NANO dev board for testing. Without them, I don't think I'd ever be able to show these Pis to you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;I posted this story to my YouTube channel, but if you're on the blog already, chances are you favor reading over video, so scroll on!&lt;/p&gt;
    &lt;head rend="h2"&gt;ED-CM0NANO&lt;/head&gt;
    &lt;p&gt;EDAtec's CM0NANO seems to be the official IO board for the CM0. It breaks out every feature on the RP3A0 chip at the heart of the Pi Zero 2 and CM0.&lt;/p&gt;
    &lt;p&gt;There's 10/100 Ethernet through a little USB to Ethernet chip (CoreChips SR9900A), two USB 2.0 ports, full-size HDMI, and USB-C for power and flashing the eMMC. Then there are display and camera connectors, GPIO, and a few more headers.&lt;/p&gt;
    &lt;p&gt;To flash the onboard eMMC, I had to switch the &lt;code&gt;RPI_BOOT_SW&lt;/code&gt; switch towards the RTC battery slot, then use rpiboot to mount it on my Mac. Then I used Raspberry Pi Imager to flash Pi OS 13 on it.&lt;/p&gt;
    &lt;p&gt;The eMMC on here is very slow compared to what I'm used to with the Pi 5 generation, like on the CM5. Its top speed seems to be around 19-20 MB/sec.&lt;/p&gt;
    &lt;p&gt;Once it's flashed, it's a full Linux computer, complete with Raspberry Pi's desktop environment.&lt;/p&gt;
    &lt;p&gt;EDAtec has a firmware support package you can install from their package repository, and once that's done, I did what nobody should do on this small of a computer: fired up Chromium.&lt;/p&gt;
    &lt;p&gt;Browsing the web on here is almost completely out of the question, since it only has 512 Megs of RAM—which is so little it pops a warning saying Chromium should only be used with 1 GB of more of RAM!&lt;/p&gt;
    &lt;p&gt;I did try browsing this website, and it took something like a minute to just quit the browser, after I was clicking the X to close the tab over and over again!&lt;/p&gt;
    &lt;p&gt;But with WiFi, Ethernet, USB, HDMI, and everything else the Pi ecosystem has to offer, some products that just want to slap a well-supported Linux environment on top of their product (and not integrate an SoC, memory, storage, and wireless chip) now have this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Global distribution possibilities&lt;/head&gt;
    &lt;p&gt;Do I think companies and makers here in the US and over in other parts of the world would also benefit from the CM0? Yes. Do I think it'll happen? Doubtful.&lt;/p&gt;
    &lt;p&gt;The Zero 2 W and CM0 share something in common, besides their entire architecture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Zero 2 W was introduced at the beginning of the COVID-induced chip shortages.&lt;/item&gt;
      &lt;item&gt;The CM0 was introduced right before the great RAM shortages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When Hackster asked Eben Upton about global availability, he was noncommittal:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;No plans to make it available outside China at the moment, but we'll see how we get on.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That was back before the RAM shortages got bad.&lt;/p&gt;
    &lt;p&gt;I followed up asking a Pi engineer about it, and it sounds like one big problem is the RP3A0 chip that integrates an LPDDR2 RAM chip stacked on top of the Pi's SoC.&lt;/p&gt;
    &lt;p&gt;He said the CM0 would compete with Pi Zero 2 for LPDDR2 memory, which is in shorter supply these days (it's not being produced anymore, so stocks will only become more limited over time), and they want to make sure the popular Zero 2 W can stay in stock for makers and education.&lt;/p&gt;
    &lt;p&gt;The CM0 is targeted squarely at the lower end market, integrated into products built on assembly lines. So because of that, it's anyone's guess if the CM0 will ever make it out of China.&lt;/p&gt;
    &lt;p&gt;I'm not doing a full review of the board here, because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It's practically the same as the Pi Zero 2 W, which I already reviewed.&lt;/item&gt;
      &lt;item&gt;It's not like you can get one (standalone, at least) anyway, at least not for the foreseeable future.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think there was a chance, before the DRAM manufacturers went all-in on an AI cash grab, but for now, stick to the Pi Zero 2's that you're used to.&lt;/p&gt;
    &lt;p&gt;You can find a little more detail and benchmark results on my sbc-reviews issue for the CM0.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46244922</guid><pubDate>Fri, 12 Dec 2025 15:19:19 +0000</pubDate></item><item><title>Berlin Approves New Expansion of Police Surveillance Powers</title><link>https://reclaimthenet.org/berlin-approves-new-expansion-of-police-surveillance-powers</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46245041</guid><pubDate>Fri, 12 Dec 2025 15:29:46 +0000</pubDate></item><item><title>Framework Raises DDR5 Memory Prices by 50% for DIY Laptops</title><link>https://www.phoronix.com/news/Framework-50p-DDR5-Memory</link><description>&lt;doc fingerprint="32ec5f9b3cb39588"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Framework Raises DDR5 Memory Prices By 50% For DIY Laptops&lt;/head&gt;
    &lt;p&gt; Framework Computer had worked to keep their memory prices lower than other laptop vendors amid the ongoing memory shortages throughput the industry worldwide. But today they've finally had to cave in and increase their DDR5 memory modules for the Framework Laptop DIY Editions by 50%. &lt;lb/&gt;Due to the ongoing price hikes around system memory with shortages throughout the supply chain, Framework raised their DDR5 memory options today by 50% for the Framework Laptop DIY Edition. Framework Computer is keeping the prior prices for existing pre-orders and also is foregoing any price changes for their pre-built laptops or the Framework Desktop. Framework Computer also lets you order DIY laptops without any memory at all if so desired for re-using existing modules or should you score a deal elsewhere.&lt;lb/&gt;Due to their memory pricing said to be more competitive below market rates, they also adjusted their return policy to prevent scalpers from purchasing DIY Edition laptops with memory while then returning just the laptops. The DDR5 must be returned now with DIY laptop order returns.&lt;lb/&gt;More details on Framework Computer needing to begin raising system memory prices can be found via the Framework Blog.&lt;/p&gt;
    &lt;p&gt;Due to the ongoing price hikes around system memory with shortages throughout the supply chain, Framework raised their DDR5 memory options today by 50% for the Framework Laptop DIY Edition. Framework Computer is keeping the prior prices for existing pre-orders and also is foregoing any price changes for their pre-built laptops or the Framework Desktop. Framework Computer also lets you order DIY laptops without any memory at all if so desired for re-using existing modules or should you score a deal elsewhere.&lt;/p&gt;
    &lt;p&gt;Due to their memory pricing said to be more competitive below market rates, they also adjusted their return policy to prevent scalpers from purchasing DIY Edition laptops with memory while then returning just the laptops. The DDR5 must be returned now with DIY laptop order returns.&lt;/p&gt;
    &lt;p&gt;More details on Framework Computer needing to begin raising system memory prices can be found via the Framework Blog.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46245331</guid><pubDate>Fri, 12 Dec 2025 15:58:10 +0000</pubDate></item><item><title>Epic celebrates "the end of the Apple Tax" after court win in iOS payments case</title><link>https://arstechnica.com/tech-policy/2025/12/epic-celebrates-the-end-of-the-apple-tax-after-appeals-court-win-in-ios-payments-case/</link><description>&lt;doc fingerprint="8684f31c96ad1135"&gt;
  &lt;main&gt;
    &lt;p&gt;Back in April, District Court Judge Yvonne Gonzalez Rogers delivered a scathing judgment finding that Apple was in “willful violation” of her 2021 injunction intended to open up iOS App Store payments. That contempt of court finding has now been almost entirely upheld by the Ninth Circuit Court of Appeals, a development that Epic Games’ Tim Sweeney tells Ars he hopes will “do a lot of good for developers and start to really change the App Store situation worldwide, I think.”&lt;/p&gt;
    &lt;p&gt;The ruling, signed by a panel of three appellate court judges, affirmed that Apple’s initial attempts to charge a 27 percent fee to iOS developers using outside payment options “had a prohibitive effect, in violation of the injunction.” Similarly, Apple’s restrictions on how those outside links had to be designed were overly broad; the appeals court suggests that Apple can only ensure that internal and external payment options are presented in a similar fashion.&lt;/p&gt;
    &lt;p&gt;The appeals court also agreed that Apple acted in “bad faith” by refusing to comply with the injunction, rejecting viable, compliant alternatives in internal discussions. And the appeals court was also not convinced by Apple’s process-focused arguments, saying the district court properly evaluated materials Apple argued were protected by attorney-client privilege.&lt;/p&gt;
    &lt;p&gt;While the district court barred Apple from charging any fees for payments made outside of its App Store, the appeals court now suggests that Apple should still be able to charge a “reasonable fee” based on its “actual costs to ensure user security and privacy.” It will be up to Apple and the district court to determine what that kind of “reasonable fee” should look like going forward.&lt;/p&gt;
    &lt;p&gt;Speaking to reporters Thursday night, though, Epic founder and CEO Tim Sweeney said he believes those should be “super super minor fees,” on the order of “tens or hundreds of dollars” every time an iOS app update goes through Apple for review. That should be more than enough to compensate the employees reviewing the apps to make sure outside payment links are not scams and lead to a system of “normal fees for normal businesses that sell normal things to normal customers,” Sweeney said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46245398</guid><pubDate>Fri, 12 Dec 2025 16:04:16 +0000</pubDate></item><item><title>Senator endorses discredited book that claims chemical treats autism, cancer</title><link>https://www.propublica.org/article/ron-johnson-wisconsin-chlorine-dioxide-pierre-kory-endorsement</link><description>&lt;doc fingerprint="fe49d453b29b11da"&gt;
  &lt;main&gt;
    &lt;p&gt;For years, Sen. Ron Johnson has been spreading conspiracy theories and misinformation about COVID-19 and the safety of vaccines.&lt;/p&gt;
    &lt;p&gt;He’s promoted disproven treatments for COVID-19 and claimed, without evidence, that athletes are “dropping dead on the field” after getting the COVID-19 vaccination. Now the Wisconsin politician is endorsing a book by a discredited doctor promoting an unproven and dangerous treatment for autism and a host of ailments: chlorine dioxide, a chemical used for disinfecting and bleaching.&lt;/p&gt;
    &lt;p&gt;The book is “The War on Chlorine Dioxide: The Medicine that Could End Medicine” by Dr. Pierre Kory, a critical care specialist who practiced in Wisconsin hospitals before losing his medical certification for statements advocating using an antiparasite medication to treat COVID-19. The action, he’s said, makes him unemployable, even though he still has a license.&lt;/p&gt;
    &lt;p&gt;Kory has said there’s a globally coordinated campaign by public health agencies, the drug industry and the media to suppress evidence of the medicinal wonders of chlorine dioxide. His book, according to its website, contends that the “remarkable molecule” works “to treat everything from cancer and malaria to autism and COVID.”&lt;/p&gt;
    &lt;p&gt;The book jacket features a prominent blurb from Johnson calling the doctor’s treatise: “A gripping tale of corruption and courage that will open eyes and prompt serious questions.”&lt;/p&gt;
    &lt;p&gt;Chlorine dioxide is a chemical compound that has a range of applications, including as a disinfectant and deodorizer. Food processing plants apply it to sanitize surfaces and equipment. Hospitals use it to sterilize medical devices, and some municipalities use low levels to treat public water supplies. Paper mills rely on it to whiten wood pulp. Safety experts advise those who handle it to work in well-ventilated spaces and to wear protective gloves.&lt;/p&gt;
    &lt;p&gt;Concentrations in drinking water systems higher than 0.8 milligrams per liter can be harmful, especially to infants, young children and fetuses, according to the Environmental Protection Agency.&lt;/p&gt;
    &lt;p&gt;Still, for many years people in online discussion groups have been promoting the use of chlorine dioxide in a mixture that they call a “miracle mineral solution,” ingested to rid people of a host of maladies. The Food and Drug Administration has warned that drinking these chlorine dioxide mixtures can cause injury and even death.&lt;/p&gt;
    &lt;p&gt;It is not medicinal, despite Kory’s contention. “It is all lunacy. Absolutely, it’s 100% nonsense,” said Joe Schwarcz, director of McGill University’s Office for Science and Society in Montreal and an expert on the threat of pseudoscience. Schwarcz has written articles about the so-called miracle mineral solution, calling it “a poison” when it’s in high concentrations.&lt;/p&gt;
    &lt;p&gt;Kory’s book, set to be released to the public in January, argues that word of chlorine dioxide’s effectiveness has been suppressed by government and medical forces that need people to remain perpetually ill to generate large profits. The use of the word “war” in the title is fitting, Kory said in a recent online video on his co-author’s Substack. “In the book I detail many, many assassination attempts of doctors who try to bring out knowledge around chlorine dioxide,” he said.&lt;/p&gt;
    &lt;p&gt;Johnson confirmed to ProPublica in an email that he authorized the statement on the cover. “After reading the entire book, yes I provided and approved that blurb,” he said. “Have you read the book?”&lt;/p&gt;
    &lt;p&gt;ProPublica asked Kory and his co-author, Jenna McCarthy, to provide an advance copy, an interview and responses to written questions. Kory did not respond. McCarthy wrote in an email to ProPublica that she was addressing some of the questions on her Substack. (She did not send a book or agree to an interview.)&lt;/p&gt;
    &lt;p&gt;The book “is a comprehensive examination of the existing evidence and a plea for open-minded inquiry and rigorous research,” she wrote on Substack. She dismissed warnings about chlorine dioxide’s toxicity in high concentrations, writing: “Everything has a toxic dose — including nutmeg, spinach, and tap water.”&lt;/p&gt;
    &lt;p&gt;She said that chlorine dioxide is being studied in controlled settings by researchers in the United States and Latin America and that “the real debate is how it should be used, at what dose, and in which clinical contexts.”&lt;/p&gt;
    &lt;p&gt;Her Substack post was signed “Jenna (&amp;amp; Pierre).”&lt;/p&gt;
    &lt;p&gt;Johnson did not agree to an interview and did not answer questions emailed to his office by ProPublica, including whether he views chlorine dioxide as a world-changing medical treatment and whether he believes the FDA warnings are false.&lt;/p&gt;
    &lt;head rend="h3"&gt;“It’s Called Snake Oil”&lt;/head&gt;
    &lt;p&gt;Johnson has been an advocate of Kory’s for years, calling the doctor as an expert witness in two 2020 Senate hearings. In one, Kory championed taking the drug ivermectin, an antiparasite medicine, to treat COVID-19.&lt;/p&gt;
    &lt;p&gt;In 2021, an analysis of data from clinical trials concluded that ivermectin could reduce deaths from COVID-19 and may produce other positive effects. McCarthy cited that analysis in her Substack response.&lt;/p&gt;
    &lt;p&gt;In 2022, however, the American Journal of Therapeutics, which had published the study, warned that suspicious data “appears to invalidate the findings” regarding ivermectin’s potential to decrease deaths.&lt;/p&gt;
    &lt;p&gt;Later clinical trials have found no beneficial effect of ivermectin for COVID-19, and the FDA has warned that taking large doses can be dangerous. The drug’s manufacturer has said it hadn’t found any scientific basis for the idea that ivermectin can effectively treat COVID-19. Kory, though, continued advocating for ivermectin.&lt;/p&gt;
    &lt;p&gt;In 2024 the American Board of Internal Medicine, which credentials physicians in certain specialties, revoked Kory’s certifications in internal medicine, pulmonary disease and critical care for making false and misleading public statements about the ability of ivermectin to treat COVID-19. Hospitals and many insurance networks typically require doctors to be board certified.&lt;/p&gt;
    &lt;p&gt;Kory vigorously fought the disciplinary action, arguing to the ABIM that he provided substantial medical and scientific evidence to support his recommendations for addressing COVID-19, though not the “consensus-driven” approach. He also sued the board in federal court, citing his free speech rights in a case that is still progressing in the 5th U.S. Circuit Court of Appeals. On Substack, McCarthy excoriated the ABIM, saying it “bullies physicians” and “enforces ideological conformity.”&lt;/p&gt;
    &lt;p&gt;In 2022, Johnson and Kory penned a Fox News op-ed opposing a California bill that would strip doctors’ licenses for espousing misinformation about COVID-19. The bill became law but was repealed after a court fight. A federal judge found the statute’s definition of misinformation to be too vague, which could infringe on doctors’ right to free speech.&lt;/p&gt;
    &lt;p&gt;Johnson, who has been in Congress since 2011, has a history of advocating for experimental treatments and viewing the government as an impediment. Dr. Peter Lurie, president and executive director of the Center for Science in the Public Interest, a public health advocacy group, said that among members of Congress, Johnson was “an early adopter of anti-science ideas.”&lt;/p&gt;
    &lt;p&gt;Lurie said that Johnson is no longer an outlier in Washington, which now has many more elected lawmakers whom he considers anti-science. “What may have started off as the cutting edge of an anti-science movement has now turned into a much more broader-based movement that is supported by millions of people,” he said.&lt;/p&gt;
    &lt;p&gt;Earlier this year, Johnson held a hearing highlighting a flawed study claiming that vaccinated children had an increased rate of serious chronic diseases when compared to children who were not vaccinated. The conclusion questions the scientific consensus that vaccines are safe. The study’s researchers chose not to publish it because of problems they found in their data and methodology.&lt;/p&gt;
    &lt;p&gt;In November, Johnson and Kory were listed among the speakers at a conference of the Children’s Health Defense, a nonprofit that stirs anti-vaccine sentiment. It was launched in 2018 by Health and Human Services Secretary Robert F. Kennedy Jr., whose FDA is considering new ways to more closely scrutinize vaccine safety.&lt;/p&gt;
    &lt;p&gt;HHS did not respond to requests from ProPublica about Kennedy’s views on chlorine dioxide. At his confirmation hearing, Kennedy praised President Donald Trump for his wide search for a COVID-19 remedy in his first term, which Kennedy said included vaccines, various drugs, “even chlorine dioxide.”&lt;/p&gt;
    &lt;p&gt;Kory’s publisher is listed as Bella Luna Press, which has issued at least two other titles by McCarthy. “Thanks to the Censorship Industrial Complex, you won’t find The War on Chlorine Dioxide on Amazon or at Barnes &amp;amp; Noble. We had to design and build this website, figure out formatting and printing and shipping, and manage every aspect of order processing ourselves,” the book’s website states. (A representative for Bella Luna could not be reached for comment.)&lt;/p&gt;
    &lt;p&gt;As this new book is released, the autism community is also grappling with another controversy: the unsubstantiated assertion by Kennedy that Tylenol use by pregnant women poses an increased risk of autism. In addition, under Kennedy, the Centers for Disease Control and Prevention revised its website in November to cast doubt on the long-held scientific conclusion that childhood vaccines do not cause autism.&lt;/p&gt;
    &lt;p&gt;Some parents of children with autism, desperate for a remedy, have long reached for dubious and at times dangerous panaceas, including hyperbaric oxygen chambers and chelation therapy, used for the treatment of heavy metal poisoning. Neither method has been proven effective.&lt;/p&gt;
    &lt;p&gt;Helen Tager-Flusberg, director of the Center for Autism Research Excellence at Boston University, said Johnson has “acted extremely irresponsibly” in lending his name to a book making claims about chlorine dioxide treating autism.&lt;/p&gt;
    &lt;p&gt;“Wisconsin is filled with experts — clinical experts, medical experts, scientists — who understand and have studied autism and treatments for autism for many many years,” she said. “He’s chosen to completely ignore the clinical and the scientific community.”&lt;/p&gt;
    &lt;p&gt;People with autism may take medication to reduce anxiety, address attention problems, or reduce severe irritability. Many benefit from behavioral interventions and special education services to help with learning and functional abilities. But there is no cure, said Tager-Flusberg.&lt;/p&gt;
    &lt;p&gt;Referring to chlorine dioxide, she said: “We have had examples of this probably throughout the history of medicine. There’s a word for this, it’s called snake oil.”&lt;/p&gt;
    &lt;p&gt;In her response on Substack to ProPublica, McCarthy wrote that “chlorine dioxide is being used to treat (nobody said ‘cure’) autism with life-changing results.”&lt;/p&gt;
    &lt;head rend="h3"&gt;The Search for Miracle Cures&lt;/head&gt;
    &lt;p&gt;The mother of an autistic son, Melissa Eaton of North Carolina, heard Kory reference his book in early November on The HighWire, an internet talk show hosted by Del Bigtree, a prominent vaccine skeptic and former communications director for Kennedy’s 2024 presidential campaign. She then looked up the book online and noticed Johnson’s endorsement.&lt;/p&gt;
    &lt;p&gt;Eaton for many years has worked to expose people who peddle chlorine dioxide and to report apparent injuries to authorities. She monitors social media forums where parents discuss giving it to their children orally or via enemas. Sometimes the families reveal that their children are sick. “They’re throwing up and vomiting and having diarrhea and rashes,” Eaton said.&lt;/p&gt;
    &lt;p&gt;Some adherents advise parents that the disturbing effects indicate that the treatment is working, ridding the body of impurities, or that the parents should alter the dosage.&lt;/p&gt;
    &lt;p&gt;“Most of these kids are nonverbal,” Eaton said. “They’re not able to say what’s hurting them or what’s happening to them. The parents feel they’re doing the right thing. That’s how they view this: They’re helping to cure autism.”&lt;/p&gt;
    &lt;p&gt;The idea that chlorine dioxide can be a miracle cure began to spread about 20 years ago when a gold prospector, Jim Humble, wrote a book claiming his team in Guyana fell ill with malaria and recovered after drinking safe amounts of chlorine dioxide.&lt;/p&gt;
    &lt;p&gt;Humble later co-founded a “health and healing” church in Florida with a man named Mark Grenon, who called himself an archbishop and sold a chlorine dioxide solution as a cure for COVID-19. They described it as a “miracle mineral solution,” or MMS.&lt;/p&gt;
    &lt;p&gt;Grenon went to prison in 2023 for conspiring to defraud the United States by distributing an unapproved and misbranded drug. The scheme took in more than $1 million, according to prosecutors.&lt;/p&gt;
    &lt;p&gt;An affidavit in the case filed by a special agent with the FDA Office of Criminal Investigations noted: “FDA has received numerous reports of adverse reactions to MMS. These adverse reactions include hospitalizations, life-threatening conditions, and death.”&lt;/p&gt;
    &lt;p&gt;Grenon, who is now out of prison, told ProPublica that he too is writing a book about chlorine dioxide. “My book will tell the truth.” He declined further comment.&lt;/p&gt;
    &lt;p&gt;Chlorine dioxide is currently used in many ways that are not harmful. It is found in some consumer products like mouthwashes, but it is not meant to be swallowed in those instances. (One popular mouthwash warns to “keep out of reach of children.”) It’s also available to consumers in do-it-yourself packages where they combine drops from two bottles of different compounds — commonly sodium chlorite and hydrochloric acid — and add it to water. Hikers often carry the drops, or tablets, using small amounts to make quarts of fresh water potable.&lt;/p&gt;
    &lt;p&gt;But numerous online shoppers post product reviews that go further, referring to it as a tonic. Various online guides, some aimed at parents of autistic children, recommend a shot-glass-size dose, sometimes given multiple times a day and even hourly. That can far exceed the threshold the EPA considers safe.&lt;/p&gt;
    &lt;p&gt;McCarthy, addressing ProPublica on Substack, wrote: “You point to various online guides that offer what could be considered dangerous dosing instructions. We agree, the internet is a terrifying wasteland of misinformation and disinformation.”&lt;/p&gt;
    &lt;p&gt;In the Substack video, Kory said he felt compelled to spread the word about chlorine dioxide much as he did about ivermectin, even though it cost him professionally.&lt;/p&gt;
    &lt;p&gt;He no longer has a valid medical license in Wisconsin or California, where he did not renew them, according to the Substack post. His medical licenses in New York and Michigan are active.&lt;/p&gt;
    &lt;p&gt;“I like to say I was excommunicated from the church of the medical establishment,” he said in the Substack video. As a result, he said, he turned to telehealth and started a practice.&lt;/p&gt;
    &lt;p&gt;In the Nov. 6 HighWire episode hosted by Bigtree, the discussion included talk not just of chlorine dioxide’s medicinal potential but also of how cheap and easy it is to obtain.&lt;/p&gt;
    &lt;p&gt;“On Amazon, it’s literally, you get two bottles, well, it comes in two,” Kory started to explain, before stopping that train of thought.&lt;/p&gt;
    &lt;p&gt;“I wouldn’t know how to make it,” he said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46245763</guid><pubDate>Fri, 12 Dec 2025 16:37:04 +0000</pubDate></item><item><title>America's Betting Craze Has Spread to Its News Networks</title><link>https://www.newyorker.com/news/the-lede/americas-betting-craze-has-spread-to-its-news-networks</link><description>&lt;doc fingerprint="b28edb3d9bb575c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Having swallowed sports media, will gambling now devour other kinds of news? Last week, CNN announced a deal with Kalshi, a federally regulated online exchange where Americans can wager on current events, from basketball games and congressional elections to whether it will rain tomorrow in New York City. This marked Kalshi’s first partnership with a major news organization and, according to several close observers of the media business and gambling industry, could foreshadow a deluge of similar deals. After all, a decade ago, many outlets refused to even mention sports-betting odds. Then, in a blink, the shilling became inescapable.&lt;/p&gt;
    &lt;p&gt;Gambling has been creeping into political coverage for a while. Prediction markets, as sites like Kalshi are called, use odds that can also be interpreted as probabilities, and, because those odds reflect the distilled wisdom of everyone willing to put skin in the game, they have the allure of a crystal ball. A prediction market associated with the magazine Le Point, for example, has anticipated the results of the past two French Presidential elections more accurately than top polling firms. It’s now routine for American journalists, when assessing the state of a political race, to cite betting odds as a counterpoint to polls. Shortly after unveiling its partnership with Kalshi, however, CNN seemed willing to integrate gambling to a far more jarring degree.&lt;/p&gt;
    &lt;p&gt;While discussing the strain of tariffs, anchor John Berman teed up Harry Enten, CNN’s chief data analyst, for a Kalshi plug, asking if Americans expect to receive stimulus checks from the government. “One of the best ways we can look at this is [through] the prediction markets,” Enten replied, “because they give you an indication of where people are putting their money where their mouth is.” The screen behind him showed that bettors on Kalshi peg the likelihood of checks arriving by next August at twenty-five per cent. Throughout the segment, a ticker displayed a stream of unrelated odds, including whether Defense Secretary Pete Hegseth would be the first person to depart from this Administration’s Cabinet (thirty-one per cent), and whether Time magazine’s Person of the Year would be Pope Leo XIV (twelve per cent) or A.I. (forty-seven per cent).&lt;/p&gt;
    &lt;p&gt;A couple of days later, CNBC announced a deal with Kalshi, too. “Every day, our reporters unpack the biggest questions facing companies and the economy,” CNBC president KC Sullivan wrote to me in an e-mail. “They will now have access to exclusive data from Kalshi to more deeply understand public sentiment in real time.” He declined to elaborate on what, exactly, makes this data exclusive. In recent years, sports-gambling operators have paid media outlets handsomely for sending customers their way, as much as five hundred dollars after a person clicks a link and creates a sportsbook account. Sullivan said CNBC’s arrangement with Kalshi includes “a customer acquisition component,” but the larger terms of that deal, as well as the one with CNN, haven’t been disclosed.&lt;/p&gt;
    &lt;p&gt;Both CNN and Kalshi declined to comment for this piece. After Kalshi’s C.E.O., Tarek Mansour, told Axios that the network isn’t paying to license Kalshi data, several people I spoke with took that to mean that no money was changing hands in the deal. A CNN source clarified that Kalshi is paying CNN to be the only prediction market referenced in coverage, though CNN’s articles won’t feature clickable links directing readers to Kalshi. Last month, meanwhile, Yahoo Finance announced an agreement to exclusively cite betting data from Kalshi’s rival Polymarket, a crypto-based prediction market.&lt;/p&gt;
    &lt;p&gt;Suddenly, prediction markets have a lot of money to spend. When I reported in 2022 on the potential for more betting on politics, those platforms existed in the U.S. either as tightly controlled academic experiments, or illegally. In the years since, federal courts have denied attempts by the Commodity Futures Trading Commission to restrict election betting, allowing Kalshi and its competitors to run rampant—at least while more lawsuits make their way through the courts. These companies argue that their product doesn’t meet the legal definition of gambling, even when users risk money on the outcomes of professional and college sporting events, which now account for the vast majority of betting on Kalshi. Several states have sued, accusing the company of functioning, in essence, as an unlicensed bookie. (The country’s two leading sportsbooks, FanDuel and DraftKings, faced an interesting dilemma: stand beside the gaming lobby in its fight against prediction markets, or join the fray, seizing on the loophole that allows sites such as Kalshi and Polymarket to operate in states that have yet to legalize bookmaking, such as California and Texas. Soon enough, both sportsbooks announced plans to launch their own prediction markets in the near future.)&lt;/p&gt;
    &lt;p&gt;Kalshi is valued at eleven billion dollars—making the co-founder Luana Lopes Lara, at twenty-nine, “the world’s youngest self-made woman billionaire,” according to Forbes. Polymarket isn’t far behind; in October, Intercontinental Exchange invested two billion dollars in the company, reflecting an eight-billion-dollar valuation. In a recent interview on “60 Minutes,” Polymarket’s twentysomething C.E.O., Shayne Coplan, said he’d like to see the company grow in the coming years from “mid-hundreds of thousands” of users to a billion. To help achieve such grand ambitions, Kalshi and Polymarket brought on Donald Trump, Jr., as an adviser. (When Saudi Arabia’s leader, Mohammed bin Salman, received a warm welcome at the White House recently, he appeared to return the favor by promoting prediction markets, noting that people could have bet on whether he would have arrived dressed in a suit, as opposed to his traditional robes.)&lt;/p&gt;
    &lt;p&gt;As they fight for survival in court, Kalshi and Polymarket are also battling to be taken seriously, though their advertising doesn’t always advance that cause. Ahead of New York’s mayoral election last month, a billboard for Polymarket in Times Square showed Zohran Mamdani and Andrew Cuomo face to face, but with their photos edited to give them bare chests, like prizefighters at a weigh-in. (The race drew nearly half a billion dollars in bets on Polymarket.) Both platforms offer a variety of “mention” markets, allowing people to bet on things like whether OpenAI C.E.O. Sam Altman would say the word “privacy” during an appearance on “The Tonight Show.”&lt;/p&gt;
    &lt;p&gt;For Kalshi, the imprimatur of CNN and CNBC could go a long way. A ticker advertising Kalshi’s betting odds and displaying Kalshi’s logo while journalists tout the credibility of that data “is a very legitimizing force for a gambling company,” Oliver Darcy, a media reporter who left CNN last year to start a newsletter, Status, told me. Like so many ailing news outlets, he said, CNN is in a tough spot; earlier this year, the company said it would lay off about two hundred employees—roughly six per cent of its workforce. “I think that’s loosened their standards for potential partners tremendously,” Darcy said, adding that the Kalshi deal seemed ethically and editorially “dubious.”&lt;/p&gt;
    &lt;p&gt;A former longtime CNN journalist, who requested to remain anonymous, objected to the deal on different grounds, saying that it seemed “gimmicky” for the network to be promoting betting odds. “Do they really believe it’s adding value to the coverage?”&lt;/p&gt;
    &lt;p&gt;The value of the data depends on the liquidity of a particular market; generally, the more money wagered, the more predictive the odds. There is no magic threshold for when a market should be taken seriously, but many of the most-cited election markets attract tens of millions of dollars in trading, at least. When Enten lauded the benefits of analyzing betting odds, on air the other day, he failed to mention that only several hundred thousand dollars had been bet on that particular market. Kalshi’s odds provided good fodder for television, but, statistically speaking, they didn’t say much.&lt;/p&gt;
    &lt;p&gt;How many news organizations, desperate for cash and for clicks, will move in a similar direction? Dan Pozner was the director of gambling content and partnerships at NBC Sports in 2020 when the company struck its first partnership with a sportsbook, PointsBet (since acquired by Fanatics, which just launched a prediction market in two dozen states). Some traditionalists at NBC were reluctant to promote gambling, Pozner recalled, but the prevailing mentality was, “They need to do what everyone else is doing to keep up, or they’re going to miss out.” Pozner doesn’t think many news organizations will get hung up on moral reservations this time, either. I heard something similar from Dustin Gouker, a reporter who also spent years facilitating affiliate marketing deals with media companies, and who now publishes a newsletter about prediction markets, Event Horizon. He agrees that CNN, CNBC, and Yahoo Finance will likely be trendsetters: “Bloomberg, the Wall Street Journal, the New York Times, Fox News, and on and on—why wouldn’t all of them do something like this?”&lt;/p&gt;
    &lt;p&gt;It’s easy to see the synergy between news and gambling on the news. Kalshi said it will create certain markets at CNBC’s request, though many news stories already have a corresponding betting market. After the Times published a front-page story last month about mounting evidence of the President showing his age, the odds on Kalshi that he’d be out of office by the end of next year increased to twenty-nine per cent. Kalshi is also accepting bets on extreme weather, like markets for whether an 8.0-magnitude earthquake will strike California before year’s end, or whether Mt. Etna will erupt in the same time frame. (There’s a one-per-cent and fifty-seven-per-cent chance, respectively.)&lt;/p&gt;
    &lt;p&gt;Of course, there’s something ghoulish about profiting from natural disasters—or wars. Polymarket takes bets on whether Israel will strike Gaza on a given day. There can also be strange feedback loops, Andrew Hall, a political scientist at Stanford, explained. (Hall also advises the venture-capital firm Andreessen Horowitz, an investor in Kalshi.) With political markets especially, “the news affects the prices, and then the prices are part of the news,” Hall told me. Coverage of Hegseth having the greatest odds of being the first Cabinet secretary ousted, for example, could boost those odds further, which could generate more coverage, which could eventually drive the President to fire him.&lt;/p&gt;
    &lt;p&gt;Entanglements with prediction markets might create other problems for journalists. Considering how significantly news coverage shapes betting odds, there’s ample opportunity for insider trading. Accentuating that conflict, news organizations are often designated as the source of truth for resolving a market. For example, Kalshi takes bets on whether certain people, such as the rapper Drake or the Pope, will visit the White House this year. The outcomes of those bets are determined by reporting in various outlets, including CNN. Kalshi’s rules prohibit any employee of a news outlet, anyone with “material non-public information,” or anyone with “the ability to influence the outcome of the contract” from trading. But, as Gouker, the Event Horizon writer, asked, “Are they actually stopping them?” Earlier this year, a Republican candidate for governor in California, Kyle Langford, said he bet a hundred dollars on Kalshi that he would win the election. Of course, he was prohibited from doing so, but the bet apparently went through. Kalshi said it was investigating.&lt;/p&gt;
    &lt;p&gt;There is a case to be made that, in moderation, engaging with prediction markets might serve a public good. Literally investing in news stories could make people more interested in keeping up with the news. It could even help combat misinformation. In 2023, Nature published results from a study in which skeptics and believers in climate change had to bet on short-term indicators of a planet in crisis, such as higher monthly temperatures, worsening air quality, and more extreme weather. Some skeptics nevertheless wagered on those outcomes, and after winning money over a couple of months, they were more inclined to call themselves believers in climate change. “If you win, whatever you win at becomes, to some extent, your identity,” one of the co-authors, the Columbia University neuroscientist Moran Cerf, told me.&lt;/p&gt;
    &lt;p&gt;Of course, persuading people that the Pope is a long shot to be Time’s Person of the Year is, as the University of Michigan economist Justin Wolfers put it, “entirely inconsequential.” (Time ultimately selected the “Architects of A.I.” On social media, many Kalshi users were irate to learn that bets on simply “A.I.,” the front-runner, were deemed a loss—illustrating how betting on the news sometimes fails to produce decisive outcomes.) A frequent guest commentator on CNN, Wolfers proposed a two-part test for whether journalists should invoke betting markets: “Is the information socially valuable, and is this the best mechanism for eliciting it?”&lt;/p&gt;
    &lt;p&gt;“One of the reasons I think it’s really useful in politics,” he explained, “is if you’re running a company and you need to know what’s going to happen to the tariffs, you could turn on Fox News and learn that tariffs are amazing and they’re going to continue forever. Or, you could turn on MSNBC and learn that tariffs are terrible and they’re going to be gone any minute now. In reality, you’re learning nothing either way. The market is a truth-telling device.”&lt;/p&gt;
    &lt;p&gt;Yet, despite being an early adopter of prediction-market data in his punditry, Wolfers said he’s grown concerned about news organizations promoting the betting platforms too aggressively. “There’s one group who are completely unrepresented in the policy debate,” he explained, “and they might be the most important, and that’s the compulsive gamblers.”&lt;/p&gt;
    &lt;p&gt;Wolfers grew up in gambling-crazed Australia, where he worked at a horse-racing track during college. He remembers placing the first-ever bet for one of his friends, who, over the next few years, developed a gambling addiction that caused him to sell his car, steal, and drive his father’s business into bankruptcy. “It was one of the saddest things I’ve ever seen,” Wolfers told me. As gambling is normalized in so many corners of American life, he said, “I’m worried about a mom or a dad throwing away their family’s future because this is addictive in a way that we know that hard drugs are.”&lt;/p&gt;
    &lt;p&gt;Signs of a potential gambling-fuelled public-health crisis emerge by the day. Last month, for instance, Bank of America reported that Sallie Mae and other student-loan giants face growing pressure because young people are losing so much money betting on sports, whether through traditional sportsbooks or through prediction markets. CNN and CNBC have devoted a significant amount of airtime to the dangers of online gambling, including a CNN documentary this past summer in which reporter Nick Watt challenged the head of the gaming lobby for refusing to concede that sports betting can be addictive.&lt;/p&gt;
    &lt;p&gt;The people behind prediction markets can be even more callous, while also offering even more opportunities to bet. (Last month, Mansour, the C.E.O. of Kalshi, said that the “long-term vision” for the company “is to financialize everything and create a tradable asset out of any difference in opinion.”) At a gaming conference in July, Josh Sterling, an attorney for Kalshi, was asked if regulators should enforce more consumer protections. “People are adults,” he answered, “and they’re allowed to spend their money however they want it, and if they lose their shirt, that’s on them.” ♦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46245843</guid><pubDate>Fri, 12 Dec 2025 16:43:04 +0000</pubDate></item><item><title>Why more American seniors are getting high</title><link>https://www.economist.com/graphic-detail/2025/12/11/why-more-american-seniors-are-getting-high</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46245971</guid><pubDate>Fri, 12 Dec 2025 16:56:31 +0000</pubDate></item></channel></rss>