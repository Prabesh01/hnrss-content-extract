<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 02 Jan 2026 08:48:40 +0000</lastBuildDate><item><title>Python numbers every programmer should know</title><link>https://mkennedy.codes/posts/python-numbers-every-programmer-should-know/</link><description>&lt;doc fingerprint="24f9e910fc8082ca"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;There are numbers every Python programmer should know. For example, how fast or slow is it to add an item to a list in Python? What about opening a file? Is that less than a millisecond? Is there something that makes that slower than you might have guessed? If you have a performance sensitive algorithm, which data structure should you use? How much memory does a floating point number use? What about a single character or the empty string? How fast is FastAPI compared to Django?&lt;/p&gt;
      &lt;p&gt;I wanted to take a moment and write down performance numbers specifically focused on Python developers. Below you will find an extensive table of such values. They are grouped by category. And I provided a couple of graphs for the more significant analysis below the table.&lt;/p&gt;
      &lt;p&gt;Acknowledgements: Inspired by Latency Numbers Every Programmer Should Know and similar resources.&lt;/p&gt;
      &lt;head rend="h3"&gt;Source code for the benchmarks&lt;/head&gt;
      &lt;p&gt;This article is posted without any code. I encourage you to dig into the benchmarks. The code is available on GitHub at:&lt;/p&gt;
      &lt;p&gt;https://github.com/mikeckennedy/python-numbers-everyone-should-know&lt;/p&gt;
      &lt;p&gt;The benchmarks were run on the sytem described in this table. While yours may be faster or slower, the most important thing to consider is relative comparisons.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Property&lt;/cell&gt;
          &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Python Version&lt;/cell&gt;
          &lt;cell&gt;CPython 3.14.2&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Hardware&lt;/cell&gt;
          &lt;cell&gt;Mac Mini M4 Pro&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Platform&lt;/cell&gt;
          &lt;cell&gt;macOS Tahoe (26.2)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Processor&lt;/cell&gt;
          &lt;cell&gt;ARM&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;CPU Cores&lt;/cell&gt;
          &lt;cell&gt;14 physical / 14 logical&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;RAM&lt;/cell&gt;
          &lt;cell&gt;24 GB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Timestamp&lt;/cell&gt;
          &lt;cell&gt;2025-12-30&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;TL;DR; Python Numbers&lt;/head&gt;
      &lt;p&gt;This first version is a quick ‚Äúpyramid‚Äù of growing time/size for common Python ops. There is much more detail below.&lt;/p&gt;
      &lt;p&gt;Python Operation Latency Numbers (the pyramid)&lt;/p&gt;
      &lt;quote&gt; Attribute read (obj.x) 14 ns Dict key lookup 22 ns 1.5x attr Function call (empty) 22 ns List append 29 ns 2x attr f-string formatting 65 ns 3x function Exception raised + caught 140 ns 10x attr orjson.dumps() complex object 310 ns 0.3 Œºs json.loads() simple object 714 ns 0.7 Œºs 2x orjson sum() 1,000 integers 1,900 ns 1.9 Œºs 3x json SQLite SELECT by primary key 3,600 ns 3.6 Œºs 5x json Iterate 1,000-item list 7,900 ns 7.9 Œºs 2x SQLite read Open and close file 9,100 ns 9.1 Œºs 2x SQLite read asyncio run_until_complete (empty) 28,000 ns 28 Œºs 3x file open Write 1KB file 35,000 ns 35 Œºs 4x file open MongoDB find_one() by _id 121,000 ns 121 Œºs 3x write 1KB SQLite INSERT (with commit) 192,000 ns 192 Œºs 5x write 1KB Write 1MB file 207,000 ns 207 Œºs 6x write 1KB import json 2,900,000 ns 2,900 Œºs 3 ms 15x write 1MB import asyncio 17,700,000 ns 17,700 Œºs 18 ms 6x import json import fastapi 104,000,000 ns 104,000 Œºs 104 ms 6x import asyncio &lt;/quote&gt;
      &lt;p&gt;Python Memory Numbers (the pyramid)&lt;/p&gt;
      &lt;quote&gt; Float 24 bytes Small int (cached 0-256) 28 bytes Empty string 41 bytes Empty list 56 bytes 2x int Empty dict 64 bytes 2x int Empty set 216 bytes 8x int __slots__ class (5 attrs) 212 bytes 8x int Regular class (5 attrs) 694 bytes 25x int List of 1,000 ints 36,056 bytes 36 KB Dict of 1,000 items 64,952 bytes 65 KB List of 1,000 __slots__ instances 81,000 bytes 81 KB List of 1,000 regular instances 169,000 bytes 169 KB 2x slots list Empty Python process 16,000,000 bytes 16 MB &lt;/quote&gt;
      &lt;head rend="h2"&gt;Python numbers you should know (detailed version)&lt;/head&gt;
      &lt;p&gt;Here is a deeper table comparing many more details.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Category&lt;/cell&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
          &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;üíæ Memory&lt;/cell&gt;
          &lt;cell&gt;Empty Python process&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;15.73 MB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty string&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;41 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;100-char string&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;141 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Small int (0-256)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Large int&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Float&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;24 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty list&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List with 1,000 ints&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;35.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List with 1,000 floats&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;32.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty dict&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;64 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict with 1,000 items&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;63.4 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty set&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;216 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set with 1,000 items&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;59.6 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Regular class instance (5 attrs)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;__slots__&lt;/code&gt; class instance (5 attrs)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 regular class instances&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;165.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 &lt;code&gt;__slots__&lt;/code&gt; class instances&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;79.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;dataclass instance&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;namedtuple instance&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;228 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;‚öôÔ∏è Basic Ops&lt;/cell&gt;
          &lt;cell&gt;Add two integers&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two floats&lt;/cell&gt;
          &lt;cell&gt;18.4 ns (54.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;String concatenation (small)&lt;/cell&gt;
          &lt;cell&gt;39.1 ns (25.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;f-string formatting&lt;/cell&gt;
          &lt;cell&gt;64.9 ns (15.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;.format()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;103 ns (9.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;%&lt;/code&gt; formatting&lt;/cell&gt;
          &lt;cell&gt;89.8 ns (11.1M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List append&lt;/cell&gt;
          &lt;cell&gt;28.7 ns (34.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List comprehension (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;9.45 Œºs (105.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Equivalent for-loop (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;11.9 Œºs (83.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;üì¶ Collections&lt;/cell&gt;
          &lt;cell&gt;Dict lookup by key&lt;/cell&gt;
          &lt;cell&gt;21.9 ns (45.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set membership check&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List index access&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List membership check (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;3.85 Œºs (259.6k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;len()&lt;/code&gt; on list&lt;/cell&gt;
          &lt;cell&gt;18.8 ns (53.3M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item list&lt;/cell&gt;
          &lt;cell&gt;7.87 Œºs (127.0k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item dict&lt;/cell&gt;
          &lt;cell&gt;8.74 Œºs (114.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;sum()&lt;/code&gt; of 1,000 ints&lt;/cell&gt;
          &lt;cell&gt;1.87 Œºs (534.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;üè∑Ô∏è Attributes&lt;/cell&gt;
          &lt;cell&gt;Read from regular class&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write to regular class&lt;/cell&gt;
          &lt;cell&gt;15.7 ns (63.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read from &lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write to &lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;16.4 ns (60.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read from &lt;code&gt;@property&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;getattr()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;13.8 ns (72.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;hasattr()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.8 ns (41.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;üìÑ JSON&lt;/cell&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (simple)&lt;/cell&gt;
          &lt;cell&gt;708 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (simple)&lt;/cell&gt;
          &lt;cell&gt;714 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;2.65 Œºs (376.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;2.22 Œºs (449.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;orjson.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;310 ns (3.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;orjson.loads()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;839 ns (1.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;ujson.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;1.64 Œºs (611.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;msgspec&lt;/code&gt; encode (complex)&lt;/cell&gt;
          &lt;cell&gt;445 ns (2.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Pydantic &lt;code&gt;model_dump_json()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;1.54 Œºs (647.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Pydantic &lt;code&gt;model_validate_json()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;2.99 Œºs (334.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;üåê Web Frameworks&lt;/cell&gt;
          &lt;cell&gt;Flask (return JSON)&lt;/cell&gt;
          &lt;cell&gt;16.5 Œºs (60.7k req/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Django (return JSON)&lt;/cell&gt;
          &lt;cell&gt;18.1 Œºs (55.4k req/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;FastAPI (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.63 Œºs (115.9k req/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Starlette (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.01 Œºs (124.8k req/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Litestar (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.19 Œºs (122.1k req/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;üìÅ File I/O&lt;/cell&gt;
          &lt;cell&gt;Open and close file&lt;/cell&gt;
          &lt;cell&gt;9.05 Œºs (110.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1KB file&lt;/cell&gt;
          &lt;cell&gt;10.0 Œºs (99.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1KB file&lt;/cell&gt;
          &lt;cell&gt;35.1 Œºs (28.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1MB file&lt;/cell&gt;
          &lt;cell&gt;207 Œºs (4.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;pickle.dumps()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.30 Œºs (769.6k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;pickle.loads()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.44 Œºs (695.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;üóÑÔ∏è Database&lt;/cell&gt;
          &lt;cell&gt;SQLite insert (JSON blob)&lt;/cell&gt;
          &lt;cell&gt;192 Œºs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;SQLite select by PK&lt;/cell&gt;
          &lt;cell&gt;3.57 Œºs (280.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;SQLite update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 Œºs (191.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;diskcache set&lt;/cell&gt;
          &lt;cell&gt;23.9 Œºs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;diskcache get&lt;/cell&gt;
          &lt;cell&gt;4.25 Œºs (235.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB insert_one&lt;/cell&gt;
          &lt;cell&gt;119 Œºs (8.4k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB find_one by _id&lt;/cell&gt;
          &lt;cell&gt;121 Œºs (8.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB find_one by nested field&lt;/cell&gt;
          &lt;cell&gt;124 Œºs (8.1k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;üìû Functions&lt;/cell&gt;
          &lt;cell&gt;Empty function call&lt;/cell&gt;
          &lt;cell&gt;22.4 ns (44.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Function with 5 args&lt;/cell&gt;
          &lt;cell&gt;24.0 ns (41.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Method call&lt;/cell&gt;
          &lt;cell&gt;23.3 ns (42.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Lambda call&lt;/cell&gt;
          &lt;cell&gt;19.7 ns (50.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;try/except (no exception)&lt;/cell&gt;
          &lt;cell&gt;21.5 ns (46.5M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;try/except (exception raised)&lt;/cell&gt;
          &lt;cell&gt;139 ns (7.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;isinstance()&lt;/code&gt; check&lt;/cell&gt;
          &lt;cell&gt;18.3 ns (54.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;‚è±Ô∏è Async&lt;/cell&gt;
          &lt;cell&gt;Create coroutine object&lt;/cell&gt;
          &lt;cell&gt;47.0 ns (21.3M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(empty)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;27.6 Œºs (36.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;asyncio.sleep(0)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;39.4 Œºs (25.4k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 10 coroutines&lt;/cell&gt;
          &lt;cell&gt;55.0 Œºs (18.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;create_task()&lt;/code&gt; + await&lt;/cell&gt;
          &lt;cell&gt;52.8 Œºs (18.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async with&lt;/code&gt; (context manager)&lt;/cell&gt;
          &lt;cell&gt;29.5 Œºs (33.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Memory Costs&lt;/head&gt;
      &lt;p&gt;Understanding how much memory different Python objects consume.&lt;/p&gt;
      &lt;head rend="h3"&gt;An empty Python process uses 15.73 MB&lt;/head&gt;
      &lt;head rend="h3"&gt;Strings&lt;/head&gt;
      &lt;p&gt;The rule of thumb for strings is the core string object takes 41 bytes. Each additional character is 1 byte.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;String&lt;/cell&gt;
          &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty string &lt;code&gt;""&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;41 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;1-char string &lt;code&gt;"a"&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;42 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;100-char string&lt;/cell&gt;
          &lt;cell&gt;141 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Numbers&lt;/head&gt;
      &lt;p&gt;Numbers are surprisingly large in Python. They have to derive from CPython‚Äôs &lt;code&gt;PyObject&lt;/code&gt; and are subject to reference counting for garabage collection, they exceed our typical mental model many of:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;2 bytes = short int&lt;/item&gt;
        &lt;item&gt;4 bytes = long int&lt;/item&gt;
        &lt;item&gt;etc.&lt;/item&gt;
      &lt;/list&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Small int (0-256, cached)&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Large int (1000)&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Very large int (10**100)&lt;/cell&gt;
          &lt;cell&gt;72 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Float&lt;/cell&gt;
          &lt;cell&gt;24 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Collections&lt;/head&gt;
      &lt;p&gt;Collections are amazing in Python. Dynamically growing lists. Ultra high-perf dictionaries and sets. Here is the empty and ‚Äúfull‚Äù overhead of each.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Collection&lt;/cell&gt;
          &lt;cell role="head"&gt;Empty&lt;/cell&gt;
          &lt;cell role="head"&gt;1,000 items&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (ints)&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
          &lt;cell&gt;35.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (floats)&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
          &lt;cell&gt;32.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict&lt;/cell&gt;
          &lt;cell&gt;64 bytes&lt;/cell&gt;
          &lt;cell&gt;63.4 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set&lt;/cell&gt;
          &lt;cell&gt;216 bytes&lt;/cell&gt;
          &lt;cell&gt;59.6 KB&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Classes and Instances&lt;/head&gt;
      &lt;p&gt;Slots are an interesting addition to Python classes. They remove the entire concept of a &lt;code&gt;__dict__&lt;/code&gt; for tracking fields and other values. Even for a single instance, slots classes are significantly smaller (212 bytes vs 694 bytes for 5 attributes). If you are holding a large number of them in memory for a list or cache, the memory savings of a slots class becomes very dramatic - over 2x less memory usage. Luckily for most use-cases, just adding a slots entry saves a ton of memory with minimal effort.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Empty&lt;/cell&gt;
          &lt;cell role="head"&gt;5 attributes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Regular class&lt;/cell&gt;
          &lt;cell&gt;344 bytes&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;32 bytes&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;dataclass&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;@dataclass(slots=True)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;namedtuple&lt;/cell&gt;
          &lt;cell&gt;‚Äî&lt;/cell&gt;
          &lt;cell&gt;228 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;Aggregate Memory Usage (1,000 instances):&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Total Memory&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 regular class instances&lt;/cell&gt;
          &lt;cell&gt;165.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 &lt;code&gt;__slots__&lt;/code&gt; class instances&lt;/cell&gt;
          &lt;cell&gt;79.1 KB&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Basic Operations&lt;/head&gt;
      &lt;p&gt;The cost of fundamental Python operations: Way slower than C/C++/C# but still quite fast. I added a brief comparison to C# to the source repo.&lt;/p&gt;
      &lt;head rend="h3"&gt;Arithmetic&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two integers&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two floats&lt;/cell&gt;
          &lt;cell&gt;18.4 ns (54.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Multiply two integers&lt;/cell&gt;
          &lt;cell&gt;19.4 ns (51.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;String Operations&lt;/head&gt;
      &lt;p&gt;String operations in Python are fast as well. f-strings are the fastest formatting style, while even the slowest style is still measured in just nano-seconds.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Concatenation (&lt;code&gt;+&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;39.1 ns (25.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;f-string&lt;/cell&gt;
          &lt;cell&gt;64.9 ns (15.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;.format()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;103 ns (9.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;%&lt;/code&gt; formatting&lt;/cell&gt;
          &lt;cell&gt;89.8 ns (11.1M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;List Operations&lt;/head&gt;
      &lt;p&gt;List operations are very fast in Python. Adding a single item usually requires 28ns. Said another way, you can do 35M appends per second. This is unless the list has to expand using something like a doubling algorithm. You can see this in the ops/sec for 1,000 items.&lt;/p&gt;
      &lt;p&gt;Surprisingly, list comprehensions are 26% faster than the equivalent for loops with append statements.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;list.append()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;28.7 ns (34.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List comprehension (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;9.45 Œºs (105.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Equivalent for-loop (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;11.9 Œºs (83.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Collection Access and Iteration&lt;/head&gt;
      &lt;p&gt;How fast can you get data out of Python‚Äôs built-in collections? Here is a dramatic example of how much faster the correct data structure is. &lt;code&gt;item in set&lt;/code&gt; or &lt;code&gt;item in dict&lt;/code&gt; is 200x faster than &lt;code&gt;item in list&lt;/code&gt; for just 1,000 items!&lt;/p&gt;
      &lt;p&gt;The graph below is non-linear in the x-axis.&lt;/p&gt;
      &lt;head rend="h3"&gt;Access by Key/Index&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict lookup by key&lt;/cell&gt;
          &lt;cell&gt;21.9 ns (45.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set membership (&lt;code&gt;in&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List index access&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List membership (&lt;code&gt;in&lt;/code&gt;, 1,000 items)&lt;/cell&gt;
          &lt;cell&gt;3.85 Œºs (259.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Length&lt;/head&gt;
      &lt;p&gt;&lt;code&gt;len()&lt;/code&gt; is very fast. Maybe we don‚Äôt have to optimize it out of the test condition on a while loop looping 100 times after all.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Collection&lt;/cell&gt;
          &lt;cell role="head"&gt;&lt;code&gt;len()&lt;/code&gt; time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;18.8 ns (53.3M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;18.0 ns (55.5M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Iteration&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item list&lt;/cell&gt;
          &lt;cell&gt;7.87 Œºs (127.0k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item dict (keys)&lt;/cell&gt;
          &lt;cell&gt;8.74 Œºs (114.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;sum()&lt;/code&gt; of 1,000 integers&lt;/cell&gt;
          &lt;cell&gt;1.87 Œºs (534.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Class and Object Attributes&lt;/head&gt;
      &lt;p&gt;The cost of reading and writing attributes, and how &lt;code&gt;__slots__&lt;/code&gt; changes things. Slots saves over 2x the memory usage on large collections, with virtually identical attribute access speed.&lt;/p&gt;
      &lt;head rend="h3"&gt;Attribute Access&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Regular Class&lt;/cell&gt;
          &lt;cell role="head"&gt;&lt;code&gt;__slots__&lt;/code&gt; Class&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read attribute&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write attribute&lt;/cell&gt;
          &lt;cell&gt;15.7 ns (63.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;16.4 ns (60.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Other Attribute Operations&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read &lt;code&gt;@property&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;getattr(obj, 'attr')&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;13.8 ns (72.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;hasattr(obj, 'attr')&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.8 ns (41.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;JSON and Serialization&lt;/head&gt;
      &lt;p&gt;Comparing standard library JSON with optimized alternatives. &lt;code&gt;orjson&lt;/code&gt; handles more data types and is over 8x faster than standard lib &lt;code&gt;json&lt;/code&gt; for complex objects. Impressive!&lt;/p&gt;
      &lt;head rend="h3"&gt;Serialization (dumps)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Library&lt;/cell&gt;
          &lt;cell role="head"&gt;Simple Object&lt;/cell&gt;
          &lt;cell role="head"&gt;Complex Object&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json&lt;/code&gt; (stdlib)&lt;/cell&gt;
          &lt;cell&gt;708 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;2.65 Œºs (376.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;orjson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;60.9 ns (16.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;310 ns (3.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;ujson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;264 ns (3.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;1.64 Œºs (611.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;msgspec&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;92.3 ns (10.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;445 ns (2.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Deserialization (loads)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Library&lt;/cell&gt;
          &lt;cell role="head"&gt;Simple Object&lt;/cell&gt;
          &lt;cell role="head"&gt;Complex Object&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json&lt;/code&gt; (stdlib)&lt;/cell&gt;
          &lt;cell&gt;714 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;2.22 Œºs (449.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;orjson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;106 ns (9.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;839 ns (1.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;ujson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;268 ns (3.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;1.46 Œºs (682.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;msgspec&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;101 ns (9.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;850 ns (1.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Pydantic&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;model_dump_json()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.54 Œºs (647.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;model_validate_json()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;2.99 Œºs (334.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;model_dump()&lt;/code&gt; (to dict)&lt;/cell&gt;
          &lt;cell&gt;1.71 Œºs (585.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;model_validate()&lt;/code&gt; (from dict)&lt;/cell&gt;
          &lt;cell&gt;2.30 Œºs (435.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Web Frameworks&lt;/head&gt;
      &lt;p&gt;Returning a simple JSON response. Benchmarked with &lt;code&gt;wrk&lt;/code&gt; against localhost running 4 works in Granian. Each framework returns the same JSON payload from a minimal endpoint. No database access or that sort of thing. This is just how much overhead/perf do we get from each framework itself. The code we write that runs within those view methods is largely the same.&lt;/p&gt;
      &lt;head rend="h3"&gt;Results&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Framework&lt;/cell&gt;
          &lt;cell role="head"&gt;Requests/sec&lt;/cell&gt;
          &lt;cell role="head"&gt;Latency (p99)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Flask&lt;/cell&gt;
          &lt;cell&gt;16.5 Œºs (60.7k req/sec)&lt;/cell&gt;
          &lt;cell&gt;20.85 ms (48.0 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Django&lt;/cell&gt;
          &lt;cell&gt;18.1 Œºs (55.4k req/sec)&lt;/cell&gt;
          &lt;cell&gt;170.3 ms (5.9 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;FastAPI&lt;/cell&gt;
          &lt;cell&gt;8.63 Œºs (115.9k req/sec)&lt;/cell&gt;
          &lt;cell&gt;1.530 ms (653.6 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Starlette&lt;/cell&gt;
          &lt;cell&gt;8.01 Œºs (124.8k req/sec)&lt;/cell&gt;
          &lt;cell&gt;930 Œºs (1.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Litestar&lt;/cell&gt;
          &lt;cell&gt;8.19 Œºs (122.1k req/sec)&lt;/cell&gt;
          &lt;cell&gt;1.010 ms (990.1 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;File I/O&lt;/head&gt;
      &lt;p&gt;Reading and writing files of various sizes. Note that the graph is non-linear in y-axis.&lt;/p&gt;
      &lt;head rend="h3"&gt;Basic Operations&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Open and close (no read)&lt;/cell&gt;
          &lt;cell&gt;9.05 Œºs (110.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1KB file&lt;/cell&gt;
          &lt;cell&gt;10.0 Œºs (99.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1MB file&lt;/cell&gt;
          &lt;cell&gt;33.6 Œºs (29.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1KB file&lt;/cell&gt;
          &lt;cell&gt;35.1 Œºs (28.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1MB file&lt;/cell&gt;
          &lt;cell&gt;207 Œºs (4.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Pickle vs JSON to Disk&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;pickle.dumps()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;1.30 Œºs (769.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;pickle.loads()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;1.44 Œºs (695.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;2.72 Œºs (367.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;2.35 Œºs (425.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Database and Persistence&lt;/head&gt;
      &lt;p&gt;Comparing SQLite, diskcache, and MongoDB using the same complex object.&lt;/p&gt;
      &lt;head rend="h3"&gt;Test Object&lt;/head&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;user_data = {
    "id": 12345,
    "username": "alice_dev",
    "email": "alice@example.com",
    "profile": {
        "bio": "Software engineer who loves Python",
        "location": "Portland, OR",
        "website": "https://alice.dev",
        "joined": "2020-03-15T08:30:00Z"
    },
    "posts": [
        {"id": 1, "title": "First Post", "tags": ["python", "tutorial"], "views": 1520},
        {"id": 2, "title": "Second Post", "tags": ["rust", "wasm"], "views": 843},
        {"id": 3, "title": "Third Post", "tags": ["python", "async"], "views": 2341},
    ],
    "settings": {
        "theme": "dark",
        "notifications": True,
        "email_frequency": "weekly"
    }
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;head rend="h3"&gt;SQLite (JSON blob approach)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Insert one object&lt;/cell&gt;
          &lt;cell&gt;192 Œºs (5.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Select by primary key&lt;/cell&gt;
          &lt;cell&gt;3.57 Œºs (280.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 Œºs (191.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Delete&lt;/cell&gt;
          &lt;cell&gt;191 Œºs (5.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Select with &lt;code&gt;json_extract()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;4.27 Œºs (234.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;diskcache&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.set(key, obj)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.9 Œºs (41.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.get(key)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;4.25 Œºs (235.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.delete(key)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;51.9 Œºs (19.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Check key exists&lt;/cell&gt;
          &lt;cell&gt;1.91 Œºs (523.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;MongoDB&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;insert_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;119 Œºs (8.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;find_one()&lt;/code&gt; by &lt;code&gt;_id&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;121 Œºs (8.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;find_one()&lt;/code&gt; by nested field&lt;/cell&gt;
          &lt;cell&gt;124 Œºs (8.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;update_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;115 Œºs (8.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;delete_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;30.4 ns (32.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Comparison Table&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;SQLite&lt;/cell&gt;
          &lt;cell role="head"&gt;diskcache&lt;/cell&gt;
          &lt;cell role="head"&gt;MongoDB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write one object&lt;/cell&gt;
          &lt;cell&gt;192 Œºs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;23.9 Œºs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;119 Œºs (8.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read by key/id&lt;/cell&gt;
          &lt;cell&gt;3.57 Œºs (280.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;4.25 Œºs (235.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;121 Œºs (8.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read by nested field&lt;/cell&gt;
          &lt;cell&gt;4.27 Œºs (234.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;N/A&lt;/cell&gt;
          &lt;cell&gt;124 Œºs (8.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 Œºs (191.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;23.9 Œºs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;115 Œºs (8.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Delete&lt;/cell&gt;
          &lt;cell&gt;191 Œºs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;51.9 Œºs (19.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;30.4 ns (32.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;Note: MongoDB is a victim of network access version in-process access.&lt;/p&gt;
      &lt;head rend="h2"&gt;Function and Call Overhead&lt;/head&gt;
      &lt;p&gt;The hidden cost of function calls, exceptions, and async.&lt;/p&gt;
      &lt;head rend="h3"&gt;Function Calls&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty function call&lt;/cell&gt;
          &lt;cell&gt;22.4 ns (44.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Function with 5 arguments&lt;/cell&gt;
          &lt;cell&gt;24.0 ns (41.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Method call on object&lt;/cell&gt;
          &lt;cell&gt;23.3 ns (42.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Lambda call&lt;/cell&gt;
          &lt;cell&gt;19.7 ns (50.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Built-in function (&lt;code&gt;len()&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;17.1 ns (58.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Exceptions&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;try/except&lt;/code&gt; (no exception raised)&lt;/cell&gt;
          &lt;cell&gt;21.5 ns (46.5M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;try/except&lt;/code&gt; (exception raised)&lt;/cell&gt;
          &lt;cell&gt;139 ns (7.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Type Checking&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;isinstance()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;18.3 ns (54.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;type() == type&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;21.8 ns (46.0M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Async Overhead&lt;/head&gt;
      &lt;p&gt;The cost of async machinery.&lt;/p&gt;
      &lt;head rend="h3"&gt;Coroutine Creation&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create coroutine object (no await)&lt;/cell&gt;
          &lt;cell&gt;47.0 ns (21.3M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create coroutine (with return value)&lt;/cell&gt;
          &lt;cell&gt;45.3 ns (22.1M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Running Coroutines&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(empty)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;27.6 Œºs (36.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(return value)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;26.6 Œºs (37.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Run nested await&lt;/cell&gt;
          &lt;cell&gt;28.9 Œºs (34.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Run 3 sequential awaits&lt;/cell&gt;
          &lt;cell&gt;27.9 Œºs (35.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;asyncio.sleep()&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;asyncio.sleep(0)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;39.4 Œºs (25.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Coroutine with &lt;code&gt;sleep(0)&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;41.8 Œºs (23.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;asyncio.gather()&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 5 coroutines&lt;/cell&gt;
          &lt;cell&gt;49.7 Œºs (20.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 10 coroutines&lt;/cell&gt;
          &lt;cell&gt;55.0 Œºs (18.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 100 coroutines&lt;/cell&gt;
          &lt;cell&gt;155 Œºs (6.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Task Creation&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;create_task()&lt;/code&gt; + await&lt;/cell&gt;
          &lt;cell&gt;52.8 Œºs (18.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create 10 tasks + gather&lt;/cell&gt;
          &lt;cell&gt;85.5 Œºs (11.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Async Context Managers &amp;amp; Iteration&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async with&lt;/code&gt; (context manager)&lt;/cell&gt;
          &lt;cell&gt;29.5 Œºs (33.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async for&lt;/code&gt; (5 items)&lt;/cell&gt;
          &lt;cell&gt;30.0 Œºs (33.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async for&lt;/code&gt; (100 items)&lt;/cell&gt;
          &lt;cell&gt;36.4 Œºs (27.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Sync vs Async Comparison&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Sync function call&lt;/cell&gt;
          &lt;cell&gt;20.3 ns (49.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Async equivalent (&lt;code&gt;run_until_complete&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;28.2 Œºs (35.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Methodology&lt;/head&gt;
      &lt;head rend="h3"&gt;Benchmarking Approach&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;All benchmarks run multiple times and with warmup not timed&lt;/item&gt;
        &lt;item&gt;Timing uses &lt;code&gt;timeit&lt;/code&gt; or &lt;code&gt;perf_counter_ns&lt;/code&gt; as appropriate&lt;/item&gt;
        &lt;item&gt;Memory measured with &lt;code&gt;sys.getsizeof()&lt;/code&gt; and &lt;code&gt;tracemalloc&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Results are median of N runs&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Environment&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;OS: macOS 26.2&lt;/item&gt;
        &lt;item&gt;Python: 3.14.2 (CPython)&lt;/item&gt;
        &lt;item&gt;CPU: ARM - 14 cores (14 logical)&lt;/item&gt;
        &lt;item&gt;RAM: 24.0 GB&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Code Repository&lt;/head&gt;
      &lt;p&gt;All benchmark code available at: https://github.com/mkennedy/python-numbers-everyone-should-know&lt;/p&gt;
      &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;Memory overhead: Python objects have significant memory overhead - even an empty list is 56 bytes&lt;/item&gt;
        &lt;item&gt;Dict/set speed: Dictionary and set lookups are extremely fast (O(1) average case) compared to list membership checks (O(n))&lt;/item&gt;
        &lt;item&gt;JSON performance: Alternative JSON libraries like &lt;code&gt;orjson&lt;/code&gt; and &lt;code&gt;msgspec&lt;/code&gt; are 3-8x faster than stdlib &lt;code&gt;json&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Async overhead: Creating and awaiting coroutines has measurable overhead - only use async when you need concurrency&lt;/item&gt;
        &lt;item&gt;&lt;code&gt;__slots__&lt;/code&gt; tradeoff: &lt;code&gt;__slots__&lt;/code&gt; saves significant memory (over 2x for collections) with virtually no performance impact&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Last updated: 2026-01-01&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454470</guid><pubDate>Thu, 01 Jan 2026 14:39:23 +0000</pubDate></item><item><title>Build a Deep Learning Library</title><link>https://zekcrates.quarto.pub/deep-learning-library/</link><description>&lt;doc fingerprint="b759cf2f81cb5b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Build a Simple Deep Learning Library&lt;/head&gt;
    &lt;head rend="h1"&gt;Preface&lt;/head&gt;
    &lt;p&gt;Instead of just learning how to use a deep learning library, we are going to learn how to create one.&lt;/p&gt;
    &lt;p&gt;We start with a blank file and NumPy, and we don‚Äôt stop until we have a functional autograd engine and a collection of layer modules. By the end, we will use it to train MNIST, simple CNN and simple ResNet.&lt;/p&gt;
    &lt;p&gt; NoteSupport This Project &lt;/p&gt;
    &lt;p&gt;This book is free to read online. If it helps you, consider paying what you want on Gumroad&lt;/p&gt;
    &lt;p&gt;Questions or feedback? zekcrates@proton.me&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454587</guid><pubDate>Thu, 01 Jan 2026 14:53:50 +0000</pubDate></item><item><title>Show HN: OpenWorkers ‚Äì Self-hosted Cloudflare workers in Rust</title><link>https://openworkers.com/introducing-openworkers</link><description>&lt;doc fingerprint="ec56b9328dac36f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing OpenWorkers&lt;/head&gt;
    &lt;p&gt;Self-hosted Cloudflare Workers in Rust&lt;/p&gt;
    &lt;p&gt;OpenWorkers is an open-source runtime for executing JavaScript in V8 isolates. It brings the Cloudflare Workers programming model to your own infrastructure.&lt;/p&gt;
    &lt;head rend="h2"&gt;What works today&lt;/head&gt;
    &lt;quote&gt;
      &lt;code&gt;export default { async fetch(request, env) { const data = await env.KV.get("key"); const rows = await env.DB.query( "SELECT * FROM users WHERE id = $1", [1] ); return Response.json({ data, rows }); } };&lt;/code&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;head rend="h3"&gt;Bindings&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚Ä¢ KV storage (get, put, delete, list)&lt;/item&gt;
      &lt;item&gt;‚Ä¢ PostgreSQL database&lt;/item&gt;
      &lt;item&gt;‚Ä¢ S3/R2-compatible storage&lt;/item&gt;
      &lt;item&gt;‚Ä¢ Service bindings&lt;/item&gt;
      &lt;item&gt;‚Ä¢ Environment variables &amp;amp; secrets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Web APIs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚Ä¢ fetch, Request, Response&lt;/item&gt;
      &lt;item&gt;‚Ä¢ ReadableStream&lt;/item&gt;
      &lt;item&gt;‚Ä¢ crypto.subtle&lt;/item&gt;
      &lt;item&gt;‚Ä¢ TextEncoder/Decoder, Blob&lt;/item&gt;
      &lt;item&gt;‚Ä¢ setTimeout, AbortController&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;quote&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ nginx (proxy) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î∏‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î∏‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ dashboard ‚îÇ ‚îÇ api ‚îÇ ‚îÇ logs * ‚îÇ ‚îÇ runner (x3) * ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î∞‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ postgate * ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î• nats ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê * ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î• PostgreSQL ‚îÇ ‚îÇ scheduler * ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò&lt;/quote&gt;
    &lt;quote&gt;+-------------+ | nginx proxy | +------+------+ | +-------+-------+-------+--------+ | | | | +--+--+ +--+--+ +--+---+ +----------+-+ | dash| | api | |logs *| | runner * x3| +-----+ +--+--+ +--+---+ +-----+------+ | | | +-----+----+ | +------+-----+ |postgate *| +----+ nats | +-----+----+ +------+-----+ | | +-----+------+ +------+-----+ *-| PostgreSQL | | scheduler *| +------------+ +------------+&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;V8 Isolates: Sandboxing with CPU (100ms) and memory (128MB) limits per worker.&lt;/item&gt;
      &lt;item&gt;Cron Scheduling: Built-in support for 5 or 6-field cron syntax.&lt;/item&gt;
      &lt;item&gt;Compatibility: Cloudflare Workers syntax compatible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Self-hosting&lt;/head&gt;
    &lt;p&gt;Deployment is designed to be simple. A single PostgreSQL database and a single Docker Compose file is all you need.&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;git clone https://github.com/openworkers/openworkers-infra cd openworkers-infra &amp;amp;&amp;amp; cp .env.example .env docker compose up -d postgres # Run migrations, generate token docker compose up -d&lt;/code&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Why I built this&lt;/head&gt;
    &lt;p&gt;This project has been evolving for about 7 years. I started experimenting with vm2 for sandboxing JS, then Cloudflare launched Workers and I got hooked on the model. When Deno came out, I switched to deno-core and ran on that for two years. Recently, with Claude's help, I rewrote everything on top of rusty_v8 directly.&lt;/p&gt;
    &lt;p&gt;The goal has always been the same: run JavaScript on your own servers, with the same DX as Cloudflare Workers but without vendor lock-in.&lt;/p&gt;
    &lt;p&gt;Next up: Execution recording &amp;amp; replay for deterministic debugging.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454693</guid><pubDate>Thu, 01 Jan 2026 15:09:06 +0000</pubDate></item><item><title>50% of U.S. vinyl buyers don't own a record player</title><link>https://lightcapai.medium.com/the-great-return-from-digital-abundance-to-analog-meaning-cfda9e428752</link><description>&lt;doc fingerprint="88b5f50b0ebe85db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Gen Z is Driving the Vinyl Record Boom?&lt;/head&gt;
    &lt;head rend="h2"&gt;The Great Return: From Digital Abundance to Analog Meaning&lt;/head&gt;
    &lt;p&gt;In the fall of 2025, I find myself living a paradox that defines my generation. Born at the peak of the digital revolution, I spent the last decade methodically reintroducing analog ‚Äúfriction‚Äù into my life. I was collecting vinyl records i can‚Äôt play, wandering car-free city streets, and entertaining offline social activities.&lt;/p&gt;
    &lt;p&gt;Younger people around me are leading an analog resurgence. Far from the stereotype that only older generations buy physical media, Gen Z has emerged as the vanguard of ‚Äúanalog seeking‚Äù behavior. According to Luminate‚Äôs 2023 entertainment report, Gen Z listeners are 27% more likely to purchase vinyl records than the average music consumer, despite having grown up in the era of Spotify. In fact, half of U.S. vinyl buyers today don‚Äôt even own a record player ‚Äì they buy records as tangible tokens of fandom and identity&lt;/p&gt;
    &lt;p&gt;Gen Z is the most online generation in history, yet they report the highest levels of loneliness and digital fatigue. A Global Web Index study found that 32% of Gen Z have taken ‚Äúdigital detox‚Äù breaks from the internet (compared to only 19% of Baby Boomers).&lt;/p&gt;
    &lt;p&gt;In a world of deepfakes and infinite streams, an experience only feels ‚Äúreal‚Äù if it offers tactile resistance. From mechanical keyboards to Polaroid photographs, young adults are gravitating toward the touch and weight of things, subconsciously assuring themselves that what they encounter isn‚Äôt just an algorithmic mirage.&lt;/p&gt;
    &lt;p&gt;I myself periodically disconnect from my devices, experiencing first-hand what Korean-German philosopher Byung-Chul Han describes as the relief from ‚Äútimeless time‚Äù ‚Äì the frenetic, unanchored temporality of the digital world.&lt;/p&gt;
    &lt;p&gt;Gen Z‚Äôs odd affinity for the bygone (film cameras, vinyl, vintage clothes) is a reclamation of the sensory and social richness that earlier technological optimism promised to replace but ultimately could not.&lt;/p&gt;
    &lt;head rend="h2"&gt;Market Data: The Vinyl &amp;amp; Physical Media Revival (2020 ‚Äì 2024)&lt;/head&gt;
    &lt;p&gt;If the cultural zeitgeist is tilting analog, the hard numbers tell the same story. Nowhere is this more evident than the music industry, which he calls the ‚Äúcanary in the coal mine‚Äù for shifts in media value. After a two-decade decline of physical formats, vinyl records have staged an extraordinary comeback in the 2020s ‚Äì both in the United States and across Europe.&lt;/p&gt;
    &lt;p&gt;According to the Recording Industry Association of America (RIAA), U.S. vinyl record revenues grew 17% in 2022 to $1.2 billion, marking the 16th consecutive year of growth for the format. For the first time since 1987, vinyl albums outsold CDs in the U.S. in units ‚Äì 41 million vinyl records vs 33 million CDs were sold in 2022.&lt;/p&gt;
    &lt;p&gt;Europe mirrors this trajectory. In the UK, vinyl LP purchases increased for the 16th straight year in 2023, surging 11.7% in units from the prior year. Nearly 5.9 million vinyl records were sold in Britain in 2023, the highest number in over three decades. Notably, the UK still sold about 11 million CDs in 2023 (about twice the units of vinyl), but CD sales are now approaching a plateau after years of decline. Germany, historically a stronghold of CD consumption, is seeing vinyl carve out a growing niche as well; industry data show German vinyl sales climbing by mid-single digits and vinyl now making up roughly 6% of total recorded music revenue in that market. And in a global context, the trend is undeniable: the International Federation of the Phonographic Industry (IFPI) reported global physical music sales grew 13.4% in 2023, a dramatic acceleration from just 3.8% growth the year prior.&lt;/p&gt;
    &lt;p&gt;Streaming (digital) remains dominant in absolute dollars, but its meteoric rise has leveled to single-digit annual growth in the saturated U.S. market. Vinyl, on the other hand, has grown from a novelty revival to a billion-dollar industry on its own. The compound annual growth rate (CAGR) of vinyl revenue from 2018 ‚Äì 2022 was in the high teens, vastly outpacing the overall music market.&lt;/p&gt;
    &lt;p&gt;When music became an all-you-can-eat digital utility, its ‚ÄúStreaming Devaluation‚Äù set in ‚Äì the marginal value of another song or stream approached zero. In response, value migrated to the scarce complementary good: physical presence and ownership. This is why concert ticket prices and vinyl album sales have surged even as per-stream payouts plummet.&lt;/p&gt;
    &lt;p&gt;the more the digital domain saturates us with instant, on-demand content, the more we prize the unique, unrepeatable or tangible experience.&lt;/p&gt;
    &lt;p&gt;‚ÄúExperience Premium‚Äù ‚Äì&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454944</guid><pubDate>Thu, 01 Jan 2026 15:45:25 +0000</pubDate></item><item><title>BYD Sells 4.6M Vehicles in 2025, Meets Revised Sales Goal</title><link>https://www.bloomberg.com/news/articles/2026-01-01/byd-sells-4-6-million-vehicles-in-2025-meets-revised-sales-goal</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454977</guid><pubDate>Thu, 01 Jan 2026 15:49:42 +0000</pubDate></item><item><title>Cameras and Lenses (2020)</title><link>https://ciechanow.ski/cameras-and-lenses/</link><description>&lt;doc fingerprint="368c0a750d314318"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cameras and Lenses&lt;/head&gt;
    &lt;p&gt;Pictures have always been a meaningful part of the human experience. From the first cave drawings, to sketches and paintings, to modern photography, we‚Äôve mastered the art of recording what we see.&lt;/p&gt;
    &lt;p&gt;Cameras and the lenses inside them may seem a little mystifying. In this blog post I‚Äôd like to explain not only how they work, but also how adjusting a few tunable parameters can produce fairly different results:&lt;/p&gt;
    &lt;p&gt;Over the course of this article we‚Äôll build a simple camera from first principles. Our first steps will be very modest √¢ we‚Äôll simply try to take any picture. To do that we need to have a sensor capable of detecting and measuring light that shines onto it.&lt;/p&gt;
    &lt;head rend="h1"&gt;Recording Light&lt;/head&gt;
    &lt;p&gt;Before the dawn of the digital era, photographs were taken on a piece of film covered in crystals of silver halide. Those compounds are light-sensitive and when exposed to light they form a speck of metallic silver that can later be developed with further chemical processes.&lt;/p&gt;
    &lt;p&gt;For better or for worse, I‚Äôm not going to discuss analog devices √¢ these days most cameras are digital. Before we continue the discussion relating to light we‚Äôll use the classic trick of turning the illumination off. Don‚Äôt worry though, we‚Äôre not going to stay in darkness for too long.&lt;/p&gt;
    &lt;p&gt;The image sensor of a digital camera consists of a grid of photodetectors. A√Ç photodetector converts photons into electric current that can be measured √¢ the more photons hitting the detector the higher the signal.&lt;/p&gt;
    &lt;p&gt;In the demonstration below you can observe how photons fall onto the arrangement of detectors represented by small squares. After some processing, the value read by each detector is converted to the brightness of the resulting image pixels which you can see on the right side. I‚Äôm also symbolically showing which photosite was hit with a short highlight. The slider below controls the flow of time:&lt;/p&gt;
    &lt;p&gt;The longer the time of collection of photons the more of them are hitting the detectors and the brighter the resulting pixels in the image. When we don‚Äôt gather enough photons the image is underexposed, but if we allow the photon collection to run for too long the image will be overexposed.&lt;/p&gt;
    &lt;p&gt;While the photons have the ‚Äúcolor‚Äù of their wavelength, the photodetectors don‚Äôt see that hue √¢ they only measure the total intensity which results in a black and white image. To record the color information we need to separate the incoming photons into distinct groups. We can put tiny color filters on top of the detectors so that they will only accept, more or less, red, green, or blue light:&lt;/p&gt;
    &lt;p&gt;This color filter array can be arranged in many different formations. One of the simplest is a Bayer filter which uses one red, one blue, and two green filters arranged in a 2x2 grid:&lt;/p&gt;
    &lt;p&gt;A Bayer filter uses two green filters because light in green part of the spectrum heavily correlates with perceived brightness. If we now repeat this pattern across the entire sensor we‚Äôre able to collect color information. For the next demo we will also double the resolution to an astonishing 1 kilopixel arranged in a 32x32 grid:&lt;/p&gt;
    &lt;p&gt;Note that the individual sensors themselves still only see the intensity, and not the color, but knowing the arrangement of the filters we can recreate the colored intensity of each sensor, as shown on the right side of the simulation.&lt;/p&gt;
    &lt;p&gt;The final step of obtaining a normal image is called demosaicing. During demosaicing we want to reconstruct the full color information by filling in the gaps in the captured RGB values. One of the simplest way to do it is to just linearly interpolate the values between the existing neighbors. I‚Äôm not going to focus on the details of many other available demosaicing algorithms and I‚Äôll just present the resulting image created by the process:&lt;/p&gt;
    &lt;p&gt;Notice that yet again the overall brightness of the image depends on the length of time for which we let the photons through. That duration is known as shutter speed or exposure time. For most of this presentation I will ignore the time component and we will simply assume that the shutter speed has been set just right so that the image is well exposed.&lt;/p&gt;
    &lt;p&gt;The examples we‚Äôve discussed so far were very convenient √¢ we were surrounded by complete darkness with the photons neatly hitting the pixels to form a coherent image. Unfortunately, we can‚Äôt count on the photon paths to be as favorable in real environments, so let‚Äôs see how the sensor performs in more realistic scenarios.&lt;/p&gt;
    &lt;p&gt;Over the course of this article we will be taking pictures of this simple scene. The almost white background of this website is also a part of the scenery √¢ it represents a bright overcast sky. You can drag around the demo to see it from other directions:&lt;/p&gt;
    &lt;p&gt;Let‚Äôs try to see what sort of picture would be taken by a sensor that is placed near the objects without any enclosure. I‚Äôll also significantly increase the sensor‚Äôs resolution to make the pixels of the final image align with the pixels of your display. In the demonstration below the left side represents a view of the scene with the small greenish sensor present, while the right one shows the taken picture:&lt;/p&gt;
    &lt;p&gt;This is not a mistake. As you can see, the obtained image doesn‚Äôt really resemble anything. To understand why this happens let‚Äôs first look at the light radiated from the scene.&lt;/p&gt;
    &lt;p&gt;If you had a chance to explore how surfaces reflect light, you may recall that most matte surfaces scatter the incoming light in every direction. While I‚Äôm only showing a few examples, every point on every surface of this scene reflects the photons it receives from the whiteish background light source all around itself:&lt;/p&gt;
    &lt;p&gt;The red sphere ends up radiating red light, the green sphere radiates green light, and the gray checkerboard floor reflects white light of lesser intensity. Most importantly, however, the light emitted from the background is also visible to the sensor.&lt;/p&gt;
    &lt;p&gt;The problem with our current approach to taking pictures is that every pixel of the sensor is exposed to the entire environment. Light radiated from every point of the scene and the white background hits every point of the sensor. In the simulation below you can witness how light from different directions hits one point on the surface of the sensor:&lt;/p&gt;
    &lt;p&gt;Clearly, to obtain a discernible image we have to limit the range of directions that affect a given pixel on the sensor. With that in mind, let‚Äôs put the sensor in a box that has a small hole in it. The first slider controls the diameter of the hole, while the second one controls the distance between the opening and the sensor:&lt;/p&gt;
    &lt;p&gt;While not shown here, the inner sides of the walls are all black so that no light is reflected inside the box. I also put the sensor on the back wall so that the light from the hole shines onto it. We‚Äôve just built a pinhole camera, let‚Äôs see how it performs. Observe what happens to the taken image as we tweak the diameter of the hole with the first slider, or change the distance between the opening and the sensor with the second one:&lt;/p&gt;
    &lt;p&gt;There are so many interesting things happening here! The most pronounced effect is that the image is inverted. To understand why this happens let‚Äôs look at the schematic view of the scene that shows the light rays radiated from the objects, going through the hole, and hitting the sensor:&lt;/p&gt;
    &lt;p&gt;As you can see the rays cross over in the hole and the formed image is a horizontal and a vertical reflection of the actual scene. Those two flips end up forming a 180√Ç¬∞ rotation. Since rotated images aren‚Äôt convenient to look at, all cameras automatically rotate the image for presentation and for the rest of this article I will do so as well.&lt;/p&gt;
    &lt;p&gt;When we change the distance between the hole and the sensor the viewing angle changes drastically. If we trace the rays falling on the corner pixels of the sensor we can see that they define the extent of the visible section of the scene:&lt;/p&gt;
    &lt;p&gt;Rays of light coming from outside of that shape still go through the pinhole, but they land outside of the sensor and aren‚Äôt recorded. As the hole moves further away from the sensor, the angle, and thus the field of view visible to the sensor gets smaller. We can see this in a top-down view of the camera:&lt;/p&gt;
    &lt;p&gt;Coincidentally, this diagram also helps us explain two other effects. Firstly, in the photograph the red sphere looks almost as big as the green one, even though the scene view shows the latter is much larger. However, both spheres end up occupying roughly the same span on the sensor and their size in the picture is similar. It‚Äôs also worth noting that the spheres seem to grow when the field of view gets narrower because their light covers larger part of the sensor.&lt;/p&gt;
    &lt;p&gt;Secondly, notice that different pixels of the sensor have different distance and relative orientation to the hole. The pixels right in the center of the sensor see the pinhole straight on, but pixels positioned at an angle to the main axis see a distorted pinhole that is further away. The ellipse in the bottom right corner of the demonstration below shows how a pixel positioned at the blue point sees the pinhole:&lt;/p&gt;
    &lt;p&gt;This change in the visible area of the hole causes the darkening we see in the corners of the photograph. The value of the cosine of the angle I‚Äôve marked with a yellow color is quite important as it contributes to the reduction of visible light in four different ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two cosine factors from the increased distance to the hole, it‚Äôs essentially the inverse square law&lt;/item&gt;
      &lt;item&gt;A cosine factor from the side squeeze of the circular hole seen at an angle&lt;/item&gt;
      &lt;item&gt;A cosine factor from the relative tilt of the receptor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These four factors conspire together to reduce the illumination by a factor of cos4(√é¬±) in what is known as cosine-fourth-power law, also described as natural vignetting.&lt;/p&gt;
    &lt;p&gt;Since we know the relative geometry of the camera and the opening we can correct for this effect by simply dividing by the falloff factor and from this point on I will make sure that the images don‚Äôt have darkened corners.&lt;/p&gt;
    &lt;p&gt;The final effect we can observe is that when the hole gets smaller the image gets sharper. Let‚Äôs see how the light radiated from two points of the scene ends up going through the camera depending on the diameter of the pinhole:&lt;/p&gt;
    &lt;p&gt;We can already see that larger hole size ends up creating a bigger spread on the sensor. Let‚Äôs see this situation up close on a simple grid of detecting cells. Notice what happens to the size of the final circle hitting the sensor as that diameter of the hole changes:&lt;/p&gt;
    &lt;p&gt;When the hole is small enough rays from the source only manage to hit one pixel on the sensor. However, at larger radii the light spreads onto other pixels and a tiny point in the scene is no longer represented by a single pixel causing the image to no longer be sharp.&lt;/p&gt;
    &lt;p&gt;It‚Äôs worth pointing out that sharpness is ultimately arbitrary √¢ it depends on the size at which the final image is seen, viewing conditions, and visual acuity of the observer. The same photograph that looks sharp on a postage stamp may in fact be very blurry when seen on a big display.&lt;/p&gt;
    &lt;p&gt;By reducing the size of the cone of light we can make sure that the source light affects a limited number of pixels. Here, however, lays the problem. The sensor we‚Äôve been using so far has been an idealized detector capable of flawless adjustment of its sensitivity to the lighting conditions. If we instead were to fix the sensor sensitivity adjustment, the captured image would look more like this:&lt;/p&gt;
    &lt;p&gt;As the relative size of the hole visible to the pixels of the sensor gets smaller, be it due to reduced diameter or increased distance, fewer photons hit the surface and the image gets dimmer.&lt;/p&gt;
    &lt;p&gt;To increase the number of photons we capture we could extend the duration of collection, but increasing the exposure time comes with its own problems √¢ if the photographed object moves or the camera isn‚Äôt held steady we risk introducing some motion blur.&lt;/p&gt;
    &lt;p&gt;Alternatively, we could increase the sensitivity of the sensor which is described using the ISO rating. However, boosting the ISO may introduce a higher level of noise. Even with these problems solved an actual image obtained by smaller and smaller holes would actually start getting blurry again due to diffraction effects of light.&lt;/p&gt;
    &lt;p&gt;If you recall how diffuse surfaces reflect light you may also realize how incredibly inefficient a pinhole camera is. A single point on the surface of an object radiates light into its surrounding hemisphere, however, the pinhole captures only a tiny portion of that light.&lt;/p&gt;
    &lt;p&gt;More importantly, however, a pinhole camera gives us minimal artistic control over which parts of the picture are blurry. In the demonstration below you can witness how changing which object is in focus heavily affects what is the primary target of attention of the photograph:&lt;/p&gt;
    &lt;p&gt;Let‚Äôs try to build an optical device that would solve both of these problems: we want to find a way to harness a bigger part of the energy radiated by the objects and also control what is blurry and how blurry it is. For the objects in the scene that are supposed to be sharp we want to collect a big chunk of their light and make it converge to the smallest possible point. In essence, we‚Äôre looking for an instrument that will do something like this:&lt;/p&gt;
    &lt;p&gt;We could then put the sensor at the focus point and obtain a sharp image. Naturally, the contraption we‚Äôll try to create has to be transparent so that the light can pass through it and get to the sensor, so let‚Äôs begin the investigation by looking at a piece of glass.&lt;/p&gt;
    &lt;head rend="h1"&gt;Glass&lt;/head&gt;
    &lt;p&gt;In the demonstration below I put a red stick behind a pane of glass. You can adjust the thickness of this pane with the gray slider below:&lt;/p&gt;
    &lt;p&gt;When you look at the stick through the surface of a thick glass straight on, everything looks normal. However, as your viewing direction changes the stick seen through the glass seems out of place. The thicker the glass and the steeper the viewing angle the bigger the offset.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs focus on one point on the surface of the stick and see how the rays of light radiated from its surface propagate through the subsection of the glass. The red slider controls the position of the source and the gray slider controls the thickness. You can drag the demo around to see it from different viewpoints:&lt;/p&gt;
    &lt;p&gt;For some reason the rays passing through glass at an angle are deflected off their paths. The change of direction happens whenever the ray enters or leaves the glass.&lt;/p&gt;
    &lt;p&gt;To understand why the light changes direction we have to peek under the covers of classical electromagnetism and talk a bit more about waves.&lt;/p&gt;
    &lt;head rend="h1"&gt;Waves&lt;/head&gt;
    &lt;p&gt;It‚Äôs impossible to talk about wave propagation without involving the time component, so the simulations in this section are animated √¢ you can play and pause them by clickingtapping on the button in their bottom left corner.&lt;/p&gt;
    &lt;p&gt;By default all animations are enabled, but if you find them distracting, or if you want to save power, you can globally pause all the following demonstrations.disabled, but if you‚Äôd prefer to have things moving as you read you can globally unpause them and see all the waves oscillating.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs begin by introducing the simplest sinusoidal wave:&lt;/p&gt;
    &lt;p&gt;A wave like this can be characterized by two components. Wavelength √é¬ª is the distance over which the shape of the wave repeats. Period T defines how much time a full cycle takes.&lt;/p&gt;
    &lt;p&gt;Frequency f, is just a reciprocal of period and it‚Äôs more commonly used √¢ it defines how many waves per second have passed over some fixed point. Wavelength and frequency define phase velocity vp which describes how quickly a point on a wave, e.g. a peak, moves:&lt;/p&gt;
    &lt;p&gt;The sinusoidal wave is the building block of a polarized electromagnetic plane wave. As the name implies electromagnetic radiation is an interplay of oscillations of electric field E and magnetic field B:&lt;/p&gt;
    &lt;p&gt;In an electromagnetic wave the magnetic field is tied to the electric field so I‚Äôm going to hide the former and just visualize the latter. Observe what happens to the electric component of the field as it passes through a block of glass. I need to note that dimensions of wavelengths are not to scale:&lt;/p&gt;
    &lt;p&gt;Notice that the wave remains continuous at the boundary and inside the glass the frequency of the passing wave remains constant, However, the wavelength and thus the phase velocity are reduced √¢ you can see it clearly from the side.&lt;/p&gt;
    &lt;p&gt;The microscopic reason for the phase velocity change is quite complicated, but it can be quantified using the index of refraction n, which is the ratio of the speed of light c to the phase velocity vp of lightwave in that medium:&lt;/p&gt;
    &lt;p&gt;The higher the index of refraction the slower light propagates through the medium. In the table below I‚Äôve presented a few different indices of refraction for some materials:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;vacuum&lt;/cell&gt;
        &lt;cell&gt;1.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;air&lt;/cell&gt;
        &lt;cell&gt;1.0003&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;water&lt;/cell&gt;
        &lt;cell&gt;1.33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;glass&lt;/cell&gt;
        &lt;cell&gt;1.53&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;diamond&lt;/cell&gt;
        &lt;cell&gt;2.43&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Light traveling through air barely slows down, but in a diamond it‚Äôs over twice as slow. Now that we understand how index of refraction affects the wavelength in the glass, let‚Äôs see what happens when we change the direction of the incoming wave:&lt;/p&gt;
    &lt;p&gt;The wave in the glass has a shorter wavelength, but it still has to match the positions of its peaks and valleys across the boundary. As such, the direction of propagation must change to ensure that continuity.&lt;/p&gt;
    &lt;p&gt;I need to note that the previous two demonstrations presented a two dimensional wave since that allowed me to show the sinusoidal component oscillating into the third dimension. In real world the lightwaves are three dimensional and I can‚Äôt really visualize the sinusoidal component without using the fourth dimension which has its own set of complications.&lt;/p&gt;
    &lt;p&gt;The alternative way of presenting waves is to use wavefronts. Wavefronts connect the points of the same phase of the wave, e.g. all the peaks or valleys. In two dimensions wavefronts are represented by lines:&lt;/p&gt;
    &lt;p&gt;In three dimensions the wavefronts are represented by surfaces. In the demonstration below a single source emits a spherical wave, points of the same phase in the wave are represented by the moving shells:&lt;/p&gt;
    &lt;p&gt;By drawing lines that are perpendicular to the surface of the wavefront we create the familiar rays. In this interpretation rays simply show the local direction of wave propagation which can be seen in this example of a section of a spherical 3D wave:&lt;/p&gt;
    &lt;p&gt;I will continue to use the ray analogy to quantify the change in direction of light passing through materials. The relation between the angle of incidence √é¬∏1 and angle of refraction √é¬∏2 can be formalized with the equation known as Snell‚Äôs law:&lt;/p&gt;
    &lt;p&gt;It describes how a ray of light changes direction relative to the surface normal on the border between two different media. Let‚Äôs see it in action:&lt;/p&gt;
    &lt;p&gt;When traveling from a less to more refractive material the ray bends towards the normal, but when the ray exits the object with higher index of refraction it bends away from the normal.&lt;/p&gt;
    &lt;p&gt;Notice that in some configurations the refracted ray completely disappears, however, this doesn‚Äôt paint a full picture because we‚Äôre currently completely ignoring reflections.&lt;/p&gt;
    &lt;p&gt;All transparent objects reflect some amount of light. You may have noticed that reflection on a surface of a calm lake or even on the other side of the glass demonstration at the beginning of the previous section. The intensity of that reflection depends on the index of refraction of the material and the angle of the incident ray. Here‚Äôs a more realistic demonstration of how light would get refracted and reflected between two media:&lt;/p&gt;
    &lt;p&gt;The relation between transmittance and reflectance is determined by Fresnel equations. Observe that the curious case of missing light that we saw previously no longer occurs √¢ that light is actually reflected. The transition from partial reflection and refraction to the complete reflection is continuous, but near the end it‚Äôs very rapid and at some point the refraction completely disappears in the effect known as total internal reflection.&lt;/p&gt;
    &lt;p&gt;The critical angle at which the total internal reflection starts to happen depends on the indices of refraction of the boundary materials. Since that coefficient is low for air, but very high for diamond a proper cut of the faces makes diamonds very shiny.&lt;/p&gt;
    &lt;p&gt;While interesting on its own, reflection in glass isn‚Äôt very relevant to our discussion and for the rest of this article we‚Äôre not going to pay much attention to it. Instead, we‚Äôll simply assume that the materials we‚Äôre using are covered with high quality anti-reflective coating.&lt;/p&gt;
    &lt;head rend="h1"&gt;Manipulating Rays&lt;/head&gt;
    &lt;p&gt;Let‚Äôs go back to the example that started the discussion of light and glass. When both sides of a piece of glass are parallel, the ray is shifted, but it still travels in the same direction. Observe what happens to the ray when we change the relative angle of the surfaces of the glass.&lt;/p&gt;
    &lt;p&gt;When we make two surfaces of the glass not parallel we gain the ability to change the direction of the rays. Recall, that we‚Äôre trying to make the rays hitting the optical device converge at a certain point. To do that we have to bend the rays in the upper part down and, conversely, bend the rays in the lower part up.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs see what happens if we shape the glass to have different angles between its walls at different height. In the demonstration below you can control how many distinct segments a piece of glass is shaped to:&lt;/p&gt;
    &lt;p&gt;As the number of segments approaches infinity we end up with a continuous surface without any edges. If we look at the crossover point from the side you may notice that we‚Äôve managed to converge the rays across one axis, but the top-down view reveals that we‚Äôre not done yet. To focus all the rays we need to replicate that smooth shape across all possible directions √¢ we need rotational symmetry:&lt;/p&gt;
    &lt;p&gt;We‚Äôve created a convex thin lens. This lens is idealized, in the later part of the article we‚Äôll discuss how real lenses aren‚Äôt as perfect, but for now it will serve us very well. Let‚Äôs see what happens to the focus point when we change the position of the red source:&lt;/p&gt;
    &lt;p&gt;When the source is positioned very far away the incoming rays become parallel and after passing through lens they converge at a certain distance away from the center. That distance is known as focal length.&lt;/p&gt;
    &lt;p&gt;The previous demonstration also shows two more general distances: so which is the distance between the object, or source, and the lens, as well as si which is the distance between the image and the lens. These two values and the focal length f are related by the thin lens equation:&lt;/p&gt;
    &lt;p&gt;Focal length of a lens depends on both the index of refraction of the material from which the lens is made and its shape:&lt;/p&gt;
    &lt;p&gt;Now that we understand how a simple convex lens works we‚Äôre ready to mount it into the hole of our camera. We will still control the distance between the sensor and the lens, but instead of controlling the diameter of the lens we‚Äôll instead control its focal length:&lt;/p&gt;
    &lt;p&gt;When you look at the lens from the side you may observe how the focal length change is tied to the shape of the lens. Let‚Äôs see how this new camera works in action:&lt;/p&gt;
    &lt;p&gt;Once again, a lot of things are going on here! Firstly, let‚Äôs try to understand how the image is formed in the first place. The demonstration below shows paths of rays from two separate points in the scene. After going through the lens they end up hitting the sensor:&lt;/p&gt;
    &lt;p&gt;Naturally, this process happens for every single point in the scene which creates the final image. Similarly to a pinhole a convex lens creates an inverted picture √¢ I‚Äôm still correcting for this by showing you a rotated photograph.&lt;/p&gt;
    &lt;p&gt;Secondly, notice that the distance between the lens and the sensor still controls the field of view. As a reminder, the focal length of a lens simply defines the distance from the lens at which the rays coming from infinity converge. To achieve a sharp image, the sensor has to be placed at the location where the rays focus and that‚Äôs what‚Äôs causing the field of view to change.&lt;/p&gt;
    &lt;p&gt;In the demonstration below I‚Äôve visualized how rays from a very far object focus through a lens of adjustable focal length, notice that to obtain a sharp image we must change the distance between the lens and the sensor which in turn causes the field of view to change:&lt;/p&gt;
    &lt;p&gt;If we want to change the object on which a camera with a lens of a fixed focal length is focused, we have to move the image plane closer or further away from the lens which affects the angle of view. This effect is called focus breathing:&lt;/p&gt;
    &lt;p&gt;A lens with a fixed focal length like the one above is often called a prime lens, while lenses with adjustable focal length are called zoom lenses. While the lenses in our eyes do dynamically adjust their focal lengths by changing their shape, rigid glass can‚Äôt do that so zoom lenses use a system of multiple glass elements that change their relative position to achieve this effect.&lt;/p&gt;
    &lt;p&gt;In the simulation above notice the difference in sharpness between the red and green spheres. To understand why this happens let‚Äôs analyze the rays emitted from two points on the surface of the spheres. In the demonstration below the right side shows the light seen by the sensor just from the two marked points on the spheres:&lt;/p&gt;
    &lt;p&gt;The light from the point in focus converges to a point, while the light from an out-of-focus point spreads onto a circle. For larger objects the multitude of overlapping out-of-focus circles creates a smooth blur called bokeh. With tiny and bright light sources that circle itself is often visible, you may have seen effects like the one in the demonstration below in some photographs captured in darker environments:&lt;/p&gt;
    &lt;p&gt;Notice that the circular shape is visible for lights both in front of and behind the focused distance. As the object is positioned closer or further away from the lens the image plane ‚Äúslices‚Äù the cone of light at different location:&lt;/p&gt;
    &lt;p&gt;That circular spot is called a circle of confusion. While in many circumstances the blurriness of the background or the foreground looks very appealing, it would be very useful to control how much blur there is.&lt;/p&gt;
    &lt;p&gt;Unfortunately, we don‚Äôt have total freedom here √¢ we still want the primary photographed object to remain in focus so its light has to converge to a point. We just want to change the size of the circle of out-of-focus objects without moving the central point. We can accomplish that by changing the angle of the cone of light:&lt;/p&gt;
    &lt;p&gt;There are two methods we can use to modify that angle. Firstly, we can change the focal length of the lens √¢ you may recall that with longer focal lengths the cone of light also gets longer. However, changing the focal length and keeping the primary object in focus requires moving the image plane which in turn changes how the picture is framed.&lt;/p&gt;
    &lt;p&gt;The alternative way of reducing the angle of the cone of light is to simply ignore some of the ‚Äúouter‚Äù rays. We can achieve that by introducing a stop with a hole in the path of light:&lt;/p&gt;
    &lt;p&gt;This hole is called an aperture. In fact, even the hole in which the lens is mounted is an aperture of some sort, but what we‚Äôre introducing is an adjustable aperture:&lt;/p&gt;
    &lt;p&gt;Let‚Äôs try to see how an aperture affects the photographs taken with our camera:&lt;/p&gt;
    &lt;p&gt;In real camera lenses an adjustable aperture is often constructed from a set of overlapping blades that constitute an iris. The movement of those blades changes the size of the aperture:&lt;/p&gt;
    &lt;p&gt;The shape of the aperture also defines the shape of bokeh. This is the reason why bokeh sometimes has a polygonal shape √¢ it‚Äôs simply the shape of the ‚Äúcone‚Äù of light after passing through the blades of the aperture. Next time you watch a movie pay a close attention to the shape of out-of-focus highlights, they‚Äôre often polygonal:&lt;/p&gt;
    &lt;p&gt;As the aperture diameter decreases, larger and larger areas of the photographed scene remain sharp. The term depth of field is used to define the length of the region over which the objects are acceptably sharp. When describing the depth of field we‚Äôre trying to conceptually demark those two boundary planes and see how far apart they are from each other.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs see the depth of field in action. The black slider controls the aperture, the blue slider controls the focal length, and the red slider changes the position of the object relative to the camera. The green dot shows the place of perfect focus, while the dark blue dots show the limits, or the depth, of positions between which the image of the red light source will be reasonably sharp, as shown by a single outlined pixel on the sensor:&lt;/p&gt;
    &lt;p&gt;Notice that the larger the diameter of aperture and the shorter the focal length the shorter the distance between the dark blue dots and thus the shallower the depth of field becomes. If you recall our discussion of sharpness this demonstration should make it easier to understand why reducing the angle of the cone increases the depth of field.&lt;/p&gt;
    &lt;p&gt;If you don‚Äôt have perfect vision you may have noticed that squinting your eyes make you see things a little better. Your eyelids covering some part of your iris simply act as an aperture that decreases the angle of the cone of light falling into your eyes making things sightly less blurry on your retina.&lt;/p&gt;
    &lt;p&gt;An interesting observation is that aperture defines the diameter of the base of the captured cone of light that is emitted from the object. Twice as large aperture diameter captures roughly four times more light due to increased solid angle. In practice, the actual size of the aperture as seen from the point of view of the scene, or the entrance pupil, depends on all the lenses in front of it as the shaped glass may scale the perceived size of the aperture.&lt;/p&gt;
    &lt;p&gt;On the other hand, when a lens is focused correctly, the focal length defines how large a source object is in the picture. By doubling the focal length we double the width and the height of the object on the sensor thus increasing the area by the factor of four. The light from the source is more spread out and each individual pixel receives less light.&lt;/p&gt;
    &lt;p&gt;The total amount of light hitting each pixel is proportional to the ratio between the focal length f and the diameter of the entrance pupil D. This ratio is known as the f-number:&lt;/p&gt;
    &lt;p&gt;A lens with a focal length of 50 mm and the entrance pupil of 25 mm would have N equal to 2 and the f-number would be known as f/2. Since the amount of light getting to each pixel of the sensor increases with the diameter of the aperture and decreases with the focal length, the f-number controls the brightness of the projected image.&lt;/p&gt;
    &lt;p&gt;The f-number with which commercial lenses are marked usually defines the maximum aperture a lens can achieve and the smaller the f-number the more light the lens passes through. Bigger amount of incoming light allows reduction of exposure time, so the smaller the f-number the faster the lens is. By reducing the size of the aperture we can modify the f-number with which a picture is taken.&lt;/p&gt;
    &lt;p&gt;The f-numbers are often multiples of 1.4 which is an approximation of 2. Scaling the diameter of an adjustable aperture by 2 scales its area by 2 which is a convenient factor to use. Increasing the f-number by a so-called stop halves the amount of received light. The demonstration below shows the relatives sizes of the aperture through which light is being seen:&lt;/p&gt;
    &lt;p&gt;To maintain the overall brightness of the image when stopping down we‚Äôd have to either increase the exposure time or the sensitivity of the sensor.&lt;/p&gt;
    &lt;p&gt;While aperture settings let us easily control the depth of field, that change comes at a cost. When the f-number increases and the aperture diameter gets smaller we effectively start approaching a pinhole camera with all its related complications.&lt;/p&gt;
    &lt;p&gt;In the final part of this article we will discuss the entire spectrum of another class of problems that we‚Äôve been conveniently avoiding all this time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Aberrations&lt;/head&gt;
    &lt;p&gt;In our examples so far we‚Äôve been using a perfect idealized lens that did exactly what we want and in all the demonstrations I‚Äôve relied on a certain simplification known as the paraxial approximation. However, the physical world is a bit more complicated.&lt;/p&gt;
    &lt;p&gt;The most common types of lenses are spherical lenses √¢ their curved surfaces are sections of spheres of different radii. These types of lenses are easier to manufacture, however, they actually don‚Äôt perfectly converge the rays of incoming light. In the demonstration below you can observe how fuzzy the focus point is for various lens radii:&lt;/p&gt;
    &lt;p&gt;This imperfection is known as spherical aberration. This specific flaw can be corrected with aspheric lenses, but unfortunately there are other types of problems that may not be easily solved by a single lens. In general, for monochromatic light there are five primary types of aberrations: spherical aberration, coma, astigmatism, field curvature, and distortion.&lt;/p&gt;
    &lt;p&gt;We‚Äôre still not out of the woods even if we manage to minimize these problems. In normal environments light is very non-monochromatic and nature sets another hurdle into optical system design. Let‚Äôs quickly go back to the dark environment as we‚Äôll be discussing a single beam of white light.&lt;/p&gt;
    &lt;p&gt;Observe what happens to that beam when it hits a piece of glass. You can make the sides non-parallel by using the slider:&lt;/p&gt;
    &lt;p&gt;What we perceive as white light is a combination of lights of different wavelengths. In fact, the index of refraction of materials depends on the wavelength of the light. This phenomena called dispersion splits what seems to be a uniform beam of white light into a fan of color bands. The very same mechanism that we see here is also responsible for a rainbow.&lt;/p&gt;
    &lt;p&gt;In a lens this causes different wavelengths of light to focus at different offsets √¢ the effect known as chromatic aberration. We can easily visualize the axial chromatic aberration even on a lens with spherical aberration fixed. I‚Äôll only use red, green, and blue dispersed rays to make things less crowded, but remember that other colors of the spectrum are present in between. Using the slider you can control the amount of dispersion the lens material introduces:&lt;/p&gt;
    &lt;p&gt;Chromatic aberration may be corrected with an achromatic lens, usually in the form of a doublet with two different types of glass fused together.&lt;/p&gt;
    &lt;p&gt;To minimize the impact of the aberrations, camera lenses use more than one optical element on their pathways. In this article I‚Äôve only shown you simple lens systems, but a high-end camera lens may consist of a lot of elements that were carefully designed to balance the optical performance, weight, and cost.&lt;/p&gt;
    &lt;p&gt;While we, in our world of computer simulations on this website, can maintain the illusion of simple and perfect systems devoid of aberrations, vignetting, and lens flares, real cameras and lenses have to deal with all these problems to make the final pictures look good.&lt;/p&gt;
    &lt;head rend="h1"&gt;Further Watching and Reading&lt;/head&gt;
    &lt;p&gt;Over on YouTube Filmmaker IQ channel has a lot of great content related to lenses and movie making. Two videos especially fitting here are The History and Science of Lenses and Focusing on Depth of Field and Lens Equivalents.&lt;/p&gt;
    &lt;p&gt;What Makes Cinema Lenses So Special!? on Potato Jet channel is a great interview with Art Adams from ARRI. The video goes over many interesting details of high-end cinema lens design, for example, how the lenses compensate for focus breathing, or how much attention is paid to the quality of bokeh.&lt;/p&gt;
    &lt;p&gt;For a deeper dive on bokeh itself Jakub Tr√É¬°vn√Ék‚Äôs On Bokeh is a great article on the subject. The author explains how aberrations may cause bokeh of non uniform intensity and shows many photographs of real cameras and lenses.&lt;/p&gt;
    &lt;p&gt;In this article I‚Äôve mostly been using geometrical optics with some soft touches of electromagnetism. For a more modern look at the nature of light and its interaction with matter I recommend Richard Feynman‚Äôs QED: The Strange Theory of Light and Matter. The book is written in a very approachable style suited for general audience, but it still lets Feynman‚Äôs wits and brilliance shine right through.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;We√¢ve barely scratched the surface of optics and camera lens design, but even the most complex systems end up serving the same purpose: to tell light where to go. In some sense optical engineering is all about taming the nature of light.&lt;/p&gt;
    &lt;p&gt;The simple act of pressing the shutter button in a camera app on a smartphone or on the body of a high-end DSLR is effortless, but it√¢s at this moment when, through carefully guided rays hitting an array of photodetectors, we immortalize reality by painting with light.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46455872</guid><pubDate>Thu, 01 Jan 2026 17:18:01 +0000</pubDate></item><item><title>Finland detains ship and its crew after critical undersea cable damaged</title><link>https://www.cnn.com/2025/12/31/europe/finland-estonia-undersea-cable-ship-detained-intl</link><description>&lt;doc fingerprint="e406048e7b88547e"&gt;
  &lt;main&gt;
    &lt;p&gt;Finland has detained a ship and its crew after a critical undersea telecommunication cable connecting the country to Estonia was damaged Wednesday, Finnish authorities said.&lt;/p&gt;
    &lt;p&gt;Finnish police said in a statement that the vessel suspected of causing the damage was found with its anchor chain lowered into the sea in Finland‚Äôs waters, while the damage site itself was in Estonia‚Äôs waters. The police later named the vessel as the Fitburg, a Saint Vincent and the Grenadines flagged cargo ship.&lt;/p&gt;
    &lt;p&gt;The Finnish National Police Commissioner Ilkka Koskim√§ki said at a news conference on Wednesday afternoon that all 14 members of the ship‚Äôs crew have been detained, adding that the crew are citizens of Russia, Georgia, Kazakhstan and Azerbaijan.&lt;/p&gt;
    &lt;p&gt;Incidents like this have become more frequent in recent years, raising suspicions they are the result of sabotage and prompting NATO to launch a project earlier this year specifically aimed at strengthening the protection of critical undersea infrastructure.&lt;/p&gt;
    &lt;p&gt;According to MarineTraffic, which tracks ship movements, the Fitburg departed the Russian port of St. Petersburg on Tuesday and was headed to Haifa in Israel.&lt;/p&gt;
    &lt;p&gt;After the damage was reported, Finnish authorities instructed the ship to stop and raise its anchor, and then took control of it, the police said.&lt;/p&gt;
    &lt;p&gt;Finnish media reported that the ship was seized by special forces police and the coast guard from helicopters.&lt;/p&gt;
    &lt;p&gt;Finland‚Äôs President Alexander Stubb said that the government was monitoring the situation closely and that Finland was ‚Äúprepared for security challenges of various kinds.‚Äù&lt;/p&gt;
    &lt;p&gt;The police said they were investigating the incident as aggravated criminal damage, attempted aggravated criminal damage, and aggravated interference with telecommunications.&lt;/p&gt;
    &lt;p&gt;The cable that was damaged runs between the Finnish capital Helsinki and the Estonian capital Talinn. While the extend of the damage was not immediately clear, the incident was serious enough to cause faults that were detected by the Finnish telecommunications provider Elisa, which operates the link.&lt;/p&gt;
    &lt;p&gt;Finnish Prime Minister Petteri Orpo said he spoke to his Estonian counterpart Kristen Michal about the situation, adding that the two countries were cooperating on the issue.&lt;/p&gt;
    &lt;p&gt;Estonia‚Äôs Ministry of Justice and Digital Affairs said in a statement that the country‚Äôs connections remained sufficiently backed up through other sea and land cables, ensuring the continuity of all services.&lt;/p&gt;
    &lt;p&gt;The ministry said that a second cable, owned by the Swedish company Arelion, was also damaged.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern of disruption&lt;/head&gt;
    &lt;p&gt;At least 10 undersea cables have been cut or damaged in the Baltic Sea since 2023. Some officials from Scandinavia, the Baltic states and the European Union have pointed the finger at Russia. They say the incidents appear to be part of what experts say is the Kremlin‚Äôs hybrid war on the West.&lt;/p&gt;
    &lt;p&gt;Russia has consistently denied involvement, but some of the ships that have caused damage to the undersea infrastructure in the past were found to have links to Russia.&lt;/p&gt;
    &lt;p&gt;Last year, a Baltic Sea power cable and several data cables were damaged after a Cook Islands-registered vessel dragged its anchor through the seabed for more than 50 miles.&lt;/p&gt;
    &lt;p&gt;Finnish and European officials said the ship, Eagle-S, was part of Russia‚Äôs shadow fleet of fuel tankers, and Finland later charged members of its crew. However, a court in Helsinki dismissed the case in October, saying Finland did not have jurisdiction over the issue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46456797</guid><pubDate>Thu, 01 Jan 2026 18:46:07 +0000</pubDate></item><item><title>Dell's version of the DGX Spark fixes pain points</title><link>https://www.jeffgeerling.com/blog/2025/dells-version-dgx-spark-fixes-pain-points</link><description>&lt;doc fingerprint="bdb351577a83619c"&gt;
  &lt;main&gt;
    &lt;p&gt;Dell sent me two of their GB10 mini workstations to test:&lt;/p&gt;
    &lt;p&gt;In this blog post, I'll cover the base system, just one of the two nodes. Cluster testing is ongoing, and I'll cover things like AI model training and networking more in depth next year, likely with comparisons to the Framework Desktop cluster and Mac Studio cluster I've also been testing.&lt;/p&gt;
    &lt;p&gt;But many of the same caveats of the DGX Spark (namely, price to performance is not great if you just want to run LLMs on a small desktop) apply to Dell's GB10 box as well.&lt;/p&gt;
    &lt;p&gt;It costs a little more than the DGX Spark, but does solve a couple pain points people experienced on the DGX Spark:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It has a power LED (seriously, why does the DGX Spark not have one?!)&lt;/item&gt;
      &lt;item&gt;The included power supply is 280W instead of 240W for a little more headroom&lt;/item&gt;
      &lt;item&gt;The thermal design (front-to-back airflow) seems less restricted, so is quieter and capable of keeping the GB10 'AI Superchip' from thermal throttling&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But if this isn't a mini PC to compete with a Mac mini, nor a good value for huge LLMs like a Mac Studio, or AMD's Ryzen AI Max+ 395 machines, what is it and who is it for?&lt;/p&gt;
    &lt;p&gt;Well, it's a $4,000+ box built specifically for developers in Nvidia's ecosystem, deploying code to Nvidia servers that cost half a million dollars each. A major part of the selling point are these built-in 200 gigabit QSFP ports, which would cost $1,500 or so to add on to another system, assuming you have the available PCIe bandwidth:&lt;/p&gt;
    &lt;p&gt;Those ports can't achieve 400 Gbps, but they do hit over 200 Gbps in the right conditions, configured for Infiniband / RDMA. And they hit over 100 Gbps for Ethernet (though only when running multiple TCP streams).&lt;/p&gt;
    &lt;p&gt;So it may seem a little bit of an odd duck for me, since I'm not an 'Nvidia developer' and I don't deploy code to Nvidia's 'AI factories'.&lt;/p&gt;
    &lt;p&gt;If I'm being honest, I'm more interested in the 'Grace' part of the GB10 (or 'Grace Blackwell 10') 'AI Superchip. It's a big.LITTLE Arm CPU co-designed by Mediatek, with 10 Cortex-X925 cores and 10 Cortex-A725 cores.&lt;/p&gt;
    &lt;p&gt;The chip is united to the Blackwell GPU, and shares the same 128 GB pool of LPDDR5X memory. And it's a pretty snappy Arm CPU‚Äîjust stuck in a $4,000+ system.&lt;/p&gt;
    &lt;p&gt;But like I said, Dell sent me these boxes to test. They aren't paying for this blog post and have no control over what I say.&lt;/p&gt;
    &lt;p&gt;In fact, one of the main things they said was "this is isn't a gaming machine, so don't focus on that."&lt;/p&gt;
    &lt;p&gt;But that got me thinking. What if... I did.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gaming on Arm Linux&lt;/head&gt;
    &lt;p&gt;Valve just announced the Steam Frame, and it runs on Arm.&lt;/p&gt;
    &lt;p&gt;Steam Frame will use FEX for its x86-Arm translation layer, and CodeWeavers' Crossover Preview for Arm64 was just released, so I thought I'd give that a try on DGX OS (Nvidia's Linux OS, currently based on Ubuntu 24.04).&lt;/p&gt;
    &lt;p&gt;I was able to quickly install Steam, and through that, games like Cyberpunk 2077, Doom Eternal, and Ultimate Epic Battle Simulator II.&lt;/p&gt;
    &lt;p&gt;I'll leave the full experience and test results for you to see in this video:&lt;/p&gt;
    &lt;p&gt;But bottom line, the Windows games I typically test on Arm systems through Steam/Proton played very well here, with no stuttering, and decent frame rates (100 fps in Cyberpunk 2077 at 1080p with low settings).&lt;/p&gt;
    &lt;p&gt;But no, I agree with Dell, this box should not be evaluated as a gaming machine. While it performs admirably for an Arm linux box, you could do a lot better with half the budget if you just wanted to build a dedicated gaming rig. Even with RAM prices as they are today.&lt;/p&gt;
    &lt;head rend="h2"&gt;General Purpose Arm Workstation (with tons of VRAM)&lt;/head&gt;
    &lt;p&gt;This machine is built for AI development, but it just so happens to have a very good Arm CPU and tons of RAM, so I wanted to test it for both running LLMs, and as a general Arm Linux workstation.&lt;/p&gt;
    &lt;p&gt;The video above has more depth, and you can find all my benchmark data here, but I wanted to focus on a few things in particular.&lt;/p&gt;
    &lt;head rend="h2"&gt;Software&lt;/head&gt;
    &lt;p&gt;Before we get to benchmarks, I wanted to mention Nvidia's DGX OS. Based on Ubuntu Linux, it's the only supported Linux distribution for GB10 systems. Regular Ubuntu LTS versions are supported for 5 years, with optional Pro support extending that out to 10 or even 15 years. But DGX OS only guarantees updates for two years, though Nvidia doesn't really offer guarantees for its hardware support.&lt;/p&gt;
    &lt;p&gt;Their track record for ongoing support for their hardware is decidedly mixed, and in the absence of any guarantees, I wouldn't expect them to continue supporting the Spark or other GB10 systems beyond a few years.&lt;/p&gt;
    &lt;p&gt;Some people have had luck getting other distros running, but they're still running Nvidia's kernel. So if you buy one of these, know there's no guarantees for ongoing support.&lt;/p&gt;
    &lt;p&gt;Running things on DGX OS, I've found most server/headless software runs great, but there are still desktop tools that are more of a hassle. Like Blender doesn't have a stable release that uses GPU acceleration on Arm. But if you compile it from sourcelike GitHub user CoconutMacaroon did, you can get full acceleration.&lt;/p&gt;
    &lt;p&gt;Just using this box as a little workstation, it is plenty fast for all the things I do, from coding, to browsing the web, to media editing. (Though media workflows are still rough on Linux in general, even on x86.)&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU benchmarks&lt;/head&gt;
    &lt;p&gt;The Grace CPU is a 20-core Arm chip co-designed by Mediatek, fused together with the Blackwell GPU.&lt;/p&gt;
    &lt;p&gt;There must be some inefficiency there, though, because the system's idle power draw is a bit higher than I'm used to for Arm, coming in around 30 watts. A lot higher than Apple's M3 Ultra with 512GB of RAM, or even AMD's Ryzen AI Max+ 395 (these names just roll right off the tongue, don't they?).&lt;/p&gt;
    &lt;p&gt;In my testing, it seems the CPU itself maxes out around 140 watts, leaving another 140 watts of headroom for the GPU, network, and USB-C ports with PD.&lt;/p&gt;
    &lt;p&gt;Geekbench 6 was a little unstable, which was weird, but when I did get it to run, it was about on par with the AMD Ryzen AI Max+ 395 system I tested earlier this year, the Framework Desktop.&lt;/p&gt;
    &lt;p&gt;Apple's 2-generation-old M3 Ultra Mac studio beats both, but it does cost quite a bit more, so that's to be expected.&lt;/p&gt;
    &lt;p&gt;And testing with High Performance Linpack, the Dell Pro Max gets about 675 Gflops:&lt;/p&gt;
    &lt;p&gt;NVIDIA's marketing said the GB10 "offers a petaflop of AI computing performance"‚Äîa thousand teraflops! This thing can't even hit one...&lt;/p&gt;
    &lt;p&gt;But in the fine print, NVIDIA says it's a petaflop at FP4 precision. HPL tests FP64, aka double precision, which is more used in scientific computing. A FLOP is not always a FLOP, and even the 'petaflop' claim seems disputed, at least if I'm reading John Carmack's tweets correctly.&lt;/p&gt;
    &lt;p&gt;But at least for FP64 on the CPU, the GB10 is fairly efficient, at least compared to x86 systems I've tested:&lt;/p&gt;
    &lt;head rend="h2"&gt;Networking Performance&lt;/head&gt;
    &lt;p&gt;A huge part of the value is the built-in ConnectX-7 networking. I tested that, and it's fast. But also a bit odd. Here's the maximum TCP performance I was able to get through the fastest interface on each of the three systems I've been comparing:&lt;/p&gt;
    &lt;p&gt;But 106 Gigabits isn't 200, is NVIDIA lying?&lt;/p&gt;
    &lt;p&gt;Well, no... it's a little complicated. For full details, I'll refer you to the ServeTheHome article The NVIDIA GB10 ConnectX-7 200GbE Networking is Really Different.&lt;/p&gt;
    &lt;p&gt;Because the ports are each connected to a x4 PCIe Gen 5 link‚Äîwhich isn't enough bandwidth for 200 Gbps per port. To get a full 200 Gbps, you have to use Infiniband/RDMA and carefully configure the network topology. You won't get more than about 206 Gbps, maximum, in real world throughput, no matter how you set it up.&lt;/p&gt;
    &lt;p&gt;That's still honestly pretty good, but it's not the same as getting 400 Gbps of networking for AI clustering, like I think some of us expected reading the initial press releases in early 2025...&lt;/p&gt;
    &lt;p&gt;From the perspective of someone replicating NVIDIA's networking stack locally, though, having ConnectX ports built in is a boon. If you want replicate this kind of developer setup on AMD, you'd have to spend around the same amount of money, for the Max+ 395 plus a Connect-X 7 card.&lt;/p&gt;
    &lt;p&gt;Many people don't care about clustering use cases, or RDMA or Infiniband, but that doesn't mean it's not extremely useful for the people who do. This stuff's expensive, but to some people, it's not a bad value.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI Performance&lt;/head&gt;
    &lt;p&gt;For now I'm just running two models, both of them with llama.cpp, optimized for each architecture.&lt;/p&gt;
    &lt;p&gt;And for a small model that requires a decent amount of CPU to keep up with the GPU, the GB10 does pretty well, almost hitting 100 tokens/s for inference, which is second to the M3 Ultra:&lt;/p&gt;
    &lt;p&gt;But for prompt processing, which is important for how quickly you start seeing a response from AI models, the GB10 chip is the winner, despite costing less than half the M3 Ultra.&lt;/p&gt;
    &lt;p&gt;And it's a similar story for a huge 'dense' model, Llama 3.1 70B, except here, it gets beat just a little by AMD's Strix Halo in the Framework Desktop:&lt;/p&gt;
    &lt;p&gt;Prompt processing is a strong selling point for these boxes. That's the reason Exo teased running a DGX Spark as the compute node for a Mac Studio cluster.&lt;/p&gt;
    &lt;p&gt;You can have the Spark, or one of these Dell's, handle the thing it's best at, prompt processing, while the Mac Studios handle the thing they're best at, memory bandwidth for token generation.&lt;/p&gt;
    &lt;p&gt;Anyway, these are just two quick AI benchmarks, and I have a lot more in the Dell Pro Max with GB10 issue in my ai-benchmarks repository. I'm doing a lot more testing, including model training and how I clustered two of these things in a tiny mini rack, but you'll have to wait until next year for that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Comparing Macs to Macs?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457027</guid><pubDate>Thu, 01 Jan 2026 19:11:53 +0000</pubDate></item><item><title>Linux is good now</title><link>https://www.pcgamer.com/software/linux/im-brave-enough-to-say-it-linux-is-good-now-and-if-you-want-to-feel-like-you-actually-own-your-pc-make-2026-the-year-of-linux-on-your-desktop/</link><description>&lt;doc fingerprint="13f391b9904a4bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I'm brave enough to say it: Linux is good now, and if you want to feel like you actually own your PC, make 2026 the year of Linux on (your) desktop&lt;/head&gt;
    &lt;p&gt;Now if you don't mind I'm going to delete the root folder and see what happens.&lt;/p&gt;
    &lt;p&gt;I'm all-in, baby. I'm committed. If upgrading any distinct component of my PC didn't require me taking out a loan right now, I'd be seriously considering switching my GPU over to some kind of AMD thing just to make my life slightly, slightly easier.&lt;/p&gt;
    &lt;p&gt;I've had it with Windows and ascended to the sunlit uplands of Linux, where the trees heave with open-source fruits and men with large beards grep things with their minds.&lt;/p&gt;
    &lt;p&gt;I'm not alone. In last month's Steam hardware survey, the number of Linux users hit a new all-time high for the second month running, reaching the heady summit of a whopping, ah, 3.2% of overall Steam users. Hey, we're beating Mac players.&lt;/p&gt;
    &lt;p&gt;I think that number will only grow as the new year goes by. More and more of us are getting sick of Windows, sure‚Äîthe AI guff, the constant upselling on Office subs, the middle taskbar*‚Äîbut also, all my experience goofing about with Linux this year has dispelled a lot of the, frankly, erroneous ideas I had about it. It's really not hard! Really! I know Linux guys have been saying this for three decades, but it's true now!&lt;/p&gt;
    &lt;head rend="h2"&gt;Goated with the open source (sorry)&lt;/head&gt;
    &lt;p&gt;As I've already written about, the bulk of my Linux-futzing time this year has been spent in Bazzite, a distro tailor-made for gaming and also tailor-made to stop idiots (me) from doing something likely to detonate their boot drive.&lt;/p&gt;
    &lt;p&gt;I grew up thinking of Linux as 'the command-line OS that lets you delete your bootloader' and, well, I suppose that's not untrue, but I've been consistently impressed at how simple Bazzite has been to run on my PC, even with my persnickety Nvidia GPU.&lt;/p&gt;
    &lt;p&gt;Everything I've played this year has been as easy‚Äîif not easier‚Äîto run on a free OS put together by a gaggle of passionate nerds as it is on Windows, the OS made by one of the most valuable corporations on planet Earth. I've never had to dip into the command line (which is, to be frank, a shame, as the command line is objectively cool).&lt;/p&gt;
    &lt;p&gt;Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.&lt;/p&gt;
    &lt;p&gt;But to be honest, it's not as if the Bazzite team has miraculously made Linux pleasant to use after decades of it seeming difficult and esoteric to normie computer users. I think mainstream Linux distros are just, well, sort of good now. Apart from my gaming PC, I also have an old laptop converted into a media server that lives underneath my television. It runs Debian 13 (which I updated to from Debian 12 earlier in the year) and requires essentially zero input from me at all.&lt;/p&gt;
    &lt;p&gt;What's more, the only software I have on there is software I actually want on there. Oh for a version of Windows that let me do something as zany as, I don't know, uninstall Edge.&lt;/p&gt;
    &lt;p&gt;That's the true nub of it, I think. The stats can say what they like (and they do! We've all heard tales of Windows games actually running better on Linux via Valve's Proton compatibility layer), but the heart of my fatigue with Windows is that, for every new worthless AI gadget Microsoft crams into it and for every time the OS inexplicably boots to a white screen and implores me to "finish setting up" my PC with an Office 365 subscription, the real problem is a feeling that my computer isn't mine, that I am somehow renting this thing I put together with my own two hands from an AI corporation in Redmond.&lt;/p&gt;
    &lt;p&gt;That's fine for consoles. Indeed, part of the whole pitch of an Xbox or PlayStation is the notion that you are handing off a lot of responsibility for your device to Sony and Microsoft's teams of techs, but my PC? That I built? Get your grubby mitts off it.&lt;/p&gt;
    &lt;p&gt;Are there issues? Sure. HDR's still a crapshoot (plus √ßa change) and, as you've no doubt heard, a lot of live-service games have anticheat software that won't play with Linux. But I think both of these issues are gradually ticking toward their solutions, particularly with Valve making its own push into the living room.&lt;/p&gt;
    &lt;p&gt;So I say make 2026 the year you give Linux a try, if you haven't already. At the very least, you can stick it on a separate boot drive and have a noodle about with it. I suspect you'll find the open (source) water is a lot more hospitable than you might think.&lt;/p&gt;
    &lt;p&gt;*I'm actually fine with the middle taskbar. I'm sorry.&lt;/p&gt;
    &lt;p&gt;2026 Games: This year's upcoming games&lt;lb/&gt;Best PC games: Our all-time favorites&lt;lb/&gt;Free PC games: Freebie fest&lt;lb/&gt;Best FPS games: Finest gunplay&lt;lb/&gt;Best RPGs: Grand adventures&lt;lb/&gt;Best co-op games: Better together&lt;/p&gt;
    &lt;p&gt;One of Josh's first memories is of playing Quake 2 on the family computer when he was much too young to be doing that, and he's been irreparably game-brained ever since. His writing has been featured in Vice, Fanbyte, and the Financial Times. He'll play pretty much anything, and has written far too much on everything from visual novels to Assassin's Creed. His most profound loves are for CRPGs, immersive sims, and any game whose ambition outstrips its budget. He thinks you're all far too mean about Deus Ex: Invisible War.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457770</guid><pubDate>Thu, 01 Jan 2026 20:35:11 +0000</pubDate></item><item><title>A website to destroy all websites</title><link>https://henry.codes/writing/a-website-to-destroy-all-websites/</link><description>&lt;doc fingerprint="3cc011532966ad5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A website to destroy all websites.&lt;/head&gt;
    &lt;head rend="h2"&gt;table of contents, of course&lt;/head&gt;
    &lt;head rend="h2"&gt;The internet is bad.&lt;/head&gt;
    &lt;p&gt;Well, the Internet mostly feels bad these days.&lt;/p&gt;
    &lt;p&gt;We were given this vast, holy realm of self-discovery and joy and philosophy and community; a thousand thousand acres of digital landscape, on which to grow our forests and grasslands of imagination, plant our gardens of learning, explore the caves of our making. We were given the chance to know anything about anything, to be our own Prometheus, to make wishes and to grant them.&lt;/p&gt;
    &lt;p&gt;But that‚Äôs not what we use the Internet for anymore. These days, instead of using it to make ourselves, most of us are using it to waste ourselves: we‚Äôre doom-scrolling brain-rot on the attention-farm, we‚Äôre getting slop from the feed.&lt;/p&gt;
    &lt;p&gt;Instead of turning freely in the HTTP meadows we grow for each other, we go to work: we break our backs at the foundry of algorithmic content as this earnest, na√Øve, human endeavoring to connect our lives with others is corrupted. Our powerful drive to learn about ourselves, each other, and our world, is broken into scant remnants ‚Äî hollow, clutching phantasms of Content Creation, speed-cut vertical video, listicle thought-leadership, ragebait and the thread emoji.&lt;/p&gt;
    &lt;head rend="h3"&gt;it wasn‚Äôt always like this.&lt;/head&gt;
    &lt;p&gt;It used to feel way better to Go Online, and some of us will remember.&lt;/p&gt;
    &lt;p&gt;We used to be able to learn about our hobbies and interests from hundreds of experts on a wealth of websites whose only shared motivation was their passion. Some of those venerable old educational blogs, forums, and wikis still stand, though most have been bulldozed.&lt;/p&gt;
    &lt;p&gt;Now, Learning On The Internet often means fighting ads and endless assaults on one‚Äôs attention ‚Äî it means watching part-1-part-2-part-3 short-form video clips, taped together by action movie psychology hacks, narrated gracelessly by TTS AI voices. We‚Äôre down from a thousand and one websites to three, and each of those remaining monolith websites is just a soullessly-regurgitated, compression-down-scaled, AI-up-scaled version of the next.&lt;/p&gt;
    &lt;p&gt;¬∂&lt;/p&gt;
    &lt;p&gt;We used to make lasting friendships with folks all over the world on shared interest and good humor.&lt;/p&gt;
    &lt;p&gt;But now those social networks, once hand-built and hand-tended, vibrant and organic, are unceremoniously swallowed by social media networks, pens built for trapping us and our little piggy attentions, turning us all into clout-chasers &amp;amp; content-creators, and removing us from what meaningful intimacy &amp;amp; community felt like.&lt;/p&gt;
    &lt;p&gt;¬∂&lt;/p&gt;
    &lt;p&gt;Even coding for the web used to be different: One could Learn To Code‚Ñ¢ to express oneself creatively, imbue one‚Äôs online presence with passion and meaning, and for some of us, build a real career.&lt;/p&gt;
    &lt;p&gt;These days, however, we write increasing amounts of complicated, unsecure code to express less and less meaning, in order to infinitely generate shareholder value. We don‚Äôt think about the art of our craft and the discipline of its application, we think about throughput and scale.&lt;/p&gt;
    &lt;p&gt;To be very clear: I‚Äôm not trying to Good Old Days the internet. None of this is meant to make you feel nostalgic ‚Äî the Internet used to be slow and less populated and less diverse, and its access was limited to those of a certain class. The Web For All is a marked improvement, widespread global internet access is a marked improvement, and what I‚Äôm asking you to consider is what it used to feel like to use these tools, and what we‚Äôve lost in the Big Tech, Web 2.0 and web3 devouring of the ‚ÄôNet.&lt;/p&gt;
    &lt;head rend="h2"&gt;The invention of the automobile&lt;/head&gt;
    &lt;p&gt;The onset of the automobile was a revelation for access and personal liberty. With the advent of cars, members of society could travel farther, get more done in their day, and bend their limited time more to their creative will!&lt;/p&gt;
    &lt;p&gt;But as time wore on and the industrialization &amp;amp; proliferation of the automobile progressed, its marginal utility diminished ‚Äî the industry started to offer society fewer &amp;amp; fewer benefits, and take more &amp;amp; more in exchange1.&lt;/p&gt;
    &lt;p&gt;In American cities, for example: though at first the automobile enabled humans to travel further distances, it now demanded that humans travel those distances, and demanded infrastructure be created &amp;amp; maintained to enable it.2 Many now must use an automobile to get everything done in their town in a day, and must pay &amp;amp; take time for that automobile‚Äôs fueling &amp;amp; maintenance.3&lt;/p&gt;
    &lt;p&gt;Further than that, the automobile asks all of us to chip in tax revenue to protect its infrastructure, but only certain classes can afford an automobile with which to use that infrastructure, and those classes who can‚Äôt afford to do so are relegated to underfunded public transit systems.4&lt;/p&gt;
    &lt;p&gt;No longer a tool to serve our societies, our societies now serve the automobile.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tools for Conviviality, &amp;amp; the industrialization of the Web.&lt;/head&gt;
    &lt;p&gt;In his book Tools For Conviviality, technology philosopher and social critic Ivan Illich identifies these two critical moments, the optimistic arrival &amp;amp; the deadening industrialization, as watersheds of technological advent. Tools are first created to enhance our capacities to spend our energy more freely and in turn spend our days more freely, but as their industrialization increases, their manipulation &amp;amp; usurpation of society increases in tow5.&lt;/p&gt;
    &lt;p&gt;Illich also describes the concept of radical monopoly, which is that point where a technological tool is so dominant that people are excluded from society unless they become its users. We saw this with the automobile, we saw it with the internet, and we even see it with social media.&lt;/p&gt;
    &lt;p&gt;&lt;del&gt;No longer a tool to serve our societies, our societies now serve the automobile.&lt;/del&gt; Instead of designing and using tools to build a society, our society changes to adapt to the demands of our tools.&lt;/p&gt;
    &lt;p&gt;¬∂&lt;/p&gt;
    &lt;p&gt;Illich‚Äôs thesis allows us to reframe our adoption and use of the technologies in our life. We can map fairly directly most technological developments in the last 100 (or even 200) years to this framework: a net lift, followed by a push to extract value and subsequent insistence upon the technology‚Äôs ubiquity:&lt;/p&gt;
    &lt;head rend="h3"&gt;the textile revolution&lt;/head&gt;
    &lt;p&gt;The preferred imagery used to mythologize the Industrial Revolution is the woodetchings of textile manufacturers, transformed in the early 19th century by the arrival of automated fabric machinery. Its proponents laud the shift of an agricultural society to a technological one, creating new sectors for labor, and raising up the middle class (we will say nothing of this period‚Äôs new punishing conditions for labor in this essay6). But the ultimate ecological and human costs engendered by the increasing availability of cheap fabric production are well-documented: In 2022, the fashion and textile industries employed around 60 million factory workers worldwide7, and less than 2% of those workers earn a living wage. Those workers also endure the full suite of labor exploitation practices, including gender-based harassment, wage theft, and unsafe conditions. On the material side, the induced consumption resulting from ever-cheaper products means the world consumed 400% more textile products globally as 20 years ago8, and bins most of it (the average American generates 82 pounds of textile waste each year).&lt;/p&gt;
    &lt;head rend="h3"&gt;antibiotic technology&lt;/head&gt;
    &lt;p&gt;The arrival of antibiotics in 19289 allowed for revolutionary leaps in fighting bacterial infections like strep throat, pneumonia, and meningitis, but an over-dependence and over-prescription of penicillin and its siblings through the 1950s-70s resulted in the proliferation of antibiotic resistance, which subsequently led to longer hospital stays, higher medical costs, and increased mortality.10&lt;/p&gt;
    &lt;head rend="h3"&gt;space exploration&lt;/head&gt;
    &lt;p&gt;Since the beginning of the space exploration era in the late 1950s, humanity has made leaps and bounds in learning about our own world and its physical systems, telecommunications, imaging, etc. The increasing frequency of commercialization missions in space for satellite systems (and lately tourism) has resulted in immense amounts of space debris being generated ‚Äî both from active satellites and from jettisoned/destroyed components of previous missions, the debris threatens future missions and has even been destructive to the field of astronomy, making it impossible to use earth-based sensors and photography devices to learn about space.11 So desperate to extract Shareholder Value from the starry sky, we‚Äôre blinding our own ability to look at it.&lt;/p&gt;
    &lt;p&gt;The web is no exception to this pattern. A vision of interoperability, accessibility, and usability, the World Wide Web was first conceived in 1989 as a way to universally link documents and other media content in a flexibly-organized system that could make information easily accessed at CERN, and be easily shared with collaborators beyond.12 But the proliferation of access and ultimate social requirement of access has spawned countless troubles for human society, including cyberstalking and bullying, the instantaneous circulation of CSAM, violent images, and misinformation, identity theft, addiction, etcetera.&lt;/p&gt;
    &lt;p&gt;The rampant industrialization and commercialization of the Web predictably develops flashy, insidious patterns of extracting capital from its users: new surfaces for information means new surfaces for advertisement, and new formats of media beget new mechanisms for divorcing you from their ownership.&lt;/p&gt;
    &lt;head rend="h3"&gt;convivial life &amp;amp; convivial tooling&lt;/head&gt;
    &lt;p&gt;Illich poses convivial tools as directly opposed to this industrialized, radically-monopolized set of social systems. Similar to E.F. Schumacher‚Äôs concept of ‚Äúintermediate technology‚Äù introduced in his 1973 book Small Is Beautiful: A Study of Economics As If People Mattered, convivial tools are sustainable, energy-efficient (though often labor intensive), local-first, and designed primarily to enhance the autonomy and creativity of their users.13 Illich cites specifically hand tools, bicycles, and telephones as examples, but with its enormous capacity for interoperability and extensibility, the Internet is the perfect workshed in which to design our own Tools For Conviviality.&lt;/p&gt;
    &lt;head rend="h2"&gt;the Web we want&lt;/head&gt;
    &lt;p&gt;let‚Äôs reconsider&lt;/p&gt;
    &lt;p&gt;the markers of a decaying 'Net I mentioned before, with convivial tooling in mind:&lt;/p&gt;
    &lt;head rend="h3"&gt;Teaching &amp;amp; learning on the Web&lt;/head&gt;
    &lt;p&gt;Monolithic platforms like YouTube, TikTok, Medium, and Substack draw a ton of creators and educators because of the promise of monetization and large audiences, but they‚Äôve shown time and time again how the lack of ownership creates a problem. When those platforms fail, when they change their rules, when they demand creators move or create a particular way to maintain their access to those audiences, they pit creators or their audiences against the loss of the other. Without adhering to the algorithm‚Äôs requirements, writers may not write an impactful document, and without bypassing a paywall, readers can‚Äôt read it.&lt;/p&gt;
    &lt;p&gt;¬∂&lt;/p&gt;
    &lt;p&gt;When those promises of exorbitant wealth and a life of decadence through per-click monetization ultimately dry up (or come with a steep moral or creative cost), creators and learners must look for new solutions for how educational content is shared on the Internet. The most self-evident, convivial answer is an old one: blogs. HTML is free to access by default, RSS has worked for about 130 years[citation needed], and combined with webmentions, it‚Äôs never been easier to read new ideas, experiment with ideas, and build upon &amp;amp; grow those ideas with other strong thinkers on the web, owning that content all along.14&lt;/p&gt;
    &lt;head rend="h3"&gt;Connecting with friends on the Web&lt;/head&gt;
    &lt;p&gt;Social media apps have imprisoned us all in this weird content prison ‚Äî in order to connect with friends we‚Äôre sort of forced to create or be vanished by capricious black box algorithms, and all that we do create is, as we‚Äôve already alluded to, subsequently owned by whatever platform we‚Äôve created it on. If Instagram goes away overnight, or decides to pivot catastrophically, your stories and your network of friends goes with it.&lt;/p&gt;
    &lt;p&gt;¬∂&lt;/p&gt;
    &lt;p&gt;The advent and development of tools &amp;amp; methodologies like POSSE (Publish On your Own Site, Syndicate Elsewhere), ActivityPub, microformats, and ATProto, it‚Äôs becoming quite achievable to generate your own social network, interoperable with other networks like Bluesky or Mastodon. That network, designed for ownership and decentralization, is durable, designed around storytelling instead of engagement, and free of the whims of weird tech billionaires.&lt;/p&gt;
    &lt;p&gt;With some basic HTML knowledge and getting-stuff-online knowledge, a handful of scrappy protocols, and a free afternoon or two, one can build their own home to post bangers for the tight homies, make friends, and snipe those new friends with those hits of dopamine they so fiendishly rely on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding for the web&lt;/head&gt;
    &lt;p&gt;Lastly, consider the discipline of web engineering:&lt;/p&gt;
    &lt;p&gt;We have been asked to build the same B2B SaaS website with the same featureset n^‚àû times, and our answers for the optimal way to do that are increasingly limited. We‚Äôve penned all of our markup into JavaScript templates just in case a product manager needs the wrapper component to post JSON somewhere down the line, and we‚Äôve whittled away at style code until it‚Äôs just a mechanism for deploying one of two border-radius-drop-shadow combos to divs. It‚Äôs an industrial, production-minded way of approaching a discipline that has all the hallmarks of being a great craft, and that‚Äôs understandably uninspiring to many of us.&lt;/p&gt;
    &lt;p&gt;¬∂&lt;/p&gt;
    &lt;p&gt;Yet our young React shepherds have no need to fear: there are countless more colors than blurple out there, and countless more fonts than Inter. HTML and CSS are better and more generative technologies than they‚Äôve ever been: Thanks to the tireless work of the CSS working groups and browser implementers, etc, there is an unbelievable amount of creative expression possible with basic web tools in a text editor. Even JavaScript is more progressively-enhanceable than ever, and enables interfacing with a rapidly-growing number of exciting browser APIs (still fuck Brendan Eich though). &lt;code&gt;${new Date.getCurrentYear()}&lt;/code&gt; is a veritable renaissance of web code, and it asks of authors only curiosity and a drive to experiment.&lt;/p&gt;
    &lt;head rend="h2"&gt;so where do we go from here?&lt;/head&gt;
    &lt;p&gt;Illich‚Äôs thesis is that technology and its derived tools should serve people in a way that enhances their freedom, creativity, independence, and will.&lt;/p&gt;
    &lt;p&gt;The distillation of those principles on the web through manual code, hand-built social networks, and blogs, points luminously to one answer to the question of how the Internet can best serve humans:&lt;/p&gt;
    &lt;head rend="h3"&gt;it‚Äôs personal websites.&lt;/head&gt;
    &lt;p&gt;Hand-coded, syndicated, and above all personal websites are exemplary: They let users of the internet to be autonomous, experiment, have ownership, learn, share, find god, find love, find purpose. Bespoke, endlessly tweaked, eternally redesigned, built-in-public, surprising UI and delightful UX. The personal website is a staunch undying answer to everything the corporate and industrial web has taken from us.&lt;/p&gt;
    &lt;p&gt;And how might one claim this ultimate toolchain of conviviality, and build a place on the web that enhances their autonomy and creativity?&lt;/p&gt;
    &lt;p&gt;How might one build a personal website?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Start small&lt;/head&gt;
        &lt;p&gt;Let yourself start small, have fun trying shit that doesn‚Äôt work, document your growth, publish failed ideas &amp;amp; successful ones. Some of the best websites in the world are just HTML, and they belong to their authors. Make friends, let yourself be inspired by others, send friendly emails asking to learn new things, and do not demand of yourself masterpieces.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;head rend="h3"&gt;Reduce friction to publishing&lt;/head&gt;&lt;p&gt;Get the resistance to ship out of your way. Don‚Äôt get caught up in tooling and frameworks, just write HTML and get something online. If you‚Äôre an engineer, delight that you‚Äôre not beholden to the same standards of quality and rigorous testing that you are at work ‚Äî draft some ideas, hit the&lt;/p&gt;&lt;code&gt;h1&lt;/code&gt;to&lt;code&gt;p&lt;/code&gt;tag combo, and publish. Update and update again; let your ideas grow like gardens, the way they do in your mind. The mutability of the web, often its great weakness, is also one of its great strengths.&lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Don‚Äôt worry about design (unless you want to)&lt;/head&gt;
        &lt;p&gt;Don‚Äôt worry about design unless that‚Äôs the part that brings you joy. Make friends with designers and trade your work for theirs, or trade tips, trade advice. Get comfortable with being joyfully bad at something ‚Äî from that soil of humility grows a million questions for those who have learned and are excited to share. Iterate until you‚Äôve something you‚Äôre proud of, or iterate so much you‚Äôve ruined it and have to go back to bald.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Use the IndieWeb&lt;/head&gt;
        &lt;p&gt;Leverage the IndieWeb and its wonderfully thought-out protocols, tools like brid.gy to syndicate your ideas out to the wider web, and then use Webmentions to bring the ensuing conversations back where the content is. That way, you can publish work where you prefer to, folks on Bluesky can enjoy and discuss it, in the same stroke as folks on Mastodon may, or folks directly on the canonical URL.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Join us in sharing what you‚Äôve made&lt;/head&gt;
        &lt;p&gt;I encourage you to join us in our auspicious website adventure, and if you do, I hope you‚Äôll further join us on personalsit.es, our happy little home for everyone building something humble or thrilling or joyful or deeply accursed, but personal.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;(denouement)&lt;/head&gt;
    &lt;p&gt;You‚Äôre not crazy. The internet does feel genuinely so awful right now, and for about a thousand and one reasons. But the path back to feeling like you have some control is to un-spin yourself from the Five Apps of the Apocalypse and reclaim the Internet as a set of tools you use to build something you can own &amp;amp; be proud of ‚Äî or in most of our cases, be deeply ashamed of. Godspeed and good luck.&lt;/p&gt;
    &lt;p&gt;‚ù¶&lt;/p&gt;
    &lt;p&gt;That‚Äôs all for me. If you find any issues with this post, please reach out to me by email. Thanks eternally for your time and patience, and thanks for reading. Find me here online at one of my personal websites like henry.codes or strange.website or stillness.digital or strangersbyspring.com, or sometimes on Bluesky and Mastodon.&lt;/p&gt;
    &lt;p&gt;As ever, unionize, free Palestine, trans rights are human rights, fix your heart or die.&lt;/p&gt;
    &lt;p&gt;fin.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457784</guid><pubDate>Thu, 01 Jan 2026 20:36:46 +0000</pubDate></item><item><title>Can Bundler be as fast as uv?</title><link>https://tenderlovemaking.com/2025/12/29/can-bundler-be-as-fast-as-uv/</link><description>&lt;doc fingerprint="7685981e496335d9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Can Bundler Be as Fast as uv?&lt;/head&gt;Dec 29, 2025 @ 12:26 pm&lt;p&gt;At RailsWorld earlier this year, I got nerd sniped by someone. They asked ‚Äúwhy can‚Äôt Bundler be as fast as uv?‚Äù Immediately my inner voice said ‚ÄúYA, WHY CAN‚ÄôT IT BE AS FAST AS UV????‚Äù&lt;/p&gt;&lt;p&gt;My inner voice likes to shout at me, especially when someone asks a question so obvious I should have thought of it myself. Since then I‚Äôve been thinking about and investigating this problem, going so far as to give a presentation at XO Ruby Portland about Bundler performance. I firmly believe the answer is ‚ÄúBundler can be as fast as uv‚Äù (where ‚Äúas fast‚Äù has a margin of error lol).&lt;/p&gt;&lt;p&gt;Fortunately, Andrew Nesbitt recently wrote a post called ‚ÄúHow uv got so fast‚Äù, and I thought I would take this opportunity to review some of the highlights of the post and how techniques applied in uv can (or can‚Äôt) be applied to Bundler / RubyGems. I‚Äôd also like to discuss some of the existing bottlenecks in Bundler and what we can do to fix them.&lt;/p&gt;&lt;p&gt;If you haven‚Äôt read Andrew‚Äôs post, I highly recommend giving it a read. I‚Äôm going to quote some parts of the post and try to reframe them with RubyGems / Bundler in mind.&lt;/p&gt;&lt;head rend="h2"&gt;Rewrite in Rust?&lt;/head&gt;&lt;p&gt;Andrew opens the post talking about rewriting in Rust:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;uv installs packages faster than pip by an order of magnitude. The usual explanation is √¢it√¢s written in Rust.√¢ That√¢s true, but it doesn√¢t explain much. Plenty of tools are written in Rust without being notably fast. The interesting question is what design decisions made the difference.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is such a good quote. I‚Äôm going to address ‚Äúrewrite in Rust‚Äù a bit later in the post. But suffice to say, I think if we eliminate bottlenecks in Bundler such that the only viable option for performance improvements is to ‚Äúrewrite in Rust‚Äù, then I‚Äôll call it a success. I think rewrites give developers the freedom to ‚Äúthink outside the box‚Äù, and try techniques they might not have tried. In the case of &lt;code&gt;uv&lt;/code&gt;, I think it gave the developers a good way to say ‚Äúif we don‚Äôt have to worry about backwards compatibility, what could we achieve?‚Äù.&lt;/p&gt;&lt;p&gt;I suspect it would be possible to write a uv in Python (PyUv?) that approaches the speeds of uv, and in fact much of the blog post goes on to talk about performance improvements that aren‚Äôt related to Rust.&lt;/p&gt;&lt;head rend="h2"&gt;Installing code without eval‚Äôing&lt;/head&gt;&lt;quote&gt;&lt;p&gt;pip√¢s slowness isn√¢t a failure of implementation. For years, Python packaging required executing code to find out what a package needed.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I didn‚Äôt know this about Python packages, and it doesn‚Äôt really apply to Ruby Gems so I‚Äôm mostly going to skip this section.&lt;/p&gt;&lt;p&gt;Ruby Gems are tar files, and one of the files in the tar file is a YAML representation of the GemSpec. This YAML file declares all dependencies for the Gem, so RubyGems can know, without evaling anything, what dependencies it needs to install before it can install any particular Gem. Additionally, RubyGems.org provides an API for asking about dependency information, which is actually the normal way of getting dependency info (again, no &lt;code&gt;eval&lt;/code&gt; required).&lt;/p&gt;&lt;p&gt;There‚Äôs only one other thing from this section I‚Äôd like to quote:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;PEP 658 (2022) put package metadata directly in the Simple Repository API, so resolvers could fetch dependency information without downloading wheels at all.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Fortunately RubyGems.org already provides the same information about gems.&lt;/p&gt;&lt;p&gt;Reading through the number of PEPs required as well as the amount of time it took to get the standards in place was very eye opening for me. I can‚Äôt help but applaud folks in the Python community for doing this. It seems like a mountain of work, and they should really be proud of themselves.&lt;/p&gt;&lt;head rend="h2"&gt;What uv drops&lt;/head&gt;&lt;p&gt;I‚Äôm mostly going to skip this section except for one point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Ignoring requires-python upper bounds. When a package says it requires python&amp;lt;4.0, uv ignores the upper bound and only checks the lower. This reduces resolver backtracking dramatically since upper bounds are almost always wrong. Packages declare python&amp;lt;4.0 because they haven√¢t tested on Python 4, not because they√¢ll actually break. The constraint is defensive, not predictive.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think this is very very interesting. I don‚Äôt know how much time Bundler spends on doing ‚Äúrequired Ruby version‚Äù bounds checking, but it feels like if uv can do it, so can we.&lt;/p&gt;&lt;head rend="h2"&gt;Optimizations that don√¢t need Rust&lt;/head&gt;&lt;p&gt;I really love that Andrew pointed out optimizations that could be made that don‚Äôt involve Rust. There are three points in this section that I want to pull out:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Parallel downloads. pip downloads packages one at a time. uv downloads many at once. Any language can do this.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is absolutely true, and is a place where Bundler could improve. Bundler currently has a problem when it comes to parallel downloads, and needs a small architectural change as a fix.&lt;/p&gt;&lt;p&gt;The first problem is that Bundler tightly couples installing a gem with downloading the gem. You can read the installation code here, but I‚Äôll summarize the method in question below:&lt;/p&gt;&lt;code&gt;def install
  path = fetch_gem_if_not_cached
  Bundler::RubyGemsGemInstaller.install path, dest
end
&lt;/code&gt;&lt;p&gt;The problem with this method is that it inextricably links downloading the gem with installing it. This is a problem because we could be downloading gems while installing other gems, but we‚Äôre forced to wait because the installation method couples the two operations. Downloading gems can trivially be done in parallel since the &lt;code&gt;.gem&lt;/code&gt; files are just archives that can be fetched independently.&lt;/p&gt;&lt;p&gt;The second problem is the queuing system in the installation code. After gem resolution is complete, and Bundler knows what gems need to be installed, it queues them up for installation. You can find the queueing code here. The code takes some effort to understand. Basically it allows gems to be installed in parallel, but only gems that have already had their dependencies installed.&lt;/p&gt;&lt;p&gt;So for example, if you have a dependency tree like ‚Äúgem &lt;code&gt;a&lt;/code&gt; depends on gem &lt;code&gt;b&lt;/code&gt; which depends on gem &lt;code&gt;c&lt;/code&gt;‚Äù (&lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt;), then no gems will be installed (or downloaded) in parallel.&lt;/p&gt;&lt;p&gt;To demonstrate this problem in an easy-to-understand way, I built a slow Gem server. It generates a dependency tree of &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; (&lt;code&gt;a&lt;/code&gt; depends on &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; depends on &lt;code&gt;c&lt;/code&gt;), then starts a Gem server.
The Gem server takes 3 seconds to return any Gem, so if we point Bundler at this Gem server and then profile Bundler, we can see the impact of the queueing system and download scheme.&lt;/p&gt;&lt;p&gt;In my test app, I have the following Gemfile:&lt;/p&gt;&lt;code&gt;source "http://localhost:9292"

gem "a"
&lt;/code&gt;&lt;p&gt;If we profile Bundle install with Vernier, we can see the following swim lanes in the marker chart:&lt;/p&gt;&lt;p&gt;The above chart is showing that we get no parallelism during installation. We spend 3 seconds downloading the &lt;code&gt;c&lt;/code&gt; gem, then we install it.
Then we spend 3 seconds downloading the &lt;code&gt;b&lt;/code&gt; gem, then we install it.
Finally we spend 3 seconds downloading the &lt;code&gt;a&lt;/code&gt; gem, and we install it.&lt;/p&gt;&lt;p&gt;Timing the &lt;code&gt;bundle install&lt;/code&gt; process shows we take over 9 seconds to install (3 seconds per gem):&lt;/p&gt;&lt;code&gt;&amp;gt; rm -rf x; rm -f Gemfile.lock; time GEM_PATH=(pwd)/x GEM_HOME=(pwd)/x bundle install
Fetching gem metadata from http://localhost:9292/...
Resolving dependencies...
Fetching c 1.0.0
Installing c 1.0.0
Fetching b 1.0.0
Installing b 1.0.0
Fetching a 1.0.0
Installing a 1.0.0
Bundle complete! 1 Gemfile dependency, 3 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.

________________________________________________________
Executed in   11.80 secs      fish           external
   usr time  341.62 millis  231.00 micros  341.38 millis
   sys time  223.20 millis  712.00 micros  222.49 millis
&lt;/code&gt;&lt;p&gt;Contrast this with a Gemfile containing &lt;code&gt;d&lt;/code&gt;, &lt;code&gt;e&lt;/code&gt;, and &lt;code&gt;f&lt;/code&gt;, which have no dependencies, but still take 3 seconds to download:&lt;/p&gt;&lt;code&gt;source "http://localhost:9292"

gem "d"
gem "e"
gem "f"
&lt;/code&gt;&lt;p&gt;Timing &lt;code&gt;bundle install&lt;/code&gt; for the above Gemfile shows it takes about 4 seconds:&lt;/p&gt;&lt;code&gt;&amp;gt; rm -rf x; rm -f Gemfile.lock; time GEM_PATH=(pwd)/x GEM_HOME=(pwd)/x bundle install
Fetching gem metadata from http://localhost:9292/.
Resolving dependencies...
Fetching d 1.0.0
Fetching e 1.0.0
Fetching f 1.0.0
Installing e 1.0.0
Installing f 1.0.0
Installing d 1.0.0
Bundle complete! 3 Gemfile dependencies, 3 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.

________________________________________________________
Executed in    4.14 secs      fish           external
   usr time  374.04 millis    0.38 millis  373.66 millis
   sys time  368.90 millis    1.09 millis  367.81 millis
&lt;/code&gt;&lt;p&gt;We were able to install the same number of gems in a fraction of the time. This is because Bundler is able to download siblings in the dependency tree in parallel, but unable to handle other relationships.&lt;/p&gt;&lt;p&gt;There is actually a good reason that Bundler insists dependencies are installed before the gems themselves: native extensions. When installing native extensions, the installation process must run Ruby code (the &lt;code&gt;extconf.rb&lt;/code&gt; file).
Since the &lt;code&gt;extconf.rb&lt;/code&gt; could require dependencies be installed in order to run, we must install dependencies first.
For example &lt;code&gt;nokogiri&lt;/code&gt; depends on &lt;code&gt;mini_portile2&lt;/code&gt;, but &lt;code&gt;mini_portile2&lt;/code&gt; is only used during the installation process, so it needs to be installed before &lt;code&gt;nokogiri&lt;/code&gt; can be compiled and installed.&lt;/p&gt;&lt;p&gt;However, if we were to decouple downloading from installation it would be possible for us to maintain the ‚Äúdependencies are installed first‚Äù business requirement but speed up installation. In the &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case, we could have been downloading gems &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; at the same time as gem &lt;code&gt;c&lt;/code&gt; (or even while waiting on &lt;code&gt;c&lt;/code&gt; to be installed).&lt;/p&gt;&lt;p&gt;Additionally, pure Ruby gems don‚Äôt need to execute any code on installation. If we knew that we were installing a pure Ruby gem, it would be possible to relax the ‚Äúdependencies are installed first‚Äù business requirement and get even more performance increases. The above &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case could install all three gems in parallel since none of them execute Ruby code during installation.&lt;/p&gt;&lt;p&gt;I would propose we split installation in to 4 discrete steps:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Download the gem&lt;/item&gt;&lt;item&gt;Unpack the gem&lt;/item&gt;&lt;item&gt;Compile the gem&lt;/item&gt;&lt;item&gt;Install the gem&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Downloading and unpacking can be done trivially in parallel. We should unpack the gem to a temporary folder so that if the process crashes or the machine loses power, the user isn‚Äôt stuck with a half-installed gem. After we unpack the gem, we can discover whether the gem is a native extension or not. If it‚Äôs not a native extension, we ‚Äúinstall‚Äù the gem simply by moving the temporary folder to the ‚Äúcorrect‚Äù location. This step could even be a ‚Äúhard link‚Äù step as discussed in the next point.&lt;/p&gt;&lt;p&gt;If we discover that the gem is a native extension, then we can ‚Äúpause‚Äù installation of that gem until its dependencies are installed, then resume (by compiling) at an appropriate time.&lt;/p&gt;&lt;p&gt;Side note: &lt;code&gt;gel&lt;/code&gt;, a Bundler alternative, works mostly in this manner today.
Here is a timing of the &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case from above:&lt;/p&gt;&lt;code&gt;&amp;gt; rm -f Gemfile.lock; time gel install
Fetching sources....
Resolving dependencies...
Writing lockfile to /Users/aaron/git/gemserver/app/Gemfile.lock
Installing c (1.0.0) 
Installing a (1.0.0)
Installing b (1.0.0)
Installed 3 gems  

________________________________________________________
Executed in    4.07 secs      fish           external
   usr time  289.22 millis    0.32 millis  288.91 millis
   sys time  347.04 millis    1.36 millis  345.68 millis
&lt;/code&gt;&lt;p&gt;Lets move on to the next point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Global cache with hardlinks. pip copies packages into each virtual environment. uv keeps one copy globally and uses hardlinks&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think this is a great idea, but I‚Äôd actually like to split the idea in two. First, RubyGems and Bundler should have a combined, global cache, full stop. I think that global cache should be in &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt;, and we should store &lt;code&gt;.gem&lt;/code&gt; files there when they are downloaded.&lt;/p&gt;&lt;p&gt;Currently, both Bundler and RubyGems will use a Ruby version specific cache folder. In other words, if you do &lt;code&gt;gem install rails&lt;/code&gt; on two different versions of Ruby, you get two copies of Rails and all its dependencies.&lt;/p&gt;&lt;p&gt;Interestingly, there is an open ticket to implement this, it just needs to be done.&lt;/p&gt;&lt;p&gt;The second point is hardlinking on installation. The idea here is that rather than unpacking the gem multiple times, once per Ruby version, we simply unpack once and then hard link per Ruby version. I like this idea, but I think it should be implemented after some technical debt is paid: namely implementing a global cache and unifying Bundler / RubyGems code paths.&lt;/p&gt;&lt;p&gt;On to the next point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;PubGrub resolver&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Actually Bundler already uses a Ruby implementation of the PubGrub resolver. You can see it here. Unfortunately, RubyGems still uses the molinillo resolver.&lt;/p&gt;&lt;p&gt;In other words you use a different resolver depending on whether you do &lt;code&gt;gem install&lt;/code&gt; or &lt;code&gt;bundle install&lt;/code&gt;.
I don‚Äôt really think this is a big deal since the vast majority of users will be doing &lt;code&gt;bundle install&lt;/code&gt; most of time.
However, I do think this discrepancy is some technical debt that should be addressed, and I think this should be addressed via unification of RubyGems and Bundler codebases (today they both live in the same repository, but the code isn‚Äôt necessarily combined).&lt;/p&gt;&lt;p&gt;Lets move on to the next section of Andrew‚Äôs post:&lt;/p&gt;&lt;head rend="h2"&gt;Where Rust actually matters&lt;/head&gt;&lt;p&gt;Andrew first mentions ‚ÄúZero-copy deserialization‚Äù. This is of course an important technique, but I‚Äôm not 100% sure where we would utilize it in RubyGems / Bundler. I think that today we parse the YAML spec on installation, and that could be a target. But I also think we could install most gems without looking at the YAML gemspec at all.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Thread-level parallelism. Python√¢s GIL forces parallel work into separate processes, with IPC overhead and data copying.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is an interesting point. I‚Äôm not sure what work pip needed to do in separate processes. Installing a pure Ruby, Ruby Gem is mostly an IO bound task, with some ZLIB mixed in. Both of these things (IO and ZLIB processing) release Ruby‚Äôs GVL, so it‚Äôs possible for us to do things truly in parallel. I imagine this is similar for Python / pip, but I really have no idea.&lt;/p&gt;&lt;p&gt;Given the stated challenges with Python‚Äôs GIL, you might wonder whether Ruby‚Äôs GVL presents similar parallelism problems for Bundler. I don‚Äôt think so, and in fact I think Ruby‚Äôs GVL gets kind of a bad rap. It prevents us from running CPU bound Ruby code in parallel. Ractors address this, and Bundler could possibly leverage them in the future, but since installing Gems is mostly an IO bound task I‚Äôm not sure what the advantage would be (possibly the version solver, but I‚Äôm not sure what can be parallelized in there). The GVL does allow us to run IO bound work in parallel with CPU bound Ruby code. CPU bound native extensions are allowed to release the GVL, allowing Ruby code to run in parallel with the native extension‚Äôs CPU bound code.&lt;/p&gt;&lt;p&gt;In other words, Ruby‚Äôs GVL allows us to safely run work in parallel. That said, the GVL can work against us because releasing and acquiring the GVL takes time.&lt;/p&gt;&lt;p&gt;If you have a system call that is very fast, releasing and acquiring the GVL could end up being a large percentage of that call. For example, if you do &lt;code&gt;File.binwrite(file, buffer)&lt;/code&gt;, and the buffer is very small, you could encounter a situation where GVL book keeping is the majority of the time.
A bummer is that Ruby Gem packages usually contain lots of very small files, so this problem could be impacting us.
The good news is that this problem can be solved in Ruby itself, and indeed some work is being done on it today.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;No interpreter startup. Every time pip spawns a subprocess, it pays Python√¢s startup cost.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Obviously Ruby has this same problem. That said, we only start Ruby subprocesses when installing native extensions. I think native extensions make up the minority of gems installed, and even when installing a native extension, it isn‚Äôt Ruby startup that is the bottleneck. Usually the bottleneck is compilation / linking time (as we‚Äôll see in the next post).&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Compact version representation. uv packs versions into u64 integers where possible, making comparison and hashing fast.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is a cool optimization, but I don‚Äôt think it‚Äôs actually Rust specific. Comparing integers is much faster than comparing version objects. The idea is that you take a version number, say &lt;code&gt;1.0.0&lt;/code&gt;, and then pack each part of the version in to a single integer.
For example, we could represent &lt;code&gt;1.0.0&lt;/code&gt; as &lt;code&gt;0x0001_0000_0000_0000&lt;/code&gt; and &lt;code&gt;1.1.0&lt;/code&gt; as &lt;code&gt;0x0001_0001_0000_0000&lt;/code&gt;, etc.&lt;/p&gt;&lt;p&gt;It should be possible to use this trick in Ruby and encode versions to integer immediates, which would unlock performance in the resolver. Rust has an advantage here - compiled native code comparing u64s will always be faster than Ruby, even with immediates. However, I would bet that with the YJIT or ZJIT in play, this gap could be closed enough that no end user would notice the difference between a Rust or Ruby implementation of Bundler.&lt;/p&gt;&lt;p&gt;I started refactoring the &lt;code&gt;Gem::Version&lt;/code&gt; object so that we might start doing this, but we ended up reverting it because of backwards compatibility (I am jealous of &lt;code&gt;uv&lt;/code&gt; in that regard).
I think the right way to do this is to refactor the solver entry point and ensure all version requirements are encoded as integer immediates before entering the solver.
We could keep the &lt;code&gt;Gem::Version&lt;/code&gt; API as ‚Äúuser facing‚Äù and design a more internal API that the solver uses.
I am very interested in reading the version encoding scheme in uv.
My intuition is that minor numbers tend to get larger than major numbers, so would minor numbers have more dedicated bits?
Would it even matter with 64 bits?&lt;/p&gt;&lt;head rend="h2"&gt;Wrapping this up&lt;/head&gt;&lt;p&gt;I‚Äôm going to quote Andrew‚Äôs last 2 paragraphs:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;uv is fast because of what it doesn√¢t do, not because of what language it√¢s written in. The standards work of PEP 518, 517, 621, and 658 made fast package management possible. Dropping eggs, pip.conf, and permissive parsing made it achievable. Rust makes it a bit faster still.&lt;/p&gt;&lt;p&gt;pip could implement parallel downloads, global caching, and metadata-only resolution tomorrow. It doesn√¢t, largely because backwards compatibility with fifteen years of edge cases takes precedence. But it means pip will always be slower than a tool that starts fresh with modern assumptions.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think these are very good points. The difference is that in RubyGems and Bundler, we already have the infrastructure in place for writing a ‚Äúfast as uv‚Äù package manager. The difficult part is dealing with backwards compatibility, and navigating two legacy codebases. I think this is the real advantage the uv developers had. That said, I am very optimistic that we could ‚Äúrepair the plane mid-flight‚Äù so to speak, and have the best of both worlds: backwards compatibility and speed.&lt;/p&gt;&lt;p&gt;I mentioned at the top of the post I would address ‚Äúrewrite it in Rust‚Äù, and I think Andrew‚Äôs own quote mostly does that for me. I think we could have 99% of the performance improvements while still maintaining a Ruby codebase. Of course if we rewrote it in Rust, you could squeeze an extra 1% out, but would it be worthwhile? I don‚Äôt think so.&lt;/p&gt;&lt;p&gt;I have a lot more to say about this topic, and I feel like this post is getting kind of long, so I‚Äôm going to end it here. Please look out for part 2, which I‚Äôm tentatively calling ‚ÄúWhat makes Bundler / RubyGems slow?‚Äù This post was very ‚Äúcan we make RubyGems / Bundler do what uv does?‚Äù (the answer is ‚Äúyes‚Äù). In part 2 I want to get more hands-on by discussing how to profile Bundler and RubyGems, what specifically makes them slow in the real world, and what we can do about it.&lt;/p&gt;&lt;p&gt;I want to end this post by saying ‚Äúthank you‚Äù to Andrew for writing such a great post about how uv got so fast.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458302</guid><pubDate>Thu, 01 Jan 2026 21:37:10 +0000</pubDate></item><item><title>WebAssembly as a Python Extension Platform</title><link>https://nullprogram.com/blog/2026/01/01/</link><description>&lt;doc fingerprint="8c499a15de87b2c0"&gt;
  &lt;main&gt;
    &lt;p&gt; nullprogram.com/blog/2026/01/01/ &lt;/p&gt;
    &lt;p&gt; (The author is currently open to employment opportunities in the United States.) &lt;/p&gt;
    &lt;p&gt;Software above some complexity level tends to sport an extension language, becoming a kind of software platform itself. Lua fills this role well, and of course there‚Äôs JavaScript for web technologies. WebAssembly generalizes this, and any Wasm-targeting programming language can extend a Wasm-hosting application. It has more friction than supplying a script in a text file, but extension authors can write in their language of choice, and use more polished development tools ‚Äî debugging, testing, etc. ‚Äî than typically available for a typical extension language. Python is traditionally extended through native code behind a C interface, but it‚Äôs recently become practical to extend Python with Wasm. That is we can ship an architecture-independent Wasm blob inside a Python library, and use it without requiring a native toolchain on the host system. Let‚Äôs discuss two different use cases and their pitfalls.&lt;/p&gt;
    &lt;p&gt;Normally we‚Äôd extend Python in order to access an external interface that Python cannot access on its own. Wasm runs in a sandbox with no access to the outside world whatsoever, so it obviously isn‚Äôt useful for that case. Extensions may also grant Python more speed, which is one of Wasm‚Äôs main selling points. We can also use Wasm to access embeddable capabilities written in a different programming language which do not require external access.&lt;/p&gt;
    &lt;p&gt;For preferred non-WASI Wasm runtime is Volodymyr Shymanskyy‚Äôs wasm3. It‚Äôs plain old C and very friendly to embedding in the same was as, say, SQLite. Performance is middling, though a C program running on wasm3 is still quite a bit faster than an equivalent Python program. It has Python bindings, pywasm3, but it‚Äôs distributed only in source code form. That is, the host machine must have a C toolchain in order to use pywasm3, which defeats my purposes here. If there‚Äôs a C toolchain, I might as well just use that instead of going through Wasm.&lt;/p&gt;
    &lt;p&gt;For the use cases in this article, the best option is wasmtime-py. The distribution includes binaries for Windows, macOS, and Linux on x86-64 and ARM64, which covers nearly all Python installations. Hosts require nothing more than a Python interpreter, no native toolchains. It‚Äôs almost as good as having Wasm built into Python itself. In my tests it‚Äôs 3x‚Äì10x faster than wasm3, so for my first use case the situation is even better. The catch is that it currently weighs ~18MiB (installed), and in the future will likely rival the Python interpreter itself. The API also breaks on a monthly basis, so you‚Äôre signing up for the upgrade treadmill lest your own program perishes to bitrot after a couple of years. This article is about version 40.&lt;/p&gt;
    &lt;head rend="h3"&gt;Usage examples and gotchas&lt;/head&gt;
    &lt;p&gt;The official examples don‚Äôt do anything non-trivial or interesting, and so to figure things out I had to study the documentation, which does not offer many hints. Basic setup looks like this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;import functools
import wasmtime

store    = wasmtime.Store()
module   = wasmtime.Module.from_file(store.engine, "example.wasm")
instance = wasmtime.Instance(store, module, ())
exports  = instance.exports(store)

memory = exports["memory"].get_buffer_ptr(store)
func1  = functools.partial(exports["func1"], store)
func2  = functools.partial(exports["func2"], store)
func3  = functools.partial(exports["func3"], store)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;A store is an allocation region from which we allocate all Wasm objects. It is not possible to free individual objects except to discard the whole store. Quite sensible, honestly. What‚Äôs not sensible is how often I have to repeat myself, passing the store back into every object in order to use it. These objects are associated with exactly one store and cannot be used with different stores. Use the wrong store and it panics: It‚Äôs already keeping track internally! I do not understand why the interface works this way. So to make things simpler, I use &lt;code&gt;functools.partial&lt;/code&gt; to
bind the &lt;code&gt;store&lt;/code&gt; parameter and so get the interface I expect.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;get_buffer_ptr&lt;/code&gt; object is a buffer protocol object, and if you‚Äôre
moving anything other than bytes that‚Äôs probably what you want to use to
access memory. The usual caveats apply for this object: If you change the
memory size you probably want to grab a fresh buffer object. For
bytes (e.g. buffers and strings) I prefer the &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt; methods.&lt;/p&gt;
    &lt;p&gt;Because multi-value is still in an experimental state in the Wasm ecosystem, you will likely not pass structs with Wasm. Anything more complicated than scalars will require pointers and copying data in and out of Wasm linear memory. This involves the usual trap that catches nearly everyone: Wasm interfaces make no distinction between pointers and integers, and Wasm runtimes interpret generally interpret all integers as signed. What that means is your pointers are signed unless you take action. Addresses start at 0, so this is bad, bad news.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;malloc = functools.partial(exports["func1"], store)

hello = b"hello"
pointer = malloc(len(hello))
assert pointer
memory = exports["memory"].write(store, hello, pointer)  # WRONG!
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To make matters worse, wasmtime-py adds its own footgun: The &lt;code&gt;read&lt;/code&gt; and
&lt;code&gt;write&lt;/code&gt; methods adopt the questionable Python convention of negative
indices acting from the end. If &lt;code&gt;malloc&lt;/code&gt; returns a pointer in the upper
half of memory, the negative pointer will pass the bounds check inside
&lt;code&gt;write&lt;/code&gt; because negative is valid, then quietly store to the wrong
address! Doh!&lt;/p&gt;
    &lt;p&gt;I wondered how common this error, so I searched online. I could find only one non-trivial wasmtime-py use in the wild, in a sandboxed PDF reader. It falls into the negative pointer trap as I expected. Not only that, it‚Äôs a buffer overflow into Python‚Äôs memory space:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;            buf_ptr = malloc(store, len(pdf_data))
            mem_data = memory.data_ptr(store)

            for i, byte in enumerate(pdf_data):
                mem_data[buf_ptr + i] = byte
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The &lt;code&gt;data_ptr&lt;/code&gt; method returns a non-bounds-checked raw &lt;code&gt;ctypes&lt;/code&gt; pointer,
so this is actually a double mistake. First, it shouldn‚Äôt trust pointers
coming out of Wasm if it cares at all about sandboxing. The second is the
potential negative pointer, which in this case would write outside of the
Wasm memory and in Python‚Äôs memory, hopefully seg-faulting.&lt;/p&gt;
    &lt;p&gt;What‚Äôs one to do? Every pointer coming out of Wasm must be truncated with a mask:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;pointer = malloc(...) &amp;amp; 0xffffffff   # correct for wasm32!
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;This interprets the result as unsigned. 64-bit Wasm needs a 64-bit mask, though in practice you will never get a valid negative pointer from 64-bit Wasm. This rule applies to JavaScript as well, where the idiom is:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;let pointer = malloc(...) &amp;gt;&amp;gt;&amp;gt; 0
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Wasm runtimes cannot help ‚Äî they lack the necessary information ‚Äî and this is perhaps a fundamental flaw in Wasm‚Äôs design. Once you know about it you see this mistake happening everywhere.&lt;/p&gt;
    &lt;p&gt;Now that you have a proper address, you can apply it to a buffer protocol view of memory. If you‚Äôre using NumPy there are various ways to interact with this memory by wrapping it in NumPy types, though only if you‚Äôre on a little endian host. (If you‚Äôre on a big endian machine, just give up on running Wasm anyway.) The first use case I have in mind typically involves copying plain Python values in and out. The &lt;code&gt;struct&lt;/code&gt; package is
quite handy here:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;vec2   = malloc(...) &amp;amp; 0xffffffff
memory = exports["memory"].get_buffer_ptr(store)
struct.pack_into("&amp;lt;ii", memory, vec2, x, y)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;It fills a similar role to JavaScript &lt;code&gt;DataView&lt;/code&gt;. If you‚Äôre copying
lots of numbers, with CPython it‚Äôs faster to construct a custom format
string rather than use a loop:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;nums: list[int] = ...
struct.pack_into(f"&amp;lt;{len(nums)}i", memory, buf, *nums)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To copy structures back out, use &lt;code&gt;struct.unpack_from&lt;/code&gt;. If you‚Äôre moving
strings, you‚Äôll need to &lt;code&gt;.encode()&lt;/code&gt; and &lt;code&gt;.decode()&lt;/code&gt; to convert to and from
&lt;code&gt;bytes&lt;/code&gt;, which are well-suited to &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In practice with real Wasm programs you‚Äôre going to be interacting with the ‚Äúguest‚Äù allocator from the outside, to request memory into which you copy inputs for a function. In my examples I‚Äôve used &lt;code&gt;malloc&lt;/code&gt; because it
requires no elaboration, but as usual a bump allocator solves
this so much better, especially because it doesn‚Äôt require stuffing a
whole general purpose allocator inside the Wasm program. Have one global
arena ‚Äî no other threads will sharing that Wasm instance ‚Äî rapid fire a
bunch of allocations as needed without any concern for memory management
in the ‚Äúhost‚Äù, call the function, which might allocate a result from that
arena, then reset the arena to clean up. In essence a stack for passing
values in and out.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly as faster Python&lt;/head&gt;
    &lt;p&gt;Suppose we noticed a computational hot spot in our Python program in a pure Python function (e.g. not calling out to an extension). Optimizing this function would be wise. Based on my experiments if I re-implement that function in C, compile it to Wasm, then run that bit of Wasm in place of the original function, I can expect around a 10x speed-up. In general C is more like 100x faster than Python, and the overhead of interfacing with Wasm ‚Äî copying stuff in and out, etc. ‚Äî can be high, but not so high as to not be profitable. This improves further if I can change the interface, e.g. require callers to use the buffer protocol.&lt;/p&gt;
    &lt;p&gt;Thanks to wasmtime-py, I could introduce this change without fussing with cross-compilers to build distribution binaries, nor require a toolchain on the target, just a hefty Python package. Might be worth it.&lt;/p&gt;
    &lt;p&gt;My main experimental benchmark is a variation on my solution to the ‚ÄúTwo Sum‚Äù problem, which I originally wrote for JavaScript, then extended to pywasm3 and later wasmtime-py. It‚Äôs simple, just interesting enough, and representative of the sort of Wasm drop-in I have in mind. It has the same interface, but implements it with Wasm.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;# Original Pythonic interface
def twosum(nums: list[int], target: int) -&amp;gt; tuple[int, int] | None:
    ...

# Stateful Wasm interface
class TwoSumWasm():
    def __init__(self):
        store    = wasmtime.Store()
        module   = wasmtime.Module.from_file(store.engine, ...)
        instance = wasmtime.Instance(store, module, ())
        ...

    def twosum(self, nums, target):
        # ... use wasm instance ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;There‚Äôs some state to it with the Wasm instance in tow. If you hide that by making it global you‚Äôll need to synchronize your threads around it. In a multi-threaded program perhaps these would be lazily-constructed thread locals. I haven‚Äôt had to solve this yet.&lt;/p&gt;
    &lt;p&gt;However, the weakness of the wasmtime ‚Äústore‚Äù really shows: Notice how compilation and instantiation are bound together in one store? I cannot compile once and then create disposable instances on the fly, e.g. as required for each run of a WASI program. Every instance permanently extends the compilation store. In practice we must wastefully re-compile the Wasm program for each disposable instance. Despite appearances, compilation and instantiation are not actually distinct steps, as they are in JavaScript‚Äôs Wasm API. &lt;code&gt;wasmtime.Instance&lt;/code&gt; accepts a store as its first
argument, suggesting use of a different store for instantiation. That
would solve this problem, but as of this writing it must be the same
store used to compile the module. This is a fatal flaw for certain real
use cases, particularly WASI.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly as embedded capabilities&lt;/head&gt;
    &lt;p&gt;Loup Vaillant‚Äôs Monocypher is a wonderful cryptography library. Lean, efficient, and embedding-friendly, so much so it‚Äôs distributed in amalgamated form. It requires no libc or runtime, so we can compile it straight to Wasm with almost any Clang toolchain:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ clang --target=wasm32 -nostdlib -O2 -Wl,--no-entry -Wl,--export-all
        -o monocypher.wasm monocypher.c
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;It‚Äôs not ‚ÄúWasm-aware‚Äù so I need &lt;code&gt;--export-all&lt;/code&gt; to expose the interface.
This is swell because, as single translation unit, anything with external
linkage is the interface. Though remember what I said about interacting
with the guest allocator? This has no allocator, nor should it. It‚Äôs not
so usable in this form because we‚Äôd need to manage memory from the
outside. Do-able, but it‚Äôs easy to improve by adding a couple more
functions, sticking to a single translation unit:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;#include "monocypher.c"

extern char  __heap_base[];
static char *heap_used;
static char *heap_high;

void *bump_alloc(ptrdiff_t size)
{
    // ...
}

void bump_reset()
{
    ptrdiff_t len = heap_used - __heap_base;
    __builtin_memset(__heap_base, 0, len);  // wipe keys, etc.
    heap_used = __heap_base;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;I‚Äôve discussed &lt;code&gt;__heap_base&lt;/code&gt; before, which is part of the ABI.
We‚Äôll push keys, inputs, etc. onto this ‚Äústack‚Äù, run our cryptography
routine, copy out the result, then reset the bump allocator, which wipes
out all sensitive data. Often &lt;code&gt;memset&lt;/code&gt; is insufficient ‚Äî typically it‚Äôs
zero-then-free, and compilers see the lifetime about to end ‚Äî but no
lifetime ends here, and stores to this ‚Äúheap‚Äù memory externally observable
as far as the abstract machine can tell. (Otherwise we couldn‚Äôt reliably
copy out our results!)&lt;/p&gt;
    &lt;p&gt;There‚Äôs a lot to this API, but I‚Äôm only going to look at the AEAD interface. We ‚Äúlock‚Äù up some data in an encrypted box, write any unencrypted label we‚Äôd like on the outside. Then later we can unlock the box, which will only open for us if neither the contents of the box nor the label were tampered with. That‚Äôs some solid API design:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;void crypto_aead_lock(uint8_t       *cipher_text,
                      uint8_t        mac  [16],
                      const uint8_t  key  [32],
                      const uint8_t  nonce[24],
                      const uint8_t *ad,         size_t ad_size,
                      const uint8_t *plain_text, size_t text_size);
int crypto_aead_unlock(uint8_t       *plain_text,
                       const uint8_t  mac  [16],
                       const uint8_t  key  [32],
                       const uint8_t  nonce[24],
                       const uint8_t *ad,          size_t ad_size,
                       const uint8_t *cipher_text, size_t text_size);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;By compiling to Wasm we can access this functionality from Python almost like it was pure Python, and interact with other systems using Monocypher.&lt;/p&gt;
    &lt;p&gt;Since Monocypher does not interact with the outside world on its own, it relies on callers to use their system‚Äôs CSPRNG to create those nonces and keys, which we‚Äôll do using the &lt;code&gt;secrets&lt;/code&gt; built-in package:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;class Monocypher:
    def __init__(self):
        ...
        self._read   = functools.partial(memory.read, store)
        self._write  = functools.partial(memory.write, store)
        self.__alloc = functools.partial(exports["bump_alloc"], store)
        self._reset  = functools.partial(exports["bump_reset"], store)
        self._lock   = functools.partial(exports["crypto_aead_lock"], store)
        self._unlock = functools.partial(exports["crypto_aead_unlock"], store)
        self._csprng = secrets.SystemRandom()

    def _alloc(self, n):
        return self.__alloc(n) &amp;amp; 0xffffffff

    def generate_key(self):
        return self._csprng.randbytes(32)

    def generate_nonce(self):
        return self._csprng.randbytes(24)

    ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;With a solid foundation, all that follows comes easily. A &lt;code&gt;finally&lt;/code&gt;
guarantees secrets are always removed from Wasm memory, and the rest is
just about copying bytes around:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    def aead_lock(self, text, key, ad = b""):
        assert len(key) == 32
        try:
            macptr   = self._alloc(16)
            keyptr   = self._alloc(32)
            nonceptr = self._alloc(24)
            adptr    = self._alloc(len(ad))
            textptr  = self._alloc(len(text))

            self._write(key, keyptr)
            nonce = self.generate_nonce()
            self._write(nonce, nonceptr)
            self._write(ad,    adptr)
            self._write(text,  textptr)

            self._lock(
                textptr,
                macptr,
                keyptr,
                nonceptr,
                adptr, len(ad),
                textptr, len(text),
            )
            return (
                self._read(macptr, macptr+16),
                nonce,
                self._read(textptr, textptr+len(text)),
            )
        finally:
            self._reset()
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;And &lt;code&gt;aead_unlock&lt;/code&gt; is basically the same in reverse, but throws if the box
fails to unlock, perhaps due to tampering:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    def aead_unlock(self, text, mac, key, nonce, ad = b""):
        assert len(mac) == 16
        assert len(key) == 32
        assert len(nonce) == 24
        try:
            macptr   = self._alloc(16)
            keyptr   = self._alloc(32)
            nonceptr = self._alloc(24)
            adptr    = self._alloc(len(ad))
            textptr  = self._alloc(len(text))

            self._write(mac, macptr)
            self._write(key, keyptr)
            self._write(nonce, nonceptr)
            self._write(ad, adptr)
            self._write(text, textptr)

            if self._unlock(
                textptr,
                macptr,
                keyptr,
                nonceptr,
                adptr, len(ad),
                textptr, len(text),
            ):
                raise ValueError("AEAD mismatch")
            return self._read(textptr, textptr+len(text))
        finally:
            self._reset()
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;mc = Monocypher()
key = mc.generate_key()
message = "Hello, world!"
mac, nonce, encrypted = mc.aead_lock(message.encode(), key)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Transmit &lt;code&gt;mac&lt;/code&gt;, &lt;code&gt;nonce&lt;/code&gt;, and &lt;code&gt;encrypted&lt;/code&gt; to the other party (or your
future self), who already has the &lt;code&gt;key&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;decrypted = mc.aead_unlock(encrypted, mac, key, nonce)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Find the complete source in my scratch repository.&lt;/p&gt;
    &lt;p&gt;While I have a few reservations about wasmtime-py, it fascinates me how well this all works. It‚Äôs been my hammer in search of a nail for some time now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458624</guid><pubDate>Thu, 01 Jan 2026 22:09:16 +0000</pubDate></item><item><title>Why users cannot create Issues directly</title><link>https://github.com/ghostty-org/ghostty/issues/3558</link><description>&lt;doc fingerprint="d3525efe9c762c38"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 1.4k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Users are not allowed to create Issues directly in this repository - we ask that you create a Discussion first.&lt;/p&gt;
    &lt;p&gt;Unlike some other projects, Ghostty does not use the issue tracker for discussion or feature requests. Instead, we use GitHub discussions for that. Once a discussion reaches a point where a well-understood, actionable item is identified, it is moved to the issue tracker. This pattern makes it easier for maintainers or contributors to find issues to work on since every issue is ready to be worked on.&lt;/p&gt;
    &lt;p&gt;This approach is based on years of experience maintaining open source projects and observing that 80-90% of what users think are bugs are either misunderstandings, environmental problems, or configuration errors by the users themselves. For what's left, the majority are often feature requests (unimplemented features) and not bugs (malfunctioning features). Of the features requests, almost all are underspecified and require more guidance by a maintainer to be worked on.&lt;/p&gt;
    &lt;p&gt;Any Discussion which clearly identifies a problem in Ghostty and can be confirmed or reproduced will be converted to an Issue by a maintainer, so as a user finding a valid problem you don't do any extra work anyway. Thank you.&lt;/p&gt;
    &lt;p&gt;For more details, see our CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460319</guid><pubDate>Fri, 02 Jan 2026 01:24:51 +0000</pubDate></item><item><title>Extensibility: The "100% Lisp" Fallacy</title><link>https://kyo.iroiro.party/en/posts/100-percent-lisp/</link><description>&lt;doc fingerprint="8622d95d221f3adc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Extensibility: The √¢100% Lisp√¢ Fallacy&lt;/head&gt;
    &lt;p&gt;So, I√¢ve seen some articles promoting Emacs-like editors written in Lisp languages, and one of the most common arguments seems to be: √¢it√¢s written in This Lisp and also scriptable in This Lisp, and that gives it great extensibility.√¢ 1&lt;/p&gt;
    &lt;p&gt;It√¢s not wrong, but I think it does overlook a few things.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By the way: Happy New Year!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;1. The argument√Ç¬∂&lt;/head&gt;
    &lt;p&gt;For example, the Lem: An Alternative To Emacs? article from Irreal claims: (emphasis preserved)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;One thing I like about it is that it√¢s 100% Common Lisp. There√¢s no C core; just Lisp all the way down. That makes it easier to customize or extend any part of the editor.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt; This argument sounds good: Looking at the repository, Lem is &lt;del&gt;100%&lt;/del&gt; 90% Lisp code; since the editor code and user customization live in the Common Lisp runtime, we should be able to extend any part of the editor on the fly, right? &lt;/p&gt;
    &lt;p&gt;Or, does it really?&lt;/p&gt;
    &lt;p&gt; Does it offer &lt;code&gt;composition-function-table&lt;/code&gt; so that you can program your font ligatures from the editor√¢s scripting language? &lt;/p&gt;
    &lt;p&gt;Does it provide an API to define an arbitrary encoding system and the corresponding charset, beyond what is supported by Unicode or any existing standard?&lt;/p&gt;
    &lt;p&gt;Does it allow you to √¢override√¢ its newline character, so that a file is displayed all on a single line?&lt;/p&gt;
    &lt;p&gt;√¢¬¶&lt;/p&gt;
    &lt;p&gt;These examples are taken from some of the more obscure features in Emacs, and I can go on and on. I don√¢t think many of the editors out there could possibly support them, as they are probably the 10% non-pure part of a √¢100% Lisp√¢ system.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To be honest, I hate these features as they√¢ve haunted me forever since I started designing an IPC protocol for my Emacs clone. But, dang it, I suffered and suffer from it exactly because of Emacs√¢s extensibility √¢ not the lack of it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;2. There is no √¢100% Lisp√¢√Ç¬∂&lt;/head&gt;
    &lt;p&gt;For example, Steel Bank Common Lisp, a common Common Lisp runtime, is only mostly written in Lisp because it has to provide threading primitives, interface with the OS, or leverage assembly code. And obviously you won√¢t be able to customize those bits.&lt;/p&gt;
    &lt;p&gt;Going back to (graphical) editors this is no less true. Usually2, as is with any GUI program, you will want to support font fallback, input methods and screen readers, all of which require interacting with platform specific APIs and are thus much less customizable. FFI helps to some degree by keeping your niche √¢pure√¢, but it can√¢t extend your customizability beyond that boundary: webviews don√¢t expose font ligature internals, and CSS is the only way to control that, albeit quite limited; input methods are trickier, as they interfere with keyboard events and are platform-specific; screen readers are √¢¬¶ I don√¢t know, you should really use a library for it if you want portability.&lt;/p&gt;
    &lt;p&gt;Anyway, it√¢s just impossible these days to have a √¢pure√¢ Lisp program with all those platform specific things to deal with, or with all those convenient library bindings to use. It√¢s not inherently bad, but it certainly limits how you√¢re allowed to extend things.&lt;/p&gt;
    &lt;p&gt;However, even with the non-pure parts, provided with suitable building blocks, it√¢s usually quite surprising to what degree people can work around all those √¢non-extensible√¢ parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Workaround-ish extensibility√Ç¬∂&lt;/head&gt;
    &lt;p&gt;Let√¢s first have a look at some approaches people adopt to extend their editors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Neovim and many TUI editors don√¢t have native scrollbar support since they are bounded by what ANSI provides. But guess what? By coloring the rightmost column with &lt;code&gt;extmark&lt;/code&gt;and&lt;code&gt;virt_text&lt;/code&gt;, it√¢s totally doable to display a pretty scrollbar for Neovim.&lt;/item&gt;
      &lt;item&gt;Stock Emacs does not support cursor animations. And yet, you√¢ve guessed it, people: &lt;list rend="ul"&gt;&lt;item&gt;Come up with patches for this,&lt;/item&gt;&lt;item&gt;Or extend Lisp code with Python, which in turn calls PyQt or compositor commands to control overlayed windows, so as to display things like moving cursors.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Similarly, Emacs Application Framework allows you to write GUI programs for Emacs by √¢¬¶ first programming them in PyQt and then sticking them onto the Emacs window, so that the PyQt window √¢fills√¢ the corresponding buffer window.&lt;/p&gt;
        &lt;p&gt;Not all Wayland compositors provide a way to programmatically position user windows, and that can be a problem for EAF.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;And, have you heard of EXWM (Emacs X Window Manager)?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The moral here is that a lot of things are more extensible than one might think. It√¢s just amazing how people keep coming up with all kinds of workarounds all the time. And yes, I do think those are all extensibility, regardless of √¢purity√¢.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. √¢Spacebar heating√¢ extensibility√Ç¬∂&lt;/head&gt;
    &lt;p&gt;Compared to extending via workarounds, extending in √¢pure Lisp√¢ can be both easier and harder, as we are still bounded by coding conventions and existing code, and one cannot possibly extend everything without breaking some of them.&lt;/p&gt;
    &lt;p&gt; Let√¢s start by overriding a single function. For example, when exporting Org-mode files to HTML, Org-mode defaults to generating random HTML ID anchors. To change that, you just override the &lt;code&gt;org-export-get-reference&lt;/code&gt; function that generates the IDs, right? &lt;/p&gt;
    &lt;code&gt;(advice-add #'org-export-get-reference :around #'org-html-stable-ids--get-reference)
&lt;/code&gt;
    &lt;p&gt; Oh no! It turns out that, sometimes Org-mode directly calls &lt;code&gt;org-html--reference&lt;/code&gt;, bypassing our override. That means we also need to redirect &lt;code&gt;org-html--reference&lt;/code&gt;: &lt;/p&gt;
    &lt;code&gt;(advice-add #'org-html--reference :override #'org-html-stable-ids--reference)
&lt;/code&gt;
    &lt;p&gt; Problem solved? No. Conventionally, Emacs Lisp code uses double dashes to tell the users √¢this function is internal√¢, as is in the &lt;code&gt;org-html--reference&lt;/code&gt; name. Yes, by being free to extend any part of the editor, you are free to modify any internal functions or states, in a way that may or may not be problematic under specific circumstances, with code that can be broken in any future updates. &lt;/p&gt;
    &lt;p&gt;And it√¢s not the end of it. The el-patch package allows you to apply √¢patches√¢ on most any Lisp code to modify behaviours nested deep inside a function:&lt;/p&gt;
    &lt;code&gt;;; Original function
(defun company-statistics--load ()
  "Restore statistics."
  (load company-statistics-file 'noerror nil 'nosuffix))

;; Patching
(el-patch-feature company-statistics)
(with-eval-after-load 'company-statistics
  (el-patch-defun company-statistics--load ()
                  "Restore statistics."
                  (load company-statistics-file 'noerror
                        ;; The patch
                        (el-patch-swap nil 'nomessage)
                        'nosuffix)))

;; Patched version
(defun company-statistics--load ()
  "Restore statistics."
  (load company-statistics-file 'noerror 'nomessage 'nosuffix))
&lt;/code&gt;
    &lt;p&gt; Luckily, el-patch provides &lt;code&gt;el-patch-validate&lt;/code&gt; so that you can worry less about your patches going ineffective or unexpectedly destructive. But you still need to maintain all your patches if anything goes wrong. &lt;/p&gt;
    &lt;p&gt;Any extensible system is not void of these problems. If you impose strong enough encapsulation, then eventually something can√¢t get customized; if you expose everything, well, good luck keeping backward compatibility (as the system maintainer) or forward compatibility (as the user doing your modifications). By making it possible to √¢extend any part of the editor,√¢ you are literally making any part of your code unextensible, and now √¢every change breaks someone√¢s workflow.√¢&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Emacs√¢s cross-language isolation/API might not be perfect, but I√¢m very grateful for it. If Emacs were written in pure Lisp code and anything is extensible, my work-in-progress Emacs clone couldn√¢t be remotely possible (because we do want to rid some of the spacebar heating problems while keeping some compatibility).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;5. Extensibility takes efforts√Ç¬∂&lt;/head&gt;
    &lt;p&gt;Allow me to be blunt: the √¢100% Lisp√¢ argument is lazy marketing. Writing a Lisp-extended editor in Lisp won√¢t immediately make your editor more extensible. Extensibility comes from careful designing of your API interfaces, it comes from learning from your history, listening to user needs, and, after all these, taking the effort and time actually write the interfacing code.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;This blog entry was adapted from a rant thread I posted to vent my shock about Emacs√¢s&lt;/p&gt;&lt;code&gt;composition-function-table&lt;/code&gt;. Emacs is just never short of wonders and surprises, especially when you√¢re creating your own Emacsen by replicating Emacs.&lt;p&gt;By the way, this post is not against Lem, from which I√¢ve got a lot of inspirations and seen quite some good designs. But, it√¢s still the most convenient example I can think of. So, pardon me!&lt;/p&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;6. Footnotes√Ç¬∂&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;√¢I want to write a better editor.√¢&lt;/cell&gt;
        &lt;cell&gt;√∞ With better accessibility, right?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;(silent gaze)&lt;/cell&gt;
        &lt;cell&gt;√∞ With better accessibility, right?&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Also, please don√¢t mistake maintainability for extensibility. For example, Racket has migrated from its previous C core to a Racket core powered by Chez Scheme. Quoting from Rebuilding Racket on Chez Scheme Experience Report:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;√¢¬¶ it√¢s hard to put a number on these things but I can give you one number at least. This is the number of people who have been willing to modify the Racket macro expander.&lt;/p&gt;
      &lt;p&gt;This is when it was in C: two of us did it. And it√¢s already six people (after the C-to-Chez migration). Remember: the C part that was in the first 16 years and the last 16 years of that implementation, and the six people and the new implementation is just in two years. So we√¢re really pretty sure it√¢s gonna be better to maintain.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So I do believe a mostly Lispy codebase can be better maintained and potentially attract more contributors than one in C. (But Emacs is mostly Lispy anyway.)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460394</guid><pubDate>Fri, 02 Jan 2026 01:36:25 +0000</pubDate></item><item><title>Happy Public Domain Day 2026</title><link>https://publicdomainreview.org/blog/2026/01/public-domain-day-2026/</link><description>&lt;doc fingerprint="3081d05f80b58200"&gt;
  &lt;main&gt;
    &lt;p&gt;The calendar turns, and once again a lively procession of books, images, films, and music leaves copyright behind and steps into the ever-growing public domain! On this year's Public Domain Day (which falls each January 1st) we welcome, in lots of countries around the world, the words of Wallace Stevens, Thomas Mann, Hannah Arendt, and Albert Einstein, and in the US a bevy of brilliant books including William Faulkner‚Äôs As I Lay Dying, Langston Hughes‚Äô Not Without Laughter, Agatha Christie‚Äôs The Murder at the Vicarage, and, in their original German, Robert Musil‚Äôs The Man Without Qualities and Hermann Hesse‚Äôs Narcissus and Goldmund.&lt;/p&gt;
    &lt;p&gt;Due to differing copyright laws around the world, there is no one single public domain, but there are three main types of copyright term for historical works which cover most cases. For these three systems, newly entering the public domain today are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;works by people who died in 1955, for countries with a copyright term of ‚Äúlife plus 70 years‚Äù (relevant in UK, most of the EU, and South America);&lt;/item&gt;
      &lt;item&gt;works by people who died in 1975, for countries with a term of ‚Äúlife plus 50 years‚Äù (relevant to most of Africa and Asia);&lt;/item&gt;
      &lt;item&gt;films and books (incl. artworks featured) published in 1929 (relevant solely to the United States).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of you may have been following our advent-style countdown calendar which revealed day-by-day through December our highlights for these new public domain entrants. The last window was opened yesterday, and while such a format was fun for the slow reveal, for the sake of a good gorgeable list we‚Äôve exploded the calendar out into a digestible array below. Enjoy!&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in the US&lt;/head&gt;
    &lt;head rend="h3"&gt;William Faulkner ‚Äì As I Lay Dying&lt;/head&gt;
    &lt;p&gt;As I Lay Dying is a Southern Gothic novel by American author William Faulkner, consistently ranked among the best novels of the 20th century. The title is derived from William Marris‚Äôs 1925 translation of Homer‚Äôs Odyssey, referring to the similar themes of both works.&lt;lb/&gt;The novel traces the story of the death of Addie Bundren and her poor, rural family‚Äôs quest to honor her wish to be buried in her hometown of Jefferson, Mississippi, as well as the motives‚Äînoble or selfish‚Äîthey show on the journey. It uses a stream-of-consciousness writing technique and varying chapter lengths, and is narrated by 15 different characters over 59 chapters.&lt;lb/&gt;Faulkner said that he wrote the novel from midnight to 4:00 a.m. over the course of six weeks and that he did not change a word of it. He spent the first eight hours of his twelve-hour shift at the University of Mississippi Power House shoveling coal or directing other works and the remaining four hours handwriting his manuscript on unlined onionskin paper. As I Lay Dying represents a progenitor of the Southern Renaissance, reflecting on being, existence, and other existential metaphysics of everyday life, and helped to solidify Faulkner‚Äôs reputation as a pioneer, like James Joyce and Virginia Woolf, of stream of consciousness. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Arthur Ransome - Swallows and Amazons&lt;/head&gt;
    &lt;p&gt;Swallows and Amazons is a children‚Äôs adventure novel by English author Arthur Ransome. It is the first book in the Swallows and Amazons series, followed by Swallowdale.&lt;lb/&gt;Set in the summer of 1929 in England‚Äôs Lake District, the book relates the outdoor adventures and play of two families of children. These involve sailing, camping, fishing, exploration and piracy. The Walker children (John, Susan, Titty and Roger) are staying at a farm near a lake in the Lake District of England, during the school holidays. They sail a borrowed dinghy named Swallow and meet the Blackett children (Nancy and Peggy), who sail a dinghy named Amazon. When the children meet, they agree to join forces against a common enemy ‚Äì the Blacketts‚Äô uncle Jim Turner whom they call ‚ÄúCaptain Flint‚Äù (after the parrot in Treasure Island).&lt;lb/&gt;The book was inspired by a summer spent by Ransome teaching the children of his friends, the Altounyans, to sail. At the time, Ransome had been working as a journalist with the Manchester Guardian, but decided to become a full-time author rather than go abroad as a foreign correspondent. Three of the Altounyan children‚Äôs names are adopted directly for the Walker family. However, later in life Ransome tried to downplay the Altounyan connections, changing the initial dedication of Swallows and Amazons and writing a new foreword which gave other sources. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Nan Shepherd ‚Äì The Weatherhouse&lt;/head&gt;
    &lt;p&gt;The Weatherhouse is the second novel by Anna ‚ÄúNan‚Äù Shepherd, a Scottish modernist writer and poet. The novel concerns interactions between people in a small rural Scottish community. It belongs to the great line of Scottish fiction dealing with the complex interactions of small communities, and especially the community of women ‚Äî a touching and hilarious network of mothers, daughters, spinsters and widows. It is also a striking meditation on the nature of truth, the power of human longing and the mystery of being.&lt;lb/&gt;Shepherd published three works of fiction. Her short non-fiction book The Living Mountain, inspired by her love for hillwalking, is the book for which she is best known and has been quoted as an influence by prominent nature writers. The landscape and weather of this area play a major role in her novels and provide a focus for her poetry.&lt;lb/&gt;Shepherd‚Äôs fiction brings out the sharp conflict between the demands of tradition and the pull of modernity, particularly in women‚Äôs lives. All three novels assign a major role to the landscape and weather in small northern Scottish communities they describe. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Langston Hughes ‚Äì Not Without Laughter&lt;/head&gt;
    &lt;p&gt;Not Without Laughter* is the debut novel of Langston Hughes, the American writer, activist, and leader of the Harlem Renaissance.&lt;lb/&gt;The novel portrays African-American life in Kansas in the 1910s, focusing on the effects of class and religion on the community. In telling the story of Sandy Rogers, a young African American boy in small-town Kansas, and of his family‚Äîhis mother, Annjee, a housekeeper for a wealthy white family; his irresponsible father, Jimboy, who plays the guitar and travels the country in search of employment; his strong-willed grandmother Hager, who clings to her faith; his Aunt Tempy, who marries a rich man; and his Aunt Harriet, who struggles to make it as a blues singer‚ÄîHughes gives the longings and lineaments of Black life in the early twentieth century an important place in the history of racially divided America.&lt;lb/&gt;Hughes said that *Not Without Laughter* is semi-autobiographical, and that a good portion of the characters and setting included in the novel are based on his memories of growing up in Lawrence, Kansas. A review in *The New York Times* said that the novel is ‚Äúvery slow, even tedious, reading in its early chapters, but once it gains its momentum it moves as swiftly as a jazz rhythm‚Äù. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Hermann Hesse ‚Äì Narcissus and Goldmund&lt;/head&gt;
    &lt;p&gt;Narcissus and Goldmund (in German, Narzi√ü und Goldmund), also published in English as Death and the Lover, is a novel written by the German-Swiss author Hermann Hesse. At its publication, it was considered Hesse‚Äôs literary triumph.&lt;lb/&gt;The novel is the story of a young man, Goldmund (German for ‚ÄúGold mouth‚Äù), who wanders aimlessly throughout Medieval Germany after leaving a Catholic monastery school in search of what could be described as ‚Äúthe meaning of life‚Äù. With the help of Narcissus, a gifted young teacher, and following an epiphanic experience with a beautiful Gypsy woman, Goldmund leaves the monastery and embarks on a wandering existence. He has numerous love affairs, studies art, and encounters human existence at its ugliest when the Black Death devastates the region. Eventually, he is reunited with his friend Narcissus, now an abbot.&lt;lb/&gt;Like most of Hesse‚Äôs works, the main theme of this book is the wanderer‚Äôs struggle to find himself, as well as the Jungian union of polar opposites (Mysterium Coniunctionis). Goldmund represents nature and the ‚Äúfeminine conscious mind‚Äù (but also anima, a man‚Äôs unconscious), while Narcissus represents science and logic and God and the ‚Äúmasculine conscious mind‚Äù (but also animus, a woman‚Äôs unconscious).&lt;lb/&gt;A film adaptation, directed by the Austrian Oscar-winning director Stefan Ruzowitzky, was released in 2020. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;All Quiet on the Western Front (1930 film)&lt;/head&gt;
    &lt;p&gt;All Quiet on the Western Front is a 1930 American pre-Code epic anti-war film based on the 1929 novel of the same name by German novelist Erich Maria Remarque. Directed by Lewis Milestone, it stars Lew Ayres, Louis Wolheim, John Wray, Slim Summerville, and William Bakewell.&lt;lb/&gt;The movie follows a group of German students moved to enlist in the army as part of the new 2nd Company. Their romantic delusions are quickly shattered during their brief but rigorous training under the abusive Sergeant Himmelstoss. After being sent to the Western Front, their idealism is destroyed by the harsh realities of combat.&lt;lb/&gt;Considered a realistic and harrowing account of warfare in World War I, the film opened to wide acclaim in the United States and made the American Film Institute‚Äôs first 100 Years... 100 Movies list in 1997. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page&lt;/p&gt;
    &lt;head rend="h3"&gt;Evelyn Waugh ‚Äì Vile Bodies&lt;/head&gt;
    &lt;p&gt;Vile Bodies is the second novel by Arthur Evelyn St. John Waugh, an English writer of novels, biographies, and travel books, and a prolific journalist and book reviewer. It satirises London‚Äôs post‚ÄìFirst World War ‚Äúbright young things‚Äù ‚Äî a group of Bohemian young aristocrats and socialites in London ‚Äî and the press coverage around them. Waugh originally considered the title Bright Young Things but changed it; the published title echoes a narrator‚Äôs remark on crowds and parties: ‚ÄúThose vile bodies‚Äù.&lt;lb/&gt;The novel follows a vivid assortment of characters, among them the struggling writer Adam Fenwick-Symes and the glamorous, aristocratic Nina Blount, who hunt fast and furiously for ever greater sensations and the hedonistic fulfillment of their desires. Waugh‚Äôs acidly funny satire reveals the darkness and vulnerability beneath the sparkling surface of the high life.&lt;lb/&gt;The book shifts in tone from light-hearted romp to bleak desolation (Waugh himself later attributed it to the breakdown of his first marriage halfway through the book‚Äôs composition). Critics have noted the novel‚Äôs fragmented scenes, jump-cuts, and telephone dialogue, often linking its method to cinema and to modernist effects. Some have defended the novel‚Äôs downbeat ending as a poetically just reversal of the conventions of comic romance.&lt;lb/&gt;David Bowie cited the novel as the primary influence in writing his song ‚ÄúAladdin Sane‚Äù, and a film adaptation, written and directed by Stephen Fry, was released in 2003. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Margaret Ayer Barnes - Years of Grace&lt;/head&gt;
    &lt;p&gt;Years of Grace is the first book by the American playwright, novelist, and short-story writer Margaret Ayer Barnes. It won the Pulitzer Prize for the Novel in 1931.&lt;lb/&gt;The story, beginning in the 1890s and continuing into the 1930s, chronicles the life of Jane Ward Carver from her teens to age 54. This novel follows many of the same themes as Barnes‚Äôs other works. Centering on the social manners of upper middle class society, her female protagonists are often traditionalists, struggling to uphold conventional morality in the face of changing social climates. Barnes‚Äôs alma mater Bryn Mawr College, along with the characters of college presidents M. Carey Thomas and Marion Park, figure prominently in this work.&lt;lb/&gt;The New York Times commented that ‚Äúthis story of the death of an old order and the birth of a new one, of the perpetually renewed conflict between succeeding generations... holds the reader‚Äôs attention to the end.‚Äù Despite the success of Years of Grace, it is not Barnes‚Äôs best-known work; that honor belongs to Dishonored Lady, a play she co-wrote with Edward Sheldon, which was adapted twice into film. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Hellbound Train&lt;/head&gt;
    &lt;p&gt;Hell-Bound Train is a 1930 film written and directed by James and Eloyce Gist. A self-taught husband-and-wife team with a shared religious mission, they produced at least three silent films for African American church audiences, touring them across the United States. Shown alongside sermons, these works used cinema as a vehicle for evangelism. In Hell-Bound Train ‚Äî which Eloyce is said to have rewritten, re-edited, and partly refilmed after James‚Äôs initial version ‚Äî the viewer passes from carriage to carriage as the filmmakers stage various ‚ÄúJazz Age‚Äù sins, including dancing, drinking, and gambling, all overseen by a mischievous devil conductor. Though Hell-Bound Train has gained some renewed attention via Kino Lorber‚Äôs Pioneers of African-American Cinema box set and a brief run on the Criterion Channel, this film ‚Äî one of the few surviving silent works by an African American woman ‚Äî is still often absent from retrospectives on early women filmmakers, perhaps because of its modest production values and overtly moralizing tone. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Watch on YouTube&lt;/p&gt;
    &lt;head rend="h3"&gt;Robert Musil ‚Äì The Man Without Qualities&lt;/head&gt;
    &lt;p&gt;The Man Without Qualities (in German Der Mann ohne Eigenschaften) is an unfinished modernist novel in three volumes and various drafts, by the Austrian writer Robert Musil, published in parts from 1930 to 1943.&lt;lb/&gt;The novel is a ‚Äústory of ideas‚Äù, which takes place in the time of the Austro-Hungarian monarchy‚Äôs last days. The plot often veers into allegorical digressions on a wide range of existential themes concerning humanity and feelings. It has a particular concern with the values of truth and opinion and how society organizes ideas about life and society. The book is well over a thousand pages long in its entirety, and no one single theme dominates.&lt;lb/&gt;The story takes place in 1913 in Vienna, the capital of Austria-Hungary, which Musil refers to by the playful term Kakanien. Part I, titled A Sort of Introduction, is an introduction to the protagonist, a mathematician named Ulrich whose ambivalence towards morals and indifference to life make him ‚Äúa man without qualities‚Äù. In Part II, Pseudoreality Prevails, Ulrich joins preparations for a celebration in honor of 70 years of the Austrian Emperor Franz Joseph‚Äôs reign. Part III, entitled Into the Millennium (The Criminals), is about Ulrich‚Äôs sister Agathe. They experience a mystically incestuous stirring upon meeting after their father‚Äôs death.&lt;lb/&gt;Musil worked on the novel for more than twenty years: his detailed portrait of a decaying fin de si√®cle world has strong autobiographical features. Musil‚Äôs almost daily preoccupation with writing left his family in dire financial straits. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read German original on Project Gutenberg&lt;/p&gt;
    &lt;head rend="h3"&gt;T. S. Eliot ‚Äì Ash Wednesday&lt;/head&gt;
    &lt;p&gt;Ash Wednesday is a long poem written by T. S. Eliot during his 1927 conversion to Anglicanism. Published in 1930, the poem deals with the struggle that ensues when one who has lacked faith in the past strives to move towards God.&lt;lb/&gt;Sometimes referred to as Eliot‚Äôs ‚Äúconversion poem‚Äù, Ash Wednesday, with a base of Dante‚Äôs Purgatorio, is richly but ambiguously allusive and deals with the move from spiritual barrenness to hope for human salvation. The style is different from his poetry which predates his conversion. Ash Wednesday and the poems that followed had a more casual, melodic, and contemplative method.&lt;lb/&gt;The poem‚Äôs title comes from the Western Christian fast day that marks the beginning of Lent, forty days before Easter. It is a poem about the difficulty of religious belief, and concerned with personal salvation in an age of uncertainty. In it, Eliot‚Äôs poetic persona, one who has lacked faith in the past, has somehow found the courage, through spiritual exhaustion, to seek faith.&lt;lb/&gt;The initial reception of Ash Wednesday was largely positive, though many of the more secular literati found its groundwork of orthodox Christianity discomfiting. Edwin Muir maintained that ‚Äú‚ÄòAsh Wednesday‚Äô is one of the most moving poems he [Eliot] has written, and perhaps the most perfect‚Äù. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on English Verse and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Agatha Christie - The Murder at the Vicarage&lt;/head&gt;
    &lt;p&gt;The Murder at the Vicarage is a work of detective fiction by the British writer Agatha Christie. It is the first novel to feature the character of Miss Marple and her village of St Mary Mead (characters that had previously appeared in short stories).&lt;lb/&gt;The story is set in the quiet English village of St Mary Mead, where life is seemingly peaceful until Colonel Protheroe, the local magistrate and a widely disliked man, is found shot dead in the vicar‚Äôs study. The vicar, Leonard Clement, is the narrator of the story. Just before the murder, he had remarked that ‚Äúanyone who murdered Colonel Protheroe would be doing the world a service‚Äù ‚Äî a comment that comes back to haunt him.&lt;lb/&gt;Several suspects quickly emerge, as well as Miss Marple, who proves, though she appears at first as a nosy old spinster, to have unmatched observational skills and a deep understanding of human nature. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Franz Kafka - The Castle (english translation)&lt;/head&gt;
    &lt;p&gt;The Castle (in German, *Das Schloss*) is a 1926 novel by Franz Kafka. In it a protagonist known only as ‚ÄúK.‚Äù arrives in a village and struggles to gain access to the mysterious authorities who govern it from a castle supposedly owned by Count Westwest. Kafka died before he could finish the work, but suggested it would end with K. dying in the village, the castle notifying him on his death bed that his ‚Äúlegal claim to live in the village was not valid, yet, taking certain auxiliary circumstances into account, he was permitted to live and work there.‚Äù Dark and at times surreal, *The Castle* is often understood to be about alienation, unresponsive bureaucracy, the frustration of trying to conduct business with non-transparent, seemingly arbitrary controlling systems, and the futile pursuit of an unobtainable goal. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Sigmund Freud ‚Äì Civilization and Its Discontents&lt;/head&gt;
    &lt;p&gt;Civilization and Its Discontents is a book by Sigmund Freud, the founder of psychoanalysis. It was written in 1929 and first published in German in 1930 as Das Unbehagen in der Kultur (‚ÄúThe Uneasiness in Civilization‚Äù).&lt;lb/&gt;Exploring what Freud saw as a clash between the desire for individuality and the expectations of society, the book is considered one of Freud‚Äôs most important and widely read works, and was described in 1989 by historian Peter Gay as one of the most influential and studied books in the field of modern psychology.&lt;lb/&gt;The book espouses a theory grounded in the notion that humans have certain characteristic instincts that are immutable. The primary tension originates from an individual attempting to find instinctive freedom, and civilization‚Äôs contrary demand for conformity and repression of instincts. Freud states that when any situation that is desired by the pleasure principle is prolonged, it creates a feeling of mild resentment as it clashes with the reality principle.&lt;lb/&gt;Primitive instincts‚Äîfor example, the desire to kill and the insatiable craving for sexual gratification‚Äîare harmful to the collective wellbeing of a human community. The historical development of laws that prohibit violence, murder, rape, and adultery, he argued, is an inherent quality of civilization that gives rise to perpetual feelings of discontent among individuals. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Stella Benson - The Far-Away Bride&lt;/head&gt;
    &lt;p&gt;The Far-Away Bride is the most famous book by the English feminist, novelist, poet, and travel writer Stella Benson. It was published in the United States first in 1930 and as Tobit Transplanted in Britain in 1931. It won the Femina Vie Heureuse Prize for English writers in 1932.&lt;lb/&gt;The novel deals with a family of Russian √©migr√©s in Manchuria. Its characters are the old, grumbling and tearfully sentimental Russian intellectual, Malinin; his disheveled, kind-hearted and unbearable wife, Anna; and Seryozha, their resourceful 19-year-old son. Spending their time in laziness, indulging in exaggerated Russian disorder and comical quarrels growing out of every trifle, they are incongruously happy. The humorous and adventurous action of the novel starts when Seryozha sets out, on foot, on a business trip to the Korean city of Seoul (where he must recover 200 yens); it is there that he finds his ‚Äúfar-away bride‚Äù ‚Äî a charming and whimsical Russian girl who has already broken seven hearts and whose heart he finally conquers.&lt;lb/&gt;Benson described the novel as an ‚Äúaccurate modernization‚Äù of the Book of Tobit, a work of Second Temple Jewish literature dating to the 3rd or early 2nd century BC; The New York Times described The Far-Away Bride, rather, as a ‚Äúspirited parody of it.‚Äù Benson‚Äôs novel, writes the reviewer, is ‚Äúa truly felicitous comedy of the human personality‚Äù. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Vladimir Nabokov - The Defense&lt;/head&gt;
    &lt;p&gt;The Defense (in Russian, Zashchita Luzhinais) is the third novel written by Vladimir Nabokov after he had emigrated to Berlin. It appeared first under Nabokov‚Äôs pen name V. Sirin in the Russian √©migr√© quarterly Sovremennye zapiski and was thereafter published by the √©migr√© publishing house Slovo as The Luzhin Defense in Berlin.&lt;lb/&gt;The novel tells the story of Luzhin. As a young boy, unattractive, withdrawn, sullen, he takes up chess as a refuge from the anxiety of his everyday life. His talent is prodigious and he rises to the rank of grandmaster, but at a cost: in Luzhin‚Äôs obsessive mind, the game of chess gradually supplants the world of reality. His own world falls apart during a crucial championship match, when the intricate defense he has devised withers under his opponent‚Äôs unexpected and unpredictable lines of assault.&lt;lb/&gt;The character of Luzhin is based on Curt von Bardeleben, a chess master Nabokov knew personally, and Nabokov links the events in the central chapters to moves as encountered in chess problems. The book was adapted to film in 2000, as The Luzhin Defence. It was directed by Marleen Gorris, and starred John Turturro as Luzhin. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Dashiell Hammett ‚Äì The Maltese Falcon&lt;/head&gt;
    &lt;p&gt;The Maltese Falcon is a detective novel by American writer Dashiell Hammett, originally serialized in the magazine Black Mask beginning with the September 1929 issue. The story is told entirely in external third-person narrative; there is no description whatsoever of any character‚Äôs thoughts or feelings, only what they say and do, and how they look. The novel has been adapted several times for the cinema and is considered part of the hardboiled genre, which Hammett played a major part in popularizing.&lt;lb/&gt;The novel follows Sam Spade, a private detective in San Francisco, in partnership with Miles Archer. The beautiful ‚ÄúMiss Wonderley‚Äù hires them to follow Floyd Thursby, who she claims has run off with her sister. Archer takes the first stint but is found shot dead that night. ‚ÄúMiss Wonderley‚Äù is soon revealed to be an acquisitive adventuress named Brigid O‚ÄôShaughnessy, who is involved in the search for a black statuette of unknown but substantial value. Red herrings abound.&lt;lb/&gt;Although Hammett himself worked for a time as a private detective for the Pinkerton Detective Agency in San Francisco (and used his given name, Samuel, for the story‚Äôs protagonist), Hammett asserted that ‚ÄúSpade has no original. He is a dream man in the sense that he is what most of the private detectives I worked with would like to have been, and, in their cockier moments, thought they approached.‚Äù (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Stanis≈Çaw Ignacy Witkiewicz ‚Äì Insatiability&lt;/head&gt;
    &lt;p&gt;Insatiability (in Polish Nienasycenie) is a speculative fiction novel by the Polish writer, dramatist, philosopher, painter and photographer, Stanis≈Çaw Ignacy Witkiewicz (Witkacy). It is Witkiewicz‚Äôs third novel, considered by some to be his best.&lt;lb/&gt;Consisting of two parts ‚Äî Przebudzenie (Awakening) and Ob≈Çƒôd (The Madness) ‚Äî the novel takes place in the future, circa 2000. Following a battle, modeled after the Bolshevik revolution, Poland is overrun by the army of the last and final Mongol conquest. The nation becomes enslaved to the Chinese leader Murti Bing. His emissaries give everyone a special pill called DAVAMESK B 2 which takes away their abilities to think and to mentally resist. East and West become one, in faceless misery fueled by sexual instincts.&lt;lb/&gt;The book combines chaotic action with deep philosophical and political discussion, and predicts many of the events and political outcomes of the subsequent years, specifically, the invasion of Poland, the postwar foreign domination as well as the totalitarian mind control exerted, first by the Germans, and then by the Soviet Union on Polish life and art. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read more about Witkiewicz‚Äôs artworks in our essay ‚ÄúDocumenting Drugs‚Äù by Juliette Bretan&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in countries with a ‚Äòlife plus 70 year‚Äô copyright term&lt;/head&gt;
    &lt;head rend="h3"&gt;Albert Einstein&lt;/head&gt;
    &lt;p&gt;Albert Einstein was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory. His mass‚Äìenergy equivalence formula E = mc^2, which arises from special relativity, has been called ‚Äúthe world‚Äôs most famous equation‚Äù. He received the 1921 Nobel Prize in Physics for ‚Äúhis services to theoretical physics, and especially for his discovery of the law of the photoelectric effect‚Äù.&lt;lb/&gt;In 1905, sometimes described as his *annus mirabilis* (miracle year), he published four groundbreaking papers. In them, he outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity, and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole. In 1917, Einstein introduced the concepts of spontaneous emission and stimulated emission, the latter of which is the core mechanism behind the laser and maser, and which helped lay the groundwork for later developments in physics such as quantum electrodynamics and quantum optics. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Wikisource&lt;/p&gt;
    &lt;head rend="h3"&gt;Wallace Stevens&lt;/head&gt;
    &lt;p&gt;Wallace Stevens was an American modernist poet. He was born in Reading, Pennsylvania, educated at Harvard and then New York Law School, and spent most of his life working as an executive for an insurance company in Hartford, Connecticut.&lt;lb/&gt;Stevens‚Äôs first period begins with the publication of Harmonium (1923), followed by a slightly revised and amended second edition in 1930. It features, among other poems, ‚ÄúThe Emperor of Ice-Cream‚Äù, ‚ÄúSunday Morning‚Äù, ‚ÄúThe Snow Man‚Äù, and ‚ÄúThirteen Ways of Looking at a Blackbird‚Äù. His second period commenced with Ideas of Order (1933), included in Transport to Summer (1947). His third and final period began with the publication of The Auroras of Autumn (1950), followed by The Necessary Angel: Essays On Reality and the Imagination (1951).&lt;lb/&gt;Many of Stevens‚Äôs poems deal with the making of art and poetry in particular. His Collected Poems (1954) won the Pulitzer Prize for Poetry in 1955 and Stevens is a rare example of a poet whose main output came largely only as he approached 40 years of age. His first major publication (four poems from a sequence titled ‚ÄúPhases‚Äù in the November 1914 edition of Poetry) was written at age 35, although as an undergraduate at Harvard, Stevens had written poetry and exchanged sonnets with Santayana. Many of his canonical works were written well after he turned 50. According to the literary scholar Harold Bloom, no Western writer since Sophocles has had such a late flowering of artistic genius. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Charlie Parker&lt;/head&gt;
    &lt;p&gt;Charles Parker Jr. was an American jazz saxophonist, bandleader, and composer. Parker was a highly influential soloist and leading figure in the development of bebop, a form of jazz characterized by fast tempos, virtuosic technique, and advanced harmonies. He was a virtuoso and introduced revolutionary rhythmic and harmonic ideas into jazz, including rapid passing chords, new variants of altered chords, and chord substitutions. Parker primarily played the alto saxophone.&lt;lb/&gt;Parker was an icon for the hipster subculture and later the Beat Generation, personifying the jazz musician as an uncompromising artist and intellectual rather than just an entertainer.&lt;lb/&gt;His style of composition involved interpolation of original melodies over existing jazz forms and standards, a practice known as contrafact and still common in jazz today. Examples include ‚ÄúOrnithology‚Äù (which borrows the chord progression of jazz standard ‚ÄúHow High the Moon‚Äù and is said to be co-written with trumpet player Little Benny Harris), and ‚ÄúMoose The Mooche‚Äù. The practice was not uncommon prior to bebop, but it became a signature of the movement as artists began to move away from arranging popular standards and toward composing their own material. Parker contributed greatly to the modern jazz solo, one in which triplets and pick-up notes were used in unorthodox ways to lead into chord tones.&lt;lb/&gt;Miles Davis once said, ‚ÄúYou can tell the history of jazz in four words: Louis Armstrong. Charlie Parker.‚Äù (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Thomas Mann&lt;/head&gt;
    &lt;p&gt;Paul Thomas Mann was a German novelist, short story writer, social critic, philanthropist, essayist, and the 1929 Nobel Prize in Literature laureate. His highly symbolic and ironic epic novels and novellas are noted for their insight into the psychology of the artist and the intellectual. His analysis and critique of the European and German soul used modernized versions of German and Biblical stories, as well as the ideas of Johann Wolfgang von Goethe, Friedrich Nietzsche, and Arthur Schopenhauer.&lt;lb/&gt;Mann was a member of the hanseatic Mann family and portrayed his family and class in his first novel, Buddenbrooks (1901). Further major novels include The Magic Mountain (1924), the tetralogy Joseph and His Brothers (1933‚Äì1943), and Doctor Faustus (1947); he also wrote short stories and novellas, including Death in Venice (1912).&lt;lb/&gt;When Adolf Hitler came to power in 1933, Mann fled to Switzerland and when World War II broke out in 1939, he moved to the United States, then returned to Switzerland in 1952. Mann is one of the best-known exponents of the so-called Exilliteratur, German literature written in exile by those who opposed the Hitler regime. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Project Gutenberg&lt;/p&gt;
    &lt;head rend="h3"&gt;Pierre Teilhard de Chardin&lt;/head&gt;
    &lt;p&gt;Pierre Teilhard de Chardin was a French Jesuit, Catholic priest, scientist, paleontologist, philosopher, mystic, and teacher. He investigated the theory of evolution from a perspective influenced by Henri Bergson and Christian mysticism, writing multiple scientific and religious works on the subject, his most popular being The Phenomenon of Man, published posthumously in 1955. His mainstream scientific achievements include his palaeontological research in China, taking part in the discovery of the significant Peking Man fossils from the Zhoukoudian cave complex near Beijing. His more speculative ideas, sometimes criticized as pseudoscientific, have included a vitalist conception of the Omega Point. Along with Vladimir Vernadsky, he contributed to the development of the concept of the noosphere.&lt;lb/&gt;In 1962, the Holy Office issued a warning regarding Teilhard‚Äôs works, alleging ambiguities and doctrinal errors without specifying them. Some eminent Catholic figures, including Pope Benedict XVI and Pope Francis, have made positive comments on some of his ideas since. The response to his writings by scientists has been divided. His work was controversial to some scientists and religious leaders because Teilhard combined theology and metaphysics with science.&lt;lb/&gt;Teilhard served in World War I as a stretcher-bearer. He received several citations, and was awarded the M√©daille militaire and the Legion of Honor, the highest French order of merit, both military and civil. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Roger Mais&lt;/head&gt;
    &lt;p&gt;Roger Mais was a Jamaican journalist, novelist, poet, and playwright. He was born to a middle-class family in Kingston, Jamaica. By 1951, he had won ten first prizes in West Indian literary competitions. His integral role in the development of political and cultural nationalism is evidenced in his being awarded the high honour of the Order of Jamaica in 1978.&lt;lb/&gt;He worked at various times as a photographer, insurance salesman, and journalist, launching his journalistic career as a contributor to the weekly newspaper Public Opinion from 1939 to 1952. Mais published more than a hundred short stories, most appearing in Public Opinion and Focus, with others collected in Face and Other Stories and And Most of All Man. He wrote more than thirty stage and radio plays, as well as three novels: The Hills Were Joyful Together (1953), Brother Man (1954), and Black Lightning (1955).&lt;lb/&gt;Mais‚Äô topics most frequently were the social injustice and inequality suffered by black, poor Jamaicans. Accused of sedition for writing the article ‚ÄúNow We Know,‚Äù a 1944 denunciation of the British Empire, the Jamaican novelist was tried, convicted and imprisoned for six months. His political activism, anti-colonial writing, and imprisonment helped galvanize Jamaican nationalism. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Saadat Hasan Manto&lt;/head&gt;
    &lt;p&gt;Saadat Hasan Manto was a Pakistani writer, playwright and novelist from Punjab, who is regarded as the greatest short-story author in Urdu literature. He was active from 1933 during British rule till his death in 1955 after independence.&lt;lb/&gt;Writing mainly in Urdu, he produced 22 collections of short stories, a novel, five series of radio plays, three collections of essays, and two collections of personal sketches. He is best known for his stories about the partition of India, which he opposed, immediately following independence in 1947. Manto‚Äôs most notable work has been archived by Rekhta.&lt;lb/&gt;Manto was tried six times for alleged obscenity in his writings; thrice before 1947 in British India, and thrice after independence in 1947 in Pakistan, but was never convicted. He started his literary career translating the works of Victor Hugo, Oscar Wilde and Russian writers such as Chekhov and Gorky. His first story was ‚ÄúTamasha‚Äù, based on the Jallianwala Bagh massacre at Amritsar. His final works, which grew from the social climate and his own financial struggles, reflected an innate sense of human impotency towards darkness and contained satire that verged on dark comedy, as seen in his last story, ‚ÄúToba Tek Singh‚Äù. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in countries with a ‚Äòlife plus 50 year‚Äô copyright term&lt;/head&gt;
    &lt;head rend="h3"&gt;Barbara Hepworth&lt;/head&gt;
    &lt;p&gt;Dame Jocelyn Barbara Hepworth was an English artist and sculptor. Along with artists such as Ben Nicholson and Naum Gabo, Hepworth was a leading figure in the colony of artists who resided in St Ives during the Second World War. Born in Wakefield, Yorkshire, Hepworth studied at Leeds School of Art and the Royal College of Art in the 1920s. She married the sculptor John Skeaping in 1925. In 1931 she fell in love with the painter Ben Nicholson, and in 1933 divorced Skeaping. At this time she was part of a circle of modern artists centred on Hampstead, London, and was one of the founders of the art movement Unit One. At the beginning of the Second World War Hepworth and Nicholson moved to St Ives, Cornwall, where she would remain for the rest of her life. Best known as a sculptor, Hepworth also produced drawings ‚Äì including a series of sketches of operating rooms following the hospitalisation of her daughter in 1944 ‚Äì and lithographs. She died in a fire at her studio in 1975. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Hannah Arendt&lt;/head&gt;
    &lt;p&gt;Hannah Arendt was a German and American historian and philosopher. She was one of the most influential political theorists of the twentieth century.&lt;lb/&gt;Her works cover a broad range of topics, but she is best known for those dealing with the nature of wealth, power, fame, and evil, as well as politics, direct democracy, authority, tradition, and totalitarianism. She is also remembered for the controversy surrounding the trial of Adolf Eichmann, for her attempt to explain how ordinary people become actors in totalitarian systems, which was considered by some an apologia, and for the phrase ‚Äúthe banality of evil‚Äù.&lt;lb/&gt;In 1933, Arendt was briefly imprisoned by the Gestapo for performing illegal research into antisemitism. On release, she fled Germany, settling in Paris. There she worked for Youth Aliyah, assisting young Jews to emigrate to the British Mandate of Palestine. When Germany invaded France she was detained as an alien, but she escaped and made her way to the United States in 1941. She became a writer and editor and worked for the Jewish Cultural Reconstruction, becoming an American citizen in 1950. With the publication of The Origins of Totalitarianism in 1951, her reputation as a thinker and writer was established, and a series of works followed. These included the books The Human Condition in 1958, as well as Eichmann in Jerusalem and On Revolution in 1963. She taught at many American universities while declining tenure-track appointments. She died suddenly of a heart attack in 1975, leaving her last work, The Life of the Mind, unfinished. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Walker Evans&lt;/head&gt;
    &lt;p&gt;Walker Evans was an American photographer and photojournalist best known for his work for the Resettlement Administration and the Farm Security Administration (FSA) documenting the effects of the Great Depression. Evans‚Äô published his first photos at the age of 27. Much of Evans‚Äô New Deal work uses the large format, 8 √ó 10-inch (200√ó250 mm) view camera. He said that his goal as a photographer was to make pictures that are ‚Äúliterate, authoritative, transcendent‚Äù.&lt;lb/&gt;Many of his works are in the permanent collections of museums and have been the subject of retrospectives at such institutions as the Metropolitan Museum of Art or the George Eastman Museum.&lt;lb/&gt;Born in St. Louis, Missouri, Evans took up photography in 1928 around the time he was living in Ossining, New York. The Great Depression years of 1935‚Äì36 were a period of remarkable productivity and accomplishment for Evans. In 1936, employed by the National Recovery Administration, he photographed three impoverished sharecropper families in Hale County, Alabama. The photographs became iconic and were praised for effectively capturing the negative effects of the Great Depression in the American South. Between 1940 and 1959, Evans was awarded three Guggenheim Fellowships in Photography to continue his work of making record photographs of contemporary American subjects. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Library of Congress&lt;/p&gt;
    &lt;head rend="h3"&gt;P. G. Wodehouse&lt;/head&gt;
    &lt;p&gt;Sir Pelham Grenville Wodehouse was an English writer and one of the most widely read humorists of the 20th century. His creations include the feather-brained Bertie Wooster and his sagacious valet, Jeeves; the immaculate and loquacious Psmith; Lord Emsworth and the Blandings Castle set; the Oldest Member, with stories about golf; and Mr. Mulliner, with tall tales on subjects ranging from bibulous bishops to megalomaniac movie moguls.&lt;lb/&gt;Born in Guildford, his early novels were mostly school stories, but he later switched to comic fiction. Most of Wodehouse‚Äôs fiction is set in his native United Kingdom, although he spent much of his life in the US and used New York and Hollywood as settings for some of his novels and short stories. Wodehouse was a prolific writer throughout his life, publishing more than ninety books, forty plays, two hundred short stories and other writings between 1902 and 1974. Early in his career Wodehouse would produce a novel in about three months, but he slowed in old age to around six months. He used a mixture of Edwardian slang, quotations from and allusions to numerous poets, and several literary techniques to produce a prose style that has been compared to comic poetry and musical comedy. Some critics of Wodehouse have considered his work flippant, but among his fans are former British prime ministers and many of his fellow writers. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Project Gutenberg&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460440</guid><pubDate>Fri, 02 Jan 2026 01:42:16 +0000</pubDate></item><item><title>Marmot ‚Äì A distributed SQLite server with MySQL wire compatible interface</title><link>https://github.com/maxpert/marmot</link><description>&lt;doc fingerprint="f99204d0da0b98c"&gt;
  &lt;main&gt;
    &lt;p&gt;Marmot v2 is a leaderless, distributed SQLite replication system built on a gossip-based protocol with distributed transactions and eventual consistency.&lt;/p&gt;
    &lt;p&gt;Key Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leaderless Architecture: No single point of failure - any node can accept writes&lt;/item&gt;
      &lt;item&gt;MySQL Protocol Compatible: Connect with any MySQL client (DBeaver, MySQL Workbench, mysql CLI)&lt;/item&gt;
      &lt;item&gt;Distributed Transactions: Percolator-style write intents with conflict detection&lt;/item&gt;
      &lt;item&gt;Multi-Database Support: Create and manage multiple databases per cluster&lt;/item&gt;
      &lt;item&gt;DDL Replication: Distributed schema changes with automatic idempotency and cluster-wide locking&lt;/item&gt;
      &lt;item&gt;Production-Ready SQL Parser: Powered by rqlite/sql AST parser for MySQL‚ÜíSQLite transpilation&lt;/item&gt;
      &lt;item&gt;CDC-Based Replication: Row-level change data capture for consistent replication&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Start a single-node cluster
./marmot-v2

# Connect with MySQL client
mysql -h localhost -P 3306 -u root

# Or use DBeaver, MySQL Workbench, etc.&lt;/code&gt;
    &lt;code&gt;# Test DDL and DML replication across a 2-node cluster
./scripts/test-ddl-replication.sh

# This script will:
# 1. Start a 2-node cluster
# 2. Create a table on node 1 and verify it replicates to node 2
# 3. Insert data on node 1 and verify it replicates to node 2
# 4. Update data on node 2 and verify it replicates to node 1
# 5. Delete data on node 1 and verify it replicates to node 2

# Manual cluster testing
./examples/start-seed.sh              # Start seed node (port 8081, mysql 3307)
./examples/join-cluster.sh 2 localhost:8081  # Join node 2 (port 8082, mysql 3308)
./examples/join-cluster.sh 3 localhost:8081  # Join node 3 (port 8083, mysql 3309)

# Connect to any node and run queries
mysql --protocol=TCP -h localhost -P 3307 -u root
mysql --protocol=TCP -h localhost -P 3308 -u root

# Cleanup
pkill -f marmot-v2&lt;/code&gt;
    &lt;p&gt;Marmot v2 uses a fundamentally different architecture from other SQLite replication solutions:&lt;/p&gt;
    &lt;p&gt;vs. rqlite/dqlite/LiteFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ùå They require a primary node for all writes&lt;/item&gt;
      &lt;item&gt;‚úÖ Marmot allows writes on any node&lt;/item&gt;
      &lt;item&gt;‚ùå They use leader election (Raft)&lt;/item&gt;
      &lt;item&gt;‚úÖ Marmot uses gossip protocol (no leader)&lt;/item&gt;
      &lt;item&gt;‚ùå They require proxy layer or page-level interception&lt;/item&gt;
      &lt;item&gt;‚úÖ Marmot uses MySQL protocol for direct database access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How It Works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write Coordination: 2PC (Two-Phase Commit) with configurable consistency (ONE, QUORUM, ALL)&lt;/item&gt;
      &lt;item&gt;Conflict Resolution: Last-Write-Wins (LWW) with HLC timestamps&lt;/item&gt;
      &lt;item&gt;Cluster Membership: SWIM-style gossip with failure detection&lt;/item&gt;
      &lt;item&gt;Data Replication: Full database replication - all nodes receive all data&lt;/item&gt;
      &lt;item&gt;DDL Replication: Cluster-wide schema changes with automatic idempotency&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot v2 supports distributed DDL (Data Definition Language) replication without requiring master election:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Cluster-Wide Locking: Each DDL operation acquires a distributed lock per database (default: 30-second lease)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Prevents concurrent schema changes on the same database&lt;/item&gt;
          &lt;item&gt;Locks automatically expire if a node crashes&lt;/item&gt;
          &lt;item&gt;Different databases can have concurrent DDL operations&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Idempotency: DDL statements are automatically rewritten for safe replay&lt;/p&gt;
        &lt;quote&gt;CREATE TABLE users (id INT) ‚Üí CREATE TABLE IF NOT EXISTS users (id INT) DROP TABLE users ‚Üí DROP TABLE IF EXISTS users&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Version Tracking: Each database maintains a schema version counter&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Incremented on every DDL operation&lt;/item&gt;
          &lt;item&gt;Exchanged via gossip protocol for drift detection&lt;/item&gt;
          &lt;item&gt;Used by delta sync to validate transaction applicability&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Quorum-Based Replication: DDL replicates like DML through the same 2PC mechanism&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;No special master node needed&lt;/item&gt;
          &lt;item&gt;Works with existing consistency levels (QUORUM, ALL, etc.)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[ddl]
# DDL lock lease duration (seconds)
lock_lease_seconds = 30

# Automatically rewrite DDL for idempotency
enable_idempotent = true&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Do: Execute DDL from a single connection/node at a time&lt;/item&gt;
      &lt;item&gt;‚úÖ Do: Use qualified table names (&lt;code&gt;mydb.users&lt;/code&gt;instead of&lt;code&gt;users&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;Caution: ALTER TABLE is less idempotent - avoid replaying failed ALTER operations&lt;/item&gt;
      &lt;item&gt;‚ùå Don't: Run concurrent DDL on the same database from multiple nodes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot v2 uses Change Data Capture (CDC) for replication instead of SQL statement replay:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row-Level Capture: Instead of replicating SQL statements, Marmot captures the actual row data changes (INSERT/UPDATE/DELETE)&lt;/item&gt;
      &lt;item&gt;Binary Data Format: Row data is serialized as CDC messages with column values, ensuring consistent replication regardless of SQL dialect&lt;/item&gt;
      &lt;item&gt;Deterministic Application: Row data is applied directly to the target database, avoiding parsing ambiguities&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistency: Same row data applied everywhere, no SQL parsing differences&lt;/item&gt;
      &lt;item&gt;Performance: Binary format is more efficient than SQL text&lt;/item&gt;
      &lt;item&gt;Reliability: No issues with SQL syntax variations between MySQL and SQLite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For UPDATE and DELETE operations, Marmot automatically extracts row keys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses PRIMARY KEY columns when available&lt;/item&gt;
      &lt;item&gt;Falls back to ROWID for tables without explicit primary key&lt;/item&gt;
      &lt;item&gt;Handles composite primary keys correctly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot can publish CDC events to external messaging systems, enabling real-time data pipelines, analytics, and event-driven architectures. Events follow the Debezium specification for maximum compatibility with existing CDC tooling.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debezium-Compatible Format: Events conform to the Debezium event structure, compatible with Kafka Connect, Flink, Spark, and other CDC consumers&lt;/item&gt;
      &lt;item&gt;Multi-Sink Support: Publish to multiple destinations simultaneously (Kafka, NATS)&lt;/item&gt;
      &lt;item&gt;Glob-Based Filtering: Filter which tables and databases to publish&lt;/item&gt;
      &lt;item&gt;Automatic Retry: Exponential backoff with configurable limits&lt;/item&gt;
      &lt;item&gt;Persistent Cursors: Survives restarts without losing position&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[publisher]
enabled = true

[[publisher.sinks]]
name = "kafka-main"
type = "kafka"                    # "kafka" or "nats"
format = "debezium"               # Debezium-compatible JSON format
brokers = ["localhost:9092"]      # Kafka broker addresses
topic_prefix = "marmot.cdc"       # Topics: {prefix}.{database}.{table}
filter_tables = ["*"]             # Glob patterns (e.g., "users", "order_*")
filter_databases = ["*"]          # Glob patterns (e.g., "prod_*")
batch_size = 100                  # Events per poll cycle
poll_interval_ms = 10             # Polling interval

# NATS sink example
[[publisher.sinks]]
name = "nats-events"
type = "nats"
format = "debezium"
nats_url = "nats://localhost:4222"
topic_prefix = "marmot.cdc"
filter_tables = ["*"]
filter_databases = ["*"]&lt;/code&gt;
    &lt;p&gt;Events follow the Debezium envelope structure:&lt;/p&gt;
    &lt;code&gt;{
  "schema": { ... },
  "payload": {
    "before": null,
    "after": {"id": 1, "name": "alice", "email": "alice@example.com"},
    "source": {
      "version": "2.0.0",
      "connector": "marmot",
      "name": "marmot",
      "ts_ms": 1702500000000,
      "db": "myapp",
      "table": "users"
    },
    "op": "c",
    "ts_ms": 1702500000000
  }
}&lt;/code&gt;
    &lt;p&gt;Operation Types (per Debezium spec):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Operation&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;op&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;before&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;after&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;INSERT&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;c&lt;/code&gt; (create)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;null&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;row data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;UPDATE&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;u&lt;/code&gt; (update)&lt;/cell&gt;
        &lt;cell&gt;old row&lt;/cell&gt;
        &lt;cell&gt;new row&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DELETE&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;d&lt;/code&gt; (delete)&lt;/cell&gt;
        &lt;cell&gt;old row&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;null&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Topics follow the pattern: &lt;code&gt;{topic_prefix}.{database}.{table}&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.myapp.users&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.myapp.orders&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.analytics.events&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-Time Analytics: Stream changes to data warehouses (Snowflake, BigQuery, ClickHouse)&lt;/item&gt;
      &lt;item&gt;Event-Driven Microservices: Trigger actions on data changes&lt;/item&gt;
      &lt;item&gt;Cache Invalidation: Keep caches in sync with database changes&lt;/item&gt;
      &lt;item&gt;Audit Logging: Capture all changes for compliance&lt;/item&gt;
      &lt;item&gt;Search Indexing: Keep Elasticsearch/Algolia in sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more details, see the Integrations documentation.&lt;/p&gt;
    &lt;p&gt;Marmot supports a wide range of MySQL/SQLite statements through its MySQL protocol server. The following table shows compatibility for different statement types:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Statement Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Support&lt;/cell&gt;
        &lt;cell role="head"&gt;Replication&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DML - Data Manipulation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;INSERT&lt;/code&gt; / &lt;code&gt;REPLACE&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names (db.table)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UPDATE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DELETE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SELECT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Read operations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;LOAD DATA&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Bulk data loading&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DDL - Data Definition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DROP TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;TRUNCATE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;RENAME TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP INDEX&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP VIEW&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP TRIGGER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Database Management&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DROP DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SHOW DATABASES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Metadata query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SHOW TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Metadata query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;USE database&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Session state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Transaction Control&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;BEGIN&lt;/code&gt; / &lt;code&gt;START TRANSACTION&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Transaction boundary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;COMMIT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Commits distributed transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ROLLBACK&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Aborts distributed transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SAVEPOINT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Full&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Yes&lt;/cell&gt;
        &lt;cell&gt;Nested transaction support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;LOCK TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;Requires distributed locking coordination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UNLOCK TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;Requires distributed locking coordination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Session Configuration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;SET&lt;/code&gt; statements&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;Session-local, not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;XA Transactions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;XA START/END/PREPARE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;Marmot uses its own 2PC protocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;XA COMMIT/ROLLBACK&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;Not compatible with Marmot's model&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DCL - Data Control&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;GRANT&lt;/code&gt; / &lt;code&gt;REVOKE&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP USER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER USER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Administrative&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;OPTIMIZE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;Node-local administrative command&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;REPAIR TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;‚úÖ Parsed&lt;/cell&gt;
        &lt;cell&gt;‚ùå No&lt;/cell&gt;
        &lt;cell&gt;Node-local administrative command&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Full: Fully supported and working&lt;/item&gt;
      &lt;item&gt;‚úÖ Parsed: Statement is parsed and recognized&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;Limited: Works but has limitations in distributed context&lt;/item&gt;
      &lt;item&gt;‚ùå No: Not supported or not replicated&lt;/item&gt;
      &lt;item&gt;N/A: Not applicable (read-only or session-local)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Changes (DDL): DDL statements are fully replicated with cluster-wide locking and automatic idempotency. See the DDL Replication section for details.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;XA Transactions: Marmot has its own distributed transaction protocol based on 2PC. MySQL XA transactions are not compatible with Marmot's replication model.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;User Management (DCL): User and privilege management statements are local to each node. For production deployments, consider handling authentication at the application or proxy level.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Table Locking:&lt;/p&gt;&lt;code&gt;LOCK TABLES&lt;/code&gt;statements are recognized but not enforced across the cluster. Use application-level coordination for distributed locking needs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Qualified Names: Marmot fully supports qualified table names (e.g.,&lt;/p&gt;&lt;code&gt;db.table&lt;/code&gt;) in DML and DDL operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot includes a MySQL-compatible protocol server, allowing you to connect using any MySQL client (DBeaver, MySQL Workbench, mysql CLI, etc.). The server supports:&lt;/p&gt;
    &lt;p&gt;Marmot provides full support for MySQL metadata queries, enabling GUI tools like DBeaver to browse databases, tables, and columns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SHOW Commands: &lt;code&gt;SHOW DATABASES&lt;/code&gt;,&lt;code&gt;SHOW TABLES&lt;/code&gt;,&lt;code&gt;SHOW COLUMNS FROM table&lt;/code&gt;,&lt;code&gt;SHOW CREATE TABLE&lt;/code&gt;,&lt;code&gt;SHOW INDEXES&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;INFORMATION_SCHEMA: Queries against &lt;code&gt;INFORMATION_SCHEMA.TABLES&lt;/code&gt;,&lt;code&gt;INFORMATION_SCHEMA.COLUMNS&lt;/code&gt;,&lt;code&gt;INFORMATION_SCHEMA.SCHEMATA&lt;/code&gt;, and&lt;code&gt;INFORMATION_SCHEMA.STATISTICS&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Type Conversion: Automatic SQLite-to-MySQL type mapping for compatibility&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These metadata queries are powered by the rqlite/sql AST parser, providing production-grade MySQL query compatibility.&lt;/p&gt;
    &lt;code&gt;# Using mysql CLI
mysql -h localhost -P 3306 -u root

# Connection string for applications
mysql://root@localhost:3306/marmot&lt;/code&gt;
    &lt;p&gt;Marmot handles various failure and recovery scenarios automatically:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Minority partition&lt;/cell&gt;
        &lt;cell&gt;Writes fail - cannot achieve quorum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Majority partition&lt;/cell&gt;
        &lt;cell&gt;Writes succeed - quorum achieved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Partition heals&lt;/cell&gt;
        &lt;cell&gt;Delta sync + LWW merges divergent data&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;How it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;During partition, only the majority side can commit writes (quorum enforcement)&lt;/item&gt;
      &lt;item&gt;When partition heals, nodes exchange transaction logs via &lt;code&gt;StreamChanges&lt;/code&gt;RPC&lt;/item&gt;
      &lt;item&gt;Conflicts resolved using Last-Writer-Wins (LWW) with HLC timestamps&lt;/item&gt;
      &lt;item&gt;Higher node ID breaks ties for simultaneous writes&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Recovery Method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Brief outage&lt;/cell&gt;
        &lt;cell&gt;Delta sync - replay missed transactions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extended outage&lt;/cell&gt;
        &lt;cell&gt;Snapshot transfer + delta sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;New node joining&lt;/cell&gt;
        &lt;cell&gt;Full snapshot from existing node&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Anti-Entropy Background Process:&lt;/p&gt;
    &lt;p&gt;Marmot v2 includes an automatic anti-entropy system that continuously monitors and repairs replication lag across the cluster:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lag Detection: Every 60 seconds (configurable), each node queries peers for their replication state&lt;/item&gt;
      &lt;item&gt;Smart Recovery Decision: &lt;list rend="ul"&gt;&lt;item&gt;Delta Sync if lag &amp;lt; 10,000 transactions AND &amp;lt; 1 hour: Streams missed transactions incrementally&lt;/item&gt;&lt;item&gt;Snapshot Transfer if lag exceeds thresholds: Full database file transfer for efficiency&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Gap Detection: Detects when transaction logs have been GC'd and automatically falls back to snapshot&lt;/item&gt;
      &lt;item&gt;Multi-Database Support: Tracks and syncs each database independently&lt;/item&gt;
      &lt;item&gt;GC Coordination: Garbage collection respects peer replication state - logs aren't deleted until all peers have applied them&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Delta Sync Process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lagging node queries &lt;code&gt;last_applied_txn_id&lt;/code&gt;for each peer/database&lt;/item&gt;
      &lt;item&gt;Requests transactions since that ID via &lt;code&gt;StreamChanges&lt;/code&gt;RPC&lt;/item&gt;
      &lt;item&gt;Gap Detection: Checks if first received txn_id has a large gap from requested ID &lt;list rend="ul"&gt;&lt;item&gt;If gap &amp;gt; delta_sync_threshold_txns, indicates missing (GC'd) transactions&lt;/item&gt;&lt;item&gt;Automatically falls back to snapshot transfer to prevent data loss&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Applies changes using LWW conflict resolution&lt;/item&gt;
      &lt;item&gt;Updates replication state tracking (per-database)&lt;/item&gt;
      &lt;item&gt;Progress logged every 100 transactions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GC Coordination with Anti-Entropy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transaction logs are retained with a two-tier policy: &lt;list rend="ul"&gt;&lt;item&gt;Min retention (2 hours): Must be &amp;gt;= delta sync threshold, respects peer lag&lt;/item&gt;&lt;item&gt;Max retention (24 hours): Force delete after this time to prevent unbounded growth&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Config validation enforces: &lt;code&gt;gc_min &amp;gt;= delta_threshold&lt;/code&gt;and&lt;code&gt;gc_max &amp;gt;= 2x delta_threshold&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Each database tracks replication progress per peer&lt;/item&gt;
      &lt;item&gt;GC queries minimum applied txn_id across all peers before cleanup&lt;/item&gt;
      &lt;item&gt;Gap detection prevents data loss if GC runs while nodes are offline&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Write Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ONE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after 1 node ACK (fast, less durable)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;QUORUM&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after majority ACK (default, balanced)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;ALL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after all nodes ACK (slow, most durable)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Conflict Resolution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All conflicts resolved via LWW using HLC timestamps&lt;/item&gt;
      &lt;item&gt;No data loss - later write always wins deterministically&lt;/item&gt;
      &lt;item&gt;Tie-breaker: higher node ID wins for equal timestamps&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Selective Table Watching: All tables in a database are replicated. Selective table replication is not supported.&lt;/item&gt;
      &lt;item&gt;WAL Mode Required: SQLite must use WAL mode for reliable multi-process changes.&lt;/item&gt;
      &lt;item&gt;Eventually Consistent: Rows may sync out of order. &lt;code&gt;SERIALIZABLE&lt;/code&gt;transaction assumptions may not hold across nodes.&lt;/item&gt;
      &lt;item&gt;Concurrent DDL: Avoid running concurrent DDL operations on the same database from multiple nodes (protected by cluster-wide lock with 30s lease).&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;IMPORTANT: Marmot automatically converts&lt;/p&gt;&lt;code&gt;INT AUTO_INCREMENT&lt;/code&gt;to&lt;code&gt;BIGINT&lt;/code&gt;&lt;p&gt;This is a breaking change from standard MySQL/SQLite behavior. Marmot does not respect 32-bit&lt;/p&gt;&lt;code&gt;INT&lt;/code&gt;for auto-increment columns - they are automatically promoted to&lt;code&gt;BIGINT&lt;/code&gt;to support distributed ID generation.&lt;/quote&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;In a distributed, leaderless system, each node must generate unique IDs independently without coordination. Marmot uses HLC-based (Hybrid Logical Clock) 64-bit IDs to ensure:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Global Uniqueness: IDs are unique across all nodes without central coordination&lt;/item&gt;
      &lt;item&gt;Monotonicity: IDs increase over time (within each node)&lt;/item&gt;
      &lt;item&gt;No Collisions: Unlike auto-increment sequences, HLC IDs cannot collide between nodes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How It Works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;DDL Transformation: When you create a table with&lt;/p&gt;&lt;code&gt;AUTO_INCREMENT&lt;/code&gt;:&lt;quote&gt;CREATE TABLE users (id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100)) -- Becomes internally: CREATE TABLE users (id BIGINT PRIMARY KEY, name TEXT)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;DML ID Injection: When inserting with&lt;/p&gt;&lt;code&gt;0&lt;/code&gt;or&lt;code&gt;NULL&lt;/code&gt;for an auto-increment column:&lt;quote&gt;INSERT INTO users (id, name) VALUES (0, 'alice') -- Becomes internally: INSERT INTO users (id, name) VALUES (7318624812345678901, 'alice')&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explicit IDs Preserved: If you provide an explicit non-zero ID, it is used as-is:&lt;/p&gt;
        &lt;quote&gt;INSERT INTO users (id, name) VALUES (12345, 'bob') -- Remains: INSERT INTO users (id, name) VALUES (12345, 'bob')&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important Considerations:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Aspect&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ID Range&lt;/cell&gt;
        &lt;cell&gt;64-bit (up to 9.2 quintillion) instead of 32-bit (4.2 billion)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ID Format&lt;/cell&gt;
        &lt;cell&gt;HLC-based, not sequential integers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SQLite ROWID&lt;/cell&gt;
        &lt;cell&gt;Not used - Marmot manages IDs explicitly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Client Libraries&lt;/cell&gt;
        &lt;cell&gt;Ensure your client handles &lt;code&gt;BIGINT&lt;/code&gt; correctly (some JSON serializers may lose precision)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Existing Data&lt;/cell&gt;
        &lt;cell&gt;Migrate existing &lt;code&gt;INT&lt;/code&gt; columns to &lt;code&gt;BIGINT&lt;/code&gt; before enabling Marmot&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Schema-Based Detection:&lt;/p&gt;
    &lt;p&gt;Marmot automatically detects auto-increment columns by querying SQLite schema directly. A column is considered auto-increment if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is a single-column &lt;code&gt;INTEGER PRIMARY KEY&lt;/code&gt;(SQLite rowid alias), or&lt;/item&gt;
      &lt;item&gt;It is a single-column &lt;code&gt;BIGINT PRIMARY KEY&lt;/code&gt;(Marmot's transformed columns)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No registration required - columns are detected from schema at runtime&lt;/item&gt;
      &lt;item&gt;Works across restarts - no need to re-execute DDL statements&lt;/item&gt;
      &lt;item&gt;Works with existing databases - tables created directly on SQLite work too&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot v2 uses a TOML configuration file (default: &lt;code&gt;config.toml&lt;/code&gt;). All settings have sensible defaults.&lt;/p&gt;
    &lt;code&gt;node_id = 0  # 0 = auto-generate
data_dir = "./marmot-data"&lt;/code&gt;
    &lt;code&gt;[transaction]
heartbeat_timeout_seconds = 10  # Transaction timeout without heartbeat
conflict_window_seconds = 10    # Conflict resolution window
lock_wait_timeout_seconds = 50  # Lock wait timeout (MySQL: innodb_lock_wait_timeout)&lt;/code&gt;
    &lt;p&gt;Note: Transaction log garbage collection is managed by the replication configuration to coordinate with anti-entropy. See &lt;code&gt;replication.gc_min_retention_hours&lt;/code&gt; and &lt;code&gt;replication.gc_max_retention_hours&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[connection_pool]
pool_size = 4              # Number of SQLite connections
max_idle_time_seconds = 10 # Max idle time before closing
max_lifetime_seconds = 300 # Max connection lifetime (0 = unlimited)&lt;/code&gt;
    &lt;code&gt;[grpc_client]
keepalive_time_seconds = 10    # Keepalive ping interval
keepalive_timeout_seconds = 3  # Keepalive ping timeout
max_retries = 3                # Max retry attempts
retry_backoff_ms = 100         # Retry backoff duration&lt;/code&gt;
    &lt;code&gt;[coordinator]
prepare_timeout_ms = 2000 # Prepare phase timeout
commit_timeout_ms = 2000  # Commit phase timeout
abort_timeout_ms = 2000   # Abort phase timeout&lt;/code&gt;
    &lt;code&gt;[cluster]
grpc_bind_address = "0.0.0.0"
grpc_port = 8080
seed_nodes = []                # List of seed node addresses
cluster_secret = ""            # PSK for cluster authentication (see Security section)
gossip_interval_ms = 1000      # Gossip interval
gossip_fanout = 3              # Number of peers to gossip to
suspect_timeout_ms = 5000      # Suspect timeout
dead_timeout_ms = 10000        # Dead timeout&lt;/code&gt;
    &lt;p&gt;Marmot supports Pre-Shared Key (PSK) authentication for cluster communication. This is strongly recommended for production deployments.&lt;/p&gt;
    &lt;code&gt;[cluster]
# All nodes in the cluster must use the same secret
cluster_secret = "your-secret-key-here"&lt;/code&gt;
    &lt;p&gt;Environment Variable (Recommended):&lt;/p&gt;
    &lt;p&gt;For production, use the environment variable to avoid storing secrets in config files:&lt;/p&gt;
    &lt;code&gt;export MARMOT_CLUSTER_SECRET="your-secret-key-here"
./marmot&lt;/code&gt;
    &lt;p&gt;The environment variable takes precedence over the config file.&lt;/p&gt;
    &lt;p&gt;Generating a Secret:&lt;/p&gt;
    &lt;code&gt;# Generate a secure random secret
openssl rand -base64 32&lt;/code&gt;
    &lt;p&gt;Behavior:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;code&gt;cluster_secret&lt;/code&gt;is empty and&lt;code&gt;MARMOT_CLUSTER_SECRET&lt;/code&gt;is not set, authentication is disabled&lt;/item&gt;
      &lt;item&gt;A warning is logged at startup when authentication is disabled&lt;/item&gt;
      &lt;item&gt;All gRPC endpoints (gossip, replication, snapshots) are protected when authentication is enabled&lt;/item&gt;
      &lt;item&gt;Nodes with mismatched secrets will fail to communicate (connection rejected with "invalid cluster secret")&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot provides admin HTTP endpoints for managing cluster membership (requires &lt;code&gt;cluster_secret&lt;/code&gt; to be configured):&lt;/p&gt;
    &lt;p&gt;Node Lifecycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New/restarted nodes auto-join via gossip - no manual intervention needed&lt;/item&gt;
      &lt;item&gt;Nodes marked REMOVED via admin API cannot auto-rejoin - must be explicitly allowed&lt;/item&gt;
      &lt;item&gt;This prevents decommissioned nodes from accidentally rejoining the cluster&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# View cluster members and quorum info
curl -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/members

# Remove a node from the cluster (excludes from quorum, blocks auto-rejoin)
curl -X POST -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/remove/2

# Allow a removed node to rejoin (node must then restart to join)
curl -X POST -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/allow/2&lt;/code&gt;
    &lt;p&gt;See the Operations documentation for detailed usage and examples.&lt;/p&gt;
    &lt;code&gt;[replication]
default_write_consistency = "QUORUM"      # Write consistency level: ONE, QUORUM, ALL
default_read_consistency = "LOCAL_ONE"    # Read consistency level
write_timeout_ms = 5000                   # Write operation timeout
read_timeout_ms = 2000                    # Read operation timeout

# Anti-Entropy: Background healing for eventual consistency
# - Detects and repairs divergence between replicas
# - Uses delta sync for small lags, snapshot for large lags
# - Includes gap detection to prevent incomplete data after GC
enable_anti_entropy = true                 # Enable automatic catch-up for lagging nodes
anti_entropy_interval_seconds = 60         # How often to check for lag (default: 60s)
delta_sync_threshold_transactions = 10000  # Delta sync if lag &amp;lt; 10K txns
delta_sync_threshold_seconds = 3600        # Snapshot if lag &amp;gt; 1 hour

# Garbage Collection: Reclaim disk space by deleting old transaction records
# - gc_min must be &amp;gt;= delta_sync_threshold (validated at startup)
# - gc_max should be &amp;gt;= 2x delta_sync_threshold (recommended)
# - Set gc_max = 0 for unlimited retention
gc_min_retention_hours = 2   # Keep at least 2 hours (&amp;gt;= 1 hour delta threshold)
gc_max_retention_hours = 24  # Force delete after 24 hours&lt;/code&gt;
    &lt;p&gt;Anti-Entropy Tuning:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small clusters (2-3 nodes): Use default settings (60s interval)&lt;/item&gt;
      &lt;item&gt;Large clusters (5+ nodes): Consider increasing interval to 120-180s to reduce network overhead&lt;/item&gt;
      &lt;item&gt;High write throughput: Increase &lt;code&gt;delta_sync_threshold_transactions&lt;/code&gt;to 50000+&lt;/item&gt;
      &lt;item&gt;Long-running clusters: Keep &lt;code&gt;gc_max_retention_hours&lt;/code&gt;at 24+ to handle extended outages&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GC Configuration Rules (Validated at Startup):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;gc_min_retention_hours&lt;/code&gt;must be &amp;gt;=&lt;code&gt;delta_sync_threshold_seconds&lt;/code&gt;(in hours)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gc_max_retention_hours&lt;/code&gt;should be &amp;gt;= 2x&lt;code&gt;delta_sync_threshold_seconds&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Violating these rules will cause startup failure with helpful error messages&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[query_pipeline]
transpiler_cache_size = 10000  # LRU cache for MySQL‚ÜíSQLite transpilation
validator_pool_size = 8        # SQLite connection pool for validation&lt;/code&gt;
    &lt;code&gt;[mysql]
enabled = true
bind_address = "0.0.0.0"
port = 3306
max_connections = 1000&lt;/code&gt;
    &lt;code&gt;[publisher]
enabled = false  # Enable CDC publishing to external systems

[[publisher.sinks]]
name = "kafka-main"              # Unique sink name
type = "kafka"                   # "kafka" or "nats"
format = "debezium"              # Debezium-compatible JSON (only option)
brokers = ["localhost:9092"]     # Kafka broker addresses
topic_prefix = "marmot.cdc"      # Topic pattern: {prefix}.{db}.{table}
filter_tables = ["*"]            # Glob patterns for table filtering
filter_databases = ["*"]         # Glob patterns for database filtering
batch_size = 100                 # Events to read per poll cycle
poll_interval_ms = 10            # Polling interval (default: 10ms)
retry_initial_ms = 100           # Initial retry delay on failure
retry_max_ms = 30000             # Max retry delay (30 seconds)
retry_multiplier = 2.0           # Exponential backoff multiplier&lt;/code&gt;
    &lt;p&gt;See the Integrations documentation for details on event format, Kafka/NATS configuration, and use cases.&lt;/p&gt;
    &lt;code&gt;[logging]
verbose = false          # Enable verbose logging
format = "console"       # Log format: console or json&lt;/code&gt;
    &lt;code&gt;[prometheus]
enabled = true  # Metrics served on gRPC port at /metrics endpoint&lt;/code&gt;
    &lt;p&gt;Accessing Metrics:&lt;/p&gt;
    &lt;code&gt;# Metrics are multiplexed with gRPC on the same port
curl http://localhost:8080/metrics

# Prometheus scrape config
scrape_configs:
  - job_name: 'marmot'
    static_configs:
      - targets: ['node1:8080', 'node2:8080', 'node3:8080']&lt;/code&gt;
    &lt;p&gt;See &lt;code&gt;config.toml&lt;/code&gt; for complete configuration reference with detailed comments.&lt;/p&gt;
    &lt;p&gt;Performance benchmarks on a local development machine (Apple M-series, 3-node cluster, single machine):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nodes&lt;/cell&gt;
        &lt;cell&gt;3 (ports 3307, 3308, 3309)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Threads&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;10 ops/transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Consistency&lt;/cell&gt;
        &lt;cell&gt;QUORUM&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;4,175 ops/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TX Throughput&lt;/cell&gt;
        &lt;cell&gt;417 tx/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Records Loaded&lt;/cell&gt;
        &lt;cell&gt;200,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Errors&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;3,370 ops/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TX Throughput&lt;/cell&gt;
        &lt;cell&gt;337 tx/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Duration&lt;/cell&gt;
        &lt;cell&gt;120 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Total Operations&lt;/cell&gt;
        &lt;cell&gt;404,930&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Errors&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Retries&lt;/cell&gt;
        &lt;cell&gt;37 (0.09%)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Operation Distribution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;READ: 20%&lt;/item&gt;
      &lt;item&gt;UPDATE: 30%&lt;/item&gt;
      &lt;item&gt;INSERT: 35%&lt;/item&gt;
      &lt;item&gt;DELETE: 5%&lt;/item&gt;
      &lt;item&gt;UPSERT: 10%&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Percentile&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;4.3ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P90&lt;/cell&gt;
        &lt;cell&gt;14.0ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P95&lt;/cell&gt;
        &lt;cell&gt;36.8ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;85.1ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All 3 nodes maintained identical row counts (346,684 rows) throughout the test, confirming consistent replication.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: These benchmarks are from a local development machine with all nodes on the same host. Production deployments across multiple machines will have different characteristics based on network latency.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460676</guid><pubDate>Fri, 02 Jan 2026 02:21:57 +0000</pubDate></item><item><title>I'm a developer for a major food delivery app</title><link>https://old.reddit.com/r/confession/comments/1q1mzej/im_a_developer_for_a_major_food_delivery_app_the/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46461563</guid><pubDate>Fri, 02 Jan 2026 04:57:25 +0000</pubDate></item><item><title>Real Biological Clock Is You're Going to Die (2018)</title><link>https://hmmdaily.com/2018/10/18/your-real-biological-clock-is-youre-going-to-die/</link><description>&lt;doc fingerprint="3fcbd17d8fe69f77"&gt;
  &lt;main&gt;
    &lt;p&gt;In April, in a small town on a small island in a small string of islands trailing down from the main part of Japan, the world‚Äôs then-oldest person died. Nabi Tajima was 117 years old‚Äîthe last surviving human being born in the 19th century. &lt;/p&gt;
    &lt;p&gt;Maybe the year 1900 sounds far away, to you. It comes closer. My father was born in 1940. Right now, I am 47 years old. Everyone who was 47 years old when my father was born is now dead. All of them. That entire group of middle-aged people, who made up the adult world when my father was a child, is gone. &lt;/p&gt;
    &lt;p&gt;My father is dead, now, too. He smoked, and it shortened his life. But I lived with him, grew up with him. I was an adult with him. We drank coffee at the dining room table together, read the newspaper, talked on the phone. He sat and talked with my firstborn son when he was little. He held my second-born, as a baby, while he could. The older boy remembers him.&lt;/p&gt;
    &lt;p&gt;The younger boy is seven now. When my father was a boy of seven, Nabi Tajima was 47. I keep sliding the numbers around, and as I do, they shove each other out of the way. The people don‚Äôt all fit. &lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;p&gt;Twelve years ago, I had not thought about any of this at all. I was 35. My wife would soon be pregnant, for the first time. We had dated for a good long while, been engaged for a good long while, been married for a good long while. We had bought a house in the suburbs and then we had sold it and moved overseas. It was all a sensibly paced series of life events, at appropriate-seeming times. &lt;/p&gt;
    &lt;p&gt;It did not occur to me, in any real way, that as we did this, we were spending down a limited resource. In our social world, in our cultural class, at our point in history, people are brought up to take the opposite view, to structure their lives as if time were something a person accumulated. One is wary of getting married too soon, of having children too young. Adulthood is a condition to enter cautiously and gradually. &lt;/p&gt;
    &lt;p&gt;From certain angles, this breeds disdain‚Äîpenniless millennials eating avocado toast, forty-year-old men skateboarding in sneakers and t-shirts, slovenly and undisciplined generations refusing to commit to lives and careers. But the complaining is halfhearted, a way for the older cohort to convince itself the younger cohort might be safely held at bay. And to point out the shortcomings of adult-aged people is, at bottom, to argue that maturity is something rarefied. The figure of the kidult exists as a warning that you should not move on to the next step until you‚Äôre certain you‚Äôre ready. &lt;/p&gt;
    &lt;p&gt;But this idea of certainty is a sham, a distraction, something to turn your attention away from the only truly certain thing, which is that your time will run out. If you intend to have children, but you don‚Äôt intend to have them just yet, you are not banking extra years as a person who is still too young to have children. You are subtracting years from the time you will share the world with your children.&lt;/p&gt;
    &lt;p&gt;All our conversations about choices and priorities and life decisions are held in the shadow of the great constraint. The closest the discussion usually comes to it is the subject of women‚Äôs anatomy, the aging of eggs, the decline of female fertility. Sometimes, for the broad-minded, there may be some mention of the fading quality of men‚Äôs sperm, as well. The possibility that it might not happen at all‚Äîthat a particular body might not be able, or might fall ill, or might die first‚Äîis almost unspeakable, and for planning purposes goes unspoken. &lt;/p&gt;
    &lt;p&gt;Instead, life and the making of more life appears as a technical problem; where the body‚Äôs ability to stall falters, cryogenics can take over. Perhaps zygotes can be spun up from ordinary DNA. Science is doing its best to push back the limits. Look to the wealthy and the entitled and see the future: Mick Jagger just sired a baby at age 73. &lt;/p&gt;
    &lt;p&gt;Now look to the future: when that baby turns 10, Mick Jagger will be 83. When that baby turns 20, Mick Jagger will be 93. When that baby turns 30, 40, 45‚ÄîMick Jagger will be almost certainly dead, all but definitely dead, unquestionably dead. &lt;/p&gt;
    &lt;p&gt;As the child of a spectacularly wealthy man, Mick Jagger‚Äôs baby will surely be provided for. But not by its dad, not in person. &lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;p&gt;Did we choose the age at which we would have children? What does it mean to choose? We use the language of autonomy to avoid thinking about powers that shape our existence. Our civilization is remarkably hostile to the needs of life, from the helplessness of babyhood to the frailty of old age. The system is set up for healthy, productive, independent individuals, and one absorbs the lesson that one should try to stay in this class as long and as securely as one can. &lt;/p&gt;
    &lt;p&gt;So we wait until we are ready. There are good reasons to wait, or reasons that appear to be good once you accept the underlying terms. My children have not had to live with parents who are working 15-hour days, the way we worked in our 20s, or who are financially desperate, as we might have been if we‚Äôd been paying for children on the salaries of our 20s. Our professional standing allows us to skip work for pediatric appointments or parent-teacher conferences. We can afford to hire babysitters. I got a promotion and a big raise just when it was time to buy a piano. We all sit down together for home-cooked meals most evenings and talk about things. &lt;/p&gt;
    &lt;p&gt;I hope someday my children can do the same things for their own children. Maybe I‚Äôll see them do it, but maybe I won‚Äôt: If my firstborn waits till he‚Äôs 35, going on 36, to have a child, the way I did, my grandchild will arrive the year I would turn 71. When that grandchild finishes fifth grade, I‚Äôll be due to turn 81. And that would be the first of my grandchildren. &lt;/p&gt;
    &lt;p&gt;Are you ready to have a child? Take your age right now and add 18. And nine months, if you want to get particular about it, but call it 18. That‚Äôs how old you‚Äôll be for high school graduation. Add 25 and picture yourself traveling to visit your grown-up child in a new city. Add 30. Add 40. &lt;/p&gt;
    &lt;p&gt;The clock is running, only it‚Äôs not a clock: It‚Äôs a sandglass. According to the Social Security Administration‚Äôs online calculator, an average man born the day I was born can expect to live 34.9 more years, for a total of 82.0 years. When I first checked it, when drafting this piece, it was 35.4. I thought it would be a lighthearted exercise, but I felt real dread as I was entering the birthdate, and, despite myself, shock when I saw how small the number was.&lt;/p&gt;
    &lt;p&gt;When I was 34.9 years old, I felt like I had just settled into life. What lies ahead is the un-settling, the inverse of the question of when you feel old enough to have a baby: When will your children be old enough to have dying parents? The later you take on your intergenerational responsibilities, the sooner you hand them off. If I hit the Social Security Administration projection, my older son will be 46 when I go. &lt;/p&gt;
    &lt;p&gt;It‚Äôs possible to beat the projection. The longer you live, the longer you can expect to live; if I make it to 70, the calculator says, then I can look forward to making it to 86.7. Ten more presidential cycles, one more visit from Comet Tempel-Tuttle, maybe a driver‚Äôs license for my oldest grandchild. You can take extreme measures‚Äîinject yourself with teenage blood, or get elected to the United States Senate‚Äîbut at best that will push things back to the hard limit. If I last as long as Nabi Tajima, I still won‚Äôt see the 22nd century. &lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;p&gt;This world devours every person and moves on. It does not stop moving, even as we pass through the middle of life telling ourselves it is the front end. Before the children arrived, there was not much difference from one year to the next. In some ways, in the adult, professional sphere, there still is not much difference. In a chair, at a computer screen, 47 doesn‚Äôt feel that far from 37. A little trouble in the lumbar region, that‚Äôs all. Some wiry gray at the temples in the bathroom mirror. &lt;/p&gt;
    &lt;p&gt;This is the illusion of adult timekeeping, and children make it unsustainable. Life moves along at an unexceptional, unexamined pace and suddenly it‚Äôs the first day of school, and then it‚Äôs the first day of school again. The jeans I remember just buying him are up above the ankles. The younger boy kisses me back when I kiss him good night, but by last year the older boy started to twist away from holding hands a few yards before the school door, to dart off ahead. Now he just walks to school on his own. There‚Äôs time still for him to circle back for a hug at day‚Äôs end. Someday, though, a hug will be the last one. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46461778</guid><pubDate>Fri, 02 Jan 2026 05:39:13 +0000</pubDate></item><item><title>FreeBSD: Home NAS, part 1 ‚Äì configuring ZFS mirror (RAID1)</title><link>https://rtfm.co.ua/en/freebsd-home-nas-part-1-configuring-zfs-mirror-raid1/</link><description>&lt;doc fingerprint="1e35285ba76e8ee5"&gt;
  &lt;main&gt;
    &lt;p&gt;I have an idea to set up a home NAS on FreeBSD.&lt;/p&gt;
    &lt;p&gt;For this purpose, I bought a Lenovo ThinkCentre M720s SFF ‚Äì it‚Äôs quiet, compact, and offers the possibility to install 2 SATA III SSDs plus a separate M.2 slot for an NVMe SSD.&lt;/p&gt;
    &lt;p&gt;What is planned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;on NVMe SSD: UFS and FreeBSD&lt;/item&gt;
      &lt;item&gt;on SATA SSDs: ZFS with RAID1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While waiting for the drives to arrive, let‚Äôs test how it all works on a virtual machine.&lt;/p&gt;
    &lt;p&gt;We will be installing FreeBSD 14.3, although version 15 is already out, but it has some interesting changes that I‚Äôll play with separately.&lt;/p&gt;
    &lt;p&gt;Of course, I could have gone with TrueNAS, which is based on FreeBSD ‚Äì but I want ‚Äúvanilla‚Äù FreeBSD to do everything manually.&lt;/p&gt;
    &lt;p&gt;All posts in this blog series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;(current) FreeBSD: Home NAS, part 1 ‚Äì configuring ZFS mirror (RAID1)&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 2 ‚Äì introduction to Packet Filter (PF) firewall&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 3 ‚Äì WireGuard VPN, Linux peer, and routing&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 4 ‚Äì Local DNS with Unbound&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 5 ‚Äì ZFS pool, datasets, snapshots, and ZFS monitoring&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 6 ‚Äì Samba server and client connections&lt;/item&gt;
      &lt;item&gt;‚Ä¶ to be continued&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contents&lt;/p&gt;
    &lt;head rend="h1"&gt;Installing FreeBSD via SSH&lt;/head&gt;
    &lt;p&gt;We will perform the installation over SSH using &lt;code&gt;bsdinstall&lt;/code&gt; ‚Äì boot the system in LiveCD mode, enable SSH, and then proceed with the installation from a workstation laptop.&lt;/p&gt;
    &lt;p&gt;The virtual machine has three disks ‚Äì mirroring the future ThinkCentre setup:&lt;/p&gt;
    &lt;p&gt;Select Live System:&lt;/p&gt;
    &lt;p&gt;Login as &lt;code&gt;root&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Bring up the network:&lt;/p&gt;
    &lt;quote&gt;# ifconfig em0 up # dhclient em0&lt;/quote&gt;
    &lt;head rend="h2"&gt;Configuring SSH on FreeBSD LiveCD&lt;/head&gt;
    &lt;p&gt;For SSH, we need to set a &lt;code&gt;root&lt;/code&gt; password and make changes to &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt;, but currently, this doesn‚Äôt work because the system is mounted as read-only:&lt;/p&gt;
    &lt;p&gt;Check the current partitions:&lt;/p&gt;
    &lt;p&gt;And apply a ‚Äúdirty hack‚Äù:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;mount a new &lt;code&gt;tmpfs&lt;/code&gt;file system in RAM at&lt;code&gt;/mnt&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;copy the contents of &lt;code&gt;/etc&lt;/code&gt;from the LiveCD there&lt;/item&gt;
      &lt;item&gt;mount &lt;code&gt;tmpfs&lt;/code&gt;over&lt;code&gt;/etc&lt;/code&gt;(overlaying the read-only directory from the ISO)&lt;/item&gt;
      &lt;item&gt;copy the prepared files from &lt;code&gt;/mnt&lt;/code&gt;back into the new&lt;code&gt;/etc&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Execute:&lt;/p&gt;
    &lt;quote&gt;# mount -t tmpfs tmpfs /mnt # cp -a /etc/* /mnt/ # mount -t tmpfs tmpfs /etc # cp -a /mnt/* /etc/&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;mount&lt;/code&gt; syntax for &lt;code&gt;tmpfs&lt;/code&gt; is &lt;code&gt;mount -t &amp;lt;fstype&amp;gt; &amp;lt;source&amp;gt; &amp;lt;mountpoint&amp;gt;&lt;/code&gt;. Since the &lt;code&gt;source&lt;/code&gt; value is required, we specify &lt;code&gt;tmpfs&lt;/code&gt; again.&lt;/p&gt;
    &lt;p&gt;Now, set the password with &lt;code&gt;passwd&lt;/code&gt; and start &lt;code&gt;sshd&lt;/code&gt; using &lt;code&gt;onestart&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# passwd # service sshd onestart&lt;/quote&gt;
    &lt;p&gt;However, SSH will still deny access because &lt;code&gt;root&lt;/code&gt; login is disabled by default:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ([email protected]) Password for root@: ([email protected]) Password for root@: ([email protected]) Password for root@:&lt;/quote&gt;
    &lt;p&gt;Set &lt;code&gt;PermitRootLogin yes&lt;/code&gt; in &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt; and restart &lt;code&gt;sshd&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# echo "PermitRootLogin yes" &amp;gt;&amp;gt; /etc/ssh/sshd_config # service sshd onerestart&lt;/quote&gt;
    &lt;p&gt;Now we can log in:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ([email protected]) Password for root@: Last login: Sun Dec 7 12:19:25 2025 FreeBSD 14.3-RELEASE (GENERIC) releng/14.3-n271432-8c9ce319fef7 Welcome to FreeBSD! ... root@:~ #&lt;/quote&gt;
    &lt;head rend="h1"&gt;Installation with &lt;code&gt;bsdinstall&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Run &lt;code&gt;bsdinstall&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# bsdinstall&lt;/quote&gt;
    &lt;p&gt;Select the components to add to the system ‚Äì &lt;code&gt;ports&lt;/code&gt; is necessary, &lt;code&gt;src&lt;/code&gt; is optional but definitely worth it for a real NAS:&lt;/p&gt;
    &lt;head rend="h2"&gt;Disk partitioning&lt;/head&gt;
    &lt;p&gt;We‚Äôll do a minimal disk partition, so select Manual:&lt;/p&gt;
    &lt;p&gt;We will install the system on &lt;code&gt;ada0&lt;/code&gt;, select it, and click Create:&lt;/p&gt;
    &lt;p&gt;Next, choose a partition scheme. It‚Äôs standard for 2025 ‚Äì GPT:&lt;/p&gt;
    &lt;p&gt;Confirm the changes, and now we have a new partition table on the system drive &lt;code&gt;ada0&lt;/code&gt;:&lt;/p&gt;
    &lt;head rend="h3"&gt;The &lt;code&gt;freebsd-boot&lt;/code&gt; Partition&lt;/head&gt;
    &lt;p&gt;Now we need to create the partitions themselves.&lt;/p&gt;
    &lt;p&gt;Select &lt;code&gt;ada0&lt;/code&gt; again, click Create, and create a partition for &lt;code&gt;freebsd-boot&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is just for the virtual machine; on the actual ThinkCentre, we would use type &lt;code&gt;efi&lt;/code&gt; with a size of about 200-500 MB.&lt;/p&gt;
    &lt;p&gt;For now, set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-boot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 512K&lt;/item&gt;
      &lt;item&gt;Mountpoint: empty&lt;/item&gt;
      &lt;item&gt;Label: empty&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Confirm and proceed to the next partition.&lt;/p&gt;
    &lt;head rend="h3"&gt;The &lt;code&gt;freebsd-swap&lt;/code&gt; Partition&lt;/head&gt;
    &lt;p&gt;Click Create again to add Swap.&lt;/p&gt;
    &lt;p&gt;Given that on the ThinkCentre we will have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8 ‚Äì 16 GB RAM&lt;/item&gt;
      &lt;item&gt;no sleep/hibernate&lt;/item&gt;
      &lt;item&gt;UFS and ZFS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2 gigabytes will be enough.&lt;/p&gt;
    &lt;p&gt;Set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-swap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 2GB&lt;/item&gt;
      &lt;item&gt;Mountpoint: empty&lt;/item&gt;
      &lt;item&gt;Label: empty&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Root Partition with UFS&lt;/head&gt;
    &lt;p&gt;The main system will be on UFS because it is very stable, doesn‚Äôt require much RAM, mounts quickly, is easy to recover, and lacks complex caching mechanisms (UPD: however, after getting to know ZFS and its capabilities better, I decided to use it for the system disk as well)&lt;/p&gt;
    &lt;p&gt;Set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-ufs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 14GB&lt;/item&gt;
      &lt;item&gt;Mountpoint: /&lt;/item&gt;
      &lt;item&gt;Label: rootfs ‚Äì just a name for us&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We‚Äôll configure the rest of the disks later; for now, select Finish and Commit:&lt;/p&gt;
    &lt;head rend="h2"&gt;Finishing Installation&lt;/head&gt;
    &lt;p&gt;Wait for the copying to complete:&lt;/p&gt;
    &lt;p&gt;Configure the network:&lt;/p&gt;
    &lt;p&gt;Select Timezone:&lt;/p&gt;
    &lt;p&gt;In System Configuration ‚Äì select &lt;code&gt;sshd&lt;/code&gt;, no mouse, enable &lt;code&gt;ntpd&lt;/code&gt; and &lt;code&gt;powerd&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;System Hardening ‚Äì considering this will be a home NAS, but I might open external access (even behind a firewall), it makes sense to tune the security a bit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;read_msgbuf&lt;/code&gt;: allow&lt;code&gt;dmesg&lt;/code&gt;access for root only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;proc_debug&lt;/code&gt;: allow&lt;code&gt;ptrace&lt;/code&gt;for root only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;random_pid&lt;/code&gt;: randomize PID numbers&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clear_tmp&lt;/code&gt;: clear&lt;code&gt;/tmp&lt;/code&gt;on reboot&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;secure_console&lt;/code&gt;: require&lt;code&gt;root&lt;/code&gt;password for login from the physical console&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add a user:&lt;/p&gt;
    &lt;p&gt;Everything is ready ‚Äì reboot the machine:&lt;/p&gt;
    &lt;head rend="h1"&gt;Creating a ZFS RAID&lt;/head&gt;
    &lt;p&gt;Log in as the regular user:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ... FreeBSD 14.3-RELEASE (GENERIC) releng/14.3-n271432-8c9ce319fef7 Welcome to FreeBSD! ... setevoy@test-nas-1:~ $&lt;/quote&gt;
    &lt;p&gt;Install &lt;code&gt;vim&lt;/code&gt; üôÇ&lt;/p&gt;
    &lt;quote&gt;# pkg install vim&lt;/quote&gt;
    &lt;p&gt;Check our disks.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;geom disk&lt;/code&gt; for physical device info, and &lt;code&gt;gpart show&lt;/code&gt; to see partitions on the disks.&lt;/p&gt;
    &lt;p&gt;Check disks ‚Äì there are three:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # geom disk list Geom name: ada0 Providers: 1. Name: ada0 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r2w2e3 descr: VBOX HARDDISK ident: VB262b53f7-adc5cd2c rotationrate: unknown fwsectors: 63 fwheads: 16 Geom name: ada1 Providers: 1. Name: ada1 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r0w0e0 descr: VBOX HARDDISK ident: VB059f9d08-4b0e1f56 rotationrate: unknown fwsectors: 63 fwheads: 16 Geom name: ada2 Providers: 1. Name: ada2 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r0w0e0 descr: VBOX HARDDISK ident: VB3941028c-3ea0d485 rotationrate: unknown fwsectors: 63 fwheads: 16&lt;/quote&gt;
    &lt;p&gt;And with &lt;code&gt;gpart&lt;/code&gt; ‚Äì current &lt;code&gt;ada0&lt;/code&gt; where the system was installed:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show =&amp;gt; 40 33554352 ada0 GPT (16G) 40 1024 1 freebsd-boot (512K) 1064 4194304 2 freebsd-swap (2.0G) 4195368 29359024 3 freebsd-ufs (14G)&lt;/quote&gt;
    &lt;p&gt;Disks &lt;code&gt;ada1&lt;/code&gt; and &lt;code&gt;ada2&lt;/code&gt; will be used for ZFS and its mirror (RAID1).&lt;/p&gt;
    &lt;p&gt;If there was anything on them ‚Äì wipe it:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart destroy -F ada1 gpart: arg0 'ada1': Invalid argument root@test-nas-1:/home/setevoy # gpart destroy -F ada2 gpart: arg0 'ada2': Invalid argument&lt;/quote&gt;
    &lt;p&gt;Since this is a VM and the disks are empty, ‚ÄúInvalid argument‚Äù is expected and fine.&lt;/p&gt;
    &lt;p&gt;Create GPT partition tables on &lt;code&gt;ada1&lt;/code&gt; and &lt;code&gt;ada2&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart create -s gpt ada1 ada1 created root@test-nas-1:/home/setevoy # gpart create -s gpt ada2 ada2 created&lt;/quote&gt;
    &lt;p&gt;Check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show ada1 =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 - free - (16G)&lt;/quote&gt;
    &lt;p&gt;Create partitions for ZFS:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart add -t freebsd-zfs ada1 ada1p1 added root@test-nas-1:/home/setevoy # gpart add -t freebsd-zfs ada2 ada2p1 added&lt;/quote&gt;
    &lt;p&gt;Check again:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show ada1 =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 1 freebsd-zfs (16G)&lt;/quote&gt;
    &lt;head rend="h2"&gt;Creating a ZFS mirror with &lt;code&gt;zpool&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The ‚Äúmagic‚Äù of ZFS is that everything works ‚Äúout of the box‚Äù ‚Äì you don‚Äôt need a separate LVM and its groups, and you don‚Äôt need &lt;code&gt;mdadm&lt;/code&gt; for RAID.&lt;/p&gt;
    &lt;p&gt;For managing disks in ZFS, the main utility is &lt;code&gt;zpool&lt;/code&gt;, and for managing data (datasets, file systems, snapshots), it‚Äôs &lt;code&gt;zfs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To combine one or more disks into a single logical storage, ZFS uses a pool ‚Äì the equivalent of a volume group in Linux LVM.&lt;/p&gt;
    &lt;p&gt;Create the pool:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool create tank mirror ada1p1 ada2p1&lt;/quote&gt;
    &lt;p&gt;Here, tank is the pool name, &lt;code&gt;mirror&lt;/code&gt; specifies that it will be RAID1, and we provide the list of partitions included in this pool.&lt;/p&gt;
    &lt;p&gt;Check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool status pool: tank state: ONLINE config: NAME STATE READ WRITE CKSUM tank ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 ada1p1 ONLINE 0 0 0 ada2p1 ONLINE 0 0 0 errors: No known data errors&lt;/quote&gt;
    &lt;p&gt;ZFS immediately mounts this pool at &lt;code&gt;/tank&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # mount /dev/ada0p3 on / (ufs, local, soft-updates, journaled soft-updates) devfs on /dev (devfs) tank on /tank (zfs, local, nfsv4acls)&lt;/quote&gt;
    &lt;p&gt;Check partitions now:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show =&amp;gt; 40 33554352 ada0 GPT (16G) 40 1024 1 freebsd-boot (512K) 1064 4194304 2 freebsd-swap (2.0G) 4195368 29359024 3 freebsd-ufs (14G) =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 1 freebsd-zfs (16G) =&amp;gt; 40 33554352 ada2 GPT (16G) 40 33554352 1 freebsd-zfs (16G)&lt;/quote&gt;
    &lt;p&gt;If we want to change the mountpoint ‚Äì execute &lt;code&gt;zfs set mountpoint&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zfs set mountpoint=/data tank&lt;/quote&gt;
    &lt;p&gt;And it immediately mounts to the new directory:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # mount /dev/ada0p3 on / (ufs, local, soft-updates, journaled soft-updates) devfs on /dev (devfs) tank on /data (zfs, local, nfsv4acls)&lt;/quote&gt;
    &lt;p&gt;Enable data compression ‚Äì useful for a NAS, see Compression and Compressing ZFS File Systems.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;lz4&lt;/code&gt; is the current default option, let‚Äôs enable it:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zfs set compression=lz4 tank&lt;/quote&gt;
    &lt;p&gt;Since we installed the system on UFS, we need to add a few parameters to autostart for ZFS to work.&lt;/p&gt;
    &lt;p&gt;Configure the boot loader in &lt;code&gt;/boot/loader.conf&lt;/code&gt; to load kernel modules:&lt;/p&gt;
    &lt;quote&gt;zfs_load="YES"&lt;/quote&gt;
    &lt;p&gt;Or, to avoid manual editing, use &lt;code&gt;sysrc&lt;/code&gt; with the &lt;code&gt;-f&lt;/code&gt; flag:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # sysrc -f /boot/loader.conf zfs_load="YES"&lt;/quote&gt;
    &lt;p&gt;And add to &lt;code&gt;/etc/rc.conf&lt;/code&gt; to start the &lt;code&gt;zfsd&lt;/code&gt; daemon and mount the file systems:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # sysrc zfs_enable="YES" zfs_enable: NO -&amp;gt; YES&lt;/quote&gt;
    &lt;p&gt;Reboot and check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool status pool: tank state: ONLINE config: NAME STATE READ WRITE CKSUM tank ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 ada1p1 ONLINE 0 0 0 ada2p1 ONLINE 0 0 0&lt;/quote&gt;
    &lt;p&gt;Everything is in place.&lt;/p&gt;
    &lt;p&gt;Now you can proceed with further tuning ‚Äì configuring separate datasets, snapshots, etc.&lt;/p&gt;
    &lt;p&gt;For a Web UI, you could try Seafile or FileBrowser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46462108</guid><pubDate>Fri, 02 Jan 2026 06:48:32 +0000</pubDate></item><item><title>Round the tree, yes, but not round the squirrel</title><link>https://www.futilitycloset.com/2026/01/02/round-and-round/</link><description>&lt;doc fingerprint="f75c93007bda195e"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄòI had quite a bit of fun playing hide-and-seek with a squirrel,‚Äô he said. ‚ÄòYou know that little round glade with a lone birch in the centre? It was on this tree that a squirrel was hiding from me. As I emerged from a thicket, I saw its snout and two bright little eyes peeping from behind the trunk. I wanted to see the little animal, so I started circling round along the edge of the glade, mindful of keeping the distance in order not to scare it. I did four rounds, but the little cheat kept backing away from me, eyeing me suspiciously from behind the tree. Try as I did, I just could not see its back.‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòBut you have just said yourself that you circled round the tree four times,‚Äô one of the listeners interjected.&lt;/p&gt;
      &lt;p&gt;‚ÄòRound the tree, yes, but not round the squirrel.‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòBut the squirrel was on the tree, wasn‚Äôt it?‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòSo it was.‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòWell, that means you circled round the squirrel too.‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòCall that circling round the squirrel when I didn‚Äôt see its back?‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòWhat has its back to do with the whole thing? The squirrel was on the tree in the centre of the glade and you circled round the tree. In other words, you circled round the squirrel.‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòOh no, I didn‚Äôt. Let us assume that I‚Äôm circling round you and you keep turning, showing me just your face. Call that circling round you?‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòOf course, what else can you call it?‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòYou mean I‚Äôm circling round you though I‚Äôm never behind you and never see your back?‚Äô&lt;/p&gt;
      &lt;p&gt;‚ÄòForget the back! You‚Äôre circling round me and that‚Äôs what counts. What has the back to do with it?‚Äô&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;‚Äî Yakov Perelman, Mathematics Can Be Fun, 1927&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46462592</guid><pubDate>Fri, 02 Jan 2026 08:16:21 +0000</pubDate></item></channel></rss>