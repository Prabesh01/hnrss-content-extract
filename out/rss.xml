<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 29 Nov 2025 22:38:09 +0000</lastBuildDate><item><title>Rare X-ray images of a 4.5-ton satellite that returned intact from space</title><link>https://www.empa.ch/web/s604/eureca-satellit-mit-roentgenmethoden-untersucht</link><description>&lt;doc fingerprint="fb0594ac68a86ed9"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;EURECA examined at Empa&lt;/head&gt;
    &lt;head rend="h1"&gt;X-raying a satellite&lt;/head&gt;
    &lt;p&gt;It is rare for satellites to return to Earth intact after their mission in space – one example is the European satellite EURECA. Empa researchers have studied the satellite using various non-destructive X-ray methods. These could be used in future in the development of reusable space technologies, as well as in aviation and the automotive industry.&lt;/p&gt;
    &lt;p&gt;Whether it's a sprained ankle or a backpack at the airport, X-ray images are an everyday occurrence in many areas. Empa researchers at the Center for X-Ray Analytics have succeeded in taking images that are far less commonplace: In collaboration with the Swiss Space Center (now Space Innovation at EPFL) and the Swiss Museum of Transport, they have X-rayed an entire satellite.&lt;/p&gt;
    &lt;p&gt;The imaged satellite is called EURECA – short for EUropean REtrievable CArrier – and is one of a kind. It was launched into space in 1992 aboard the Space Shuttle Atlantis. Swiss astronaut Claude Nicollier deployed EURECA into orbit. There, the 4.5-ton satellite remained for the next eleven months – until it was captured by the crew of the Space Shuttle Endeavour on July 1, 1993, and brought back to Earth. This makes EURECA one of the very few satellites to have returned from its mission in space intact.&lt;/p&gt;
    &lt;p&gt;The European Space Agency (ESA) originally planned several missions for the reusable satellite. EURECA carried 15 interchangeable instruments for scientific experiments ranging from biology to astrophysics. However, the budget for the program was cut, and EURECA's first flight was also its last. At the end of 2000, the satellite was put on exhibit at the Swiss Museum of Transport in Lucerne. From there, the journey to Dübendorf and Empa was a short one. Examining a flown satellite was not an opportunity Empa researchers were willing to miss.&lt;/p&gt;
    &lt;head rend="h5"&gt;A deep look inside&lt;/head&gt;
    &lt;p&gt;EURECA was first X-rayed in 2016. The complete results of the investigation were published in 2025 in the journal Acta Astronautica. Using the high-energy X-ray facility, the researchers were able to X-ray the five-meter-long, three-meter-high, and two-and-a-half-meter-wide satellite in one piece. They also used other X-ray techniques for parts of the satellite and for the two scientific instruments that remained on board.&lt;/p&gt;
    &lt;p&gt;The big advantage of X-ray imaging is the same for satellites as it is for ankles in hospitals and hand luggage at airports: It allows a non-destructive view of the interior. ”Our analysis covers several orders of magnitude, from the entire carrier structure of the satellite to investigations of materials at nanometer scale,“ says Empa researcher Robert Zboray, lead author of the publication. The researchers found several defects, such as cracks in EURECA's composite struts and fractures and deformations in the scientific instruments.&lt;/p&gt;
    &lt;p&gt;“Satellites are exposed to strong radiation, large temperature fluctuations, and collisions with particles from meteorites and space debris,” says Zboray. “Our methods enable us to identify weak points, especially in reusable satellites.” Damage can also occur during launch and landing. According to Zboray, further experiments would be necessary to pinpoint the exact event that caused the damage. “Ideally, such satellites should be X-rayed both before launch and after landing,” says the scientist.&lt;/p&gt;
    &lt;p&gt;Although these days, EURECA only unfolds its solar panels at the Swiss Museum of Transport, the topic of reusable space technologies is more relevant today than ever before. By 2025, there will be over 10,000 satellites in Earth's orbit – and the number is growing every year. The issue is compounded by countless rocket stages, fragments of old satellites, and other space debris that pose a danger to active satellites and manned spaceflight. Reusable satellites could help reduce this flood of space junk – and Zboray is convinced that X-ray techniques could be used to optimize their design. However, high-energy X-ray imaging also has terrestrial applications, such as examining components for the aviation and automotive industries, or even in forensics.&lt;/p&gt;
    &lt;p&gt;PD Dr. Robert Zboray&lt;lb/&gt; Empa, Center for X-ray Analytics&lt;lb/&gt; Phone +41 58 765 46 02&lt;/p&gt;
    &lt;p&gt;R Zboray, C Roeoesli, A Flisch, M Plamondon, R Kaufmann, C von Deschwanden, K Zweiacker, T Lüthi, T Bandi, G Bourban, V Gass, D Amstutz, A Dommann, A Neels: Multi-scale and multi-energy non-destructive X-ray analysis of the European Retrievable Carrier (EURECA); Acta Astronautica (2025); doi: 10.1016/j.actaastro.2025.05.042&lt;/p&gt;
    &lt;p&gt;Space technologies&lt;/p&gt;
    &lt;p&gt;Empa's research is conquering space. From components for space probes and imaging techniques for satellites to materials development on the International Space Station: Empa researchers are working on a wide range of materials and technologies for use at the “final frontier”. But no matter how high the research flies, it is still down-to-earth, as technologies for space have a variety of applications on Earth, whether for innovative electronics, medical devices, or monitoring environmental agreements.&lt;/p&gt;
    &lt;p&gt;Read the EmpaQuarterly online or download the PDF version.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Share&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46060432</guid><pubDate>Wed, 26 Nov 2025 18:02:45 +0000</pubDate></item><item><title>An update on the Farphone's battery</title><link>https://far.computer/battery-update/</link><description>&lt;doc fingerprint="b7f9a493f30e3f33"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;an update on the farphone's battery&lt;/head&gt;
    &lt;p&gt;28/11/2025&lt;/p&gt;
    &lt;p&gt;earlier this week i shared the how-to for this website, which is running on an old smartphone&lt;/p&gt;
    &lt;p&gt;folks on lobste.rs and hacker news pointed out the fire risk of keeping a lithium battery plugged in all the time&lt;/p&gt;
    &lt;p&gt;french blogger korben even mentioned it in an article about the device&lt;/p&gt;
    &lt;p&gt;the fairphone 2 being as modular as it is, it was trivial to open the phone, pop the battery out, and duct-tape the micro-usb charging cable to the phone to avoid accidental dos&lt;/p&gt;
    &lt;p&gt;forty seconds later the farphone had booted and the web server was back up&lt;/p&gt;
    &lt;p&gt;thank you lobsters, thank you hackers, you might have prevented a tragic server explosion&lt;/p&gt;
    &lt;p&gt;built by louis merlin under the cc by-nc-sa 4.0 license&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46078684</guid><pubDate>Fri, 28 Nov 2025 13:57:43 +0000</pubDate></item><item><title>The CRDT Dictionary: A Field Guide to Conflict-Free Replicated Data Types</title><link>https://www.iankduncan.com/engineering/2025-11-27-crdt-dictionary/</link><description>&lt;doc fingerprint="5f77706e84b8bed5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The CRDT Dictionary: A Field Guide to Conflict-Free Replicated Data Types&lt;/head&gt;
    &lt;p&gt;Back around 2014, I kept hearing about this cool database called Riak, a distributed database that could survive network partitions and keep accepting writes. Some really interesting companies were using it at massive scale, and I was curious about it. One of the big selling points was that it could handle concurrent writes without any coordination or consensus. I was intrigued, and I started reading about it. Underlying all of this was the concept of CRDTs, Conflict-free Replicated Data Types.1&lt;/p&gt;
    &lt;p&gt;At the time, I was working on a beer startup called Brewtown with a friend: a beer review social site and delivery subscription service. It failed for other reasons, but I was a little too enamored with shiny tech back then, and CRDTs and Riak fit the bill for shiny tech. I kept trying to find excuses to shoehorn CRDT stuff into our codebase when, honestly, we didn’t need any of it. Postgres would’ve been fine. Live and learn.&lt;/p&gt;
    &lt;p&gt;Anyways, the idea sounded like pure sorcery: data structures that replicate across nodes and merge deterministically, without coordination, without losing information. I got excited, read a few papers, played with some toy implementations… and then we gave up on the beer startup. I didn’t really have a reason to mess with CRDTs for a while.&lt;/p&gt;
    &lt;p&gt;Fast forward to 2025, I’ve just had Thanksgiving dinner, and I’m curious again. What’s the state of the art? What have I forgotten? Which CRDT should I reach for when? So I’m writing this, both as a refresher for myself and a reference for the next time I need to remember why OR-Sets exist or what WOOT stands for. (“WithOut Operational Transformation.” Yes, really.2)&lt;/p&gt;
    &lt;p&gt;So, grab a coffee.&lt;/p&gt;
    &lt;p&gt;Commutative. Replicated. Data Types.&lt;/p&gt;
    &lt;p&gt;In isolation, all of the words make sense. But when you look at the literature, it’s overwhelming:&lt;/p&gt;
    &lt;p&gt;Suddenly you’ve moved beyond the simple terms and start seeing things like G-Counters, PN-Counters, LWW-Sets, OR-Sets, 2P-Sets, RGAs, WOOTs, Logoots (wtf?)… Each with subtle tradeoffs. Each paper assuming you’ve read the previous five. It’s overwhelming.&lt;/p&gt;
    &lt;p&gt;This guide will hopefully cut through that. We’ll build intuition through interactive demos and concrete examples. You’ll see how merges actually work, watch conflicts resolve (or not resolve), and develop a feel for which CRDT fits which problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;What You Need to Know&lt;/head&gt;
    &lt;p&gt;You don’t need a PhD in distributed systems. If you understand:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why network failures happen&lt;/item&gt;
      &lt;item&gt;What “eventual consistency” means&lt;/item&gt;
      &lt;item&gt;Basic set operations (union, intersection)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;…you’re good.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Picture this: Alice and Bob are both editing a shared counter. Alice increments it. Bob increments it. The network is flaky, so neither sees the other’s change immediately. Later, they reconnect. What should the counter show?&lt;/p&gt;
    &lt;p&gt;Option 1: Consensus - Use Paxos/Raft to agree on who went first. Works great! Until the network partitions and half your users can’t write because they can’t reach a quorum. Not ideal for offline-first apps.&lt;/p&gt;
    &lt;p&gt;Option 2: Last-Write-Wins - Use timestamps. Whoever wrote last “wins.” Easy to implement! Except Bob’s increment gets completely erased if Alice’s timestamp was later. Data loss.&lt;/p&gt;
    &lt;p&gt;Option 3: CRDTs - Design the data structure so that merging is deterministic. Both increments survive. No coordination needed. No data loss. However, you have to be okay with some level of eventual consistency.&lt;/p&gt;
    &lt;p&gt;What’s the trick? How do CRDTs achieve this?&lt;/p&gt;
    &lt;p&gt;Roughly speaking, you are working with a CRDT if your merge operation is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;commutative (order doesn’t matter)&lt;/item&gt;
      &lt;item&gt;associative (grouping doesn’t matter)&lt;/item&gt;
      &lt;item&gt;idempotent (duplicates don’t matter)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you achieve these properties, then you can use your merge operation to ensure that replicas automatically converge to the same state.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Quick Detour: Lattices and Why They Matter&lt;/head&gt;
    &lt;p&gt;Before we dive into specific CRDTs, let’s build some intuition about what makes merging work. In CRDT literature, this is often referred to as a “lattice”.&lt;/p&gt;
    &lt;p&gt;Think about natural numbers with &lt;code&gt;max&lt;/code&gt; as the merge operation. If you have &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt;, taking &lt;code&gt;max(3, 5) = 5&lt;/code&gt; makes sense. It doesn’t matter if you compute &lt;code&gt;max(3, max(5, 7))&lt;/code&gt; or &lt;code&gt;max(max(3, 5), 7)&lt;/code&gt; - you get &lt;code&gt;7&lt;/code&gt; either way. And &lt;code&gt;max(5, 5) = 5&lt;/code&gt;, so duplicates are harmless.&lt;/p&gt;
    &lt;p&gt;This forms a partial order: some values are “greater than” others (&lt;code&gt;5 &amp;gt; 3&lt;/code&gt;), and there’s a join operation (&lt;code&gt;max&lt;/code&gt;) that gives you the least upper bound. The fancy math term is “join-semilattice,” but think of it as: a way to consistently pick “more recent” or “more complete” information.&lt;/p&gt;
    &lt;p&gt;Here’s the key insight: if your data structure’s states form a lattice, and updates only move “upward” in the ordering, then:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can apply updates in any order&lt;/item&gt;
      &lt;item&gt;You can apply the same update twice&lt;/item&gt;
      &lt;item&gt;Eventually, everyone agrees on the maximum state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider a counter where each replica tracks its own count: &lt;code&gt;{A: 3, B: 5}&lt;/code&gt;. The partial order is pointwise: &lt;code&gt;{A: 3, B: 5} ≥ {A: 2, B: 5}&lt;/code&gt; because each component is greater-or-equal. To join, take the &lt;code&gt;max&lt;/code&gt; of each component. This is exactly how the G-Counter CRDT works!&lt;/p&gt;
    &lt;p&gt;Why does this matter? Because if you can design your data structure so that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;States form a lattice (there’s always a sensible “join”)&lt;/item&gt;
      &lt;item&gt;Operations only move upward (you can’t un-increment a counter)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then merging becomes trivial: just take the join. No coordination needed. No conflicts possible. The math guarantees convergence.&lt;/p&gt;
    &lt;p&gt;Not all CRDTs fit this clean model (some need timestamps or version vectors to determine what’s “greater”), but the lattice intuition often guides the design. When you see &lt;code&gt;merge = unionWith max&lt;/code&gt; or &lt;code&gt;merge = union&lt;/code&gt;, you’re seeing some pure, beautiful math-brained lattice thinking.&lt;/p&gt;
    &lt;head rend="h3"&gt;State-Based vs Operation-Based&lt;/head&gt;
    &lt;p&gt;Moving on…&lt;/p&gt;
    &lt;p&gt;There are two fundamental approaches to CRDTs:&lt;/p&gt;
    &lt;p&gt;State-based CRDTs (CvRDTs) send the entire state to other replicas, which merge it with their local state using a join operation. The state must form a join-semilattice.3&lt;/p&gt;
    &lt;p&gt;Operation-based CRDTs (CmRDTs) send operations to other replicas, which apply them to their local state. Operations must be commutative when applied concurrently.4&lt;/p&gt;
    &lt;p&gt;In this guide, we’ll primarily discuss state-based CRDTs, as they’re conceptually simpler and the ideas translate naturally to the operation-based variants.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Core Laws&lt;/head&gt;
    &lt;p&gt;For a data structure to be a state-based CRDT, its merge operation must satisfy:&lt;/p&gt;
    &lt;p&gt;Associativity: &lt;code&gt;(a ⊔ b) ⊔ c = a ⊔ (b ⊔ c)&lt;/code&gt; where &lt;code&gt;⊔&lt;/code&gt; denotes the merge/join operation&lt;/p&gt;
    &lt;p&gt;Commutativity: &lt;code&gt;a ⊔ b = b ⊔ a&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Idempotence: &lt;code&gt;a ⊔ a = a&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;These properties ensure that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Merging in any order produces the same result&lt;/item&gt;
      &lt;item&gt;Re-receiving the same state is harmless&lt;/item&gt;
      &lt;item&gt;Partial merges can be composed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, the state must form a monotonic semilattice: updates only move “upward” in the partial order, never downward. This ensures convergence: once all updates have been delivered, all replicas reach the same state.&lt;/p&gt;
    &lt;p&gt;For the curious, The symbol ⊔ is called (square cup) or square union. I have no idea why regular union symbol isn’t used. Pointy-headed researchers, I guess.&lt;/p&gt;
    &lt;p&gt;Anyways, it’s commonly used to denote:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disjoint union - union of sets treated as disjoint&lt;/item&gt;
      &lt;item&gt;Join operation in lattice theory - the least upper bound (supremum) of two elements&lt;/item&gt;
      &lt;item&gt;Merge operation in CRDTs - combining two states by taking their least upper bound&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With these foundations in place, let’s explore the CRDT zoo.&lt;/p&gt;
    &lt;head rend="h2"&gt;G-Counter: Grow-Only Counter&lt;/head&gt;
    &lt;p&gt;Let’s start with the simplest CRDT: a counter that only goes up.5&lt;/p&gt;
    &lt;head rend="h3"&gt;The Idea&lt;/head&gt;
    &lt;p&gt;Instead of storing one global count, each replica tracks its own count. The total is the sum of all replica counts. When replicas merge, they take the &lt;code&gt;max&lt;/code&gt; of each replica’s count.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;max&lt;/code&gt;? Because counts only increase. If replica A shows that replica B has counted to 5, and replica B shows it’s counted to 3, we know A has seen newer information. Taking the max ensures we never lose increments.6&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation&lt;/head&gt;
    &lt;code&gt;type GCounter = Map ReplicaId Nat

value :: GCounter -&amp;gt; Nat
value counter = sum (Map.elems counter)

increment :: ReplicaId -&amp;gt; GCounter -&amp;gt; GCounter
increment r counter = Map.insertWith (+) r 1 counter

merge :: GCounter -&amp;gt; GCounter -&amp;gt; GCounter
merge = Map.unionWith max&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation forms a join-semilattice where the partial order is defined pointwise: &lt;code&gt;c1 ≤ c2&lt;/code&gt; if for all replicas &lt;code&gt;r&lt;/code&gt;, &lt;code&gt;c1[r] ≤ c2[r]&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Associative: &lt;code&gt;max&lt;/code&gt;is associative&lt;/item&gt;
      &lt;item&gt;Commutative: &lt;code&gt;max&lt;/code&gt;is commutative&lt;/item&gt;
      &lt;item&gt;Idempotent: &lt;code&gt;max(x, x) = x&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monotonic: Each replica’s count only increases&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Think of each replica as having its own tally marks. When replicas sync, they each adopt the maximum tally for each replica they’ve seen. Since tallies only grow, taking the maximum ensures we never lose increments.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;No metadata overhead beyond replica counts&lt;/item&gt;
      &lt;item&gt;Perfect for increment-only scenarios (page views, likes, etc.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot decrement&lt;/item&gt;
      &lt;item&gt;Size grows with number of replicas (though typically small)&lt;/item&gt;
      &lt;item&gt;No garbage collection (all replica counts retained forever)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use G-Counter when you need to count upward-only events in a distributed system: analytics counters, like counts, view counts, or any monotonically increasing metric. (If you need to decrement, well… keep reading.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Try it yourself! Increment counters on different replicas and see how the merge operation works:&lt;/p&gt;
    &lt;head rend="h3"&gt;G-Counter: Grow-Only Counter&lt;/head&gt;
    &lt;p&gt;Try it: Click "Increment" on different replicas multiple times without merging. Each replica tracks its own count independently. Then click the merge buttons (← A, ← B, ← C) to sync state. Watch how the merge uses &lt;code&gt;max&lt;/code&gt; - you never lose increments, and all replicas eventually reach the same total.&lt;/p&gt;
    &lt;head rend="h2"&gt;PN-Counter: Positive-Negative Counter&lt;/head&gt;
    &lt;p&gt;What if we need to decrement? Enter the PN-Counter. The trick is beautifully simple.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A PN-Counter contains two G-Counters: one for increments, one for decrements:&lt;/p&gt;
    &lt;code&gt;data PNCounter = PNCounter
  { increments :: GCounter
  , decrements :: GCounter
  }&lt;/code&gt;
    &lt;p&gt;The value is the difference:&lt;/p&gt;
    &lt;code&gt;value :: PNCounter -&amp;gt; Int
value (PNCounter inc dec) = value inc - value dec&lt;/code&gt;
    &lt;p&gt;What I love about PN-Counters as a broader insight for CRDTs is that you can often build more complex CRDTs by combining simpler ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Increment (on replica &lt;code&gt;r&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;increment r (PNCounter inc dec) = PNCounter (increment r inc) dec&lt;/code&gt;
    &lt;p&gt;Decrement (on replica &lt;code&gt;r&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;decrement r (PNCounter inc dec) = PNCounter inc (increment r dec)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (PNCounter i1 d1) (PNCounter i2 d2) =
  PNCounter (merge i1 i2) (merge d1 d2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Since both components are G-Counters with valid merge operations, the PN-Counter’s merge inherits their properties and forms a semilattice.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;A PN-Counter is like having two separate tally sheets: one for additions, one for subtractions. The current value is the difference between them. When replicas sync, they merge both sheets independently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports both increment and decrement&lt;/item&gt;
      &lt;item&gt;Deterministic convergence&lt;/item&gt;
      &lt;item&gt;Simple to understand and implement&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Double the space of a G-Counter&lt;/item&gt;
      &lt;item&gt;Can never truly garbage collect old replica entries&lt;/item&gt;
      &lt;item&gt;No bound on the value range (can overflow)&lt;/item&gt;
      &lt;item&gt;Cannot reset the counter atomically&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use PN-Counter for any metric that can increase or decrease over time: inventory counts, resource pools, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Variants&lt;/head&gt;
    &lt;p&gt;Some implementations use a single map with integer values instead of two separate maps, but the principle is the same.&lt;/p&gt;
    &lt;head rend="h2"&gt;G-Set: Grow-Only Set&lt;/head&gt;
    &lt;p&gt;Moving from numbers to collections, we consider the simplest CRDT set.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A G-Set is simply a set that supports addition but not removal:&lt;/p&gt;
    &lt;code&gt;type GSet a = Set a&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add:&lt;/p&gt;
    &lt;code&gt;add :: Ord a =&amp;gt; a -&amp;gt; GSet a -&amp;gt; GSet a
add = insert&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge :: Ord a =&amp;gt; GSet a -&amp;gt; GSet a -&amp;gt; GSet a
merge = union&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Sets with union form a semilattice under the subset relation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Associative: Set union is associative&lt;/item&gt;
      &lt;item&gt;Commutative: Set union is commutative&lt;/item&gt;
      &lt;item&gt;Idempotent: &lt;code&gt;A ∪ A = A&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monotonic: Sets only grow&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Once an element is added to any replica, it will eventually appear in all replicas. There’s no way to remove it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimal overhead (just the set elements)&lt;/item&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;Familiar set semantics&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot remove elements&lt;/item&gt;
      &lt;item&gt;Grows unbounded&lt;/item&gt;
      &lt;item&gt;No garbage collection&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use G-Set for append-only collections where removal is never needed: event logs, collected tags, or immutable registries.&lt;/p&gt;
    &lt;head rend="h2"&gt;2P-Set: Two-Phase Set&lt;/head&gt;
    &lt;p&gt;The natural extension of G-Set to support removal.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A 2P-Set (Two-Phase Set) contains two G-Sets: one for added elements, one for removed elements:&lt;/p&gt;
    &lt;code&gt;data TwoPhaseSet a = TwoPhaseSet
  { added :: GSet a
  , removed :: GSet a
  }&lt;/code&gt;
    &lt;p&gt;An element is in the set if it’s been added but not removed:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; TwoPhaseSet a -&amp;gt; Bool
member x (TwoPhaseSet a r) = x `Set.member` a &amp;amp;&amp;amp; x `Set.notMember` r&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add:&lt;/p&gt;
    &lt;code&gt;add x (TwoPhaseSet a r) = TwoPhaseSet (insert x a) r&lt;/code&gt;
    &lt;p&gt;Remove:&lt;/p&gt;
    &lt;code&gt;remove x (TwoPhaseSet a r) = TwoPhaseSet a (insert x r)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (TwoPhaseSet a1 r1) (TwoPhaseSet a2 r2) =
  TwoPhaseSet (union a1 a2) (union r1 r2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Bias toward removal: If an element appears in the removed set, it’s not in the 2P-Set, even if it’s also in the added set.&lt;/p&gt;
    &lt;p&gt;Once removed, forever removed: Once an element is removed at any replica, it will eventually be removed from all replicas and cannot be re-added.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;The 2P-Set is like marking items in a ledger: you can add entries and you can cross them out, but you can’t un-cross-out an entry. Once something is crossed out (removed), that decision is permanent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports both add and remove&lt;/item&gt;
      &lt;item&gt;Simple to understand&lt;/item&gt;
      &lt;item&gt;Deterministic convergence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cannot re-add removed elements (the “2P” means two-phase: add, then remove, no going back)&lt;/item&gt;
      &lt;item&gt;Both sets grow monotonically (removed items never truly disappear)&lt;/item&gt;
      &lt;item&gt;No garbage collection&lt;/item&gt;
      &lt;item&gt;Not suitable for scenarios where elements might be removed and re-added&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use 2P-Set when elements have a lifecycle of “not present → added → removed” and never need to be re-added: task completion tracking, tombstones, or revoked permissions.&lt;/p&gt;
    &lt;head rend="h2"&gt;LWW-Element-Set: Last-Write-Wins Element Set&lt;/head&gt;
    &lt;p&gt;What if we want to re-add elements? We need timestamps.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An LWW-Element-Set associates each element with a timestamp for additions and removals:&lt;/p&gt;
    &lt;code&gt;data LWWSet a = LWWSet
  { addTimes :: Map a Timestamp
  , removeTimes :: Map a Timestamp
  }&lt;/code&gt;
    &lt;p&gt;An element is in the set if its most recent operation was an add:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; LWWSet a -&amp;gt; Bool
member x (LWWSet adds removes) =
  case (Map.lookup x adds, Map.lookup x removes) of
    (Just t1, Just t2) -&amp;gt; t1 &amp;gt; t2
    (Just _, Nothing) -&amp;gt; True
    _ -&amp;gt; False&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;add x t (LWWSet adds removes) = LWWSet (insert x t adds) removes&lt;/code&gt;
    &lt;p&gt;Remove (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;remove x t (LWWSet adds removes) = LWWSet adds (insert x t removes)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge (LWWSet a1 r1) (LWWSet a2 r2) =
  LWWSet (unionWith max a1 a2) (unionWith max r1 r2)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation is well-defined because &lt;code&gt;max&lt;/code&gt; over timestamps forms a semilattice.&lt;/p&gt;
    &lt;p&gt;Timestamp monotonicity: Each replica must generate increasing timestamps (typically using wall clocks plus replica IDs as tiebreakers).&lt;/p&gt;
    &lt;p&gt;Bias: We must decide what happens when add and remove timestamps are equal. Common choices: bias toward add, or bias toward remove.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Each element has a timestamp for when it was last added and when it was last removed. The most recent operation wins. When merging, we take the latest add timestamp and latest remove timestamp we’ve seen.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports add, remove, and re-add&lt;/item&gt;
      &lt;item&gt;Can garbage collect old timestamps (carefully)&lt;/item&gt;
      &lt;item&gt;Natural semantics for many applications&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires synchronized clocks (or logical clocks with careful replica ID handling)&lt;/item&gt;
      &lt;item&gt;Concurrent add/remove on the same element may surprise users (one operation is discarded)&lt;/item&gt;
      &lt;item&gt;Loses information: if two users concurrently add the same element, only one timestamp survives&lt;/item&gt;
      &lt;item&gt;The “last write wins” semantics mean data loss is possible&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use LWW-Element-Set when you need a set with add/remove/re-add capability and can tolerate last-write-wins semantics: user preferences, feature flags, or cached collections where perfect consistency isn’t critical.&lt;/p&gt;
    &lt;head rend="h3"&gt;Clock Considerations&lt;/head&gt;
    &lt;p&gt;The biggest pitfall of LWW-Element-Set is clock skew. If replica A’s clock is ahead of replica B’s, then A’s operations will always “win” over B’s, even if B’s operations happened later in real time. Solutions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use hybrid logical clocks (HLC) instead of wall clocks&lt;/item&gt;
      &lt;item&gt;Use replica IDs as tiebreakers (e.g., timestamps are &lt;code&gt;(wall_time, replica_id)&lt;/code&gt;pairs)&lt;/item&gt;
      &lt;item&gt;Accept the inconsistency as a tradeoff&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;OR-Set: Observed-Remove Set&lt;/head&gt;
    &lt;p&gt;The most sophisticated set CRDT, solving the re-add problem without LWW semantics.7&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An OR-Set (Observed-Remove Set) associates each element with a set of unique tags:&lt;/p&gt;
    &lt;code&gt;type ORSet a = Map a (Set Tag)&lt;/code&gt;
    &lt;p&gt;Tags are unique identifiers generated when adding an element (e.g., &lt;code&gt;(replica_id, sequence_number)&lt;/code&gt; pairs).&lt;/p&gt;
    &lt;p&gt;An element is in the set if it has any tags:&lt;/p&gt;
    &lt;code&gt;member :: Ord a =&amp;gt; a -&amp;gt; ORSet a -&amp;gt; Bool
member x set = case Map.lookup x set of
  Just tags -&amp;gt; not (Set.null tags)
  Nothing -&amp;gt; False&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add (with fresh tag &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;add x t set = Map.insertWith union x (singleton t) set&lt;/code&gt;
    &lt;p&gt;Remove (with observed tags &lt;code&gt;ts&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;remove x ts set = Map.update (\\tags -&amp;gt;
  let remaining = tags \\ ts
  in if Set.null remaining then Nothing else Just remaining) x set&lt;/code&gt;
    &lt;p&gt;The critical insight: removal removes only the tags that were observed. If concurrent adds create new tags, those survive.&lt;/p&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge = Map.unionWith union&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation forms a semilattice where &lt;code&gt;s1 ≤ s2&lt;/code&gt; if for all elements &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;s1[x] ⊆ s2[x]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Add wins: If an add and remove happen concurrently (the add’s tag wasn’t observed by the remove), the add wins.&lt;/p&gt;
    &lt;p&gt;Causal consistency: You can only remove tags you’ve observed (seen in a prior state).&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Think of each addition as dropping a unique token into a bucket for that element. Removal takes specific tokens out of the bucket. If someone concurrently added a new token you haven’t seen, your removal doesn’t affect it. An element is present if its bucket has any tokens.&lt;/p&gt;
    &lt;p&gt;This gives us add-wins semantics: concurrent add and remove means the element stays in the set (because the remove didn’t observe the add’s tag).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports add, remove, and re-add with intuitive semantics&lt;/item&gt;
      &lt;item&gt;No timestamp requirements&lt;/item&gt;
      &lt;item&gt;Add-wins semantics are often more desirable than LWW&lt;/item&gt;
      &lt;item&gt;Properly handles concurrent operations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Larger space overhead (tags per element)&lt;/item&gt;
      &lt;item&gt;More complex implementation&lt;/item&gt;
      &lt;item&gt;Need garbage collection strategy for tags&lt;/item&gt;
      &lt;item&gt;Remove operations need to carry the observed tags (larger messages)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use OR-Set when you need a set with full add/remove/re-add support and can’t tolerate LWW’s data loss: collaborative editing, shopping carts, or any scenario where concurrent adds should be preserved.&lt;/p&gt;
    &lt;head rend="h3"&gt;Garbage Collection&lt;/head&gt;
    &lt;p&gt;Old tags can accumulate. Strategies include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tombstones: Keep removed tags for a grace period before discarding&lt;/item&gt;
      &lt;item&gt;Version vectors: Use causal history to determine which tags are safe to remove&lt;/item&gt;
      &lt;item&gt;Bounded tags: Limit the number of tags per element, using LWW within that bound&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Experience the add-wins semantics of OR-Set:&lt;/p&gt;
    &lt;head rend="h3"&gt;OR-Set: Observed-Remove Set&lt;/head&gt;
    &lt;p&gt;Try it: Add the same element (e.g., "apple") on both replicas without merging first. Each creates a unique tag. Then remove it from one replica and immediately merge. Notice the element stays because the remove only deleted the tags it could see - the other replica's tag survives. This is add-wins semantics.&lt;/p&gt;
    &lt;head rend="h2"&gt;LWW-Register: Last-Write-Wins Register&lt;/head&gt;
    &lt;p&gt;Registers store single values. The simplest register CRDT uses last-write-wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An LWW-Register pairs a value with a timestamp:&lt;/p&gt;
    &lt;code&gt;data LWWRegister a = LWWRegister
  { value :: a
  , timestamp :: Timestamp
  }&lt;/code&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Write (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;write x t _ = LWWRegister x t&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge r1@(LWWRegister v1 t1) r2@(LWWRegister v2 t2)
  | t1 &amp;gt; t2 = r1
  | t1 &amp;lt; t2 = r2
  | otherwise = r1  -- tiebreaker (could use replica ID)&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge operation is a semilattice with partial order defined by timestamps.&lt;/p&gt;
    &lt;p&gt;One value wins: When concurrent writes occur, only one survives (the one with the higher timestamp).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple and efficient&lt;/item&gt;
      &lt;item&gt;Small size (just value + timestamp)&lt;/item&gt;
      &lt;item&gt;Easy to understand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Loses concurrent updates&lt;/item&gt;
      &lt;item&gt;Requires clock synchronization&lt;/item&gt;
      &lt;item&gt;No way to detect or recover lost updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use LWW-Register for single-value cells where you can tolerate lost updates: user profile fields, configuration settings, or cached computed values.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;See data loss in action with last-write-wins semantics:&lt;/p&gt;
    &lt;head rend="h3"&gt;LWW-Register: Last-Write-Wins Register&lt;/head&gt;
    &lt;p&gt;⚠️ Data Loss Alert: Write different values on replica A and B without merging. Each gets its own timestamp. Then merge A into B (or vice versa). Watch one value completely disappear! The higher timestamp wins. This is why LWW is dangerous for important data - concurrent writes = lost data.&lt;/p&gt;
    &lt;head rend="h2"&gt;MV-Register: Multi-Value Register&lt;/head&gt;
    &lt;p&gt;What if we want to preserve concurrent writes instead of discarding them?&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An MV-Register stores a set of value-timestamp pairs:&lt;/p&gt;
    &lt;code&gt;type MVRegister a = Set (a, Timestamp)&lt;/code&gt;
    &lt;p&gt;When reading, you get back all concurrently written values (values with incomparable timestamps).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Write (with timestamp &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;write x t reg = Set.singleton (x, t)&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge reg1 reg2 =
  let combined = union reg1 reg2
      maxTime = maximum (map snd combined)
      concurrent = filter (\\(_, t) -&amp;gt; t == maxTime) combined
  in fromList concurrent&lt;/code&gt;
    &lt;p&gt;More sophisticated: keep values with causally concurrent timestamps, not just the maximum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The merge preserves all values that might be “current” from different replicas’ perspectives.&lt;/p&gt;
    &lt;p&gt;Concurrent values preserved: If two writes happened concurrently, both values appear until a subsequent write supersedes them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No data loss on concurrent updates&lt;/item&gt;
      &lt;item&gt;Application can detect and resolve conflicts&lt;/item&gt;
      &lt;item&gt;More information available for conflict resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Returns sets of values, not single values&lt;/item&gt;
      &lt;item&gt;Application must handle conflict resolution&lt;/item&gt;
      &lt;item&gt;More complex semantics&lt;/item&gt;
      &lt;item&gt;Slightly larger space overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use MV-Register when concurrent updates must be detected and resolved by application logic: collaborative text fields, conflict-aware configuration, or any scenario where losing an update is unacceptable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conflict Resolution&lt;/head&gt;
    &lt;p&gt;When reading an MV-Register returns multiple values, the application must resolve the conflict. Strategies include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Present all values to the user (collaborative editing)&lt;/item&gt;
      &lt;item&gt;Apply a deterministic merge function (e.g., union of tags)&lt;/item&gt;
      &lt;item&gt;Use application-specific semantics (e.g., prefer non-empty values)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;OR-Map: Observed-Remove Map&lt;/head&gt;
    &lt;p&gt;Maps are common. How do we make them CRDTs?&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;An OR-Map is a map where each key is associated with an OR-Set of tagged values:&lt;/p&gt;
    &lt;code&gt;type ORMap k v = Map k (ORSet (v, Tag))&lt;/code&gt;
    &lt;p&gt;Alternatively, implement as a composition of OR-Set (for keys) with per-key CRDTs (for values).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Put (with fresh tag &lt;code&gt;t&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;put k v t map = Map.insertWith union k (singleton (v, t)) map&lt;/code&gt;
    &lt;p&gt;Remove key:&lt;/p&gt;
    &lt;code&gt;removeKey k map = Map.delete k map&lt;/code&gt;
    &lt;p&gt;Remove specific value:&lt;/p&gt;
    &lt;code&gt;removeValue k v tags map = -- similar to OR-Set remove&lt;/code&gt;
    &lt;p&gt;Merge:&lt;/p&gt;
    &lt;code&gt;merge = Map.unionWith (OR-Set merge)&lt;/code&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full map operations with CRDT semantics&lt;/item&gt;
      &lt;item&gt;Can nest other CRDTs as values&lt;/item&gt;
      &lt;item&gt;Compositional&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex metadata management&lt;/item&gt;
      &lt;item&gt;Garbage collection challenges&lt;/item&gt;
      &lt;item&gt;Larger overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use OR-Map when you need a distributed key-value store with CRDT guarantees: collaborative JSON documents, distributed configuration, or nested data structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;RGA: Replicated Growable Array&lt;/head&gt;
    &lt;p&gt;Sequences are hard. How do you handle insertions in the middle when replicas disagree on positions?8&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;RGA (Replicated Growable Array) assigns each element a unique ID and stores the sequence as a tree structure based on insertion order and causality.9&lt;/p&gt;
    &lt;code&gt;data RGA a = RGA
  { elements :: Map UID (a, UID)  -- element ID -&amp;gt; (value, parent ID)
  , root :: UID
  }&lt;/code&gt;
    &lt;p&gt;Each element knows its “parent” (the element after which it was inserted).&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (after element with ID &lt;code&gt;p&lt;/code&gt;, with fresh ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert p x uid rga = -- complex tree manipulation&lt;/code&gt;
    &lt;p&gt;Delete (element with ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;delete uid rga = -- mark as tombstone, don't actually remove&lt;/code&gt;
    &lt;p&gt;Merge: Merge trees by reconciling insertion orders.&lt;/p&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;The challenge is that positional indices change as elements are inserted/removed. RGA solves this by using immutable IDs and causal relationships.&lt;/p&gt;
    &lt;p&gt;Causal order preserved: If element A was inserted before element B on the same replica, that relationship is preserved globally.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intuition&lt;/head&gt;
    &lt;p&gt;Instead of “insert at position 5,” you say “insert after element X.” Since X has a unique ID, this instruction is unambiguous even when other replicas are concurrently inserting elsewhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports arbitrary insertions and deletions&lt;/item&gt;
      &lt;item&gt;Eventual consistency for sequences&lt;/item&gt;
      &lt;item&gt;Handles concurrent edits intuitively&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex implementation&lt;/item&gt;
      &lt;item&gt;Large overhead (IDs, tombstones)&lt;/item&gt;
      &lt;item&gt;No compaction without coordination&lt;/item&gt;
      &lt;item&gt;Performance degrades with many deletes (tombstones accumulate)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use RGA for collaborative text editing or any replicated sequence where insertions at arbitrary positions must be supported: shared lists, collaborative documents, or distributed queues.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternatives&lt;/head&gt;
    &lt;p&gt;Other sequence CRDTs include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WOOT (Without Operational Transformation): similar idea, different structure&lt;/item&gt;
      &lt;item&gt;Logoot: uses position identifiers between elements&lt;/item&gt;
      &lt;item&gt;LSEQ: adaptive allocation of position identifiers&lt;/item&gt;
      &lt;item&gt;YATA: optimizations for text editing workloads10&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each has different tradeoffs in space overhead, time complexity, and behavior under specific edit patterns.&lt;/p&gt;
    &lt;head rend="h2"&gt;Causal CRDTs: Adding Causality&lt;/head&gt;
    &lt;p&gt;Advanced CRDTs incorporate causal tracking using version vectors or similar mechanisms. This enables more sophisticated semantics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Version Vectors&lt;/head&gt;
    &lt;p&gt;A version vector tracks the logical clock for each replica:11&lt;/p&gt;
    &lt;code&gt;type VersionVector = Map ReplicaId Nat&lt;/code&gt;
    &lt;p&gt;Operations include the version vector, allowing replicas to determine causality: whether one operation happened-before another, or whether they were concurrent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Causal Register&lt;/head&gt;
    &lt;p&gt;Pairs an MV-Register with version vectors:&lt;/p&gt;
    &lt;code&gt;data CausalRegister a = CausalRegister
  { values :: Map VersionVector a
  }&lt;/code&gt;
    &lt;p&gt;Only keeps values with concurrent version vectors, discarding those that are causally dominated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advantages of Causality&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More precise conflict detection (concurrent vs. causally ordered)&lt;/item&gt;
      &lt;item&gt;Better garbage collection (can discard superseded operations)&lt;/item&gt;
      &lt;item&gt;Foundation for stronger consistency guarantees&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Disadvantages of Causality&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Larger metadata (version vectors grow with number of replicas)&lt;/item&gt;
      &lt;item&gt;More complex logic&lt;/item&gt;
      &lt;item&gt;Still doesn’t eliminate conflicts, just detects them more precisely&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Interactive Demo&lt;/head&gt;
    &lt;p&gt;Explore how version vectors track causality:&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Clocks: Tracking Causality&lt;/head&gt;
    &lt;p&gt;Try it: Click "Perform Operation" on replica A, then on replica B (without clicking "Receive from"). These operations don't know about each other - they're concurrent! Now click two operations in the history to compare their vector clocks. Concurrent operations show as yellow warnings - they need special conflict resolution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Delta CRDTs: Efficient State Transmission&lt;/head&gt;
    &lt;p&gt;State-based CRDTs have a problem: sending the entire state on every sync is wasteful. Delta CRDTs solve this.12&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Consider a G-Counter with 1000 replicas. If replica A increments its count, must it send all 1000 entries to replica B? That’s inefficient: only one entry changed!&lt;/p&gt;
    &lt;head rend="h3"&gt;The Solution&lt;/head&gt;
    &lt;p&gt;Instead of sending full state, send only the delta: the part of the state that changed since the last sync.&lt;/p&gt;
    &lt;code&gt;type Delta a = a  -- same type as state, but represents only changes

merge :: CRDT a =&amp;gt; a -&amp;gt; Delta a -&amp;gt; a&lt;/code&gt;
    &lt;p&gt;For G-Counter, a delta might be just &lt;code&gt;{A: 1}&lt;/code&gt; instead of the full map.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;A Delta CRDT extends a state-based CRDT with delta operations:&lt;/p&gt;
    &lt;code&gt;data DeltaCRDT a = DeltaCRDT
  { state :: a
  , lastSent :: Map ReplicaId a  -- track what we've sent to each replica
  }

delta :: ReplicaId -&amp;gt; DeltaCRDT a -&amp;gt; Delta a
delta replica crdt = state crdt `since` lastSent[replica]&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Delta CRDTs must satisfy the same semilattice properties as regular state-based CRDTs, plus:&lt;/p&gt;
    &lt;p&gt;Delta-state equivalence: Merging deltas incrementally must be equivalent to merging full states.&lt;/p&gt;
    &lt;p&gt;Delta composition: Deltas can be composed: &lt;code&gt;delta1 ⊔ delta2&lt;/code&gt; is itself a valid delta.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dramatically reduced bandwidth (send only changes)&lt;/item&gt;
      &lt;item&gt;Same convergence guarantees as state-based CRDTs&lt;/item&gt;
      &lt;item&gt;Can batch multiple deltas together&lt;/item&gt;
      &lt;item&gt;Easier to implement than operation-based CRDTs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Must track what has been sent to each replica&lt;/item&gt;
      &lt;item&gt;Slightly more complex than pure state-based&lt;/item&gt;
      &lt;item&gt;Still need full state for new replicas joining&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Delta CRDTs when network bandwidth is a concern or state size is large. Most production CRDT systems use delta-state internally (Riak, Automerge). If you’re implementing your own CRDT system from scratch, start with deltas. Your future self will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example: Delta G-Counter&lt;/head&gt;
    &lt;code&gt;increment :: ReplicaId -&amp;gt; GCounter -&amp;gt; (GCounter, Delta GCounter)
increment r counter =
  let newCounter = insertWith (+) r 1 counter
      delta = singleton r 1  -- only the change!
  in (newCounter, delta)&lt;/code&gt;
    &lt;p&gt;The delta is just the single updated entry, not the entire counter.&lt;/p&gt;
    &lt;head rend="h2"&gt;WOOT: Without Operational Transformation&lt;/head&gt;
    &lt;p&gt;WOOT is a sequence CRDT that predates RGA, with different design choices.2&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;WOOT represents a sequence as a set of character objects with unique IDs, where each character stores references to its previous and next characters:&lt;/p&gt;
    &lt;code&gt;data WChar a = WChar
  { charId :: UID
  , value :: a
  , prevId :: UID
  , nextId :: UID
  , isVisible :: Bool
  }

type WOOT a = Set (WChar a)&lt;/code&gt;
    &lt;head rend="h3"&gt;Key Insight&lt;/head&gt;
    &lt;p&gt;Instead of storing a linear sequence, WOOT stores constraints: “this character comes after X and before Y.” When multiple characters claim to be between X and Y, a deterministic ordering (based on UID) resolves the conflict.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (after character with ID &lt;code&gt;prev&lt;/code&gt;, before character with ID &lt;code&gt;next&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert :: a -&amp;gt; UID -&amp;gt; UID -&amp;gt; UID -&amp;gt; WOOT a -&amp;gt; WOOT a
insert val uid prev next woot =
  insert (WChar uid val prev next True) woot&lt;/code&gt;
    &lt;p&gt;Delete (character with ID &lt;code&gt;uid&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;delete :: UID -&amp;gt; WOOT a -&amp;gt; WOOT a
delete uid woot = -- mark character as invisible, don't remove&lt;/code&gt;
    &lt;head rend="h3"&gt;Linearization&lt;/head&gt;
    &lt;p&gt;To read the sequence, perform a topological sort respecting the prev/next constraints, filtering out invisible characters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strong eventual consistency&lt;/item&gt;
      &lt;item&gt;No need for causal delivery (constraints handle ordering)&lt;/item&gt;
      &lt;item&gt;Intuitive model (characters reference neighbors)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tombstones accumulate (deleted characters remain)&lt;/item&gt;
      &lt;item&gt;Linearization has O(n²) worst case&lt;/item&gt;
      &lt;item&gt;More complex than RGA&lt;/item&gt;
      &lt;item&gt;UIDs must be globally unique and ordered&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Comparison with RGA&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RGA: Uses a tree structure, parent-child relationships&lt;/item&gt;
      &lt;item&gt;WOOT: Uses bidirectional constraints, more flexible but slower linearization&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;WOOT is primarily of historical interest. Modern implementations prefer RGA or YATA for better performance. But it’s a neat design, and the name alone makes it worth knowing about.&lt;/p&gt;
    &lt;head rend="h2"&gt;Logoot: Scalable Position Identifiers&lt;/head&gt;
    &lt;p&gt;Logoot takes a different approach to sequences: instead of linking elements, assign each element a position in a dense order.13&lt;/p&gt;
    &lt;head rend="h3"&gt;Definition&lt;/head&gt;
    &lt;p&gt;Each element has a position identifier that is a sequence of (digit, replicaId) pairs:&lt;/p&gt;
    &lt;code&gt;type Position = [(Int, ReplicaId)]

data LogootElement a = LogootElement
  { position :: Position
  , value :: a
  , isDeleted :: Bool
  }

type Logoot a = Set (LogootElement a)&lt;/code&gt;
    &lt;p&gt;Positions are ordered lexicographically.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Insight&lt;/head&gt;
    &lt;p&gt;Positions form a dense order: between any two positions, you can always allocate a new position. To insert between positions &lt;code&gt;p1&lt;/code&gt; and &lt;code&gt;p2&lt;/code&gt;, generate a new position &lt;code&gt;p&lt;/code&gt; such that &lt;code&gt;p1 &amp;lt; p &amp;lt; p2&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Insert (between positions &lt;code&gt;before&lt;/code&gt; and &lt;code&gt;after&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;insert :: a -&amp;gt; Position -&amp;gt; Position -&amp;gt; Logoot a -&amp;gt; Logoot a
insert val before after logoot =
  let newPos = allocatePosition before after currentReplicaId
      element = LogootElement newPos val False
  in insert element logoot&lt;/code&gt;
    &lt;p&gt;Position Allocation:&lt;/p&gt;
    &lt;code&gt;allocatePosition :: Position -&amp;gt; Position -&amp;gt; ReplicaId -&amp;gt; Position
allocatePosition before after replicaId =
  -- Find a position between before and after
  -- Use replicaId as tiebreaker for deterministic ordering&lt;/code&gt;
    &lt;p&gt;Delete:&lt;/p&gt;
    &lt;code&gt;delete :: Position -&amp;gt; Logoot a -&amp;gt; Logoot a
delete pos logoot = -- mark element at pos as deleted&lt;/code&gt;
    &lt;head rend="h3"&gt;Laws and Invariants&lt;/head&gt;
    &lt;p&gt;Deterministic ordering: Elements are always ordered by their positions.&lt;/p&gt;
    &lt;p&gt;Unique positions: Each insert generates a unique position (using replica ID in the position).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No need to reference other elements by ID&lt;/item&gt;
      &lt;item&gt;Simpler merge than WOOT&lt;/item&gt;
      &lt;item&gt;Positions are self-describing (no need to look up IDs)&lt;/item&gt;
      &lt;item&gt;Can insert without knowing the full document structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Position identifiers grow over time (especially with many edits)&lt;/item&gt;
      &lt;item&gt;Still accumulates tombstones&lt;/item&gt;
      &lt;item&gt;Position allocation algorithm is complex&lt;/item&gt;
      &lt;item&gt;Pathological cases where positions become very long&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;LSEQ: Adaptive Positions&lt;/head&gt;
    &lt;p&gt;LSEQ improves on Logoot by using an adaptive allocation strategy. Instead of always allocating positions the same way, LSEQ alternates between strategies to keep positions shorter on average.14&lt;/p&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Logoot/LSEQ when you need a sequence CRDT and want simpler semantics than RGA/WOOT. The tradeoff is position identifier growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tree CRDTs: Hierarchical Data&lt;/head&gt;
    &lt;p&gt;Extending CRDTs to trees is challenging because parent-child relationships must be maintained consistently.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;Trees have structural constraints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each node has exactly one parent (except root)&lt;/item&gt;
      &lt;item&gt;No cycles allowed&lt;/item&gt;
      &lt;item&gt;Moving a node changes parent-child relationships&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How do we handle concurrent operations like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two replicas move the same node to different parents?&lt;/item&gt;
      &lt;item&gt;One replica moves node A under node B while another moves B under A?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Approaches&lt;/head&gt;
    &lt;p&gt;OR-Tree: Combine OR-Set with parent pointers, using conflict resolution strategies when multiple parents are observed.&lt;/p&gt;
    &lt;p&gt;CRDT-Tree: Use causal ordering to determine which move operations take precedence.&lt;/p&gt;
    &lt;p&gt;Log-based Trees: Store operations in a replicated log and rebuild tree structure on read.&lt;/p&gt;
    &lt;head rend="h3"&gt;OR-Tree Definition&lt;/head&gt;
    &lt;code&gt;type ORTree a = Map NodeId (ORSet ParentId, a)&lt;/code&gt;
    &lt;p&gt;Each node stores an OR-Set of potential parents. Conflict resolution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Last-write-wins: Use timestamps to pick winning parent&lt;/item&gt;
      &lt;item&gt;First-wins: The first parent observed wins&lt;/item&gt;
      &lt;item&gt;Merge: Allow nodes to have multiple parents temporarily, application resolves&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can represent hierarchical data distributedly&lt;/item&gt;
      &lt;item&gt;Handles concurrent structural changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complex conflict resolution strategies&lt;/item&gt;
      &lt;item&gt;Must prevent cycles (may require rejecting some operations)&lt;/item&gt;
      &lt;item&gt;Moving subtrees is complicated&lt;/item&gt;
      &lt;item&gt;High metadata overhead&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;When to Use&lt;/head&gt;
    &lt;p&gt;Use Tree CRDTs for file systems, organizational charts, or document outlines where the hierarchy must be replicated. Be prepared for complexity in handling concurrent structural changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternatives&lt;/head&gt;
    &lt;p&gt;For many use cases, an OR-Map with explicit parent fields is simpler than a full Tree CRDT, even if it doesn’t enforce tree constraints at the CRDT level.&lt;/p&gt;
    &lt;head rend="h2"&gt;Observed-Remove Shopping Cart&lt;/head&gt;
    &lt;p&gt;A practical example combining multiple CRDT concepts.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Domain&lt;/head&gt;
    &lt;p&gt;An e-commerce shopping cart must support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add product to cart&lt;/item&gt;
      &lt;item&gt;Remove product from cart&lt;/item&gt;
      &lt;item&gt;Change quantity&lt;/item&gt;
      &lt;item&gt;Work offline and sync later&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Naive Approach: LWW Map&lt;/head&gt;
    &lt;code&gt;type CartLWW = Map ProductId (Int, Timestamp)&lt;/code&gt;
    &lt;p&gt;Problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Concurrent additions of the same product (one wins)&lt;/item&gt;
      &lt;item&gt;Remove on one device, add on another (one wins, data loss)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Better: OR-Set + PN-Counter&lt;/head&gt;
    &lt;code&gt;type ShoppingCart = Map ProductId PNCounter&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use OR-Set semantics for which products are in cart&lt;/item&gt;
      &lt;item&gt;Use PN-Counter for quantities&lt;/item&gt;
      &lt;item&gt;Add-wins semantics for products (if concurrently added and removed, item stays)&lt;/item&gt;
      &lt;item&gt;Quantities merge correctly (concurrent +1 and +2 becomes +3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Operations&lt;/head&gt;
    &lt;p&gt;Add product with quantity:&lt;/p&gt;
    &lt;code&gt;addToCart :: ProductId -&amp;gt; Int -&amp;gt; ReplicaId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
addToCart pid qty replica cart =
  let counter = lookupOr emptyCounter pid cart
      incremented = incrementN replica qty counter
  in insert pid incremented cart&lt;/code&gt;
    &lt;p&gt;Remove product:&lt;/p&gt;
    &lt;code&gt;removeFromCart :: ProductId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
removeFromCart pid cart = delete pid cart&lt;/code&gt;
    &lt;p&gt;Change quantity:&lt;/p&gt;
    &lt;code&gt;changeQuantity :: ProductId -&amp;gt; Int -&amp;gt; ReplicaId -&amp;gt; ShoppingCart -&amp;gt; ShoppingCart
changeQuantity pid delta replica cart =
  let counter = lookupOr emptyCounter pid cart
      updated = if delta &amp;gt; 0
                then incrementN replica delta counter
                else decrementN replica (-delta) counter
  in insert pid updated cart&lt;/code&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Handles all operations correctly&lt;/item&gt;
      &lt;item&gt;No data loss on concurrent modifications&lt;/item&gt;
      &lt;item&gt;Intuitive semantics for users&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PN-Counters can go negative (need validation)&lt;/item&gt;
      &lt;item&gt;Must track all replicas (for PN-Counter)&lt;/item&gt;
      &lt;item&gt;Slightly more overhead than simple LWW&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This example shows how combining basic CRDTs creates sophisticated application-level data structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Practical Considerations&lt;/head&gt;
    &lt;head rend="h3"&gt;Choosing a CRDT&lt;/head&gt;
    &lt;p&gt;The choice of CRDT depends on your requirements:&lt;/p&gt;
    &lt;p&gt;Do you need only additions? Use G-Counter or G-Set.&lt;/p&gt;
    &lt;p&gt;Do you need removals but not re-additions? Use 2P-Set.&lt;/p&gt;
    &lt;p&gt;Can you tolerate last-write-wins? Use LWW-Element-Set or LWW-Register.&lt;/p&gt;
    &lt;p&gt;Do you need to preserve concurrent operations? Use OR-Set or MV-Register.&lt;/p&gt;
    &lt;p&gt;Do you have sequences? Use RGA or similar sequence CRDT.&lt;/p&gt;
    &lt;p&gt;Do you need nested structures? Use OR-Map with nested CRDTs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Garbage Collection&lt;/head&gt;
    &lt;p&gt;Garbage collection is one of the most challenging practical problems with CRDTs. The fundamental tension: CRDTs achieve convergence by monotonically accumulating information, but production systems can’t grow unbounded forever.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Problem in Detail&lt;/head&gt;
    &lt;p&gt;Consider an OR-Set used for a collaborative todo list. Each time someone adds a task and removes it, we accumulate:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A unique tag for the addition (never removed)&lt;/item&gt;
      &lt;item&gt;A tombstone tracking the removal (never removed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After 10,000 tasks have been created and completed, our “empty” todo list still contains 10,000 tags worth of metadata. In a G-Counter tracking page views, we keep a separate count for every replica that has ever incremented the counter—even if that replica hasn’t been online in years. For sequence CRDTs like RGA or WOOT, every deleted character becomes a tombstone that must be retained indefinitely. A 1000-character document that’s been heavily edited might internally contain 50,000 tombstones.&lt;/p&gt;
    &lt;p&gt;The core issue: CRDTs converge by retaining enough information to handle any possible merge. If replica A discards metadata about some operation, and replica B (which has been offline for weeks) later tries to merge its state—which still references that metadata—the merge may produce incorrect results.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Can’t We Just Delete Old Data?&lt;/head&gt;
    &lt;p&gt;Let’s make this concrete with an OR-Set example:&lt;/p&gt;
    &lt;code&gt;-- Replica A's state
orset_a = {"todo-1": {tag_1, tag_2}}

-- Replica B's state (has been offline)
orset_b = {"todo-1": {tag_1, tag_2, tag_3}}

-- Replica A removes todo-1, observing tags {tag_1, tag_2}
-- Now A's state is:
orset_a = {}

-- If A garbage collects and forgets about tags {tag_1, tag_2},
-- then later merges with B:
merge(orset_a, orset_b) = {"todo-1": {tag_3}}

-- The element reappears! (Zombie resurrection)&lt;/code&gt;
    &lt;p&gt;The element we removed comes back because we lost the causal information about which tags we had observed and removed. This is the fundamental safety problem with CRDT garbage collection.&lt;/p&gt;
    &lt;head rend="h4"&gt;Strategies and Tradeoffs&lt;/head&gt;
    &lt;p&gt;Time-Based Expiry&lt;/p&gt;
    &lt;p&gt;The simplest approach: discard metadata older than some threshold (e.g., 30 days). This works well when you can guarantee all replicas sync within that window.&lt;/p&gt;
    &lt;code&gt;gcTombstones :: Timestamp -&amp;gt; ORSet a -&amp;gt; ORSet a
gcTombstones cutoff set =
  -- Remove tags older than cutoff
  Map.mapMaybe (\\tags -&amp;gt;
    let recent = Set.filter (\\t -&amp;gt; tagTime t &amp;gt; cutoff) tags
    in if Set.null recent then Nothing else Just recent) set&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple to implement&lt;/item&gt;
      &lt;item&gt;No coordination required&lt;/item&gt;
      &lt;item&gt;Works well for frequently-syncing systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unsafe if replicas can be offline longer than the grace period&lt;/item&gt;
      &lt;item&gt;Must choose grace period conservatively (wasted space)&lt;/item&gt;
      &lt;item&gt;Zombie resurrection if threshold is too aggressive&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Mobile apps where you can bound offline time (e.g., “you must sync at least once per week”).&lt;/p&gt;
    &lt;p&gt;Coordinated Garbage Collection&lt;/p&gt;
    &lt;p&gt;Use distributed consensus to agree on what’s safe to discard. Once all replicas acknowledge they’ve received a particular update, the corresponding metadata can be safely removed.&lt;/p&gt;
    &lt;code&gt;data GCState = GCState
  { pendingGC :: Set Tag  -- Tags eligible for GC
  , replicaAcks :: Map ReplicaId (Set Tag)  -- What each replica has seen
  }

-- When all replicas have acked a tag, it's safe to remove
safeToDiscard :: GCState -&amp;gt; Set Tag
safeToDiscard (GCState pending acks) =
  -- Tags that all known replicas have acknowledged
  Set.filter (\\tag -&amp;gt; all (Set.member tag) (Map.elems acks)) pending&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Completely safe (no zombie resurrections)&lt;/item&gt;
      &lt;item&gt;Can garbage collect aggressively once consensus is reached&lt;/item&gt;
      &lt;item&gt;Works with arbitrary offline periods&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires coordination (defeats CRDT’s main selling point!)&lt;/item&gt;
      &lt;item&gt;Slow convergence if some replicas are rarely online&lt;/item&gt;
      &lt;item&gt;Must track all replicas (what about replicas that never come back?)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: When you have a bounded, known set of replicas and can tolerate periodic coordination rounds.&lt;/p&gt;
    &lt;p&gt;Version Vectors for Causal Tracking&lt;/p&gt;
    &lt;p&gt;Use version vectors to track causal history. Metadata can be discarded once it’s been causally superseded at all replicas.&lt;/p&gt;
    &lt;code&gt;data CausalORSet a = CausalORSet
  { elements :: Map a (Set (Tag, VersionVector))
  , replicaVersions :: Map ReplicaId VersionVector  -- Last known VV per replica
  }

-- A tag can be GC'd if its version vector is dominated by all known replicas
canDiscardTag :: (Tag, VersionVector) -&amp;gt; Map ReplicaId VersionVector -&amp;gt; Bool
canDiscardTag (_, tagVV) replicaVVs =
  all (\\replicaVV -&amp;gt; tagVV `happenedBefore` replicaVV) (Map.elems replicaVVs)&lt;/code&gt;
    &lt;p&gt;This is more sophisticated: we track causality explicitly and can safely discard tags that are in the causal past of all known replicas.&lt;/p&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More precise than time-based expiry&lt;/item&gt;
      &lt;item&gt;No coordination needed for the happy path&lt;/item&gt;
      &lt;item&gt;Safe as long as causal tracking is correct&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Version vectors add significant overhead (O(replicas) per operation)&lt;/item&gt;
      &lt;item&gt;Still requires tracking all replicas&lt;/item&gt;
      &lt;item&gt;Complex to implement correctly&lt;/item&gt;
      &lt;item&gt;What about new replicas that join later?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Systems already using version vectors for causal consistency (Riak, Cassandra-style systems).&lt;/p&gt;
    &lt;p&gt;Bounded Structures with Fallback&lt;/p&gt;
    &lt;p&gt;Limit metadata size and use LWW semantics when bounds are exceeded. For example, keep at most 1000 tags per element in an OR-Set. If we exceed that, discard the oldest tags and accept potential anomalies.&lt;/p&gt;
    &lt;code&gt;addWithBound :: Ord a =&amp;gt; a -&amp;gt; Tag -&amp;gt; Int -&amp;gt; ORSet a -&amp;gt; ORSet a
addWithBound x tag maxTags set =
  let currentTags = Map.findWithDefault Set.empty x set
      newTags = Set.insert tag currentTags
      boundedTags = if Set.size newTags &amp;gt; maxTags
                    then Set.fromList $ take maxTags $ 
                         sortBy (comparing tagTimestamp) (Set.toList newTags)
                    else newTags
  in Map.insert x boundedTags set&lt;/code&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bounded space overhead (guaranteed)&lt;/item&gt;
      &lt;item&gt;No coordination needed&lt;/item&gt;
      &lt;item&gt;Graceful degradation (becomes LWW-ish when bounded)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Correctness sacrificed for space&lt;/item&gt;
      &lt;item&gt;May lose concurrent operations&lt;/item&gt;
      &lt;item&gt;Choosing the bound is difficult (too small = frequent anomalies, too large = still wasteful)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: When you must have bounded space (embedded systems, strict SLAs) and can tolerate occasional anomalies.&lt;/p&gt;
    &lt;p&gt;Checkpoint and Rebase&lt;/p&gt;
    &lt;p&gt;Periodically create a “checkpoint” snapshot and discard history before that point. New replicas joining after the checkpoint start from the snapshot.&lt;/p&gt;
    &lt;code&gt;data CheckpointedCRDT a = CheckpointedCRDT
  { baselineState :: a  -- Snapshot at checkpoint
  , checkpointTime :: Timestamp
  , deltaSince :: [Delta a]  -- Operations since checkpoint
  }

-- Create a new checkpoint, discarding old deltas
checkpoint :: CheckpointedCRDT a -&amp;gt; CheckpointedCRDT a
checkpoint crdt = CheckpointedCRDT
  { baselineState = foldl merge (baselineState crdt) (deltaSince crdt)
  , checkpointTime = currentTime
  , deltaSince = []
  }&lt;/code&gt;
    &lt;p&gt;Replicas that haven’t synced since before the checkpoint must do a full state sync rather than incremental merge.&lt;/p&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can aggressively prune old history&lt;/item&gt;
      &lt;item&gt;Conceptually clean (like Git’s shallow clones)&lt;/item&gt;
      &lt;item&gt;Works well with mostly-online systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Replicas offline during checkpoint period lose incremental sync&lt;/item&gt;
      &lt;item&gt;Need to track which replicas are pre-checkpoint&lt;/item&gt;
      &lt;item&gt;Full state sync is expensive&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use: Collaborative editing systems where most users are online most of the time (Google Docs, Figma).&lt;/p&gt;
    &lt;head rend="h4"&gt;Practical Recommendations&lt;/head&gt;
    &lt;p&gt;For most applications, a hybrid approach works best:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use time-based expiry with a conservative grace period (90 days)&lt;/item&gt;
      &lt;item&gt;Track the oldest unsynced replica timestamp&lt;/item&gt;
      &lt;item&gt;Only discard metadata older than: &lt;code&gt;min(graceperiod, oldest_unsynced - safety_margin)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Provide manual “compact” operations for administrators&lt;/item&gt;
      &lt;item&gt;Use bounded structures for untrusted/public replicas&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without some form of garbage collection, CRDT state grows unbounded and will eventually exhaust memory or storage. The question isn’t whether to implement GC, but which tradeoffs you’re willing to accept.&lt;/p&gt;
    &lt;p&gt;And, realistically speaking, you’re unlikely to implement a system that only uses CRDTs and no other data storage. You’ll almost certainly have some sort of traditional database to store your data, which you can probably use to periodically coordinate garbage collection.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on Causal Consistency&lt;/head&gt;
    &lt;p&gt;CRDTs themselves don’t enforce causal delivery. You need a causal broadcast protocol to ensure operations are delivered respecting happens-before relationships. Without causal delivery, some CRDTs (especially operation-based ones) may behave incorrectly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance&lt;/head&gt;
    &lt;p&gt;Different CRDTs have different performance characteristics. Consider your read/write ratio, expected contention, and replica count when choosing:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;CRDT Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Space Complexity&lt;/cell&gt;
        &lt;cell role="head"&gt;Add/Insert&lt;/cell&gt;
        &lt;cell role="head"&gt;Remove/Delete&lt;/cell&gt;
        &lt;cell role="head"&gt;Merge&lt;/cell&gt;
        &lt;cell role="head"&gt;Read/Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;G-Counter&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;Space: one counter per replica&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;PN-Counter&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;O(r)&lt;/cell&gt;
        &lt;cell&gt;Double the space of G-Counter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;G-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Standard set operations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;2P-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Both added and removed sets grow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LWW-Element-Set&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(e)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Can GC old timestamps carefully&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OR-Set&lt;/cell&gt;
        &lt;cell&gt;O(e × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(t)&lt;/cell&gt;
        &lt;cell&gt;O(e × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Tags accumulate, needs GC&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LWW-Register&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Minimal overhead&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MV-Register&lt;/cell&gt;
        &lt;cell&gt;O(concurrent)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;O(c)&lt;/cell&gt;
        &lt;cell&gt;O(c)&lt;/cell&gt;
        &lt;cell&gt;Returns set of concurrent values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;OR-Map&lt;/cell&gt;
        &lt;cell&gt;O(k × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;O(t)&lt;/cell&gt;
        &lt;cell&gt;O(k × t)&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Per-key OR-Set overhead&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;RGA&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;Tombstones accumulate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WOOT&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n²) worst&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n + d)&lt;/cell&gt;
        &lt;cell&gt;O(n²) worst&lt;/cell&gt;
        &lt;cell&gt;Linearization is expensive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Logoot/LSEQ&lt;/cell&gt;
        &lt;cell&gt;O(n × p)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;O(n log n)&lt;/cell&gt;
        &lt;cell&gt;Position identifiers grow&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Legend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;r&lt;/code&gt;= number of replicas&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;e&lt;/code&gt;= number of elements in set&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;t&lt;/code&gt;= average tags per element (OR-Set)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;k&lt;/code&gt;= number of keys in map&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;n&lt;/code&gt;= number of visible elements in sequence&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d&lt;/code&gt;= number of deleted elements (tombstones)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c&lt;/code&gt;= number of concurrent writes&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;p&lt;/code&gt;= average position identifier length&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key Observations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Counter CRDTs scale with replica count, not operation count. A billion increments still cost O(replicas) space.&lt;/item&gt;
      &lt;item&gt;Set CRDTs generally have constant-time operations, but OR-Set’s space grows with tags unless garbage collected.&lt;/item&gt;
      &lt;item&gt;Sequence CRDTs suffer from tombstone accumulation. RGA is typically faster than WOOT in practice despite similar asymptotic complexity.&lt;/item&gt;
      &lt;item&gt;Position-based sequences (Logoot/LSEQ) trade time complexity for avoiding explicit parent pointers, but position identifiers can grow pathologically.&lt;/item&gt;
      &lt;item&gt;Merge operations are often the bottleneck in high-throughput systems. Delta CRDTs dramatically improve merge performance by sending only changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Libraries and Implementations&lt;/head&gt;
    &lt;p&gt;Many CRDT libraries exist:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automerge: Full-featured CRDT library for JSON-like documents15&lt;/item&gt;
      &lt;item&gt;Yjs: Optimized for collaborative editing16&lt;/item&gt;
      &lt;item&gt;Riak: Database with built-in CRDT support17&lt;/item&gt;
      &lt;item&gt;Redis Enterprise: CRDT-enabled Redis18&lt;/item&gt;
      &lt;item&gt;AntidoteDB: CRDT-native database19&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each makes different tradeoff decisions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further Reading&lt;/head&gt;
    &lt;p&gt;The CRDT literature is vast and honestly a bit scattered across conference proceedings. Here are the key papers worth reading:&lt;/p&gt;
    &lt;p&gt;Foundational:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shapiro et al., “Conflict-Free Replicated Data Types” (2011): The original CRDT paper, defining state-based and operation-based CRDTs.&lt;/item&gt;
      &lt;item&gt;Shapiro et al., “A Comprehensive Study of Convergent and Commutative Replicated Data Types” (2011): Detailed technical report covering many CRDTs. This is the one you want to bookmark.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sequence CRDTs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Oster et al., “Data Consistency for P2P Collaborative Editing” (2006): Introduces WOOT.&lt;/item&gt;
      &lt;item&gt;Roh et al., “Replicated Abstract Data Types: Building Blocks for Collaborative Applications” (2011): Introduces RGA.&lt;/item&gt;
      &lt;item&gt;Weiss et al., “Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing” (2009): Introduces Logoot.&lt;/item&gt;
      &lt;item&gt;Nédelec et al., “LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing” (2013): Introduces LSEQ.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Advanced Topics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Baquero et al., “Making Operation-based CRDTs Operation-based” (2014): Pure operation-based CRDTs without state.&lt;/item&gt;
      &lt;item&gt;Almeida et al., “Delta State Replicated Data Types” (2018): Efficiency improvements for state-based CRDTs.&lt;/item&gt;
      &lt;item&gt;Kleppmann et al., “A Conflict-Free Replicated JSON Datatype” (2017): Automerge’s design.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Surveys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shapiro et al., “Convergent and Commutative Replicated Data Types” (2011): The comprehensive technical report. Start here if you want depth.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;CRDTs are not a silver bullet. They trade coordination for metadata, strong consistency for eventual consistency, and simplicity for convergence guarantees. But in scenarios where availability matters more than immediate consistency, they’re remarkably powerful.&lt;/p&gt;
    &lt;p&gt;There is no “best” CRDT, only CRDTs suited to different problems; the CRDT you choose depends entirely on your application’s semantics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What operations do you need (add, remove, re-add)?&lt;/item&gt;
      &lt;item&gt;Can you tolerate lost updates?&lt;/item&gt;
      &lt;item&gt;Do you need to detect conflicts or resolve them automatically?&lt;/item&gt;
      &lt;item&gt;What’s your tolerance for metadata overhead?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The CRDT abstraction is elegant in theory, but bewildering in practice because there are so many instances with subtle differences. Hopefully this guide has cut through some of the confusion, and given you a good intuition for how they work and when to use them.&lt;/p&gt;
    &lt;p&gt;I honestly still haven’t hit a use case for CRDTs that I couldn’t solve with a traditional database and some custom coordination logic. But sometimes we just want to learn for the sake of learning. If you beat me to it, let me know!&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The term “Conflict-free Replicated Data Type” was coined by Marc Shapiro, Nuno Preguiça, Carlos Baquero, and Marek Zawirski in their 2011 paper “Conflict-free Replicated Data Types” (technical report) and the 2011 SSS conference paper “A comprehensive study of Convergent and Commutative Replicated Data Types”. The theoretical foundations draw from earlier work on commutative replicated data types and optimistic replication. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WOOT was introduced by Oster, Urso, Molli, and Imine in “Data Consistency for P2P Collaborative Editing” (2006). The name is a play on “OT” (Operational Transformation), emphasizing that it achieves similar goals “WithOut OT.” WOOT was one of the first practical sequence CRDTs and influenced many subsequent designs. ↩ ↩2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;State-based CRDTs are also called “convergent” replicated data types (CvRDT). The “Cv” stands for “convergent” - emphasizing that replicas converge to the same state by repeatedly applying the join operation. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Operation-based CRDTs are also called “commutative” replicated data types (CmRDT). They require causal delivery of operations - if operation A happened before operation B on the same replica, B must not be delivered before A at any other replica. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The G-Counter appears in Shapiro et al.’s 2011 technical report “A Comprehensive Study of Convergent and Commutative Replicated Data Types” as one of the foundational examples demonstrating CRDT principles. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The space complexity is O(n) where n is the number of replicas, not the number of increments. This means G-Counters scale well with the number of operations but require tracking all replicas that have ever incremented the counter. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The OR-Set (Observed-Remove Set) was introduced by Shapiro et al. in their 2011 technical report. It’s also known as the “Add-Wins Set” because concurrent add and remove operations result in the element remaining in the set. The key innovation is using unique tags to distinguish between different additions of the same element. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sequence CRDTs are particularly challenging because positional indices change as elements are inserted or deleted. Unlike sets or counters where elements have stable identity, sequences must maintain ordering despite concurrent modifications at arbitrary positions. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;RGA was introduced by Roh et al. in “Replicated Abstract Data Types: Building Blocks for Collaborative Applications” (2011). The name “Replicated Growable Array” emphasizes that it’s an array-like structure that can grow through replication. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;YATA (Yet Another Transformation Approach) was developed by Kevin Jahns for the Yjs collaborative editing library. It combines ideas from RGA and WOOT while optimizing for the common case of sequential insertions (typing). Yjs is used in production by companies like Braid, Row Zero, and others for real-time collaboration. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Version vectors were introduced by Parker et al. in “Detection of Mutual Inconsistency in Distributed Systems” (1983). They extend Lamport’s logical clocks to track causality in distributed systems. Each replica maintains a vector of logical clocks (one for each replica), enabling precise causal ordering without requiring synchronized physical clocks. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Delta CRDTs were introduced by Almeida, Shoker, and Baquero in “Delta State Replicated Data Types” (2018). They bridge the gap between state-based and operation-based CRDTs, achieving operation-based bandwidth efficiency while maintaining state-based simplicity. Most production CRDT systems (Riak, Automerge) use delta-state internally. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Logoot was introduced by Weiss, Urso, and Molli in “Logoot: A Scalable Optimistic Replication Algorithm for Collaborative Editing” (2009). The name combines “log” (logarithmic complexity) with “oot” from WOOT, its predecessor. Logoot’s position-based approach influenced many subsequent CRDTs including LSEQ and Treedoc. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LSEQ was introduced by Nédelec, Molli, Mostéfaoui, and Desmontils in “LSEQ: An Adaptive Structure for Sequences in Distributed Collaborative Editing” (2013). The key innovation is using different allocation strategies (boundary+ vs boundary-) based on tree depth, which keeps position identifiers shorter in practice compared to Logoot’s fixed strategy. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automerge, created by Martin Kleppmann and collaborators, implements a JSON CRDT described in “A Conflict-Free Replicated JSON Datatype” (2017). It uses a columnar encoding for efficiency and has been rewritten in Rust for performance. Used by production apps like Inkandswitch’s Pushpin. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yjs, created by Kevin Jahns, is optimized for text editing and uses the YATA algorithm. It’s notably faster than Automerge for text operations and includes bindings for popular editors like CodeMirror, Monaco, Quill, and ProseMirror. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Riak, a distributed database from Basho, was one of the first production systems to adopt CRDTs (2012). It implements counters, sets, and maps as native data types, using Delta CRDTs internally to minimize bandwidth. Sadly, the company collapsed dramatically, and the project was abandoned for quite some time. I think it’s still around in a diminished form, but haven’t tried it in a while. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Redis Enterprise’s CRDT support (Active-Active deployment) uses operation-based CRDTs with causal consistency. It supports strings, hashes, sets, and sorted sets with CRDT semantics, enabling multi-master Redis deployments. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AntidoteDB is a research database from the SyncFree project that makes CRDTs the primary abstraction. Unlike other databases where CRDTs are a feature, AntidoteDB is designed from the ground up around CRDT semantics, providing highly available transactions over CRDTs. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087022</guid><pubDate>Sat, 29 Nov 2025 12:25:35 +0000</pubDate></item><item><title>Hachi: An Image Search Engine</title><link>https://eagledot.xyz/hachi.md.html</link><description>&lt;doc fingerprint="f79f98f728b79b98"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Hachi: An (Image) Search engine&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Only the dead have seen the end of war .. George Santayana&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For quite some time now, i have been working on and off on a fully self-hosted search engine, in hope to make it easier to search across Personal data in an &lt;code&gt;end to end&lt;/code&gt; manner. Even as individuals, we are hoarding and generating more and more data with no end in sight. Such "personal" data is being stored from local hard-disks to corporate controlled cloud-centers which makes it distributed in nature. So for following discussion, "Personal" meaning would be flexible enough to accommodate resources on a remote server and/or on different devices, as long the user could prove authentication and/or authorization to that data.
Current implementation supports only "images", but eventual goal is also to support other modalities like &lt;code&gt;video&lt;/code&gt;, &lt;code&gt;text&lt;/code&gt; and &lt;code&gt;audio&lt;/code&gt;, some code would be shared, while some new code would be required to better extract &lt;code&gt;Features&lt;/code&gt; for each modality.&lt;/p&gt;
    &lt;p&gt;Such distributed nature of data and potential capabilities of current self-hosted Machine learning models to extract semantic information, only to be queried through a single interface seemed enticing enough for me start this experiment in the first place. Following post at times may seem in-coherent, as i try to articulate my thoughts on the journey of development, challenges faced and future ideas. I hope to treat this as a personal essay with multiple themes, anecdotes and even random thoughts aiming to provide a higher level view of the journey and philosophy so far in more concrete terms.&lt;lb/&gt; Also, following post doesn't aim to cover every technical choice and implementation in finer details, such discussions would instead be part of dedicated future posts!&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivation:&lt;/head&gt;
    &lt;p&gt;As Humans we tend to remember different attributes/parts of an entity/information at different times, and most of search engines' interfaces refuse to accomodate that. User generally end up with an unidirectional flow of information, with no recourse of providing feedback to improve upon the on-going query. Even most advanced interfaces fail to handle the stochastic nature of queries and humans' pre-disposition towards partial information to keep moving, it should be default for search-engines to present best-effort suggestions for queries even if they couldn't be fully resolved.&lt;/p&gt;
    &lt;p&gt;I also note that, it is not always easy to model the imperfect information like handelling a mis-spelling, which itself could be mis-spelled in many ways. It would require a conscious effort to put in a better search interface, as most digital machines make it easy to model when "something" is "correct" or when something is "incorrect". Conveying "Why" something is incorrect takes a lot more code, effort and time, hence indicating that economic realities are more to blame for such cases than bad intentions!&lt;/p&gt;
    &lt;p&gt;It also presents an opportunity to analyze the capabilities of a good interface, as personal data would make it very easy to notice its limitations, which couldn't be observed through seemingly complete interfaces exposed by many e-commerce companies.&lt;/p&gt;
    &lt;p&gt;Inspired by above stated ideas, My try has been to expose multiple (if not all) attributes for a resource directly to user and then letting user recursively refine query to get to desired result. Implementation is still far from complete, but this theme has served me well to set a basic roadmap for the project. Other themes such as self-hosting, hostile behaviour towards users in terms of privacy-invading features, limited or no options to refine a search by google, github etc has contributed to evolution of this experiment. Distributed queries being served by a cluster of (refurbished) smart-phones or single-board-computers remains a lofty goal of this experiment too!&lt;/p&gt;
    &lt;p&gt;Despite all the good intentions and ideas, any search interface should pass that threshold of being fast enough to not end up as another impractical experiment. Efforts were involved from the beginning to embrace the inevitable complexity such projects come to include despite many refactorings. Below is a minimal video to help visualize the current state and capabilities of the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Broader Ideas:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Minimalism: Minimalism in terms of number of external dependencies required for this project to be bootstraped, could explain a lot about downstream choices and evolution of the project to its current form. This has of any existing (source) code if possible or writing it from scratch which itself would require reading of a lot of existing code before i could port it to extend the project in a pure source sense. If it would be practical to reuse some code from existing capable projects/databases, i would have done so but most of such projects are designed to be de-coupled from application code for good reasons, as they are supposed to offer much more guarantees and stay robust even under heavy load. Being an (embedded) part of personal application we can choose to do away with such guarantees and yet expose much more information by tightly integrating ML models pipeline. In the end, application would handle much more complex indexing and inferencing pipelines, which would require a lot more code apart from search and storage interface generally expose!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Experimentation: Thinking more about in terms of augmenting the existing information, rather than to duplicate it, while fusing traditional (deterministic) attributes with semantic(ML) attributes. I think this is an interesting problem and which have not been fully utilized/explored for personal applications. Most of traditional databases were written to only handle "text" modality, but current ML models allow us to query semantic information too, which opens up a new space to experiment in. I treat semantic information as necessary and independent, but not the only signal useful to implement great search interfaces.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Hackability: For this project i wanted it be very easy for someone to start modifying it according to their needs, and this mostly co-relates with the first point about minimalism, lesser the number of dependencies, lesser is the amount of configuration required to bootstrap the developing environment. Both Python and Nim are stable, cross-platform languages and are easier to extend just using a C compiler. Nim source code it easy to compile and/or cross-compile to on almost all platforms. There are already python bridges for many languages, so all such languages are fair game to extend the codebase in any desired way!&lt;/p&gt;&lt;lb/&gt;Python environments (in)famously have the reputation of being difficult to bootstrap, whole parallel ecosystem is there to do so which itself creates another dependency. But i think project has made great progress in this regard, with now having a requirement of just 3 dependencies as&lt;code&gt;numpy&lt;/code&gt;,&lt;code&gt;regex&lt;/code&gt;and&lt;code&gt;markupsafe&lt;/code&gt;and optionally&lt;code&gt;requests&lt;/code&gt;, with no hard-dependence on versioning. Almost all python environments could be used to run the project with no changes, which also removes any need to bootstrap dev environment using Docker like huge dependency or any complex unwarranted build-systems plaguing many of the interesting projects. If i had money, i would pay someone to just make such projects easier to install and start with, by removing any redundant configuration or making it possible to use one common build-system !&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even though above ideas may seem worthy to follow on, there is always an on-going fight to prevent dilution of agreed upon principals. Counter-intuitively i think there is some kind of &lt;code&gt;activation-enery&lt;/code&gt; (https://en.wikipedia.org/wiki/Activation_energy) requirement for each project, past that it actually is much easier to extend, modify, optimize the codebase somewhat like paying a debt to live debt free:)     &lt;/p&gt;
    &lt;p&gt;There are already very capable projects like &lt;code&gt;Sqlite&lt;/code&gt;, &lt;code&gt;Lucene&lt;/code&gt; offering full-text search capabilities, but they implement their own storage backends which require all data to be transformed to the compatible format which leads to duplication of data . This is something i wanted to avoid, as we would be continuously transforming every newer data and this would become computationally expensive when such data wouldn't even reside on same physical machine/node. If we could get away with fast-enough queries through a much simpler index/database, that seems like something worthy to pursue further.&lt;lb/&gt; Most of such projects were created to handle only text queries, But current ML models expose semantic information through "vectors" or "embeddings", generated after a series of linear and non-linear operations on some text or/and an image. &lt;code&gt;Top-k&lt;/code&gt; matching results are later retrived through a "Comparison" procedure with user query (embedding) as one of inputs. 
Such extensions are being gradually added in many older engines, so a hackable codebase like this project may offer more flexibilities while accomodating future ideas in this rapidly evolving field!  &lt;/p&gt;
    &lt;p&gt;It leads to a design comprising a meta-data indexing engine, coupled with vector-search engines for semantic search. We never intend to duplicate the original-data and don't care where it actually resides, once indexing is done. As i think search is more about reaching to a desired file/resource before that resource could be used! Pin-pointing that resource location quickly is the major motivation by incorporating the user intentions and context recursively!&lt;/p&gt;
    &lt;p&gt;(C)Python is used as the major language for backend and Nim (and C) is used to speed up the bottleneck portions of the codebase where-ever warranted. Writing from scratch allows me to update the api as i fit to handle a bottleneck portion of the pipeline (querying or indexing), without asking or waiting for a change in some upstream dependency. Nim itself is a language with relatively smaller community, so i am getting a bit comfortable porting code from other languages to my projects with only standard library and even experimenting with my own data-structures based on (protected) reference semantics than default value semantics that Nim use!&lt;/p&gt;
    &lt;head rend="h1"&gt;Meta-Index:&lt;/head&gt;
    &lt;p&gt;Its a minimal module/database to handle (store and query) meta-data being extracted from resources(images) and has been written in Nim. Currently it is single-threaded, column-oriented database using Json as data-exchange mechanism between python and Nim. In future idea is to shift to leveraging multiple threads for workloads/size greater than a threshold, to better use the current hardware capabilities. It is possible to generate an &lt;code&gt;auxilary&lt;/code&gt; index to speed up queries for a column/attribute on demand, which internally would use cache-friendly and hierichal data-structures to achieve so for most of scenarios! &lt;lb/&gt; Through development of this module, it has been easier to note that why most of databases end-up with some kind of dedicated &lt;code&gt;query language&lt;/code&gt;, as situations arise requiring composing multiple operations in one go which seems like a cleaner way to model such intentions. (and this also seems to validate the requirement of a  &lt;code&gt;query-planner&lt;/code&gt; to better execute a query by analyzing the order and nature of operations and some internal details).
Since it would be written for &lt;code&gt;hachi&lt;/code&gt; itself, it remains possible for me to  speed up a frequent operation by sharing a pointer/memory directly across Nim and python to prevent costly &lt;code&gt;copy&lt;/code&gt; operations, or to directly serve &lt;code&gt;raw json&lt;/code&gt; to the frontend in some cases without serializing and de-serializing at python boundary.&lt;/p&gt;
    &lt;p&gt;I have also experimented with multi-versioning storage design as Lmdb, to protect the original information created by code itself from user revisions. But current implementation instead favours creation of a dedicated field/attribute for user to modify/update. For example during face clustering process, backend will assign an unique Id for each new &lt;code&gt;cluster&lt;/code&gt;, to which user may want to change to a more descriptive name, this leads to presence of attributes like &lt;code&gt;personML&lt;/code&gt; and &lt;code&gt;person&lt;/code&gt; in the final schema. By default, any attribute/information generated through during indexing pipeline is supposed to be immutable to be easily reset to genesis state.&lt;lb/&gt; It still is a bit rigid implementation, as schema is locked once initialized (lazily or explicit), as adding new columns dynamically will require me to reallocate data in the memory and more syncing logic which i am off-putting for now and will work on in the future! Current iteration supports &lt;code&gt;string&lt;/code&gt;, &lt;code&gt;int32&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;array[string]&lt;/code&gt; data-types, which seems to be enough for the application needs, but could be evolved in the future. I am not particularly content with current "string" querying, one reason is that Nim  by default does not have a concept of no-copy slice, and it is difficult to even expose such a user-defined type. As &lt;code&gt;strings&lt;/code&gt; are null-terminated, so most of other composed data-structures with string as one of fields have that underlying assumption which that user-defined type will break. Also i think for a lot of meta-data attributes, i could use &lt;code&gt;ShortString&lt;/code&gt;    kind of data-type to speed up scanning/querying by better leveraging the cache. Some of these issues are being experimented through an independent project and if found to improve performance could be implemented in this codebase too!    &lt;/p&gt;
    &lt;p&gt;There are also Simd opportunities inside the "querying" code, but since its design is being guided by overall needs for the product itself, i hope to add those architecture specific optimizations only after system-design becomes stable enough for most of the features supposed to be supported!&lt;/p&gt;
    &lt;head rend="h1"&gt;Face-Recognition:&lt;/head&gt;
    &lt;p&gt;Being able to group same person(s) with a high probability, as another attribute to search for or mix with other attributes, would be a very quality addition to any search interface. Current DL models for some-time now have been able to distinguish faces with a high accuracy. But being able to distinguish real-life faces still requires a conformance to the pipeline such models would have been trained with.&lt;lb/&gt; There are multiple architectures for such models that have been proposed to tackle this problem, but most pipelines could be assumed to follow a generic flow, which begins with detection of facial bounding boxes from a given image or camera frame, then followed by detection of facial-landmarks for each such face, and ends with generation of &lt;code&gt;embeddings/vectors&lt;/code&gt; which figuratively would represent some kind of latent representation of that face.  At this point, this would be reduced to a Vector Spaces problem and hence much easier to deal with traditional tools like nearest neighbour search !&lt;/p&gt;
    &lt;p&gt;It almost always overwhelming to decide on a particular Implementation to build upon, while accommodating various factors like &lt;code&gt;latency&lt;/code&gt;, &lt;code&gt;accuracy&lt;/code&gt; , &lt;code&gt;hardware requirements&lt;/code&gt;, and most of such intensive pro-bono work would never even be visible to the end-user. For me atleast this goes much further, as i would be implementing each such model using an independent ML framework, which would require me to understand also all the pre-processing and post-processing code, to be  faithfully ported to Nim.&lt;lb/&gt; Spending time on reading papers and existing implementations helps me to get an idea about overall "capability" of the model and potential requirements during fine-tuning of the model in future. Sometimes it has been enough for me to come across an interesting concept through a paper or some nice optimization trick, even if i end up not using that particular implementation. &lt;lb/&gt; Most of face embeddings generation models are trained on a &lt;code&gt;Siamese-loss&lt;/code&gt; like objective to try to explicitly distinguish both positive-positive and positive-negative pairs. This generally involves manually collecting such pairs and hence prone to bias ! Such features predictors are also very sensitive to &lt;code&gt;face-alignment&lt;/code&gt; code used, and hence may require you to faithfully follow the training code!

Dataset being used for training and choice of the objective function are two very major factors influencing the performance of any model. Leakage of evaluation data into training set has been a real issue in recent years for many experiments. Face-recognition itself is a very subjective problem and generally require more "visual" testing apart from (mathematical) metrics proposed for this problem/sub-domain.  &lt;/p&gt;
    &lt;p&gt;Current pipeline uses &lt;code&gt;retina-face&lt;/code&gt; model to predict faces and landmarks in one go which helps producing stable facial-landmarks and speeding up the pipeline. (As predicting facial-landmarks would be much cheaper from internal features than through a dedicated model, and it also helps stabilizing  the training of the model).
Though it could make sense to argue about a model's ability to internalize learning correlated features without adding an explicit loss, but in practice it is always (very) beneficial to use multiple losses explicitly.
Interestingly, &lt;code&gt;residual&lt;/code&gt; connection in &lt;code&gt;ResNets&lt;/code&gt; was an important innovation making it possible to train much deeper networks at that time, even though it would be just mimicing an &lt;code&gt;identity&lt;/code&gt; function.



  &lt;/p&gt;
    &lt;p&gt;In my experience, dataset being used for training and choice of the objective function are two very major factors influencing the performance of your model on real-life (bit out-of-distribution datasets). I find it a good practice to always visually debug some of the random samples to get a "feel" for the dataset!&lt;/p&gt;
    &lt;p&gt;Even after having a good pipeline to generate "embeddings" for a face, &lt;code&gt;clustering&lt;/code&gt; remains a very challenging problem, due to various reasons.
Like with almost all clustering algorithms, we start out with no prior information about of the underlying (number) distribution of the data (faces). (as this is what we would be trying to estimate). As we keep encountering the newer information, possible &lt;code&gt;updates&lt;/code&gt; through &lt;code&gt;back-filling&lt;/code&gt; are required for the underlying index, which somewhat resembles of an auto-regressive operation and hence the error-accumulation rate is relatively high. We would also need to wait for some "initial" amount of data/information to be collected, to estimate initial stable centroids. This difficulty is further compounded by the choices for various thresholds like face-detection, some measure for blurness in the detected face, and a dependence on order of information being encountered.&lt;/p&gt;
    &lt;p&gt;As indicated, choosing same model to predict landmarks and face-bounding boxes, helps reduce the impedance mismatch that occurs when output of one model is being fed through another model. We would need to a dedicate model for facial-features though as earlier features may not be dense enough to distinguish among individual faces!&lt;/p&gt;
    &lt;p&gt;Currently Implementation works by collecting some minimal amount of information before &lt;code&gt;Cluster&lt;/code&gt; creation process could begin. 
Each Cluster is a union of a set of main/master embeddings and a set of follower/slave embeddings. Selection of main embeddings is a crucial part to maintain the stability of a cluster even when new information would be encountered. Initial filtering of unfeasible (master) embeddings is done through some static criterias, for example we strive to filter any of &lt;code&gt;blurred&lt;/code&gt; faces, face-profile is estimated through facial-landmarks, stable forward-facing profiles make face-alignment easier further in the pipeline. Such (static) criterias definitely help to reduce the number of invalid candidates, but may not be enough for many real-life datasets. A further minimal module comparing the &lt;code&gt;hog-features&lt;/code&gt; with a set of pre-saved hog-features is introduced to help invalidate faces with &lt;code&gt;sunglasses&lt;/code&gt; and some false positives not caught by earlier criterias!&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;no-categorical-info&lt;/code&gt;, when not able to being fit into any of the clusters.
Note that a lot of empirical data comes into effect as multiple decisions would be required while choosing many thresholds and may require multiple runs .&lt;/p&gt;
    &lt;p&gt;Since face-recognition is very subjective and i myself have to compare other features to make sure that indeed the correct person(s) have been grouped together by the pipeline. But with a latency of around 25 ms, it seems to do very good on a held out dataset of persons with close up faces, (Zen-Z) selfies and sunglasses occluded eyes. Personal photos are much easier to classify/recognize compared to such a dataset!&lt;/p&gt;
    &lt;p&gt;For any practical ML integrated product, We would need to have a very performant concurrent pipeline to keep feeding the model while being constantly aware of any data-distribution impedance mismatch, to reach anywhere near the 'accuracy' and &lt;code&gt;speed&lt;/code&gt; promised in a research paper. This touches upon the issue of having good understanding of software engineering basics, while being aware of possible regressions resulting from a newer functionality like ML.&lt;lb/&gt; Though bigger VLM/LLM (base) models have potential to handle data-impedance mismatch issues due to their sheer size, their usage would definitely hamper the application responsiveness and have proven to be relatively rigid to be fine-tuned for a specific domain! &lt;/p&gt;
    &lt;head rend="h2"&gt;Indexing:&lt;/head&gt;
    &lt;p&gt;Indexing pipeline begins with desired data location as its input to recursively scanning &lt;code&gt;directories&lt;/code&gt; to collect &lt;code&gt;raw-data&lt;/code&gt; in batches.
Multiple meta attributes such as &lt;code&gt;exif-data&lt;/code&gt;, &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;mount location&lt;/code&gt;, &lt;code&gt;name&lt;/code&gt; are extracted to be later queried through the Meta-indexing engine. Focus has been on designing a good schema to accomodate future use-cases, but since we would be collecting only meta-information without ever modifying the original or duplicating the original data, it remains relatively easier to shift to a newer version/schema even through automatic means.&lt;lb/&gt; ML models extract semantic information which can be later queried through a vector-indexing engine. By default resources to be indexed are assumed to be residing on a local-disk but any protocol could be leveraged, if proper authorization and authentication could be provided.&lt;lb/&gt; Monolithic nature of the code helps me to share &lt;code&gt;raw-data&lt;/code&gt; read/collected once for various components like &lt;code&gt;hash generation&lt;/code&gt;, &lt;code&gt;preprocessing&lt;/code&gt; code for ML models, reducing the number of costly I/O calls. This pipeline has come a long way from a blocking implementation to its current (almost) fully async nature, resulting in very high saturation of computing resources. Apart from running multiple threads, dedicated kernels/functions are used to speed up pipeline by &lt;code&gt;fusion&lt;/code&gt; of operations wherever possible.
 One such example/scenario  has been shown below.&lt;/p&gt;
    &lt;code&gt;def preprocess_kernel(
    image:Tensor[uint8],
    new_shape:tuple[int,int], 
    rgb_to_bgr:bool = True, 
    normalize:bool = True):
    # Preprocess kernel, may fuse resize, color_conversion and normalization into one function!

    # Pseudo-code!

    result = newEmptyTensor[uint8](new_shape)
    for i in new_height:
        for j in new_width:
            inp_h, inp_w = get_corresponding_pixel(image, i, j)
            for k in 0..&amp;lt;3:
                if rgb_to_bgr:
                    result[i,j , 3-k-1] = image[inp_h, inp_w, k]
                    # normalize based on mean and deviation used for training dataset further...
                else:
                    result[ i,j,k] = image[inp_h, inp_w, k]
&lt;/code&gt;
    &lt;p&gt;Each &lt;code&gt;resource&lt;/code&gt; could be assumed to go through a flow like this:&lt;/p&gt;
    &lt;code&gt;resource_location = "file://xyz.jpg"
# OR
resource_location = "remoteProtocol://xyz.jpg"

raw_data = download_raw_data(resource_location)

embeddings = ML_model( preprocess(raw_data))
exif_data = extract_exif_data(raw_data)
preview = generate_preview(raw_data)
write_preview(preview)
....
&lt;/code&gt;
    &lt;head rend="h2"&gt;Vector Index:&lt;/head&gt;
    &lt;p&gt;It is another minimal module to store vector-embeddings as shards on the disk. Necessary meta-data is stashed along with that shard, to make it self-contained, which in future will help in distributed/parallel retrieval. For now each shard is just a numpy (float32) Tensor, and comparison routine is a &lt;code&gt;np.dot&lt;/code&gt; operator, which itself use the &lt;code&gt;blas/openblas&lt;/code&gt; library to speed up this operation! Each shard is loaded from the Disk during a query, and &lt;code&gt;top-k&lt;/code&gt; candidates are collected to be fused together with other deterministic meta-attributes. Loading from Disk do add some latency, but it allows me to regulate RAM usage through &lt;code&gt;shard-size&lt;/code&gt; hyper-parameter, to allow running this on different platforms  with diverse specifications including single-board computers. &lt;code&gt;Shard-size&lt;/code&gt; could be kept relatively high for higher RAM systems to speed up shard querying.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Matmul&lt;/code&gt; is one of the most optimized algorithms which run at almost 90% of theoretical capacity on most of intel/amd Cpus when leveraging &lt;code&gt;Blas&lt;/code&gt; like libraries. So every further optimization from here-on would involve some kind of information loss. There is a whole literature now to speed up this comparison/retrieval process through &lt;code&gt;quantization&lt;/code&gt; and/or &lt;code&gt;nearest neighbour&lt;/code&gt; indices like HNSW. Fast SSDs are also leveraged to run such comparisons at very high speed for upto billion vectors on just a single node in near real time!&lt;/p&gt;
    &lt;p&gt;But such all techniques involve compression of information (which itself is best-effort being the result of modeling a large amount of biased data) through out-of-band mechanisms, for example creating centroids/clusters is just based on the vector values and taking some mean without a way to pass back the information to the model which produced those vectors in the first place. This way is quick and you would get great speed-ups, and there is an active debate among vector-database vendors across various metrics and implementations. In my experience only visual results on a personal data would be a good metric a user should test for. Product-quantization is something i would be implementing if were to choose one, as i think coupled with &lt;code&gt;top-k&lt;/code&gt;, it should work reasonably well to include (subjectively) correct results (high recall!) .&lt;/p&gt;
    &lt;p&gt;Another worthy and very effective solution i think is to instead train a linear layer to finetune the original model depending upon the task. ML Features/embeddings from a big enough model, could assumed to have a knowledge about diverse topics, but for example, a user may be trying to distinguish between different plants. A linear layer could easily be trained with just few thousand samples, to achieve so with much higher accuracy than original models, and even with half the size/dimension of original embeddings. Intuitively it could be thought that we freed the information channels to just focus on plants, decreasing the entropy model earlier had to deal with. Any such layer could be trained even without any framework, as it would just be one &lt;code&gt;backward&lt;/code&gt; operation to implement.
OpenAI has a nice cookbook if a reader would want to explore this further!



  https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb&lt;/p&gt;
    &lt;p&gt;An interesting thing sharding allows is to use any available &lt;code&gt;hardware&lt;/code&gt; to speed up retrieval. Since we need just &lt;code&gt;comparison&lt;/code&gt; routine and corresponding shard(s) to return top-k candidates, it de-couples it from any of application code. A new smartphone could be detected, and some &lt;code&gt;shards&lt;/code&gt; could be transferred during initial set-up, optimal percentage/number of shards could be easily calculated by running same &lt;code&gt;comparsion&lt;/code&gt; operation on new device. Like running a &lt;code&gt;2048 x 2048&lt;/code&gt; , inner-product op and comparing latency with &lt;code&gt;master/main&lt;/code&gt; device, would tell us the capacity of the new device and so that number of shards would be transferred to speed up retrieval process!&lt;/p&gt;
    &lt;p&gt;There are performance gains to be have in the current implementation, would like to atleast start using &lt;code&gt;float16&lt;/code&gt; data-type, but its a bit tricky on intel cpus with no compiler support for this type. Printing of CPU capabilities do show the presence of float16 hardware support on my system ! 
ARM(v8 64) seems to offer native float16/floatH types, there seems to be difference in that type either supported natively by compiler or as an intrinsics/assembly code. I have not been able to get expected speed up for now! Such code is still being experimented upon in the limited time i have.&lt;/p&gt;
    &lt;head rend="h2"&gt;Backend:&lt;/head&gt;
    &lt;p&gt;Backend is written in python, which exposes a pure API server, to let the client/frontend to make API calls to. Starting with very naive code to just return all the meta-data for a &lt;code&gt;directory&lt;/code&gt; to current pagination support it have gone through many revisions and design iterations and now i have much clearer idea about how to architect/wrap a big enough piece of functionality. I wanted the app to be end to end, but this also put extra pressure on app to be responsive enough for all user events. Current indexing code is capable of providing rich details such as directory currently being scanned, estimated time (eta) and allows robust Cancellation of an ongoing task/threads. 
It has not been easy to model such communication b/w concurrent tasks and touches upon much discussed &lt;code&gt;structured-concurrency&lt;/code&gt; debate i.e how to run multiple tasks asynchronously, while being able to robustly cancel them at any point in time, all while being able to collect all errors cleanly!&lt;/p&gt;
    &lt;p&gt;From C days, i have been a user of (Posix) &lt;code&gt;threads&lt;/code&gt; type implementations, since major OSes provide those minimal but stable APIs, it helps me during context switching to different languages. Both C and Nim expose that, Python itself let the OS manage threads without its own runtime implementation, but bypassing the GIL when makes sense is something user have to do to fully utilize the resources! Also this kind of code requires user to handle a lot of code as to communicate b/w threads but atleast i (think) understand the basic ideas to prevent deadlocking if occurs and iron out initial bugs. As you run such threads deeper and deeper inside application stack , it keeps getting harder to communicate information back to the client. But when it starts working, it is really cool to have a central interface to see all the stuff backend is doing and predict very good ETA !&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Flask&lt;/code&gt; was initially used to easily map &lt;code&gt;functions&lt;/code&gt; to a particular route/url to wrap up initial implementation, current implementation now just uses &lt;code&gt;werkzeug&lt;/code&gt; (main engine behind flask) directly, hence doing away with a lot of unrequired dependencies like a template engines that Flask ships with. Even though this would not effect the end user in any visible way, this has been a very nice quality-of-life improvement like stuff for me as a developer. Since werkzeug is pure python, it can now be shipped/bundled directly as source code. Also each request is now handled by an available thread (from a pool) by reading &lt;code&gt;http environment&lt;/code&gt; from a shared queue following conventional model. By default for multi-threaded option, werkzeug would create a new fresh thread for handling that request. This does away with lots of OS/system calls for each new request and latency now seems more consistent and predictive. I have also  stumbled upon a pattern to actually make it easier to &lt;code&gt;mount&lt;/code&gt; multiple &lt;code&gt;apps&lt;/code&gt;  cleanly given i never liked and even understood the &lt;code&gt;blueprint&lt;/code&gt; that flask offers to make it easier to distribute the logic of your app to other modules too.
Since WSGI protocol just expect a callable python object, it should be much easier to develop independent apps without having any knowledge where it would be called/used. It also makes it quite fun to actually write/expose python code to handle client inputs. &lt;/p&gt;
    &lt;code&gt;class SimpleApp():
    """Each instance could be used a WSGI compatible callable"""
    def __init__(self, allow_local_cors:bool = False):
        self.initialized = False
        self.http_methods = ["GET", "POST", "PUT", "DELETE", "OPTIONS"] 
        self.url_map = None # we will lazily initialize it!
        self.extension_prefix = "ext" # as apps would be registered/
        self.registered_extensions:dict[str, SimpleApp] = {}

        ....

    def add_url_rule(self
                     rule:str, 
                     view_function:Callable, # corresponding view.
                     endpoint:Optional[str] = None, # set to view_function
                     methods:list[str]= ["GET"]):

        ... # some validation code.

        self.endpoint_2_uri[endpoint] = (Rule
            (rule, endpoint = endpoint), methods
            )
        self.endpoint_2_viewFunction[endpoint]  = view_function
        self.initialized = False

    def register(self, app:SimpleApp, name:str):
        """
        Here we register another such `app`.
        It would be mounted at `/ext/&amp;lt;name&amp;gt;` , so all requests to /ext/&amp;lt;name&amp;gt;/&amp;lt;route&amp;gt;, would be forwarded to this `app` .
        """

        ... # some validation code.
        self.registered_extensions[name] = app
        print("Extension registered at: {}/{}".format(self.extension_prefix, name))


    def __call__(self, environ, start_response) -&amp;gt; Iterable[bytes]:
        # This is called 
        if not (self.initialized):
            print("[Initializing]: Parent")
            self.initialize()

        for ext in self.registered_extensions:
            if not (self.registered_extensions[ext].initialized):
                print("[Initializing]: {}".format(ext))
                self.registered_extensions[ext].initialize()

        # If a call to such an extension.. we modify the environment a bit.
        active_app = self
        extension_name = None
        temp_path = environ['PATH_INFO']
        temp_split = temp_path.split("/")
        if temp_split[1] == self.extension_prefix:

            extension_name = temp_split[2]
            assert extension_name in self.registered_extensions, 
            extension_path = temp_path.replace("/{}/{}".format(self.extension_prefix, extension_name), "")


            environ['PATH_INFO'] = extension_path
            environ['REQUEST_URI'] = extension_path
            environ['RAW_URI'] = extension_path

            active_app = self.registered_extensions[extension_name]

    ## -----------------------------------------------
    # NOTE: only werkzeug specific code is here!
    # ---------------------------------------------
    request = Request(environ = environ) # minimal wrapping code!
    urls = active_app.url_map.bind_to_environ(environ)
    endpoint, args = urls.match()

    # view function can choose to return iterable[bytes] are the result of view function or call , or further wrap it to be as expected by werkzeug!
    iterable_bytes = active_app.endpoint_2_viewFunction[endpoint](request, **args) 
    return iterable_bytes  # as WSGI protocol expects!
    # ---------------------------------------------------------
&lt;/code&gt;
    &lt;p&gt;Note that, any existing Python object, can be made to accept &lt;code&gt;client&lt;/code&gt; requests on demand by adding very minimal code and could be done for selective functionality. For example, during setup of a new android device, i may have to ask user to choose one of the existing &lt;code&gt;devices&lt;/code&gt;, this kind of interactive input can be modeled easily now, as i just add a new routine in the Corresponding class to accept requests on a route such as &lt;code&gt;/ext/android/beginSetup&lt;/code&gt;, once i get that, all the existing logic already written could be used to finish setup. It is as easy as &lt;code&gt;parent_app.register(app = thisApp, name = "android")&lt;/code&gt; to start routing corresponding requests to this app!&lt;/p&gt;
    &lt;head rend="h2"&gt;ML:&lt;/head&gt;
    &lt;p&gt;Machine learning is being powered by a framework written completely in Nim, most of work was done on that framework before i even stared working on this project. This has allowed me to wrap CLIP and Face-Recognition Pipeline along with the application while only depending on OneDNN for some routines. OneDNN (mkldnn) (https://github.com/uxlfoundation/oneDNN) is one of the libraries to speed up various Deep learning operations with great documentation.&lt;/p&gt;
    &lt;p&gt;Ported models run faster on intel/Amd Cpus than pytorch counterparts, owing to fusion of operations like Batch Normalization and Convolution, and high re-use of pre-allocated memory (similar to in-place operations). Current &lt;code&gt;torch.compile&lt;/code&gt; like engine would end up making some of those optimizations after analyzing the graph, but for at-least 2.0 version it is not supported on Windows for me to compare against!&lt;/p&gt;
    &lt;p&gt;It took a lot of effort during one-two years i was working on it to be complete enough for me to start porting Deep-learning models using it. Also OneDNN shifted to V3 during that time, and only some code was updated to newer API and this has left the project in a unstable state with no visible stable APIs for users to work with. For each model i have to manually analyze the locations/requirements for fusion of operations, port quite a lot of pre-processing and post-processing code to make it end to end. These reasons contributed to a lot of technical debt, which i have not found the resources to tackle yet. Without removing that debt it never made sense to open-source it, besides there are now projects like GGML, and tiny-grad to serve inference only needs with minimal resources!&lt;/p&gt;
    &lt;p&gt;Porting of each model is quite an involved task, as you have to read enough papers to understand ideas about model if want to later fine-tune that model too. You may want to find first find or create a simpler implementation in pytorch to make it easier to port to a new language. All experimentation could be done in pytorch/python, for example i experimented with alternate quantized attention layers for CLIP model, and it indeed had a better performance for eval datasets mentioned in CLIP paper. Tangentially it was really cool to read through Open-AI implementations and papers, papers were written in an approachable manner to let the read indulge in hypothesis, codebases were clean with minimal dependencies. Its really a shame what that company/organisation chose to become under the guise of "user-safety" effectively clipping the (open) ethos of this field, but at same time i am grateful for all the researchers' work in this current DL/ML era and seeing the evolution of this field in such an open manner!&lt;/p&gt;
    &lt;p&gt;I would like to work on the project though atleast enough to tackle that debt and open-source it in state for users to extend upon, if found useful. Even though i am using OneDNN for some routines, i think it is better to have a common and easier to extend codebase to allow more experimentation and aggressive optimizations , but this itself is a huge-task and now with multiple GPU architectures its just something that couldn't be tackled without a lot of time and money. Even in this age where H100 is the baseline for benchmarks in testing, i find it worthwhile to work on a minimal DL Compiler to just tackle ARM/Intel/Risc Cpus to start taking advantage of these cheaper machines. Being able to pin-point a tennis ball in a 3D space remains the dream !&lt;/p&gt;
    &lt;head rend="h2"&gt;Frontend / App:&lt;/head&gt;
    &lt;p&gt;Current front-end is completely written in Html, Js(Ts) and (tailwind) css as multi page webapp. Earlier frontend was written in Svelte, but lack of internal documentation and too much "magic" became too "viral" for me to handle. For me, abstractions and APIs exposed by Browsers are more than enough to maintain required precision during DOM updates. Care is taken to use batch updates, prevent redundant rendering, judicial usage of resources to prevent unrequired pressure through pagination, even for a local backend server. It has passed our litmus test for search over 180 Gb of indexed Pexels dataset on a (minimal) remote server. My friend Akshay helped a lot in frontend development, testing various datasets and offering detailed bug reports which helped uncover a lot of edge cases during development of the project. There would always be room for improvements on the UX/UI side, but we have found it is much easier to extend and improve frontend with a stable backend!&lt;/p&gt;
    &lt;p&gt;Pexels dataset: https://huggingface.co/datasets/opendiffusionai/pexels-photos-janpf&lt;/p&gt;
    &lt;p&gt;Apart from webapp, there is also a Windows App, which under the hood uses the &lt;code&gt;webview&lt;/code&gt; to render the frontend. All native Windows APIs remain available to use from the Nim code, which puts it into a hybrid category. It is not ideal, but atleast it doesn't require me to ship a full web-browser, which i think is waste of compute resources, but at the same time leaves me wondering how current GUI development became so resource intensive for a single developer to manage while offering little benefits! I have been looking into forks of earlier GTK versions for linux to keep the complexity/learning contained, but that also seems nothing less than an adventure!  &lt;/p&gt;
    &lt;head rend="h2"&gt;Tools/libraries:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Nimpy (https://github.com/yglukhov/nimpy) : A minimal python-Nim bridge to make it easier to write extensions in Nim to be called from python and to use python modules in Nim. Unlike many such bridges which includes a lot of boiler-plate code, there are no complex classes/interfaces to be included in the extension. It targets necessary features like marshaling of native python types to and from Nim, targets the minimal Python API to not depend on python versions, finding underlying python.dll at runtime.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stb Image (https://github.com/nothings/stb): A big fan of such single header libraries, this one implements encoders for most of image formats in pure C. Its very easy to modify it pass pointer to the raw-data and writing raw-data to a pre-allocated memory saving costly memory copying particularly visible for 4k photos! It helps remove dependency on OpenCV for image reading ! Nim made it very easy to just compile this along with other Nim code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;LibWebp (https://github.com/webmproject/libwebp): Allows decoding and encoding for webp formats, Though documentation is a bit sparse on some internal API usage, lot of examples are included in the repository to read. I managed to use&lt;/p&gt;&lt;code&gt;argb&lt;/code&gt;field directly to pass&lt;code&gt;argb&lt;/code&gt;format data to do away with transformation logic and some (memory) allocations. It follows callback passing convention to implement custom behaviour like a progress bar and to write encoded data to a user provided buffer. Written completely in C and very easy to compile and read, it is being used for writing image previews, helping remove dependency on OpenCV.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Zig-cc (https://ziglang.org): Using&lt;/p&gt;&lt;code&gt;zig/clang&lt;/code&gt;as a C compiler, allowed me to easily cross-compile a lot of Nim code for Linux, targeting&lt;code&gt;2.27 libc&lt;/code&gt;. Making it easier to set a LibC target has proved very useful to bypass that&lt;code&gt;libC&lt;/code&gt;mismatching stuff! Really cool work by Zig community to tackle a lot of such technical debt to make software development much easier !&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As mentioned earlier i try to use a lot of existing open-source code if i can, even it would be for reading/understanding purposes only. It still blows my mind even after many years, to just read/understand some complex implementation and modify it for personal use-case for Free. For example even though &lt;code&gt;OpenCV&lt;/code&gt; is a big/complex dependency, its still has a very readable codebase and i read code from it a few times during this project to understand differences b/w my port and OpenCV one.&lt;/p&gt;
    &lt;p&gt;Being able to integrate multiple languages has its own challenges too, as it would require us to understand boundaries, internal details, assumptions that each runtime would want developer to respect. It gets complex to reproduce and understand bugs while running multiple multi-threaded runtimes as debugging gets more difficult. Debugging is one of things i would like to get better at, i have very limited knowledge of GDB as of now, which is expected to be table stakes for debugging in such environments. I have had some nasty bugs , but being able to compile all required pieces made it a bit easier to debug even with print-style debugging :)&lt;/p&gt;
    &lt;head rend="h2"&gt;Current State:&lt;/head&gt;
    &lt;p&gt;A lot of functionality is working, than not and having tested over 500k images i could be a bit satisfied about internals' performance and robustness. I would like to say that it can easily handle 10 millions of images/resources, and there is nothing to suggest that it won't, but it is different from using a production database to extrapolate the performance confidently. Despite writing from (almost) scratch in a number of languages, both indexing and inferencing pipeline are more expressive, robust and faster than many similar images search apps, but benchmarking for such complex apps could be subjective and more so when you mix in semantic search.&lt;/p&gt;
    &lt;p&gt;There are still some hardcoded constants and also intentionally some low performing components, like using ViT B/32 variant of CLIP model, which are acting as placeholders, and would be replaced easily with better counterparts in the future.&lt;/p&gt;
    &lt;p&gt;It has been tested on Windows 10/11 and on Fedora 42/43 with an assumption of &lt;code&gt;x64&lt;/code&gt; architecture. Compiled extensions are also packaged to quickly test the application, but users are free to compile code as they see fit. Linux shared objects target &lt;code&gt;LibC 2.27&lt;/code&gt;, so should work on most of recent distributions out of the box. Except some ML code there is main requirement of any/a C compiler to further extend the codebase by the user. Most of testing is done on my Laptop with  &lt;code&gt;i5-8300H&lt;/code&gt; processor and 8 GB memory. I don't have a MacOS to test on, ML code would need to be modified to target ARM architecture, except that very minor modifications should be needed if any. It is quite possible for initial users to encounter minor bugs, due to its limited run in diverse dev environments, but installation and usage on Cloud servers during testing has been quite smooth.&lt;/p&gt;
    &lt;p&gt;Below is a video showcasing workflow to index data from multiple MTP/Android devices. (Still a WIP).&lt;/p&gt;
    &lt;head rend="h2"&gt;Random Thoughts/Philosophy:&lt;/head&gt;
    &lt;p&gt;I think it gets easier with time to grok larger codebases to isolate/find the functionality/implementation reader would be interested in. Most of mature codebases are organized to help navigating the source-tree anyway, and have detailed documentation. Being able to have enough patience to make yourself comfortable is a necessary part of growing as a developer, as initial documentation/codebase would always seem alien and big enough to trigger that flight reaction!&lt;/p&gt;
    &lt;p&gt;Batching and Caching are two generic strategies that could be applied to speed up most of bottleneck portions. Both strategies lead to better/denser utilization of CPUs by (trying to) minimise the costly load/store instructions during a hot loop. Batching for example could do it by allocating necessary memory up-front for a batch and de-allocating all at once when no longer required, reducing the number of costly system-calls. Caching may involve designing or using a (hardware)cache friendly data-structure, when it is possible to do so.&lt;/p&gt;
    &lt;p&gt;Each optimization would involve assumptions and each subsequent optimization would become harder and harder to implement, may preventing the clean refactoring of code when future functionalities may need to be accommodated. It itself is a kind of rabbit-hole, and user should know when to stop as there would always be something else to be optimized!&lt;/p&gt;
    &lt;p&gt;With (coding) tools involving AI/LLMs it is easier than ever to get a piece of desired functionality, as a developer i understand it is another useful tool in a long-history of improvements, that most of developers would come to use in their workflow. Current LLMs have undeniable ability to handle complex instructions, explain non-trivial code and that so for various mixed modalities! It has been a bit unreasonable to end up with such abilities with just next token prediction as primary objective, even for a researcher working in this field. My usage for such tools is only through a (free) search engine(s), Although for now there has been no scenario in such tools have helped me, that i wouldn't have got to using traditional means. But i can admit such tools/engines are really effective in helping us to get unstuck in a variety of situations, arguably helping us to learn faster. &lt;code&gt;Functions/routines&lt;/code&gt; are nice and enough abstractions to provide enough context to such engines, to get the required help, without ever needing &lt;code&gt;review/edit/rewrite&lt;/code&gt; cycle.&lt;lb/&gt; I have always been benefited from visiting the original documentation, if AI is spitting out good enough arguments, there must be a good documentation out there for that topic . Our minds capability to extract abstract patterns resulted from studying one topic and applying it to another seemingly unrelated domain is uncanny to say the least. Also tone/motivation for developer writing about a topic matters to me, and many times i have visited a concept further just because writer himself/herself was very excited about it . Again, these are just personal factors and biases and people should be free to choose workflow they feel most comfortable in , without any judgments from either side.&lt;lb/&gt; It has been difficult to access SOTA models actual abilities, with fewer and fewer details being published for each newer version, but it has been a wild-ride for me to see the evolution from RNNs to bi-directional RNNs to LSTMs to Transformer architecture (finally founding atleast one stable architecture be able to support training on whole internet without exploding or vanishing gradients). Arguably there are also more more open family of models like &lt;code&gt;Qwen&lt;/code&gt; or &lt;code&gt;Deepseek&lt;/code&gt; from other labs which could run on local infrastructure. Even at this stage, ideas behind LLMs are simple enough for anybody to understand without burdening them with terms like AGI . There is already great work from OLMO and Smollm  to build upon and start with, for personal needs, without spending a lot of money. On technical front  there is still much more to explore and it comes down to doing more experiments by smaller companies to prevent ending up with another monopoly/duopoly in this field only to later blame such for their incompetence!&lt;lb/&gt; I literally have no idea what would be the end game with this ever increasing ability of AI models and what social consequences we would end up with in an already fragmented and struggling world. But it would be a mistake to abandon learning, however inconvenient it may seem at any time, if we were to survive ! &lt;lb/&gt; Thing that really boils my blood is these (AI) companies lawless laundering of all the open-source code, art, poetry without any attribution only to be packaged as a product for users to pay for. Constant attacks on all the infrastructure even run by very small or single-developer companies/communities, not respecting any of the &lt;code&gt;robots.txt&lt;/code&gt;, proxying through residential networks, damaging the very core of the information-sharing/internet while coming up with ironical headlines is bordering on criminal-behaviour for me! Waiting for tens of seconds just for a (community written) stack-overflow post through many layers of security, for wanting to understand various perspectives for some concept without all the bullshit summarization, is new bleak reality with nothing for end-users to have a say in.  &lt;/p&gt;
    &lt;p&gt;Despite the dominant usage of LLMs there exist equally interesting smaller models/architectures representing the huge potential that this field of deep-learning holds. Neural-networks allow us to (good enough)model any arbitrary function/flow using an iterative framework from a few thousand samples representing the function space, effectively equipping us with a very power statistical tool Self-supervised learning don't even need explicit outputs, how cool is that.. See https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/ this work for more information. to introduce a new independent signal to reduce the entropy of the problem in many domains. I am a fan of smaller personalized models' potential to tackle everyday problems, and myself uses cheap off-the-self cameras coupled with a DL model to detect those Damn Monkeys, and for local voice-synthesis. Monkey Capturing was even on the manifesto of one of the candidates at city-level elections! In country like India, where even (traditional) Automation is limited to products of very few big companies, I can't help smiling whenever i point remote at my "AI" controlled AC :)&lt;/p&gt;
    &lt;p&gt;Living in a two-tier town in northern India with very minimal fixed-costs has allowed me to work on this for quite a long time without any savings or continuous financial freedom. But i cannot be a hypocrite about it, as it was a conscious decision to learn, explore and implement some of the ideas i had for some time. In return, this has allowed me to stay in touch with friends, played a lot of outdoor games, and help me in reflecting on the things i would want to spend more time in future.&lt;/p&gt;
    &lt;p&gt;Timely financial grants during the last one and half year from Samagata foundation and FossUnited has allowed me to complete a bulk of work to point, where i am satisfied with the current state of the project, for which i will always be grateful.&lt;/p&gt;
    &lt;p&gt;I would very much like to continue on this or adjacent projects, as there are still a lot of ideas and code pending, to make it a very stable everyday engine for users to use . But for that i will have to figure out a way to sustain this , without ever compromising the Core features/functionality in any way, As those were some of reasons i started working on it in the first place! Extensions to allow indexing remote storage like Google Drive or Android devices smoothly from the app itself seems like a good direction in that regard for now!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087549</guid><pubDate>Sat, 29 Nov 2025 13:56:04 +0000</pubDate></item><item><title>DNS LOC Record (2014)</title><link>https://blog.cloudflare.com/the-weird-and-wonderful-world-of-dns-loc-records/</link><description>&lt;doc fingerprint="f4b44f95b957b696"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;A cornerstone of CloudFlare's infrastructure is our ability to serve DNS requests quickly and handle DNS attacks. To do both those things we wrote our own authoritative DNS server called RRDNS in Go. Because of it we've been able to fight off DNS attacks, and be consistenly one of the fastest DNS providers on the web.&lt;/p&gt;
      &lt;p&gt;Implementing an authoritative DNS server is a large task. That's in part because DNS is a very old standard (RFC 1035 dates to 1987), in part because as DNS has developed it has grown into a more and more complex system, and in part because what's written in the RFCs and what happens in the real-world aren't always the same thing.&lt;/p&gt;
      &lt;p&gt;One little used type of DNS record is the LOC (or location). It allows you to specify a physical location. CloudFlare handles millions of DNS records; of those just 743 are LOCs. Nevertheless, it's possible to set up a LOC record in the CloudFlare DNS editor.&lt;/p&gt;
      &lt;p&gt;My site geekatlas.com has a LOC record as an Easter Egg. Here's how it's configured in the CloudFlare DNS settings:&lt;/p&gt;
      &lt;p&gt;When you operate at CloudFlare scale the little-used nooks and crannies turn out to be important. And even though there are only 743 LOC records in our entire database, at least one customer contacted support to find out why their LOC record wasn't being served.&lt;/p&gt;
      &lt;p&gt;And that sent me into the RRDNS source code to find out why.&lt;/p&gt;
      &lt;p&gt;The answer was simple. Although RRDNS had code for receiving requests for LOC records, creating response packets containing LOC data, there was a missing link. The CloudFlare DNS server stores the LOC record as a string (such as the &lt;code&gt;33 40 31 N 106 28 29 W 10m&lt;/code&gt; above) and no one had written the code to parse that and turn it into the internal format. Oops.&lt;/p&gt;
      &lt;p&gt;The textual LOC format and the binary, on-the-wire, format are described in RFC 1876 and it's one of many RFCs that updated the original 1987 standard. RFC 1876 is from 1996.&lt;/p&gt;
      &lt;p&gt;The textual format is fairly simple. Here's what the RFC says:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;The LOC record is expressed in a primary file in the following format:

owner TTL class LOC ( d1 [m1 [s1]] {"N"|"S"} d2 [m2 [s2]]
                           {"E"|"W"} alt["m"] [siz["m"] [hp["m"]
                           [vp["m"]]]] )

where:

   d1:     [0 .. 90]            (degrees latitude)
   d2:     [0 .. 180]           (degrees longitude)
   m1, m2: [0 .. 59]            (minutes latitude/longitude)
   s1, s2: [0 .. 59.999]        (seconds latitude/longitude)
   alt:    [-100000.00 .. 42849672.95] BY .01 (altitude in meters)
   siz, hp, vp: [0 .. 90000000.00] (size/precision in meters)

If omitted, minutes and seconds default to zero, size defaults to 1m,
horizontal precision defaults to 10000m, and vertical precision
defaults to 10m.  These defaults are chosen to represent typical
ZIP/postal code area sizes, since it is often easy to find
approximate geographical location by ZIP/postal code.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;So, there are required latitude, longitude and altitude and three optional values for the size of the location and precision information. Pretty simple.&lt;/p&gt;
      &lt;p&gt;Then there's the on-the-wire format. Unlike a TXT record the LOC record data is parsed and turned into a fixed size binary format. Back to RFC 1876:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  MSB                                           LSB
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  0|        VERSION        |         SIZE          |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  2|       HORIZ PRE       |       VERT PRE        |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  4|                   LATITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  6|                   LATITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
  8|                   LONGITUDE                   |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 10|                   LONGITUDE                   |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 12|                   ALTITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
 14|                   ALTITUDE                    |
   +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;So, 32 bits of latitude, longitude and altitude and then three 8 bit values for the size and precision. The latitude and longitude values have a pretty simple encoding that treats the 32 bits as an unsigned integer:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;The latitude of the center of the sphere described by the SIZE field, expressed as a 32-bit integer, most significant octet first (network standard byte order), in thousandths of a second of arc.  2^31 represents the equator; numbers above that are north latitude.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And the altitude can be below sea-level but still unsigned:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;The altitude of the center of the sphere described by the SIZE field, expressed as a 32-bit integer, most significant octet first (network standard byte order), in centimeters, from a base of 100,000m below the [WGS 84] reference spheroid used by GPS.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;But the 8 bit values use a very special encoding that allows a wide range of approximate values to be packed into 8 bits and also be human-readable when dumped out in hex!&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;The diameter of a sphere enclosing the described entity, in centimeters, expressed as a pair of four-bit unsigned integers, each ranging from zero to nine, with the most significant four bits representing the base and the second number representing the power of ten by which to multiply the base.  This allows sizes from 0e0 (&amp;lt;1cm) to 9e9 (90,000km) to be expressed.  This representation was chosen such that the hexadecimal representation can be read by eye; 0x15 = 1e5.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;For example, the value &lt;code&gt;0x12&lt;/code&gt; means &lt;code&gt;1 * 10^2&lt;/code&gt; or 100cm. &lt;code&gt;0x99&lt;/code&gt; means &lt;code&gt;9 * 10^9&lt;/code&gt; or 90,000,000m. The smallest value that can be represented is 1cm (it's &lt;code&gt;0x10&lt;/code&gt;). So, in just 8 bits there's a range values from 1cm to larger than the diameter of Jupiter.&lt;/p&gt;
      &lt;p&gt;To fix this I wrote a parser for the LOC text record type (and associated tests). It can be found here.&lt;/p&gt;
      &lt;p&gt;We've now rolled out the fix and all the existing LOC records are being served by RRDNS. For example, my &lt;code&gt;geekatlas.com&lt;/code&gt; LOC record can be queried like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ dig geekatlas.com LOC
; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.8.3-P1 &amp;lt;&amp;lt;&amp;gt;&amp;gt; geekatlas.com LOC
;; global options: +cmd
;; Got answer:
;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NOERROR, id: 2997
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:    
;geekatlas.com.         IN  LOC

;; ANSWER SECTION:
geekatlas.com.      299 IN  LOC 33 40 31.000 N 106 28 29.000 W 10.00m 1m 10000m 10m

;; Query time: 104 msec
;; SERVER: 192.168.14.1#53(192.168.14.1)
;; WHEN: Tue Apr  1 14:13:48 2014
;; MSG SIZE  rcvd: 59&lt;/code&gt;
      &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087596</guid><pubDate>Sat, 29 Nov 2025 14:02:23 +0000</pubDate></item><item><title>Iceland declares ocean-current instability a national security risk</title><link>https://edition.cnn.com/2025/11/15/climate/iceland-warming-current-amoc-collapse-threat</link><description>&lt;doc fingerprint="fdc699ff6e34bfd9"&gt;
  &lt;main&gt;
    &lt;p&gt;Iceland’s relatively mild climate is shaped by a crucial network of currents that winds its away around the Atlantic Ocean transporting heat northward — without it, the island would be much icier and stormier. As evidence mounts these currents could be on course for collapse, Iceland’s government has made the unusual move of designating the risk a national security threat, prompting a a high-level response into how to prepare for this “existential threat.”&lt;/p&gt;
    &lt;p&gt;“Our climate, economy and security are deeply tied to the stability of the ocean currents around us,” said Jóhann Páll Jóhannsson, Iceland’s Minister for Environment, Energy and Climate.&lt;/p&gt;
    &lt;p&gt;The Atlantic Meridional Overturning Circulation — known as the AMOC — is a looping system of currents that works like a giant conveyor belt, pulling warm water from the Southern Hemisphere and tropics to the Northern Hemisphere, where it cools, sinks and flows back south.&lt;/p&gt;
    &lt;p&gt;When scientists are asked which potential climate impact terrifies them most, the collapse of the AMOC is often top of the list.&lt;/p&gt;
    &lt;p&gt;A growing body of research points to the AMOC slowing down, as higher global temperatures disrupt the delicate balance of heat and salinity on which its strength relies. The science is still unsettled on the likelihood and timing of any collapse, but some studies have projected it could be on course to happen this century.&lt;/p&gt;
    &lt;p&gt;A shutdown of the AMOC “cannot be considered a low likelihood risk anymore in view of the evolving science over the past years,” said Stefan Rahmstorf, a physical oceanographer and climatologist who has studied the AMOC at Potsdam University in Germany.&lt;/p&gt;
    &lt;p&gt;The impacts would be catastrophic — ushering in huge global weather and climate shifts, including rising sea levels in parts of the US and Europe, disrupted monsoon systems affecting countries in Asia and Africa, and a winter deep freeze in Europe, with sea ice potentially creeping southward as far as the United Kingdom.&lt;/p&gt;
    &lt;p&gt;Iceland “would be close to the center of a serious regional cooling,” meaning the country could be surrounded by sea ice, Rahmstorf told CNN.&lt;/p&gt;
    &lt;p&gt;It’s an “an existential threat,” Jóhannsson told CNN. The AMOC regulates Iceland’s weather, and its collapse could devastate infrastructure, transport and vital industries including fishing, he said.&lt;/p&gt;
    &lt;p&gt;Jóhannsson briefed the government on the latest science after research published in August raised “grave concerns” about the AMOC’s future stability. In September, Iceland’s National Security Council designated the current’s potential collapse as a national security risk, marking the first time a climate impact has received this designation in the country.&lt;/p&gt;
    &lt;p&gt;The decision “reflects the seriousness of the issue and ensures that the matter gets the attention it deserves,” Jóhannsson said. In practice, the designation will mean a high-level, coordinated government response to understand the threat and work out how to prevent and mitigate the worst consequences, he said.&lt;/p&gt;
    &lt;p&gt;Rahmstorf commended Iceland for its decision and said other countries should follow suit. The impacts of an AMOC collapse would be felt across the globe. Scientists are trying to understand the full range of potential impacts on societies and economies, but research has pointed to destroyed crops and catastrophic flooding.&lt;/p&gt;
    &lt;p&gt;Iceland’s decision marks a shift in how the country understands climate risks, Jóhannsson said.&lt;/p&gt;
    &lt;p&gt;“What we do know is that the current climate might change so drastically that it could become impossible for us to adapt,” he said. “In short, this is not just a scientific concern — it’s a matter of national survival and security.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46088192</guid><pubDate>Sat, 29 Nov 2025 15:21:29 +0000</pubDate></item><item><title>Major AI conference flooded with peer reviews written by AI</title><link>https://www.nature.com/articles/d41586-025-03506-6</link><description>&lt;doc fingerprint="ad80911009ab07aa"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.&lt;/p&gt;
    &lt;p&gt;What can researchers do if they suspect that their manuscripts have been peer reviewed using artificial intelligence (AI)? Dozens of academics have raised concerns on social media about manuscripts and peer reviews submitted to the organizers of next year’s International Conference on Learning Representations (ICLR), an annual gathering of specialists in machine learning. Among other things, they flagged hallucinated citations and suspiciously long and vague feedback on their work.&lt;/p&gt;
    &lt;p&gt;Graham Neubig, an AI researcher at Carnegie Mellon University in Pittsburgh, Pennsylvania, was one of those who received peer reviews that seemed to have been produced using large language models (LLMs). The reports, he says, were “very verbose with lots of bullet points” and requested analyses that were not “the standard statistical analyses that reviewers ask for in typical AI or machine-learning papers.”&lt;/p&gt;
    &lt;p&gt;But Neubig needed help proving that the reports were AI-generated. So, he posted on X (formerly Twitter) and offered a reward for anyone who could scan all the conference submissions and their peer reviews for AI-generated text. The next day, he got a response from Max Spero, chief executive of Pangram Labs in New York City, which develops tools to detect AI-generated text. Pangram screened all 19,490 studies and 75,800 peer reviews submitted for ICLR 2026, which will take place in Rio de Janeiro, Brazil, in April. Neubig and more than 11,000 other AI researchers will be attending.&lt;/p&gt;
    &lt;p&gt;Pangram’s analysis revealed that around 21% of the ICLR peer reviews were fully AI-generated, and more than half contained signs of AI use. The findings were posted online by Pangram Labs. “People were suspicious, but they didn’t have any concrete proof,” says Spero. “Over the course of 12 hours, we wrote some code to parse out all of the text content from these paper submissions,” he adds.&lt;/p&gt;
    &lt;p&gt;The conference organizers say they will now use automated tools to assess whether submissions and peer reviews breached policies on using AI in submissions and peer reviews. This is the first time that the conference has faced this issue at scale, says Bharath Hariharan, a computer scientist at Cornell University in Ithaca, New York, and senior programme chair for ICLR 2026. “After we go through all this process … that will give us a better notion of trust.”&lt;/p&gt;
    &lt;p&gt;AI-written peer review&lt;/p&gt;
    &lt;p&gt;The Pangram team used one of its own tools, which predicts whether text is generated or edited by LLMs. Pangram’s analysis flagged 15,899 peer reviews that were fully AI-generated. But it also identified many manuscripts that had been submitted to the conference with suspected cases of AI-generated text: 199 manuscripts (1%) were found to be fully AI-generated; 61% of submissions were mostly human-written; but 9% contained more than 50% AI-generated text.&lt;/p&gt;
    &lt;p&gt;Pangram described the model in a preprint1, which it submitted to ICLR 2026. Of the four peer reviews received for the manuscript, one was flagged as fully AI-generated and another as lightly AI-edited, the team’s analysis found.&lt;/p&gt;
    &lt;p&gt;For many researchers who received peer reviews for their submissions to ICLR, the Pangram analysis confirmed what they had suspected. Desmond Elliott, a computer scientist at the University of Copenhagen, says that one of three reviews he received seemed to have missed “the point of the paper”. His PhD student who led the work suspected that the review was generated by LLMs, because it mentioned numerical results from the manuscript that were incorrect and contained odd expressions.&lt;/p&gt;
    &lt;p&gt;When Pangram released its findings, Elliott adds, “the first thing I did was I typed in the title of our paper because I wanted to know whether my student’s gut instinct was correct”. The suspect peer review, which Pangram’s analysis flagged as fully AI-generated, gave the manuscript the lowest rating, leaving it “on the borderline between accept and reject”, says Elliott. “It's deeply frustrating”.&lt;/p&gt;
    &lt;p&gt;Repercussions&lt;/p&gt;
    &lt;p&gt;Enjoying our latest content? Log in or create an account to continue&lt;/p&gt;
    &lt;p&gt;Access the most recent journalism from Nature's award-winning team&lt;/p&gt;
    &lt;p&gt;Explore the latest features &amp;amp; opinion covering groundbreaking research&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46088236</guid><pubDate>Sat, 29 Nov 2025 15:26:58 +0000</pubDate></item><item><title>AccessOwl (YC S22) Is Hiring a Technical Account Manager (IAM)</title><link>https://www.ycombinator.com/companies/accessowl/jobs/dGC3pcO-technical-account-manager-identity-access-management</link><description>&lt;doc fingerprint="1550a21a95a2b36c"&gt;
  &lt;main&gt;
    &lt;p&gt;Managing your Employees' Access to SaaS&lt;/p&gt;
    &lt;p&gt;TL;DR: You want to work directly with IT and security teams at some of the world’s fastest growing companies, helping them implement, optimize, and expand AccessOwl. You enjoy solving technical problems, guiding integrations, and shaping how a startup delivers value after the deal closes.&lt;/p&gt;
    &lt;p&gt;AccessOwl is building the first AI native Access Governance Suite. We make it radically easier for IT and security teams to manage SaaS access, stay compliant, and eliminate shadow IT, without the overhead of traditional identity systems.&lt;/p&gt;
    &lt;p&gt;We founded AccessOwl out of frustration with manual onboarding, offboarding, and compliance workflows that slow companies down. By combining automation with agentic AI, we are redefining how modern IT admins govern SaaS.&lt;/p&gt;
    &lt;p&gt;We are a profitable, Y Combinator backed startup working with companies like Harvey AI, Monarch, and Motion. Our team is customer centric, pragmatic, and ambitious.&lt;/p&gt;
    &lt;p&gt;To apply, include three sentences on what personally got you interested in talking to us. Skip the generic stuff. We want to hear your real motivation.&lt;/p&gt;
    &lt;p&gt;AccessOwl revolutionizes how businesses manage SaaS applications. Our mission is to simplify SaaS access, spending, and compliance, providing the easiest way to centrally manage apps and user access. AccessOwl replaces Okta, outdated ticketing systems, and spreadsheets, fundamentally transforming how modern IT admins work.&lt;/p&gt;
    &lt;p&gt;We founded AccessOwl out of frustration with the inefficiency caused by SaaS companies exploiting the SSO Tax, which made onboarding and offboarding manual and time-consuming. Our innovative approach leverages RPA and agentic AI workflows to change this.&lt;/p&gt;
    &lt;p&gt;We are a fully remote, customer-centric team dedicated to solving real problems for IT and security teams. Our goal is to build a sustainable business while delivering an exceptional experience our customers genuinely love.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089008</guid><pubDate>Sat, 29 Nov 2025 17:00:23 +0000</pubDate></item><item><title>Zero knowlege proof of compositeness</title><link>https://www.johndcook.com/blog/2025/11/29/zkp-composite/</link><description>&lt;doc fingerprint="dedcb3f5bedbd58"&gt;
  &lt;main&gt;
    &lt;p&gt;A zero knowledge proof (ZKP) answers a question without revealing anything more than answer. For example, a digital signature proves your possession of a private key without revealing that key.&lt;/p&gt;
    &lt;p&gt;Here’s another example, one that’s more concrete than a digital signature. Suppose you have a deck of 52 cards, 13 of each of spades, hearts, diamonds, and clubs. If I draw a spade from the deck, I can prove that I drew a spade without showing which card I drew. If I show you that all the hearts, diamonds, and clubs are still in the deck, then you know that the missing card must be a spade.&lt;/p&gt;
    &lt;head rend="h2"&gt;Composite numbers&lt;/head&gt;
    &lt;p&gt;You can think of Fermat’s primality test as a zero knowledge proof. For example, I can convince you that the following number is composite without telling you what its factors are.&lt;/p&gt;
    &lt;p&gt;n = 244948974278317817239218684105179099697841253232749877148554952030873515325678914498692765804485233435199358326742674280590888061039570247306980857239550402418179621896817000856571932268313970451989041&lt;/p&gt;
    &lt;p&gt;Fermat’s little theorem says that if n is a prime and b is not a multiple of n, then&lt;/p&gt;
    &lt;p&gt;bn−1 = 1 (mod n).&lt;/p&gt;
    &lt;p&gt;A number b such that bn−1 ≠ 1 (mod n) is a proof that n is not prime, i.e. n is composite. So, for example, b = 2 is a proof that n above is composite. This can be verified very quickly using Python:&lt;/p&gt;
    &lt;quote&gt;&amp;gt;&amp;gt;&amp;gt; pow(2, n-1, n) 10282 ... 4299&lt;/quote&gt;
    &lt;p&gt;I tried the smallest possible base [1] and it worked. In general you may have to try a few bases. And for a few rare numbers (Carmichael numbers) you won’t be able to find a base. But if you do find a base b such that bn−1 is not congruent to 1 mod n, you know with certainty that b is composite.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prime numbers&lt;/head&gt;
    &lt;p&gt;The converse of Fermat’s little theorem is false. It can be used to prove a number is not prime, but it cannot prove that a number is prime. But it can be used to show that a number is probably prime. (There’s some subtlety as to what it means for a number to probably be prime. See here.)&lt;/p&gt;
    &lt;p&gt;Fermat’s little theorem can give you a zero knowledge proof that a number is composite. Can it give you a zero knowledge proof that a number is prime? There are a couple oddities in this question.&lt;/p&gt;
    &lt;p&gt;First, what would it mean to have a zero knowledge proof that a number is prime? What knowledge are you keeping secret? When you prove that a number is composite, the prime factors are secret (or even unknown), but what’s the secret when you say a number is prime? Strictly speaking a ZKP doesn’t have to keep anything secret, but in practice it always does.&lt;/p&gt;
    &lt;p&gt;Second, what about the probability of error? Zero knowledge proofs do not have to be infallible. A ZKP can have some negligible probability of error, and usually do.&lt;/p&gt;
    &lt;p&gt;It’s not part of the definition, but n practice ZKPs are supposed to be easier to verify than the direct approach to what they prove. So you could have something like a primality certificate that takes far less computation to verify than the computation needed to determine from scratch that a number is prime.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proving other things&lt;/head&gt;
    &lt;p&gt;You could think of non-constructive proofs as ZKPs. For example, you could think of the intermediate value theorem as a ZKP: it proves that a function has a zero in an interval without giving you any information about where that zero may be located.&lt;/p&gt;
    &lt;p&gt;What makes ZKPs interesting in application is that they can prove things of more general interest than mathematical statements [2]. For example, cryptocurrencies can provide ZKPs that accounting constraints hold without revealing the inputs or outputs of the transaction. You could prove that nobody tried to spend a negative amount and that the sum of the inputs equals the sum of the outputs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related posts&lt;/head&gt;
    &lt;p&gt;[1] You could try b = 1, but then bn−1 is always 1. This example shows that the existence of a base for which bn−1 = 1 (mod n) doesn’t prove anything.&lt;/p&gt;
    &lt;p&gt;[2] You might object that accounting rules are mathematical statements, and of course they are. But they’re of little interest to mathematicians and of great interest to the parties in a transaction.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089394</guid><pubDate>Sat, 29 Nov 2025 17:53:57 +0000</pubDate></item><item><title>Electric vehicle sales are booming in South America – without Tesla</title><link>https://www.reuters.com/sustainability/climate-energy/electric-vehicle-sales-are-booming-south-america-without-tesla-2025-11-17/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089971</guid><pubDate>Sat, 29 Nov 2025 19:12:48 +0000</pubDate></item><item><title>Framework Computer Now Sponsoring LVFS / Fwupd Development</title><link>https://www.phoronix.com/news/Framework-Sponsoring-LVFS</link><description>&lt;doc fingerprint="163eda1118c391d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Framework Computer Now Sponsoring LVFS / Fwupd Development&lt;/head&gt;
    &lt;p&gt;Red Hat in employing lead developer Richard Hughes has contributed the most to LVFS/Fwupd's success, the Linux Foundation has also hosted the project since it has shifted into their umbrella, AMD's Mario Limonciello is among the significant contributors, and now Framework Computer is a new sponsor to the project.&lt;/p&gt;
    &lt;p&gt;Richard Hughes posted on Mastodon on Friday:&lt;/p&gt;
    &lt;quote&gt;"I'm also happy to announce we've got a new sponsor for the LVFS: #Framework&lt;lb/&gt;Although there are about a half a dozen OEMs that have promised to sponsor LVFS, Framework is the first to have actually signed the paperwork."&lt;/quote&gt;
    &lt;p&gt;The graphic on the Fwupd.org page shows Framework as a Startup Sponsor, which puts them in the ballpark of around $10k USD in annual dues to the project.&lt;/p&gt;
    &lt;p&gt;Richard Hughes also followed up with a comment on the positive impact that Framework has also applied on their suppliers around Fwupd/LVFS support too:&lt;/p&gt;
    &lt;quote&gt;"I also wanted to say a huge thanks to Framework, not just for the sponsorship -- but also for the pressure they've put on their suppliers to support fwupd and the LVFS."&lt;/quote&gt;
    &lt;p&gt;Framework has already been supporting LVFS/Fwupd with their devices and ensuring good Linux support in general, such as with the recent Framework 16 laptop upgrade.&lt;/p&gt;
    &lt;p&gt;Hopefully the sponsorship agreements with the other major OEMs pan out and that these other vendors also continue ramping up in mandating LVFS/Fwupd hardware support for ensuring a better firmware updating experience for Linux customers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46089980</guid><pubDate>Sat, 29 Nov 2025 19:14:13 +0000</pubDate></item><item><title>Be Like Clippy</title><link>https://be-clippy.com/</link><description>&lt;doc fingerprint="2d35a339896b64a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Fed up with trillion-dollar companies exploiting your data? Forced to use their services? Your data held for ransom? Your data used to train their AI models? Opt-outs for data collection instead of opt-ins?&lt;/p&gt;
    &lt;p&gt;Join the movement to make companies more like Clippy. Set your profile picture to Clippy, make your voice heard.&lt;/p&gt;
    &lt;p&gt;Below is a video that explains the Be Like Clippy movement. It’s a call to action for developers, companies, and users alike to embrace a more open, transparent, and user-friendly approach to technology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090172</guid><pubDate>Sat, 29 Nov 2025 19:41:55 +0000</pubDate></item><item><title>Learning Feynman's Trick for Integrals</title><link>https://zackyzz.github.io/feynman.html</link><description>&lt;doc fingerprint="dd489f0901f7d271"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Feynman's Trick&lt;/head&gt;
    &lt;head rend="h3"&gt;a.k.a. Differentiation under the Integral Sign &amp;amp; Leibniz Integral Rule&lt;/head&gt;
    &lt;p&gt;Among a few other integral tricks and techniques, Feynman's trick was a strong reason that made me love evaluating integrals, and although the technique itself goes back to Leibniz being commonly known as the Leibniz integral rule, it was Richard Feynman who popularized it, which is why it is also referred to as Feynman's trick. Here's an excerpt from his book, Surely You're Joking, Mr. Feynman:&lt;/p&gt;
    &lt;p&gt;"One thing I never did learn was contour integration. I had learned to do integrals by various methods shown in a book that my high school physics teacher Mr. Bader had given me.&lt;/p&gt;
    &lt;p&gt;One day he told me to stay after class. "Feynman," he said, "you talk too much and you make too much noise. I know why. You're bored. So I'm going to give you a book. You go up there in the back, in the corner, and study this book, and when you know everything that's in this book, you can talk again."&lt;/p&gt;
    &lt;p&gt;So every physics class, I paid no attention to what was going on with Pascal's Law, or whatever they were doing. I was up in the back with this book: Advanced Calculus, by Woods. Bader knew I had studied Calculus for the Practical Man a little bit, so he gave me the real works -- it was for a junior or senior course in college. It had Fourier series, Bessel functions, determinants, elliptic functions -- all kinds of wonderful stuff that I didn't know anything about.&lt;/p&gt;
    &lt;p&gt;That book also showed how to differentiate parameters under the integral sign -- it's a certain operation. It turns out that's not taught very much in the universities; they don't emphasize it. But I caught on how to use that method, and I used that one damn tool again and again. So because I was self-taught using that book, I had peculiar methods of doing integrals.&lt;/p&gt;
    &lt;p&gt;The result was, when guys at MIT or Princeton had trouble doing a certain integral, it was because they couldn't do it with the standard methods they had learned in school. If it was contour integration, they would have found it; if it was a simple series expansion, they would have found it. Then I come along and try differentiating under the integral sign, and often it worked. So I got a great reputation for doing integrals, only because my box of tools was different from everybody else's, and they had tried all their tools on it before giving the problem to me."&lt;/p&gt;
    &lt;p&gt;For me, employing this trick felt like I was using cheat codes to deal with integrals. At the same time, it enabled a lot of creativity and wishful thinking, which transformed integrals into puzzles. Unfortunately, this also means that there is no clear path on how and when to use this technique. In addition, what Feynman wrote still applies today since the method isn't taught much, if at all, in universities. Therefore, the trick can seem obscure and difficult to grasp for newcomers.&lt;/p&gt;
    &lt;p&gt;In the following section, we will embark on a journey to develop some rules of thumb to have at our disposal when using Feynman's trick. These are merely some heuristics that I tend to use, so deviating from them can be perfectly acceptable. However, I hope that they can provide a path to follow when nothing obvious or intuitive occurs when someone tries to use this trick, or even better, so that they can serve as motivation for someone to start using the method.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello, World!&lt;/head&gt;
    &lt;p&gt;Feynman already provided a significant hint about the trick when he mentioned differentiating under the integral sign, which is also an alternative name for the technique. More explicitly, if \(f(x,t)\) and \(\frac{\partial f(x,t)}{\partial t}\) is continuous with respect to both variables over the \([a,b]\) interval, then the following holds:&lt;/p&gt;
    &lt;p&gt;This is nice, but not so useful by itself since it doesn't say anything about how and when to apply it. Moreover, learning is not a spectator sport and one has to get their hands dirty as there are no shortcuts to it. Take for example chess, most people could read and understand the rules in a few minutes, however, if they would go on to play a game then most likely they would get stomped by a more experienced player. This is because the other player, through practice, learned some strategies to use when playing.&lt;/p&gt;
    &lt;p&gt;Thus, with the goal to develop some strategies here as well, we will dive straight into action and approach Feynman's trick using practical examples. As a "Hello, World!" introduction, let's take a look at the following integral:&lt;/p&gt;
    &lt;p&gt;You are encouraged to try and evaluate the integral using basic methods, but the logarithm being in the denominator makes this integral quite stubborn to deal with. Feynman's trick aims to get rid of this issue by differentiating under the intgeral sign, with respect to a parameter, in order to obtain an integral that is easier to evaluate.&lt;/p&gt;
    &lt;p&gt;Unfortunately in the integral from above we lack a parameter, therefore the first step is to parameterise the integral, which can even mean introducing a whole function, but for this example we will simply consider:&lt;/p&gt;
    &lt;p&gt;Keep in mind that our original integral is just \(I(1)\). Also, surely we could've placed a parameter in many different places, such as:&lt;/p&gt;
    &lt;p&gt;However, the main idea behind the trick is to obtain an integral that we can evaluate easier, after differentiating with respect to the new parameter. Let's put this in action and see what happens to \(I(t)\).&lt;/p&gt;
    &lt;p&gt;Notice how easy it was to evaluate the integral \(I'(t)=\int_0^1x^tdx\) from above, had we kept \(I(a)\), \(I(b)\) or \(I(c)\) the things wouldn't had simplified at all after differentiating, and most significantly is that we would still have the \(\ln x\) in the denominator, a thing which made the integral hard to deal with in the first place.&lt;/p&gt;
    &lt;p&gt;We can already sense that the following might be an important question in the future: How to parameterise the integral when using Feynman's Trick?&lt;/p&gt;
    &lt;p&gt;We will worry about that a bit later, for now let's finish the integral as we only found \(I'(t)\). Since we are looking to find \(I(1)\) we need to integrate \(I'(t)\) back and set \(t=1\) in order to arrive there. Here it's useful to recall that:&lt;/p&gt;
    &lt;p&gt;For us, \(f(x)\) is just \(I(t)\) in the above expression. Luckily \(I(0)=0\), and as we are looking for \(I=I(1)\) we have:&lt;/p&gt;
    &lt;p&gt;So that is the big picture of Feynman's trick - we have an integral that is hard to evaluate in it's original form, therefore by differentiating under the integral sign we attempt to transform the integral so that it can be easier integrated, and in the end we go back to undo the differentiation step.&lt;/p&gt;
    &lt;head rend="h2"&gt;The parameter&lt;/head&gt;
    &lt;p&gt;As emphasized above, the main goal of the technique is to obtain an integral that is easier to evaluate after differentiating with respect to a parameter, and one issue is that it is not always obvious how to parameterise the integral. In order to make things more intuitively we will play around with the integral from below.&lt;/p&gt;
    &lt;p&gt;The most annoying thing is the logarithm, so if we get rid of it everything should be straightforward. There are a few parameter possibilities which makes sense to consider, namely:&lt;/p&gt;
    &lt;p&gt;With the first one we are out of luck, as differentiating with respect to \(a\) gives:&lt;/p&gt;
    &lt;p&gt;Therefore, if we would try to go back to what we're looking, which is \(I=I(1)-I(0)\), we would end up with \(I=I+\text{other stuff}\). This cancels out \(I\) and we wouldn't be able to recover it. Unfortunately, there's no magic formula that tells a priori whether placing a parameter in a specific place would succeed or fail in evaluating an integral - and sometimes we are simply unlucky.&lt;/p&gt;
    &lt;p&gt;In contrast, things work out nicely with the second choice from above.&lt;/p&gt;
    &lt;p&gt;Again, we are looking to find \(I(1)\), and as \(I(0) = 0\), we have:&lt;/p&gt;
    &lt;p&gt;This works, but we can do even better. Looking at the Hello, World! integral we can see that there we simplified the logarithm in the denominator while performing \(\frac{\partial}{\partial t}x^t\). This is also the first thing that I always attempt to look for when using this technique - namely, to simplify something from the integrand which is independent to the parameter when differentiating. Surely for the current integral we got rid of the logarithm, but the denominator remained intact.&lt;/p&gt;
    &lt;p&gt;In short this will be our first rule of thumb: if possible, place the parameter so that something from the integral, which is not related to the parameter, gets simplified.&lt;/p&gt;
    &lt;p&gt;In order to achieve this with our integral we would need to get rid of \(1+x^2\), and by using \(\ln x=\frac12\ln(x^2)\) we can rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;Finally, in this form it's more natural to place the parameter so that it simplifies \(1+x^2\) when differentiating with respect to \(t\), namely we can consider:&lt;/p&gt;
    &lt;p&gt;Like for \(I(b)\) we are looking to find \(I(1)\), however here \(I(0)\) is equal to \(\frac12\int_0^1\frac{\ln(2x)}{1+x^2}dx\) not \(0\).&lt;/p&gt;
    &lt;p&gt;For this specific integral we only avoided performing partial fractions so there wasn't really a big improvement by simplifying the denominator. However I want to emphasize the importance of this because it will make things come way more natural when deciding where place the parameter. Of course, in case there's not an appropiate or immediate way to achieve this, it's perfectly fine to place the parameter elsewhere too.&lt;/p&gt;
    &lt;p&gt;As mentioned previously, practicing is the best approach to get along with new techniques, therefore below are more integrals to evaluate alongside some hidden steps in case those will be needed. However, I strongly recommend to try and deal with the integrals before looking at any hints, and only check them afterwards for correctness.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\frac{\pi}{2} \frac{\ln(1-t\sin x)}{\sin x}dx \Rightarrow I'(t)= -\frac{2\arctan\left(\sqrt{\frac{1+t}{1-t}}\right)}{\sqrt{1-t^2}}\] This should lead to: \[\int_0^\frac{\pi}{2} \frac{\ln(1-\sin x)}{\sin x}dx = I(1) - I(0)=\int_0^1 I'(t) dt \overset{\sqrt{\frac{1-t}{1+t}}=x} = -\frac{3\pi^2}{8}\] But it would be even better if the integral would be parameterised as: \[I(t)=\int_0^\frac{\pi}{2} \frac{\ln(1-\sin t\sin x)}{\sin x}dx\] That is because usually when having trigonometric functions, parameterising the integral with another trigonometric function, leads to a more smoother result.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^1 \frac{\ln(1-t(x-x^2))}{x-x^2}dx\Rightarrow I'(t) = \frac{4\arctan\left(\sqrt{\frac{t}{4-t}}\right)}{\sqrt{t(4-t)}}\] This should lead to: \[I(1)=\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx = I(1) - I(0) = \int_0^1 I'(t)dt \overset{\sqrt{\frac{4-t}{t}}= x}= -\frac{\pi^2}{9}\]&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\frac{\pi}{2} \frac{\arctan(t\sin x)}{\sin x}dx\Rightarrow I'(t)=\frac{\pi}{2\sqrt{1+t^2}}\] This should lead to: \[I(1)=\int_0^\frac{\pi}{2} \frac{\arctan(t\sin x)}{\sin x}dx = I(1)-I(0) = \int_0^1 I'(t)dt = \frac{\pi}{2}\ln(1+\sqrt 2)\] It will also work if the integral is parameterised as: \[I(t)=\int_0^\frac{\pi}{2} \frac{\arctan(\tan t\sin x)}{\sin x}dx\] However, in this case the first variant is simple enough to integrate back.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\infty x^2e^{-\left(4x^2+\frac{t}{x^2}\right)}dx\Rightarrow I'(t)=-\frac{\sqrt \pi}{4} e^{-4\sqrt t}\] Where the above result follows by using Glasser's master theorem alongside the Gaussian integral. This should lead to: \[\int_0^\infty x^2e^{-\left(4x^2+\frac{9}{x^2}\right)}dx = I(9)- I(0) + I(0) = \int_0^9 I'(t) dt +\frac{\sqrt \pi}{32}=\frac{13}{32}\frac{\sqrt \pi}{e^{12}}\]&lt;/p&gt;
    &lt;p&gt;Consider parameterising the integral as: \[I(t)=\frac12\int_0^1\frac{\ln(1-t(1-x^2))}{1-x^2}dx\Rightarrow I'(t)=\frac{\arctan\left(\sqrt{\frac{t}{1-t}}\right)}{2\sqrt{t(1-t)}}\] This should lead to: \[\int_0^1 \frac{\ln x}{1-x^2}dx = I(1)- I(0) = \int_0^1 I'(t)dt \overset{\sqrt{\frac{1-t}{t}} = x}= -\frac{\pi^2}{8}\]&lt;/p&gt;
    &lt;p&gt;Consider parameterising the integral as: \[I(t)=\int_0^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\Rightarrow I'(t) = -\frac{\sqrt \pi}{2\sqrt t}e^{-t}\] This should lead to: \[\int_0^\infty \frac{e^{-x^2}}{1+x^2}dx = e\left(I(1)-I(\infty)\right) = -e\int_1^\infty I'(t)dt= \frac{\pi e}{2}\operatorname{erfc}(1)\] Where \(\operatorname{erfc}(x)\) is the complementary error function.&lt;/p&gt;
    &lt;p&gt;Since \(1-x^2+x^4=(1+x^2)^2-3x^2\), consider parameterising the integral as: \[I(t)=\int_0^\infty \frac{\ln\left(\frac{t(1+x^2)^2-3x^2}{(1-x^2)^2}\right)}{(1+x^2)^2}dx\Rightarrow I'(t)=\frac{\pi}{2\sqrt{t(4t-3)}}\] And in order to go back it should be observed that \(\frac34(1+x^2)^2-3x^2=\frac34(1-x^2)^2\). \[\int_0^\infty \frac{\ln\left(\frac{1-x^2+x^4}{(1-x^2)^2}\right)}{(1+x^2)^2}dx=I(1)- I\left(\frac34\right)+ I\left(\frac34\right)\] \[=\int_\frac34^1 I'(t)dt + \frac{\pi}{4}\ln\left(\frac{3}{4}\right) = \frac{\pi}{2}\ln\left(\frac32\right)\]&lt;/p&gt;
    &lt;head rend="h2"&gt;Accelerated Feynman's trick&lt;/head&gt;
    &lt;p&gt;The previous chapter emphasized to parameterise integrals so that something from the integral, which is not related to the parameter, gets simplified when differentiating (if possible). However there are times when even though we can introduce a parameter to accomplish that, it wouldn't be enough to finish the integral.&lt;/p&gt;
    &lt;p&gt;In this chapter we will look at a different way to obtain this simplification. Let's start by looking at a modified version of an integral that was previously given as an exercise.&lt;/p&gt;
    &lt;p&gt;With \(\int_{-\infty}^\infty \frac{e^{-x^2}}{1+x^2}dx\) it was quite direct to parameterise the integral as \(\int_{-\infty}^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\) since it simplifies the denominator, however the similar way to do that for our integral, \(\int_{-\infty}^\infty \frac{e^{-x^2-t(1+x^4)}}{1+x^4}dx\), doesn't seem to work as it complicates things a bit too much.&lt;/p&gt;
    &lt;p&gt;There is however a way to simplify the denominator and in the same time to obtain a decent integral afterwards. Without getting into too much details I will parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;This will seem obscure, but fear not as we will never use this approach again. The whole point is to simplify \(1+x^4\), and the above function was created explicitly to achieve that, as \(\frac{\partial}{\partial t}e^{-tx^2}(x^2\sin t+\cos t)\) is \(-(1+x^4)e^{-tx^2}\sin t\). Note that even though we introduced a couple other terms, those aren't disturbing.&lt;/p&gt;
    &lt;p&gt;Here we are looking to find \(I=I(0)\), and we also have \(I(\infty)=0\), therefore:&lt;/p&gt;
    &lt;p&gt;Where \(S(x)\) and \(C(x)\) are the Fresnel integrals. However, the approach is important here, not the result itself.&lt;/p&gt;
    &lt;p&gt;We can avoid the parametrisation from above by directly using \(\frac{1}{1+x^4}=\int_0^\infty e^{-tx^2}\sin t \, dt\), and then switch to double integrals, or put in other words: employ the accelerated Feynman's trick (in which we skip the usual parameterisation step).&lt;/p&gt;
    &lt;p&gt;The rest goes exactly as with the previous method, as all we did here was to skip differentiation step and instead we switched to double integrals.&lt;/p&gt;
    &lt;p&gt;A natural question that arises here is how did \(\frac{1}{1+x^4}=\int_0^\infty e^{-tx^2}\sin t\, dt\) appear? Or even better, how can someone come up with similar results for other integrals? In the case from above, simply the Laplace transform of the sine function was used, however in general it's useful to have a list of such identities. There are tables of integral results that can be used - for example: Table of Integrals, Series, and Products by Gradshteyn and Ryzhik - but alternatively one can build up their own list of results which tend to appear often while evaluating other integrals.&lt;/p&gt;
    &lt;p&gt;Let's conclude this chapter by evaluating one of the most popular integrals that appears when Feynman's trick gets into the conversation.&lt;/p&gt;
    &lt;p&gt;Since \(\int_0^\infty e^{-xt} dt = \frac{1}{x}\), we can make use of this to rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;Alternatively, we can also consider the parameter version of this integral, \(\int_0^\infty \frac{\sin x}{x}e^{-xt}dx\), however I feel like switching to double integrals is way more intuitively.&lt;/p&gt;
    &lt;p&gt;It might be worth to highlight again that this method should be used preferable when parameterising the integral leads to nowhere. For the above integral, the natural introduction of \(\int_0^\infty \frac{\sin(tx)}{x}dx\) unfortunatelly does fail, as we obtain a divergent integral after differentiating under the integral sign.&lt;/p&gt;
    &lt;p&gt;Like in the previous chapter below are more integrals alongside some hints in order to practice with the accelerated variation of Feynman's trick. However in this case I do recommend to peek at hints faster in case nothing obvious comes to mind, and afterwards to attempt and understand why the mentioned identity can be used.&lt;/p&gt;
    &lt;p&gt;Start by substituting \(x^2\to x\) and then switch to double integrals using: \[\int_0^\infty e^{-xt^2}dt = \frac{\sqrt \pi}{2\sqrt x}\] Where the latter result is due to the Gaussian integral. Also, this integral is one particular case of the Fresnel integral.&lt;/p&gt;
    &lt;p&gt;Switch directly to double integrals by using: \[\int_0^1 \frac{\ln t}{t-\frac{1}{x}}dt = \operatorname{Li}_2(x)\]&lt;/p&gt;
    &lt;p&gt;Switch to double integrals by using the following result: \[\int_0^x \frac{\arctan t}{1+xt}dt = \frac{\arctan x \ln(1+x^2)}{2x}\]&lt;/p&gt;
    &lt;p&gt;Consider switching to double integrals with: \[\frac{x}{\pi^2+x^2}=\Im\left(-\frac{1}{\pi+ix}\right)=-\Im\int_0^\infty e^{-(\pi+ix)t}dt\] It's also really useful to try and see what happens when the Laplace transform of the cosine function is used instead, or the equivalent: \[\frac{x}{\pi^2+x^2}=\Re\left(\frac{1}{i\pi+x}\right)=\Re\int_0^\infty e^{-(i\pi+x)t}dt\]&lt;/p&gt;
    &lt;p&gt;Consider switching to double integrals using: \[\operatorname{Ci}^2(x)+\operatorname{si}^2(x)=\int_0^\infty \frac{e^{-xy}\ln(1+y^2)}{y}dy\]&lt;/p&gt;
    &lt;p&gt;Above \(\operatorname{Li}_2(x)\) denotes the dilogarithm function and \(\operatorname{Ci}(x)\), \(\operatorname{si}(x)\) are the cosine and the sine integral functions, defined as:&lt;/p&gt;
    &lt;head rend="h2"&gt;More Feynman's trick variants&lt;/head&gt;
    &lt;p&gt;We already got familiar with a popular version of Feynman's trick in the previous chapter. Similarly, now we will take a look at other interesting variants of Feynman's trick, which although might appear less often, they can still help to expand the applicability of the technique.&lt;/p&gt;
    &lt;head rend="h3"&gt;Differentiating under the integral sign&lt;/head&gt;
    &lt;p&gt;We will start by taking a look at a much simpler case of Feynman's trick, namely, in the situation when it would be enough to simply differentiate under the integral sign without performing that "undo" step to integrate back.&lt;/p&gt;
    &lt;p&gt;As a small note, it's true that "differentiating under the integral sign" tends to be used as an alternative name for Feynman's trick, however I prefer to keep this for the variant where only the differentiating process takes part, or as mentioned above, when there's no need to integrate back the result, and the name describes quite literally what we are doing.&lt;/p&gt;
    &lt;p&gt;Let's make this more clear by looking at the following integral:&lt;/p&gt;
    &lt;p&gt;We are already aware from the Hello, World! integral how \(\ln x\) can be simplified, since \(\frac{\partial}{\partial a}x^a = x^a \ln x\). However, by introducing the parameter in that original form as \(x^a \ln^2 x\), we would just produce a third logarithm, so that's going in the opposite direction.&lt;/p&gt;
    &lt;p&gt;Fortunatelly, if we take a step back, we can observe that after we find the result of \(\int_0^1 x^a dx\), then differentiating it w.r.t.\(a\) would give us as many logarithms as we want. So, let's put that integral to use.&lt;/p&gt;
    &lt;p&gt;Of course the integral itself was quite simple this time, however the important part that should be highlighted is that not always we need to perform that "undo" step after differentiating under the integral sign - and sometimes knowing a general integral result can provide us more useful integrals by differentiating it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; indefinite integrals&lt;/head&gt;
    &lt;p&gt;Further, we will take a look at how Feynman's trick can be applied to indefinite integrals. Let's consider:&lt;/p&gt;
    &lt;p&gt;In this form it makes no sense to differentiate the integral with respect to any parameter, but we can extend the integral with temporary bounds by writing:&lt;/p&gt;
    &lt;p&gt;After this we can go on apply Feynman's trick, however, first we are going get rid of the square root via the substitution \(\frac{1}{\sqrt x}\to x\).&lt;/p&gt;
    &lt;p&gt;Here, we can notice that the derivative of \(ax-\frac{b}{x}\) is \(a+\frac{b}{x^2}\) so it would be quite helpful if we had that additional term. In the same time if we differentiate the integrand with respect to \(b\) we'll produce \(a-\frac{b}{x^2}\), which is really useful as \((ax-b/x)^2\) is equal to \((ax+b/x)^2+4ab\) and the derivative of \(ax+\frac{b}{x}\) is \(a-\frac{b}{x^2}\). So let's differentiate as mentioned above:&lt;/p&gt;
    &lt;p&gt;Where \(\operatorname{erfc}(x)\) is the complementary error function. Now we'll go back to \(I(a,b,t)\), but we should be careful to replace the dummy variable \(b\), with something else as the \(b\) parameter does also appear in the bounds.&lt;/p&gt;
    &lt;p&gt;Or for the indefinite integral, this would lead to:&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; power series&lt;/head&gt;
    &lt;p&gt;Next, we will take a look at how to combine Feynman's trick with power series. For this we are going to look at:&lt;/p&gt;
    &lt;p&gt;We are already got familiar with what to do when there is a logarithm in the denominator as we saw that we can get rid of them by using \(\frac{d}{dt} x^t = x^t\ln x\), however here also the \(1-xy\) term appears. In order to solve this issue we'll make use of the geoemtric series, namely \(\frac{1}{1-x}=\sum_{n=0}^\infty x^n\), but we will expand into series a bit later and for now continue with the following integral:&lt;/p&gt;
    &lt;p&gt;Now we have to to get back to \(I(n)\):&lt;/p&gt;
    &lt;p&gt;And finally, we'll put the geometric series to use.&lt;/p&gt;
    &lt;p&gt;So the result is simply \(1-2\gamma\), where \(\gamma\) is the Euler-Mascheroni constant.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; differential equations&lt;/head&gt;
    &lt;p&gt;In what's to come we are going to take a look at a combination between Feynman's trick and differential equations. Let's consider the following integral:&lt;/p&gt;
    &lt;p&gt;We can start by parameterising the cosine function and then employ the accelerated Feynman's trick:&lt;/p&gt;
    &lt;p&gt;We haven't made much progress above, since we simply arrived at another integral with \(x\sin(tx)\) instead of \(\cos(tx)\), thus complexity is the same. However, as \(\frac{\partial}{\partial t}\cos(tx)\) is \(x\sin(tx)\), differentiating \(I(t)\) gives us a differential equation to work with, namely:&lt;/p&gt;
    &lt;p&gt;As a small note for the starting step, although employing the accelerated Feynman's trick was rather obvious as to get rid of the denominator, the additional introduction of the \(t\) parameter might be weird first. However performing the same steps without this parameter gives us:&lt;/p&gt;
    &lt;p&gt;Which indicates that one might put to use the fact that \(I(1)=-I'(1)\), by adding the additional \(t\) parameter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Generalizing Feynman's trick&lt;/head&gt;
    &lt;p&gt;So far we've seen the Feynman's trick applied only when the parameter was inside the integrand, however it can also be used when the bounds are parameterised as well. More generally, the following holds:&lt;/p&gt;
    &lt;p&gt;We'll put this to use with the integral from below.&lt;/p&gt;
    &lt;p&gt;Above we can see that the same \(\sqrt 2\) appears in both the lower bound and the \(\operatorname{arccosh}\) function, so we'll parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;We're looking to find \(I=I\left(\sqrt 2\right)\), and since \(I\left(1\right)=0\), we have:&lt;/p&gt;
    &lt;head rend="h3"&gt;Generating integrals using Feynman's trick&lt;/head&gt;
    &lt;p&gt;Now we'll take a look at a fancier way to use Feynman's trick, especially in order to generate new integrals, for this we're considering:&lt;/p&gt;
    &lt;p&gt;Note that we are not trying to evaluate the above integral, instead we are simply using it in order to build up new integrals with the result that follows after differentiating w.r.t. \(t\).&lt;/p&gt;
    &lt;p&gt;We also have that \(I(\pi)=-\frac{\pi^2}{4}\) and \(I(0)=\frac{\pi^2}{8}\), therefore:&lt;/p&gt;
    &lt;p&gt;In retrospect, this integral also appeared as an exercise in the second chapter, and with the same suggestion from there, we can evaluate the integral by applying Feynman's trick to:&lt;/p&gt;
    &lt;p&gt;Admittedly, following this parameterisation is much more intuitevely than what we've shown with the new variation, however it's also useful to have this trick in the bag.&lt;/p&gt;
    &lt;p&gt;To keep the practice going, underneath are listed some integrals that can be evaluated with one version of Feynman's trick described in this chapter.&lt;/p&gt;
    &lt;p&gt;Start by showing that: \[I(t)=\int_1^\infty \int_1^\infty e^{-t(x+y)}dxdy = \left(\frac{e^{-t}}{t}\right)^2\] Then differentiate both sides two times with respect to \(t\) and set \(t=1\).&lt;/p&gt;
    &lt;p&gt;Differentiate four times with respect to \(n\) the following extended indefinite integral: \[ I(n,t) = \int_0^t \cos(nx) dx \]&lt;/p&gt;
    &lt;p&gt;Combine the geometric series \(\sum\limits_{n=0}^\infty (-1)^n x^n = \frac{1}{1+x}\) with: \[I(t)=\int_0^1 \int_0^1 \frac{(xy)^t}{\ln(xy)}dxdy\]&lt;/p&gt;
    &lt;p&gt;Solve the resulting differential equation after differentiating twice the following integral: \[ I(t) = \int_0^\infty \frac{\sin^2 (tx)}{x^2(1+x^2)}dx \]&lt;/p&gt;
    &lt;p&gt;Split the integral in two parts, then substitute \(x\to tx\) and respectively \(x\to \frac{x}{t}\) in the resulting integrals to obtain: \[I(t)= \int_0^1 \ln x \left(\frac{1}{t+x}+\frac{1}{\frac{1}{t}+x}\right)dx = \int_0^\frac{1}{t} \frac{\ln(tx)}{1+x}dx + \int_0^t \frac{\ln\left(\frac{x}{t}\right)}{1+x}dx\] Now employ the Feynman's trick (in the generalized variant).&lt;/p&gt;
    &lt;p&gt;In disguise this exercise is a reformulation of the following identity: \[ \operatorname{Li}_2(-t)+\operatorname{Li}_2\left(-\frac{1}{t}\right) = -\frac{\pi^2}{6} - \frac{1}{2}\ln^2 t \] Where \(\operatorname{Li}_2(x)\) is the dilogarithm function.&lt;/p&gt;
    &lt;p&gt;This integral can be generated starting from: \[I(t)=\int_0^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\Rightarrow I'(t)=-e^{-t}\int_0^\infty e^{-tx^2} dx\overset{\sqrt t x \to x}= -\frac{e^{-t}}{\sqrt t}\int_0^\infty e^{-x^2} dx\] Afterwards it should be observed that \(I(0)=\frac{\pi}{2}\) and \(I(\infty)=0\), therefore: \[\frac{\pi}{2} = \int_0^\infty \frac{e^{-t}}{\sqrt t} dt \int_0^\infty e^{-x^2} dx \overset{t\to t^2} = 2\left(\int_0^\infty e^{-x^2}dx\right)^2 \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Feynman's trick in practice&lt;/head&gt;
    &lt;p&gt;In this last chapter we'll dive into some "real-world" integrals and observe how Feynman's trick can be adapted for them. Additionally, we will also attempt to build up some heuristics that'll help us manipulate the integrals up to a point where we can introduce an useful parameter. This is due to the fact that with most of the previous examples we had the luxury to parameterise the integrals as they appeared, however often this might not be the case.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the rules&lt;/head&gt;
    &lt;p&gt;In the second chapter we've observed how parameterising the integral so that it also simplifies some parts of the integral can make things more intuitively. However, we can always look for better, especially when our approach doesn't seem elegant enough.&lt;/p&gt;
    &lt;p&gt;Let's see how can we break this rule with the following integral:&lt;/p&gt;
    &lt;p&gt;As with our first rule of thumb it's straightforward to introduce the new parameter as:&lt;/p&gt;
    &lt;p&gt;You are encouraged to proceed forward with the above integral, however computing \(I'(a)\) might not give the most pleasant result. So to overcome this, we'll manipulate the integral a bit before parametersing it.&lt;/p&gt;
    &lt;p&gt;One extremely useful substitution (which deserves its own special chapter) is \(x\to \frac{1-x}{1+x}\). This one is great here especially since it has the following property (among many others):&lt;/p&gt;
    &lt;p&gt;Above, although we could have introduced a simple parameter, for a smoother result we parameterised the integral as:&lt;/p&gt;
    &lt;p&gt;Finally, to find to find \(I\) we can combine \(I(\operatorname{arccosh} 7)\) with \(I(0)\).&lt;/p&gt;
    &lt;p&gt;The result from above follows since includes \(I(0) = \frac{\pi^2}{6}\), which is also the Basel problem in disguise, but you're further encouraged to approach it by employing Feynman's trick.&lt;/p&gt;
    &lt;p&gt;One idea is to rewrite the integral as: \[ I=\int_0^1 \frac{\ln(1+x)}{x}dx\overset{x\to x^3}=3\int_0^1 \frac{\ln(1+x^3)}{x}dx\] \[\Rightarrow \frac13I-I=\int_0^1 \frac{\ln\left(\frac{1+x^3}{1+x}\right)}{x}dx\Rightarrow I = -\frac32 \int_0^1 \frac{\ln(1-x+x^2)}{x}dx \] Now put Feynman's trick to use for: \[I(t)=\int_0^1 \frac{\ln(1-tx+x^2)}{x}dx\] But also take into account that: \[I(0)=\int_0^1 \frac{\ln(1+x^2)}{x}dx\overset{x^2\to x}=\frac12 \int_0^1 \frac{\ln(1+x)}{x}dx=\frac12I\]&lt;/p&gt;
    &lt;head rend="h3"&gt;Switching to rational functions&lt;/head&gt;
    &lt;p&gt;Maybe it's just a personal preference, but for me working with rational functions tends to give a higher visibility on how to parameterise the integrals. We'll see what is meant by this when dealing with the following integral:&lt;/p&gt;
    &lt;p&gt;In this form, two immediate ways to parameterise the integral are as follows:&lt;/p&gt;
    &lt;p&gt;It turns out that both variants work, but we can do better. Another rule of thumb that I follow is to use almost exclusive rational functions as they often provide the most visibility to work with. Let's do that for our integral too.&lt;/p&gt;
    &lt;p&gt;And at this point it should be obvious where to place the parameter so that we can simplify the denominator.&lt;/p&gt;
    &lt;p&gt;Obviously, if you're already more used to trigonometric functions, hyperbolic functions or anything else, then this can be ignored. I however recommend switching to rational functions, a few exceptions being when there's a clear way to parameterise the integral directly or when at least one of the bounds is \(\infty\).&lt;/p&gt;
    &lt;p&gt;For more practice you can also attempt to tackle the following integral:&lt;/p&gt;
    &lt;p&gt;Start by substituting \(\tan x \to x\) then employ Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cleaning up the functions&lt;/head&gt;
    &lt;p&gt;Before parameterising the integrals it's quite useful to clean the disturbing functions as much as possible before any parameterisation. Let's demonstrate this by considering:&lt;/p&gt;
    &lt;p&gt;As we have trigonometric functions all over the place, we will perform the Weierstrass substitution \(\tan\left(\frac{x}{2}\right)\to x\) in order to obtain only rational functions, as per the previous heuristic.&lt;/p&gt;
    &lt;p&gt;It would be great to parameterise the integral so that we get rid of the arctangent function, but unfortunately here it is a bit overloaded, therefore we'll clean it by splitting it into two. In case it's not obvious how to do that directly, we can differentiate it, perform partial fractions and then integrate back.&lt;/p&gt;
    &lt;p&gt;However since we're integrating over the \((0,1)\) interval we have that \(x \cdot 3 x&amp;gt;1\) for \(x &amp;gt; \frac{1}{\sqrt 3}\), so we need to rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;At this point there's only left to evaluate the first integral, \(\mathcal J\), for which we'll employ Feynman's trick. There isn't any way to place the parameter so that we get rid of anything from the denominator so we'll simply introduce the following parameterisation:&lt;/p&gt;
    &lt;p&gt;In general when we have rational functions it is prefered to integrate over \((0,\infty)\), if possible, as it drastically reduces the result, and when there's the derivative of \(\arcsin x\) in the denominator one way to map \((0,1)\) to \((0,\infty)\) is to directly substitute \(x \to \frac{1}{\sqrt{1+x^2}}\).&lt;/p&gt;
    &lt;p&gt;Now in order to go back to \(\mathcal J(3)\), we'll make use of \(\mathcal J(-1)=0\), so:&lt;/p&gt;
    &lt;p&gt;Finally, to finish this integral we'll subsitute \(t = \frac{1-x}{1+x}\).&lt;/p&gt;
    &lt;p&gt;As such, we can conclude that:&lt;/p&gt;
    &lt;p&gt;Similarly you can try and tackle the following integral:&lt;/p&gt;
    &lt;p&gt;Start by substituting \(\tan\left(\frac{x}{2}\right) \to x\), then split the arctangent function into two parts and use Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparing better integral bounds&lt;/head&gt;
    &lt;p&gt;Another useful thing to consider before using Feynman's trick is to manipulate the bounds prior to parameterising the integral so that they get rid of any complicated functions for the differentiated integral, \(I'(t)\). This will almost always give a smoother and easier integral that we have to undo. We will show this in action with the integral from below.&lt;/p&gt;
    &lt;p&gt;With the previous heuristic we've already seen that it is useful to integrate over \((0,\infty)\) when there are some rational functions - therefore we will attempt to do the same with this integral. You are encouraged to try and see what kind of mess it would be produced by parameterising the integral as it is, when the bounds are \((0,1)\). However we will jump straightforward to get our bounds to \((0,\infty)\).&lt;/p&gt;
    &lt;p&gt;In order to obtain that we can notice that the integrand is even, so we can move the bounds to \((-1,1)\) and then substitute \(x\to \frac{1-x}{1+x}\) again, which is another useful way to get our bounds at \((0,\infty)\).&lt;/p&gt;
    &lt;p&gt;Now we can split the logarithm into three parts and use that:&lt;/p&gt;
    &lt;p&gt;Therefore our integral is:&lt;/p&gt;
    &lt;p&gt;To evaluate the emerging integral we will perform Feynman's trick. There's not an obvious way to place the parameter as to simplify the denominator, so we'll parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;The partial fraction was ommited above, as what's really important here is that we're left only with a simple \(\ln a\) as a "disturbing" function. In contrast, if the bounds were \((0,1)\) things would have been way more complicated.&lt;/p&gt;
    &lt;p&gt;Let's finish this integral as we still have to undo the differentiating step.&lt;/p&gt;
    &lt;p&gt;The result from above follows since:&lt;/p&gt;
    &lt;p&gt;Where \(\operatorname{Li}_2(x)\) is the dilogarithm and \(\operatorname{Ti}_2(x)\) is the inverse tangent integral.&lt;/p&gt;
    &lt;p&gt;Finally, collecting all the results yields:&lt;/p&gt;
    &lt;p&gt;With the same idea one can attempt to calculate the following integral:&lt;/p&gt;
    &lt;p&gt;Map the bounds from \((0,1)\) to \((0,\infty)\) by substituting \(x\to\frac{1}{x}\) - here it is necessary to add the resulting integral with the original one - afterwards use Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiple parameters&lt;/head&gt;
    &lt;p&gt;We mostly got familiar to apply Feynman's trick by introducing a parameter somewhere, however sometimes even multiple parameters can be used when encountering new integrals. To exemplify such a situation, let's take a look at the following unit square integral arising in geometric probability:&lt;/p&gt;
    &lt;p&gt;In order to generate the \(\ln(xy)\) part it's straightforward to consider the following integral:&lt;/p&gt;
    &lt;p&gt;Differentiating with respect to \(a\), \(m\) times followed by setting \(a=0\) will gives us the desired integral, ignoring the \((-1)^m\) term. However the denominator is still troublesome, and to deal with that we will introduce one more parameter:&lt;/p&gt;
    &lt;p&gt;This is perfect now, as we can recover our original integral by differentiating with respect to \(a\), \(m\) times, and with respect to \(z\), \(m-1\) times, followed by setting \(a=0\) and respectively \(z=1\) - ignoring some coefficients.&lt;/p&gt;
    &lt;p&gt;One way to evaluate \(I(a,z)\) is to expand the denominator into geometric series as:&lt;/p&gt;
    &lt;p&gt;Now we will take \(m\) derivatives with respect to \(a\) and then set it to \(0\).&lt;/p&gt;
    &lt;p&gt;This can be also rewriten in terms of the polylogarithm function as:&lt;/p&gt;
    &lt;p&gt;Finally, we can arrive at our original integral by taking \(m-1\) derivatives with respect to \(z\) and setting it to \(1\).&lt;/p&gt;
    &lt;p&gt;Although the derivation from above was the important part since it shows the main idea on how to differentiate in order to produce the desired integral, by using \(\frac{\partial}{\partial z} \operatorname{Li}_n(z) = \frac{\operatorname{Li}_{n-1}(z)}{z}\) the result can be also written, with the help of OEIS, as:&lt;/p&gt;
    &lt;p&gt;Where \(s(n,m)\) is the Stirling number of the first kind and \(\zeta(z)\) is the Riemann zeta function.&lt;/p&gt;
    &lt;p&gt;A similar idea can be applied for the following integral:&lt;/p&gt;
    &lt;p&gt;Make use of a more general integral that was evaluated in the fourth chapter, namely: \[\int_0^\infty \frac{\cos(tx)}{a^2+x^2}dx = \frac{\pi}{2a}e^{-at}\] Then differentiate \(3\) times w.r.t. \(a\).&lt;/p&gt;
    &lt;head rend="h3"&gt;Cascaded Feynman's trick&lt;/head&gt;
    &lt;p&gt;Sometimes to enable an application of Feynman's trick we needed to actually apply another Feynman's trick. Let's look at the following integral:&lt;/p&gt;
    &lt;p&gt;The first step should be pretty obvious by now, namely to get rid of the trigonometric functions.&lt;/p&gt;
    &lt;p&gt;Now we can notice that we have two logarithms and only one of them contains the \(x\) term, however since the bounds are \((0,1)\) dealing with that integral won't produce much success (as the result would be quite complicated).&lt;/p&gt;
    &lt;p&gt;However we also have the bounds as \((0,\infty)\) for the \(y\) integral, and it would be even better if we could have a single logarithm. To further obtain such a favorable integral form, we can use the following result:&lt;/p&gt;
    &lt;p&gt;In the third chapter we saw how it's useful to have a list of integral results. One more such useful integral that tend to appear quite often is:&lt;/p&gt;
    &lt;p&gt;You can differentiate either \(I(a)\) or \(I(b)\), and even directly employ the accelerated Feynman's trick by writing the logarithm as an integral.&lt;/p&gt;
    &lt;p&gt;Now back to our integral, by using the above result, we can easily finish the integral.&lt;/p&gt;
    &lt;p&gt;Although this marks the conclusion of the essay, this isn't a static website, and I might update it when I encounter new interesting integrals that are worth to be shown. So far, the integrals comes from my posts on Mathematics Stack Exchange, combined with some of the most popular integrals - thus you can also check them directly there.&lt;/p&gt;
    &lt;p&gt;For further exercises, I can recommend you to explore math forums and magazines such as Art of Problem Solving, Mathematics Stack Exchange, The American Mathematical Monthly, Crux Mathematicorum, or the Romanian Mathematical Magazine, where dozens of fascinating integrals are often posted or published. Additionally, delving into other fields like Statistics, Physics, or Quantum Physics will present you with many remarkable integrals — some of which might be computed using Feynman's trick.&lt;/p&gt;
    &lt;p&gt;I would appreciate a notice on my email address (rxzacky@gmail.com) in case you find any mistakes or if something feels unclear in this essay - and even better if you have some further ideas or suggestions.&lt;/p&gt;
    &lt;p&gt; This work is licensed under a Creative Commons Attribution 4.0 International License, and it can be cited as: &lt;lb/&gt; Zaharia Burghelea, "Feynman's Trick," https://zackyzz.github.io/feynman. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090269</guid><pubDate>Sat, 29 Nov 2025 19:55:17 +0000</pubDate></item><item><title>The Origins of Scala (2009)</title><link>https://www.artima.com/articles/the-origins-of-scala</link><description>&lt;doc fingerprint="b7c0e83945a38bd0"&gt;
  &lt;main&gt;
    &lt;p&gt;Scala, a general-purpose, object-oriented, functional language for the JVM, is the brainchild of Martin Odersky, a professor at Ecole Polytechnique Fédérale de Lausanne (EPFL). In the first part of a multi-part interview series, Martin Odersky discusses Scala's history and origins with Artima's Bill Venners.&lt;/p&gt;
    &lt;p&gt;Bill Venners: Let's start at the beginning. How did you first become involved with programming languages?&lt;/p&gt;
    &lt;p&gt;Martin Odersky: My favorite subject was always compilers and programming languages. When I first discovered what a compiler was, as an undergrad in 1980, I immediately wanted to build one. The only computer I could remotely afford at the time would have been a Sinclair ZX 80 which had one kilobyte of RAM. I was very close to giving it a try, but, fortunately, soon after got access to a much more powerful machine, an Osborne-1. It was the world's first “portable” (meaning luggable) computer, and it looked remotely like a sewing machine tilted by 90 degrees. It had a five-inch screen which displayed 52 tiny characters per line. But it also had a very impressive 56 usable kilobytes of RAM and two floppy drives of 90K each.&lt;/p&gt;
    &lt;p&gt;In those days, I spent some time with another student in my college named Peter Sollich. We had read about a new language called Modula-2, which we found very elegant and well-engineered. So the plan was born to write a Modula-2 compiler for 8-bit Z80 computers. There was a small problem in that the only language that came with the Osborne was Microsoft Basic, which was utterly unsuitable for what we had in mind, because it did not even support procedures with parameters—all you had was global variables. Other compilers at the time were too expensive for our means. So we decided to apply the classic bootstrapping technique. Peter had written a first compiler for a small subset of Pascal in Z80 assembly language. We then used this compiler to compile a slightly larger language, and so on, during several generations, until we could compile all of Modula-2. It could produce interpreted bytecode as well as Z80 binaries. The bytecode was the most compact of any system at the time, and the binary version was the fastest for 8-bit computers. It was a pretty capable system for its time.&lt;/p&gt;
    &lt;p&gt;Shortly before we finished our compiler, Borland came out with Turbo Pascal, and they were considering going into the Modula-2 market as well. In fact, Borland decided to buy our Modula-2 compiler to be sold under the name of Turbo Modula-2 for CP/M alongside an IBM PC version they wanted to develop. We offered to do the IBM PC version for them, but they told us they had it already covered. Unfortunately that version took them much longer than planned. By the time it came out, three or four years later, their implementor team had split from the company, and it became known as TopSpeed Modula-2. In the absence of an IBM-PC version, Borland never put any marketing muscle behind Turbo-Modula-2, so it remained rather obscure.&lt;/p&gt;
    &lt;p&gt;When we had finished the Modula-2 compiler, Borland offered to hire both Peter and me on the spot. Peter went to join them. I was very close to doing the same, but had the problem that I still had a year of classes and a Masters project ahead of me. I was very tempted at the time to become a college dropout. In the end, I decided to stick it out at university. During my masters project (which was about incremental parsing), I discovered that I liked doing research a lot. So in the end, I gave up on the idea of joining Borland to write compilers, and went on instead to do a Ph.D with Niklaus Wirth, the inventor of Pascal and Modula-2, at ETH Zurich.&lt;/p&gt;
    &lt;p&gt;Bill Venners: How did Scala come about? What is its history?&lt;/p&gt;
    &lt;p&gt;Martin Odersky: Towards the end of my stay in Zurich, around 1988/89, I became very fond of functional programming. So I stayed in research and eventually became a university professor in Karlsruhe, Germany. I initially worked on the more theoretical side of programming, on things like call-by-need lambda calculus. That work was done together with Phil Wadler, who at the time was at the University of Glasgow. One day, Phil told me that a wired-in assistant in his group had heard that there was a new language coming out, still in alpha stage, called Java. This assistant told Phil: "Look at this Java thing. It's portable. It has bytecode. It runs on the web. It has garbage collection. This thing is going to bury you. What are you going to do about it?" Phil said, well, maybe he's got a point there.&lt;/p&gt;
    &lt;p&gt;The answer was that Phil Wadler and I decided take some of the ideas from functional programming and move them into the Java space. That effort became a language called Pizza, which had three features from functional programming: generics, higher-order functions, and pattern matching. Pizza's initial distribution was in 1996, a year after Java came out. It was moderately successful in that it showed that one could implement functional language features on the JVM platform.&lt;/p&gt;
    &lt;p&gt;Then we got in contact with Gilad Bracha and David Stoutamire from the Sun core developer team. They said, "We're really interested in the generics stuff you've been doing; let's do a new project that does just that." And that became GJ (Generic Java). So we developed GJ in 1997/98, and six years later it became the generics in Java 5, with some additions that we didn't do at the time. In particular, the wildcards in Java generics were developed later independently by Gilad Bracha and people at Aarhus university.&lt;/p&gt;
    &lt;p&gt;Although our generics extensions were put on hold for six years, Sun developed a much keener interest in the compiler I had written for GJ. It proved to be more stable and maintainable than their first Java compiler. So they decided to make the GJ compiler the standard javac compiler from their 1.3 release on, which came out in 2000.&lt;/p&gt;
    &lt;p&gt;So I decided that even though I wanted to design a language that was different from Java, it would always connect to the Java infrastructure—to the JVM and its libraries. That was the idea. It was a great opportunity for me that at that time I became a professor at EPFL, which provides an excellent environment for independent research. I could form a small group of researchers that could work without having to chase all the time after external grants.&lt;/p&gt;
    &lt;p&gt;At first we were pretty radical. We wanted to create something that built on a very beautiful model of concurrency called the join calculus. We created an object-oriented version of the join calculus called Functional Nets and a language called Funnel. After a while, however, we found out that Funnel, being a very pure language, wasn't necessarily very practical to use. Funnel was built on a very small core. A lot of things that people usually take for granted (such as classes, or pattern matching) were provided only by encodings into that core. This is a very elegant technique from an academic point of view. But in practice it does not work so well. Beginners found the necessary encodings rather difficult, whereas experts found it boring to have to do them time and time again.&lt;/p&gt;
    &lt;p&gt;As a result, we decided to start over again and do something that was sort of midway between the very pure academic language Funnel, and the very pragmatic but at some points restrictive GJ. We wanted to create something that would be at the same time practical and useful and more advanced than what we could achieve with Java. We started working on this language, which we came to call Scala, in about 2002. The first public release was in 2003. A relatively large redesign happened early 2006. And it's been growing and stabilizing since.&lt;/p&gt;
    &lt;p&gt;Bill Venners: You said you found it frustrating at times to have the constraints of needing to be backwards compatible with Java. Can you give some specific examples of things you couldn't do when you were trying to live within those constraints, which you were then able to do when you changed to doing something that's binary but not source compatible?&lt;/p&gt;
    &lt;p&gt;Martin Odersky: In the generics design, there were a lot of very, very hard constraints. The strongest constraint, the most difficult to cope with, was that it had to be fully backwards compatible with ungenerified Java. The story was the collections library had just shipped with 1.2, and Sun was not prepared to ship a completely new collections library just because generics came about. So instead it had to just work completely transparently.&lt;/p&gt;
    &lt;p&gt;That's why there were a number of fairly ugly things. You always had to have ungenerified types with generified types, the so called raw types. Also you couldn't change what arrays were doing so you had unchecked warnings. Most importantly you couldn't do a lot of the things you wanted to do with arrays, like generate an array with a type parameter T, an array of something where you didn't know the type. You couldn't do that. Later in Scala we actually found out how to do that, but that was possible only because we could drop in Scala the requirement that arrays are covariant.&lt;/p&gt;
    &lt;p&gt;Bill Venners: Can you elaborate on the problem with Java's covariant arrays?&lt;/p&gt;
    &lt;p&gt;Martin Odersky: When Java first shipped, Bill Joy and James Gosling and the other members of the Java team thought that Java should have generics, only they didn't have the time to do a good job designing it in. So because there would be no generics in Java, at least initially, they felt that arrays had to be covariant. That means an array of &lt;code&gt;String&lt;/code&gt; is a subtype of array of &lt;code&gt;Object&lt;/code&gt;, for example. The reason for that was they wanted to be able to write, say, a “generic” sort method that took an array of &lt;code&gt;Object&lt;/code&gt; and a comparator and that would sort this array of &lt;code&gt;Object&lt;/code&gt;. And then let you pass an array of &lt;code&gt;String&lt;/code&gt; to it. It turns out that this thing is type unsound in general. That's why you can get an array store exception in Java. And it actually also turns out that this very same thing blocks a decent implementation of generics for arrays. That's why arrays in Java generics don't work at all. You can't have an array of list of string, it's impossible. You're forced to do the ugly raw type, just an array of list, forever. So it was sort of like an original sin. They did something very quickly and thought it was a quick hack. But it actually ruined every design decision later on. So in order not to fall into the same trap again, we had to break off and say, now we will not be upwards compatible with Java, there are some things we want to do differently.&lt;/p&gt;
    &lt;p&gt;Come back Monday, May 11 for the next installment of this conversation with Martin Odersky. If you'd like to receive a brief weekly email announcing new articles at Artima.com, please subscribe to the Artima Newsletter by clicking its checkbox in your account settings.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Martin Odersky is coauthor of Programming in Scala:&lt;p&gt;http://www.artima.com/shop/programming_in_scala&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The Scala programming language website is at:&lt;lb/&gt; http://www.scala-lang.org&lt;/p&gt;
    &lt;p&gt;More information on the Funnel language can be found at:&lt;lb/&gt; http://lamp.epfl.ch/funnel/&lt;/p&gt;
    &lt;p&gt;More information on the Functional Nets can be found at:&lt;lb/&gt; /http://lampwww.epfl.ch/fn/&lt;/p&gt;
    &lt;p&gt;Join calculus is described on wikipedia:&lt;lb/&gt; http://en.wikipedia.org/wiki/Join-calculus&lt;/p&gt;
    &lt;p&gt;Phil Wadler's home page is here:&lt;lb/&gt; http://homepages.inf.ed.ac.uk/wadler/&lt;/p&gt;
    &lt;p&gt;Nicklaus Wirth's wikipedia entry:&lt;lb/&gt; http://en.wikipedia.org/wiki/Niklaus_Wirth&lt;/p&gt;
    &lt;p&gt;The Pizza language on Source Forge. Why not try a slice?:&lt;lb/&gt; http://pizzacompiler.sourceforge.net/&lt;/p&gt;
    &lt;p&gt;The Generic Java Language Extension (GJ):&lt;lb/&gt; http://homepages.inf.ed.ac.uk/wadler/pizza/gj/&lt;/p&gt;
    &lt;p&gt;The Modula-2 home page is here:&lt;lb/&gt; http://www.modula2.org/&lt;/p&gt;
    &lt;p&gt;Have an opinion? Readers have already posted 76 comments about this article. Why not add yours?&lt;/p&gt;
    &lt;p&gt;Bill Venners is president of Artima, Inc., publisher of Artima Developer (www.artima.com). He is author of the book, Inside the Java Virtual Machine, a programmer-oriented survey of the Java platform's architecture and internals. His popular columns in JavaWorld magazine covered Java internals, object-oriented design, and Jini. Active in the Jini Community since its inception, Bill led the Jini Community's ServiceUI project, whose ServiceUI API became the de facto standard way to associate user interfaces to Jini services. Bill is also the lead developer and designer of ScalaTest, an open source testing tool for Scala and Java developers, and coauthor with Martin Odersky and Lex Spoon of the book, Programming in Scala.&lt;/p&gt;
    &lt;p&gt;Frank Sommers is an editor with Artima Developer. He is also founder and president of Autospaces, Inc., a company providing collaboration and workflow tools in the financial services industry.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Artima provides consulting and training services to help you make the most of Scala, reactive and functional programming, enterprise systems, big data, and testing. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090294</guid><pubDate>Sat, 29 Nov 2025 19:59:12 +0000</pubDate></item><item><title>Baboon: Data Modeling with Automatic Evolutions and tagless binary codecs</title><link>https://github.com/7mind/baboon</link><description>&lt;doc fingerprint="248c2e190e17b3a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Let the Baboon do the monkey job.&lt;/p&gt;
    &lt;p&gt;Baboon is a minimal Data Modeling Language and compiler that provides ergonomic, declarative schemas and enforces reliable schema evolution. The compiler runs as a fast immutable multi-phase DAG transform, and is easy to understand and maintain.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set-based structural inheritance with &lt;code&gt;+&lt;/code&gt;,&lt;code&gt;-&lt;/code&gt;, and&lt;code&gt;^&lt;/code&gt;operators&lt;/item&gt;
      &lt;item&gt;Automatic codec derivation for JSON and UEBA (Ultra-Efficient Binary Aggregate, a custom tagless binary format)&lt;/item&gt;
      &lt;item&gt;Evolution-aware codegen: derives migrations when possible, emits stubs when manual work is required&lt;/item&gt;
      &lt;item&gt;Structural and nominal inheritance (contracts)&lt;/item&gt;
      &lt;item&gt;Namespaces, includes, and imports&lt;/item&gt;
      &lt;item&gt;Collections (&lt;code&gt;opt&lt;/code&gt;,&lt;code&gt;lst&lt;/code&gt;,&lt;code&gt;set&lt;/code&gt;,&lt;code&gt;map&lt;/code&gt;) and timestamps/UID primitives&lt;/item&gt;
      &lt;item&gt;Codegen targets: C#, Scala, will be more.&lt;/item&gt;
      &lt;item&gt;Deduplicated C# output (reuse as much code as possible for lower binary footprint)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Detailed language walkthrough with copy-paste examples: docs/language-features.md.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intellij Idea Plugin (source: baboon-intellij)&lt;/item&gt;
      &lt;item&gt;VSCode Extension (source: baboon-vscode)&lt;/item&gt;
      &lt;item&gt;VSCodium Extension&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;No templates&lt;/item&gt;
      &lt;item&gt;Only Enums, DTOs and ADTs&lt;/item&gt;
      &lt;item&gt;Nominal inheritance support is limited to trait model&lt;/item&gt;
      &lt;item&gt;Generic/type constructor support is limited to builtin collections&lt;/item&gt;
      &lt;item&gt;(*) This is a DML, not an IDL, service/interface definitions support is extremely limited at the moment&lt;/item&gt;
      &lt;item&gt;(*) Comments are not preserved in the cogen output&lt;/item&gt;
      &lt;item&gt;(*) No structural inheritance information is preserved in the transpiler output&lt;/item&gt;
      &lt;item&gt;(*) Only integer constants may be associated with enum members&lt;/item&gt;
      &lt;item&gt;(*) No newtypes/type aliases&lt;/item&gt;
      &lt;item&gt;(*) No inheritance-based lenses/projections/conversions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Points marked with (*) will/may be improved in the future.&lt;/p&gt;
    &lt;p&gt;See build configuration in .mdl/defs/actions.md and test configuration in .mdl/defs/tests.md.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;All the types which are not transitively referenced by &lt;code&gt;root&lt;/code&gt;types will be eliminated from the compiler output.&lt;/item&gt;
      &lt;item&gt;Usages in structural inheritance are not considered references, so structural parents which are not directly referenced as fields and not marked as &lt;code&gt;root&lt;/code&gt;s will be eliminated&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Be careful about foreign types. It is your responsibility to wire codecs correctly.&lt;/p&gt;
    &lt;p&gt;For every foreign type:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a custom codec&lt;/item&gt;
      &lt;item&gt;Override the generated dummy codec with &lt;code&gt;BaboonCodecs#Register&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Override the generated dummy codec using the setter on &lt;code&gt;${Foreign_Type_Name}_UEBACodec#Instance&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Override the generated dummy codec using the setter on &lt;code&gt;${Foreign_Type_Name}_JsonCodec#Instance&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Make sure your foreign types are NOT primitive types or other generated types. It's a funny idea, but it will explode in runtime.&lt;/p&gt;
    &lt;p&gt;Foreign types may hold any position in generics but it's up to you to ensure correctness.&lt;/p&gt;
    &lt;p&gt;This project uses mudyla for build orchestration.&lt;/p&gt;
    &lt;code&gt;# Format code
direnv exec . mdl :fmt

# Build the compiler
direnv exec . mdl :build

# Run complete test suite
direnv exec . mdl :build :test

# Run full build pipeline (format, build, test)
direnv exec . mdl :full-build

# Run specific test suites
direnv exec . mdl :build :test-gen-regular-adt :test-cs-regular :test-scala-regular
direnv exec . mdl :build :test-gen-wrapped-adt :test-cs-wrapped :test-scala-wrapped
direnv exec . mdl :build :test-gen-manual :test-gen-compat-scala :test-gen-compat-cs :test-manual-cs :test-manual-scala

# Create distribution packages
direnv exec . mdl :build :mkdist

# Build with custom distribution paths
direnv exec . mdl --mkdist-source=./custom/path --mkdist-target=./output :build :mkdist&lt;/code&gt;
    &lt;code&gt;# Enter the nix development shell
nix develop

# Or use direnv for automatic shell activation
direnv allow&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090372</guid><pubDate>Sat, 29 Nov 2025 20:12:37 +0000</pubDate></item><item><title>All it takes is for one to work out</title><link>https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/</link><description>&lt;doc fingerprint="64659b5db9008759"&gt;
  &lt;main&gt;
    &lt;p&gt;More than a decade ago, when I was applying to graduate school, I went through a period of deep uncertainty. I had tried the previous year and hadn’t gotten in anywhere. I wanted to try again, but I had a lot going against me.&lt;/p&gt;
    &lt;p&gt;I’d spent most of my undergrad building a student job-portal startup and hadn’t balanced it well with academics. My GPA needed explaining. My GMAT score was just okay. I didn’t come from a big-brand employer. And there was no shortage of people with similar or stronger profiles applying to the same schools.&lt;/p&gt;
    &lt;p&gt;Even though I had learned a few things from the first round, the second attempt was still difficult. There were multiple points after I submitted applications where I lost hope.&lt;/p&gt;
    &lt;p&gt;But during that stretch, a friend and colleague kept repeating one line to me:&lt;/p&gt;
    &lt;p&gt;“All it takes is for one to work out.”&lt;/p&gt;
    &lt;p&gt;He’d say it every time I spiraled. And as much as it made me smile, a big part of me didn’t fully believe it. Still, it became a little maxim between us. And eventually, he was right – that one did work out. And it changed my life.&lt;/p&gt;
    &lt;p&gt;I’ve thought about that framing so many times since then.&lt;/p&gt;
    &lt;p&gt;It’s unbelievably powerful in any high-stakes search:&lt;/p&gt;
    &lt;p&gt;You don’t need every job to choose you. You just need the one that’s the right fit.&lt;/p&gt;
    &lt;p&gt;You don’t need every house to accept your offer. You just need the one that feels like home.&lt;/p&gt;
    &lt;p&gt;You don’t need every person to want to build a life with you. You just need the one.&lt;/p&gt;
    &lt;p&gt;You don’t need ten universities to say yes. You just need the one that opens the right door.&lt;/p&gt;
    &lt;p&gt;These processes – college admissions, job searches, home buying, finding a partner – can be emotionally brutal. They can get you down in ways that feel personal. But in those moments, that truth can be grounding.&lt;/p&gt;
    &lt;p&gt;All it takes is for one to work out.&lt;/p&gt;
    &lt;p&gt;And that one is all you need.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090433</guid><pubDate>Sat, 29 Nov 2025 20:22:44 +0000</pubDate></item><item><title>Joe Armstrong interviews Alan Kay (2016) [video]</title><link>https://www.youtube.com/watch?v=fhOHn9TClXY</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090574</guid><pubDate>Sat, 29 Nov 2025 20:37:59 +0000</pubDate></item><item><title>Show HN: Nano PDF – A CLI Tool to Edit PDFs with Gemini's Nano Banana</title><link>https://github.com/gavrielc/Nano-PDF</link><description>&lt;doc fingerprint="781429fa99a77b84"&gt;
  &lt;main&gt;
    &lt;p&gt;A CLI tool to edit PDF slides using natural language prompts, powered by Google's Gemini 3 Pro Image ("Nano Banana") model.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Natural Language Editing: "Update the graph to include data from 2025", "Change the chart to a bar graph".&lt;/item&gt;
      &lt;item&gt;Add New Slides: Generate entirely new slides that match your deck's visual style.&lt;/item&gt;
      &lt;item&gt;Non-Destructive: Preserves the searchable text layer of your PDF using OCR re-hydration.&lt;/item&gt;
      &lt;item&gt;Multi-page &amp;amp; Parallel: Edit multiple pages in a single command with concurrent processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nano PDF uses Gemini 3 Pro Image (aka Nano Banana) and PDF manipulation to enable quick edits of PDFs with natural language editing:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Page Rendering: Converts target PDF pages to images using Poppler&lt;/item&gt;
      &lt;item&gt;Style References: Optionally includes style reference pages with generation request to understand visual style (fonts, colors, layout)&lt;/item&gt;
      &lt;item&gt;AI Generation: Sends images + prompts to Gemini 3 Pro Image, which generates edited versions&lt;/item&gt;
      &lt;item&gt;OCR Re-hydration: Uses Tesseract to restore searchable text layer to generated images&lt;/item&gt;
      &lt;item&gt;PDF Stitching: Replaces original pages with AI-edited versions while preserving document structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The tool processes multiple pages in parallel for speed, with configurable resolution (4K/2K/1K) to balance quality vs. cost.&lt;/p&gt;
    &lt;code&gt;pip install nano-pdf&lt;/code&gt;
    &lt;p&gt;You need a paid Google Gemini API key with billing enabled. Free tier keys do not support image generation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Get an API key from Google AI Studio&lt;/item&gt;
      &lt;item&gt;Enable billing on your Google Cloud project&lt;/item&gt;
      &lt;item&gt;Set it as an environment variable:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;export GEMINI_API_KEY="your_api_key_here"&lt;/code&gt;
    &lt;p&gt;Note: This tool uses Gemini 3 Pro Image which requires a paid API tier. See pricing for details.&lt;/p&gt;
    &lt;p&gt;Edit a single page (e.g., Page 2):&lt;/p&gt;
    &lt;code&gt;nano-pdf edit my_deck.pdf 2 "Change the title to 'Q3 Results'"&lt;/code&gt;
    &lt;p&gt;Edit multiple pages in one go:&lt;/p&gt;
    &lt;code&gt;nano-pdf edit my_deck.pdf \
  1 "Update date to Oct 2025" \
  5 "Add company logo" \
  10 "Fix typo in footer"&lt;/code&gt;
    &lt;p&gt;Insert a new AI-generated slide into your deck:&lt;/p&gt;
    &lt;code&gt;# Add a title slide at the beginning
nano-pdf add my_deck.pdf 0 "Title slide with 'Q3 2025 Review'"

# Add a slide after page 5
nano-pdf add my_deck.pdf 5 "Summary slide with key takeaways as bullet points"&lt;/code&gt;
    &lt;p&gt;The new slide will automatically match the visual style of your existing slides and uses document context by default for better relevance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--use-context&lt;/code&gt;/&lt;code&gt;--no-use-context&lt;/code&gt;: Include the full text of the PDF as context for the model. Disabled by default for&lt;code&gt;edit&lt;/code&gt;, enabled by default for&lt;code&gt;add&lt;/code&gt;. Use&lt;code&gt;--no-use-context&lt;/code&gt;to disable.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--style-refs "1,5"&lt;/code&gt;: Manually specify which pages to use as style references.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--output "new.pdf"&lt;/code&gt;: Specify the output filename.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--resolution "4K"&lt;/code&gt;: Image resolution - "4K" (default), "2K", or "1K". Higher quality = slower processing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--disable-google-search&lt;/code&gt;: Prevents the model from using Google Search to find information before generating (enabled by default).&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Fix typos across multiple slides
nano-pdf edit pitch_deck.pdf \
  3 "Fix the typo 'recieve' to 'receive'" \
  7 "Change 'Q4 2024' to 'Q1 2025'"&lt;/code&gt;
    &lt;code&gt;# Update branding and colors
nano-pdf edit slides.pdf 1 "Make the header background blue and text white" \
  --style-refs "2,3" --output branded_slides.pdf&lt;/code&gt;
    &lt;code&gt;# Update financial data
nano-pdf edit report.pdf 12 "Update the revenue chart to show Q3 at $2.5M instead of $2.1M"&lt;/code&gt;
    &lt;code&gt;# Use full document context for consistency
nano-pdf edit presentation.pdf \
  5 "Update the chart colors to match the theme" \
  8 "Add the company logo in the bottom right" \
  --use-context&lt;/code&gt;
    &lt;code&gt;# Add a new agenda slide at the beginning
nano-pdf add quarterly_report.pdf 0 "Agenda slide with: Overview, Financial Results, Q4 Outlook"&lt;/code&gt;
    &lt;code&gt;# Google Search is enabled by default - the model can look up current information
nano-pdf edit deck.pdf 5 "Update the market share data to latest figures"

# Disable Google Search if you want the model to only use provided context
nano-pdf add deck.pdf 3 "Add a summary slide" --disable-google-search&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.10+&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;poppler&lt;/code&gt;(for PDF rendering)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tesseract&lt;/code&gt;(for OCR)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;brew install poppler tesseract&lt;/code&gt;
    &lt;code&gt;choco install poppler tesseract&lt;/code&gt;
    &lt;p&gt;Note: After installation, you may need to restart your terminal or add the installation directory to your PATH.&lt;/p&gt;
    &lt;code&gt;sudo apt-get install poppler-utils tesseract-ocr&lt;/code&gt;
    &lt;p&gt;Make sure you've installed poppler and tesseract for your platform. After installation, restart your terminal to refresh PATH. Run &lt;code&gt;which pdftotext&lt;/code&gt; and &lt;code&gt;which tesseract&lt;/code&gt; to verify they're accessible.&lt;/p&gt;
    &lt;p&gt;Set your API key as an environment variable:&lt;/p&gt;
    &lt;code&gt;export GEMINI_API_KEY="your_key_here"&lt;/code&gt;
    &lt;p&gt;Gemini 3 Pro Image requires a paid API tier. Visit Google AI Studio to enable billing on your project.&lt;/p&gt;
    &lt;p&gt;Try using &lt;code&gt;--style-refs&lt;/code&gt; to specify reference pages that have the desired visual style. The model will analyze these pages to better match fonts, colors, and layout.&lt;/p&gt;
    &lt;p&gt;The tool uses Tesseract OCR to restore searchable text. For best results, ensure your generated images are high resolution (&lt;code&gt;--resolution "4K"&lt;/code&gt;). Note that OCR may not be perfect for stylized fonts or small text.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use &lt;code&gt;--resolution "2K"&lt;/code&gt;or&lt;code&gt;--resolution "1K"&lt;/code&gt;for faster processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to run the latest development version:&lt;/p&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/gavrielc/Nano-PDF.git
cd Nano-PDF

# Install dependencies
pip install -e .

# Run the tool
nano-pdf edit my_deck.pdf 2 "Your edit here"&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090619</guid><pubDate>Sat, 29 Nov 2025 20:44:24 +0000</pubDate></item><item><title>Europe's New War on Privacy</title><link>https://unherd.com/2025/11/europes-new-war-on-privacy/</link><description>&lt;doc fingerprint="bf4cd1750eb0f74d"&gt;
  &lt;main&gt;
    &lt;p&gt;In theory, Chat Control should have been buried last month. The EU’s ominous plan to mass-scan citizens’ private messages was met with overwhelming public resistance in Germany, with the country’s government refusing to approve it. But Brussels rarely retreats merely because the public demands it. And so, true to form, a reworked version of the text is already being pushed forward — this time out of sight, behind closed doors.&lt;/p&gt;
    &lt;p&gt;Chat Control, formally known as the Child Sexual Abuse Regulation, was first proposed by the European Commission in 2022. The original plan would have made it mandatory for email and messenger providers to scan private, even encrypted, communications — with the purported aim of detecting child sexual abuse material.&lt;/p&gt;
    &lt;p&gt;The tool was sold as a noble crusade against some of the world’s most horrific crimes. But critics argued that the tool risked becoming a blueprint for generalised surveillance, by essentially giving states and EU institutions the ability to scan every private message. Indeed, a public consultation preceding the proposal revealed that a majority of respondents opposed such obligations, with over 80% explicitly rejecting its application to end-to-end encrypted communications.&lt;/p&gt;
    &lt;p&gt;Yet despite repeated blockages, and widespread criticism for violating privacy and fundamental rights, the text was never abandoned. Instead, it was repackaged, and continually pushed forward from one Council presidency to the next. Each time democratic resistance stopped the original plan, it kept returning in new forms, under new labels, each time dressed up as a “necessary” and “urgent” tool to protect children online, yet always preserving its core logic: normalising government-mandated monitoring of private communications on an unprecedented scale.&lt;/p&gt;
    &lt;p&gt;In May, the European Commission once again presented its proposal. Yet several states objected. That included Germany, but also Poland, Austria and the Netherlands. As a result, Denmark, which currently holds the rotating presidency of the European Council, immediately began drafting a new version, known as “Chat Control 2.0” and unveiled earlier this month, which removed the requirement for general monitoring of private chats; the searches would now remain formally voluntary for providers. All this happened under the auspices of Coreper, the Committee of Permanent Representatives — one of the most powerful, but least visible, institutions in the EU decision-making process. It is where most EU legislation is actually negotiated; if Coreper agrees on a legislative file, member states almost always rubber-stamp it.&lt;/p&gt;
    &lt;p&gt;The gamble worked. Yesterday, this revised version was quietly greenlit by Coreper, essentially paving the way for the text’s adoption by the Council, possibly as early as December. As digital rights campaigner and former MEP Patrick Breyer put it, this manoeuvre amounts to “a deceptive sleight of hand” aimed at bypassing meaningful democratic debate and oversight.&lt;/p&gt;
    &lt;p&gt;While the removal of mandatory on-device detection is an improvement on the first draft, the new text still contains two extremely problematic features. First, it encourages “voluntary” mass scanning by online platforms — a practice already allowed in “temporary” form, which would now become a lasting feature of EU law. Second, it effectively outlaws anonymous communication by introducing mandatory age-verification systems.&lt;/p&gt;
    &lt;p&gt;An open letter signed by 18 of Europe’s leading cybersecurity and privacy academics warned that the latest proposal poses “high risks to society without clear benefits for children”. The first, in their view, is the expansion of “voluntary” scanning, including automated text analysis using AI to identify ambiguous “grooming” behaviours. This approach, they argue, is deeply flawed. Current AI systems are incapable of properly distinguishing between innocent conversation and abusive behaviour. As the experts explain, AI-driven grooming detection risks sweeping vast numbers of normal, private conversations into a dragnet, overwhelming investigators with false positives and exposing intimate communications to third parties.&lt;/p&gt;
    &lt;p&gt;Breyer further emphasised this danger by noting that no AI can reliably distinguish between innocent flirtation, humorous sarcasm — and criminal grooming. He warned that this amounts to a form of digital witch-hunt, whereby the mere appearance of words like “love” or “meet” in a conversation between family members, partners or friends could trigger intrusive scrutiny. This is not child protection, Breyer has argued, but mass suspicion directed at the entire population. Even under the existing voluntary regime, German federal police warn that roughly half of all reports received are criminally irrelevant, representing tens of thousands of leaked legal chats annually. According the Swiss Federal Police, meanwhile, 80% of machine-reported content is not illegal. It might, for example, encompass harmless holiday photos showing nude children playing at a beach. The new text would expand these risks dramatically.&lt;/p&gt;
    &lt;p&gt;Further concerns arise from Article 4 of the new compromise proposal, which requires providers to implement “all appropriate risk mitigation measures”. This clause could allow authorities to pressure encrypted messaging services to enable scanning, even if this undermines their core security model. In practice, this could mean requiring providers such as WhatsApp, Signal or Telegram to scan messages on users’ devices before encryption is applied.&lt;/p&gt;
    &lt;p&gt;The Electronic Frontier Foundation has noted that this approach risks creating a permanent security infrastructure, one which could gradually become universal. Meta, Google and Microsoft already scan unencrypted content voluntarily; extending this practice to encrypted content would merely require technical changes. Moreover, what begins as a voluntary option can easily become compulsory in practice, as platforms face reputational, legal and market pressure to “cooperate” with the authorities. Furthermore, this doesn’t affect just people in the EU, but everyone around the world, including the United States. If platforms decide to stay in the EU, they would be forced to scan the conversations of everyone in the bloc. If you’re not in the EU, but you chat with someone who is, then your privacy is compromised too.&lt;/p&gt;
    &lt;p&gt;Another major danger is the introduction of mandatory age-verification systems for app stores and private messaging services. Though the Council claims these systems can be designed to “preserve privacy”, critics insist that the very concept is technologically unworkable. Age assessments inevitably rely on biometric and behavioural data, both of which require invasive data collection. Far from protecting children, these systems would increase the volume of sensitive personal information being stored and potentially exploited.&lt;/p&gt;
    &lt;p&gt;Requiring official identity documents for online verification would exclude millions of people who lack easy access to digital IDs or who won’t provide such sensitive documentation merely to use a messaging service. In practice, this would spell the end of anonymous communication online, forcing users to present ID or face scans simply to open an email or messaging account. Breyer has warned that such measures would be particularly disastrous for whistleblowers, journalists, political activists and others reliant on online anonymity. It would also push under-16s towards less safe, poorly regulated alternatives that lack encryption or basic safety protections.&lt;/p&gt;
    &lt;p&gt;Ultimately, critics argue that mass surveillance is simply the wrong approach to combating child sexual exploitation. Scanning private messages does not stop the circulation of child abuse material. Platforms such as Facebook have used scanning technologies for years, yet the number of automated reports continues to rise. Moreover, mandatory scanning would still fail to detect perpetrators who distribute material through decentralised secret forums or via encrypted archives shared using only links and passwords — methods that scanning algorithms cannot successfully penetrate. The most effective strategy would be to delete known abuse material from online hosts, something Europol has repeatedly failed to do.&lt;/p&gt;
    &lt;p&gt;Chat Control, in short, would do little to actually help victims of child sexual exploitation while harming everyone else. Every message would become subject to surveillance, without any judicial oversight, contrary to long-standing guarantees of private correspondence. There’s a legal question here too. The EU Court of Justice has previously ruled that general and automatic analysis of private communications violates fundamental rights, yet the EU is now poised to adopt legislation that contravenes this precedent. Once adopted, it could take years for a new judicial challenge to overturn it.&lt;/p&gt;
    &lt;p&gt;The confidentiality of electronic communication — essential for personal privacy, business secrecy and democratic participation — would be sacrificed. Sensitive conversations could be read, analysed, wrongly flagged or even misused, as past scandals involving intelligence officials and tech employees have shown. One of the most notorious cases of intelligence abuse came from the US National Security Agency, in which multiple NSA employees were caught using the agency’s powerful surveillance tools to spy on romantic partners and ex-lovers. Leaked documents have also shown that the UK intelligence agency GCHQ captured and stored images from Yahoo webcam chats, including millions of sexually explicit images of completely innocent users. There have also been several cases of Big Tech employees — from Google to Facebook — using internal tools to spy on unsuspecting users.&lt;/p&gt;
    &lt;p&gt;Furthermore, secure encryption, a foundation of cybersecurity, would be compromised by introducing backdoors or client-side scanning tools that foreign intelligence services or criminal actors could exploit. At the same time, the responsibility for criminal investigations would shift from democratically accountable authorities to opaque corporate algorithms, with minimal transparency or oversight.&lt;/p&gt;
    &lt;p&gt;Opponents therefore argue that the EU should instead adopt a fundamentally different approach: one that protects children without undermining fundamental rights. They propose ending the current voluntary scanning of private messages by US internet companies — restoring the principle that targeted surveillance requires a judicial warrant and must be limited to individuals reasonably suspected of wrongdoing — and maintain that secure end-to-end encryption, and the right to anonymous communication, must be preserved.&lt;/p&gt;
    &lt;p&gt;Particularly worrying is the issue of function creep, the process by which a technology introduced for a narrowly defined purpose gradually expands to serve broader, and sometimes entirely different, purposes over time. The UK’s Online Safety Act, passed in October 2023, obliges firms to develop child sexual abuse detection systems, even though the British government itself admits that such infrastructure is not yet technically available, creating legal authority awaiting technical capability. In the United States, “temporary” surveillance measures introduced under the post-9/11 Patriot Act became permanent, and indeed expanded in scope. Once a technological infrastructure for comprehensive online surveillance exists, it can easily be repurposed and is hard to dismantle. Technologies designed to detect harmful content can quickly be extended to political repression; examples from authoritarian states demonstrate how similar systems are used to identify and target dissidents.&lt;/p&gt;
    &lt;p&gt;Breyer summarised this pattern starkly: “They are selling us security but delivering a total surveillance machine. They promise child protection but punish our children and criminalise privacy.” The implications are ominous. Europe effectively stands on the threshold of building a machine that can see everything. Once constructed, it will serve not only the current political authorities — the idea of Ursula von der Leyen spying on everyone’s messages is disturbing enough — but whoever wields power next. With yet another vote approaching, the window to stop Chat Control is narrowing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090794</guid><pubDate>Sat, 29 Nov 2025 21:10:15 +0000</pubDate></item><item><title>Landlock-Ing Linux</title><link>https://blog.prizrak.me/post/landlock/</link><description>&lt;doc fingerprint="7d7b98108eb566c1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Landlock-ing Linux&lt;/head&gt;
    &lt;p&gt;Nov 29, 2025&lt;/p&gt;
    &lt;head rend="h3"&gt;Landlock: What Is It?&lt;/head&gt;
    &lt;p&gt;Landlock is a Linux API that lets applications explicitly declare which resources they are allowed to access. Its philosophy is similar to OpenBSDâs &lt;code&gt;unveil()&lt;/code&gt; and (less so) &lt;code&gt;pledge()&lt;/code&gt;: programs can make a contract with the kernel stating, âI only need these files or resources â deny me everything else if Iâm compromised.â&lt;/p&gt;
    &lt;p&gt;It provides a simple, developer-friendly way to add defense-in-depth to applications. Compared to traditional Linux security mechanisms, Landlock is vastly easier to understand and integrate.&lt;/p&gt;
    &lt;p&gt;This post is meant to be an accessible introduction, and hopefully persuade you to give Landlock a try.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Does It Work?&lt;/head&gt;
    &lt;p&gt;Landlock is a Linux Security Module (LSM) available since Linux 5.13. Unlike MAC frameworks such as SELinux or AppArmor, Landlock applies transient restrictions: policies are created at runtime, enforced on the current thread and its future descendants, and disappear when the process exits.&lt;/p&gt;
    &lt;p&gt;You donât tag files with labels or extended attributes. Instead, applications create policies dynamically.&lt;/p&gt;
    &lt;p&gt;A Landlock policy consists of two pieces:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Handled accesses â the categories of operations you want to restrict (e.g., filesystem read/write).&lt;/item&gt;
      &lt;item&gt;Access grants â an explicit allowlist of which objects are permitted for those operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, you could create a policy that handles all filesystem reads/writes and network binds, and grants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;read-only access to &lt;code&gt;/home/user&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;read/write access to &lt;code&gt;/tmp&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;permission to bind to port &lt;code&gt;2222&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The application then calls &lt;code&gt;landlock_restrict_self()&lt;/code&gt; to enter the restricted domain. From that point on, that thread’s child threads and child processes are permanently constrained. Restrictions cannot be revoked.&lt;/p&gt;
    &lt;p&gt;Policies can be layered (up to 16 layers). A child layer may further reduce access, but cannot reintroduce permissions the parent layer removed. For example, a child thread may add a layer to this policy to restrict itself to only reading &lt;code&gt;/home/user&lt;/code&gt;, but it cannot regain permission to bind to port &lt;code&gt;2222&lt;/code&gt; once a layer omits this grant.&lt;/p&gt;
    &lt;p&gt;Landlock is unprivileged â any application can sandbox itself. It also uses ABI versioning, allowing programs to apply best-effort sandboxing even on older kernels lacking newer features.&lt;/p&gt;
    &lt;p&gt;It’s also a stackable LSM, meaning you can combine it with selinux or apparmor in a supplemental layer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Should You Use It?&lt;/head&gt;
    &lt;p&gt;Landlock shines when an application has a predictable set of files or directories it needs. For example, a web server could restrict itself to accessing only &lt;code&gt;/var/www/html&lt;/code&gt; and &lt;code&gt;/tmp&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Unlike SELinux or AppArmor, Landlock policies donât require administrator involvement or system-wide configuration. Developers can embed policies directly in application code, making sandboxing a natural part of the development process.&lt;/p&gt;
    &lt;p&gt;Because Landlock requires no privileges to use, adding it to most programs is straightforward.&lt;/p&gt;
    &lt;p&gt;Bindings exist for languages such as Rust, Go, and Haskell, and several projects provide user-friendly &lt;code&gt;unveil&lt;/code&gt;-style wrappers.&lt;/p&gt;
    &lt;p&gt;A official c library doesn’t exist yet unfortunately, but there’s several out there you can try.&lt;/p&gt;
    &lt;p&gt;Here’s a quick rust example:&lt;/p&gt;
    &lt;code&gt;use landlock::{
    ABI, Access, AccessFs, Ruleset, RulesetAttr, RulesetCreatedAttr, RulesetStatus, RulesetError,
    path_beneath_rules,
};

fn restrict_thread() -&amp;gt; Result&amp;lt;(), RulesetError&amp;gt; {
    let abi = ABI::V1;
    let status = Ruleset::default()
        .handle_access(AccessFs::from_all(abi))?
        .create()?
        // Read-only access to /usr, /etc and /dev.
        .add_rules(path_beneath_rules(&amp;amp;["/usr", "/etc", "/dev"], AccessFs::from_read(abi)))?
        // Read-write access to /home and /tmp.
        .add_rules(path_beneath_rules(&amp;amp;["/home", "/tmp"], AccessFs::from_all(abi)))?
        .restrict_self()?;

    match status.ruleset {
        RulesetStatus::FullyEnforced =&amp;gt; println!("Fully sandboxed."),
        RulesetStatus::PartiallyEnforced =&amp;gt; println!("Partially sandboxed."),
        RulesetStatus::NotEnforced =&amp;gt; println!("Not sandboxed! Please update your kernel."),
    }
    Ok(())
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;The State of Linux Sandboxing: Why This Matters&lt;/head&gt;
    &lt;p&gt;As Linux adoption grows, so does the amount of malware targeting desktop users. While Linux has historically enjoyed relative safety, this is largely due to smaller market share and higher technical barriers compared to Windows â not because Linux is inherently safer.&lt;/p&gt;
    &lt;p&gt;Linux is not a security panacea. For example, on most major distributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users can download and execute untrusted binaries with no warnings.&lt;/item&gt;
      &lt;item&gt;Shell scripts can be piped from the internet and executed blindly.&lt;/item&gt;
      &lt;item&gt;Many users run passwordless sudo, giving them root access on demand.&lt;/item&gt;
      &lt;item&gt;Unprivileged applications can typically: &lt;list rend="ul"&gt;&lt;item&gt;Read &lt;code&gt;~/.ssh&lt;/code&gt;,&lt;code&gt;~/.bashrc&lt;/code&gt;, browser cookies, and anything else in&lt;code&gt;$HOME&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Modify environment variables and &lt;code&gt;$PATH&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Create systemd user services&lt;/item&gt;&lt;item&gt;(on X11) log keystrokes and read input devices&lt;/item&gt;&lt;item&gt;Bind to arbitrary network ports&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Read &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Several tools try to improve the state of security on linux, but each has significant drawbacks:&lt;/p&gt;
    &lt;head rend="h4"&gt;Containerization (docker, podman)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Designed for service isolation, not desktop apps.&lt;/item&gt;
      &lt;item&gt;Managing home directory access is clunky.&lt;/item&gt;
      &lt;item&gt;Many users break isolation by using &lt;code&gt;--privileged&lt;/code&gt;or&lt;code&gt;--network host&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Flatpak / Snap&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Great for graphical applications (Flatpak especially).&lt;/item&gt;
      &lt;item&gt;Often require overly broad permissions.&lt;/item&gt;
      &lt;item&gt;Less suitable for CLI tools.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Firejail&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires per-application profiles.&lt;/item&gt;
      &lt;item&gt;Must be explicitly invoked each time, or you need a wrapper script.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From the developer side:&lt;/p&gt;
    &lt;head rend="h4"&gt;seccomp&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Powerful syscall filtering.&lt;/item&gt;
      &lt;item&gt;Tedious and error-prone to configure.&lt;/item&gt;
      &lt;item&gt;Blacklists are fragile; new syscalls can break things.&lt;/item&gt;
      &lt;item&gt;Argument filtering is difficult and full of TOCTOU hazards.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;SELinux&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extremely powerful, but difficult to understand.&lt;/item&gt;
      &lt;item&gt;Requires system-wide policies and admin involvement.&lt;/item&gt;
      &lt;item&gt;Many users disable it due to complexity.&lt;/item&gt;
      &lt;item&gt;Not enabled on most distributions by default. (used a lot in android)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;AppArmor&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Easier than SELinux, but still requires admin-defined profiles.&lt;/item&gt;
      &lt;item&gt;Applies system-wide and lacks per-process namespacing.&lt;/item&gt;
      &lt;item&gt;Gets disabled by many distributions, but is more commonly used in the desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Landlock&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unprivileged&lt;/item&gt;
      &lt;item&gt;Application-centric&lt;/item&gt;
      &lt;item&gt;Easy to integrate&lt;/item&gt;
      &lt;item&gt;Deny-by-default&lt;/item&gt;
      &lt;item&gt;Widely supported since 5.13&lt;/item&gt;
      &lt;item&gt;Backward and forward compatibility mechanisms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Landlock isnât perfect, but it fills a major gap: a simple, self-contained unprivileged sandboxing tool.&lt;/p&gt;
    &lt;head rend="h3"&gt;What landlock could bring to the table:&lt;/head&gt;
    &lt;p&gt;Long-running system daemons that run with elevated privileges could benefit from landlock restrictions.&lt;/p&gt;
    &lt;p&gt;Desktop applications dealing with binary formats, like pdf readers, image viewers web browsers, and word processors can be restricted to accessing the files they originally opened.&lt;/p&gt;
    &lt;p&gt;FTP and HTTP servers can be bound to the files they need. Even if nginx is running as root, if an attacker gets a full reverse shell, they won’t be able to see access files outside the policy.&lt;/p&gt;
    &lt;p&gt;If the supervisor proposal gets added, we could bring an android-like permissions system to the linux desktop. Flatpak does a decent job at this, but imagine if every process in your desktop would need to explicitly ask (at least once) before accessing sensitive files or resources.&lt;/p&gt;
    &lt;p&gt;Pair that with an accessible GUI and a system for handling updates and saving permission grants, and we have potential for a safer, more secure linux user experience on the desktop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ongoing Work in Landlock&lt;/head&gt;
    &lt;p&gt;Several promising features are under active development:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Supervise Mode&lt;/p&gt;&lt;lb/&gt;Lets a userspace âsupervisorâ interactively allow or deny access â similar to Android-style permission prompts.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Socket Restrictions Fine-grained control over which types of sockets or ports processes may use.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_RESTRICT_SELF_TSYNC Ensures restrictions propagate to all threads in a process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_ADD_RULE_QUIET Allows suppressing audit messages for certain objects.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_ADD_RULE_NO_INHERIT (disclosure: this is my patch series) Prevents rules from unintentionally inheriting permissions from parent directories, giving finer-grained filesystem control.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Landlock is a simple, unprivileged, deny-by-default sandboxing mechanism for Linux.&lt;lb/&gt; Itâs easy to understand, easy to integrate, and has tremendous potential for improving desktop and application security.&lt;/p&gt;
    &lt;p&gt;Give it a try in your application.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090969</guid><pubDate>Sat, 29 Nov 2025 21:30:53 +0000</pubDate></item></channel></rss>