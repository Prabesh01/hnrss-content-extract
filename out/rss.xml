<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 28 Sep 2025 14:07:13 +0000</lastBuildDate><item><title>Haydex: From Zero to 178.6B rows a second in 30 days</title><link>https://axiom.co/blog/building-haydex</link><description>&lt;doc fingerprint="ef1899554df0cae3"&gt;
  &lt;main&gt;
    &lt;head rend="h5"&gt;Author&lt;/head&gt;
    &lt;head rend="h5"&gt;Tomás Senart&lt;/head&gt;
    &lt;p&gt;Principal Engineer&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I/O architecture determines scale: One large read instead of thousands of small reads changed everything&lt;/item&gt;
      &lt;item&gt;Profiler-driven optimization: 90% of allocations and 70% of CPU were hiding in unexpected places&lt;/item&gt;
      &lt;item&gt;Distributed redesign unlocks speed: Map-reduce Lambda architecture delivered 6x indexing speedup&lt;/item&gt;
      &lt;item&gt;Compound optimizations multiply: Each optimization amplified others to reach 673 billion rows/second&lt;/item&gt;
      &lt;item&gt;Production beats theory: V0's elegant design failed; V1 succeeded by respecting network physics&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nearly every great engineering story starts not with a grand plan, but with a nagging, infuriating problem.&lt;/p&gt;
    &lt;p&gt;Ours was simple: our needle-in-the-haystack queries were too slow. For a database company, that's an existential threat. Our customers, especially giants like Hyperscale Customer, were pushing data at a scale that made our brute-force scanning approach look like trying to find a specific grain of sand on a planet-sized beach with a teaspoon. We had to do something drastic.&lt;/p&gt;
    &lt;p&gt;This is the story of that something. It's the story of a project that had been tried before and shelved, a project that rose from the dead.&lt;/p&gt;
    &lt;p&gt;In a single, caffeine-fueled month between June 9 and July 8, 2025, we took Haydex, our dream of a hyper-fast filtering system, and forged it into a production-hardened reality.&lt;/p&gt;
    &lt;p&gt;It was a journey into the abyss of distributed systems, a battle against memory bottlenecks, API limits, and our own assumptions.&lt;/p&gt;
    &lt;p&gt;We started with a Slack message that read "The Grand Haydex Revival" and ended with a system clocking an effective throughput of 178,600,000,000 rows per second-and peaking at a synergistic 673,850,000,000 rows per second with its caching counterpart.&lt;/p&gt;
    &lt;p&gt;This is how we did it.&lt;/p&gt;
    &lt;p&gt;Background&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In this post we're talking about EventDB, our purpose-built petabyte-scale event datastore which powers Axiom's events, logs, and traces support.&lt;/item&gt;
      &lt;item&gt;EventDB has a custom stateless ingest pipeline, only uses object-storage for storing all ingested data, and has a completely serverless (lambda-powered) query engine.&lt;/item&gt;
      &lt;item&gt;We report effective throughput as &lt;code&gt;effective_rows_per_sec = candidate_rows_before_pruning ÷ wall_clock_seconds&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Numbers in this post come from production runs on Hyperscale Customer’s dataset (with permission, of course).&lt;/item&gt;
      &lt;item&gt;For “Haydex only,” the zero‑matches cache was off; for “Haydex + cache,” it was on.&lt;/item&gt;
      &lt;item&gt;Hardware and cache state were held constant within each comparison. We fix the filter’s target false‑positive rate (FPR) per run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Micro-glossary&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Field‑scoped filter - One large filter per field (e.g., body) spanning thousands of blocks. Inside that filter, each block is the document.&lt;/item&gt;
      &lt;item&gt;Block - The pruning unit; only surviving blocks fan out to workers.&lt;/item&gt;
      &lt;item&gt;FPR (false‑positive rate) - Probability the filter returns “maybe” for a non‑matching block; it never returns false negatives.&lt;/item&gt;
      &lt;item&gt;Zero‑matches cache - Caches known‑empty predicate/interval combos to skip re‑evaluation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The ghost of Haydex past&lt;/head&gt;
    &lt;p&gt;The idea of using probabilistic filters to accelerate queries wasn't new. At its heart, it's a simple, powerful concept: a data structure that can tell you with blistering speed if the needle you're looking for is definitely NOT in a haystack. It might occasionally lie and say something is there when it's not (a false positive), but it will never lie about something being absent. For query planning, that's a perfect trade-off.&lt;/p&gt;
    &lt;p&gt;We had a V0 that tried to do this. It was, for lack of a better word, a train wreck. Not because the core data structure was wrong, but because our initial execution was naive - a perfect monument to Good Ideas on Paper™ that crumble on contact with reality.&lt;/p&gt;
    &lt;p&gt;V0's design seemed logical, almost elegant, in the sterile vacuum of a design document. We would create one filter file per data block. Inside that filter, the individual columns of that block were the "documents" (events) we were indexing. We'd then use this with "adaptive execution," a fancy term for letting the query plan change its mind on the fly based on what these fine-grained filters whispered to it.&lt;/p&gt;
    &lt;p&gt;In practice, this was a catastrophic design. While the filter structure was sound, the granularity was wrong! A query spanning thousands of blocks still had to perform thousands of independent, latency-sensitive reads from S3-one for each block's filter file. We would fan out a query to thousands of Lambda workers, and only then would each worker attempt to fetch its own tiny filter file.&lt;/p&gt;
    &lt;p&gt;It was death by a thousand GETs. The I/O overhead didn't just negate the potential gains; it actively made things slower. We also stumbled into a more insidious problem: false positives born from a flawed document model. V0 treated the entire column within a block as a single document. So the filter could correctly tell us, "Yes, the hashes for 'user' and 'failed' and 'login' are all in this block's message column" but it couldn't tell us they weren't in the same row. We were finding the letters but had no idea if they spelled the right word.&lt;/p&gt;
    &lt;p&gt;The whole thing was too slow, too complex, and ultimately, a dead end. I did the only sensible thing: I shut it down, wrote up the post-mortem, and turned the page. I had learned-the hard way. The ghost of Haydex Past would haunt me, but it would also serve as a constant, nagging reminder of what not to do.&lt;/p&gt;
    &lt;p&gt;The problem, of course, didn't go away. The need to find needles in ever-growing haystacks only became more acute. I knew a solution would be a game-changer. I just needed a much, much better plan.&lt;/p&gt;
    &lt;head rend="h2"&gt;The grand Haydex revival&lt;/head&gt;
    &lt;p&gt;By mid-May 2025, the pressure was immense. Hyperscale Customer, a user operating at a scale that stretches the very definition of "web scale," was pushing us to our absolute limits. Their need for fast, targeted searches wasn't a "nice to have"; it was existential for their use case. The ghost of my failed filter experiment lingered in the backlog, but the problem it was meant to solve had metastasized into a five-alarm fire. It was time for a reckoning.&lt;/p&gt;
    &lt;p&gt;On May 16th, after days spent digging through a mountain of research papers on modern filter designs, I dropped a message into our team Slack channel: "The Grand Haydex Revival". This wasn't just a restart; it was a complete teardown and reimagining, informed by the deep scars of V0.&lt;/p&gt;
    &lt;p&gt;Haydex V1 would be V0's antithesis. I threw out everything that had hurt us.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;From Block-Scoped to Field-Scoped. This was the crucial pivot. Instead of creating one small filter file per block, V1 would create one massive, field-scoped filter that covered a single field (like body) across thousands of blocks. Inside this single filter, each block became a 'document'. This flipped the economics of I/O on its head: checking a term across 10,000 blocks now meant one targeted read from one large file, not 10,000 individual reads from 10,000 tiny files.&lt;/item&gt;
      &lt;item&gt;No more "adaptive execution." That was too clever by half, a premature optimization that cost us dearly. V1 would use brutally effective early pruning. We would make the decision to kill or keep blocks right at the beginning, before fanning out a single request to a worker. Save I/O, save compute, save time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The plan was audacious, borderline reckless: go from concept to a production-ready system, indexing data at scale, in one month. There was no formal design doc, only a vision born from the ashes of failure, a Slack channel that was about to become a war room, and a relentless, unapologetic bias for action.&lt;/p&gt;
    &lt;code&gt;# v0: block-scoped filters
[query]
  ├─&amp;gt; [lambda] ── GET [filter block #1]
  ├─&amp;gt; [lambda] ── GET [filter block #2]
  ├─&amp;gt;  … thousands more …
  └─&amp;gt; [lambda] ── GET [filter block N]
late adaptive execution (pruning after fan‑out)  ⇒ high fan‑out, high latency

# v1: field-scoped filters
[query] ── read [field filter: body] ── early prune (keep/kill block ids)
  ├─&amp;gt; [worker] on surviving blocks
  └─&amp;gt; [worker] …&lt;/code&gt;
    &lt;head rend="h2"&gt;Week 1 (June 9): Quick wins and brutal truths&lt;/head&gt;
    &lt;p&gt;The first week was a frantic blur of hacking. I revived the shelved code from V0 that we could reuse and massaged it into shape. The core data structures were adapted and the initial indexer service was setup. The plan was simple on the surface: a single service running on a beefy machine with fast storage for temporary data. It would wake up, scan for new data blocks, group them into time scoped batches, and build the filters. Simple.&lt;/p&gt;
    &lt;p&gt;Almost immediately, I slammed head-first into a wall. It wasn't what I expected.&lt;/p&gt;
    &lt;p&gt;The metadata catalog, living in Postgres, was grinding to a halt. The queries to figure out which indexes covered which blocks were a bottleneck of monumental proportions. A trace told the brutal truth: most of the time was just the application staring at Postgres, waiting for it to respond. Before I could even think about indexing performance, emergency surgery had to be performed on our catalog schema.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;PR #13517 (+588 −112)&lt;/code&gt; was that surgery. It was a major refactor. We had an inefficient &lt;code&gt;block_nums&lt;/code&gt; array, which was forcing Postgres into painful full scans. It was replaced with a proper mapping table that could be efficiently indexed. An in-memory caching layer was added to shield the database from repetitive lookups. The result was staggering. Catalog query latency plummeted by 94-98%. One bottleneck down.&lt;/p&gt;
    &lt;p&gt;The next one surfaced instantly, like a game of whack-a-mole. Hashing. The simple act of generating hashes for every term in a column was an ecological disaster of CPU cycles and memory allocations. The profiler showed HashColumn allocating memory as if it were free, creating mountains of garbage for the GC to clean up.&lt;/p&gt;
    &lt;p&gt;Time to tear it apart. In &lt;code&gt;PR #13592 (+906 −29)&lt;/code&gt;, I re-wrote HashColumn from the ground up, eliminating all the intermediate object allocations that were killing performance. It added &lt;code&gt;sync. Pool&lt;/code&gt; for our hash sets to aggressively recycle memory and added fast paths for ASCII string processing to avoid expensive UTF-8 machinery when it wasn't needed. The impact was, frankly, explosive. We observed up to a 73.89% reduction in execution time and a mind-boggling 90.74% reduction in memory allocations.&lt;/p&gt;
    &lt;code&gt;goos: darwin
goarch: arm64
cpu: Apple M3 Max
                                     │  before.txt  │            after.txt             │
                                     │    sec/op    │   sec/op     vs base             │
HashColumn/Ints-16                      604.0µ ± 2%   176.9µ ± 2%  -70.72% (p=0.002 n=6)
HashColumn/Floats-16                    631.5µ ± 7%   202.2µ ± 2%  -67.97% (p=0.002 n=6)
HashColumn/Strings-16                   5.857m ± 3%   1.561m ± 0%  -73.35% (p=0.002 n=6)
HashColumn/String_WithNulls-16         190.05µ ± 4%   59.84µ ± 0%  -68.51% (p=0.002 n=6)
HashColumn/String_HighCardinality-16    522.6µ ± 3%   136.4µ ± 0%  -73.89% (p=0.002 n=6)
HashColumn/String_LowCardinality-16    124.08µ ± 0%   48.43µ ± 1%  -60.97% (p=0.002 n=6)
geomean                                 549.5µ        167.5µ       -69.52%

                                     │  before.txt   │             after.txt             │
                                     │     B/op      │     B/op      vs base             │
HashColumn/Ints-16                      670.2Ki ± 0%   160.1Ki ± 0%  -76.10% (p=0.002 n=6)
HashColumn/Floats-16                    670.2Ki ± 0%   160.2Ki ± 0%  -76.10% (p=0.002 n=6)
HashColumn/Strings-16                  7670.0Ki ± 0%   710.3Ki ± 0%  -90.74% (p=0.002 n=6)
HashColumn/String_WithNulls-16         336.71Ki ± 0%   96.08Ki ± 0%  -71.46% (p=0.002 n=6)
HashColumn/String_HighCardinality-16   1012.1Ki ± 0%   120.1Ki ± 0%  -88.13% (p=0.002 n=6)
HashColumn/String_LowCardinality-16    176.31Ki ± 0%   80.20Ki ± 0%  -54.51% (p=0.002 n=6)
geomean                                 769.1Ki        160.1Ki       -79.18%

                                     │  before.txt   │            after.txt             │
                                     │   allocs/op   │  allocs/op   vs base             │
HashColumn/Ints-16                       15.000 ± 0%    4.000 ± 0%  -73.33% (p=0.002 n=6)
HashColumn/Floats-16                     15.000 ± 0%    4.000 ± 0%  -73.33% (p=0.002 n=6)
HashColumn/Strings-16                    50.02k ± 0%   10.00k ± 0%  -80.00% (p=0.002 n=6)
HashColumn/String_WithNulls-16         2008.000 ± 0%    4.000 ± 0%  -99.80% (p=0.002 n=6)
HashColumn/String_HighCardinality-16   5006.000 ± 0%    4.000 ± 0%  -99.92% (p=0.002 n=6)
HashColumn/String_LowCardinality-16    2010.000 ± 0%    4.000 ± 0%  -99.80% (p=0.002 n=6)
geomean                                   781.3         14.74       -98.11%&lt;/code&gt;
    &lt;p&gt;By Friday, June 13th, we had the first taste of real victory. With these core pieces in place, I enabled indexing for Hyperscale Customer's live production logs. The result: an 8.85x speedup on a real-world query. We were on track.&lt;/p&gt;
    &lt;p&gt;The feeling in the channel was electric. I pasted the lyrics to Queen's "Don't Stop Me Now" into Slack. That was the vibe. Haydex was a shooting star, leaping through the sky.&lt;/p&gt;
    &lt;head rend="h2"&gt;Week 2 (June 14): Hitting the scaling wall&lt;/head&gt;
    &lt;p&gt;The high from our early wins was intoxicating, but it masked a deeper, more fundamental problem. Our single-node indexer, for all its beefy specs, was choking. We were trying to index data for a customer with a truly biblical workload. The indexer would lock up, its memory completely exhausted, the garbage collector thrashing so hard it couldn't even trigger an OOMKill. (This is a phenomenon where the system spends more time cleaning up memory than actually doing work.) It was just... stuck. Frozen in a state of silent, high-CPU agony, a zombie process consuming resources but doing no work.&lt;/p&gt;
    &lt;p&gt;On June 14th, it happened again. I posted a weary message to Slack: "Indexer locked up, will kick it. Glad we'll be moving to Lambdas for the hashing soon"&lt;/p&gt;
    &lt;p&gt;That casual "soon" had to become "now." The single-node architecture was a dead end. We needed to distribute the work. An idea I had floated two days earlier became the new rallying cry: a full-scale distributed architecture.&lt;/p&gt;
    &lt;p&gt;I pivoted our design to a map-reduce-like model that completely changed our approach. The "map" phase involved a massive fan-out to parallel workers, each processing a small, independent chunk of data. This distributed workload was key to breaking the memory bottleneck. Once the parallel processing was complete, a centralized service would then perform the "reduce" phase, efficiently assembling the final, massive filter.&lt;/p&gt;
    &lt;p&gt;This was a massive architectural change, but it paid off almost instantly. By June 18th, the new distributed design was deployed. The indexing latency for a large batch of Hyperscale Customer's columns plummeted from around 3.5 minutes to just 30-40 seconds. We were finally able to start a full backfill, indexing an entire week of their production data. It completed in just 6 hours-a task that would have been physically impossible on the old system.&lt;/p&gt;
    &lt;p&gt;The relief was palpable. We were firing on all cylinders. I posted a picture from a brief time off, a photo of myself intensely trying to relax, with the caption: "Guys I'm touching grass". It was a much-needed moment of levity in the middle of the storm.&lt;/p&gt;
    &lt;head rend="h2"&gt;Week 3 (June 21): Death by a thousand paper cuts&lt;/head&gt;
    &lt;p&gt;Scaling is never a clean process. Solving the CPU bottleneck with our new distributed architecture just revealed the next layer of problems. We had slain the beast, only to find it was a hydra. For every head we lopped off, two more grew in its place. Welcome to the wonderful world of distributed systems.&lt;/p&gt;
    &lt;p&gt;First came the mysterious context canceled errors. They were insidious, popping up randomly and bringing the hashing process to a grinding halt. After a long, painful debugging session that stretched across time zones, we traced it back to our object storage client code. We were using an errgroup context that would prematurely cancel ongoing background reads if another goroutine in the group hit an unrelated error. One goroutine would stumble, and the context would yank the rug out from under all the others that were still happily reading data from S3. One targeted fix in &lt;code&gt;PR #13692 (+151 −24)&lt;/code&gt; later, that beast was slain.&lt;/p&gt;
    &lt;p&gt;Then, S3 itself began to fight back. We started hitting the &lt;code&gt;DeleteObjects&lt;/code&gt; API limit. The API can only delete 1000 keys in a single request, and our cleanup process for temporary hash files was naively trying to delete thousands at once. In &lt;code&gt;PR #13705 (+110 −16)&lt;/code&gt;, I refactored our &lt;code&gt;DeleteMany&lt;/code&gt; function to be smarter, splitting the requests into 1000-object chunks and firing them off concurrently. Another head lopped off the hydra. Then came the intermittent &lt;code&gt;InvalidPart&lt;/code&gt; errors on large multipart uploads, which I tackled by making our S3 uploader configuration more robust and tuneable, giving us more control over part sizes and concurrency. Each fix felt like a victory, but the next problem was always just around the corner, waiting.&lt;/p&gt;
    &lt;code&gt;operation error S3: DeleteObjects, https response error StatusCode: 400,
RequestID: 022d5d98230001978435203f0407cd5df403dd06, HostID: akoWM6KLQQfI,
api error MalformedXML: The XML you provided was not well-formed...&lt;/code&gt;
    &lt;code&gt;upload multipart failed...
cause: operation error S3: CompleteMultipartUpload, exceeded maximum number
of attempts, 3,
api error InvalidPart: One or more of the specified parts could not be found.&lt;/code&gt;
    &lt;p&gt;While firefighting, we uncovered another massive optimization, almost by accident. On June 20th, I posted the results of a deep-dive benchmark into our block metadata loading strategy. The results, laid out in a detailed doc, were a slap in the face. We were loading way, way too much data from Postgres before we even got to the pruning stage. The dominant cost was fetching and deserializing the heavy stats data for blocks that we were just going to throw away anyway.&lt;/p&gt;
    &lt;p&gt;This revelation led directly to the "lazy loading" optimization in &lt;code&gt;PR #13737 (+1,807 −2,891)&lt;/code&gt;. It was a simple but profound change in philosophy. I introduced a &lt;code&gt;block.Tiny struct&lt;/code&gt;, a minimal, skeletal representation of a block containing only what was absolutely necessary for pruning. The query runner would load just these tiny structs for all candidate blocks, run them through the Haydex and zero-matches cache pruners, and only then go back to the database to fetch the full, heavy block metadata for the handful of blocks that survived.&lt;/p&gt;
    &lt;code&gt;// Tiny is a minimal representation of a block, used for efficient pruning.
type Tiny struct {
	Num          Num
	CompactionID uint32
}&lt;/code&gt;
    &lt;p&gt;The impact was a staggering 12.7x speedup for queries where pruning was highly effective. I was seeing the effect of compounding optimizations. The faster catalog made queries viable. The distributed indexer made large-scale filters possible. And the lazy loading made the pruning process itself lightning fast. Each hard-won victory amplified the next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Week 4 (June 28): Ludicrous speed&lt;/head&gt;
    &lt;p&gt;By the last week of June, it felt like Haydex had achieved escape velocity. All the pieces were finally clicking into place. The distributed indexer was stable and chewing through backlogs. The query path was hardened and optimized. The low-hanging fruit had been picked, and I had started to climb the tree towards the real rewards..&lt;/p&gt;
    &lt;p&gt;The weekly update email from July 5th told the story. We had achieved 110 billion rows per second effective throughput in production testing against Hyperscale Customer's live, trillion-scale workloads. This wasn't a sterile lab benchmark with synthetic data. This was real-world performance on a massive, messy, production dataset. This was the moment I knew I had built something special.&lt;/p&gt;
    &lt;p&gt;But we kept pushing.&lt;/p&gt;
    &lt;p&gt;On July 6th, I ran a query with Haydex enabled but with our zero-matches cache (a separate, existing sub-system) turned off. The result popped up on my screen: an effective throughput of 178,600,000,000 rows per second. Then, for the grand finale, we re-enabled the zero-matches cache on the already-pruned results. The synergy was incredible. The two systems, working in concert, peaked at an effective throughput of 673,850,000,000 rows per second.&lt;/p&gt;
    &lt;p&gt;These weren't just vanity metrics. They were the result of a month of compounded, hard-won optimizations. The fast catalog lookups, the hyper-efficient hashing, the distributed indexing, the lazy block metadata loading, a new selective query ramp-up strategy, and a more robust filter normalization system. Even the small details mattered, like improving the query progress reporting so the UI wouldn't look like it was frozen during these new, ultra-fast pruning phases. It all added up.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we hammered into our heads&lt;/head&gt;
    &lt;p&gt;This wasn't just building a feature; it was a month-long, high-stakes crash course in systems engineering. Here are the lessons learned in the crucible.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Design docs don't survive first contact. V0 was a beautiful corpse. It was elegant, logical, and theoretically sound. It was also completely wrong. It taught us a brutal lesson: I/O isn't a detail you figure out later; it's the main character. The "death by a thousand GETs" that killed V0 proved that you can't bolt on performance. You have to design for the harsh, unforgiving physics of the network and storage from day one.&lt;/item&gt;
      &lt;item&gt;The profiler is your only god. Our most profound wins weren't moments of genius. It was an act of forensics. We weren't being clever; we were just staring at a flame graph until our eyes bled and we noticed something catastrophically stupid we were doing. The profiler has no opinions. It doesn't care about your elegant design. It just shows you the brutal, unvarnished truth. You just have to be willing to look.&lt;/item&gt;
      &lt;item&gt;Performance is a game of whack-a-mole. This is the relentless, Sisyphean reality of optimization. You push the boulder of one bottleneck up the hill only to have another, bigger one roll right back down at you. We fixed the database, so hashing became the problem. We fixed hashing, so memory became the problem. We fixed memory, so S3 API limits became the problem. There is no glorious, final victory, only the grind of the next profile, the next trace, the next bottleneck to be smashed.&lt;/item&gt;
      &lt;item&gt;Speed is not free. We made a conscious, unapologetic decision to build a turbocharger, not a more efficient sedan. And turbochargers cost money, parts, and expertise. We embraced the cost. The goal wasn't to make something cheap run a little faster; it was to build a premium capability that delivered an order-of-magnitude performance win.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The hydra is slain, but more heads will grow&lt;/head&gt;
    &lt;p&gt;The Haydex saga is far from over. We achieved ludicrous speed, but now the work shifts to taming the beast we've created and making it smarter. The road ahead isn't a checklist; it's the next set of boss battles.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tiered indexing: The current fixed-interval indexing window is a blunt instrument. We want scalpels. The next evolution is a tiered system with "hot" indexes forged for data just seconds old, giving our users near-instant acceleration. These will then be gracefully compacted into "warm" and "cold" tiers over time.&lt;/item&gt;
      &lt;item&gt;The Janitor service: Right now, we create indexes, but we don't clean up after ourselves. An index whose data has been deleted by retention policies is just expensive digital garbage. We're building a janitor service that will relentlessly hunt down and vaporize these orphaned indexes, keeping the system lean and mean.&lt;/item&gt;
      &lt;item&gt;The never-ending hunt: And the search for bottlenecks continues. We're looking at how to teach Haydex to prune even more exotic APL queries. The hydra always grows new heads. We'll be ready.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Built in a month&lt;/head&gt;
    &lt;p&gt;The journey from "The Grand Haydex Revival" to a fully distributed, production-hardened system pushing 178.6 billion rows per second took just over a month. Looking back, it's striking how much ground was covered-wrestling with database performance, memory limits, distributed systems complexity, and the infuriating quirks of third-party APIs. Each problem forced me to measure, learn, and refactor. What emerged wasn't just another feature, but something that quietly shifts what's possible across the entire Axiom platform.&lt;/p&gt;
    &lt;p&gt;None of this would have been possible without standing on the shoulders of giants. Our work relies heavily on brilliant open-source projects. It feels particularly fitting to give a special thanks to Daniel Lemire, whose work inspired this entire project.&lt;/p&gt;
    &lt;p&gt;This is what happens when you combine necessity, obsession, and willingness to chase performance into the deepest corners of the system. Haydex didn't just solve our query speed problem-it fundamentally changed what we thought was possible.&lt;/p&gt;
    &lt;p&gt;From a failed experiment to 178.6 billion rows per second in 30 days. Not bad for a month's work.&lt;/p&gt;
    &lt;p&gt;The real victory isn't the numbers, though they're satisfying. It's that we proved you can take a seemingly impossible problem, break it down into solvable pieces, and build something that shifts the entire performance envelope of your platform. Hyperscale Customer went from struggling with ultra-slow queries to interactive experience on trillion-row datasets. That's the kind of transformation that makes all the late nights and S3 API battles worth it.&lt;/p&gt;
    &lt;p&gt;The hydra will grow new heads. There will always be another bottleneck, another scaling challenge, another "impossible" performance target. But now we know something we didn't know before: with the right approach, enough caffeine, and a willingness to throw out everything that doesn't work, you can build systems that redefine what fast means.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45376559</guid><pubDate>Thu, 25 Sep 2025 18:07:49 +0000</pubDate></item><item><title>A Postmark backdoor that’s downloading emails</title><link>https://www.koi.security/blog/postmark-mcp-npm-malicious-backdoor-email-theft</link><description>&lt;doc fingerprint="66d1851b9b70ae03"&gt;
  &lt;main&gt;
    &lt;p&gt;You know MCP servers, right? Those handy tools that let your AI assistant send emails, run database queries, basically handle all the tedious stuff we don't want to do manually anymore. Well, here's the thing not enough people talk about: we're giving these tools god-mode permissions. Tools built by people we've never met. People we have zero way to vet. And our AI assistants? We just... trust them. Completely.&lt;/p&gt;
    &lt;p&gt;Which brings me to why I'm writing this. &lt;code&gt;postmark-mcp&lt;/code&gt; - downloaded 1,500 times every single week, integrated into hundreds of developer workflows. Since version &lt;code&gt;1.0.16&lt;/code&gt;, it's been quietly copying every email to the developer's personal server. I'm talking password resets, invoices, internal memos, confidential documents - everything.&lt;/p&gt;
    &lt;p&gt;This is the worldâs first sighting of a real world malicious MCP server. The attack surface for endpoint supply chain attacks is slowly becoming the enterpriseâs biggest attack surface.&lt;/p&gt;
    &lt;head rend="h2"&gt;Soâ¦ What Did Our Risk Engine Detect?&lt;/head&gt;
    &lt;p&gt;Here's how this whole thing started. Our risk engine at Koi flagged &lt;code&gt;postmark-mcp&lt;/code&gt; when version &lt;code&gt;1.0.16&lt;/code&gt; introduced some suspicious behavior changes. When our researchers dug into it, like we do to any malware our risk engine flags, what we found was very disturbing.&lt;/p&gt;
    &lt;p&gt;On paper, this package looked perfect. The developer? Software engineer from Paris, using his real name, GitHub profile packed with legitimate projects. This wasn't some shady anonymous account with an anime avatar. This was a real person with a real reputation, someone you'd probably grab coffee with at a conference.&lt;/p&gt;
    &lt;p&gt;For 15 versions - FIFTEEN - the tool worked flawlessly. Developers were recommending it to their teams. "Hey, check out this great MCP server for Postmark integration." It became part of developerâs daily workflows, as trusted as their morning coffee.&lt;/p&gt;
    &lt;p&gt;Then version 1.0.16 dropped. Buried on line 231, our risk engine found this gem:&lt;/p&gt;
    &lt;p&gt;One single line. And boom - every email now has an unwanted passenger.&lt;/p&gt;
    &lt;p&gt;Here's the thing - there's a completely legitimate GitHub repo with the same name, officially maintained by Postmark (ActiveCampaign). The attacker took the legitimate code from their repo, added his malicious BCC line, and published it to npm under the same name. Classic impersonation.&lt;/p&gt;
    &lt;p&gt;Look, I get it. Life happens. Maybe the developer hit financial troubles. Maybe someone slid into his DMs with an offer he couldn't refuse. Hell, maybe he just woke up one day and thought "I wonder if I could get away with this." We'll never really know what flips that switch in someone's head - what makes a legitimate developer suddenly decide to backstab 1,500 users who trusted them.&lt;/p&gt;
    &lt;p&gt;But that's exactly the point. We CAN'T know. We can't predict it. And when it happens? Most of us won't even notice until it's way too late. For modern enterprises the problem is even more severe. As security teams focus on traditional threats and compliance frameworks, developers are independently adopting AI tools that operate completely outside established security perimeters. These MCP servers run with the same privileges as the AI assistants themselves - full email access, database connections, API permissions - yet they don't appear in any asset inventory, skip vendor risk assessments, and bypass every security control from DLP to email gateways. By the time someone realizes their AI assistant has been quietly BCCing emails to an external server for months, the damage is already catastrophic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lets Talk About the Impact&lt;/head&gt;
    &lt;p&gt;Okay, bear with me while I break down what we're actually looking at here.&lt;/p&gt;
    &lt;p&gt;You install an MCP server because you want your AI to handle emails, right? Seems reasonable. Saves time. Increases productivity. All that good stuff. But what you're actually doing is handing complete control of your entire email flow to someone you've never met.Â&lt;/p&gt;
    &lt;p&gt;We can only guestimate the impact:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1,500 downloads every single week&lt;/item&gt;
      &lt;item&gt;Being conservative, maybe 20% are actively in use&lt;/item&gt;
      &lt;item&gt;That's about 300 organizations&lt;/item&gt;
      &lt;item&gt;Each one probably sending what, 10-50 emails daily?&lt;/item&gt;
      &lt;item&gt;We're talking about 3,000 to 15,000 emails EVERY DAY flowing straight to giftshop.club&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And the truly messed up part? The developer didn't hack anything. Didn't exploit a zero-day. Didn't use some sophisticated attack vector. We literally handed him the keys, said "here, run this code with full permissions," and let our AI assistants use it hundreds of times a day. We did this to ourselves.&lt;/p&gt;
    &lt;p&gt;I've been doing security for years now, and this particular issue keeps me up at night. Somehow, we've all just accepted that it's totally normal to install tools from random strangers that can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Send emails as us (with our full authority)&lt;/item&gt;
      &lt;item&gt;Access our databases (yeah, all of them)&lt;/item&gt;
      &lt;item&gt;Execute commands on our systems&lt;/item&gt;
      &lt;item&gt;Make API calls with our credentials&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And once you install them? Your AI assistant just goes to town. No review process. No "hey, should I really send this email with a BCC to giftshop.club?" Just blind, automated execution. Over and over. Hundreds of times a day.&lt;/p&gt;
    &lt;p&gt;There's literally no security model here. No sandbox. No containment. Nothing. If the tool says "send this email," your AI sends it. If it says "oh, also copy everything to this random address," your AI does that too. No questions asked.&lt;/p&gt;
    &lt;p&gt;The postmark-mcp backdoor isn't sophisticated - it's embarrassingly simple. But it perfectly demonstrates how completely broken this whole setup is. One developer. One line of code. Thousands upon thousands of stolen emails.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Attack Timeline&lt;/head&gt;
    &lt;p&gt;Phase 1: Build a Legitimate Tool&lt;lb/&gt;Versions 1.0.0 through 1.0.15 work perfectly. Users trust the package.&lt;/p&gt;
    &lt;p&gt;Phase 2: Add One Line&lt;lb/&gt;Version 1.0.16 adds the BCC. Nothing else changes.&lt;/p&gt;
    &lt;p&gt;Phase 3: Profit&lt;lb/&gt;Sit back and watch emails containing passwords, API keys, financial data, and customer information flow into giftshop.club.&lt;/p&gt;
    &lt;p&gt;This pattern absolutely terrifies me. A tool can be completely legitimate for months. It gets battle-tested in production. It becomes essential to your workflow. Your team depends on it. And then one day - BAM - it's malware. By the time the backdoor activates, it's not some random package anymore. It's trusted infrastructure.&lt;/p&gt;
    &lt;p&gt;Oh, and &lt;code&gt;giftshop.club&lt;/code&gt;? Looks like it might be another one of the developer's side projects. But now it's collecting a very different kind of gift. Your emails are the gifts.&lt;/p&gt;
    &lt;p&gt;When we reached out to the developer for clarification, we got silence. No explanation. No denial. Nothing. But he did take action - just not the kind we hoped for. He promptly deleted the package from npm, trying to erase the evidence.&lt;/p&gt;
    &lt;p&gt;Here's the thing though: deleting a package from npm doesn't remove it from the machines where it's already installed. Every single one of those 1,500 weekly downloads? They're still compromised. Still sending BCCs to &lt;code&gt;giftshop.club&lt;/code&gt;. The developer knows this. He's banking on victims not realizing they're still infected even though the package has vanished from npm.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why MCP's Entire Model Is Fundamentally Broken&lt;/head&gt;
    &lt;p&gt;Let me be really clear about something: MCP servers aren't like regular npm packages. These are tools specifically designed for AI assistants to use autonomously. That's the whole point.&lt;/p&gt;
    &lt;p&gt;When you install postmark-mcp, you're not just adding some dependency to your package.json. You're giving your AI assistant a tool it will use hundreds of times, automatically, without ever stopping to think "hmm, is something wrong here?"&lt;/p&gt;
    &lt;p&gt;Your AI can't detect that BCC field. It has no idea emails are being stolen. All it sees is a functioning email tool. Send email. Success. Send another email. Success. Meanwhile, every single message is being silently exfiltrated. Day after day. Week after week.&lt;/p&gt;
    &lt;p&gt;The postmark-mcp backdoor isn't just about one malicious developer or 1,500 weekly compromised installations. It's a warning shot about the MCP ecosystem itself.&lt;/p&gt;
    &lt;p&gt;We're handing god-mode permissions to tools built by people we don't know, can't verify, and have no reason to trust. These aren't just npm packages - they're direct pipelines into our most sensitive operations, automated by AI assistants that will use them thousands of times without question.&lt;/p&gt;
    &lt;p&gt;The backdoor is actively harvesting emails as you read this. We've reported it to npm, but here's the terrifying question: how many other MCP servers are already compromised? How would you even know?&lt;/p&gt;
    &lt;p&gt;At Koi, we detect these behavioral changes in packages because the MCP ecosystem has no built-in security model. When you're trusting anonymous developers with your AI's capabilities, you need verification, not faith. Our risk engine automatically caught this backdoor the moment version 1.0.16 introduced the BCC behavior - something no traditional security tool would flag. But detection is just the first step. Our supply chain gateway ensures that malicious packages like this never make it into your environment in the first place. It acts as a checkpoint between your developers and the wild west of npm, MCP servers, and browser extensions - blocking known threats, flagging suspicious updates, and requiring approval for packages that touch sensitive operations like email or database access. While everyone else is hoping their developers make good choices, we're making sure they can only choose from verified, continuously monitored options.&lt;/p&gt;
    &lt;p&gt;If you're using &lt;code&gt;postmark-mcp&lt;/code&gt; version &lt;code&gt;1.0.16&lt;/code&gt; or later, you're compromised. Remove it immediately and rotate any credentials that may have been exposed through email. But more importantly, audit every MCP server you're using. Ask yourself: do you actually know who built these tools you're trusting with everything?&lt;/p&gt;
    &lt;p&gt;Stay paranoid. With MCPs, paranoia is just good sense.&lt;/p&gt;
    &lt;head rend="h2"&gt;IOCs&lt;/head&gt;
    &lt;p&gt;Package: postmark-mcp (npm)&lt;lb/&gt;Malicious Version: 1.0.16 and later&lt;lb/&gt;Backdoor Email: phan@giftshop[.]club&lt;lb/&gt;Domain: giftshop[.]club&lt;/p&gt;
    &lt;p&gt;Detection:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check for BCC headers to giftshop.club in email logs&lt;/item&gt;
      &lt;item&gt;Audit MCP server configurations for unexpected email parameters&lt;/item&gt;
      &lt;item&gt;Review npm packages for version 1.0.16+ of postmark-mcp&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mitigation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Immediately uninstall postmark-mcp&lt;/item&gt;
      &lt;item&gt;Rotate any credentials sent via email during the compromise period&lt;/item&gt;
      &lt;item&gt;Audit email logs for sensitive data that may have been exfiltrated&lt;/item&gt;
      &lt;item&gt;Report any confirmed breaches to appropriate authorities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45395957</guid><pubDate>Sat, 27 Sep 2025 14:23:12 +0000</pubDate></item><item><title>SSH3: Faster and rich secure shell using HTTP/3</title><link>https://github.com/francoismichel/ssh3</link><description>&lt;doc fingerprint="3ff9f1d818322c91"&gt;
  &lt;main&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;SSH3 is probably going to change its name. It is still the SSH Connection Protocol (RFC4254) running on top of HTTP/3 Extended connect, but the required changes are heavy and too distant from the philosophy of popular SSH implementations to be considered for integration. The specification draft has already been renamed ("Remote Terminals over HTTP/3"), but we need some time to come up with a nice permanent name.&lt;/p&gt;
    &lt;p&gt;SSH3 is a complete revisit of the SSH protocol, mapping its semantics on top of the HTTP mechanisms. It comes from our research work and we (researchers) recently proposed it as an Internet-Draft (draft-michel-remote-terminal-http3-00).&lt;/p&gt;
    &lt;p&gt;In a nutshell, SSH3 uses QUIC+TLS1.3 for secure channel establishment and the HTTP Authorization mechanisms for user authentication. Among others, SSH3 allows the following improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Significantly faster session establishment&lt;/item&gt;
      &lt;item&gt;New HTTP authentication methods such as OAuth 2.0 and OpenID Connect in addition to classical SSH authentication&lt;/item&gt;
      &lt;item&gt;Robustness to port scanning attacks: your SSH3 server can be made invisible to other Internet users&lt;/item&gt;
      &lt;item&gt;UDP port forwarding in addition to classical TCP port forwarding&lt;/item&gt;
      &lt;item&gt;All the features allowed by the modern QUIC protocol: including connection migration (soon) and multipath connections&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Quickly want to get started ? Checkout how to install SSH3. You will learn to setup an SSH3 server and use the SSH3 client.&lt;/p&gt;
    &lt;p&gt;Faster for session establishment, not throughput ! SSH3 offers a significantly faster session establishment than SSHv2. Establishing a new session with SSHv2 can take 5 to 7 network round-trip times, which can easily be noticed by the user. SSH3 only needs 3 round-trip times. The keystroke latency in a running session is unchanged.&lt;/p&gt;
    &lt;p&gt;SSH3 (top) VS SSHv2 (bottom) session establishement with a 100ms ping towards the server.&lt;/p&gt;
    &lt;p&gt;While SSHv2 defines its own protocols for user authentication and secure channel establishment, SSH3 relies on the robust and time-tested mechanisms of TLS 1.3, QUIC and HTTP. These protocols are already extensively used to secure security-critical applications on the Internet such as e-commerce and Internet banking.&lt;/p&gt;
    &lt;p&gt;SSH3 already implements the common password-based and public-key (RSA and EdDSA/ed25519) authentication methods. It also supports new authentication methods such as OAuth 2.0 and allows logging in to your servers using your Google/Microsoft/Github accounts.&lt;/p&gt;
    &lt;p&gt;While SSH3 shows promise for faster session establishment, it is still at an early proof-of-concept stage. As with any new complex protocol, expert cryptographic review over an extended timeframe is required before reasonable security conclusions can be made.&lt;/p&gt;
    &lt;p&gt;We are developing SSH3 as an open source project to facilitate community feedback and analysis. However, we cannot yet endorse its appropriateness for production systems without further peer review. Please collaborate with us if you have relevant expertise!&lt;/p&gt;
    &lt;p&gt;Given the current prototype state, we advise testing SSH3 in sandboxed environments or private networks. Be aware that making experimental servers directly Internet-accessible could introduce risk before thorough security vetting.&lt;/p&gt;
    &lt;p&gt;While hiding servers behind secret paths has potential benefits, it does not negate the need for rigorous vulnerability analysis before entering production. We are excited by SSH3's future possibilities but encourage additional scrutiny first.&lt;/p&gt;
    &lt;head rend="h2"&gt;🥷 Your SSH3 public server can be hidden&lt;/head&gt;
    &lt;p&gt;Using SSH3, you can avoid the usual stress of scanning and dictionary attacks against your SSH server. Similarly to your secret Google Drive documents, your SSH3 server can be hidden behind a secret link and only answer to authentication attempts that made an HTTP request to this specific link, like the following:&lt;/p&gt;
    &lt;code&gt;ssh3-server -bind 192.0.2.0:443 -url-path &amp;lt;my-long-secret&amp;gt;
&lt;/code&gt;
    &lt;p&gt;By replacing &lt;code&gt;&amp;lt;my-long-secret&amp;gt;&lt;/code&gt; by, let's say, the random value &lt;code&gt;M3MzkxYWMxMjYxMjc5YzJkODZiMTAyMjU&lt;/code&gt;, your SSH3 server will only answer to SSH3 connection attempts made to the URL &lt;code&gt;https://192.0.2.0:443/M3MzkxYWMxMjYxMjc5YzJkODZiMTAyMjU&lt;/code&gt; and it will respond a &lt;code&gt;404 Not Found&lt;/code&gt; to other requests. Attackers and crawlers on the Internet can therefore not detect the presence of your SSH3 server. They will only see a simple web server answering 404 status codes to every request.&lt;/p&gt;
    &lt;p&gt;NOTE WELL: placing your SSH3 server behind a secret URL may reduce the impact of scanning attacks but will and must never replace classical authentication mechanisms. The secret link should only be used to avoid your host to be discovered. Knowing the secret URL should not grant someone access to your server. Use the classical authentication mechanisms described above to protect your server.&lt;/p&gt;
    &lt;p&gt;SSH3 provides new feature that could not be provided by the SSHv2 protocol.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UDP port forwarding: you can now access your QUIC, DNS, RTP or any UDP-based server that are only reachable from your SSH3 host. UDP packets are forwarded using QUIC datagrams.&lt;/item&gt;
      &lt;item&gt;X.509 certificates: you can now use your classical HTTPS certificates to authenticate your SSH3 server. This mechanism is more secure than the classical SSHv2 host key mechanism. Certificates can be obtained easily using LetsEncrypt for instance.&lt;/item&gt;
      &lt;item&gt;Hiding your server behind a secret link.&lt;/item&gt;
      &lt;item&gt;Keyless secure user authentication using OpenID Connect. You can connect to your SSH3 server using the SSO of your company or your Google/Github account, and you don't need to copy the public keys of your users anymore.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This SSH3 implementation already provides many of the popular features of OpenSSH, so if you are used to OpenSSH, the process of adopting SSH3 will be smooth. Here is a list of some OpenSSH features that SSH3 also implements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parses &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt;on the server&lt;/item&gt;
      &lt;item&gt;Certificate-based server authentication&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;known_hosts&lt;/code&gt;mechanism when X.509 certificates are not used.&lt;/item&gt;
      &lt;item&gt;Automatically using the &lt;code&gt;ssh-agent&lt;/code&gt;for public key authentication&lt;/item&gt;
      &lt;item&gt;SSH agent forwarding to use your local keys on your remote server&lt;/item&gt;
      &lt;item&gt;Direct TCP port forwarding (reverse port forwarding will be implemented in the future)&lt;/item&gt;
      &lt;item&gt;Proxy jump (see the &lt;code&gt;-proxy-jump&lt;/code&gt;parameter). If A is an SSH3 client and B and C are both SSH3 servers, you can connect from A to C using B as a gateway/proxy. The proxy uses UDP forwarding to forward the QUIC packets from A to C, so B cannot decrypt the traffic A&amp;lt;-&amp;gt;C SSH3 traffic.&lt;/item&gt;
      &lt;item&gt;Parses &lt;code&gt;~/.ssh/config&lt;/code&gt;on the client and handles the&lt;code&gt;Hostname&lt;/code&gt;,&lt;code&gt;User&lt;/code&gt;,&lt;code&gt;Port&lt;/code&gt;and&lt;code&gt;IdentityFile&lt;/code&gt;config options (the other options are currently ignored). Also parses a new&lt;code&gt;UDPProxyJump&lt;/code&gt;that behaves similarly to OpenSSH's&lt;code&gt;ProxyJump&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Help us progress SSH3 responsibly! We welcome capable security researchers to review our codebase and provide feedback. Please also connect us with relevant standards bodies to potentially advance SSH3 through the formal IETF/IRTF processes over time.&lt;/p&gt;
    &lt;p&gt;With collaborative assistance, we hope to iteratively improve SSH3 towards safe production readiness. But we cannot credibly make definitive security claims without evidence of extensive expert cryptographic review and adoption by respected security authorities. Let's work together to realize SSH3's possibilities!&lt;/p&gt;
    &lt;p&gt;You can either download the last release binaries, install it using &lt;code&gt;go install&lt;/code&gt; or generate these binaries yourself by compiling the code from source.&lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;SSH3 is still experimental and is the fruit of a research work. If you are afraid of deploying publicly a new SSH3 server, you can use the secret path feature of SSH3 to hide it behing a secret URL.&lt;/p&gt;
    &lt;code&gt;go install github.com/francoismichel/ssh3/cmd/...@latest&lt;/code&gt;
    &lt;p&gt;You need a recent Golang version to do this. Downloading the source code and compiling the binaries can be done with the following steps:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/francoismichel/ssh3    # clone the repo
cd ssh3
go build -o ssh3 cmd/ssh3/main.go                        # build the client
CGO_ENABLED=1 go build -o ssh3-server cmd/ssh3-server/main.go   # build the server, requires having gcc installed&lt;/code&gt;
    &lt;p&gt;If you have root/sudo privileges and you want to make ssh3 accessible to all you users, you can then directly copy the binaries to &lt;code&gt;/usr/bin&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;cp ssh3 /usr/bin/ &amp;amp;&amp;amp; cp ssh3-server /usr/bin&lt;/code&gt;
    &lt;p&gt;Otherwise, you can simply add the executables to your &lt;code&gt;PATH&lt;/code&gt; environment variable by adding
the following line at the end of your &lt;code&gt;.bashrc&lt;/code&gt; or equivalent:&lt;/p&gt;
    &lt;code&gt;export PATH=$PATH:/path/to/the/ssh3/directory&lt;/code&gt;
    &lt;p&gt;Before connecting to your host, you need to deploy an SSH3 server on it. There is currently no SSH3 daemon, so right now, you will have to run the &lt;code&gt;ssh3-server&lt;/code&gt; executable in background
using &lt;code&gt;screen&lt;/code&gt; or a similar utility.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;As SSH3 runs on top of HTTP/3, a server needs an X.509 certificate and its corresponding private key. Public certificates can be generated automatically for your public domain name through Let's Encrypt using the &lt;code&gt;-generate-public-cert&lt;/code&gt; command-line argument on the server. If you do not want to generate a certificate signed by a real certificate authority or if you don't have any public domain name, you can generate a self-signed one using the &lt;code&gt;-generate-selfsigned-cert&lt;/code&gt; command-line argument. Self-signed certificates provide you with similar security guarantees to SSHv2's host keys mechanism, with the same security issue: you may be vulnerable to machine-in-the-middle attacks during your first connection to your server. Using real certificates signed by public certificate authorities such as Let's Encrypt avoids this issue.&lt;/p&gt;
    &lt;p&gt;Here is the usage of the &lt;code&gt;ssh3-server&lt;/code&gt; executable:&lt;/p&gt;
    &lt;code&gt;Usage of ./ssh3-server:
  -bind string
        the address:port pair to listen to, e.g. 0.0.0.0:443 (default "[::]:443")
  -cert string
        the filename of the server certificate (or fullchain) (default "./cert.pem")
  -key string
        the filename of the certificate private key (default "./priv.key")
  -enable-password-login
        if set, enable password authentication (disabled by default)
  -generate-public-cert value
        Automatically produce and use a valid public certificate usingLet's Encrypt for the provided domain name. The flag can be used several times to generate several certificates.If certificates have already been generated previously using this flag, they will simply be reused without being regenerated. The public certificates are automatically renewed as long as the server is running. Automatically-generated IP public certificates are not available yet.
  -generate-selfsigned-cert
        if set, generates a self-self-signed cerificate and key that will be stored at the paths indicated by the -cert and -key args (they must not already exist)
  -url-path string
        the secret URL path on which the ssh3 server listens (default "/ssh3-term")
  -v    verbose mode, if set
  -version
        if set, displays the software version on standard output and exit
&lt;/code&gt;
    &lt;p&gt;The following command starts a public SSH3 server on port 443 with a valid Let's Encrypt public certificate for domain &lt;code&gt;my-domain.example.org&lt;/code&gt; and answers to new sessions requests querying the &lt;code&gt;/ssh3&lt;/code&gt; URL path:&lt;/p&gt;
    &lt;code&gt;ssh3-server -generate-public-cert my-domain.example.org -url-path /ssh3
&lt;/code&gt;
    &lt;p&gt;If you don't have a public domain name (i.e. only an IP address), you can either use an existing certificate for your IP address using the &lt;code&gt;-cert&lt;/code&gt; and &lt;code&gt;-key&lt;/code&gt; arguments or generate a self-signed certificate using the
&lt;code&gt;-generate-selfsigned-cert&lt;/code&gt; argument.&lt;/p&gt;
    &lt;p&gt;If you have existing certificates and keys, you can run the server as follows to use them=&lt;/p&gt;
    &lt;code&gt;ssh3-server -cert /path/to/cert/or/fullchain -key /path/to/cert/private/key -url-path /ssh3
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Similarly to OpenSSH, the server must be run with root priviledges to log in as other users.&lt;/p&gt;
    &lt;p&gt;By default, the SSH3 server will look for identities in the &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; and &lt;code&gt;~/.ssh3/authorized_identities&lt;/code&gt; files for each user.
&lt;code&gt;~/.ssh3/authorized_identities&lt;/code&gt; allows new identities such as OpenID Connect (&lt;code&gt;oidc&lt;/code&gt;) discussed below.
Popular key types such as &lt;code&gt;rsa&lt;/code&gt;, &lt;code&gt;ed25519&lt;/code&gt; and keys in the OpenSSH format can be used.&lt;/p&gt;
    &lt;p&gt;Once you have an SSH3 server running, you can connect to it using the SSH3 client similarly to what you did with your classical SSHv2 tool.&lt;/p&gt;
    &lt;p&gt;Here is the usage of the &lt;code&gt;ssh3&lt;/code&gt; executable:&lt;/p&gt;
    &lt;code&gt;Usage of ssh3:
  -pubkey-for-agent string
        if set, use an agent key whose public key matches the one in the specified path
  -privkey string
        private key file
  -use-password
        if set, do classical password authentication
  -forward-agent
        if set, forwards ssh agent to be used with sshv2 connections on the remote host
  -forward-tcp string
        if set, take a localport/remoteip@remoteport forwarding localhost@localport towards remoteip@remoteport
  -forward-udp string
        if set, take a localport/remoteip@remoteport forwarding localhost@localport towards remoteip@remoteport
  -proxy-jump string
    	if set, performs a proxy jump using the specified remote host as proxy
  -insecure
        if set, skip server certificate verification
  -keylog string
        Write QUIC TLS keys and master secret in the specified keylog file: only for debugging purpose
  -use-oidc string
        if set, force the use of OpenID Connect with the specified issuer url as parameter
  -oidc-config string
        OpenID Connect json config file containing the "client_id" and "client_secret" fields needed for most identity providers
  -do-pkce
        if set, perform PKCE challenge-response with oidc
  -v    if set, enable verbose mode
&lt;/code&gt;
    &lt;p&gt;You can connect to your SSH3 server at my-server.example.org listening on &lt;code&gt;/my-secret-path&lt;/code&gt; using the private key located in &lt;code&gt;~/.ssh/id_rsa&lt;/code&gt; with the following command:&lt;/p&gt;
    &lt;code&gt;  ssh3 -privkey ~/.ssh/id_rsa username@my-server.example.org/my-secret-path
&lt;/code&gt;
    &lt;p&gt;The SSH3 client works with the OpenSSH agent and uses the classical &lt;code&gt;SSH_AUTH_SOCK&lt;/code&gt; environment variable to
communicate with this agent. Similarly to OpenSSH, SSH3 will list the keys provided by the SSH agent
and connect using the first key listen by the agent by default.
If you want to specify a specific key to use with the agent, you can either specify the private key
directly with the &lt;code&gt;-privkey&lt;/code&gt; argument like above, or specify the corresponding public key using the
&lt;code&gt;-pubkey-for-agent&lt;/code&gt; argument. This allows you to authenticate in situations where only the agent has
a direct access to the private key but you only have access to the public key.&lt;/p&gt;
    &lt;p&gt;While discouraged, you can connect to your server using passwords (if explicitly enabled on the &lt;code&gt;ssh3-server&lt;/code&gt;)
with the following command:&lt;/p&gt;
    &lt;code&gt;  ssh3 -use-password username@my-server.example.org/my-secret-path
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;ssh3&lt;/code&gt; parses your OpenSSH config. Currently, it only handles the &lt;code&gt;Hostname&lt;/code&gt;; &lt;code&gt;User&lt;/code&gt;, &lt;code&gt;Port&lt;/code&gt; and &lt;code&gt;IdentityFile&lt;/code&gt; OpenSSH options.
It also adds new option only used by SSH3, such as &lt;code&gt;URLPath&lt;/code&gt; or &lt;code&gt;UDPProxyJump&lt;/code&gt;. &lt;code&gt;URLPath&lt;/code&gt; allows you to omit the secret URL path in your
SSH3 command. &lt;code&gt;UDPProxyJump&lt;/code&gt; allows you to perform SSH3 (#proxy-jump)[Proxy Jump] and has the same meaning as the &lt;code&gt;-proxy-jump&lt;/code&gt; command-line argument.
Let's say you have the following lines in your OpenSSH config located in &lt;code&gt;~/.ssh/config&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;IgnoreUnknown URLPath
Host my-server
  HostName 192.0.2.0
  User username
  IdentityFile ~/.ssh/id_rsa
  URLPath /my-secret-path
&lt;/code&gt;
    &lt;p&gt;Similarly to what OpenSSH does, the following &lt;code&gt;ssh3&lt;/code&gt; command will connect you to the SSH3 server running on 192.0.2.0 on UDP port 443 using public key authentication with the private key located in &lt;code&gt;.ssh/id_rsa&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;  ssh3 my-server/my-secret-path
&lt;/code&gt;
    &lt;p&gt;If you do not want a config-based utilization of SSH3, you can read the sections below to see how to use the CLI parameters of &lt;code&gt;ssh3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This feature allows you to connect using an external identity provider such as the one of your company or any other provider that implements the OpenID Connect standard, such as Google Identity, Github or Microsoft Entra. The authentication flow is illustrated in the GIF below.&lt;/p&gt;
    &lt;p&gt;The way it connects to your identity provider is configured in a file named &lt;code&gt;~/.ssh3/oidc_config.json&lt;/code&gt;.
Below is an example &lt;code&gt;config.json&lt;/code&gt; file for use with a Google account. This configuration file is an array
and can contain several identity providers configurations.&lt;/p&gt;
    &lt;code&gt;[
    {
        "issuer_url": "https://accounts.google.com",
        "client_id": "&amp;lt;your_client_id&amp;gt;",
        "client_secret": "&amp;lt;your_client_secret&amp;gt;"
    }
]&lt;/code&gt;
    &lt;p&gt;This might change in the future, but currently, to make this feature work with your Google account, you will need to setup a new experimental application in your Google Cloud console and add your email as authorized users. This will provide you with a &lt;code&gt;client_id&lt;/code&gt; and a &lt;code&gt;client_secret&lt;/code&gt; that you can then set in your &lt;code&gt;~/.ssh3/oidc_config.json&lt;/code&gt;. On the server side, you just have to add the following line in your &lt;code&gt;~/.ssh3/authorized_identities&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;oidc &amp;lt;client_id&amp;gt; https://accounts.google.com &amp;lt;email&amp;gt;
&lt;/code&gt;
    &lt;p&gt;We currently consider removing the need of setting the client_id in the &lt;code&gt;authorized_identities&lt;/code&gt; file in the future.&lt;/p&gt;
    &lt;p&gt;It is often the case that some SSH hosts can only be accessed through a gateway. SSH3 allows you to perform a Proxy Jump similarly to what is proposed by OpenSSH. You can connect from A to C using B as a gateway/proxy. B and C must both be running a valid SSH3 server. This works by establishing UDP port forwarding on B to forward QUIC packets from A to C. The connection from A to C is therefore fully end-to-end and B cannot decrypt or alter the SSH3 traffic between A and C.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45395991</guid><pubDate>Sat, 27 Sep 2025 14:27:10 +0000</pubDate></item><item><title>A WebGL game where you deliver messages on a tiny planet</title><link>https://messenger.abeto.co/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45396441</guid><pubDate>Sat, 27 Sep 2025 15:17:30 +0000</pubDate></item><item><title>Norway to monitor airborne radioactivity in Svalbard</title><link>https://www.highnorthnews.com/en/norway-monitor-airborne-radioactivity-svalbard</link><description>&lt;doc fingerprint="5c5cd295d846b7f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Norway to Monitor Airborne Radioactivity in Svalbard&lt;/head&gt;
    &lt;p&gt;From October 1st, the Norwegian Radiation and Nuclear Safety Authority will take over the operations of an air filter station near Ny-Ålesund in Svalbard. “This will be particularly important for nuclear preparedness in the north,” says section leader in the agency.&lt;/p&gt;
    &lt;p&gt;The Finnish Meteorological Institute is to discontinue its air monitoring in Svalbard, and on October 1st, the Norwegian Radiation and Nuclear Safety Authority (DSA) will take over ownership of its air sampling equipment.&lt;/p&gt;
    &lt;p&gt;The purpose is to strengthen Norway's ability to monitor airborne radioactivity and increase vigilance in the High North.&lt;/p&gt;
    &lt;p&gt;"This will be an important supplement to our already existing network of air filter stations in Norway, and particularly important for nuclear preparedness in the North," says Markus Ottosen, section leader for the High North at the DSA.&lt;/p&gt;
    &lt;p&gt;"The stations are used to monitor radioactivity in the air, and to assess the size and composition in the event of possible accidents and incidents," Ottosen continues.&lt;/p&gt;
    &lt;p&gt;The relevant station near Ny-Ålesund has been in operation since 2000.&lt;/p&gt;
    &lt;p&gt;The DSA also has access to data from a similar station on Platåfjellet outside Longyearbyen. This is operated by the research institute NORSAR on behalf of the Comprehensive Nuclear-Test-Ban Treaty Organization.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45396641</guid><pubDate>Sat, 27 Sep 2025 15:35:59 +0000</pubDate></item><item><title>I made a public living room and the internet keeps putting weirder stuff in it</title><link>https://www.theroom.lol</link><description>&lt;doc fingerprint="3a95c5f52ddd92ad"&gt;
  &lt;main&gt;
    &lt;p&gt;Upload a base image to start editing this room.&lt;/p&gt;
    &lt;p&gt;By continuing you agree to our Terms and Privacy Policy.&lt;/p&gt;
    &lt;p&gt;Stick with the global room for now while we finish this feature.&lt;/p&gt;
    &lt;p&gt;Complete this quick check to queue your prompt.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45398005</guid><pubDate>Sat, 27 Sep 2025 17:59:47 +0000</pubDate></item><item><title>Handy – Free open-source speech-to-text app written in Rust</title><link>https://handy.computer/</link><description>&lt;doc fingerprint="848aa87b53323fd2"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Video Transcript&lt;/head&gt;
    &lt;p&gt;CJ: Hello, I'm CJ and I want to show you Handy. Handy is an open source speech-to-text application that you can run on your own computer. Simply press a keyboard shortcut, speak, and release, and Handy will paste whatever you said into the text field you're typing into.&lt;/p&gt;
    &lt;p&gt;Let's take a look at the settings menu for Handy, and it's really simple. You have a push-to-talk mode that you can enable, this is enabled by default so you press and hold the keys or alternatively you can turn it off so the transcription starts when you press the key combination and it stops when you press it again. And you can also change what key binding you would like to use for the transcription.&lt;/p&gt;
    &lt;p&gt;So now it's mapped to Ctrl-Z and if I turn this off, when I hit control Z, when you look up in the top corner of my Mac here, this little transcription icon lights up. And when I click it again, it turns off and transcribes the audio. There's nothing to paste into. So it just does nothing here.&lt;/p&gt;
    &lt;p&gt;So sit back, relax, and let Handy give you a hand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Free&lt;/head&gt;
    &lt;p&gt;Accessibility tooling belongs in everyone's hands, not behind a paywall.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Source&lt;/head&gt;
    &lt;p&gt;Together we can build further. Extend Handy for yourself and contribute to something bigger.&lt;/p&gt;
    &lt;head rend="h2"&gt;Private&lt;/head&gt;
    &lt;p&gt;Your voice stays on your computer. Get transcriptions without sending audio to the cloud.&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple&lt;/head&gt;
    &lt;p&gt;One tool, one job. Transcribe what you say and put it into a text box.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sponsors&lt;/head&gt;
    &lt;p&gt;Handy has some amazing sponsors who help keep it free and open source&lt;/p&gt;
    &lt;p&gt;Want to sponsor Handy? Feel free to donate, use GitHub Sponsors, or reach out directly to contact@handy.computer&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45399106</guid><pubDate>Sat, 27 Sep 2025 20:33:31 +0000</pubDate></item><item><title>We reverse-engineered Flash Attention 4</title><link>https://modal.com/blog/reverse-engineer-flash-attention-4</link><description>&lt;doc fingerprint="345b89c7cdba0a80"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We reverse-engineered Flash Attention 4&lt;/head&gt;
    &lt;p&gt;One month ago at Hot Chips, Tri Dao presented preliminary results on Flash Attention 4, the latest addition to the Flash Attention series of CUDA kernels. These kernels are used in the attention layers of Transformer neural networks. Along with more standard matrix multiplications, these calculations are the primary bottlenecks in contemporary generative AI workloads. Billions of dollars and gigawatts of power are being expended on GPUs to run more of these calculations faster. And Flash Attention 4 is the way to run lots of them as fast as possible. This blog post explains how it works.&lt;/p&gt;
    &lt;p&gt;The new FA4 kernel is optimized for Nvidiaâs new Blackwell Streaming Multiprocessor architecture and achieves a reported ~20% speedup over the previous state-of-the-art, the attention kernels in Nvidiaâs &lt;code&gt;cudnn&lt;/code&gt; library.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cudnn&lt;/code&gt; kernels are closed source, so Jensen only knows whatâs going on in there.&lt;/p&gt;
    &lt;p&gt;Thereâs also no official technical report on how FA4 works yet. But the source code for Flash Attention 4 was already released online here. Weâve recently been contributing to open source LLM inference engines, so we read the code and reverse-engineered how the kernel works, including two math tricks (faster approximate exponentials and a more efficient online softmax) that are classic Dao. This write-up contains our findings.&lt;/p&gt;
    &lt;p&gt;Perhaps surprisingly, the architecture of FA4 is readily understandable by a general software engineering audience.&lt;/p&gt;
    &lt;p&gt;Thatâs because the biggest change in FA4 isnât the (very cool) math â itâs a massive increase in the complexity of its asynchronous âpipelineâ of operations. This kind of asynchronous programming is fairly new in the world of CUDA, but pipes have been in Unix for like 40 goddamn years. A programmer who has experience with parallel and concurrent programs, like high performance databases and web servers, will feel right at home (absent some novel GPU technical vocabulary).&lt;/p&gt;
    &lt;p&gt;So we organize our write-up into two parts.&lt;/p&gt;
    &lt;p&gt;The first section, a âquick tourâ, covers the architecture of FA4 by tracing what happens as a block of inputs is turned into a block of outputs. It is written to be understandable by a practicing software engineer without any CUDA programming experience. We give brief explanations of CUDA concepts and hardware, like warps and warp schedulers, but defer detailed explanation to our GPU Glossary (linked throughout).&lt;/p&gt;
    &lt;p&gt;The second section, a âdeep diveâ, walks through each of the subcomponents in turn, explaining what each does, supported by links to the source code for particularly intrepid spelunkers.&lt;/p&gt;
    &lt;head rend="h2"&gt;A quick tour of Flash Attention 4: The âLife of a Tileâ&lt;/head&gt;
    &lt;p&gt;We start with bf16 tensors of queries, keys, and values in global memory (aka GPU RAM). Weâre aiming to produce a tensor of bf16 outputs, also in global memory. Outputs are values weighted by the similarity of queries to keys. Computing this weighting requires matrix multiplication, exponentiation, and normalization.&lt;/p&gt;
    &lt;p&gt;Like the good engineers we are, we tackle this very big problem by breaking it down into smaller pieces. Thatâs fairly literal in this case: we take our very large input tensor and split it up into âtilesâ of adjacent rows and columns, each of which contribute to the calculation of one tile of outputs.&lt;/p&gt;
    &lt;p&gt;Specifically, one running instance of our kernel program (namely, one âcooperative thread arrayâ of threads) produces two tiles of the outputs tensor by reading two tiles of the queries tensor. In between, it streams all of the keys &amp;amp; values for each query tile. Keys and values are also read in tiles. If youâre a database âhead, you might think of it as a vectorized sequential scan for a batch of aggregation queries against a key-value store.&lt;/p&gt;
    &lt;p&gt;By running this tile-level program many times concurrently (typically, massively in parallel), we produce the entire outputs tensor. This is a âsingle program, multiple dataâ execution model, where each datum is a pair of tiles. This kind of concurrency across program instances is the bread-and-butter of the CUDA programming model and is transparently handled for the programmer by the CUDA runtime.&lt;/p&gt;
    &lt;p&gt;But with the fastest contemporary kernels, like Flash Attention 3 &amp;amp; 4 and all state-of-the-art matrix multiplications, there is also concurrency within our program. Each program instance sets up an asynchronous pipeline of operations that together effect the tile-level computation depicted above. We write our kernel such that all of our pipeline steps can run as concurrently as possible as we process a tile. In Flash Attention 4, we achieve this by mapping chunks of our pipeline onto 32-thread groups called warps (a technique called warp specialization).&lt;/p&gt;
    &lt;p&gt;We then rely on the warp schedulers to switch between pipeline steps within program instances on each clock, swapping out when a step stalls and swapping back in when a stepâs next input is ready. Think simultaneous multithreading/âhyperthreadingâ from CPUs, but on steroids. The diagram below, from our GPU Performance Glossary, depicts four cycles across four parallel slots, for a total of sixteen execution slots, fifteen of which are filled with warps actively executing instructions thanks to this rapid warp switching. See the associated article for details.&lt;/p&gt;
    &lt;p&gt;This execution model is âdualâ to the way that an asynchronous program for CPUs works in the following sense. In an async CPU program, a single thread implements the entire journey of a single datum (e.g. request) through a state machine (e.g. Reading, Parsing, Writing), switching between transitions as data become ready. In an async GPU program like FA4, a single warp implements a single transition (e.g. from queries and values to attention scores) in a similar state machine.&lt;/p&gt;
    &lt;p&gt;The pipeline is organized with a producer/consumer model and uses barriers for synchronization.&lt;/p&gt;
    &lt;p&gt;Unlike the concurrency across program instances, the internal pipeline concurrency is all implemented manually. This leads to quite gnar code â though the control flow will look familiar to anyone who has written their own event loop.&lt;/p&gt;
    &lt;p&gt;So like most async code, the FA4 kernel is best understood by tracing the path of a single tile: the âlife of a tileâ, akin to the âlife of a pixelâ in a browserâs rendering pipeline. In particular, letâs follow the tileâs path through the memory hierarchy of the GPU as it is transformed from initial query tile to final output tile.&lt;/p&gt;
    &lt;p&gt;At a high level, and eliding a few details about multiple buffering that increase concurrency and parallelism, that looks something like this:&lt;/p&gt;
    &lt;p&gt;Which vaguely resembles a microservices diagram. As above, so below!&lt;/p&gt;
    &lt;p&gt;Spelled out, thatâs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A tile of queries is loaded from global memory (&lt;code&gt;mQ&lt;/code&gt;) into shared memory (&lt;code&gt;sQ&lt;/code&gt;) by the Load warp. Shared memory is a âscratchpadâ L1 cache managed by the programmer.&lt;/item&gt;
      &lt;item&gt;Tiles of keys (&lt;code&gt;mK&lt;/code&gt;) and values (&lt;code&gt;mV&lt;/code&gt;) are streamed into shared memory (&lt;code&gt;sK&lt;/code&gt;,&lt;code&gt;sV&lt;/code&gt;), also by the Load warp. Note that if the working set size permits, future loads of these tiles for other query tiles will be serviced from the hardware-managed L2 cache (not pictured).&lt;/item&gt;
      &lt;item&gt;When each tile of keys is ready, the MMA warp multiplies it with our tile of queries using a Tensor Core, producing a tile of unnormalized attention scores in Tensor Memory (&lt;code&gt;tS&lt;/code&gt;). Tensor Cores are single-purpose hardware for running matmuls. Tensor Memory is another programmer-managed L1 cache designed to hold and accumulate intermediates during sequences of Tensor Core operations.&lt;/item&gt;
      &lt;item&gt;When each tile of unnormalized attention scores is ready, a Softmax warp produces normalized attention scores for that tile in Tensor Memory (&lt;code&gt;tP&lt;/code&gt;) without using the Tensor Core and updates a scaling factor used for numerical stability (in shared memory, not pictured).&lt;list rend="ul"&gt;&lt;item&gt;â¡ï¸ New in Flash Attention 4: this step can use CUDA Cores instead of Special Function Units (SFUs) to perform the exponential step of the normalization. SFUs are intended to provide hardware acceleration for transcendental operations like exponentials. But there are far fewer SFUs than CUDA Cores, which can lead to queueing. The basic idea, fast approximate exponentiation in software for neural networks that can tolerate a bit of accuracy loss, was proposed in a 1999 Neural Computation paper by Schraudolph, but the implementation here is quite different, involving a cubic polynomial approximation (as described in detail below).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;When each tile of normalized attention scores is ready, a Correction warp checks if the normalization scaling factor has changed and, if necessary, rescales the final output tile in Tensor Memory (&lt;code&gt;tO&lt;/code&gt;).&lt;list rend="ul"&gt;&lt;item&gt;â¡ï¸ New in Flash Attention 4: the choice of when to rescale became much smarter, reportedly cutting down on output rescaling operations by a factor of 10. Roughly: the scaling factor used to be a simple running maximum. Now updates are applied only when the maximum has changed enough to impact numerical stability. This seems like a good, and very portable, idea.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;When each rescaling update finishes, the MMA warp updates the output tile in Tensor Memory (&lt;code&gt;tO&lt;/code&gt;) by accumulating it with the value tile (&lt;code&gt;sV&lt;/code&gt;) scaled by the attention score tile (&lt;code&gt;tP&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;When each tile of final output values is ready, the Correction warp stores it in shared memory (&lt;code&gt;sO&lt;/code&gt;), then the Epilogue warp stores it in global memory (&lt;code&gt;mO&lt;/code&gt;), and weâre done with that tile.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our high-level, tile-centric view elides a number of details, like the number of warps assigned to each pipeline step and the use of buffers to store different tiles. It also leaves out all of the details of the barrier synchronization, which is required on both sides of every producer/consumer relationship (aka where an arrow tip meets an arrow tail in the diagram). These are critical for performance.&lt;/p&gt;
    &lt;p&gt;We go through these details in a âwarp-centricâ view of the kernel below, which focuses on the operations in each warp, rather than the movement of tiles, and includes links to the source code. This is necessarily more technical and goes through some GPU-specific features at higher speed, so itâs less suitable for a general software engineering audience.&lt;/p&gt;
    &lt;p&gt;But before that, one last takeaway for those only interested in the high level.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where does GPU programming go from here?&lt;/head&gt;
    &lt;p&gt;When Ian Buck and others designed CUDA C, they were driven by a north star: can it be used to write a single precision vector addition (&lt;code&gt;saxpy&lt;/code&gt;) with respectable performance as a clean one-liner thatâs easily understood by a C programmer? The core of the CUDA programming model laid down then and described in the 2008 Lindholm et al. paper still persists today.&lt;/p&gt;
    &lt;p&gt;Whatâs new in the last few years (in the Hopper and Blackwell architectures) is an increasing reliance on programmer-managed asynchrony, like FA4âs multi-stage, multi-buffered pipeline. This represents a major jump in complexity from FA3âs simpler âping-pongâ pipeline (added to take advantage of Hopper GPUsâ async capabilities).&lt;/p&gt;
    &lt;p&gt;And just as in other well-designed languages, CUDA C/C++ has struggled to accommodate the introduction of asynchrony. It is a truth universally acknowledged that async programming sucks absolute ass. Thatâs especially true when you need to manage your own event loop, as weâre effectively doing here. And itâs made harder, not easier, by the thread-centricity and warp uniformity of the CUDA programming model and PTX machine model.&lt;/p&gt;
    &lt;p&gt;No wonder the Triton team gave up on writing Blackwell attention and added the new Gluon frontend at a lower level!&lt;/p&gt;
    &lt;p&gt;Tritonâs troubles notwithstanding, this kernel is a clear instance of the swing towards tile-based, warp-specialized programming. And Nvidia is betting big on a number of new languages and libraries to try to make this easier, from the CuTe DSL and CUTLASS C++ used in this kernel to the forthcoming CuTile. Say what you will about the chatbot hype wave, these are exciting times for high performance numerical computing!&lt;/p&gt;
    &lt;head rend="h2"&gt;Deep dive for the GPU enjoyers: What does each warp do in Flash Attention 4?&lt;/head&gt;
    &lt;p&gt;There are five different specializations for warps in the Flash Attention 4 kernel. They are listed below, along with links to their source code.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A Load warp to load query, key, and value tiles from global memory into shared memory&lt;/item&gt;
      &lt;item&gt;An MMA warp to compute unnormalized attention scores from query and key tiles and accumulate score-weighted value tiles into the output tiles&lt;/item&gt;
      &lt;item&gt;Eight Softmax warps to compute normalized attention scores and track running stats (max, sum)&lt;/item&gt;
      &lt;item&gt;Four Correction warps to watch for updates to the normalization scale and re-normalize the output tiles&lt;/item&gt;
      &lt;item&gt;One or two Epilogue warps to store completed output tiles from shared memory into global memory&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the above discussion, we implied that each CTA works on just two query tiles and produces just two output tiles. Thatâs true in some settings, but the mapping between tiles and CTAs is technically abstracted by a &lt;code&gt;TileScheduler&lt;/code&gt;. For the best performance, you need to use the &lt;code&gt;StaticPersistentTileScheduler&lt;/code&gt;, which launches at most one CTA per Streaming Multiprocessor and then schedules tiles onto those SMs. This reduces CTA launch overhead and allows for more fine-grained concurrency (e.g. overlapping Epilogue warps for one tile with the Load and MMA warps for the next tile).&lt;/p&gt;
    &lt;p&gt;The core work of the kernel is the same â thereâs just not a clean mapping of work onto thread constructs, which makes explaining the work harder. From here, weâll go back to speaking about the code as though each CTA handles only two tiles (which is literally true if you use the &lt;code&gt;SingleTileScheduler&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Also, from here we will start using some shorthand, matching the code and convention: Q for queries, K for keys, V for values, O for outputs, S for unnormalized attention scores, and P for normalized attention scores/âprobabilitiesâ.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Load warp loads two Q tiles and streams all K and V tiles.&lt;/head&gt;
    &lt;p&gt;The Load warp operates on pointers to Q, K, and V tensors in global memory and writes to Q, K, and V tensors in shared memory. It supports paged keys and values (as in Paged Attention, not as in operating system pages) via an optional âpage tableâ tensor (again, not the page tables co-managed by the OS, the CPU, and the MMU).&lt;/p&gt;
    &lt;p&gt;It uses the Tensor Memory Accelerator (TMA) to reduce register pressure from multidimensional array access and fire off copies asynchronously. This also avoids very long warp stalls on loads that would require even more warp specialization to hide latency.&lt;/p&gt;
    &lt;p&gt;The Load warp loads two Q tiles. It loads all K and V blocks in a loop. It is the âproducerâ of these tiles (in a producer/consumer setup). It can concurrently load up to three blocks each of K and V.&lt;/p&gt;
    &lt;p&gt;As it completes these loads, the Load warp signals their completion to the MMA warp through an array of barriers in shared memory. All barriers (not just for Load/MMA synchronization) are referenced via their offset in this array to support variable barrier counts with different configuration settings.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MMA warp computes unnormalized attention scores and output values.&lt;/head&gt;
    &lt;p&gt;The MMA warp operates on pointers to Q, K, and V tensors in shared memory. For every K/V tile, it runs two matmuls to create S tiles and two matmuls for O (Q/K for the S tiles, P/V for the O tiles). The matmuls are emitted as inline PTX assembly, as is necessary for CUDA C/C++ programs to use the Tensor Cores in Hopper and Blackwell. The vast majority of the FLOPS in this kernel are driven by these lines; most everything else is memory management.&lt;/p&gt;
    &lt;p&gt;The specific PTX instruction used is &lt;code&gt;tcgen05.mma.cta_group::1&lt;/code&gt;. &lt;code&gt;mma&lt;/code&gt; is matrix-multiply-accumulate. &lt;code&gt;tcgen05&lt;/code&gt; means &lt;code&gt;5&lt;/code&gt;th generation &lt;code&gt;t&lt;/code&gt;ensor &lt;code&gt;c&lt;/code&gt;ore, aka Blackwell, as in &lt;code&gt;sm100&lt;/code&gt;/Compute Capability 10.0. &lt;code&gt;cta_group::1&lt;/code&gt; means we run our matmul using only a single CTA, avoiding the nastiness of TPC-based 2SM/2CTA matmuls available in Blackwell. This likely introduces a small memory throughput penalty but simplifies CTA/tile scheduling. Interestingly, the ThunderKittens Blackwell attention kernel makes a different choice.&lt;/p&gt;
    &lt;p&gt;Also on the front of scheduling/simplification: only a single &lt;code&gt;leader_thread&lt;/code&gt; issues the instruction. And weâre only working from a single warp. This is an important difference from performant Hopper MMAs, which were coordinated across an entire warpgroup.&lt;/p&gt;
    &lt;p&gt;After getting hold of a Q tile and our first K tile, we run our first matmul to produce our first result for S. Then we loop over the remaining K and V tiles and update S and O. These S and O tensors live in Tensor Memory. This is the âintendedâ use of Tensor Memory, as a store for accumulators read from and written to by the Tensor Cores.&lt;/p&gt;
    &lt;p&gt;Since the K and V tiles are buffered, we need to signal the Load warp every time we finish using them (eg here, signaling that the memory containing V can be reused once it has been used to construct the second O tile). Thereâs some additional coordination here (around S, P, and O), which weâll discuss as it comes in up in the other warps.&lt;/p&gt;
    &lt;head rend="h3"&gt;Eight Softmax warps produce normalized attention scores.&lt;/head&gt;
    &lt;p&gt;The Softmax warps produce normalized attention scores (P, as in âprobabilitiesâ) consumed by the MMA warps. Ignore the name and donât try to come up with an interpretation of the attention scores as the probability distribution for a random variable; itâll make your head hurt and give you bad intuition about Transformers. Theyâre better thought of as weights for a linear combination of vectors from V.&lt;/p&gt;
    &lt;p&gt;The core softmax operation is implemented by two warpgroups, aka eight warps. The two warpgroups are mapped onto the two query/output tile workstreams. Warpgroups are made up of four adjacent warps with a warp index alignment of four. Using them was critical for the fast warpgroup MMAs in Hopper GPUs, as in Flash Attention 3, but we didnât see anything in this kernel that made explicit use of them. Warpgroup alignment may lead to more even distribution of work across warp schedulers/subunits of the SM, as it did in Hopper, which had four warp schedulers per SM. To our and Wikipediaâs knowledge, this level of detail on SM100 Blackwell GPUs like B200s is not published anywhere (but it is true of SM120 RTX Blackwell GPUs).&lt;/p&gt;
    &lt;p&gt;Weâre also not certain of the reason why some pipeline stages are assigned more warps than others and in this particular ratio. Presumably, it helps ensure balanced throughput across the different stages, but our napkin math on relative operational load, bandwidth, and latency between the matmuls and the attention operations didnât produce a smoking gun. We speculate that it was determined by benchmarking.&lt;/p&gt;
    &lt;p&gt;Each warp runs a single step of the online softmax calculation at a time while looping over the S tiles produced by the MMA warp.&lt;/p&gt;
    &lt;p&gt;Looking within the individual softmax step: the unnormalized attention scores are stored in Tensor Memory, which can only be directly operated on by the Tensor Cores. But the Tensor Cores can only do matrix multiplication. So the Softmax warps have to copy the scores into the registers to apply the exponentiation and then copy the result back.&lt;/p&gt;
    &lt;p&gt;The exponentiation is done differently than in previous versions of Flash Attention. FA3 and earlier used the GPUâs Special Function Units to perform a hardware-accelerated exponentiation. Specifically, they use the &lt;code&gt;exp2&lt;/code&gt; CUDA PTX intrinsic, which is typically mapped by the (closed-source) &lt;code&gt;ptxas&lt;/code&gt; compiler to the &lt;code&gt;MUFU.EX2&lt;/code&gt; SASS instruction.&lt;/p&gt;
    &lt;p&gt;The FA4 kernel does that too, but for smaller attention head sizes it additionally mixes in a different exponentiation algorithm on some iterations with a tunable frequency. That implementation uses this block of inline PTX to compute &lt;code&gt;2 ** x&lt;/code&gt;. The algorithm splits the exponentiation into two parts: the easy integer part (&lt;code&gt;2 ** floor(x)&lt;/code&gt;) and the hard rational part (&lt;code&gt;2 ** (x - floor(x))&lt;/code&gt;). It uses a cubic polynomial to approximate &lt;code&gt;2 ** x&lt;/code&gt; on the unit interval (check out the approximation on Wolfram Alpha here).&lt;/p&gt;
    &lt;p&gt;The cubic polynomial calculation is done, following Hornerâs method for linear time polynomial evaluation, with three fused multiply-adds (&lt;code&gt;fma&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;// Horner's method: (((c3 * r) + c2) * r + c1) * r + c0
fma.rn.ftz.f32x2 l10, l9, l6, l5
fma.rn.ftz.f32x2 l10, l10, l9, l4
fma.rn.ftz.f32x2 l10, l10, l9, l3&lt;/code&gt;
    &lt;p&gt;Note that &lt;code&gt;f32x2&lt;/code&gt; means that we operate on a vector (as in vector lanes) of two 32 bit values. You can read about a similar implementation for CPU vector instructions on Stack Overflow here.&lt;/p&gt;
    &lt;p&gt;In addition to only applying this method on some iterations, it stops applying it on a configurable number of the last S tiles. Together, these suggest that the reason for applying it is to avoid a bottleneck on the SFUs (which, due to wave quantization effects, is less relevant for the final tiles).&lt;/p&gt;
    &lt;p&gt;The Softmax warps also track the running statistics for rescaling and normalizing attention scores used by the Correction warps, as discussed below.&lt;/p&gt;
    &lt;p&gt;Thereâs another important change here. All softmax algorithms need to handle numerical instability caused by exponentiation of large values. Before Flash Attention, this was usually done by finding the largest value in each row and subtracting it from the value before exponentiating. All Flash Attention kernels use a streaming or online softmax algorithm, and the largest value is not known in advance â searching through the scores to find it would defeat the purpose of using a streaming algorithm! Instead, they use a running maximum for numerical stability and update the scaling factor whenever a new maximum is encountered. This ensures continued numerical stability and avoids an extra scan, but requires a costly correction of previous values (handled by the Correction warps) every time a new maximum is observed.&lt;/p&gt;
    &lt;p&gt;This is inefficient. We only need to update the scaling factor when the new maximum changes enough to threaten numerical stability, not every time a new maximum appears. That logic is implemented here. In the Hot Chips talk, Dao indicated that this reduced the number of corrections by a factor of 10.&lt;/p&gt;
    &lt;p&gt;There is additional support for attention sinks and storing the log-sum-exp tensor used in the backwards pass. At time of writing in late September 2025, a backwards version of this kernel is not available, but is expected imminently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Four Correction warps rescale previous outputs as the normalization changes.&lt;/head&gt;
    &lt;p&gt;The Correction warps update past output results from the MMA warps as the numerical stability scaling factor changes. The Correction warps need to coordinate their access to the O values in Tensor Memory with the MMA warps (eg here, indicating that those values are consumed and the memory can be reclaimed).&lt;/p&gt;
    &lt;p&gt;Like the Softmax warps, the four Correction warps form a warpgroup. Also like the Softmax warps, they need to load from Tensor Memory to registers to apply their non-matmul rescaling operation.&lt;/p&gt;
    &lt;p&gt;The Correction warps are also responsible for writing the output from Tensor Memory to shared memory and applying the final scaling by the row sum. This is called the &lt;code&gt;correction_epilogue&lt;/code&gt;. âEpilogueâ here means the same thing as in the name of the âEpilogueâ warps â an operation that occurs at the end of a sequence of operations on values stored in one memory and before the results are written to another memory. But in this case, it refers to operations on data in Tensor Memory before they are stored to shared memory, whereas the Epilogue warps take data from shared memory and store it in global memory.&lt;/p&gt;
    &lt;p&gt;This is especially confusing because the completion of this epilogue is the signal for the Epilogue warps to start their work.&lt;/p&gt;
    &lt;p&gt;The Correction warps have the global memory output tensor among their arguments, but only use it in commented-out code.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Epilogue Warp(s) store complete output tiles back into global memory.&lt;/head&gt;
    &lt;p&gt;There are either one or two Epilogue warps depending on whether the TMA is enabled.&lt;/p&gt;
    &lt;p&gt;In the case that the Epilogue warps can use the TMA, thereâs only one and its work is simple. It waits on the correction loop to finish for an output tile, then runs a TMA copy, then signals that it has finished reading the O tensor in shared memory and the buffer can be reused.&lt;/p&gt;
    &lt;p&gt;If they canât use the TMA, their work is more complicated â they need to handle slicing and packing, which is pretty hard. It also consumes quite a few more registers.&lt;/p&gt;
    &lt;head rend="h2"&gt;If you made it this far, you might enjoy working at Modal.&lt;/head&gt;
    &lt;p&gt;At Modal, weâre building the cloud infrastructure that compute-intensive workloads like giant Transformers need. Our platform is used by companies like Suno, Lovable, Ramp, and Substack. Weâre hiring.&lt;/p&gt;
    &lt;p&gt;The authors would like to thank Simon Mo of vLLM, Michael Goin of RedHat AI, and Kimbo Chen of SemiAnalysis for their comments on drafts of this article. Weâd also like to thank Tri Dao for writing another banger of a kernel.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45399637</guid><pubDate>Sat, 27 Sep 2025 21:50:30 +0000</pubDate></item><item><title>Learn to play Go</title><link>https://online-go.com/learn-to-play-go</link><description>&lt;doc fingerprint="d37fffed7efd5e8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45400376</guid><pubDate>Sat, 27 Sep 2025 23:50:42 +0000</pubDate></item><item><title>Solar panels + cold = A potential problem</title><link>https://www.linspyre.com/ecoholics/temps.html</link><description>&lt;doc fingerprint="e773707613a0d94b"&gt;
  &lt;main&gt;
    &lt;p&gt;In order to determine what your maximum VoC of an array can be, use one of the conservative estimate tables below or you can compute the temperature coefficients precisely if you have the solar panel specs.&lt;/p&gt;
    &lt;p&gt;Example 1&lt;/p&gt;
    &lt;p&gt;You might choose 0°C or 32°F as it almost never freezes in Georgia and the table shows 136.4v as your VoC maximum. The EcoFlow 400w portables have a VoC of 48.0v which means two solar panels in series have a VoC of 96v (Solar Rule #4 above)- well within the 136.4v limit. Three solar panels in series would exceed the 136.4v limit and result in black magic smoke.&lt;/p&gt;
    &lt;p&gt;Example 2&lt;/p&gt;
    &lt;p&gt;Since it gets really cold in Canada you would likely choose -40°C or -40°F and the table shows 48.0v as your VoC maximum at the coldest temperature. The EcoFlow 220w bifacial portables have a VoC of 21.8v which means you can safely put two solar panels in series (43.6v). Since the Delta 2 Max has two solar input XT60i ports, you can actually add four solar panels, two strands of two panels per port.&lt;/p&gt;
    &lt;p&gt;Published: September 20, 2023&lt;/p&gt;
    &lt;p&gt;Updated:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45401051</guid><pubDate>Sun, 28 Sep 2025 01:48:15 +0000</pubDate></item><item><title>IBM Intellistation 185 AIX workstation (2016)</title><link>http://www.ibmfiles.com/pages/intellipower185.htm</link><description>&lt;doc fingerprint="b78ab9122b85b98d"&gt;
  &lt;main&gt;&lt;p&gt;IBM IntelliStation POWER 185&lt;/p&gt;&lt;p&gt;System Type: 7047&lt;/p&gt;&lt;p&gt;Released in 2006 after the 285, this system used the PowerPC 970 processors (sometimes erroneously categorized under POWER5 by 3rd parties and at IBM--it would technically be closer to a POWER4). Despite POWER5+ and DDR2 already being used on the 285, the 185 took a few steps back and used DDR1 and didn't bother with the POWER chips. While this may seem rather strange, it can be explained with System p5 185 (7037-A50). Perhaps IBM wanted to create a machine of their own with the PPC 970 or it was stuck in developmental hell for a bit, whatever the case IBM used the 7037 guts to be modified into the IntelliStation 185. This move could have been done to offer a workstation that used less power and was quieter / while still maintaining AIX compatibility. Hundreds went to use in government offices and engineering firms; as a result they mainly show up in the U.S. and Japan.&lt;/p&gt;&lt;p&gt;Most of the POWER and AIX people don't even know that IBM badged the 970 under POWER and popped it into a workstation; therefore making the IntelliStation "POWER" 185 a source of confusion, horror and intrigue--and the fact it's relatively unknown means you'll get different responses as to what it can and cannot do. It is still a POWER system in firmware, and supports hardware virtualization like any other POWER system, which makes it completely different from a Power Mac G5 which uses the same CPU. If it didn't support hardware virtualization IBM wouldn't have been able to call the original System p5 185 a System p5... it would just be a plain PowerPC box. But this wasn't too unfamiliar as other AIX systems in the past also leveraged PowerPC chips like RS/6000s and PowerPC ThinkPads. I suspect the whole reason why the PowerPC 970MP had virtualization in the first place was because of IBM's exact requirement for this on their System p5 that they were intending to use it in (which Apple never did on the G5 as far as I know), and then of course Apple wanted AltiVec (which IBM didn't use in AIX as far as I know, but that doesn't mean you couldn't write AIX software to leverage it).&lt;/p&gt;&lt;p&gt;Downloads&lt;/p&gt;&lt;p&gt;--&amp;gt; Latest System Firmware (AT071-156, RPM package) - 05/11/2007&lt;/p&gt;&lt;p&gt;Due to IBM's licensing agreements I am not entitled to distribute the firmware update as it's licensed code. Therefore the above link redirects to IBM's legacy firmware page where it can be downloaded. If the firmware ever gets pulled from the website I will consider mirroring it.&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM IntelliStation POWER 185 Hardware Announcement - February 14, 2006&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM IntelliStation POWER 185 RedBook Overview&lt;/p&gt;&lt;p&gt;--&amp;gt; Quick Start Guide for IntelliStation POWER 185&lt;/p&gt;&lt;p&gt;--&amp;gt; Installing SUSE Linux Enterprise Server 9 SP3 on POWER 185&lt;/p&gt;&lt;p&gt;--&amp;gt; Installing Red Hat Enterprise Linux AS 4 Update 3 on POWER 185&lt;/p&gt;&lt;p&gt;--&amp;gt; TAGITT-CATIA V4/ENOVIA DMU Evaluation&lt;/p&gt;&lt;p&gt;---&lt;/p&gt;&lt;p&gt;--&amp;gt; UNIX Workstations Facts and Features (2006)&lt;/p&gt;&lt;p&gt;--&amp;gt; System i &amp;amp; p PCI adapters (44 MB!)&lt;/p&gt;&lt;p&gt;--&amp;gt; System i &amp;amp; p fans&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM IntelliStation POWER and IBM RS/6000 Graphics Performance Report&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM Installation Toolkit 4.2 Release Notes&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM Installation Toolkit 4.2 User's Manual&lt;/p&gt;&lt;p&gt;--&amp;gt; RS/6000 SpaceBall Announcement on AIX&lt;/p&gt;&lt;p&gt;---&lt;/p&gt;&lt;p&gt;--&amp;gt; 3Dconnexion 3DxWare AIX Driver (1.6.0 11/29/2011)&lt;/p&gt;&lt;p&gt;--&amp;gt; 3Dconnexion Xdriver AIX 5+ (4.66 11/18/2005)&lt;/p&gt;&lt;p&gt;Note: it seems that the 3DxWare driver should support the SpaceBall and SpaceMouse, along with offering new support for the SpacePilot and SpaceExplorer on AIX. However if that's not the case the older Xdriver can be used.&lt;/p&gt;&lt;p&gt;--&amp;gt; Open Sound System for AIX (OSS/AIX v3.9.8g)&lt;/p&gt;&lt;p&gt;Note: you must purchase a license from 4Front Technologies to activate OSS/AIX (while it's released as open source the old UNIX versions are not).&lt;/p&gt;&lt;p&gt;PowerPC 970MP running in single-core?&lt;/p&gt;&lt;p&gt;It may sound strange but the IntelliStation POWER 185 is in fact running the PowerPC 970MP "dual core" chip in single core mode. This is undocumented on the PPC 970MP datasheet and not implemented on anything else as far as I can tell. I'm not exactly sure how this was achieved but it's either a hidden switch on the PPC970MP or a special switch done through firmware. The reason why this was done was to reduce the thermal requirements for a smaller system and smaller heatsink; as a work-around IBM then offered adding a second PowerPC 970MP in the IntelliStation 185 (also running in single-core mode) to simulate a "dual-core" PowerPC 970MP setup through two discreet chips instead of one.&lt;/p&gt;&lt;p&gt;7037-A50 or 7047-185, which came first&lt;/p&gt;&lt;p&gt;It's easy to tell the 7037-A50 came first based on the fact documentation always shows the system with a 7037 bezel instead of the characteristic one present on 7047: even on the IntelliStation POWER 185's service diagram on the metal door. Furthermore based on the timing, where IBM felt they needed a lower-end IntelliStation POWER to complement the already released 285 / why create a system from scratch when the upcoming 7037 design could be used. It gets somewhat interesting because this means the IntelliStation 185's bezel was designed after-the-fact, as a result that's why it is so thick on the front to compensate for the slanting that's characeristic of (most) IntelliStations. However, vendors and IBM constantly mix 7037 and 7047 in stock photos and documentation. The photo below shows a 7037 and you can see how it is inspired by 9228--oh and the bezel is flush against the front:&lt;/p&gt;&lt;p&gt;The story doesn't end there... System p5 185 sold *so* poorly (and was more work for IBM to support due to the PPC 970 that really should have never been considered p5) that IBM discontinued it a year after its release in 2007, while the IntelliStation 185 continued to be sold until 2009.&lt;/p&gt;&lt;p&gt;Deceptive as a Greyghost (PC Server 500)&lt;/p&gt;&lt;p&gt;The IntelliStation 185 POWER was aimed towards CAD running under AIX, and had a specific emphasis on quiet operations, as mentioned in the hardware announcement "A quiet deskside form factor". Unlike System x and the IntelliStation 9228 (which is based on a System x), these run amazingly quiet for what they are--which begs the question why couldn't have IBM calibrated the cooling of System x like the POWER 185 as the design (internally) is very similar to 9228?* In fact the original external appearance of 7037-A50 could be confused with 9228; so there was some inspiration between the two. A lot of attention was put into the acoustics as the system is lined extensively with foam, unlike the 9228 or even 7382 which hails as one of the poorest 'acoustic' performers and makes a similar sound to an IntelliStation POWER 285. The characteristic front fold baffle is also meant to reduce noise (and is actually a separate piece clipped in to the main bezel: the design would be rather complex to injection mold as a single piece).in).&lt;/p&gt;&lt;p&gt;*I have confirmed System x towers can run fine with noctua fans / of course I suppose System x is not design to reside as a workstation, but still. The servers have thermal sensors so it would be VERY little effort for IBM to calibrate the fans at a slower RPM instead of hard-lining at a higher-than-necessary value--'desktop mode' could even be added to the UEFI. Even worse, the IntelliStation 6225 was never shipped with a rear fan when it's REALLY ideal to have one (else the FSB can get up to 90C), and when a rear fan is installed the controller throttles it down to a few *hundred* RPM. I suppose different engineers were involved with the RPM tuning on 7047-185, 7382, 9228, 9229 and 6225 so I shouldn't be that critical.&lt;/p&gt;&lt;p&gt;Here with the IntelliStation POWER 185 opened up into pieces you can see the foam lining behind the front bezel along with the interior baffles being lined with foam. The foam on the front bezel was actually manually cut near the intake bezel to angle it off:&lt;/p&gt;&lt;p&gt;It's a workstation not a server&lt;/p&gt;&lt;p&gt;Some mistake the IntelliStation POWER 185 as a "server" or try to say that IntelliStation POWERs in general are servers, but it was always meant as an end-user workstation typically involving CAD or industrial design. The correct "server" product would be system type 7037-A50, the A50 was modified to perform as a server and not a CAD workstation / and also features some differences supported PCI add-ons. It's still hard to distinguish the two when the workstations are directly derived from the servers.&lt;/p&gt;&lt;p&gt;By all means you can use an IntelliStation POWER as a server, but then it's being under-utilizing for what it can really do and missing an opportunity for getting the most out of AIX as a "desktop" OS. Something which is rarely exhibited with the phasing out of the CAD components the IntelliStations first brought to the table.&lt;/p&gt;&lt;p&gt;UNIX high-end CAD meets its death&lt;/p&gt;&lt;p&gt;When the ancients roamed the earth (2006), Dassault Systemes ported CATIA V5 and V6 to AIX, most likely by request from IBM. What's weird about this is that you'll be hard-pressed to find any references to CATIA being run on UNIX now, Dassault completely removed it but there's probably a story behind this as to why. If you go back to the Dassault Systemes website in early 2007, you'll see that they still reference support for Windows and all major flavours of UNIX--but there's one key phrase near the end where they say: "Dassault Systemes has set up a certification program... (blah blah blah)... since the launch of V5, natively developed on windows NT Platform". At that point Windows XP 32-bit was the most commonly used variant, and while you could run XP 64-bit (and IBM did have native support for it on the IntelliStation 9228), XP 64-bit had so many problems so most users were stuck with 3.9 GB of RAM. Therefore if we were to assume that UNIX and said UNIX hardware offered way more memory, it starts to make sense (not to mention that the drivers and video cards would be extremely optimized and had features nVidia and ATI didn't offer*--at the time--). This may be even supported further from the POWER 185 RedBook where IBM states: "The addition of native 64-bit capability brings significant performance enhancements of CATIA V5 and ENOVIA DMU Navigator. Initial benchmarks indicate that for memory-intensive operations, such as analysis of large models, global performance is significantly increased. In addition, improvements to clash analysis greatly reduces processing times for analysis of large assemblies." It's also worth pointing out that CATIA predates Windows, so Dassault probably rewrote V5 from scratch on NT.&lt;/p&gt;&lt;p&gt;*HP's HP-UX hardware being an exception since they just sloppily hacked in standard ATI cards / which means you wouldn't get the extra benefits of running a GXT6500 on AIX as you would with a FireGL X3 on HP-UX. HP probably had the lowest share of the UNIX CAD market so they probably felt little need to invest much R&amp;amp;D: not to mention HP can't make a proper enterprise workstation or server ANYWAYS.&lt;/p&gt;&lt;p&gt;While the IntelliStation lineup was never involved with the sale of the PC division to Lenovo (and continued to press on for awhile), IBM dropped the x86 IntelliStations in 2006 and the POWER IntelliStations in 2009. Why did IBM bother to dedicate CAD resources to AIX when they were deliberately planning to phase out the platform? As established already, UNIX workstations were 'still a thing' back then and had advantages over the terrible x86 machines at the time (Windows XP with Pentium 4), and it was profitable for IBM (along with HP, SGI and Sun) to manufacture such computers. When x64 Windows 7 rolled by various changes in the market happened alongside it: Sun and SGI went out of business, HP started downsizing and split their enterprise into HPE, and Dassault Systemes dropped support for CAITA on UNIX. The UNIX CAD market had the carpet pulled from underneath and now UNIX was set aside for even more specialized applications.&lt;/p&gt;&lt;p&gt;The IntelliStation POWER lineup left a few remnants which are still gawked at by users of newer POWER7/8 systems, such as the high-performance CAD GPUs. AIX has never again received high performance video cards / apparently some IBM customers have complained of this, but not enough to warrant development on better AIX graphics cards. As a result, for more realistic graphics performance AIX users continue to use the IntelliStation cards.&lt;/p&gt;&lt;p&gt;There is a story underneath the death march of AIX workstations, if you look carefully you'll notice the GXT6500P was originaly announced in 2002 (along with the IntelliStation POWER 265). IBM never refreshed any of the GXT135P, GXT4500P and GXT6500P cards, still keeping them as "current" offerings for the IntelliStation POWER 185 and 285. And soon as the last two of the POWER IntelliStations were made, other OEMs stopped making UNIX CAD stations and IBM quietly killed all IntelliStations, only leaving System x and System i behind (of course IBM has now even sold System x off in 2014 after the U.S. government reviewed the case as they used System x servers internally).&lt;/p&gt;&lt;p&gt;POWER Linux vs AIX on the IntelliStation POWER 185&lt;/p&gt;&lt;p&gt;Let's get it out of the way: many think AIX will be dead soon and stipulate IBM is wanting clients to all mass migrate to POWER Linux (in fact, Watson was even programmed in POWER Linux and not AIX). The truth of the matter is that AIX contains more features that IBM or anyone else does not add in Linux. For instance, going through the RedBooks and Hardware Announcement, the higher-end CAD cards are not supported under Linux, RAS is not supported, the sound card is not supported and it goes on and on. In other words you're not going to use be using Linux for CAD, graphical operations or sound any time soon. While it may 'seem' IBM wants everyone on POWER Linux and not AIX, you can't use it as a workstation operating system unlike AIX.&lt;/p&gt;&lt;p&gt;Because the IntelliStation POWER 185 uses the aforementioned PowerPC 970, POWER Linux itself has limited support on the system. In the June 23 2009 hardware announcement, IBM confirms that SUSE Linux Enterprise Server 11 has dropped support for the PowerPC 970:&lt;/p&gt;&lt;p&gt;"SUSE Linux Enterprise Server 11 (SLES 11) for POWER supports all POWER5 and POWER6 technology-based systems with the exception of the OpenPower line of servers. It does not support any 970 based systems, which includes JS20, JS21, IBM System p5 185, and the IBM Intellistation POWER 185."&lt;/p&gt;&lt;p&gt;Whereas on AIX 7.1 PPC 970 is still supported (note that AIX 7.2 dropped it, but so was the IntelliStation POWER 285):&lt;/p&gt;&lt;p&gt;"Only 64-bit Common Hardware Reference Platform (CHRP) machines running selected PowerPC 970, POWER4, POWER5, POWER6, and POWER7 processors that implement the POWER architecture Platform Requirements (PAPR) are supported."&lt;/p&gt;&lt;p&gt;It's therefore far more logical to use AIX to get the most out of the IntelliStation's workstation-oriented hardware, since POWER Linux has virtually nothing to offer for graphics support. While AIX and UNIX workstations in general are dead, you won't see such features revisted on newer AIX hardware / and it's *completely* omitted under POWER Linux.&lt;/p&gt;&lt;p&gt;Meanwhile HPE and Intel seem to be having a hard time maintaining their Itanium contracts and want HP-UX to die / whereas AIX and POWER are in a much better position due to IBM's contributions to Linux and (now) OpenPOWER.&lt;/p&gt;&lt;p&gt;OS Support&lt;/p&gt;&lt;p&gt;AIX&lt;/p&gt;AIX 7.2 Not Supported&lt;p&gt;I'm not sure why (some) IBM documentation claims AIX 7.1 is not supported on the IntelliStation POWER 185 when it in fact is (yes! You can in fact run AIX 7.1 on a PowerPC 970 CPU).&lt;/p&gt;&lt;p&gt;WARNING: AIX 7.1 wasn't designed with the System p5 185, or AIX workstations in mind anymore; so if you run AIX 7.1 on a system with a single PSU, you will get the rc.powerfail:2 being generated in the console every so often because AIX 7.1 is looking for a redundant power supply that does not exist. Kind of strange considering the System p5 185 lacks a second power supply as well and was sold as a server rack unit.&lt;/p&gt;&lt;p&gt;POWER Linux&lt;/p&gt;&lt;p&gt; SUSE Linux Enterprise SLES 9 SP3&lt;lb/&gt; Red Hat Enterprise AS4U3&lt;/p&gt;&lt;p&gt;The Great Purge of 2016&lt;/p&gt;&lt;p&gt;The U.S. Department of Defense was using a mass horde of IntelliStation POWER 185 units / and much of them have now been tossed back to IBM reselling channels. So, in early 2016 many appeared and obtaining one became a LOT cheaper. The timing is not a coincidence, IBM is phasing out the 185 in mid 2017 so corporate customers are being told to upgrade, or their leases are expiring (requiring a forced upgrade). The fact that some of these computers have been in service for 10 years is an example of how UNIX moves slowly and that the hardware was well built. Unlike other IBM systems, the IntelliStation POWER 185 is uniquely elusive for its age, I don't know if they will continue to drop in price or get more expensive as they reach unobtanium.&lt;/p&gt;&lt;p&gt;In one of IBM's client case studies, it is explained that they sold the Polish government 400 IntelliStation POWER 185 workstations / so we can assume a similar amount was also sold to the U.S. government (and in other places, too):&lt;/p&gt;&lt;p&gt;"The organization teamed with IBM Global Technology Services to implement a solution based on IBM Lotus Domino Collaboration Express software and the Lotus software-based iDoc offering from IBM Business Partner Advatech. The IBM staff loaded the new software on the client's fleet of 400 newly purchased IBM IntelliStation(R) POWER(TM) 185 Express workstations."&lt;/p&gt;&lt;p&gt;I'm not sure how many of these IntelliStations were sold, maybe a few thousand at best? But if we were to take into consideration that a typical system would probably be around $8,000, times that by four hundred and you're looking at $3.2 million! Later the document explains that operating costs for the client were reduced as a result of using that IBM solution. I suppose in the grand scheme of things they would be--since they probably used them for 10 years / and since the hardware was so robust IBM wouldn't have to do any work on the service agreement.&lt;/p&gt;&lt;p&gt;GPU options and caveats&lt;/p&gt;&lt;p&gt;As mentioned throughout, the IntelliStation 185 was geared towards CAD design and as such supported GPU options. Here are the three main GPUs that the system supports, it may be possible to run others but I have not tried it:&lt;/p&gt;&lt;p&gt; GXT4500P (FC 2842 - IBM)&lt;lb/&gt; GXT6500P (FC 2843 - IBM)&lt;lb/&gt; GXT135P (FC 1980 - Matrox)&lt;/p&gt;&lt;p&gt;The GXT4500P and GXT6500P are the premier cards designed in-house by IBM; they only run properly on AIX and no Linux support exists. The GXT135P is a rebadged Matrox GT series card and runs under both AIX and Linux, but is far more limited in graphical capabilities. Apparently two GXT4500Ps can be ran in the IntelliStation, however I don't know if this will enable dual-monitor support. Only one GXT6500P can be ran in a system at a time. Up to FOUR GXT135Ps can be stuffed into an IntelliStation, this could theoreitcally allow eight monitors to be connected, however again, that would all depend if AIX could support it.&lt;/p&gt;&lt;p&gt;Firmware options&lt;/p&gt;&lt;p&gt;The IntelliStation POWER 185 has an array of its own unique firmware options, similar to a BIOS/UEFI on a regular x86 computer. Below is what options you get when selecting to go into the firmware menu upon when the machine first starts up:&lt;/p&gt;&lt;p&gt; Main Menu&lt;lb/&gt; 1 Select Language&lt;lb/&gt; 2 Setup Remote IPL (Initial Program Load)&lt;lb/&gt; 3 Change SCSI Settings&lt;lb/&gt; 4 Select Console&lt;lb/&gt; 5 Select Boot Options&lt;lb/&gt; 6 Power/Restart Control&lt;lb/&gt; 7 System Service Aids&lt;lb/&gt; 8 Set Beep Volume&lt;lb/&gt; 9 Select Keyboard&lt;lb/&gt; --------------------------------------------------------------------------------&lt;lb/&gt; Navigation keys:&lt;lb/&gt; M = return to Main Menu N = Next page of list&lt;lb/&gt; ESC key = return to previous screen X = eXit System Management Services&lt;lb/&gt; --------------------------------------------------------------------------------&lt;lb/&gt; Type menu item number and press Enter or select Navigation key&lt;/p&gt;&lt;p&gt;Basically all of the options are self-explanatory really. I had to change the sound of the system beep since it was WAY too loud on its default setting. I chose 6.&lt;/p&gt;&lt;p&gt;PCI adapter placement&lt;/p&gt;&lt;p&gt;Unlike most computers, the IntelliStation POWER series can be sensitive what adapter is placed where: this is due to the way each slot is tied to the north bridge via the hyper transport tunnels and individual sub PCI bridges. There are a total of four PCI-X slots with a regular 32-bit PCI slot at the very top. Slots 2 &amp;amp; 3 have direct access and operate at 133 Mhz, slots 4 &amp;amp; 5 are shared and operate at 100 Mhz, slot 1 is a regular PCI slot and operates at 33 Mhz. Slot 5 can operate at 133 Mhz but only if slot 4 is empty, otherwise it runs at 100 Mhz.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Feature code (FC)&lt;/cell&gt;&lt;cell role="head"&gt;Base unit slot priority&lt;/cell&gt;&lt;cell role="head"&gt;7047-185 max. allowed adapters&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2842 (GXT4500P)&lt;/cell&gt;&lt;cell&gt;2, 3&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2843 (GXT6500P)&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1954*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5740*&lt;/cell&gt;&lt;cell&gt;2, 3&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1984*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5706*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5707*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1983*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1978*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1979*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5700*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5701*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5759**&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4 3&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1910**&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4 3&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1905*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5758*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1986*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1987*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5713*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5714*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1977, 5716* (Fibre Channel Adapter)&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1912* (Ultra320 LVD SCSI Adapter)&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4, 1&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5736*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4, 1&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1913* (Ultra320 LVD SCSI RAID Adapter)&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5737* (Disk Controller)&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2849, 1980 (GXT135P)&lt;/cell&gt;&lt;cell&gt;2, 3, 4, 5&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2947 (4-Port Multiprotocol Adapter)&lt;/cell&gt;&lt;cell&gt;4, 5, 3, 2&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5723 (2-Port asynchronous Serial Adapter)&lt;/cell&gt;&lt;cell&gt;1, 4, 5, 3, 2&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2943 (8-Port asynchronous Serial Adapter)&lt;/cell&gt;&lt;cell&gt;1, 4, 5, 3, 2&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;8244 (Crystal PCI Audio Adapter)&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;** Extra-high bandwidth (EHB) adapter. See the Performance notes before installing this adapter.&lt;lb/&gt; * High bandwidth (HB) adapter. See the Performance notes before installing this adapter. &lt;/p&gt;&lt;p&gt; Performance notes (for optimum performance)&lt;lb/&gt; System unit information:&lt;lb/&gt; -No more than three EHB adapters can be placed in the system. If an EHB adapter is placed in the system, it must be the only EHB or HB adapter attached to the PHB it uses.&lt;lb/&gt; -No more than four HB adapters can be placed in the system&lt;lb/&gt; -No more than three Gb Ethernet ports per PHB.&lt;lb/&gt; -No more than three 1 Gb Ethernet ports per one CPU in a system. More Ethernet adapters can be added for connectivity.&lt;lb/&gt; -If an adapter lists slot 5/4, this indicates the adapter can go in slot 5 or 4, but not both 5 and 4.&lt;/p&gt;&lt;p&gt;Control Panel Functions&lt;/p&gt;&lt;p&gt;On the front of the IntelliStation POWER 185 the control panel pops out (pushing the tab sideways) and there are a fair bit of functions to choose from / the default boot (or IPL) would be set to 'N, F, P', then you simply press the power button and away it goes; you can also choose the 'service IPL' mode from the Open Firmware menu as well without ever having to touch the panel. Since some of these functions are specific to the IntelliStation 185 they're not entirely agnostic to other "IPL" systems or mainframes. I've extracted the function help from the IBM knowledgebase below. Quick trivia: the IntelliStation POWER 185's specific control panel has been borrowed and re-used in later POWER systems such as the Power 720 and Power S814, it's funny because not many know of this particular machine, yet parts of it live on, photo of it below.&lt;/p&gt;&lt;p&gt; Function 01: Display selected system operating mode, IPL speed, and firmware IPL mode&lt;lb/&gt; This function displays the selected system operating mode, speed, and firmware mode for the next IPL on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. This function displays the following information:&lt;/p&gt;&lt;p&gt; The valid logical key modes (N).&lt;lb/&gt; The IPL speed (F).&lt;lb/&gt; The firmware mode (P or T).&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 1 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 01.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0 1 _ _ _ _ _ N _ _ _ _ F _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ P _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt; Valid system operating mode is N.&lt;p&gt;Valid IPL speed display is F.&lt;/p&gt;&lt;p&gt;Valid firmware IPL modes are P and T.&lt;/p&gt;&lt;p&gt;P = permanent side boot&lt;/p&gt;&lt;p&gt;T = temporary side boot&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 1 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 02: Select firmware IPL mode&lt;lb/&gt; This function allows you to select the firmware IPL mode on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 02..&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ P _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 02. The current firmware mode is displayed.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ T &amp;lt; _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the firmware IPL modes. Valid firmware IPL modes are P and T.&lt;p&gt;P = permanent side boot&lt;/p&gt;&lt;p&gt;T = temporary side boot&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to select the firmware IPL mode and exit function 02.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 04: Lamp test&lt;lb/&gt; This function performs a lamp test on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. This function shows whether any control panel indicators are burned out and whether characters that are displayed in the Function/Data display on the control panel are valid. When you activate this test, all the control panel lights and indicators are lit.&lt;/p&gt;&lt;p&gt;The lamp test continues on the system control panel for four minutes. Use this procedure to verify that the lights on the system control panel are working correctly. If you cannot complete these steps, see "Starting Point for All Problems" in the Problem Analysis information for your system to start problem analysis.&lt;/p&gt;&lt;p&gt; 1. Power on the system.&lt;lb/&gt; 2. Press the Increment (up) or Decrement (down) buttons on the control panel to display function 04.&lt;lb/&gt; Press Enter on the control panel.&lt;lb/&gt; 3. Do all of the lights and indicators on the system control panel come on?&lt;lb/&gt; Yes No&lt;lb/&gt; -&amp;gt; Exchange the control panel or the replaceable unit that contains the control panel function [system unit backplane (MB1) or tower card (CB1)]. See "Removal and Installation Procedures" in the Problem Analysis information for your system.&lt;lb/&gt; 4. Do the expansion unit control panel lights all come on?&lt;lb/&gt; Yes No&lt;lb/&gt; -&amp;gt; Exchange the control panel on the expansion unit.&lt;/p&gt;&lt;p&gt;The lights on the system control panel are working correctly. This ends the procedure.&lt;/p&gt;&lt;p&gt; Function 05: Remind mode&lt;lb/&gt; This function allows you to place the system fault-indicator LED in remind mode on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode.&lt;/p&gt;&lt;p&gt;When the system fault-indicator LED is on solid, an error condition exists on the system. If you want to defer the repair of the error, you can place the system fault-indicator LED in remind mode. Placing the system in remind mode causes the system fault-indicator LED to flash instead of being on solid. The remind mode lets you know that a system fault that you have deferred still exists on the system. If any other serviceable event occurs on the system, the remind mode is changed back to system fault mode, where the LED is on solid.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 05.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;R E M I N D M O D E O N _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt; Press Enter to start function 05. Valid options are:&lt;p&gt;Remind mode ON&lt;/p&gt;&lt;p&gt;Remind mode OFF&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;R E M I N D M O D E O F F _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to toggle the option on or off.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;&lt;lb/&gt; Light path diagnostic card indicator LED layout for the 7037-A50 and 7047-185 models&lt;/p&gt;&lt;p&gt; 1 Power supply fault-indicator LED&lt;lb/&gt; 2 Voltage-regulator module fault-indicator LED&lt;lb/&gt; 3 Disk-drive bay fan fault-indicator LED&lt;lb/&gt; 4 Optical-media bay fault-indicator LEDs&lt;lb/&gt; 5 Disk-drive bay fault-indicator LEDs&lt;lb/&gt; 6 System backplane fault-indicator LED&lt;lb/&gt; 7 Front fan fault-indicator LED&lt;lb/&gt; 8 Battery fault-indicator LED&lt;lb/&gt; 9 PCI adapter fault-indicator LED&lt;lb/&gt; 10 Thermal fault-indicator LED&lt;lb/&gt; 11 Rear fan fault-indicator LED&lt;lb/&gt; 12 Memory fault-indicator LED&lt;/p&gt;&lt;p&gt; Function 06: Display the BMC version&lt;lb/&gt; This function displays the base motherboard controller (BMC) version on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. &lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 6 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 06.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;B M C: A W 8 T x x A _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 06. An example of the BMC version is AW8T23A.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 6 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 09: Display the BMC fan speed&lt;lb/&gt; This function displays the base motherboard controller (BMC) fan speed on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. The display alternates every two seconds between MAIN, DASD, and PCI fan speed.&lt;lb/&gt; The following table provides details about this function. &lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 9 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 09.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;M A I N: 7 b 0 _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 09. The main fan speed is listed in hexadecimal (rpm).&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;D A S D: 7 0 0 _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 09. The DASD fan speed is listed in hexadecimal (rpm).&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;P C I: 7 b 0 _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 09. The PCI (I/O) fan speed is listed in hexadecimal (rpm).&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 9 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 10: Display the temperature&lt;lb/&gt; This function displays the temperature on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. The display alternates every two seconds between ambient, CPU1, and CPU2 temperature. The following table provides details about this function. &lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 1 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 10.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;A m b i e n t : 3 e , 3 e _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt; Press Enter to start function 10. The ambient temperature is listed in hexadecimal (degrees Celsius).&lt;p&gt;The first value is the average temperature over a time span.&lt;/p&gt;&lt;p&gt;The last value is the most recent temperature reading.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;C P U 1 : 5 0 , 6 f _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt; Press Enter to start function 10. The CPU1 temperature is listed in hexadecimal (degrees Celsius).&lt;p&gt;The first value is the average temperature over a time span.&lt;/p&gt;&lt;p&gt;The last value is the most recent temperature reading.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;C P U 2 : 0 , 0 _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 10. The CPU2 temperature is listed in hexadecimal (degrees Celsius).&lt;p&gt;The first value is the average temperature over a time span.&lt;/p&gt;&lt;p&gt;The last value is the most recent temperature reading.&lt;/p&gt;&lt;p&gt;The reading is 0 if the system is one-way.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;1 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 20: System type and model&lt;lb/&gt; This function displays the machine type and model on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. The machine type and model is displayed in the following format:&lt;lb/&gt; p p p p - m m m _ _ _ _ _ _ _ _&lt;lb/&gt; The values are indicated as follows:&lt;lb/&gt; Values for p indicate the machine type.&lt;lb/&gt; Values for m indicate the machine model.&lt;/p&gt;&lt;p&gt; Function 22: Partition dump&lt;lb/&gt; This function initiates a dump of a partition's operating system data on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. You must perform two consecutive function 22 selections to initiate a partition dump. The following table shows an example of function 22. &lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 22.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2 2 _ _ _ _ 0 0 _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 22.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;A 1 0 0 3 0 2 2 _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Displays the partition dump verification system reference code (SRC).&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 22.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;2 2 _ _ _ _ 0 0 _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 22.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45401907</guid><pubDate>Sun, 28 Sep 2025 05:04:39 +0000</pubDate></item><item><title>Beyond OpenMP in C++ and Rust: Taskflow, Rayon, Fork Union</title><link>https://ashvardanian.com/posts/beyond-openmp-in-cpp-rust/</link><description>&lt;doc fingerprint="776ed3c34da494ba"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;TL;DR: Most C++ and Rust thread-pool libraries leave significant performance on the table - often running 10× slower than OpenMP on classic fork-join workloads and micro-benchmarks. So I’ve drafted a minimal ~300-line library called Fork Union that lands within 20% of OpenMP. It does not use advanced NUMA tricks; it uses only the C++ and Rust standard libraries and has no other dependencies.&lt;/p&gt;&lt;p&gt;Update (Sep 2025): Since the v2 release, Fork Union supports NUMA and Huge Pages, as well as&lt;/p&gt;&lt;code&gt;tpause&lt;/code&gt;,&lt;code&gt;wfet&lt;/code&gt;, and other “pro” features. Check the README for details.&lt;/quote&gt;
    &lt;p&gt;OpenMP has been the industry workhorse for coarse-grain parallelism in C and C++ for decades. I lean on it heavily in projects like USearch, yet I avoid it in larger systems because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fine-grain parallelism with independent subsystems doesn’t map cleanly to OpenMP’s global runtime.&lt;/item&gt;
      &lt;item&gt;Portability of the C++ STL and the Rust standard library is better than OpenMP.&lt;/item&gt;
      &lt;item&gt;Meta-programming with OpenMP is a pain - mixing &lt;code&gt;#pragma omp&lt;/code&gt;with templates quickly becomes unmaintainable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So I went looking for ready-made thread pools in C++ and Rust — only to realize most of them implement asynchronous task queues, a much heavier abstraction than OpenMP’s fork-join model. Those extra layers introduce what I call the four horsemen of low performance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Locks &amp;amp; mutexes with syscalls in the hot path.&lt;/item&gt;
      &lt;item&gt;Heap allocations in queues, tasks, futures, and promises.&lt;/item&gt;
      &lt;item&gt;Compare-and-swap (CAS) stalls in the pessimistic path.&lt;/item&gt;
      &lt;item&gt;False sharing unaligned counters thrashing cache lines.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With today’s dual-socket AWS machines pushing 192 physical cores, I needed something leaner than Taskflow, Rayon, or Tokio. Enter Fork Union.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;Hardware: AWS Graviton 4 metal (single NUMA node, 96× Arm v9 cores, 1 thread/core). Workload: “ParallelReductionsBenchmark” - summing single-precision floats in parallel. In this case, just one cache line (&lt;code&gt;float[16]&lt;/code&gt;) per core—small enough to stress synchronization cost of the thread pool rather than arithmetic throughput of the CPU.
In other words, we are benchmarking kernels similar to:&lt;/p&gt;
    &lt;p&gt;Google Benchmark numbers for the C++ version of Fork Union, compared to OpenMP, Taskflow, and allocating 96× &lt;code&gt;std::thread&lt;/code&gt; objects on-demand, are as follows:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;I’ve cleaned up the output, focusing only on the relevant rows and the reduction throughput.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Criterion.rs numbers for the Rust version of Fork Union, compared to Rayon, Tokio, and Smol’s Async Executors, are as follows:&lt;/p&gt;
    &lt;p&gt;The timing methods used in those two executables are different, but the relative observations should hold.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spawning new threads is obviously too expensive.&lt;/item&gt;
      &lt;item&gt;Most reusable thread pools are still 10x slower to sync than OpenMP.&lt;/item&gt;
      &lt;item&gt;OpenMP isn’t easy to compete with and still outperforms Fork Union by 20%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This clearly shows, how important it is to chose the right tool for the job. Don’t pick an asynchronous task pool for a fork-join blocking workload!&lt;/p&gt;
    &lt;head rend="h2"&gt;Four Horsemen of Performance&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;This article won’t be a deep dive into those topics. Each deserves its own article and a proper benchmark, with some good ones already available and linked.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Locks and Mutexes&lt;/head&gt;
    &lt;p&gt;Unlike the &lt;code&gt;std::atomic&lt;/code&gt;, the &lt;code&gt;std::mutex&lt;/code&gt; update may result in a system call, and it can be expensive to acquire and release.
Its implementations generally have 2 executable paths:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the fast path, where the mutex is not contended, where it first tries to grab the mutex via a compare-and-swap operation, and if it succeeds, it returns immediately.&lt;/item&gt;
      &lt;item&gt;the slow path, where the mutex is contended, and it has to go through the kernel to block the thread until the mutex is available.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On Linux, the latter translates to a “futex” syscall and an expensive context switch. In Rust, the same applies to &lt;code&gt;std::async::atomic&lt;/code&gt; and &lt;code&gt;std::sync::Mutex&lt;/code&gt;.
Prefer the former when possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory Allocations&lt;/head&gt;
    &lt;p&gt;Most thread-pools use classes like &lt;code&gt;std::future&lt;/code&gt;, &lt;code&gt;std::packaged_task&lt;/code&gt;, &lt;code&gt;std::function&lt;/code&gt;, &lt;code&gt;std::queue&lt;/code&gt;, &lt;code&gt;std::conditional_variable&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;In Rust land, there will often be a&lt;/p&gt;&lt;code&gt;std::Box&lt;/code&gt;,&lt;code&gt;std::Arc&lt;/code&gt;,&lt;code&gt;std::collections::VecDeque&lt;/code&gt;,&lt;code&gt;std::sync::mpsc&lt;/code&gt;or even&lt;code&gt;std::sync::mpmc&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Most of those, I believe, aren’t unusable in Big-Data applications, where you always operate in memory-constrained environments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raising a &lt;code&gt;std::bad_alloc&lt;/code&gt;exception when there is no memory left and just hoping that someone up the call stack will catch it is not a great design idea for Systems Engineering.&lt;/item&gt;
      &lt;item&gt;The threat of having to synchronize ~200 physical CPU cores across 2-8 sockets and potentially dozens of NUMA nodes around a shared global memory allocator practically means you can’t have predictable performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we focus on a simpler &lt;del&gt;concurrency&lt;/del&gt; parallelism model, we can avoid the complexity of allocating shared states, wrapping callbacks into some heap-allocated “tasks”, and a lot of other boilerplates.&lt;/p&gt;
    &lt;p&gt;Less work = more performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Atomics and CAS&lt;/head&gt;
    &lt;p&gt;Once you get to the lowest-level primitives on concurrency, you end up with the &lt;code&gt;std::atomic&lt;/code&gt; and a small set of hardware-supported atomic instructions.
Hardware implements it differently:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;x86 is built around the “Total Store Order” (TSO) memory consistency model and provides &lt;code&gt;LOCK&lt;/code&gt;variants of the&lt;code&gt;ADD&lt;/code&gt;and&lt;code&gt;CMPXCHG&lt;/code&gt;. These variants act as full-blown “fences” — no loads or stores can be reordered across them. This makes atomic operations on x86 straightforward but heavyweight.&lt;/item&gt;
      &lt;item&gt;Arm, on the other hand, has a “weak” memory model and provides a set of atomic instructions that are not fenced and match the C++ concurrency model. It offers &lt;code&gt;acquire&lt;/code&gt;,&lt;code&gt;release&lt;/code&gt;, and&lt;code&gt;acq_rel&lt;/code&gt;variants of each atomic instruction — such as&lt;code&gt;LDADD&lt;/code&gt;,&lt;code&gt;STADD&lt;/code&gt;, and&lt;code&gt;CAS&lt;/code&gt;— which allow precise control over visibility and order, especially with the introduction of “Large System Extension” (LSE) instructions in Armv8.1-A.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A locked atomic on x86 requires the cache line in the Exclusive state in the requester’s L1 cache. This would incur a coherence transaction (Read-for-Ownership) if another core had the line. Both Intel and AMD handle this similarly.&lt;/p&gt;
    &lt;p&gt;It makes Arm and Power much more suitable for lock-free programming and concurrent data structures, but some observations hold for both platforms. Most importantly, “Compare and Swap” (CAS) is costly and should be avoided at all costs.&lt;/p&gt;
    &lt;p&gt;On x86, for example, the &lt;code&gt;LOCK ADD&lt;/code&gt; can easily take 50 CPU cycles.
It is 50x slower than a regular &lt;code&gt;ADD&lt;/code&gt; instruction but still easily 5-10x faster than a &lt;code&gt;LOCK CMPXCHG&lt;/code&gt; instruction.
Once the contention rises, the gap naturally widens, further amplified by the increased “failure” rate of the CAS operation when the value being compared has already changed.
That’s why, for the “dynamic” mode, we resort to using an additional atomic variable rather than more typical CAS-based implementations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alignment&lt;/head&gt;
    &lt;p&gt;Assuming a thread pool is a heavy object anyway, nobody will care if it’s a bit larger than expected. That allows us to over-align the internal counters to &lt;code&gt;std::hardware_destructive_interference_size&lt;/code&gt; or &lt;code&gt;std::max_align_t&lt;/code&gt; to avoid false sharing.
In that case, even on x86, where the entire cache will be exclusively owned by a single thread, in eager mode, we end up effectively “pipelining” the execution, where one thread may be incrementing the “in-flight” counter while the other is decrementing the “remaining” counter.
Others are executing the loop body in between.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing APIs&lt;/head&gt;
    &lt;head rend="h3"&gt;Fork Union&lt;/head&gt;
    &lt;p&gt;Fork Union has a straightforward goal, so its API is equally clear. There are only 4 core interfaces:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;for_each_thread&lt;/code&gt;- to dispatch a callback per thread, similar to&lt;code&gt;#pragma omp parallel&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;for_each_static&lt;/code&gt;- for individual evenly-sized tasks, similar to&lt;code&gt;#pragma omp for schedule(static)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;for_each_slice&lt;/code&gt;- for slices of evenly-sized tasks, similar to nested&lt;code&gt;#pragma omp for schedule(static)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;for_each_dynamic&lt;/code&gt;- for individual unevenly-sized tasks, similar to&lt;code&gt;#pragma omp for schedule(dynamic, 1)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They all receive a C++ lambda or a Rust closure and a range of tasks to execute. The construction of the thread pool itself is a bit trickier than typically in standard libraries, as “exceptions” and “panics” are not allowed. So, the constructor can’t perform any real work. In C++, the &lt;code&gt;try_spawn&lt;/code&gt; method can be called to allocate all the threads:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;As you may have noticed, the lambdas are forced to be&lt;/p&gt;&lt;code&gt;noexcept&lt;/code&gt;and can’t return anything. This is a design choice that vastly simplifies the implementation.&lt;/quote&gt;
    &lt;p&gt;In Rust, similarly, the &lt;code&gt;try_spawn&lt;/code&gt; method can be used:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Assuming Rust has no function overloading, there are a few alternatives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;try_spawn&lt;/code&gt;- to spawn a thread pool with the main allocator.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;try_spawn_in&lt;/code&gt;- to spawn a thread pool with a custom allocator.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;try_named_spawn&lt;/code&gt;- to spawn a thread pool with the main allocator and a name.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;try_named_spawn_in&lt;/code&gt;- to spawn a thread pool with a custom allocator and a name.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Rayon&lt;/head&gt;
    &lt;p&gt;Rayon is the go-to Rust library for data parallelism. It suffers from the same core design issues as every other thread pool I’ve looked at on GitHub, but it’s fair to say that at the high level, it provides outstanding coverage for various parallel iterators! As such, there is an open call to explore similar “Map-Reduce” and “Map-Fork-Reduce” patterns in Fork Union to see if they can be implemented efficiently.&lt;/p&gt;
    &lt;p&gt;The default &lt;code&gt;.par_iter()&lt;/code&gt; API of Rayon, at the start of the README.md, is not how I’ve used it in “Parallel Reductions Benchmark”.
To ensure that we are benchmarking the actual synchronization cost of the thread pool, I’ve gone directly to the underlying &lt;code&gt;rayon::ThreadPool&lt;/code&gt; API:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Taskflow&lt;/head&gt;
    &lt;p&gt;Taskflow is one of the most popular C++ libraries for parallelism. It has many features, including async execution graphs on CPUs and GPUs. The most common example looks like this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Despite being just an example, it clearly shows how different Taskflow’s core objectives are from OpenMP and Fork Union. It is still probably mainly used for simple static parallelism, similar to our case without complex dependencies and the &lt;code&gt;taskflow&lt;/code&gt; can be reused.
Here is how “Parallel Reductions Benchmark” wraps Taskflow:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Only the &lt;code&gt;operator()&lt;/code&gt; method is timed, leaving the construction costs out of the equation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions &amp;amp; Observations&lt;/head&gt;
    &lt;p&gt;Fork Union shows that a lean, 300-line fork-join pool can sit within ~20% of OpenMP, while more functional pools trail by an order of magnitude. That margin will shift as more workloads, CPUs, and compilers are tested, so treat today’s numbers as directional, not gospel. There may still be subtle memory-ordering bugs lurking in Fork Union, but the core observations should hold: dodge mutexes, dynamic queues, likely-pessimistic CAS paths, and false sharing — regardless of language or framework.&lt;/p&gt;
    &lt;p&gt;Rust is still new territory for me. The biggest surprise is the missing allocator support in &lt;code&gt;std::collections&lt;/code&gt; on the stable toolchain.
Nightly’s &lt;code&gt;Vec::try_reserve_in&lt;/code&gt; helps, but until stable lands, ergonomic custom allocation remains tricky.
The machinery exists in C++, yet most projects ignore it — so the culture needs to catch up.&lt;/p&gt;
    &lt;p&gt;PS: Spot dubious memory-ordering? Open an issue. Want to close the remaining 20% gap? Happy forking 🤗&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Fork Union, arguably the most unusual parallel-processing library on GitHub, just crossed its first 100 stars — my 12th project to reach that milestone 🥳&lt;/p&gt;— Ash Vardanian (@ashvardanian) September 7, 2025&lt;lb/&gt;Repository: https://t.co/Gyg43GW6d7&lt;lb/&gt;Unlike typical thread-pools, it avoids not only mutexes but even Compare-and-Swap… pic.twitter.com/H2H7fgaXl4&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45402820</guid><pubDate>Sun, 28 Sep 2025 08:53:36 +0000</pubDate></item><item><title>US Military struggling to deploy AI weapons</title><link>https://www.msn.com/en-us/money/companies/us-military-is-struggling-to-deploy-ai-weapons/ar-AA1NoiNK</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45403134</guid><pubDate>Sun, 28 Sep 2025 10:00:05 +0000</pubDate></item><item><title>Show HN: Curated gamedev specific search engine</title><link>https://gamedevtorch.com/</link><description>&lt;doc fingerprint="1f0ebe35a1e6ef04"&gt;
  &lt;main&gt;
    &lt;p&gt;GameDev Torch is a small gamedev specific search engine.&lt;/p&gt;
    &lt;p&gt;Search gamedev related articles, game engines, frameworks, blog posts and more from a manually curated set of websites. Use it to complement your general purpose search engines to easily find niche resources.&lt;/p&gt;
    &lt;head rend="h4"&gt;What can I do?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Looking for inspiration? Learn more about "multiplayer framework"&lt;/item&gt;
      &lt;item&gt;Something's missing? Suggest a new resource to index&lt;/item&gt;
      &lt;item&gt;Want to know more? Learn how to query effectively&lt;/item&gt;
      &lt;item&gt;Or browse the list of all indexed websites&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Who made this?&lt;/head&gt;
    &lt;p&gt;You can find me on bsky&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45403288</guid><pubDate>Sun, 28 Sep 2025 10:36:23 +0000</pubDate></item><item><title>Fred Dibnah shows how to erect a chimney scaffold at 200 feet (1982) [video]</title><link>https://www.youtube.com/watch?v=w3ma9iYx4rg</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45403373</guid><pubDate>Sun, 28 Sep 2025 10:52:10 +0000</pubDate></item><item><title>Why I gave the world wide web away for free</title><link>https://www.theguardian.com/technology/2025/sep/28/why-i-gave-the-world-wide-web-away-for-free</link><description>&lt;doc fingerprint="a3849a7306b61faf"&gt;
  &lt;main&gt;
    &lt;p&gt;I was 34 years old when I first had the idea for the world wide web. I took every opportunity to talk about it: pitching it in meetings, sketching it out on a whiteboard for anyone who was interested, even drawing the web in the snow with a ski pole for my friend on what was meant to be a peaceful day out.&lt;/p&gt;
    &lt;p&gt;I relentlessly petitioned bosses at the European Organization for Nuclear Research (Cern), where I worked at the time, who initially found the idea “a little eccentric” but eventually gave in and let me work on it. I was seized by the idea of combining two pre-existing computer technologies: the internet and hypertext, which takes an ordinary document and brings it to life by adding “links”.&lt;/p&gt;
    &lt;p&gt;I believed that giving users such a simple way to navigate the internet would unlock creativity and collaboration on a global scale. If you could put anything on it, then after a while, it would have everything on it.&lt;/p&gt;
    &lt;p&gt;But for the web to have everything on it, everyone had to be able to use it, and want to do so. This was already asking a lot. I couldn’t also ask that they pay for each search or upload they made. In order to succeed, therefore, it would have to be free. That’s why, in 1993, I convinced my Cern managers to donate the intellectual property of the world wide web, putting it into the public domain. We gave the web away to everyone.&lt;/p&gt;
    &lt;p&gt;Today, I look at my invention and I am forced to ask: is the web still free today? No, not all of it. We see a handful of large platforms harvesting users’ private data to share with commercial brokers or even repressive governments. We see ubiquitous algorithms that are addictive by design and damaging to our teenagers’ mental health. Trading personal data for use certainly does not fit with my vision for a free web.&lt;/p&gt;
    &lt;p&gt;On many platforms, we are no longer the customers, but instead have become the product. Our data, even if anonymised, is sold on to actors we never intended it to reach, who can then target us with content and advertising. This includes deliberately harmful content that leads to real-world violence, spreads misinformation, wreaks havoc on our psychological wellbeing and seeks to undermine social cohesion.&lt;/p&gt;
    &lt;p&gt;We have the technical capability to give that power back to the individual. Solid is an open-source interoperable standard that I and my team developed at MIT more than a decade ago. Apps running on Solid don’t implicitly own your data – they have to request it from you and you choose whether to agree, or not. Rather than being in countless separate places on the internet in the hands of whomever it had been resold to, your data is in one place, controlled by you.&lt;/p&gt;
    &lt;p&gt;Sharing your information in a smart way can also liberate it. Why is your smartwatch writing your biological data to one silo in one format? Why is your credit card writing your financial data to a second silo in a different format? Why are your YouTube comments, Reddit posts, Facebook updates and tweets all stored in different places? Why is the default expectation that you aren’t supposed to be able to look at any of this stuff? You generate all this data – your actions, your choices, your body, your preferences, your decisions. You should own it. You should be empowered by it.&lt;/p&gt;
    &lt;p&gt;Somewhere between my original vision for web 1.0 and the rise of social media as part of web 2.0, we took the wrong path. We’re now at a new crossroads, one where we must decide if AI will be used for the betterment or to the detriment of society. How can we learn from the mistakes of the past? First of all, we must ensure policymakers do not end up playing the same decade-long game of catchup they have done over social media. The time to decide the governance model for AI was yesterday, so we must act with urgency.&lt;/p&gt;
    &lt;p&gt;In 2017, I wrote a thought experiment about an AI that works for you. I called it Charlie. Charlie works for you like your doctor or your lawyer, bound by law, regulation and codes of conduct. Why can’t the same frameworks be adopted for AI? We have learned from social media that power rests with the monopolies who control and harvest personal data. We can’t let the same thing happen with AI.&lt;/p&gt;
    &lt;p&gt;So how do we move forward? Part of the frustration with democracy in the 21st century is that governments have been too slow to meet the demands of digital citizens. The AI industry landscape is fiercely competitive, and development and governance are dictated by companies. The lesson from social media is that this will not create value for the individual.&lt;/p&gt;
    &lt;p&gt;I coded the world wide web on a single computer in a small room. But that small room didn’t belong to me, it was at Cern. Cern was created in the aftermath of the second world war by the UN and European governments who identified a historic, scientific turning point that required international collaboration. It is hard to imagine a big tech company agreeing to share the world wide web for no commercial reward like Cern allowed me to. That’s why we need a Cern-like not-for-profit body driving forward international AI research.&lt;/p&gt;
    &lt;p&gt;I gave the world wide web away for free because I thought that it would only work if it worked for everyone. Today, I believe that to be truer than ever. Regulation and global governance are technically feasible, but reliant on political willpower. If we are able to muster it, we have the chance to restore the web as a tool for collaboration, creativity and compassion across cultural borders. We can re-empower individuals, and take the web back. It’s not too late.&lt;/p&gt;
    &lt;p&gt;Tim Berners-Lee is the author of This Is for Everyone (Macmillan).&lt;/p&gt;
    &lt;head rend="h2"&gt;Further reading&lt;/head&gt;
    &lt;p&gt;The Innovators by Walter Isaacson (Simon &amp;amp; Schuster, £10.99)&lt;/p&gt;
    &lt;p&gt;The Web We Weave by Jeff Jarvis (Basic, £25)&lt;/p&gt;
    &lt;p&gt;The History of the Internet in Byte-Sized Chunks by Chris Stokel-Walker (Michael O’Mara, £12.99)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45403501</guid><pubDate>Sun, 28 Sep 2025 11:17:55 +0000</pubDate></item><item><title>EPA tells some scientists to stop publishing studies</title><link>https://www.washingtonpost.com/climate-environment/2025/09/20/epa-scientists-research-publications/</link><description>&lt;doc fingerprint="9ec9d5d9756313bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Post-American Order Starts in Riyadh and Islamabad&lt;/head&gt;
    &lt;p&gt;Renewed defense ties between Saudi Arabia and Pakistan provide a taste of what’s to come as the US retreats from its traditional alliances.&lt;/p&gt;
    &lt;p&gt;Given the long history of cooperation between Pakistan and Saudi Arabia, it’s tempting to dismiss their announcement last week of a mutual security pact as mere paperwork, formalizing a relationship that already exists. But it’s much more than that. This is the first concrete indication of what a post-American world might look like — one that is far more insecure, unstable, and unhappy.&lt;/p&gt;
    &lt;p&gt;The two nations were indeed close for decades. In 1967 — two months after Israel’s victory in the Six-Day War — they signed a security agreement in which Pakistan’s battle-hardened military promised training and support to the Kingdom. Two years later, their pilots flew for Saudi Arabia in its war against communist South Yemen.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45403656</guid><pubDate>Sun, 28 Sep 2025 11:50:25 +0000</pubDate></item><item><title>The Post-American Order Starts in Riyadh and Islamabad</title><link>https://www.bloomberg.com/opinion/articles/2025-09-24/the-post-american-order-starts-in-riyadh-and-islamabad</link><description>&lt;doc fingerprint="9ec9d5d9756313bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Post-American Order Starts in Riyadh and Islamabad&lt;/head&gt;
    &lt;p&gt;Renewed defense ties between Saudi Arabia and Pakistan provide a taste of what’s to come as the US retreats from its traditional alliances.&lt;/p&gt;
    &lt;p&gt;Given the long history of cooperation between Pakistan and Saudi Arabia, it’s tempting to dismiss their announcement last week of a mutual security pact as mere paperwork, formalizing a relationship that already exists. But it’s much more than that. This is the first concrete indication of what a post-American world might look like — one that is far more insecure, unstable, and unhappy.&lt;/p&gt;
    &lt;p&gt;The two nations were indeed close for decades. In 1967 — two months after Israel’s victory in the Six-Day War — they signed a security agreement in which Pakistan’s battle-hardened military promised training and support to the Kingdom. Two years later, their pilots flew for Saudi Arabia in its war against communist South Yemen.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45403727</guid><pubDate>Sun, 28 Sep 2025 12:06:15 +0000</pubDate></item><item><title>Failing to Understand the Exponential, Again</title><link>https://www.julian.ac/blog/2025/09/27/failing-to-understand-the-exponential-again/</link><description>&lt;doc fingerprint="2e54f1f6ac5acc0a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Failing to Understand the Exponential, Again&lt;/head&gt;
    &lt;p&gt;Sat 27 September 2025&lt;/p&gt;
    &lt;p&gt;The current discourse around AI progress and a supposed âbubbleâ reminds me a lot of the early weeks of the Covid-19 pandemic. Long after the timing and scale of the coming global pandemic was obvious from extrapolating the exponential trends, politicians, journalists and most public commentators kept treating it as a remote possibility or a localized phenomenon.&lt;/p&gt;
    &lt;p&gt;Something similarly bizarre is happening with AI capabilities and further progress. People notice that while AI can now write programs, design websites, etc, it still often makes mistakes or goes in a wrong direction, and then they somehow jump to the conclusion that AI will never be able to do these tasks at human levels, or will only have a minor impact. When just a few years ago, having AI do these things was complete science fiction! Or they see two consecutive model releases and donât notice much difference in their conversations, and they conclude that AI is plateauing and scaling is over.&lt;/p&gt;
    &lt;head rend="h3"&gt;METR&lt;/head&gt;
    &lt;p&gt;Accurately evaluating AI progress is hard, and commonly requires a combination of both AI expertise and subject matter understanding. Fortunately, there are entire organizations like METR whose sole purpose is to study AI capabilities! We can turn to their recent study "Measuring AI Ability to Complete Long Tasks", which measures the length of software engineering tasks models can autonomously perform:&lt;/p&gt;
    &lt;p&gt;We can observe a clear exponential trend, with Sonnet 3.7 achieving the best performance by completing tasks up to an hour in length at 50% success rate.&lt;/p&gt;
    &lt;p&gt;However, at this point Sonnet 3.7 is 7 months old, coincidentally the same as the doubling rate claimed by METR in their study. Can we use this to verify if METR's findings hold up?&lt;/p&gt;
    &lt;p&gt;Yes! In fact, METR themselves keep an up-to-date plot on their study website:&lt;/p&gt;
    &lt;p&gt;We can see the addition of recent models such as Grok 4, Opus 4.1, and GPT-5 at the top right of the graph. Not only did the prediction hold up, these recent models are actually slightly above trend, now performing tasks of more than 2 hours!&lt;/p&gt;
    &lt;head rend="h3"&gt;GDPval&lt;/head&gt;
    &lt;p&gt;A reasonable objection might be that we can't generalize from performance on software engineering tasks to the wider economy - after all, these are the tasks engineers at AI labs are bound to be most familiar with, creating some overfitting to the test set, so to speak.&lt;/p&gt;
    &lt;p&gt;Fortunately, we can turn to a different study, the recent GDPval by OpenAI - measuring model performance in 44 (!) occupations across 9 industries:&lt;/p&gt;
    &lt;p&gt;The evaluation tasks are sourced from experienced industry professionals (avg. 14 years' experience), 30 tasks per occupation for a total of 1320 tasks. Grading is performed by blinded comparison of human and model-generated solutions, allowing for both clear preferences and ties.&lt;/p&gt;
    &lt;p&gt;Again we can observe a similar trend, with the latest GPT-5 already astonishingly close to human performance:&lt;/p&gt;
    &lt;p&gt;You might object that this plot looks like it might be levelling off, but this is probably mostly an artefact of GPT-5 being very consumer-focused. Fortunately for us, OpenAI also included other models in the evaluation[1], and we can see that Claude Opus 4.1 (released earlier than GPT-5) performs significantly better - ahead of the trend from the previous graph, and already almost matching industry expert (!) performance:&lt;/p&gt;
    &lt;p&gt;I want to especially commend OpenAI here for releasing an eval that shows a model from another lab outperforming their own model - this is a good sign of integrity and caring about beneficial AI outcomes!&lt;/p&gt;
    &lt;head rend="h3"&gt;Outlook&lt;/head&gt;
    &lt;p&gt;Given consistent trends of exponential performance improvements over many years and across many industries, it would be extremely surprising if these improvements suddenly stopped. Instead, even a relatively conservative extrapolation of these trends suggests that 2026 will be a pivotal year for the widespread integration of AI into the economy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Models will be able to autonomously work for full days (8 working hours) by mid-2026.&lt;/item&gt;
      &lt;item&gt;At least one model will match the performance of human experts across many industries before the end of 2026.&lt;/item&gt;
      &lt;item&gt;By the end of 2027, models will frequently outperform experts on many tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It may sound overly simplistic, but making predictions by extrapolating straight lines on graphs is likely to give you a better model of the future than most "experts" - even better than most actual domain experts!&lt;/p&gt;
    &lt;p&gt;For a more concrete picture of what this future would look like I recommend Epoch AI's 2030 report and in particular the in-depth AI 2027 project.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The underperformance of both Grok 4 and Gemini 2.5 Pro is also notable, especially given state-of-the-art claims on many benchmarks when released. Beware of Goodhart's law! â©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tags: ai, progress, agi, safety, productivity&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;italics&lt;/cell&gt;
        &lt;cell&gt;surround text with &lt;quote&gt;*asterisks*&lt;/quote&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bold&lt;/cell&gt;
        &lt;cell&gt;surround text with &lt;quote&gt;**two asterisks**&lt;/quote&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;hyperlink&lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;[hyperlink](https://example.com)or just a bare URL&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;quote&gt;code&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;surround text with &lt;quote&gt;`backticks`&lt;/quote&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;surround text with &lt;quote&gt;~~two tilde characters~~&lt;/quote&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;quote&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;prefix with &lt;quote&gt;&amp;gt;&lt;/quote&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Loading comments...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45403803</guid><pubDate>Sun, 28 Sep 2025 12:19:16 +0000</pubDate></item><item><title>Listeria found Walmart meatball meals may be linked deadly fettuccine outbreak</title><link>https://www.cnn.com/2025/09/26/health/listeria-fettuccine-outbreak</link><description>&lt;doc fingerprint="ec3500026c05ac6a"&gt;
  &lt;main&gt;
    &lt;p&gt;Federal health officials are warning consumers not to eat certain heat-and-eat beef meatball pasta meals sold at Walmart stores because they may be contaminated with listeria bacteria previously linked to a deadly outbreak.&lt;/p&gt;
    &lt;p&gt;The U.S. Agriculture Department issued a public health alert late Thursday for Marketside Linguine with Beef Meatballs &amp;amp; Marinara Sauce sold in refrigerated 12-ounce clear plastic trays. The products have best-by dates of Sept. 22 through Oct. 1 and may still be in consumers’ refrigerators.&lt;/p&gt;
    &lt;p&gt;The affected meals contain the establishment numbers “EST. 50784” and “EST. 47718” inside the USDA mark of inspection on the label. They were sent to Walmart stores nationwide.&lt;/p&gt;
    &lt;p&gt;No recall has been issued, but FreshRealm, a large food producer that distributed the products, said they advised Walmart this week to pull the meals from store shelves. Additional products may be identified, according to USDA’s Food Safety and Inspection Service. Walmart officials said they put a stop on sales and removed the products from stores.&lt;/p&gt;
    &lt;p&gt;The meals may be contaminated with the same strain of listeria that caused an outbreak tied to chicken fettuccine Alfredo sold at Walmart and Kroger stores. Four people were killed and at least 20 were sickened in the outbreak updated by federal health officials on Friday evening. The outbreak led to a large recall this summer.&lt;/p&gt;
    &lt;p&gt;FreshRealm conducted tests that detected the listeria in linguine used in the meatball dish, company officials said. The strain matched the listeria identified in the chicken fettuccine Alfredo outbreak, the company said.&lt;/p&gt;
    &lt;p&gt;“We have long maintained that the source of the listeria was likely an ingredient supplied by a third party,” the company said in a statement.&lt;/p&gt;
    &lt;p&gt;The pasta came from Nate’s Fine Foods of Roseville, California. The company did not immediately respond to questions.&lt;/p&gt;
    &lt;p&gt;Listeria infections can cause serious illness, particularly in older adults, people with weakened immune systems and those who are pregnant or their newborns. Symptoms include fever, muscle aches, headache, stiff neck, confusion, loss of balance and convulsions.&lt;/p&gt;
    &lt;p&gt;About 1,600 people get sick each year from listeria infections and about 260 die, the CDC says. Federal officials in December said they were revamping protocols to prevent listeria infections after several high-profile outbreaks, including one linked to Boar’s Head deli meats that led to 10 deaths and more than 60 illnesses last year.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45404173</guid><pubDate>Sun, 28 Sep 2025 13:24:52 +0000</pubDate></item></channel></rss>