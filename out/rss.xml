<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 29 Nov 2025 02:24:26 +0000</lastBuildDate><item><title>So you wanna build a local RAG?</title><link>https://blog.yakkomajuri.com/blog/local-rag</link><description>&lt;doc fingerprint="fb5fccd27d06c5d6"&gt;
  &lt;main&gt;
    &lt;p&gt;When we launched Skald, we wanted it to not only be self-hostable, but also for one to be able to run it without sending any data to third-parties.&lt;/p&gt;
    &lt;p&gt;With LLMs getting better and better, privacy-sensitive organizations shouldn't have to choose between being left behind by not accessing frontier models and doing away with their committment to (or legal requirement for) data privacy.&lt;/p&gt;
    &lt;p&gt;So here's what we did to support this use case and also some benchmarks comparing performance when using proprietary APIs vs self-hosted open-source tech.&lt;/p&gt;
    &lt;head rend="h2"&gt;RAG components and their OSS alternatives&lt;/head&gt;
    &lt;p&gt;A basic RAG usually has the following core components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A vector database&lt;/item&gt;
      &lt;item&gt;A vector embeddings model&lt;/item&gt;
      &lt;item&gt;An LLM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And most times it also has these as well:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A reranker&lt;/item&gt;
      &lt;item&gt;Document parsing (for PDFs, PowerPoints, etc)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What that means is that when you're looking to build a fully local RAG setup, you'll need to substitute whatever SaaS providers you're using for a local option for each of those components.&lt;/p&gt;
    &lt;p&gt;Here's a table with some examples of what we might use in a scenario where we can use third-party Cloud services and one where we can't:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Proprietary Options&lt;/cell&gt;
        &lt;cell role="head"&gt;Open-Source Options&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vector Database&lt;/cell&gt;
        &lt;cell&gt;Pinecone, Turbopuffer, Weaviate Cloud, Qdrant Cloud&lt;/cell&gt;
        &lt;cell&gt;Qdrant, Weaviate, Postgres with pgvector&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vector Embeddings Provider&lt;/cell&gt;
        &lt;cell&gt;OpenAI, Cohere, Voyage&lt;/cell&gt;
        &lt;cell&gt;Sentence Transformers, BGE, E5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;GPT, Claude, Gemini&lt;/cell&gt;
        &lt;cell&gt;Llama, Mistral, GPT-OSS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reranker&lt;/cell&gt;
        &lt;cell&gt;Cohere, Voyage&lt;/cell&gt;
        &lt;cell&gt;BGE Reranker, Sentence Transformers Cross-Encoder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Document Parsing&lt;/cell&gt;
        &lt;cell&gt;Reducto, Datalab&lt;/cell&gt;
        &lt;cell&gt;Docling&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Do note that running something locally does not mean it needs to be open-source, as one could pay for a license to self-host proprietary software. But at Skald our goal was to use fully open-source tech, which is what I'll be convering here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The table above is far from covering all available options on both columns, but basically it gives you an indication of what to research into in order to pick a tool that works for you.&lt;/p&gt;
    &lt;p&gt;As with anything, what works for you will greatly depend on your use case. And you need to be prepared to run a few more services than you're used to if you've just been calling APIs.&lt;/p&gt;
    &lt;p&gt;For our local stack, we went with the easiest setup for now to get it working (and it does! see writeup on this lower down) but will be running benchmarks on all other options to determine the best possible setup.&lt;/p&gt;
    &lt;p&gt;This is what we have today:&lt;/p&gt;
    &lt;p&gt;Vector DB: Postgres + pgvector. We already use Postgres and didn't want to bundle another service into our stack, but this is controversial and we will be running benchmarks to make a better informed decision here. Note that pgvector will serve a lot of use cases well all the way up to hundreds of thousands of documents, though.&lt;/p&gt;
    &lt;p&gt;Vector embeddings: Users can configure this in Skald and we use Sentence Transformers (all-MiniLM-L6-v2) as our default (solid all-around performer for speed and retrieval, English-only). I also ran Skald with bge-m3 (larger, multi-language) and share the results later in this post.&lt;/p&gt;
    &lt;p&gt;LLM: We don't even bundle a default with Skald and it's up to the users to run and manage this. I tested our setup with GPT-OSS 20B on EC2 (results shown below).&lt;/p&gt;
    &lt;p&gt;Reranker: Users can also configure this in Skald, and the default is the Sentence Transformers cross encoder (solid, English-only). I've also used bge-reranker-v2-m3 and mmarco-mMiniLMv2-L12-H384-v1 which offer multi-lingual support.&lt;/p&gt;
    &lt;p&gt;Document parsing: There isn't much of a question on this one. We're using Docling. It's great. We run it via docling-serve.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does it perform though?&lt;/head&gt;
    &lt;p&gt;So the main goal here was first to get something working then ensure it worked well with our platform and could be easily deployed. From here we'll be running extensive benchmarks and working with our clients to provide a solid setup that both performs well but is also not a nightmare to deploy and manage.&lt;/p&gt;
    &lt;p&gt;From that perspective, this was a great success.&lt;/p&gt;
    &lt;p&gt;Deploying a production instance of Skald with this whole stack took me 8 minutes, and that comes bundled with the vector database (well, Postgres), a reranking and embedding service, and Docling.&lt;/p&gt;
    &lt;p&gt;The only thing I needed to run separately was the LLM, which I did via llama.cpp.&lt;/p&gt;
    &lt;p&gt;Having gotten this sorted, I imported all the content from the PostHog website [1] and set up a tiny dataset [2] of questions and expected answers inside of Skald, then used our Experiments feature to run the RAG over this dataset.&lt;/p&gt;
    &lt;p&gt;I explicitly kept the topK values really high (100 for the vector search and 50 for post-reranking), as I was mostly testing for accuracy and wanted to see the performance when questions required e.g. aggregating context over 15+ documents.&lt;/p&gt;
    &lt;head&gt;Full config&lt;/head&gt;
    &lt;p&gt;Here are the params configured in the Skald UI for the the experiment.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Config option&lt;/cell&gt;
        &lt;cell role="head"&gt;Selection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extra system prompt&lt;/cell&gt;
        &lt;cell&gt;Be really concise in your answers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Query rewriting&lt;/cell&gt;
        &lt;cell&gt;Off&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vector search topK&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vector search distance threshold&lt;/cell&gt;
        &lt;cell&gt;0.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reranking&lt;/cell&gt;
        &lt;cell&gt;On&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reranking topK&lt;/cell&gt;
        &lt;cell&gt;50&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;References&lt;/cell&gt;
        &lt;cell&gt;Off&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So without any more delay, here are the results of my not-very-scientific at all benchmark using the experimentation platform inside of Skald.&lt;/p&gt;
    &lt;head rend="h3"&gt;Voyage + Claude&lt;/head&gt;
    &lt;p&gt;This is our default Cloud setup. We use voyage-3-large and rerank-2.5 from Voyage AI as our embedding and reranking models respectively, and we default to Claude Sonnet 3.7 for responses (users can configure the model though).&lt;/p&gt;
    &lt;p&gt;It passed with flying colors.&lt;/p&gt;
    &lt;p&gt;Our LLM-as-a-Judge gave an average score of 9.45 to the responses, and I basically agree with the assessment. All answers were correct, with one missing a few extra bits of context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Voyage + GPT-OSS 20B&lt;/head&gt;
    &lt;p&gt;With the control experiment done, I then moved on to a setup where I kept Voyage as the embeddings provider and reranker, and then used GPT-OSS 20B running on a llama.cpp server on a g5.2xlarge EC2 instance as the LLM.&lt;/p&gt;
    &lt;p&gt;The goal here was to see how well the open-source LLM model itself stacked up against a frontier model accessed via API.&lt;/p&gt;
    &lt;p&gt;And it did great!&lt;/p&gt;
    &lt;p&gt;We don't yet support LLM-as-a-Judge on fully local deployments, so the only score we have here is mine. I scored the answers an average of 9.18 and they were all correct, with two of them just missing a few bits of information or highlighting less relevant information from the context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fully local + GPT-OSS 20B&lt;/head&gt;
    &lt;p&gt;Lastly, it was time for the moment of truth: running a fully local setup.&lt;/p&gt;
    &lt;p&gt;For this I ran two tests:&lt;/p&gt;
    &lt;p&gt;1. Default sentence transformers embedding and reranking models&lt;/p&gt;
    &lt;p&gt;The most popular open-source models are all-MiniLM-L6-v2 for embeddings and ms-marco-MiniLM-L6-v2 as the reranker, so I used those for my first benchmark.&lt;/p&gt;
    &lt;p&gt;Here the average score was 7.10. Not bad, but definitely not great. However, when we dig into the results, we can get a better understanding of how this setup fails.&lt;/p&gt;
    &lt;p&gt;Basically, it got all point queries right, which are questions where the answer is somewhere in the mess of documents, but can be found from one specific place.&lt;/p&gt;
    &lt;p&gt;Where it failed was:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-english query: The embeddings model and the reranker are English-based, so my question in Portuguese obviously got no answer&lt;/item&gt;
      &lt;item&gt;An ambiguous question with very little context ("what's ch")&lt;/item&gt;
      &lt;item&gt;Aggregating information from multiple documents/chunks e.g. it only found 5 out of PostHog's 7 funding rounds, and only a subset of the PostHog competitors that offer session replay (as mentioned in the source data)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In my view, this is good news. That means that the default options will go a long way and should give you very good performance if your use case is only doing point queries in English. The other great thing is that these models are also fast.&lt;/p&gt;
    &lt;p&gt;Now, if you need to handle ambiguity better, or handle questions in other languages, then this setup is simply not for you.&lt;/p&gt;
    &lt;p&gt;2. Multi-lingual models&lt;/p&gt;
    &lt;p&gt;The next test I did used bge-m3 as the embeddings model and mmarco-mMiniLMv2-L12-H384-v1 as the reranker. The embeddings model is supposedly much better than the one used in the previous test and is also multi-lingual. The reranker on the other hand uses the same cross-encoder from the previous test as the base model but also adds multi-lingual support. The more standard option here would have been the much more popular bge-reranker-v2-m3 model, but I found it to be much slower. I intend to tweak my setup and test it again, however.&lt;/p&gt;
    &lt;p&gt;Anyway, onto the results! I scored it 8.63 on average, which is very good. There were no complete failures, and it handled the question in Portuguese well.&lt;/p&gt;
    &lt;p&gt;The mistakes it made were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This new setup also did not do the best job at aggregating information, missing 2 of PostHog's funding rounds, and a couple of its session replay competitors&lt;/item&gt;
      &lt;item&gt;It also answered a question correctly, but added incorrect additional context after it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So overall it performed quite well. Again what we what saw was the main problem is when the context needed for the response is scattered across multiple documents. There are various techniques to help with this and we'll be trialing some soon! They haven't been needed on the Cloud version because better models save you from having to add complexity for minimal performance gains, but as we're focused on building a really solid setup for local deploys, we'll be looking into this more and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Now what?&lt;/head&gt;
    &lt;p&gt;I hope this writeup has provided you with at least some insight and context into building a local RAG, and also the fact that it does work, it can serve a lot of use cases, and that the tendency is for this setup to get better and better as a) models improve b) we get more open-source models across the board, with both being things that we seem to be trending towards.&lt;/p&gt;
    &lt;p&gt;As for us at Skald, we intend to polish this setup further in order to serve even more use cases really well, as well as intend to soon be publishing more legitimate benchmarks for models in the open-source space, from LLMs to rerankers.&lt;/p&gt;
    &lt;p&gt;If you're a company that needs to run AI tooling in air-gapped infrastructure, let's chat -- feel free to email me at yakko [at] useskald [dot] com.&lt;/p&gt;
    &lt;p&gt;Lastly, if you want to get involved, feel free to chat to us over on our GitHub repo (MIT-licensed) or catch us on Slack.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;[1] I used the PostHog website here because the website content is MIT-licensed (yes, wild) and readily-available as markdown on GitHub and having worked there I know a lot of answers off the top of my head making it a great dataset of ~2000 documents that I know well.&lt;/p&gt;
    &lt;p&gt;[2] The questions and answers dataset I used for the experiments was the following:&lt;/p&gt;
    &lt;head&gt;Dataset&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Question&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected answer&lt;/cell&gt;
        &lt;cell role="head"&gt;Comments&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;How many raises did PostHog do?&lt;/cell&gt;
        &lt;cell&gt;PostHog has raised money 7 times: it raised $150k from YCombinator, then did a seed round ($3.025M), a series A ($12M), a series B ($15M), a series C ($10M), a series D ($70M), and a series E ($75M).&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating context from at least 7 documents&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;When did group analytics launch?&lt;/cell&gt;
        &lt;cell&gt;December 16, 2021.&lt;/cell&gt;
        &lt;cell&gt;Point query, multiple mentions to "group analytics" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Why was the sessions page removed?&lt;/cell&gt;
        &lt;cell&gt;The sessions page was removed because it was confusing and limited in functionality. It was replaced by the 'Recordings' tab.&lt;/cell&gt;
        &lt;cell&gt;Point query, multiple mentions to "sessions" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;What's the difference between a product engineer and other roles?&lt;/cell&gt;
        &lt;cell&gt;Compared to product managers, product engineers focus more on building rather than deep research and planning. When it comes to software engineers, both product and software engineers write code, but software engineers focus on building great software, whereas product engineers focus on building great products.&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating context from multiple docs + there are a ton of mentions of "product engineer" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;What were the main benefits of YC?&lt;/cell&gt;
        &lt;cell&gt;The main benefits of YC were: Network Access, Investor Reviews, Office Hours, Funding Opportunities, Hiring Resources, Angel Investing Opportunities, Accelerated Growth and Experience, Shift in Self-Perception, Customer Acquisition, Product Market Fit, Ambitious Goal Setting, Access to Thought Leaders, Community Support&lt;/cell&gt;
        &lt;cell&gt;Point query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;quem foi o primeiro investidor da posthogg?&lt;/cell&gt;
        &lt;cell&gt;O primeiro investidor da PostHog foi o YCombinator.&lt;/cell&gt;
        &lt;cell&gt;Question in Portuguese, with PostHog misspelled&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what posthog competitors also offer session replays&lt;/cell&gt;
        &lt;cell&gt;LogRocket, Smartlook, FullStory, Microsoft Clarity, Contentsquare, Mouseflow, Heap, Pendo, Hotjar, Glassbox, and Amplitude.&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating content from at least 11 docs (more because I actually missed some in my expected answer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;top tips find client&lt;/cell&gt;
        &lt;cell&gt;1. Leverage your inner circle 2. Join relevant communities 3. Be laser-focused 4. Set achievable goals 5. Frame conversations properly&lt;/cell&gt;
        &lt;cell&gt;Point query, worded weirdly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what's ch&lt;/cell&gt;
        &lt;cell&gt;CH most likely refers to ClickHouse, a column-oriented OLAP database.&lt;/cell&gt;
        &lt;cell&gt;Really ambiguous. I meant ClickHouse with my question.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what is mixedpanel&lt;/cell&gt;
        &lt;cell&gt;Mixpanel is a popular product analytics tool that was founded in 2009&lt;/cell&gt;
        &lt;cell&gt;Mixpanel misspelled as Mixedpanel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;how was prpoerty filtering made faster?&lt;/cell&gt;
        &lt;cell&gt;Using materialized columns allowed ClickHouse to skip JSON parsing during queries and made queries with property filtering 25x faster.&lt;/cell&gt;
        &lt;cell&gt;Point query with a typo&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080364</guid><pubDate>Fri, 28 Nov 2025 16:54:56 +0000</pubDate></item><item><title>Bringing Sexy Back. Internet surveillance has killed eroticism</title><link>https://lux-magazine.com/article/privacy-eroticism/</link><description>&lt;doc fingerprint="3cc0d15432a41fc2"&gt;
  &lt;main&gt;
    &lt;p&gt;I don’t remember when I first started noticing that people I knew out in the world had lost their sense of erotic privacy, but I do remember the day it struck me as a phenomenon that had escaped my timeline and entered my real, fleshy life. It was last year, when I was having a conversation with a friend of mine, who, for the record, is five years younger than me (I’m 31). I told my friend about an erotic encounter I’d just experienced and very much delighted in, in which I had my hair brushed at the same time by two very beautiful women at the hair salon — one was teaching the other how to do it a certain way. When I finished my story, my friend looked at me, horrified.&lt;/p&gt;
    &lt;p&gt;“They had no idea you felt something sexual about them,” she said. “What if they found out? Lowkey, I hate to say this but: you took advantage of them.” I was shocked. I tried to explain — and it felt extremely absurd to explain — that this had happened in my body and in my thoughts, which were private to me and which nobody had the right to know about. But they did have the right, my friend argued. She demanded that I apologize to the women for sexualizing them. Offended at having been accused — in my view, in extremely bad faith — of being some kind of peep-show creep, I tried to argue that I’d simply responded in a physical way to an unexpected, direct, and involuntary stimulus. Back and forth, back and forth, we fought like this for a while. In fact, it ended the friendship.&lt;/p&gt;
    &lt;p&gt;There were other conversations, too, that suggested to me that conceptions of love and sex have changed fundamentally among people I know. Too many of my friends and acquaintances — of varying degrees of “onlineness,” from veteran discourse observers to casual browsers — seem to have internalized the internet’s tendency to reach for the least charitable interpretation of every glancing thought and, as a result, to have pathologized what I would characterize as the normal, internal vagaries of desire.&lt;/p&gt;
    &lt;p&gt;Hence, there was the friend who justified her predilection for being praised in bed as a “kink” inherited through the “trauma” of her father always harping on her because of her grades. There was the friend who felt entitled to posting screenshots of intimate conversations on Twitter after a messy breakup so that she could get a ruling on “who was the crazy one.” Then there was the friend who bitterly described a man he was dating as a “fuckboy” because he stood him up, claiming that their having enjoyed sex together beforehand was “emotionally manipulative.” When I dug a bit deeper, it turned out the man in question had just gotten out of a seven-year relationship and realized he wasn’t ready to be sexually intimate, and while he was rude to stand my friend up, it shocked me how quick my friend was to categorize his rightfully hurt feelings as something pathological or sinister in the other person, and that he did this in order to preemptively shield himself from being cast as the villain in what was a multi-party experience. This last friend I asked: “Who are you defending yourself against?” To which he answered, to my astonishment: “I don’t know. The world.”&lt;/p&gt;
    &lt;p&gt;I choose these examples from my personal life because they express sentiments that were once the kind of stuff I encountered only in the messy battlegrounds of Twitter, amid discussions about whether Sabrina Carpenter is being oversexualized, whether kinks are akin to a sexual orientation, whether a woman can truly consent in an age-gap relationship, and whether exposure to sex scenes in movies violates viewer consent. It is quite easy to dismiss these “discourse wars” as a “puritanism” afflicting the young, a reactionary current to be solved with a different, corrective discourse of pro-sex liberation, distributed via those same channels. If only it were so! To me, the reality goes deeper and is bleaker.&lt;/p&gt;
    &lt;p&gt;The fact is that our most intimate interactions with others are now governed by the expectation of surveillance and punishment from an online public. One can never be sure that this public or someone who could potentially expose us to it isn’t there, always secretly filming, posting, taking notes, ready to pounce the second one does something cringe or problematic (as defined by whom?). To claim that these matters are merely discursive in nature is to ignore the problem. Because love and sex are so intimate and vulnerable, the stakes of punishment are higher, and the fear of it penetrates deeper into the psyche and is harder to rationalize away than, say, fear of pushback from tweeting a divisive political opinion.&lt;/p&gt;
    &lt;p&gt;I should state at this point that this is not an essay about “cancel culture going too far,” a topic which can now be historicized as little more than a rhetorical cudgel wielded successfully by the right to wrest cultural power back from an ascendant progressive liberalism. This was especially true after the prominence of organized campaigns such as #MeToo. #MeToo was smeared by liberals and conservatives alike (united, as they always are, in misogyny) as being inherently punitive in nature, meant to punish men who’d fallen into a rough patch of bad behavior, or who, perhaps, might not have done anything at all (the falsely accused or the misinterpreted man became the real victim, in this view). #MeToo did make use of the call-out — the story shared in a spreadsheet anonymously or in a signed op-ed — but the call-outs had a purpose: to end a long-standing and long-permitted norm of sexual abuse within institutions. Underlying this was a discursive practice and a form of solidarity building in which people believed that sharing their stories of trauma en masse could bring about structural change. As someone who participated myself, I too believed in this theory and saw it as necessary, cathartic, and political, and far from vigilante justice.&lt;/p&gt;
    &lt;p&gt;But the pushback against #MeToo reveals a certain peril to storytelling as politics, not only in the retraumatization evident in the practice of revealing one’s most intimate harms before an infinite online audience, which could always include those listening in bad faith. But also, a discursive market opened up in which trauma became a kind of currency of authenticity, resulting in a doubled exploitation. This idea, while not very nice, lingers in the use of harm as an authoritative form of rhetorical defense. The problem here is not what is said, but how it is used. A friction has since emerged between an awareness of weaponization of harm and emotion and the continued need to express oneself as vulnerably as possible in order to come off as sincere. This friction is unresolved.&lt;/p&gt;
    &lt;p&gt;The organized goals of the #MeToo movement are missing from the new puritanism. I think that the prudish revulsion I’ve seen online and in my own life has as much to do with surveillance as with sex. Punishing strangers for their perceived perversion is a form of compensation for a process that is already completed: the erosion of erotic and emotional privacy through internet-driven surveillance practices, practices we have since turned inward on ourselves. In short, we have become our own panopticons.&lt;/p&gt;
    &lt;p&gt;On the rightmost side of the spectrum, punitive anti-erotic surveillance is very explicit and very real, especially for women. The Andrew Tates of the world and the practitioners of extreme forms of misogyny have no problem with using internet tools and social media websites for mass shaming and explicit harm. Covert filming of sex acts, AI deep fakes, extortion, and revenge porn are all realities one has to contend with when thinking about hooking up or going to public places such as nightclubs and gay bars. This is blackmail at its most explicit and extreme, meant to further solidify a link between sex and fear.&lt;/p&gt;
    &lt;p&gt;But that link between sex and fear is operating in more “benign” or common modes of internet practice. There is an online culture that thinks nothing of submitting screenshots, notes, videos, and photos with calls for collective judgement. When it became desirable and permissible to transform our own lives into content, it didn’t take long before a sense of entitlement emerged that extended that transformation to people we know and to strangers. My ex sent me this text, clearly she is the crazy one, right? Look at this dumb/funny/cringe Hinge profile! Look at this note some guy sent me, is this a red flag? Look at this random woman I photographed buying wine, coconut oil, and a long cucumber at the supermarket!&lt;/p&gt;
    &lt;p&gt;I think these kinds of posts sometimes amount to little more than common bullying, but they are on a continuum with a puritan discourse in which intimate questions, practices, and beliefs about queerness, sexuality, gender presentation, and desire are also subjected to days-long piles-on. In both instances, the instinct to submit online strangers to viral discipline is given a faux-radical sheen. It’s a kind of casual blackmail that warns everyone to conform or be exposed; a way of saying if you don’t cave to my point of view, redefine yourself in my image of what sexuality is or should be, and (most importantly) apologize to me and the public, I will subject you to my large following and there will be hell to pay. Such unproductive and antisocial behavior is justified as a step toward liberation from predation, misogyny, or any number of other harms. But the punitive mindset we’ve developed towards relationships is indicative of an inability to imagine a future of gendered or sexual relations without subjugation. To couch that in the language of harm reduction and trauma delegitimizes both.&lt;/p&gt;
    &lt;p&gt;There are other ways the politics of surveillance have become a kind of funhouse mirror. It is seen as more and more normal to track one’s partner through Find My iPhone or an AirTag, even though the potential for abuse of this technology is staggering and obvious. There are all kinds of new products, such as a biometric ring that is allegedly able to tell you whether your partner is cheating, that expand this capability into more and more granular settings. That’s all before we get into the endless TikToks about “why I go through my partner’s text messages.” That men use these tactics and tools to control women is a known threat. What is astonishing is the lengths to which some women will go to use these same technologies, claiming that they are necessary to prevent harm — especially that caused by cheating, which is now seen as some kind of lifelong trauma or permanently damnable offense instead of one of the rather quotidian, if very painful, ways we hurt one another. Each of these surveillance practices operates from a feeling of entitlement and control over other people, their bodies, and what they do.&lt;/p&gt;
    &lt;p&gt;Pundits like to decree sexlessness as a Gen-Z problem, to argue no one is fucking because they are too on their phones. However, it is always too easy to blame the young. It was my generation that failed to instill the social norms necessary to prevent a situation where fear of strangers on the internet has successfully replaced the disciplinary apparatus more commonly held by religious or conservative doctrine. Even when, as in my experience in the salon, I am acting in the privacy of my own body, someone is always there watching, ready to interpret my actions, problematize them so as to share in the same sense of magical thinking, the same insecurities, and to be punished for not being insecure in the same way.&lt;/p&gt;
    &lt;p&gt;It’s only in retrospect that I’m able to realize the toll that constant, nagging interaction with my devices and the internet has taken on my thinking life and my sex life. I remember very viscerally when I’d just come out of the closet as bisexual in 2016. When I embarked on a journey to find the kind of lover I wanted to be, my only experience with the world of queerness was online through memes, articles, and others’ social media presentation of themselves and of politics. Queer sex was not something that could be discovered through sensation, through physical interaction, but was rather a catalog of specific acts and roles one was already expected to know. I was terrified of making some kind of mistake, of being the wrong kind of bisexual, of misrepresenting myself in an offensive way (could I use the term “soft butch” if I wasn’t a lesbian?), of being exposed somehow as a fraud. When the time came for me to have sex for the first time, what should have been a joyous occasion was instead burdened with a sense of being watched. I could not let the natural processes of erotic discovery take their course, so caught up was I in judging myself from the perspective of strangers to whom I owed nothing.&lt;/p&gt;
    &lt;p&gt;But it wasn’t just a matter of queerness, either. When I hooked up with men, I could only perceive of sex the same way, not as situational but as a set of prescribed acts and scenes, many of which I wanted to explore. However, this time I interrogated these urges as being sociogenic in nature and somehow harmful to me, when they were, in fact, private, and I did not, in reality, feel harmed. Because I wanted, at one point in my life, to be tied up and gagged, the disempowering nature of such a want necessitated trying to justify it against invisible accusations with some kind of traumatogenic and immutable quality. Maybe it was because I was raped in college. Maybe I was just inherently submissive. One of the great ironies in the history of sex is that pathologization used to be a way of controlling sexual desire. (All are familiar with the many myths that masturbation would turn one blind.) Now it is a way of exempting oneself, of relinquishing control of one’s actions so as to absolve them of scrutiny. My little bondage moment couldn’t be problematic if it couldn’t be helped. It couldn’t be subjected to interrogation if there was something I could point to to say “it’s beyond my control, don’t judge me!” One day, however, I came to an important revelation: The reality was much simpler. It was a passing phase, a desire that originated with a specific man and lost its charm after I moved on from him. There wasn’t some deterministic quality in myself that made me like this. My desire was not fixed in nature. My sexual qualities were transient and not inborn. What aroused me was wonderfully, entirely situational.&lt;/p&gt;
    &lt;p&gt;A situational eroticism is what is needed now, in our literalist times. It’s exhausting, how everything is so readily defined by types, acts, traumas, kinks, fetishes, pathology, and aesthetics. To me, our predilection for determinism is an expected psychological response to excessive surveillance. A situational eroticism decouples sensation from narrative and typology. It allows us to feel without excuse and to relate our feelings to our immediate embodied moment, grounded in a fundamental sense of personal privacy. While it is admirable to try and understand ourselves and important to protect ourselves from harm and investigate critically the ways in which what we want may put us at risk of that harm — or at risk of doing harm to others — sometimes desires just are, and they are not that way for long. Arousal is a matter of the self, which takes place within the body, a space no one can see into. It is often a mystery, a surprise, a discovery. It can happen at a small scale, say, the frisson of two sets of fingers in one’s hair at once. It is beautiful, unplanned and does not judge itself because it is an inert sensation, unimbued with premeditated meaning. This should liberate rather than frighten us. Maybe what it means doesn’t matter. Maybe we don’t have to justify it even to ourselves.&lt;/p&gt;
    &lt;p&gt;But in order to facilitate a return to situational eroticism, we need to kill the panopticon in our heads. That means first killing the panopticon we’ve built for others. There is no purpose in vindictive or thoughtless exposure. Not everything needs to be subjected to public opinion, not every anecdote is worth sharing, not every debate needs engagement, especially those debates which have no material basis to them, no ask, no funnel for all that energy. We need to stop confusing vigilantism with justice and posting with politics. That does not mean we stop the work that #MeToo started, but that revenge is a weapon best utilized collectively against the enemies of liberation. We need to protect the vulnerable from exploitative technologies and practices, repeatedly denounce their use, and work towards a world without sexual coercion, digital or otherwise.&lt;/p&gt;
    &lt;p&gt;On an individual level, we need to abandon or reshape our relationships with our phones and regain a sense of our own personal and mental privacy. It’s a matter of existential, metaphysical importance. Only when this decoupling from ourselves and the mediated performance of ourselves is complete, can we begin the process of returning to our own bodies out there, in the world, with no one watching or reading our thoughts except those we want to. The truth is, we are very afraid not of sex, but of exposure. Only when we are unafraid can we begin to let desire flourish. Only when we return to ourselves can we really know what we want.&lt;/p&gt;
    &lt;p&gt;Kate Wagner is the architecture critic at The Nation. Her award-winning cultural writing has been featured in magazines ranging from The Baffler to the New Republic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080473</guid><pubDate>Fri, 28 Nov 2025 17:05:31 +0000</pubDate></item><item><title>C++ Web Server on my custom hobby OS</title><link>https://oshub.org/projects/retros-32/posts/getting-a-webserver-running</link><description>&lt;doc fingerprint="185cec506d2433dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Getting a Web Server running on my hobby OS&lt;/head&gt;
    &lt;p&gt;November 27, 2025joexbayerkernel, networking, tcp, http, web&lt;/p&gt;
    &lt;head rend="h1"&gt;Demo&lt;/head&gt;
    &lt;p&gt;After a long break from working on my hobby operating system, I finally got back into it and finished a very important milestone: a working web server.&lt;/p&gt;
    &lt;p&gt;Full Github repo: https://github.com/joexbayer/RetrOS-32&lt;/p&gt;
    &lt;p&gt;Networking was always integral to my hobby project. The first goal was getting the basic networking stack working: Ethernet, IP, ARP, UDP, TCP, DHCP and DNS. Besides TCP this was rather straightforward, but when moving onto HTTP things broke.&lt;/p&gt;
    &lt;p&gt;Networking stack code&lt;/p&gt;
    &lt;p&gt;This led to my first break from the project, but also left a nagging thought in my mind, wanting to make it work. I finally sat down and started debugging.&lt;/p&gt;
    &lt;p&gt;I eventually found the culprit after hours of dissecting my own code, the problem was a broken implementation of the terminal buffer, overwriting a lock in another process… fun. Additionally, the E1000 network driver did not correctly handle incoming packets, which I finally got working by handling bursts of packets.&lt;/p&gt;
    &lt;head rend="h1"&gt;Performance and hardening&lt;/head&gt;
    &lt;p&gt;After getting an HTML page returned from the web engine I started noticing lots of performance errors and hangs from TCP, mainly because quickly refreshing the browser led to a spam of RST packets which were not handled correctly.&lt;/p&gt;
    &lt;p&gt;After a few hours of tinkering I finally got the RST packets working and the network stack is now able to handle a packet spam from the browser.&lt;/p&gt;
    &lt;head rend="h1"&gt;The HTTP Engine&lt;/head&gt;
    &lt;p&gt;Next step was actually implementing a HTTP engine, parsing the requests from the user. Before this engine I simply returned a static HTTP response no matter the actual request.&lt;lb/&gt;Keeping with the spirit of this hobby OS I want to write everything from scratch, luckily I already had implemented a pretty complete HTTP parser for my other project c-web-modules. So I extracted the HTTP parser as a standalone library and ported it to my OS.&lt;/p&gt;
    &lt;p&gt;Keeping with the spirit of this hobby OS I want to write everything from scratch, luckily I already had implemented a pretty complete HTTP parser for my other project c-web-modules. So I extracted the HTTP parser as a standalone library and ported it to my OS.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Web Engine.&lt;/head&gt;
    &lt;p&gt;After the HTTP engine was done I moved onto the web engine, focusing on something small, rather than big and fancy. Mainly routing was important and adding route handlers. Allowing the user to specify a route, method and lambda function handler.&lt;/p&gt;
    &lt;quote&gt;/* Simple routing */ engine.get("/", [](const http::Request&amp;amp; req, http::Response&amp;amp; res) { (void)req; res.setBody("hello world!"); .... });&lt;/quote&gt;
    &lt;p&gt;It’s a tiny example, but it mirrors how a lot of modern C++ and web frameworks think about routing: match a path + method, call a handler, build a response.&lt;/p&gt;
    &lt;quote&gt;#lang:plaintext [ Browser ] | v [ Web Server (userspace): WebEngine | HTTPEngine | FileRepository ] | v [ Network stack: TCP/UDP | IP | ARP | DHCP | DNS | Ethernet(E1000) ]&lt;/quote&gt;
    &lt;head rend="h1"&gt;The Web Server.&lt;/head&gt;
    &lt;p&gt;The last step was updating the userspace program with the new HTTP and Web engine. Finally I added a way to serve files using a FileRepository which supports caching. Now I can edit the files inside the operating system and then serve them with the web server.&lt;/p&gt;
    &lt;quote&gt;#lang:cpp WebEngine webEngine(80, 16); web::FileRepository fileRepo; /* Simple static pages */ webEngine.get("/home", [&amp;amp;fileRepo](const http::Request&amp;amp; req, http::Response&amp;amp; res) { (void)req; res.sendFile(fileRepo, "/web/index.htm"); }); webEngine.get("/about", [&amp;amp;fileRepo](const http::Request&amp;amp; req, http::Response&amp;amp; res) { (void)req; res.sendFile(fileRepo, "/web/about.htm"); }); webEngine.get("/status", [&amp;amp;fileRepo](const http::Request&amp;amp; req, http::Response&amp;amp; res) { (void)req; res.sendFile(fileRepo, "/web/status.htm"); }); webEngine.run();&lt;/quote&gt;
    &lt;head rend="h1"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;The next thing on the TODO list will be to add a more fancy UI for the webserver and a way to close it gracefully.&lt;/p&gt;
    &lt;p&gt;(Graceful shutdown is one of those “boring” features you don’t miss… until the first time you corrupt something on exit.)&lt;/p&gt;
    &lt;p&gt;When this is finished, the biggest task so far will begin… the web browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments&lt;/head&gt;
    &lt;p&gt;4.06k views&lt;/p&gt;
    &lt;p&gt;Anonymousabout 4 hours ago&lt;/p&gt;
    &lt;p&gt;Anonymousabout 5 hours ago&lt;/p&gt;
    &lt;p&gt;I'm a hacker hahahahahahaah&lt;/p&gt;
    &lt;p&gt;This dude clearly is a hacker, I don't know what you're on about.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080579</guid><pubDate>Fri, 28 Nov 2025 17:15:20 +0000</pubDate></item><item><title>Show HN: An LLM-Powered Tool to Catch PCB Schematic Mistakes</title><link>https://netlist.io/</link><description>&lt;doc fingerprint="91353e48075d4f2b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Catch PCB schematic mistakes before fabrication&lt;/head&gt;
    &lt;p&gt;AI-driven electrical design checks, powered by your datasheets and netlist from KiCad or Altium. Try it now, no credit card required.&lt;/p&gt;
    &lt;p&gt;Loading sample chat...&lt;/p&gt;
    &lt;p&gt;Ok cool. Thanks!&lt;/p&gt;
    &lt;p&gt;Effort&lt;/p&gt;
    &lt;p&gt;11.5%34.5k / 300k Per Chat&lt;/p&gt;
    &lt;p&gt;AI can make mistakes, check important info. Layout suggestions are based on logic-only; layout files aren't visible to Netlist.io.&lt;/p&gt;
    &lt;p&gt;Loading sample chat...&lt;/p&gt;
    &lt;p&gt;Ok cool. Thanks!&lt;/p&gt;
    &lt;p&gt;Effort&lt;/p&gt;
    &lt;p&gt;11.5%34.5k / 300k Per Chat&lt;/p&gt;
    &lt;p&gt;AI can make mistakes, check important info. Layout suggestions are based on logic-only; layout files aren't visible to Netlist.io.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080737</guid><pubDate>Fri, 28 Nov 2025 17:30:41 +0000</pubDate></item><item><title>JSON Schema Demystified: Dialects, Vocabularies and Metaschemas</title><link>https://www.iankduncan.com/engineering/2025-11-24-json-schema-demystified/</link><description>&lt;doc fingerprint="225d6903a4e42be0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;JSON Schema Demystified: Understanding Schemas, Dialects, Vocabularies, and Metaschemas&lt;/head&gt;
    &lt;p&gt;If you’ve ever tried to dive into JSON Schema, you’ve probably encountered a wall of terminology that makes your head spin: schemas, metaschemas, dialects, vocabularies, keywords, anchors, dynamic references. It feels like the community invented new words for things that already had perfectly good names, just to make the rest of us feel inadequate.&lt;/p&gt;
    &lt;p&gt;I’ve been working on a Haskell JSON Schema library that’s actually fully spec-compliant, which meant I had to figure all of this out. The problem isn’t that the concepts are inherently difficult. The terminology creates artificial barriers to understanding.&lt;/p&gt;
    &lt;p&gt;This post will break down the key concepts in JSON Schema in a way that actually makes sense, connecting the dots between all these terms that seem designed to confuse. By the end, you’ll understand not just what these words mean, but how they fit together into a coherent system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Starting simple&lt;/head&gt;
    &lt;p&gt;Before we dive into terminology, let’s look at what we’re actually trying to accomplish. JSON Schema is fundamentally about describing the shape and constraints of JSON data. Here’s a simple example:&lt;/p&gt;
    &lt;code&gt;{
  "type": "object",
  "properties": {
    "name": { "type": "string" },
    "age": { "type": "number", "minimum": 0 }
  },
  "required": ["name"]
}&lt;/code&gt;
    &lt;p&gt;This schema says: “I expect a JSON object with a string &lt;code&gt;name&lt;/code&gt; field (required) and an optional numeric &lt;code&gt;age&lt;/code&gt; field that must be non-negative.” Simple enough, right?&lt;/p&gt;
    &lt;p&gt;Now here’s where it gets interesting: this schema is itself valid JSON. And since JSON can describe the structure of JSON documents, we can describe the structure of schemas using more schemas. This recursive property is what gives rise to metaschemas, and where the terminology starts to get confusing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s a schema anyway?&lt;/head&gt;
    &lt;p&gt;A schema is just a JSON document that describes constraints on other JSON documents. That’s it. The example above is a schema.&lt;/p&gt;
    &lt;p&gt;Schemas tell you what type a value should be (string, number, object, array), what values are allowed or disallowed, what properties must or may exist on an object, how many items should be in an array. When you write a schema, you’re essentially writing rules that say “valid JSON documents that I care about look like this.”&lt;/p&gt;
    &lt;code&gt;{
  "type": "string",
  "minLength": 1,
  "maxLength": 100
}&lt;/code&gt;
    &lt;p&gt;This schema says: “I want a string between 1 and 100 characters long.” Any JSON validator that understands JSON Schema can take this schema and your data and tell you whether your data follows the rules.&lt;/p&gt;
    &lt;p&gt;The confusing part is that schemas themselves are JSON documents. So naturally, you might ask: “What describes the structure of a schema?” And that leads us to the next layer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Metaschemas: schemas all the way down&lt;/head&gt;
    &lt;p&gt;A metaschema is a schema that describes the structure of other schemas. The “schema of schemas,” if you will.&lt;/p&gt;
    &lt;p&gt;This sounds abstract and philosophical, but it’s actually quite practical. Remember how our simple schema used keywords like &lt;code&gt;"type"&lt;/code&gt;, &lt;code&gt;"properties"&lt;/code&gt;, and &lt;code&gt;"minimum"&lt;/code&gt;? The metaschema defines what those keywords mean, what values they can have, and how they work together.&lt;/p&gt;
    &lt;p&gt;Here’s a tiny excerpt of what a metaschema might look like:&lt;/p&gt;
    &lt;code&gt;{
  "$id": "https://json-schema.org/draft/2020-12/schema",
  "type": ["object", "boolean"],
  "properties": {
    "type": {
      "anyOf": [
        { "enum": ["null", "boolean", "object", "array", "number", "string"] },
        { "type": "array", "items": { "$ref": "#/properties/type/anyOf/0" } }
      ]
    },
    "properties": {
      "type": "object",
      "additionalProperties": { "$dynamicRef": "#meta" }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;This fragment says things like: “The &lt;code&gt;type&lt;/code&gt; keyword can be a single type string or an array of type strings” and “The &lt;code&gt;properties&lt;/code&gt; keyword should be an object where each value is itself a schema.”&lt;/p&gt;
    &lt;p&gt;Why does this matter? Well, you can validate that your schema is well-formed by checking it against the metaschema. If someone writes &lt;code&gt;"type": "stirng"&lt;/code&gt; (typo!), the metaschema validation will catch it. The metaschema is also the formal specification of what’s allowed in schemas. Tools that process schemas (validators, code generators, documentation generators) use the metaschema to understand what they’re working with.&lt;/p&gt;
    &lt;p&gt;The relationship is simple: schemas validate data, metaschemas validate schemas.&lt;/p&gt;
    &lt;code&gt;Data → validated by → Schema → validated by → Metaschema&lt;/code&gt;
    &lt;p&gt;Here’s where it gets recursive: since a metaschema is also a schema (JSON describing JSON structure), it can validate itself. The JSON Schema metaschema is designed to be self-describing. This is similar to how a compiler written in its own language can compile itself (bootstrapping).&lt;/p&gt;
    &lt;head rend="h2"&gt;Dialects: when versions matter&lt;/head&gt;
    &lt;p&gt;So we have schemas and metaschemas. But JSON Schema has evolved over time. Different versions have added new keywords, changed behavior, and deprecated old features. How do we keep track of which version of JSON Schema we’re using?&lt;/p&gt;
    &lt;p&gt;A dialect is a specific version or flavor of JSON Schema, defined by a particular metaschema. When someone says they’re using “Draft 2020-12” or “Draft 7,” they’re referring to specific dialects.&lt;/p&gt;
    &lt;p&gt;Each dialect has its own metaschema that defines which keywords are available, its own set of behaviors and validation rules, and is identified by a URI (usually something like &lt;code&gt;https://json-schema.org/draft/2020-12/schema&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;You declare which dialect your schema uses with the &lt;code&gt;$schema&lt;/code&gt; keyword:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": {
    "name": { "type": "string" }
  }
}&lt;/code&gt;
    &lt;p&gt;This tells validators: “Hey, interpret this schema according to the Draft 2020-12 rules.”&lt;/p&gt;
    &lt;p&gt;Different dialects can have different keywords and different behaviors. Draft 4 didn’t have the &lt;code&gt;const&lt;/code&gt; keyword, but Draft 6 added it. The &lt;code&gt;$id&lt;/code&gt; keyword worked differently in Draft 4 versus Draft 7. Draft 2019-09 introduced the concept of vocabularies (we’ll get to that).&lt;/p&gt;
    &lt;p&gt;If you write a schema using Draft 2020-12 features and someone tries to validate it with a Draft 4 validator, things won’t work correctly. The &lt;code&gt;$schema&lt;/code&gt; keyword ensures everyone is on the same page.&lt;/p&gt;
    &lt;p&gt;Think of dialects like programming language versions. Python 2 and Python 3 are different dialects of Python. Your code needs to declare which one it’s written for, or chaos ensues.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vocabularies: the modular twist&lt;/head&gt;
    &lt;p&gt;Here’s where JSON Schema gets really interesting (and where my initial confusion peaked). Starting with Draft 2019-09, JSON Schema introduced the concept of vocabularies.&lt;/p&gt;
    &lt;p&gt;A vocabulary is a named collection of keywords that work together to provide a specific kind of functionality. Instead of having one monolithic metaschema that defines all possible keywords, you can compose metaschemas from smaller, focused vocabularies.&lt;/p&gt;
    &lt;p&gt;Think of vocabularies as modules or packages. Each vocabulary provides a set of related keywords. The core vocabulary has fundamental keywords like &lt;code&gt;$id&lt;/code&gt;, &lt;code&gt;$schema&lt;/code&gt;, &lt;code&gt;$ref&lt;/code&gt;, and &lt;code&gt;$defs&lt;/code&gt;. The applicator vocabulary has keywords that apply schemas to different parts of the data like &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;items&lt;/code&gt;, and &lt;code&gt;additionalProperties&lt;/code&gt;. The validation vocabulary has keywords for constraints like &lt;code&gt;minimum&lt;/code&gt;, &lt;code&gt;maxLength&lt;/code&gt;, &lt;code&gt;pattern&lt;/code&gt;, and &lt;code&gt;enum&lt;/code&gt;. The metadata vocabulary has keywords for human-readable information like &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, and &lt;code&gt;examples&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here’s a schema using keywords from different vocabularies:&lt;/p&gt;
    &lt;code&gt;{
  "$id": "https://example.com/my-schema",
  "title": "User Name",
  "description": "The user's full name",
  "type": "string",
  "minLength": 5,
  "pattern": "^[A-Z]"
}&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;$id&lt;/code&gt; comes from the core vocabulary, &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;description&lt;/code&gt; from the metadata vocabulary, &lt;code&gt;type&lt;/code&gt; from the applicator vocabulary, and &lt;code&gt;minLength&lt;/code&gt; and &lt;code&gt;pattern&lt;/code&gt; from the validation vocabulary.&lt;/p&gt;
    &lt;p&gt;Why vocabularies? They enable modularity and extensibility. You can pick and choose which vocabularies your dialect supports. Maybe you want validation but not format checking? Just include the vocabularies you need. You can define your own vocabulary with custom keywords specific to your domain. For example, a database schema dialect might add keywords like &lt;code&gt;indexed&lt;/code&gt; or &lt;code&gt;foreignKey&lt;/code&gt;. Each vocabulary is independently specified, making it easier to understand and implement different parts of JSON Schema.&lt;/p&gt;
    &lt;p&gt;Here’s how a metaschema declares which vocabularies it uses:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true,
    "https://json-schema.org/draft/2020-12/vocab/applicator": true,
    "https://json-schema.org/draft/2020-12/vocab/validation": true,
    "https://json-schema.org/draft/2020-12/vocab/meta-data": false
  }
}&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;true&lt;/code&gt; versus &lt;code&gt;false&lt;/code&gt; values indicate whether the vocabulary is required or optional. If a validator doesn’t understand a required vocabulary, it should refuse to process the schema. If it doesn’t understand an optional vocabulary, it can safely ignore those keywords.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extending with your own keywords&lt;/head&gt;
    &lt;p&gt;Here’s where this all gets practical. Once you understand vocabularies, you realize you can extend JSON Schema with your own domain-specific keywords. This is incredibly powerful.&lt;/p&gt;
    &lt;p&gt;In fact, you’ve probably already used extended JSON Schema without realizing it. OpenAPI (the spec for describing REST APIs) is exactly this: JSON Schema extended with custom keywords for HTTP-specific concerns like &lt;code&gt;operationId&lt;/code&gt;, &lt;code&gt;responses&lt;/code&gt;, &lt;code&gt;parameters&lt;/code&gt;, and so on. OpenAPI is JSON Schema plus a vocabulary for APIs. And you could extend OpenAPI further with your own vocabulary for framework-specific behaviors or company-specific conventions.&lt;/p&gt;
    &lt;p&gt;Say you’re building an API framework and you want to annotate your schemas with HTTP-specific metadata. Standard JSON Schema doesn’t have keywords for things like “this field comes from a query parameter” or “this response uses status code 201.” So you create your own vocabulary.&lt;/p&gt;
    &lt;p&gt;First, you define your custom keywords in a vocabulary document:&lt;/p&gt;
    &lt;code&gt;{
  "$id": "https://api.example.com/vocab/http",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true
  },
  "type": "object",
  "properties": {
    "httpSource": {
      "enum": ["query", "path", "header", "body"]
    },
    "httpStatus": {
      "type": "integer",
      "minimum": 100,
      "maximum": 599
    }
  }
}&lt;/code&gt;
    &lt;p&gt;This vocabulary document describes the structure of your custom keywords. Now you can use them in your schemas:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://api.example.com/schema",
  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true,
    "https://json-schema.org/draft/2020-12/vocab/validation": true,
    "https://api.example.com/vocab/http": true
  },
  "type": "object",
  "properties": {
    "userId": {
      "type": "string",
      "httpSource": "path",
      "pattern": "^[0-9]+$"
    },
    "filter": {
      "type": "string",
      "httpSource": "query"
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Your validator needs to understand what to do with &lt;code&gt;httpSource&lt;/code&gt;, of course. When it encounters a schema using your custom vocabulary, it checks whether it supports that vocabulary. If the vocabulary is marked as required and the validator doesn’t support it, validation should fail with an error saying “I don’t understand this vocabulary.” If it’s optional, the validator can safely ignore those keywords.&lt;/p&gt;
    &lt;p&gt;The beauty of this approach is that your extensions are explicit and discoverable. Someone reading your schema can see exactly which vocabularies it uses. A validator can definitively say whether it supports your schema or not. You’re not just stuffing random properties into schemas and hoping validators ignore them.&lt;/p&gt;
    &lt;p&gt;You can extend validation rules too. Maybe you’re working with database schemas and want to validate that certain string fields match database identifier conventions. You could define a custom keyword like &lt;code&gt;dbIdentifier&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "type": "string",
  "dbIdentifier": true,
  "description": "Must be a valid PostgreSQL identifier"
}&lt;/code&gt;
    &lt;p&gt;Your validator would implement the logic to check PostgreSQL identifier rules (no leading numbers, only certain special characters, length limits, etc.). Standard JSON Schema validators would ignore this keyword if you mark the vocabulary as optional, or refuse to process the schema if you mark it as required.&lt;/p&gt;
    &lt;p&gt;This extensibility is why JSON Schema has all this vocabulary machinery. It’s not just academic complexity for its own sake. The vocabulary system lets you build domain-specific validation languages on top of JSON Schema’s foundation, while maintaining clear boundaries about what’s standard and what’s custom.&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting it all together&lt;/head&gt;
    &lt;p&gt;Let’s connect all the dots. You write a schema that describes your data structure (like a User object). The schema uses keywords like &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;, and &lt;code&gt;minimum&lt;/code&gt; to express constraints. These keywords are defined by vocabularies (the validation vocabulary, applicator vocabulary). The vocabularies are bundled into a dialect (like Draft 2020-12). The dialect is defined by a metaschema that describes which keywords are available and how they work. Your schema declares its dialect using the &lt;code&gt;$schema&lt;/code&gt; keyword.&lt;/p&gt;
    &lt;p&gt;Here’s a visual:&lt;/p&gt;
    &lt;code&gt;Your Data (JSON)
    ↓ validated by
Your Schema (JSON)
    ↓ uses keywords from
Vocabularies (sets of related keywords)
    ↓ bundled into
Dialect (specific version/flavor)
    ↓ defined by
Metaschema (schema of schemas)&lt;/code&gt;
    &lt;p&gt;A concrete example showing all the layers:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://example.com/schemas/user",

  "title": "User",
  "description": "A registered user in the system",

  "type": "object",
  "properties": {
    "username": {
      "type": "string",
      "pattern": "^[a-zA-Z0-9_]+$",
      "minLength": 3,
      "maxLength": 20
    },
    "email": {
      "type": "string",
      "format": "email"
    },
    "age": {
      "type": "integer",
      "minimum": 13,
      "maximum": 120
    }
  },
  "required": ["username", "email"]
}&lt;/code&gt;
    &lt;p&gt;Breaking this down: &lt;code&gt;$schema&lt;/code&gt; declares we’re using the Draft 2020-12 dialect. &lt;code&gt;$id&lt;/code&gt; is a core vocabulary keyword that uniquely identifies this schema. &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;description&lt;/code&gt; are metadata vocabulary keywords for documentation. &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;, and &lt;code&gt;required&lt;/code&gt; are applicator vocabulary keywords that apply constraints. &lt;code&gt;pattern&lt;/code&gt;, &lt;code&gt;minLength&lt;/code&gt;, &lt;code&gt;minimum&lt;/code&gt;, and &lt;code&gt;format&lt;/code&gt; are validation vocabulary keywords that enforce rules.&lt;/p&gt;
    &lt;p&gt;All of these keywords are defined in the Draft 2020-12 metaschema, which specifies their meaning and behavior.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other terms you’ll encounter&lt;/head&gt;
    &lt;p&gt;While we’ve covered the big four (schema, metaschema, dialect, vocabulary), there are a few other terms worth understanding.&lt;/p&gt;
    &lt;p&gt;A keyword is a specific property name with defined semantics in a schema. Examples: &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;minimum&lt;/code&gt;, &lt;code&gt;$ref&lt;/code&gt;. Keywords are the building blocks defined by vocabularies. Some keywords are universal (&lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;), while others are specific to certain vocabularies (&lt;code&gt;contentMediaType&lt;/code&gt; from the content vocabulary, &lt;code&gt;deprecated&lt;/code&gt; from the metadata vocabulary).&lt;/p&gt;
    &lt;p&gt;There’s also a distinction between annotations and assertions. Assertions are keywords that can make validation fail (like &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;minimum&lt;/code&gt;, &lt;code&gt;required&lt;/code&gt;, &lt;code&gt;pattern&lt;/code&gt;). If your data violates an assertion, validation fails. Annotations are keywords that just provide information and never cause validation to fail (like &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, &lt;code&gt;examples&lt;/code&gt;, &lt;code&gt;default&lt;/code&gt;). Annotations are useful for documentation and tooling but don’t affect validity. Some keywords can produce both annotations and assertions. For instance, &lt;code&gt;properties&lt;/code&gt; asserts the types of the properties while also annotating which properties were validated.&lt;/p&gt;
    &lt;p&gt;Anchors provide named locations within a schema that you can reference. They’re like bookmarks:&lt;/p&gt;
    &lt;code&gt;{
  "$defs": {
    "address": {
      "$anchor": "addr",
      "type": "object",
      "properties": {
        "street": { "type": "string" }
      }
    }
  },
  "properties": {
    "billingAddress": { "$ref": "#addr" },
    "shippingAddress": { "$ref": "#addr" }
  }
}&lt;/code&gt;
    &lt;p&gt;Dynamic anchors (&lt;code&gt;$dynamicAnchor&lt;/code&gt; and &lt;code&gt;$dynamicRef&lt;/code&gt;) are more advanced. They allow references to be resolved differently depending on the “context” of evaluation. This is mostly useful for extending metaschemas and creating recursive schemas that can be overridden. Honestly, you can probably ignore dynamic anchors until you’re doing very advanced schema composition.&lt;/p&gt;
    &lt;p&gt;A bundled schema is a single schema document that contains multiple schema resources, usually via &lt;code&gt;$defs&lt;/code&gt;. This is handy for distributing related schemas together:&lt;/p&gt;
    &lt;code&gt;{
  "$id": "https://example.com/schemas/bundle",
  "$defs": {
    "user": {
      "$id": "user",
      "type": "object",
      "properties": { "name": { "type": "string" } }
    },
    "product": {
      "$id": "product",
      "type": "object",
      "properties": { "title": { "type": "string" } }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Now you can reference &lt;code&gt;https://example.com/schemas/user&lt;/code&gt; and &lt;code&gt;https://example.com/schemas/product&lt;/code&gt; from other schemas, even though they’re defined in the same document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is the terminology so confusing?&lt;/head&gt;
    &lt;p&gt;You might be wondering: why did they make this so complicated? The answer is that JSON Schema has evolved significantly over more than a decade, and the terminology evolved with it.&lt;/p&gt;
    &lt;p&gt;Early versions (Draft 3, Draft 4) had simpler, more monolithic metaschemas. As the specification matured, the community recognized the need for modularity, extensibility, and clearer versioning. That’s when concepts like dialects and vocabularies were formalized.&lt;/p&gt;
    &lt;p&gt;The terminology can feel academic because it comes from formal specification work. These are precise technical terms designed for specification writers and implementers, not necessarily for end users. Unfortunately, they leaked into the documentation that everyone reads, creating a steep learning curve.&lt;/p&gt;
    &lt;p&gt;But here’s the thing: you don’t need to think about most of this complexity to use JSON Schema effectively.&lt;/p&gt;
    &lt;head rend="h2"&gt;What do you actually need to know?&lt;/head&gt;
    &lt;p&gt;For 95% of JSON Schema usage, you need to understand that schemas describe data structure and constraints, &lt;code&gt;$schema&lt;/code&gt; declares which version (dialect) you’re using, and keywords like &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;, and &lt;code&gt;minimum&lt;/code&gt; define your rules.&lt;/p&gt;
    &lt;p&gt;That’s it. You can write perfectly good schemas for years without ever thinking about metaschemas or vocabularies in depth.&lt;/p&gt;
    &lt;p&gt;The deeper concepts matter when you’re building tools that process schemas (validators, code generators), extending JSON Schema with custom keywords, working on the specification itself, or debugging complex reference resolution issues. For everyone else, just know that these concepts exist and form a coherent system. If you encounter them in documentation, you’ll know what they mean, but you probably won’t need to think about them day-to-day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some practical advice&lt;/head&gt;
    &lt;p&gt;Always specify &lt;code&gt;$schema&lt;/code&gt; to make it explicit which dialect you’re using:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object"
}&lt;/code&gt;
    &lt;p&gt;This ensures validators interpret your schema correctly.&lt;/p&gt;
    &lt;p&gt;Start with the latest stable dialect. As of this writing, that’s Draft 2020-12. It has the most features and best tooling support. Don’t worry about older drafts unless you’re maintaining legacy schemas.&lt;/p&gt;
    &lt;p&gt;Use clear, descriptive metadata. Even though &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, and &lt;code&gt;examples&lt;/code&gt; don’t affect validation, they make your schemas much more useful:&lt;/p&gt;
    &lt;code&gt;{
  "type": "object",
  "title": "User Account",
  "description": "Represents a user account in the system",
  "properties": {
    "username": {
      "type": "string",
      "description": "Unique username for login (alphanumeric and underscores only)",
      "examples": ["john_doe", "alice123"]
    }
  }
}&lt;/code&gt;
    &lt;p&gt;For complex schemas, use &lt;code&gt;$defs&lt;/code&gt; to break things into reusable pieces:&lt;/p&gt;
    &lt;code&gt;{
  "$defs": {
    "timestamp": {
      "type": "string",
      "format": "date-time"
    },
    "identifier": {
      "type": "string",
      "pattern": "^[a-z0-9-]+$"
    }
  },
  "type": "object",
  "properties": {
    "id": { "$ref": "#/$defs/identifier" },
    "createdAt": { "$ref": "#/$defs/timestamp" }
  }
}&lt;/code&gt;
    &lt;p&gt;If you’re ever confused about whether a keyword exists or how it works, check the metaschema. The Draft 2020-12 metaschema lives at: https://json-schema.org/draft/2020-12/schema&lt;/p&gt;
    &lt;p&gt;Test your schemas with online validators or schema testing tools to ensure they work as expected. The official JSON Schema website has a validator you can try: https://www.jsonschemavalidator.net/&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;JSON Schema’s terminology can feel intimidating, but the core ideas are straightforward. Schemas describe data. Metaschemas describe schemas. Dialects are specific versions of JSON Schema. Vocabularies are modular collections of keywords. Keywords are the actual properties you use in schemas.&lt;/p&gt;
    &lt;p&gt;The terminology exists to support a powerful, extensible system for describing JSON data structures. But for everyday use, you can mostly ignore the academic terminology and focus on writing clear, useful schemas.&lt;/p&gt;
    &lt;p&gt;The next time you see “metaschema” or “vocabulary” in JSON Schema documentation, don’t panic. You know what these terms mean now, and more importantly, you understand how they fit together. That’s the real goal: building a mental model of how the system works, not memorizing definitions.&lt;/p&gt;
    &lt;p&gt;Now go forth and write some schemas. And remember: if you find yourself confused by JSON Schema terminology again, you’re not alone. The important thing is that underneath the jargon, there’s a well-designed system for a genuinely useful purpose.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080769</guid><pubDate>Fri, 28 Nov 2025 17:34:26 +0000</pubDate></item><item><title>Molly: An Improved Signal App</title><link>https://molly.im/</link><description>&lt;doc fingerprint="fa4c68be1bd036dc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;An improved Signal app&lt;/head&gt;
    &lt;p&gt;Molly is an independent Signal fork for Android with improved features:&lt;/p&gt;
    &lt;p&gt; Fully FOSS Contains no proprietary blobs, unlike Signal &lt;/p&gt;
    &lt;p&gt; Encrypted Protects database with Passphrase Encryption &lt;/p&gt;
    &lt;p&gt; Multi-Device Pair multiple devices to a single account &lt;/p&gt;
    &lt;p&gt; Material You Extra theme that follows your device palette &lt;/p&gt;
    &lt;p&gt; UnifiedPush Ungoogled notification system &lt;/p&gt;
    &lt;p&gt; Automatic Locking When you are gone for a set period of time &lt;/p&gt;
    &lt;p&gt; RAM Shredding Securely shreds sensitive data &lt;/p&gt;
    &lt;p&gt; And more New and better features to come &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080916</guid><pubDate>Fri, 28 Nov 2025 17:48:25 +0000</pubDate></item><item><title>28M Hacker News comments as vector embedding search dataset</title><link>https://clickhouse.com/docs/getting-started/example-datasets/hackernews-vector-search-dataset</link><description>&lt;doc fingerprint="11938b322ad7669e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Generate embeddings for search query&lt;/head&gt;
      &lt;p&gt;Sentence Transformers provide local, easy to use embedding models for capturing the semantic meaning of sentences and paragraphs.&lt;/p&gt;
      &lt;p&gt;The dataset in this HackerNews dataset contains vector emebeddings generated from the all-MiniLM-L6-v2 model.&lt;/p&gt;
      &lt;p&gt;An example Python script is provided below to demonstrate how to programmatically generate embedding vectors using &lt;code&gt;sentence_transformers1 Python package. The search embedding vector is then passed as an argument to the [&lt;/code&gt;cosineDistance()&lt;code&gt;](/sql-reference/functions/distance-functions#cosineDistance) function in the &lt;/code&gt;SELECT` query.&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;from sentence_transformers import SentenceTransformer
import sys

import clickhouse_connect

print("Initializing...")

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

chclient = clickhouse_connect.get_client() # ClickHouse credentials here

while True:
    # Take the search query from user
    print("Enter a search query :")
    input_query = sys.stdin.readline();
    texts = [input_query]

    # Run the model and obtain search vector
    print("Generating the embedding for ", input_query);
    embeddings = model.encode(texts)

    print("Querying ClickHouse...")
    params = {'v1':list(embeddings[0]), 'v2':20}
    result = chclient.query("SELECT id, title, text FROM hackernews ORDER BY cosineDistance(vector, %(v1)s) LIMIT %(v2)s", parameters=params)
    print("Results :")
    for row in result.result_rows:
        print(row[0], row[2][:100])
        print("---------")

&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;p&gt;An example of running the above Python script and similarity search results are shown below (only 100 characters from each of the top 20 posts are printed):&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;Initializing...

Enter a search query :
Are OLAP cubes useful

Generating the embedding for  "Are OLAP cubes useful"

Querying ClickHouse...

Results :

27742647 smartmic:
slt2021: OLAP Cube is not dead, as long as you use some form of:&amp;lt;p&amp;gt;1. GROUP BY multiple fi
---------
27744260 georgewfraser:A data mart is a logical organization of data to help humans understand the schema. Wh
---------
27761434 mwexler:&amp;amp;quot;We model data according to rigorous frameworks like Kimball or Inmon because we must r
---------
28401230 chotmat:
erosenbe0: OLAP database is just a copy, replica, or archive of data with a schema designe
---------
22198879 Merick:+1 for Apache Kylin, it&amp;amp;#x27;s a great project and awesome open source community. If anyone i
---------
27741776 crazydoggers:I always felt the value of an OLAP cube was uncovering questions you may not know to as
---------
22189480 shadowsun7:
_Codemonkeyism: After maintaining an OLAP cube system for some years, I&amp;amp;#x27;m not that
---------
27742029 smartmic:
gengstrand: My first exposure to OLAP was on a team developing a front end to Essbase that
---------
22364133 irfansharif:
simo7: I&amp;amp;#x27;m wondering how this technology could work for OLAP cubes.&amp;lt;p&amp;gt;An OLAP cube
---------
23292746 scoresmoke:When I was developing my pet project for Web analytics (&amp;lt;a href="https:&amp;amp;#x2F;&amp;amp;#x2F;github
---------
22198891 js8:It seems that the article makes a categorical error, arguing that OLAP cubes were replaced by co
---------
28421602 chotmat:
7thaccount: Is there any advantage to OLAP cube over plain SQL (large historical database r
---------
22195444 shadowsun7:
lkcubing: Thanks for sharing. Interesting write up.&amp;lt;p&amp;gt;While this article accurately capt
---------
22198040 lkcubing:Thanks for sharing. Interesting write up.&amp;lt;p&amp;gt;While this article accurately captures the issu
---------
3973185 stefanu:
sgt: Interesting idea. Ofcourse, OLAP isn't just about the underlying cubes and dimensions,
---------
22190903 shadowsun7:
js8: It seems that the article makes a categorical error, arguing that OLAP cubes were r
---------
28422241 sradman:OLAP Cubes have been disrupted by Column Stores. Unless you are interested in the history of
---------
28421480 chotmat:
sradman: OLAP Cubes have been disrupted by Column Stores. Unless you are interested in the
---------
27742515 BadInformatics:
quantified: OP posts with inverted condition: “OLAP != OLAP Cube” is the actual titl
---------
28422935 chotmat:
rstuart4133: I remember hearing about OLAP cubes donkey&amp;amp;#x27;s years ago (probably not far
---------
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Summarization demo application&lt;/head&gt;
      &lt;p&gt;The example above demonstrated semantic search and document retrieval using ClickHouse.&lt;/p&gt;
      &lt;p&gt;A very simple but high potential generative AI example application is presented next.&lt;/p&gt;
      &lt;p&gt;The application performs the following steps:&lt;/p&gt;
      &lt;list class="custom-ol" rend="ol"&gt;
        &lt;item class="custom-li"&gt;Accepts a topic as input from the user&lt;/item&gt;
        &lt;item class="custom-li"&gt;Generates an embedding vector for the topic by using the &lt;code&gt;SentenceTransformers&lt;/code&gt; with model &lt;code&gt;all-MiniLM-L6-v2&lt;/code&gt;&lt;/item&gt;
        &lt;item class="custom-li"&gt;Retrieves highly relevant posts/comments using vector similarity search on the &lt;code&gt;hackernews&lt;/code&gt; table&lt;/item&gt;
        &lt;item class="custom-li"&gt;Uses &lt;code&gt;LangChain&lt;/code&gt; and OpenAI &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; Chat API to summarize the content retrieved in step #3.
The posts/comments retrieved in step #3 are passed as context to the Chat API and are the key link in Generative AI.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;An example from running the summarization application is first listed below, followed by the code for the summarization application. Running the application requires an OpenAI API key to be set in the environment variable &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. The OpenAI API key can be obtained after registering at https://platform.openai.com.&lt;/p&gt;
      &lt;p&gt;This application demonstrates a Generative AI use-case that is applicable to multiple enterprise domains like : customer sentiment analysis, technical support automation, mining user conversations, legal documents, medical records, meeting transcripts, financial statements, etc&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;$ python3 summarize.py

Enter a search topic :
ClickHouse performance experiences

Generating the embedding for ----&amp;gt;  ClickHouse performance experiences

Querying ClickHouse to retrieve relevant articles...

Initializing chatgpt-3.5-turbo model...

Summarizing search results retrieved from ClickHouse...

Summary from chatgpt-3.5:
The discussion focuses on comparing ClickHouse with various databases like TimescaleDB, Apache Spark,
AWS Redshift, and QuestDB, highlighting ClickHouse's cost-efficient high performance and suitability
for analytical applications. Users praise ClickHouse for its simplicity, speed, and resource efficiency
in handling large-scale analytics workloads, although some challenges like DMLs and difficulty in backups
are mentioned. ClickHouse is recognized for its real-time aggregate computation capabilities and solid
engineering, with comparisons made to other databases like Druid and MemSQL. Overall, ClickHouse is seen
as a powerful tool for real-time data processing, analytics, and handling large volumes of data
efficiently, gaining popularity for its impressive performance and cost-effectiveness.
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;p&gt;Code for the above application :&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;print("Initializing...")

import sys
import json
import time
from sentence_transformers import SentenceTransformer

import clickhouse_connect

from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
import textwrap
import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -&amp;gt; int:
    encoding = tiktoken.encoding_for_model(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

chclient = clickhouse_connect.get_client(compress=False) # ClickHouse credentials here

while True:
    # Take the search query from user
    print("Enter a search topic :")
    input_query = sys.stdin.readline();
    texts = [input_query]

    # Run the model and obtain search or reference vector
    print("Generating the embedding for ----&amp;gt; ", input_query);
    embeddings = model.encode(texts)

    print("Querying ClickHouse...")
    params = {'v1':list(embeddings[0]), 'v2':100}
    result = chclient.query("SELECT id,title,text FROM hackernews ORDER BY cosineDistance(vector, %(v1)s) LIMIT %(v2)s", parameters=params)

    # Just join all the search results
    doc_results = ""
    for row in result.result_rows:
        doc_results = doc_results + "\n" + row[2]

    print("Initializing chatgpt-3.5-turbo model")
    model_name = "gpt-3.5-turbo"

    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
        model_name=model_name
    )

    texts = text_splitter.split_text(doc_results)

    docs = [Document(page_content=t) for t in texts]

    llm = ChatOpenAI(temperature=0, model_name=model_name)

    prompt_template = """
Write a concise summary of the following in not more than 10 sentences:


{text}


CONSCISE SUMMARY :
"""

    prompt = PromptTemplate(template=prompt_template, input_variables=["text"])

    num_tokens = num_tokens_from_string(doc_results, model_name)

    gpt_35_turbo_max_tokens = 4096
    verbose = False

    print("Summarizing search results retrieved from ClickHouse...")

    if num_tokens &amp;lt;= gpt_35_turbo_max_tokens:
        chain = load_summarize_chain(llm, chain_type="stuff", prompt=prompt, verbose=verbose)
    else:
        chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=prompt, combine_prompt=prompt, verbose=verbose)

    summary = chain.run(docs)

    print(f"Summary from chatgpt-3.5: {summary}")
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46081053</guid><pubDate>Fri, 28 Nov 2025 18:02:04 +0000</pubDate></item><item><title>Imgur geo-blocked the UK, so I geo-unblocked my network</title><link>https://blog.tymscar.com/posts/imgurukproxy/</link><description>&lt;doc fingerprint="b582e1f56725aa92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Imgur Geo-Blocked the UK, So I Geo-Unblocked My Entire Network&lt;/head&gt;
    &lt;p&gt;Imgur decided to block UK users. Honestly? I don’t really care that much. I haven’t actively browsed the site in years. But it used to be everywhere. Back when Reddit embedded everything on Imgur, maybe fifteen years ago, it was genuinely useful. Then Reddit built their own image hosting, Discord did the same, and Imgur slowly faded into the background.&lt;/p&gt;
    &lt;p&gt;Except it never fully disappeared. And since the block, I keep stumbling across Imgur links that just show “unavailable.” It’s mildly infuriating.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Minecraft shader problem#&lt;/head&gt;
    &lt;p&gt;Here’s a concrete example. I was playing Minecraft with some work colleagues and wanted to try different shaders. Most shader pages embed preview images hosted on Imgur. So I’d click through shader after shader, and every single preview was just gone. I couldn’t see what any of them looked like without the images.&lt;/p&gt;
    &lt;p&gt;This kind of thing happens constantly now. Old forum posts, Reddit threads, documentation pages, random project READMEs. Imgur links are still scattered across the internet, and in the UK, they’re all broken.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I didn’t just install a VPN#&lt;/head&gt;
    &lt;p&gt;The obvious solution is to use a VPN. Change your location, problem solved. But I have a few issues with that approach.&lt;/p&gt;
    &lt;p&gt;First, I just upgraded to 2.5 Gbps internet and I don’t want to route all my traffic through a VPN and take the speed hit. I have this bandwidth for a reason.&lt;/p&gt;
    &lt;p&gt;Second, even if I installed a VPN on my main machine, what about my phone? My laptop? My desktop? Every device would need the VPN running, and I’d have to remember to connect it before browsing. It’s messy.&lt;/p&gt;
    &lt;p&gt;I wanted something cleaner: a solution that works for every device on my network, automatically, without any client-side configuration.&lt;/p&gt;
    &lt;head rend="h2"&gt;The network-level approach#&lt;/head&gt;
    &lt;p&gt;I already run a homelab with Traefik as my reverse proxy, Pi-hole for DNS, and everything declaratively configured with NixOS. If you’ve read my previous post on Docker containers with secrets, you’ll recognise the pattern.&lt;/p&gt;
    &lt;p&gt;The idea was simple: intercept all requests to &lt;code&gt;i.imgur.com&lt;/code&gt; at the DNS level, route them through a VPN-connected container, and serve the images back. Every device on my network automatically uses Pi-hole for DNS via DHCP, so this would be completely transparent.&lt;/p&gt;
    &lt;p&gt;Here’s the flow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Device requests &lt;code&gt;i.imgur.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Pi-hole returns my Traefik instance’s IP instead&lt;/item&gt;
      &lt;item&gt;Traefik sees the SNI hostname and routes to Gluetun&lt;/item&gt;
      &lt;item&gt;Gluetun tunnels the request through a VPN&lt;/item&gt;
      &lt;item&gt;Nginx (attached to Gluetun’s network) proxies to the real Imgur&lt;/item&gt;
      &lt;item&gt;Image comes back through the tunnel to the device&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why Nginx when I already have Traefik?#&lt;/head&gt;
    &lt;p&gt;Good question. Gluetun isn’t a reverse proxy. It’s a container that provides VPN connectivity to other containers attached to its network namespace. So I needed something inside Gluetun’s network to actually handle the proxying. Nginx was the simplest choice.&lt;/p&gt;
    &lt;p&gt;The Nginx config is minimal. It just does TCP passthrough with SNI:&lt;/p&gt;
    &lt;code&gt;user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
}

stream {
    resolver 127.0.0.1 valid=30s;
    resolver_timeout 5s;

    server {
        listen 443;
        ssl_preread on;
        proxy_pass i.imgur.com:443;
        proxy_connect_timeout 10s;
        proxy_timeout 60s;
    }
}
&lt;/code&gt;
    &lt;p&gt;This listens on port 443, reads the SNI header to confirm the destination, and passes the connection through to the real &lt;code&gt;i.imgur.com&lt;/code&gt;. The TLS handshake happens end-to-end; Nginx never sees the decrypted traffic.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Docker setup#&lt;/head&gt;
    &lt;p&gt;The compose file runs two containers. Gluetun handles the VPN connection, and Nginx attaches to Gluetun’s network:&lt;/p&gt;
    &lt;code&gt;version: '3.8'

services:
  gluetun:
    image: qmcgaw/gluetun:latest
    container_name: gluetun
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      - VPN_SERVICE_PROVIDER=${VPN_SERVICE_PROVIDER}
      - VPN_TYPE=wireguard
      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY}
      - SERVER_COUNTRIES=${SERVER_COUNTRIES}
      - FIREWALL=on
      - FIREWALL_INPUT_PORTS=443
      - FIREWALL_OUTBOUND_SUBNETS=10.0.0.0/8
      - DOT=on
      - DOT_PROVIDERS=cloudflare
      - HEALTH_VPN_DURATION_INITIAL=30s
    volumes:
      - ./gluetun:/gluetun
    restart: unless-stopped
    networks:
      - proxy
    healthcheck:
      test: ["CMD", "/gluetun-entrypoint", "healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  imgur-proxy:
    image: nginx:alpine
    container_name: imgur-proxy
    depends_on:
      gluetun:
        condition: service_healthy
    network_mode: "service:gluetun"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped

networks:
  proxy:
    external: true
&lt;/code&gt;
    &lt;p&gt;The key detail is &lt;code&gt;network_mode: "service:gluetun"&lt;/code&gt;. This makes Nginx share Gluetun’s network stack, so all its traffic automatically goes through the VPN tunnel.&lt;/p&gt;
    &lt;p&gt;I’m not going to mention which VPN provider I use. It’s one of the major ones with WireGuard support, but honestly I’m not thrilled with it. Use whatever you have.&lt;/p&gt;
    &lt;head rend="h2"&gt;Traefik routing#&lt;/head&gt;
    &lt;p&gt;The final piece is telling Traefik to route &lt;code&gt;i.imgur.com&lt;/code&gt; traffic to the Gluetun container. This uses TCP routing with TLS passthrough:&lt;/p&gt;
    &lt;code&gt;tcp:
  routers:
    imgur-router:
      rule: "HostSNI(`i.imgur.com`)"
      entryPoints:
        - https
      service: imgur-service
      tls:
        passthrough: true
  services:
    imgur-service:
      loadBalancer:
        servers:
          - address: "gluetun:443"
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;passthrough: true&lt;/code&gt; is important. It means Traefik doesn’t terminate TLS; it just inspects the SNI header and forwards the connection.&lt;/p&gt;
    &lt;head rend="h2"&gt;NixOS integration#&lt;/head&gt;
    &lt;p&gt;Following the same pattern from my Docker with secrets post, I created a systemd service that runs the compose stack with Agenix-managed secrets:&lt;/p&gt;
    &lt;code&gt;{ pkgs, config, ... }:
let
  docker-env = config.age.secrets.docker-imgur-proxy.path;
in
{
  systemd.services.imgur-proxy = {
    description = "Imgur Proxy with VPN";
    after = [
      "network.target"
      "docker.service"
      "docker-create-proxy-network.service"
    ];
    wants = [
      "docker.service"
      "docker-create-proxy-network.service"
    ];
    serviceConfig = {
      ExecStart = "${pkgs.docker}/bin/docker compose --env-file ${docker-env} -f docker-compose.yml up --force-recreate";
      ExecStop = "${pkgs.docker}/bin/docker compose -f docker-compose.yml down";
      WorkingDirectory = "/home/tymscar/dotfiles/apps/nixos/docker/imgur-proxy";
      Restart = "always";
    };
    wantedBy = [ "multi-user.target" ];
  };
}
&lt;/code&gt;
    &lt;p&gt;The VPN credentials are stored encrypted with Agenix, so my entire dotfiles repo stays public while keeping secrets safe.&lt;/p&gt;
    &lt;head rend="h2"&gt;The result#&lt;/head&gt;
    &lt;p&gt;Now when any device on my network requests an Imgur image, it works. My phone, my laptop, guest devices, everything. No VPN apps to install, no browser extensions, no manual configuration. Pi-hole intercepts the DNS, Traefik routes the connection, and Gluetun tunnels it through a non-UK exit point.&lt;/p&gt;
    &lt;p&gt;The latency increase is negligible for loading images, and it only affects Imgur traffic. Everything else still goes direct at full speed.&lt;/p&gt;
    &lt;p&gt;Is this overkill for viewing the occasional Imgur image? Probably. But it’s a clean solution that requires minimal ongoing maintenance, and it scratches the homelab itch. Plus I can finally see what those Minecraft shaders look like.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46081188</guid><pubDate>Fri, 28 Nov 2025 18:15:50 +0000</pubDate></item><item><title>Show HN: Pulse 2.0 – Live co-listening rooms where anyone can be a DJ</title><link>https://473999.net/pulse</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46081443</guid><pubDate>Fri, 28 Nov 2025 18:39:34 +0000</pubDate></item><item><title>Effective harnesses for long-running agents</title><link>https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents</link><description>&lt;doc fingerprint="3d429ef368998aee"&gt;
  &lt;main&gt;
    &lt;p&gt;As AI agents become more capable, developers are increasingly asking them to take on complex tasks requiring work that spans hours, or even days. However, getting agents to make consistent progress across multiple context windows remains an open problem.&lt;/p&gt;
    &lt;p&gt;The core challenge of long-running agents is that they must work in discrete sessions, and each new session begins with no memory of what came before. Imagine a software project staffed by engineers working in shifts, where each new engineer arrives with no memory of what happened on the previous shift. Because context windows are limited, and because most complex projects cannot be completed within a single window, agents need a way to bridge the gap between coding sessions.&lt;/p&gt;
    &lt;p&gt;We developed a two-fold solution to enable the Claude Agent SDK to work effectively across many context windows: an initializer agent that sets up the environment on the first run, and a coding agent that is tasked with making incremental progress in every session, while leaving clear artifacts for the next session. You can find code examples in the accompanying quickstart.&lt;/p&gt;
    &lt;head rend="h2"&gt;The long-running agent problem&lt;/head&gt;
    &lt;p&gt;The Claude Agent SDK is a powerful, general-purpose agent harness adept at coding, as well as other tasks that require the model to use tools to gather context, plan, and execute. It has context management capabilities such as compaction, which enables an agent to work on a task without exhausting the context window. Theoretically, given this setup, it should be possible for an agent to continue to do useful work for an arbitrarily long time.&lt;/p&gt;
    &lt;p&gt;However, compaction isn’t sufficient. Out of the box, even a frontier coding model like Opus 4.5 running on the Claude Agent SDK in a loop across multiple context windows will fall short of building a production-quality web app if it’s only given a high-level prompt, such as “build a clone of claude.ai.”&lt;/p&gt;
    &lt;p&gt;Claude’s failures manifested in two patterns. First, the agent tended to try to do too much at once—essentially to attempt to one-shot the app. Often, this led to the model running out of context in the middle of its implementation, leaving the next session to start with a feature half-implemented and undocumented. The agent would then have to guess at what had happened, and spend substantial time trying to get the basic app working again. This happens even with compaction, which doesn’t always pass perfectly clear instructions to the next agent.&lt;/p&gt;
    &lt;p&gt;A second failure mode would often occur later in a project. After some features had already been built, a later agent instance would look around, see that progress had been made, and declare the job done.&lt;/p&gt;
    &lt;p&gt;This decomposes the problem into two parts. First, we need to set up an initial environment that lays the foundation for all the features that a given prompt requires, which sets up the agent to work step-by-step and feature-by-feature. Second, we should prompt each agent to make incremental progress towards its goal while also leaving the environment in a clean state at the end of a session. By “clean state” we mean the kind of code that would be appropriate for merging to a main branch: there are no major bugs, the code is orderly and well-documented, and in general, a developer could easily begin work on a new feature without first having to clean up an unrelated mess.&lt;/p&gt;
    &lt;p&gt;When experimenting internally, we addressed these problems using a two-part solution:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Initializer agent: The very first agent session uses a specialized prompt that asks the model to set up the initial environment: an &lt;code&gt;init.sh&lt;/code&gt;script, a claude-progress.txt file that keeps a log of what agents have done, and an initial git commit that shows what files were added.&lt;/item&gt;
      &lt;item&gt;Coding agent: Every subsequent session asks the model to make incremental progress, then leave structured updates.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key insight here was finding a way for agents to quickly understand the state of work when starting with a fresh context window, which is accomplished with the claude-progress.txt file alongside the git history. Inspiration for these practices came from knowing what effective software engineers do every day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Environment management&lt;/head&gt;
    &lt;p&gt;In the updated Claude 4 prompting guide, we shared some best practices for multi-context window workflows, including a harness structure that uses “a different prompt for the very first context window.” This “different prompt” requests that the initializer agent set up the environment with all the necessary context that future coding agents will need to work effectively. Here, we provide a deeper dive on some of the key components of such an environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feature list&lt;/head&gt;
    &lt;p&gt;To address the problem of the agent one-shotting an app or prematurely considering the project complete, we prompted the initializer agent to write a comprehensive file of feature requirements expanding on the user’s initial prompt. In the claude.ai clone example, this meant over 200 features, such as “a user can open a new chat, type in a query, press enter, and see an AI response.” These features were all initially marked as “failing” so that later coding agents would have a clear outline of what full functionality looked like.&lt;/p&gt;
    &lt;code&gt;{
    "category": "functional",
    "description": "New chat button creates a fresh conversation",
    "steps": [
      "Navigate to main interface",
      "Click the 'New Chat' button",
      "Verify a new conversation is created",
      "Check that chat area shows welcome state",
      "Verify conversation appears in sidebar"
    ],
    "passes": false
  }
&lt;/code&gt;
    &lt;p&gt;We prompt coding agents to edit this file only by changing the status of a passes field, and we use strongly-worded instructions like “It is unacceptable to remove or edit tests because this could lead to missing or buggy functionality.” After some experimentation, we landed on using JSON for this, as the model is less likely to inappropriately change or overwrite JSON files compared to Markdown files.&lt;/p&gt;
    &lt;head rend="h3"&gt;Incremental progress&lt;/head&gt;
    &lt;p&gt;Given this initial environment scaffolding, the next iteration of the coding agent was then asked to work on only one feature at a time. This incremental approach turned out to be critical to addressing the agent’s tendency to do too much at once.&lt;/p&gt;
    &lt;p&gt;Once working incrementally, it’s still essential that the model leaves the environment in a clean state after making a code change. In our experiments, we found that the best way to elicit this behavior was to ask the model to commit its progress to git with descriptive commit messages and to write summaries of its progress in a progress file. This allowed the model to use git to revert bad code changes and recover working states of the code base.&lt;/p&gt;
    &lt;p&gt;These approaches also increased efficiency, as they eliminated the need for an agent to have to guess at what had happened and spend its time trying to get the basic app working again.&lt;/p&gt;
    &lt;head rend="h3"&gt;Testing&lt;/head&gt;
    &lt;p&gt;One final major failure mode that we observed was Claude’s tendency to mark a feature as complete without proper testing. Absent explicit prompting, Claude tended to make code changes, and even do testing with unit tests or &lt;code&gt;curl&lt;/code&gt; commands against a development server, but would fail recognize that the feature didn’t work end-to-end.&lt;/p&gt;
    &lt;p&gt;In the case of building a web app, Claude mostly did well at verifying features end-to-end once explicitly prompted to use browser automation tools and do all testing as a human user would.&lt;/p&gt;
    &lt;p&gt;Providing Claude with these kinds of testing tools dramatically improved performance, as the agent was able to identify and fix bugs that weren’t obvious from the code alone.&lt;/p&gt;
    &lt;p&gt;Some issues remain, like limitations to Claude’s vision and to browser automation tools making it difficult to identify every kind of bug. For example, Claude can’t see browser-native alert modals through the Puppeteer MCP, and features relying on these modals tended to be buggier as a result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting up to speed&lt;/head&gt;
    &lt;p&gt;With all of the above in place, every coding agent is prompted to run through a series of steps to get its bearings, some quite basic but still helpful:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;pwd&lt;/code&gt;to see the directory you’re working in. You’ll only be able to edit files in this directory.&lt;/item&gt;
      &lt;item&gt;Read the git logs and progress files to get up to speed on what was recently worked on.&lt;/item&gt;
      &lt;item&gt;Read the features list file and choose the highest-priority feature that’s not yet done to work on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach saves Claude some tokens in every session since it doesn’t have to figure out how to test the code. It also helps to ask the initializer agent to write an init.sh script that can run the development server, and then run through a basic end-to-end test before implementing a new feature.&lt;/p&gt;
    &lt;p&gt;In the case of the claude.ai clone, this meant that the agent always started the local development server and used the Puppeteer MCP to start a new chat, send a message, and receive a response. This ensured that Claude could quickly identify if the app had been left in a broken state, and immediately fix any existing bugs. If the agent had instead started implementing a new feature, it would likely make the problem worse.&lt;/p&gt;
    &lt;p&gt;Given all this, a typical session starts off with the following assistant messages:&lt;/p&gt;
    &lt;code&gt;[Assistant] I'll start by getting my bearings and understanding the current state of the project.
[Tool Use] &amp;lt;bash - pwd&amp;gt;
[Tool Use] &amp;lt;read - claude-progress.txt&amp;gt;
[Tool Use] &amp;lt;read - feature_list.json&amp;gt;
[Assistant] Let me check the git log to see recent work.
[Tool Use] &amp;lt;bash - git log --oneline -20&amp;gt;
[Assistant] Now let me check if there's an init.sh script to restart the servers.
&amp;lt;Starts the development server&amp;gt;
[Assistant] Excellent! Now let me navigate to the application and verify that some fundamental features are still working.
&amp;lt;Tests basic functionality&amp;gt;
[Assistant] Based on my verification testing, I can see that the fundamental functionality is working well. The core chat features, theme switching, conversation loading, and error handling are all functioning correctly. Now let me review the tests.json file more comprehensively to understand what needs to be implemented next.
&amp;lt;Starts work on a new feature&amp;gt;&lt;/code&gt;
    &lt;p&gt;Agent failure modes and solutions&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Problem&lt;/cell&gt;
        &lt;cell role="head"&gt;Initializer Agent Behavior&lt;/cell&gt;
        &lt;cell role="head"&gt;Coding Agent Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude declares victory on the entire project too early.&lt;/cell&gt;
        &lt;cell&gt;Set up a feature list file: based on the input spec, set up a structured JSON file with a list of end-to-end feature descriptions.&lt;/cell&gt;
        &lt;cell&gt;Read the feature list file at the beginning of a session. Choose a single feature to start working on.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude leaves the environment in a state with bugs or undocumented progress.&lt;/cell&gt;
        &lt;cell&gt;An initial git repo and progress notes file is written.&lt;/cell&gt;
        &lt;cell&gt;Start the session by reading the progress notes file and git commit logs, and run a basic test on the development server to catch any undocumented bugs. End the session by writing a git commit and progress update.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude marks features as done prematurely.&lt;/cell&gt;
        &lt;cell&gt;Set up a feature list file.&lt;/cell&gt;
        &lt;cell&gt;Self-verify all features. Only mark features as “passing” after careful testing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Claude has to spend time figuring out how to run the app.&lt;/cell&gt;
        &lt;cell&gt;Write an &lt;code&gt;init.sh&lt;/code&gt; script that can run the development server.&lt;/cell&gt;
        &lt;cell&gt;Start the session by reading &lt;code&gt;init.sh&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;This research demonstrates one possible set of solutions in a long-running agent harness to enable the model to make incremental progress across many context windows. However, there remain open questions.&lt;/p&gt;
    &lt;p&gt;Most notably, it’s still unclear whether a single, general-purpose coding agent performs best across contexts, or if better performance can be achieved through a multi-agent architecture. It seems reasonable that specialized agents like a testing agent, a quality assurance agent, or a code cleanup agent, could do an even better job at sub-tasks across the software development lifecycle.&lt;/p&gt;
    &lt;p&gt;Additionally, this demo is optimized for full-stack web app development. A future direction is to generalize these findings to other fields. It’s likely that some or all of these lessons can be applied to the types of long-running agentic tasks required in, for example, scientific research or financial modeling.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Written by Justin Young. Special thanks to David Hershey, Prithvi Rajasakeran, Jeremy Hadfield, Naia Bouscal, Michael Tingley, Jesse Mu, Jake Eaton, Marius Buleandara, Maggie Vo, Pedram Navid, Nadine Yasser, and Alex Notov for their contributions.&lt;/p&gt;
    &lt;p&gt;This work reflects the collective efforts of several teams across Anthropic who made it possible for Claude to safely do long-horizon autonomous software engineering, especially the code RL &amp;amp; Claude Code teams. Interested candidates who would like to contribute are welcome to apply at anthropic.com/careers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;1. We refer to these as separate agents in this context only because they have different initial user prompts. The system prompt, set of tools, and overall agent harness was otherwise identical.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46081704</guid><pubDate>Fri, 28 Nov 2025 19:05:16 +0000</pubDate></item><item><title>The original ABC language, Python's predecessor (1991)</title><link>https://github.com/gvanrossum/abc-unix</link><description>&lt;doc fingerprint="2fe8520fe9cb456a"&gt;
  &lt;main&gt;
    &lt;p&gt;ABC was Python's most direct predecessor; I worked on it from around 1983-1986.&lt;/p&gt;
    &lt;p&gt;I downloaded these sources from cwi.nl, specifically the abc-unix tarball.&lt;/p&gt;
    &lt;p&gt;Another copy of the ABC sources lives in Luciano Ramalho's GitHub. I hope one day to compare the two trees and unify them.&lt;/p&gt;
    &lt;p&gt;Most files have 1991 as their latest modification time in the tar ball; a few have 1996 or 2021.&lt;/p&gt;
    &lt;p&gt;The old README file has build instructions.&lt;/p&gt;
    &lt;p&gt;The current sources assume a 32-bit system where int and pointers have the same size. I hope to eventually upgrade the source code to work on 64-bit systems too (where int is 32 bits and pointers are 64 bits).&lt;/p&gt;
    &lt;p&gt;CWI never put a license on ABC, but it says:&lt;/p&gt;
    &lt;p&gt;Copyright (c) Stichting Mathematisch Centrum, Amsterdam, 1988-2011.&lt;/p&gt;
    &lt;p&gt;I'll try to negotiate with Steven Pemberton eventually (hopefully MIT).&lt;/p&gt;
    &lt;p&gt;Eddy Boeve, Frank van Dijk, Leo Geurts, Timo Krijnen, Lambert Meertens, Steven Pemberton, Guido van Rossum.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leo Geurts, Lambert Meertens and Steven Pemberton, The ABC Programmer's Handbook, Prentice-Hall, Englewood Cliffs, New Jersey, 1990, ISBN 0-13- 000027-2.&lt;/item&gt;
      &lt;item&gt;Steven Pemberton, An Alternative Simple Language and Environment for PCs, IEEE Software, Vol. 4, No. 1, January 1987, pp. 56-64. http://www.cwi.nl/~steven/abc.html&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Steven Pemberton's home page. https://homepages.cwi.nl/~steven/abc/&lt;/item&gt;
      &lt;item&gt;Lambert Meertens, The Origins of Python. https://inference-review.com/article/the-origins-of-python&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46082204</guid><pubDate>Fri, 28 Nov 2025 19:58:13 +0000</pubDate></item><item><title>How good engineers write bad code at big companies</title><link>https://www.seangoedecke.com/bad-code-at-big-companies/</link><description>&lt;doc fingerprint="bf4dd1049e34635e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How good engineers write bad code at big companies&lt;/head&gt;
    &lt;p&gt;Every couple of years somebody notices that large tech companies sometimes produce surprisingly sloppy code. If you haven’t worked at a big company, it might be hard to understand how this happens. Big tech companies pay well enough to attract many competent engineers. They move slowly enough that it looks like they’re able to take their time and do solid work. How does bad code happen?&lt;/p&gt;
    &lt;head rend="h3"&gt;Most code changes are made by relative beginners&lt;/head&gt;
    &lt;p&gt;I think the main reason is that big companies are full of engineers working outside their area of expertise. The average big tech employee stays for only a year or two1. In fact, big tech compensation packages are typically designed to put a four-year cap on engineer tenure: after four years, the initial share grant is fully vested, causing engineers to take what can be a 50% pay cut. Companies do extend temporary yearly refreshes, but it obviously incentivizes engineers to go find another job where they don’t have to wonder if they’re going to get the other half of their compensation each year.&lt;/p&gt;
    &lt;p&gt;If you count internal mobility, it’s even worse. The longest I have ever stayed on a single team or codebase was three years, near the start of my career. I expect to be re-orged at least every year, and often much more frequently.&lt;/p&gt;
    &lt;p&gt;However, the average tenure of a codebase in a big tech company is a lot longer than that. Many of the services I work on are a decade old or more, and have had many, many different owners over the years. That means many big tech engineers are constantly “figuring it out”. A pretty high percentage of code changes are made by “beginners”: people who have onboarded to the company, the codebase, or even the programming language in the past six months.&lt;/p&gt;
    &lt;head rend="h3"&gt;Old hands&lt;/head&gt;
    &lt;p&gt;To some extent, this problem is mitigated by “old hands”: engineers who happen to have been in the orbit of a particular system for long enough to develop real expertise. These engineers can give deep code reviews and reliably catch obvious problems. But relying on “old hands” has two problems.&lt;/p&gt;
    &lt;p&gt;First, this process is entirely informal. Big tech companies make surprisingly little effort to develop long-term expertise in individual systems, and once they’ve got it they seem to barely care at all about retaining it. Often the engineers in question are moved to different services, and have to either keep up their “old hand” duties on an effectively volunteer basis, or abandon them and become a relative beginner on a brand new system.&lt;/p&gt;
    &lt;p&gt;Second, experienced engineers are always overloaded. It is a busy job being one of the few engineers who has deep expertise on a particular service. You don’t have enough time to personally review every software change, or to be actively involved in every decision-making process. Remember that you also have your own work to do: if you spend all your time reviewing changes and being involved in discussions, you’ll likely be punished by the company for not having enough individual output.&lt;/p&gt;
    &lt;head rend="h3"&gt;The median productive engineer&lt;/head&gt;
    &lt;p&gt;Putting all this together, what does the median productive2 engineer at a big tech company look like? They are usually:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;competent enough to pass the hiring bar and be able to do the work, but either&lt;/item&gt;
      &lt;item&gt;working on a codebase or language that is largely new to them, or&lt;/item&gt;
      &lt;item&gt;trying to stay on top of a flood of code changes while also juggling their own work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They are almost certainly working to a deadline, or to a series of overlapping deadlines for different projects. In other words, they are trying to do their best in an environment that is not set up to produce quality code.&lt;/p&gt;
    &lt;p&gt;That’s how “obviously” bad code happens. For instance, a junior engineer picks up a ticket for an annoying bug in a codebase they’re barely familiar with. They spend a few days figuring it out and come up with a hacky solution. One of the more senior “old hands” (if they’re lucky) glances over it in a spare half-hour, vetoes it, and suggests something slightly better that would at least work. The junior engineer implements that as best they can, tests that it works, it gets briefly reviewed and shipped, and everyone involved immediately moves on to higher-priority work. Five years later somebody notices this3 and thinks “wow, that’s hacky - how did such bad code get written at such a big software company”?&lt;/p&gt;
    &lt;head rend="h3"&gt;Big tech companies are fine with this&lt;/head&gt;
    &lt;p&gt;I have written a lot about the internal tech company dynamics that contribute to this. Most directly, in Seeing like a software company I argue that big tech companies consistently prioritize internal legibility - the ability to see at a glance who’s working on what and to change it at will - over productivity. Big companies know that treating engineers as fungible and moving them around destroys their ability to develop long-term expertise in a single codebase. That’s a deliberate tradeoff. They’re giving up some amount of expertise and software quality in order to gain the ability to rapidly deploy skilled engineers onto whatever the problem-of-the-month is.&lt;/p&gt;
    &lt;p&gt;I don’t know if this is a good idea or a bad idea. It certainly seems to be working for the big tech companies, particularly now that “how fast can you pivot to something AI-related” is so important. But if you’re doing this, then of course you’re going to produce some genuinely bad code. That’s what happens when you ask engineers to rush out work on systems they’re unfamiliar with.&lt;/p&gt;
    &lt;p&gt;Individual engineers are entirely powerless to alter this dynamic. This is particularly true in 2025, when the balance of power has tilted away from engineers and towards tech company leadership. The most you can do as an individual engineer is to try and become an “old hand”: to develop expertise in at least one area, and to use it to block the worst changes and steer people towards at least minimally-sensible technical decisions. But even that is often swimming against the current of the organization, and if inexpertly done can cause you to get PIP-ed or worse.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pure and impure engineering&lt;/head&gt;
    &lt;p&gt;I think a lot of this comes down to the distinction between pure and impure software engineering. To pure engineers - engineers working on self-contained technical projects, like a programming language - the only explanation for bad code is incompetence. But impure engineers operate more like plumbers or electricians. They’re working to deadlines on projects that are relatively new to them, and even if their technical fundamentals are impeccable, there’s always something about the particular setup of this situation that’s awkward or surprising. To impure engineers, bad code is inevitable. As long as the overall system works well enough, the project is a success.&lt;/p&gt;
    &lt;p&gt;At big tech companies, engineers don’t get to decide if they’re working on pure or impure engineering work. It’s not their codebase! If the company wants to move you from working on database infrastructure to building the new payments system, they’re fully entitled to do that. The fact that you might make some mistakes in an unfamiliar system - or that your old colleagues on the database infra team might suffer without your expertise - is a deliberate tradeoff being made by the company, not the engineer.&lt;/p&gt;
    &lt;p&gt;It’s fine to point out examples of bad code at big companies. If nothing else, it can be an effective way to get those specific examples fixed, since execs usually jump at the chance to turn bad PR into good PR. But I think it’s a mistake4 to attribute primary responsibility to the engineers at those companies. If you could wave a magic wand and make every engineer twice as strong, you would still have bad code, because almost nobody can come into a brand new codebase and quickly make changes with zero mistakes. The root cause is that most big company engineers are forced to do most of their work in unfamiliar codebases.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;I struggled to find a good original source on this. There’s a 2013 PayScale report citing a 1.1 year median turnover at Google, which seems low.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Many engineers at big tech companies are not productive, but that’s a post all to itself. I don’t want to get into it here for two reasons. First, I think competent engineers produce enough bad code that it’s fine to be a bit generous and just scope the discussion to them. Second, even if an incompetent engineer wrote the code, there’s almost always competent engineers who could have reviewed it, and the question of why that didn’t happen is still interesting.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The example I’m thinking of here is not the recent GitHub Actions one, which I have no first-hand experience of. I can think of at least ten separate instances of this happening to me.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In my view, mainly a failure of imagination: thinking that your own work environment must be pretty similar to everyone else’s.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News. Here's a preview of a related post that shares tags with this one.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;How I influence tech company politics as a staff software engineer&lt;/p&gt;&lt;p&gt;Many software engineers are fatalistic about company politics. They believe that it’s pointless to get involved, because:&lt;/p&gt;&lt;p&gt;The general idea here is that software engineers are simply not equipped to play the game at the same level as real political operators. This is true! It would be a terrible mistake for a software engineer to think that you ought to start scheming and plotting like you’re in Game of Thrones. Your schemes will be immediately uncovered and repurposed to your disadvantage and other people’s gain. Scheming takes practice and power, and neither of those things are available to software engineers.&lt;/p&gt;&lt;lb/&gt;Continue reading...&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46082223</guid><pubDate>Fri, 28 Nov 2025 19:59:32 +0000</pubDate></item><item><title>Electron vs. Tauri</title><link>https://www.dolthub.com/blog/2025-11-13-electron-vs-tauri/</link><description>&lt;doc fingerprint="a50a94511423a794"&gt;
  &lt;main&gt;
    &lt;p&gt;The Dolt Workbench is an open-source SQL workbench supporting MySQL, Postgres, Dolt, and Doltgres databases. We built the workbench using Electron, which is a popular framework that allows you to convert web apps built with traditional web technologies like HTML, CSS, and Javascript to desktop applications. Since the workbench shares much in common with DoltHub and Hosted Dolt, the architecture is very similar to those products. That is, the workbench uses Next.js for the frontend with an additional GraphQL layer that handles database interactions. For this reason, it made a lot of sense to use Electron to get the desktop version of our application up and running.&lt;/p&gt;
    &lt;p&gt;That said, Electron comes with a few rather significant drawbacks, and those drawbacks have started to become more apparent as the workbench has matured. Because of this, I spent some time exploring Tauri, a newer framework that supports the same web-to-desktop use case as Electron. In this article, weâll discuss how well Electron and Tauri integrate with the workbench, and weigh some pros and cons between the two frameworks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next.js Support#&lt;/head&gt;
    &lt;p&gt;Next.js doesnât translate very cleanly to a desktop application context. This is primarily due to the frameworkâs architecture around server-side rendering and API routing features. In a desktop app, thereâs no application server interacting with a client; we just need to render HTML, CSS, and JavaScript in a window. For these reasons, Electron only loosely supports Next.js applications. Thatâs not to say you canât build an Electron app with Next.js, but it requires some workarounds to make it function properly. One of the more popular workarounds is a project called Nextron, which aims to wire Next.js applications to the Electron framework and streamline the build process. This is the project we use for the workbench. The issue is that, at the time of writing, it appears that Nextron is no longer being maintained, and we started hitting a few bugs with it.&lt;/p&gt;
    &lt;p&gt;Tauri is largely frontend-framework agnostic. For Next, specifically, you still canât use the server-side features, but Tauri makes the integration process much simpler by relying on Nextâs static-site generation capabilities. To make a Next app work with Tauri, you just need to set &lt;code&gt;output: 'export'&lt;/code&gt; in your Next configuration file, and Tauri handles the rest.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Webview#&lt;/head&gt;
    &lt;p&gt;The biggest difference between Electron and Tauri comes from how they render the UI. The Electron framework comes with a full Chromium browser engine bundled in your application, which is the same engine that backs Google Chrome. This is useful because it means you donât have to worry about browser compatibility issues. Regardless of the end userâs machine or architecture, the same Chromium instance renders your application UI. This results in a very standardized experience that ensures your app will look the same regardless of where itâs running. However, this also results in a fair amount of bloat. For the vast majority of desktop apps, a full Chromium browser engine is overkill. Even the simplest âHello Worldâ applications using Electron can run you up to 150 megabytes of disk space.&lt;/p&gt;
    &lt;p&gt;Tauri solves this problem by leveraging the systemâs native webview. Instead of bundling a full browser engine, Tauri uses a library called WRY, which provides a cross-platform interface to the appropriate webview for the operating system. As youâd expect, this makes Tauri apps far more lightweight. The downside here is that you no longer have a hard guarantee on compatibility. From what I can tell, however, this mostly seems to be a non-issue. Compatibility issues across system webviews are exceedingly rare, especially for the major operating systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Node.js vs. Rust#&lt;/head&gt;
    &lt;p&gt;Another major difference between the two frameworks is how they handle the âmainâ process. This refers to the backend process that orchestrates the application windows, menus, and other components of a desktop app that require interaction with system APIs. In Electron, the main process runs in a Node.js environment. This means you get access to all the typical Node APIs, you can import things like normal, and, perhaps most importantly, you can write your Electron-specific code in pure JavaScript. This is a huge bonus for Electronâs target audience: web developers.&lt;/p&gt;
    &lt;p&gt;Tauri, on the other hand, uses Rust. All the framework code and the main process entrypoint are written in Rust. Obviously, this makes it a bit less accessible to the average web developer. That said, Tauri provides a fairly robust set of JavaScript APIs to interact with the Rust layer. For most applications, these APIs will be sufficient to do what you need to do. In the case of the workbench, I was able to fully replicate the functionality of the Electron version using the JavaScript APIs and some minimal Rust code.&lt;/p&gt;
    &lt;p&gt;In my experience, I found the Tauri APIs to fit more naturally in our application code. With Electron, if you need the main process to do something, you must always use inter-process communication, even for the simplest of tasks. If you want to write to a file on the host machine, for instance, your frontend needs to send a signal to the Electron main process, which will then spawn a new process and run the function you wrote that performs the write. With Tauri, you can just use Tauriâs filesystem API directly in your application code. Under the hood, the same sort of IPC pattern is happening, but I think the Tauri abstraction is a bit nicer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sidecars#&lt;/head&gt;
    &lt;p&gt;Since Electron runs on Node.js, it also bundles a full Node.js runtime with your application. This comes with some pros and cons. For the workbench, specifically, this is beneficial because the GraphQL layer is itself a separate Node.js application that needs to run alongside the frontend. Since Electron ships with Node.js, this means we can directly spin up the GraphQL server from the Electron main process using the Node runtime. This eliminates a lot of the headache associated with bundling and running a typical sidecar process. For instance, our app also ships with a copy of Dolt, which allows users to start up local Dolt servers directly from the workbench. To make this work, we have to bundle the appropriate Dolt binary with each workbench release that corresponds to the correct architecture. Without the Node runtime, weâd have to do something similar for the GraphQL layer.&lt;/p&gt;
    &lt;p&gt;With Tauri, this is exactly the problem we run into. To get around it, we need to compile the GraphQL server into a binary using a tool like &lt;code&gt;pkg&lt;/code&gt;, then run it as a sidecar the same way we run Dolt. Thankfully, this seems to be a fairly common use case for Tauri applications, and they have a useful guide on how to run Node.js apps as a sidecar.&lt;/p&gt;
    &lt;p&gt;Itâs also worth mentioning that the full Node.js runtime is quite heavy, which also contributes to bloated Electron app sizes. After building the workbench using both Electron and Tauri, the difference in size was substantial. The left is the Electron version and the right is Tauri:&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations#&lt;/head&gt;
    &lt;p&gt;After replicating the workbenchâs functionality in Tauri, weâre holding off on making the full transition for a couple reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lack of support for .appx and .msix bundles on Windows - Currently, Tauri only support .exe and .msi bundles on Windows. This means your Microsoft Store entry will only link to the unpacked application. The workbench is currently bundled and published using the .appx format. To address this, we would need to take down the workbench entirely from the Microsoft store and create a new application that uses the .exe format.&lt;/item&gt;
      &lt;item&gt;Issues with MacOS universal binaries - This is more an annoyance than a bug, but I ran into a few issues related to codesigning universal binaries for MacOS. Namely, Tauri doesnât seem to be able to create Mac universal binaries from their arm64 and x64 subcomponents. It also seems to be codesigning the Mac builds twice.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Neither of these are hard blockers, but theyâre annoying enough that Iâm holding off on migrating until theyâre resolved or our issues with Nextron become more problematic. For now, Iâm leaving my branch with the migration open and hope to revisit soon. If youâre on the Tauri team, let us know if you have solutions!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;Overall, Iâm impressed with Tauri. It eliminates much of the classic Electron bloat and integrates naturally with our existing codebase. If youâre curious about Tauri or the Dolt Workbench, let us know on Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46082291</guid><pubDate>Fri, 28 Nov 2025 20:06:30 +0000</pubDate></item><item><title>Flight disruption warning as Airbus requests modifications to 6k planes</title><link>https://www.bbc.com/news/live/cvg4y6g74ert</link><description>&lt;doc fingerprint="63a2ff552af5b578"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;How does solar radiation affect aircraft electronics?published at 02:11 GMT&lt;/head&gt;
    &lt;p&gt;Former Qantas captain Dr Ian Getley, who holds a PHD in cosmic and solar radiation in aviation, says flights can be affected by coronal mass ejections (CME), which is when plasma is ejected from the sun into space.&lt;/p&gt;
    &lt;p&gt;The higher the severity of the CME, the more likely it is that issues could arise with satellites and aircraft electronics above 28,000 ft (8.5 km), he tell us.&lt;/p&gt;
    &lt;p&gt;A CME releases heavily charged particles that shoot into the Earth's atmosphere.&lt;/p&gt;
    &lt;p&gt;These create more charged particles in the upper atmosphere, which in turn can interfere with aircraft electronics.&lt;/p&gt;
    &lt;p&gt;The former pilot says his research began after a 2003 flight between LA and New York, where he experienced this phenomenon first hand.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46082825</guid><pubDate>Fri, 28 Nov 2025 21:16:46 +0000</pubDate></item><item><title>Airbus A320 – intense solar radiation may corrupt data critical for flight</title><link>https://www.airbus.com/en/newsroom/press-releases/2025-11-airbus-update-on-a320-family-precautionary-fleet-action</link><description>&lt;doc fingerprint="359e27eeea79718f"&gt;
  &lt;main&gt;
    &lt;p&gt;Toulouse, France, 28 November 2025 – Analysis of a recent event involving an A320 Family aircraft has revealed that intense solar radiation may corrupt data critical to the functioning of flight controls.&lt;/p&gt;
    &lt;p&gt;Airbus has consequently identified a significant number of A320 Family aircraft currently in-service which may be impacted.&lt;/p&gt;
    &lt;p&gt;Airbus has worked proactively with the aviation authorities to request immediate precautionary action from operators via an Alert Operators Transmission (AOT) in order to implement the available software and/or hardware protection, and ensure the fleet is safe to fly. This AOT will be reflected in an Emergency Airworthiness Directive from the European Union Aviation Safety Agency (EASA).&lt;/p&gt;
    &lt;p&gt;Airbus acknowledges these recommendations will lead to operational disruptions to passengers and customers. We apologise for the inconvenience caused and will work closely with operators, while keeping safety as our number one and overriding priority.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contacts&lt;/head&gt;
    &lt;head rend="h3"&gt;Guillaume Steuer&lt;/head&gt;
    &lt;p&gt;AIRBUS&lt;/p&gt;
    &lt;head rend="h3"&gt;Sara Ricci&lt;/head&gt;
    &lt;p&gt;AIRBUS | Commercial Aircraft&lt;/p&gt;
    &lt;head rend="h3"&gt;Justin Dubon&lt;/head&gt;
    &lt;p&gt;AIRBUS | Commercial Aircraft&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46083004</guid><pubDate>Fri, 28 Nov 2025 21:40:21 +0000</pubDate></item><item><title>A first look at Django's new background tasks</title><link>https://roam.be/notes/2025/a-first-look-at-djangos-new-background-tasks/</link><description>&lt;doc fingerprint="7f08801f19f010ee"&gt;
  &lt;main&gt;
    &lt;p&gt;Django 6.0 introduces a built-in background tasks framework in &lt;code&gt;django.tasks&lt;/code&gt;. But don't expect to phase out Celery, Huey or other preferred solutions just yet. &lt;/p&gt;
    &lt;p&gt;The release notes are quite clear on this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Django handles task creation and queuing, but does not provide a worker mechanism to run tasks. Execution must be managed by external infrastructure, such as a separate process or service.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The main purpose of the new &lt;code&gt;django.tasks&lt;/code&gt; module is to provide a common API for task queues implementations. Jake Howard is the driving force behind this enhancement. Check out the introduction on the Django forum.&lt;/p&gt;
    &lt;p&gt;His reference implementation, and simultaneously a backport for earlier versions of Django, is available as &lt;code&gt;django-tasks&lt;/code&gt; on GitHub.&lt;/p&gt;
    &lt;p&gt;But let's ignore that and play with the more minimal version included in Django 6.0 instead. By creating our very own backend and worker.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our project: notifications&lt;/head&gt;
    &lt;p&gt;We're going to create an app to send notifications to phones and other devices using ntfy.sh. (I'm a fan!)&lt;/p&gt;
    &lt;p&gt;If you prefer to dive into the code yourself, check out the final version of the project on GitHub.&lt;/p&gt;
    &lt;p&gt;All that's required to send a notification to your phone using nfty is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Register for an account&lt;/item&gt;
      &lt;item&gt;Create a topic.&lt;/item&gt;
      &lt;item&gt;Install the app for your phone and log in.&lt;/item&gt;
      &lt;item&gt;Send HTTP requests to &lt;code&gt;https://ntfy.sh/&amp;lt;yourtopic&amp;gt;&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The free version only provides public topics and messages. Meaning anyone can see the stuff you're sending if they subscribe to the topic. For our purpose we can simply create a topic with a randomized name, like a UUID.&lt;/p&gt;
    &lt;p&gt;The project's settings expect the URL from step 4 to be supplied as an environment variable. For example:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;NTFY_URL=https://ntfy.sh/062519693d9c4913826f0a39aeea8a4c&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Here's our function that does the heavy lifting:&lt;/p&gt;
    &lt;code&gt;import httpx
from django.conf import settings

def send_notification(message: str, title: str | None):
    # Pass the title if specified.
    headers = {"title": title} if title else {}
    httpx.post(
        settings.NTFY_URL,
        content=message,
        headers=headers,
    )
&lt;/code&gt;
    &lt;p&gt;Really. That's all there is to it to start sending and receiving notifications.&lt;/p&gt;
    &lt;head rend="h2"&gt;A quick primer&lt;/head&gt;
    &lt;p&gt;You really should have a look at the Django documentation on the Task framework for details, but we'll save you a bit of time and give a quick primer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Defining a task&lt;/head&gt;
    &lt;p&gt;This is the main goal of the new framework: defining tasks using Django's standard API, rather than using task queue specific decorators, or other methods.&lt;/p&gt;
    &lt;p&gt;So here it goes:&lt;/p&gt;
    &lt;code&gt;# ...
from django.tasks import task

@task
def send_notification(message: str, title: str | None):
    # ...as before
&lt;/code&gt;
    &lt;p&gt;Our function is now a task. In fact it's a &lt;code&gt;django.tasks.Task&lt;/code&gt;. &lt;/p&gt;
    &lt;p&gt;You cannot call &lt;code&gt;send_notification&lt;/code&gt; directly anymore. Tasks can only be run by using the &lt;code&gt;enqueue&lt;/code&gt; method. It might not be the behavior you'd expect or want, but this seems to be the best option. This design eliminates the possibility of accidentally invoking a task in-process, rather than in the background.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;task&lt;/code&gt; decorator allows you to specify the task's priority, queue name and backend name. You can override these settings with the &lt;code&gt;using&lt;/code&gt; method, which returns a new &lt;code&gt;django.tasks.Task&lt;/code&gt; instance.&lt;/p&gt;
    &lt;p&gt;If you need more control over task behavior, you can set &lt;code&gt;takes_context&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; in the decorator and add &lt;code&gt;context&lt;/code&gt; as the first argument. This context currently provides you with access to the task result and thereby useful information like the number of attempts. &lt;/p&gt;
    &lt;p&gt;There's no way of defining retries and backoffs, or other fancy things you might expect from a full-blown task queue implementation. But that's not what this is. You can easily add your own retry logic by inspecting the task context if needed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enqueuing a task&lt;/head&gt;
    &lt;p&gt;Adding a task to the queue is easy:&lt;/p&gt;
    &lt;code&gt;task_result = send_notification.enqueue(
    message="Season's greeting!", 
    title="Santa has something to tell you"
)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Executing a task&lt;/head&gt;
    &lt;p&gt;This is where things start to fall short. At least right now. Django 6.0 will ship with the &lt;code&gt;ImmediateBackend&lt;/code&gt; and the &lt;code&gt;DummyBackend&lt;/code&gt;. The first will execute the task immediately, while the latter will not execute the task at all. &lt;/p&gt;
    &lt;p&gt;Which is why our project includes a (demo) backend backed by the database and a worker process!&lt;/p&gt;
    &lt;head rend="h3"&gt;Fetching the result&lt;/head&gt;
    &lt;p&gt;If you're not going to wait around for the result, you can get a hold of it later on using its id. Simply call &lt;code&gt;get_result(result_id)&lt;/code&gt; on your task. &lt;/p&gt;
    &lt;p&gt;Our project includes a view that's polled periodically for outstanding results using htmx.&lt;/p&gt;
    &lt;p&gt;The list underneath the form shows the results for each execution of our task. When the form's submitted, a new result is added to the top of the list. Htmx is instructed to keep polling for changes as long as the result's status isn't &lt;code&gt;FAILED&lt;/code&gt; or &lt;code&gt;SUCCESSFUL&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;def task_result(request, result_id, status):
    result = send_notification.get_result(result_id)
    if result.status == status:
        # No need to swap the result.
        return HttpResponse(status=204)
    return TemplateResponse(request, "index.html#result", {"result": result})
&lt;/code&gt;
    &lt;p&gt;Wondering what &lt;code&gt;index.html#results&lt;/code&gt; is doing? Django 6.0 also introduces template partials. In this case our view effectively sends a response containing only the template partial named &lt;code&gt;result&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Behind the scenes&lt;/head&gt;
    &lt;p&gt;When you decorate a callable with &lt;code&gt;task&lt;/code&gt;, the configured backend's &lt;code&gt;task_class&lt;/code&gt; is used to wrap the callable. The default's &lt;code&gt;django.task.Task&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;That class's &lt;code&gt;enqueue&lt;/code&gt; method will in turn invoke the configured backend's &lt;code&gt;enqueue&lt;/code&gt; method.&lt;/p&gt;
    &lt;p&gt;Calling its &lt;code&gt;get_result&lt;/code&gt; method is similar: call the configured backend's &lt;code&gt;get_result&lt;/code&gt; method and pass on the result.&lt;/p&gt;
    &lt;p&gt;Since there's no workers, that's basically all a task backend needs to provide. Cool. Let's add one, shall we?&lt;/p&gt;
    &lt;head rend="h2"&gt;A task database backend&lt;/head&gt;
    &lt;p&gt;Our goals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A basic task backend, backed by our database.&lt;/item&gt;
      &lt;item&gt;We want to support "automagic" retries&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our &lt;code&gt;enqueue&lt;/code&gt; and &lt;code&gt;get_result&lt;/code&gt; methods will return an instance of the default &lt;code&gt;django.tasks.TaskResult&lt;/code&gt;. This determines the minimum amount of data we need to store, and we're going to do so in a model called &lt;code&gt;Task&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Models&lt;/head&gt;
    &lt;p&gt;Let's create a first draft of our &lt;code&gt;Task&lt;/code&gt; model, based on the properties of &lt;code&gt;TaskResult&lt;/code&gt; and &lt;code&gt;Task&lt;/code&gt; in &lt;code&gt;django.tasks&lt;/code&gt; (the "dataclasses"):&lt;/p&gt;
    &lt;code&gt;class Task(models.Model):
    priority = models.IntegerField(default=0)
    callable_path = models.CharField(max_length=255)
    backend = models.CharField(max_length=200)
    queue_name = models.CharField(max_length=100)
    run_after = models.DateTimeField(null=True, blank=True)
    takes_context = models.BooleanField(default=False)
    # Stores args and kwargs
    arguments = models.JSONField(null=True, blank=True)
    status = models.CharField(
        choices=TaskResultStatus.choices, max_length=10, default=TaskResultStatus.READY
    )
    enqueued_at = models.DateTimeField()
    started_at = models.DateTimeField(blank=True, null=True)
    finished_at = models.DateTimeField(blank=True, null=True)
    last_attempted_at = models.DateTimeField(blank=True, null=True)
    return_value = models.JSONField(null=True, blank=True)
&lt;/code&gt;
    &lt;p&gt;What's missing? For one, the &lt;code&gt;TaskResult&lt;/code&gt; also includes a list of encountered errors, and ids of the workers that processed the task. Something that we could perhaps ignore. &lt;/p&gt;
    &lt;p&gt;Except the &lt;code&gt;TaskResult.attempts&lt;/code&gt; property is based on the number of worker ids. And if you're using the task context within a task, you're bound to be relying on that type of information.&lt;/p&gt;
    &lt;p&gt;We could add these details to the &lt;code&gt;Task&lt;/code&gt; model by adding a &lt;code&gt;JSONField&lt;/code&gt; for each. This is the current approach in the reference implementation. &lt;/p&gt;
    &lt;p&gt;But let's be more explicit in our approach and define models for these as well. We'll record each attempt to execute a task and its potential error, linking them to the task with a foreign key:&lt;/p&gt;
    &lt;code&gt;class Error(models.Model):
    exception_class_path = models.TextField()
    traceback = models.TextField()

class AttemptResultStatus(TextChoices):
    # Subset of TaskResultStatus.
    FAILED = TaskResultStatus.FAILED
    SUCCESSFUL = TaskResultStatus.SUCCESSFUL

class Attempt(models.Model):
    task = models.ForeignKey(Task, related_name="attempts", on_delete=models.CASCADE)
    error = models.OneToOneField(
        Error, related_name="attempt", on_delete=models.CASCADE, null=True, blank=True
    )
    worker_id = models.CharField(max_length=MAX_LENGTH_WORKER_ID)
    started_at = models.DateTimeField()
    stopped_at = models.DateTimeField(blank=True, null=True)
    status = models.CharField(
        choices=AttemptResultStatus.choices, max_length=10, blank=True
    )
&lt;/code&gt;
    &lt;p&gt;This setup ensures we have all necessary information to execute a task, plus we can provide every single bit of detail when a &lt;code&gt;TaskResult&lt;/code&gt; is requested.&lt;/p&gt;
    &lt;p&gt;All fine and dandy, but we need to think about the worker's requirements as well. It needs to be able to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Quickly check for outstanding tasks&lt;/item&gt;
      &lt;item&gt;Claim one of those tasks&lt;/item&gt;
      &lt;item&gt;Process that task and either mark it as failed, successful or ready (to retry later)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We could do all of that with how it's set up right now, but I'd like to refine things a bit.&lt;/p&gt;
    &lt;code&gt;class Task(models.Model):
    # ...
    # This field is used to keep track of when to run a task (again).
    # run_after remains unchanged after enqueueing.
    available_after = models.DateTimeField()
    # Denormalized count of attempts.
    attempt_count = models.IntegerField(default=0)
    # Set when a worker starts processing this task.
    worker_id = models.CharField(max_length=MAX_LENGTH_WORKER_ID, blank=True)
    # ...
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;available_after&lt;/code&gt; field will contain the earliest time at which the task can be executed. If the task's &lt;code&gt;run_after&lt;/code&gt; is specified (which can be done by using a task's... &lt;code&gt;using()&lt;/code&gt; method), &lt;code&gt;available_after&lt;/code&gt; is set to that value. Otherwise we're using the current datetime; all in UTC.&lt;/p&gt;
    &lt;p&gt;Once a task needs to be retried, &lt;code&gt;available_after&lt;/code&gt; will be set to the next possible point in time the task can be executed. In other words: we can back off.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;attempt_count&lt;/code&gt; field makes querying for available tasks a bit easier. Any tasks with an &lt;code&gt;attempt_count&lt;/code&gt; greater than the maximum allowed value can be ignored. Yes, their status should have been set to &lt;code&gt;FAILED&lt;/code&gt; which means they should be excluded by default, but we could change the configuration and tweak the maximum number of attempts.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;worker_id&lt;/code&gt; field is filled when a worker claims a task. This, among other things, prevents any other workers from picking up the task. Assuming the worker id is unique.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enqueueing and fetching a result&lt;/head&gt;
    &lt;p&gt;Enqueueing a task could not be easier: create a &lt;code&gt;Task&lt;/code&gt; model instance from the &lt;code&gt;Task&lt;/code&gt; dataclass instance, save it, done! Well, at least after turning the end result into a &lt;code&gt;TaskResult&lt;/code&gt;. &lt;/p&gt;
    &lt;p&gt;We use the string version of the model's database id as the id of the result.&lt;/p&gt;
    &lt;p&gt;Retrieving a result is likewise only a matter of loading the task and its attempts, and turning that into a &lt;code&gt;TaskResult&lt;/code&gt;. &lt;/p&gt;
    &lt;p&gt;Here's a simplified version of our task backend as it stands:&lt;/p&gt;
    &lt;code&gt;class DatabaseBackend(BaseTaskBackend):
    supports_defer = True
    supports_async_task = False
    supports_get_result = True
    supports_priority = True

    def enqueue(self, task: Task, args, kwargs):
        self.validate_task(task)
        model = self.queue_store.enqueue(task, args, kwargs)
        task_result = TaskResult(
            task=task,
            id=str(model.pk),
            # ...
            # More properties being set
            # ...
        )
        return task_result

    def get_result(self, result_id):
        return self.model_to_result(
            self.queue_store.get(result_id)
        )

    def model_to_result(self, model: models.Task) -&amp;gt; TaskResult:
        ...
&lt;/code&gt;
    &lt;p&gt;At lot of functionality is deferred to this &lt;code&gt;queue_store&lt;/code&gt; property. Before we dive into that, we'll explain the configuration options for this backend.&lt;/p&gt;
    &lt;head rend="h3"&gt;Configuration&lt;/head&gt;
    &lt;p&gt;We want to be able to specify defaults for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the maximum number of attempts (retries)&lt;/item&gt;
      &lt;item&gt;the backoff factor; i.e. we'll back off using &lt;code&gt;math.pow(factor, attempts)&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These can be customized for each individual queue. So we end up with something like this in our &lt;code&gt;OPTIONS&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;TASKS = {
    "default": {
        "BACKEND": "messagecenter.dbtasks.backend.DatabaseBackend",
        "OPTIONS": {
            "queues": {
                "low_priority": {
                    "max_attempts": 5,
                }
            },
            "max_attempts": 10,
            "backoff_factor": 3,
            "purge": {"finished": "10 days", "unfinished": "20 days"},
        },
    }
}
&lt;/code&gt;
    &lt;p&gt;A task added to the &lt;code&gt;low_priority&lt;/code&gt; queue will be attempted up to five times, with a backoff factor of &lt;code&gt;3&lt;/code&gt;. Other tasks will be attempted up to ten times with the same backoff factor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Queue store&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;QueueStore&lt;/code&gt; class is a companion of our backend. It's focus is on retrieving and enqueueing tasks, checking for tasks to execute and claiming tasks. &lt;/p&gt;
    &lt;p&gt;However the main reason it's included is to simplify the worker. As we'll see, the worker gets it's own copy of the queue store, limited to the queues it needs to process.&lt;/p&gt;
    &lt;head rend="h3"&gt;The worker&lt;/head&gt;
    &lt;p&gt;The worker's job, at least in this project, is to provide information on outstanding tasks to the runner and to drive the processing of those tasks by the backend. Which means it looks like this:&lt;/p&gt;
    &lt;code&gt;class Worker:
    def __init__(
        self,
        id_: str | None,
        backend_name: str,
        only: set[str] | None,
        excluding: set[str] | None,
    ):
        # Grab the backend and its queue_store.
        self.backend = task_backends[backend_name]
        queue_store: QueueStore = self.backend.queue_store
        # Limit the queue_store to the select queues.
        if only or excluding:
            queue_store = queue_store.subset(only=only, excluding=excluding)
        self.queue_store = queue_store
        # Use or create and id. "Must" be unique.
        self.id = (
            id_ if id_ else create_id(backend_name, queues=queue_store.queue_names)
        )

    def has_more(self) -&amp;gt; bool:
        return self.queue_store.has_more()

    def process(self):
        with transaction.atomic():
            tm = self.queue_store.claim_first_available(worker_id=self.id)
        if tm is not None:
            self.backend.process_task(tm)
&lt;/code&gt;
    &lt;p&gt;All we need to do to have a functioning worker runner:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create an instance of the worker.&lt;/item&gt;
      &lt;item&gt;Ask it if there's tasks to execute using &lt;code&gt;has_more&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If so: tell it to &lt;code&gt;process&lt;/code&gt;the first available task. If not: go to 4.&lt;/item&gt;
      &lt;item&gt;Wait, then return to 2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's what our &lt;code&gt;dbtasks_worker&lt;/code&gt; command does.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claiming a task&lt;/head&gt;
    &lt;p&gt;Our queue store provides a &lt;code&gt;peek&lt;/code&gt; method which returns the id of the task in our queues with the most urgency; a combination of &lt;code&gt;available_after&lt;/code&gt;, &lt;code&gt;priority&lt;/code&gt; and &lt;code&gt;attempt_count&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This lets the runner know whether there's more tasks to process. The next step is to claim one of those tasks. So we call &lt;code&gt;peek&lt;/code&gt; again and if it returns a task id, we'll try to claim that particular task.&lt;/p&gt;
    &lt;p&gt;Here's a more basic, clearer version than the one included in our project's &lt;code&gt;QueueStore&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;def claim_first_available(
    self, worker_id: str, attempts: int = 3
) -&amp;gt; models.Task | None:
    qs = models.Task.filter(
        worker_id="", 
        status=TaskResultStatus.READY,
    )
    for _ in range(attempts):
        task_id = self.peek()
        if not task_id:
            return None
        count = qs.filter(pk=task_id).update(
            worker_id=self.id_,
            status=TaskResultStatus.RUNNING,
        )
        if count:
            return models.Task.objects.get(pk=task_id)
    return None
&lt;/code&gt;
    &lt;p&gt;If the &lt;code&gt;count&lt;/code&gt; is zero, we failed to claim the task. Otherwise we retrieve it from the database and can start processing. &lt;/p&gt;
    &lt;p&gt;The loop is included because we ended up here after trying to claim the task identified by &lt;code&gt;peek&lt;/code&gt;. Which apparently has already been picked up by another worker. We might as well make the most of it and try to grab another task from the queue.&lt;/p&gt;
    &lt;head rend="h3"&gt;Processing the task&lt;/head&gt;
    &lt;p&gt;And finally the thing that actually does something!&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;process_task&lt;/code&gt; method of our backend:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Creates an &lt;code&gt;Attempt&lt;/code&gt;and constructs the current&lt;code&gt;TaskResult&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Executes the task, capturing anything extending &lt;code&gt;BaseException&lt;/code&gt;, or returning the&lt;code&gt;return_value&lt;/code&gt;of the task when everything went according to plan.&lt;/item&gt;
      &lt;item&gt;Either updates the &lt;code&gt;Task&lt;/code&gt;model, the&lt;code&gt;Attempt&lt;/code&gt;and the&lt;code&gt;TaskResult&lt;/code&gt;with the final details of the successful execution, or with details about the failure to do so.&lt;/item&gt;
      &lt;item&gt;And in the latter case: check if the task can be retried.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Again: if you want to dive into the details, have a look at the repository.&lt;/p&gt;
    &lt;head rend="h2"&gt;That's it&lt;/head&gt;
    &lt;p&gt;Of course this demo project leaves out all the things you really need to think hard about. Like signals for the worker. Or database transaction logic. That's not to say it's impossible. Far from it. It just wasn't the goal of this article.&lt;/p&gt;
    &lt;p&gt;The inclusion of this functionality in Django will certainly allow new libraries or adapters for existing task queues to pop up. And we'll probably soon see some complaints that &lt;code&gt;django.tasks&lt;/code&gt; isn't extensive enough.&lt;/p&gt;
    &lt;p&gt;Because, if you're currently using the more advanced functionality of your task queue, there's probably a few things you're missing in &lt;code&gt;django.tasks&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complex orchestration&lt;/head&gt;
    &lt;p&gt;Some task queue libraries, like Celery, provide ways of combining tasks. You can feed the result of one task into another, enqueue tasks for each item in a list, and so on.&lt;/p&gt;
    &lt;p&gt;It should be clear by now that supporting this kind of orchestration isn't the goal of &lt;code&gt;django.tasks&lt;/code&gt;. And I don't mind at all. There's no feasible way of creating a unified API to support this. I've had my share of problems with libraries that do claim to support it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Retry&lt;/head&gt;
    &lt;p&gt;As mentioned before, there's currently no way to automatically retry a failed task, unless your backend does the heavy lifting. Like ours does.&lt;/p&gt;
    &lt;p&gt;Depending on your backend this might be easy enough to handle yourself. For example using a decorator:&lt;/p&gt;
    &lt;code&gt;def retry(func):
    @functools.wraps(func)
    def wrapper(context: TaskContext, *args, **kwargs):
        try:
            return func(context, *args, **kwargs)
        except BaseException as e:
            result = context.task_result
            backoff = math.pow(2, result.attempts)
            run_after = datetime.now(tz=UTC) + timedelta(seconds=backoff)
            result.task.using(run_after=run_after).enqueue(*args, **kwargs)
            raise e
    return wrapper


@task(takes_context=True)
@retry
def send_email(context: TaskContext, to: str, subject: str, body: str):
    # Do your thing 
    ...
&lt;/code&gt;
    &lt;head rend="h3"&gt;An actual worker mechanism&lt;/head&gt;
    &lt;p&gt;True. But the reference implementation does provide actual workers. Be patient, or even better: start helping out!&lt;/p&gt;
    &lt;head rend="h2"&gt;There is no perfect solution&lt;/head&gt;
    &lt;p&gt;I reckon &lt;code&gt;django.tasks&lt;/code&gt; will soon result in covering at least the most common 80% of use cases. Yes, its API is simple and limited, but to me that's more a benefit rather than a fault. I think this is as close as you can get to a standardized approach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46083051</guid><pubDate>Fri, 28 Nov 2025 21:46:07 +0000</pubDate></item><item><title>How to Short the Bubbliest Firms</title><link>https://www.economist.com/finance-and-economics/2025/11/26/how-to-short-the-bubbliest-firms</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46083137</guid><pubDate>Fri, 28 Nov 2025 21:56:58 +0000</pubDate></item><item><title>Confessions of a Software Developer: No More Self-Censorship</title><link>https://kerrick.blog/articles/2025/confessions-of-a-software-developer-no-more-self-censorship/</link><description>&lt;doc fingerprint="2f4551170fb65fb1"&gt;
  &lt;main&gt;
    &lt;p&gt;I haven’t published since April because I’ve been afraid. I also avoided social media, news aggregators, and discussion forums for months. I’m done letting fear stop me. What was I afraid of? In this post I detail every single thing I’ve avoided admitting on this blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Knowledge Gap Confessions&lt;/head&gt;
    &lt;p&gt;First, why am I admitting these things now? I realized I am not the only working software developer missing crucial skills. My learning path through my career looked a lot like a slime mold seeking morsels of food: strengthening what has utility, but letting the rest wither. But lately, I’ve been building a better base of knowledge. Writing or speaking about what I learn–which helps me learn better–requires me to admit I didn’t know. Plus, I’d like to show others in my situation that it’s never too late to learn what you don’t know. I can fill in those fundamentals, and so can you.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It’s from that very ignorance that sprouts the drive for knowledge.&lt;/p&gt;
      &lt;p&gt;— Leticia Portella, A Friendly Guide to Software Development&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;I Didn’t Understand Polymorphism For a Decade&lt;/head&gt;
    &lt;p&gt;Learning about polymorphism over the past twelve months was the first time I was embarrassed to admit I didn’t already know something. I’ve been writing ostensibly object-oriented software since 2012. And yet, my lack of awareness of polymorphism showed me I’ve been writing little more than structured programs. That I could replace conditionals and case staments with specialized classes had never crossed my mind.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;As a hiring manager I interviewed software engineers and tried to filter for object-oriented knowledge. Retroactively, it’s clear I was hypocritical. This gap reveals that I spent the early part of my career learning tools, not principles. Plus, it highlights my lack of formal education. Polymorphism is covered in every college OO course.&lt;/p&gt;
    &lt;head rend="h3"&gt;I Forgot SQL&lt;/head&gt;
    &lt;p&gt;I took a college database course as a student. As a working professional, I read and worked through the exercises in Learning SQL, 3rd Edition. For a while, I could write SQL. But I specialized in front-end web development, and had no professional use for SQL. Like any unused skill, it atrophied. I remember how to write basic queries, but not much more. For example, I cannot tell you the difference between a left inner join and an outer join without looking it up.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;I’m not accustomed to forgetting. Growing up I had a remarkable ability to remember almost everything. It didn’t matter whether I did it, read it, or heard it. It could be a fact, a skill, or an event. Four years later I could access that knowledge, with the slightest reminder unlocking a flood of memories. Now that I’m in my mid-thirties, that isn’t always true. SQL is the first time I’ve lost an entire skill to atrophy. It’s tough to come to terms with the start of aging. It’s tougher to admit it publicly.&lt;/p&gt;
    &lt;head rend="h3"&gt;I Don’t Write Automated Tests&lt;/head&gt;
    &lt;p&gt;An estimated 95% of the code I’ve shipped to production had no automated tests. Early in my career, I had no exposure to the concept. Later, I was writing front-ends in Ember, whose testing story was looked good but felt pretty bad at the time. More recently, I’ve been working legacy code, and I haven’t put in the work to make it testable. The only time I tend to write new tests is when I’m writing a new subsystem, which can be designed testable from the start. I’m convinced that writing automated tests needs to be part of my daily practice, but I haven’t gotten there yet.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;This may be my most professionally-damaging confession. If you believe Uncle Bob, shipping production code without tests is not more than risky, it’s unethical. I stopped myself from posting about my learning journey for fear that a future hiring manager would decide that I was unfit to work with them on this basis.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How much of the code should be tested with these automated unit tests? Do I really need to answer that question? All of it! All. Of. It.&lt;/p&gt;
      &lt;p&gt;Am I suggesting 100% test coverage? No, I’m not suggesting it. I’m demanding it. Every single line of code that you write should be tested. Period.&lt;/p&gt;
      &lt;p&gt;Isn’t that unrealistic? Of course not. You only write code because you expect it to get executed. If you expect it to get executed, you ought to know that it works. The only way to know this is to test it.&lt;/p&gt;
      &lt;p&gt;— Robert C. Martin, The Clean Coder, Chapter 1: Professionalism&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Personal Confessions&lt;/head&gt;
    &lt;head rend="h3"&gt;I Didn’t End Up Learning Blazor&lt;/head&gt;
    &lt;p&gt;People have been waiting for a follow-up from me about my journey learning C#, .NET, and Blazor. This isn’t that post. I don’t know if that post will ever come.&lt;/p&gt;
    &lt;p&gt;C# was never the language I wanted to learn for side projects. .NET was never the platform I wanted to work with professionally. I was learning them for one reason: my job. My engineering department decided to switch our tech stack from Angular to Blazor. I was the only person on the team with no C# skills. I started fixing that immediately.&lt;/p&gt;
    &lt;p&gt;A couple months later, almost as suddenly, the decision was undone. Our tech stack would not change after all. With no intrinsic motivation to push me along, I abandoned the C# / .NET book I was reading without finishing. I’ve got more important things to learn.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;No matter what software thoughts crossed my mind, I intended to post about them. Writing helps me solidify ephemeral thoughts. Publishing offers an opportunity for feedback. But I made two errors that locked me into a pattern of fear. First, I promised a follow up article at the end of my last post about the Blazor stack. I then felt worse every time I published an article other than the promised follow-up. Second, I began to see value in the amount of traffic a blog post got. The posts about my first steps in that learning journey were the winners. Admitting I changed tack when the company did felt like admitting defeat.&lt;/p&gt;
    &lt;head rend="h3"&gt;I Want to Write More Ruby&lt;/head&gt;
    &lt;p&gt;I love Ruby. I use it in my code examples. It’s my default language for open source projects. I write Ruby for code katas, etudes, and hackathons. But I haven’t been paid to write Ruby since 2013.&lt;/p&gt;
    &lt;p&gt;The best possible option–and yet the most improbable–would be for my current employer to start a Ruby project. I’ve worked with a few of my teammates for 12 years across two companies. I’ve always chosen to keep working with fantastic people at the cost of working with a less-than-fantastic language.&lt;/p&gt;
    &lt;p&gt;Sadly that means I’m limited to being a Rubyist after work and on weekends. I spend fewer of those hours than I’d like writing Ruby, instead favoring other obligations, hobbies, and professional development goals. The only way I foresee getting to work with Ruby as much as I’d like would be to paid for it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;My manager and his manager, the CTO, read this blog. I found it difficult to write freely about my distaste for the tools I use every day. I found it ever harder to admit that I actively want my daily job duties to be different. I feared they might take it as a hint that I’m quitting (I’m not), or that I’d push to use a tool at work nobody else is familiar with at work (I won’t).&lt;lb/&gt;A bonus, even more personal confession…&lt;/p&gt;
    &lt;head&gt;A bonus, even more personal confession…&lt;/head&gt;
    &lt;head rend="h3"&gt;Cyber Bullying Hurts, Even as an Adult&lt;/head&gt;
    &lt;p&gt;I spent a lot of my young adulthood online. My early days on the internet were spent in intellectual spaces, where the interactions felt like The Nets of Ender’s Game: a marketplace of ideas. Strong criticism came swiftly, but it was about the ideas, not the person. Even Reddit and Hacker News–forums which have a reputation for harsh comments sections–don’t bother me, because the vitriol is aimed at taking down bad ideas, not insulting people.&lt;/p&gt;
    &lt;p&gt;Other websites, though, are different. I learned this when I got bullied on a different online forum. I was called incapable, sneaky, disgusting, incompetent, uncaring, and a representative of a threat to human expression.&lt;/p&gt;
    &lt;p&gt;What triggered this vitriol? I requested a small feature in an open source project, and the maintainer said they would accept a PR. The project was written in a language I haven’t used. I used an LLM to generate a small commit (a few dozen lines), reviewed and tested the patch, and submitted a pull request. This was months ago–there were few social norms around AI-assisted patches, and AI policies were rare. Since the project didn’t have a policy, I did not disclose my use of AI.&lt;/p&gt;
    &lt;p&gt;When I told the story of that pull request on the forum and defended my ethical position, the bullying started. I was followed across websites, contacted via email and SMS, and even called on the phone. I no longer felt safe having a presence on that website. I deleted my comments, removed PII from my profile, and asked the administrators of the forum to scrub my real name to prevent further harassment. Instead, they attached more PII to my profile, locked me out of editing it, and permanently vandalized it with the false claim that I lied about being contacted about the discussion outside the forum.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;This incident was one of the most toxic things I’ve ever experienced, and it lasted for days. Writing about it leaves me feeling its echoes even now. I was afraid that one of these people will use the comments section, or email, or even my phone number, to re-litigate the issue. Even now, I am afraid that the administrator’s (possibly defamatory) statement on my profile will make me less employable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Workplace Confessions&lt;/head&gt;
    &lt;head rend="h3"&gt;Your SaaS Team Doesn’t Need a Special Process&lt;/head&gt;
    &lt;p&gt;Hundreds of companies, thousands of researchers, tens of thousands of workers, and millions of dollars have gone into shaping our industry’s best practices. The agile manifesto is old enough to drink. Software as a Service has dominated the market for over a decade. Your company has a limited innovation budget. Do you want to spend it on coming up with a custom software development lifecycle, or making a product that wins in the marketplace? Follow Scrum, Lean / Kanban, or eXtreme Programming to the letter, and let your team focus on the product.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;Like any author, I write what I know. Here, I was motivated to write because a co-worker pushed to create a custom software development process. I don’t know that I have the tact to avoid it seeming to be a takedown of that person or their ideas. I admire the ability of authors like Kent Beck and Martin Fowler to write about how to work better without calling out coworkers who made mistakes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Remote Work Sucks&lt;/head&gt;
    &lt;p&gt;Remote work eliminates a lot of problems with office work: commutes, inefficient use of real estate, and land value distortion. But software development is better when you breathe the same air as the folks you work with. Even with a camera-on policy, video calls are a low-bandwidth medium. You lose ambient awareness of coworkers’ problems, and asking for help is a bigger burden. Pair programming is less fruitful. Attempts to represent ideas spatially get mutilated by online whiteboard and sticky note software. Even conflict gets worse: it’s easy to form an enemy image of somebody at the end of video call, but difficult to keep that image when you share a room with them and sense their pain.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why I Was Afraid to Admit It&lt;/head&gt;
    &lt;p&gt;When COVID-19 hit, the company I worked for went remote “for a couple of weeks.” After a few months of productive work without an office and with no vaccine in sight, it became permanent. I took the opportunity to move to a rural area. Geographic arbitrage meant I could afford 27 acres, and I even bought a family milking cow. My family has since put down roots: close friendships, community involvement, and a lifestyle built around the lack of a commute.&lt;/p&gt;
    &lt;p&gt;I feared that writing negatively about remote work might jeopardize my current remote job–and every future remote job I might look for. I thought, “who would hire a remote worker who prefers in-office work?” Because even though I prefer working side-by-side with others, I won’t likely move for a job. I have a 30-year mortgage with a low interest rate. My house was purchased before the post-pandemic price spike. I have an acre of lawn &amp;amp; garden, not to mention the farm acreage. I’d need to double my current income to maintain my current lifestyle in a city, which is unlikely.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Now?&lt;/head&gt;
    &lt;p&gt;Now that the dam has burst, nothing is holding me back from publishing. I’m going to continue to work on skill building, but now I feel free to write about it. If this article resonated with you–whether you also have knowledge gaps you’d like to fill, you’d like to help me fill mine, or you just want to see what happens–please let me know in the comments. Subscribe via Mastodon to see everything I post, use RSS to customize your subscription, or subscribe to my mailing list to get notified when I post a larger article.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mastodon &amp;amp; Fediverse&lt;/head&gt;
    &lt;p&gt;To help you follow along, I’ve enabled ActivityPub on this blog, meaning it’s a fully-functioning Mastodon account: &lt;code&gt;@[email protected]&lt;/code&gt;. &lt;/p&gt;
    &lt;head rend="h3"&gt;RSS &amp;amp; Feed Readers&lt;/head&gt;
    &lt;p&gt;If you prefer to kick it old school, grab an XML feed reader like NetNewsWire for MacOS, Feedmill for Windows, or Newsflash for Linux and choose one or more feeds:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The “Everything” feed &lt;list rend="ul"&gt;&lt;item&gt;Just the “Articles” feed &lt;list rend="ul"&gt;&lt;item&gt;Specifically the “Advice” feed&lt;/item&gt;&lt;item&gt;Specifically the “Blog Posts” feed&lt;/item&gt;&lt;item&gt;Specifically the “Book Reviews” feed&lt;/item&gt;&lt;item&gt;Specifically the “News” feed&lt;/item&gt;&lt;item&gt;Specifically the “Tutorials” feed&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Just the “Micro-Posts” feed &lt;list rend="ul"&gt;&lt;item&gt;Specifically the “Responses” feed&lt;/item&gt;&lt;item&gt;Specifically the “Quick Tips” feed&lt;/item&gt;&lt;item&gt;Specifically the “Shared Links” feed&lt;/item&gt;&lt;item&gt;Specifically the “Tutorials” feed&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Just the “Articles” feed &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Email Newsletter&lt;/head&gt;
    &lt;p&gt;Of course, the classic (and my favorite) way is to subscribe to email notifications. I’ll only send emails when I publish a new article, not for every micro-post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46083303</guid><pubDate>Fri, 28 Nov 2025 22:21:25 +0000</pubDate></item><item><title>Fabric Project</title><link>https://github.com/Fabric-Project/Fabric</link><description>&lt;doc fingerprint="ab5bd85145309005"&gt;
  &lt;main&gt;
    &lt;p&gt;Fabric is a creative code and rapid prototyping environment focusing on interactive visuals, image and video processing and analysis and 3D content authoring.&lt;/p&gt;
    &lt;p&gt;Fabric&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Provides an intuitive Visual Node based content authoring environment&lt;/item&gt;
      &lt;item&gt;Provides an SDK to load an common interchange file format&lt;/item&gt;
      &lt;item&gt;Provides an SDK to extend Fabric by creating custom nodes via a plugin architecture&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fabric is inspired by Apple's deprecated Quartz Composer ecosystem, and its design philosophy.&lt;/p&gt;
    &lt;p&gt;Fabric is intended to be used as&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Creative coding tool requires little to no programming experience.&lt;/item&gt;
      &lt;item&gt;Pro User tool to create reusable documents (similar to Quartz Composer Compositions) that can be loaded in the Fabric runtime and embedded into 3rd party applications.&lt;/item&gt;
      &lt;item&gt;Developer environment built on Satin that can render high fidelity visual output in a procedural way, using modern rendering techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An early alpha of Satin rendering a instances of a sphere geometry, along with an HDRI environment and a PBR Shader at 120Hz:&lt;/p&gt;
    &lt;p&gt;Think of Fabric as a playground of visual capabilies you can combine together.&lt;/p&gt;
    &lt;p&gt;Author&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Interacive 3D graphics&lt;/item&gt;
      &lt;item&gt;Image processing and effects&lt;/item&gt;
      &lt;item&gt;Audio reactive scenes&lt;/item&gt;
      &lt;item&gt;images and video analysis pipelines&lt;/item&gt;
      &lt;item&gt;embed your scenes into your own apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check out the Samples&lt;/p&gt;
    &lt;p&gt;Fabric supports, thanks to Satin and Lygia, high fidelity modern rendering techniques including&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physically based rendering&lt;/item&gt;
      &lt;item&gt;Scene graph&lt;/item&gt;
      &lt;item&gt;Lighting and Shadow casting&lt;/item&gt;
      &lt;item&gt;Realtime shader editing (live coding, hot reloading)&lt;/item&gt;
      &lt;item&gt;GPU Compute&lt;/item&gt;
      &lt;item&gt;Image Based Lighting&lt;/item&gt;
      &lt;item&gt;3D Model Loading&lt;/item&gt;
      &lt;item&gt;Material System&lt;/item&gt;
      &lt;item&gt;ML based realtime segmentation and keypoint detection&lt;/item&gt;
      &lt;item&gt;Shader based Image Processing and Mixing&lt;/item&gt;
      &lt;item&gt;Local LLM calling&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fabric is authored by by Anton Marini.&lt;/p&gt;
    &lt;p&gt;Fabric uses Satin 3D engine Satin rendering engine written by @Reza Ali.&lt;/p&gt;
    &lt;p&gt;Fabric includes a licensed Metal port of Lygia shader library, powering Image effects and more, written by @Patricio Gonzalez Vivo and contributors.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;Please note Fabric is heavily under construction.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 14 +&lt;/item&gt;
      &lt;item&gt;XCode 15 +&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please See Releases for code signed App downloads.&lt;/p&gt;
    &lt;p&gt;For Developers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Checkout Fabric and ensure you check out with submodules enabled, as Satin is a dependency.&lt;/item&gt;
      &lt;item&gt;Open the XCode project&lt;/item&gt;
      &lt;item&gt;Ensure that &lt;code&gt;Fabric Editor&lt;/code&gt;is the active target.&lt;/item&gt;
      &lt;item&gt;Build and run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Checkout our Architecture Document to understand the underlying paradigms of working with Fabric's Nodes and execution model, learn what a &lt;code&gt;Node&lt;/code&gt; and a &lt;code&gt;Port&lt;/code&gt; is, the types of data Fabric can generate and process, and how Fabric executes your compositions.&lt;/p&gt;
    &lt;p&gt;We also provide a set of evolving tutorial / getting started and sample Fabric compositions along with a readme walk through. You can use the Sample Compositions to learn and build off of.&lt;/p&gt;
    &lt;p&gt;You can view a comprehensive list of available and planned Nodes here to explore and learn how to compose more advanced and custom setups with Fabric.&lt;/p&gt;
    &lt;p&gt;Don't hesitate to file a feature request if a Node is missing!&lt;/p&gt;
    &lt;p&gt;Checkout our Roadmap Document&lt;/p&gt;
    &lt;p&gt;I ( Anton Marini ) are looking to build a community of developers who long for the ease of use and interoperability of Quartz Composer, its ecosystem and plugin community.&lt;/p&gt;
    &lt;p&gt;If you are interested in contributing, please do not hesitate to reach out / comment in the git repository, or join our discord via invite&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Will Fabric ever be cross platform?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;No. Fabric is purpose built on top of Satin and aims to provide a best in class Apple platform experience using Metal.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;What languages are used?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Fabric Editor is written in Swift and SwiftUI. Satin is written in Swift and C++&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Why not just use Vuo or Touch Designer or some other node based tool?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;I do not like them.&lt;/item&gt;
          &lt;item&gt;Don't get me wrong, they are incredible tools, but they are not for me.&lt;/item&gt;
          &lt;item&gt;They do not think the way I think.&lt;/item&gt;
          &lt;item&gt;They do not expose the layers of abstraction I want to work with.&lt;/item&gt;
          &lt;item&gt;They do not provide the user experience I want.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46083375</guid><pubDate>Fri, 28 Nov 2025 22:32:03 +0000</pubDate></item><item><title>A triangle whose interior angles sum to zero</title><link>https://www.johndcook.com/blog/2025/11/28/tricusp-triangle/</link><description>&lt;doc fingerprint="eb9b90f084dff3a0"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Spherical geometry&lt;/head&gt;
    &lt;p&gt;In spherical geometry, the interior angles of a triangle add up to more than π. And in fact you can determine the area of a spherical triangle by how much the angle sum exceeds π. On a sphere of radius 1, the area equals the triangle excess&lt;/p&gt;
    &lt;p&gt;Area = E = interior angle sum − π.&lt;/p&gt;
    &lt;p&gt;Small triangles have interior angle sum near π. But you could, for example, have a triangle with three right angles: put a vertex on the north pole and two vertices on the equator 90° longitude apart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hyperbolic geometry&lt;/head&gt;
    &lt;p&gt;In hyperbolic geometry, the sum of the interior angles of a triangle is always less than π. In a space with curvature −1, the area equals the triangle defect, the difference between π and the angle sum.&lt;/p&gt;
    &lt;p&gt;Area = D = π − interior angle sum.&lt;/p&gt;
    &lt;p&gt;Again small triangles have an interior angle sum near π. Both spherical and hyperbolic geometry are locally Euclidean.&lt;/p&gt;
    &lt;p&gt;The interior angle sum can be any value less than π, and so as the angle sum goes to 0, the triangle defect, and hence the area, goes to π. Since the minimum angle sum is 0, the maximum area of a triangle is π.&lt;/p&gt;
    &lt;p&gt;The figure below has interior angle sum 0 and area π in hyperbolic geometry.&lt;/p&gt;
    &lt;p&gt;Strictly speaking this is an improper triangle because the three hyperbolic lines (i.e. half circles) don’t intersect within the hyperbolic plane per se but at ideal points on the real axis. But you could come as close to this triangle as you like, staying within the hyperbolic plane.&lt;/p&gt;
    &lt;p&gt;Note that the radii of the (Euclidean) half circles doesn’t change the area. Any three semicircles that intersect on the real line as above make a triangle with the same area. Note also that the triangle has infinite perimeter but finite area.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46084122</guid><pubDate>Sat, 29 Nov 2025 00:26:42 +0000</pubDate></item></channel></rss>