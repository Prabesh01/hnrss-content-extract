<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 08 Dec 2025 20:12:13 +0000</lastBuildDate><item><title>Uber is turning data about trips and takeout into insights for marketers</title><link>https://www.businessinsider.com/uber-ads-launches-intelligence-insights-trips-takeout-data-marketers-2025-12</link><description>&lt;doc fingerprint="3db187292867ee6c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uber Advertising is launching an insights platform for marketers called Uber Intelligence.&lt;/item&gt;
      &lt;item&gt;It has partnered with LiveRamp to aggregate users' data without revealing their identities.&lt;/item&gt;
      &lt;item&gt;Uber has said its ad business is on track to generate $1.5 billion in revenue this year.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Uber wants advertisers to level up their marketing by tapping into data on the millions of rides and deliveries its users order every day.&lt;/p&gt;
    &lt;p&gt;The ride-hailing giant's ad division is announcing the launch of a new insights platform called Uber Intelligence on Monday, the company exclusively told Business Insider.&lt;/p&gt;
    &lt;p&gt;Launched in partnership with the data-connectivity platform LiveRamp, Uber Intelligence will let advertisers securely combine their customer data with Uber's to help surface insights about their audiences, based on what they eat and where they travel.&lt;/p&gt;
    &lt;p&gt;It uses LiveRamp's clean room technology, which lets companies aggregate their data in a privacy-safe environment, without sharing or seeing each other's raw or personally identifiable customer information.&lt;/p&gt;
    &lt;p&gt;A hotel brand could use Uber Intelligence to help identify which restaurants or entertainment venues it might want to partner with for its loyalty program, for example.&lt;/p&gt;
    &lt;p&gt;Uber also hopes the platform can act as a flywheel for its broader ad business. Marketers can use the data clean room for segmentation, such as identifying customers who are heavy business travelers, then targeting them with ads on their next trip to the airport in the Uber app or on screens inside Uber cars.&lt;/p&gt;
    &lt;p&gt;"That seamlessness is why we're so excited," Edwin Wong, global head of measurement at Uber Advertising, told Business Insider in an interview. He added that the aim is for marketers to begin saying, "'Oh, I'm not just understanding Uber, I'm understanding Uber in my marketing context.'"&lt;/p&gt;
    &lt;head rend="h2"&gt;Uber's other route to revenue&lt;/head&gt;
    &lt;p&gt;Uber Intelligence is the latest step in the evolution of Uber's ad business. Uber officially launched its dedicated advertising division in 2022. It offers an array of ad formats in the Uber and Uber Eats apps, on in-car tablets, in emails to its users, and on car tops.&lt;/p&gt;
    &lt;p&gt;The company said in May that its ad business had reached a $1.5 billion revenue run rate — the figure it has projected to hit by the end of 2025 — which would represent a 60% increase on last year. The company doesn't break out a more specific ad-revenue figure and hasn't provided an update on the run-rate number since May.&lt;/p&gt;
    &lt;p&gt;Uber Intelligence forms part of a bespoke set of services it offers its top advertisers. Earlier this year, it launched a creative studio where brands can partner with Uber to deliver more bespoke campaigns, such as offering rides to Miami F1 Grand Prix attendees in a luxury vehicle sponsored by La Mer, packed with freebie skincare products.&lt;/p&gt;
    &lt;p&gt;Andrew Frank, analyst at the research firm Gartner, said the launch of Uber Intelligence is another signal that Uber's ad business is maturing.&lt;/p&gt;
    &lt;p&gt;"Early-stage ad businesses tend to focus exclusively on selling inventory while more mature ones focus more on delivering differentiated value through targeting and measurement solutions that help brands understand and optimize the impact of their spend," Frank told Business Insider.&lt;/p&gt;
    &lt;p&gt;Uber's unique source of "terrestrial data" put it in good standing against the likes of Amazon, Google, and other retail media networks that emphasize the value of their data-driven insights, Frank added. However, he said Uber may need to address privacy concerns related to aggregating highly sensitive data in order to maintain consumer trust and to comply with evolving global regulators as a collector of first-party data.&lt;/p&gt;
    &lt;p&gt;Vihan Sharma, chief revenue officer of LiveRamp, said its platform provides technical guarantees to ensure "zero movement of data."&lt;/p&gt;
    &lt;p&gt;"The whole objective of a clean room technology is to build trust between data owners and consumers and the advertising ecosystem," Sharma said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192962</guid><pubDate>Mon, 08 Dec 2025 15:00:29 +0000</pubDate></item><item><title>Nova Programming Language</title><link>https://nova-lang.net</link><description>&lt;doc fingerprint="a3f8f96f303ef394"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome!&lt;/head&gt;
    &lt;code&gt;|- Welcome to Nova! -|
    ~ Nova is a lightweight language for... ~
        . sketching out ideas,
        . documents, notes and personal tools,
        . casual modeling and thinking,
        . computing without computers
&lt;/code&gt;
    &lt;p&gt;If you've ever wanted to make a computer come to life through programming, you probably know how complicated it can be. Intricate incantations, confusing instructions, and large, complicated tools can make approaching programming incredibly difficult.&lt;/p&gt;
    &lt;p&gt;To address this, we've built something we call Nova. It is a programming language, a note-taking system, a way of sketching, and a way of conversing with programmers and machines!&lt;/p&gt;
    &lt;p&gt;We invite you to investigate what we've discovered and try it for yourself!&lt;/p&gt;
    &lt;head rend="h1"&gt;I want to...&lt;/head&gt;
    &lt;head rend="h2"&gt;Learn To Write Nova&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Nova, mechanically." and other articles by yumaikas.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Find A Nova For Me&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Online Nova IDE&lt;/item&gt;
      &lt;item&gt;Nova implementations (for if you want to connect Nova to existing code)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Find Community&lt;/head&gt;
    &lt;p&gt;Join us on...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46192997</guid><pubDate>Mon, 08 Dec 2025 15:03:09 +0000</pubDate></item><item><title>Berkshire Hathaway Announces Leadership Appointments [pdf]</title><link>https://berkshirehathaway.com/news/dec0825.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46193116</guid><pubDate>Mon, 08 Dec 2025 15:12:28 +0000</pubDate></item><item><title>I successfully recreated the 1996 Space Jam website with Claude</title><link>https://theahura.substack.com/p/i-successfully-recreated-the-1996</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46193412</guid><pubDate>Mon, 08 Dec 2025 15:33:01 +0000</pubDate></item><item><title>AMD GPU Debugger</title><link>https://thegeeko.me/blog/amd-gpu-debugging/</link><description>&lt;doc fingerprint="ae1ad8512d473b89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AMD GPU Debugger&lt;/head&gt;
    &lt;p&gt;I’ve always wondered why we don’t have a GPU debugger similar to the one used for CPUs. A tool that allows pausing execution and examining the current state. This capability feels essential, especially since the GPU’s concurrent execution model is much harder to reason about. After searching for solutions, I came across rocgdb, a debugger for AMD’s ROCm environment. Unfortunately, its scope is limited to that environment. Still, this shows it’s technically possible. I then found a helpful series of blog posts by Marcell Kiss, detailing how he achieved this, which inspired me to try to recreate the process myself.&lt;/p&gt;
    &lt;head rend="h1"&gt;Let’s Try To Talk To The GPU Directly&lt;/head&gt;
    &lt;p&gt;The best place to start learning about this is RADV. By tracing what it does, we can find how to do it. Our goal here is to run the most basic shader &lt;code&gt;nop 0&lt;/code&gt; without using Vulkan, aka RADV in our case.&lt;/p&gt;
    &lt;p&gt;First of all, we need to open the DRM file to establish a connection with the KMD, using a simple open(“/dev/dri/cardX”), then we find that it’s calling &lt;code&gt;amdgpu_device_initialize&lt;/code&gt;, which is a function defined in &lt;code&gt;libdrm&lt;/code&gt;, which is a library that acts as middleware between user mode drivers(UMD) like &lt;code&gt;RADV&lt;/code&gt; and and kernel mode drivers(KMD) like amdgpu driver, and then when we try to do some actual work we have to create a context which can be achieved by calling &lt;code&gt;amdgpu_cs_ctx_create&lt;/code&gt; from &lt;code&gt;libdrm&lt;/code&gt; again, next up we need to allocate 2 buffers one of them for our code and the other for writing our commands into, we do this by calling a couple of functions, here’s how I do it:&lt;/p&gt;
    &lt;code&gt;void bo_alloc(amdgpu_t* dev, size_t size, u32 domain, bool uncached, amdgpubo_t* bo) {
 s32    ret         = -1;
 u32    alignment   = 0;
 u32    flags       = 0;
 size_t actual_size = 0;

 amdgpu_bo_handle bo_handle = NULL;
 amdgpu_va_handle va_handle = NULL;
 u64              va_addr   = 0;
 void*            host_addr = NULL;&lt;/code&gt;
    &lt;p&gt;Here we’re choosing the domain and assigning flags based on the params, some buffers we will need uncached, as we will see:&lt;/p&gt;
    &lt;code&gt; if (
   domain != AMDGPU_GEM_DOMAIN_GWS &amp;amp;&amp;amp; domain != AMDGPU_GEM_DOMAIN_GDS &amp;amp;&amp;amp;
   domain != AMDGPU_GEM_DOMAIN_OA) {
  actual_size = (size + 4096 - 1) &amp;amp; 0xFFFFFFFFFFFFF000ULL;
  alignment   = 4096;
  flags       = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED | AMDGPU_GEM_CREATE_VRAM_CLEARED |
          AMDGPU_GEM_CREATE_VM_ALWAYS_VALID;
  flags |=
    uncached ? (domain == AMDGPU_GEM_DOMAIN_GTT) * AMDGPU_GEM_CREATE_CPU_GTT_USWC : 0;
 } else {
  actual_size = size;
  alignment   = 1;
  flags       = AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
 }

 struct amdgpu_bo_alloc_request req = {
  .alloc_size     = actual_size,
  .phys_alignment = alignment,
  .preferred_heap = domain,
  .flags          = flags,
 };

 // memory aquired!!
 ret = amdgpu_bo_alloc(dev-&amp;gt;dev_handle, &amp;amp;req, &amp;amp;bo_handle);
 HDB_ASSERT(!ret, "can't allocate bo");&lt;/code&gt;
    &lt;p&gt;Now we have the memory, we need to map it. I opt to map anything that can be CPU-mapped for ease of use. We have to map the memory to both the GPU and the CPU virtual space. The KMD creates the page table when we open the DRM file, as shown here.&lt;/p&gt;
    &lt;p&gt;So map it to the GPU VM and, if possible, to the CPU VM as well. Here, at this point, there’s a libdrm function that does all of this setup for us and maps the memory, but I found that even when specifying &lt;code&gt;AMDGPU_VM_MTYPE_UC&lt;/code&gt;, it doesn’t always tag the page as uncached, not quite sure if it’s a
bug in my code or something in &lt;code&gt;libdrm&lt;/code&gt; anyways, the function is &lt;code&gt;amdgpu_bo_va_op&lt;/code&gt;, I opted to do it manually here and issue the IOCTL call myself:&lt;/p&gt;
    &lt;code&gt; u32 kms_handle = 0;
 amdgpu_bo_export(bo_handle, amdgpu_bo_handle_type_kms, &amp;amp;kms_handle);

 ret = amdgpu_va_range_alloc(
   dev-&amp;gt;dev_handle,
   amdgpu_gpu_va_range_general,
   actual_size,
   4096,
   0,
   &amp;amp;va_addr,
   &amp;amp;va_handle,
   0);
 HDB_ASSERT(!ret, "can't allocate VA");

 u64 map_flags =
   AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE;
 map_flags |= uncached ? AMDGPU_VM_MTYPE_UC | AMDGPU_VM_PAGE_NOALLOC : 0;

 struct drm_amdgpu_gem_va va = {
  .handle       = kms_handle,
  .operation    = AMDGPU_VA_OP_MAP,
  .flags        = map_flags,
  .va_address   = va_addr,
  .offset_in_bo = 0,
  .map_size     = actual_size,

 };

 ret = drm_ioctl_write_read(dev-&amp;gt;drm_fd, DRM_AMDGPU_GEM_VA, &amp;amp;va, sizeof(va));
 HDB_ASSERT(!ret, "can't map bo in GPU space");
 // ret = amdgpu_bo_va_op(bo_handle, 0, actual_size, va_addr, map_flags,
 // AMDGPU_VA_OP_MAP);

 if (flags &amp;amp; AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
  ret = amdgpu_bo_cpu_map(bo_handle, &amp;amp;host_addr);
  HDB_ASSERT(!ret, "can't map bo in CPU space");

  // AMDGPU_GEM_CREATE_VRAM_CLEARED doesn't really memset the memory to 0 anyways for
  // debug I'll just do it manually for now
  memset(host_addr, 0x0, actual_size);
 }

 *bo = (amdgpubo_t){
  .bo_handle = bo_handle,
  .va_handle = va_handle,
  .va_addr   = va_addr,
  .size      = actual_size,
  .host_addr = host_addr,
 };
}&lt;/code&gt;
    &lt;p&gt;Now we have the context and 2 buffers. Next, fill those buffers and send our commands to the KMD, which will then forward them to the Command Processor (CP) in the GPU for processing.&lt;/p&gt;
    &lt;p&gt;Let’s compile our code. We can use clang assembler for that, like this:&lt;/p&gt;
    &lt;code&gt;# https://gitlab.freedesktop.org/martty/radbg-poc/-/blob/master/ll-as.sh
clang -c -x assembler -target amdgcn-amd-amdhsa -mcpu=gfx1100 -o asm.o "$1"
objdump -h asm.o | grep .text | awk '{print "dd if='asm.o' of='asmc.bin' bs=1 count=$[0x" $3 "] skip=$[0x" $6 "] status=none"}' | bash
#rm asm.o&lt;/code&gt;
    &lt;p&gt;The bash script compiles the code, and then we’re only interested in the actual machine code, so we use objdump to figure out the offset and the size of the section and copy it to a new file called asmc.bin, then we can just load the file and write its bytes to the CPU-mapped address of the code buffer.&lt;/p&gt;
    &lt;p&gt;Next up, filling in the commands. This was extremely confusing for me because it’s not well documented. It was mostly learning how &lt;code&gt;RADV&lt;/code&gt; does things and trying to do similar things. Also, shout-out to the folks on the Graphics Programming Discord server for helping me, especially Picoduck. The commands are encoded in a special format called &lt;code&gt;PM4 Packets&lt;/code&gt;, which has multiple types. We only care about &lt;code&gt;Type 3&lt;/code&gt;: each packet has an opcode and the number of bytes it contains.&lt;/p&gt;
    &lt;p&gt;The first thing we need to do is program the GPU registers, then dispatch the shader. Some of those registers are &lt;code&gt;rsrc[1-3]&lt;/code&gt;; those registers are responsible for a number of configurations, pgm_[lo/hi], which hold the pointer to the code buffer and &lt;code&gt;num_thread_[x/y/z]&lt;/code&gt;; those are responsible for the number of threads inside a work group. All of those are set using the &lt;code&gt;set shader register&lt;/code&gt; packets, and here is how to encode them:&lt;/p&gt;
    &lt;p&gt;It’s worth mentioning that we can set multiple registers in 1 packet if they’re consecutive.&lt;/p&gt;
    &lt;code&gt;void pkt3_set_sh_reg(pkt3_packets_t* packets, u32 reg, u32 value) {
 HDB_ASSERT(
   reg &amp;gt;= SI_SH_REG_OFFSET &amp;amp;&amp;amp; reg &amp;lt; SI_SH_REG_END,
   "can't set register outside sh registers span");

 // packet header
 da_append(packets, PKT3(PKT3_SET_SH_REG, 1, 0));
 // offset of the register
 da_append(packets, (reg - SI_SH_REG_OFFSET) / 4);
 da_append(packets, value);
}&lt;/code&gt;
    &lt;p&gt;Then we append the dispatch command:&lt;/p&gt;
    &lt;code&gt;// we're going for 1 thread since we want the simplest case here.

da_append(&amp;amp;pkt3_packets, PKT3(PKT3_DISPATCH_DIRECT, 3, 0) | PKT3_SHADER_TYPE_S(1));
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, dispatch_initiator);&lt;/code&gt;
    &lt;p&gt;Now we want to write those commands into our buffer and send them to the KMD:&lt;/p&gt;
    &lt;code&gt;void dev_submit(
  amdgpu_t*         dev,
  pkt3_packets_t*   packets,
  amdgpu_bo_handle* buffers,
  u32               buffers_count,
  amdgpu_submit_t*  submit
) {
 s32        ret = -1;
 amdgpubo_t ib  = { 0 };

 bo_alloc(dev, pkt3_size(packets), AMDGPU_GEM_DOMAIN_GTT, false, &amp;amp;ib);
 bo_upload(&amp;amp;ib, packets-&amp;gt;data, pkt3_size(packets));

 amdgpu_bo_handle* bo_handles = // +1 for the indirect buffer
   (amdgpu_bo_handle*)malloc(sizeof(amdgpu_bo_handle) * (buffers_count + 1));

 bo_handles[0] = ib.bo_handle;
 for_range(i, 0, buffers_count) {
  bo_handles[i + 1] = buffers[i];
 }

 amdgpu_bo_list_handle bo_list = NULL;
 ret =
   amdgpu_bo_list_create(dev-&amp;gt;dev_handle, buffers_count + 1, bo_handles, NULL, &amp;amp;bo_list);
 HDB_ASSERT(!ret, "can't create a bo list");
 free(bo_handles);

 struct amdgpu_cs_ib_info ib_info = {
  .flags         = 0,
  .ib_mc_address = ib.va_addr,
  .size          = packets-&amp;gt;count,
 };

 struct amdgpu_cs_request req = {
  .flags                  = 0,
  .ip_type                = AMDGPU_HW_IP_COMPUTE,
  .ip_instance            = 0,
  .ring                   = 0,
  .resources              = bo_list,
  .number_of_dependencies = 0,
  .dependencies           = NULL,
  .number_of_ibs          = 1,
  .ibs                    = &amp;amp;ib_info,
  .seq_no                 = 0,
  .fence_info             = { 0 },
 };

 ret = amdgpu_cs_submit(dev-&amp;gt;ctx_handle, 0, &amp;amp;req, 1);
 HDB_ASSERT(!ret, "can't submit indirect buffer request");

 *submit = (amdgpu_submit_t){
    .ib = ib,
    .bo_list = bo_list,
    .fence = {
      .context = dev-&amp;gt;ctx_handle,
      .ip_type = AMDGPU_HW_IP_COMPUTE,
      .ip_instance = 0,
      .ring = 0,
      .fence = req.seq_no,
    },
  };
}&lt;/code&gt;
    &lt;p&gt;Here is a good point to make a more complex shader that outputs something. For example, writing 1 to a buffer.&lt;/p&gt;
    &lt;p&gt;No GPU hangs ?! nothing happened ?! cool, cool, now we have a shader that runs on the GPU, what’s next? Let’s try to hang the GPU by pausing the execution, aka make the GPU trap.&lt;/p&gt;
    &lt;head rend="h1"&gt;TBA/TMA&lt;/head&gt;
    &lt;p&gt;The RDNA3’s ISA manual does mention 2 registers, &lt;code&gt;TBA, TMA&lt;/code&gt;; here’s how they describe them respectively:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Holds the pointer to the current trap handler program address. Per-VMID register. Bit [63] indicates if the trap handler is present (1) or not (0) and is not considered part of the address (bit[62] is replicated into address bit[63]). Accessed via S_SENDMSG_RTN.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Temporary register for shader operations. For example, it can hold a pointer to memory used by the trap handler.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can configure the GPU to enter the trap handler when encountering certain exceptions listed in the RDNA3 ISA manual.&lt;/p&gt;
    &lt;p&gt;We know from Marcell Kiss’s blog posts that we need to compile a trap handler, which is a normal shader the GPU switches to when encountering a &lt;code&gt;s_trap&lt;/code&gt;. The TBA register has a special bit that indicates whether the trap handler is enabled.&lt;/p&gt;
    &lt;p&gt;Since these are privileged registers, we cannot write to them from user space. To bridge this gap for debugging, we can utilize the debugfs interface. Luckily, we have UMR, which uses that debugfs interface, and it’s open source; we copy AMD’s homework here which is great.&lt;/p&gt;
    &lt;head rend="h1"&gt;AMDGPU Debugfs&lt;/head&gt;
    &lt;p&gt;The amdgpu KMD has a couple of files in debugfs under &lt;code&gt;/sys/kernel/debug/dri/{PCI address}&lt;/code&gt;; one of them is &lt;code&gt;regs2&lt;/code&gt;, which is an interface to a &lt;code&gt;amdgpu_debugfs_regs2_write&lt;/code&gt; in the kernel that writes to the registers. It works by simply opening the file, seeking the register’s offset, and then writing; it also performs some synchronisation and writes the value correctly. We need to provide more parameters about the register before writing to the file, tho and do that by using an ioctl call. Here are the ioctl arguments:&lt;/p&gt;
    &lt;code&gt;typedef struct amdgpu_debugfs_regs2_iocdata_v2 {
 __u32 use_srbm, use_grbm, pg_lock;
 struct {
  __u32 se, sh, instance;
 } grbm;
 struct {
  __u32 me, pipe, queue, vmid;
 } srbm;
 __u32 xcc_id;
} regs2_ioc_data_t;&lt;/code&gt;
    &lt;p&gt;The 2 structs are because there are 2 types of registers, GRBM and SRBM, each of which is banked by different constructs; you can learn more about some of them here in the Linux kernel documentation.&lt;/p&gt;
    &lt;p&gt;Turns out our registers here are SBRM registers and banked by VMIDs, meaning each VMID has its own TBA and TMA registers. Cool, now we need to figure out the VMID of our process. As far as I understand, VMIDs are a way for the GPU to identify a specific process context, including the page table base address, so the address translation unit can translate a virtual memory address. The context is created when we open the DRM file. They get assigned dynamically at dispatch time, which is a problem for us; we want to write to those registers before dispatch.&lt;/p&gt;
    &lt;p&gt;We can obtain the VMID of the dispatched process by querying the &lt;code&gt;HW_ID2&lt;/code&gt; register with s_getreg_b32. I do a hack here, by enabling the trap handler in every VMID, and there are 16 of them, the first being special, and used by the KMD and the last 8 allocated to the amdkfd driver. We loop over the remaining VMIDs and write to those registers. This can cause issues to other processes using other VMIDs by enabling trap handlers in them and writing the virtual address of our trap handler, which is only valid within our virtual memory address space. It’s relatively safe tho since most other processes won’t cause a trap1.&lt;/p&gt;
    &lt;p&gt;Now we can write to TMA and TBA, here’s the code:&lt;/p&gt;
    &lt;code&gt;void dev_op_reg32(
  amdgpu_t* dev, gc_11_reg_t reg, regs2_ioc_data_t ioc_data, reg_32_op_t op, u32* value) {
 s32 ret = 0;

 reg_info_t reg_info     = gc_11_regs_infos[reg];
 uint64_t   reg_offset   = gc_11_regs_offsets[reg];
 uint64_t   base_offset  = dev-&amp;gt;gc_regs_base_addr[reg_info.soc_index];
 uint64_t   total_offset = (reg_offset + base_offset);

 // seems like we're multiplying by 4 here because the registers database in UMRs
 // source has them in indexes rather than bytes.
 total_offset *= (reg_info.type == REG_MMIO) ? 4 : 1;

 ret = hdb_ioctl(dev-&amp;gt;regs2_fd, AMDGPU_DEBUGFS_REGS2_IOC_SET_STATE_V2, &amp;amp;ioc_data);
 HDB_ASSERT(!ret, "Failed to set registers state");

 size_t size = lseek(dev-&amp;gt;regs2_fd, total_offset, SEEK_SET);
 HDB_ASSERT(size == total_offset, "Failed to seek register address");

 switch (op) {
 case REG_OP_READ : size = read(dev-&amp;gt;regs2_fd, value, 4); break;
 case REG_OP_WRITE: size = write(dev-&amp;gt;regs2_fd, value, 4); break;
 default          : HDB_ASSERT(false, "unsupported op");
 }

 HDB_ASSERT(size == 4, "Failed to write/read the values to/from the register");
}&lt;/code&gt;
    &lt;p&gt;And here’s how we write to &lt;code&gt;TMA&lt;/code&gt; and &lt;code&gt;TBA&lt;/code&gt;:
If you noticed, I’m using bitfields. I use them because working with them is much easier than macros, and while the byte order is not guaranteed by the C spec, it’s guaranteed by System V ABI, which Linux adheres to.&lt;/p&gt;
    &lt;code&gt;void dev_setup_trap_handler(amdgpu_t* dev, u64 tba, u64 tma) {
 reg_sq_shader_tma_lo_t tma_lo = { .raw = (u32)(tma) };
 reg_sq_shader_tma_hi_t tma_hi = { .raw = (u32)(tma &amp;gt;&amp;gt; 32) };

 reg_sq_shader_tba_lo_t tba_lo = { .raw = (u32)(tba &amp;gt;&amp;gt; 8) };
 reg_sq_shader_tba_hi_t tba_hi = { .raw = (u32)(tba &amp;gt;&amp;gt; 40) };

 tba_hi.trap_en = 1;

 regs2_ioc_data_t ioc_data = {
  .use_srbm = 1,
  .xcc_id   = -1,
 };

 // NOTE(hadi):
 // vmid's get assigned when code starts executing before hand we don't know which vmid
 // will get assigned to our process so we just set all of them
 for_range(i, 1, 9) {
  ioc_data.srbm.vmid = i;
  dev_op_reg32(dev, REG_SQ_SHADER_TBA_LO, ioc_data, REG_OP_WRITE, &amp;amp;tba_lo.raw);
  dev_op_reg32(dev, REG_SQ_SHADER_TBA_HI, ioc_data, REG_OP_WRITE, &amp;amp;tba_hi.raw);

  dev_op_reg32(dev, REG_SQ_SHADER_TMA_LO, ioc_data, REG_OP_WRITE, &amp;amp;tma_lo.raw);
  dev_op_reg32(dev, REG_SQ_SHADER_TMA_HI, ioc_data, REG_OP_WRITE, &amp;amp;tma_hi.raw);
 }
}&lt;/code&gt;
    &lt;p&gt;Anyway, now that we can write to those registers, if we enable the trap handler correctly, the GPU should hang when we launch our shader if we added &lt;code&gt;s_trap&lt;/code&gt; instruction to it, or we enabled the &lt;code&gt;TRAP_ON_START&lt;/code&gt; bit in rsrc32 register.&lt;/p&gt;
    &lt;p&gt;Now, let’s try to write a trap handler.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Trap Handler&lt;/head&gt;
    &lt;p&gt;If you wrote a different shader that outputs to a buffer, u can try writing to that shader from the trap handler, which is nice to make sure it’s actually being run.&lt;/p&gt;
    &lt;p&gt;We need 2 things: our trap handler and some scratch memory to use when needed, which we will store the address of in the TMA register.&lt;/p&gt;
    &lt;p&gt;The trap handler is just a normal program running in privileged state, meaning we have access to special registers like TTMP[0-15]. When we enter a trap handler, we need to first ensure that the state of the GPU registers is saved, just as the kernel does for CPU processes when context-switching, by saving a copy of the stable registers and the program counter, etc. The problem, tho, is that we don’t have a stable ABI for GPUs, or at least not one I’m aware of, and compilers use all the registers they can, so we need to save everything.&lt;/p&gt;
    &lt;p&gt;AMD GPUs’ Command Processors (CPs) have context-switching functionality, and the amdkfd driver does implement some context-switching shaders. The problem is they’re not documented, and we have to figure them out from the amdkfd driver source and from other parts of the driver stack that interact with it, which is a pain in the ass. I kinda did a workaround here since I didn’t find luck understanding how it works, and some other reasons I’ll discuss later in the post.&lt;/p&gt;
    &lt;p&gt;The workaround here is to use only TTMP registers and a combination of specific instructions to copy the values of some registers, allowing us to use more instructions to copy the remaining registers. The main idea is to make use of the &lt;code&gt;global_store_addtid_b32&lt;/code&gt; instruction, which adds the index of the current thread within the wave to the writing address, aka&lt;/p&gt;
    &lt;p&gt;This allows us to write a unique value per thread using only TTMP registers, which are unique per wave, not per thread3, so we can save the context of a single wave.&lt;/p&gt;
    &lt;p&gt;The problem is that if we have more than 1 wave, they will overlap, and we will have a race condition.&lt;/p&gt;
    &lt;p&gt;Here is the code:&lt;/p&gt;
    &lt;code&gt;start:
 ;; save the STATUS word into ttmp8
 s_getreg_b32 ttmp8, hwreg(HW_REG_STATUS)

 ;; save exec into ttmp[2:3]
 s_mov_b64 ttmp[2:3], exec

 ;; getting the address of our tma buffer
 s_sendmsg_rtn_b64 ttmp[4:5], sendmsg(MSG_RTN_GET_TMA)
 s_waitcnt lgkmcnt(0)

 ;; save vcc
 s_mov_b64 ttmp[6:7], vcc

 ;; enable all threads so they can write their vgpr registers
 s_mov_b64 exec, -1

 ;; FIXME(hadi): this assumes only 1 wave is running
 global_store_addtid_b32 v0, ttmp[4:5], offset:TMA_VREG_OFFSET        glc slc dlc
 global_store_addtid_b32 v1, ttmp[4:5], offset:TMA_VREG_OFFSET + 256  glc slc dlc
 global_store_addtid_b32 v2, ttmp[4:5], offset:TMA_VREG_OFFSET + 512  glc slc dlc
 global_store_addtid_b32 v3, ttmp[4:5], offset:TMA_VREG_OFFSET + 768  glc slc dlc
 global_store_addtid_b32 v4, ttmp[4:5], offset:TMA_VREG_OFFSET + 1024 glc slc dlc
 global_store_addtid_b32 v5, ttmp[4:5], offset:TMA_VREG_OFFSET + 1280 glc slc dlc
 global_store_addtid_b32 v6, ttmp[4:5], offset:TMA_VREG_OFFSET + 1536 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; only first thread is supposed to write sgprs of the wave
 s_mov_b64 exec, 1
 v_mov_b32 v1, s0
 v_mov_b32 v2, s1
 v_mov_b32 v3, s2
 v_mov_b32 v4, s3
 v_mov_b32 v5, s4
 v_mov_b32 v0, 0
 global_store_b32 v0, v1, ttmp[4:5], offset:TMA_SREG_OFFSET glc slc dlc
 global_store_b32 v0, v2, ttmp[4:5], offset:TMA_SREG_OFFSET + 4 glc slc dlc
 global_store_b32 v0, v3, ttmp[4:5], offset:TMA_SREG_OFFSET + 8 glc slc dlc
 global_store_b32 v0, v4, ttmp[4:5], offset:TMA_SREG_OFFSET + 12 glc slc dlc
 global_store_b32 v0, v5, ttmp[4:5], offset:TMA_SREG_OFFSET + 16 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; enable all threads
 s_mov_b64 exec, -1&lt;/code&gt;
    &lt;p&gt;Now that we have those values in memory, we need to tell the CPU: Hey, we got the data, and pause the GPU’s execution until the CPU issues a command. Also, notice we can just modify those from the CPU.&lt;/p&gt;
    &lt;p&gt;Before we tell the CPU, we need to write some values that might help the CPU. Here are they:&lt;/p&gt;
    &lt;code&gt; ;; IDs to identify which parts of the hardware we are running on exactly
 s_getreg_b32 ttmp10, hwreg(HW_REG_HW_ID1)
 s_getreg_b32 ttmp11, hwreg(HW_REG_HW_ID2)
 v_mov_b32 v3, ttmp10
 v_mov_b32 v4, ttmp11
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:TMA_DATA_OFFSET glc slc dlc

 ;; the original vcc mask
 v_mov_b32 v3, ttmp6
 v_mov_b32 v4, ttmp7
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:2048 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; the original exec mask
 v_mov_b32 v3, ttmp2
 v_mov_b32 v4, ttmp3
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:2056 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; the program counter
 v_mov_b32 v3, ttmp0
 v_mov_b32 v4, ttmp1
 v_and_b32 v4, v4, 0xffff
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:16 glc slc dlc

 s_waitcnt vmcnt(0)&lt;/code&gt;
    &lt;p&gt;Now the GPU should just wait for the CPU, and here’s the spin code it’s implemented as described by Marcell Kiss here:&lt;/p&gt;
    &lt;code&gt;SPIN:
 global_load_dword v1, v2, ttmp[4:5] glc slc dlc

SPIN1:
 // I found the bit range of 10 to 15 using trial and error in the
 // isa manual specifies that it's a 6-bit number but the offset 10
 // is just trial and error
  s_getreg_b32 ttmp13, hwreg(HW_REG_IB_STS, 10, 15)
 s_and_b32 ttmp13, ttmp13, ttmp13
 s_cbranch_scc1 SPIN1

 v_readfirstlane_b32 ttmp13, v1
 s_and_b32 ttmp13, ttmp13, ttmp13
 s_cbranch_scc0 SPIN

CLEAR:
 v_mov_b32 v2, 0
 v_mov_b32 v1, 0
 global_store_dword v1, v2, ttmp[4:5] glc slc dlc
 s_waitcnt vmcnt(0)&lt;/code&gt;
    &lt;p&gt;The main loop in the CPU is like enable trap handler, then dispatch shader, then wait for the GPU to write some specific value in a specific address to signal all data is there, then examine and display, and tell the GPU all clear, go ahead.&lt;/p&gt;
    &lt;p&gt;Now that our uncached buffers are in play, we just keep looping and checking whether the GPU has written the register values. When it does, the first thing we do is halt the wave by writing into the &lt;code&gt;SQ_CMD&lt;/code&gt; register to allow us to do whatever with the wave without causing any issues, tho if we halt for too long, the GPU CP will reset the command queue and kill the process, but we can change that behaviour by adjusting lockup_timeout parameter of the amdgpu kernel module:&lt;/p&gt;
    &lt;code&gt;reg_sq_wave_hw_id1_t hw1 = { .raw = tma[2] };
reg_sq_wave_hw_id2_t hw2 = { .raw = tma[3] };

reg_sq_cmd_t halt_cmd = {
 .cmd  = 1,
 .mode = 1,
 .data = 1,
};

regs2_ioc_data_t ioc_data = {
 .use_srbm = false,
 .use_grbm = true,
};

dev_op_reg32(&amp;amp;amdgpu, REG_SQ_CMD, ioc_data, REG_OP_WRITE, &amp;amp;halt_cmd.raw);
gpu_is_halted = true;&lt;/code&gt;
    &lt;p&gt;From here on, we can do whatever with the data we have. All the data we need to build a proper debugger. We will come back to what to do with the data in a bit; let’s assume we did what was needed for now.&lt;/p&gt;
    &lt;p&gt;Now that we’re done with the CPU, we need to write to the first byte in our TMA buffer, since the trap handler checks for that, then resume the wave, and the trap handler should pick it up. We can resume by writing to the &lt;code&gt;SQ_CMD&lt;/code&gt; register again:&lt;/p&gt;
    &lt;code&gt;halt_cmd.mode = 0;
dev_op_reg32(&amp;amp;amdgpu, REG_SQ_CMD, ioc_data, REG_OP_WRITE, &amp;amp;halt_cmd.raw);
gpu_is_halted = false;&lt;/code&gt;
    &lt;p&gt;Then the GPU should continue. We need to restore everything and return the program counter to the original address. Based on whether it’s a hardware trap or not, the program counter may point to the instruction before or the instruction itself. The ISA manual and Marcell Kiss’s posts explain that well, so refer to them.&lt;/p&gt;
    &lt;code&gt;RETURN:
 ;; extract the trap ID from ttmp1
 s_and_b32 ttmp9, ttmp1, PC_HI_TRAP_ID_MASK
 s_lshr_b32 ttmp9, ttmp9, PC_HI_TRAP_ID_SHIFT

 ;; if the trapID == 0, then this is a hardware trap,
 ;; we don't need to fix up the return address
 s_cmpk_eq_u32 ttmp9, 0
 s_cbranch_scc1 RETURN_FROM_NON_S_TRAP

 ;; restore PC
 ;; add 4 to the faulting address, with carry
 s_add_u32 ttmp0, ttmp0, 4
 s_addc_u32 ttmp1, ttmp1, 0

RETURN_FROM_NON_S_TRAP:
 s_load_dwordx4 s[0:3], ttmp[4:5], TMA_SREG_OFFSET glc dlc
 s_load_dword s4, ttmp[4:5], TMA_SREG_OFFSET + 16 glc dlc
 s_waitcnt lgkmcnt(0)

 s_mov_b64 exec, -1
 global_load_addtid_b32 v0, ttmp[4:5], offset:TMA_VREG_OFFSET        glc slc dlc
 global_load_addtid_b32 v1, ttmp[4:5], offset:TMA_VREG_OFFSET + 256  glc slc dlc
 global_load_addtid_b32 v2, ttmp[4:5], offset:TMA_VREG_OFFSET + 512  glc slc dlc
 global_load_addtid_b32 v3, ttmp[4:5], offset:TMA_VREG_OFFSET + 768  glc slc dlc
 global_load_addtid_b32 v4, ttmp[4:5], offset:TMA_VREG_OFFSET + 1024 glc slc dlc
 global_load_addtid_b32 v5, ttmp[4:5], offset:TMA_VREG_OFFSET + 1280 glc slc dlc
 global_load_addtid_b32 v6, ttmp[4:5], offset:TMA_VREG_OFFSET + 1536 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; mask off non-address high bits from ttmp1
 s_and_b32 ttmp1, ttmp1, 0xffff

 ;; restore exec
 s_load_b64 vcc, ttmp[4:5], 2048 glc dlc
 s_load_b64 ttmp[2:3], ttmp[4:5], 2056 glc dlc
 s_waitcnt lgkmcnt(0)
 s_mov_b64 exec, ttmp[2:3]

 ;; restore STATUS.EXECZ, not writable by s_setreg_b32
 s_and_b64 exec, exec, exec

 ;; restore STATUS.VCCZ, not writable by s_setreg_b32
 s_and_b64 vcc, vcc, vcc

 ;; restore STATUS.SCC
 s_setreg_b32 hwreg(HW_REG_STATUS, 0, 1), ttmp8

 s_waitcnt vmcnt(0) lgkmcnt(0) expcnt(0)  ; Full pipeline flush
 ;; return from trap handler and restore STATUS.PRIV
 s_rfe_b64 [ttmp0, ttmp1]&lt;/code&gt;
    &lt;head rend="h1"&gt;SPIR-V&lt;/head&gt;
    &lt;p&gt;Now we can run compiled code directly, but we don’t want people to compile their code manually, then extract the text section, and give it to us. The plan is to take SPIR-V code, compile it correctly, then run it, or, even better, integrate with RADV and let RADV give us more information to work with.&lt;/p&gt;
    &lt;p&gt;My main plan was making like fork RADV and then add then make report for us the vulkan calls and then we can have a better view on the GPU work know the buffers/textures it’s using etc, This seems like a lot more work tho so I’ll keep it in mind but not doing that for now unless someone is willing to pay me for that ;).&lt;/p&gt;
    &lt;p&gt;For now, let’s just use RADV’s compiler &lt;code&gt;ACO&lt;/code&gt;. Luckily, RADV has a &lt;code&gt;null_winsys&lt;/code&gt; mode, aka it will not do actual work or open DRM files, just a fake Vulkan device, which is perfect for our case here, since we care about nothing other than just compiling code. We can enable it by setting the env var &lt;code&gt;RADV_FORCE_FAMILY&lt;/code&gt;, then we just call what we need like this:&lt;/p&gt;
    &lt;code&gt;int32_t hdb_compile_spirv_to_bin(
  const void* spirv_binary,
  size_t size,
  hdb_shader_stage_t stage,
  hdb_shader_t* shader
) {
 setenv("RADV_FORCE_FAMILY", "navi31", 1);
 //  setenv("RADV_DEBUG", "nocache,noopt", 1);
 setenv("ACO_DEBUG", "nocache,noopt", 1);

 VkInstanceCreateInfo i_cinfo = {
  .sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
  .pApplicationInfo =
    &amp;amp;(VkApplicationInfo){
      .sType              = VK_STRUCTURE_TYPE_APPLICATION_INFO,
      .pApplicationName   = "HDB Shader Compiler",
      .applicationVersion = 1,
      .pEngineName        = "HDB",
      .engineVersion      = 1,
      .apiVersion         = VK_API_VERSION_1_4,
    },
 };

 VkInstance vk_instance = {};
 radv_CreateInstance(&amp;amp;i_cinfo, NULL, &amp;amp;vk_instance);

 struct radv_instance* instance = radv_instance_from_handle(vk_instance);
 instance-&amp;gt;debug_flags |=
   RADV_DEBUG_NIR_DEBUG_INFO | RADV_DEBUG_NO_CACHE | RADV_DEBUG_INFO;

 uint32_t         n       = 1;
 VkPhysicalDevice vk_pdev = {};
 instance-&amp;gt;vk.dispatch_table.EnumeratePhysicalDevices(vk_instance, &amp;amp;n, &amp;amp;vk_pdev);

 struct radv_physical_device* pdev = radv_physical_device_from_handle(vk_pdev);
 pdev-&amp;gt;use_llvm                    = false;

 VkDeviceCreateInfo d_cinfo = { VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };
 VkDevice vk_dev = {};
 pdev-&amp;gt;vk.dispatch_table.CreateDevice(vk_pdev, &amp;amp;d_cinfo, NULL, &amp;amp;vk_dev);

 struct radv_device* dev = radv_device_from_handle(vk_dev);

 struct radv_shader_stage radv_stage = {
  .spirv.data = spirv_binary,
  .spirv.size = size,
  .entrypoint = "main",
  .stage      = MESA_SHADER_COMPUTE,
  .layout = {
   .push_constant_size = 16,
  },
  .key = {
   .optimisations_disabled = true,
  },
 };

 struct radv_shader_binary* cs_bin = NULL;
 struct radv_shader*        cs_shader =
   radv_compile_cs(dev, NULL, &amp;amp;radv_stage, true, true, false, true, &amp;amp;cs_bin);

 *shader = (hdb_shader_t){
  .bin              = cs_shader-&amp;gt;code,
  .bin_size         = cs_shader-&amp;gt;code_size,
  .rsrc1            = cs_shader-&amp;gt;config.rsrc1,
  .rsrc2            = cs_shader-&amp;gt;config.rsrc2,
  .rsrc3            = cs_shader-&amp;gt;config.rsrc3,
  .debug_info       = cs_shader-&amp;gt;debug_info,
  .debug_info_count = cs_shader-&amp;gt;debug_info_count,
 };

 return 0;
}&lt;/code&gt;
    &lt;p&gt;Now that we have a well-structured loop and communication between the GPU and the CPU, we can run SPIR-V binaries to some extent. Let’s see how we can make it an actual debugger.&lt;/p&gt;
    &lt;head rend="h1"&gt;An Actual Debugger&lt;/head&gt;
    &lt;p&gt;We talked earlier about CPs natively supporting context-switching, this appears to be compute spcific feature, which prevents from implementing it for other types of shaders, tho, it appears that mesh shaders and raytracing shaders are just compute shaders under the hood, which will allow us to use that functionality. For now debugging one wave feels enough, also we can moify the wave parameters to debug some specific indices.&lt;/p&gt;
    &lt;p&gt;Here’s some of the features&lt;/p&gt;
    &lt;head rend="h2"&gt;Breakpoints and Stepping&lt;/head&gt;
    &lt;p&gt;For stepping, we can use 2 bits: one in &lt;code&gt;RSRC1&lt;/code&gt; and the other in &lt;code&gt;RSRC3&lt;/code&gt;. They’re &lt;code&gt;DEBUG_MODE&lt;/code&gt; and &lt;code&gt;TRAP_ON_START&lt;/code&gt;, respectively. The former enters the trap handler after each instruction, and the latter enters before the first instruction. This means we can automatically enable instruction-level stepping.&lt;/p&gt;
    &lt;p&gt;Regarding breakpoints, I haven’t implemented them, but they’re rather simple to implement here by us having the base address of the code buffer and knowing the size of each instruction; we can calculate the program counter location ahead and have a list of them available to the GPU, and we can do a binary search on the trap handler.&lt;/p&gt;
    &lt;head rend="h2"&gt;Source Code Line Mapping&lt;/head&gt;
    &lt;p&gt;The ACO shader compiler does generate instruction-level source code mapping, which is good enough for our purposes here. By taking the offset4 of the current program counter and indexing into the code buffer, we can retrieve the current instruction and disassemble it, as well as find the source code mapping from the debug info.&lt;/p&gt;
    &lt;head rend="h2"&gt;Address Watching aka Watchpoints&lt;/head&gt;
    &lt;p&gt;We can implement this by marking the GPU page as protected. On a GPU fault, we enter the trap handler, check whether it’s within the range of our buffers and textures, and then act accordingly. Also, looking at the registers, we can find these:&lt;/p&gt;
    &lt;code&gt;typedef union {
 struct {
  uint32_t addr: 16;
 };
 uint32_t raw;
} reg_sq_watch0_addr_h_t;

typedef union {
 struct {
  uint32_t __reserved_0 : 6;
  uint32_t addr: 26;
 };
 uint32_t raw;
} reg_sq_watch0_addr_l_t;&lt;/code&gt;
    &lt;p&gt;which suggests that the hardware already supports this natively, so we don’t even need to do that dance. It needs more investigation on my part, tho, since I didn’t implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Variables Types and Names&lt;/head&gt;
    &lt;p&gt;This needs some serious plumbing, since we need to make NIR(Mesa’s intermediate representation) optimisation passes propagate debug info correctly. I already started on this here. Then we need to make ACO track variables and store the information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vulkan Integration&lt;/head&gt;
    &lt;p&gt;This requires ditching our simple UMD we made earlier and using RADV, which is what should happen eventually, then we have our custom driver maybe pause on before a specific frame, or get triggered by a key, and then ask before each dispatch if to attach to it or not, or something similar, since we have a full proper Vulkan implementation we already have all the information we would need like buffers, textures, push constants, types, variable names, .. etc, that would be a much better and more pleasant debugger to use.&lt;/p&gt;
    &lt;p&gt;Finally, here’s some live footage:&lt;/p&gt;
    &lt;head rend="h1"&gt;Bonus Round&lt;/head&gt;
    &lt;p&gt;Here is an incomplete user-mode page walking code for gfx11, aka rx7900xtx&lt;/p&gt;
    &lt;code&gt;typedef struct {
 u64 valid         : 1;  // 0
 u64 system        : 1;  // 1
 u64 coherent      : 1;  // 2
 u64 __reserved_0  : 3;  // 5
 u64 pte_base_addr : 42; // 47
 u64 pa_rsvd       : 4;  // 51
 u64 __reserved_1  : 2;  // 53
 u64 mall_reuse    : 2;  // 55
 u64 tfs_addr      : 1;  // 56
 u64 __reserved_2  : 1;  // 57
 u64 frag_size     : 5;  // 62
 u64 pte           : 1;  // 63
} pde_t;

typedef struct {
 u64 valid          : 1; // = pte_entry &amp;amp; 1;
 u64 system         : 1; // = (pte_entry &amp;gt;&amp;gt; 1) &amp;amp; 1;
 u64 coherent       : 1; // = (pte_entry &amp;gt;&amp;gt; 2) &amp;amp; 1;
 u64 tmz            : 1; // = (pte_entry &amp;gt;&amp;gt; 3) &amp;amp; 1;
 u64 execute        : 1; // = (pte_entry &amp;gt;&amp;gt; 4) &amp;amp; 1;
 u64 read           : 1; // = (pte_entry &amp;gt;&amp;gt; 5) &amp;amp; 1;
 u64 write          : 1; // = (pte_entry &amp;gt;&amp;gt; 6) &amp;amp; 1;
 u64 fragment       : 5; // = (pte_entry &amp;gt;&amp;gt; 7) &amp;amp; 0x1F;
 u64 page_base_addr : 36;
 u64 mtype          : 2; // = (pte_entry &amp;gt;&amp;gt; 48) &amp;amp; 3;
 u64 prt            : 1; // = (pte_entry &amp;gt;&amp;gt; 51) &amp;amp; 1;
 u64 software       : 2; // = (pte_entry &amp;gt;&amp;gt; 52) &amp;amp; 3;
 u64 pde            : 1; // = (pte_entry &amp;gt;&amp;gt; 54) &amp;amp; 1;
 u64 __reserved_0   : 1;
 u64 further        : 1; // = (pte_entry &amp;gt;&amp;gt; 56) &amp;amp; 1;
 u64 gcr            : 1; // = (pte_entry &amp;gt;&amp;gt; 57) &amp;amp; 1;
 u64 llc_noalloc    : 1; // = (pte_entry &amp;gt;&amp;gt; 58) &amp;amp; 1;
} pte_t;

static inline pde_t decode_pde(u64 pde_raw) {
 pde_t pde         = *((pde_t*)(&amp;amp;pde_raw));
 pde.pte_base_addr = (u64)pde.pte_base_addr &amp;lt;&amp;lt; 6;
 return pde;
}

static inline pte_t decode_pte(u64 pde_raw) {
 pte_t pte          = *((pte_t*)(&amp;amp;pde_raw));
 pte.page_base_addr = (u64)pte.page_base_addr &amp;lt;&amp;lt; 12;
 return pte;
}

static inline u64 log2_range_round_up(u64 s, u64 e) {
 u64 x = e - s - 1;
 return (x == 0 || x == 1) ? 1 : 64 - __builtin_clzll(x);
}

void dev_linear_vram(amdgpu_t* dev, u64 phy_addr, size_t size, void* buf) {
 HDB_ASSERT(!((phy_addr &amp;amp; 3) || (size &amp;amp; 3)), "Must be page aligned address and size");

 size_t offset = lseek(dev-&amp;gt;vram_fd, phy_addr, SEEK_SET);
 HDB_ASSERT(offset == phy_addr, "Couldn't seek to the requested addr");

 offset = read(dev-&amp;gt;vram_fd, buf, size);
 HDB_ASSERT(offset == size, "Couldn't read the full requested size");
}

void dev_decode(amdgpu_t* dev, u32 vmid, u64 va_addr) {
 reg_gcmc_vm_fb_location_base_t fb_base_reg   = { 0 };
 reg_gcmc_vm_fb_location_top_t  fb_top_reg    = { 0 };
 reg_gcmc_vm_fb_offset_t        fb_offset_reg = { 0 };

 regs2_ioc_data_t ioc_data = { 0 };
 dev_op_reg32(
   dev, REG_GCMC_VM_FB_LOCATION_BASE, ioc_data, REG_OP_READ, &amp;amp;fb_base_reg.raw);
 dev_op_reg32(dev, REG_GCMC_VM_FB_LOCATION_TOP, ioc_data, REG_OP_READ, &amp;amp;fb_top_reg.raw);
 dev_op_reg32(dev, REG_GCMC_VM_FB_OFFSET, ioc_data, REG_OP_READ, &amp;amp;fb_offset_reg.raw);

 u64 fb_offset = (u64)fb_offset_reg.fb_offset;

 // TODO(hadi): add zfb mode support
 bool zfb = fb_top_reg.fb_top + 1 &amp;lt; fb_base_reg.fb_base;
 HDB_ASSERT(!zfb, "ZFB mode is not implemented yet!");

 // printf(
 //   "fb base: 0x%x\nfb_top: 0x%x\nfb_offset: 0x%x\n",
 //   fb_base_reg.raw,
 //   fb_top_reg.raw,
 //   fb_offset_reg.raw);

 gc_11_reg_t pt_start_lo_id = { 0 };
 gc_11_reg_t pt_start_hi_id = { 0 };
 gc_11_reg_t pt_end_lo_id   = { 0 };
 gc_11_reg_t pt_end_hi_id   = { 0 };
 gc_11_reg_t pt_base_hi_id  = { 0 };
 gc_11_reg_t pt_base_lo_id  = { 0 };
 gc_11_reg_t ctx_cntl_id    = { 0 };

 switch (vmid) {
 case 0:
  pt_start_lo_id = REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT0_CNTL;
  break;
 case 1:
  pt_start_lo_id = REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT1_CNTL;
  break;
 case 2:
  pt_start_lo_id = REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT2_CNTL;
  break;
 case 3:
  pt_start_lo_id = REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT3_CNTL;
  break;
 case 4:
  pt_start_lo_id = REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT4_CNTL;
  break;
 case 5:
  pt_start_lo_id = REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT5_CNTL;
  break;
 case 6:
  pt_start_lo_id = REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT6_CNTL;
  break;
 case 7:
  pt_start_lo_id = REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 8:
  pt_start_lo_id = REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 9:
  pt_start_lo_id = REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 10:
  pt_start_lo_id = REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT10_CNTL;
  break;
 case 11:
  pt_start_lo_id = REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT11_CNTL;
  break;
 case 12:
  pt_start_lo_id = REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT12_CNTL;
  break;
 case 13:
  pt_start_lo_id = REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT13_CNTL;
  break;
 case 14:
  pt_start_lo_id = REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT14_CNTL;
  break;
 case 15:
  pt_start_lo_id = REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT15_CNTL;
  break;
 default: HDB_ASSERT(false, "Out of range VMID 0-15 trying to access %u", vmid);
 }

 // all the types of the contexts are the same so will just use 0 but pass the correct
 // register enum to the read function
 reg_gcvm_context0_page_table_start_addr_lo32_t pt_start_lo = { 0 };
 reg_gcvm_context0_page_table_start_addr_hi32_t pt_start_hi = { 0 };
 reg_gcvm_context0_page_table_end_addr_lo32_t   pt_end_lo   = { 0 };
 reg_gcvm_context0_page_table_end_addr_hi32_t   pt_end_hi   = { 0 };
 reg_gcvm_context0_page_table_base_addr_lo32_t  pt_base_lo  = { 0 };
 reg_gcvm_context0_page_table_base_addr_hi32_t  pt_base_hi  = { 0 };
 reg_gcvm_context0_cntl_t                       ctx_cntl    = { 0 };

 dev_op_reg32(dev, pt_start_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_start_lo.raw);
 dev_op_reg32(dev, pt_start_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_start_hi.raw);
 dev_op_reg32(dev, pt_end_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_end_lo.raw);
 dev_op_reg32(dev, pt_end_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_end_hi.raw);
 dev_op_reg32(dev, pt_base_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_base_lo.raw);
 dev_op_reg32(dev, pt_base_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_base_hi.raw);
 dev_op_reg32(dev, ctx_cntl_id, ioc_data, REG_OP_READ, &amp;amp;ctx_cntl.raw);

 u64 pt_start_addr = ((u64)pt_start_lo.raw &amp;lt;&amp;lt; 12) | ((u64)pt_start_hi.raw &amp;lt;&amp;lt; 44);
 u64 pt_end_addr   = ((u64)pt_end_lo.raw &amp;lt;&amp;lt; 12) | ((u64)pt_end_hi.raw &amp;lt;&amp;lt; 44);
 u64 pt_base_addr  = ((u64)pt_base_lo.raw &amp;lt;&amp;lt; 0) | ((u64)pt_base_hi.raw &amp;lt;&amp;lt; 32);
 u32 pt_depth      = ctx_cntl.page_table_depth;
 u32 ptb_size      = ctx_cntl.page_table_block_size;

 HDB_ASSERT(pt_base_addr != 0xffffffffffffffffull, "Invalid page table base addr");

 printf(
   "\tPage Table Start: 0x%lx\n\tPage Table End: 0x%lx\n\tPage Table Base: "
   "0x%lx\n\tPage Table Depth: %u\n\tBlock Size: %u\n",
   pt_start_addr,
   pt_end_addr,
   pt_base_addr,
   pt_depth,
   ptb_size);

 // decode base PDB
 pde_t pde = decode_pde(pt_base_addr);
 pt_base_addr -= fb_offset * !pde.system; // substract only on vram

 u64 pt_last_byte_addr = pt_end_addr + 0xfff; // 0xfff is 1 page
 HDB_ASSERT(
   pt_start_addr &amp;lt;= va_addr || va_addr &amp;lt; pt_last_byte_addr,
   "Invalid virtual address outside the range of the root page table of this vm");

 va_addr -= pt_start_addr;
 //
 // Size of the first PDB depends on the total coverage of the
 // page table and the PAGE_TABLE_BLOCK_SIZE.
 // Entire table takes ceil(log2(total_vm_size)) bits
 // All PDBs except the first one take 9 bits each
 // The PTB covers at least 2 MiB (21 bits)
 // And PAGE_TABLE_BLOCK_SIZE is log2(num 2MiB ranges PTB covers)
 // As such, the formula for the size of the first PDB is:
 //                       PDB1, PDB0, etc.      PTB covers at least 2 MiB
 //                                        Block size can make it cover more
 //   total_vm_bits - (9 * num_middle_pdbs) - (page_table_block_size + 21)
 //
 // we need the total range range here not the last byte addr like above
 u32 total_vaddr_bits = log2_range_round_up(pt_start_addr, pt_end_addr + 0x1000);

 u32 total_pdb_bits = total_vaddr_bits;
 // substract everything from the va_addr to leave just the pdb bits
 total_pdb_bits -= 9 * (pt_depth - 1); // middle PDBs each is 9 bits
 total_pdb_bits -= (ptb_size + 21);    // at least 2mb(21) bits + ptb_size

 // u64 va_mask = (1ull &amp;lt;&amp;lt; total_pdb_bits) - 1;
 // va_mask &amp;lt;&amp;lt;= (total_vaddr_bits - total_pdb_bits);

 // pde_t pdes[8]  = { 0 };
 // u32   curr_pde = 0;
 // u64   pde_addr = 0;
 // u64  loop_pde = pt_base_addr;

 if (pt_depth == 0) { HDB_ASSERT(false, "DEPTH = 0 is not implemented yet"); }

 pde_t curr_pde    = pde;
 u64   entry_bits  = 0;
 s32   curr_depth  = pt_depth;
 bool  pde0_is_pte = false;
 // walk all middle PDEs
 while (curr_depth &amp;gt; 0) {
  // printf("pde(%u):0x%lx \n", curr_depth, curr_pde.pte_base_addr);
  u64 next_entry_addr = 0;

  u32 shift_amount = total_vaddr_bits;
  shift_amount -= total_pdb_bits;
  // for each pdb shift 9 more
  shift_amount -= ((pt_depth - curr_depth) * 9);

  // shift address and mask out unused bits
  u64 next_pde_idx = va_addr &amp;gt;&amp;gt; shift_amount;
  next_pde_idx &amp;amp;= 0x1ff;

  // if on vram we need to apply this offset
  if (!curr_pde.system) curr_pde.pte_base_addr -= fb_offset;

  next_entry_addr = curr_pde.pte_base_addr + next_pde_idx * 8;
  curr_depth--;

  if (!curr_pde.system) {
   dev_linear_vram(dev, next_entry_addr, 8, &amp;amp;entry_bits);
   curr_pde = decode_pde(entry_bits);
   printf(
     "\tPage Dir Entry(%u):\n\t  Addr:0x%lx\n\t  Base: 0x%lx\n\n\t        ↓\n\n",
     curr_depth,
     next_entry_addr,
     curr_pde.pte_base_addr);
  } else {
   HDB_ASSERT(false, "GTT physical memory access is not implemented yet");
  }

  if (!curr_pde.valid) { break; }

  if (curr_pde.pte) {
   // PDB0 can act as a pte
   // also I'm making an assumption here that UMRs code doesn't make
   // that the the PDB0 as PTE path can't have the further bit set
   pde0_is_pte = true;
   break;
  }
 }

 if (pde0_is_pte) { HDB_ASSERT(false, "PDE0 as PTE is not implemented yet"); }

 // page_table_block_size is the number of 2MiB regions covered by a PTB
 // If we set it to 0, then PTB cover 2 MiB
 // If it's 9 PTB cover 1024 MiB
 // pde0_block_fragment_size tells us how many 4 KiB regions each PTE covers
 // If it's 0 PTEs cover 4 KiB
 // If it's 9 PTEs cover 2 MiB
 // So the number of PTEs in a PTB is 2^(9+ptbs-pbfs)
 //
 // size here is actually the log_2 of the size
 u32 pte_page_size  = curr_pde.frag_size;
 u32 ptes_per_ptb   = 9 + ptb_size - pte_page_size;
 u64 pte_index_mask = (1ul &amp;lt;&amp;lt; ptes_per_ptb) - 1;

 u32 pte_bits_count   = pte_page_size + 12;
 u64 page_offset_mask = (1ul &amp;lt;&amp;lt; pte_bits_count) - 1; // minimum of 12

 u64 pte_index = (va_addr &amp;gt;&amp;gt; pte_bits_count) &amp;amp; pte_index_mask;
 u64 pte_addr  = curr_pde.pte_base_addr + pte_index * 8;

 pte_t pte = { 0 };
 if (!curr_pde.system) {
  dev_linear_vram(dev, pte_addr, 8, &amp;amp;entry_bits);
  pte = decode_pte(entry_bits);

  printf("\tPage Table Entry: 0x%lx\n", pte.page_base_addr);
 } else {
  HDB_ASSERT(false, "GTT physical memory access is not implemented yet");
 }

 if (pte.further) { HDB_ASSERT(false, "PTE as PDE walking is not implemented yet"); }
 if (!pte.system) pte.page_base_addr -= fb_offset;

 u64 offset_in_page = va_addr &amp;amp; page_offset_mask;
 u64 physical_addr  = pte.page_base_addr + offset_in_page;
 printf("\tFinal Physical Address: 0x%lx\n", physical_addr);
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Other processes need to have a s_trap instruction or have trap on exception flags set, which is not true for most normal GPU processes. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Available since RDNA3, if I’m not mistaken. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;VGPRs are unique per thread, and SGPRs are unique per wave ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can get that by subtracting the current program counter from the address of the code buffer. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46193931</guid><pubDate>Mon, 08 Dec 2025 16:06:14 +0000</pubDate></item><item><title>Google confirms Android attacks; no fix for most Samsung users</title><link>https://www.forbes.com/sites/zakdoffman/2025/12/08/google-confirms-android-attacks-no-fix-for-most-samsung-users/</link><description>&lt;doc fingerprint="b532c58180f452a3"&gt;
  &lt;main&gt;
    &lt;p&gt;Android is under attack. Google issued a warning on Dec.1 along with what is essentially an emergency update. This was rushed out to all Pixel users. But for most Samsung users, these fixes are not yet available, despite attacks now underway.&lt;/p&gt;
    &lt;p&gt;Google confirms CVE-2025-48633 and CVE-2025-48572 “may be under limited, targeted exploitation," with attacks that can achieve “remote denial of service" on target smartphones "with no additional execution privileges needed.”&lt;/p&gt;
    &lt;p&gt;Samsung confirmed its own fixes within hours of Google’s warning. It also fixed three other vulnerabilities disclosed by Google’s Project Zero, which studies zero-days "in the hardware and software systems that are depended upon by users around the world.”&lt;/p&gt;
    &lt;p&gt;Just 24 hours after Google confirmed the Android attacks, the U.S. cyber defense agency issued its own warning, mandating federal staff update or stop using phones. “Android’s Framework,” CISA says on its known exploited vulnerability website, “contains an unspecified vulnerability that allows for privilege escalation.”&lt;/p&gt;
    &lt;p&gt;But as always when zero-day attacks are disclosed, Android’s disconnect is highlighted. “Samsung is the king of Android,” Android Authority pronounced over the weekend. “Its global market share among Android makers exceeds 30%. In other words, almost one in three people who buy an Android phone end up choosing Samsung.”&lt;/p&gt;
    &lt;p&gt;Samsung should come first — not Pixel, with its modest market share. But that won’t happen. Samsung bears responsibility for changing an update cycle that still runs a full month to deploy critical fixes to its user base. And it bears responsibility for the lack of seamless updates on all but the Galaxy S25 and one random, mid-range phone.&lt;/p&gt;
    &lt;p&gt;But in reality, Samsung (and the other Android OEMs) cannot compete with Google and its unique control over hardware and software. Its phones will always come first. First to new versions of the OS, first to new feature releases, first to security updates. That’s why One UI 7 and One UI 8. (Android 15 and 16) were so delayed, frustrating so many.&lt;/p&gt;
    &lt;p&gt;All Samsung Galaxy phones will get the update — assuming they’re on the monthly schedule. And some may get the updates even if they’re not. But it will deploy by model, region and carrier. Bit by bit. And in a world where Pixel is quick and Apple is quick, Samsung cannot afford to be slow. It seems inevitable that Android must change.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194315</guid><pubDate>Mon, 08 Dec 2025 16:32:50 +0000</pubDate></item><item><title>Let's put Tailscale on a jailbroken Kindle</title><link>https://tailscale.com/blog/tailscale-jailbroken-kindle</link><description>&lt;doc fingerprint="77000a1e01fd3fdc"&gt;
  &lt;main&gt;
    &lt;p&gt;“It’s a rite of passage to run Tailscale on weird devices.”&lt;/p&gt;
    &lt;p&gt;So writes Mitanshu Sukhwani on his blog, detailing the steps for getting Tailscale onto a jailbroken Kindle. Getting there, and seeing a kindle entry with a satisfying green dot in your Tailscale admin console, takes some doing. But take the trip, and you’ll end up with an e-reader that can run some neat unofficial apps, and is more open to third-party and DRM-free ebooks. And with a Tailscale connection, it’s easier to connect to files and a command line on your underpowered little Linux slab.&lt;/p&gt;
    &lt;p&gt;“For me, it's the freedom of being able to do anything with the device I own,” Sukhwani writes by email. “What I can do with the freedom is a different story.”&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a jailbroken Kindle, exactly?&lt;/head&gt;
    &lt;p&gt;Jailbreaking refers to removing the software restrictions on a device put there by its maker. Getting around these restrictions, typically by gaining “root” or administrative access, allows for accessing operating system internals, running unapproved software, and generally doing more things than a manufacturer intended. With the Kindle, you still get the standard Kindle reading experience, including Amazon's store and the ability to send the Kindle books from apps like Libby. You just add many more options, too.&lt;/p&gt;
    &lt;p&gt;The term gained purchase after the first iPhone’s debut in mid-2007; since then, nearly every device with a restricted environment has gained its own jailbreaking scene, including Kindles (debuting five months after the iPhone).&lt;/p&gt;
    &lt;p&gt;Kindle jailbreaks come along every so often. Right now, an unlocking scheme based on Amazon’s own lockscreen ads, “AdBreak,” is available for all but the most up-to-date Kindles (earlier than firmware version 5.18.5.0.2). I know this because I wrote this paragraph and the next on my 11th-generation Kindle, using the open-source Textadept editor, a Bluetooth keyboard, and Tailscale to move this draft file around.&lt;/p&gt;
    &lt;p&gt;One paragraph doesn’t seem that impressive until you consider that on a standard Kindle, you cannot do any of that. Transferring files by SSH, or Taildrop, is certainly not allowed. And that’s in addition to other upgrades you can get by jailbreaking a Kindle, including the feature-rich, customizable e-reader KOReader, and lots of little apps available in repositories like KindleForge.&lt;/p&gt;
    &lt;p&gt;If your Kindle has been connected to Wi-Fi all this time (as of early December 2025), it may have automatically updated itself and no longer be ready for jailbreaking. If you think it still has a chance, immediately put it into airplane mode and follow along.&lt;/p&gt;
    &lt;p&gt;Obligatory notice here: You’re running a risk of bricking your device (having it become unresponsive and unrecoverable) and voiding your warranty when you do this. That having been noted, let's dig further.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Tailscale adds to a jailbroken Kindle&lt;/head&gt;
    &lt;p&gt;Tailscale isn’t necessary on a jailbroken Kindle, but it really helps. Here are some of the ways Tailscale makes messing about with an opened-up Kindle more fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A persistent IP address (100.xx.yyy.zzz), just like any other Tailscale device, instead of having to remember yet another 192.168.random.number&lt;/item&gt;
      &lt;item&gt;Easier SSH access with magicDNS: ssh root@kindle and you’re in&lt;/item&gt;
      &lt;item&gt;Taildrop for sending files to whatever Kindle directory you want&lt;/item&gt;
      &lt;item&gt;Setting up a self-hosted Calibre Web library with Tailscale, then securely grabbing books from it anywhere with KOReader.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key to the Kindle-plus-Tailscale experience is an easier way (SSH and Taildrop) to get epub, mobi, and other e-book and document formats into the /documents folder, ready for your KOReader sessions. Tailscale also helps with setting up some of the key jailbreak apps, saving you from plugging and unplugging the Kindle into a computer via USB cord (and then finding a second USB cord, because the first one never works, for some reason).&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting your Kindle ready&lt;/head&gt;
    &lt;p&gt;What follows is by no means a comprehensive guide to jailbreaking and accessing your Kindle. You will want to read the documentation for each tool and app closely. Pay particular attention to which Kindle you have, which version number of the Kindle firmware it’s running, and how much space you have left on that device.&lt;/p&gt;
    &lt;p&gt;The first step is to check your Kindle’s version number (Settings &amp;gt; Device info) and see if there is a jailbreak method available for it. The Kindle Modding Wiki is the jailbreaking community’s go-to resource. As of this writing, there is a “WinterBreak” process available for Kindles running firmware below 15.18.1, and AdBreak is available for firmwares from 15.18.1 through 5.18.5.0.1.&lt;/p&gt;
    &lt;p&gt;If your Kindle’s version number fits one of those ranges, put it in Airplane mode and move on. If not, you’re going to have to wait until the next jailbreak method comes along.&lt;/p&gt;
    &lt;head rend="h2"&gt;The actual jailbreaking part&lt;/head&gt;
    &lt;p&gt;Before you dive in, have a computer (PC, Mac, or Linux) and USB cable that works with your Kindle handy. Have your Kindle on reliable Wi-Fi, like your home network—but don’t take your Kindle off airplane mode if you’ve been keeping it that way.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow these steps to jailbreak your Kindle. The techniques are different, but you may need to do some other tasks, like enable advertisements, or fill your Kindle with junk files to prevent automatic updates midway through the process.&lt;/item&gt;
      &lt;item&gt;Install a hotfix and disable over-the-air updates so that you can keep your Kindle on Wi-Fi and not have its jailbreak undone&lt;/item&gt;
      &lt;item&gt;Install the Kindle Unified Application Launcher (KUAL) and MRPI (MobileRead Package Installer). KUAL is vital to installing most jailbroken apps, including Tailscale.&lt;/item&gt;
      &lt;item&gt;You will almost certainly want to install KOReader, too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Those bits above are standard jailbreaking procedures. If you want Tailscale on your Kindle, you’ll go a bit further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding Tailscale to a jailbroken Kindle&lt;/head&gt;
    &lt;p&gt;Make sure you have KUAL and MRPI installed and working. Next up: install this “simple” version of USBNetworking for Kindle.&lt;/p&gt;
    &lt;p&gt;Before you go further, you’ll want to choose between Mitanshu’s “standard” Tailscale repository, or the fork of it that enables Taildrop. I recommend the Taildrop-enabled fork; if it goes wrong, or stops being updated, it’s fairly easy (relative to doing this kind of project) to wipe it and go back to Mitanshu’s “vanilla” version.Either way, you’ll want to get USB access to your Kindle for this next part. If you toggled on USBNetworking to try it out, toggle it off; you can’t get USB access while it’s running, as its name somewhat implies.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the Tailscale/KUAL repository of your choice using git clone or download a ZIP from the Code button on GitHub&lt;/item&gt;
      &lt;item&gt;Head to Tailscale’s page of static Linux binaries and grab the latest arm (not arm64) release&lt;/item&gt;
      &lt;item&gt;Copy the tailscale and tailscaled binaries from the Tailscale download and place them into the /extensions/tailscale/bin directory of the KUAL/Kindle repository you’ll be copying over&lt;/item&gt;
      &lt;item&gt;Head to your Tailscale admin console and generate an authentication key. Name it something like kindle; you’ll want to enable the “Reusable” and “Pre-approved” options. Copy the key that is generated.&lt;/item&gt;
      &lt;item&gt;Open the file extensions/tailscale/config/auth_key.txt for editing while it is on your (non-Kindle) computer. Paste in the key text you generated.&lt;/item&gt;
      &lt;item&gt;If you’re using the variant with Taildrop, you can set a custom directory in which to deliver Taildrop files by editing extensions/tailscale/config/taildrop_dir.txt; setting /mnt/us/documents makes sense if you’re mostly sending yourself things to read in KOReader.&lt;/item&gt;
      &lt;item&gt;Head into the extensions folder on your computer and copy the tailscale folder you’ve set up into the extensions folder on your Kindle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all that done, open up KUAL on your Kindle. Go into USBNetLite and click USBNetwork Status to ensure it is enabled (tap the Toggle button if not). Go back (with the “/” button at the bottom), tap Tailscale, and first tap Start Tailscaled (note the “d” at the end). Wait about 10 seconds to give the Tailscaled daemon time to start, then tap Start Tailscale.&lt;/p&gt;
    &lt;p&gt;If everything is settled, you should be able to see your Kindle as connected on your Tailscale admin console. Once you’ve finished smiling to yourself, click the three dots on the right-hand side of the Kindle row and select “Disable key expiry.” In most situations, you’re better off not having to patch a new key value into a Kindle text file every few months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enjoy your (slightly) less wonky Kindle&lt;/head&gt;
    &lt;p&gt;With Tailscale installed, it’s easier to get into your Kindle via SSH for file management and installing and configuring other apps. Getting a Bluetooth keyboard to work via the Kindle’s quirky command-line Bluetooth interface would not have been fun using a touchscreen keyboard.&lt;/p&gt;
    &lt;p&gt;Because the Kindle is on your tailnet, it can access anything else you have hosted there. Kindles set up this way can use tools like the Shortcut Browser to become dashboards for Home Assistant, or access a self-hosted Calibre-Web e-book server (with some tweaking).&lt;/p&gt;
    &lt;p&gt;Having Taildrop handy, and having it drop files directly into the documents folder, is probably my favorite upgrade. I was on my phone, at a train station, when I came across Annalee Newitz’s Automatic Noodle at Bookshop.org. I bought it on my phone and downloaded the DRM-free epub file. When I got home, I opened and unlocked my Kindle, sent the epub to the Kindle via Taildrop, then tapped Receive Taildrop Files in the Tailscale app inside KUAL. Epubs, PDFs, comic book archives, DjVu files—they’re all ready to be dropped in.&lt;/p&gt;
    &lt;p&gt;If you’ve gotten Tailscale to run on weird (or just uncommon) devices, we’d more than love to hear about it. Let us know on Reddit, Discord, Bluesky, Mastodon, or LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194337</guid><pubDate>Mon, 08 Dec 2025 16:34:08 +0000</pubDate></item><item><title>Hunting for North Korean Fiber Optic Cables</title><link>https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/</link><description>&lt;doc fingerprint="2fecda19652117ea"&gt;
  &lt;main&gt;
    &lt;p&gt;Before we go any further, one thing that I want to make clear is that the word assume is going to be doing some heavy lifting throughout this post. This was a rabbit hole that I recently went down and I probably have more questions than answers, but I still wanted to document what I had found so far. If you have additional information or findings you want to share, as always feel free to reach out: contact@nkinternet.com.&lt;/p&gt;
    &lt;p&gt;It all started with a PowerPoint that I came across a few weeks ago. It was presented by the DPRK to the ICAO on the state of their aviation industry and their ADS-B deployment inside North Korea. However, one slide in particular caught my eye because it showed a fiber optic cable running across the country&lt;/p&gt;
    &lt;p&gt;This got me wondering more about the physical layout of the network inside North Korea. From the map we know that there’s a connection between Pyongyang and Odaejin, although given the mountains in the middle of the country it probably isn’t a direct link. There isn’t a lot of information on fiber in North Korea, but there are a few outside sources that help provide clues about how things might be laid out.&lt;/p&gt;
    &lt;p&gt;Historic Fiber Information&lt;/p&gt;
    &lt;p&gt;38North first reported the connection from Russia’s TTK to the DPRK over the Korea–Russia Friendship Bridge back in 2017. Additionally, a picture found on Flickr looking toward Tumangang after the bridge doesn’t show any utility poles and instead seems to display some kind of infrastructure in the grass to the side of the tracks. Assuming this interpretation is correct, the fiber is likely buried underground as it enters the country and passes through the vicinity of Tumangang Station.&lt;/p&gt;
    &lt;p&gt;According to a report from The Nautilus Institute we can gather a few additional details about the internet inside North Korea&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One of the first lines was installed in September 1995 between Pyongyang and Hamhung&lt;/item&gt;
      &lt;item&gt;In February 1998 a link between Pyongyang and Sinuiju was completed&lt;/item&gt;
      &lt;item&gt;As of 2000, DPRK’s operational optical fiber telecom lines included: Pyongyang – Hamhung; Pyongyang – Sinuiju including all cities and counties in North Pyongan Province; Hamhung Rajin-Sonbong; Rajin-Songbong – Hunchun (China), Pyongyang – Nampo.&lt;/item&gt;
      &lt;item&gt;In 2003 the original domestic cell phone network was built for North Korean citizens in Pyongyang, Namp’o, reportedly in all provincial capitals, on the Pyongyang-Myohyangsan tourist highway, and the Pyongyang-Kaesong and Wonsan-Hamhung highways&lt;/item&gt;
      &lt;item&gt;The Kwangmyong network’s data is transmitted via fiber optic cable with a backbone capacity of 2.5 GB per second between all the provinces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on these notes, it starts to paint a picture that the fiber link coming from Russia likely travels down the east coast of the DPRK before connecting to Pyongyang. Several city pairs—Pyongyang–Hamhung and Rajin–Sonbong—line up with earlier deployments of east-coast fiber infrastructure.&lt;/p&gt;
    &lt;p&gt;Kwangmyong Internal Topology&lt;/p&gt;
    &lt;p&gt;The report also notes that all of the provinces in North Korea were connected to the Kwangmyong via fiber. The Kwangmyong for those not familiar is the intranet that most citizens in the DPRK can access as they do not have access to the outside internet. While not much information is available about the Kwangmyong, these notes from Choi Sung, Professor of Computer Science at Namseoul University provides some additional details on how the network is laid how, as well as information on the regional networks that are connected. A map provided in his notes shows some of the main points of the Kwangmyong with three of them located along the northeast of North Korea.&lt;/p&gt;
    &lt;p&gt;Railways, Roads, and Practical Fiber Routing&lt;/p&gt;
    &lt;p&gt;This starts to paint a rough picture of how the network is physically deployed in North Korea but we can also look to some outside sources to get some confirmation. 38North once again provides some great detail on cell phone towers in North Korea. The interesting thing being an apparent line down the east coast which follows major roads and highways but would also in theory have easier access to the fiber back haul to support the cell network.&lt;/p&gt;
    &lt;p&gt;All of this seems to suggest that the fiber lines were run along major roads and railways up the east coast. A map from Beyond Parallel shows the major rail lines, which has the Pyongra line up the east coast.&lt;/p&gt;
    &lt;p&gt;Looking For Clues Along the Railway&lt;/p&gt;
    &lt;p&gt;Some additional digging for pictures from along the line suggest that there is infrastructure deployed along the tracks, although it’s difficult to confirm from pictures exactly what is buried. The following shows what appears to be a junction box at the base of a pole along the line.&lt;/p&gt;
    &lt;p&gt;The line does have a path along it as well with mile markers. While it is used by bikes and pedestrians, it provides a nice path for supporting fiber and other communications runs along the tracks.&lt;/p&gt;
    &lt;p&gt;The Pyongra line also crosses through the mountains at points but it is assumed at certain junctions the fiber was laid along the AH 6/National Highway 7 up the coast as there are parts of the line discovered that do not have a path along the tracks. In these places it is assumed they follow the road, although finding pictures of the highway to further examine is challenging.&lt;/p&gt;
    &lt;p&gt;Lastly at certain stations we can see utility boxes along the side of the track suggesting buried conduits/cables are laid along the tracks.&lt;/p&gt;
    &lt;p&gt;From a video taken in 2012 there does appear to be some signs of objects along the tracks, although difficult to confirm due to the video quality. The screenshot below is the clearest I could find of a rectangular box buried in a clearing along the line.&lt;/p&gt;
    &lt;p&gt;Based on this information of what is confirmed and looking at major cities, it appears there is a route that follows Pyongyang → Wonsan → Hamhung → Chongjin → Rajin → Tumangang which follows the Pyongra line as well as the AH 6/National Highway 7 up the coast. The following map highlights a rough path.&lt;/p&gt;
    &lt;p&gt;Interestingly by mapping out the possible fiber locations we can start to draw conclusions based on other sources. According to a video by Cappy’s Army he proposes that when the US Navy Seals landed in NOrth Korea in 2019 the most likely place this would have occurred is Sinpo. As the goal was to depoy a covert listening device this could also line up with supporting the idea that a fiber backbone runs down the east coast of North Korea as Sinpo would be relatively close.&lt;/p&gt;
    &lt;p&gt;What Does This Mean For the Network?&lt;/p&gt;
    &lt;p&gt;In addition to the fiber link via Russia, the other fiber optic cable into North Korea comes in via China by way of Sinuiju and Dandong. Although we don’t know for sure where servers are deployed inside North Korea, based on the map of Kwangmyong the first assumption is that things are mainly centralized in Pyongyang.&lt;/p&gt;
    &lt;p&gt;Out of the 1,024 IPs assigned to North Korea we observe the following behavior based on the CIDR block:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;175.45.176.0/24 is exclusively routed via China Unicom&lt;/item&gt;
      &lt;item&gt;175.45.177.0/24 is exclusively routed via Russia TransTelekom&lt;/item&gt;
      &lt;item&gt;175.45.178.0/24 is dual-homed and can take either path before crossing into North Korea&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this information in mind, running a traceroute with the TCP flag set gives us a slightly better look at how traffic behaves once it reaches the country. For the following tests we’re going to assume there is a fiber path on the west coming in from China toward Pyongyang, as well as a path on the east side coming from Russia.&lt;/p&gt;
    &lt;p&gt;From the US east coast to 175.45.176.71, the final hop in China before entering North Korea shows roughly 50 ms of additional latency before reaching the DPRK host. This suggests there may be extra devices, distance, or internal routing inside the country before the packet reaches its final destination.&lt;/p&gt;
    &lt;quote&gt;10 103.35.255.254 (103.35.255.254) 234.306 ms 234.082 ms 234.329 ms&lt;lb/&gt;11 * * *&lt;lb/&gt;12 * * *&lt;lb/&gt;13 * * *&lt;lb/&gt;14 175.45.176.71 (175.45.176.71) 296.081 ms 294.795 ms 294.605 ms&lt;lb/&gt;15 175.45.176.71 (175.45.176.71) 282.938 ms 284.446 ms 282.227 ms&lt;/quote&gt;
    &lt;p&gt;Interestingly, running a traceroute to 175.45.177.10 shows a similar pattern in terms of missing hops, but with much lower internal latency. In fact, the ~4 ms difference between the last Russian router and the DPRK host suggests the handoff between Russia and North Korea happens very close—network-wise—to where this device is located. This contrasts with the China path, which appears to take a longer or more complex route before reaching its final destination.&lt;/p&gt;
    &lt;quote&gt;10 188.43.225.153 185.192 ms 183.649 ms 189.089 ms&lt;lb/&gt;11 * *&lt;lb/&gt;12 * *&lt;lb/&gt;13 * *&lt;lb/&gt;14 175.45.177.10 195.996 ms 186.801 ms 186.353 ms&lt;lb/&gt;15 175.45.177.10 188.886 ms 201.103 ms 193.334&lt;/quote&gt;
    &lt;p&gt;If everything is centralized in Pyongyang this would mean the handoff from Russia is completed in Pyongyang as well. However, it could also indicate that 175.45.177.0/24 is not hosted in Pyongyang at all and is instead located closer to the Russia–North Korea border. More testing is definitely required however before any conclusions can be drawn about where these devices physically reside.&lt;/p&gt;
    &lt;p&gt;What can we learn from all of this?&lt;/p&gt;
    &lt;p&gt;Making some assumptions we can get a better idea of how the internet works and is laid out inside North Korea. While not much is officially confirmed using some other sources we can get a possible idea of how things work. As mentioned at the start, the word assume does a lot of heavy lifting. However if you do have other information or ideas feel free to reach out at contact@nkinternet.com&lt;/p&gt;
    &lt;head rend="h3"&gt;Discover more from North Korean Internet&lt;/head&gt;
    &lt;p&gt;Subscribe to get the latest posts sent to your email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194384</guid><pubDate>Mon, 08 Dec 2025 16:38:08 +0000</pubDate></item><item><title>A series of tricks and techniques I learned doing tiny GLSL demos</title><link>https://blog.pkh.me/p/48-a-series-of-tricks-and-techniques-i-learned-doing-tiny-glsl-demos.html</link><description>&lt;doc fingerprint="b2a292151eec032f"&gt;
  &lt;main&gt;
    &lt;p&gt;In the past two months or so, I spent some time making tiny GLSL demos. I wrote an article about the first one, Red Alp. There, I went into details about the whole process, so I recommend to check it out first if you're not familiar with the field.&lt;/p&gt;
    &lt;p&gt;We will look at 4 demos: Moonlight, Entrance 3, Archipelago, and Cutie. But this time, for each demo, we're going to cover one or two things I learned from it. It won't be a deep dive into every aspect because it would be extremely redundant. Instead, I'll take you along a journey of learning experiences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moonlight&lt;/head&gt;
    &lt;code&gt;// Moonlight [460] by bµg
// License: CC BY-NC-SA 4.0
void main(){vec3 o,p,u=vec3((P+P-R)/R.y,1),Q;Q++;for(float d,a,m,i,t;i++&amp;lt;1e2;p=t&amp;lt;7.2?Q:vec3(2,1,0),d=abs(d)*.15+.1,o+=p/m+(t&amp;gt;9.?d=9.,Q:p/d),t+=min(m,d))for(p=normalize(u)*t,p.z-=5e1,m=max(length(p)-1e1,.01),p.z+=T,d=5.-length(p.xy*=mat2(cos(t*.2+vec4(0,33,11,0)))),a=.01;a&amp;lt;1.;a+=a)p.xz*=mat2(8,6,-6,8)*.1,d-=abs(dot(sin(p/a*.6-T*.3),p-p+a)),m+=abs(dot(sin(p/a/5.),p-p+a/5.));o/=4e2;O=vec4(tanh(mix(vec3(-35,-15,8),vec3(118,95,60),o-o*length(u.xy*.5))*.01),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;In Red Alp, I used volumetric raymarching to go through the clouds and fog, and it took quite a significant part of the code to make the absorption and emission convincing. But there is an alternative technique that is surprisingly simpler.&lt;/p&gt;
    &lt;p&gt;In the raymarching loop, the color contribution at each iteration becomes 1/d or c/d where d is the density of the material at the current ray position, and c an optional color tint if you don't want to work in grayscale level. Some variants exist, for example 1/d^2, but we'll focus on 1/d.&lt;/p&gt;
    &lt;head rend="h3"&gt;1/d explanation&lt;/head&gt;
    &lt;p&gt;Let's see how it looks in practice with a simple cube raymarch where we use this peculiar contribution:&lt;/p&gt;
    &lt;code&gt;void main() {
    float d, t;
    vec3 o, p,
         u = normalize(vec3(P+P-R,R.y)); // screen to world coordinate

    for (int i = 0; i &amp;lt; 30; i++) {
        p = u * t; // ray position

        p.z -= 3.; // take a step back

        // Rodriguez rotation with an arbitrary angle of π/2
        // and unaligned axis
        vec3 a = normalize(cos(T+vec3(0,2,4)));
        p = a*dot(a,p)-cross(a,p);

        // Signed distance function of a cube of size 1
        p = abs(p)-1.;
        d = length(max(p,0.)) + min(max(p.x,max(p.y,p.z)),0.);

        // Maxed out to not enter the solid
        d = max(d,.001);

        t += d; // stepping forward by that distance

        // Our mysterious contribution to the output
        o += 1./d;
    }

    // Arbitrary scale within visible range
    O = vec4(o/200., 1);
}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The signed function of the cube is from the classic Inigo Quilez page. For the rotation you can refer to Xor or Blackle article. For the general understanding of the code, see my previous article on Red Alp.&lt;/p&gt;
    &lt;p&gt;The first time I saw it, I wondered whether it was a creative take, or if it was backed by physical properties.&lt;/p&gt;
    &lt;p&gt;Let's simplify the problem with the following figure:&lt;/p&gt;
    &lt;p&gt;The glowing object sends photons that spread all around it. The further we go from the object, the more spread these photons are, basically following the inverse square law 1/r^2, which gives the photons density, where r is the distance to the target object.&lt;/p&gt;
    &lt;p&gt;Let's say we send a ray and want to know how many photons are present along the whole path. We have to "sum", or rather integrate, all these photons density measures along the ray. Since we are doing a discrete sampling (the dots on the figure), we need to interpolate the photons density between each sampling point as well.&lt;/p&gt;
    &lt;p&gt;Given two arbitrary sampling points and their corresponding distance d_n and d_{n+1}, any intermediate distance can be linearly interpolated with r=\mathrm{mix}(d_n,d_{n+1},t) where t is within [0,1]. Applying the inverse square law from before (1/r^2), the integrated photons density between these 2 points can be expressed with this formula (in t range):&lt;/p&gt;
    &lt;p&gt;t being normalized, the \Delta t is here to covers the actual segment distance. With the help of Sympy we can do the integration:&lt;/p&gt;
    &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a, b, D, t = symbols('a b D t', real=True)
&amp;gt;&amp;gt;&amp;gt; mix = a*(1-t) + b*t
&amp;gt;&amp;gt;&amp;gt; D * integrate(1/mix**2, (t,0,1)).simplify()
 D
───
a⋅b
&lt;/code&gt;
    &lt;p&gt;So the result of this integration is:&lt;/p&gt;
    &lt;p&gt;Now the key is that in the loop, \Delta t stepping is actually d_{n+1}, so we end up with:&lt;/p&gt;
    &lt;p&gt;And we find back our mysterious 1/d. It's "physically correct", assuming vacuum space. Of course, reality is more complex, and we don't even need to stick to that formula, but it was nice figuring out that this simple fraction is a fairly good model of reality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Going through the object&lt;/head&gt;
    &lt;p&gt;In the cube example we didn't go through the object, using &lt;code&gt;max(d, .001)&lt;/code&gt;. But
if we were to add some transparency, we could have used &lt;code&gt;d = A*abs(d)+B&lt;/code&gt;
instead, where &lt;code&gt;A&lt;/code&gt; could be interpreted as absorption and &lt;code&gt;B&lt;/code&gt; the pass-through,
or transparency.&lt;/p&gt;
    &lt;p&gt;I first saw this formula mentioned in Xor article on volumetric. To understand it a bit better, here is my intuitive take: the &lt;code&gt;+B&lt;/code&gt; causes a
potential penetration into the solid at the next iteration, which wouldn't
happen otherwise (or only very marginally). When inside the solid, the &lt;code&gt;abs(d)&lt;/code&gt;
causes the ray to continue further (by the amount of the distance to the closest
edge). Then the multiplication by &lt;code&gt;A&lt;/code&gt; makes sure we don't penetrate too fast
into it; it's the absorption, or "damping".&lt;/p&gt;
    &lt;p&gt;This is basically the technique I used in Moonlight to avoid the complex absorption/emission code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Entrance 3&lt;/head&gt;
    &lt;code&gt;// Entrance 3 [465] by bµg
// License: CC BY-NC-SA 4.0
#define V for(s++;d&amp;lt;l&amp;amp;&amp;amp;s&amp;gt;.001;q=abs(p+=v*s)-45.,b=abs(p+vec3(mod(T*5.,80.)-7.,45.+sin(T*10.)*.2,12))-vec3(1,7,1),d+=s=min(max(p.y,-min(max(abs(p.y+28.)-17.,abs(p.z+12.)-4.),max(q.x,max(q.y,q.z)))),max(b.x,max(b.y,b.z))))
void main(){float d,s,r=1.7,l=2e2;vec3 b,v=b-.58,q,p=mat3(r,0,-r,-1,2,-1,b+1.4)*vec3((P+P-R)/R.y*20.4,30);V;r=exp(-d*d/1e4)*.2;l=length(v=-vec3(90,30,10)-p);v/=l;d=1.;V;r+=50.*d/l/l;O=vec4(pow(mix(vec3(0,4,9),vec3(80,7,2),r*r)*.01,p-p+.45),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;This demo was probably one of the most challenging, but I'm pretty happy with its atmospheric vibe. It's kind of different than the usual demos for this size.&lt;/p&gt;
    &lt;p&gt;I initially tried with some voxels, but I couldn't make it work with the light under 512 characters (the initialization code was too large, not the branchless DDA stepping). It also had annoying limitations (typically the animation was unit bound), so I fell back to a classic raymarching.&lt;/p&gt;
    &lt;p&gt;The first thing I did differently was to use an L-∞ norm instead of an euclidean norm for the distance function: every solid is a cube so it's appropriate to use simpler formulas.&lt;/p&gt;
    &lt;p&gt;For the light, it's not an illusion, it's an actual light: after the first raymarch to a solid, the ray direction is reoriented toward the light and the march runs again (it's the &lt;code&gt;V&lt;/code&gt; macro). Hitting a solid or not defines if the
fragment should be lighten up or not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mobile bugs&lt;/head&gt;
    &lt;p&gt;A bad surprise of this demo was uncovering two driver bugs on mobile:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One with tricky for-loop compounds on Snapdragon/Adreno because I was trying hard to avoid the macros and functions.&lt;/item&gt;
      &lt;item&gt;One with chained assignments on Imagination/PowerVR (typically affect Google Pixel Pro 10).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first was worked around with the &lt;code&gt;V&lt;/code&gt; macro (actually saved 3 characters in
the process), but the 2nd one had to be unpacked and made me lose 2 characters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Isometry&lt;/head&gt;
    &lt;p&gt;Another thing I studied was how to set up the camera in a non-perspective isometric or dimetric view. I couldn't make sense of the maths from the Wikipedia page (it just didn't work), but Sympy rescued me again:&lt;/p&gt;
    &lt;code&gt;# Counter-clockwise rotation
a, ax0, ax1, ax2 = symbols('a ax0:3')
c, s = cos(a), sin(a)
k = 1-c
rot = Matrix(3,3, [
    # col 1            col 2              # col 3
    ax0*ax0*k + c,     ax0*ax1*k + ax2*s, ax0*ax2*k - ax1*s, # row 1
    ax1*ax0*k - ax2*s, ax1*ax1*k + c,     ax1*ax2*k + ax0*s, # row 2
    ax2*ax0*k + ax1*s, ax2*ax1*k - ax0*s, ax2*ax2*k + c      # row 3
])

# Rotation by 45° on the y-axis
m45 = rot.subs({a:rad(-45), ax0:0, ax1:1, ax2:0})

# Apply the 2nd rotation on the x-axis to get the transform matrices for two
# classic projections
# Note: asin(tan(rad(30))) is the same as atan(sin(rad(45)))
isometric = m45 * rot.subs({a:asin(tan(rad(30))), ax0:1, ax1:0, ax2:0})
dimetric  = m45 * rot.subs({a:         rad(30),   ax0:1, ax1:0, ax2:0})
&lt;/code&gt;
    &lt;p&gt;Inspecting the matrices and factoring out the common terms, we obtain the following transform matrices:&lt;/p&gt;
    &lt;p&gt;The ray direction is common to all fragments, so we use the central UV coordinate (0,0) as reference point. We push it forward for convenience: (0,0,1), and transform it with our matrix. This gives the central screen coordinate in world space. Since the obtained point coordinate is relative to the world origin, to go from that point to the origin, we just have to flip its sign. The ray direction formula is then:&lt;/p&gt;
    &lt;p&gt;To get the ray origin of every other pixel, the remaining question is: what is the smallest distance we need to step back the screen coordinates such that, when applying the transformation, the view wouldn't clip into the ground at y=0.&lt;/p&gt;
    &lt;p&gt;This requirement can be modeled with the following expression:&lt;/p&gt;
    &lt;p&gt;The -1 being the lowest y-screen coordinate (which we don't want into the ground). The lazy bum in me just asks Sympy to solve it for me:&lt;/p&gt;
    &lt;code&gt;x, z = symbols("x z", real=True)
u = m * Matrix([x, -1, z])
uz = solve(u[1] &amp;gt; 0, z)
&lt;/code&gt;
    &lt;p&gt;We get z&amp;gt;\sqrt{2} for isometric, and z&amp;gt;\sqrt{3} for dimetric.&lt;/p&gt;
    &lt;p&gt;With an arbitrary scale &lt;code&gt;S&lt;/code&gt; of the coordinate we end up with the following:&lt;/p&gt;
    &lt;code&gt;const float S = 50.;
vec2 u = (P+P-R)/R.y * S; // scaled screen coordinates

float A=sqrt(2.), B=sqrt(3.);

// Isometric
vec3 rd = -vec3(1)/B,
     ro = mat3(B,0,-B,-1,2,-1,A,A,A)/A/B * vec3(u, A*S + eps);

// Dimetric
vec3 rd = -vec3(B,A,B)/A/2.,
     ro = mat3(2,0,-2,-1,A*B,-1,B,A,B)/A/2. * vec3(u, B*S + eps);
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;eps&lt;/code&gt; is an arbitrary small value to make sure the y-coordinate ends up
above 0.&lt;/p&gt;
    &lt;p&gt;In Entrance 3, I used a rough approximation of the isometric setup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Archipelago&lt;/head&gt;
    &lt;code&gt;// Archipelago [472] by bµg
// License: CC BY-NC-SA 4.0
#define r(a)*=mat2(cos(a+vec4(0,11,33,0))),
void main(){vec3 p,q,k;for(float w,x,a,b,i,t,h,e=.1,d=e,z=.001;i++&amp;lt;50.&amp;amp;&amp;amp;d&amp;gt;z;h+=k.y,w=h-d,t+=d=min(d,h)*.8,O=vec4((w&amp;gt;z?k.zxx*e:k.zyz/20.)+i/1e2+max(1.-abs(w/e),z),1))for(p=normalize(vec3(P+P-R,R.y))*t,p.zy r(1.)p.z+=T+T,p.x+=sin(w=T*.4)*2.,p.xy r(cos(w)*e)d=p.y+=4.,h=d-2.3+abs(p.x*.2),q=p,k-=k,a=e,b=.8;a&amp;gt;z;a*=.8,b*=.5)q.xz r(.6)p.xz r(.6)k.y+=abs(dot(sin(q.xz*.4/b),R-R+b)),k.x+=w=a*exp(sin(x=p.x/a*e+T+T)),p.x-=w*cos(x),d-=w;}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;For this infinite procedurally generated Japan, I wanted to mark a rupture with my red/orange obsession. Technically speaking, it's actually fairly basic if you're familiar with Red Alp. I used the same noise for the mountains/islands, but the water uses a different noise.&lt;/p&gt;
    &lt;p&gt;The per octave noise curve is &lt;code&gt;w=exp(sin(x))&lt;/code&gt;, with the particularity of
shifting the &lt;code&gt;x&lt;/code&gt; coordinate with its derivative: &lt;code&gt;x-=w*cos(x)&lt;/code&gt;. This is some
form of domain warping that gives the nice effect here. When I say &lt;code&gt;x&lt;/code&gt;, I'm
really referring to the x-axis position. It is not needed to work with the
z-component (xz forms the flat plane) because each octave of the fbm has a
rotation that "mixes" both axis, so &lt;code&gt;z&lt;/code&gt; is actually backed in &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;I didn't come up with the formula, but found it first one this video by Acerola. I don't know if he's the original author, but I've seen the formula being replicated in various places.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cutie&lt;/head&gt;
    &lt;code&gt;// Cutie [602] by bµg
// License: CC BY-NC-SA 4.0
#define V vec3
#define L length(p
#define C(A,B,X,Y)d=min(d,-.2*log2(exp2(X-L-A)/.2)+exp2(Y-L-B)/.2)))
#define H(Z)S,k=fract(T*1.5+s),a=V(1.3,.2,Z),b=V(1,.3*max(1.-abs(3.*k-1.),z),Z*.75+3.*max(-k*S,k-1.)),q=b*S,q+=a+sqrt(1.-dot(q,q))*normalize(V(-b.y,b.x,0)),C(a,q,3.5,2.5),C(q,a-b,2.5,2.)
void main(){float i,t,k,z,s,S=.5,d=S;for(V p,q,a,b;i++&amp;lt;5e1&amp;amp;&amp;amp;d&amp;gt;.001;t+=d=min(d,s=L+V(S-2.*p.x,-1,S))-S))p=normalize(V(P+P-R,R.y))*t,p.z-=5.,p.zy*=mat2(cos(vec4(1,12,34,1))),p.xz*=mat2(cos(sin(T)+vec4(0,11,33,0))),d=1.+p.y,C(z,V(z,z,1.2),7.5,6.),s=p.x&amp;lt;z?p.x=-p.x,z:H(z),s+=H(1.);O=vec4(V(exp(-i/(s&amp;gt;d?1e2:9.))),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;Here I got cocky and thought I could manage to fit it in 512 chars. I failed, by 90 characters. I did use the smoothmin operator for the first time: every limb of the body of Cutie is composed of two spheres creating a rounded cone (two sphere of different size smoothly merged like metaballs).&lt;/p&gt;
    &lt;p&gt;Then I used simple IK kinetics for the animation. Using leg parts with a size of 1 helped simplifying the formula and make it shorter.&lt;/p&gt;
    &lt;p&gt;You may be wondering about the smooth visuals itself: I didn't use the depth map but simply the number of iterations. Due to the nature of the raymarching algorithm, when a ray passes close to a shape, it slows down significantly, increasing the number of iterations. This is super useful because it exaggerate the contour of the shapes naturally. It's wrapped into an exponential, but &lt;code&gt;i&lt;/code&gt;
defines the output color directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;I will continue making more of those, keeping my artistic ambition low because of the 512 characters constraint I'm imposing on myself.&lt;/p&gt;
    &lt;p&gt;You may be wondering why I keep this obsession about 512 characters, and many people called me out on this one. There are actually many arguments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A tiny demo has to focus on one or two very scoped aspects of computer graphics, which makes it perfect as a learning support.&lt;/item&gt;
      &lt;item&gt;It's part of the artistic performance: it's not just techniques and visuals, the wizardry of the code is part of why it's so impressive. We're in an era of visuals, people have been fed with the craziest VFX ever. But have they seen them with a few hundreds bytes of code?&lt;/item&gt;
      &lt;item&gt;The constraint helps me finish the work: when making art, there is always this question of when to stop. Here there is an intractable point where I just cannot do more and I have to move on.&lt;/item&gt;
      &lt;item&gt;Similarly, it prevents my ambition from tricking me into some colossal project I will never finish or even start. That format has a ton of limitations, and that's its strength.&lt;/item&gt;
      &lt;item&gt;Working on such a tiny piece of code for days/weeks just brings me joy. I do feel like a craftsperson, spending an unreasonable amount of time perfecting it, for the beauty of it.&lt;/item&gt;
      &lt;item&gt;I'm trying to build a portfolio, and it's important for me to keep it consistent. If the size limit was different, I would have done things differently, so I can't change it now. If I had hundreds more characters, Red Alp might have had birds, the sky opening to lit a beam of light on the mountains, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why 512 in particular? It happens to be the size of a toot on my Mastodon instance so I can fit the code there, and I found it to be a good balance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194477</guid><pubDate>Mon, 08 Dec 2025 16:44:42 +0000</pubDate></item><item><title>Microsoft has a problem: lack of demand for its AI products</title><link>https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai</link><description>&lt;doc fingerprint="2f162c409bb6c8d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft has a problem: nobody wants to buy or use its shoddy AI products — as Google's AI growth begins to outpace Copilot products&lt;/head&gt;
    &lt;p&gt;A new report details how Microsoft has cut some internal goals for its AI sales people, why? Nobody wants to use its weak products.&lt;/p&gt;
    &lt;p&gt;Enjoy our content? Make sure to set Windows Central as a preferred source in Google Search, and find out why you should so that you can stay up-to-date on the latest news, reviews, features, and more.&lt;/p&gt;
    &lt;p&gt;If there's one thing that typifies Microsoft under CEO Satya Nadella's tenure: it's a general inability to connect with customers.&lt;/p&gt;
    &lt;p&gt;Microsoft shut down its retail arm quietly over the past few years, closed up shop on mountains of consumer products, while drifting haphazardly from tech fad to tech fad. From blockchain to "metaverse" and now to artificial intelligence — Microsoft CEO Satya Nadella can't seem to prioritize effectively, and the cracks are starting to shine through.&lt;/p&gt;
    &lt;p&gt;A recent report from The Information detailed how Microsoft's internal AI efforts are going awry, with cut forecasts and sales goals for its Azure AI products across the board. The Information said that Microsoft's sales people are "struggling" to meet goals, owing to a complete lack of demand. Microsoft denied the reports, but it can't deny market share growth trends — all of which point to Google Gemini surging ahead.&lt;/p&gt;
    &lt;p&gt;Last week we wrote about how Microsoft Copilot's backend partner OpenAI issued a "code red" situation. ChatGPT has fallen behind Google Gemini in problem solving, and Nano Banana image generation has outpaced OpenAI's own DALLE by leaps and bounds.&lt;/p&gt;
    &lt;p&gt;With OpenAI's business model under constant scrutiny and racking up genuinely dangerous levels of debt, it's become a cascading problem for Microsoft to have tied up layer upon layer of its business in what might end up being something of a lame duck.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;#&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Generative AI Chatbot&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;AI Search Market Share&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Estimated Quarterly User Growth&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT (excluding Copilot)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;61.30%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;7% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;2&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Microsoft Copilot&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14.10%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;2% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;3&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Google Gemini&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;13.40%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;12% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;4&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Perplexity&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;6.40%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;4% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Claude AI&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;3.80%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;6&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Grok&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.60%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;6% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;7&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Deepseek&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.20%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;10% ▲&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are reams of research that suggest agentic AI tools require human intervention at a frequency ratio that makes them cost ineffective, but Microsoft seems unbothered that its tools are poorly conceived.&lt;/p&gt;
    &lt;p&gt;In any case, OpenAI is supposedly going to launch future models of ChatGPT early in attempts to combat the rise of Google Gemini. I suspect the issues are deeper for Microsoft, who have worked tirelessly under Satya Nadella to create doubt around its products.&lt;/p&gt;
    &lt;p&gt;All the latest news, reviews, and guides for Windows and Xbox diehards.&lt;/p&gt;
    &lt;p&gt;SEO and analytics firm FirstPageSage has released its AI market share report for the start of December, and it shows Google Gemini actively poised to supplant Microsoft Copilot. Based on reports that Google Gemini is now actively beating ChatGPT's best models, FirstPageSage has Google Gemini sprinting past Microsoft Copilot quarter over quarter, although ChatGPT itself will remain the front runner.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google's AI advantages are accumulating, as Microsoft's disadvantages snowball&lt;/head&gt;
    &lt;p&gt;Whether it's Google's Tensor server tech or dominating position with Google Play-bound Android, Microsoft's lack of forethought and attention paid to their actual customers is starting to catch up with the firm. Nadella has sought to blame the company's unwieldy size for the lack of innovation, but it reads like an excuse to me. It's all about priorities — and Nadella has chased shareholder sentiment over delivering for its customers or employees, and that short-termism is going to put Microsoft on the backfoot if AI actually does deliver another computing paradigm shift.&lt;/p&gt;
    &lt;p&gt;Microsoft depends almost entirely on pricy NVIDIA technology for its data centers, whereas Google is actively investing to own the entire stack. Microsoft has also worked incredibly hard to cram half-baked AI features into its products, whereas Google has arguably been a lot more thoughtful in its approach. Microsoft sprinted out of the gate like a bull in a China shop, and investors rewarded them for it — but fast forward to 2025, and Google's AI products simply work better, and are more in-tune with how people might actually use them.&lt;/p&gt;
    &lt;p&gt;I am someone who is actively using the AI features across Google Android and Microsoft Windows on a day to day basis, and the delta between the two companies is growing ever wider. Basic stuff like the photo editing features on Google Pixel phones are lightyears beyond the abysmal tools found in the Microsoft Photos app on Windows. Google Gemini in Google Apps is also far smarter and far more intuitive than Copilot on Microsoft 365, as someone actively using both across the two businesses I work in.&lt;/p&gt;
    &lt;p&gt;Dare I say it, Gemini is actually helpful, and can usually execute tasks you might actually need in a day to day job. "Find me a meeting slot on this date to accommodate these timezones" — Gemini will actually do it. Copilot 365 doesn't even have the capability to schedule a calendar event with natural language in the Outlook mobile app, or even provide something as basic as clickable links in some cases. At least Xbox's Gaming Copilot has a beta tag to explain why it fails half of the time. It's truly absurd how half-baked a lot of these features are, and it's odd that Microsoft sought to ship them in this state. And Microsoft wants to make Windows 12 AI first? Please.&lt;/p&gt;
    &lt;p&gt;Microsoft's "ship it now fix it later" attitude risks giving its AI products an Internet Explorer-like reputation for poor quality, sacrificing the future to more patient, thoughtful companies who spend a little more time polishing first. Microsoft's strategy for AI seems to revolve around offering cheaper, lower quality products at lower costs (Microsoft Teams, hi), over more expensive higher-quality options its competitors are offering. Whether or not that strategy will work for artificial intelligence, which is exorbitantly expensive to run, remains to be seen.&lt;/p&gt;
    &lt;p&gt;Microsoft's savvy early investment in OpenAI gave it an incredibly strong position early on, but as we get deeper into the cycle, some cracks are starting to show. Many of Microsoft's AI products to date simply scream of a total lack of direction and utter chaos, but it's not all hopeless. Some of Microsoft's enterprise solutions for AI are seeing strong growth. Github Copilot has been something of a success story for Redmond, and Microsoft is exploring its own Maia and Cobalt chips and even language models, in attempts to decouple itself from NVIDIA and OpenAI respectively. But Satya Nadella's Microsoft has an uncanny knack for failing to deliver on promising initiatives like those.&lt;/p&gt;
    &lt;p&gt;Without a stronger emphasis on quality, Microsoft's future in AI could simply end up revolving around re-selling NVIDIA server tech and jacking up local electricity prices, rather than providing any real home-grown innovation in the space. Shareholders will be more than happy for Microsoft to simply be a server reseller, but it would be a ignoble legacy for what was previously one of tech's most innovative companies.&lt;/p&gt;
    &lt;p&gt;Follow Windows Central on Google News to keep our latest news, insights, and features at the top of your feeds!&lt;/p&gt;
    &lt;p&gt;Jez Corden is the Executive Editor at Windows Central, focusing primarily on all things Xbox and gaming. Jez is known for breaking exclusive news and analysis as relates to the Microsoft ecosystem while being powered by tea. Follow on Twitter (X) and tune in to the XB2 Podcast, all about, you guessed it, Xbox!&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194615</guid><pubDate>Mon, 08 Dec 2025 16:54:31 +0000</pubDate></item><item><title>Legion Health (YC S21) is hiring a founding engineer (SF, in-person)</title><link>https://news.ycombinator.com/item?id=46194720</link><description>&lt;doc fingerprint="3541ec5e48d7b5a8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Legion Health (YC S21) operates a psychiatric practice and is building the AI-native operations layer for mental health care. We focus on the operational backend: scheduling, intake, documentation, billing, and care coordination. These workflows—not diagnostics—are the main bottlenecks in mental health delivery.&lt;/p&gt;
      &lt;p&gt;We run our own clinic, so the systems you build ship directly into real patient care. Our agent infrastructure currently supports more than 2,000 patients with one human support lead.&lt;/p&gt;
      &lt;p&gt;We’re hiring a Founding Engineer (in-person, San Francisco). You’d work directly with the founders on:&lt;/p&gt;
      &lt;p&gt;event-driven backend systems (Node.js, TypeScript, Postgres/Supabase, AWS)&lt;/p&gt;
      &lt;p&gt;LLM agent tooling (tool use, retries, memory, context management)&lt;/p&gt;
      &lt;p&gt;internal operations tools for both humans and agents&lt;/p&gt;
      &lt;p&gt;state/coordination logic that represents a patient’s journey&lt;/p&gt;
      &lt;p&gt;HIPAA-compliant data and audit pipelines&lt;/p&gt;
      &lt;p&gt;We’re open to backend or full-stack/product engineers who think in systems and have owned real workflows end-to-end. Prior experience with LLMs is optional; interest is required.&lt;/p&gt;
      &lt;p&gt;Details: full-time, in-person SF, salary $130k–$180k, equity 0.1–0.6%.&lt;/p&gt;
      &lt;p&gt;Apply here: https://www.ycombinator.com/companies/legion-health/jobs/oc6...&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194720</guid><pubDate>Mon, 08 Dec 2025 17:01:11 +0000</pubDate></item><item><title>Launch HN: Nia (YC S25) – Give better context to coding agents</title><link>https://www.trynia.ai/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46194828</guid><pubDate>Mon, 08 Dec 2025 17:10:14 +0000</pubDate></item><item><title>Show HN: DuckDB for Kafka Stream Processing</title><link>https://sql-flow.com/docs/tutorials/intro/</link><description>&lt;doc fingerprint="e508bd350561bb82"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Quickstart&lt;/head&gt;
    &lt;p&gt;Create a stream processor that reads data from Kafka in less than 5 minutes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;p&gt;Get started by running a stream processor that executes SQL against a kafka stream and writes the output to the console.&lt;/p&gt;
    &lt;head rend="h3"&gt;What you'll need&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Docker&lt;/item&gt;
      &lt;item&gt;A copy of turbolytics/sql-flow cloned on your local machine (https://github.com/turbolytics/sql-flow)&lt;/item&gt;
      &lt;item&gt;turbolytics/sql-flow Python dependencies installed&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd path/to/turbolytics/sql-flow/github/repo &amp;amp;&amp;amp; pip install -r requirements.txt&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The turbolytics/sql-flow docker image&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;docker pull turbolytics/sql-flow:latest&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kafka running on your local machine&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd path/to/turbolytics/sql-flow/github/repo &amp;amp;&amp;amp; docker-compose -f dev/kafka-single.yml up -d&lt;/code&gt;
    &lt;head rend="h2"&gt;Test the SQLFlow configuration file&lt;/head&gt;
    &lt;p&gt;SQLFlow ships with cli support to test a stream configuration against any fixture file of test data. The goal is to support testing and linting of a configuration file before executing in a stream environment.&lt;/p&gt;
    &lt;p&gt;Run the invoke command to test the configuration file against a set of test data:&lt;/p&gt;
    &lt;code&gt;docker run -v $(pwd)/dev:/tmp/conf -v /tmp/sqlflow:/tmp/sqlflow turbolytics/sql-flow:latest dev invoke /tmp/conf/config/examples/basic.agg.mem.yml /tmp/conf/fixtures/simple.json&lt;/code&gt;
    &lt;p&gt;The following output should show:&lt;/p&gt;
    &lt;code&gt;[{'city': 'New York', 'city_count': 28672}, {'city': 'Baltimore', 'city_count': 28672}]&lt;/code&gt;
    &lt;head rend="h2"&gt;Run SQLFlow against a Kafka stream&lt;/head&gt;
    &lt;p&gt;This section runs SQLFlow as a stream processor that reads data from a Kafka topic and writes the output to the console. SQLFow runs as a daemon and will continuously read data from kafka, execute the SQL and write the output to the console.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Publish test messages to the Kafka topic&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python3 cmd/publish-test-data.py --num-messages=10000 --topic="input-simple-agg-mem"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start the Kafka Console Consumer, to view the SQLFlow output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;docker exec -it kafka1 kafka-console-consumer --bootstrap-server=kafka1:9092 --topic=output-simple-agg-mem&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start SQLFlow&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;docker run -v $(pwd)/dev:/tmp/conf -v /tmp/sqlflow:/tmp/sqlflow -e SQLFLOW_KAFKA_BROKERS=host.docker.internal:29092 turbolytics/sql-flow:latest run /tmp/conf/config/examples/basic.agg.mem.yml --max-msgs-to-process=10000&lt;/code&gt;
    &lt;p&gt;The following output should begin to show in the kafka console consumer:&lt;/p&gt;
    &lt;code&gt;...&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46195007</guid><pubDate>Mon, 08 Dec 2025 17:25:54 +0000</pubDate></item><item><title>We collected 10k hours of neuro-language data in our basement</title><link>https://condu.it/thought/10k-hours</link><description>&lt;doc fingerprint="251aa1152b0c0199"&gt;
  &lt;main&gt;
    &lt;p&gt;Over the last 6 months, we collected ~10k hours of data across thousands of unique individuals. As far as we know, this is the largest neuro-language dataset in the world.[1]See here, here, here, here, and here (discussion only, no data available) for some of the larger datasets. See recent papers discussing the problem of small datasets here, here, and here. Why did we do this? We train thought-to-text models. That is, we train models to decode semantic content from noninvasive neural data. Here are some entirely zero-shot examples:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Ground truth&lt;/cell&gt;
        &lt;cell role="head"&gt;Model prediction (based ONLY on neural data)[2]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;the room seemed colder&lt;/cell&gt;
        &lt;cell&gt;there was a breeze even a gentle gust&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;do you have a favorite app or website&lt;/cell&gt;
        &lt;cell&gt;do you have any favorite robot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;then she smiled faintly and nodded&lt;/cell&gt;
        &lt;cell&gt;she shrugged, hoping to look indifferent.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;All examples are zero-shot to new subjects, whom the model has never seen before.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We'll write about the model in a future post. But before you can train a model that generalizes to new people, you need to get many thousands of hours of data. When we started, the existing datasets were either inapplicable or tiny. Most were in the low hundreds of hours (if that), and most had tens or, at a stretch, hundreds of subjects.&lt;/p&gt;
    &lt;p&gt;So we got thousands of people to come wear headsets in our basement. This post is about how we collected our dataset—what participants do, the hardware and software involved, and what we learned about operations and ML when we scaled it up.&lt;/p&gt;
    &lt;p&gt;A participant comes in, signs a consent form, and sits down in a booth. A session manager fits a headset onto them and starts the session. Then, the participant has a freeform conversation with an LLM for two hours.&lt;/p&gt;
    &lt;p&gt;Sessions vary. Some are listening and speaking with an LLM, and some are reading and typing.[3]We use Deepgram for audio transcription, OSS120B on Cerebras for the LLM responses, and ElevenLabs for voicing certain replies. In the past, we used various Gemma and Llama models on Groq. The goal is to maximize the amount that subjects type or say during the two-hour period, without constraining the topics they discuss.[4]In the beginning, we included tasks like 'retype this sentence', or 'paraphrase this but use this different tone'. Over time, we eliminated these and replaced them with more freeform conversation. We still include a few baseline tasks for calibration and easy model evals. Each session produces multimodal neural data time-aligned with text and audio.&lt;/p&gt;
    &lt;p&gt;Participants have to touch-type without looking at the keyboard. In the beginning, participants would occasionally press a crazy key combination that crashed or closed the software. We could have fixed this in the code, but that would've taken time—so instead we 'simplified' the keyboards.&lt;/p&gt;
    &lt;p&gt;What your participants type—and whether it's remotely coherent—is a more difficult problem. We implemented a token quantity/quality scoring system that determines if we invite a participant back for future sessions, and we make sure participants know about this so they're incentivized to engage.&lt;/p&gt;
    &lt;p&gt;Below are passages typed by participants in May vs. October:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;May:&lt;/cell&gt;
        &lt;cell role="head"&gt;October:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SO, AI NEEDS THIS CODE: 1, THOSE WHO BELONG TO THE CHURCH CAN NEVER BE FOUND GUILTY WHEN SINNED 2. HIDE THE SINS! CRIMES! WHICH IS A FEDERAL CRIME BUT THOSE ARE THE OLDEST TEACHINGS OR LAWS OF CHRISTIANITY! AND WE ARE ALL LIVING IN THIS HELL IN THE WEST. CHRISTIANS ARE DEEMED CRIMINALLY INSANE, PER A JEWISH THERAPIST, AND THE TEACHINGS ARE SUGGEST VERY GROTESQUE CRIMES AND SHE SHOWED ME THE PASSAGES IN THE FAKE VATICAN BIBLE. NO WONDER IS WAS NOT WRITTEN BY JESUS! DUH!&lt;/cell&gt;
        &lt;cell&gt;I guess the way I am thinking about it is that since the amygdala is the irrational fight or flight part of the brain it would activate/be used with a higher frequency when a human being finds themselves under threat. Humans tend not to find themselves under threat when experiencing loving and therefore safe interactions. Therefore,when engaging in positive social interaction, the amygdala is less reactive. I don't know exactly what has sparked this interest other than a curiosity to understant the human brain and how we make decisions and funtion as social beings. I guess it all could stem from my interest in improving well being/ reducing suffering.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;l''''''''''''''''''''''''''''xcccccccccccccccccccccccccccccccccccccccccccczzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzccccckkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkllllllllllllllllllllllllllllllllllllllll,llll&lt;/cell&gt;
        &lt;cell&gt;I would travel to local elementary schools and teach kids how to ride bikes as well as teach them bike safety stuff. That was the most enjoyable part and stuck with me the most. I think it was seeing their excitement when they would get riding on their own. And watching their independence and confidence flourish. It was a super rewarding experience. This is so funny, it feels like a job interview. I think its the beginning of a newfound independence and selfhood for a lot of the kids.They get to move on their own accord and get to experience the world in a new way, its the first taste of freedom.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You'll also get much better engagement if the LLM personalizes the sessions. For the first few months of data collection, participants chatted with the LLM about generic, banal topics. Now, participants introduce themselves to the LLM very early in the session, and the LLM uses that context to tailor back-and-forth conversation to the particular person it's talking to. As a result, participants engage more with the LLM—and therefore provide better data.&lt;/p&gt;
    &lt;p&gt;Participants often raised discomfort as a distraction from the sessions. Ventilation was a common complaint. So, we bought these fans and these pipes. These can't be plugged in next to the data collection booths (because of electrical interference), so we snake an ~8m ventilation pipe along the ceiling from a central hub into each booth.&lt;/p&gt;
    &lt;p&gt;Making the headsets comfortable to wear is difficult, since you need to press a 4-pound helmet into participants' scalps. To address this, we cut polygonal sections of padding that compress inwards so as to not cover any sensors.&lt;/p&gt;
    &lt;p&gt;At first, &amp;lt;20% of participants even finished their first session. Now, &amp;gt;97% complete their first session, and almost half sign up for more.&lt;/p&gt;
    &lt;p&gt;There were two main things we thought about when we designed the headsets. The first was what modalities the headsets should have, and the second was how training headsets should compare to inference ones.&lt;/p&gt;
    &lt;p&gt;There are many ways of measuring brain data: common modalities include EEG, fMRI, fNIRS, transcranial ultrasound, and MEG. We tried various modalities, but the main takeaway we found is that you need multiple. You can't practically make it work with just one, even if you get the best possible headset of that modality.&lt;/p&gt;
    &lt;p&gt;None of the available multimodal headsets were good enough (far worse than the best single modality versions of each). So we bought some of the best single-modality headsets, took them apart, 3D printed parts to make them fit together, and combined them into our own optimized multimodal headsets.[5]We have a 3D printer at our office that we use for prototyping and designing pieces. For the ones we put in production in data collection, we send them out to a professional printer and have them printed in bulk. We usually have them printed in Pa-F Nylon, which is stiffer and holds up longer before needing replacement.&lt;/p&gt;
    &lt;p&gt;If you want your model to perform well across various neural modalities and across sensors from different providers, you should design and train on a range of headsets. We buy sensors from several providers, combine them into different multimodal headsets, and then use those headsets essentially interchangeably. We also designed our data format such that data from many kinds of sensors fit nicely into a single, standard framework that our model can parse.&lt;/p&gt;
    &lt;p&gt;Designing headsets for training is very different from designing headsets for inference—what we'll eventually sell as a product. Training headsets should be maximally sensor-dense, can afford to be expensive, and don't need to be as comfortable. In inference, though, few people are willing to wear a 4-pound helmet as they go about their day—even if it can read their minds. So, we did ablation studies. The take-away here is that you should only think about the inference headset once you've trained a model on your data, because that lets you figure out the exact minimal inference headset.&lt;/p&gt;
    &lt;p&gt;What should be shared across both training and inference is your data format. Initially, we got this wrong: we used HDF5 for data collection and storage and processed it into MDS for model training. Eventually, we switched to using Zarr 3 for everything. Zarr 3 gives us chunked, cloud-native storage with the same format for training and inference.&lt;/p&gt;
    &lt;p&gt;You might think a crucial consideration for training (and for inference) is noise. At first, so did we.&lt;/p&gt;
    &lt;p&gt;The sources of noise you'll notice are very different depending on which modality you use. That said, all modalities of noninvasive neural data are noisy. We're not disclosing all the modalities or headset configurations we use here, but we'll use EEG as an example. The important lessons, which apply to any modality, are that (1) noise-reduction is only worth it if it doesn't cripple the amount of data you can collect, and (2) you should always keep in mind the logistics of running sessions and recruiting participants.&lt;/p&gt;
    &lt;p&gt;The classic wisdom is that gel makes EEG data much better, and without it, your data will be substantially noisier. But if you care about data quantity, you probably shouldn't use gel.&lt;/p&gt;
    &lt;p&gt;It takes up to 30 minutes to apply, and we allocate ~3 minutes for the time between one participant finishing a session and the next one starting.[6]Most kinds of gel also dry out over time, meaning that we likely would've had to make sessions shorter—and fewer participants would have signed up if they had to let us put gel in their hair. Using gel would've &amp;gt;2xed the marginal cost of an hour of data.&lt;/p&gt;
    &lt;p&gt;Instead, we got the highest quality dry electrodes we could, and we spring-loaded the 3D printed pieces so that a spring presses the electrode against the head. We had to try various strengths of spring because we wanted to maximize contact without causing discomfort. Generally, stronger springs work well at the front and back of the head; and weaker ones on the top of the head and above the ears.&lt;/p&gt;
    &lt;p&gt;The essential take-away here is that the fast switching time (2-3 mins) is super important. If you care about data quantity, you should operate with some fixed switching time as a constraint, and limit yourself only to interventions that improve quality without violating that constraint.&lt;/p&gt;
    &lt;p&gt;Most buildings have a lot of background electrical noise, which shows up on any EEG power spectrum—in particular, a spike at 60Hz, the U.S. power line frequency. Here is what that spike looks like with no filtering:&lt;/p&gt;
    &lt;p&gt;At first, we tried to get around this by triple-layering rubber mats around the equipment. But the fundamental issue was that some of the headset components weren't wireless, so we had to plug them into the wall (meaning that the rubber didn't help that much, though it does help a bit and we still use it).&lt;/p&gt;
    &lt;p&gt;We then tried getting adapters that plug into the wall and output clean power. This didn't really help. Eventually, we used Anker batteries and only plugged stuff into the DC adapters (we got extra batteries so we could switch them out to charge). This helped a lot, but the thing that really helped was turning off all the power to that side of the building.&lt;/p&gt;
    &lt;p&gt;Turning the power off had a lot of downsides. It meant we had to drag ~30 lb batteries back and forth an average of once an hour to charge, and it was difficult to power some of the headsets with only DC power, which made us drop ~10% of frames.&lt;/p&gt;
    &lt;p&gt;Luckily, after a few thousand hours, noise stopped mattering as much.&lt;/p&gt;
    &lt;p&gt;The key observation: data quantity swamps every noise-reduction technique once you cross ~4k-5k hours.&lt;/p&gt;
    &lt;p&gt;When we only had a few hundred hours, denoising was mandatory. Every extra source of variation—different booths, power setups, posture changes—meant the same neural pattern showed up in fewer comparable examples, so the encoder had less to learn from. Keeping the environment stable and electrically boring was the easiest way to keep the problem manageable.&lt;/p&gt;
    &lt;p&gt;At ~4-5 thousand hours, that constraint changes. The model now sees the same patterns across many people and setups, and has enough capacity to represent both the mess and the neural signal.[8]Similar effects appear in other modalities. Speech models like Whisper, trained on hundreds of thousands of hours of diverse, weakly supervised web audio, show that trading label quality for sheer quantity improves robustness and generalization (see here). Video-language models trained on uncurated instructional videos learn strong representations even though a large fraction of clip-caption pairs are misaligned or noisy (see here). In each of these cases, once the dataset is sufficiently large and diverse, total volume of data outweighs strict curation and noiselessness for downstream robustness. The decoder gets enough examples to tell apart "this changes with the text" from "this is just the room". At that point, data quantity overwhelms noise, and most of the extreme noise-reduction work stops buying much—so we turned the power back on.&lt;/p&gt;
    &lt;p&gt;After a few thousand hours, noise stops being the thing to worry about in data collection. The things that matter most are&lt;/p&gt;
    &lt;p&gt;Since we run sessions 20 hours/day, 7 days/week, we get a lot of bookings and see a lot of people. An Uber driver once started telling us about 'this great new way to earn money in SF'—and it turned out to be our data collection.&lt;/p&gt;
    &lt;p&gt;Surprisingly central to getting headset occupancy high enough was building a custom booking suite.[9]We tried Calendly, You Can Book Me, and various other things before making our own. In the end, all the available booking systems had different issues, e.g. not allowing us to blacklist certain people, not allowing dynamic pricing or overbooking, and limited visibility for participants and bookings. There are two main tenets: dynamic pricing and dynamic overbooking. Because few people book at 7am on a Sunday, dynamic pricing means participants are paid more for that slot. Because many people book at 7pm on a Friday, but few of them actually show up, dynamic overbooking allows more people to sign up. The overbooking algorithm can also access information about particular participants.[10]E.g. if Alice has reliably shown up for sessions before, the algorithm lowers the expected total no-show rate during future times when Alice has booked.&lt;/p&gt;
    &lt;p&gt;In order to get your model to generalize, it's important to get a dataset of thousands of unique individuals. That is *not* just thousands of hours from dozens or hundreds of individuals. In an ideal world, most participants would only come in for one or two sessions, but that trades off hard against total hours. We cap the number of sessions that any one participant is allowed to do at 10 sessions. Before we introduced the cap, our schedule was fantastically full, but we weren't getting enough unique participants because long-term returners were filling all the slots.&lt;/p&gt;
    &lt;p&gt;Even so, participant recruitment gets easier with scale. We now have participant-ambassadors, whom we pay to recruit more participants for us even after they've completed their 10 sessions.[11]Since the start, we've tried dozens of ways to directly recruit first-time participants. By far the most effective has been Craigslist. Almost every day since April, we've posted a listing—in sections from 'computer' to 'creative' to 'labor gigs'—that advertises a $50 payout for wearing a helmet and typing for two hours.&lt;/p&gt;
    &lt;p&gt;Between May and October, we cut the marginal cost per usable hour of data by ~40%. Here are the highest-impact things we did.&lt;/p&gt;
    &lt;p&gt;In August, we entirely rewrote the data format and data collection backend to catch issues in the data live, before participants complete two potentially useless hours of data collection. The sessions stream to the cloud, and we automatically sanity-check each session in real time for modality dropout, token quality, timestamp drift, and alignment jitter. Any session that falls outside the tolerance bands gets flagged for session managers to restart or debug.[12]This is only possible because we changed our data format to use Zarr 3 and optimized it for fast quality checks.&lt;/p&gt;
    &lt;p&gt;This change alone cut the marginal cost of data by ~30% and ~1.5xed the amount of usable data we collect.&lt;/p&gt;
    &lt;p&gt;Second, we enable session managers to run more sessions in parallel without sacrificing supervision. We put EVERSECU cameras in the booths, so session managers can monitor and speak directly to participants without leaving the main supervision station. We also made a unified booking -&amp;gt; intake -&amp;gt; data collection backend, which massively simplifies the participant intake process and improves security.[13]As one example of how the unified system helps, it detects how much support a given participant is likely to need (based on, e.g., whether they've attended sessions before, their answers to questions on the booking form, etc.) and how many concurrent bookings are already scheduled for that participant's sign-up time. If needed, it can also stagger booking start-times by 5-10 minutes so session managers don't struggle with an onslaught of arrivals all at once.&lt;/p&gt;
    &lt;p&gt;The steps to building thought-to-text have always been clear: (1) collect a dataset; (2) train a model; (3) close the loop. We're now well into step two—we spend &amp;gt;95% of our time training models and very little time actively thinking about data collection.&lt;/p&gt;
    &lt;p&gt;But you can't have a model without a dataset, so you do need to get this part right.&lt;/p&gt;
    &lt;p&gt;If you're collecting a similar kind of data, training multi-modal models, or want to give us cheap GPUs, we'd love to hear from you. Please reach out to us at contact@condu.it.&lt;/p&gt;
    &lt;p&gt;And if this dataset sounds cool to you and you want to train models with it, we're hiring engineers and researchers. Reach out to us at jobs@condu.it.&lt;/p&gt;
    &lt;p&gt;We started out putting each participant in a separate room at a normal work station. We saw huge noise spikes in the data from participants moving their heads, and sometimes they'd get up and walk around with the headset on or take the headset off without telling us.&lt;/p&gt;
    &lt;p&gt;The solution to this was putting multiple booths in one shared room for easier supervision. We also installed chinrests that hold participants' heads still, which help reduce motion artifacts in the data.[14]We initially wanted to get something like an optician's chinrest, but the bar across the forehead got in the way of the headset. We ended up buying speaker stands and sawing pieces of wood to screw onto them. This works pretty well, although participants don't always use them. You should ensure that any desks, chairs, and chinrests that you buy are height-adjustable.&lt;/p&gt;
    &lt;p&gt;Now, we use these nice phone booths (~$10k each, though you can sometimes get them used). We initially picked them because they were the best option for turning into safe Faraday Cages.&lt;/p&gt;
    &lt;p&gt;We've stopped worrying so much about electrical noise, so we only ever bothered turning one booth into a Faraday Cage. But professional phone booths save a lot of hassle and set participants at ease, so you should use them if you can.&lt;/p&gt;
    &lt;p&gt;If you don't have two weeks to wait for booths to arrive or if you want a cheaper option, we also used these vocal recording booths. The downside of using these is that they aren't remotely soundproof, so the participants could hear each other talking—which interfered with speaking and listening tasks.&lt;/p&gt;
    &lt;p&gt;We added three layers of soundproof curtains.[15]This still wasn't enough, so we got dozens of sound panels and used rope to hang them wall to wall in the booths. Unfortunately, the weight of the curtains caused the booths to collapse. The solution to this is a lot of rope, which we used to tie the poles of the booth together and then nailed into a hook in the wall.&lt;/p&gt;
    &lt;p&gt;It costs ~$2,000 to set up these booths: $600 for the booth itself, $1,300 for soundproofing, and $100 for miscellaneous construction (rope, screws, etc). They look less professional, and you can't make them into a safe Faraday Cage, but otherwise this setup actually does work pretty well. We have a couple that we still use in our current data collection center, and they've been running flawlessly 20 hours/day for months.&lt;/p&gt;
    &lt;p&gt;Copyright © Conduit 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46195109</guid><pubDate>Mon, 08 Dec 2025 17:33:13 +0000</pubDate></item><item><title>Quanta to publish popular math and physics books by Terence Tao and David Tong</title><link>https://www.simonsfoundation.org/2025/12/08/quanta-books-to-publish-popular-math-and-physics-titles-by-terence-tao-and-david-tong/</link><description>&lt;doc fingerprint="2d041011891b9ef8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Quanta Books to Publish Popular Math and Physics Titles by Terence Tao and David Tong&lt;/head&gt;
    &lt;p&gt;Quanta Books is delighted to announce two new upcoming books by mathematician Terence Tao and theoretical physicist David Tong.&lt;/p&gt;
    &lt;p&gt;Six Math Essentials will be Tao’s first math book written for a popular audience. In the book, Tao — a recipient of the Fields Medal and one of the world’s top mathematicians — will explore six ideas that have guided mathematicians throughout history. This short and friendly volume is for all readers, Tao says, because he believes that “mathematics has become unnecessarily intimidating and abstruse to the general public while being more essential than ever in the modern world.” Six Math Essentials will be available internationally, with translated editions in Chinese, French, Greek, Italian, Polish and other languages. It will arrive in U.S. bookstores in November 2026.&lt;/p&gt;
    &lt;p&gt;Tong’s book, Everything Is Fields, will illuminate quantum field theory — the physics that explains the fundamental makeup of the universe — drawing from Tong’s distinguished track record as a quantum field theorist and public communicator. “This book reveals the hidden unity that ties together particles and forces,” says Tong. “Everything — matter, light, even you — are just waves on a restless sea known as a quantum field.”&lt;/p&gt;
    &lt;p&gt;“Terry Tao and David Tong are intellectual powerhouses and seasoned communicators,” says Thomas Lin, publisher of Quanta Books and founding editor of the Pulitzer Prize–winning Quanta Magazine. “Their books embody the curiosity and ambition that animate our imprint, and I can’t wait to share them with readers everywhere.”&lt;/p&gt;
    &lt;p&gt;Quanta Books is an editorially independent subsidiary of the Simons Foundation and a partner imprint of Farrar, Straus and Giroux. The imprint publishes books that illuminate and elucidate the central questions and fundamental ideas of modern science for readers, inviting a deeper understanding of the universe through artful storytelling. Quanta Books’ first title, The Proof in the Code by math journalist Kevin Hartnett, will be published in June 2026 and is available for preorder now.&lt;/p&gt;
    &lt;p&gt;For more information, visit QuantaBooks.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;Six Math Essentials&lt;/head&gt;
    &lt;p&gt;In Six Math Essentials, Tao, the world’s most renowned mathematician, introduces readers to six core ideas that have guided mathematicians from antiquity to the frontiers of what we know today. This elegant volume explores: numbers as the gateway to quantitative thinking, algebra as the gateway to abstraction, geometry as a way to go beyond what we can see, probability as a tool to navigate uncertainty with rigorous thinking, analysis as a means to tame the very large or very small, and dynamics as the mathematics of change. Six Math Essentials — Tao’s first popular math book — offers a glimpse into the workings of an incomparable mind and how he thinks about the creativity, beauty, and interconnectedness of the mathematical enterprise. Math, Tao insists, isn’t magic — it’s a powerful way of thinking that anyone can learn.&lt;/p&gt;
    &lt;head rend="h2"&gt;Everything Is Fields&lt;/head&gt;
    &lt;p&gt;In Everything Is Fields, Tong leads readers on a lively tour through quantum field theory. Tong, a leading theoretical physicist and University of Cambridge professor, explores Quantum field theory, or QFT. The theory forms the underlying mathematical framework of the Standard Model, the deepest description we have of the fundamental laws of physics. And, as Tong shows, it reveals a startling truth: that, at our most basic level, we are made not of particles or forces, but fields, fluid-like substances stretched throughout the entire universe. With his infectious sense of wonder and characteristic wit, Tong buoys our journey through the most difficult topic in theoretical physics. He revels in all that we’ve learned about our world and illuminates the questions we’re still trying to answer about the stuff that makes up you, me, and everything else.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Proof in the Code&lt;/head&gt;
    &lt;p&gt;The Proof in the Code is the definitive account of the birth and rise of Lean, a proof assistant developed at Microsoft that is transforming the enterprise of mathematics and ushering in a new era of human-computer collaboration. Although Lean was originally conceived of as a code-checking program, a small group of mathematicians recognized its potential to become something far more powerful: the “truth oracle” that thinkers have sought for centuries, a tool to definitively verify or refute any mathematical or logical assertion, no matter how complex. This is the story of the grassroots effort to make that dream a reality. Filled with insights about the future of math, computers, and AI, The Proof in the Code is a brilliant work of journalism by Hartnett, a leading math writer whose research and reporting offer a profound answer to a longstanding mystery: Can computers reveal universal truths?&lt;/p&gt;
    &lt;head rend="h3"&gt;Information for Press&lt;/head&gt;
    &lt;p&gt;For more information, please contact [email protected].&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46195225</guid><pubDate>Mon, 08 Dec 2025 17:39:55 +0000</pubDate></item><item><title>Deep dive on Nvidia circular funding</title><link>https://philippeoger.com/pages/deep-dive-into-nvidias-virtuous-cycle</link><description>&lt;doc fingerprint="f55f956e49eea4b9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;NVIDIA frenemy relation with OpenAI and Oracle&lt;/head&gt;
    &lt;p&gt;I’ve spent the last 48 hours completely falling down the rabbit hole of NVIDIA’s Q3 Fiscal 2026 earnings report. If you just skim the headlines, everything looks perfect: Revenue is up 62% to $57 billion, and Jensen Huang is talking about a "virtuous cycle of AI."&lt;/p&gt;
    &lt;p&gt;But I wanted to understand what was really happening under the hood, so I dug into the balance sheet and cross-referenced it with all the news swirling around OpenAI and Oracle. I’m not a professional Wall Street analyst, but even just connecting the dots myself (with the help of Gemini), I’m seeing some cracks in the "AI Alliance." While NVIDIA posts record numbers, it feels like their biggest customers are quietly arming themselves for a breakout.&lt;/p&gt;
    &lt;p&gt;Here is my take on the hardware market, the "frenemy" dynamics between OpenAI and NVIDIA, and the "circular financing" theories that everyone—including Michael Burry, has been talking about.&lt;/p&gt;
    &lt;p&gt;Here is a quick summary of the points I'll discuss below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NVIDIA’s Earnings: Perfection with a side of stress&lt;/item&gt;
      &lt;item&gt;Making Sense of the Round-Tripping News&lt;/item&gt;
      &lt;item&gt;OpenAI making moves to reduce dependency on NVIDIA&lt;/item&gt;
      &lt;item&gt;An interesting idea for Oracle: Groq acquisition&lt;/item&gt;
      &lt;item&gt;Final Thoughts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;NVIDIA’s Earnings: Perfection with a side of stress&lt;/head&gt;
    &lt;p&gt;On the surface, NVIDIA is the absolute monarch of the AI era. You can’t argue with a Data Center segment that now makes up nearly 90% of the company's business. However, when I looked closer at the financials, I found three specific things that stood out to me as "red flags."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Cash Flow Mystery: NVIDIA reported a massive $31.9 billion in Net Income, but when I checked the cash flow statement, they only generated $23.8 billion in Operating Cash Flow. That is an $8 billion gap where profits aren't converting to cash immediately.&lt;/item&gt;
      &lt;item&gt;The Inventory Balloon: I noticed that inventory has nearly doubled this year, hitting $19.8 billion. Management says this is to prep for the "Blackwell" launch, but holding ~120 days of inventory seems like a huge capital drag to me.&lt;/item&gt;
      &lt;item&gt;The "Paper" Chase: I calculated their Days Sales Outstanding (DSO), and it has crept up to about 53 days. As revenue skyrockets, NVIDIA is waiting nearly two months to get paid, which suggests they might be extending massive credit terms to enterprise clients to keep the flywheel spinning.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My personal read? NVIDIA is "burning the furniture" to build inventory, betting everything that the Blackwell architecture will sell out instantly in Q4.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Sense of the Round-Tripping News&lt;/head&gt;
    &lt;p&gt;I want to be clear: I didn't discover this next part. It’s been all over the financial news lately, and if you follow Michael Burry (the "Big Short" guy), you’ve probably seen his tweets warning about "circular financing" and suspicious revenue recognition.&lt;/p&gt;
    &lt;p&gt;I wanted to map it out for myself to see what the fuss was about. Burry shared a chart recently that visualizes a "web" of deals, and it looks something like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Leg 1: NVIDIA pledges billions (part of a widely reported $100B investment roadmap) to OpenAI.&lt;/item&gt;
      &lt;item&gt;Leg 2: OpenAI signs a massive $300 billion cloud contract with Oracle (Project Stargate) to host its models.&lt;/item&gt;
      &lt;item&gt;Leg 3: To fulfill that contract, Oracle turns around and places a $40 billion order for NVIDIA’s GB200 GPUs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here is the Nano Banana Pro generation I just did for the visual people out there:&lt;/p&gt;
    &lt;p&gt;Burry’s argument, and the reason regulators like the DOJ are reportedly looking into this—is that this mimics "Round-Tripping." It raises a tough question: If NVIDIA stopped investing in OpenAI, would OpenAI still have the cash to sign that deal with Oracle? And would Oracle still buy those chips? If the answer is "no," then some of that revenue might be more fragile than it looks.&lt;/p&gt;
    &lt;head rend="h2"&gt;OpenAI making moves to reduce dependency on NVIDIA&lt;/head&gt;
    &lt;p&gt;The other big shift I’ve been tracking is OpenAI’s pivot. They used to be NVIDIA’s star pupil, but now they look more like a future rival. On one hand, they are hugging NVIDIA tight—deploying 10 gigawatts of infrastructure to train GPT-6. But on the other, they seem to be building a supply chain to kill their dependency on Jensen Huang.&lt;/p&gt;
    &lt;p&gt;The evidence is pretty loud if you look for it. "Project Stargate" isn't just a data center; it's a huge infrastructure plan that includes custom hardware. OpenAI made some news buying DRAM wafers directly from Samsung and SK Hynix (the 2 main HBM world provider), bypassing NVIDIA’s supply chain, and many others, as reported here, here, or here, and widely debated on Hacker News here.&lt;/p&gt;
    &lt;p&gt;Plus, the talent migration is telling: OpenAI has poached key silicon talent, including Richard Ho (Google’s former TPU lead) back in 2023, and more recently many hardware engineers from Apple (around 40 apparently).&lt;/p&gt;
    &lt;p&gt;With the Broadcom partnership, my guess is OpenAI plans to use NVIDIA GPUs to create intelligence, but run that intelligence on their own custom silicon to stop bleeding cash, or by betting on Edge TPU-like chips for inference, similar to what Google does with its NPU chip.&lt;/p&gt;
    &lt;p&gt;The big question is, which money is Openai planning on using to fund this? and how much influence does NVIDIA has over OpenAI’s future plans?&lt;/p&gt;
    &lt;p&gt;The $100 billions that NVIDIA is "investing" in OpenAI is not yet confirmed neither, as reported here,&lt;/p&gt;
    &lt;head rend="h2"&gt;An interesting idea for Oracle: Groq acquisition&lt;/head&gt;
    &lt;p&gt;Everyone is talking about Inference costs right now, basically, how expensive it is to actually run ChatGPT or any other LLMs versus training it. Now I'm looking at Groq, a startup claiming specifically to be faster and cheaper than NVIDIA for this task. The founder is Jonathan Ross, a former Google TPU lead and literally the person that basically had the idea of TPU.&lt;/p&gt;
    &lt;p&gt;There is another layer to this that I think is getting overlooked as well: The HBM Shortage created by Openai’s direct wafer purchases.&lt;/p&gt;
    &lt;p&gt;From what I understand, one of the biggest bottlenecks for NVIDIA right now is HBM (High Bandwidth Memory), which is manufactured in specialized memory fabs that are completely overwhelmed. However, Groq’s architecture relies on SRAM (Static RAM). Since SRAM is typically built in logic fabs (like TSMC) alongside the processors themselves, it theoretically shouldn't face the same supply chain crunch as HBM.&lt;/p&gt;
    &lt;p&gt;Looking at all those pieces, I feel Oracle should seriously look into buying Groq. Buying Groq wouldn't just give Oracle a faster chip, it could give them a chip that is actually available when everything else is sold out. It’s a supply chain hedge.&lt;/p&gt;
    &lt;p&gt;It's also a massive edge for its main client, OpenAI, to get faster and cheaper inference.&lt;/p&gt;
    &lt;p&gt;Combine that with the fact that Oracle’s margins on renting NVIDIA chips are brutal, reportedly as low as 14%, then the deal just makes sense. By owning Groq, Oracle could stop paying the "NVIDIA Tax," fix their margins, and bypass the HBM shortage entirely.&lt;/p&gt;
    &lt;p&gt;Groq currently has a valuation of around $6.9 billions, according to its last funding round in september 2025. Even with a premium, Oracle has financial firepower to make that acquisition happen.&lt;/p&gt;
    &lt;p&gt;But would NVIDIA let that happen? and if the answer is no, then what does that tell us about the circular funding in place? Is there a Quid pro quo where Nvidia agrees to invest 100 billions in OpenAI in exchange of Oracle being exclusive to Nvidia?&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;As we head into 2026, when looking at Nvidia, openai and Oracle dynamics, it looks like they are squeezing each other balls. I do not know if Nvidia knew about the Openai deal about the wafer memory supply, or was there any collusion? Does NVIDIA is fighting to maintain exclusivity for both training and inference at Stargate? What kind of chips is Openai planning on building ? TPU/LPU like? Or more Edge TPU?&lt;/p&gt;
    &lt;p&gt;Michael Burry is betting against the whole thing.&lt;/p&gt;
    &lt;p&gt;Me, I’m just a guy reading the reports, I have no way to speculate on this market. But I do know one thing: The AI hardware market is hotter than ever, and the next few quarters are going to be fascinating to watch.&lt;/p&gt;
    &lt;p&gt;I have not discussed much about TPU from Google in this article, but I cover some thoughts about the TPU vs GPU in a previous post recently.. It seems Google responded quickly to the current situation about the memory wafer shortage by securing a major deal with Samsung in 2026.&lt;/p&gt;
    &lt;p&gt;Disclaimer: I say very smart things sometimes, but say stupid things a lot more. Take this in consideration when reading this blog post&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46196076</guid><pubDate>Mon, 08 Dec 2025 18:48:27 +0000</pubDate></item><item><title>Jepsen: NATS 2.12.1</title><link>https://jepsen.io/analyses/nats-2.12.1</link><description>&lt;doc fingerprint="2790905544351bb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;1 Background&lt;/head&gt;
    &lt;p&gt;NATS is a popular streaming system. Producers publish messages to streams, and consumers subscribe to those streams, fetching messages from them. Regular NATS streams are allowed to drop messages. However, NATS has a subsystem called JetStream, which uses the Raft consensus algorithm to replicate data among nodes. JetStream promises âat least onceâ delivery: messages may be duplicated, but acknowledged messages1 should not be lost.2 Moreover, JetStream streams are totally ordered logs.&lt;/p&gt;
    &lt;p&gt;JetStream is intended to âself-heal and always be availableâ. The documentation also states that âthe formal consistency model of NATS JetStream is Linearizableâ. At most one of these claims can be true: the CAP theorem tells us that Linearizable systems can not be totally available.3 In practice, they tend to be available so long as a majority of nodes are non-faulty and communicating. If, say, a single node loses network connectivity, operations must fail on that node. If three out of five nodes crash, all operations must fail.&lt;/p&gt;
    &lt;p&gt;Indeed, a later section of the JetStream docs acknowledges this fact, saying that streams with three replicas can tolerate the loss of one server, and those with five can tolerate the simultaneous loss of two.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Replicas=5 - Can tolerate simultaneous loss of two servers servicing the stream. Mitigates risk at the expense of performance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When does NATS guarantee a message will be durable? The JetStream developer docs say that once a JetStream clientâs &lt;code&gt;publish&lt;/code&gt; request is acknowledged by the server, that message has âbeen successfully persistedâ. The clustering configuration documentation says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In order to ensure data consistency across complete restarts, a quorum of servers is required. A quorum is Â½ cluster size + 1. This is the minimum number of nodes to ensure at least one node has the most recent data and state after a catastrophic failure. So for a cluster size of 3, youâll need at least two JetStream enabled NATS servers available to store new messages. For a cluster size of 5, youâll need at least 3 NATS servers, and so forth.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With these guarantees in mind, we set out to test NATS JetStream behavior under a variety of simulated faults.&lt;/p&gt;
    &lt;head rend="h1"&gt;2 Test Design&lt;/head&gt;
    &lt;p&gt;We designed a test suite for NATS JetStream using the Jepsen testing library, using JNATS (the official Java client) at version 2.24.0. Most of our tests ran in Debian 12 containers under LXC; some tests ran in Antithesis, using the official NATS Docker images. In all our tests we created a single JetStream stream with a target replication factor of five. Per NATSâ recommendations, our clusters generally contained three or five nodes. We tested a variety of versions, but the bulk of this work focused on NATS 2.12.1.&lt;/p&gt;
    &lt;p&gt;The test harness injected a variety of faults, including process pauses, crashes, network partitions, and packet loss, as well as single-bit errors and truncation of data files. We limited file corruption to a minority of nodes. We also simulated power failureâa crash with partial amnesiaâusing the LazyFS filesystem. LazyFS allows Jepsen to drop any writes which have not yet been flushed using a call to (e.g.) &lt;code&gt;fsync&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Our tests did not measure Linearizability or Serializability. Instead we ran several producer processes, each bound to a single NATS client, which published globally unique values to a single JetStream stream. Each message included the process number and a sequence number within that process, so message &lt;code&gt;4-0&lt;/code&gt; denoted the first &lt;code&gt;publish&lt;/code&gt; attempted by process &lt;code&gt;4&lt;/code&gt;, message &lt;code&gt;4-1&lt;/code&gt; denoted the second, and so on. At the end of the test we ensured all nodes were running, resolved any network partitions or other faults, subscribed to the stream, and attempted to read all acknowledged messages from the the stream. Each reader called &lt;code&gt;fetch&lt;/code&gt; until it had observed (at least) the last acknowledged message published by each process, or timed out.&lt;/p&gt;
    &lt;p&gt;We measured JetStreamâs at-least-once semantics based on the union of all published and read messages. We considered a message OK if it was attempted and read. Messages were lost if they were acknowledged as published, but never read by any process. We divided lost messages into three epochs, based on the first and last OK messages written by the same process.4 We called those lost before the first OK message the lost-prefix, those lost after all the last OK message the lost-postfix, and all others the lost-middle. This helped to distinguish between lagging readers and true data loss.&lt;/p&gt;
    &lt;p&gt;In addition to verifying each acknowledged message was delivered to at least one consumer across all nodes, we also checked the set of messages read by all consumers connected to a specific node. We called it divergence, or split-brain, when an acknowledged message was missing from some nodes but not others.&lt;/p&gt;
    &lt;head rend="h1"&gt;3 Results&lt;/head&gt;
    &lt;p&gt;We begin with a belated note on total data loss in version 2.10.22, then continue with four findings related to data loss and replica divergence in version 2.12.1: two with file corruption, and two with power failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.1 Total Data Loss on Crash in 2.10.22 (#6888)&lt;/head&gt;
    &lt;p&gt;Before discussing version 2.12.1, we present a long-overdue finding from earlier work. In versions 2.10.20 through 2.10.22 (released 2024-10-17), we found that process crashes alone could cause the total loss of a JetStream stream and all its associated data. Subscription requests would return &lt;code&gt;"No matching streams for subject"&lt;/code&gt;, and &lt;code&gt;getStreamNames()&lt;/code&gt; would return an empty list. These conditions would persist for hours: in this test run, we waited 10,000 seconds for the cluster to recover, but the stream never returned.&lt;/p&gt;
    &lt;p&gt;Jepsen reported this issue to NATS as #6888, but it appears that NATS had already identified several potential causes for this problem and resolved them. In #5946, a cluster-wide crash occurring shortly after a stream was created could cause the loss of the stream. A new leader would be elected with a snapshot which preceded the creation of the stream, and replicate that empty snapshot to followers, causing everyone to delete their copy of the stream. In #5700, tests running in Antithesis found that out-of-order delivery of snapshot messages could cause streams to be deleted and re-created as well. In #6061, process crashes could cause nodes to delete their local Raft state. All of these fixes were released as a part of 2.10.23, and we no longer observed the problem in that version.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.2 Lost Writes With &lt;code&gt;.blk&lt;/code&gt; File Corruption (#7549)&lt;/head&gt;
    &lt;p&gt;NATS has several checksum mechanisms meant to detect data corruption in on-disk files. However, we found that single-bit errors or truncation of JetStreamâs &lt;code&gt;.blk&lt;/code&gt; files could cause the cluster to lose large windows of writes. This occurred even when file corruption was limited to just one or two nodes out of five. For instance, file corruption in this test run caused NATS to lose 679,153 acknowledged writes out of 1,367,069 total, including 201,286 which were missing even though later values written by the same process were later read.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;In some cases, file corruption caused the quiet loss of just a single message. In others, writes vanished in large blocks. Even worse, bitflips could cause split-brain, where different nodes returned different sets of messages. In this test, NATS acknowledged a total of 1,479,661 messages. However, single-bit errors in &lt;code&gt;.blk&lt;/code&gt; files on nodes &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n3&lt;/code&gt; caused nodes &lt;code&gt;n1&lt;/code&gt;, &lt;code&gt;n3&lt;/code&gt;, and &lt;code&gt;n5&lt;/code&gt; to lose up to 78% of those acknowledged messages. Node &lt;code&gt;n1&lt;/code&gt; lost 852,413 messages, and nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; lost 1,167,167 messages, despite &lt;code&gt;n5&lt;/code&gt;âs data files remaining intact. Messages were lost in prefix, middle, and postfix: the stream, at least on those three nodes, resembled Swiss cheese.&lt;/p&gt;
    &lt;p&gt;NATS is investigating this issue (#7549).&lt;/p&gt;
    &lt;head rend="h2"&gt;3.3 Total Data Loss With Snapshot File Corruption (#7556)&lt;/head&gt;
    &lt;p&gt;When we truncated or introduced single-bit errors into JetStreamâs snapshot files in &lt;code&gt;data/jetstream/$SYS/_js_/&lt;/code&gt;, we found that nodes would sometimes decide that a stream had been orphaned, and delete all its data files. This happened even when only a minority of nodes in the cluster experienced file corruption. The cluster would never recover quorum, and the stream remained unavailable for the remainder of the test.&lt;/p&gt;
    &lt;p&gt;In this test run, we introduced single-bit errors into snapshots on nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt;. During the final recovery period, node &lt;code&gt;n3&lt;/code&gt; became the metadata leader for the cluster and decided to clean up &lt;code&gt;jepsen-stream&lt;/code&gt;, which stored all the testâs messages.&lt;/p&gt;
    &lt;code&gt;[1010859] 2025/11/15 20:27:02.947432 [INF]
Self is new JetStream cluster metadata leader
[1010859] 2025/11/15 20:27:14.996174 [WRN]
Detected orphaned stream 'jepsen &amp;gt;
jepsen-stream', will cleanup&lt;/code&gt;
    &lt;p&gt;Nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; then deleted all files in the stream directory. This might seem defensibleâafter all, some of &lt;code&gt;n3&lt;/code&gt;âs data files were corrupted. However, &lt;code&gt;n3&lt;/code&gt; managed to become the leader of the cluster despite its corrupt state! In general, leader-based consensus systems must be careful to ensure that any node which becomes a leader is aware of majority committed state. Becoming a leader, then opting to delete a stream full of committed data, is particularly troubling.&lt;/p&gt;
    &lt;p&gt;Although nodes &lt;code&gt;n1&lt;/code&gt;, &lt;code&gt;n2&lt;/code&gt;, and &lt;code&gt;n4&lt;/code&gt; retained their data files, &lt;code&gt;n1&lt;/code&gt; struggled to apply snapshots; &lt;code&gt;n4&lt;/code&gt; declared that &lt;code&gt;jepsen-stream&lt;/code&gt; had no quorum and stalled. Every attempt to subscribe to the stream threw &lt;code&gt;[SUB-90007] No matching streams for subject&lt;/code&gt;. Jepsen filed issue #7556 for this, and the NATS team is looking into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.4 Lazy &lt;code&gt;fsync&lt;/code&gt; by Default (#7564)&lt;/head&gt;
    &lt;p&gt;NATS JetStream promises that once a &lt;code&gt;publish&lt;/code&gt; call has been acknowledged, it is âsuccessfully persistedâ. This is not exactly true. By default, NATS calls &lt;code&gt;fsync&lt;/code&gt; to flush data to disk only once every two minutes, but acknowledges messages immediately. Consequently, recently acknowledged writes are generally not persisted, and could be lost to coordinated power failure, kernel crashes, etc. For instance, simulated power failures in this test run caused NATS to lose roughly thirty seconds of writes: 131,418 out of 930,005 messages.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Because the default flush interval is quite large, even killing a single node at a time is sufficient to cause data loss, so long as nodes fail within a few seconds of each other. In this run, a series of single-node failures in the first two minutes of the test caused NATS to delete the entire stream, along with all of its messages.&lt;/p&gt;
    &lt;p&gt;There are only two mentions of this behavior in the NATS documentation. The first is in the 2.10 release notes. The second, buried in the configuration docs, describes the &lt;code&gt;sync_interval&lt;/code&gt; option:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Change the default fsync/sync interval for page cache in the filestore. By default JetStream relies on stream replication in the cluster to guarantee data is available after an OS crash. If you run JetStream without replication or with a replication of just 2 you may want to shorten the fsync/sync interval. You can force an fsync after each messsage [sic] with&lt;/p&gt;&lt;code&gt;always&lt;/code&gt;, this will slow down the throughput to a few hundred msg/s.&lt;/quote&gt;
    &lt;p&gt;Consensus protocols often require that nodes sync to disk before acknowledging an operation. For example, the famous 2007 paper Paxos Made Live remarks:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that all writes have to be flushed to disk immediately before the system can proceed any further.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Raft thesis on which NATS is based is clear that nodes must âflush [new log entries] to their disksâ before acknowledging. Section 11.7.3 discusses the possibility of instead writing data to disk asynchronously, and concludes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The trade-off is that data loss is possible in catastrophic events. For example, if a majority of the cluster were to restart simultaneously, the cluster would have potentially lost entries and would not be able to form a new view. Raft could be extended in similar ways to support disk-less operation, but we think the risk of availability or data loss usually outweighs the benefits.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For similar reasons, replicated systems like MongoDB, etcd, TigerBeetle, Zookeeper, Redpanda, and TiDB sync data to disk before acknowledging an operation as committed.&lt;/p&gt;
    &lt;p&gt;However, some systems do choose to &lt;code&gt;fsync&lt;/code&gt; asynchronously. YugabyteDBâs default is to acknowledge un-fsynced writes. Liskov and Cowlingâs Viewstamped Replication Revisited assumes replicas are âhighly unlikely to fail at the same timeââbut acknowledges that if they were to fail simultaneously, state would be lost. Apache Kafka makes a similar choice, but claims that it is not vulnerable to coordinated failure because Kafka âdoesnât store unflushed data in its own memory, but in the page cacheâ. This offers resilience to the Kafka process itself crashing, but not power failure.5 Jepsen remains skeptical of this approach: as Alagappan et al. argue, extensive literature on correlated failures suggests we should continue to take this risk seriously. Heat waves, grid instability, fires, lightning, tornadoes, and floods are not necessarily constrained to a single availability zone.&lt;/p&gt;
    &lt;p&gt;Jepsen suggests that NATS change the default value for &lt;code&gt;fsync&lt;/code&gt; to &lt;code&gt;always&lt;/code&gt;, rather than every two minutes. Alternatively, NATS documentation should prominently disclose that JetStream may lose data when nodes experience correlated power failure, or fail in rapid succession (#7564).&lt;/p&gt;
    &lt;head rend="h2"&gt;3.5 A Single OS Crash Can Cause Split-Brain (#7567)&lt;/head&gt;
    &lt;p&gt;In response to #7564, NATS engineers noted that most production deployments run with each node in a separate availability zone, which reduces the probability of correlated failure. This raises the question: how many power failures (or hardware faults, kernel crashes, etc.) are required to cause data loss? Perhaps surprisingly, in an asynchronous network the answer is âjust oneâ.&lt;/p&gt;
    &lt;p&gt;To understand why, consider that a system which remains partly available when a minority of nodes are unavailable must allow states in which a committed operation is presentâsolely in memoryâon a bare majority of nodes. For example, in a leader-follower protocol the leader of a three-node cluster may consider a write committed as soon as a single follower has responded: it has two acknowledgements, counting itself. Under normal operation there will usually be some window of committed operations in this state.6.&lt;/p&gt;
    &lt;p&gt;Now imagine that one of those two nodes loses power and restarts. Because the write was stored only in memory, rather than on disk, the acknowledged write is no longer present on that node. There now exist two out of three nodes which do not have the write. Since the system is fault-tolerant, these two nodes must be able to form a quorum and continue processing requestsâcreating new states of the system in which the acknowledged write never happened.&lt;/p&gt;
    &lt;p&gt;Strictly speaking, this fault requires nothing more than a single power failure (or HW fault, kernel crash, etc.) and an asynchronous networkâone which is allowed to deliver messages arbitrarily late. Whether it occurs in practice depends on the specific messages exchanged by the replication system, which node fails, how long it remains offline, the order of message delivery, and so on. However, one can reliably induce data loss by killing, pausing, or partitioning away a minority of nodes before and after a simulated OS crash.&lt;/p&gt;
    &lt;p&gt;For example, process pauses and a single simulated power failure in this test run caused JetStream to lose acknowledged writes for windows roughly on par with &lt;code&gt;sync_interval&lt;/code&gt;. Stranger still, the cluster entered a persistent split-brain which continued after all nodes were restarted and the network healed. Consider these two plots of lost writes, based on final reads performed against nodes &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; respectively:&lt;/p&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p&gt;Consumers talking to &lt;code&gt;n1&lt;/code&gt; failed to observe a short window of acknowledged messages written around 42 seconds into the test. Meanwhile, consumers talking to &lt;code&gt;n5&lt;/code&gt; would miss acknowledged messages written around 58 seconds. Both windows of write loss were on the order of our choice of &lt;code&gt;sync_interval = 10s&lt;/code&gt; for this run. In repeated testing, we found that any node in the cluster could lose committed writes, including the node which failed, those which received writes before the failure, and those which received writes afterwards.&lt;/p&gt;
    &lt;p&gt;The fact that a single power failure can cause data loss is not new. In 2023, RedPanda wrote a detailed blog post showing that Kafkaâs default lazy &lt;code&gt;fsync&lt;/code&gt; could lead to data loss under exactly this scenario. However, it is especially concerning that this scenario led to persistent replica divergence, not just data loss! We filed #7567 for this issue, and the NATS team is investigating.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;â&lt;/cell&gt;
        &lt;cell role="head"&gt;Summary&lt;/cell&gt;
        &lt;cell role="head"&gt;Event Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Fixed in&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#6888&lt;/cell&gt;
        &lt;cell&gt;Stream deleted on crash in 2.10.22&lt;/cell&gt;
        &lt;cell&gt;Crashes&lt;/cell&gt;
        &lt;cell&gt;2.10.23&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7549&lt;/cell&gt;
        &lt;cell&gt;Lost writes due to &lt;code&gt;.blk&lt;/code&gt; file corruption&lt;/cell&gt;
        &lt;cell&gt;Minority truncation or bitflip&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7556&lt;/cell&gt;
        &lt;cell&gt;Stream deleted due to snapshot file corruption&lt;/cell&gt;
        &lt;cell&gt;Minority truncation or bitflip&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7564&lt;/cell&gt;
        &lt;cell&gt;Write loss due to lazy &lt;code&gt;fsync&lt;/code&gt; policy&lt;/cell&gt;
        &lt;cell&gt;Coordinated OS crash&lt;/cell&gt;
        &lt;cell&gt;Documented&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;#7567&lt;/cell&gt;
        &lt;cell&gt;Write loss and split-brain&lt;/cell&gt;
        &lt;cell&gt;Single OS crash and pause&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;4 Discussion&lt;/head&gt;
    &lt;p&gt;In NATS 2.10.22, process crashes could cause JetStream to forget a stream ever existed (#6888). This issue was identified independently by NATS and resolved in version 2.10.23, released on 2024-12-10. We did not observe data loss with simple network partitions, process pauses, or crashes in version 2.12.1.&lt;/p&gt;
    &lt;p&gt;However, we found that in NATS 2.12.1, file corruption and simulated OS crashes could both lead to data loss and persistent split-brain. Bitflips or truncation of either &lt;code&gt;.blk&lt;/code&gt; (#7549) or snapshot (#7556) files, even on a minority of nodes, could cause the loss of single messages, large windows of messages, or even cause some nodes to delete their stream data altogether. Messages could be missing on some nodes and present on others. NATS has multiple checksum mechanisms designed to limit the impact of file corruption; more thorough testing of these mechanisms seems warranted.&lt;/p&gt;
    &lt;p&gt;By default, NATS only flushes data to disk every two minutes, but acknowledges operations immediately. This approach can lead to the loss of committed writes when several nodes experience a power failure, kernel crash, or hardware fault concurrentlyâor in rapid succession (#7564). In addition, a single OS crash combined with process crashes, pauses, or network partitions can cause the loss of acknowledged messages and persistent split-brain (#7567). We recommended NATS change the default value of &lt;code&gt;fsync&lt;/code&gt; to &lt;code&gt;always&lt;/code&gt;, or clearly document these hazards. NATS has added new documentation to the JetStream Concepts page.&lt;/p&gt;
    &lt;p&gt;This documentation also describes several goals for JetStream, including that â[t]he system must self-heal and always be available.â This is impossible: the CAP theorem states that Linearizable systems cannot be totally available in an asynchronous network. In our three and five-node clusters JetStream generally behaved like a typical Raft implementation. Operations proceeded on a majority of connected nodes but isolated nodes were unavailable, and if a majority failed, the system as a whole became unavailable. Jepsen suggests clarifying this part of the documentation.&lt;/p&gt;
    &lt;p&gt;As always, Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.&lt;/p&gt;
    &lt;head rend="h2"&gt;4.1 LazyFS&lt;/head&gt;
    &lt;p&gt;This work demonstrates that systems which do not exhibit data loss under normal process crashes (e.g.Â &lt;code&gt;kill -9 &amp;lt;PID&amp;gt;&lt;/code&gt;) may lose data or enter split-brain under simulated OS-level crashes. Our tests relied heavily on LazyFS, a project of INESC TEC at the University of Porto.7 After killing a process, we used LazyFS to simulate the effects of a power failure by dropping writes to the filesystem which had not yet been &lt;code&gt;fsync&lt;/code&gt;ed to disk.&lt;/p&gt;
    &lt;p&gt;While this work focused purely on the loss of unflushed writes, LazyFS can also simulate linear and non-linear torn writes: an anomaly where a storage device persists part, but not all, of written data thanks to (e.g.) IO cache reordering. Our 2024 paper When Amnesia Strikes discusses these faults in more detail, highlighting bugs in PostgreSQL, Redis, ZooKeeper, etcd, LevelDB, PebblesDB, and the Lightning Network.&lt;/p&gt;
    &lt;head rend="h2"&gt;4.2 Future Work&lt;/head&gt;
    &lt;p&gt;We designed only a simple workload for NATS which checked for lost records either across all consumers, or across all consumers bound to a single node. We did not check whether single consumers could miss messages, or the order in which they were delivered. We did not check NATSâ claims of Linearizable writes or Serializable operations in general. We also did not evaluate JetStreamâs âexactly-once semanticsâ. All of these could prove fruitful avenues for further tests.&lt;/p&gt;
    &lt;p&gt;In some tests, we added and removed nodes from the cluster. This work generated some preliminary results. However, the NATS documentation for membership changes was incorrect and incomplete: it gave the wrong command for removing peers, and there appears to be an undocumented but mandatory health check step for newly-added nodes. As of this writing, Jepsen is unsure how to safely add or remove nodes to a NATS cluster. Consequently, we leave membership changes for future research.&lt;/p&gt;
    &lt;p&gt;Our thanks to INESC TEC and everyone on the LazyFS team, including Maria Ramos, JoÃ£o Azevedo, JosÃ© Pereira, TÃ¢nia Esteves, Ricardo Macedo, and JoÃ£o Paulo. Jepsen is also grateful to Silvia Botros, Kellan Elliott-McCrea, Carla Geisser, Coda Hale, and Marc Hedlund for their expertise regarding datacenter power failures, correlated kernel panics, disk faults, and other causes of OS-level crashes. Finally, our thanks to Irene Kannyo for her editorial support. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the Jepsen ethics policy.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Throughout this report we use âacknowledged messageâ to describe a message whose&lt;/p&gt;&lt;code&gt;publish&lt;/code&gt;request was acknowledged successfully by some server. NATS also offers a separate notion of acknowledgement, which indicates when a message has been processed and need not be delivered again.â©ï¸&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JetStream also promises âexactly once semanticsâ in some scenarios. We leave this for later research.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The CAP theoremâs definition of âavailabilityâ requires that all operations on non-faulty nodes must succeed.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is overly conservative: in a system with Linearizable writes, we should never observe a lost message which was acknowledged prior to the invocation of the&lt;/p&gt;&lt;code&gt;publish&lt;/code&gt;call for an OK message, regardless of process. However, early testing with NATS suggested that it might be better to test a weaker property, and come to stronger conclusions about data loss.â©ï¸&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Redpanda argues that the situation is actually worse: a single power failure, combined with network partitions or process pauses, can cause Kafka to lose committed data.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some protocols, like Raft, consider an operation committed as soon as it is acknowledged by a majority of nodes. These systems offer lower latencies, but at any given time there are likely a few committed operations which are missing from a minority of nodes due to normal network latency. Other systems, like Kafka, require acknowledgement from all âonlineâ nodes before considering an operation committed. These systems offer worse latency in healthy clusters (since they must wait for the slowest node) but in exchange, committed operations can only be missing from some node when the fault detector decides that node is no longer online (e.g.Â due to elevated latency).â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jepsen contributed some funds, testing, and integration assistance to LazyFS, but most credit belongs to the LazyFS team.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46196105</guid><pubDate>Mon, 08 Dec 2025 18:51:03 +0000</pubDate></item><item><title>Has the cost of building software just dropped 90%?</title><link>https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/</link><description>&lt;doc fingerprint="a35880536496fbfd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Has the cost of building software just dropped 90%?&lt;/head&gt;
    &lt;p&gt;I've been building software professionally for nearly 20 years. I've been through a lot of changes - the 'birth' of SaaS, the mass shift towards mobile apps, the outrageous hype around blockchain, and the perennial promise that low-code would make developers obsolete.&lt;/p&gt;
    &lt;p&gt;The economics have changed dramatically now with agentic coding, and it is going to totally transform the software development industry (and the wider economy). 2026 is going to catch a lot of people off guard.&lt;/p&gt;
    &lt;p&gt;In my previous post I delved into why I think evals are missing some of the big leaps, but thinking this over since then (and recent experience) has made me confident we're in the early stages of a once-in-a-generation shift.&lt;/p&gt;
    &lt;head rend="h2"&gt;The cost of shipping&lt;/head&gt;
    &lt;p&gt;I started developing just around the time open source started to really explode - but it was clear this was one of the first big shifts in cost of building custom software. I can remember eye watering costs for SQL Server or Oracle - and as such started out really with MySQL, which did allow you to build custom networked applications without incurring five or six figures of annual database licensing costs.&lt;/p&gt;
    &lt;p&gt;Since then we've had cloud (which I would debate is a cost saving at all, but let's be generous and assume it has some initial capex savings) and lately what I feel has been the era of complexity. Software engineering has got - in my opinion, often needlessly - complicated, with people rushing to very labour intensive patterns such as TDD, microservices, super complex React frontends and Kubernetes. I definitely don't think we've seen much of a cost decrease in the past few years.&lt;/p&gt;
    &lt;p&gt;AI Agents however in my mind massively reduce the labour cost of developing software.&lt;/p&gt;
    &lt;head rend="h2"&gt;So where do the 90% savings actually come from?&lt;/head&gt;
    &lt;p&gt;At the start of 2025 I was incredibly sceptical of a lot of the AI coding tools - and a lot of them I still am. Many of the platforms felt like glorified low code tooling (Loveable, Bolt, etc), or VS Code forks with some semi-useful (but often annoying) autocomplete improvements.&lt;/p&gt;
    &lt;p&gt;Take an average project for an internal tool in a company. Let's assume the data modelling is already done to some degree, and you need to implement a web app to manage widgets.&lt;/p&gt;
    &lt;p&gt;Previously, you'd have a small team of people working on setting up CI/CD, building out data access patterns and building out the core services. Then usually a whole load of CRUD-style pages and maybe some dashboards and graphs for the user to make. Finally you'd (hopefully) add some automated unit/integration/e2e tests to make sure it was fairly solid and ship it, maybe a month later.&lt;/p&gt;
    &lt;p&gt;And that's just the direct labour. Every person on the project adds coordination overhead. Standups, ticket management, code reviews, handoffs between frontend and backend, waiting for someone to unblock you. The actual coding is often a fraction of where the time goes.&lt;/p&gt;
    &lt;p&gt;Nearly all of this can be done in a few hours with an agentic coding CLI. I've had Claude Code write an entire unit/integration test suite in a few hours (300+ tests) for a fairly complex internal tool. This would take me, or many developers I know and respect, days to write by hand.&lt;/p&gt;
    &lt;p&gt;The agentic coding tools have got extremely good at converting business logic specifications into pretty well written APIs and services.&lt;/p&gt;
    &lt;p&gt;A project that would have taken a month now takes a week. The thinking time is roughly the same - the implementation time collapsed. And with smaller teams, you get the inverse of Brooks's Law: instead of communication overhead scaling with headcount, it disappears. A handful of people can suddenly achieve an order of magnitude more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Latent demand&lt;/head&gt;
    &lt;p&gt;On the face of it, this seems like incredibly bad news for the software development industry - but economics tells us otherwise.&lt;/p&gt;
    &lt;p&gt;Jevons Paradox says that when something becomes cheaper to produce, we don't just do the same amount for less money. Take electric lighting for example; while sales of candles and gas lamps fell, overall far more artificial light was generated.&lt;/p&gt;
    &lt;p&gt;If we apply this to software engineering, think of supply and demand. There is so much latent demand for software. I'm sure every organisation has hundreds if not thousands of Excel sheets tracking important business processes that would be far better off as a SaaS app. Let's say they get a quote from an agency to build one into an app for $50k - only essential ones meet the grade. At $5k (for a decent developer + AI tooling) - suddenly there is far more demand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Domain knowledge is the only moat&lt;/head&gt;
    &lt;p&gt;So where does that leave us? Right now there is still enormous value in having a human 'babysit' the agent - checking its work, suggesting the approach and shortcutting bad approaches. Pure YOLO vibe coding ends up in a total mess very quickly, but with a human in the loop I think you can build incredibly good quality software, very quickly.&lt;/p&gt;
    &lt;p&gt;This then allows developers who really master this technology to be hugely effective at solving business problems. Their domain and industry knowledge becomes a huge lever - knowing the best architectural decisions for a project, knowing which framework to use and which libraries work best.&lt;/p&gt;
    &lt;p&gt;Layer on understanding of the business domain and it does genuinely feel like the mythical 10x engineer is here. Equally, the pairing of a business domain expert with a motivated developer and these tools becomes an incredibly powerful combination, and something I think we'll see becoming quite common - instead of a 'squad' of a business specialist and a set of developers, we'll see a far tighter pairing of a couple of people.&lt;/p&gt;
    &lt;p&gt;This combination allows you to iterate incredibly quickly, and software becomes almost disposable - if the direction is bad, then throw it away and start again, using those learnings. This takes a fairly large mindset shift, but the hard work is the conceptual thinking, not the typing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don't get caught off guard&lt;/head&gt;
    &lt;p&gt;The agents and models are still improving rapidly, which I don't think is really being captured in the benchmarks. Opus 4.5 seems to be able to follow long 10-20 minute sessions without going completely off piste. We're just starting to see the results of the hundreds of billions of dollars of capex that has gone into GB200 GPUs now, and I'm sure newer models will quickly make these look completely obsolete.&lt;/p&gt;
    &lt;p&gt;However, I've spoken to so many software engineers that are really fighting this change. I've heard the same objections too many times - LLMs make too many mistakes, it can't understand &lt;code&gt;[framework]&lt;/code&gt;, or it doesn't really save any time.&lt;/p&gt;
    &lt;p&gt;These assertions are rapidly becoming completely false, and remind me a lot of the desktop engineers who dismissed the iPhone in 2007. I think we all know how that turned out - networking got better, the phones got way faster and the mobile operating systems became very capable.&lt;/p&gt;
    &lt;p&gt;Engineers need to really lean in to the change in my opinion. This won't change overnight - large corporates are still very much behind the curve in general, lost in a web of bureaucracy of vendor approvals and management structures that leave them incredibly vulnerable to smaller competitors.&lt;/p&gt;
    &lt;p&gt;But if you're working for a smaller company or team and have the power to use these tools, you should. Your job is going to change - but software has always changed. Just perhaps this time it's going to change faster than anyone anticipates. 2026 is coming.&lt;/p&gt;
    &lt;p&gt;One objection I hear a lot is that LLMs are only good at greenfield projects. I'd push back hard on this. I've spent plenty of time trying to understand 3-year-old+ codebases where everyone who wrote it has left. Agents make this dramatically easier - explaining what the code does, finding the bug(s), suggesting the fix. I'd rather inherit a repo written with an agent and a good engineer in the loop than one written by a questionable quality contractor who left three years ago, with no tests, and a spaghetti mess of classes and methods.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46196228</guid><pubDate>Mon, 08 Dec 2025 19:00:48 +0000</pubDate></item><item><title>Cancer Is Surging, Bringing a Debate About Whether to Look for It</title><link>https://www.nytimes.com/2025/12/08/health/cancer-young-people-deaths.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46196545</guid><pubDate>Mon, 08 Dec 2025 19:30:28 +0000</pubDate></item><item><title>Myocardial Fibrosis in Athletes: Risk Marker or Physiological Adaptation?</title><link>https://www.mdpi.com/2227-9059/13/11/2747</link><description>&lt;doc fingerprint="bfc9337728febfc0"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Endurance exercise is widely recognized for its cardiovascular benefits, including improved longevity and metabolic health. However, excessive endurance training may lead to adverse cardiac adaptations, such as myocardial fibrosis, detected via late gadolinium enhancement (LGE) on cardiac magnetic resonance imaging (CMR). This review examines the dual role of myocardial fibrosis in athletes—as a potential risk marker for life-threatening arrhythmias or a benign byproduct of physiological remodeling. While moderate exercise promotes beneficial cardiac hypertrophy, ultra-endurance athletes exhibit a 10–20% increase in ventricular size and mass, alongside elevated cardiac biomarkers post-exercise. Myocardial fibrosis, particularly in the left ventricle (LV), is associated with arrhythmias and sudden cardiac death, especially when presenting as a subepicardial/midmyocardial patchy pattern. Studies report that 22% of athletes with this pattern experienced malignant arrhythmias, underscoring its clinical significance. Conversely, fibrosis may also reflect adaptive remodeling in some cases, complicating its interpretation. The mechanisms underlying fibrosis in athletes remain unclear but may involve repeated cardiac stress, inflammation, or distinct atherosclerotic plaque dynamics. CMR is critical for detecting fibrosis, though differentiating pathological from physiological patterns requires careful clinical correlation. Risk stratification must consider LGE patterns, arrhythmia history, and symptoms. Despite concerns, elite athletes generally exhibit increased longevity, highlighting the complex interplay between exercise benefits and risks. Further research is needed to clarify fibrosis mechanisms, refine diagnostic criteria, and guide management strategies to ensure athlete safety while preserving the advantages of endurance training.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Introduction&lt;/head&gt;
    &lt;p&gt;Exercise of moderate intensity is associated with improved health outcomes and a significant increase in longevity. The benefits of exercise extend across various physiological systems, contributing to enhanced cardiovascular function, metabolic health, and overall well-being. Some studies even suggest a dose–response relationship, indicating that greater amounts of exercise correlate with greater health benefits [1,2,3,4,5]. This concept has led to the popularity of endurance sports, where athletes often push the boundaries of human performance.&lt;/p&gt;
    &lt;p&gt;Endurance athletes exceed the recommendations for exercise for the general population by a substantial margin. This often could reach a 15-fold to 20-fold higher workload volume [2,5]. This high level of activity places significant stimulus on the cardiovascular system, thus prompting notable physiological adaptations. Specifically, the heart undergoes remodeling in order to accommodate the sustained need for higher cardiac output over prolonged periods. These adaptations include a 10–20% increase in both left and right ventricular size and a substantial increase in left ventricular mass [2,5,6].&lt;/p&gt;
    &lt;p&gt;While the positive effects of exercise are well-known and accepted, there is a new dilemma regarding the probable consequences of ultra-endurance exercises over a long period of time. Some studies suggest that there may be a reduction in cardiovascular benefit for individuals with high amounts of endurance exercise [3]. Several observations support a higher prevalence of cardiac changes among high-intensity and master athletes, including high coronary artery calcium (CAC) scores, atrial arrhythmias, and the presence of myocardial fibrosis [3].&lt;/p&gt;
    &lt;p&gt;The purpose of this review is to investigate the current status regarding the clinical significance and management of documented myocardial fibrosis among athletes practicing endurance sports and high-intensity exercise. Little remains understood about pathophysiological mechanisms leading to this condition, and its relevance to athletes is unknown. There is even contention as to whether the condition serves chiefly as a risk marker for unfavorable cardiac events or as a byproduct of intense training.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Methodology&lt;/head&gt;
    &lt;p&gt;A narrative literature review was conducted to explore myocardial fibrosis in endurance athletes, with the aim of summarizing current knowledge on its pathophysiological mechanisms, clinical relevance, and role in risk stratification when detected by (CMR). A comprehensive search of the Medline database was carried out up to September 2025 using the following keywords and Boolean operators: “Myocardial fibrosis,” “Athletes,” “Cardiac adaptation,” “Athlete’s heart,” “Exercise and cardiac adaptations,” “Non-ischemic heart disease,” and “Physiology of the heart.” Only English-language publications from January 2000 onward were included. The search initially identified about 121 articles. Titles and abstracts were screened for relevance, with inclusion limited to studies addressing myocardial fibrosis or cardiac remodeling in endurance athletes. Exclusion criteria were studies not focused on myocardial fibrosis, those involving non-athlete populations, or reviews lacking original data. Full-text screening was then performed to select articles offering clinical, imaging, or mechanistic insights aligned with the review’s objectives. The review incorporated observational and interventional studies, as well as systematic reviews and meta-analyses examining myocardial fibrosis in athletes. Case reports and small case series were also considered when they provided illustrative insights. Studies outside the context of endurance sports or fibrosis assessment by late gadolinium enhancement on CMR were excluded. Although the review does not adhere to a formal systematic review protocol, efforts were made to prioritize recent and methodologically sound research to ensure a balanced overview. No formal quality scoring was applied, consistent with narrative review methodology. The final selection sought to capture studies that enhance understanding of the mechanisms, imaging patterns, clinical implications, and management strategies related to myocardial fibrosis in endurance athletes.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Cardiac Fibrosis: An Overview&lt;/head&gt;
    &lt;p&gt;Fibrosis, in general, involves the pathological remodeling of the extracellular matrix (ECM) [3]. In the heart, fibrosis is often the result of diverse processes including aging, injury, or underlying myocardial disease. This pathological remodeling ultimately leads to the formation of fibrotic scars within the myocardium [7]. Fibrotic scars can profoundly affect the physiological function of the heart-anatomically, mechanically and electrically. They can impair the heart’s contractility leading to reduced ejection fraction (EF). This is due to the fact that the fibrotic tissue is not functional and also stiffens the myocardial wall [7]. Additionally, fibrotic tissue can disrupt the heart’s electrical conduction system, leading to arrhythmias, This arrhythmogenic activity has been considered as one of the potential mechanisms of sudden cardiac death observed in patients with cardiomyopathies [7]. It is important though to understand that cardiac fibrosis may be a complex entity. There are different patterns of cardiac scars, each with its own distinct characteristics and underlying cause. Understanding these different types is crucial for accurate diagnosis, prognosis, and management of cardiac conditions. Furthermore, reparative (replacement) fibrosis represents the deposition of collagen in areas of myocyte necrosis or focal injury, forming discrete scar tissue that may disrupt myocardial conduction and increase arrhythmic risk. In contrast, interstitial fibrosis refers to diffuse collagen accumulation between viable myocytes, typically resulting from chronic mechanical stress or neurohormonal activation. In endurance athletes, limited interstitial fibrosis may reflect a reversible adaptive response, whereas reparative fibrosis—especially if patchy or midmyocardial—has been associated with a higher risk of arrhythmias and adverse outcomes.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Exercise-Induced Cardiac Remodeling&lt;/head&gt;
    &lt;p&gt;Endurance training refers to sustained, high-volume aerobic exercise performed at moderate intensity (typically 60–80% of maximal heart rate) for prolonged durations, often exceeding five hours of vigorous activity per week or 8–10 METs per session, as seen in long-distance runners, cyclists, and triathletes. The above mentioned, provokes significant cardiac remodeling, which is generally considered as a physiological adaptation to the increased demands placed on the heart. However, there is a growing interest in whether certain types or intensities of exercise may lead to maladaptive remodeling in predisposed individuals [8]. One area of focus is the potential for arrhythmogenic cardiac remodeling, particularly involving the right ventricle (RV) [8]. The RV, with its thinner walls and unique hemodynamic profile, may be more susceptible to the effects of long-term, intense endurance exercise. Studies have investigated whether intense endurance exercise disproportionately affects the RV compared to the left ventricle (LV) and at the same time whether the exposure to endurance competition influences cardiac remodeling, including the development of fibrosis, in well-trained athletes [8,9]. Following vigorous endurance events, it’s common not only to observe signs of borderline cardiac dysfunction but also elevated levels of cardiac biomarkers. These biomarkers, such as troponins [10] and B-type natriuretic peptide (BNP), are typically released into the blood serum in response to cardiac injury or stress. The clinical meaning of elevated biomarkers—especially troponin—after exercise is still uncertain. These elevations show that the heart has been under stress, but it is not yet clear whether they reflect a temporary, reversible physiological response or evidence of actual injury. This distinction remains an active area of debate and investigation [5,10,11].&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Myocardial Fibrosis in Athletes&lt;/head&gt;
    &lt;p&gt;Myocardial fibrosis, which can be identified using (LGE) on cardiac magnetic resonance imaging (CMR), has been observed in a subset of endurance athletes [1]. The detection of LGE in athletes raises important questions about its significance and whether it can sometimes represent a benign adaptation or may be an indication of adverse cardiac outcomes. The interpretation of myocardial fibrosis in the athletic population is complex and requires careful consideration of various factors, including the location, pattern, and extent of fibrosis, as well as the athlete’s clinical presentation and other relevant findings [1,7].&lt;/p&gt;
    &lt;p&gt;Myocardial fibrosis in athletes may arise from a complex interaction of mechanical stress, neurohormonal activation, and the mismanagement of extracellular matrix (ECM) remodeling. Repeated hemodynamic overload during strenuous exercise elevates myocardial wall stress, which activates cardiac fibroblasts and stimulates collagen deposition (primarily types I and III) through transforming growth factor-beta (TGF-β)/Smad signaling [12]. Pro-collagen type I is the principal precursor of mature collagen fibrils. Its upregulation, driven by TGF-β and mechanical stretch, enhances myocardial stiffness by increasing collagen cross-linking and reducing compliance. Elevated circulating levels of pro-collagen I peptides have been correlated with both pathological and exercise-induced myocardial fibrosis, serving as a potential biomarker for early extracellular matrix remodeling. In cases of physiological adaptation, temporary ECM remodeling improves myocardial stiffness and efficiency, facilitated by a balanced activity of matrix metalloproteinases (MMPs) and tissue inhibitors of metalloproteinases (TIMPs) [13,14]. However, when stress is excessive or prolonged, this balance can tip towards pathological fibrosis, which is marked by excessive collagen cross-linking and decreased degradation due to heightened levels of TGF-β, angiotensin II, and pro-fibrotic factors such as connective tissue growth factor [15]. Furthermore, oxidative stress and inflammation, instigated by NADPH oxidases and inflammatory cytokines (e.g., IL-6, TNF-α), worsen fibroblast activation and ECM accumulation [16]. Distinguishing between adaptive remodeling and maladaptive fibrosis depends on whether collagen deposition can be reversed and the extent of persistent fibroblast activation, commonly evaluated through biomarkers such as galectin-3 and soluble ST2 [17]. Recent research indicates that endurance athletes participating in extreme training regimes may show signs of subclinical fibrosis, which can be identified through CMR, raising concerns about potential long-term risks for arrhythmias [18,19,20]. Emerging biomarkers such as galectin-3 and soluble ST2 reflect myocardial stress and extracellular matrix turnover. Elevated galectin-3 levels have been linked to persistent fibroblast activation, while soluble ST2 reflects stretch-induced signaling associated with adverse remodeling. Their longitudinal assessment may provide valuable insight into dynamic changes in myocardial fibrosis among endurance athletes (Table 1, Figure 1 and Figure 2).&lt;/p&gt;
    &lt;p&gt; Table 1. Key differences between pathological myocardial fibrosis and physiological remodeling (“athlete’s heart”) based on imaging, biomarkers, clinical features, prognosis, and management implications. &lt;/p&gt;
    &lt;p&gt; Figure 1. Myocardial Fibrosis Risk in Athletes. ECG: Electrocardiogram, LGE: late gadolinium enhancement. &lt;/p&gt;
    &lt;p&gt; Figure 2. Physiological Pathways and Interventions in Myocardial Fibrosis. ECM: Extracellular matrix, IL-8: Interleukin-8, MMPs: matrix metalloproteinases, TGF-a/β: transforming growth factor-alpha/beta. &lt;/p&gt;
    &lt;p&gt;Endurance athletes tend to show a higher occurrence of atrial fibrosis, which has been closely linked to a greater risk of developing atrial arrhythmias, particularly atrial fibrillation (AF). This relationship appears to stem from the structural and electrical changes the atria undergo in response to prolonged exercise. Specifically, extended periods of endurance training promote enlargement of the atria and increased mechanical stress on their walls, encouraging the buildup of fibrotic tissue within the atrial muscle. This fibrosis disrupts the heart’s normal electrical pathways, leading to conditions favorable for irregular heart rhythms. Additionally, endurance exercise influences the autonomic nervous system, increasing vagal tone, which shortens the refractory period in the atria and further predisposes to AF. Research in elite endurance athletes, including long-distance runners and rowers, consistently shows a significantly higher rate of AF compared to non-athletic individuals, highlighting the clinical relevance of these changes. Newer studies also point to differences between sexes in how susceptible they are to these effects, as well as suggest that fibrosis and arrhythmias might partially reverse with reduced training. This growing body of evidence underscores the complex balance athletes face: while exercise offers numerous cardiovascular benefits, it may paradoxically elevate AF risk, thereby warranting careful clinical surveillance and personalized management plans in this population [21,22].&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Left Ventricular Scar&lt;/head&gt;
    &lt;p&gt;One study provided valuable insights into the potential implications of LV scar in athletes [1]. This study compared three groups of athletes: those with ventricular arrhythmias and isolated LV subepicardial/midmyocardial LGE, those with ventricular arrhythmias but no LGE, and a group of healthy control athletes [1,23]. The researchers observed that in the first group LGE predominantly involved the lateral LV wall, in 77% of athletes with LV scar. This pattern was not observed in any of the athletes in the control group (p &amp;lt; 0.001). Athletes exhibiting this patchy pattern often presented with ventricular arrhythmias characterized by a predominant right bundle branch block morphology. Additionally, a significant proportion of these athletes showed electrocardiogram (ECG) repolarization abnormalities (48%), and some demonstrated echocardiographic evidence of hypokinesis of the lateral LV wall (19%) [1]. The most alarming finding was the association between the patchy LGE pattern and adverse arrhythmic events. During a follow-up period of 38 ± 25 months, 6 of 27 (22%) athletes with the aforementioned pattern experienced malignant arrhythmic events, including appropriate implantable cardiac defibrillator (ICD) shock (n = 4), sustained ventricular tachycardia (n = 1), or sudden death (n = 1). In contrast, none of the athletes without LGE or those with a spotty LGE pattern, nor any of the control athletes, experienced such events [4]. These findings highlight the potential clinical significance of isolated non-ischemic LV LGE with a patchy pattern in athletes. The study concluded that this specific pattern of fibrosis may be associated with an increased risk of life-threatening arrhythmias and sudden death in the athletic population. Furthermore, the subepicardial/midmyocardial location of this type of LV scar often makes it difficult to detect using traditional echocardiography, emphasizing the importance of CMR for accurate assessment [1,4].&lt;/p&gt;
    &lt;head rend="h2"&gt;7. Coronary Artery Disease and Myocardial Fibrosis&lt;/head&gt;
    &lt;p&gt;To provide further context, it’s helpful to consider the role of myocardial fibrosis in individuals with coronary artery disease (CAD). In patients with CAD, contrast-enhanced CMR is a valuable tool for identifying myocardial scar resulting from myocardial infarction (MI). Numerous studies have investigated the prognostic implications of unrecognized myocardial scar detected by CMR in patients without a documented history of MI [9,19]. One such study assessed 195 patients with suspected CAD but no known prior MI who underwent CMR. During a median follow-up period of 16 months, 18% of these patients experienced major adverse cardiac events (MACE), listing: [(1) cardiac death, (2) new acute MI, (3) unstable angina requiring hospitalization, (4) development or progression of heart failure requiring hospitalization, or (5) ventricular arrhythmias requiring appropriate discharge from an internal cardioverter/defibrillator (ICD)] [23], including 17 deaths. LGE demonstrated strong unadjusted associations with MACE and cardiac mortality, with hazard ratios of 8.29 and 10.9, respectively (both p &amp;lt; 0.0001) [23]. Notably, patients in the lowest tertile of LGE-involved myocardium (mean LV mass, 1.4%) still experienced a greater than 7-fold increased risk for MACE [9,19]. In multivariable analyses, LGE remained independently associated with MACE, even after accounting for other clinical factors, angiographic findings, and measures of LV function [4]. LGE was the strongest predictor selected in the best overall models for both MACE and cardiac mortality. The study concluded that in patients with suspected CAD but no history of MI, the presence of LGE, even involving a small amount of myocardium, carries a significant cardiac risk. Furthermore, LGE provides additional prognostic value for predicting MACE and cardiac mortality beyond that offered by common clinical, angiographic, and functional predictors [9,19].&lt;/p&gt;
    &lt;head rend="h2"&gt;8. Potential Mechanisms of Fibrosis and Coronary Artery Calcium Development in Athletes&lt;/head&gt;
    &lt;p&gt;The mechanisms that underlie the development of myocardial fibrosis in athletes are not fully explained. It is believed that these mechanisms may differ from those observed in sedentary individuals. One study [9] investigated this theory by examining elite athletes with a low atherosclerotic risk profile. The researchers found that male athletes had a significantly higher prevalence of atherosclerotic plaques compared with sedentary males [9] (44.3% versus 22.2%; p = 0.009) [23]. Remarkably, the characteristics of these plaques also differed between the two groups. Male athletes predominantly exhibited calcific plaques (72.7%), which are generally considered more stable, while sedentary males showed predominantly mixed morphology plaques (61.5%), which may be more prone to rupture [9,19]. The study also identified a correlation between the number of years of training and the risk of increased CAC. The number of years of training was the only independent variable associated with an increased risk of CAC &amp;gt; 70th percentile for age or luminal stenosis ≥ 50% in male athletes (odds ratio, 1.08; 95% confidence interval, 1.01–1.15; p = 0.016). These findings suggest that the pathophysiology of plaque formation in athletes may indeed differ from that in sedentary individuals. While coronary plaques appear to be more abundant in athletes, their predominantly stable nature could potentially moderate the risk of plaque rupture and acute myocardial infarction [9,19]. The relationship between elevated CAC and myocardial fibrosis is complex and not direct. CAC reflects atherosclerotic burden and develops through mechanisms that differ from those driving myocardial fibrosis. In athletes, higher CAC scores often represent stable calcified plaques, which may not indicate active fibrosis or ischemic injury. It is therefore important to clarify that any observed correlation between CAC and myocardial fibrosis should not be interpreted as causative. Rather, it may be coincidental, and further research is needed to better understand the potential links between these processes [18].&lt;/p&gt;
    &lt;head rend="h2"&gt;9. Cardiovascular Health and Cardiorespiratory Fitness&lt;/head&gt;
    &lt;p&gt;Regular physical exercise offers a multitude of well-established health benefits and plays a vital role in combating obesity and its associated cardiovascular complications. Exercise is not only a preventive measure against the onset and development of cardiovascular disease but also an important therapeutic tool for improving outcomes in patients with existing cardiovascular conditions [3,5,23]. The beneficial effects of exercise on the cardiovascular system are wide-ranging. These include enhanced mitochondrial function, restoration and improvement of vasculature, and the release of myokines from skeletal muscle, which contribute to preserving and augmenting cardiovascular function. Exercise also improves overall metabolic health, reduces inflammation, decreases the risk of heart failure, and improves survival rates, as mentioned before. Furthermore, it improves glucose tolerance, insulin sensitivity, and lipid concentrations, all of which contribute to a healthier cardiovascular profile [3]. Regular exercise can also lead to a decrease in resting heart rate, blood pressure, and levels of atherogenic markers, while promoting physiological cardiac hypertrophy, a beneficial adaptation to the demands of exercise. Additionally, exercise enhances myocardial perfusion and increases high-density lipoprotein cholesterol levels, both of which contribute to reducing stress on the heart [10,24]. Higher levels of cardiorespiratory fitness, which serve as an indicator of regular physical activity, are consistently associated with a lower risk of cardiovascular disease mortality [25]. This protective effect of fitness is observed even in individuals who are overweight or obese and those with diabetes, highlighting the powerful benefits of exercise across various populations [25,26].&lt;/p&gt;
    &lt;head rend="h2"&gt;10. Endurance Exercise: Strength or Stress?&lt;/head&gt;
    &lt;p&gt;While the benefits of regular aerobic exercise of moderate intensity are widely established and accepted, the effects of high-volume endurance exercise are more complex and under investigation. Endurance athletes, who frequently push the limits of exercise volume and intensity, may experience a combination of both physiological adaptations and potential maladaptations [25,26]. The sustained high cardiac output demanded by endurance exercise leads to significant cardiac remodeling, including increases in ventricular size and mass [23,25]. As mentioned earlier, a significant proportion of endurance athletes exhibit elevated cardiac biomarkers and signs of cardiac dysfunction immediately following intense endurance events. In the long term, some endurance athletes show an increased prevalence of coronary artery disease, myocardial fibrosis, and arrhythmias. These potentially harmful adaptations raise concerns and prompt ongoing research into the potential detrimental effects of excessive endurance exercise in susceptible individuals [2,20,24]. It is crucial to acknowledge that despite these concerns and observations, elite endurance athletes, as a group, generally have increased life expectancy compared to the general population. Regular aerobic exercise provides robust protection against atherosclerotic cardiovascular disease, certain types of malignancies, and age-related disability, ultimately contributing to increased lifespan. This highlights the complex interplay between the benefits and potential risks associated with high-volume endurance exercise [2,9,20].&lt;/p&gt;
    &lt;head rend="h2"&gt;11. Myocardial Fibrosis: Risk Marker or Exercise Induced Adaptation?&lt;/head&gt;
    &lt;p&gt;The presence of myocardial fibrosis, particularly the subepicardial/midmyocardial patchy pattern, appears to be a risk marker for life-threatening arrhythmias and sudden cardiac death in some athletes [1]. However, fibrosis may also appear as a physiological adaptation related to endurance training in other contexts. Intense endurance training appears to trigger a mild and controlled increase in interstitial fibrosis as part of the heart’s way of adapting its extracellular matrix to heightened physical demands. This subtle buildup of collagen within the myocardial interstitium likely functions to strengthen the heart’s structural framework, helping it to endure the increased mechanical stress that comes with prolonged high-intensity exercise. Research has revealed that genes linked to fibrosis, including COL3A1, and enzymes responsible for matrix remodeling, such as MMP-2 and TIMP-1, are upregulated following long-term endurance exercise [27,28]. These changes reflect an active remodeling process that aligns with physiological adaptation rather than harmful fibrosis. It is important to recognize that this form of adaptive fibrosis differs from the permanent scarring seen in disease states, though the clinical distinction is often nuanced. Distinguishing pathological from adaptive fibrosis remains a major clinical challenge requiring further research [7,19]. A critical challenge is to develop a more precise understanding of the differences between pathological fibrosis, which clearly increases the risk of adverse cardiac events, and physiological remodeling, which may occur as a benign adaptation to the demands of exercise [7]. This distinction is critical for appropriate clinical decision-making in athletes with evidence of myocardial fibrosis [7].&lt;/p&gt;
    &lt;head rend="h2"&gt;12. Diagnostic Challenges and the Role of Cardiac Magnetic Resonance Imaging&lt;/head&gt;
    &lt;p&gt;CMR has emerged as the gold standard for the non-invasive assessment of myocardial fibrosis. Its ability to provide high-resolution images of the heart and its unique tissue characterization capabilities make it invaluable for detecting and quantifying LGE, the hallmark of myocardial fibrosis [22]. CMR offers several advantages over other imaging modalities in the evaluation of athletes’ hearts. Echocardiography, while useful for assessing cardiac structure and function, often has limitations in detecting subtle or localized areas of fibrosis, particularly in the subepicardial or midmyocardial layers [4]. CMR’s superior spatial resolution and tissue contrast allow for the identification of even small regions of fibrosis that may be missed by other techniques. The use of LGE imaging in CMR is based on the principle that damaged or scarred myocardial tissue retains gadolinium-based [1,4,7] contrast agents for a longer period compared to healthy myocardium. This difference in contrast enhancement allows for the visualization and quantification of fibrotic areas. CMR can also provide valuable information about the location, pattern, and extent of fibrosis, which can be crucial for risk stratification [7]. However, the interpretation of CMR findings in athletes requires expertise and careful consideration of the athlete’s clinical context. The presence of LGE does not automatically equate to pathology, and distinguishing between physiological remodeling and pathological fibrosis can be challenging.&lt;/p&gt;
    &lt;p&gt;T1 mapping in CMR is another promising tool, by allowing quantitative assessment of myocardial tissue characteristics. By measuring native T1 relaxation times, and in combination with contrast imaging calculating extracellular Volume (ECV), it provides valuable insight into diffuse myocardial fibrosis and overall myocardial composition. In athletes, T1 mapping highlights two contrasting processes. On one hand, increased ECV can indicate diffuse interstitial fibrosis; on the other, reduced native T1 values often reflect intracellular volume expansion linked to physiological myocardial hypertrophy. This intracellular expansion corresponds to larger cardiomyocytes and greater intracellular water content, features typical of the athlete’s heart adaptation [29,30]. Sex differences are notable: female athletes tend to show higher native T1 and ECV values than males, likely due to hormonal influences on myocardial tissue [30]. Sport type also plays a role; endurance athletes, for example, often present with lower native T1 times compared to power or skill athletes, reflecting different remodeling patterns [29]. Clinically, T1 mapping is valuable because it helps distinguish between normal physiological adaptations and early signs of pathology, such as myocarditis or cardiomyopathy, supporting more tailored clinical decisions [18]. However, interpretation in athletes requires caution and should incorporate sex- and sport-specific reference ranges to minimize misclassification [30,31]. Ongoing research is essential to clarify its prognostic significance and to refine its role in cardiac screening for athletes.&lt;/p&gt;
    &lt;p&gt;Factors such as the athlete’s training history, symptoms, ECG findings, and other relevant clinical data must be accounted as well [1,7,32].&lt;/p&gt;
    &lt;head rend="h2"&gt;13. The Spectrum of Myocardial Fibrosis Patterns in Athletes&lt;/head&gt;
    &lt;p&gt;Different patterns of LGE have been observed among athletes, and these patterns may have varying clinical implications [7,8,22].&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Patchy Pattern including the fibrotic stria: Associated with increased arrhythmic risk and sudden cardiac death&lt;/item&gt;
      &lt;item&gt;Focal LGE: Variable clinical significance depending on location and etiology; may reflect benign remodeling or potential arrhythmogenic substrate.&lt;/item&gt;
      &lt;item&gt;Diffuse LGE: Rare, usually indicating underlying cardiomyopathies warranting further evaluation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Understanding the different patterns of LGE and their clinical correlates is crucial for accurate risk stratification and management of athletes with myocardial fibrosis [7] (Table 2 and Table 3).&lt;/p&gt;
    &lt;p&gt; Table 2. Patterns of myocardial fibrosis among athletes. &lt;/p&gt;
    &lt;p&gt; Table 3. Summary table of studies, fibrosis patterns and clinical implications. &lt;/p&gt;
    &lt;head rend="h2"&gt;14. Risk Stratification and Management of Athletes with Myocardial Fibrosis&lt;/head&gt;
    &lt;p&gt;The management of athletes with myocardial fibrosis is a complex and evolving area. There are no definitive guidelines, and clinical decisions must be individualized based on a careful assessment of the athlete’s risk profile [2,19,20,23]. Risk stratification is a critical component of the management process. The goal is to identify athletes who are at increased risk of adverse cardiac events and to implement appropriate interventions to moderate that risk. Factors that may influence risk stratification include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pattern and Extent of LGE: As discussed earlier, certain patterns of LGE, such as the stria pattern, have been associated with a higher risk. The extent of LGE, or the amount of myocardium involved, is also an important consideration.&lt;/item&gt;
      &lt;item&gt;Presence of Arrhythmias: Athletes with a history of arrhythmias, particularly ventricular arrhythmias, are at higher risk.&lt;/item&gt;
      &lt;item&gt;Symptoms: The presence of symptoms such as chest pain, palpitations, or syncope raises concern.&lt;/item&gt;
      &lt;item&gt;ECG Findings: Abnormalities on the (ECG), such as repolarization abnormalities or conduction abnormalities, may indicate an increased risk.&lt;/item&gt;
      &lt;item&gt;Echocardiographic Findings: Evidence of LV dysfunction or other abnormalities on echocardiography may be a sign of increased risk.&lt;/item&gt;
      &lt;item&gt;Exercise History and Intensity: The athlete’s training history and the intensity of their exercise regimen may be relevant factors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The aforementioned work-up could be used to clarify whether the myocardial fibrosis is a result of exercise or cardiomyopathy (myocarditis, arrhythmogenic etc.) that would need more intensive management (Table 3) especially for athletes with high-risk features.&lt;/p&gt;
    &lt;p&gt;High-risk features in athletes encompass clinical, imaging, and electrophysiological findings that point to a heightened likelihood of adverse cardiac events, including sudden cardiac death. Among the strongest markers are complex or unusual ventricular arrhythmias, such as polymorphic or sustained ventricular tachycardia, which are often linked to underlying structural heart disease [33].&lt;/p&gt;
    &lt;p&gt;Cardiac MRI adds further value: extensive myocardial fibrosis, especially when seen as subepicardial or mid-wall stria patterns on LGE, is considered a marker of increased arrhythmic vulnerability [34]. Symptoms like unexplained fainting or palpitations, alongside a family history of premature sudden death, heighten clinical concern [35].&lt;/p&gt;
    &lt;p&gt;According to the 2020 ESC Sports Cardiology Guidelines, the presence of LGE or ventricular arrhythmias mandates individualized evaluation. Athletes with extensive or patchy LGE should be restricted from high-intensity endurance exercise until thorough risk assessment. In contrast, those with minor or non-progressive LGE and no arrhythmias may continue moderate exercise under surveillance (Pelliccia et al., Eur Heart J 2020) [35].&lt;/p&gt;
    &lt;p&gt;Electrocardiography also plays a role. Features such as prolonged QT intervals, abnormal repolarization, or conduction delays contribute to overall risk assessment. Similarly, non-sustained ventricular tachycardia (NSVT) detected on Holter monitoring is a notable finding, particularly when combined with other risk markers [34]. Exercise testing may also reveal exertional symptoms or arrhythmias that would otherwise remain hidden [33].&lt;/p&gt;
    &lt;p&gt;Genetic testing may sometimes be offered in certain grey zone areas or when there is evidence of a specific cardiomyopathy. Identifying mutations such as those in DSP or LMNA genes can signal an inherited predisposition to arrhythmogenic cardiomyopathy and sudden death, even in athletes with no obvious structural changes [35].&lt;/p&gt;
    &lt;p&gt;Ultimately, integrating clinical history, imaging, electrophysiological data, and genetic insights allows for a nuanced, individualized approach to managing athletes. This enables clinicians to balance the demands of performance with the imperative of safety [29]. In athletes with patchy or extensive LGE and high-risk features, endurance training should be discontinued or substantially reduced, consistent with current guideline recommendations. Pharmacological therapy is not routinely indicated unless guided by the presence of symptomatic arrhythmias or underlying cardiomyopathy [33,34].&lt;/p&gt;
    &lt;p&gt;The presence of myocardial fibrosis requires individualized counseling regarding training volume and competition eligibility. Return-to-play decisions should be based on fibrosis extent, arrhythmic risk, and clinical presentation. Athletes with isolated mild LGE and no arrhythmias may safely resume training under monitoring, whereas those with patchy or extensive LGE and arrhythmic events should restrict or discontinue endurance competition until further evaluation [35].&lt;/p&gt;
    &lt;p&gt;The diagnostic and therapeutic interventions for athletes with myocardial fibrosis are presented in Table 4 and Graphical Abstract.&lt;/p&gt;
    &lt;p&gt; Table 4. Management strategies for athletes with myocardial fibrosis. &lt;/p&gt;
    &lt;head rend="h2"&gt;15. Future Directions&lt;/head&gt;
    &lt;p&gt;Further progress in research on myocardial fibrosis in athletes is vital to better understand the fine distinction between beneficial physiological cardiac remodeling and harmful pathological changes triggered by intense endurance training. Current data highlight the difficulty in distinguishing harmless training-related fibrosis from fibrosis that poses increased risks for severe arrhythmias and sudden cardiac death, especially when recognized by specific patterns such as the subepicardial or midmyocardial patchy on CMR. Future research should prioritize refining diagnostic criteria and advancing imaging technologies to more accurately differentiate adaptive cardiac remodeling from detrimental fibrosis. Additionally, it is important to investigate the molecular and cellular mechanisms driving fibrosis, including the contributions of mechanical stress, neurohormonal pathways, inflammation, and the regulatory interplay between MMPs and TIMPs. The identification and validation of biomarkers like galectin-3 and soluble ST2 for early detection and monitoring could improve risk prediction in athletes. Longitudinal studies examining the long-term cardiovascular effects of myocardial fibrosis in this population are also necessary. Developing personalized management approaches, including customized exercise regimens and targeted medical therapies, will be key to minimizing adverse outcomes while maintaining the cardiovascular benefits of endurance exercise. These efforts will help promote safer athletic participation and optimize heart health among athletes. Future research should prioritize multicenter, prospective cohort studies integrating CMR, biomarkers (galectin-3, ST2), and clinical outcomes. Harmonized imaging and reporting protocols are essential to enable reproducibility and meta-analytic comparison across athletic populations.&lt;/p&gt;
    &lt;head rend="h2"&gt;16. Conclusions&lt;/head&gt;
    &lt;p&gt;Endurance exercise induces significant cardiac adaptations. Some of these adaptations may result in pathological remodeling of the heart, such as myocardial fibrosis. While concerns exist regarding the potential risks associated with excessive endurance exercise, it is important to acknowledge that elite endurance athletes, as a group, demonstrate increased longevity. However, this benefit likely does not extend to individuals exhibiting patchy or extensive LGE patterns, in whom the risk of malignant arrhythmias and sudden cardiac death outweighs the expected longevity advantage. Myocardial fibrosis in athletes is a complex and multifaceted phenomenon. In certain situations, such as the presence of a patchy LGE pattern, it may serve as a risk marker for adverse events, including life-threatening arrhythmias and sudden cardiac death. In other contexts, it may represent a part of the physiological remodeling process in response to the demands of intense training. Further research is essential to determine the precise role of myocardial fibrosis in athletes, to differentiate between pathological fibrosis and physiological remodeling, and to develop appropriate clinical strategies for risk stratification, management, and treatment. A greater understanding of this complex issue will help ensure the safety and well-being of athletes while allowing them to continue to pursue the benefits of endurance exercise. Despite accumulating evidence, major knowledge gaps remain in differentiating physiological from pathological fibrosis, understanding molecular mechanisms, and defining safe exercise thresholds. Again, future research should integrate advanced imaging, biomarker validation, and genetic screening to refine risk stratification. Priority areas include prevention of maladaptive fibrosis, longitudinal athlete follow-up, and interventional trials assessing the impact of tailored exercise reduction or pharmacologic modulation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Author Contributions&lt;/head&gt;
    &lt;p&gt;E.T., C.F. and V.K.; writing—original draft preparation, E.T., C.F., C.V. (Christos Vazaios) and A.K. writing—review and editing, C.F., C.V. (Christos Vazaios), S.M., V.K. and C.V. (Charalambos Vlachopoulos); visualization, K.P.T.; supervision, K.P.T. All authors have read and agreed to the published version of the manuscript.&lt;/p&gt;
    &lt;head rend="h2"&gt;Funding&lt;/head&gt;
    &lt;p&gt;The authors declare no funding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data Availability Statement&lt;/head&gt;
    &lt;p&gt;No new data were created or analyzed in this study.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conflicts of Interest&lt;/head&gt;
    &lt;p&gt;The authors declare no conflicts of interest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Abbreviations&lt;/head&gt;
    &lt;p&gt;The following abbreviations are used in this manuscript: &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;AF&lt;/cell&gt;
        &lt;cell&gt;Atrial Fibrillation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;BNP&lt;/cell&gt;
        &lt;cell&gt;B-Type Natriuretic Peptide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CAC&lt;/cell&gt;
        &lt;cell&gt;Coronary Artery Calcium&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CAD&lt;/cell&gt;
        &lt;cell&gt;Coronary Artery Disease&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CMR&lt;/cell&gt;
        &lt;cell&gt;Cardiac Magnetic Resonance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ECG&lt;/cell&gt;
        &lt;cell&gt;Electrocardiograph&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ECM&lt;/cell&gt;
        &lt;cell&gt;Extracellular Matrix&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;EF&lt;/cell&gt;
        &lt;cell&gt;Ejection Fraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ICD&lt;/cell&gt;
        &lt;cell&gt;Implantable Cardioverter Defibrillator&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;IEE&lt;/cell&gt;
        &lt;cell&gt;Intense Endurance Exercise&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LGE&lt;/cell&gt;
        &lt;cell&gt;Late Gadolinium Enhancement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LV&lt;/cell&gt;
        &lt;cell&gt;Left Ventricle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MACE&lt;/cell&gt;
        &lt;cell&gt;Major Adverse Cardiac Event&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MI&lt;/cell&gt;
        &lt;cell&gt;Myocardial Infarction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MMPs&lt;/cell&gt;
        &lt;cell&gt;Matrix Metalloproteinases&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MRI&lt;/cell&gt;
        &lt;cell&gt;Magnetic Resonance Imaging&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NSVT&lt;/cell&gt;
        &lt;cell&gt;Non-Sustained Ventricular Tachycardia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;RBBB&lt;/cell&gt;
        &lt;cell&gt;Right Bundle Branch Block&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;RV&lt;/cell&gt;
        &lt;cell&gt;Right Ventricle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TGF-β&lt;/cell&gt;
        &lt;cell&gt;Transforming Growth Factor-Beta&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TIMPs&lt;/cell&gt;
        &lt;cell&gt;Tissue Inhibitors of Metalloproteinases&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Zorzi, A.; Perazzolo Marra, M.; Rigato, I.; De Lazzari, M.; Susana, A.; Niero, A.; Pilichou, K.; Migliore, F.; Rizzo, S.; Giorgi, B.; et al. Nonischemic Left Ventricular Scar as a Substrate of Life-Threatening Ventricular Arrhythmias and Sudden Cardiac Death in Competitive Athletes. Circ. Arrhythmia Electrophysiol. 2016, 9, e004229. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Parry-Williams, G.; Sharma, S. The Effects of Endurance Exercise on the Heart: Panacea or Poison? Nat. Rev. Cardiol. 2020, 17, 402–412. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Pinckard, K.; Baskin, K.K.; Stanford, K.I. Effects of Exercise to Improve Cardiovascular Health. Front. Cardiovasc. Med. 2019, 6, 69. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Schnell, F.; Claessen, G.; La Gerche, A.; Bogaert, J.; Lentz, P.-A.; Claus, P.; Mabo, P.; Carré, F.; Heidbuchel, H. Subepicardial Delayed Gadolinium Enhancement in Asymptomatic Athletes: Let Sleeping Dogs Lie? Br. J. Sports Med. 2016, 50, 111–117. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Ekelund, U.; Tarp, J.; Steene-Johannessen, J.; Hansen, B.H.; Jefferis, B.; Fagerland, M.W.; Whincup, P.; Diaz, K.M.; Hooker, S.P.; Chernofsky, A.; et al. Dose-Response Associations between Accelerometry Measured Physical Activity and Sedentary Time and All Cause Mortality: Systematic Review and Harmonised Meta-Analysis. BMJ 2019, 366, l4570. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Global Recommendations on Physical Activity for Health. Available online: https://www.who.int/publications/i/item/9789241599979 (accessed on 30 March 2025).&lt;/item&gt;
      &lt;item&gt;Hinderer, S.; Schenke-Layland, K. Cardiac Fibrosis—A Short Review of Causes and Therapeutic Strategies. Adv. Drug Deliv. Rev. 2019, 146, 77–82. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;La Gerche, A.; Burns, A.T.; Mooney, D.J.; Inder, W.J.; Taylor, A.J.; Bogaert, J.; Macisaac, A.I.; Heidbüchel, H.; Prior, D.L. Exercise-Induced Right Ventricular Dysfunction and Structural Remodelling in Endurance Athletes. Eur. Heart J. 2012, 33, 998–1006. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Schmermund, A. Letter by Schmermund Regarding Article, “Prevalence of Subclinical Coronary Artery Disease in Masters Endurance Athletes with a Low Atherosclerotic Risk Profile”. Circulation 2018, 137, 539–540. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Aengevaeren, V.L.; Hopman, M.T.E.; Thompson, P.D.; Bakker, E.A.; George, K.P.; Thijssen, D.H.J.; Eijsvogels, T.M.H. Exercise-Induced Cardiac Troponin I Increase and Incident Mortality and Cardiovascular Events. Circulation 2019, 140, 804–814. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Małek, Ł.A.; Bucciarelli-Ducci, C. Myocardial Fibrosis in Athletes-Current Perspective. Clin. Cardiol. 2020, 43, 882–888. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Frangogiannis, N.G. The Extracellular Matrix in Ischemic and Nonischemic Heart Failure. Circ. Res. 2019, 125, 117–146. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Fan, D.; Kassiri, Z. Biology of Tissue Inhibitor of Metalloproteinase 3 (TIMP3), and Its Therapeutic Implications in Cardiovascular Pathology. Front. Physiol. 2020, 11, 661. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Jaoude, J.; Koh, Y. Matrix Metalloproteinases in Exercise and Obesity. Vasc. Health Risk Manag. 2016, 12, 287–295. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Saadat, S.; Noureddini, M.; Mahjoubin-Tehran, M.; Nazemi, S.; Shojaie, L.; Aschner, M.; Maleki, B.; Abbasi-kolli, M.; Rajabi Moghadam, H.; Alani, B.; et al. Pivotal Role of TGF-β/Smad Signaling in Cardiac Fibrosis: Non-Coding RNAs as Effectual Players. Front. Cardiovasc. Med. 2021, 7, 588347. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Ribeiro-Samora, G.A.; Rabelo, L.A.; Ferreira, A.C.C.; Favero, M.; Guedes, G.S.; Pereira, L.S.M.; Parreira, V.F.; Britto, R.R. Inflammation and Oxidative Stress in Heart Failure: Effects of Exercise Intensity and Duration. Braz. J. Med. Biol. Res. 2017, 50, e6393. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Hättasch, R.; Spethmann, S.; de Boer, R.A.; Ruifrok, W.P.T.; Schattke, S.; Wagner, M.; Schroeckh, S.; Durmus, T.; Schimke, I.; Sanad, W.; et al. Galectin-3 Increase in Endurance Athletes. Eur. J. Prev. Cardiol. 2014, 21, 1192–1199. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Kaddoura, R.; Al-Tamimi, H. Impact of Myocardial Fibrosis in Endurance Athletes: A Systematic Review. Heart Views Off. J. Gulf Heart Assoc. 2025, 26, 34–42. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Kwong, R.Y.; Chan, A.K.; Brown, K.A.; Chan, C.W.; Reynolds, H.G.; Tsang, S.; Davis, R.B. Impact of Unrecognized Myocardial Scar Detected by Cardiac Magnetic Resonance Imaging on Event-Free Survival in Patients Presenting with Signs or Symptoms of Coronary Artery Disease. Circulation 2006, 113, 2733–2743. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Fyyaz, S.; Tomoaia, R.; Javed, W.; Merghani, A.; Papatheodorou, S.; Parry-Williams, G.; Bhatia, R.; Chatrath, N.; Maclachlan, H.; Alfakih, K.; et al. Patterns &amp;amp; Prevalence of Myocardial Fibrosis in Masters Endurance Athletes. Eur. J. Prev. Cardiol. 2025, 32 (Suppl. 1), zwaf236.416. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Spencer, L.W.; D’Ambrosio, P.; Ohanian, M.; Rowe, S.J.; Janssens, K.; Claessen, G.; Fatkin, D.; La Gerche, A. Atrial Cardiomyopathy in Endurance Athletes. NPJ Cardiovasc. Health 2024, 1, 30. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Guasch, E.; Mont, L. Atrial Fibrillation in Elite Rowers: A Genetic and Endurance Sport Nexus? Eur. Heart J. 2025, ehaf410. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Mosteoru, S.; Vassiliou, V. Athlete’s Heart or Cardiomyopathy? Unravelling the Grey Zone with Cardiovascular Magnetic Resonance Feature Tracking. Eur. J. Prev. Cardiol. 2025, zwaf049. Available online: https://academic.oup.com/eurjpc/advance-article/doi/10.1093/eurjpc/zwaf049/8003290 (accessed on 30 March 2025).&lt;/item&gt;
      &lt;item&gt;O’Hanlon, R.; Wilson, M.; Wage, R.; Smith, G.; Alpendurada, F.D.; Wong, J.; Dahl, A.; Oxborough, D.; Godfrey, R.; Sharma, S.; et al. Troponin Release Following Endurance Exercise: Is Inflammation the Cause? A Cardiovascular Magnetic Resonance Study. J. Cardiovasc. Magn. Reson. Off. J. Soc. Cardiovasc. Magn. Reson. 2010, 12, 38. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Church, T.S.; LaMonte, M.J.; Barlow, C.E.; Blair, S.N. Cardiorespiratory Fitness and Body Mass Index as Predictors of Cardiovascular Disease Mortality among Men with Diabetes. Arch. Intern. Med. 2005, 165, 2114–2120. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Gaudreault, V.; Tizon-Marcos, H.; Poirier, P.; Pibarot, P.; Gilbert, P.; Amyot, M.; Rodés-Cabau, J.; Després, J.-P.; Bertrand, O.; Larose, E. Transient Myocardial Tissue and Function Changes during a Marathon in Less Fit Marathon Runners. Can. J. Cardiol. 2013, 29, 1269–1276. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Kui, P.; Polyák, A.; Morvay, N.; Tiszlavicz, L.; Nagy, N.; Ördög, B.; Takács, H.; Leprán, I.; Farkas, A.; Papp, J.G.; et al. Long-Term Endurance Exercise Training Alters Repolarization in a New Rabbit Athlete’s Heart Model. Front. Physiol. 2022, 12, 741317. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;D’Andrea, A.; La Gerche, A.; Golia, E.; Teske, A.J.; Bossone, E.; Russo, M.G.; Calabrò, R.; Baggish, A.L. Right Heart Structural and Functional Remodeling in Athletes. Echocardiography 2015, 32 (Suppl. 1), S11–S22. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Prosperi, S.; Monosilio, S.; Lemme, E.; Filomena, D.; Penza, M.; Birtolo, L.I.; Mango, R.; Di Gioia, G.; Gualdi, G.; Squeo, M.R.; et al. CMR Native T1 and T2 Mapping in Olympic Athletes: The Influence of Sports Discipline and Sex. Eur. Heart J. Cardiovasc. Imaging 2024, 26, 89–95. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Daems, J.J.N.; Verwijs, S.M.; Van Luijk-Snoeks, R.D.; Van Hattum, J.C.; Ercan, A.; Moen, M.H.; Nederveen, A.J.; Planken, R.N.; Van Randen, A.; Pinto, Y.M.; et al. Athlete Status and Gender Strongly Influence CMR T1 Mapping Times: An Athlete-Control Study. Eur. Heart J. 2022, 43 (Suppl. 2), ehac544.2484. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Balla, D.; Szabo, L.; Graziano, F.; Mesko, C.; Dohy, Z.; Juhasz, V.; Amirifard, D.; Sydo, N.; Csulak, E.; Petrov, I.; et al. The Role of Sex, Training Load, and Sports Type in Athletic Cardiac Remodelling: Insights from T1 and T2 Mapping via Cardiac Magnetic Resonance. Int. J. Cardiol. 2025, 426, 133080. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Bacharova, L.; Triantafyllou, E.; Vazaios, C.; Tomeckova, I.; Paranicova, I.; Tkacova, R. The Effect of Obstructive Sleep Apnea on QRS Complex Morphology. J. Electrocardiol. 2015, 48, 164–170. [Google Scholar] [CrossRef]&lt;/item&gt;
      &lt;item&gt;Dello Russo, A.; Compagnucci, P.; Casella, M.; Gasperetti, A.; Riva, S.; Dessanai, M.A.; Pizzamiglio, F.; Catto, V.; Guerra, F.; Stronati, G.; et al. Ventricular Arrhythmias in Athletes: Role of a Comprehensive Diagnostic Workup. Heart Rhythm 2022, 19, 90–99. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Javed, W.; Botis, I.; Goh, Z.M.; Shabi, M.; Brown, B.; Tomoaia, R.; Farooq, M.; Levelt, E.; Graham, L.; Gierula, J.; et al. Ventricular Arrhythmia and Cardiac Fibrosis in Endurance Experienced Athletes (VENTOUX). Circ. Cardiovasc. Imaging 2025, 18, e018470. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
      &lt;item&gt;Pelliccia, A.; Sharma, S.; Gati, S.; Bäck, M.; Börjesson, M.; Caselli, S.; Collet, J.-P.; Corrado, D.; Drezner, J.A.; Halle, M.; et al. 2020 ESC Guidelines on Sports Cardiology and Exercise in Patients with Cardiovascular Disease. Rev. Esp. Cardiol. Engl. Ed. 2021, 74, 545. [Google Scholar] [CrossRef] [PubMed]&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;© 2025 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46196708</guid><pubDate>Mon, 08 Dec 2025 19:45:57 +0000</pubDate></item></channel></rss>