<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 21 Dec 2025 23:36:24 +0000</lastBuildDate><item><title>Show HN: RenderCV – Open-source CV/resume generator, YAML to PDF</title><link>https://github.com/rendercv/rendercv</link><description>&lt;doc fingerprint="b1267b1f50e40ad3"&gt;
  &lt;main&gt;
    &lt;p&gt;Write your CV or resume as YAML, then run RenderCV,&lt;/p&gt;
    &lt;code&gt;rendercv render John_Doe_CV.yaml&lt;/code&gt;
    &lt;p&gt;and get a PDF with perfect typography. No template wrestling. No broken layouts. Consistent spacing, every time.&lt;/p&gt;
    &lt;p&gt;With RenderCV, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Version-control your CV — it's just text.&lt;/item&gt;
      &lt;item&gt;Focus on content — don't wory about the formatting.&lt;/item&gt;
      &lt;item&gt;Get perfect typography — pixel-perfect alignment and spacing, handled for you.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A YAML file like this:&lt;/p&gt;
    &lt;code&gt;cv:
  name: John Doe
  location: San Francisco, CA
  email: john.doe@email.com
  website: https://rendercv.com/
  social_networks:
    - network: LinkedIn
      username: rendercv
    - network: GitHub
      username: rendercv
  sections:
    Welcome to RenderCV:
      - RenderCV reads a CV written in a YAML file, and generates a PDF with professional typography.
      - See the [documentation](https://docs.rendercv.com) for more details.
    education:
      - institution: Princeton University
        area: Computer Science
        degree: PhD
        date:
        start_date: 2018-09
        end_date: 2023-05
        location: Princeton, NJ
        summary:
        highlights:
          - "Thesis: Efficient Neural Architecture Search for Resource-Constrained Deployment"
          - "Advisor: Prof. Sanjeev Arora"
          - NSF Graduate Research Fellowship, Siebel Scholar (Class of 2022)
    ...&lt;/code&gt;
    &lt;p&gt;becomes one of these PDFs. Click on the images to preview.&lt;/p&gt;
    &lt;p&gt;RenderCV's JSON Schema lets you fill out the YAML interactively, with autocompletion and inline documentation.&lt;/p&gt;
    &lt;p&gt;You have full control over every detail.&lt;/p&gt;
    &lt;code&gt;design:
  theme: classic
  page:
    size: us-letter
    top_margin: 0.7in
    bottom_margin: 0.7in
    left_margin: 0.7in
    right_margin: 0.7in
    show_footer: true
    show_top_note: true
  colors:
    body: rgb(0, 0, 0)
    name: rgb(0, 79, 144)
    headline: rgb(0, 79, 144)
    connections: rgb(0, 79, 144)
    section_titles: rgb(0, 79, 144)
    links: rgb(0, 79, 144)
    footer: rgb(128, 128, 128)
    top_note: rgb(128, 128, 128)
  typography:
    line_spacing: 0.6em
    alignment: justified
    date_and_location_column_alignment: right
    font_family: Source Sans 3
  # ...and much more&lt;/code&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Want to set up a live preview environment like the one shown above? See how to set up VS Code for RenderCV.&lt;/p&gt;
    &lt;p&gt;No surprises. If something's wrong, you'll know exactly what and where. If it's valid, you get a perfect PDF.&lt;/p&gt;
    &lt;p&gt;Fill out the locale field for your language.&lt;/p&gt;
    &lt;code&gt;locale:
  language: english
  last_updated: Last updated in
  month: month
  months: months
  year: year
  years: years
  present: present
  month_abbreviations:
    - Jan
    - Feb
    - Mar
  ...&lt;/code&gt;
    &lt;p&gt;Install RenderCV (Requires Python 3.12+):&lt;/p&gt;
    &lt;code&gt;pip install "rendercv[full]"
&lt;/code&gt;
    &lt;p&gt;Create a new CV yaml file:&lt;/p&gt;
    &lt;code&gt;rendercv new "John Doe"
&lt;/code&gt;
    &lt;p&gt;Edit the YAML, then render:&lt;/p&gt;
    &lt;code&gt;rendercv render "John_Doe_CV.yaml"
&lt;/code&gt;
    &lt;p&gt;For more details, see the user guide.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46344616</guid><pubDate>Sun, 21 Dec 2025 13:15:28 +0000</pubDate></item><item><title>Three ways to solve problems</title><link>https://andreasfragner.com/writing/three-ways-to-solve-problems</link><description>&lt;doc fingerprint="b290151b05b68da4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Three ways to solve problems&lt;/head&gt;
    &lt;p&gt;One of my favorite definitions of a problem comes from the late Gerald Weinberg [1]&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A problem is the difference between things as perceived and desired.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This definition is great because it’s actionable. It tells you that there are three ways to approach a problem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Move the world towards the desired state&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Change your perception of the current state&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Change your desired state&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Points two and three seem like cop-outs at first — you basically avoid solving the problem. But they often turn out to be not just viable but optimal since they force you to re-frame and re-contextualize the problem.&lt;/p&gt;
    &lt;p&gt;Changing your desired state allows you to solve a different, possibly easier problem. For example, rather than solving the full problem (to get to the original desired state) you might find that a partial solution (the new desired state) gets you 80% there at 20% of the cost.&lt;/p&gt;
    &lt;p&gt;Changing your perception of the current state, you might realize it’s close enough to the desired state and so the problem doesn’t need to be dealt with at all right now. Deciding not to solve a problem can also be a solution.&lt;/p&gt;
    &lt;p&gt;As a strategy, deciding not to solve a problem or to solve a different version seems generally underutilized. I suspect this is largely because (a) we’re bad at understanding tradeoffs and quantifying opportunity costs, and (b) because it’s just hard to say no to people who feel strongly about reaching a desired state but might lack the full picture. For example, startups most often have messy finances and HR at the beginning. Not having good accounting or employment contracts is definitely a problem but one with lower weight than failing to ship product or hit growth targets. So deciding to minimally solve for it at first is the rational move. There will be pressure to do it to a high standard from the get-go — from investors or people on your team — but you have to resist it.&lt;/p&gt;
    &lt;p&gt;As a founder you’re constantly confronted with these kinds of tradeoffs and being good at mastering them is underrated I think. It takes discipline and clarity of mind to say no in the face of pressure. It’s especially hard when the pressure comes from yourself — when your own high, uncompromising standards get in the way. This is where I think repeat founders set themselves apart from first-time ones.&lt;/p&gt;
    &lt;p&gt;Similar situation as a product manager. All products have all sorts of problems all the time — missing features, broken edge cases, high-friction UI, bad UX, etc. But reaching the desired state (perfect product) is neither possible nor sensible. The key is to say no to solving 90% of the problems you have and to focus on the 10% that really matter.&lt;/p&gt;
    &lt;p&gt;[1] Gause and Weinberg, Are your lights on?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345125</guid><pubDate>Sun, 21 Dec 2025 14:35:29 +0000</pubDate></item><item><title>Structured outputs create false confidence</title><link>https://boundaryml.com/blog/structured-outputs-create-false-confidence</link><description>&lt;doc fingerprint="15180b5c38045eba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Structured Outputs Create False Confidence&lt;/head&gt;
    &lt;p&gt;Constrained decoding seems like the greatest thing since sliced bread, but it often forces models to prioritize output conformance over output quality.&lt;/p&gt;
    &lt;p&gt;Sam Lijin&lt;/p&gt;
    &lt;p&gt;Update (Dec 21): this post is now on the Hacker News front page! We've updated this post to be more precise about our claims and have also added some clarifications at the end. You can see the original version of this post here.&lt;/p&gt;
    &lt;p&gt;If you use LLMs, you've probably heard about structured outputs. You might think they're the greatest thing since sliced bread. Unfortunately, structured outputs also often degrade response quality.&lt;/p&gt;
    &lt;p&gt;Specifically, if you use an LLM provider's structured outputs API, you're likely to get a lower quality response than if you use their normal text output API:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;⚠️ you're more likely to make mistakes when extracting data, even in simple cases;&lt;/item&gt;
      &lt;item&gt;⚠️ you're probably not modeling errors correctly;&lt;/item&gt;
      &lt;item&gt;⚠️ it's harder to use techniques like chain-of-thought reasoning; and&lt;/item&gt;
      &lt;item&gt;⚠️ in the extreme case, it can be easier to steal your customer data using prompt injection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are very contentious claims, so let's start with an example: extracting data from a receipt.&lt;/p&gt;
    &lt;p&gt;If I use an LLM to extract the receipt entries, it should be able to tell me that one of the items is &lt;code&gt;(name="banana", quantity=0.46)&lt;/code&gt;, right?&lt;/p&gt;
    &lt;p&gt;Well, using OpenAI's structured outputs API with &lt;code&gt;gpt-5.2&lt;/code&gt; - released literally this week! - it will claim that the banana quantity is &lt;code&gt;1.0&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "establishment_name": "PC Market of Choice",
  "date": "2007-01-20",
  "total": 0.32,
  "currency": "USD",
  "items": [
    {
      "name": "Bananas",
      "price": 0.32,
      "quantity": 1
    }
  ]
}
&lt;/code&gt;
    &lt;p&gt;However, with the same model, if you just use the completions API and then parse the output, it will return the correct quantity:&lt;/p&gt;
    &lt;code&gt;{
  "establishment_name": "PC Market of Choice",
  "date": "2007-01-20",
  "total": 0.32,
  "currency": "USD",
  "items": [
    {
      "name": "Bananas",
      "price": 0.69,
      "quantity": 0.46
    }
  ]
}
&lt;/code&gt;
    &lt;head&gt;Click here to see the code that was used to generate the above outputs.&lt;/head&gt;
    &lt;p&gt;This code is also available on GitHub.&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env -S uv run

# /// script
# requires-python = "&amp;gt;=3.10"
# dependencies = ["openai", "pydantic", "rich"]
# ///

"""
If you have uv, you can run this code by saving it as structured_outputs_quality_demo.py and then running:

  chmod u+x structured_outputs_quality_demo.py
  ./structured_outputs_quality_demo.py

This script is a companion to https://boundaryml.com/blog/structured-outputs-create-false-confidence
"""

import json
import re
from openai import OpenAI
from pydantic import BaseModel, Field
from rich.console import Console
from rich.pretty import Pretty


class Item(BaseModel):
    name: str
    price: float = Field(description="per-unit item price")
    quantity: float = Field(default=1, description="If not specified, assume 1")


class Receipt(BaseModel):
    establishment_name: str
    date: str = Field(description="YYYY-MM-DD")
    total: float = Field(description="The total amount of the receipt")
    currency: str = Field(description="The currency used for everything on the receipt")
    items: list[Item] = Field(description="The items on the receipt")


client = OpenAI()
console = Console()


def run_receipt_extraction_structured(image_url: str):
    """Call the LLM to extract receipt data from an image URL and return the raw response."""
    prompt_text = (
        """
Extract data from the receipt.
"""
    )

    response = client.beta.chat.completions.parse(
        model="gpt-5.2-2025-12-11",
        messages=[
            {
                "role": "system",
                "content": "You are a precise receipt extraction engine. Return only structured data matching the Receipt schema.",
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt_text,
                    },
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            },
        ],
        response_format=Receipt,
    )
    return response.choices[0].message.content, response.choices[0].message.parsed


def run_receipt_extraction_freeform(image_url: str):
    """Call the LLM to extract receipt data from an image URL and return the raw response."""
    prompt_text = (
        """
Extract data from the receipt.

Explain your reasoning, then answer in JSON:
{
  establishment_name: string,
  // YYYY-MM-DD
  date: string,
  // The total amount of the receipt
  total: float,
  // The currency used for everything on the receipt
  currency: string,
  // The items on the receipt
  items: [
    {
      name: string,
      // per-unit item price
      price: float,
      // If not specified, assume 1
      quantity: float,
    }
  ],
}
"""
    )

    response = client.beta.chat.completions.parse(
        model="gpt-5.2-2025-12-11",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": prompt_text,
                    },
                    {"type": "image_url", "image_url": {"url": image_url}},
                ],
            },
        ],
    )
    return response.choices[0].message.content, json.loads(re.search(r"```json(.*?)```", response.choices[0].message.content, flags=re.DOTALL).group(1))



def main() -&amp;gt; None:
    images = [
        {
            "title": "Parsing receipt: fractional quantity",
            "url": "https://boundaryml.com/receipt-fractional-quantity.jpg",
            "expected": "You should expect quantity to be 0.46."
        },
        {
            "title": "Parsing receipt: elephant",
            "url": "https://boundaryml.com/receipt-elephant.jpg",
            "expected": "You should expect an error."
        },
        {
            "title": "Parsing receipt: currency exchange",
            "url": "https://boundaryml.com/receipt-currency-exchange.jpg",
            "expected": "You should expect a warning about mixed currencies."
        },
    ]

    print("This is a demonstration of how structured outputs create false confidence.")

    for entry in images:
        title = entry["title"]
        url = entry["url"]

        completion_structured_content, _ = run_receipt_extraction_structured(url)
        completion_freeform_content, _ = run_receipt_extraction_freeform(url)

        console.print("[cyan]--------------------------------[/cyan]")
        console.print(f"[cyan]{title}[/cyan]")
        console.print(f"Asking LLM to parse receipt from {url}")
        console.print(entry['expected'])
        console.print()
        console.print("[cyan]Using structured outputs:[/cyan]")
        console.print(completion_structured_content)
        console.print()
        console.print("[cyan]Parsing free-form output:[/cyan]")
        console.print(completion_freeform_content)


if __name__ == "__main__":
    main()
&lt;/code&gt;
    &lt;p&gt;Now, what happens if someone submits a picture of an elephant?&lt;/p&gt;
    &lt;p&gt;Or a currency exchange receipt?&lt;/p&gt;
    &lt;p&gt;In these scenarios, you want to let the LLM respond using text. You want it to be able to say that, hey, you're asking me to parse a receipt, but you gave me a picture of an elephant, I can't parse an elephant into a receipt.&lt;/p&gt;
    &lt;p&gt;If you force the LLM to respond using structured outputs, you take that ability away from the LLM. Sure, you'll get an object that satisfies your output format, but it'll be meaningless. It's like when you file a bug report, and the form has 5 mandatory fields about things that have nothing to do with your bug, but you have to put something in those fields to file the bug report: the stuff you put in those fields will probably be useless.&lt;/p&gt;
    &lt;head rend="h1"&gt;I can design my output format better!&lt;/head&gt;
    &lt;p&gt;Yes and no.&lt;/p&gt;
    &lt;p&gt;Yes, you can tell your LLM to return &lt;code&gt;{ receipt data } or { error }&lt;/code&gt; . But what kinds of errors are you going to ask it to consider?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What kind of error should it return if there's no &lt;code&gt;total&lt;/code&gt;listed on the receipt? Should it even return an error or is it OK for it to return&lt;code&gt;total = null&lt;/code&gt;?&lt;/item&gt;
      &lt;item&gt;What if it can successfully parse 7 of 8 items on the receipt, but it's not sure about the 8th item? Should it return (1) the 7 successfully parsed items and a partial parse of the 8th item, (2) only the 7 successfully parsed items and discard the 8th or (3) fail parsing entirely?&lt;/item&gt;
      &lt;item&gt;What if someone submits a picture of an elephant? What kind of error should be returned in that case?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition, as you start enumerating all of these errors, you run into the pink elephant problem: the more your prompt talks about errors, the more likely the LLM is to respond with an error.&lt;/p&gt;
    &lt;p&gt;Think of it this way: if someone presses Ctrl-C when running your binary, it is a Good Thing that the error can propagate all the way up through your binary, without you having to explicitly write &lt;code&gt;try { ... } catch CtrlCError { ... }&lt;/code&gt; in every function in your codebase.&lt;/p&gt;
    &lt;p&gt;In the same way that you often want to allow errors to just propagate up while writing software, and only explicitly handle some errors, your LLM should be allowed to respond with errors in whatever fashion it wants to.&lt;/p&gt;
    &lt;head rend="h1"&gt;Chain-of-thought is crippled by structured outputs&lt;/head&gt;
    &lt;p&gt;"Explain your reasoning step by step" is a magic incantation that seemingly makes LLMs much smarter. It also turns out that this trick doesn't work nearly as well when using structured outputs&lt;del&gt;, and we've known this since Aug 2024&lt;/del&gt; (edit: it turns out the "Let Me Speak Freely" paper is fundamentally flawed, but we do still believe the claim to be true).&lt;/p&gt;
    &lt;p&gt;To understand this finding, the intuition I like to use, is to think of every model of having an intelligence "budget", and that if you try to force an LLM to reason in a very specific format, you're making the LLM spend intelligence points on useless work.&lt;/p&gt;
    &lt;p&gt;To make this more concrete, let's use another example. If you prompt an LLM to give you JSON output and reason about it step-by-step, its response will look something like this:&lt;/p&gt;
    &lt;code&gt;If we think step by step we can see that:

1. The email is from Amazon, confirming the status of a specific order.
2. The subject line says "Your Amazon.com order of 'Wood Dowel Rods...' has shipped!" which indicates that the order status is 'SHIPPED'.
3. [...]

Combining all these points, the output JSON is:

```json
{
     "order_status": "SHIPPED",
     [...]
}
```
&lt;/code&gt;
    &lt;p&gt;Notice that although the response contains valid JSON, the response itself is not valid JSON, because of the reasoning text at the start. In other words, you can't use basic chain-of-thought reasoning with structured outputs.&lt;/p&gt;
    &lt;p&gt;You could modify your schema, and add &lt;code&gt;reasoning: string&lt;/code&gt; fields to your output schema, and let the LLM respond with something like this:&lt;/p&gt;
    &lt;code&gt;{
  "reasoning": "If we think step by step we can see that:\n\n 1. The email is from Amazon, confirming the status of a specific order.\n2. The subject line says \"Your Amazon.com order of 'Wood Dowel Rods...' has shipped!\" [...]
  ...
}
&lt;/code&gt;
    &lt;p&gt;In other words, if you're using a &lt;code&gt;reasoning&lt;/code&gt; field with structured outputs, instead of simply asking the LLM to reason about its answer, you're also forcing it to escape newlines and quotes and format that correctly as JSON. You're basically asking the LLM to put a cover page on its TPS report.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why are structured outputs often worse?&lt;/head&gt;
    &lt;p&gt;(To understand this section, you'll need a bit of background on transformer models, specifically how logit sampling works. Feel free to skip this section if you don't have this background.)&lt;/p&gt;
    &lt;p&gt;Model providers like OpenAI and Anthropic implement structured outputs using a technique called constrained decoding:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By default, when models are sampled to produce outputs, they are entirely unconstrained and can select any token from the vocabulary as the next output. This flexibility is what allows models to make mistakes; for example, they are generally free to sample a curly brace token at any time, even when that would not produce valid JSON. In order to force valid outputs, we constrain our models to only tokens that would be valid according to the supplied schema, rather than all available tokens.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, constrained decoding applies a filter during sampling that says, OK, given the output that you've produced so far, you're only allowed to consider certain tokens.&lt;/p&gt;
    &lt;p&gt;For example, if the LLM has so far produced &lt;code&gt;{"quantity": 51&lt;/code&gt;, and you're constraining output decoding to satisfy  &lt;code&gt;{ quantity: int, ... }&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 51.7&lt;/code&gt;would not satisfy the constraint, so&lt;code&gt;.7&lt;/code&gt;is not allowed to be the next token,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 51,&lt;/code&gt;would satisfy the constraint, so&lt;code&gt;,&lt;/code&gt;is allowed to be the next token,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;{"quantity": 510&lt;/code&gt;would satisfy the constraint, so&lt;code&gt;0&lt;/code&gt;is allowed to be the next token (albeit, in this example, with low probability!),&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But if the LLM actually wants to answer with &lt;code&gt;51.7&lt;/code&gt; instead of &lt;code&gt;51&lt;/code&gt;, it isn't allowed to, because of our constraint! (Also, &lt;code&gt;51&lt;/code&gt; is less correct than &lt;code&gt;52&lt;/code&gt; in this scenario.)&lt;/p&gt;
    &lt;p&gt;Sure, if you're using constrained decoding to force it to return &lt;code&gt;{"quantity": 51.7}&lt;/code&gt; instead of &lt;code&gt;{"quantity": 51.7,}&lt;/code&gt; - because trailing commas are not allowed in JSON - it'll probably do the right thing. But that's something you can write code to handle, which leads me to my final point.&lt;/p&gt;
    &lt;head rend="h1"&gt;Just parse the output&lt;/head&gt;
    &lt;p&gt;OK, so if structured outputs are bad, then what's the solution?&lt;/p&gt;
    &lt;p&gt;It turns out to be really simple: let the LLM do what it's trained to do. Allow it to respond in a free-form style:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;let it refuse to count the number of entries in a list&lt;/item&gt;
      &lt;item&gt;let it warn you when you've given it contradictory information&lt;/item&gt;
      &lt;item&gt;let it tell you the correct approach when you inadvertently ask it to use the wrong approach&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using structured outputs, via constrained decoding, makes it much harder for the LLM to do any of this. Even though you've crafted a guarantee that the LLM will return a response in exactly your requested output format, that guarantee comes at the cost of the quality of that response, because you're forcing the LLM to prioritize complying with your output format over returning a high-quality response. That's why structured outputs create false confidence: it's entirely non-obvious that you're sacrificing output quality to achieve output conformance.&lt;/p&gt;
    &lt;p&gt;Parsing the LLM's free-form output, by contrast, enables you to retain that output quality. In fact, last year we demonstrated that using this technique, not only could you outperform constrained decoding, but you could also make &lt;code&gt;gpt-4o-mini&lt;/code&gt; outperform baseline &lt;code&gt;gpt-4o&lt;/code&gt; (constrained decoding at the time was described as "function calling (strict)").&lt;/p&gt;
    &lt;p&gt;(In a scenario where an attacker is trying to convince your agent to do something you didn't design it to do, the parsing also serves as an effective defense-in-depth layer against malicious prompt injection.)&lt;/p&gt;
    &lt;p&gt;Doing this parsing effectively, though, is rather involved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you need a way to embed the output format in the prompt, preferably something less verbose than JSON schema;&lt;/item&gt;
      &lt;item&gt;you need a parser that can find JSON in your output and, when working with non-frontier models, can handle unquoted strings, key-value pairs without comma delimiters, unescaped quotes and newlines; and&lt;/item&gt;
      &lt;item&gt;you need a parser that can coerce the JSON into your output schema, if the model, say, returns a float where you wanted an int, or a &lt;code&gt;string&lt;/code&gt;where you wanted a&lt;code&gt;string[]&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It turns out that not only are these individually pretty hard problems to solve, but it's also really hard to wrap these in an ergonomic API. That's how we ended up with the implementation of schema-aligned parsing that we've made available in BAML, our open-source, local-only DSL.&lt;/p&gt;
    &lt;head rend="h1"&gt;Clarifications&lt;/head&gt;
    &lt;p&gt;In response to a lot of the comments we've gotten so far:&lt;/p&gt;
    &lt;p&gt;To be more precise about my claim, it's not that constrained decoding always gives definitively bad outputs (I went too clickbait with my original tagline, I admit it), so much as that it's really easy, with constrained decoding, to create the illusion that you're getting good responses.&lt;/p&gt;
    &lt;p&gt;Because it forces the LLM to prioritize conforming to your output schema (and in particular forces conforming to JSON), without giving the LLM an escape hatch, it's really easy when using constrained decoding to shift errors from very visible "JSON.parse failed" quantitative errors to very subtle "my users are complaining that I give them crappy results" quality errors.&lt;/p&gt;
    &lt;p&gt;(This is a very hard message to convey at the start of an article, and depends on a lot of nuance that needs to first be explained with examples, which is why I wrote it the way I did.)&lt;/p&gt;
    &lt;p&gt;In response to more specific points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Give me evals.&lt;/p&gt;&lt;p&gt;Here you go: last year on BFCL with&lt;/p&gt;&lt;code&gt;gpt-4o&lt;/code&gt;we achieved 93.63% accuracy, vs 91.37% accuracy with constrained decoding (then called "Function Calling (Strict)"). (We should re-run these at some point, but we haven't blocked out the time for it.)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Can't you just use reasoning models instead of chain-of-thought?&lt;/p&gt;&lt;p&gt;Yes, you can use reasoning models to combine chain-of-thought with constrained decoding, but it requires the model to be explicitly trained with&lt;/p&gt;&lt;code&gt;&amp;lt;analysis&amp;gt;&lt;/code&gt;tokens or the equivalent thereof. This is only true of expensive, frontier models (gpt-5, gpt-5-mini, Opus 4.5, Gemini 3) and does not work with gpt-4o-mini, gpt-4.5-mini, nor many of the most commonly used models.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"Let Me Speak Freely" is a fundamentally flawed paper.&lt;/p&gt;
        &lt;p&gt;I hadn't seen dottxt's "Say What You Mean" response to the "Let Me Speak Freely" paper before, and their critique of the "Let Me Speak Freely" paper's methodology is entirely valid.&lt;/p&gt;
        &lt;p&gt;I still believe that combining reasoning with structured outputs can really mess with your final response quality, because putting reasoning fields in JSON forces the model to escape tokens, which as Aider documented last year, causes models to perform substantially worse. Sure, if your reasoning doesn't need to escape text, then this isn't an issue, but now this is another thing that everyone on your team has to remember while they're iterating on prompts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks all for challenging us on this post: it's super valuable feedback, and we really appreciate the time y'all are taking to read and respond to us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345333</guid><pubDate>Sun, 21 Dec 2025 15:06:46 +0000</pubDate></item><item><title>ARIN Public Incident Report – 4.10 Misissuance Error</title><link>https://www.arin.net/announcements/20251212/</link><description>&lt;doc fingerprint="f00af2df20eded25"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Public Incident Report â 4.10 Issuance Error&lt;/head&gt;
    &lt;p&gt;Posted: Friday, 12 December 2025 &lt;lb/&gt; ARIN &lt;/p&gt;
    &lt;head rend="h2"&gt;Executive Summary&lt;/head&gt;
    &lt;p&gt;On 2 December 2025, an IPv4 block 23.150.164.0/24, correctly allocated to the Original Customer, was inadvertently removed and reissued to the Requesting Customer during a 4.10 allocation process. This error stemmed from the current manual and partially offline 4.10 inventory process.&lt;/p&gt;
    &lt;p&gt;The incorrect state persisted until 9 December 2025, when the Original Customer reported the issue. ARIN restored the 23.150.164.0/24 to the Original Customer, issued a replacement /24 to the Requesting Customer, coordinated withdrawal of the incorrect route announcement, and notified the affected parties.&lt;/p&gt;
    &lt;p&gt;This incident highlights known weaknesses in ARINâs current Internet Number Resources (INR) Inventory handling for 4.10 transition space and underscores the need to complete the transition to a fully automated, integrated online inventory architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incident Description&lt;/head&gt;
    &lt;p&gt;Following the current allocation process for 4.10 space, an RSD analyst:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Relied on legacy/manual 4.10 inventory artifacts, including a flat file and sparse allocation spreadsheet&lt;/item&gt;
      &lt;item&gt;Did not recognize indicators in ARIN Online showing that 23.150.164.0/24 was already allocated to the Original Customer&lt;/item&gt;
      &lt;item&gt;Removed 23.150.164.0/24 from the Original Customer&lt;/item&gt;
      &lt;item&gt;Reissued that same /24 to the Requesting Customer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a result, the registration record and associated ROAs for the Original Customer were deleted in error, and the /24 appeared as allocated to the Requesting Customer in ARINâs systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Customer Impact and Risk&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The /24 was removed from the Original Customer account and assigned to another organization.&lt;/item&gt;
      &lt;item&gt;The ROA associated with the block was removed and had to be recreated after restoration.&lt;/item&gt;
      &lt;item&gt;The block was announced by a third-party provider under the incorrect registration, introducing risk of routing conflict and confusion.&lt;/item&gt;
      &lt;item&gt;The incorrect state persisted for approximately seven days before detection.&lt;/item&gt;
      &lt;item&gt;The Original Customer reported the issue via Ask ARIN and a Help Desk call on 9 December 2025.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The customer has not provided a technical impact statement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline of Events&lt;/head&gt;
    &lt;p&gt;(All event times are ET â Eastern Time)&lt;/p&gt;
    &lt;head rend="h3"&gt;25â26 November 2025&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;25 November, 11:59 AM â 4.10 space request received from the Requesting Customer.&lt;/item&gt;
      &lt;item&gt;26 November, 6:25 AM â Ticket assigned to an RSD Analyst for processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2 December 2025 â Incident Occurs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;12:10 PM â Ticket approved for issuance of 4.10 space by designated RSD Analyst.&lt;/item&gt;
      &lt;item&gt;~12:10â12:30 PM â In the process of fulfilling the 4.10 request, the designated Analyst: &lt;list rend="ul"&gt;&lt;item&gt;Opened the e-black-book (an offline Excel-based inventory file, separate from the primary online inventory system), reviewed the existing 4.10 allocations, and selected 23.150.164.0 as the next available sparse entry.&lt;/item&gt;&lt;item&gt;Returned to the ARIN Online management application and queried for 23.150.164.0 based on the entry identified in the e-black-book. At this time the analyst did not recognize that the /24 was already allocated to the Original Customer.&lt;/item&gt;&lt;item&gt;Performed a block split and deleted 23.150.164.0/24 â not recognizing that it was allocated to the Original Customer â which removed associated registry services (ROAs, reverse DNS, etc.).&lt;/item&gt;&lt;item&gt;Issued 23.150.164.0/24 to the Requesting Customer.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;2â9 December 2025 â Incorrect State Persists&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The /24 remained misregistered.&lt;/item&gt;
      &lt;item&gt;The Requesting Customer upstream provider announced the block.&lt;/item&gt;
      &lt;item&gt;No automated detection of the error occurred.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;9 December 2025 â Detection and Resolution&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10:12 AM â The Original Customer submitted an Ask ARIN ticket regarding the problem.&lt;/item&gt;
      &lt;item&gt;10:14 AM â The Original Customer contacted the Help Desk; escalation to Director at 10:20 AM.&lt;/item&gt;
      &lt;item&gt;10:20â10:30 AM â Director reviewed block history and directed corrective actions.&lt;/item&gt;
      &lt;item&gt;10:30 AM â Director and CXO approved: &lt;list rend="ul"&gt;&lt;item&gt;Removal of the /24 from the Requesting Customer&lt;/item&gt;&lt;item&gt;Issuance of a replacement /24 to the Requesting Customer&lt;/item&gt;&lt;item&gt;Restoration of 23.150.164.0/24 to the Original Customer&lt;/item&gt;&lt;item&gt;Coordination of route withdrawal&lt;/item&gt;&lt;item&gt;Update of inaccurate POC information&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;10:44 AM â First notification email sent to the Requesting Customer.&lt;/item&gt;
      &lt;item&gt;10:54 AM â Second email sent noting invalid phone contact.&lt;/item&gt;
      &lt;item&gt;12:01 PM â Corrective actions completed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Root Cause&lt;/head&gt;
    &lt;p&gt;A manual 4.10 workflow that relies on a combination of online systems and offline flat files/spreadsheets for inventory management allowed a current customer allocation to be mistakenly identified as available for issuance. This reliance on offline spreadsheets is a legacy constraint where post-runout 4.10 inventory is maintained outside the primary online system to keep it reserved. The lack of a unified view of inventory and related business-rule-driven system controls enabled the error to proceed without detection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing Factors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hybrid inventory architecture (online + offline) for 4.10 space.&lt;/item&gt;
      &lt;item&gt;Sparse allocation methods implemented through manual tools rather than integrated system logic.&lt;/item&gt;
      &lt;item&gt;Generic warning messages that are not routing aware or business-rule driven.&lt;/item&gt;
      &lt;item&gt;High demand on analysts to catch procedural errors in a manual “swivel chair” workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Mitigation Plan and Next Steps&lt;/head&gt;
    &lt;head rend="h3"&gt;Immediate / Near-Term Controls (Completed)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updated Process Controls (completed) &lt;list rend="ul"&gt;&lt;item&gt;RSD has implemented additional process controls that require a dual review for all ticketing type workflows that include a network delete.&lt;/item&gt;&lt;item&gt;Only a limited set of experienced analysts are permitted to perform this function.&lt;/item&gt;&lt;item&gt;Reviews and approvals are performed at set times each day with a second reviewer involved for any ticket that includes a delete step.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Updated 4.10 Issuing Playbook &lt;list rend="ul"&gt;&lt;item&gt;Document and enforce a revised playbook for issuing 4.10 (with checklists) that includes: &lt;list rend="ul"&gt;&lt;item&gt;Required checks for existing allocations and ROAs&lt;/item&gt;&lt;item&gt;Explicit verification steps prior to any delete/reissue action&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Document and enforce a revised playbook for issuing 4.10 (with checklists) that includes: &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;System and Architecture Improvements (Medium-Term)&lt;/head&gt;
    &lt;p&gt;Accelerate the ongoing INR Inventory Management Roadmap item: This incident reinforces the urgency of the architecture work already underway (reviewed Oct 2025) to move legacy offline inventories into a modern, online architecture. Specific alignment actions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stronger business ruleâbased warnings and controls &lt;list rend="ul"&gt;&lt;item&gt;Enhance warning logic when issuing or modifying 4.10 space to include: &lt;list rend="ul"&gt;&lt;item&gt;Clear alerts if the /24 is already allocated to an Org&lt;/item&gt;&lt;item&gt;Clear alerts if active ROAs exist for the exact block or covering prefix&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Implement system controls for resource types and staff roles, with flags and audit trails for review and auditing.&lt;/item&gt;&lt;item&gt;Replace generic, nonâROA-aware warnings that are easily treated as noise.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Enhance warning logic when issuing or modifying 4.10 space to include: &lt;/item&gt;
      &lt;item&gt;Continue Engineering solution for offline inventory &lt;list rend="ul"&gt;&lt;item&gt;Move offline 4.10 and microallocation inventories, and the viip file for IPv6, into the integrated online inventory architecture.&lt;/item&gt;&lt;item&gt;Eliminate reliance on separate spreadsheets and flat files for production issuing.&lt;/item&gt;&lt;item&gt;Implement business-rule-driven warnings for existing allocations and ROAs&lt;/item&gt;&lt;item&gt;Introduce role-based controls, flags, and audit trails&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Advance the âUpdated Resource Status Taxonomyâ work &lt;list rend="ul"&gt;&lt;item&gt;Ensure 4.10 status and history are fully visible and consistent inside the primary system.&lt;/item&gt;&lt;item&gt;Provide analysts with a clear, unified view of current holder, status, and ROA/IRR context.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Fast-track automation of all inventory issuing &lt;list rend="ul"&gt;&lt;item&gt;Reduce or eliminate manual issuing where possible, with priority for higher-risk categories such as 4.10 and RPKI-covered space.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regards,&lt;/p&gt;
    &lt;p&gt;American Registry for Internet Numbers (ARIN)&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent Announcements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Volunteer to Serve on the 2026 ARIN Fellowship Selection Committee&lt;/item&gt;
      &lt;item&gt;Sponsorship Opportunities Available for 2026 ARIN Public Policy and Members Meetings&lt;/item&gt;
      &lt;item&gt;Public Incident Report â 4.10 Issuance Error&lt;/item&gt;
      &lt;item&gt;ARIN Academy Adds IPv6 Planning Course&lt;/item&gt;
      &lt;item&gt;Reclassification of Inactive General Members Completed 19 November 2025&lt;/item&gt;
      &lt;item&gt;ARIN 56 Meeting Report Now Available&lt;/item&gt;
      &lt;item&gt;Concluding the Second Consultation on the Draft RIR Governance Document&lt;/item&gt;
      &lt;item&gt;2025 ARIN Election Results&lt;/item&gt;
      &lt;item&gt;Public Incident Report â ARIN Hosted RPKI Service&lt;/item&gt;
      &lt;item&gt;IPv4 Waiting List Distribution&lt;/item&gt;
      &lt;item&gt;» View Archive&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345444</guid><pubDate>Sun, 21 Dec 2025 15:19:41 +0000</pubDate></item><item><title>CO2 batteries that store grid energy take off globally</title><link>https://spectrum.ieee.org/co2-battery-energy-storage</link><description>&lt;doc fingerprint="267d315709b6a538"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grid-Scale Bubble Batteries Will Soon Be Everywhere&lt;/head&gt;
    &lt;p&gt;When the sun sets on solar panels, these gas-filled domes take over&lt;/p&gt;
    &lt;p&gt;This giant bubble on the island of Sardinia holds 2,000 tonnes of carbon dioxide. But the gas wasn’t captured from factory emissions, nor was it pulled from the air. It came from a gas supplier, and it lives permanently inside the dome’s system to serve an eco-friendly purpose: to store large amounts of excess renewable energy until it’s needed.&lt;/p&gt;
    &lt;p&gt;Developed by the Milan-based company Energy Dome, the bubble and its surrounding machinery demonstrate a first-of-its-kind “CO2 Battery,” as the company calls it. The facility compresses and expands CO2 daily in its closed system, turning a turbine that generates 200 megawatt-hours of electricity, or 20 MW over 10 hours. And in 2026, replicas of this plant will start popping up across the globe.&lt;/p&gt;
    &lt;p&gt;We mean that literally. It takes just half a day to inflate the bubble. The rest of the facility takes less than two years to build and can be done just about anywhere there’s 5 hectares of flat land.&lt;/p&gt;
    &lt;p&gt;The first to build one outside of Sardinia will be one of India’s largest power companies, NTPC Limited. The company expects to complete its CO2 Battery sometime in 2026 at the Kudgi power plant in Karnataka, in India. In Wisconsin, meanwhile, the public utility Alliant Energy received the all clear from authorities to begin construction of one in 2026 to supply power to 18,000 homes.&lt;/p&gt;
    &lt;p&gt;And Google likes the concept so much that it plans to rapidly deploy the facilities in all of its key data-center locations in Europe, the United States, and the Asia-Pacific region. The idea is to provide electricity-guzzling data centers with round-the-clock clean energy, even when the sun isn’t shining or the wind isn’t blowing. The partnership with Energy Dome, announced in July, marked Google’s first investment in long-duration energy storage.&lt;/p&gt;
    &lt;p&gt;“We’ve been scanning the globe seeking different solutions,” says Ainhoa Anda, Google’s senior lead for energy strategy, in Paris. The challenge the tech giant has encountered is not only finding a long-duration storage option, but also one that works with the unique specs of every region. “So standardization is really important, and this is one of the aspects that we really like” about Energy Dome, she says. “They can really plug and play this.”&lt;/p&gt;
    &lt;p&gt;Google will prioritize placing the Energy Dome facilities where they’ll have the most impact on decarbonization and grid reliability, and where there’s a lot of renewable energy to store, Anda says. The facilities can be placed adjacent to Google’s data centers or elsewhere within the same grid. The companies did not disclose the terms of the deal.&lt;/p&gt;
    &lt;p&gt;Anda says Google expects to help the technology “reach a massive commercial stage.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting creative with long-duration energy storage&lt;/head&gt;
    &lt;p&gt;All this excitement is based on Energy Dome’s one full-size, grid-connected plant in Ottana, Sardinia, which was completed in July. It was built to help solve one of the energy transition’s biggest challenges: the need for grid-scale storage that can provide power for more than 8 hours at a time. Called long-duration energy storage, or LDES in industry parlance, the concept is the key to maximizing the value of renewable energy.&lt;/p&gt;
    &lt;p&gt;When sun and wind are abundant, solar and wind farms tend to produce more electricity than a grid needs. So storing the excess for use when these resources are scarce just makes sense. LDES also makes the grid more reliable by providing backup and supplementary power.&lt;/p&gt;
    &lt;p&gt;The problem is that even the best new grid-scale storage systems on the market—mainly lithium-ion batteries—provide only about 4 to 8 hours of storage. That’s not long enough to power through a whole night, or multiple cloudy and windless days, or the hottest week of the year, when energy demand hits its peak.&lt;/p&gt;
    &lt;p&gt;After the CO2 leaves the dome, it is compressed, cooled, reduced to a liquid, and stored in pressure vessels. To release the energy, the process reverses: The liquid is evaporated, heated, expanded, and then fed through a turbine that generates electricity. Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;Lithium-ion battery systems could be increased in size to store more and last longer, but systems of that size usually aren’t economically viable. Other grid-scale battery chemistries and approaches are in development, such as sodium-based, iron-air, and vanadium redox flow batteries. But the energy density, costs, degradation, and funding complications have challenged the developers of those alternatives.&lt;/p&gt;
    &lt;p&gt;Researchers have also experimented with storing energy by compressing air, heating up blocks or sand, using hydrogen or methanol, pressurizing water deep underground, and even dangling heavy objects in the air and dropping them. (The creativity devoted to LDES is impressive.) But geologic constraints, economic viability, efficiency, and scalability have hindered the commercialization of these strategies.&lt;/p&gt;
    &lt;p&gt;The tried-and-true grid-scale storage option—pumped hydro, in which water is pumped between reservoirs at different elevations—lasts for decades and can store thousands of megawatts for days. But these systems require specific topography, a lot of land, and can take up to a decade to build.&lt;/p&gt;
    &lt;p&gt;CO2 Batteries check a lot of boxes that other approaches don’t. They don’t need special topography like pumped-hydro reservoirs do. They don’t need critical minerals like electrochemical and other batteries do. They use components for which supply chains already exist. Their expected lifetime stretches nearly three times as long as lithium-ion batteries. And adding size and storage capacity to them significantly decreases cost per kilowatt-hour. Energy Dome expects its LDES solution to be 30 percent cheaper than lithium-ion.&lt;/p&gt;
    &lt;p&gt;China has taken note. China Huadian Corp. and Dongfang Electric Corp. are reportedly building a CO2-based energy-storage facility in the Xinjiang region of northwest China. Media reports show renderings of domes but give widely varying storage capacities—including 100 MW and 1,000 MW. The Chinese companies did not respond to IEEE Spectrum’s requests for information.&lt;/p&gt;
    &lt;p&gt;“What I can say is that they are developing something very, very similar [to Energy Dome’s CO2 Battery] but quite large in scale,” says Claudio Spadacini, Energy Dome’s founder and CEO. The Chinese companies “are good, they are super fast, and they have a lot of money,” he says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is Google investing in CO2 Batteries?&lt;/head&gt;
    &lt;p&gt;When I visited Energy Dome’s Sardinia facility in October, the CO2 had just been pumped out of the dome, so I was able to peek inside. It was massive, monochromatic, and pretty much empty. The inner membrane, which had been holding the uncompressed CO2, had collapsed across the entire floor. A few pockets of the gas remained, making the off-white sheet billow up in spots.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the translucent outer dome allowed some daylight to pass through, creating a creamy glow that enveloped the vast space. With no structural framing, the only thing keeping the dome upright was the small difference in pressure between the inside and outside air.&lt;/p&gt;
    &lt;p&gt;“This is incredible,” I said to my guide, Mario Torchio, Energy Dome’s global marketing and communications director.&lt;/p&gt;
    &lt;p&gt;“It is. But it’s physics,” he said.&lt;/p&gt;
    &lt;p&gt;Outside the dome, a series of machines connected by undulating pipes moves the CO2 out of the dome for compressing and condensing. First, a compressor pressurizes the gas from 1 bar (100,000 pascals) to about 55 bar (5,500,000 pa). Next, a thermal-energy-storage system cools the CO2 to an ambient temperature. Then a condenser reduces it into a liquid that is stored in a few dozen pressure vessels, each about the size of a school bus. The whole process takes about 10 hours, and at the end of it, the battery is considered charged.&lt;/p&gt;
    &lt;p&gt;To discharge the battery, the process reverses. The liquid CO2 is evaporated and heated. It then enters a gas-expander turbine, which is like a medium-pressure steam turbine. This drives a synchronous generator, which converts mechanical energy into electrical energy for the grid. After that, the gas is exhausted at ambient pressure back into the dome, filling it up to await the next charging phase.&lt;/p&gt;
    &lt;p&gt;Energy Dome engineers inspect the dryer system, which keeps the gaseous CO₂ in the dome at optimal dryness levels at all times.Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;It’s not rocket science. Still, someone had to be the first to put it together and figure out how to do it cost-effectively, which Spadacini says his company has accomplished and patented. “How we seal the turbo machinery, how we store the heat in the thermal-energy storage, how we store the heat after condensing…can really cut costs and increase the efficiency,” he says.&lt;/p&gt;
    &lt;p&gt;The company uses pure, purpose-made CO2 instead of sourcing it from emissions or the air, because those sources come with impurities and moisture that degrade the steel in the machinery.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happens if the dome is punctured?&lt;/head&gt;
    &lt;p&gt;On the downside, Energy Dome’s facility takes up about twice as much land as a comparable capacity lithium-ion battery would. And the domes themselves, which are about the height of a sports stadium at their apex, and longer, might stand out on a landscape and draw some NIMBY pushback.&lt;/p&gt;
    &lt;p&gt;And what if a tornado comes? Spadacini says the dome can withstand wind up to 160 kilometers per hour. If Energy Dome can get half a day’s warning of severe weather, the company can just compress and store the CO2 in the tanks and then deflate the outer dome, he says.&lt;/p&gt;
    &lt;p&gt;If the worst happens and the dome is punctured, 2,000 tonnes of CO2 will enter the atmosphere. That’s equivalent to the emissions of about 15 round-trip flights between New York and London on a Boeing 777. “It’s negligible compared to the emissions of a coal plant,” Spadacini says. People will also need to stay back 70 meters or more until the air clears, he says.&lt;/p&gt;
    &lt;p&gt;Worth the risk? The companies lining up to build these systems seem to think so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grid-Scale Battery Stabilizes Scottish Power Supply ›&lt;/item&gt;
      &lt;item&gt;Backing Up the Power Grid With Green Methanol ›&lt;/item&gt;
      &lt;item&gt;DOE Places Compressed-Air Energy Storage Loan Under Review ›&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345506</guid><pubDate>Sun, 21 Dec 2025 15:27:36 +0000</pubDate></item><item><title>E.W.Dijkstra Archive</title><link>https://www.cs.utexas.edu/~EWD/welcome.html</link><description>&lt;doc fingerprint="e615734b00f3a2b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Edsger Wybe Dijkstra was one of the most influential members of computing science’s founding generation. Among the domains in which his scientific contributions are fundamental are&lt;/p&gt;
    &lt;p&gt;algorithm design&lt;/p&gt;
    &lt;p&gt;programming languages&lt;/p&gt;
    &lt;p&gt;program design&lt;/p&gt;
    &lt;p&gt;operating systems&lt;/p&gt;
    &lt;p&gt;distributed processing&lt;/p&gt;
    &lt;p&gt;formal specification and verification&lt;/p&gt;
    &lt;p&gt;design of mathematical arguments&lt;/p&gt;
    &lt;p&gt;In addition, Dijkstra was intensely interested in teaching, and in the relationships between academic computing science and the software industry.&lt;/p&gt;
    &lt;p&gt;During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra’s contributions brought him many prizes and awards, including computing science’s highest honor, the ACM Turing Award.&lt;/p&gt;
    &lt;p&gt;The Manuscripts&lt;/p&gt;
    &lt;p&gt;Like most of us, Dijkstra always believed it a scientist’s duty to maintain a lively correspondence with his scientific colleagues. To a greater extent than most of us, he put that conviction into practice. For over four decades, he mailed copies of his consecutively numbered technical notes, trip reports, insightful observations, and pungent commentaries, known collectively as “EWDs”, to several dozen recipients in academia and industry. Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra’s writings, the informal circulation of many of the EWDs eventually reached into the thousands.&lt;/p&gt;
    &lt;p&gt;Although most of Dijkstra’s publications began life as EWD manuscripts, the great majority of his manuscripts remain unpublished. They have been inaccessible to many potential readers, and those who have received copies have been unable to cite them in their own work. To alleviate both of these problems, the department has collected over a thousand of the manuscripts in this permanent web site, in the form of PDF bitmap documents (to read them, you’ll need a copy of Acrobat Reader). We hope you will find it convenient, useful, inspiring, and enjoyable.&lt;/p&gt;
    &lt;p&gt;The original manuscripts, along with diaries, correspondence, photographs, and other papers, are housed at The Center for American History of The University of Texas at Austin.&lt;/p&gt;
    &lt;p&gt;Indexes&lt;/p&gt;
    &lt;p&gt;Each manuscript file is accessible through either of two indexes:&lt;/p&gt;
    &lt;p&gt;0. BibTeX index. Each entry includes all the available bibliographic data.&lt;/p&gt;
    &lt;p&gt;1. Ad-hoc indexes. These contain titles only, but are faster if you know what you’re looking for.&lt;/p&gt;
    &lt;p&gt;EWD-numbered documents(This index gives an approximate correspondence between manuscripts’ EWD numbers and the year in which they appeared.)&lt;/p&gt;
    &lt;p&gt;Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica)&lt;/p&gt;
    &lt;p&gt;You can find a table relating EWD numbers to publication years here.&lt;/p&gt;
    &lt;p&gt;Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers.&lt;/p&gt;
    &lt;p&gt;Transcripts and translations&lt;/p&gt;
    &lt;p&gt;A growing number of the PDF bitmap documents have been transcribed to make them searchable and accessible to visitors who are visually impaired.&lt;/p&gt;
    &lt;p&gt;A few of the manuscripts written in Dutch have been translated into English, and one —EWD1036— has been translated into Spanish. EWD28 has been translated from English into Russian.&lt;/p&gt;
    &lt;p&gt;For these transcriptions and translations we are grateful to over sixty contributors. Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations).&lt;/p&gt;
    &lt;p&gt;Proofreading Each transcription gets a cursory scan as it’s prepared for uploading, but since a web page can always be updated, I don’t strive for (unattainable) perfection before installing it. On the web, proofreading is a game that can be played by every reader; if you spot an error, please&lt;/p&gt;
    &lt;p&gt;Links between EWDs&lt;/p&gt;
    &lt;p&gt;A compilation of cross-references has been contributed by Diethard Michaelis. As its author notes, the collection is incomplete, and all readers are invited to add to it.&lt;/p&gt;
    &lt;p&gt;Dijkstra often returned to topics about which he had already written, when he had something new to say or even just a better way of saying it. When Dijkstra himself didn’t provide the backward references, we indicate the relationship by "see also" links in the index, leaving the judgment of the extent to which the earlier EWD is superseded by the later one to the reader. Any reader who notices such a relationship is invited to&lt;/p&gt;
    &lt;p&gt;Summaries&lt;/p&gt;
    &lt;p&gt;We have begun adding summaries of the EWDs. This innovation was suggested by Günter Rote, who contributed the first dozen summaries. Additional contributions of summaries—especially summaries in English of EWDs in Dutch—are most welcome.&lt;/p&gt;
    &lt;p&gt;Copyrights&lt;/p&gt;
    &lt;p&gt;Copyrights in most EWDs are held by his children, one of whom — — handles requests for permission to publish reproductions. The exceptions are documents that were published, and whose copyrights are held by their publishers; those documents are listed here, and each one is provided with a cover page identifying the copyright holder.&lt;/p&gt;
    &lt;p&gt;Because the original manuscripts are in possession of the Briscoe Center for American History at The University of Texas, the Center’s policies are also applicable.&lt;/p&gt;
    &lt;p&gt;Video and audio&lt;/p&gt;
    &lt;p&gt;In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews.&lt;/p&gt;
    &lt;p&gt;About Dijkstra and his work&lt;/p&gt;
    &lt;p&gt;An interview with Dijkstra (Spanish translation here) was conducted in 1985 by Rogier F. van Vlissingen, who has also written a personal reflection on “Dijkstra’s sense of what computer science and programming are and what they aren’t.”&lt;/p&gt;
    &lt;p&gt;Another interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute.&lt;/p&gt;
    &lt;p&gt;To mark the occasion of Dijkstra’s retirement in November 1999 from the Schlumberger Centennial Chair in Computer Sciences, which he had occupied since 1984, and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, In Pursuit of Simplicity, which took place on his birthday in May 2000. The symposium’s program (10 MB) contains an outline of Dijkstra’s career, as well as a collection of quotes culled from his writings, from his blackboard, and from what others have said about him. Banquet speeches by David Gries, Fred Schneider, Krzysztof Apt, W.M. Turski, and H. Richards were recorded on a video.&lt;/p&gt;
    &lt;p&gt;Dijkstra’s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department’s memorial celebration.&lt;/p&gt;
    &lt;p&gt;A remembrance of Dijkstra was posted in May 2008 by Maarten van Emden (thanks to Tristram Brelstaff for noting it).&lt;/p&gt;
    &lt;p&gt;In 2021 Krzysztof R. Apt and Tony Hoare edited a commemoration of Edsger Dijkstra written by more than twenty computer scientists who knew him as a colleague, teacher, and friend.&lt;/p&gt;
    &lt;p&gt;A blog devoted to Dijkstra’s works and thoughts has been created, and is being maintained, by the historian of computing Edgar G. Daylight. An article by Daylight, “Dijkstra’s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,” appeared in The Computer Journal, March 2011.&lt;/p&gt;
    &lt;p&gt;In his blog A Programmer’s Place, Maarten van Emden has an entry entitled “Another scoop by Dijkstra?”. The entry describes Dijkstra’s “remarkable insight [in “Notes on Structured Programming” (EWD 249)] that resolves the stand-off between the Sieve of Eratosthenes (efficient in terms of time, but not memory) and the method of Trial Division (efficient in terms of memory, but not time)” by applying the Assembly-line Principle.&lt;/p&gt;
    &lt;p&gt;The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstra’s “foundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.”&lt;/p&gt;
    &lt;p&gt;The Dijkstra Memorial Lectures&lt;/p&gt;
    &lt;p&gt;A series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010.&lt;/p&gt;
    &lt;p&gt;About this site&lt;/p&gt;
    &lt;p&gt;Recent significant changes in the site are listed here; the most recent change was posted on 30 March 2021.&lt;/p&gt;
    &lt;p&gt;The folks who contributed most significantly to the site’s creation are acknowledged here.&lt;/p&gt;
    &lt;p&gt;Comments and suggestions about the site are always welcome; please email them to the&lt;/p&gt;
    &lt;p&gt;Related site&lt;/p&gt;
    &lt;p&gt;If you find this site interesting, you may also be interested in another site:&lt;/p&gt;
    &lt;p&gt;Discipline in Thought which is a website dedicated to disciplined thinking, calculational mathematics, and mathematical methodology. The members of this site are markedly influenced by the works of EWD, and the material shared through the website continues in the traditions set by EWD (among others).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345523</guid><pubDate>Sun, 21 Dec 2025 15:29:31 +0000</pubDate></item><item><title>Show HN: WalletWallet – create Apple passes from anything</title><link>https://walletwallet.alen.ro/</link><description>&lt;doc fingerprint="46fc1f19d956373"&gt;
  &lt;main&gt;
    &lt;p&gt; Totally free &lt;/p&gt;
    &lt;head rend="h1"&gt;WalletWallet&lt;/head&gt;
    &lt;p&gt;A simple utility to convert physical barcodes into digital passes for Apple Wallet®. Entirely free and runs directly from your browser.&lt;/p&gt;
    &lt;p&gt; 1 Enter your membership or loyalty card barcode data. &lt;/p&gt;
    &lt;p&gt; 2 Configure the appearance and titles for your pass. &lt;/p&gt;
    &lt;p&gt; 3 Download and open the file to add it to your Wallet. &lt;/p&gt;
    &lt;p&gt; No Sign-up &lt;/p&gt;
    &lt;p&gt; Private &lt;/p&gt;
    &lt;p&gt; No Install &lt;/p&gt;
    &lt;p&gt; Pass Configuration &lt;/p&gt;
    &lt;p&gt;Generates a standard .pkpass file&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345745</guid><pubDate>Sun, 21 Dec 2025 16:04:05 +0000</pubDate></item><item><title>Show HN: Books mentioned on Hacker News in 2025</title><link>https://hackernews-readings-613604506318.us-west1.run.app</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345897</guid><pubDate>Sun, 21 Dec 2025 16:21:04 +0000</pubDate></item><item><title>Autoland Saves King Air, Everyone Reported Safe</title><link>https://avbrief.com/autoland-saves-king-air-everyone-reported-safe/</link><description>&lt;doc fingerprint="f93e2be520c9cf49"&gt;
  &lt;main&gt;
    &lt;p&gt;Garmin has confirmed the first emergency use of its Autoland system occurred on Saturday in Colorado. “Garmin can confirm that an emergency Autoland activation occurred at Rocky Mountain Metropolitan Airport in Broomfield, Colorado,” the company said in a statement Sunday. “The Autoland took place on Sat., Dec. 20, resulting in a successful landing. We look forward to sharing additional details at the appropriate time.” Social media posts from flight tracking hobbyists reported a King Air 200 squawked 7700 about 2 p.m. local time today. The Autoland system was initiated and landed the aircraft at Rocky Mountain Metropolitan Airport near Denver. A recording from LiveATC’s feed of the airport’s tower frequency includes a robotic female voice declaring a pilot incapacitation and the intention to land on Runway 30. The tape is below and first mention of the incident by ATC is at about 5:00. The Autoland system announces its intentions at about 11:10. (The time stamps are approximate.) There is no word on the condition of the pilot but social media posts suggest all aboard were safe.&lt;/p&gt;
    &lt;p&gt;The aircraft, N479BR, was being operated by Buffalo River Outfitters from Aspen to Rocky Mountain Metropolitan. It’s not clear how many people were on board. The system appeared to work flawlessly, and the controller at Rocky Mountain Metropolitan seemed to take it in stride, accommodating as many requests as he could before shutting down the airport for the landing. We’ll have more detail on this as it becomes available.&lt;/p&gt;
    &lt;p&gt;Larry Anglisano recorded this video demonstration of the Autoland system in the Beechcraft King Air.&lt;/p&gt;
    &lt;p&gt;A reader was at the airport Saturday and shared this video that he had posted to Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46346214</guid><pubDate>Sun, 21 Dec 2025 16:57:57 +0000</pubDate></item><item><title>Get an AI code review in 10 seconds</title><link>https://oldmanrahul.com/2025/12/19/ai-code-review-trick/</link><description>Get an AI code review in 10 seconds December 19, 2025

Here’s a trick I don’t see enough people using:

Add .diff to the end of any PR URL and copy&amp;paste into a LLM

You can get an instant feedback on any GitHub PR.

No Copilot Enterprise. No browser extensions. No special tooling.

That’s it.

Example

PR Link: https://github.com/RahulPrabha/oldmanrahul.com/pull/11 Add .diff to the end: https://github.com/RahulPrabha/oldmanrahul.com/pull/11.diff Copy the raw diff Paste it into Claude, ChatGPT, or any LLM (Maybe add a short instuction like: please review. )

So no more human reviewers?

This isn’t a replacement for a real code review by a peer. But it’s a great way to get a first pass in short order.

Before you ping a teammate, run your PR through an LLM. You’ll catch obvious issues, get suggestions for edge cases you missed, and show up to the real review with cleaner code.

It’ll shorten your cycle times and be a courtesy to others.</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46346391</guid><pubDate>Sun, 21 Dec 2025 17:21:05 +0000</pubDate></item><item><title>Logging Sucks</title><link>https://loggingsucks.com/</link><description>&lt;doc fingerprint="665cb3815fe18e2c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Logging sucks.&lt;/head&gt;&lt;p&gt;And here's how to make it better.&lt;/p&gt;&lt;p&gt;Your logs are lying to you. Not maliciously. They're just not equipped to tell the truth.&lt;/p&gt;&lt;p&gt;You've probably spent hours grep-ing through logs trying to understand why a user couldn't check out, why that webhook failed, or why your p99 latency spiked at 3am. You found nothing useful. Just timestamps and vague messages that mock you with their uselessness.&lt;/p&gt;&lt;p&gt;This isn't your fault. Logging, as it's commonly practiced, is fundamentally broken. And no, slapping OpenTelemetry on your codebase won't magically fix it.&lt;/p&gt;&lt;p&gt;Let me show you what's wrong, and more importantly, how to fix it.&lt;/p&gt;&lt;head rend="h2"&gt;The Core Problem&lt;/head&gt;&lt;p&gt;Logs were designed for a different era. An era of monoliths, single servers, and problems you could reproduce locally. Today, a single user request might touch 15 services, 3 databases, 2 caches, and a message queue. Your logs are still acting like it's 2005.&lt;/p&gt;&lt;p&gt;Here's what a typical logging setup looks like:&lt;/p&gt;&lt;p&gt;That's 13 log lines for a single successful request. Now multiply that by 10,000 concurrent users. You've got 130,000 log lines per second. Most of them saying absolutely nothing useful.&lt;/p&gt;&lt;p&gt;But here's the real problem: when something goes wrong, these logs won't help you. They're missing the one thing you need: context.&lt;/p&gt;&lt;head rend="h2"&gt;Why String Search is Broken&lt;/head&gt;&lt;p&gt;When a user reports "I can't complete my purchase," your first instinct is to search your logs. You type their email, or maybe their user ID, and hit enter.&lt;/p&gt;&lt;p&gt;String search treats logs as bags of characters. It has no understanding of structure, no concept of relationships, no way to correlate events across services.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;When you search for "user-123", you might find it logged 47 different ways across your codebase: &lt;/item&gt;&lt;item&gt;&lt;code&gt;user-123&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;user_id=user-123&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;{"userId": "user-123"}&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;[USER:user-123]&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;processing user: user-123&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;And those are just the logs that include the user ID. What about the downstream service that only logged the order ID? Now you need a second search. And a third. You're playing detective with one hand tied behind your back.&lt;/p&gt;&lt;quote&gt;The fundamental problem: logs are optimized for writing, not for querying.&lt;/quote&gt;&lt;p&gt;Developers write &lt;code&gt;console.log("Payment failed")&lt;/code&gt; because it's easy in the moment. Nobody thinks about the poor soul who'll be searching for this at 2am during an outage.&lt;/p&gt;&lt;head rend="h2"&gt;Let's Define Some Terms&lt;/head&gt;&lt;p&gt;Before I show you the fix, let me define some terms. These get thrown around a lot, often incorrectly.&lt;/p&gt;&lt;p&gt;Structured Logging: Logs emitted as key-value pairs (usually JSON) instead of plain strings. &lt;code&gt;{"event": "payment_failed", "user_id": "123"}&lt;/code&gt; instead of &lt;code&gt;"Payment failed for user 123"&lt;/code&gt;. Structured logging is necessary but not sufficient.&lt;/p&gt;&lt;p&gt;Cardinality: The number of unique values a field can have. &lt;code&gt;user_id&lt;/code&gt; has high cardinality (millions of unique values). &lt;code&gt;http_method&lt;/code&gt; has low cardinality (GET, POST, PUT, DELETE, etc.). High cardinality fields are what make logs actually useful for debugging.&lt;/p&gt;&lt;p&gt;Dimensionality: The number of fields in your log event. A log with 5 fields has low dimensionality. A log with 50 fields has high dimensionality. More dimensions = more questions you can answer.&lt;/p&gt;&lt;p&gt;Wide Event: A single, context-rich log event emitted per request per service. Instead of 13 log lines for one request, you emit 1 line with 50+ fields containing everything you might need to debug.&lt;/p&gt;&lt;p&gt;Canonical Log Line: Another term for wide event, popularized by Stripe. One log line per request that serves as the authoritative record of what happened.&lt;/p&gt;&lt;head rend="h2"&gt;OpenTelemetry Won't Save You&lt;/head&gt;&lt;p&gt;I see this take constantly: "Just use OpenTelemetry and your observability problems are solved."&lt;/p&gt;&lt;p&gt;No. OpenTelemetry is a protocol and a set of SDKs. It standardizes how telemetry data (logs, traces, metrics) is collected and exported. This is genuinely useful: it means you're not locked into a specific vendor's format.&lt;/p&gt;&lt;p&gt;But here's what OpenTelemetry does NOT do:&lt;/p&gt;&lt;p&gt;1. It doesn't decide what to log. You still have to instrument your code deliberately.&lt;lb/&gt;2. It doesn't add business context. If you don't add the user's subscription tier, their cart value, or the feature flags enabled, OTel won't magically know.&lt;lb/&gt;3. It doesn't fix your mental model. If you're still thinking in terms of "log statements," you'll just emit bad telemetry in a standardized format.&lt;/p&gt;&lt;p&gt;OpenTelemetry is a delivery mechanism. It doesn't know that &lt;code&gt;user-789&lt;/code&gt; is a premium customer who's been with you for 3 years and just tried to spend $160. You have to tell it.&lt;/p&gt;&lt;head rend="h2"&gt;The Fix: Wide Events / Canonical Log Lines&lt;/head&gt;&lt;p&gt;Here's the mental model shift that changes everything:&lt;/p&gt;&lt;quote&gt;Instead of logging what your code is doing, log what happened to this request.&lt;/quote&gt;&lt;p&gt;Stop thinking about logs as a debugging diary. Start thinking about them as a structured record of business events.&lt;/p&gt;&lt;p&gt;For each request, emit one wide event per service hop. This event should contain every piece of context that might be useful for debugging. Not just what went wrong, but the full picture of the request.&lt;/p&gt;&lt;p&gt;Here's what a wide event looks like in practice:&lt;/p&gt;&lt;code&gt;{
  "timestamp": "2025-01-15T10:23:45.612Z",
  "request_id": "req_8bf7ec2d",
  "trace_id": "abc123",

  "service": "checkout-service",
  "version": "2.4.1",
  "deployment_id": "deploy_789",
  "region": "us-east-1",

  "method": "POST",
  "path": "/api/checkout",
  "status_code": 500,
  "duration_ms": 1247,

  "user": {
    "id": "user_456",
    "subscription": "premium",
    "account_age_days": 847,
    "lifetime_value_cents": 284700
  },

  "cart": {
    "id": "cart_xyz",
    "item_count": 3,
    "total_cents": 15999,
    "coupon_applied": "SAVE20"
  },

  "payment": {
    "method": "card",
    "provider": "stripe",
    "latency_ms": 1089,
    "attempt": 3
  },

  "error": {
    "type": "PaymentError",
    "code": "card_declined",
    "message": "Card declined by issuer",
    "retriable": false,
    "stripe_decline_code": "insufficient_funds"
  },

  "feature_flags": {
    "new_checkout_flow": true,
    "express_payment": false
  }
}&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;One event. Everything you need. When this user complains, you search for &lt;/item&gt;&lt;item&gt;They're a premium customer (high priority)&lt;/item&gt;&lt;item&gt;They've been with you for over 2 years (very high priority)&lt;/item&gt;&lt;item&gt;The payment failed on the 3rd attempt&lt;/item&gt;&lt;item&gt;The actual reason: insufficient funds&lt;/item&gt;&lt;item&gt;They were using the new checkout flow (potential correlation?)&lt;/item&gt;&lt;/list&gt;&lt;code&gt;user_id = "user_456"&lt;/code&gt; and you instantly know:
&lt;p&gt;No grep-ing. No guessing. No second search.&lt;/p&gt;&lt;head rend="h2"&gt;The Queries You Can Now Run&lt;/head&gt;&lt;p&gt;With wide events, you're not searching text anymore. You're querying structured data. The difference is night and day.&lt;/p&gt;&lt;p&gt;This is the superpower of wide events combined with high-cardinality, high-dimensionality data. You're not searching logs anymore. You're running analytics on your production traffic.&lt;/p&gt;&lt;head rend="h2"&gt;Implementing Wide Events&lt;/head&gt;&lt;p&gt;Here's a practical implementation pattern. The key insight: build the event throughout the request lifecycle, then emit once at the end.&lt;/p&gt;&lt;code&gt;// middleware/wideEvent.ts
export function wideEventMiddleware() {
  return async (ctx, next) =&amp;gt; {
    const startTime = Date.now();

    // Initialize the wide event with request context
    const event: Record&amp;lt;string, unknown&amp;gt; = {
      request_id: ctx.get('requestId'),
      timestamp: new Date().toISOString(),
      method: ctx.req.method,
      path: ctx.req.path,
      service: process.env.SERVICE_NAME,
      version: process.env.SERVICE_VERSION,
      deployment_id: process.env.DEPLOYMENT_ID,
      region: process.env.REGION,
    };

    // Make the event accessible to handlers
    ctx.set('wideEvent', event);

    try {
      await next();
      event.status_code = ctx.res.status;
      event.outcome = 'success';
    } catch (error) {
      event.status_code = 500;
      event.outcome = 'error';
      event.error = {
        type: error.name,
        message: error.message,
        code: error.code,
        retriable: error.retriable ?? false,
      };
      throw error;
    } finally {
      event.duration_ms = Date.now() - startTime;

      // Emit the wide event
      logger.info(event);
    }
  };
}&lt;/code&gt;
&lt;p&gt;Then in your handlers, you enrich the event with business context:&lt;/p&gt;&lt;code&gt;app.post('/checkout', async (ctx) =&amp;gt; {
  const event = ctx.get('wideEvent');
  const user = ctx.get('user');

  // Add user context
  event.user = {
    id: user.id,
    subscription: user.subscription,
    account_age_days: daysSince(user.createdAt),
    lifetime_value_cents: user.ltv,
  };

  // Add business context as you process
  const cart = await getCart(user.id);
  event.cart = {
    id: cart.id,
    item_count: cart.items.length,
    total_cents: cart.total,
    coupon_applied: cart.coupon?.code,
  };

  // Process payment
  const paymentStart = Date.now();
  const payment = await processPayment(cart, user);

  event.payment = {
    method: payment.method,
    provider: payment.provider,
    latency_ms: Date.now() - paymentStart,
    attempt: payment.attemptNumber,
  };

  // If payment fails, add error details
  if (payment.error) {
    event.error = {
      type: 'PaymentError',
      code: payment.error.code,
      stripe_decline_code: payment.error.declineCode,
    };
  }

  return ctx.json({ orderId: payment.orderId });
});&lt;/code&gt;
&lt;head rend="h2"&gt;Sampling: Keeping Costs Under Control&lt;/head&gt;&lt;p&gt;"But Boris," I hear you saying, "if I log 50 fields per request at 10,000 requests per second, my observability bill will bankrupt me."&lt;/p&gt;&lt;p&gt;Valid concern. This is where sampling comes in.&lt;/p&gt;&lt;p&gt;Sampling means keeping only a percentage of your events. Instead of storing 100% of traffic, you might store 10% or 1%. At scale, this is the only way to stay sane (and solvent).&lt;/p&gt;&lt;p&gt;But naive sampling is dangerous. If you randomly sample 1% of traffic, you might accidentally drop the one request that explains your outage.&lt;/p&gt;&lt;head rend="h3"&gt;Tail Sampling&lt;/head&gt;&lt;p&gt;Tail sampling means you make the sampling decision after the request completes, based on its outcome.&lt;/p&gt;&lt;p&gt;The rules are simple:&lt;lb/&gt;1. Always keep errors. 100% of 500s, exceptions, and failures get stored.&lt;lb/&gt;2. Always keep slow requests. Anything above your p99 latency threshold.&lt;lb/&gt;3. Always keep specific users. VIP customers, internal testing accounts, flagged sessions.&lt;lb/&gt;4. Randomly sample the rest. Happy, fast requests? Keep 1-5%.&lt;/p&gt;&lt;p&gt;This gives you the best of both worlds: manageable costs, but you never lose the events that matter.&lt;/p&gt;&lt;code&gt;// Tail sampling decision function
function shouldSample(event: WideEvent): boolean {
  // Always keep errors
  if (event.status_code &amp;gt;= 500) return true;
  if (event.error) return true;

  // Always keep slow requests (above p99)
  if (event.duration_ms &amp;gt; 2000) return true;

  // Always keep VIP users
  if (event.user?.subscription === 'enterprise') return true;

  // Always keep requests with specific feature flags (debugging rollouts)
  if (event.feature_flags?.new_checkout_flow) return true;

  // Random sample the rest at 5%
  return Math.random() &amp;lt; 0.05;
}&lt;/code&gt;

&lt;head rend="h2"&gt;Misconceptions&lt;/head&gt;&lt;head rend="h3"&gt;"Structured logging is the same as wide events"&lt;/head&gt;&lt;p&gt;No. Structured logging means your logs are JSON instead of strings. That's table stakes. Wide events are a philosophy: one comprehensive event per request, with all context attached. You can have structured logs that are still useless (5 fields, no user context, scattered across 20 log lines).&lt;/p&gt;&lt;head rend="h3"&gt;"We already use OpenTelemetry, so we're good"&lt;/head&gt;&lt;p&gt;You're using a delivery mechanism. OpenTelemetry doesn't decide what to capture. You do. Most OTel implementations I've seen capture the bare minimum: span name, duration, status. That's not enough. You need to deliberately instrument with business context.&lt;/p&gt;&lt;head rend="h3"&gt;"This is just tracing with extra steps"&lt;/head&gt;&lt;p&gt;Tracing gives you request flow across services (which service called which). Wide events give you context within a service. They're complementary. Ideally, your wide events ARE your trace spans, enriched with all the context you need.&lt;/p&gt;&lt;head rend="h3"&gt;"Logs are for debugging, metrics are for dashboards"&lt;/head&gt;&lt;p&gt;This distinction is artificial and harmful. Wide events can power both. Query them for debugging. Aggregate them for dashboards. The data is the same, just different views.&lt;/p&gt;&lt;head rend="h3"&gt;"High-cardinality data is expensive and slow"&lt;/head&gt;&lt;p&gt;It's expensive on legacy logging systems built for low-cardinality string search. Modern columnar databases (ClickHouse, BigQuery, etc.) are specifically designed for high-cardinality, high-dimensionality data. The tooling has caught up. Your practices should too.&lt;/p&gt;&lt;head rend="h2"&gt;The Payoff&lt;/head&gt;&lt;p&gt;When you implement wide events properly, debugging transforms from archaeology to analytics.&lt;/p&gt;&lt;p&gt;Instead of: "The user said checkout failed. Let me grep through 50 services and hope I find something."&lt;/p&gt;&lt;p&gt;You get: "Show me all checkout failures for premium users in the last hour where the new checkout flow was enabled, grouped by error code."&lt;/p&gt;&lt;p&gt;One query. Sub-second results. Root cause identified.&lt;/p&gt;&lt;p&gt;Your logs stop lying to you. They start telling the truth. The whole truth.&lt;/p&gt;&lt;p&gt;Complete the form below to get a personalized report on your stack. I'll tell you what's working, what's not, and where you can save money. I genuinely want to hear about your logging nightmares :)&lt;/p&gt;&lt;p&gt;Free in 30 seconds&lt;/p&gt;&lt;head rend="h2"&gt;Get your stack roasted.&lt;lb/&gt;Get a plan to fix it.&lt;/head&gt;&lt;p&gt;Answer a few questions and I'll send you a personalized report with:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Where wide events would have the biggest impact on your stack&lt;/item&gt;&lt;item&gt;What to log (and what to stop logging)&lt;/item&gt;&lt;item&gt;Which tools are worth the cost (and which aren't)&lt;/item&gt;&lt;item&gt;Quick wins you can ship this week&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Questions? Logging horror stories? Drop them in the comments below.&lt;/p&gt;&lt;head rend="h3"&gt;Your stack has been judged.&lt;/head&gt;&lt;p&gt;Check your email for a detailed analysis.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46346796</guid><pubDate>Sun, 21 Dec 2025 18:09:52 +0000</pubDate></item><item><title>You’re not burnt out, you’re existentially starving</title><link>https://neilthanedar.com/youre-not-burnt-out-youre-existentially-starving/</link><description>&lt;doc fingerprint="7a329257eea0cf5f"&gt;
  &lt;main&gt;
    &lt;p&gt;“Those who have a ‘Why’ to live, can bear with almost any ‘How’.”&lt;/p&gt;
    &lt;p&gt;― Viktor Frankl quoting Friedrich Nietzsche, Man’s Search for Meaning&lt;/p&gt;
    &lt;p&gt;Let me guess:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Your life is going pretty darn well by any objective metric. &lt;list rend="ul"&gt;&lt;item&gt;Nice place to live. More than enough stuff. Family and friends who love you.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;But you’re tired, burnt out, and more. &lt;list rend="ul"&gt;&lt;item&gt;It feels like you’re stuck in the ordinary when all you want to do is chase greatness.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Viktor Frankl calls this feeling the “existential vacuum” in his famous book Man’s Search for Meaning. Frankl was a psychologist who survived the Holocaust, and in this book he explains that the inmates who survived with him found and focused on a higher purpose in life, like caring for other inmates and promising to stay alive to reconnect with loved ones outside the camps. But these survivors also struggled in their new lives after the war, desperately searching for meaning when every decision was no longer life or death.&lt;/p&gt;
    &lt;p&gt;Frankl realized that this existential anxiety is not a nuisance to eliminate, but actually an important signal pointing us towards our need for meaning. Similarly, while Friedrich Nietzsche would argue that life inherently lacks meaning, he’d also implore us to zoom out and find our highest purpose now:&lt;/p&gt;
    &lt;p&gt;“This is the most effective way: to let the youthful soul look back on life with the question, ‘What have you up to now truly loved, what has drawn your soul upward, mastered it and blessed it too?’… for your true being lies not deeply hidden within you, but an infinite height above you, or at least above that which you commonly take to be yourself.“&lt;/p&gt;
    &lt;p&gt;— Friedrich Nietzsche, Untimely Meditations, 1874&lt;/p&gt;
    &lt;p&gt;Nihilists get both Nietzsche and YOLO wrong. Neither mean that you give up. Instead, both mean that your efforts are everything.&lt;/p&gt;
    &lt;p&gt;So when you get those Sunday Scaries, the existential anxiety that your time is ending and the rest of your life is spent working for someone else, the answer isn’t escapism.&lt;/p&gt;
    &lt;p&gt;Instead, visualize your ideal self, the truest childhood dream of who you wanted to be when you grew up. What would that person be doing now? Go do that thing!&lt;/p&gt;
    &lt;p&gt;When facing the existential vacuum, there’s only one way out — up, towards your highest purpose.&lt;/p&gt;
    &lt;p&gt;On a 0-10 scale, how happy did you feel when you started working this Monday?&lt;/p&gt;
    &lt;p&gt;Why wasn’t your answer a 10?&lt;/p&gt;
    &lt;p&gt;You got the great job. You built the startup. You took the vacations. But that’s not what you really needed. You kept coming back Monday after Monday realizing you were doing the same job again.&lt;/p&gt;
    &lt;p&gt;So you tried to improve yourself. You optimized your morning routine. You perfected your productivity system. You bought a sleep mask and mouth tape. Yet you’re still dragging yourself out of bed each Monday morning tired and unmotivated.&lt;/p&gt;
    &lt;p&gt;We’re optimizing for less suffering instead of more meaning. We’ve confused comfort with fulfillment. And we’re getting really, really good at it. Millennials are the first generation in history to expect our jobs to provide a higher meaning beyond survival. That’s a good thing. It means that the essentials of life are nearly universally available now.&lt;/p&gt;
    &lt;p&gt;But, as I write in my book Positive Politics:&lt;/p&gt;
    &lt;p&gt;“The last two hundred years of progress pulled most of the world’s population over the poverty line. The next hundred years is about lifting everyone above the abundance line… Positive Politics seeks to democratize this abundance.“&lt;/p&gt;
    &lt;p&gt;Those of us who have already achieved abundance in our own lives now have two responsibilities:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spread that abundance to as many other people as possible.&lt;/item&gt;
      &lt;item&gt;Find something more meaningful to do than chase more stuff.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;“The existential vacuum is a widespread phenomenon of the twentieth century“&lt;/p&gt;
    &lt;p&gt;― Viktor Frankl, Man’s Search for Meaning&lt;/p&gt;
    &lt;p&gt;When I was a kid, I knew exactly what I wanted to do — the most important job in the world. And I wasn’t afraid to tell you either. At five years old, I would talk your ear off about training to be goalie for the St. Louis Blues. By seven, it was astronaut for NASA. By eleven, it was President of the United States. Then middle school hit, I got made fun of more than a few times, and that voice went silent.&lt;/p&gt;
    &lt;p&gt;After three startups, three nonprofits, and especially three kids knocked the imposter syndrome out of me, I spent a lot of time training my inner voice to get loud again. And what I heard reinforced what I knew all along — that my highest purpose is way above where I commonly take myself now.&lt;/p&gt;
    &lt;p&gt;Imposter syndrome can be a good thing. That external voice saying “this is not you” may actually be telling you the truth. I got into the testing lab industry to save our family business. Fifteen years and three startups later, I had become “the lab expert” to the world. But I cringed at that label. First, there was no room to grow. I had already done it. I didn’t want to be eighty and still running labs. Second, and most importantly, I knew that my skills could be used for much more than money.&lt;/p&gt;
    &lt;p&gt;I’d love to say I transformed overnight, but really it took 5+ years from 2020 to 2025 for me to fully embody my new identity. You can see it in my writing, which became much more ambitious in 2020, when I relaunched this site and started blogging consistently. That led to my World’s Biggest Problems project, which convinced me that Positive Politics is the #1 solution we need now!&lt;/p&gt;
    &lt;p&gt;There are two key components to my highest mission now:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Help people find their highest purpose.&lt;/item&gt;
      &lt;item&gt;Be a model for the pursuit of greatness.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That means consistently chasing my highest purpose — helping ambitious optimists get into politics! After nearly a decade of doing this behind the scenes as a political volunteer and advisor, 2025 was the first year where I went full-time in politics. Leading MCFN and publishing Positive Politics at the same time was a ton of work. But nothing energizes me more than fighting two of the biggest battles in the world now — anticorruption and Positive Politics!&lt;/p&gt;
    &lt;p&gt;I love politics because it’s full of meta solutions — solutions that create more solutions. My Positive Politics Accelerator is a classic example — recruiting and training more ambitious optimists into politics will lead to them making positive political change at all levels of government. But I’ve also tackled challenges like independent testing with startups and led a nonprofit to drive investigative journalism.&lt;/p&gt;
    &lt;p&gt;There are so many paths to positive impact, including politics, startups, nonprofits, medicine, law, education, science, engineering, journalism, art, faith, parenting, mentorship, and more! Choose the path that both best fits you now and is pointed towards your long-term highest purpose.&lt;/p&gt;
    &lt;p&gt;I woke up today so excited to get to work thinking it was Monday morning already. Instead of jumping right into it, I spent all morning making breakfast and playing with my kids, then wrote this post. When I’m writing about something personal, 1,000+ words can easily flow for me in an afternoon. This part will be done just in time to go to a nerf battle birthday party with my boys and their friends.&lt;/p&gt;
    &lt;p&gt;Both the hustle and anti-hustle cultures get it wrong. Working long hours isn’t inherently good or bad. If I really had to count how much I’m “on” vs. doing whatever I want, it’s easy 100+ hours per week. But that includes everything from investigative journalism and operations work for MCFN, social media and speaking events for Positive Politics, reading and writing for my site, and 40+ hours every week with my kids.&lt;/p&gt;
    &lt;p&gt;I want to help more ambitious optimists chase your highest potential! Whether the best solution is in startups, politics, nonprofits, science, crypto, or some new technology that’s yet to be invented, I’m happy to point you where I think you’ll be most powerful. I’ve thought, written, and worked on many of these ideas in my 15+ year career.&lt;/p&gt;
    &lt;p&gt;Now with 10+ years of writing, I’ve focused on publicly inspiring more people to take on these challenges too. We should be flexible on how we solve the problems but firm in our resolve to consistently organize people and launch solutions.&lt;/p&gt;
    &lt;p&gt;As Steve Jobs said, “Life can be much broader once you discover one simple fact, and that is everything around you that you call ‘life’ was made up by people that were no smarter than you… You can change it, you can mold it… the most important thing…is to shake off this erroneous notion that life is there and you’re just going to live in it, versus embrace it, change it, improve it, make your mark upon it… Once you learn that, you’ll never be the same again.”&lt;/p&gt;
    &lt;p&gt;Remember how it felt as a young child to openly tell the world about your dream job? Find the work that makes you feel this way and jump on whatever rung of that career ladder you can start now. The pay may be a little lower, but the existential payoff will be exponentially higher for the rest of your life.&lt;/p&gt;
    &lt;p&gt;You don’t have to go all-in right away! In fact, after a long diet of low existential work, it’s probably best to ease into public work. You can even volunteer one hour or less per week for a political campaign or nonprofit to get started. Pick the smallest first step, and do it. Not in January, now. Do it before the end of the year. And see how different you feel when 2026 starts!&lt;/p&gt;
    &lt;p&gt;And you don’t have to choose politics like me! Do you have the next great ambitious optimistic science fiction novel in your head? That book could spark movies and movements that positively change millions of lives! Choose the path will inspire and energize you for decades!&lt;/p&gt;
    &lt;p&gt;What matters most is you go straight towards your highest potential right now. Pause once a month to make sure you’re still on the right track. Stop once a year to triple-check you’re on the right track. But never get off this path towards your highest potential. Anything else will starve you existentially.&lt;/p&gt;
    &lt;p&gt;When you truly chase your highest potential, everything you thought was burnout will melt away. Because you weren’t suffering from too much work, you were suffering from too little truly important work. Like a boy who thought he was full until dessert arrives, you’ll suddenly find your hunger return!&lt;/p&gt;
    &lt;p&gt;If you’re sick of politics as usual and ready to change the system, join Positive Politics!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Buy the book: positivepoliticsbook.com&lt;/item&gt;
      &lt;item&gt;Join the accelerator: positivepolitics.org/apply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46346958</guid><pubDate>Sun, 21 Dec 2025 18:28:56 +0000</pubDate></item><item><title>Mullvad VPN: "This is a Chat Control 3.0 attempt."</title><link>https://mastodon.online/@mullvadnet/115742530333573065</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46347080</guid><pubDate>Sun, 21 Dec 2025 18:39:46 +0000</pubDate></item><item><title>I can't upgrade to Windows 11, now leave me alone</title><link>https://idiallo.com/byte-size/cant-update-to-windows-11-leave-me-alone</link><description>&lt;doc fingerprint="149e0877eae63cf4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Microsoft won't let you dismiss the upgrade notification&lt;/head&gt;
    &lt;p&gt;So support for Windows 10 has ended. Yes, millions of users are still on it. One of my main laptops runs Windows 10. I can't update to Windows 11 because of the hardware requirements. It's not that I don't have enough RAM, storage, or CPU power. The hardware limitation is specifically TPM 2.0.&lt;/p&gt;
    &lt;p&gt;What is TPM 2.0, you say? It stands for Trusted Platform Module. It's basically a security chip on the motherboard that enables some security features. It's good and all, but Windows says my laptop doesn't support it. Great! Now leave me alone.&lt;/p&gt;
    &lt;p&gt;Well, every time I turn on my computer, I get a reminder that I need to update to Windows 11. OK, at this point a Windows machine only belongs to you in name. Microsoft can run arbitrary code on it. They already ran the code to decide that my computer doesn't support Windows 11. So why do they keep bothering me?&lt;/p&gt;
    &lt;p&gt;Fine, I'm frustrated. That's why I'm complaining. I've accepted the fact that my powerful, yet 10-year-old laptop won't get the latest update. But if Microsoft's own systems have determined my hardware is incompatible, why are they harassing? I'll just have to dismiss this notification and call it a day.&lt;/p&gt;
    &lt;p&gt;But wait a minute. How do I dismiss it?&lt;/p&gt;
    &lt;p&gt;I cannot dismiss it. I can only be reminded later or... I have to learn more. If I click "remind me later," I'm basically telling Microsoft that I consent to being shown the same message again whenever they feel like it. If I click "learn more"? I'm taken to the Windows Store, where I'm shown ads for different laptops I can buy instead. Apparently, I'm also probably giving them consent to show me this ad the next time I log in.&lt;/p&gt;
    &lt;p&gt;It's one thing to be at the forefront of enshittification, but Microsoft is now actively hostile to its users. I've written about this passive-aggressive illusion of choice before. They are basically asking "Do you want to buy a new laptop?" And the options they are presenting are "Yes" and "OK."&lt;/p&gt;
    &lt;p&gt;This isn't a bug. This is intentional design. Microsoft has deliberately removed the ability to decline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dear Microsoft&lt;/head&gt;
    &lt;p&gt;Listen. You said my device doesn't support Windows 11. You're right. Now leave me alone. I have another device running Windows 11. It's festered with ads, and you're trying everything in your power to get me to create a Microsoft account.&lt;/p&gt;
    &lt;p&gt;I paid for that computer. I also paid for a pro version of the OS. I don't want OneDrive. I don't want to sign up with my Microsoft account. Whether I use my computer online or offline is none of your business. In fact, if you want me to create an account on your servers, you are first required to register your OS on my own website. The terms and conditions are simple. Every time you perform any network access, you have to send a copy of the payload and response back to my server. Either that, or you're in breach of my terms.&lt;/p&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;p&gt;By the way, the application showing this notification is called Reusable UX Interaction Manager sometimes. Other times it appears as Campaign Manager.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46347108</guid><pubDate>Sun, 21 Dec 2025 18:43:20 +0000</pubDate></item><item><title>Rue: Higher level than Rust, lower level than Go</title><link>https://rue-lang.dev/</link><description>&lt;doc fingerprint="73e8df8eb97a78ff"&gt;
  &lt;main&gt;
    &lt;p&gt;No garbage collector, no manual memory management. A work in progress, though.&lt;/p&gt;
    &lt;p&gt;Familiar syntax inspired by various programming languages. If you know one, you'll feel at home with Rue.&lt;/p&gt;
    &lt;p&gt;Direct compilation to native code.&lt;/p&gt;
    &lt;code&gt;// It's a classic for a reason
 

 
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348262</guid><pubDate>Sun, 21 Dec 2025 20:46:02 +0000</pubDate></item><item><title>More on whether useful quantum computing is "imminent"</title><link>https://scottaaronson.blog/?p=9425</link><description>&lt;doc fingerprint="6f11911b6025ddfc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;More on whether useful quantum computing is “imminent”&lt;/head&gt;
    &lt;p&gt;These days, the most common question I get goes something like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A decade ago, you told people that scalable quantum computing wasn’t imminent. Now, though, you claim it plausibly is imminent. Why have you reversed yourself??&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I appreciated the friend of mine who paraphrased this as follows: “A decade ago you said you were 35. Now you say you’re 45. Explain yourself!”&lt;/p&gt;
    &lt;p&gt;A couple weeks ago, I was delighted to attend Q2B in Santa Clara, where I gave a keynote talk entitled “Why I Think Quantum Computing Works” (link goes to the PowerPoint slides). This is one of the most optimistic talks I’ve ever given. But mostly that’s just because, uncharacteristically for me, here I gave short shrift to the challenge of broadening the class of problems that achieve huge quantum speedups, and just focused on the experimental milestones achieved over the past year. With every experimental milestone, the little voice in my head that asks “but what if Gil Kalai turned out to be right after all? what if scalable QC wasn’t possible?” grows quieter, until now it can barely be heard.&lt;/p&gt;
    &lt;p&gt;Going to Q2B was extremely helpful in giving me a sense of the current state of the field. Ryan Babbush gave a superb overview (I couldn’t have improved a word) of the current status of quantum algorithms, while John Preskill’s annual where-we-stand talk was “magisterial” as usual (that’s the word I’ve long used for his talks), making mine look like just a warmup act for his. Meanwhile, Quantinuum took a victory lap, boasting of their recent successes in a way that I considered basically justified.&lt;/p&gt;
    &lt;p&gt;After returning from Q2B, I then did an hour-long podcast with “The Quantum Bull” on the topic “How Close Are We to Fault-Tolerant Quantum Computing?” You can watch it here:&lt;/p&gt;
    &lt;p&gt;As far as I remember, this is the first YouTube interview I’ve ever done that concentrates entirely on the current state of the QC race, skipping any attempt to explain amplitudes, interference, and other basic concepts. Despite (or conceivably because?) of that, I’m happy with how this interview turned out. Watch if you want to know my detailed current views on hardware—as always, I recommend 2x speed.&lt;/p&gt;
    &lt;p&gt;Or for those who don’t have the half hour, a quick summary:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In quantum computing, there are the large companies and startups that might succeed or might fail, but are at least trying to solve the real technical problems, and some of them are making amazing progress. And then there are the companies that have optimized for doing IPOs, getting astronomical valuations, and selling a narrative to retail investors and governments about how quantum computing is poised to revolutionize optimization and machine learning and finance. Right now, I see these two sets of companies as almost entirely disjoint from each other.&lt;/item&gt;
      &lt;item&gt;The interview also contains my most direct condemnation yet of some of the wild misrepresentations that IonQ, in particular, has made to governments about what QC will be good for (“unlike AI, quantum computers won’t hallucinate because they’re deterministic!”)&lt;/item&gt;
      &lt;item&gt;The two approaches that had the most impressive demonstrations in the past year are trapped ions (especially Quantinuum but also Oxford Ionics) and superconducting qubits (especially Google but also IBM), and perhaps also neutral atoms (especially QuEra but also Infleqtion and Atom Computing).&lt;/item&gt;
      &lt;item&gt;Contrary to a misconception that refuses to die, I haven’t dramatically changed my views on any of these matters. As I have for a quarter century, I continue to profess a lot of confidence in the basic principles of quantum computing theory worked out in the mid-1990s, and I also continue to profess ignorance of exactly how many years it will take to realize those principles in the lab, and of which hardware approach will get there first.&lt;/item&gt;
      &lt;item&gt;But yeah, of course I update in response to developments on the ground, because it would be insane not to! And 2025 was clearly a year that met or exceeded my expectations on hardware, with multiple platforms now boasting &amp;gt;99.9% fidelity two-qubit gates, at or above the theoretical threshold for fault-tolerance. This year updated me in favor of taking more seriously the aggressive pronouncements—the “roadmaps”—of Google, Quantinuum, QuEra, PsiQuantum, and other companies about where they could be in 2028 or 2029.&lt;/item&gt;
      &lt;item&gt;One more time for those in the back: the main known applications of quantum computers remain (1) the simulation of quantum physics and chemistry themselves, (2) breaking a lot of currently deployed cryptography, and (3) eventually, achieving some modest benefits for optimization, machine learning, and other areas (but it will probably be a while before those modest benefits win out in practice). To be sure, the detailed list of quantum speedups expands over time (as new quantum algorithms get discovered) and also contracts over time (as some of the quantum algorithms get dequantized). But the list of known applications “from 30,000 feet” remains fairly close to what it was a quarter century ago, after you hack away the dense thickets of obfuscation and hype.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m going to close this post with a warning. When Frisch and Peierls wrote their now-famous memo in March 1940, estimating the mass of Uranium-235 that would be needed for a fission bomb, they didn’t publish it in a journal, but communicated the result through military channels only. As recently as February 1939, Frisch and Meitner had published in Nature their theoretical explanation of recent experiments, showing that the uranium nucleus could fission when bombarded by neutrons. But by 1940, Frisch and Peierls realized that the time for open publication of these matters had passed.&lt;/p&gt;
    &lt;p&gt;Similarly, at some point, the people doing detailed estimates of how many physical qubits and gates it’ll take to break actually deployed cryptosystems using Shor’s algorithm are going to stop publishing those estimates, if for no other reason than the risk of giving too much information to adversaries. Indeed, for all we know, that point may have been passed already. This is the clearest warning that I can offer in public right now about the urgency of migrating to post-quantum cryptosystems, a process that I’m grateful is already underway.&lt;/p&gt;
    &lt;p&gt;Update: Someone on Twitter who’s “long $IONQ” says he’ll be posting about and investigating me every day, never resting until UT Austin fires me, in order to punish me for slandering IonQ and other “pure play” SPAC IPO quantum companies. And also, because I’ve been anti-Trump and pro-Biden. He confabulates that I must be trying to profit from my stance (eg by shorting the companies I criticize), it being inconceivable to him that anyone would say anything purely because they care about what’s true.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348318</guid><pubDate>Sun, 21 Dec 2025 20:53:34 +0000</pubDate></item><item><title>A guide to local coding models</title><link>https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude</link><description>&lt;doc fingerprint="2f539dd2343f89db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;[Revised] You Don’t Need to Spend $100/mo on Claude Code: Your Guide to Local Coding Models&lt;/head&gt;
    &lt;head rend="h3"&gt;What you need to know about local model tooling and the steps for setting one up yourself&lt;/head&gt;
    &lt;p&gt;[Edit 1] This article has been edited after initial release for clarity. Both the tl;dr and the end section have added information.&lt;/p&gt;
    &lt;p&gt;[Edit 2] This hypothesis was actually wrong and thank you to everyone who commented!&lt;/p&gt;
    &lt;p&gt;Here’s a full explanation of where I went wrong. I want to address this mistake as I realize it might have a meaningful impact on someone's financial position.&lt;/p&gt;
    &lt;p&gt;I’m not editing the actual article except where absolutely necessary so it doesn’t look like I’m covering up the mistake—I want to address it. Instead, I’ve included the important information below.&lt;/p&gt;
    &lt;p&gt;There is one takeaway this article provides that definitely holds true:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local models are far more capable than they’re given credit for, even for coding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It also explains the process of setting up a local coding model and technical information about doing so which is helpful for anyone wanting to set up a local coding model. I would still recommend doing so.&lt;/p&gt;
    &lt;p&gt;But do I want someone reading this to immediately drop their coding subscription and buy a maxed out MacBook Pro? No, and for that reason I need to correct my hypothesis from ‘Yes, with caveats’ to ‘No’.&lt;/p&gt;
    &lt;p&gt;This article was not an empirical assessment, but should have been to make these claims. Here’s where I went wrong:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;While local models can likely complete ~90% of the software development tasks that something like Claude Code can, the last 10% is the most important. When it comes to your job, that last 10% is worth paying more for to get that last bit of performance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I realized I looked at this more from the angle of a hobbiest paying for these coding tools. Someone doing little side projects—not someone in a production setting. I did this because I see a lot of people signing up for $100/mo or $200/mo coding subscriptions for personal projects when they likely don’t need to. I would not recommend running local models as a company instead of giving employees access to a tool like Claude Code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While larger local models are very capable, as soon as you run other development tools (Docker, etc.) that also eat into your RAM, your model needs to be much smaller and becomes a lot less capable. I didn’t factor this in in my experiment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, really, the takeaway should be that these are incredible supplemental models to frontier models when coding and could potentially save you on your subscription by dropping it down a tier, but practically they’re not worth the effort in situations that might affect your livelihood.&lt;/p&gt;
    &lt;p&gt;Exactly a month ago, I made a hypothesis: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price (and have better hardware too!).&lt;/p&gt;
    &lt;p&gt;So, to create by far the most expensive article I’ve ever written, I put my money where my mouth is and bought a MacBook Pro with 128 GB of RAM to get to work. My idea was simple: Over the life of the MacBook I’d recoup the costs of it by not paying for an AI coding subscription.&lt;/p&gt;
    &lt;p&gt;After weeks of experimenting and setting up local AI models and coding tools, I’ve come to the conclusion that my hypothesis was &lt;del&gt;correct, with nuance&lt;/del&gt;, not correct [see edit 2 above] which I’ll get into later in this article.&lt;/p&gt;
    &lt;p&gt;In this article, we cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Why local models matter and the benefits they provide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How to view memory usage and make estimates for which models can run on your machine and the RAM demands for coding applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Walk through setting up your own local coding model and tool step-by-step.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Don’t worry if you don’t have a high-RAM machine! You can still follow this guide. I’ve included some models to try out with a lower memory allotment. I think you’ll be surprised at how performant even the smallest of models is. In fact, there hasn’t really been a time during this experiment that I’ve been disappointed with model performance.&lt;/p&gt;
    &lt;p&gt;If you’re only here for the local coding tool setup, skip to the section at the bottom. I’ve even included a link to my modelfiles in that section to make setup even easier for you. Otherwise, let’s get into what you need to know.&lt;/p&gt;
    &lt;head rend="h2"&gt;tl;dr:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local coding models are very capable. Using the right model and the right tooling feels only half a generation behind the frontier cloud tools. I would say that for about 90% of developer work local models are more than sufficient. Even small 7B parameter models can be very capable. [Edited to add in this next part] Local models won’t compete with frontier models at the peak of performance, but can complete many coding tasks just as well for a fraction of the cost. They’re worth running to bring costs down on plenty of tasks but potentially not worth using if there’s a free tier available that performs better.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tools matter a lot. This is where I experienced the most disappointment. I tried many different tools with many different models and spent a lot of time tinkering. I ran into situations where the models wouldn’t call tools properly or their thinking traces wouldn’t close. Both of these rendered the tool essentially useless. Currently, tooling seems very finicky and if there’s anything developers need to be successful, it’s good tools.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There’s a lot to consider when you’re actually working within hardware constraints. We take the tooling set up for us in the cloud for granted. When setting up local models, I had to think a lot about trade-offs in performance versus memory usage, how different tools compared and affected performance, nuances in types of models, how to quantize, and other user-facing factors such as time-to-first-token and tokens per second.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google threw a wrench into my hypothesis. The local setup is almost a no-brainer when compared to a $100/mo+ subscription. Compared to free or nearly-free tooling (such as Gemini CLI, Jules, or Antigravity) there isn’t quite as strong of a monetary justification to spend more on hardware. There are benefits to local models outside of code, though, and I discuss those below.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the tl;dr was helpful, don’t forget to subscribe to get more in your inbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why local models?&lt;/head&gt;
    &lt;p&gt;You might wonder why local models are worth investing in at all. The obvious answer is cost. By using your own hardware, you don’t need to pay a subscription fee to a cloud provider for your tool. There are also a few less obvious and underrated reasons that make local models useful.&lt;/p&gt;
    &lt;p&gt;First: Reliability. Each week there seems to be complaints about performance regression within AI coding tools. Many speculate companies are pulling tricks to save resources that hurt model performance. With cloud providers, you’re at the mercy of the provider for when this happens. With local models, this only happens when you cause it to.&lt;/p&gt;
    &lt;p&gt;Second: Local models can apply to far more applications. Just the other day I was having a discussion with my dad about AI tooling he could use to streamline his work. His job requires studying a lot of data—a perfect application for an LLM-based tool—but his company blocks tools like Gemini and ChatGPT because a lot of this analysis is done on intellectual property. Unfortunately, he isn’t provided a suitable alternative to use.&lt;/p&gt;
    &lt;p&gt;With a local model, he wouldn’t have to worry about these IP issues. He could run his analyses without data ever leaving his machine. Of course, any tool calling would also need to ensure data never leaves the machine, but local models get around one of the largest hurdles for useful enterprise AI adoption. Running models on a local machine opens up an entire world of privacy- and security-centric AI applications that are expensive for cloud providers to provide.&lt;/p&gt;
    &lt;p&gt;Finally: Availability. Local models are available to you as long as your machine is. This means no worrying about your provider being down or rate limiting you due to high traffic. It also means using AI coding tools on planes or in other situations where internet access is locked down (think highly secure networks).&lt;/p&gt;
    &lt;p&gt;While local models do provide significant cost savings, the flexibility and reliability they provide can be even more valuable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding memory&lt;/head&gt;
    &lt;p&gt;To get going with local models you must understand the memory needed to run them on your machine. Obviously, if you have more memory you’ll be able to run better models, but understanding the nuances of that memory management will help you pick out the right model for your use case.&lt;/p&gt;
    &lt;p&gt;Local AI has two parts that eat up your memory: The model itself and the model’s context window.&lt;/p&gt;
    &lt;p&gt;The actual model has billions of parameters and all those parameters need to fit into your memory at once. Excellent local coding models start at around 30 billion (30B, for short) parameters in size. By default, these models use 16 bits to represent parameters. At 16 bits with 30B parameters, a model will take 60 GB of space in RAM (16 bits = 2 bytes per parameter, 30 billion parameters = 60 billion bytes which equals about 60 GB).&lt;/p&gt;
    &lt;p&gt;The second (and potentially larger) memory consuming part of local AI is the model’s context window. This is the model inputs and outputs that are stored so the model can reference them in future requests. This gives the model memory.&lt;/p&gt;
    &lt;p&gt;When coding with AI, we prefer this window to be as large as it can because we need to fit our codebase (or pieces of it) within our context window. This means we target a context window of 64,000 tokens or larger. All of these tokens will also be stored in RAM.&lt;/p&gt;
    &lt;p&gt;The important thing to understand about context windows is that the memory requirement per-token for a model depends on the size of that model. Models with more parameters tend to have large architectures (more hidden layers and larger dimensions to those layers). Larger architectures mean the model must store more information for each token within its key-value cache (context window) because it stores information for each token for each layer.&lt;/p&gt;
    &lt;p&gt;This means choosing an 80B parameter model over a 30B parameter model requires more memory for the model itself and also more memory for the same size context window. For example, a 30B parameter model might have a hidden dimension of 5120 with 64 layers while an 80B model has a hidden dimension of 8192 with 80 layers. Doing some back-of-the-napkin math shows us that the larger model requires approximately 2x more RAM to maintain the same context window as the 30B parameter model (see formula below).&lt;/p&gt;
    &lt;p&gt;Luckily, there are tricks to better manage memory. First, there are architectural changes that can be made to make model inference more efficient so it requires less memory. The model we set up at the end of this article uses Hybrid Attention which enables a much smaller KV cache enabling us to fit our model and context window in less memory. I won’t get into more detail in this article, but you can read more about that model and how it works here.&lt;/p&gt;
    &lt;p&gt;The second trick is quantizing the values you’re working with. Quantization means converting a continuous set of values into a smaller amount of distinct values. In our case, that means taking a set of numbers represented by a certain number of bits (16, for example) and reducing it to a set of numbers represented by fewer bits (8, for example). To put it simply, in our case we’re converting the numbers representing our model to a smaller bit representation to save memory while keeping the value representations within the model relatively equal.&lt;/p&gt;
    &lt;p&gt;You can quantize both your model weights and the values stored in your context window. When you quantize your model weights, you “remove intelligence” from the model because it’s less precise in its representation of innate information. I’ve also found the performance hit when going from 16 to 8 bits within the model to be much less than 8 to 4.&lt;/p&gt;
    &lt;p&gt;We can also quantize the values in our context window to reduce its memory requirement. This means we’re less precisely representing the model’s memory. Generally speaking, KV cache (context window) quantization is considered more destructive to model performance than weight quantization because it causes the model to forget details in long reasoning traces. Thus, you should test quantizing the KV cache to ensure it doesn’t degrade model performance for your specific task.&lt;/p&gt;
    &lt;p&gt;In reality, like the rest of machine learning, optimizing local model performance is an experimentation process and real-world machine learning requires understanding the practical limitations and capabilities of models when applied to specific applications.&lt;/p&gt;
    &lt;p&gt;Here are a few more factors to understand when setting up a local coding model on your hardware:&lt;/p&gt;
    &lt;head rend="h3"&gt;Instruct versus non-instruct&lt;/head&gt;
    &lt;p&gt;Instruct models are post-trained to be well-suited for chat-based interactions. They’re given chat pairings in their training to be optimized for excellent back-and-forth chat output. Non-instruct models are still trained LLMs, but focus on next-token prediction instead of chatting with a user. For our case, when using a chat-based coding tool (CLI or chat agent in your IDE) we need to use an instruct model. If you’re setting up an autocomplete model, you’ll want to find a model specifically post-trained for it (such as Qwen2.5-Coder-Base or DeepSeek-Coder-V2).&lt;/p&gt;
    &lt;head rend="h3"&gt;Serving tools&lt;/head&gt;
    &lt;p&gt;You need a tool to serve your local LLM for your coding tool to send it requests. On a MacBook, there are two primary options: MLX and Ollama.&lt;/p&gt;
    &lt;p&gt;Ollama is the industry standard and works on non-Mac hardware. It’s a great serving setup on top of llama.cpp that makes model serving almost plug-and-play. Users can download model weights from Ollama easily and can configure modelfiles with custom parameters for serving. Ollama can also serve a model once and make it available to multiple tools.&lt;/p&gt;
    &lt;p&gt;MLX is a Mac-specific framework for machine learning that is optimized specifically for Mac hardware. It also retrieves models for the user from a community collection. I’ve found Ollama to be very reliable in its model catalog, while MLX’s catalog is community sourced and can sometimes be missing specific models. Models are sourced from the community so a user can convert a model to MLX format themselves. MLX requires a bit more setup on the user’s end, but serves models faster because it doesn’t have a layer providing the niceties of Ollama on top of it.&lt;/p&gt;
    &lt;p&gt;Either of these is great, but I chose MLX to maximize what I can get with my RAM, but Ollama is probably the more beginner-friendly tool here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time-to-first-token and tokens per second&lt;/head&gt;
    &lt;p&gt;In real-world LLM applications it’s important that the model is able to serve its first token for a request in a reasonable amount of time and continue serving tokens at a speed that enables the user to use the model for its given purpose. If we have a high-performance model running locally, but it only serves a few tokens per second, it wouldn’t be useful for coding.&lt;/p&gt;
    &lt;p&gt;This is something taken for granted with cloud-hosted models that is a real consideration when working locally on constrained hardware. Another reason I chose MLX as my serving platform is because it served tokens up to 20% faster than Ollama. In reality, Ollama served tokens fast enough so I don’t think using MLX is necessary specifically for this reason for the models I tried.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance trade-offs&lt;/head&gt;
    &lt;p&gt;There are many ways to optimize local models and save RAM. It’s difficult to know which optimization method works best and the impact each has on a model especially when using them in tandem with other methods.&lt;/p&gt;
    &lt;p&gt;The right optimization method also depends on the application. In my experience, I find it best to prioritize larger models with more aggressive model quantization over smaller models with more precise model weights. Since our application is coding, I would also prioritize a less-quantized KV cache and using a smaller model to ensure reasoning works properly while not sacrificing the size of our context window.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding tools&lt;/head&gt;
    &lt;p&gt;There are many tools to code with local models and I suggest trying until you find one you like. Some top recommendations are OpenCode, Aider, Qwen Code, Roo Code, and Continue. Make sure to use a tool compatible with OpenAI’s API standard. While this should be most tools, this ensures a consistent model/tool connection. This makes it easier to switch between tools and models as needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting set up&lt;/head&gt;
    &lt;p&gt;I’ll spare you the trial and error I experienced getting this set up. The one thing I learned is that tooling matters a lot. Not all coding tools are created equal and not all of the models interact with tools equally. I experienced many times where tool calling or even running a tool at all was broken. I also had to tinker quite a bit with many of them to get them to work.&lt;/p&gt;
    &lt;p&gt;If you’re a PC enthusiast, an apt comparison to setting up local coding tools versus using the cloud offerings available is the difference between setting up a MacBook versus a Linux Laptop. With the Linux laptop, you might get well through the distro installation only to find that the drivers for your trackpad aren’t yet supported. Sometimes it felt like that with local models and hooking them to coding tools.&lt;/p&gt;
    &lt;p&gt;For my tool, I ended up going with Qwen Code. It was pretty plug-and-play as it’s a fork of Gemini CLI. It supports the OpenAI compatibility standard so I can easily sub in different models and affords me all of the niceties built into Gemini CLI that I’m familiar with using. I also know it’ll be supported because both the Qwen team and Google DeepMind are behind the tool. The tool is also open source so anyone can support it as needed.&lt;/p&gt;
    &lt;p&gt;For models, I focused on GPT-OSS and Qwen3 models since they were around the size I was looking for and had great reviews for coding. I ended up deciding to use Qwen3-Coder models because I found it performed best and because GPT-OSS frequently gave me “I cannot fulfill this request” responses when I asked it to build features.&lt;/p&gt;
    &lt;p&gt;I decided to serve my local models on MLX, but if you’re using a non-Mac device give Ollama a shot. A MacBook is an excellent machine for serving local models because of its unified memory architecture. This means the RAM can be allotted to the CPU or GPU as needed. MacBooks can also be configured with a ton of RAM. For serving local coding models, more is always better.&lt;/p&gt;
    &lt;p&gt;I’ve shared my modelfiles repo for you to reference and use as needed. I’ve got a script set up that automates much of the below process. Feel free to fork it and create your own modelfiles or star it to come back later.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install MLX or download Ollama (the rest of this guide will continue with MLX but details for serving on Ollama can be found here).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increase the VRAM limitation on your MacBook. macOS will automatically limit VRAM to 75% of the total RAM. We want to use more than that. Run sudo sysctl iogpu.wired_limit_mb=110000 in your terminal to set this up (adjust the mb setting according to the RAM on your MacBook). This needs to be set each time you restart your MacBook.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run pip install -U mlx-lm to install MLX for serving community models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Serve the model as an OpenAI compatible API using python -m mlx_lm.server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit. This command both runs the server and downloads the model for you if you haven’t yet. This particular model is what I’m using with 128GB of RAM. If you have less RAM, check out smaller models such as mlx-community/Qwen3-4B-Instruct-2507-4bit (8 GB RAM), mlx-community/Qwen2.5-14B-Instruct-4bit (16 GB RAM), mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (32 GB RAM), or mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit (64-96 GB RAM).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download Qwen Code. You might need to install Node Package Manager for this. I recommend using Node Version Manager (nvm) for managing your npm version.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Set up your tool to access an OpenAI compatible API by entering the following settings:&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Base URL: http://localhost:8080/v1 (should be the default MLX serves your model at)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;API Key: mlx&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Model Name: mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit (or whichever model you chose).&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Voila! Your coding model tool should be working with your local coding model.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I recommend opening Activity Monitor on your Mac to monitor memory usage. I’ve had cases where I thought a model should fit within my memory allotment but it didn’t and I ended up using a lot of swap memory. When this happens your model will run very slowly.&lt;/p&gt;
    &lt;p&gt;One tip I have for using local coding models: Focus on managing your context. This is a great skill even with cloud-based models. People tend to YOLO their chats and fill their context window, but I’ve found greater performance by ensuring that just what my model needs is sitting in my context window. This is even more important with local models that may need an extra boost in performance and are limited in their context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was my hypothesis correct?&lt;/head&gt;
    &lt;p&gt;My original hypothesis was: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price.&lt;/p&gt;
    &lt;p&gt;I would argue that&lt;del&gt;—yes!—&lt;/del&gt;no [see edit 2 above], it is correct. If we crunch the numbers, a MacBook with 128 GB is $4700 plus tax. If I spend $100/mo for 5 years, a coding subscription would cost $6000 in that same amount of time. Not only do I save money, but I also get a much more capable machine for anything else I want to do with it.&lt;/p&gt;
    &lt;p&gt;[This paragraph was added in after initial release of this article] It’s important to note that local models will not reach the peak performance of frontier models; however, they will likely be able to do most tasks just as well. The value of using a local model doesn’t come from raw performance, but from supplementing the cost of higher performance models. A local model could very well let you drop your subscription tier for a frontier coding tool or utilize a free tier as needed for better performance and run the rest of your tasks for free.&lt;/p&gt;
    &lt;p&gt;It’s also important to note that local models are only going to get better and smaller. This is the worst your local coding model will perform. I also wouldn’t be surprised if cloud-based AI coding tools get more expensive. If you figure you’re using greater than the $100/mo tier right now or that the $100/mo tier will cost $200/mo in the future, the purchase is a no-brainer. It’s just difficult to stomach the upfront cost.&lt;/p&gt;
    &lt;p&gt;From a performance standpoint, I would say the maximum model running on my 128 GB RAM MacBook right now feels about half a generation behind the frontier coding tools. That’s excellent, but something to keep in mind as that half a generation might matter to you.&lt;/p&gt;
    &lt;p&gt;One wrench thrown into my experiment is how much free quota Google hands out with their different AI coding tools. It’s easy to purchase expensive hardware when it saves you money in the long run. It’s much more difficult when the alternative is free.&lt;/p&gt;
    &lt;p&gt;Initially, I considered my local coding setup to be a great pair to Google’s free tier. It definitely performs better than Gemini 2.5 Flash and makes a great companion to Gemini 3 Pro. Gemini 3 Pro can solve more complex tasks with the local model doing everything else. This not only saves quota on 3 Pro but also provides a very capable fallback for when quota is hit.&lt;/p&gt;
    &lt;p&gt;However, this is foiled a bit now that Gemini 3 Flash was just announced a few days ago. It shows benchmark numbers much more capable than Gemini 2.5 Flash (and even 2.5 Pro!) and I’ve been very impressed with its performance. If that’s the free tier Google offers, it makes local coding models less fiscally reasonable. The jury is still out on how well Gemini 3 Flash will perform and how quota will be structured, but we’ll have to see if local models can keep up.&lt;/p&gt;
    &lt;p&gt;I’m very curious to hear what you think! Tell me about your local coding setup or ask any questions below.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Always be (machine) learning,&lt;/p&gt;
    &lt;p&gt;Logan&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348329</guid><pubDate>Sun, 21 Dec 2025 20:55:15 +0000</pubDate></item><item><title>The gift card accountability sink</title><link>https://www.bitsaboutmoney.com/archive/gift-card-accountability-sink/</link><description>&lt;doc fingerprint="6ecc901d1d372faa"&gt;
  &lt;main&gt;
    &lt;p&gt;Programming note: Merry Christmas! There will likely be another Bits about Money after the holiday but before New Year.&lt;/p&gt;
    &lt;p&gt;Bits about Money is supported by our readers. If your education budget or business can underwrite the coming year of public goods in financial-infrastructure education, commentary, and policy analysis, please consider supporting it. I’m told this is particularly helpful for policymakers and others who cannot easily expense a subscription, and who benefit from all issues remaining publicly available with no paywall.&lt;/p&gt;
    &lt;p&gt;The American Association of Retired People (AARP, an advocacy non-profit for older adults) has paid for ads on podcasts I listen to. The ad made a claim which felt raspberry-worthy (in service of an important public service announcement), which they repeat in writing: Asking to be paid by gift card is always a scam.&lt;/p&gt;
    &lt;p&gt;Of course it isn’t. Gift cards are a payments rail, and an enormous business independently of being a payments rail. Hundreds of firms will indeed ask you to pay them on gift cards! They also exist, and are marketed, explicitly to do the thing that the AARP implicitly asserts no business or government entity will ever do: provide a method for transacting for people who do not have a banked method of transacting. [0]&lt;/p&gt;
    &lt;p&gt;Gift card scams are also enormous. The FBI’s Internet Crime Complaint Center received $16.6 billion in reports in 2024 across several payment methods; this is just for those consumers who bothered reporting it, in spite of the extremely real received wisdom that reporting is unlikely to improve one’s direct situation.&lt;/p&gt;
    &lt;p&gt;The flavor texts of scams vary wildly, but in substance they’ll attempt to convince someone, often someone socially vulnerable, to part with sometimes very large sums of money by buying gift cards and conveying card information (card number and PIN number, both printed on the card) to the scammer. The scammer will then use the fraud supply chain, generally to swap the value on the card to another actor in return for value unconnected to the card. This can be delivered in many ways: cash, crypto, products and services in the scamming economy (such as purloined credit cards or even “lead lists” of vulnerable people to run more scams on), or laundered funds within regulated financial institutions which obscure the link between the crime and the funds (layering, in the parlance of AML professionals). A huge portion of running a gift card marketplace is trying to prevent yourself from being exploited or made into an instrumentality in exploiting others.&lt;/p&gt;
    &lt;p&gt;It surprises many people to learn that the United States aggressively defends customers from fraud over some payment methods, via a liability transfer to their financial institution, which transfers it to intermediaries, who largely transfer it to payment-accepting businesses. Many people think the U.S. can’t make large, effective, pro-consumer regulatory regimes. They are straightforwardly wrong… some of the time.&lt;/p&gt;
    &lt;p&gt;But the AARP, the FBI, and your friendly local payments nerd will all tell you that if you’re abused on your debit card you are quite likely to be made whole, and if you’re abused via purchasing gift cards, it is unlikely any deep pockets will cover for you. The difference in treatment is partially regulatory carveouts, partially organized political pressure, and partly a side effect of an accountability sink specific to the industrial organization of gift cards.&lt;/p&gt;
    &lt;head rend="h2"&gt;Most businesses do not run their own gift card programs&lt;/head&gt;
    &lt;p&gt;There exists an ecosystem of gift card program managers, who are essentially financial services businesses with a sideline in software. (I should probably mention that I previously worked for and am currently an advisor to Stripe, whose self conception would not be precisely that, but which a) supports many ways for people to pay money for things and b) does not necessarily endorse what I say in my personal spaces.)&lt;/p&gt;
    &lt;p&gt;Why does the program manager exist? Why not simply have the retailer keep some internal database of who the retailer owes money to, updating this when someone buys or loads a gift card and when they spend the balance at the store? Because this implies many capabilities that retailers do not necessarily have, such as e.g. software development teams.&lt;/p&gt;
    &lt;p&gt;There is also a large regulatory component to running a gift card program, despite gift cards’ relatively lax regulatory drag (we’ll return to that in a moment). Card programs are regulated at both the federal and state levels. One frequent requirement in several states is escheatment. (Essentially all states have a requirement for escheatment; many but not all exempt gift cards from it.)&lt;/p&gt;
    &lt;p&gt;As discussed previously in Bits about Money, a major component of the gift card business model is abandonment (“breakage”). Consumer advocates felt this was unfair to consumers, bordering on fraudulent really. They convinced states to take the money that retailers were keeping for themselves. (Many states didn’t take all that much convincing.)&lt;/p&gt;
    &lt;p&gt;In theory, and sometimes even in practice, a consumer can convince a state treasurer’s office of unclaimed property (e.g. Illinois’) that the $24.37 that Target remitted as part of its quarterly escheatment payment for an unused gift card 13 years ago was actually theirs. A consumer who succeeds at this, which is neither easy nor particularly inexpensive to do, will receive a $24.37 check in the mail. The state keeps the interest income; call it a fee for service. It also keeps the interest income of the tens of billions of dollars of accumulated unclaimed property, which it generally promises to dutifully custody awaiting a legitimate claim for as long as the United States shall exist.&lt;/p&gt;
    &lt;p&gt;And so if you are a regional or national retailer who wants to offer gift cards, you have a choice. You can dedicate a team of internal lawyers and operations specialists to understanding both what the laws of the several states require with respect to gift cards, which are a tiny portion of your total operations, not merely today but as a result of the next legislative session in Honolulu, because you absolutely must order the software written to calculate the payment to remit accurately several quarters in advance of the legal requirement becoming effective. Or you can make the much more common choice, and outsource this to a specialist.&lt;/p&gt;
    &lt;p&gt;That specialist, the gift card program manager, will sell you a Solution™ which integrates across all the surfaces you need: your point-of-sale systems, your website, your accounting software, the 1-800 number and website for customers to check balances, ongoing escheatment calculation and remittance, cash flow management, carefully titrated amounts of attention to other legal obligations like AML compliance, etc. Two representative examples: Blackhawk Network and InComm Payments. You’ve likely never heard of them, even if you have their product on your person right now. Their real customer has the title Director of Payments at e.g. a Fortune 500 company.&lt;/p&gt;
    &lt;p&gt;And here begins the accountability sink: by standard practice and contract, when an unsophisticated customer is abused by being asked to buy a BigCo gift card, BigCo will say, truthfully and unhelpfully, that BigCo does not issue BigCo gift cards. It sells them. It accepts them. But it does not issue them. Your princess is in another castle.&lt;/p&gt;
    &lt;p&gt;BigCo may very well have a large, well-staffed fraud department. But, not due to any sort of malfeasance whatsoever, that fraud department may consider BigCo gift cards entirely out of their own scope. They physically cannot access the database with the cards. Their security teams, sensitive that gift card numbers are dangerous to keep lying around, very likely made it impossible for anyone at BigCo to reconstruct what happened to a particular gift card between checkout and most recent use. “Your privacy is important to us!” they will say, and they are not cynically invoking it in this case.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gift cards are not regulated like other electronic payments instruments&lt;/head&gt;
    &lt;p&gt;As mentioned above, Regulation E is the primary driver for the private enforcement edifice that makes scarily smart professionals (and their attached balance sheets) swing into action on behalf of consumers. Reg E has a carveout for certain prepaid payments. Per most recent guidance, that includes prepaid gift cards, gift certificates, and similar.&lt;/p&gt;
    &lt;p&gt;And so, if you call your bank and say, “I was defrauded! Someone called me and pretended to be the IRS, and I read them my debit card number, and now I’ve lost money,” the state machine obligates the financial institution to have the customer service representative click a very prominent button on their interface. This will restore your funds very quickly and have some side effects you probably care about much less keenly. One of those is an “investigation,” which is not really an investigation in the commanding majority of cases.&lt;/p&gt;
    &lt;p&gt;And if you call the program manager and say, “I was defrauded! Someone called me and pretended to be the IRS, and I read them a gift card number, and now I’ve lost money,” there is… no state machine. There is no legal requirement to respond with alacrity, no statutorily imposed deadline, no button for a CS rep to push, and no investigation to launch. You will likely be told by a low-paid employee that this is unfortunate and that you should file a police report. The dominant reason for this is that suggesting a concrete action to you gets you off the phone faster, and the call center aggressively minimizes time to resolution of calls and recidivism, where you call back because your problem is not solved. Filing a police report will, in most cases, not restore your money—but if it causes you not to call the 1-800 number again, then from the card program manager’s perspective this issue has been closed successfully.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why do we choose this difference in regulation?&lt;/head&gt;
    &lt;p&gt;The people of the United States, through their elected representatives and the civil servants who labor on their behalf, intentionally exempt gift cards from the Reg E regime in the interest of facilitating commerce.&lt;/p&gt;
    &lt;p&gt;It is the ordinary and appropriate work of a democracy to include input from citizens in the rulemaking process. The Retail Industry Leaders Association participated, explaining to FinCEN that it would be quite burdensome for retailers to fall into KYC scope, etc etc. Many other lobbyists and industry associations made directionally similar comments.&lt;/p&gt;
    &lt;p&gt;The Financial Crimes Enforcement Network, for example, has an explicit carveout in its regulations: while FinCEN will aggressively police rogue bodegas, it has no interest in you if you sell closed-loop gift cards of less than $2,000 face value. This is explicitly to balance the state’s interest in law enforcement against, quote, preserving innovation and the many legitimate uses and societal benefits offered by prepaid access, endquote.&lt;/p&gt;
    &lt;p&gt;FinCEN’s rules clarify that higher-value activity—such as selling more than $10,000 in gift cards to a single individual in a day—brings sellers back into scope. Given the relatively lax enforcement environment for selling a $500 gift card, you very likely might not build out systems which will successfully track customer identities and determine that the same customer has purchased twenty-one $500 gift cards in three transactions. That likely doesn’t rate as a hugely important priority for Q3.&lt;/p&gt;
    &lt;p&gt;And so the fraud supply chain comes to learn which firms haven’t done that investment, and preferentially suggests those gift cards to their launderers, mules, brick movers, and scam victims.&lt;/p&gt;
    &lt;p&gt;And that’s why the AARP tells fibs about gift cards: we have, with largely positive intentions and for good reasons, exposed them to less regulation than most formal payment systems in the United States received. That decision has a cost. Grandma sometimes pays it.&lt;/p&gt;
    &lt;p&gt;[0] Indeed, there are entire companies which exist to turn gift cards into an alternate financial services platform, explicitly to give unbanked and underbanked customers a payments rail. Paysafe, for example, is a publicly traded company with thousands of employees, the constellation of regulatory supervision you’d expect, and a subsidiary Openbucks which is designed to give businesses the ability to embed Pay Us With A Cash Voucher in their websites/invoices/telephone collection workflows. This is exactly the behavior that “never happens from a legitimate business” except when it does by the tens of billions of dollars.&lt;/p&gt;
    &lt;p&gt;As Bits about Money has frequently observed, people who write professionally about money—including professional advocates for financially vulnerable populations—often misunderstand alternative financial services, largely because those services are designed to serve a social class that professionals themselves do not belong to, rarely interact with directly, and do not habitually ask how they pay rent, utilities, or phone bills.&lt;/p&gt;
    &lt;head rend="h2"&gt;Want more essays in your inbox?&lt;/head&gt;
    &lt;p&gt;I write about the intersection of tech and finance, approximately biweekly. It's free.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348455</guid><pubDate>Sun, 21 Dec 2025 21:07:01 +0000</pubDate></item><item><title>Engineering dogmas it's time to retire</title><link>https://newsletter.manager.dev/p/5-engineering-dogmas-its-time-to</link><description>&lt;doc fingerprint="af411814dda4669a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;5 engineering dogmas it's time to retire&lt;/head&gt;
    &lt;p&gt;A few months ago, I wrote about 13 software engineering laws, which are observations about how software projects behave.&lt;/p&gt;
    &lt;p&gt;Today, I’ll cover 5 practices that are considered ‘common wisdom’, and why I think worth reconsidering them.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Don’t reinvent the wheel - find a package&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every PR must be reviewed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2-4 week sprints are how modern teams work&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every code change should be behind a feature flag/gate&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If a comment is needed, the code is too complex&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;1. Don’t reinvent the wheel - find a package&lt;/head&gt;
    &lt;p&gt;Why waste time writing code that someone already wrote before you?&lt;/p&gt;
    &lt;p&gt;The CTO of a startup I worked at hated dependencies. We worked with some 3D calculations (software for drones), and he was writing tens of mathematical functions himself. He insisted that even though it’s slower, he at least understands every part and can fix any bug that will pop up, and wouldn’t depend on anyone else for critical parts of our software.&lt;/p&gt;
    &lt;p&gt;I used to make fun of that paranoia, and he sent me to read some crazy stories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The developer of left-pad took it down from NPM, breaking the builds of Facebook, Spotify, Netflix, and many more. It’s basically an 11-line for loop that adds spaces to a string.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The is-even npm package has 160k weekly downloads(!). The author published it back when he was learning to code. Here’s what it does:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You are also much more vulnerable to security incidents (and need to spend a significant amount of time chasing updates). In most smaller companies, there is no vetting process for packages (unlike for vendors) - every engineer does as they please.&lt;/p&gt;
    &lt;p&gt;With LLMs, it’s easier to both get into this mess and get out of it: it’s much easier to install an unneeded dependency by mistake, but it’s also quicker to implement ‘known’ solutions from scratch.&lt;/p&gt;
    &lt;p&gt;It’s a tricky balance.&lt;/p&gt;
    &lt;p&gt;Another ‘common wisdom’ among software engineers is that project management software sucks. It’s a necessary evil you just have to deal with.&lt;/p&gt;
    &lt;p&gt;I also thought this was just part of my job - spending hours chasing engineers to ‘keep the tickets updated’, so I could understand what’s going on and make better decisions. Switching to another tool felt like a huge project that was just not worth it.&lt;/p&gt;
    &lt;p&gt;Linear took that to heart, and made switching super simple with a 2-way sync, keeping your legacy tool updated.&lt;/p&gt;
    &lt;p&gt;I haven’t met an engineer who tried Linear and didn’t like it. Teams that switch to Linear see 2x more reported issues - engineers actually want to use the tool. More visibility =&amp;gt; fewer meetings =&amp;gt; happer engineers.&lt;/p&gt;
    &lt;p&gt;Thanks Linear for supporting today’s article!&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Every code change must be reviewed &lt;/head&gt;
    &lt;p&gt;In every company I worked for in the last 15 years, we had mandatory code reviews (at least one, usually two reviewers per PR).&lt;/p&gt;
    &lt;p&gt;A couple of months ago I wrote a deeper dive about the price of mandatory code reviews.&lt;/p&gt;
    &lt;p&gt;In short - they help you improve the quality, but slow you down significantly.&lt;/p&gt;
    &lt;p&gt;I’m definitely not against code reviews - there is a TON of value in them. What I am against is a lengthy process with strict rules (like having to re-request a review for every commit at a PR, even after it was approved).&lt;/p&gt;
    &lt;p&gt;I love the process at Pylon: engineers merge their own code and only request reviews if they need input, think they have a risky change, or are still onboarding. Their thought process is: if we hire skilled engineers and trust them, there’s no reason to bottleneck every change with mandatory reviews.&lt;/p&gt;
    &lt;p&gt;Pair programming is also a great alternative to async reviews.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. 2-4 week sprints are how modern teams work&lt;/head&gt;
    &lt;p&gt;I believe that sprints are taking the joy out of building software.&lt;/p&gt;
    &lt;p&gt;If you had to think about the ideal way to organize your team’s efforts, do you honestly think you would have gone with the current way of doing things?&lt;/p&gt;
    &lt;p&gt;Do you feel it brings the most value to your customers? Do you think your engineers truly enjoy the process, feeling they contribute from their own creativity?&lt;/p&gt;
    &lt;p&gt;Shape Up is a great alterinative. Here’s the gist of it (from here):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We work in 6-week cycles. Once a cycle is over, we take one or two weeks off of scheduled projects so everyone can roam independently, fix stuff up, pick up some pet projects we’ve wanted to do, and generally wind down prior to starting the next six week cycle.&lt;/p&gt;
      &lt;p&gt;Note: These are not sprints. I despise the word sprints. Sprints and work don’t go together. This isn’t about running all out as fast as you can, it’s about working calmly, at a nice pace, and making smart calls along the way. No brute force here, no catching our collective breath at the end.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;My point is not that you should adopt Shape Up.&lt;/p&gt;
    &lt;p&gt;It is that there are alternatives to Scrum/Kanban. You can build a way of working that actually fits your team and company, without leaving everyone exhausted.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Every code change should be behind a feature flag&lt;/head&gt;
    &lt;p&gt;In many cases, feature flags are ruining your codebase:&lt;/p&gt;
    &lt;p&gt;Once you introduce the feature flags capability, PMs will come up with other ideas for using it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Why should we rely on developers for the configuration change? Let’s move it to somewhere the PMs can access, and then we’ll be able to do it without bothering anyone.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While we are at it, let’s also make it adjustable per user! That way, the PMs can safely release the feature for a couple of users to gather feedback and test it themselves, and only afterwards release it to everyone.&lt;/p&gt;
        &lt;p&gt;This is similar to a Canary Release. The difference between them is that a Canary Released feature is exposed to a randomly selected cohort of users while here, the feature is exposed to a specific set of users.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;And why limit ourselves to using it for turning features on? Let’s put the most resource-heavy feature behind one, and let the Ops people turn it off in case of an unusual overload.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Oh and we have those premium users, let’s have some features enabled only for them!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;One last thing - you can put everything under a feature flag, right? So let’s not risk any changes without it, please hide any bug fix under a flag, in case the fix needs to be reverted.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus you find yourself in deep shit, with hundreds of active feature flags - making your codebase more complex and much harder to test.&lt;/p&gt;
    &lt;p&gt;Feature flags also give you a sense of false security - I’ve seen multiple bugs caused by developers releasing to production code that was supposed to be hidden behind a flag, but wasn’t (especailly in React).&lt;/p&gt;
    &lt;p&gt;I definitely think you should use feature flags - just not abuse them. It’s ok to just properly test a code change on staging and release it without any flags/gates.&lt;/p&gt;
    &lt;head rend="h3"&gt;5. If a comment is needed, the code is too complex &lt;/head&gt;
    &lt;p&gt;The V1 of commenting advice was: “Comment your code so it’ll be understandable by whoever comes after you”.&lt;/p&gt;
    &lt;p&gt;Then, the V2 version of the advice gained popularity: “If you need to add comments, it means your code sucks. It needs to be self-explanatory”.&lt;/p&gt;
    &lt;p&gt;Any extreme doesn’t make sense imo.&lt;/p&gt;
    &lt;p&gt;Yes, you can write code that will require fewer comments.&lt;/p&gt;
    &lt;p&gt;Still, in some cases a line or two can save someone else hours of frustration a couple of years in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final words&lt;/head&gt;
    &lt;p&gt;There are many other ‘common wisdom’ bits, like ‘don’t release on Friday’ and ‘microservices help with scale and ownership’.&lt;/p&gt;
    &lt;p&gt;None of them, and of the 5 covered in the article, is complete nonsense. There is a reason they became so common.&lt;/p&gt;
    &lt;p&gt;My point is that good Engineering Managers know to balance such dogmas with reality and constantly assess what’s best for their teams.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348660</guid><pubDate>Sun, 21 Dec 2025 21:26:09 +0000</pubDate></item><item><title>Disney Imagineering Debuts Next-Generation Robotic Character, Olaf</title><link>https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/</link><description>&lt;doc fingerprint="ebf30b7df696007b"&gt;
  &lt;main&gt;
    &lt;p&gt;Disneyland Paris saw a groundbreaking moment today, where Bruce Vaughn, President and Chief Creative Officer of Walt Disney Imagineering, and Natacha Rafalski, Présidente of Disneyland Paris, introduced a next-generation robotic character representing Olaf, the beloved snowman from Walt Disney Animation Studios’ Frozen.&lt;/p&gt;
    &lt;p&gt;This debut marks a new chapter in Disney character innovation, one where technology, storytelling, and collaboration come together to bring screen to reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Innovation at the Core: From Screen to Reality&lt;/head&gt;
    &lt;p&gt;From the way he moves to the way he looks, every gesture and detail is crafted to reflect the Olaf audiences have seen in the film — alive, curious, and unmistakably himself. As for his snow-like shimmer that catches the light just like fresh snow, this was enhanced by iridescent fibers. These details make Olaf one of the most expressive and true-to-life characters built, and he’s soon making his debut at Disney parks.&lt;/p&gt;
    &lt;p&gt;Our roots are in animation with Walt Disney pioneering early hand-drawn films and today, Walt Disney Animation Studios and Pixar Animation Studios continue that tradition. We collaborated closely with the film’s original animators at Walt Disney Animation Studios to ensure every gesture felt true to the character. This isn’t just about replicating the animation, it’s about emulating the creators’ intent.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Technology Behind the Magic&lt;/head&gt;
    &lt;p&gt;Home to some of the best storytellers in the world, we’re continuously pushing the boundaries of innovation and technology — in fact it is in our DNA.&lt;/p&gt;
    &lt;p&gt;Like everything at Disney, we always start with the story, and our number one priority is to build storytelling technology that empowers our Disney Imagineers to breathe life into our characters.&lt;/p&gt;
    &lt;p&gt;While the BDX droids — the Star Wars free roaming robotic characters that mimic movements in a simulation — have been interacting with guests for a while now, Olaf presents a far greater challenge: an animated character with non-physical movements. To make Olaf as authentic as possible, the team used a branch of artificial intelligence called reinforcement learning, pushing the limits of hardware to achieve the creative intent of the artists.&lt;/p&gt;
    &lt;p&gt;It takes humans years to master walking and even longer to perform graceful motions. Deep reinforcement learning helps him acquire these skills in a fraction of the time.&lt;/p&gt;
    &lt;p&gt;Olaf’s “snow” also moves differently than the hard shells of other robotic characters, and he can fully articulate his mouth, eyes, and removable carrot nose and arms. Most importantly, Olaf can speak and engage in conversations, creating a truly one-of-a-kind experience.&lt;/p&gt;
    &lt;p&gt;Innovation takes many forms across our parks, experiences, and products – all focused on improving the guest experience and bringing joy to fans around the world. And what’s most exciting is that we’re just getting started!&lt;/p&gt;
    &lt;p&gt;The BDX Droids, self-balancing H.E.R.B.I.E., and now Olaf represent increasing levels of performance and innovation in bringing Disney characters to life. The speed at which we can create new characters and introduce them to guests is unprecedented. We’re scaling bigger than ever, working to bring more emotive, expressive, and surprising characters to our experiences around the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Guests Can See Olaf&lt;/head&gt;
    &lt;p&gt;Olaf will soon venture out into the unknown, eager to see guests at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arendelle Bay Show in World of Frozen, the new immersive world coming soon to Disney Adventure World at Disneyland Paris.&lt;/item&gt;
      &lt;item&gt;Limited-time special appearances at World of Frozen at Hong Kong Disneyland Resort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Looking for a warm hug now? You can discover how Olaf, along with other exciting breakthroughs from Walt Disney Imagineering Research &amp;amp; Development, came to life at in the latest episode of We Call It Imagineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348847</guid><pubDate>Sun, 21 Dec 2025 21:46:20 +0000</pubDate></item></channel></rss>