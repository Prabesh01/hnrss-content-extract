<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 31 Dec 2025 09:15:19 +0000</lastBuildDate><item><title>The British empire's resilient subsea telegraph network</title><link>https://subseacables.blogspot.com/2025/12/the-british-empires-resilient-subsea.html</link><description>&lt;doc fingerprint="a5d313b1b0391af3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;The British Empire's Resilient Subsea Telegraph Network&lt;/head&gt;
    &lt;p&gt;The British empire had largely completed its Red Line cable network by 1902. This network allowed news and messages to be delivered in a few minutes or several hours at most depending on the message queue's length. It spanned the globe and formed a network ring so traffic could be routed in the opposite direction in case of disruption. It was, as Dr. Michael Delaunay has argued, a highly resilient network. Besides the ring configuration, the network relied on multiple cables between any pair of given end points to ensure uptime. The British military believed it would be impossible for an enemy to cut enough cables on any route to sever all communications between any given pair of end points. The Committee of Imperial Defense concluded that 57 cables must be shut down to isolate the British Isles from the Red Line network. The figure was 15 for Canada and 7 for South Africa. The Empire was self sufficient in terms of manufacturing the components for a subsea telegraph cable and repairing it. Its navy had no peers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46432999</guid><pubDate>Tue, 30 Dec 2025 13:10:56 +0000</pubDate></item><item><title>No strcpy either</title><link>https://daniel.haxx.se/blog/2025/12/29/no-strcpy-either/</link><description>&lt;doc fingerprint="8202bb45bad1df9a"&gt;
  &lt;main&gt;
    &lt;p&gt;Some time ago I mentioned that we went through the curl source code and eventually got rid of all &lt;code&gt;strncpy&lt;/code&gt;() calls.&lt;/p&gt;
    &lt;p&gt;strncpy() is a weird function with a crappy API. It might not null terminate the destination and it pads the target buffer with zeroes. Quite frankly, most code bases are probably better off completely avoiding it because each use of it is a potential mistake.&lt;/p&gt;
    &lt;p&gt;In that particular rewrite when we made strncpy calls extinct, we made sure we would either copy the full string properly or return error. It is rare that copying a partial string is the right choice, and if it is, we can just as well &lt;code&gt;memcpy&lt;/code&gt; it and handle the null terminator explicitly. This meant no case for using strlcpy or anything such either.&lt;/p&gt;
    &lt;head rend="h2"&gt;But strcpy?&lt;/head&gt;
    &lt;p&gt;strcpy however, has its valid uses and it has a less bad and confusing API. The main challenge with strcpy is that when using it we do not specify the length of the target buffer nor of the source string.&lt;/p&gt;
    &lt;p&gt;This is normally not a problem because in a C program &lt;code&gt;strcpy&lt;/code&gt; should only be used when we have full control of both.&lt;/p&gt;
    &lt;p&gt;But normally and always are not necessarily the same thing. We are but all human and we all do mistakes. Using strcpy implies that there is at least one or maybe two, buffer size checks done prior to the function invocation. In a good situation.&lt;/p&gt;
    &lt;p&gt;Over time however – let’s imagine we have code that lives on for decades – when code is maintained, patched, improved and polished by many different authors with different mindsets and approaches, those size checks and the function invoke may glide apart. The further away from each other they go, the bigger is the risk that something happens in between that nullifies one of the checks or changes the conditions for the strcpy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enforce checks close to code&lt;/head&gt;
    &lt;p&gt;To make sure that the size checks cannot be separated from the copy itself we introduced a string copy replacement function the other day that takes the target buffer, target size, source buffer and source string length as arguments and only if the copy can be made and the null terminator also fits there, the operation is done.&lt;/p&gt;
    &lt;p&gt;This made it possible to implement the replacement using memcpy(). Now we can completely ban the use of strcpy in curl source code, like we already did strncpy.&lt;/p&gt;
    &lt;p&gt;Using this function version is a little more work and more cumbersome than strcpy since it needs more information, but we believe the upsides of this approach will help us have an oversight for the extra pain involved. I suppose we will see how that will fare down the road. Let’s come back in a decade and see how things developed!&lt;/p&gt;
    &lt;quote&gt;void curlx_strcopy(char *dest,&lt;lb/&gt;size_t dsize,&lt;lb/&gt;const char *src,&lt;lb/&gt;size_t slen)&lt;lb/&gt;{&lt;lb/&gt;DEBUGASSERT(slen &amp;lt; dsize);&lt;lb/&gt;if(slen &amp;lt; dsize) {&lt;lb/&gt;memcpy(dest, src, slen);&lt;lb/&gt;dest[slen] = 0;&lt;lb/&gt;}&lt;lb/&gt;else if(dsize)&lt;lb/&gt;dest[0] = 0;&lt;lb/&gt;}&lt;/quote&gt;
    &lt;head rend="h2"&gt;AI slop&lt;/head&gt;
    &lt;p&gt;An additional minor positive side-effect of this change is of course that this should effectively prevent the AI chatbots to report strcpy uses in curl source code and insist it is insecure if anyone would ask (as people still apparently do). It has been proven numerous times already that strcpy in source code is like a honey pot for generating hallucinated vulnerability claims.&lt;/p&gt;
    &lt;p&gt;Still, this will just make them find something else to make up a report about, so there is probably no net gain. AI slop is not a game we can win.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46433029</guid><pubDate>Tue, 30 Dec 2025 13:14:40 +0000</pubDate></item><item><title>Hive (YC S14) Is Hiring a Staff Software Engineer (Data Systems)</title><link>https://jobs.ashbyhq.com/hive.co/cb0dc490-0e32-4734-8d91-8b56a31ed497</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46433661</guid><pubDate>Tue, 30 Dec 2025 14:31:34 +0000</pubDate></item><item><title>Show HN: 22 GB of Hacker News in SQLite</title><link>https://hackerbook.dosaygo.com</link><description>&lt;doc fingerprint="68ef37d60d5d7ecf"&gt;
  &lt;main&gt;
    &lt;p&gt;Hacker Book new | front | start | ask | show | jobs | query | me Someday, Month 00, 0000 &amp;lt; &amp;gt; ARCHIVE Loading… Y Combinator | Apply | Companies | Blog | Live HN | Contact &amp;lt; &amp;gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46435308</guid><pubDate>Tue, 30 Dec 2025 17:01:59 +0000</pubDate></item><item><title>Toro: Deploy Applications as Unikernels</title><link>https://github.com/torokernel/torokernel</link><description>&lt;doc fingerprint="3069f0a7771d385"&gt;
  &lt;main&gt;
    &lt;p&gt;Toro is a unikernel dedicated to deploy applications as microVMs. Toro leverages on virtio-fs and virtio-vsocket to provide a minimalistic architecture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support x86-64 architecture&lt;/item&gt;
      &lt;item&gt;Support up to 512GB of RAM&lt;/item&gt;
      &lt;item&gt;Support QEMU-KVM microvm and Firecracker&lt;/item&gt;
      &lt;item&gt;Cooperative and I/O bound threading scheduler&lt;/item&gt;
      &lt;item&gt;Support virtio-vsocket for networking&lt;/item&gt;
      &lt;item&gt;Support virtio-fs for filesystem&lt;/item&gt;
      &lt;item&gt;Fast boot up&lt;/item&gt;
      &lt;item&gt;Tiny image&lt;/item&gt;
      &lt;item&gt;Built-in gdbstub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can try Toro by running the HelloWorld example using a Docker image that includes all the required tools. To do so, execute the following commands in a console (these steps require you to install before KVM and Docker):&lt;/p&gt;
    &lt;code&gt;wget https://raw.githubusercontent.com/torokernel/torokernel/master/ci/Dockerfile
sudo docker build -t torokernel-dev .
sudo docker run --privileged --rm -it torokernel-dev
cd examples/HelloWorld
python3 ../CloudIt.py -a HelloWorld&lt;/code&gt;
    &lt;p&gt;If these commands execute successfully, you will get the output of the HelloWorld example. You can also pull the image from dockerhub instead of building it:&lt;/p&gt;
    &lt;code&gt;sudo docker pull torokernel/torokernel-dev:latest
sudo docker run --privileged --rm -it torokernel/torokernel-dev:latest&lt;/code&gt;
    &lt;p&gt;You can share a directory from the host by running:&lt;/p&gt;
    &lt;code&gt;sudo docker run --privileged --rm --mount type=bind,source="$(pwd)",target=/root/torokernel -it torokernel/torokernel-dev:latest&lt;/code&gt;
    &lt;p&gt;You will find $pwd from host at &lt;code&gt;/root/torokernel&lt;/code&gt; in the container.&lt;/p&gt;
    &lt;p&gt;Execute the commands in &lt;code&gt;ci/Dockerfile&lt;/code&gt; to install the required components locally. Then, Go to &lt;code&gt;torokernel/examples&lt;/code&gt; and edit &lt;code&gt;CloudIt.py&lt;/code&gt; to set the correct paths to Qemu and fpc. Optionally, you can install vsock-socat from here and virtio-fs from here. You need to set the correct path to virtiofsd and socat.&lt;/p&gt;
    &lt;p&gt;Go to &lt;code&gt;examples/HelloWorld/&lt;/code&gt; and execute:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a HelloWorld&lt;/code&gt;
    &lt;p&gt;To run the StaticWebserver, you require virtiofsd and socat. To compile socat, execute the following commands:&lt;/p&gt;
    &lt;code&gt;git clone git@github.com:stefano-garzarella/socat-vsock.git
cd socat-vsock
autoreconf -fiv
./configure
make socat&lt;/code&gt;
    &lt;p&gt;Set the path to socat binary in CloudIt.py and then execute:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a StaticWebServer -r -d /path-to-directory/ -f 4000:80&lt;/code&gt;
    &lt;p&gt;You have to replace the &lt;code&gt;/path-to-directory/&lt;/code&gt; to a directory that containing the files, e.g., index.html. To try it, you can execute:&lt;/p&gt;
    &lt;code&gt;wget http://127.0.0.1:4000/index.html
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-f&lt;/code&gt; parameter indicates a forwarding of the 4000 port from the host to the 80 port in the guest using vsock.&lt;/p&gt;
    &lt;p&gt;This example shows how cores can communicate by using the VirtIOBus device. In this example, core #0 sends a packet to every core in the system with the ping string. Each core responds with a packet that contains the message pong. This example is configured to use three cores. To launch it, simply executes the following commands in the context of the container presented above:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a InterCoreComm&lt;/code&gt;
    &lt;p&gt;You will get the following output:&lt;/p&gt;
    &lt;p&gt;You have many ways to contribute to Toro. One of them is by joining the Google Group here. In addition, you can find more information here.&lt;/p&gt;
    &lt;p&gt;GPLv3&lt;/p&gt;
    &lt;p&gt;[0] A Dedicated Kernel named Toro. Matias Vara. FOSDEM 2015.&lt;/p&gt;
    &lt;p&gt;[1] Reducing CPU usage of a Toro Appliance. Matias Vara. FOSDEM 2018.&lt;/p&gt;
    &lt;p&gt;[2] Toro, a Dedicated Kernel for Microservices. Matias Vara and Cesar Bernardini. Open Source Summit Europe 2018.&lt;/p&gt;
    &lt;p&gt;[3] Speeding Up the Booting Time of a Toro Appliance. Matias Vara. FOSDEM 2019.&lt;/p&gt;
    &lt;p&gt;[4] Developing and Deploying Microservices with Toro Unikernel. Matias Vara. Open Source Summit Europe 2019.&lt;/p&gt;
    &lt;p&gt;[5] Leveraging Virtio-fs and Virtio-vsocket in Toro Unikernel. Matias Vara. DevConfCZ 2020.&lt;/p&gt;
    &lt;p&gt;[6] Building a Cloud Infrastructure to Deploy Microservices as Microvm Guests. Matias Vara. KVM Forum 2020.&lt;/p&gt;
    &lt;p&gt;[7] Running MPI applications on Toro unikernel. Matias Vara. FOSDEM 2023.&lt;/p&gt;
    &lt;p&gt;[8] Is Toro unikernel faster for MPI?. Matias Vara. FOSDEM 2024.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46435418</guid><pubDate>Tue, 30 Dec 2025 17:09:57 +0000</pubDate></item><item><title>A Vulnerability in Libsodium</title><link>https://00f.net/2025/12/30/libsodium-vulnerability/</link><description>&lt;doc fingerprint="1c17b053a8636291"&gt;
  &lt;main&gt;
    &lt;p&gt;Libsodium is now 13 years old!&lt;/p&gt;
    &lt;p&gt;I started that project to pursue Dan Bernsteinâs desire to make cryptography simple to use. That meant exposing a limited set of high-level functions and parameters, providing a simple API, and writing documentation for users, not cryptographers. Libsodiumâs goal was to expose APIs to perform operations, not low-level functions. Users shouldnât even have to know or care about what algorithms are used internally. This is how Iâve always viewed libsodium.&lt;/p&gt;
    &lt;p&gt;Never breaking the APIs is also something Iâm obsessed with. APIs may not be great, and if I could start over from scratch, I would have made them very different, but as a developer, the best APIs are not the most beautifully designed ones, but the ones that you donât have to worry about because they donât change and upgrades donât require any changes in your application either. Libsodium started from the NaCl API, and still adheres to it.&lt;/p&gt;
    &lt;p&gt;These APIs exposed high-level functions, but also some lower-level functions that high-level functions wrap or depend on. Over the years, people started using these low-level functions directly. Libsodium started to be used as a toolkit of algorithms and low-level primitives.&lt;/p&gt;
    &lt;p&gt;That made me sad, especially since it is clearly documented that only APIs from builds with &lt;code&gt;--enable-minimal&lt;/code&gt; are guaranteed to be tested and stable. But after all, it makes sense. When building custom protocols, having a single portable library with a consistent interface for different functions is far better than importing multiple dependencies, each with their own APIs and sometimes incompatibilities between them.&lt;/p&gt;
    &lt;p&gt;Thatâs a lot of code to maintain. It includes features and target platforms I donât use but try to support for the community. I also maintain a large number of other open source projects.&lt;/p&gt;
    &lt;p&gt;Still, the security track record of libsodium is pretty good, with zero CVEs in 13 years even though it has gotten a lot of scrutiny.&lt;/p&gt;
    &lt;p&gt;However, while recently experimenting with adding support for batch signatures, I noticed inconsistent results with code originally written in Zig. The culprit was a check that was present in a function in Zig, but that I forgot to add in libsodium.&lt;/p&gt;
    &lt;head rend="h2"&gt;The bug&lt;/head&gt;
    &lt;p&gt;The function &lt;code&gt;crypto_core_ed25519_is_valid_point()&lt;/code&gt;, a low-level function used to check if a given elliptic curve point is valid, was supposed to reject points that arenât in the main cryptographic group, but some points were slipping through.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this matter?&lt;/head&gt;
    &lt;p&gt;Edwards25519 is like a special mathematical playground where cryptographic operations happen.&lt;/p&gt;
    &lt;p&gt;It is used internally for Ed25519 signatures, and includes multiple subgroups of different sizes (order):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Order 1: just the identity (0, 1)&lt;/item&gt;
      &lt;item&gt;Order 2: identity + point (0, -1)&lt;/item&gt;
      &lt;item&gt;Order 4: 4 points&lt;/item&gt;
      &lt;item&gt;Order 8: 8 points&lt;/item&gt;
      &lt;item&gt;Order L: the âmain subgroupâ (L = ~2^252 points) where all operations are expected to happen&lt;/item&gt;
      &lt;item&gt;Order 2L, 4L, 8L: very large, but not prime order subgroups&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The validation function was designed to reject points not in the main subgroup. It properly rejected points in the small-order subgroups, but not points in the mixed-order subgroups.&lt;/p&gt;
    &lt;head rend="h2"&gt;What went wrong technically?&lt;/head&gt;
    &lt;p&gt;To check if a point is in the main subgroup (the one of order L), the function multiplies it by L. If the order is L, multiplying any point by L gives the identity point (the mathematical equivalent of zero). So, the code does the multiplication and checks that we ended up with the identity point.&lt;/p&gt;
    &lt;p&gt;Points are represented by coordinates. In the internal representation used here, there are three coordinates: X, Y, and Z. The identity point is represented internally with coordinates where X = 0 and Y = Z. Z can be anything depending on previous operations; it doesnât have to be 1.&lt;/p&gt;
    &lt;p&gt;The old code only checked X = 0. It forgot to verify Y = Z. This meant some invalid points (where X = 0 but Y â Z after the multiplication) were incorrectly accepted as valid.&lt;/p&gt;
    &lt;p&gt;Concretely: take any main-subgroup point Q (for example, the output of &lt;code&gt;crypto_core_ed25519_random&lt;/code&gt;) and add the order-2 point (0, -1), or equivalently negate both coordinates. Every such Q + (0, -1) would have passed validation before the fix, even though itâs not in the main subgroup.&lt;/p&gt;
    &lt;head rend="h2"&gt;The fix&lt;/head&gt;
    &lt;p&gt;The fix is trivial and adds the missing check:&lt;/p&gt;
    &lt;code&gt;// OLD:
return fe25519_iszero(pl.X);
&lt;/code&gt;
    &lt;code&gt;// NEW:
fe25519_sub(t, pl.Y, pl.Z);
return fe25519_iszero(pl.X) &amp;amp; fe25519_iszero(t);
&lt;/code&gt;
    &lt;p&gt;Now it properly verifies both conditions: X must be zero and Y must equal Z.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who is affected?&lt;/head&gt;
    &lt;p&gt;You may be affected if you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a point release &amp;lt;= &lt;code&gt;1.0.20&lt;/code&gt;or a version of&lt;code&gt;libsodium&lt;/code&gt;released before December 30, 2025.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;crypto_core_ed25519_is_valid_point()&lt;/code&gt;to validate points from untrusted sources&lt;/item&gt;
      &lt;item&gt;Implement custom cryptography using arithmetic over the Edwards25519 curve&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But donât panic. Most users are not affected.&lt;/p&gt;
    &lt;p&gt;None of the high-level APIs (&lt;code&gt;crypto_sign_*&lt;/code&gt;) are affected; they donât even use or need that function. Scalar multiplication using &lt;code&gt;crypto_scalarmult_ed25519&lt;/code&gt; wonât leak anything even if the public key is not on the main subgroup. And public keys created with the regular &lt;code&gt;crypto_sign_keypair&lt;/code&gt; and &lt;code&gt;crypto_sign_seed_keypair&lt;/code&gt; functions are guaranteed to be on the correct subgroup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommendation&lt;/head&gt;
    &lt;p&gt;Support for the Ristretto255 group was added to libsodium in 2019 specifically to solve cofactor-related issues. With Ristretto255, if a point decodes, itâs safe. No further validation is required.&lt;/p&gt;
    &lt;p&gt;If you implement custom cryptographic schemes doing arithmetic over a finite field group, using Ristretto255 is recommended. Itâs easier to use, and as a bonus, low-level operations will run faster than over Edwards25519.&lt;/p&gt;
    &lt;p&gt;If you canât update libsodium and need an application-level workaround, use the following function:&lt;/p&gt;
    &lt;code&gt;int is_on_main_subgroup(const unsigned char p[crypto_core_ed25519_BYTES])
{
    /* l - 1 (group order minus 1) */
    static const unsigned char L_1[crypto_core_ed25519_SCALARBYTES] = {
        0xec, 0xd3, 0xf5, 0x5c, 0x1a, 0x63, 0x12, 0x58,
        0xd6, 0x9c, 0xf7, 0xa2, 0xde, 0xf9, 0xde, 0x14,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x10
    };
    /* Identity point encoding: (x=0, y=1) */
    static const unsigned char ID[crypto_core_ed25519_BYTES] = {
        0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
    };
    unsigned char t[crypto_core_ed25519_BYTES];
    unsigned char r[crypto_core_ed25519_BYTES];
    if (crypto_scalarmult_ed25519_noclamp(t, L_1, p) != 0 ||
        crypto_core_ed25519_add(r, t, p) != 0) {
        return 0;
    }
    return sodium_memcmp(r, ID, sizeof ID) == 0;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Fixed packages&lt;/head&gt;
    &lt;p&gt;This issue was fixed immediately after discovery. All &lt;code&gt;stable&lt;/code&gt; packages released after December 30, 2025 include the fix:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;official tarballs&lt;/item&gt;
      &lt;item&gt;binaries for Visual Studio&lt;/item&gt;
      &lt;item&gt;binaries for MingW&lt;/item&gt;
      &lt;item&gt;NuGet packages for all architectures including Android&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;swift-sodium&lt;/code&gt;xcframework (but&lt;code&gt;swift-sodium&lt;/code&gt;doesnât expose low-level functions anyway)&lt;/item&gt;
      &lt;item&gt;rust &lt;code&gt;libsodium-sys-stable&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;libsodium.js&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A new point release is also going to be tagged.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;libsodium&lt;/code&gt; is useful to you, please keep in mind that it is maintained by one person, for free, in time I could spend with my family or on other projects. The best way to help the project would be to consider sponsoring it, which helps me dedicate more time to improving it and making it great for everyone, for many more years to come.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46435614</guid><pubDate>Tue, 30 Dec 2025 17:24:57 +0000</pubDate></item><item><title>Electrolysis can solve one of our biggest contamination problems</title><link>https://ethz.ch/en/news-and-events/eth-news/news/2025/11/electrolysis-can-solve-one-of-our-biggest-contamination-problems.html</link><description>&lt;doc fingerprint="cb108501eca1c785"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Electrolysis can solve one of our biggest contamination problems&lt;/head&gt;
    &lt;p&gt;ETH Zurich researchers have developed a process that can be used on site to render environmental toxins such as DDT and lindane harmless and convert them into valuable chemicals – a breakthrough for the remediation of contaminated sites and a sustainable circular economy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read&lt;/item&gt;
      &lt;item&gt;Number of comments&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;In brief&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Persistent organic pollutants such as DDT and lindane still pollute the environment and affect humans decades after their use.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ETH researchers have developed a new electrochemical process that completely dehalogenates these long-lived toxins and converts them into valuable industrial chemicals.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The method uses cheap equipment, prevents side reactions and could be used on contaminated landfills, soils or sludge.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mobile systems could be used on site in the future – an important step towards the remediation of contaminated sites and the creation of a sustainable circular economy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They were once considered miracle workers – insecticides such as lindane or DDT were produced and used millions of times during the 20th century. But what was hailed as progress led to a global environmental catastrophe: persistent organic pollutants (POPs) are so chemically stable that they remain in soil, water and organisms for decades. They accumulate in the fatty tissue of animals and thus enter the human food chain. Many of these substances were banned long ago, but their traces can still be found today – even in human blood.&lt;/p&gt;
    &lt;p&gt;How to remediate such contaminated sites, be they soils, bodies of water or landfills, is one of the major unresolved questions of environmental protection. How can highly stable poisons be rendered harmless without creating new problems? Researchers at ETH Zurich, led by Bill Morandi, Professor of Synthetic Organic Chemistry, have now found a promising approach. Using an innovative electrochemical method, they are not only able to break down these long-lived pollutants but also to convert them into valuable raw materials for the chemical industry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Converting pollutants into raw materials&lt;/head&gt;
    &lt;p&gt;A key distinction between this and previous work is that the carbon skeleton of the pollutants is recycled and made reusable, while the halide component is sequestered as a harmless inorganic salt. “The previous methods were also energetically inefficient,” says Patrick Domke, a doctoral student in Morandi’s group. He explains: “The processes were expensive and still led to outcomes that were harmful to the environment.”&lt;/p&gt;
    &lt;p&gt;Together with electrochemistry specialist Alberto Garrido-Castro, a former postdoc in this group, Domke developed a process that renders the pollutants in question completely harmless. During this project, the two researchers were able to draw on the many years of experience of ETH professor Morandi, who has been working on the transformation of such compounds for years. “The key advance of this new technology is the use of alternating current to sequester the problematic halogen atoms as innocuous salts such as NaCl (table salt), while still generating valuable hydrocarbons,” says Morandi.&lt;/p&gt;
    &lt;head rend="h2"&gt;Using electricity to break down toxins&lt;/head&gt;
    &lt;p&gt;Electrolysis enables almost complete dehalogenation of pollutants under mild, environmentally friendly and cost-effective conditions. It cleaves the stable carbon-halogen bonds, leaving behind only harmless salts such as table salt and useful hydrocarbons such as benzene, diphenylethane or cyclododecatriene. These are actually sought-after intermediates in the chemical industry, for example, for plastics, varnishes, coatings and pharmaceutical applications. In this way, the technology not only contributes to the remediation of contaminated sites but also to the sustainable circular economy.&lt;/p&gt;
    &lt;p&gt;“What makes our process so special from a technical point of view is that we supply electricity using alternating current, similar to the electrical waveform delivered to households. It is one of the most cost-effective resources in chemistry,” explains Garrido-Castro. “Alternating current protects the electrodes from wear, which is why we can reuse them for many subsequent electrolysis cycles. In addition, the alternating current suppresses unwanted side reactions and the formation of poisonous chlorine gas, allowing the pollutant’s halogen atoms to be fully converted to inorganic salts.” The reactor used by the researchers consists of an undivided electrolysis cell in which dimethyl sulfoxide (DMSO) is used as a solvent – itself a by-product of the pulp process in paper production.&lt;/p&gt;
    &lt;head rend="h2"&gt;A fully thought-out circular economy&lt;/head&gt;
    &lt;p&gt;The process can be applied not only to pure substances but also to mixtures from contaminated soils. Soil or sludge can therefore be treated without pre-treatment or further separation processes. A prototype of the reactor has already been successfully tested on classic environmental toxins such as lindane and DDT. “Our system is mobile and can be assembled on site. This eliminates the need to transport these hazardous substances,” explains Domke.&lt;/p&gt;
    &lt;quote&gt;“Our motivation was to solve one of the biggest environmental problems of the last century. We cannot simply leave the pollution to future generations.”Alberto Garrido-Castro&lt;/quote&gt;
    &lt;head rend="h2"&gt;Spark Award 2025 – these projects have made it to the finals&lt;/head&gt;
    &lt;p&gt;On 27 November 2025 at ETH Zurich @ Open-i, ETH Zurich will award the Spark Award for the best invention of the year for the 14th time. The criteria for this award are originality, patent strength and market potential.&lt;/p&gt;
    &lt;p&gt;Click here to find all the Spark Award nominees of 2025.&lt;/p&gt;
    &lt;p&gt;Spark Award ceremony, Industry Day @ Open-i, Thursday, 27 November 2025, 1.30 p.m., Zurich Convention Center. Registration is required.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46436127</guid><pubDate>Tue, 30 Dec 2025 18:08:32 +0000</pubDate></item><item><title>A faster heart for F-Droid</title><link>https://f-droid.org/2025/12/30/a-faster-heart-for-f-droid.html</link><description>&lt;doc fingerprint="fdf7b599e0066ed7"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;A faster heart for F-Droid. Our new server is here!&lt;/head&gt;Posted on Dec 30, 2025 by F-Droid&lt;p&gt;Donations are a key part of what keeps F-Droid independent and reliable and our latest hardware update is a direct result of your support. Thanks to donations from our incredible community, F-Droid has replaced one of its most critical pieces of infrastructure, our core server hardware. It was overdue for a refresh, and now we are happy to give you an update on the new server and how it impacts the project.&lt;/p&gt;&lt;p&gt;This upgrade touches a core part of the infrastructure that builds and publishes apps for the main F-Droid repository. If the server is slow, everything downstream gets slower too. If it is healthy, the entire ecosystem benefits.&lt;/p&gt;&lt;head rend="h2"&gt;Why did we wait?&lt;/head&gt;&lt;p&gt;This server replacement took a bit longer than we would have liked. The biggest reason is that sourcing reliable parts right now is genuinely hard. Ongoing global trade tensions have made supply chains unpredictable, and that hit the specific components we needed. We had to wait for quotes, review, replan, and wait again when quotes turned out to have unexpected long waits, before we finally managed to receive hardware that met our requirements.&lt;/p&gt;&lt;p&gt;Even with the delays, the priority never changed. We were looking for the right server set up for F-Droid, built to last for the long haul.&lt;/p&gt;&lt;head rend="h2"&gt;A note about the host&lt;/head&gt;&lt;p&gt;Another important part of this story is where the server lives and how it is managed. F-Droid is not hosted in just any data center where commodity hardware is managed by some unknown staff. We worked out a special arrangement so that this server is physically held by a long time contributor with a proven track record of securely hosting services. We can control it remotely, we know exactly where it is, and we know who has access. That level of transparency and trust is not common in infrastructure, but it is central to how we think about resilience and stewardship.&lt;/p&gt;&lt;p&gt;This was not the easiest path, and it required careful coordination and negotiation. But we are glad we did it this way. It fits our values and our threat model, and it keeps the project grounded in real people rather than anonymous systems.&lt;/p&gt;&lt;head rend="h2"&gt;Old hardware, new momentum&lt;/head&gt;&lt;p&gt;The previous server was 12 year old hardware and had been running for about five years. In infrastructure terms, that is a lifetime. It served F-Droid well, but it was reaching the point where speed and maintenance overhead were becoming a daily burden.&lt;/p&gt;&lt;p&gt;The new system is already showing a huge improvement. Stats of the running cycles from the last two months suggest it can handle the full build and publish actions much faster than before. E.g. this year, between January and September, we published updates once every 3 or 4 days, that got down to once every 2 days in October, to every day in November and itâs reaching twice a day in December. (You can see this in the frequency of index publishing after October 18, 2025 in our f-droid.org transparency log). That extra capacity gives us more breathing room and helps shorten the gap between when apps are updated and when those updates reach users. We can now build all the auto-updated apps in the (UTC) morning in one cycle, and all the newly included apps, fixed apps and manually updated apps, through the day, in the evening cycle.&lt;/p&gt;&lt;p&gt;We are being careful here, because real world infrastructure always comes with surprises. But the performance gains are real, and they are exciting.&lt;/p&gt;&lt;head rend="h2"&gt;What donations make possible&lt;/head&gt;&lt;p&gt;This upgrade exists because of community support, pooled over time, turned into real infrastructure, benefiting everyone who relies on F-Droid.&lt;/p&gt;&lt;p&gt;A faster server does not just make our lives easier. It helps developers get timely builds. It reduces maintenance risk. It strengthens the health of the entire repository.&lt;/p&gt;&lt;p&gt;So thank you. Every donation, whether large or small, is part of how this project stays reliable, independent, and aligned with free software values.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46436409</guid><pubDate>Tue, 30 Dec 2025 18:36:37 +0000</pubDate></item><item><title>Escaping containment: A security analysis of FreeBSD jails [video]</title><link>https://media.ccc.de/v/39c3-escaping-containment-a-security-analysis-of-freebsd-jails</link><description>&lt;doc fingerprint="1512f8d2f0f5cfeb"&gt;
  &lt;main&gt;
    &lt;p&gt;ilja and Michael Smith&lt;/p&gt;
    &lt;p&gt;FreeBSD’s jail mechanism promises strong isolation—but how strong is it really? &lt;lb/&gt;In this talk, we explore what it takes to escape a compromised FreeBSD jail by auditing the kernel’s attack surface, identifying dozens of vulnerabilities across exposed subsystems, and developing practical proof-of-concept exploits. We’ll share our findings, demo some real escapes, and discuss what they reveal about the challenges of maintaining robust OS isolation.&lt;/p&gt;
    &lt;p&gt;FreeBSD’s jail feature is one of the oldest and most mature OS-level isolation mechanisms in use today, powering hosting environments, container frameworks, and security sandboxes. But as with any large and evolving kernel feature, complexity breeds opportunity. This research asks a simple but critical question: If an attacker compromises root inside a FreeBSD jail, what does it take to break out?&lt;/p&gt;
    &lt;p&gt;To answer that, we conducted a large-scale audit of FreeBSD kernel code paths accessible from within a jail. We systematically examined privileged operations, capabilities, and interfaces that a jailed process can still reach, hunting for memory safety issues, race conditions, and logic flaws. The result: roughly 50 distinct issues uncovered across multiple kernel subsystems, ranging from buffer overflows and information leaks to unbounded allocations and reference counting errors—many of which could crash the system or provide vectors for privilege escalation beyond the jail.&lt;/p&gt;
    &lt;p&gt;We’ve developed proof-of-concept exploits and tools to demonstrate some of these vulnerabilities in action. We’ve responsibly disclosed our findings to the FreeBSD security team and are collaborating with them on fixes. Our goal isn’t to break FreeBSD, but to highlight the systemic difficulty of maintaining strict isolation in a large, mature codebase.&lt;/p&gt;
    &lt;p&gt;This talk will present our methodology, tooling, and selected demos of real jail escapes. We’ll close with observations about kernel isolation boundaries, lessons learned for other OS container systems, and a call to action for hardening FreeBSD’s jail subsystem against the next generation of threats.&lt;/p&gt;
    &lt;p&gt;Licensed to the public under http://creativecommons.org/licenses/by/4.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Download&lt;/head&gt;
    &lt;head rend="h4"&gt;Video&lt;/head&gt;
    &lt;head rend="h4"&gt;These files contain multiple languages.&lt;/head&gt;
    &lt;p&gt;This Talk was translated into multiple languages. The files available for download contain all languages as separate audio-tracks. Most desktop video players allow you to choose between them.&lt;/p&gt;
    &lt;p&gt;Please look for "audio tracks" in your desktop video player.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46436828</guid><pubDate>Tue, 30 Dec 2025 19:15:05 +0000</pubDate></item><item><title>FediMeteo: A €4 FreeBSD VPS Became a Global Weather Service</title><link>https://it-notes.dragas.net/2025/02/26/fedimeteo-how-a-tiny-freebsd-vps-became-a-global-weather-service-for-thousands/</link><description>&lt;doc fingerprint="b20d9871206db6ab"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Personal Introduction&lt;/head&gt;
    &lt;p&gt;Weather has always significantly influenced my life. When I was a young athlete, knowing the forecast in advance would have allowed me to better plan my training sessions. As I grew older, I could choose whether to go to school on my motorcycle or, for safety reasons, have my grandfather drive me. And it was him, my grandfather, who was my go-to meteorologist. He followed all weather patterns and forecasts, a remnant of his childhood in the countryside and his life on the move. It's to him that I dedicate FediMeteo.&lt;/p&gt;
    &lt;p&gt;The idea for FediMeteo started almost by chance while I was checking the holiday weather forecast to plan an outing. Suddenly, I thought how nice it would be to receive regular weather updates for my city directly in my timeline. After reflecting for a few minutes, I registered a domain and started planning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Principles&lt;/head&gt;
    &lt;p&gt;The choice of operating system was almost automatic. The idea was to separate instances by country, and FreeBSD jails are one of the most useful tools for this purpose.&lt;/p&gt;
    &lt;p&gt;I initially thought the project would generate little interest. I was wrong. After all, weather affects many of our lives, directly or indirectly. So I decided to structure everything in this way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I would use a test VPS to see how things would go. The VPS was a small VM on a German provider with 4 shared cores, 4GB of RAM, 120GB of SSD disk space, and a 1Gbit/sec internet connection and now is a 4 euro per month VPS in Milano, Italy - 4 shared cores, 8 GB RAM and 75GB disk space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would separate various countries into different instances, for both management and security reasons, as well as to have the possibility of relocating just some of them if needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weather data would come from a reliable and open-source friendly source. I narrowed it down to two options: wttr.in and Open-Meteo, two solutions I know and that have always given me reliable results.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would pay close attention to accessibility: forecasts would be in local languages, consultable via text browsers, with emojis to give an idea even to those who don't speak local languages, and everything would be accessible without JavaScript or other requirements. One's mother tongue is always more "familiar" than a second language, even if you're fluent.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would manage everything according to Unix philosophy: small pieces working together. The more years pass, the more I understand how valuable this approach is.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The software chosen to manage the instances is snac. Snac embodies my philosophy of minimal and effective software, perfect for this purpose. It provides clear web pages for those who want to consult via the web, "speaks" the ActivityPub protocol perfectly, produces RSS feeds for each user (i.e., city), has extremely low RAM and CPU consumption, compiles in seconds, and is stable. The developer is an extremely helpful and positive person, and in my opinion, this carries equal weight as everything else.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would do it for myself. If there was no interest, I would have kept it running anyway, without expanding it. So no anxiety or fear of failure.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Technical Implementation&lt;/head&gt;
    &lt;p&gt;I started setting up the first "pieces" during the days around Christmas 2024. The scheme was clear: each jail would handle everything internally. A Python script would download data, city by city, and produce markdown. The city coordinates would be calculated via the geopy library and passed to wttr.in and Open-Meteo. No data would be stored locally. This approach gives the ability to process all cities together. Just pass the city and country to the script, and the markdown would be served. At that point, snac comes into play: without the need to use external utilities, the "snac note" command allows posting from stdin by specifying the instance directory and the user to post from. No need to make API calls with external utilities, having to manage API keys, permissions, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Setting Up for Italy&lt;/head&gt;
    &lt;p&gt;To simplify things, I first structured the jail for Italy. I made a list of the main cities, normalizing them. For example, La Spezia became la_spezia. ForlÃ¬, with an accent, became forli - this for maximum compatibility since each city would be a snac user. I then created a script that takes this list and creates snac users via "snac adduser." At that point, after creating all the users, the script would modify the JSON of each user to convert the city name to uppercase, insert the bio (a standard text), activate the "bot" flag, and set the avatar, which was the same for all users at the time. This script is also able to add a new city: just run the script with the (normalized) name of the city, and it will add it - also adding it to the "cities.txt" file, so it will be updated in the next weather update cycle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core Application Development&lt;/head&gt;
    &lt;p&gt;I then created the heart of the service. A Python application (initially only in Italian, then multilingual, separating the operational part from the text) able to receive (via command line) the name of a city and a country code (corresponding to the file with texts in the local language). The script determines the coordinates and then, using API calls, requests the current weather conditions, those for the next 12 hours, and the next 7 days. I conducted experiments with both wttr.in and Open-Meteo, and both gave good results. However, I settled on Open-Meteo because, for my uses, it has always provided very reliable results. This application directly provides an output in Markdown since snac supports it, at least partially.&lt;/p&gt;
    &lt;p&gt;The cities.txt file is also crucial for updates. I created a script - post.sh, in pure sh, that scrolls through all cities, and for each one, launches the FediMeteo application and publishes its output using snac directly via command line. Once the job is finished, it makes a call to my instance of Uptime-Kuma, which keeps an eye on the situation. In case of failure, the monitoring will alert me that there have been no recent updates, and I can check.&lt;/p&gt;
    &lt;p&gt;At this point, the system cron takes care of launching post.sh every 6 hours. The requests are serialized, so the cities will update one at a time, and the posts will be sent to followers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Growth and Unexpected Success&lt;/head&gt;
    &lt;p&gt;After listing all Italian provincial capitals, I started testing everything. It worked perfectly. Of course, I had to make some adjustments at all levels. For example, one of the problems encountered was that snac did not set the language of the posts, and some users could have missed them. The developer was very quick and, as soon as I exposed the problem, immediately modified the program so that the post could keep the system language, set as an environment variable in the sh script.&lt;/p&gt;
    &lt;p&gt;After two days, I decided to start adding other countries and announce the project. And the announcement was unexpectedly well received: there were many boosts, and people started asking me to add their cities or countries. I tried to do what I could, within the limits of my physical condition, as in those days, I had the flu that kept me at home with a fever and illness for several days. I started adding many countries in the heart of Europe, translating the main indications into local languages but maintaining emojis so that everything would be understandable even to those who don't speak the local language. There were some small problems reported by some users. One of them: not all weather conditions had been translated, so sometimes they appeared in Italian - as well as errors. In bilingual countries, I tried to include all local languages. Sometimes, unfortunately, making mistakes as I encountered dynamics unknown to me or difficult to interpret. For example, in Ireland, forecasts were published in Irish, but it was pointed out to me that not everyone speaks it, so I modified and published in English.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Turning Point&lt;/head&gt;
    &lt;p&gt;The turning point was when FediFollows (@FediFollows@social.growyourown.services - who also manages the site Fedi Directory) started publishing the list of countries and cities, highlighting the project. Many people became aware of FediMeteo and started following the various accounts, the various cities. And from here came requests to add new countries and some new information, such as wind speed. Moreover, I was asked (rightly, to avoid flooding timelines) to publish posts as unlisted - this way, followers would see the posts, but they wouldn't fill local timelines. Snac didn't support this, but again, the snac dev came to my rescue in a few hours.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scaling Challenges&lt;/head&gt;
    &lt;p&gt;But with new countries came new challenges. For example, in my original implementation, all units of measurement were in metric/decimal/Celsius - and this doesn't adapt well to realities like the USA. Moreover, focusing on Europe, almost all countries were located in a single timezone, while for larger countries (such as Australia, USA, Canada, etc.), this is totally different. So I started developing a more complete and global version and, in the meantime, added almost all of Europe. The new version would have to be backward compatible, would have to take into account timezone differences for each city, different measurements (e.g., degrees C and F), as well as, initially more difficult part, being able to separate cities with the same name based on states or provinces. I had already seen a similar problem with the implementation of support for Germany, so it had to be addressed properly.&lt;/p&gt;
    &lt;p&gt;The original goal was to have a VPS for each continent, but I soon realized that thanks to the quality of snac's code and FreeBSD's efficient management, even keeping countries in separate jails, the load didn't increase much. So I decided to challenge myself and the limits of the economical 4 euros per month VPS. That is, to insert as much as possible until seeing what the limits were. Limits that, to date, I have not yet reached. I would also soon exhaust the available API calls for Open-Meteo's free accounts, so I tried to contact the team and explain everything. I was positively surprised to read that they appreciated the project and provided me with a dedicated API key.&lt;/p&gt;
    &lt;p&gt;Compatible with my free time, I managed to complete the richer and more complete version of my Python program. I'm not a professional dev, I'm more oriented towards systems, so the code is probably quite poor in the eyes of an expert dev. But, in the end, it just needs to take an input and give me an output. It's not a daemon, it's not a service that responds on the network. For that, snac takes care of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expansion to North America&lt;/head&gt;
    &lt;p&gt;So I decided to start with a very important launch: the USA and Canada. A non-trivial part was identifying the main cities in order to cover, state by state, all the territory. In the end, I identified more than 1200 cities. A number that, by itself, exceeded the sum of all other countries (at that time). And the program, now, is able to take an input with a separator (two underscores: __) between city and state. In this way, it's possible to perfectly understand the differences between city and state: new_york__new_york is an example I like to make, but there are many.&lt;/p&gt;
    &lt;p&gt;The launch of the USA was interesting: despite having had many previous requests, the reception was initially quite lukewarm, to my extreme surprise. The number of followers in Canada, in a few hours, far exceeded that of the USA. On the contrary, the country with the most followers (in a few days, more than 1000) was Germany. Followed by the UK - which I expected would have been the first.&lt;/p&gt;
    &lt;head rend="h2"&gt;System Performance&lt;/head&gt;
    &lt;p&gt;The VPS held up well. Except for the moments when FediFollows launched (after fixing some FreeBSD tuning, the service slowed slightly but didn't crash), the load remained extremely low. So I continued to expand: Japan, Australia, New Zealand, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Current Status&lt;/head&gt;
    &lt;p&gt;At the time of the last update of this article (30 December 2025), the supported countries are 38: Argentina, Australia, Austria, Belgium, Brazil, Bulgaria, Canada, Croatia, Czechia, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, India, Ireland, Italy, Japan, Latvia, Lithuania, Malta, Mexico, Netherlands, New Zealand, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, Switzerland, Taiwan, the United Kingdom, and the United States of America (with more regions coming soon!).&lt;/p&gt;
    &lt;p&gt;Direct followers in the Fediverse are around 7,707 and growing daily, excluding those who follow hashtags or cities via RSS, whose number I can't estimate. However, a quick look at the logs suggests there are many more.&lt;/p&gt;
    &lt;p&gt;The cities currently covered are 2937 - growing based on new countries and requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges Encountered&lt;/head&gt;
    &lt;p&gt;There have been some problems. The most serious, by my fault, was the API key leak: I had left a debug code active and, the first time Open-Meteo had problems, the error message also included the API call - including the API key. Some users reported it to me (others just mocked) and I fixed the code and immediately reported everything to the Open-Meteo team, who kindly gave me a new API Key and deactivated the old one.&lt;/p&gt;
    &lt;p&gt;A further problem was related to geopy. It makes a call to Nominatim to determine coordinates. One of the times Nominatim didn't respond, my program wasn't able to determine the position and went into error. I solved this by introducing coordinate caching: now the program, the first time it encounters a city, requests and saves the coordinates. If present, they will be used in the future without making a new request via geopy. This is both lighter on their servers and faster and safer for us.&lt;/p&gt;
    &lt;head rend="h2"&gt;Infrastructure Details&lt;/head&gt;
    &lt;p&gt;And the VPS? It has no problems and is surprisingly fast and effective. FreeBSD 14.3-RELEASE, BastilleBSD to manage the jails. Currently, there are 39 jails - one for haproxy, the FediMeteo website, so nginx, and the snac instance for FediMeteo announcements and support - the other 38 for the individual instances. Each of them, therefore, has its autonomous ZFS dataset. Every 15 minutes, there is a local snapshot of all datasets. Every hour, the homepage is regenerated: a small script calculates the number of followers (counting, instance by instance, the followers of individual cities, since I don't publish except in aggregate to avoid possible triangulations and privacy leaks of users). Every hour, moreover, an external backup is made via zfs-autobackup (on encrypted at rest dataset), and once a day, a further backup is made in my datacenter, on disks encrypted with geli. The occupied RAM is 501 MB (yes, exactly: 501 MB), which rises slightly when updates are in progress. Updates normally occur every 6 hours. I have tried, as much as possible, to space them out to avoid overloads in timelines (or on the server itself). Only for the USA, I added a sleep of 5 seconds between one city and another, to give snac the opportunity to better organize the sending of messages. It probably wouldn't be necessary, with the current numbers, but better safe than sorry. In this way, the USA is processed in about 2 and a half hours, but the other jails (thus countries) can work autonomously and send their updates.&lt;/p&gt;
    &lt;p&gt;The average load of the VPS (taking as reference both the last 24 hours and the last two weeks) is about 25%, as it rises to 70/75% when updates occur for larger instances (such as the USA), or when it is announced by FediFollows. Otherwise, it is on average less than 10%. So, the VPS still has huge margin, and new instances, with new nations, will still be inside it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This article, although in some parts very conversational, aims to demonstrate how it's possible to build solid, valid, and efficient solutions without the need to use expensive and complex services. Moreover, this is the demonstration of how it's possible to have your online presence without the need to put your data in the hands of third parties or without necessarily having to resort to complex stacks. Sometimes, less is more.&lt;/p&gt;
    &lt;p&gt;The success of this project demonstrates, once again, that my grandfather was right: weather forecasts interest everyone. He worried about my health and, thanks to his concerns, we spent time together. In the same way, I see many followers and friends talking to me or among themselves about the weather, their experiences, what happens. Again, in my life, weather forecasts have helped sociality and socialization.&lt;/p&gt;
    &lt;p&gt;Thank you, Grandpa.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46436889</guid><pubDate>Tue, 30 Dec 2025 19:21:48 +0000</pubDate></item><item><title>Zpdf: PDF text extraction in Zig</title><link>https://github.com/Lulzx/zpdf</link><description>&lt;doc fingerprint="64071221de9d50da"&gt;
  &lt;main&gt;
    &lt;p&gt;A PDF text extraction library written in Zig.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memory-mapped file reading for efficient large file handling&lt;/item&gt;
      &lt;item&gt;Streaming text extraction with efficient arena allocation&lt;/item&gt;
      &lt;item&gt;Multiple decompression filters: FlateDecode, ASCII85, ASCIIHex, LZW, RunLength&lt;/item&gt;
      &lt;item&gt;Font encoding support: WinAnsi, MacRoman, ToUnicode CMap&lt;/item&gt;
      &lt;item&gt;XRef table and stream parsing (PDF 1.5+)&lt;/item&gt;
      &lt;item&gt;Configurable error handling (strict or permissive)&lt;/item&gt;
      &lt;item&gt;Multi-threaded parallel page extraction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Text extraction performance on Apple M4 Pro (parallel, stream order):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Document&lt;/cell&gt;
        &lt;cell role="head"&gt;Pages&lt;/cell&gt;
        &lt;cell role="head"&gt;zpdf&lt;/cell&gt;
        &lt;cell role="head"&gt;pdfium&lt;/cell&gt;
        &lt;cell role="head"&gt;MuPDF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Intel SDM&lt;/cell&gt;
        &lt;cell&gt;5,252&lt;/cell&gt;
        &lt;cell&gt;227ms&lt;/cell&gt;
        &lt;cell&gt;3,632ms&lt;/cell&gt;
        &lt;cell&gt;2,331ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Pandas Docs&lt;/cell&gt;
        &lt;cell&gt;3,743&lt;/cell&gt;
        &lt;cell&gt;762ms&lt;/cell&gt;
        &lt;cell&gt;2,379ms&lt;/cell&gt;
        &lt;cell&gt;1,237ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;C++ Standard&lt;/cell&gt;
        &lt;cell&gt;2,134&lt;/cell&gt;
        &lt;cell&gt;671ms&lt;/cell&gt;
        &lt;cell&gt;1,964ms&lt;/cell&gt;
        &lt;cell&gt;1,079ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Acrobat Reference&lt;/cell&gt;
        &lt;cell&gt;651&lt;/cell&gt;
        &lt;cell&gt;120ms&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;US Constitution&lt;/cell&gt;
        &lt;cell&gt;85&lt;/cell&gt;
        &lt;cell&gt;24ms&lt;/cell&gt;
        &lt;cell&gt;63ms&lt;/cell&gt;
        &lt;cell&gt;58ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Lower is better. Build with &lt;code&gt;zig build -Doptimize=ReleaseFast&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Peak throughput: 23,137 pages/sec (Intel SDM)&lt;/p&gt;
    &lt;p&gt;All tools achieve ~99%+ character accuracy vs MuPDF reference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Char Accuracy&lt;/cell&gt;
        &lt;cell role="head"&gt;WER&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;zpdf&lt;/cell&gt;
        &lt;cell&gt;99.3-99.9%&lt;/cell&gt;
        &lt;cell&gt;1-8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;pdfium&lt;/cell&gt;
        &lt;cell&gt;99.2-100%&lt;/cell&gt;
        &lt;cell&gt;0-4%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MuPDF&lt;/cell&gt;
        &lt;cell&gt;100% (ref)&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Build with &lt;code&gt;zig build -Doptimize=ReleaseFast&lt;/code&gt; for best performance.&lt;/p&gt;
    &lt;p&gt;Run &lt;code&gt;PYTHONPATH=python python benchmark/accuracy.py&lt;/code&gt; to reproduce (requires &lt;code&gt;pypdfium2&lt;/code&gt;).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.15.2 or later&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;zig build              # Build library and CLI
zig build test         # Run tests&lt;/code&gt;
    &lt;code&gt;const zpdf = @import("zpdf");

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const doc = try zpdf.Document.open(allocator, "file.pdf");
    defer doc.close();

    var buf: [4096]u8 = undefined;
    var writer = std.fs.File.stdout().writer(&amp;amp;buf);
    defer writer.interface.flush() catch {};

    for (0..doc.pages.items.len) |page_num| {
        try doc.extractText(page_num, &amp;amp;writer.interface);
    }
}&lt;/code&gt;
    &lt;code&gt;zpdf extract document.pdf              # Extract all pages to stdout
zpdf extract -p 1-10 document.pdf      # Extract pages 1-10
zpdf extract -o out.txt document.pdf   # Output to file
zpdf extract --reading-order doc.pdf   # Use visual reading order (experimental)
zpdf info document.pdf                 # Show document info
zpdf bench document.pdf                # Run benchmark&lt;/code&gt;
    &lt;code&gt;import zpdf

with zpdf.Document("file.pdf") as doc:
    print(doc.page_count)

    # Single page
    text = doc.extract_page(0)

    # All pages (parallel by default)
    all_text = doc.extract_all()

    # Reading order extraction (experimental)
    ordered_text = doc.extract_all(reading_order=True)

    # Page info
    info = doc.get_page_info(0)
    print(f"{info.width}x{info.height}")&lt;/code&gt;
    &lt;p&gt;Build the shared library first:&lt;/p&gt;
    &lt;code&gt;zig build -Doptimize=ReleaseFast
PYTHONPATH=python python3 examples/basic.py&lt;/code&gt;
    &lt;code&gt;src/
├── root.zig         # Document API and core types
├── capi.zig         # C ABI exports for FFI
├── parser.zig       # PDF object parser
├── xref.zig         # XRef table/stream parsing
├── pagetree.zig     # Page tree resolution
├── decompress.zig   # Stream decompression filters
├── encoding.zig     # Font encoding and CMap parsing
├── interpreter.zig  # Content stream interpreter
├── simd.zig         # SIMD string operations
└── main.zig         # CLI

python/zpdf/         # Python bindings (cffi)
examples/            # Usage examples
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;zpdf&lt;/cell&gt;
        &lt;cell role="head"&gt;pdfium&lt;/cell&gt;
        &lt;cell role="head"&gt;MuPDF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Text Extraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Stream order&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Reading order&lt;/cell&gt;
        &lt;cell&gt;Experimental&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Word bounding boxes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Font Support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;WinAnsi/MacRoman&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ToUnicode CMap&lt;/cell&gt;
        &lt;cell&gt;Partial*&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CID fonts (Type0)&lt;/cell&gt;
        &lt;cell&gt;Partial*&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;FlateDecode, LZW, ASCII85/Hex&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;JBIG2, JPEG2000&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Other&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Encrypted PDFs&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Rendering&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Multi-threaded&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No**&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;*ToUnicode/CID: Works when CMap is embedded directly. **pdfium requires multi-process for parallelism (forked before thread support).&lt;/p&gt;
    &lt;p&gt;Use zpdf when: Batch processing, simple text extraction, Zig integration.&lt;/p&gt;
    &lt;p&gt;Use pdfium when: Browser integration, full PDF support, proven stability.&lt;/p&gt;
    &lt;p&gt;Use MuPDF when: Reading order matters, complex layouts, rendering needed.&lt;/p&gt;
    &lt;p&gt;WTFPL&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46437288</guid><pubDate>Tue, 30 Dec 2025 19:57:10 +0000</pubDate></item><item><title>Professional software developers don't vibe, they control</title><link>https://arxiv.org/abs/2512.14012</link><description>&lt;doc fingerprint="8ddd161bc8db90cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Software Engineering&lt;/head&gt;&lt;p&gt; [Submitted on 16 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.SE&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46437391</guid><pubDate>Tue, 30 Dec 2025 20:06:46 +0000</pubDate></item><item><title>Sabotaging Bitcoin</title><link>https://blog.dshr.org/2025/12/sabotaging-bitcoin.html</link><description>&lt;doc fingerprint="37e79084a8231e31"&gt;
  &lt;main&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;In 2024 Soroush Farokhnia &amp;amp; Amir Kafshdar Goharshady published Options and Futures Imperil Bitcoin's Security and:&lt;/p&gt;&lt;quote&gt;showed that (i) a successful block-reverting attack does not necessarily require ... a majority of the hash power; (ii) obtaining a majority of the hash power ... costs roughly 6.77 billion ... and (iii) Bitcoin derivatives, i.e. options and futures, imperil Bitcoin’s security by creating an incentive for a block-reverting/majority attack.&lt;/quote&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;quote&gt;90% of transaction volume on the Bitcoin blockchain is not tied to economically meaningful activities but is the byproduct of the Bitcoin protocol design as well as the preference of many participants for anonymity ... exchanges play a central role in the Bitcoin system. They explain 75% of real Bitcoin volume.Of course, just because they aren't "economically meaningful" doesn't mean they aren't worth attacking! The average block has ~3.2K transactions, so ~$121.6M/block. As a check. $121.6M * 144 block/day = $17.5B. So to recover their cost for a 51% attack would require double-spending about 8 hours worth of transactions.&lt;/quote&gt;&lt;p&gt;I agree with their technical analysis of the attack, but I believe there would be significant difficulties in putting it into practice. Below the fold I try to set out these difficulties.&lt;/p&gt;&lt;p&gt; brevity is for the weak&lt;lb/&gt;Maciej Cegłowski&lt;/p&gt;&lt;p&gt;Maciej Cegłowski&lt;/p&gt;&lt;p&gt;First, I should point out that I wrote about using derivatives to profit from manipulating Bitcoin's price more than three years ago in Pump-and-Dump Schemes. These schemes have a long history in cryptocurrencies, but they are not the attack involved here. I don't claim expertise in derivatives trading, so it is possible my analysis is faulty. If so, please point out the problems in a comment.&lt;/p&gt;&lt;head rend="h3"&gt;The Attack&lt;/head&gt;Farokhnia &amp;amp; Goharshady build on the 2018 work of Ittay Eyal &amp;amp; Emin Gün Sirer in Majority is not enough: Bitcoin mining is vulnerable:&lt;quote&gt;The key idea behind this strategy, called Selfish Mining, is for a pool to keep its discovered blocks private, thereby intentionally forking the chain. The honest nodes continue to mine on the public chain, while the pool mines on its own private branch. If the pool discovers more blocks, it develops a longer lead on the public chain, and continues to keep these new blocks private. When the public branch approaches the pool's private branch in length, the selfish miners reveal blocks from their private chain to the public.In April 2024 Farokhnia &amp;amp; Goharshady observed that:&lt;lb/&gt;...&lt;lb/&gt;We further show that the Bitcoin mining protocol will never be safe against attacks by a selfish mining pool that commands more than 1/3 of the total mining power of the network. Such a pool will always be able to collect mining rewards that exceed its proportion of mining power, even if it loses every single block race in the network. The resulting bound of 2/3 for the fraction of Bitcoin mining power that needs to follow the honest protocol to ensure that the protocol remains resistant to being gamed is substantially lower than the 50% figure currently assumed, and difficult to achieve in practice.&lt;/quote&gt;&lt;quote&gt;Given that the rule of thumb followed by most practitioners is to wait for 6 confirmations, a fork that goes 6 levels deep can very likely diminish the public’s trust in Bitcoin and cause a crash in its market price. It is also widely accepted that a prolonged majority attack (if it happens) would be catastrophic to the cryptocurrency and can cause its downfall.But, as they lay out, this possibility is discounted:&lt;/quote&gt;&lt;quote&gt;The conventional wisdom in the blockchain community is to assume that such block-reverting attacks are highly unlikely to happen. The reasoning goes as follows:&lt;item&gt;Reverting multiple blocks and specifically double-spending a transaction that has 6 confirmations requires control of a majority of the mining power;&lt;/item&gt;&lt;item&gt;Having a majority of the mining power is prohibitively expensive and requires an outlandish investment in hardware;&lt;/item&gt;&lt;item&gt;Even if a miner, mining pool or group of pools does control a majority of the mining power, they have no incentive to act dishonestly and revert the blockchain, as that would crash the price of Bitcoin, which is ultimately not in their favor, since they rely on mining rewards denominated in BTC for their income.&lt;/item&gt;&lt;/quote&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;These huge futures markets enable Farokhnia &amp;amp; Goharshady's attack:&lt;/p&gt;&lt;quote&gt;In short, an attacker can first use the Bitcoin derivatives market to short Bitcoin by purchasing a sufficient amount of put options or other equivalent financial instruments. She can then invest any of the amounts calculated above, depending on the timeline of the attack, to obtain the necessary hardware and hash power to perform the attack. If the attacker chooses to obtain a majority of the hash power, her success is guaranteed and she can revert the blocks as deeply as she wishes. However, she also has the option of a smaller upfront investment in hardware in exchange for longer wait times to achieve a high probability of success. In any case, as long as her earnings from shorting Bitcoin and then causing an intentional price crash outweighs her investments in hardware, there is a clear financial incentive to perform such an attack. The numbers above show that the annual trade volume in Bitcoin derivatives is more than three orders of magnitude larger than the required investment in hardware. Thus, it is possible and profitable to perform such an attack.&lt;/quote&gt;&lt;head rend="h3"&gt;Assumptions&lt;/head&gt;Farokhnia &amp;amp; Goharshady make some simplifying assumptions:&lt;quote&gt;&lt;list&gt;The justification for the first assumption is that it keeps our analysis sound, i.e. we can only over-approximate the cost by making this assumption. As for the second assumption, we note that electricity costs are often negligible in comparison to hardware costs and that our main argument, i.e. the vulnerability of Bitcoin to majority attacks and block-reverting attacks, remains intact even if the estimates we obtain here are doubled. Indeed, as we will soon see, the trade volume of Bitcoin derivatives is more than three orders of magnitude larger than the numbers obtained here.&lt;/list&gt;&lt;item&gt;We only consider the cost of hardware at the time of writing. We assume the attacker is buying the hardware, rather than renting it and do not consider potential discounts on bulk orders.&lt;/item&gt;&lt;item&gt;We ignore electricity costs as they vary widely based on location.&lt;/item&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Goal&lt;/head&gt;As Farokhnia &amp;amp; Goharshady stress, the success of a block-reverting attack is probabilistic, so the attacker needs to have a high enough probability of making a large enough profit to make up for the risk of failure.&lt;p&gt;My analysis thus assumes that the goal of the attacker is to have a 95% probability of earning at least double the cost of the attack.&lt;/p&gt;&lt;head rend="h3"&gt;Attacker&lt;/head&gt;There are two different kinds of attackers with different sets of difficulties:&lt;list rend="ul"&gt;&lt;item&gt;Outsiders: someone who has to acquire or rent sufficient hash power.&lt;/item&gt;&lt;item&gt;Insiders: someone or some mining pool who already controls sufficient hash power.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;Obtaining and maintaining for the duration of the attack sufficient hash power without detection.&lt;/item&gt;&lt;item&gt;Obtaining and maintaining for the duration of the attack a sufficient short position in Bitcoin without detection.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Hash Power&lt;/head&gt;The outsider's problems are more complex than the insider's.&lt;head rend="h4"&gt;Outsider Attack&lt;/head&gt;The outsider attacker requires three kinds of resource:&lt;list rend="ul"&gt;&lt;item&gt;Mining rigs.&lt;/item&gt;&lt;item&gt;Power to run the rigs.&lt;/item&gt;&lt;item&gt;Data center space to hold the rigs.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h5"&gt;Mining rigs&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Could they acquire mining rigs sufficient to provide 30% of the combined insider and outsider hash power, or ~43% of the pre-attack hash power?&lt;/item&gt;&lt;item&gt;How long would it take to acquire the rigs?&lt;/item&gt;&lt;item&gt;Would their acquisition of the rigs be detected?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Because the economic life of mining rigs is less than two years, the first part of Bitmain's production goes into maintaining the hash rate by replacing obsolete rigs. The second part goes into increasing the hash rate. If we assume that the outsider attacker could absorb the second part of Bitmain's production, how long would it take to get the necessary 43% of the previous hash power?&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The lack of rigs to increase the hash rate over a period of much less than two years would clearly be detectable.&lt;/p&gt;&lt;head rend="h5"&gt;Power&lt;/head&gt;The Cambridge Bitcoin Energy Consumption Index's current estimate is that the network consumes 22GW. The outside attacker would need 43% of this, or about 9.5GW, for the duration of the attack. For context, Meta's extraordinarily aggressive AI data center plans claim to bring a single 1GW data center online in 2026, and the first 2GW phase of their planned $27B 5GW Louisiana data center in 2030. The constraint on the roll-out is largely that lack of access to sufficient power. The attacker would need double the power Meta's Louisiana data center plans to have in 2030.&lt;p&gt;Access to gigawatts of power is available only on long-term contracts and only after significant delays.&lt;/p&gt;&lt;head rend="h5"&gt;Data centers&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Hyperion&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Estimates for AI data centers are that 60% of the capital cost is the hardware and 40% everything else. Thus the "everything else" for Meta's $27B 5GW data center is $10.8B. "Everything else" for the attacker's two similar data centers would thus be $21.6B. Plus say 5 years of interest at 5% or $5.4B.&lt;/p&gt;&lt;head rend="h5"&gt;Operational cost&lt;/head&gt;Ignoring the evident impossibility of the outsider attacker amassing the necessary mining rigs, power and data center space, what would the operational costs of the attack be?&lt;p&gt;It is hard to estimate the costs for power, data center space, etc. But an estimate can be based upon the cost to rent hash power, noting that in practice renting 43% of the total would be impossible, and guessing that renters have a 30% margin. A typical rental fee would be $0.10/TH/day so the costs might be $0.07/TH/day. The attack would have a 95% probability of needing 482EH/s over 34 days or less, so $516M or less.&lt;/p&gt;&lt;p&gt;Thus the estimated total cost for the hash power used in the attack would have a 95% probability of being no more than $7.66B. Plus about $27B in data center cost, which could presumably be repurposed to AI after the attack.&lt;/p&gt;&lt;head rend="h4"&gt;Insider Attack&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The insider's loss of income from the blocks they would otherwise have mined would have a 95% probability of being 4,590 BTC or less, or about $425M.&lt;/p&gt;&lt;head rend="h3"&gt;Short Position&lt;/head&gt;Both kinds of attackers need to ensure that, when the attack succeeds, they have a large enough short position in Bitcoin that would generate their expected return from the attack's decrease in the Bitcoin price. There are two possibilities:&lt;list rend="ul"&gt;&lt;item&gt;When the attacker's chain is within one block of being the longest, they have ten minutes to purchase the shorts. There is unlikely to be enough liquidity in the market to accommodate this sudden demand, which in any case would greatly increase the price of the shorts. I will ignore this possibility in what follows.&lt;/item&gt;&lt;item&gt;At the start of the attack the attacker gradually accumulates sufficient shorts. Even assuming there were enough liquidity, and that the purchases didn't increase the price, the attacker has to bear both the cost of maintaining the shorts for the duration of the attack, and the risk of the market moving up enough to cause the position to be liquidated.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Farokhnia &amp;amp; Goharshady note that:&lt;/p&gt;&lt;quote&gt;At the time of writing, the open interest of BTC options is a bit more than 20 billion USD. Thus, a malicious party performing the attack mentioned in this work would need to obtain a considerable amount of the available put contracts. This may lead to market disruptions whose analysis is beyond the scope of this work. This being said, if the derivatives market continues to grow and becomes much larger than it currently is, purchasing this amount of contracts might not even be detected.There are two different kinds of market in which Bitcoin shorts are available:&lt;/quote&gt;&lt;list rend="ul"&gt;&lt;item&gt;Regulated exchanges such as the CME offering options on Bitcoin and stock exchanges with Bitcoin ETFs and Bitcoin treasury companies such as Strategy.&lt;/item&gt;&lt;item&gt;Unregulated exchanges such as Binance offering "perpetual futures" (perps) on Bitcoin.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Unregulated Exchanges&lt;/head&gt;Patrick McKenzie's Perpetual futures, explained is a clear and comprehensive description of the derivative common on unregulated exchanges:&lt;quote&gt;Instead of all of a particular futures vintage settling on the same day, perps settle multiple times a day for a particular market on a particular exchange. The mechanism for this is the funding rate. At a high level: winners get paid by losers every e.g. 4 hours and then the game continues, unless you’ve been blown out due to becoming overleveraged or for other reasons (discussed in a moment).So the exchange makes money from commissions, and from the spread against the actual spot price. The price of the perp is maintained close to the spot price by the "basis trade", traders providing liquidity by shorting the perp and buying the spot when the perp is above spot, and vice versa. Of course, the spot price itself may have been manipulated, for example by Pump-and-Dump Schemes.&lt;lb/&gt;Consider a toy example: a retail user buys 0.1 Bitcoin via a perp. The price on their screen, which they understand to be for Bitcoin, might be $86,000 each, and so they might pay $8,600 cash. Should the price rise to $90,000 before the next settlement, they will get +/- $400 of winnings credited to their account, and their account will continue to reflect exposure to 0.1 units of Bitcoin via the perp. They might choose to sell their future at this point (or any other). They’ll have paid one commission (and a spread) to buy, one (of each) to sell, and perhaps they’ll leave the casino with their winnings, or perhaps they’ll play another game.&lt;lb/&gt;Where did the money come from? Someone else was symmetrically short exposure to Bitcoin via a perp. It is, with some very important caveats incoming, a closed system: since no good or service is being produced except the speculation, winning money means someone else lost.&lt;/quote&gt;&lt;p&gt;How else does the exchange make money?&lt;/p&gt;&lt;quote&gt;Perp funding rates also embed an interest rate component. This might get quoted as 3 bps a day, or 1 bps every eight hours, or similar. However, because of the impact of leverage, gamblers are paying more than you might expect: at 10X leverage that’s 30 bps a day.A "basis point (bps)" is "one hundredth of 1 percentage point", so 30bps/day is 0.3%/day or around 120%/year. But the lure of leverage is the competitive advantage of unregulated exchanges:&lt;/quote&gt;&lt;quote&gt;In a standard U.S. brokerage account, Regulation T has, for almost 100 years now, set maximum leverage limits (by setting minimums for margins). These are 2X at position opening time and 4X “maintenance” (before one closes out the position). Your brokerage would be obligated to forcibly close your position if volatility causes you to exceed those limits.Unregulated markets are different:&lt;/quote&gt;&lt;quote&gt;Binance allows up to 125x leverage on BTC.Although these huge amounts of leverage greatly increase the reward from a small market movement in favor of the position, they greatly reduce the amount the market has to move against the position before something bad happens. The first bad thing is liquidation:&lt;/quote&gt;&lt;quote&gt;One reason perps are structurally better for exchanges and market makers is that they simplify the business of blowing out leveraged traders. The exact mechanics depend on the exchange, the amount, etc, but generally speaking you can either force the customer to enter a closing trade or you can assign their position to someone willing to bear the risk in return for a discount.The bigger and faster the market move, the more likely the loss exceeds your collateral:&lt;lb/&gt;Blowing out losing traders is lucrative for exchanges except when it catastrophically isn’t. It is a priced service in many places. The price is quoted to be low (“a nominal fee of 0.5%” is one way Binance describes it) but, since it is calculated from the amount at risk, it can be a large portion of the money lost. If the account’s negative balance is less than the liquidation fee, wonderful, thanks for playing and the exchange / “the insurance fund” keeps the rest, as a tip.&lt;/quote&gt;&lt;quote&gt;In the case where the amount an account is negative by is more than the fee, that “insurance fund” can choose to pay the winners on behalf of the liquidated user, at management’s discretion. Management will usually decide to do this, because a casino with a reputation for not paying winners will not long remain a casino.The second bad thing is automatic de-leveraging (ADL):&lt;lb/&gt;But tail risk is a real thing. The capital efficiency has a price: there physically does not exist enough money in the system to pay all winners given sufficiently dramatic price moves. Forced liquidations happen. Sophisticated participants withdraw liquidity (for reasons we’ll soon discuss) or the exchange becomes overwhelmed technically / operationally. The forced liquidations eat through the diminished / unreplenished liquidity in the book, and the magnitude of the move increases.&lt;/quote&gt;&lt;quote&gt;Risk in perps has to be symmetric: if (accounting for leverage) there are 100,000 units of Somecoin exposure long, then there are 100,000 units of Somecoin exposure short. This does not imply that the shorts or longs are sufficiently capitalized to actually pay for all the exposure in all instances.McKenzie illustrates ADL with an example:&lt;lb/&gt;In cases where management deems paying winners from the insurance fund would be too costly and/or impossible, they automatically deleverage some winners.&lt;/quote&gt;&lt;quote&gt;So perhaps you understood, prior to a 20% move, that you were 4X leveraged. You just earned 80%, right? Ah, except you were only 2X leveraged, so you earned 40%. Why were you retroactively only 2X? That’s what automatic deleveraging means. Why couldn’t you get the other 40% you feel entitled to? Because the collective group of losers doesn’t have enough to pay you your winnings and the insurance fund was insufficient or deemed insufficient by management.For our purposes, this is an important note:&lt;/quote&gt;&lt;quote&gt;In theory, this can happen to the upside or the downside. In practice in crypto, this seems to usually happen after sharp decreases in prices, not sharp increases. For example, October 2025 saw widespread ADLing as (more than) $19 billion of liquidations happened, across a variety of assets.How does this affect the outsider attacker? Lets assume that the attack has a 95% probability of costing no more than $7.5B and would reduce the Bitcoin price from $100K to $80K in a single 4-hour period. With 10X leverage this would generate $200K/BTC in gains.&lt;/quote&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The way liquidation of a short works is that as the market moves up, the initial leverage increases. Each exchange will have a limit on the leverage it will allow so, allowing for the liquidation fee, if the leverage of the short position gets to this limit the exchange will liquidate it.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Move %&lt;/cell&gt;&lt;cell&gt;Leverage&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;10&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;11.1&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;12.5&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;cell&gt;14.3&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;16.7&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;20&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;cell&gt;25&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;33.3&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;50&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;9&lt;/cell&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;In the unlikely event that the attack succeeds early enough to avoid liquidation there would have been one of those "sharp decreases in prices" that cause ADL, so as a huge winner it would be essentially certain that the attacker would suffer ADL and most of the winnings needed to justify the attack would evaporate.&lt;/p&gt;&lt;head rend="h4"&gt;Regulated Exchanges&lt;/head&gt;The peak open interest in Bitcoin futures on the Chicago Mercantile Exchange over the past year was less than $20B, so even if we add together both kinds of exchange, the peak open interest over the last year isn't enough for the attacker.&lt;head rend="h3"&gt;Conclusions&lt;/head&gt;Neither an outsider nor an insider attack appears feasible.&lt;head rend="h4"&gt;Outsider Attack&lt;/head&gt;An outsider attack seems infeasible because in practice:&lt;list rend="ul"&gt;&lt;item&gt;They could not acquire 43% or more of the hash power.&lt;/item&gt;&lt;item&gt;Even if they could it would take so long as to make detection inevitable.&lt;/item&gt;&lt;item&gt;Even if they could and they were not detected, the high cost of the rigs makes the necessary shorts large relative to the open interest, and expensive to maintain.&lt;/item&gt;&lt;item&gt;These large shorts would need to be leveraged perpetual futures, bringing significant risks of loss of collateral through liquidation, and of the potential payoff being reduced through automatic de-leveraging.&lt;/item&gt;&lt;item&gt;The attacker would need more than the peak aggregate open interest in Bitcoin futures over the past year.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Insider Attack&lt;/head&gt;The order-of-magnitude lower direct cost of an insider attack makes it appear less infeasible, but insiders have to consider the impact on their continuing mining business. If the assumed 20% drop in the Bitcoin price were sustained for a year, the cost to the miner controlling 30% of the hash rate would be about 15,750 BTC or nearly $1.5B making the total cost of the attack (excluding the cost of carrying the shorts) almost $2B.&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;quote&gt;mining-company stocks are still flying, even with cryptocurrency prices in retreat. That's because these firms have something in common with the hottest investment theme on the planet: the massive, electricity-hungry data centers expected to power the artificial-intelligence boom. Some companies are figuring out how to remake themselves as vital suppliers to Alphabet, Amazon, Meta, Microsoft and other "hyperscalers" bent on AI dominance.I wonder why the date is 2028! As profit-driven miners use their bouyant stock price to fund a pivot to AI the hash rate and the network difficuty will decrease, making an insider attack less infeasible. The drop in their customer's income will likely encourage Bitmain to similarly pivot to AI, devoting an increasing proportion of their wafers to AI chips, especially given the Chinese government's goal of localizing AI.&lt;lb/&gt;...&lt;lb/&gt;Miners often have to build new, specialized facilities, because running AI requires more-advanced cooling and network systems, as well as replacing bitcoin-mining computers with AI-focused graphics processing units. But signing deals with miners allows AI giants to expand faster and cheaper than starting new facilities from scratch.&lt;lb/&gt;...&lt;lb/&gt;Shares of Core Scientific quadrupled in 2024 after the company signed its first AI contract that February. The stock has gained 10% this year. The company now expects to exit bitcoin mining entirely by 2028.&lt;/quote&gt;&lt;p&gt;A 30% miner whose rigs were fully depreciated might consider an insider attack shortly before the halvening as a viable exit strategy, since their future earnings from mining would be greatly reduced. But they would still be detected.&lt;/p&gt;&lt;head rend="h3"&gt;Counter-measures&lt;/head&gt;Even if we assume the feasibility of both the hash rate and the short position aspects of the attack, it is still the case that for example, an attack with 30% of the hash power and a 95% probability of success will, on average, last 17 days. it seems very unlikely that the coincidence over an extended period of a large reduction in the expected hash rate and a huge increase in short interest would escape attention from Bitcoin HODl-ers, miners and exchanges, not to mention Bitmain. What counter-measures could they employ?&lt;table&gt;&lt;row&gt;&lt;cell&gt;Source&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;list rend="ul"&gt;&lt;item&gt;The 6-block rule is just a convention, there is no dial that can be turned.&lt;/item&gt;&lt;item&gt;Much of the access to the Bitcoin blockchain is via APIs that typically have the 6-block rule hard-codded in.&lt;/item&gt;&lt;item&gt;Many, typically low-value, transactions do not wait for even a single confirmation.&lt;/item&gt;&lt;item&gt;Even it were possible, changing from a one-hour to a four-hour confirmation would have significant negative impacts on the Bitcoin ecosystem.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46437876</guid><pubDate>Tue, 30 Dec 2025 20:53:59 +0000</pubDate></item><item><title>OpenAI's cash burn will be one of the big bubble questions of 2026</title><link>https://www.economist.com/leaders/2025/12/30/openais-cash-burn-will-be-one-of-the-big-bubble-questions-of-2026</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46438390</guid><pubDate>Tue, 30 Dec 2025 21:44:07 +0000</pubDate></item><item><title>Honey's Dieselgate: Detecting and tricking testers</title><link>https://vptdigital.com/blog/honey-detecting-testers/</link><description>&lt;doc fingerprint="a28bb2d50b110d3d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Honey’s Dieselgate: Detecting and Tricking Testers&lt;/head&gt;
    &lt;p&gt;MegaLag’s December 2024 video introduced 18 million viewers to serious questions about Honey, the widely-used browser shopping plug-in—in particular, whether Honey abides by the rules set by affiliate networks and merchants, and whether Honey takes commissions that should flow to other affiliates. I wrote in January that I thought Honey was out of line. In particular, I pointed out the contracts that limit when and how Honey may present affiliate links, and I applied those contracts to the behavior MegaLag documented. Honey was plainly breaking the rules.&lt;/p&gt;
    &lt;p&gt;As it turns out, Honey’s misconduct is considerably worse than MegaLag, I, or others knew. When Honey is concerned that a user may be a tester—a “network quality” employee, a merchant’s affiliate manager, an affiliate, or an enthusiast—Honey designs its software to honor stand down in full. But when Honey feels confident that it’s being used by an ordinary user, Honey defies stand down rules. Multiple methods support these conclusions: I extracted source code from Honey’s browser plugin and studied it at length, plus I ran Honey through a packet sniffer to collect its config files, and I cross-checked all of this with actual app behavior. Details below. MegaLag tested too, and has a new video with his updated assessment.&lt;/p&gt;
    &lt;p&gt;(A note on our relationship: MegaLag figured out most of this, but asked me to check every bit from first principles, which I did. I added my own findings and methods, and cross-checked with VPT records of prior observations as well as historic Honey config files. More on that below, too.)&lt;/p&gt;
    &lt;p&gt;Behaving better when it thinks it’s being tested, Honey follows in Volkswagen’s “Dieselgate” footsteps. Like Volkswagen, the cover-up is arguably worse than the underlying conduct. Facing the allegations MegaLag presented last year, Honey could try to defend presenting its affiliate links willy-nilly—argue users want this, claim to be saving users money, suggest that network rules don’t apply or don’t mean what they say. But these new allegations are more difficult to defend. Designing its software to perform differently when under test, Honey reveals knowing what the rules require and knowing they’d be in trouble if caught. Hiding from testers reveals that Honey wanted to present affiliate links as widely as possible, despite the rules, so long as it doesn’t get caught. It’s not a good look. Affiliates, merchants, and networks should be furious.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the rules require&lt;/head&gt;
    &lt;p&gt;The basic bargain of affiliate marketing is that a publisher presents a link to a user, who clicks, browses, and buys. If the user makes a purchase, commission flows to the publisher whose link was last clicked.&lt;/p&gt;
    &lt;p&gt;Shopping plugins and other client-side software undermine the basic bargain of affiliate marketing. If a publisher puts software on a user’s computer, that software can monitor where the user browses, present its affiliate link, and always (appear to) be “last”—even if it had minimal role in influencing the customer’s purchase decision.&lt;/p&gt;
    &lt;p&gt;Affiliate networks and merchants established rules to restore and preserve the bargain between what we might call “web affiliates” versus software affiliates. One, a user has to actually click a software affiliate’s link; decades ago, auto-clicks were common, but that’s long-since banned (yet nonetheless routine from “adware”-style browser plugins— example). Two, software must “stand down”—must not even show its link to users—when some prior web affiliate P has already referred a user to a given merchant. This reflects a balancing of interests: P wants a reasonable opportunity for the user to make a purchase, so P can get paid. If a shopping plugin could always present its offer, the shopping plugin would claim the commission that P had fairly earned. Meanwhile P wouldn’t get sufficient payment for its effort—and might switch to promoting some other merchant with rules P sees as more favorable. Merchants and networks need to maintain a balance in order to attract and retain web affiliates, which are understood to send traffic that’s substantially incremental (customers who wouldn’t have purchased anyway), whereas shopping plugins often take credit for nonincremental purchases. So if a merchant is unsure, it has good reason to err on the side of web affiliates.&lt;/p&gt;
    &lt;p&gt;All of this was known and understood literally decades ago. Stand-down rules were first established in 2002. Since then, they’ve been increasingly routine, and overall have become clearer and better enforced. Crucially, merchants and networks include stand-down rules in their contracts, making this not just a principle and a norm, but a binding contractual obligation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Detecting testers&lt;/head&gt;
    &lt;p&gt;How can Honey tell when a user may be a tester? Honey’s code and config files show that they’re using four criteria:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New accounts. If an account is less than 30 days old, Honey concludes the user might be a tester, so it disables its prohibited behavior.&lt;/item&gt;
      &lt;item&gt;Low earnings-to-date. In general, under Honey’s current rules, if an account has less than 65,000 points of Honey earning, Honey concludes the user might be a tester, so it disables its prohibited behavior. Since 1,000 points can be redeemed for $10 of gift cards, this threshold requires having earned $650 worth of points. That sounds like a high requirement, and it is. But it’s actually relatively new: As of June 2022, there was no points requirement for most merchants, and for merchants in Rakuten Advertising, the requirement was just 501 points (about $5 of points). (Details below.)&lt;/item&gt;
      &lt;item&gt;Honey periodically checks a server-side blacklist. The server can condition its decision on any factor known to the server, including the user’s Honey ID and cookie, or IP address inside a geofence or on a ban list. Suppose the user has submitted prior complaints about Honey, as professional testers frequently do. Honey can blacklist the user ID, cookie, and IP or IP range. Then any further requests from that user, cookie, or IP will be treated as high-risk, and Honey disables its prohibited behavior.&lt;/item&gt;
      &lt;item&gt;Affiliate industry cookies. Honey checks whether a user has cookies indicating having logged into key affiliate industry tools, including the CJ, Rakuten Advertising, and Awin dashboards. If the user has such a cookie, the user is particularly likely to be a tester, so Honey disables its prohibited behavior.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If even one of these factors indicates a user is high-risk, Honey honors stand-down. But if all four pass, then Honey ignores stand-down rules and presents its affiliate links regardless of a prior web publisher’s role and regardless of stand-down rules. This isn’t a probabilistic or uncertain dishonoring of stand-down (as plaintiffs posited in litigation against Honey). Rather, Honey’s actions are deterministic: If a high-risk factor hits, Honey will completely and in every instance honor stand-down; and if no such factor hits, then Honey will completely and in every instance dishonor stand-down (meaning, present its link despite networks’ rules).&lt;/p&gt;
    &lt;p&gt;These criteria indicate Honey’s attempt to obstruct and frankly frustrate testers. In my experience from two decades of testing affiliate misconduct, it is routine for a tester to install a new shopping plugin on a new PC, create a new account, and check for immediate wrongdoing. By always standing down on new accounts (&amp;lt;30 days), Honey prevents this common test scenario from catching its stand-down violations. Of course diligent testers will check way past 30 days, but a tester on a short-term contract will perceive nothing amiss. Some clients may not pay for testers to stick with the task. And a client for some reason needing an immediate finding—perhaps to dispute Honey’s recent earnings—can’t get it.&lt;/p&gt;
    &lt;p&gt;The requirement of achieving sufficiently many points similarly stymies many testers. Over the years, a handful of my clients granted a budget for test purchases. But most did not. If I wanted to perform test purchases, I would have needed to spend my own money. Meanwhile, there’s no proper reason why a shopping app should stand down differently for users with points versus new users, so if testers assumed good faith by Honey, they’d have no reason to build up a points history. This criteria certainly prevented testers from uncovering what Honey was doing.&lt;/p&gt;
    &lt;p&gt;The interaction of account age, points history, and blacklist further stymies testing. Once a tester reports a specific observation of misconduct, Honey can blacklist the tester’s account—which will still do most things normally, but will begin to stand down even though unbanned accounts don’t stand down in the same circumstances. A tester who suspected this, would have to create a new account, wait 30 days, and build up a sizable points balance to see what Honey is actually doing for other users. That’s a genuine barrier, impeding even a tester who correctly proposes this hypothesis.&lt;/p&gt;
    &lt;p&gt;Honey’s detection of affiliate industry cookies is particularly revealing and, in my view, devastatingly incriminating. Maybe Honey could concoct a story about why it stands down less for established accounts and for high-points-earners. It would be a tall tale, but maybe Honey could convince some people. In contrast, it is inconceivable why a shopping plugin should care whether a user previously logged into an affiliate dashboard. There’s only one reason to check: to hide from testers. But that reveals Honey’s intent and Honey’s knowledge that its conduct is prohibited.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evidence from hands-on testing&lt;/head&gt;
    &lt;p&gt;Multiple forms of evidence support my finding of Honey detecting testers. First, consider hands-on testing. With a standard test account with few or no points, Honey honored stand-down. See video 1. But when I tricked the Honey plugin into thinking I had tens of thousands of points (details below about how I did this), Honey popped up despite stand-down rules. See video 2. I repeated this test over multiple days, as to multiple merchants. The finding was the same every time. The only thing I changed between the “video 1” tests and “video 2” tests was the number of points supposedly associated with my account.&lt;/p&gt;
    &lt;p&gt;To demonstrate Honey checking for affiliate industry cookies, I added a step to my test scenario. With Honey tricked into thinking I had ample points, same as video 2, I began a test run by logging into a CJ portal used by affiliates. In all other respects, my test run was the same as video 2. Seeing the CJ portal cookie, Honey stood down. See video 3.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evidence from technical analysis&lt;/head&gt;
    &lt;p&gt;Some might ask whether the findings in the prior section could be coincidence. Maybe Honey just happened to open in some scenarios and not others. Maybe I’m ascribing intentionality to acts that are just coincidence. Let me offer two responses to this hypothesis. One, my findings are repeatable, countering any claim of coincidence. Second, separate from hands-on testing, three separate types of technical analysis—config files, telemetry, and source code—all confirm the accuracy of the prior section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evidence from configuration files&lt;/head&gt;
    &lt;p&gt;Honey retrieves its configuration settings from JSON files on a Honey server. Honey’s core stand-down configuration is in standdown-rules.json, while the selective stand-down—declining to stand down according to the criteria described above—is in the separate config file ssd.json. Here’s the contents of ssd.json as of October 22, 2025, with // comments added by me&lt;/p&gt;
    &lt;quote&gt;{"ssd": { "base": { "gca": 1, //enable affiliate console cookie check "bl": 1, //enable blacklist check "uP": 65000, //min points to disable standdown "adb": 26298469858850 }, "affiliates": ["https://www.cj.com", "https://www.linkshare", "https://www.rakuten.com", "https://ui.awin.com", "https://www.swagbucks.com"], //affiliate console cookie domains to check "LS": { //override points threshold for LinkShare merchants "uP": 5001 }, "PAYPAL": { "uL": 1, "uP": 5000001, "adb": 26298469858850 } }, "ex": { //ssd exceptions "7555272277853494990": { //TJ Maxx "uP": 5001 }, "7394089402903213168": { //booking.com "uL": 1, "adb": 120000, "uP": 1001 }, "243862338372998182": { //kayosports "uL": 0, "uP": 100000 }, "314435911263430900": { "adb": 26298469858850 }, "315283433846717691": { "adb": 26298469858850 }, "GA": ["CONTID", "s_vi", "_ga", "networkGroup", "_gid"] //which cookies to check on affiliate console cookie domains } }&lt;/quote&gt;
    &lt;p&gt;On its own, the ssd config file is not a model of clarity. But source code (discussed below) reveals the meaning of abbreviations in ssd. uP (yellow) refers to user points—the minimum number of points a user must have in order for Honey to dishonor stand-down. Note the current base (default) requirement of uP user points at least 65,000 (green), though the subsequent section LS sets a lower threshold of just 5001 for merchants on the Rakuten Advertising (LinkShare) network. bl set to 1 instructs the Honey plugin to stand down if the server-side blacklist so instructs.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the affiliates and ex GA data structures (blue), establish the affiliate industry cookie checks mentioned above. The “affiliates” entry lists domain where cookies are to be checked. The ex GA data structure lists which cookie is to be checked for each domain. Though these are presented as two one-dimensional lists, Honey’s code actually checks them in conjunction – checks the first-listed affiliate network domain for the first-listed cookie, then the second, and so forth. One might ask why Honey stored the domain names and cookie names in two separate one-dimensional lists, rather than in a two-dimensional list, name-value pair, or similar. The obvious answer is that Honey’s approach kept the domain names more distant from the cookies on those domains, making its actions that much harder for testers to notice even if they got as far as this config file.&lt;/p&gt;
    &lt;p&gt;The rest of ex (red) sets exceptions to the standard (“base”) ssd. This lists five specific ecommerce sites (each referenced with an 18-digit ID number previously assigned by Honey) with adjusted ssd settings. For Booking.com and Kayosports, the ssd exceptions set even higher points requirements to cancel standdown (120,000 and 100,000 points, respectively), which I interpret as response to complaints from those sites.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evidence from telemetry&lt;/head&gt;
    &lt;p&gt;Honey’s telemetry is delightfully verbose and, frankly, easy to understand, including English explanations of what data is being collected and why. Perhaps Google demanded improvements as part of approving Honey’s submission to Chrome Web Store. (Google enforces what it calls “strict guidelines” for collecting user data. Rule 12: data collection must be “necessary for a user-facing feature.” The English explanations are most consistent with seeking to show Google that Honey’s data collection is proper and arguably necessary.) Meanwhile, Honey submitted much the same code to Apple as an iPhone app, and Apple is known to be quite strict in its app review. Whatever the reason, Honey telemetry reveals some important aspects of what it is doing and why.&lt;/p&gt;
    &lt;p&gt;When a user with few points gets a stand-down, Honey reports that in telemetry with the JSON data structure “method”:”suspend”. Meanwhile, the nearby JSON variable state gives the specific ssd requirement that the user didn’t satisfy—in my video 1: “state”:”uP:5001” reporting that, in this test run, my Honey app had less than 5001 points, and the ssd logic therefore decided to stand down. See video 1 at 0:37-0:41, or screenshots below for convenience. (My network tracing tool converted the telemetry from plaintext to a JSON tree for readability.)&lt;/p&gt;
    &lt;p&gt;When I gave myself more points (video 2), state instead reported ssd—indicating that all ssd criteria were satisfied, and Honey presented its offer and did not stand down. See video 2 at 0:32.&lt;/p&gt;
    &lt;p&gt;Finally, when I browsed an affiliate network console and allowed its cookie to be placed on my PC, Honey telemetry reported “state”:“gca”. Like video 1, the state value reports that ssd criteria were not satisfied, in this case because the gca (affiliate dashboard cookie) requirement was triggered, causing ssd to decide to stand down. See video 3 at 1:04-1:14.&lt;/p&gt;
    &lt;p&gt;In each instance, the telemetry matched identifiers from the config file (ssd, uP, gca). And as I changed from one test run to another, the telemetry transmissions tracked my understanding of Honey’s operation. Readers can check this in my videos: After Honey does or doesn’t stand down, I opened Fiddler to show what Honey reported in telemetry, in each instance in one continuous video take.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evidence from code&lt;/head&gt;
    &lt;p&gt;As a browser extension, Honey provides client-side code in JavaScript. Google’s Code Readability Requirements allow minification—removing whitespace, shortening variable and function names. Honey’s code is substantial—after deminification, more than 1.5 million lines. But a diligent analyst can still find what’s relevant. In fact the relevant parts are clustered together, and easily found via searches for obvious string such as “ssd”.&lt;/p&gt;
    &lt;p&gt;In a surprising twist, Honey in one instance released something approaching original code to Apple as an iPhone app. In particular, Honey included sourceMappingURL metadata that allows an analyst to recover original function names and variable names. (Instructions.) That release was from a moment in time, and Honey subsequently made revisions. But where that code is substantially the same as the code currently in use, I present the unobfuscated version for readers’ convenience. Here’s how it works:&lt;/p&gt;
    &lt;p&gt;First, there’s setup, including periodically checking the Honey killswitch URL /ck/alive:&lt;/p&gt;
    &lt;quote&gt;return e.next = 7, fetch("".concat("https://s.joinhoney.com", "/ck/alive"));&lt;/quote&gt;
    &lt;p&gt;If the killswitch returns “alive”, Honey sets the bl value to 0:&lt;/p&gt;
    &lt;quote&gt;c = S().then((function(e) { e &amp;amp;&amp;amp; "alive" === e.is &amp;amp;&amp;amp; (o.bl = 0) }))&lt;/quote&gt;
    &lt;p&gt;The ssd logic later checks this variable bl, among others, to decide whether to cancel standdown.&lt;/p&gt;
    &lt;p&gt;The core ssd logic is in a long function called R() which runs an infinite loop with a switch syntax to proceed through a series of numbered cases.&lt;/p&gt;
    &lt;quote&gt;function(e) { for (;;) switch (e.prev = e.next) {&lt;/quote&gt;
    &lt;p&gt;Focusing on the sections relevant to the behavior described above: Honey makes sure the user’s email address doesn’t include the string “test”, and checks whether the user is on the killswitch blacklist.&lt;/p&gt;
    &lt;quote&gt;if (r.email &amp;amp;&amp;amp; r.email.match("test") &amp;amp;&amp;amp; (o.bl = 0), !r.isLoggedIn || t) { e.next = 7; break&lt;/quote&gt;
    &lt;p&gt;Honey computes the age of the user’s account by subtracting the account creation date (r.created) from the current time:&lt;/p&gt;
    &lt;quote&gt;case 8: o.uL = r.isLoggedIn ? 1 : 0, o.uA = Date.now() - r.created;&lt;/quote&gt;
    &lt;p&gt;Honey checks for the most recent time a resource was blocked by an ad blocker:&lt;/p&gt;
    &lt;quote&gt;case 20: return p = e.sent, l &amp;amp;&amp;amp; a.A.getAdbTab(l) ? o.adb = a.A.getAdbTab(l) : a.A.getState().resourceLastBlockedAt &amp;gt; 0 ? o.adb = a.A.getState().resourceLastBlockedAt : o.adb = 0&lt;/quote&gt;
    &lt;p&gt;Honey checks whether any of the affiliate domains listed in the ssd affiliates data structure has the console cookie named in the GA data structure.&lt;/p&gt;
    &lt;quote&gt;m = p.ex &amp;amp;&amp;amp; p.ex.GA || [] g = i().map(p.ssd &amp;amp;&amp;amp; p.ssd.affiliates, (function(e) { return f += 1, u.A.get({ name: m[f], //cookie name from GA array url: e //domain to be checked }).then((function(e) { e &amp;amp;&amp;amp; (o.gca = 0) //if cookie found, set gca to 0 }))&lt;/quote&gt;
    &lt;p&gt;Then the comparison function P() compares each retrieved or calculated value to the threshold from ssd.json. The fundamental logic is that if any retrieved or calculated value (received in variable e below) is less than the threshold t from ssd, the ssd logic will honor standdown. In contrast, if all four values exceed the threshold, ssd will cancel the standdown. If this function elects to honor standdown, the return value gives the name of the rule (a) and the threshold (s) that caused the decision (yellow highlighting). If this function elects to dishonor standdown, it returns “ssd” (red) (which is the function’s default if not overridden by the logic that folllows). This yields the state= values I showed in telemetry and presented in screenshots and videos above.&lt;/p&gt;
    &lt;quote&gt;function P(e, t) { var r = "ssd"; return Object.entries(t).forEach((function(t) { var n, o, i = (o = 2, _(n = t) || b(n, o) || y(n, o) || g()), a = i[0], // field name (e.g., uP, gca, adb) s = i[1]; // threshold value from ssd.json "adb" === a &amp;amp;&amp;amp; (s = s &amp;gt; Date.now() ? s : Date.now() - s), // special handling for adb timestamps void 0 !== e[a] &amp;amp;&amp;amp; e[a] &amp;lt; s &amp;amp;&amp;amp; (r = "".concat(a, ":").concat(s)) })), r }&lt;/quote&gt;
    &lt;head rend="h2"&gt;Special treatment of eBay&lt;/head&gt;
    &lt;p&gt;Reviewing both config files and code, I was intrigued to see eBay called out for greater protections than others. Where Honey stands down for other merchant and networks for 3,600 seconds (one hour), eBay gets 86,400 seconds (24 hours).&lt;/p&gt;
    &lt;quote&gt;"regex": "^https?\\:\\/\\/rover\\.ebay((?![\\?\\&amp;amp;]pub=5575133559).)*$", "provider": "LS", "overrideBl": true, "ttl": 86400&lt;/quote&gt;
    &lt;p&gt;Furthermore, Honey’s code includes an additional eBay backstop. No matter what any config file might stay, Honey’s ssd selective stand-down logic will always stand down on ebay.com, even if standard ssd logic and config files would otherwise decide to disable stand-down. See this hard-coded eBay stand-down code:&lt;/p&gt;
    &lt;quote&gt;... const r = e.determineSsdState ? await e.determineSsdState(_.provider, v.id, i).catch() : null, a = "ssd" === r &amp;amp;&amp;amp; !/ebay/.test(p); ...&lt;/quote&gt;
    &lt;p&gt;Why such favorable treatment of eBay? Affiliate experts may remember the 2008 litigation in which eBay and the United States brought civil and criminal charges against Brian Dunning and Shawn Hogan, who were previously eBay’s two largest affiliates—jointly paid more than $20 million in just 18 months. I was proud to have caught them—a fact I can only reveal because an FBI agent’s declaration credited me. After putting its two largest affiliates in jail and demanding repayment of all the money they hadn’t spent or lost, eBay got a well-deserved reputation for being smart and tough at affiliate compliance. Honey is right to want to stay on eBay’s good side. At the same time, it’s glaring to see Honey treat eBay so much better than other merchants and networks. Large merchants on other networks could look at this and ask: If eBay gets a 24 hour stand-down and a hard-coded ssd exception, why are they treated worse?&lt;/p&gt;
    &lt;head rend="h2"&gt;Change over time&lt;/head&gt;
    &lt;p&gt;I mentioned above that I have historic config files. First, VPT (the affiliate marketing compliance company where I am Chief Scientist) preserved a ssd.json from June 2022. As of that date, Honey ssd had no points requirement for most networks. See yellow “base” below, notably in this version including a uP section. For LinkShare (Rakuten Advertising), the June 2022 ssd file required 501 points (green), equal to about $5 of earning to date.&lt;/p&gt;
    &lt;quote&gt;{"ssd": { "base": {"gca": 1, "bl": 1}, "affiliates": ["https://www.cj.com", "https://www.linkshare", "https://www.rakuten.com", "https://ui.awin.com", "https://www.swagbucks.com"], "LS": {"uL": 1, "uA": 2592000, "uP": 501, "SF": {"uP": 200} }, ...&lt;/quote&gt;
    &lt;p&gt;In April 2023, Archive.org preserved ssd.json, with the same settings.&lt;/p&gt;
    &lt;p&gt;Notice the changes from 2022-2023 to the present—most notably, a huge increase in points required for Honey to not stand-down. The obvious explanation for the change is MegaLag’s December 2024 video, and resulting litigation, which brought new scrutiny to whether Honey honors stand-down.&lt;/p&gt;
    &lt;p&gt;A second relevant change is that, as of 2022-2023, the ssd.json included a uA setting for LinkShare, requiring an account age of at least 2,592,000 seconds (30 days). But the current version of ssd.json has no uA setting, not for LinkShare merchants nor for any other merchants. Perhaps Honey thinks the high points requirement (65,000) now obviates the need for a 30-day account age.&lt;/p&gt;
    &lt;p&gt;In litigation, plaintiffs should be able to obtain copies of Honey config files indicating when the points requirement increased, and for that matter management discussions about whether and why to make this change. If the config files show ssd in similar configuration from 2022 through to fall 2024, but cutoffs increased shortly after MegaLag’s video, it will be easy to infer that Honey reduced ssd, and increased standdown, after getting caught.&lt;/p&gt;
    &lt;p&gt;Despite Honey’s recently narrowing ssd to more often honor stand-down, this still isn’t what the rules require. Rather than comply in full, Honey continued not to comply for the highest-spending users, those with &amp;gt;65k points—who Honey seems to figure must be genuine users, not testers or industry insiders.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tensions between Honey and LinkShare (Rakuten Advertising)&lt;/head&gt;
    &lt;p&gt;Honey’s LinkShare exception presents a puzzle. In 2022 and 2023, Honey was stricter for LinkShare merchants—more often honoring stand-down, and dishonoring stand-down only for users with at least 501 points. But in the current configuration, Honey applies a looser standard for LinkShare merchants: Honey now dishonors LinkShare stand-down once a user has 5,001 points, compared to the much higher 65,000-point requirement for merchants on other networks. What explains this reversal? Honey previously wanted to be extra careful for LinkShare merchants—so why now be less careful?&lt;/p&gt;
    &lt;p&gt;The best interpretation is a two-step sequence. First, at some point Honey raised the LinkShare threshold from 501 to 5,001 points—likely in response to a merchant complaint or LinkShare network quality concerns. Second, when placing that LinkShare-specific override into ssd.json, Honey staff didn’t consider how it would interact with later global rules—especially since the overall points requirement (base uA) didn’t yet exist. Later, MegaLag’s video pushed Honey to impose a 65,000-point threshold for dishonoring stand-down across all merchants—and when Honey staff imposed that new rule, they overlooked the lingering LinkShare override. A rule intended to be stricter for LinkShare now inadvertently makes LinkShare more permissive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reflections on hiding from testers&lt;/head&gt;
    &lt;p&gt;In a broad sense, the closest analogue to Honey’s tactics is Volkswagen Dieselgate Recall the 2015 discovery that Volkswagen programmed certain diesel engines to activate their emission controls only during laboratory testing, but not in real-world driving. Revelation of Volkswagen’s misconduct led to the resignation of Volkswagen’s CEO. Fines, penalties, settlements, and buyback costs exceeded $33 billion.&lt;/p&gt;
    &lt;p&gt;In affiliate marketing, numbers are smaller, but defeating testing is, regrettably, more common. For decades I’ve been tracking cookie-stuffers, which routinely use tiny web elements (1×1 IFRAMEs and IMG tags) to load affiliate cookies, and sometimes further conceal those elements using CSS such as visibility:none. Invisibility quite literally conceals what occurs. In parallel, affiliates also deployed additional concealment methods. Above, I mentioned Dunning and Hogan, who concealed their miscondudct in two additional ways. First, they stuffed each IP address at most once. Consider a researcher who suspected a problem, but didn’t catch it the first time. (Perhaps the screen-recorder and packet sniffer weren’t running. Or maybe this happened on a tester’s personal machine, not a dedicated test device.) With a once-per-IP-address rule, the researcher couldn’t easily get the problem to recur. (Source: eBay complaint, paragraph 27: “… only on those computers that had not been previously stuffed…”) Second, they geofenced eBay and CJ headquarters. (Source.) Shawn Hogan even admitted intentionally not targeting the geographic areas where he thought I might go. Honey’s use of a server-side blacklist allows similar IP filtering and geofencing, as well as more targeted filtering such as always standing down for the specific IPs, cookies, and accounts that previously submitted complaints.&lt;/p&gt;
    &lt;p&gt;A 2010 blog from affiliate trademark testers BrandVerity uncovered an anti-test strategy arguably even closer to what Honey is doing. In this period, history sniffing vulnerabilities let web sites see what other pages a user had visited: Set visited versus unvisited links to different colors, link to a variety of pages, and check the color of each link. BV’s perpetrator used this tactic to see whether a user had visited tools used by affiliate compliance staff (BV’s own login page, LinkShare’s dashboard and internal corporate email, and ad-buying dashboards for Google and Microsoft search ads). If a user had visited any of these tools, the perpetrator would not invoke its affiliate link—thereby avoiding revealing its prohibited behavior (trademark bidding) to users who were plainly affiliate marketing professionals. For other users, the affiliate bid on prohibited trademark terms and invoked affiliate links. Like Honey, this affiliate distinguished normal users from industry insiders based on prior URL visits. Of course Honey’s superior position, as a browser plugin, lets it directly read cookies without resorting to CSS history. But that only makes Honey worse. No one defended the affiliate BV caught, and I can’t envision anyone defending Honey’s tactic here.&lt;/p&gt;
    &lt;p&gt;In a slightly different world, it might be considered part of the rough-and-tumble world of commerce that Honey sometimes takes credit for referrals that others think should accrue to them. (In fact, that’s an argument Honey recently made in litigation: “any harm [plaintiffs] may have experienced is traceable not to Honey but to the industry standard ‘last-click’ attribution rules.”) There, Honey squarely ignores network rules, which require Honey to stand down although MegaLag showed Honey does not. But if Honey just ignored network stand-down rules, brazenly, it could push the narrative that networks and merchants agreed since, admittedly, they didn’t stop Honey. By hiding, Honey instead reveals that they know their conduct is prohibited. When we see networks and merchants that didn’t ban Honey, the best interpretation (in light of Honey’s trickery) is not that they approved of Honey’s tactics, but rather that Honey’s concealment prevented them from figuring out what Honey was doing. And the effort Honey expended, to conceal its behavior from industry insiders, makes it particularly clear that Honey knew it would be in trouble if it was caught. Honey’s knowledge of misconduct is precisely opposite to its media response to MegaLag’s video, and equally opposite to its position in litigation.&lt;/p&gt;
    &lt;p&gt;Five years ago Amazon warned shoppers that Honey was a “security risk.” At the time, I wrote this off as sour grapes—a business dispute between two goliaths. I agreed with Amazon’s bottom line that Honey was up to no good, but I thought the real problems with Honey were harm to other affiliates and harm to merchants’ marketing programs, not harms to security. With the passage of time, and revelation of Honey’s tactics including checking other companies’ cookies and hiding from testers, Amazon is vindicated. Notice Honey’s excessive permission—which includes letting Honey read users’ cookies at all sites. That’s well beyond what a shopping assistant truly needs, and it allows all manner of misconduct including, unfortunately, what I explain above. Security risk, indeed. Kudos to Amazon for getting this right from the outset.&lt;/p&gt;
    &lt;p&gt;At VPT, we monitor shopping plugins for abusive behavior. We hope shopping plugins will behave forthrightly—doing the same thing in our test lab that they do for users. But we don’t assume it, and we have multiple strategies to circumvent the techniques that bad actors use to trick those monitoring their methods. We constantly iterate on these approaches as we find new ways of concealment. And when we catch a shopping plugin hiding from us, we alert our clients not just to their misconduct but also to their concealment—an affirmative indication that this plugin can’t be trusted. We have scores of historic test runs showing misconduct by Honey in a variety of configurations, targeting dozens of merchants on all the big networks, including both low points and high points, with both screen-cap video and packet log evidence of Honey’s actions. We’re proud that we’ve been testing Honey’s misconduct for years.&lt;/p&gt;
    &lt;head rend="h2"&gt;What comes next&lt;/head&gt;
    &lt;p&gt;I’m looking forward to Honey’s response. Can Honey leaders offer a proper reason why their product behaves differently when under test, versus when used by normal users? I’m all ears.&lt;/p&gt;
    &lt;p&gt;Honey should expect skepticism from Google, operator of the Chrome Web Store. Google is likely to take a dim view of a Chrome plugin hiding from testers. Chrome Web Store requires “developer transparency” and specifically bans “dishonest behavior.” Consider also Google’s prohibition on “conceal[ing] functionality”. Here, Honey was hiding not from Google staff but from merchants and networks, but this still violates the plain language of Google’s policy as written.&lt;/p&gt;
    &lt;p&gt;Honey also distributes its Safari extension through the Apple App Store, requiring compliance with Apple Developer Program policies. Apple’s extension policies are less developed, yet Apple’s broader app review process is notoriously strict. Meanwhile Apple operates an affiliate marketing program, making it particularly natural for Apple to step into the shoes of merchants who were tricked by Honey’s concealment. I expect a tough sanction from Apple too.&lt;/p&gt;
    &lt;p&gt;Meanwhile, class action litigation is ongoing on behalf of publishers who lose marketing commissions when Honey didn’t stand down. Nothing in the docket indicates that Plaintiff’s counsel know the depths of Honey’s efforts to conceal its stand-down violations. With evidence that Honey was intentionally hiding from testers, Plaintiffs should be able to strengthen their allegations of both the underlying misconduct and Honey’s knowledge of wrongdoing. My analysis also promises to simplify other factual aspects of the litigation. The consolidated class action complaint discusses unpredictability of Honey’s standdown but doesn’t identify the factors that make Honey seem unpredictable—by all indications because plaintiffs (quite understandably) don’t know. Faced with unpredictability, plaintiffs resorted to monte carlo simulation to analyze the probability that Honey harmed a given publisher in a series of affiliate referrals. But with clarity on what’s really going on, there’s no need for statistical analysis, and the case gets correspondingly simpler. The court recently instructed plaintiffs to amend their complaint, and surely counsel will emphasize Honey’s concealment in their next filing.&lt;/p&gt;
    &lt;p&gt;See also my narrated explainer video.&lt;/p&gt;
    &lt;head rend="h2"&gt;Notes on hands-on testing methods&lt;/head&gt;
    &lt;p&gt;Hands-on testing of the relevant scenarios presented immediate challenges. Most obviously, I needed to test what Honey would do if it had tens of thousands of points, valued at hundreds of dollars. But I didn’t want to make hundreds or thousands of dollars of test purchases through Honey.&lt;/p&gt;
    &lt;p&gt;To change the Honey client’s understanding of my points earned to date, I used Fiddler, a standard network forensics tool. I wrote a few lines of FiddlerScript to intercept messages between the Honey plug-in and the Honey server to report that I had however many points I wanted for a given test. Here’s my code, in case others want to test themselves:&lt;/p&gt;
    &lt;quote&gt;//buffer responses for communications to/from joinhoney.com //buffer allows response revisions by Fiddler static function OnBeforeRequest(oSession: Session) { if (oSession.fullUrl.Contains("joinhoney.com")) { oSession.bBufferResponse = true; } } //rewrite Honey points response to indicate high values static function OnBeforeResponse(oSession: Session) { if (oSession.HostnameIs("d.joinhoney.com") &amp;amp;&amp;amp; oSession.PathAndQuery.Contains("ext_getUserPoints")){ s = '{"data":{"getUsersPointsByUserId":{"pointsPendingDeposit":67667,"pointsAvailable":98765,"pointsPendingWithdrawal":11111,"pointsRedeemed":22222}}}'; oSession.utilSetResponseBody(s); } }&lt;/quote&gt;
    &lt;p&gt;This fall, VPT added this method, and variants of it, to our automated monitoring of shopping plugins.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46438522</guid><pubDate>Tue, 30 Dec 2025 21:59:35 +0000</pubDate></item><item><title>Readings in Database Systems (5th Edition)</title><link>http://www.redbook.io/</link><description>&lt;doc fingerprint="9edc717d08966a2a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Readings in Database Systems (commonly known as the "Red Book") has offered readers an opinionated take on both classic and cutting-edge research in the field of data management since 1988. Here, we present the Fifth Edition of the Red Book — the first in over ten years.&lt;/p&gt;
      &lt;p&gt;CHAPTERS&lt;/p&gt;
      &lt;list id="entries" rend="ol"&gt;
        &lt;item style="list-style-type: none"&gt;Preface &lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46440510</guid><pubDate>Wed, 31 Dec 2025 02:01:53 +0000</pubDate></item><item><title>L1TF Reloaded</title><link>https://github.com/ThijsRay/l1tf_reloaded</link><description>&lt;doc fingerprint="9112986c9da653cb"&gt;
  &lt;main&gt;
    &lt;p&gt;The Rain research project shows how a malicious virtual machine can abuse transient execution vulnerabilities to leak data from the host, as well as from other virtual machines. This repository contains the research artifact: the L1TF Reloaded exploit and instructions on how to reproduce our results.&lt;/p&gt;
    &lt;p&gt;For details, we refer you to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;our S&amp;amp;P'26 paper: "Rain: Transiently Leaking Data from Public Clouds Using Old Vulnerabilities"&lt;/item&gt;
      &lt;item&gt;our project page: "Rain: Cloud Leakage via Hardware Vulnerabilities"&lt;/item&gt;
      &lt;item&gt;our blog post together with Google: "Project Rain:L1TF"&lt;/item&gt;
      &lt;item&gt;AWS's blog post: "Amazon EC2 defenses against L1TF Reloaded"&lt;/item&gt;
      &lt;item&gt;WHY2025 public disclosure: "Spectre in the real world: Leaking your private data from the cloud with CPU vulnerabilities"&lt;/item&gt;
      &lt;item&gt;Hardware.io NL 2025 talk: "Real-World Exploitation of Transient Execution Vulnerabilities to Leak Private Data from Public Clouds"&lt;/item&gt;
      &lt;item&gt;39C3 talk: "Spectre in the real world: Leaking your private data from the cloud with CPU vulnerabilities"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our end-to-end exploit, called "L1TF Reloaded", abuses two long-known transient execution vulnerabilities: L1TF and (Half-)Spectre. By combining them, commonly deployed software-based mitigations against L1TF, such as L1d flushing and core scheduling, can be circumvented.&lt;/p&gt;
    &lt;p&gt;We have launched our exploit against the production clouds of both AWS and Google. Below is a (fast-forwarded) recording of our exploit running within a VM on GCE. The exploit, at runtime, finds another VM on the same physical host, detects that it is running an Nginx webserver, and leaks its private TLS key.&lt;/p&gt;
    &lt;p&gt;This repository is structured as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;deps&lt;/code&gt;: exploit dependencies&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;include&lt;/code&gt;: exploit headers files&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scripts&lt;/code&gt;: utility scripts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setup&lt;/code&gt;: reproduction resources&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src&lt;/code&gt;: exploit source code&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We provide detailed reproduction instructions for:&lt;/p&gt;
    &lt;p&gt;The specific gadgets that we leverage have been patched in KVM. On Intel CPUs that are affected by L1TF, only stable kernel releases before 5.4.298, 5.10.242, 5.15.191, 6.1.150, 6.6.104, 6.12.45 or 6.16.5 are vulnerable to this specific attack. The underlying issue is still there, but a different half-Spectre gadget is necessary to exploit L1TF Reloaded on up-to-date production systems. As discussed in our paper, we recommend deploying additional blanket mitigations against L1TF Reloaded's attack strategy, as well as other microarchitectural attacks in general.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46440695</guid><pubDate>Wed, 31 Dec 2025 02:33:50 +0000</pubDate></item><item><title>LLVM AI tool policy: human in the loop</title><link>https://discourse.llvm.org/t/rfc-llvm-ai-tool-policy-human-in-the-loop/89159</link><description>&lt;doc fingerprint="9f50d2136c36c7a3"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hey folks, I got a lot of feedback from various meetings on the proposed LLVM AI contribution policy, and I made some significant changes based on that feedback. The current draft proposal focuses on the idea of requiring a human in the loop who understands their contribution well enough to answer questions about it during review. The idea here is that contributors are not allowed to offload the work of validating LLM tool output to maintainers. I’ve mostly removed the Fedora policy in an effort to move from the vague notion of “owning the contribution” to a more explicit “contributors have to review their contributions and be prepared to answer questions about them”. Contributors should never find themselves in the position of saying “I don’t know, an LLM did it”. I felt the change here was significant, and deserved a new thread.&lt;/p&gt;
      &lt;p&gt;From an informal show of hands at the round table at the US LLVM developer meeting, most contributors (or at least the subset with the resources and interest in attending this round table in person) are interested in using LLM assistance to increase their productivity, and I really do want to enable them to do so, while also making sure we give maintainers a useful policy tool for pushing back against unwanted contributions.&lt;/p&gt;
      &lt;p&gt;I’ve updated the PR, and I’ve pasted the markdown below as well, but you can also view it on GitHub.&lt;/p&gt;
      &lt;head rend="h1"&gt;LLVM AI Tool Use Policy&lt;/head&gt;
      &lt;head rend="h2"&gt;Policy&lt;/head&gt;
      &lt;p&gt;LLVM’s policy is that contributors can use whatever tools they would like to&lt;lb/&gt; craft their contributions, but there must be a human in the loop.&lt;lb/&gt; Contributors must read and review all LLM-generated code or text before they&lt;lb/&gt; ask other project members to review it. The contributor is always the author&lt;lb/&gt; and is fully accountable for their contributions. Contributors should be&lt;lb/&gt; sufficiently confident that the contribution is high enough quality that asking&lt;lb/&gt; for a review is a good use of scarce maintainer time, and they should be able&lt;lb/&gt; to answer questions about their work during review.&lt;/p&gt;
      &lt;p&gt;We expect that new contributors will be less confident in their contributions,&lt;lb/&gt; and our guidance to them is to start with small contributions that they can&lt;lb/&gt; fully understand to build confidence. We aspire to be a welcoming community&lt;lb/&gt; that helps new contributors grow their expertise, but learning involves taking&lt;lb/&gt; small steps, getting feedback, and iterating. Passing maintainer feedback to an&lt;lb/&gt; LLM doesn’t help anyone grow, and does not sustain our community.&lt;/p&gt;
      &lt;p&gt;Contributors are expected to be transparent and label contributions that&lt;lb/&gt; contain substantial amounts of tool-generated content. Our policy on&lt;lb/&gt; labelling is intended to facilitate reviews, and not to track which parts of&lt;lb/&gt; LLVM are generated. Contributors should note tool usage in their pull request&lt;lb/&gt; description, commit message, or wherever authorship is normally indicated for&lt;lb/&gt; the work. For instance, use a commit message trailer like Assisted-by: . This transparency helps the community develop best practices&lt;lb/&gt; and understand the role of these new tools.&lt;/p&gt;
      &lt;p&gt;An important implication of this policy is that it bans agents that take action&lt;lb/&gt; in our digital spaces without human approval, such as the GitHub &lt;code&gt;@claude&lt;/code&gt;&lt;lb/&gt; agent. Similarly, automated review tools that&lt;lb/&gt; publish comments without human review are not allowed. However, an opt-in&lt;lb/&gt; review tool that keeps a human in the loop is acceptable under this policy.&lt;lb/&gt; As another example, using an LLM to generate documentation, which a contributor&lt;lb/&gt; manually reviews for correctness, edits, and then posts as a PR, is an approved&lt;lb/&gt; use of tools under this policy.&lt;/p&gt;
      &lt;p&gt;This policy includes, but is not limited to, the following kinds of&lt;lb/&gt; contributions:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Code, usually in the form of a pull request&lt;/item&gt;
        &lt;item&gt;RFCs or design proposals&lt;/item&gt;
        &lt;item&gt;Issues or security vulnerabilities&lt;/item&gt;
        &lt;item&gt;Comments and feedback on pull requests&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;Extractive Contributions&lt;/head&gt;
      &lt;p&gt;The reason for our “human-in-the-loop” contribution policy is that processing&lt;lb/&gt; patches, PRs, RFCs, and comments to LLVM is not free – it takes a lot of&lt;lb/&gt; maintainer time and energy to review those contributions! Sending the&lt;lb/&gt; unreviewed output of an LLM to open source project maintainers extracts work&lt;lb/&gt; from them in the form of design and code review, so we call this kind of&lt;lb/&gt; contribution an “extractive contribution”.&lt;/p&gt;
      &lt;p&gt;Our golden rule is that a contribution should be worth more to the project&lt;lb/&gt; than the time it takes to review it. These ideas are captured by this quote&lt;lb/&gt; from the book Working in Public by Nadia Eghbal:&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;"When attention is being appropriated, producers need to weigh the costs and&lt;lb/&gt; benefits of the transaction. To assess whether the appropriation of attention&lt;lb/&gt; is net-positive, it’s useful to distinguish between extractive and&lt;lb/&gt; non-extractive contributions. Extractive contributions are those where the&lt;lb/&gt; marginal cost of reviewing and merging that contribution is greater than the&lt;lb/&gt; marginal benefit to the project’s producers. In the case of a code&lt;lb/&gt; contribution, it might be a pull request that’s too complex or unwieldy to&lt;lb/&gt; review, given the potential upside." -- Nadia Eghbal&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;Prior to the advent of LLMs, open source project maintainers would often review&lt;lb/&gt; any and all changes sent to the project simply because posting a change for&lt;lb/&gt; review was a sign of interest from a potential long-term contributor. While new&lt;lb/&gt; tools enable more development, it shifts effort from the implementor to the&lt;lb/&gt; reviewer, and our policy exists to ensure that we value and do not squander&lt;lb/&gt; maintainer time.&lt;/p&gt;
      &lt;p&gt;Reviewing changes from new contributors is part of growing the next generation&lt;lb/&gt; of contributors and sustaining the project. We want the LLVM project to be&lt;lb/&gt; welcoming and open to aspiring compiler engineers who are willing to invest&lt;lb/&gt; time and effort to learn and grow, because growing our contributor base and&lt;lb/&gt; recruiting new maintainers helps sustain the project over the long term. Being&lt;lb/&gt; open to contributions and liberally granting commit access&lt;lb/&gt; is a big part of how LLVM has grown and successfully been adopted all across&lt;lb/&gt; the industry. We therefore automatically post a greeting comment to pull&lt;lb/&gt; requests from new contributors and encourage maintainers to spend their time to&lt;lb/&gt; help new contributors learn.&lt;/p&gt;
      &lt;head rend="h2"&gt;Handling Violations&lt;/head&gt;
      &lt;p&gt;If a maintainer judges that a contribution is extractive (i.e. it doesn’t&lt;lb/&gt; comply with this policy), they should copy-paste the following response to&lt;lb/&gt; request changes, add the &lt;code&gt;extractive&lt;/code&gt; label if applicable, and refrain from&lt;lb/&gt; further engagement:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;This PR appears to be extractive, and requires additional justification for
why it is valuable enough to the project for us to review it. Please see
our developer policy on AI-generated contributions:
http://llvm.org/docs/AIToolPolicy.html
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Other reviewers should use the label to prioritize their review time.&lt;/p&gt;
      &lt;p&gt;The best ways to make a change less extractive and more valuable are to reduce&lt;lb/&gt; its size or complexity or to increase its usefulness to the community. These&lt;lb/&gt; factors are impossible to weigh objectively, and our project policy leaves this&lt;lb/&gt; determination up to the maintainers of the project, i.e. those who are doing&lt;lb/&gt; the work of sustaining the project.&lt;/p&gt;
      &lt;p&gt;If a contributor responds but doesn’t make their change meaningfully less&lt;lb/&gt; extractive, maintainers should escalate to the relevant moderation or admin&lt;lb/&gt; team for the space (GitHub, Discourse, Discord, etc) to lock the conversation.&lt;/p&gt;
      &lt;head rend="h2"&gt;Copyright&lt;/head&gt;
      &lt;p&gt;Artificial intelligence systems raise many questions around copyright that have&lt;lb/&gt; yet to be answered. Our policy on AI tools is similar to our copyright policy:&lt;lb/&gt; Contributors are responsible for ensuring that they have the right to&lt;lb/&gt; contribute code under the terms of our license, typically meaning that either&lt;lb/&gt; they, their employer, or their collaborators hold the copyright. Using AI tools&lt;lb/&gt; to regenerate copyrighted material does not remove the copyright, and&lt;lb/&gt; contributors are responsible for ensuring that such material does not appear in&lt;lb/&gt; their contributions. Contributions found to violate this policy will be removed&lt;lb/&gt; just like any other offending contribution.&lt;/p&gt;
      &lt;head rend="h2"&gt;Examples&lt;/head&gt;
      &lt;p&gt;Here are some examples of contributions that demonstrate how to apply&lt;lb/&gt; the principles of this policy:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;This PR contains a proof from Alive2, which is a strong signal of&lt;lb/&gt; value and correctness.&lt;/item&gt;
        &lt;item&gt;This generated documentation was reviewed for correctness by a&lt;lb/&gt; human before being posted.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;References&lt;/head&gt;
      &lt;p&gt;Our policy was informed by experiences in other communities:&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46440833</guid><pubDate>Wed, 31 Dec 2025 03:06:07 +0000</pubDate></item><item><title>Google Opal</title><link>https://opal.google/landing/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46441068</guid><pubDate>Wed, 31 Dec 2025 03:49:58 +0000</pubDate></item><item><title>Show HN: Use Claude Code to Query 600 GB Indexes over Hacker News, ArXiv, etc.</title><link>https://exopriors.com/scry</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46442245</guid><pubDate>Wed, 31 Dec 2025 07:47:44 +0000</pubDate></item></channel></rss>