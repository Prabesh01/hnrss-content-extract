<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 01 Sep 2025 18:13:35 +0000</lastBuildDate><item><title>UK's largest battery storage facility at Tilbury substation</title><link>https://www.nationalgrid.com/national-grid-connects-uks-largest-battery-storage-facility-tilbury-substation</link><description>&lt;doc fingerprint="95c5807b1a9ef1c2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;National Grid connects UK’s largest battery storage facility at Tilbury substation&lt;/head&gt;
    &lt;p&gt;National Grid has connected the UK’s largest battery energy storage system (BESS) to its transmission network at Tilbury substation in Essex.&lt;/p&gt;
    &lt;p&gt;The 300MW Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.&lt;/p&gt;
    &lt;p&gt;With a total capacity of 600MWh, Thurrock Storage is capable of powering up to 680,000 homes, and can help to balance supply and demand by soaking up surplus clean electricity and discharging it instantaneously when the grid needs it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our Tilbury substation once served a coal plant, and with battery connections like this, it’s today helping to power a more sustainable future for the region and the country.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;National Grid reinforced its Tilbury substation to ensure the network in the region could safely carry the battery’s significant additional load, with new protection and control systems installed to ensure a robust connection.&lt;/p&gt;
    &lt;p&gt;The substation previously served the coal-fired Tilbury A and B power stations on adjacent land prior to their demolition, so the connection of the Thurrock Storage facility marks a symbolic transition from coal to clean electricity at the site.&lt;/p&gt;
    &lt;p&gt;John Twomey, director of customer and network development at National Grid Electricity Transmission, said:&lt;/p&gt;
    &lt;p&gt;“Battery storage plays a vital role in Britain’s clean energy transition. Connecting Thurrock Storage, the UK’s biggest battery, to our transmission network marks a significant step on that journey.&lt;/p&gt;
    &lt;p&gt;“Our Tilbury substation once served a coal plant, and with battery connections like this, it’s today helping to power a more sustainable future for the region and the country.”&lt;/p&gt;
    &lt;p&gt;Tom Vernon, Statera Energy CEO and founder, said:&lt;/p&gt;
    &lt;p&gt;“We are delighted that Thurrock Storage is now energised, following its successful connection to the grid by National Grid Electricity Transmission. Increasing BESS capacity is essential for supporting the grid when renewable generation, such as solar and wind, is low or changes quickly. It ensures that energy can be stored efficiently and returned to the grid whenever it’s needed.”&lt;/p&gt;
    &lt;p&gt;National Grid is continuing work at Tilbury substation to connect the 450MW Thurrock Flexible Generation facility, another Statera project that is set to support the energy needs of the region.&lt;/p&gt;
    &lt;p&gt;The connection of the UK’s biggest battery follows energisation in July of the 373MW Cleve Hill Solar Park in Kent – the largest solar plant in the country – which National Grid connected to its adjacent Cleve Hill substation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45091119</guid></item><item><title>CocoaPods trunk read-only plan</title><link>https://blog.cocoapods.org/CocoaPods-Specs-Repo/</link><description>&lt;doc fingerprint="42f18d113d47e29d"&gt;
  &lt;main&gt;&lt;p&gt;30 November 2024&lt;/p&gt;Follow @orta&lt;p&gt;TLDR: In two years we plan to turn CocoaPods trunk to be read-only. At that point, no new versions or pods will be added to trunk. - Note, this post has been updated in May 2025.&lt;/p&gt;&lt;p&gt;Last month I wrote about how CocoaPods is currently being maintained, I also noted that we were discussing converting the main CocoaPods spec repo "trunk" to be read-only:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We are discussing that on a very long, multi-year, basis we can drastically simplify the security of CocoaPods trunk by converting the Specs Repo to be read-only. Infrastructure like the Specs repo and the CDN would still operate as long as GitHub and jsDelivr continue to exist, which is pretty likely to be a very long time. This will keep all existing builds working.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I plan to implement the read-only mode so that when someone submits a new Podspec to CocoaPods, it will always be denied at the server level. I would then convert the "CocoaPods/Specs" repo to be marked as "Archived" on GitHub which should cover all of our bases.&lt;/p&gt;&lt;p&gt;Making the switch will not break builds for people using CocoaPods in 2026 onwards, but at that point, you're not getting any more updates to dependencies which come though CocoaPods trunk. This shouldn't affect people who use CocoaPods with their own specs repos, or have all of their dependencies vendored (e.g. they all come from npm.)&lt;/p&gt;&lt;p&gt;May 2025 Update: Since this post was originally written, we've had enough security researchers abusing scripting capabilities in CocoaPods that we are now introducing a block on allowing new CocoaPods to use the &lt;code&gt;prepare_command&lt;/code&gt; field in a Podspec. Any existing Pods using &lt;code&gt;prepare_command&lt;/code&gt; are hard-coded to bypass this check.&lt;/p&gt;&lt;head rend="h2"&gt;Timeline&lt;/head&gt;&lt;p&gt;My goal is to send 2 very hard-to-miss notifications en-masse, and then do a test run a month before the final shutdown.&lt;/p&gt;&lt;head rend="h3"&gt;May 2025&lt;/head&gt;&lt;p&gt;We are stopping new CocoaPods from being added which use the &lt;code&gt;prepare_command&lt;/code&gt; field&lt;/p&gt;&lt;head rend="h3"&gt;Mid-late 2025&lt;/head&gt;&lt;p&gt;I will email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post.&lt;/p&gt;&lt;head rend="h3"&gt;September-October 2026&lt;/head&gt;&lt;p&gt;I will, again, email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post, noting that they have roughly a month before we do a test run of going read-only.&lt;/p&gt;&lt;head rend="h3"&gt;November 1-7th 2026&lt;/head&gt;&lt;p&gt;I will trigger a test run, giving automation a chance to break early&lt;/p&gt;&lt;head rend="h3"&gt;December 2nd 2026&lt;/head&gt;&lt;p&gt;I will switch trunk to not accept new Podspecs permanently. This is a Wednesday after American Thanksgiving, so I think folks won't be in rush mode.&lt;/p&gt;&lt;head rend="h2"&gt;Contact&lt;/head&gt;&lt;p&gt;These dates are not set in stone, and maybe someone out there has a good reason for us to amend the timeline. I don't think I'm amenable to moving it forwards, but within reason there's space for backwards.&lt;/p&gt;&lt;p&gt;If you have questions, you can contact the team via [email protected], me personally at [email protected] or reach out to me via Bluesky: @orta.io.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45091493</guid></item><item><title>Tetris is NP-hard even with O(1) rows or columns (2020) [pdf]</title><link>https://martindemaine.org/papers/ThinTetris_JIP/paper.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092324</guid></item><item><title>Bear is now source-available</title><link>https://herman.bearblog.dev/license/</link><description>&lt;doc fingerprint="1f43f87875e18099"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bear is now source-available&lt;/head&gt;
    &lt;p&gt;When I started building Bear I made the code available under an MIT license. I didn't give it much thought at the time, but knew that I wanted the code to be available for people to learn from, and to make it easily auditable so users could validate claims I have made about the privacy and security of the platform.&lt;/p&gt;
    &lt;p&gt;Unfortunately over the years there have been cases of people forking the project in the attempt to set up a competing service. And it hurts. It hurts to see something you've worked so hard on for so long get copied and distributed with only a few hours of modification. It hurts to have poured so much love into a piece of software to see it turned against you and threaten your livelihood. It hurts to believe in open-source and then be bitten by it.&lt;/p&gt;
    &lt;p&gt;After the last instance of this I have come to the difficult decision to change Bear's license from MIT to a version of copyleft called the Elastic License—created by the Elastic Search people.&lt;/p&gt;
    &lt;p&gt;This license is almost identical to the MIT license but with the stipulation that the software cannot be provided as a hosted or managed service. You can view the specific wording here.&lt;/p&gt;
    &lt;p&gt;After spending time researching how other projects are handling this, I realise I'm not alone. Many other open-source projects have updated their licenses to prevent "free-ride competition" in the past few years.123456&lt;/p&gt;
    &lt;p&gt;We're entering a new age of AI powered coding, where creating a competing product only involves typing "Create a fork of this repo and change its name to something cool and deploy it on an EC2 instance".&lt;/p&gt;
    &lt;p&gt;While Bear's code is good, what makes the platform special is the people who use it, and the commitment to longevity.&lt;/p&gt;
    &lt;p&gt;I will ensure the platform is taken care of, even if it means backtracking on what people can do with the code itself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092490</guid></item><item><title>Zfsbackrest: Pgbackrest style encrypted backups for ZFS filesystems</title><link>https://github.com/gargakshit/zfsbackrest</link><description>&lt;doc fingerprint="bf55c3c41a079343"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Experimental:&lt;lb/&gt;Do not use it as your only way for backups. This is something I wrote over a weekend. There's a lot of things that need work here.&lt;/quote&gt;
    &lt;p&gt;pgbackrest style encrypted backups for ZFS filesystems.&lt;/p&gt;
    &lt;p&gt;You need age installed to generate encryption keys. Encryption is NOT optional.&lt;/p&gt;
    &lt;code&gt;$ go install github.com/gargakshit/zfsbackrest/cmd/zfsbackrest@latest&lt;/code&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/zfsbackrest.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;debug = true # warning, may log sensitive data

[repository]
# zfsbackrest does not support changing the list of datasets after a repository
# is initialized YET. That's one feature I need.
included_datasets = ["storage/*"] # Glob is supported

[repository.s3]
# zfsbackrest does NOT support non-secure S3 endpoints.
endpoint = "todo"
bucket = "todo"
key = "todo"
secret = "todo"
region = "todo"

[repository.expiry]
# Child backups expire if the parent expires. See the model below for a better
# explanation.
full = "336h" # 14 days
diff = "120h" # 5 days
incr = "24h" # 1 day

[upload_concurrency]
full = 2
diff = 4
incr = 4&lt;/code&gt;
    &lt;code&gt;$ zfsbackrest init --age-recipient-public-key="&amp;lt;your age public key&amp;gt;"&lt;/code&gt;
    &lt;code&gt;$ zfsbackrest backup --type &amp;lt;full | diff | incr&amp;gt;&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;full&lt;/code&gt; backups are standalone. They do not depend on any other backups. They are
also huge in size because of that.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;diff&lt;/code&gt; backups are sent incrementally from the latest &lt;code&gt;full&lt;/code&gt; backup. They depend
on the parent &lt;code&gt;full&lt;/code&gt; backup to be present in the repository to restore.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;incr&lt;/code&gt; backups are send incrementally from the latest &lt;code&gt;diff&lt;/code&gt; backup. They depend
on the parent &lt;code&gt;diff&lt;/code&gt; backup to restore.&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest detail&lt;/code&gt;
    &lt;p&gt;It shows a list of backups, orphans and all.&lt;/p&gt;
    &lt;p&gt;Sometimes, orphaned backups are left as an artefact of incomplete or cancelled backups. You can clean those by running&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest cleanup --orphans --dry-run=false&lt;/code&gt;
    &lt;p&gt;You can clean up expired backups by running&lt;/p&gt;
    &lt;code&gt;$ zfsbackrest cleanup --expired --dru-run=false&lt;/code&gt;
    &lt;p&gt;To restore the backups, you'll need your age identity file (private key).&lt;/p&gt;
    &lt;code&gt;zfsbackrest restore -i &amp;lt;path-to-age-identity-file&amp;gt; \
  -s &amp;lt;name of the dataset to restore from&amp;gt; \
  -b &amp;lt;optionally, the backup ID to restore from, leave empty to restore the latest&amp;gt; \
  -d &amp;lt;name of the dataset to restore to&amp;gt; # Restoring to a dataset that already exists on your local FS will fail.&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;zfsbackrest&lt;/code&gt; doesn't write or modify actual &lt;code&gt;zfs&lt;/code&gt; datasets. It makes extensive
use of snapshots. List of &lt;code&gt;zfs&lt;/code&gt; operations used by &lt;code&gt;zfsbackrest&lt;/code&gt; are&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;backup&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;zfs snapshot&lt;/code&gt;- Creating a&lt;code&gt;zfs&lt;/code&gt;snapshot for&lt;code&gt;zfsbackrest&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;zfs hold&lt;/code&gt;- Creating a reference to that snapshot to prevent removal&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;zfs send&lt;/code&gt;- Sending the snapshot incrementally&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cleanup&lt;/code&gt;/&lt;code&gt;force-destroy&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;zfs release&lt;/code&gt;- Release the held snapshot&lt;/item&gt;&lt;item&gt;&lt;code&gt;zfs destroy&lt;/code&gt;- Destroy the snapshot&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;restore&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;zfs recv&lt;/code&gt;- Receiving the remote snapshot&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092605</guid></item><item><title>Show HN: Simple modenized .NET NuGet server reached RC</title><link>https://github.com/kekyo/nuget-server</link><description>&lt;doc fingerprint="8d11fb69a8142539"&gt;
  &lt;main&gt;
    &lt;p&gt;Simple modenized NuGet server implementation.&lt;/p&gt;
    &lt;p&gt;A simple NuGet server implementation built on Node.js that provides essential NuGet v3 API endpoints.&lt;/p&gt;
    &lt;p&gt;Compatible with &lt;code&gt;dotnet restore&lt;/code&gt; and standard NuGet clients for package publishing, querying, and manually downloading.&lt;/p&gt;
    &lt;p&gt;A modern browser-based UI is also provided:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can refer to registered packages. You can check various package attributes.&lt;/item&gt;
      &lt;item&gt;You can download packages by version.&lt;/item&gt;
      &lt;item&gt;You can also publish (upload) packages.&lt;/item&gt;
      &lt;item&gt;You can manage user accounts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Browse package list:&lt;/p&gt;
    &lt;p&gt;Publishing packages:&lt;/p&gt;
    &lt;p&gt;User account managements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Easy setup, run NuGet server in 10 seconds!&lt;/item&gt;
      &lt;item&gt;NuGet V3 API compatibility: Support for modern NuGet client operations&lt;/item&gt;
      &lt;item&gt;No need database management: Store package file and nuspecs into filesystem directly, feel free any database managements&lt;/item&gt;
      &lt;item&gt;Package publish: Flexible client to upload &lt;code&gt;.nupkg&lt;/code&gt;files via&lt;code&gt;HTTP POST&lt;/code&gt;using cURL and others&lt;/item&gt;
      &lt;item&gt;Basic authentication: Setup authentication for publish and general access when you want it&lt;/item&gt;
      &lt;item&gt;Reverse proxy support: Configurable trusted reverse proxy handling for proper URL resolution&lt;/item&gt;
      &lt;item&gt;Modern Web UI with enhanced features: &lt;list rend="ul"&gt;&lt;item&gt;Multiple package upload: Drag &amp;amp; drop multiple .nupkg files at once&lt;/item&gt;&lt;item&gt;User account management: Add/delete users, reset passwords (admin only)&lt;/item&gt;&lt;item&gt;API password regeneration: Self-service API password updates&lt;/item&gt;&lt;item&gt;Password change: Users can change their own passwords&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Package importer: Included package importer from existing NuGet server&lt;/item&gt;
      &lt;item&gt;Docker image available&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm install -g nuget-server&lt;/code&gt;
    &lt;p&gt;For using Docker images, refer to a separate chapter.&lt;/p&gt;
    &lt;code&gt;# Start server on default port 5963
nuget-server

# Custom port
nuget-server --port 3000

# Multiple options
nuget-server --port 3000 --config-file config/config.json --users-file config/users.json&lt;/code&gt;
    &lt;p&gt;The NuGet V3 API is served on the &lt;code&gt;/v3&lt;/code&gt; path.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default nuget-server served URL (Show UI): &lt;code&gt;http://localhost:5963&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Actual NuGet V3 API endpoint: &lt;code&gt;http://localhost:5963/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The default URL provided by nuget-server can be changed using the &lt;code&gt;--base-url&lt;/code&gt; option.
This is particularly necessary when public endpoint service using a reverse proxy. For details, refer to below chapter.&lt;/p&gt;
    &lt;p&gt;nuget-server only supports the NuGet V3 API. Therefore, NuGet clients must always access it using the V3 API.&lt;/p&gt;
    &lt;p&gt;If you do not explicitly specify to use the V3 API, some implementations may fall back to the V3 API while others may not, potentially causing unstable behavior. Therefore, you must always specify it. Example below.&lt;/p&gt;
    &lt;p&gt;Add as package source:&lt;/p&gt;
    &lt;p&gt;For HTTP endpoints:&lt;/p&gt;
    &lt;code&gt;dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" --protocol-version 3 --allow-insecure-connections&lt;/code&gt;
    &lt;p&gt;For HTTPS endpoints:&lt;/p&gt;
    &lt;code&gt;dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3&lt;/code&gt;
    &lt;p&gt;Or specify in &lt;code&gt;nuget.config&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;configuration&amp;gt;
  &amp;lt;packageSources&amp;gt;
    &amp;lt;add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" /&amp;gt;
  &amp;lt;/packageSources&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;
    &lt;p&gt;Upload packages by &lt;code&gt;HTTP POST&lt;/code&gt; method, using cURL or any HTTP client with &lt;code&gt;/api/publish&lt;/code&gt; endpoint:&lt;/p&gt;
    &lt;code&gt;# Upload "MyPackage.1.0.0.nupkg" file
curl -X POST http://localhost:5963/api/publish \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"&lt;/code&gt;
    &lt;p&gt;You may be dissatisfied with publishing using this method. The dotnet command includes &lt;code&gt;dotnet nuget push&lt;/code&gt;, which is the standard approach.
However, in my experience, this protocol uses &lt;code&gt;multipart/form-data&lt;/code&gt; for transmission, which has caused issues with gateway services, reverse proxies, load balancers, and similar components.
Therefore, the current nuget-server does not implement this method and instead uses the simplest binary transmission procedure.&lt;/p&gt;
    &lt;p&gt;Another advantage is that when authentication is enabled, you don't need to manage Basic authentication and V3 API keys separately. You might still feel issue with managing read operations and publish operation with the same key, but in that case, you can simply separate the users.&lt;/p&gt;
    &lt;p&gt;For authentication feature, please refer to below chapter.&lt;/p&gt;
    &lt;p&gt;By default, packages are stored in the &lt;code&gt;./packages&lt;/code&gt; directory relative to where you run nuget-server.
You can customize this location using the &lt;code&gt;--package-dir&lt;/code&gt; option:&lt;/p&gt;
    &lt;code&gt;# Use default ./packages directory
nuget-server

# Use custom directory (relative or absolute path)
nuget-server --package-dir /another/package/location&lt;/code&gt;
    &lt;p&gt;Packages are stored in the filesystem using the following structure:&lt;/p&gt;
    &lt;code&gt;packages/
├── PackageName/
│   ├── 1.0.0/
│   │   ├── PackageName.1.0.0.nupkg
│   │   ├── PackageName.nuspec
│   │   └── icon.png            # Package icon (if present)
│   └── 2.0.0/
│       ├── PackageName.2.0.0.nupkg
│       ├── PackageName.nuspec
│       └── icon.jpg            # Package icon (if present)
└── AnotherPackage/
    └── 1.5.0/
        ├── AnotherPackage.1.5.0.nupkg
        ├── AnotherPackage.nuspec
        └── icon.png            # Package icon (if present)
&lt;/code&gt;
    &lt;p&gt;You can backup the package directory using simply &lt;code&gt;tar&lt;/code&gt; or other achiver:&lt;/p&gt;
    &lt;code&gt;cd /your/server/base/dir
tar -cf - ./packages | lz4 &amp;gt; backup-packages.tar.lz4&lt;/code&gt;
    &lt;p&gt;Restore is simply extract it and re-run nuget-server with the same package directory configuration, because nuget-server does not use any specialized storage such as databases.&lt;/p&gt;
    &lt;p&gt;nuget-server supports configuration through command-line options, environment variables, and JSON file.&lt;/p&gt;
    &lt;p&gt;Settings are applied in the following order (highest to lowest priority):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Command-line options&lt;/item&gt;
      &lt;item&gt;Environment variables&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;config.json&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;Default values&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can specify a custom configuration file:&lt;/p&gt;
    &lt;code&gt;# Using command line option
nuget-server --config-file /path/to/config.json
# or short alias
nuget-server -c /path/to/config.json

# Using environment variable
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
nuget-server&lt;/code&gt;
    &lt;p&gt;If not specified, nuget-server looks for &lt;code&gt;./config.json&lt;/code&gt; in the current directory.&lt;/p&gt;
    &lt;p&gt;Create a &lt;code&gt;config.json&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "packageDir": "./packages",
  "usersFile": "./users.json",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "trustedProxies": ["127.0.0.1", "::1"],
  "authMode": "none",
  "sessionSecret": "&amp;lt;your-secret-here&amp;gt;",
  "passwordMinScore": 2,
  "passwordStrengthCheck": true
}&lt;/code&gt;
    &lt;p&gt;All fields are optional. Only include the settings you want to override. Both &lt;code&gt;packageDir&lt;/code&gt; and &lt;code&gt;usersFile&lt;/code&gt; paths can be absolute or relative. If relative, they are resolved from the directory containing the &lt;code&gt;config.json&lt;/code&gt; file.&lt;/p&gt;
    &lt;p&gt;nuget-server also supports authentication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Authentication Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
        &lt;cell role="head"&gt;Auth Initialization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;none&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Default. No authentication required&lt;/cell&gt;
        &lt;cell&gt;Not required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;publish&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Authentication required only for package publishing&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;full&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Authentication required for all operations (must login first)&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To enable authentication on the NuGet server, first register an initial user using the &lt;code&gt;--auth-init&lt;/code&gt; option.&lt;/p&gt;
    &lt;p&gt;Create an initial admin user interactively:&lt;/p&gt;
    &lt;code&gt;nuget-server --auth-init&lt;/code&gt;
    &lt;p&gt;This command will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prompt for admin username (default: &lt;code&gt;admin&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Prompt for password (with strength checking, masked input)&lt;/item&gt;
      &lt;item&gt;Create &lt;code&gt;users.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Exit after initialization (server does not start)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When enabling authentication using a Docker image, use this option to generate the initial user.&lt;/p&gt;
    &lt;code&gt;Initializing authentication...
Enter admin username [admin]:
Enter password: ********
Confirm password: ********

============================================================
Admin user created successfully!
============================================================
Username: admin
Password: *********************
============================================================
&lt;/code&gt;
    &lt;p&gt;Users added with &lt;code&gt;--auth-init&lt;/code&gt; automatically become administrator users.
Administrator users can add or remove other users via the UI. They can also reset user passwords.&lt;/p&gt;
    &lt;p&gt;While administrator users can also be assigned API passwords (described later), we recommend separating users for management whenever possible.&lt;/p&gt;
    &lt;p&gt;The NuGet server distinguishes between the password used to log in to the UI and the password used by NuGet clients when accessing the server. The password used by NuGet clients when accessing the server is called the "API password," and access is granted using the combination of the user and the API password.&lt;/p&gt;
    &lt;p&gt;Please log in by displaying the UI in the browser. Select the “API password” menu from the UI menu to generate an API password. Using this API password will enable access from the NuGet client.&lt;/p&gt;
    &lt;p&gt;Here is an example of using the API password:&lt;/p&gt;
    &lt;code&gt;# Add source with API password
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" \
  -u admin \
  -p xxxxxxxxxxxxxxxxxxxxxx \
  --protocol-version 3 --store-password-in-clear-text --allow-insecure-connections&lt;/code&gt;
    &lt;p&gt;Or specify &lt;code&gt;nuget.config&lt;/code&gt; with credentials:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;configuration&amp;gt;
  &amp;lt;packageSources&amp;gt;
    &amp;lt;add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" /&amp;gt;
  &amp;lt;/packageSources&amp;gt;
  &amp;lt;packageSourceCredentials&amp;gt;
    &amp;lt;local&amp;gt;
      &amp;lt;add key="Username" value="reader" /&amp;gt;
      &amp;lt;add key="ClearTextPassword" value="xxxxxxxxxxxxxxxxxxxxxx" /&amp;gt;
    &amp;lt;/local&amp;gt;
  &amp;lt;/packageSourceCredentials&amp;gt;
&amp;lt;/configuration&amp;gt;&lt;/code&gt;
    &lt;p&gt;For package publishing:&lt;/p&gt;
    &lt;code&gt;# Publish packages with API password
curl -X POST http://localhost:5963/api/publish \
  -u admin:xxxxxxxxxxxxxxxxxxxxxx \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"&lt;/code&gt;
    &lt;p&gt;When publishing a package, you can send the package by setting Basic authentication in the &lt;code&gt;Authorization&lt;/code&gt; header.&lt;/p&gt;
    &lt;p&gt;nuget-server uses the &lt;code&gt;zxcvbn&lt;/code&gt; library to enforce strong password requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Evaluates password strength on a scale of 0-4 (Weak to Very Strong)&lt;/item&gt;
      &lt;item&gt;Default minimum score: 2 (Good)&lt;/item&gt;
      &lt;item&gt;Checks against common passwords, dictionary words, and patterns&lt;/item&gt;
      &lt;item&gt;Provides real-time feedback during password creation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Configure password requirements in &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "passwordMinScore": 2, // 0-4, default: 2 (Good)
  "passwordStrengthCheck": true // default: true
}&lt;/code&gt;
    &lt;p&gt;The NuGet server stores both "password" and "API password" as SALT hashed information, so no plaintext passwords are ever saved. However, if you do not use HTTPS (TLS), be aware that the &lt;code&gt;Authorization&lt;/code&gt; header will contain the plaintext password, making it vulnerable to sniffing.
When makes public endpoint, protect communications using HTTPS.&lt;/p&gt;
    &lt;p&gt;Import all packages from another NuGet server to your local nuget-server instance. This feature can be used when migrating the foreign NuGet server to nuget-server.&lt;/p&gt;
    &lt;p&gt;Import packages interactively in CLI:&lt;/p&gt;
    &lt;code&gt;nuget-server --import-packages --package-dir ./packages&lt;/code&gt;
    &lt;p&gt;This command will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prompt for source NuGet server URL&lt;/item&gt;
      &lt;item&gt;Ask if authentication is required&lt;/item&gt;
      &lt;item&gt;If needed, prompt for username and password (masked input)&lt;/item&gt;
      &lt;item&gt;Discover all packages from the source server&lt;/item&gt;
      &lt;item&gt;Download and import all packages to local storage&lt;/item&gt;
      &lt;item&gt;Display progress for each package (1% intervals)&lt;/item&gt;
      &lt;item&gt;Exit after import (server does not start)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Existing packages with the same version will be overwritten&lt;/item&gt;
      &lt;item&gt;Failed imports are logged with error details&lt;/item&gt;
      &lt;item&gt;Progress is reported at 1% intervals to reduce log noise&lt;/item&gt;
      &lt;item&gt;Package icons are preserved during import&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Parallel downloads are not done. This is to avoid making a large number of requests to the repository.&lt;/p&gt;
    &lt;p&gt;This feature is a type of downloader. Therefore, it does not need to be run on the actual host where it will operate. You can perform the import process in advance on a separate host and then move the &lt;code&gt;packages&lt;/code&gt; directory as-is.&lt;/p&gt;
    &lt;code&gt;Starting package import...
Enter source NuGet server URL [http://host.example.com/repository/nuget/]: https://nexus.example.com/repository/nuget/
Does the server require authentication? [y/N]: y
Enter username: reader
Enter password: **********

============================================================
Import Configuration:
Source: https://nexus.example.com/repository/nuget/
Target: ./packages
Authentication: reader (password hidden)
============================================================

Start importing packages? (existing packages will be overwritten) [y/N]: y

Discovering packages from source server...
Found 125 packages with 563 versions total.
Starting package import...
Progress: 100/563 packages (17%) - MyPackage.Core@1.2.3
Progress: 563/563 packages (100%) - AnotherPackage@2.0.0

============================================================
Import Complete!
============================================================
Total packages: 125
Total versions: 563
Successfully imported: 563
Failed: 0
Time elapsed: 125.3 seconds
============================================================
&lt;/code&gt;
    &lt;p&gt;The server supports running behind a reverse proxy. For example, when you have a public URL like &lt;code&gt;https://nuget.example.com&lt;/code&gt; and run nuget-server on a host within your internal network via a gateway.&lt;/p&gt;
    &lt;p&gt;In such cases, you MUST specify the base URL of the public URL to ensure the NuGet V3 API can provide the correct sub-endpoint address.&lt;/p&gt;
    &lt;p&gt;The server resolves URLs using the following priority order:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fixed base URL (highest priority): When &lt;code&gt;--base-url&lt;/code&gt;option is specified, it always takes precedence&lt;/item&gt;
      &lt;item&gt;Trusted proxy headers: When trusted proxies are configured with &lt;code&gt;--trusted-proxies&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;HTTP &lt;code&gt;Forwarded&lt;/code&gt;header (proto, host, port)&lt;/item&gt;&lt;item&gt;Traditional &lt;code&gt;X-Forwarded-*&lt;/code&gt;headers (&lt;code&gt;X-Forwarded-Proto&lt;/code&gt;,&lt;code&gt;X-Forwarded-Host&lt;/code&gt;,&lt;code&gt;X-Forwarded-Port&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;HTTP &lt;/item&gt;
      &lt;item&gt;Standard request information (fallback): Uses &lt;code&gt;Host&lt;/code&gt;header when proxy headers are not available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example &lt;code&gt;--base-url&lt;/code&gt; option:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;nuget-server served public base URL: &lt;code&gt;https://packages.example.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Actual NuGet V3 API endpoint: &lt;code&gt;https://packages.example.com/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Configure served base URL (do not include /v3 path)
nuget-server --base-url https://packages.example.com

# Add as NuGet source (HTTPS - no --allow-insecure-connections needed)
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3&lt;/code&gt;
    &lt;p&gt;Another option, you can configure with trusted proxy addresses:&lt;/p&gt;
    &lt;code&gt;# Configure trusted proxies for proper host header handling
nuget-server --trusted-proxies "10.0.0.1,192.168.1.100"&lt;/code&gt;
    &lt;p&gt;Environment variables are also supported:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_BASE_URL=https://packages.example.com
export NUGET_SERVER_TRUSTED_PROXIES=10.0.0.1,192.168.1.100
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
export NUGET_SERVER_USERS_FILE=/path/to/users.json
export NUGET_SERVER_SESSION_SECRET=your-secret-key-here&lt;/code&gt;
    &lt;p&gt;Docker images are available for multiple architectures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;linux/amd64&lt;/code&gt;(x86_64)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;linux/arm64&lt;/code&gt;(aarch64)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When pulling the image, Docker automatically selects the appropriate architecture for your platform.&lt;/p&gt;
    &lt;p&gt;Suppose you have configured the following directory structure for persistence (recommended):&lt;/p&gt;
    &lt;code&gt;docker-instance/
├── data/
│   ├── config.json
│   └── user.json
└── packages/
    └── (package files)
&lt;/code&gt;
    &lt;p&gt;Execute as follows:&lt;/p&gt;
    &lt;code&gt;# Pull and run the latest version
docker run -d -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# Or with Docker Compose
cat &amp;gt; docker-compose.yml &amp;lt;&amp;lt; EOF
version: '3'
services:
  nuget-server:
    image: kekyo/nuget-server:latest
    ports:
      - "5963:5963"
    volumes:
      - ./data:/data
      - ./packages:/packages
    environment:
      - NUGET_SERVER_AUTH_MODE=publish
EOF

docker-compose up -d&lt;/code&gt;
    &lt;p&gt;Your NuGet server is now available at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Web UI: &lt;code&gt;http://localhost:5963&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NuGet V3 API: &lt;code&gt;http://localhost:5963/v3/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Docker container runs as the &lt;code&gt;nugetserver&lt;/code&gt; user (UID 1001) for security reasons. You need to ensure that the mounted directories have the appropriate permissions for this user to write files.&lt;/p&gt;
    &lt;p&gt;Set proper permissions for mounted directories:&lt;/p&gt;
    &lt;code&gt;# Create directories if they don't exist
mkdir -p ./data ./packages

# Set ownership to UID 1001 (matches the container's nugetserver user)
sudo chown -R 1001:1001 ./data ./packages&lt;/code&gt;
    &lt;p&gt;Important: Without proper permissions, you may encounter &lt;code&gt;500 Permission Denied&lt;/code&gt; errors when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating or updating user accounts&lt;/item&gt;
      &lt;item&gt;Publishing packages&lt;/item&gt;
      &lt;item&gt;Writing configuration files&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run with default settings (port 5963, packages and data stored in mounted volumes)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# With authentication (users.json will be created in /data)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  -e NUGET_SERVER_AUTH_MODE=publish \
  kekyo/nuget-server:latest&lt;/code&gt;
    &lt;p&gt;You can also change settings using environment variables or command-line options, but the easiest way to configure settings is to use &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since the Docker image has mount points configured, you can mount &lt;code&gt;/data&lt;/code&gt; and &lt;code&gt;/packages&lt;/code&gt; as shown in the example above and place &lt;code&gt;/data/config.json&lt;/code&gt; there to flexibly configure settings. Below is an example of &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "authMode": "publish"
}&lt;/code&gt;
    &lt;p&gt;When initializing credentials or importing packages, configure &lt;code&gt;config.json&lt;/code&gt; and perform the operation via the CLI before launching the Docker image:&lt;/p&gt;
    &lt;code&gt;# Initialize authentication
nuget-server -c ./data/config.json --auth-init&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/data&lt;/code&gt;: Default data directory for&lt;code&gt;config.json&lt;/code&gt;,&lt;code&gt;users.json&lt;/code&gt;and other persistent data&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/packages&lt;/code&gt;: Default package storage directory (mounted to persist packages)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Default behavior: The Docker image runs with &lt;code&gt;--users-file /data/users.json --package-dir /packages&lt;/code&gt; by default.&lt;/p&gt;
    &lt;p&gt;Configuration priority (highest to lowest):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Custom command line arguments (when overriding CMD)&lt;/item&gt;
      &lt;item&gt;Environment variables (e.g., &lt;code&gt;NUGET_SERVER_PACKAGE_DIR&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;config.json&lt;/code&gt;file (if explicitly specified)&lt;/item&gt;
      &lt;item&gt;Default command line arguments in Dockerfile&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Various methods exist for automatically starting containers with systemd. Below is a simple example of configuring a systemd service using Podman. This is a simple service unit file used before quadlets were introduced to Podman. By placing this file and having systemd recognize it, you can automatically start the nuget-server:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;/etc/systemd/system/container-nuget-server.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# container-nuget-server.service

[Unit]
Description=Podman container-nuget-server.service
Documentation=man:podman-generate-systemd(1)
Wants=network-online.target
After=network-online.target
RequiresMountsFor=%t/containers

[Service]
Environment=PODMAN_SYSTEMD_UNIT=%n
Restart=always
RestartSec=30
TimeoutStopSec=70
ExecStart=/usr/bin/podman run \
        --cidfile=%t/%n.ctr-id \
        --cgroups=no-conmon \
        --rm \
        --sdnotify=conmon \
        --replace \
        -d \
        -p 5963:5963 \
        --name nuget_server \
        -v /export/data:/data -v /export/packages:/packages docker.io/kekyo/nuget-server:latest
ExecStop=/usr/bin/podman stop \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
ExecStopPost=/usr/bin/podman rm \
        -f \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
Type=notify
NotifyAccess=all

[Install]
WantedBy=default.target&lt;/code&gt;
    &lt;p&gt;The build of the nuget-server Docker image uses Podman.&lt;/p&gt;
    &lt;p&gt;Use the provided multi-platform build script that uses Podman to build for all supported architectures:&lt;/p&gt;
    &lt;code&gt;# Build for all platforms (local only, no push)
./build-docker-multiplatform.sh

# Build and push to Docker Hub
./build-docker-multiplatform.sh --push

# Build for specific platforms only
./build-docker-multiplatform.sh --platforms linux/amd64,linux/arm64

# Push with custom Docker Hub username
OCI_SERVER_USER=yourusername ./build-docker-multiplatform.sh --push

# Inspect existing manifest
./build-docker-multiplatform.sh --inspect&lt;/code&gt;
    &lt;p&gt;Important: For cross-platform builds, QEMU emulation must be configured first:&lt;/p&gt;
    &lt;code&gt;# Option 1: Use QEMU container (recommended)
sudo podman run --rm --privileged docker.io/multiarch/qemu-user-static --reset -p yes

# Option 2: Install system packages
# Ubuntu/Debian:
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y qemu-user-static
# Fedora/RHEL:
sudo dnf install -y qemu-user-static

# Verify QEMU is working:
podman run --rm --platform linux/arm64 alpine:latest uname -m
# Should output: aarch64&lt;/code&gt;
    &lt;p&gt;Without QEMU, you can only build for your native architecture.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;--auth-init&lt;/code&gt; and &lt;code&gt;--import-packages&lt;/code&gt; options require interactive responses from the operator.
Therefore, attempting to automate these may not work properly.
In such cases, you can provide credentials via environment variables:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_ADMIN_USERNAME=admin
export NUGET_SERVER_ADMIN_PASSWORD=MySecurePassword123!
nuget-server --auth-init --config-file ./config.json&lt;/code&gt;
    &lt;p&gt;This allows initialization in CI/CD pipelines without user interaction.&lt;/p&gt;
    &lt;p&gt;For special configurations (or to support persistent sessions), you can set a fixed session secret. Specify a sufficiently long value for the secret:&lt;/p&gt;
    &lt;code&gt;export NUGET_SERVER_SESSION_SECRET=$(openssl rand -base64 32)
nuget-server&lt;/code&gt;
    &lt;p&gt;(Or use &lt;code&gt;config.json&lt;/code&gt;.)&lt;/p&gt;
    &lt;p&gt;If not set, a random secret is generated (warning will be logged).&lt;/p&gt;
    &lt;p&gt;The server implements a subset of the NuGet V3 API protocol:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Service index: &lt;code&gt;/v3/index.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Package content: &lt;code&gt;/v3/package/{id}/index.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Package downloads: &lt;code&gt;/v3/package/{id}/{version}/{filename}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Registration index: &lt;code&gt;/v3/registrations/{id}/index.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Under MIT.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092734</guid></item><item><title>AI enters the grant game, picking winners</title><link>https://www.science.org/content/article/ai-enters-grant-game-picking-winners</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092880</guid></item><item><title>Git for Music – Using Version Control for Music Production (2023)</title><link>https://grechin.org/2023/05/06/git-and-reaper.html</link><description>&lt;doc fingerprint="1774f05c4fe586da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;git for music. Using version control for music production.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;last updated on 6 Apr 2024&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Being both a musician and a software engineer, I always felt that these two areas are almost completely separated. My developer skill-set seemed to have little to no use for my work as a musician. Which is a pity considering how cool it would be if there was some kind of a sinergy across these two sides of my life.&lt;/p&gt;
    &lt;p&gt;Recently, though, I have found a useful possibility to utilize something I previously used solely for my development work, namely, git, the version control tool, for my music production.&lt;/p&gt;
    &lt;p&gt;Okay, and now let’s get to the point and…&lt;/p&gt;
    &lt;head rend="h2"&gt;meet Git for music production&lt;/head&gt;
    &lt;p&gt;Did you notice yourself creating a dozen of versions of your project? Are the names like this familiar to you?&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;my-cool-song-new-vocals-brighter-mix-4.rpp&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Did you ever feel frustrated about unmanageability of all this and how sloppy you project directory ends up looking?&lt;/p&gt;
    &lt;p&gt;This version nightmare problem for software people has a solid and well-recognized solution: version control systems. Such as “git”, which is not only the most widely used one in the industry, but also completely free, open source and cross platform (that is working flawlessly on Win/Mac/Linux).&lt;/p&gt;
    &lt;p&gt;For music production, I use Reaper, and instead of creating dozens of copies of my project file (&lt;code&gt;my-cool-song.rpp&lt;/code&gt;), such as &lt;code&gt;my-cool-song-new-vocals-brighter-mix-4.rpp&lt;/code&gt;, I simply initialize a git repository in the project folder and put the file under version control. This git repository will be the “home” for managing the version of our music project.&lt;/p&gt;
    &lt;p&gt;By the way, a good supplementary for this reading could be this video of me going through an example. If you are not fan of watching videos, feel free to read on.&lt;/p&gt;
    &lt;head rend="h2"&gt;My git-based music production workflow&lt;/head&gt;
    &lt;p&gt;Although, when wearing a developer hat, I am normally in linux, for the music production stuff, due to the better availability of plugins and such, Windows is a better option. For Windows, you can install &lt;code&gt;git-bash&lt;/code&gt;, and have all the git functionality at your fingertips through a command-line interface.&lt;/p&gt;
    &lt;p&gt;First, I initialize a repository in the project directory. For me, it is most convenient to use a git bash command line terminal:&lt;/p&gt;
    &lt;code&gt;Acer@DESKTOP-NRN84IB MINGW64 /c/home/music
$ cd test_git_project/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project
$ git init .
Initialized empty Git repository in C:/home/music/test_git_project/.git/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project (master)
$
&lt;/code&gt;
    &lt;p&gt;in the example above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I first navigated to the directory with my project with &lt;code&gt;cd&lt;/code&gt;command&lt;/item&gt;
      &lt;item&gt;initialized a repository with &lt;code&gt;git init .&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;on the last, third line, my command prompt starts having a little &lt;code&gt;(master)&lt;/code&gt;thing, which is the default “branch” in my repository that Git has created for me&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I also create a &lt;code&gt;.gitignore&lt;/code&gt; file and that this is this particular project file that I want to track, and not any other, such as media or peak files:&lt;/p&gt;
    &lt;code&gt;*

!in_your_eyes_remix_git_managed.rpp
&lt;/code&gt;
    &lt;p&gt;Then I am free to work with the project in my DAW as usual. When I am done working on a specific version, I make a commit and give it a descriptive name, e.g. “bass vst settings adjusted”.&lt;/p&gt;
    &lt;p&gt;Then I can see all the versions of my project in &lt;code&gt;git gui&lt;/code&gt; tool.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;side note: you can use any git frontend, not only &lt;code&gt;git gui&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not only that, but I can also open any historical version of the project, create branches and so on. In other words, I can fully benefit from the version control system! If you are already using git, you know what I mean.&lt;/p&gt;
    &lt;p&gt;The days of versioned files mess in my project folder are finally gone! I wonder, though, if Reaper developers will be willing to incorporate that into their product one day.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managing other files (WAVs etc.)&lt;/head&gt;
    &lt;p&gt;Git is not super suited for managing big binary files (such as WAV samples and stems), but this is not a problem for me since I only manage the main project file.&lt;/p&gt;
    &lt;p&gt;About other files I do not care. Why? Becase I never remove them. The media files are either WAVs related to this project (and which are therefore kept in the project folder) or samples from my library. In both cases, these files are normally (at least withing the lifespan of the project under construction) never deleted.&lt;/p&gt;
    &lt;p&gt;This approach, which, I guess, I share with most producers, makes it easy to return to any historical version of the project and rely on the media files to be found.&lt;/p&gt;
    &lt;head rend="h3"&gt;collaborating with GIT? Not sure…&lt;/head&gt;
    &lt;p&gt;GIT is not only about versioning, but also about collaboration, with remote repositories and so on. Frankly, I don’t see it feasible for collaboration over music projects since the project files are normally opaque and we should not expect git or any other version control system to be able to merge/diff them.&lt;/p&gt;
    &lt;p&gt;And let’s not forget that to be able to work on your project, the collaborator needs to have very close set up: the DAW, the plugins and all the media files.&lt;/p&gt;
    &lt;p&gt;Another note of the remote repositories: I do find it useful that I can push my music project to github and this kind of a backup that will outlive my current PC. This is nice, but we can’t really consider it a real backup - because of missing media.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tracking TODO items for your music project in GitHub&lt;/head&gt;
    &lt;p&gt;Interesting use-case I’m currently testing is to have a “todo list”, think of an small per-project issue tracker with a list of things you plan to do later. Just a version-tracked text file of the format similar to this:&lt;/p&gt;
    &lt;code&gt;fix panning issues in chorus TODO
add one more synth layer TODO
&lt;/code&gt;
    &lt;p&gt;Once it’s in Github, you can update it from anywhere (GitHub allows you to edit files right in the browser), so, basically, you project gets its own, private, read/write website. On the go and got a cool idea? Now you know where to record it (don’t forget to &lt;code&gt;pull&lt;/code&gt; your update, though, once you are back to your DAW PC).&lt;/p&gt;
    &lt;p&gt;In conclusion, when we inspect this idea of “git for music” a bit closer, we can see that it does have a few viable applications. Yes, this tool is not magical, but still pretty useful!&lt;/p&gt;
    &lt;p&gt;Thanks for reading.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092895</guid></item><item><title>Google AI Overview made up an elaborate story about me</title><link>https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z</link><description>&lt;doc fingerprint="d0eece56e5def5e6"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. &lt;/p&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Post&lt;/head&gt;
      &lt;p&gt;Benn Jordan&lt;/p&gt;
      &lt;p&gt;bennjordan.bsky.social&lt;/p&gt;
      &lt;p&gt;did:plc:yhpu2wsyvdbsehluhclpqyyk&lt;/p&gt;
      &lt;p&gt;This is SO messed up. 🫥 I had a few messages and tags today asking me to clarify my stance on Israel, which was odd as I've been pretty outspoken against genocide and in full support of Palestinian statehood...&lt;/p&gt;
      &lt;p&gt;2025-08-31T07:09:14.503Z&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45092925</guid></item><item><title>Effective learning: Rules of formulating knowledge (1999)</title><link>https://www.supermemo.com/en/blog/twenty-rules-of-formulating-knowledge</link><description>&lt;doc fingerprint="3ed6d2664c2f1e8c"&gt;
  &lt;main&gt;
    &lt;p&gt;Dr Piotr Wozniak, February, 1999 (updated)&lt;/p&gt;
    &lt;p&gt;This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledge&lt;/p&gt;
    &lt;p&gt;The speed of learning will depend on the way you formulate the material. The same material can be learned many times faster if well formulated! The difference in speed can be stunning!&lt;/p&gt;
    &lt;p&gt;The rules are listed in the order of importance. Those listed first are most often violated or bring most benefit if complied with!&lt;/p&gt;
    &lt;p&gt;There is an underlying assumption that you will proceed with learning using spaced repetition, i.e. you will not just learn once but you will repeat the material optimally (as in SuperMemo).&lt;/p&gt;
    &lt;p&gt;The 20 rules of formulating knowledge in learning&lt;/p&gt;
    &lt;p&gt;1) Do not learn if you do not understand&lt;/p&gt;
    &lt;p&gt;Trying to learn things you do not understand may seem like an utmost nonsense. Still, an amazing proportion of students commit the offence of learning without comprehension. Very often they have no other choice! The quality of many textbooks or lecture scripts is deplorable while examination deadlines are unmovable.If you are not a speaker of German, it is still possible to learn a history textbook in German. The book can be crammed word for word. However, the time needed for such “blind learning” is astronomical. Even more important: The value of such knowledge is negligible. If you cram a German book on history, you will still know nothing of history.The German history book example is an extreme. However, the materials you learn may often seem well structured and you may tend to blame yourself for lack of comprehension. Soon you may pollute your learning process with a great deal of useless material that treacherously makes you believe “it will be useful some day”. &lt;/p&gt;
    &lt;p&gt;2) Learn before you memorize&lt;/p&gt;
    &lt;p&gt;Before you proceed with memorizing individual facts and rules, you need to build an overall picture of the learned knowledge. Only when individual pieces fit to build a single coherent structure, will you be able to dramatically reduce the learning time. This is closely related to the problem comprehension mentioned in Rule 1: Do not learn if you do not understand. A single separated piece of your picture is like a single German word in the textbook of history.Do not start from memorizing loosely related facts! First read a chapter in your book that puts them together (e.g. the principles of the internal combustion engine). Only then proceed with learning using individual questions and answers (e.g. What moves the pistons in the internal combustion engine?), etc.&lt;/p&gt;
    &lt;p&gt;3) Build upon the basics&lt;/p&gt;
    &lt;p&gt;The picture of the learned whole (as discussed in Rule 2: Learn before you memorize) does not have to be complete to the last detail. Just the opposite, the simpler the picture the better. The shorter the initial chapter of your book the better. Simple models are easier to comprehend and encompass. You can always build upon them later on.Do not neglect the basics. Memorizing seemingly obvious things is not a waste of time! Basics may also appear volatile and the cost of memorizing easy things is little. Better err on the safe side. Remember that usually you spend 50% of your time repeating just 3-5% of the learned material! Basics are usually easy to retain and take a microscopic proportion of your time. However, each memory lapse on basics can cost you dearly!&lt;/p&gt;
    &lt;p&gt;4) Stick to the minimum information principle&lt;/p&gt;
    &lt;p&gt;The material you learn must be formulated in as simple way as it is&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple is easy&lt;lb/&gt;By definition, simple material is easy to remember. This comes from the fact that its simplicity makes is easy for the brain to process it always in the same way. Imagine a labyrinth. When making a repetition of a piece of material, your brain is running through a labyrinth (you can view a neural network as a tangle of paths). While running through the labyrinth, the brain leaves a track on the walls. If it can run in only one unique way, the path is continuous and easy to follow. If there are many combinations, each run may leave a different trace that will interfere with other traces making it difficult to find the exit. The same happens on the cellular level with different synaptic connections being activated at each repetition of complex material&lt;/item&gt;
      &lt;item&gt;Repetitions of simple items are easier to schedule&lt;lb/&gt;I assume you will make repetitions of the learned material using optimum inter-repetition intervals (as in SuperMemo). If you consider an item that is composed of two sub-items, you will need to make repetitions that are frequent enough to keep the more difficult item in memory. If you split the complex item into sub-items, each can be repeated at its own pace saving your time. Very often, inexperienced students create items that could easily be split into ten or more simpler sub-items! Although the number of items increases, the number of repetitions of each item will usually be small enough to greatly outweigh the cost of (1) forgetting the complex item again and again, (2) repeating it in excessively short intervals or (3) actually remembering it only in part!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here is a striking example:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Ill-formulated knowledge – Complex and wordy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What are the characteristics of the Dead Sea?&lt;p&gt;A: Salt lake located on the border between Israel and Jordan. Its shoreline is the lowest point on the Earth’s surface, averaging 396 m below sea level. It is 74 km long. It is seven times as salty (30% by volume) as the ocean. Its density keeps swimmers afloat. Only simple organisms can live in its saline waters&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-formulated knowledge – Simple and specific&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Where is the Dead Sea located?&lt;p&gt;A: on the border between Israel and Jordan&lt;/p&gt;&lt;p&gt;Q: What is the lowest point on the Earth’s surface?&lt;/p&gt;&lt;p&gt;A: The Dead Sea shoreline&lt;/p&gt;&lt;p&gt;Q: What is the average level on which the Dead Sea is located?&lt;/p&gt;&lt;p&gt;A: 400 meters (below sea level)&lt;/p&gt;&lt;p&gt;Q: How long is the Dead Sea?&lt;/p&gt;&lt;p&gt;A: 70 km&lt;/p&gt;&lt;p&gt;Q: How much saltier is the Dead Sea than the oceans?&lt;/p&gt;&lt;p&gt;A: 7 times&lt;/p&gt;&lt;p&gt;Q: What is the volume content of salt in the Dead Sea?&lt;/p&gt;&lt;p&gt;A: 30%&lt;/p&gt;&lt;p&gt;Q: Why can the Dead Sea keep swimmers afloat?&lt;/p&gt;&lt;p&gt;A: due to high salt content&lt;/p&gt;&lt;p&gt;Q: Why is the Dead Sea called Dead?&lt;/p&gt;&lt;p&gt;A: because only simple organisms can live in it&lt;/p&gt;&lt;p&gt;Q: Why only simple organisms can live in the Dead Sea?&lt;/p&gt;&lt;p&gt;A: because of high salt content&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You might want to experiment and try to learn two subjects using the two above approaches and see for yourself what advantage is brought by minimum information principle. This is particularly visible in the long perspective, i.e. the longer the time you need to remember knowledge, the more you benefit from simplifying your items!&lt;/p&gt;
    &lt;p&gt;Note in the example above how short the questions are. Note also that the answers are even shorter! We want a minimum amount of information to be retrieved from memory in a single repetition! We want answer to be as short as imaginably possible!&lt;/p&gt;
    &lt;p&gt;You will notice that the knowledge learned in the ill-structured example is not entirely equivalent to the well-structured formulation. For example, although you will remember why the Dead Sea can keep swimmers afloat, you may forget that it at all has such a characteristic in the first place! Additionally, rounding 396 to 400 and 74 to 70 produces some loss of information. These can be remedied by adding more questions or making the present ones more precise.&lt;/p&gt;
    &lt;p&gt;You will also lose the ability to fluently recite the description of the Dead Sea when called up to the blackboard by your teachers. I bet, however, that shining in front of the class is not your ultimate goal in learning. To see how to cope with recitations and poems, read further (section devoted to enumerations)&lt;/p&gt;
    &lt;p&gt;5) Cloze deletion is easy and effective&lt;/p&gt;
    &lt;p&gt;Cloze deletion is a sentence with its parts missing and replaced by three dots. Cloze deletion exercise is an exercise that uses cloze deletion to ask the student to fill in the gaps marked with the three dots. For example, Bill …[name] was the second US president to go through impeachment.If you are a beginner and if you find it difficult to stick to the minimum information principle, use cloze deletion! If you are an advanced user, you will also like cloze deletion. It is a quick and effective method of converting textbook knowledge into knowledge that can be subject to learning based on spaced repetition. Cloze deletion makes the core of the fast reading and learning technique called incremental reading.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Ill-formulated knowledge – Complex and wordy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What was the history of the Kaleida company?&lt;p&gt;A: Kaleida, funded to the tune of $40 million by Apple Computer and IBM in 1991. Hyped as a red-hot startup, Kaleida’s mission was to create a multimedia programming language It finally produced one, called Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in 1995.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-formulated knowledge – Simple cloze deletion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Kaleida was funded to the tune of …(amount) by Apple Computer and IBM in 1991&lt;p&gt;A: $40 million&lt;/p&gt;&lt;p&gt;Q: Kaleida was funded to the tune of $40 million by …(companies) in 1991&lt;/p&gt;&lt;p&gt;A: Apple and IBM&lt;/p&gt;&lt;p&gt;Q: Kaleida was funded to the tune of $40 million by Apple Computer and IBM in … (year)&lt;/p&gt;&lt;p&gt;A: 1991&lt;/p&gt;&lt;p&gt;Q: …(company) mission was to create a multimedia programming language. It finally produced one, called Script X. But it took three years&lt;/p&gt;&lt;p&gt;A: Kaleida’s&lt;/p&gt;&lt;p&gt;Q: Kaleida’s mission was to create a … It finally produced one, called Script X. But it took three years&lt;/p&gt;&lt;p&gt;A: multimedia programming language&lt;/p&gt;&lt;p&gt;Q: Kaleida’s mission was to create a multimedia programming language. It finally produced one, called … But it took three years&lt;/p&gt;&lt;p&gt;A: Script X&lt;/p&gt;&lt;p&gt;Q: Kaleida’s mission was to create a multimedia programming language. It finally produced one, called Script X. But it took …(time)&lt;/p&gt;&lt;p&gt;A: three years&lt;/p&gt;&lt;p&gt;Q: Kaleida’s mission was to create a multimedia programming language: Script X. But it took three years. Meanwhile, companies such as … had snapped up all the business&lt;/p&gt;&lt;p&gt;A: Macromedia/Asymetrix&lt;/p&gt;&lt;p&gt;Q: Kaleida’s mission was to create Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in …(year)&lt;/p&gt;&lt;p&gt;A: 1995&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Optional: SuperMemo Recipe:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SuperMemo 2002&lt;/cell&gt;
        &lt;cell&gt;SuperMemo 2000&lt;/cell&gt;
        &lt;cell&gt;SuperMemo 98/99&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Creating cloze deletions in new SuperMemos:select the keyword that is to be replaced with tree dots and press Alt+Z&lt;/cell&gt;
        &lt;cell&gt;Generating a cloze deletions from texts placed in the clipboard in SuperMemo 2000:&lt;p&gt;1. Press Ctrl+Alt+N to paste the text to SuperMemo&lt;/p&gt;&lt;p&gt;2. Select the part that is to be replaced with three dots 3. Right-click to open the component menu and select&lt;/p&gt;&lt;p&gt;Reading : Remember cloze (or click one of cloze icons on the reading toolbar)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Cloze deletions in SuperMemo 98/99:&lt;p&gt;1. Press Ctrl+A to add a standard question-and-answer item&lt;/p&gt;&lt;p&gt;2. Paste the text into the question field. This will create the outline of your items&lt;/p&gt;&lt;p&gt;3. Press Ctrl+Alt+U to Duplicate the element&lt;/p&gt;&lt;p&gt;4. Select the part that is to be replaced with three dots&lt;/p&gt;&lt;p&gt;5. Cut the selection to the clipboard (e.g. with Shift+Del)&lt;/p&gt;&lt;p&gt;6. Type in three dots (optionally, add the explanation in parentheses as in above examples)&lt;/p&gt;&lt;p&gt;7. Press Ctrl+T to save the question field and move to the answer field&lt;/p&gt;&lt;p&gt;8. Paste the text cut in Step 5 (e.g. with Shift+Ins or Ctrl+V). Your first item is ready&lt;/p&gt;&lt;p&gt;9. Press PgUp to go back to the outline item created in Step 2&lt;/p&gt;&lt;p&gt;10. Goto Step 3 and continue adding new items&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;6) Use imagery&lt;/p&gt;
    &lt;p&gt;Visual cortex is that part of the brain in which visual stimuli are interpreted. It has been very well developed in the course of evolution and that is why we say one picture is worth a thousand words. Indeed if you look at the number of details kept in a picture and the easiness with which your memory can retain them, you will notice that our verbal processing power is greatly inferior as compared with the visual processing power. The same refers to memory. A graphic representation of information is usually far less volatile.Usually it takes much less time to formulate a simple question-and-answer pair than to find or produce a neat graphic image. This is why you will probably always have to weigh up cost and profits in using graphics in your learning material. Well-employed images will greatly reduce your learning time in areas such as anatomy, geography, geometry, chemistry, history, and many more.The power of imagery explains why the concept of Tony Buzan’s mind maps is so popular. A mind map is an abstract picture in which connections between its components reflect the logical connections between individual concepts.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Less beneficial formulation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What African country is located between Kenya, Zambia and Mozambique?A: Tanzania&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-formulated knowledge – Simple cloze deletion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What African country is marked white on the map?&lt;p&gt;A: Tanzania&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;7) Use mnemonic techniques&lt;/p&gt;
    &lt;p&gt;Mnemonic techniques are various techniques that make remembering easier. They are often amazingly effective. For most students, a picture of a 10-year-old memorizing a sequence of 50 playing cards verges on discovering a young genius. It is very surprising then to find out how easy it is to learn the techniques that make it possible with a dose of training. These techniques are available to everyone and do not require any special skills!Before you start believing that mastering such techniques will provide you with an eternal solution to the problem of forgetting, be warned that the true bottleneck towards long-lasting and useful memories is not in quickly memorizing knowledge! This is indeed the easier part. The bottleneck lies in retaining memories for months, years or for lifetime! To accomplish the latter you will need SuperMemo and the compliance with the 20 rules presented herein.There have been dozens of books written about mnemonic techniques. Probably those written by Tony Buzan are most popular and respected. You can search the web for keywords such as: mind maps, peg lists, mnemonic techniques, etc.Experience shows that with a dose of training you will need to consciously apply mnemonic techniques in only 1-5% of your items. With time, using mnemonic techniques will become automatic!&lt;lb/&gt;Exemplary mind map:&lt;/p&gt;
    &lt;p&gt;8) Graphic deletion is as good as cloze deletion&lt;/p&gt;
    &lt;p&gt;Graphic deletion works like cloze deletion but instead of a missing phrase it uses a missing image component. For example, when learning anatomy, you might present a complex illustration. Only a small part of it would be missing. The student’s job is to name the missing area. The same illustration can be used to formulate 10-20 items! Each item can ask about a specific subcomponent of the image. Graphic deletion works great in learning geography!Exemplary graphic deletion:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SuperMemo 2000/2002&lt;/cell&gt;
        &lt;cell&gt;SuperMemo 99&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;This is how you can quickly generate graphic deletion using a picture from the clipboard:&lt;p&gt;1. Press Shift+Ins to paste the picture to SuperMemo&lt;/p&gt;&lt;p&gt;2. Press Ctrl+Shift+M and choose Occlusion template to apply graphic deletion template&lt;/p&gt;&lt;p&gt;3. SuperMemo 2000 only: Choose Ctrl+Shift+F2 to impose and detach the Occlusion template&lt;/p&gt;&lt;p&gt;4. Fill out the fields and place the occlusion rectangle to cover the appropriate part of the picture (use Alt+click twice to set the rectangle in the dragging mode)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;In SuperMemo 99 you will need a few more steps:&lt;p&gt;1.Create an item containing the following components:&lt;/p&gt;&lt;p&gt;– question text: What is the name of the area covered with the red rectangle?&lt;/p&gt;&lt;p&gt;– empty answer text (click Answer on the component menu)&lt;/p&gt;&lt;p&gt;– your illustration (use Import file on the image component menu)&lt;/p&gt;&lt;p&gt;– red rectangle component (choose red color with Color on the rectangle component menu)&lt;/p&gt;&lt;p&gt;2. Choose Duplicate on the element menu (e.g. by pressing Ctrl+Alt+U)&lt;/p&gt;&lt;p&gt;3. Ctrl+click the rectangle component twice to place it in the dragging mode&lt;/p&gt;&lt;p&gt;4. Drag and size the red rectangle to cover the area in question&lt;/p&gt;&lt;p&gt;5. Type in the answer in the answer field&lt;/p&gt;&lt;p&gt;6. Press PgUp to go back to the original element created in Step 1&lt;/p&gt;&lt;p&gt;7. Go to Step 2 to add generate more graphic deletions&lt;/p&gt;&lt;p&gt;Note that you could also paint covering rectangles or circles on the original image but this would greatly increase the size of your collection. The above method makes sure that you reuse the same image many times in all items of the same template. For example, the collection Brain Anatomy available from &amp;gt;SuperMemo Library and on SuperMemo MegaMix CD-ROM uses the above technique&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;9) Avoid sets&lt;/p&gt;
    &lt;p&gt;A set is a collection of objects. For example, a set of fruits might be an apple, a pear and a peach. A classic example of an item that is difficult to learn is an item that asks for the list of the members of a set. For example: What countries belong to the European Union? You should avoid such items whenever possible due to the high cost of retaining memories based on sets. If sets are absolutely necessary, you should always try to convert them into enumerations. Enumerations are ordered lists of members (for example, the alphabetical list of the members of the EU). Enumerations are also hard to remember and should be avoided. However, the great advantage of enumerations over sets is that they are ordered and they force the brain to list them always in the same order. An ordered list of countries contains more information than the set of countries that can be listed in any order. Paradoxically, despite containing more information, enumerations are easier to remember. The reason for this has been discussed earlier in the context of the minimum information principle: you should always try to make sure your brain works in the exactly same way at each repetition. In the case of sets, listing members in varying order at each repetition has a disastrous effect on memory. It is nearly impossible to memorize sets containing more than five members without the use of mnemonic techniques, enumeration, grouping, etc. Despite this claim, you will often succeed due to subconsciously mastered techniques that help you go around this problem. Those techniques, however, will fail you all too often. For that reason: Avoid sets! If you need them badly, convert them into enumerations and use techniques for dealing with enumerations&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Ill-formulated knowledge – Sets are unacceptable!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What countries belong to the European Union (2002)?&lt;p&gt;A: Austria, Belgium, Denmark, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, Sweden, and the United Kingdom.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Well-formulated knowledge – Converting a set into a meaningful listing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Which country hosted a meeting to consider the creation of a European Community of Defence in 1951?&lt;p&gt;A: France&lt;/p&gt;&lt;p&gt;Q: Which countries apart from France joined the European Coal and Steel Community in 1952?&lt;/p&gt;&lt;p&gt;A: Germany, Italy and the Benelux&lt;/p&gt;&lt;p&gt;Q: What countries make up the Benelux?&lt;/p&gt;&lt;p&gt;A: Belgium, Luxembourg, and the Netherlands&lt;/p&gt;&lt;p&gt;Q: Whose membership did Charles de Gaulle oppose in the 1960s?&lt;/p&gt;&lt;p&gt;A: that of UK&lt;/p&gt;&lt;p&gt;Q: Which countries joined the EEC along the UK in 1973?&lt;/p&gt;&lt;p&gt;A: Ireland and Denmark&lt;/p&gt;&lt;p&gt;Q: Which country joined the EEC in 1981?&lt;/p&gt;&lt;p&gt;A: Greece&lt;/p&gt;&lt;p&gt;Q: Which countries joined the EEC in 1986?&lt;/p&gt;&lt;p&gt;A: Spain and Portugal&lt;/p&gt;&lt;p&gt;Q: Which countries joined the EU in 1995?&lt;/p&gt;&lt;p&gt;A: Austria, Sweden and Finland&lt;/p&gt;&lt;p&gt;Q: What was the historic course of expansion of the European Union membership?&lt;/p&gt;&lt;p&gt;A: (1) France and (2) Germany, Italy and the Benelux, (3) UK and (4) Ireland and Denmark, (5) Greece, (6) Spain and Portugal and (7) Austria, Sweden and Finland&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note that in the example above, we converted a 15-member set into 9 items, five of which are 2-3 member sets, and one is a six member enumeration. Put it to your SuperMemo, and see how easy it is to generate the list of the European Union members using the historic timeline! Note the tricks used with France and the UK. They joined the union in the company of others but have been listed as separate items to simplify the learning process. Note also that the sum of information included in this well-formulated approach is far greater than that of the original set. Thus along simplicity, we gained some useful knowledge. All individual items effectively comply with the minimum information principle! You could go further by trying to split the Germany-Italy-Benelux set or using mnemonic techniques to memorize the final seven-member enumeration (i.e. the last of the questions above). However, you should take those steps only if you have any problems with retaining the proposed set in memory.&lt;/p&gt;
    &lt;p&gt;10) Avoid enumerations&lt;/p&gt;
    &lt;p&gt;Enumerations are also an example of classic items that are hard to learn. They are still far more acceptable than sets. Avoid enumerations wherever you can. If you cannot avoid them, deal with them using cloze deletions (overlapping cloze deletions if possible). Learning the alphabet can be a good example of an overlapping cloze deletion:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Hard to learn item&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What is the sequence of letters in the alphabet?&lt;p&gt;A: abcdefghijklmnopqrstuvwxyz&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Easy to learn items&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What three letters does the alphabet begin with?&lt;p&gt;A: ABCQ: Fill out the missing letters of the alphabet A … … … E&lt;/p&gt;&lt;p&gt;A: B, C, D&lt;/p&gt;&lt;p&gt;Q: Fill out the missing letters of the alphabet B … … … F&lt;/p&gt;&lt;p&gt;A: C, D, E&lt;/p&gt;&lt;p&gt;Q: Fill out the missing letters of the alphabet C … … … G&lt;/p&gt;&lt;p&gt;A: D, E, F&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The above items will make learning the alphabet much faster. The greatest advantage of the above approach is that is it easier for psychological reasons: the student does not have to stop repetitions to recite the whole sequence and can only focus on a small part of the learned material. Still it is recommended that he recite the whole alphabet after making the repetition. However, once all individual pieces are well remembered, reciting the whole should be a pleasant and speedy action that produces little frustration.&lt;lb/&gt;The cloze deletion used above is an overlapping cloze deletion, i.e. the same parts of the enumeration are strengthened in memory using different items (for example, the sequence C-D will be needed to recall the second and the third item). This redundancy does not contradict the minimum information principle because the extra information is added in extra items.&lt;/p&gt;
    &lt;p&gt;You can also deal with enumerations by using grouping like in the case of sets (see the European Union example) but cloze deletions should be simpler and should suffice in most cases.&lt;lb/&gt;Learning poems is an example of learning enumerations (all words and sentences have to be uttered in a predefined sequence); however, due to strong semantic connections, the rhyme and the rhythm, it may often be possible to effectively remember poems without using cloze deletion and without the frustration of forgetting small subcomponents again and again. However, once you notice you stumble with your poem, you should dismember it using cloze deletion and thus make sure that the learning is fast, easy, effective and pleasurable&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;A poem that is hard to remember&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: The credit belongs … (Teddy Roosevelt)&lt;p&gt;A: The credit belongs to the man who’s actually in the arena, whose face is marred by dust and sweat; a man who knows the great enthusiasm and the great devotions, who spends himself in a worthy cause, who in the end knows the triumph of high achievement, so that his place shall never be with those cold and timid souls who know neither victory nor defeat&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A poem split into easy items&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: The credit belongs … (Teddy Roosevelt)&lt;p&gt;A: to the man who’s actually in the arena&lt;/p&gt;&lt;p&gt;Q: The credit belongs to the man who’s actually in the arena …&lt;/p&gt;&lt;p&gt;A: whose face is marred by dust and sweat (a man who knows the great enthusiasm)&lt;/p&gt;&lt;p&gt;Q: whose face is marred by dust and sweat … (The credit belongs)&lt;/p&gt;&lt;p&gt;A: a man who knows the great enthusiasm and the great devotions (who spends himself in a worthy cause)&lt;/p&gt;&lt;p&gt;Q: a man who knows the great enthusiasm and the great devotions … (The credit belongs)&lt;/p&gt;&lt;p&gt;A: who spends himself in a worthy cause (who in the end knows the triumph of high achievement)&lt;/p&gt;&lt;p&gt;Q: who spends himself in a worthy cause … (The credit belongs)&lt;/p&gt;&lt;p&gt;A: who in the end knows the triumph of high achievement (so that his place shall never be), etc. etc.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Does it all sound artificial? It does! But you will never know how effective this approach is until you try it by yourself!&lt;/p&gt;
    &lt;p&gt;11) Combat interference&lt;/p&gt;
    &lt;p&gt;When you learn about similar things you often confuse them. For example, you may have problems distinguishing between the meanings of the words historic and historical. This will even be more visible if you memorize lots of numbers, e.g. optimum dosages of drugs in pharmacotherapy. If knowledge of one item makes it harder to remember another item, we have a case of memory interference. You can often remember an item for years with straight excellent grades until … you memorize another item that makes it nearly impossible to remember either! For example, if you learn geography and you memorize that the country located between Venezuela, Suriname and Brazil is Guyana, you are likely to easily recall this fact for years with just a couple of repetitions. However, once you add similar items asking about the location of all these countries, and French Guyana, and Colombia and more, you will suddenly notice strong memory interference and you may experience unexpected forgetting. In simple terms: you will get confused about what is what.&lt;lb/&gt;Interference is probably the single greatest cause of forgetting in collections of an experienced user of SuperMemo. You can never be sure when it strikes, and the only hermetic procedure against it is to detect and eliminate. In other words, in many cases it may be impossible to predict interference at the moment of formulating knowledge. Interference can also occur between remotely related items like Guyana, Guyard and Guyenne, as well as Guyana, kayman and … aspirin. It may work differently for you and for your colleague. It very hard to predict.Still you should do your best to prevent interference before it takes its toll. This will make your learning process less stressful and mentally bearable. Here are some tips:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;make items as unambiguous as possible&lt;/item&gt;
      &lt;item&gt;stick to the minimum information principle (many of the remaining rules in this text are based on avoiding interference!)&lt;/item&gt;
      &lt;item&gt;eliminate interference as soon as you spot it, i.e. before it becomes your obsession (e.g. as soon as you see the word inept you think “I know the meanings of inept and inapt but I will never know which is which!”)&lt;/item&gt;
      &lt;item&gt;in SuperMemo use View : Other browsers : Leeches(Shift+F3) to regularly review and eliminate most difficult items&lt;/item&gt;
      &lt;item&gt;read more: Memory interference&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;12) Optimize wording&lt;/p&gt;
    &lt;p&gt;The wording of your items must be optimized to make sure that in minimum time the right bulb in your brain lights up. This will reduce error rates, increase specificity, reduce response time, and help your concentration.Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based … blew past. PageMaker, now owned by Adobe, remains No. 2&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Less optimum item: cloze deletion that is too wordy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based … blew past. PageMaker, now owned by Adobe, remains No. 2&lt;p&gt;A: Quark&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Better item: fewer words will speed up learning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: Aldus invented desktop publishing in 1985 with PageMaker but failed to improve. Then … blew past (PageMaker remains No. 2)&lt;p&gt;A: Quark&lt;/p&gt;&lt;p&gt;Or better:&lt;/p&gt;&lt;p&gt;Q: Aldus invented desktop publishing with PageMaker but failed to improve. It was soon outdistanced by …&lt;/p&gt;&lt;p&gt;A: Quark&lt;/p&gt;&lt;p&gt;Or better:&lt;/p&gt;&lt;p&gt;Q: PageMaker failed to improve and was outdistanced by …&lt;/p&gt;&lt;p&gt;A: Quark&lt;/p&gt;&lt;p&gt;Or better:&lt;/p&gt;&lt;p&gt;Q: PageMaker lost ground to …&lt;/p&gt;&lt;p&gt;A: Quark&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note that the loss of information content in this item is inconsequential. During repetition you are only supposed to learn the name: Quark. You should not hope that the trailing messages on the ownership of PageMaker and the year of its development will somehow trickle to your memory as a side effect. You should decide if the other pieces of information are important to you and if so, store them in separate items (perhaps reusing the above text, employing cloze deletion again and optimizing the wording in a new way). Otherwise the redundant information will only slow down your learning process!&lt;/p&gt;
    &lt;p&gt;13) Refer to other memories&lt;/p&gt;
    &lt;p&gt;Referring to other memories can place your item in a better context, simplify wording, and reduce interference. In the example below, using the words humble and supplicant helps the student focus on the word shamelessly and thus strengthen the correct semantics. Better focus helps eliminating interference. Secondly, the use of the words humble and supplicant makes it possible to avoid interference of cringing with these words themselves. Finally, the proposed wording is shorter and more specific. Naturally, the rules basics-to-details and do not learn what you do not understand require that the words humble and supplicant be learned beforehand (or at least at the same time)&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Item subject to strong interference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: derog: adj: shamelessly conscious of one’s failings and asking in a begging way&lt;p&gt;A: cringing&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Item that uses interfering memories to amplify the correct meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: derog: adj: shamelessly humble and supplicant&lt;p&gt;A: cringing&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;14) Personalize and provide examples&lt;/p&gt;
    &lt;p&gt;One of the most effective ways of enhancing memories is to provide them with a link to your personal life. In the example below you will save time if you use a personal reference rather than trying to paint a picture that would aptly illustrate the question&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Item subject to strong interference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What is the name of a soft bed without arms or back?&lt;p&gt;A: divan&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Item that uses interfering memories to amplify the correct meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What is the name of a soft bed without arms or back? (like the one at Robert’s parents)&lt;p&gt;A: divan&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;lb/&gt;If you remember exactly what kind of soft bed can be found in Robert’s parents’ apartment you will save time by not having to dig exactly into the semantics of the definition and/or looking for an appropriate graphic illustration for the piece of furniture in question. Personalized examples are very resistant to interference and can greatly reduce your learning time&lt;/p&gt;
    &lt;p&gt;15) Rely on emotional states&lt;/p&gt;
    &lt;p&gt;If you can illustrate your items with examples that are vivid or even shocking, you are likely to enhance retrieval (as long as you do not overuse same tools and fall victim of interference!). Your items may assume bizarre form; however, as long as they are produced for your private consumption, the end justifies the means. Use objects that evoke very specific and strong emotions: love, sex, war, your late relative, object of your infatuation, Linda Tripp, Nelson Mandela, etc. It is well known that emotional states can facilitate recall; however, you should make sure that you are not deprived of the said emotional clues at the moment when you need to retrieve a given memory in a real-life situation&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Harder item&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: a light and joking conversation&lt;p&gt;A: banter&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Easier item&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: a light and joking conversation (e.g. Mandela and de Klerk in 1992)&lt;p&gt;A: banter&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;lb/&gt;If you have vivid and positive memories related to the meetings between Nelson Mandela and F.W. de Klerk, you are likely to quickly grasp the meaning of the definition of banter. Without the example you might struggle with interference from words such as badinage or even chat. There is no risk of irrelevant emotional state in this example as the state helps to define the semantics of the learned concept! A well-thought example can often reduce your learning time several times! I have recorded examples in which an item without an example was forgotten 20 times within one year, while the same item with a subtle interference-busting example was not forgotten even once in ten repetitions spread over five years. This is roughly equivalent to 25-fold saving in time in the period of 20 years! Such examples are not rare! They are most effectively handled with the all the preceding rules targeted on simplicity and against the interference&lt;/p&gt;
    &lt;p&gt;16) Context cues simplify wording&lt;/p&gt;
    &lt;p&gt;You can use categories in SuperMemo 2000/2002, provide different branches of knowledge with a different look (different template), use reference labels (Title, Author, Date, etc.) and clearly label subcategories (e.g. with strings such as chem for chemistry, math for mathematics, etc.). This will help you simplify the wording of your items as you will be relieved from the need to specify the context of your question. In the example below, the well-defined prefix bioch: saves you a lot of typing and a lot of reading while still making sure you do not confuse the abbreviation GRE with Graduate Record Examination. Note that in the recommended case, you process the item starting from the label bioch which puts your brain immediately in the right context. While processing the lesser optimum case, you will waste precious milliseconds on flashing the standard meaning of GRE and … what is worse … you will light up the wrong areas of your brain that will now perhaps be prone to interference!&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Wordy item can cause accidental lapses through interference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: What does GRE stand for in biochemistry?&lt;p&gt;A: glucocorticoid response element&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Context-labeled items increase success rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Q: bioch: GRE&lt;p&gt;A: glucocorticoid response element&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;17) Redundancy does not contradict minimum information principle&lt;/p&gt;
    &lt;p&gt;Redundancy in simple terms is more information than needed or duplicate information, etc. Redundancy does not have to contradict the minimum information principle and may even be welcome. The problem of redundancy is too wide for this short text. Here are some examples that are only to illustrate that minimum information principle cannot be understood as minimum number of characters or bits in your collections or even items:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;passive and active approach: if you learn a foreign language, e.g. Esperanto, you will often build word pairs such as phone-telefono, language-lingvo, hope-esperanto, etc. These pairs require active recall of the foreign word. Active recall does not, however, guarantee passive recognition and you may fail with telefono-phone, lingvo-language, or esperanto-hope. Adding new elements with swapped questions and answers may in some cases be redundant but it does not contradict the minimum information principle! Your items are still as simple as possible. You just get more of themIn SuperMemo 2000/2002, you can quickly generate swapped word-pair items with Duplicate (Ctrl+Alt+D) and Swap (Ctrl+Shift+S)&lt;/item&gt;
      &lt;item&gt;reasoning cues: you will often want to boost your reasoning ability by asking about a solution to the problem. Instead of just memorizing the answer you would like to quickly follow the reasoning steps (e.g. solve a simple mathematical equation) and generate the answer. In such a case, providing the hint on the reasoning steps in the answer will only serve helping you always follow the right path at repetitions&lt;/item&gt;
      &lt;item&gt;derivation steps: in more complex problems to solve, memorizing individual derivation steps is always highly recommended (e.g. solving complex mathematical problems). It is not cramming! It is making sure that the brain can always follow the fastest path while solving the problem. For more on boosting creativity and intelligence read: Roots of genius and creativity, as well as more specific: Derivation, reasoning and intelligence&lt;/item&gt;
      &lt;item&gt;multiple semantic representation: very often the same knowledge can be represented and viewed from different angles. Memorizing different representations of the same fact or rule is recommended in cases where a given memory is of high value. This will increase the expected recall rate (beyond that specified with the forgetting index)!&lt;/item&gt;
      &lt;item&gt;flexible repetition: if there are many valid responses to the same question make sure that your representation makes it possible to identify the equivalence and reward you with good grades by providing just one of the equivalent choices. For example, if you learn a language, it rarely make sense to learn all synonyms that meet a definition of a concept. It is more adequate to consider a single synonym as the sufficient answer (e.g. a mark made by ink spilt on sth = blot/blob/blotch)&lt;/item&gt;
      &lt;item&gt;more&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;18) Provide sources&lt;/p&gt;
    &lt;p&gt;Except for well-tested and proven knowledge (such as 2+2=4), it is highly recommended that you include sources from which you have gathered your knowledge. In real-life situation you will often be confronted with challenges to your knowledge. Sources can come to your rescue. You will also find that facts and figures differ depending on the source. You can really be surprised how frivolously reputable information agencies publish figures that are drastically different from other equally reputable sources. Without SuperMemo, those discrepancies are often difficult to notice: before you encounter the new fact, the old one is often long forgotten. With sources provided, you will be able to make more educated choices on which pieces of information are more reliable. Adding reliability labels may also be helpful (e.g. Watch out!, Other sources differ!, etc.). Sources should accompany your items but should not be part of the learned knowledge (unless it is critical for you to be able to recall the source whenever asked).&lt;/p&gt;
    &lt;p&gt;19) Provide date stamping&lt;/p&gt;
    &lt;p&gt;Knowledge can be relatively stable (basic math, anatomy, taxonomy, physical geography, etc.) and highly volatile (economic indicators, high-tech knowledge, personal statistics, etc.). It is important that you provide your items with time stamping or other tags indicating the degree of obsolescence. In case of statistical figures, you might stamp them with the year they have been collected. When learning software applications, it is enough you stamp the item with the software version. Once you have newer figures you can update your items. Unfortunately, in most cases you will have to re-memorize knowledge that became outdated. Date stamping is useful in editing and verifying your knowledge; however, you will rarely want to memorize stamping itself. If you would like to remember the changes of a given figure in time (e.g. GNP figures over a number of years), the date stamping becomes the learned knowledge itself.&lt;/p&gt;
    &lt;p&gt;20) Prioritize&lt;/p&gt;
    &lt;p&gt;You will always face far more knowledge that you will be able to master. That is why prioritizing is critical for building quality knowledge in the long-term. The way you prioritize will affect the way your knowledge slots in. This will also affect the speed of learning (e.g. see: learn basics first). There are many stages at which prioritizing will take place; only few are relevant to knowledge representation, but all are important:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prioritizing sources – there will always be a number of sources of your knowledge. If you are still at student years: these will most likely be books and notes pertaining to different subjects. Otherwise you will probably rely more on journals, Internet, TV, newspapers, encyclopedias, dictionaries, etc. It is always worth being aware what is the optimum proportion of time devoted to those varied sources. As you progress with learning, you will quickly develop a good sense of which learning slots bring better results and which might be extended at the cost of others&lt;/item&gt;
      &lt;item&gt;Extracting knowledge – unless you are about to pass an important exam, it nearly never makes sense to memorize whole books or whole articles. You will need to extract those parts that are most likely to impact the quality of your knowledge. You can do it by (1) marking paragraphs in a book or journal, (2) pasting relevant web pages to SuperMemo, (3) pasting relevant passages to SuperMemo, (4) typing facts and figures directly to SuperMemo notes, etc. You will need some experience before you can accurately measure how much knowledge you can indeed transfer to your brain and what degree of detail you can feasibly master. Your best way to prioritize the flow of knowledge into your memory is to use incremental reading tools&lt;/item&gt;
      &lt;item&gt;Transferring knowledge to SuperMemo – you may try to stick with the 20 rules of formulating knowledge at the moment of introducing your material to SuperMemo. However, you can also literally transfer your notes or import whole files and later use the mechanisms provided by SuperMemo to determine the order of processing the imported material. Probably the best criterion for choosing between formulating or just importing is the time needed for accurately formulating the item or items. If formulation requires more knowledge, more time, comparing with other sources, etc. you can just import. Otherwise, if you believe that formulating an accurate item is a matter of seconds, formulate it&lt;/item&gt;
      &lt;item&gt;Formulating items – make sure that explanatory or optional components of the answer are placed in the parentheses so that your attention is focused on the most important part of the item. The parts in the parentheses can be read after the repetition to strengthen the memory in its context&lt;/item&gt;
      &lt;item&gt;Using forgetting index – you can use the forgetting index to prioritize pending items. The sequence of repetitions will naturally be determined by SuperMemo; however, you can request higher retention level for items that are more important and lower retention level for items of lower priority&lt;/item&gt;
      &lt;item&gt;Learning – the process of prioritizing does not end with the onset of repetitions. Here are the tools you can use to continue setting your priorities while the learning process is under way:&lt;list rend="ul"&gt;&lt;item&gt;Remember (Ctrl+M) – re-memorize items of high priority that have changed or which are extremely important to your knowledge at a given moment. If you choose Ctrl+M you will be able to determine the next interval for the currently reviewed item (its repetition counter will be reset to zero). It is recommended that you always re-memorize items whose content has changed significantly&lt;/item&gt;&lt;item&gt;Reschedule (Ctrl+J) – manually schedule the date of the next repetition&lt;/item&gt;&lt;item&gt;Execute repetition (Ctrl+Shift+R) – manually execute a repetition even before the repetition’s due date (e.g. when reviewing particularly important material)&lt;/item&gt;&lt;item&gt;Forget (Ctrl+R)- remove the current item from the learning process and place it at the end of the pending queue&lt;/item&gt;&lt;item&gt;Dismiss (Ctrl+D) – ignore the current item in the learning process altogether&lt;/item&gt;&lt;item&gt;Delete (Ctrl+Shift+Del) – remove the current item from your collection&lt;/item&gt;&lt;item&gt;Change the forgetting index of memorized items or change the ordinal of pending items (Ctrl+Shift+P)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Summary&lt;/p&gt;
    &lt;p&gt;Here again are the twenty rules of formulating knowledge. You will notice that the first 16 rules revolve around making memories simple! Some of the rules strongly overlap. For example: do not learn if you do not understand is a form of applying the minimum information principle which again is a way of making things simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Do not learn if you do not understand&lt;/item&gt;
      &lt;item&gt;Learn before you memorize – build the picture of the whole before you dismember it into simple items in SuperMemo. If the whole shows holes, review it again!&lt;/item&gt;
      &lt;item&gt;Build upon the basics – never jump both feet into a complex manual because you may never see the end. Well remembered basics will help the remaining knowledge easily fit in&lt;/item&gt;
      &lt;item&gt;Stick to the minimum information principle – if you continue forgetting an item, try to make it as simple as possible. If it does not help, see the remaining rules (cloze deletion, graphics, mnemonic techniques, converting sets into enumerations, etc.)&lt;/item&gt;
      &lt;item&gt;Cloze deletion is easy and effective – completing a deleted word or phrase is not only an effective way of learning. Most of all, it greatly speeds up formulating knowledge and is highly recommended for beginners&lt;/item&gt;
      &lt;item&gt;Use imagery – a picture is worth a thousand words&lt;/item&gt;
      &lt;item&gt;Use mnemonic techniques – read about peg lists and mind maps. Study the books by Tony Buzan. Learn how to convert memories into funny pictures. You won’t have problems with phone numbers and complex figures&lt;/item&gt;
      &lt;item&gt;Graphic deletion is as good as cloze deletion – obstructing parts of a picture is great for learning anatomy, geography and more&lt;/item&gt;
      &lt;item&gt;Avoid sets – larger sets are virtually un-memorizable unless you convert them into enumerations!&lt;/item&gt;
      &lt;item&gt;Avoid enumerations – enumerations are also hard to remember but can be dealt with using cloze deletion&lt;/item&gt;
      &lt;item&gt;Combat interference – even the simplest items can be completely intractable if they are similar to other items. Use examples, context cues, vivid illustrations, refer to emotions, and to your personal life&lt;/item&gt;
      &lt;item&gt;Optimize wording – like you reduce mathematical equations, you can reduce complex sentences into smart, compact and enjoyable maxims&lt;/item&gt;
      &lt;item&gt;Refer to other memories – building memories on other memories generates a coherent and hermetic structure that forgetting is less likely to affect. Build upon the basics and use planned redundancy to fill in the gaps&lt;/item&gt;
      &lt;item&gt;Personalize and provide examples – personalization might be the most effective way of building upon other memories. Your personal life is a gold mine of facts and events to refer to. As long as you build a collection for yourself, use personalization richly to build upon well established memories&lt;/item&gt;
      &lt;item&gt;Rely on emotional states – emotions are related to memories. If you learn a fact in the sate of sadness, you are more likely to recall it if when you are sad. Some memories can induce emotions and help you employ this property of the brain in remembering&lt;/item&gt;
      &lt;item&gt;Context cues simplify wording – providing context is a way of simplifying memories, building upon earlier knowledge and avoiding interference&lt;/item&gt;
      &lt;item&gt;Redundancy does not contradict minimum information principle – some forms of redundancy are welcome. There is little harm in memorizing the same fact as viewed from different angles. Passive and active approach is particularly practicable in learning word-pairs. Memorizing derivation steps in problem solving is a way towards boosting your intellectual powers!&lt;/item&gt;
      &lt;item&gt;Provide sources – sources help you manage the learning process, updating your knowledge, judging its reliability, or importance&lt;/item&gt;
      &lt;item&gt;Provide date stamping – time stamping is useful for volatile knowledge that changes in time&lt;/item&gt;
      &lt;item&gt;Prioritize – effective learning is all about prioritizing. In incremental reading you can start from badly formulated knowledge and improve its shape as you proceed with learning (in proportion to the cost of inappropriate formulation). If need be, you can review pieces of knowledge again, split it into parts, reformulate, reprioritize, or delete. See also: Incremental reading, Devouring knowledge, Flow of knowledge, Using tasklists&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093022</guid></item><item><title>Cloudflare Radar: AI Insights</title><link>https://radar.cloudflare.com/ai-insights</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093090</guid></item><item><title>Ask HN: Who wants to be hired? (September 2025)</title><link>https://news.ycombinator.com/item?id=45093190</link><description>&lt;doc fingerprint="301182753412e0e7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Share your information if you are looking for work. Please use this format:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  Location:
  Remote:
  Willing to relocate:
  Technologies:
  Résumé/CV:
  Email:
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Please only post if you are personally looking for work. Agencies, recruiters, job boards, and so on, are off topic here.&lt;/p&gt;
      &lt;p&gt;Readers: please only email these addresses to discuss work opportunities.&lt;/p&gt;
      &lt;p&gt;There's a site for searching these posts at https://www.wantstobehired.com.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093190</guid></item><item><title>Ask HN: Who is hiring? (September 2025)</title><link>https://news.ycombinator.com/item?id=45093192</link><description>&lt;doc fingerprint="3651195a341ae364"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, https://amber-williams.github.io/hackernews-whos-hiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss these other fine threads:&lt;/p&gt;&lt;p&gt;Who wants to be hired? https://news.ycombinator.com/item?id=45093190&lt;/p&gt;&lt;p&gt;Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45093191&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093192</guid></item><item><title>Isolated(any)</title><link>https://nshipster.com/isolated-any/</link><description>&lt;doc fingerprint="3e129a13cc2d9639"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;@isolated(any)&lt;/head&gt;&lt;p&gt;Ahh, &lt;code&gt;@isolated(any)&lt;/code&gt;.
                It’s an attribute of contradictions.
                You might see it a lot, but it’s ok to ignore it.
                You don’t need to use it, but I think it should be used more.
                It must always take an argument, but that argument cannot vary.&lt;/p&gt;&lt;p&gt;Confusing? Definitely. But we’ll get to it all.&lt;/p&gt;&lt;p&gt;To understand why &lt;code&gt;@isolated(any)&lt;/code&gt; was introduced,
                we need to take a look at async functions.&lt;/p&gt;&lt;code&gt;let respond&lt;/code&gt;&lt;p&gt;This is about as simple a function type as we can get. But, things start to get a little more interesting when we look at how a function like this is used. A variable with this type must always be invoked with &lt;code&gt;await&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;await respond&lt;/code&gt;&lt;p&gt;This, of course, makes sense. All async functions must be called with &lt;code&gt;await&lt;/code&gt;.
                But! Consider this:&lt;/p&gt;&lt;code&gt;let send&lt;/code&gt;&lt;p&gt;The explicit types are there to help make what’s going on clear. We first define a synchronous function that must run on the &lt;code&gt;Main&lt;/code&gt;.
              And then we assign that to a plain old,
              non-&lt;code&gt;Main&lt;/code&gt; async function.
            We’ve changed so much that you might find it surprising this even compiles.&lt;/p&gt;&lt;p&gt;Remember what &lt;code&gt;await&lt;/code&gt; actually does. It allows the current task to suspend. That doesn’t just let the task wait for future work to complete. It also is an opportunity to change isolation. This makes async functions very flexible!&lt;/p&gt;&lt;p&gt;Just like a dispatcher doesn’t sit there doing nothing while waiting for the ambulance to arrive, a suspended task doesn’t block its thread. When the dispatcher puts you on hold to coordinate with the ambulance team, that’s the isolation switch - they’re transferring your request to a different department that specializes in that type of work.&lt;/p&gt;&lt;head rend="h2"&gt;But change to where, exactly?&lt;/head&gt;&lt;p&gt;Ok, so we know that async functions, because they must always be &lt;code&gt;await&lt;/code&gt;ed, gain a lot of flexibility. We are close, but have to go just a little further to find the motivation for this attribute.&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;&lt;p&gt;We now have a function that accepts other functions as arguments. It’s possible to pass in lots of different kinds of functions to &lt;code&gt;dispatch&lt;/code&gt;. They could be async functions themselves, or even be synchronous. And they can be isolated to any actor. All thanks to the power of &lt;code&gt;await&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Except there’s a little problem now. Have a look at &lt;code&gt;dispatch&lt;/code&gt; on its own:&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;&lt;p&gt;The type of &lt;code&gt;responder&lt;/code&gt; fully describes everything about this function,
        except for one thing.
        We have no way to know its isolation.
        That information is only available at callsites.
        The isolation is still present,
        so the right thing happens at runtime.
        It’s just not possible to inspect it statically or even programmatically.
        If you’ve encountered type erasure before,
        this should seem familiar.
        The flexibility of &lt;code&gt;async&lt;/code&gt; has come with a price -
        a loss of information.&lt;/p&gt;&lt;p&gt;This is where &lt;code&gt;@isolated(any)&lt;/code&gt; comes in.&lt;/p&gt;&lt;head rend="h2"&gt;&lt;code&gt;@isolated(any)&lt;/code&gt;&lt;/head&gt; Using &lt;p&gt;We can change the definition of &lt;code&gt;dispatch&lt;/code&gt; to fix this.&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;
    &lt;p&gt;When you apply &lt;code&gt;@isolated(any)&lt;/code&gt; to a function type, it does two things. Most importantly, it gives you access to a special &lt;code&gt;isolation&lt;/code&gt; property. You can use this property to inspect the isolation of the function. The isolation could be an actor. Or it could be non-isolated. This is expressible in the type system with &lt;code&gt;(any Actor)?&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Functions with properties felt really strange to me at first. But, after thinking for a minute, it became quite natural. Why not? It’s just a type like any other. In fact, we can simulate how this all works with another feature: &lt;code&gt;call&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;struct Isolated&lt;/code&gt;
&lt;p&gt;This analogy is certainly not perfect, but it’s close enough that it might help.&lt;/p&gt;&lt;p&gt;There is one other subtle change that &lt;code&gt;@isolated(any)&lt;/code&gt; makes to a function
  that you should be aware of.
  Its whole purpose is to capture the isolation of a function.
  Since that could be anything,
  callsites need an opportunity to switch.
  And that means an &lt;code&gt;@isolated(any)&lt;/code&gt; function must be called with an &lt;code&gt;await&lt;/code&gt; —
  even if it isn’t itself explicitly async.&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;
&lt;p&gt;This makes synchronous functions marked with &lt;code&gt;@isolated(any)&lt;/code&gt; a little strange.
  They still must be called with &lt;code&gt;await&lt;/code&gt;,
  yet they aren’t allowed to suspend internally?&lt;/p&gt;&lt;p&gt;As it turns out, there are some valid (if rare) situations where such an arrangement can make sense. But adding this kind of constraint to your API should at least merit some extra documentation.&lt;/p&gt;&lt;head rend="h2"&gt;How @isolated(any) Affects Callers&lt;/head&gt;&lt;p&gt;All of the task creation APIs — &lt;code&gt;Task&lt;/code&gt; initializers and &lt;code&gt;Task&lt;/code&gt; —
make use of &lt;code&gt;@isolated(any)&lt;/code&gt;.
These are used a lot
and are usually encountered very early on when learning about concurrency.
So, it’s completely natural to run into this attribute and think:&lt;/p&gt;&lt;p&gt;“Ugh another thing to understand!”&lt;/p&gt;&lt;p&gt;It’s reasonable because the components of a function type dictate how it can be used. They are all essential qualities for API consumers. They are the interface.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Parameters&lt;/item&gt;&lt;item&gt;Return value&lt;/item&gt;&lt;item&gt;Does it throw?&lt;/item&gt;&lt;item&gt;Is it async?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is not an exhaustive list, but what’s important is all of these are things callers must care about. Except for &lt;code&gt;@isolated(any)&lt;/code&gt;, which is the opposite.
  It doesn’t affect callers at all.&lt;/p&gt;&lt;p&gt;This, I think, is the root of a lot of confusion around &lt;code&gt;@isolated(any)&lt;/code&gt;.
  Unlike other qualities of a function,
  this attribute is used to capture information for the API producer.&lt;/p&gt;&lt;p&gt;I’m so close to saying “you can and should just ignore &lt;code&gt;@isolated(any)&lt;/code&gt;“.
  But I just cannot quite go that far,
  because there is one situation you should be aware of.&lt;/p&gt;&lt;head rend="h2"&gt;Scheduling&lt;/head&gt;&lt;p&gt;To help understand when you should be thinking about using &lt;code&gt;@isolated(any)&lt;/code&gt;,
  I’m going to quote
  the proposal:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;This allows the API to make more intelligent scheduling decisions about the function.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I’ve highlighted “intelligent scheduling”, because this is the key component of &lt;code&gt;@isolated(any)&lt;/code&gt;.
  The attribute gives you access to the isolation of a function argument.
  But what would you use that for?&lt;/p&gt;&lt;p&gt;Did you know that, before Swift 6.0, the ordering of the following code was undefined?&lt;/p&gt;&lt;code&gt;@Main&lt;/code&gt;
&lt;p&gt;Ordering turns out to be a very tricky topic when working with unstructured tasks. And while it will always require care, Swift 6.0 did improve the situation. We now have some stronger guarantees about scheduling work on the &lt;code&gt;Main&lt;/code&gt;,
and &lt;code&gt;@isolated(any)&lt;/code&gt; was needed to make that possible.&lt;/p&gt;&lt;p&gt;Take a look at this:&lt;/p&gt;&lt;code&gt;@Main&lt;/code&gt;
&lt;p&gt;These are three ways to achieve the same goal. But, there is a subtle difference in how the last form is scheduled. &lt;code&gt;Task&lt;/code&gt; takes an &lt;code&gt;@isolated(any)&lt;/code&gt; function
  so it can look at its isolation
  and synchronously submit it to an actor.
  This is how ordering can be preserved!
  But, it cannot do that in the last case.
  That closure passed into &lt;code&gt;Task&lt;/code&gt; isn’t actually itself &lt;code&gt;Main&lt;/code&gt; —
it has inherited nonisolated from the enclosing function.&lt;/p&gt;&lt;p&gt;I think it might help to translate this into GCD.&lt;/p&gt;&lt;code&gt;func dispatch&lt;/code&gt;
&lt;p&gt;Look really closely at that last one! What we are doing there is introducing a new async closure that then calls our &lt;code&gt;Main&lt;/code&gt; function.
There are two steps.
This doesn’t always matter,
but it certainly could.
And if you need to precisely schedule asynchronous work,
&lt;code&gt;@isolated(any)&lt;/code&gt; can help.&lt;/p&gt;&lt;head rend="h2"&gt;isolated(all)&lt;/head&gt;&lt;p&gt;All this talk about &lt;code&gt;@isolated(any)&lt;/code&gt; got me thinking…&lt;/p&gt;&lt;p&gt;It’s kinda strange that only some functions get to have this &lt;code&gt;isolation&lt;/code&gt; property.
  It would certainly feel more consistent to me if all functions had it.
  In fact, I think we can go further.
  I can imagine a future where an explicit &lt;code&gt;@isolated(any)&lt;/code&gt;
  isn’t even necessary for async functions.
  As far as I can tell, there is no downside.&lt;/p&gt;&lt;p&gt;And a little less syntactic noise would be nice. Perhaps one day!&lt;/p&gt;&lt;head rend="h2"&gt;isolated(some)&lt;/head&gt;&lt;p&gt;We do have to talk about that &lt;code&gt;any&lt;/code&gt;.
  It’s surprising that this attribute requires an argument,
  yet permits only one possible value.
  The reason here comes down to future considerations.&lt;/p&gt;&lt;p&gt;The concrete actor type that this &lt;code&gt;isolation&lt;/code&gt; property returns
  is always &lt;code&gt;(any Actor)?&lt;/code&gt;.
  This is the most generic type for isolation and matches the &lt;code&gt;#isolation&lt;/code&gt; macro.
  Today, there is no way to constrain a function to only specific actor types,
  such as &lt;code&gt;@isolated(My&lt;/code&gt;.
The &lt;code&gt;any&lt;/code&gt; keyword here was chosen to mirror how protocols handle this.
But accepting an argument leaves the door open
to more sophisticated features in the future.&lt;/p&gt;&lt;p&gt;And that really fits the spirit of &lt;code&gt;@isolated(any)&lt;/code&gt;.
  Doing a little work now in exchange for flexibility down the road.&lt;/p&gt;&lt;p&gt;Because you’ll see it in many foundational concurrency APIs, it’s very natural to feel like you must understand &lt;code&gt;@isolated(any)&lt;/code&gt;.
  I’m 100% behind technical curiosity!
  In this case, however, it is not required.
  For the most part, you can just ignore this attribute.
  You will rarely, if ever, need to use it yourself.&lt;/p&gt;&lt;p&gt;But if you ever find yourself capturing isolated functions and passing them along to other APIs that use &lt;code&gt;@isolated(any)&lt;/code&gt;,
  you should consider adopting it.
  It could prove useful.
  It’s even a source-compatible change
  to add or remove this attribute from an async function.&lt;/p&gt;&lt;p&gt;So there you have it.&lt;/p&gt;&lt;p&gt;As with many parts of the concurrency system, there’s a surprising depth to &lt;code&gt;@isolated(any)&lt;/code&gt;.
  Thankfully, from a practical perspective,
  we can enjoy the ordering guarantees of task creation
  that it enables without needing to master it.
  And one less thing on this journey is most welcome.&lt;/p&gt;&lt;p&gt;Isolated maybe, but never alone.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093590</guid></item><item><title>Search engine referral report for 2025 Q2</title><link>https://radar.cloudflare.com/reports/search-engine-market-share-2025-q2</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093693</guid></item><item><title>The time picker on the iPhone's alarm app isn't circular, it's just a long list</title><link>https://old.reddit.com/r/interestingasfuck/comments/1n5lztw/the_time_picker_on_the_iphones_alarm_app_isnt/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093765</guid></item><item><title>A Unique, High-Tech (Family) Computer</title><link>https://nicole.express/2025/a-computer-in-your-home.html</link><description>&lt;doc fingerprint="19154c3141e6a0a5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Unique, High-Tech (Family) Computer&lt;/head&gt;
    &lt;p&gt;Thereâs a concept that many people have tried, with varying effects: the âeducational computerâ, a device that a parent can buy for their children to learn the basics of the computer, which everyone will need to know in the future, and can also play games, so the children will actually want to use it. These have ranged from plasticky VTech toys with little more than an electronic organizer, to the Wonder Computer of the 1980âs, the Commodore VIC-20, which was a full computer. This is a prime market fit for an aging 8-bit platform, so of course, the Famicom has been wedged into it tooâ¦ but not by Nintendo.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unique, High-Tech, What more could you want?&lt;/head&gt;
    &lt;p&gt;This is it: a unique, high-tech computer. As we can see, itâs also advertising Contra on the box, along with â8 Bitâ games, so immediately, you know that this is a Famiclone, and itâs got a Famicom cartridge slot underneath the cartridge flap. Thereâs been more than a few of these out there; theyâre unique to me because they rarely show up in the United States (I bought this from Goodwill.com), but I would bet to many of the readers of this blog they wonât see this as unique at all.&lt;/p&gt;
    &lt;p&gt;Whatâs in the box?&lt;/p&gt;
    &lt;p&gt;In addition to the computer, you can see a whole selection of peripherals: two controllers, a mouse, a light-gun. And a power supply with a Europlug; further evidence that this is definitely not for the US market. Thankfully, itâs just 9V center-negative, so any plug you can use to power a Famicom should work here as well.&lt;/p&gt;
    &lt;p&gt;The sticker on the bottom of the system doesnât match the sticker on the front of the box, but it does give us a release year for this model of the product: 2003. By 2003, the Famicom hardware was definitely old hat; in fact, thatâs the same year Nintendo of Japan officially discontinued the system. You can definitely tell this sticker is trying to get you thinking this is relevant to the Windows XP world.&lt;/p&gt;
    &lt;p&gt;The sticker in the top left corner is long gone. Underneath is interesting, though; you can see three holes that look to the world like the Caps Lock, Num Lock, and Scroll Lock lights youâd see in the corner of a standard Windows keyboard of the era. Was this top case also used for standard keyboards? And if so, what did they do with the cartridge slot?&lt;/p&gt;
    &lt;p&gt;More evidence of plastics reuse is on the back, which shows a blanking plate covering nothing, and a speaker grille with no speaker behind it.&lt;/p&gt;
    &lt;p&gt;The actual ports you get are paltry; the common DB-9 ports you see for Famiclones, a power plug, and three RCA jacks. Think thatâs stereo audio? (Something we have discussed as a Famicom mod on this blog before) Look closer!&lt;/p&gt;
    &lt;p&gt;The white RCA port is actually the RF modulator! Audio is the red jack. Iâm guessing white, yellow, and red triplets of RCA ports were just extremely cheap at the time of this computerâs manufacture, so why not use them?&lt;/p&gt;
    &lt;p&gt;This is held together by screws, not plastic clips, which actually surprised me. But inside is just a standard keyboard membrane and a few small PCBs.&lt;/p&gt;
    &lt;p&gt;The keyboard mechanism is self-contained in the top plastic, and is actually a bit more elaborate than I expected; this is a âslider over membraneâ design, where pressing a key causes a tiny point-like piece of plastic to connect the membrane. It works fairly well; you could definitely learn to type on this. Assuming it didnât bind as much when it was new and clean, anyway.&lt;/p&gt;
    &lt;p&gt;Whereâs the Famiclone itself? Itâs just underneath the cartridge port, of course! And also of course, itâs an epoxy blob.&lt;/p&gt;
    &lt;p&gt;On the epoxy blob was a small piece of masking tape, which I removed for the earlier screenshot. I canât quite make it out as the ink has unfortunately bled a lot; the first letter seems to be a âVâ. A major series of Famiclone chips from V.R. Technology has serial numbers beginning with âVTâ, which could be related.&lt;/p&gt;
    &lt;p&gt;One thing about that controller. You might notice that on a real NES controller, the A button is on the outside edge, and the B button closer to the center. This is labeled in the opposite wayâ and this is how the buttons are arranged, too. Why did they swap the button positions? I donât know, perhaps they just donât like games being playable. The X and Y buttons are turbo buttons, as is commonly the case on four-button controllers being used for the Famicom.&lt;/p&gt;
    &lt;head rend="h2"&gt;Built-in hardware&lt;/head&gt;
    &lt;p&gt;This Famiclone has no built-in software or games. That seems to be pretty standard for models with cartridge slots; everything that makes this an educational computer is on the â48 in 1â cartridge. 48 is a much more achievable goal than many multicarts claim.&lt;/p&gt;
    &lt;p&gt;Whatâs inside?&lt;/p&gt;
    &lt;p&gt;An epoxy blob, of course, and 32kiB of SRAM. Itâs a shame this is an epoxy blob, because Iâm actually quite curious how that SRAM is wired. The NES memory map does not have room for 32kiB of cartridge PRG-ROM (usual amount of area mapped to the ROM) and 32kiB of cartridge RAM, so my assumption is that some sort of banking much be going on here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Turn it on&lt;/head&gt;
    &lt;p&gt;But letâs boot the damn thing up already! Worth noting that this is a PAL 50Hz console; that shouldâve been evident from the Europlug. I donât think anywhere uses the Europlug and 60Hz NTSC; though possibly parts of Latin America?&lt;/p&gt;
    &lt;p&gt;The UI is clearly inspired by Microsoft Windows, though not the Windows XP that the sticker on the console tries to hint at. Itâs actually pretty adorable, though having to move the cursor to the top corner is annoying. (Protip: use the page up and page down keys on the keyboard) The cursor can be moved with the mouse, or the controller. What is Super Hero?&lt;/p&gt;
    &lt;p&gt;Itâs a rhythm game of some sort. I canât recognize the track, and I donât know how to play the game either; it doesnât seem like controller inputs are what itâs looking for, or the arrow keys on the keyboard? So Iâll just let it be for now.&lt;/p&gt;
    &lt;p&gt;UPDATE: Thanks to The_Opponent for finding the track: Boys by smile.dk. Still not sure why itâs called âSuper Heroâ, though.&lt;/p&gt;
    &lt;p&gt;This actually has a lot of unique elements. For example, like any good version of Microsoft Windows, it has Solitaire.&lt;/p&gt;
    &lt;p&gt;And like any good multicart, it pads things out. Not only does it break up Duck Hunt (remember that gun in the package?) into multiple gamesâ¦&lt;/p&gt;
    &lt;p&gt;And yes, it is Nintendoâs Duck Hunt. What else did you expect?&lt;/p&gt;
    &lt;p&gt;The most extreme case is Konamiâs Track &amp;amp; Field. Itâs here, sure.&lt;/p&gt;
    &lt;p&gt;But itâs been broken up into so many individual options for individual events that an entire page of the menu is taken up by it.&lt;/p&gt;
    &lt;p&gt;Also, you know what Konami game is not present on this multicart? Contra. Which was advertised on the box.&lt;/p&gt;
    &lt;p&gt;There are some educational games. Not really worth noting too much; mostly focused on typing, though it can also sing âHappy Birthday to Youâ. Since the keyboard is pretty decent, thatâs probably actually the best usecase, but making games focused on typing is always a bit limiting. Hereâs a classic âpress the key listedâ game, with a âMy First Missile Commandâ theme.&lt;/p&gt;
    &lt;p&gt;But we were promised an Electronic Organ. So what does it have for a âMUSIC BOARDâ?&lt;/p&gt;
    &lt;p&gt;Thatâs right; itâs MUSIC BOARD, from Nintendo and Hudsonâs Family BASIC. Just separated into its own option on the menu, just like they did for Duck Hunt and Track and Field. Family BASIC MUSIC BOARD is fine, though I wouldnât call it an electronic organ. I feel robbed.&lt;/p&gt;
    &lt;p&gt;But if Family BASICâs MUSIC BOARD is hereâ¦&lt;/p&gt;
    &lt;p&gt;Then Family BASICâs GAME BASIC should be here too. And it is! Or at least, I assume this is Family BASIC. (V3, judging by the version number) 32kiB of RAM is much more than it usually has access to, but is likely the purpose of the extra RAM on the cartridge. Very nice.&lt;/p&gt;
    &lt;p&gt;Unfortunately, this has some severe downsides compared to the real Family BASIC, despite the extra RAM. The biggest being that there is no way to save your work between sessions; neither battery-backed RAM nor a way to interface with a cassette tape. This pretty much relegates G BASIC to a novelty, though it always was one anyways.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part of the FAMILY?&lt;/head&gt;
    &lt;p&gt;One thing I wondered here was, if it has Family BASIC on board, would the original one work?&lt;/p&gt;
    &lt;p&gt;Wellâ¦ unfortunately, Family BASIC has a very annoying UI where you have to talk to the computer using text. And so I learned that while the keyboard is compatible in the sense that pressing keys makes letters appear, the keyboard matrix has been remapped.&lt;/p&gt;
    &lt;p&gt;I didnât even make it to the actual BASIC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Computers for the whole family&lt;/head&gt;
    &lt;p&gt;As I noted, the educational computer market has a lot of entries. Many had features like printers; I wonder if that was what the blanking plate was for. This one is very bare-bones. But letâs face it; it was mostly a way for kids to get their parents to get something into the house which could play games like Super Mario Bros. 3, albiet at a PAL 50Hz slowdown.&lt;/p&gt;
    &lt;p&gt;Still, I think itâs a pretty cool bit of computing history, especially important outside the wealthier countries whose markets I usually look at. I hope you enjoyed!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093956</guid></item><item><title>Lessons from building an AI data analyst</title><link>https://www.pedronasc.com/articles/lessons-building-ai-data-analyst</link><description>&lt;doc fingerprint="b94bb29a65f9dac8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Lessons on building an AI data analyst&lt;/head&gt;
    &lt;p&gt;August 31, 2025 • 12 min read&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Text-to-SQL is not enough. Answering real user questions requires going the extra mile like multi-step plans, external tools (coding) and external context.&lt;/item&gt;
      &lt;item&gt;Context is the product. A semantic layer (we use Malloy ⎋) encodes business meaning and sharply reduces SQL complexity.&lt;/item&gt;
      &lt;item&gt;Use a multi-agent, research-oriented system. Break problems down using context / domain knowledge, retrieve precisely, write code, interact with the environment and learn from it.&lt;/item&gt;
      &lt;item&gt;Retrieval is a recommendation problem. Mix keyword, embeddings, and a fine-tuned reranker; optimise for precision, recall, and latency.&lt;/item&gt;
      &lt;item&gt;Benchmarks ≠ production. Users expect human-level answers, drill-downs, and defensible reasoning, not just pass@k.&lt;/item&gt;
      &lt;item&gt;Latency and quality are a tight bar. Route between fast and reasoning models; cache aggressively; keep contexts short. Continuous model evaluation is needed to avoid drifts as new models are launched.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The short story&lt;/head&gt;
    &lt;p&gt;I spent years on ML for Analytics and Knowledge Discovery at Google and Twitter. For the past 3 years I've been building an AI data analyst at Findly (findly.ai ⎋). We entered Y Combinator with a different idea, but quickly realised the real problem for most teams wasn't "lack of data" — it was data discovery and use.&lt;/p&gt;
    &lt;p&gt;We started the company as Conversion Pattern, tackling post-iOS 14 attribution and the privacy-driven collapse of cookie-based measurement. What we kept seeing: our customers already had most of the data they needed. They either didn't know it existed or couldn't stitch it together to answer business questions. The job wasn't to generate new data; it was to unlock the value of existing data.&lt;/p&gt;
    &lt;p&gt;We started with a toy problem — &lt;code&gt;text-to-SQL&lt;/code&gt; — and then let users pull us forward. The product evolved into a generative BI platform: it generates SQL, draws charts, writes Python for complex calculations, grounds itself in enterprise context, and pulls in external sources (web, PDFs) when the data story demands it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why &lt;code&gt;text-to-SQL&lt;/code&gt; isn't enough&lt;/head&gt;
    &lt;p&gt;Real questions rarely map to a single query:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Give me a study on the crude oil market."&lt;/item&gt;
      &lt;item&gt;"Create a trading strategy…."&lt;/item&gt;
      &lt;item&gt;"Compare these cohorts over the last four releases and explain the variance."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can sometimes force these into one monstrous SQL statement, but it's brittle and hard for current models. In practice, the system should run a multi-step workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;01Plan the analysis by breaking down the problem, defining the required tools/capabilities.&lt;/item&gt;
      &lt;item&gt;02Issue targeted SQL queries.&lt;/item&gt;
      &lt;item&gt;03Join/transform in Python (safer merges, custom calcs, charting).&lt;/item&gt;
      &lt;item&gt;04Validate assumptions with checks &amp;amp; sanity tests.&lt;/item&gt;
      &lt;item&gt;05Visualise and explain the result.&lt;/item&gt;
      &lt;item&gt;06Offer drill-downs and next questions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bottom line: Text-to-SQL is a capability. The product is end-to-end analysis that stands up to scrutiny.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context Engineering &amp;amp; Semantic Metadata&lt;/head&gt;
    &lt;p&gt;When building AI-powered data tools, context and metadata can mean the difference between the right and wrong answer. We invest heavily in a semantic layer for our data because it brings several critical benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Encodes business meaning: All the important context – dimensions, measures, relationships, and constraints – lives in a maintained semantic model instead of being buried inside prompts. Business logic (like how "revenue" is calculated or what qualifies as a "customer") is explicitly defined in one place and can be reused everywhere, rather than re-explained in every prompt. This also allows faster prototyping and testing.&lt;/item&gt;
      &lt;item&gt;Shrinks the search space: By providing structured context, our LLM-based planner avoids guesswork with ambiguous table or column names. The model knows exactly which fields are relevant and won't wander off into nonexistent or irrelevant data. This drastically improves the reliability of generated SQL, because the AI isn't brainstorming schema details – it's selecting from a known set.&lt;/item&gt;
      &lt;item&gt;Enables compile-time checks: Because the LLM works against a defined schema and semantic model, we can validate its output before execution. If it tries to use a field that doesn’t exist or apply a metric incorrectly, the semantic layer’s compiler catches it early. This leads to fewer silent failures and much more predictable behavior when the SQL or code runs, allowing us to self-correct intermediary steps along the process.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Our Choice: Malloy for Semantic Modeling&lt;/head&gt;
    &lt;p&gt;To implement this semantic layer, we chose Malloy ⎋, an open-source semantic modeling language. Malloy lets us model our data relationships as a graph of sources (tables) and joins, then define metrics (measures) and dimensions in that graph. We express complex queries at the semantic level, and Malloy's compiler translates them into optimized SQL with strong guarantees of correctness – essentially acting as our "knowledge graph plus compiler". In other words, Malloy serves as a single source of truth for business logic that ensures consistent, accurate SQL generation across the board.&lt;/p&gt;
    &lt;p&gt;Another advantage of Malloy is the ability to attach rich metadata and documentation directly to the model. We annotate each measure and dimension with human-readable descriptions and tags (units, currencies, etc.) right alongside its definition. For example, we might tag a metric with its unit and add a description:&lt;/p&gt;
    &lt;head rend="h3"&gt;Malloy Model&lt;/head&gt;
    &lt;code&gt;# "Total revenue from an order, in US Dollars"
# currency=US_DOLLARS
measure: total_revenue is sum(price * quantity)&lt;/code&gt;
    &lt;p&gt;In the snippet above, the &lt;code&gt;total_revenue&lt;/code&gt; measure is clearly defined as the sum of &lt;code&gt;price&lt;/code&gt; × &lt;code&gt;quantity&lt;/code&gt;, and it's annotated with a description as well as a currency tag. Malloy's flexible annotation system allows us to store arbitrary metadata like this (in this case, noting that &lt;code&gt;total_revenue&lt;/code&gt; is in USD). These descriptions and facts are not just for show – they are programmatically accessible. Our application can retrieve them and pass them into the LLM's context, so the model knows, for instance, that "&lt;code&gt;total_revenue&lt;/code&gt;" means "sum of &lt;code&gt;price&lt;/code&gt;×&lt;code&gt;quantity&lt;/code&gt; in USD" without having to infer it purely from the name. We can also have operations like casting or even case statements to create maps that are directly related to the business logic.&lt;/p&gt;
    &lt;head rend="h3"&gt;Integrating the Semantic Layer with LLMs (Functions &amp;amp; RAG)&lt;/head&gt;
    &lt;p&gt;How do we actually feed this context to the LLM? We employ a combination of retrieval-augmented generation (RAG) and the LLM's function-calling capabilities to integrate Malloy's semantic layer into our AI workflow. Instead of dumping the entire data schema into every prompt, we maintain a lightweight knowledge base of the semantic model. When a user asks a question, we first retrieve the relevant model fragments (e.g. the definitions of any measures or dimensions that the question mentions) and include only those in the prompt. This keeps prompts concise and focused. The LLM sees only the pertinent pieces of context, which dramatically narrows its search space to the correct solution.&lt;/p&gt;
    &lt;p&gt;Furthermore, we define a set of tools that the LLM can invoke as needed. Using &lt;code&gt;function-calling API&lt;/code&gt; the model can ask for more info or actions. For example, if it needs additional detail about a field, it can call something like &lt;code&gt;get_definition("trading_day_window")&lt;/code&gt; and our system will return the stored description/metadata for that term. Or the LLM might decide to call &lt;code&gt;run_query(model, params)&lt;/code&gt; to execute a Malloy-defined query plan and retrieve some data. This way, the LLM doesn't have to guess or hallucinate schema details – it can query the semantic layer directly for clarification. After gathering the needed context via these function calls, we can generate the final code (SQL or Python) with much higher confidence.&lt;/p&gt;
    &lt;p&gt;It's worth noting that this semantic context benefits Python code generation as much as SQL. Because our model includes things like unit conversions and custom calendar logic, the LLM can produce Python code that's aware of those definitions. For instance, if certain measures are tagged as currency in USD or a &lt;code&gt;trading_day&lt;/code&gt; dimension delineates business days vs. weekends, the assistant can incorporate that knowledge (maybe by calling a &lt;code&gt;convert_currency()&lt;/code&gt; helper or using a pre-defined &lt;code&gt;trading-days&lt;/code&gt; list) in the Python code it writes. By making the model more focused with a well-defined semantic layer, we get code that is not only plausible but also correct and aligned with our business rules.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example: Malloy Semantic Layer in Action&lt;/head&gt;
    &lt;p&gt;Let’s tie it all together with a concrete example using Malloy. Suppose we have an e-commerce dataset with orders and customers. We define a semantic model as follows:&lt;/p&gt;
    &lt;p&gt;Here's a more complete example of our Malloy semantic model:&lt;/p&gt;
    &lt;head rend="h3"&gt;Malloy Semantic Model&lt;/head&gt;
    &lt;code&gt;## Define the primary orders table with business logic
source: orders is db.table("orders") extend {
  primary_key: order_id
  
  # "Total revenue of an order (Price × Quantity, in USD)"
  # unit="USD"
  measure: total_revenue is sum(price * quantity)
  
  # "Order date (truncated to day)"
  dimension: order_date is cast(order_timestamp as date)
  
  # Join customers to enrich order context
  join_one: customers with customer_id
}

## Define the customers table with a business-friendly dimension
source: customers is db.table("customers") extend {
  primary_key: customer_id
  
  # "Geographic sales region of the customer (e.g. 'EMEA', 'NA')"
  dimension: region is region_name
}&lt;/code&gt;
    &lt;p&gt;Here we've explicitly modeled the relationships (linking orders to customers) and defined a metric &lt;code&gt;total_revenue&lt;/code&gt; with a clear meaning and unit. Now imagine a user asks: "What was our total revenue by region last quarter?" Our system will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;01Retrieve context&lt;p&gt;Recognize that the question involves the total_revenue measure and the region dimension. It pulls their definitions from the Malloy model (including the knowledge that total_revenue is price × quantity in USD, and that region comes from the customers table related to orders).&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;02Provide context to the LLM&lt;p&gt;Construct a prompt that includes the user's question along with the retrieved semantic definitions. This might look like a short snippet of Malloy model info or a brief text explanation for each relevant field, injected before asking the LLM to formulate an answer.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;03Generate code via function call&lt;p&gt;The LLM analyzes the question with the given context and decides on a plan. It might output a structured function call such as generate_sql(query_params…) rather than a raw answer. For example, it could produce a call like generate_sql(model="orders", measure="total_revenue", dimension="customers.region", filter="order_date in last_quarter"). This is the signal to take over and produce the actual query.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;04Compile and validate&lt;p&gt;Our backend function receives that structured request and uses Malloy to compile the corresponding query. Malloy knows about the orders→customers join and the definitions of each field, so it can generate the correct SQL. If the LLM's request referenced something incorrectly (say an undefined field), Malloy would throw an error here – catching the issue before execution.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;05Execute or return code&lt;p&gt;Once the query is successfully compiled, we execute it on the database. In our example, Malloy would produce a SQL that joins the orders and customers tables, filters to last quarter's dates, groups by region, and sums the total_revenue. The end result might be a neat table of regions with their respective revenue, or the SQL code for it – either way, it's guaranteed to be using the right tables, joins, and formulas.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: This is a very simple example, the schema in practice for enterprises is much more complex. It usually even requires multiple calls to the system in order to get disjoint tables etc.&lt;/p&gt;
    &lt;p&gt;The heavy lifting of "knowing the data" is handled by the semantic layer, and not left to the LLM. By making the business logic explicit and shareable, we ensure that both AI and humans are always speaking the same language – and that language is formally defined (in Malloy, in our case). The outcome is answers and code that are not just plausible, but correct, maintainable, and aligned with the business's reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Python code generation (and why it matters)&lt;/head&gt;
    &lt;p&gt;A lot of business analysis is post-SQL computation: statistical tests, time-series transforms, strategy backtests, data quality checks. We run these in a sandboxed Python environment with pre-installed libraries tuned to the customer's domain. Two big benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fewer tokens, more leverage. Libraries abstract tools and capabilities, allowing the model to just recall them instead of creating them from scratch.&lt;/item&gt;
      &lt;item&gt;Better generalisation. The model composes short, readable Python blocks instead of over‑fitting giant prompts. Pre-built, well-tested functions encode the general solution and its edge cases (missing timestamps, time zones, irregular sampling, NaNs, etc.). The model only has to compose these building blocks, so the resulting code is shorter, clearer, and behaves correctly across many datasets—i.e., it generalizes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A simple pattern that works well:&lt;/p&gt;
    &lt;p&gt;Store facts with reasoning traces and explanations. When generating code, retrieve the relevant traces and let the model adapt them.&lt;/p&gt;
    &lt;p&gt;Treat these snippets as natural‑language programs: general, succinct, and reusable. Reasoning models are good at recombining past programs (in the form of CoTs) — it’s how they’re trained. Storing traces in your business context “reminds” the model of the right approach and narrows the search space. AlphaEvolve ⎋ and Gemini 2.5 Pro Capable of Winning Gold at IMO 2025 ⎋ are good examples on how "hints" can significantly increase the results from the models.&lt;/p&gt;
    &lt;p&gt;Also: treat prompts and reasoning traces as company assets. Store them, version them, test them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-agent planning, memory, and grounding&lt;/head&gt;
    &lt;p&gt;Complex requests benefit from decomposition. Our architecture uses cooperating agents that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;01Plan the analysis&lt;p&gt;Decompose tasks, choose tools, define checks.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;02Retrieve precisely&lt;p&gt;See next section, iterating when gaps are detected.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;03Generate SQL/Python&lt;p&gt;Run it in sandboxes.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;04Validate&lt;p&gt;With unit checks / sanity tests.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;05Explain results&lt;p&gt;And propose next questions.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This reduces hallucinations and ambiguity, sharpens accountability by having more self-contained problems, and makes debugging possible. Memory (short- and long-term) keeps the system grounded in prior decisions and user preferences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Retrieval systems: treat RAG like recommendations&lt;/head&gt;
    &lt;p&gt;LLM speed suffers as context grows (transformer attention is ~quadratic in input length), so good retrieval is non-negotiable. The shorter and well curated the data you put in the LLM is, the better and faster the results will be. Think of it as a recommendation pipeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Candidate generation: keyword search for internal acronyms and exact terms; embeddings for semantic matches. People should look more at their "&lt;code&gt;RAG&lt;/code&gt;" systems as a full recommendation system. You can always improve the latency, precision and recall of the system.&lt;/item&gt;
      &lt;item&gt;Reranking: a fine-tuned instruction-following reranker optimises for the current question style. (Off-the-shelf rerankers underperform without this.) Fine-tuning the reranker is important, otherwise it won't perform as well as needed. You can see a lot of companies have been releasing better instruction following reranker models — this is quite important as the LLMs will be doing the queries.&lt;/item&gt;
      &lt;item&gt;Multi-stage ranking: keep the early stages cheap; spend budget late where it matters. Aim for both precision (fewer irrelevant docs) and recall (don't miss the key one). This helps to keep latency and cost in check.&lt;/item&gt;
      &lt;item&gt;Query rewriting: LLMs can write long, precise queries — use that to drive search, not just retrieval.&lt;/item&gt;
      &lt;item&gt;Chunking and keys: design retrieval keys to match how analysts think (e.g., metric → dimension → time), not how files are stored.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Current search and recommendation systems are heavily optimized for humans: LLMs search is different from human search. LLMs are able to write complex, precise and verbose queries. This information needs to be used to make the search systems as precise as possible as part of a multi-agent framework. This also helps to reduce the information needed to be passed as context to the LLM.&lt;/p&gt;
    &lt;p&gt;Note: the specific &lt;code&gt;top@k&lt;/code&gt; thresholds and models used in an AI data analytics system really depends on the specific tasks being solved. Some problems require bigger models specially at the later stages, other ones not that much. Some of them might even require LLMs using a &lt;code&gt;map-reduce&lt;/code&gt; / &lt;code&gt;divide-and-conquer&lt;/code&gt; approach to get the right accuracy.&lt;/p&gt;
    &lt;p&gt;Some of the companies that provide good reranking models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Open source: &lt;code&gt;Qwen3&lt;/code&gt;reranker - https://qwenlm.github.io/blog/qwen3-embedding/&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Voyage AI&lt;/code&gt;- https://blog.voyageai.com/2025/08/11/rerank-2-5/&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Contextual AI&lt;/code&gt;- https://contextual.ai/blog/introducing-instruction-following-reranker/&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Cohere&lt;/code&gt;- https://cohere.com/blog/rerank-3pt5&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The latest &lt;code&gt;Voyage&lt;/code&gt;, &lt;code&gt;Contextual&lt;/code&gt; and &lt;code&gt;Qwen3&lt;/code&gt; models all emphasize the instruction following capabilities, showing the need for it on multi-agent systems.&lt;/p&gt;
    &lt;p&gt;The picture below shows the improvements given by the instructions following models from &lt;code&gt;Voyage AI&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I am excited about the usage of specialized hardware ⎋ and diffusion LLM language models (e.g., the ones by Inception Labs ⎋) for rerankers. While most people focus a lot on the top-tier LLMs, having strong and extremely low-latency rerankers might be a much better performance improvement for AI systems than the top model itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Different LLM choices&lt;/head&gt;
    &lt;p&gt;Reasoning-style models are already excellent for &lt;code&gt;text-to-SQL&lt;/code&gt;. They handle ambiguous or very hard questions well, and outright hallucinations are now uncommon. The trade-off is latency (and often cost), so you can't run them end-to-end across every step of a real-time pipeline.&lt;/p&gt;
    &lt;p&gt;Key takeaways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hallucinations aren't the main risk anymore. Modern reasoning models rarely fabricate facts outright.&lt;/item&gt;
      &lt;item&gt;Context is the real failure mode. Missing schema details, vague user intent, or unclear join paths lead to wrong queries.&lt;/item&gt;
      &lt;item&gt;Context engineering matters most. Invest in precise retrieval, schema selection, examples/constraints, and clear problem framing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recommendation. If you're building an AI data-analyst workflow, use top-tier reasoning models for the SQL generation + schema reasoning step (e.g., &lt;code&gt;Gemini 2.5 Pro&lt;/code&gt;, &lt;code&gt;o4-mini&lt;/code&gt;, &lt;code&gt;Claude 4 Sonnet&lt;/code&gt;). &lt;code&gt;O3&lt;/code&gt; and &lt;code&gt;Claude 4 Opus&lt;/code&gt; are extremely strong, but their latency and cost typically make them impractical for interactive production use.&lt;/p&gt;
    &lt;p&gt;A practical pattern is a hybrid setup: route easy or routine requests to a faster model, and automatically escalate the hard/ambiguous ones to a reasoning model. This preserves quality where it matters without blowing up response times.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common failure modes (and fixes)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ambiguous tables/joins — push grain/joins into the semantic layer; add compile-time checks.&lt;/item&gt;
      &lt;item&gt;Over-long contexts — narrow retrieval keys; teach query rewriting; cache partial results.&lt;/item&gt;
      &lt;item&gt;Quiet wrong answers — add validators and reconciliation tests; require citations.&lt;/item&gt;
      &lt;item&gt;Latency spikes — stage reranking; cap tokens; route early to fast paths.&lt;/item&gt;
      &lt;item&gt;Brittle prompts — store &amp;amp; version traces; test against real user questions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What is next&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adaptive models that switch between fast and reasoning modes and know how much to think. This will lead to faster models on par with human expectations on how long a task should take given the difficulty.&lt;/item&gt;
      &lt;item&gt;More agentic systems that explore alternative plans, fill knowledge gaps, and critique their own outputs.&lt;/item&gt;
      &lt;item&gt;Automated knowledge extraction that continuously harvests and organises metadata and business logic. With curated knowledge, today's multi-agent systems can already tackle surprisingly complex tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the next posts I will dig more on the specifics about semantic layer choices, what is missing to make enterprise program synthesis better, etc.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45094256</guid></item><item><title>93% of GPT-4 performance at 1/4 cost: LLM routing with weak bandit feedback</title><link>https://arxiv.org/abs/2508.21141</link><description>&lt;doc fingerprint="cc18340bf0f78482"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 28 Aug 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Adaptive LLM Routing under Budget Constraints&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45094421</guid></item><item><title>Optery (YC W22) Is Hiring in Engineering, Legal, Sales, Marketing (U.S., Latam)</title><link>https://www.optery.com/careers/</link><description>&lt;doc fingerprint="738af7694497baac"&gt;
  &lt;main&gt;
    &lt;p&gt;Skip to content Use promo code: Xi8TJRBw at checkout for 20% Off 🎉 with Optery’s Labor Day Sale! 🎇 Getting Started Personal Business Pricing Personal Business Sites We Cover Family Business Optery for Business Partnership Program API for Partners Book Optery for Business Demo Client Stories Business Use Cases PII Removal for Execs is Not Enough Guide to Enterprise Data Removal Services Resources Help Desk Blog Data Broker Directory For High-Risk Communities About Us Opt Out Guides Product Updates Customer Reviews Optery in the Press Search Toggle search Sign In Sign Up Free Getting Started Personal Business Pricing Personal Business Sites We Cover Family Business Optery for Business Partnership Program API for Partners Book Optery for Business Demo Client Stories Business Use Cases PII Removal for Execs is Not Enough Guide to Enterprise Data Removal Services Resources Help Desk Blog Data Broker Directory For High-Risk Communities About Us Opt Out Guides Product Updates Customer Reviews Optery in the Press Careers Ready to safeguard your personal data? Join the movement of people strengthening their privacy Sign Up Free&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45094471</guid></item></channel></rss>