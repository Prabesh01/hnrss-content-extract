<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 22 Dec 2025 23:10:40 +0000</lastBuildDate><item><title>Claude Code gets native LSP support</title><link>https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355165</guid><pubDate>Mon, 22 Dec 2025 15:59:01 +0000</pubDate></item><item><title>Flock Exposed Its AI-Powered Cameras to the Internet. We Tracked Ourselves</title><link>https://www.404media.co/flock-exposed-its-ai-powered-cameras-to-the-internet-we-tracked-ourselves/</link><description>&lt;doc fingerprint="2643fb572e61a1da"&gt;
  &lt;main&gt;
    &lt;p&gt;I am standing on the corner of Harris Road and Young Street outside of the Crossroads Business Park in Bakersfield, California, looking up at a Flock surveillance camera bolted high above a traffic signal. On my phone, I am watching myself in real time as the camera records and livestreams me—without any password or login—to the open internet. I wander into the intersection, stare at the camera and wave. On the livestream, I can see myself clearly. Hundreds of miles away, my colleagues are remotely watching me too through the exposed feed.&lt;/p&gt;
    &lt;p&gt;Flock left livestreams and administrator control panels for at least 60 of its AI-enabled Condor cameras around the country exposed to the open internet, where anyone could watch them, download 30 days worth of video archive, and change settings, see log files, and run diagnostics.&lt;/p&gt;
    &lt;p&gt;Unlike many of Flock’s cameras, which are designed to capture license plates as people drive by, Flock’s Condor cameras are pan-tilt-zoom (PTZ) cameras designed to record and track people, not vehicles. Condor cameras can be set to automatically zoom in on people’s faces as they walk through a parking lot, down a public street, or play on a playground, or they can be controlled manually, according to marketing material on Flock’s website. We watched Condor cameras zoom in on a woman walking her dog on a bike path in suburban Atlanta; a camera followed a man walking through a Macy’s parking lot in Bakersfield; surveil children swinging on a swingset at a playground; and film high-res video of people sitting at a stoplight in traffic. In one case, we were able to watch a man rollerblade down Brookhaven, Georgia’s Peachtree Creek Greenway bike path. The Flock camera zoomed in on him and tracked him as he rolled past. Minutes later, he showed up on another exposed camera livestream further down the bike path. The camera’s resolution was good enough that we were able to see that, when he stopped beneath one of the cameras, he was watching rollerblading videos on his phone.&lt;/p&gt;
    &lt;p&gt;The exposure was initially discovered by YouTuber and technologist Benn Jordan and was shared with security researcher Jon “GainSec” Gaines, who recently found numerous vulnerabilities in several other models of Flock’s automated license plate reader (ALPR) cameras. They shared the details of what they found with me, and I verified many of the details seen in the exposed portals by driving to Bakersfield to walk in front of two cameras there while I watched myself on the livestream. I also pulled Flock’s contracts with cities for Condor cameras, pulled details from company presentations about the technology, and geolocated a handful of the cameras to cities and towns across the United States. Jordan also filmed himself in front of several of the cameras on the Peachtree Creek Greenway bike path. Jordan said he and Gaines discovered many of the exposed cameras with Shodan, an internet of things search engine that researchers regularly use to identify improperly secured devices.&lt;/p&gt;
    &lt;p&gt;After finding links to the feed, “immediately, we were just without any username, without any password, we were just seeing everything from playgrounds to parking lots with people, Christmas shopping and unloading their stuff into cars,” Jordan told me in an interview. “I think it was like the first time that I actually got like immediately scared … I think the one that affected me most was as playground. You could see unattended kids, and that’s something I want people to know about so they can understand how dangerous this is.” In a YouTube video about his research, Jordan said he was able to use footage pulled from the exposed feed to identify specific people using open source investigation tools in order to show how trivially an exposure like this could be abused.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355548</guid><pubDate>Mon, 22 Dec 2025 16:31:40 +0000</pubDate></item><item><title>Jimmy Lai Is a Martyr for Freedom</title><link>https://reason.com/2025/12/19/jimmy-lai-is-a-martyr-for-freedom/</link><description>&lt;doc fingerprint="7e22995531a79948"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Jimmy Lai Is a Martyr for Freedom&lt;/head&gt;
    &lt;head rend="h2"&gt;The self-made tycoon was convicted this week of violating Hong Kong's "national security" law. But he could have escaped it.&lt;/head&gt;
    &lt;p&gt;There are many interesting tidbits about the life of the political prisoner Jimmy Lai. He hid in the bottom of a fishing boat to escape mainland Communist China for Hong Kong at the ripe age of 12. He built a garment empire after spending his adolescence working, and sleeping, in garment factories. Without media experience, he started several successful news ventures—most notably the plucky and irreverent Apple Daily—which forcefully advocated for democracy and free speech. And he may be sentenced to die in prison in connection with his efforts promoting liberty in China.&lt;/p&gt;
    &lt;p&gt;But the most interesting fact, by far, is that Lai is a citizen of the United Kingdom (U.K.).&lt;/p&gt;
    &lt;p&gt;The dissident was convicted in Hong Kong earlier this week of two counts of conspiring to collude with foreign forces and one count of publishing seditious material—charges stemming from his crusade against illiberalism, a fight he has been waging for decades. Lai finding himself in trouble was not a surprise. That's especially true amid the backdrop of Hong Kong's "national security" law, which sought to cripple dissent, that took effect in 2020. He was arrested in August of that year and released on bail; authorities revoked it four months later. Lai has been in custody since.&lt;/p&gt;
    &lt;p&gt;That he would probably end up in prison, however, was never really in doubt. Which brings me back to his U.K. citizenship.&lt;/p&gt;
    &lt;p&gt;Lai did not have to stay in Hong Kong as the walls closed in on him. The self-made business tycoon—once a billionaire before the government froze his assets—could have fled to a residence abroad. His friend Mark Clifford, formerly the editor in chief of the South China Morning Post, told me in an interview earlier this year that many people in Lai's circle urged him to do just that.&lt;/p&gt;
    &lt;p&gt;He declined. "Everything I have was given to me by Hong Kong. I won't be leaving," Lai told Radio Free Asia in June 2020. "I'm going to stay here and fight to the bitter end."&lt;/p&gt;
    &lt;p&gt;Lawmakers would go on to formally approve the national security law, essentially a foregone conclusion, about three weeks later. The legislation broadly criminalized political dissent and hamstrung the civil liberties that once distinguished Hong Kong from mainland China. A defense of those freedoms—which were already under increasing attack—had come to define Lai's legacy. Lai not only unapologetically advanced democracy and free expression in the region, but he also met with then–Vice President Mike Pence and Secretary of State Mike Pompeo; at trial, Lai testified that he had asked them to voice their support for Hong Kong. He knew the law was coming, and he knew what it meant for him.&lt;/p&gt;
    &lt;p&gt;But some things, he decided, are more important than personal freedom. In this case, the absence of it was more important—in part to show the world what happens when an authoritarian government severely curtails basic liberties.&lt;/p&gt;
    &lt;p&gt;In some sense, there was no better person than Lai to send this message. His Cinderella story is impossible to divorce from Hong Kong itself. There, he was able to find refuge from the Chinese Communist Party, which had imprisoned his mother, deemed a "class enemy," in a labor camp. But he was also able to make something from nothing: from living in factories, while rats scampered across his body, to running them. His story came full circle. It demands people ask: Do you prefer Hong Kong's past? Or its future?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355888</guid><pubDate>Mon, 22 Dec 2025 16:56:35 +0000</pubDate></item><item><title>Uplane (YC F25) Is Hiring Founding Engineers (Full-Stack and AI)</title><link>https://www.useparallel.com/uplane1/careers</link><description>&lt;doc fingerprint="d37fffed7efd5e8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355932</guid><pubDate>Mon, 22 Dec 2025 17:00:34 +0000</pubDate></item><item><title>NIST was 5 μs off UTC after last week's power cut</title><link>https://www.jeffgeerling.com/blog/2025/nist-was-5-μs-utc-after-last-weeks-power-cut</link><description>&lt;doc fingerprint="b5889d520676c0f9"&gt;
  &lt;main&gt;
    &lt;p&gt;If you were 5 microseconds late today, blame it on NIST.&lt;/p&gt;
    &lt;p&gt;Their facility in Boulder Colorado just had its power cut for multiple days. After a backup generator failed, their main ensemble clock lost track of UTC, or Universal Time Coordinated.&lt;/p&gt;
    &lt;p&gt;But even if you used the NTP timing servers they run, they were never off by more than 5 microseconds.&lt;/p&gt;
    &lt;p&gt;5 μs might seem insignificant. But it is significant for scientists and universities who rely on NIST's more specialized timing signals.&lt;/p&gt;
    &lt;p&gt;But no, you don't need to panic. And yes, they have it under control now.&lt;/p&gt;
    &lt;p&gt;But I thought I'd go over what happened, what it means, and what we can learn from NIST's near-outage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;This blog post is a lightly-edited transcript of my most recent YouTube video:&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened&lt;/head&gt;
    &lt;p&gt;The NIST campus, which distributes Internet time on six of the most popular NTP servers, lost power last Wednesday.&lt;/p&gt;
    &lt;p&gt;The power company was forced to cut power because of wind gusts over 100 mph (160 km/h). Power lines were coming down and they didn't want to risk starting a wildfire.&lt;/p&gt;
    &lt;p&gt;The whole campus was locked down for safety, so nobody could enter or exit.&lt;/p&gt;
    &lt;p&gt;They have backup generators. And those were working... but apparently one of the generators failed after a couple days. Specifically, the generator that powered the main ensemble clock that's used by the NTP servers.&lt;/p&gt;
    &lt;p&gt;Things were dicey last Friday, and they couldn't get any more staff in to fix it.&lt;/p&gt;
    &lt;p&gt;It got to the point Jeff Sherman, the Group Leader for NIST's Time Realization and Distribution Group, considered shutting down the backup generator that powered the time servers. That would've prevented them from sending out inaccurate time, which would be worse than no time at all for a lot of applications.&lt;/p&gt;
    &lt;p&gt;NTP's designed so you have multiple servers you look at, and if one fails, it won't cause you to lose time.&lt;/p&gt;
    &lt;p&gt;And luckily for NIST, they have another building in their Boulder campus with more clocks, and that building could transfer time back to the one that had the power failure, if they needed to.&lt;/p&gt;
    &lt;p&gt;But yesterday Jeff posted another update: power was restored, and apparently there were still some staff on-site who saved the clocks.&lt;/p&gt;
    &lt;p&gt;They were able to re-route emergency power after the main backup generator went down.&lt;/p&gt;
    &lt;p&gt;Battery backups, I'm assuming some big UPSes, were able to bridge the gap, until they got the backup backup power going.&lt;/p&gt;
    &lt;p&gt;When all was said and done, their monitoring showed deviation from UTC was less than 5 μs.&lt;/p&gt;
    &lt;p&gt;Seeing all that, Jeff and the team at NIST decided to keep their time servers online.&lt;/p&gt;
    &lt;p&gt;But why would they do that, if they were off? Well, time scales are important here. If you're on a Mac like I am, go in the Terminal and run &lt;code&gt;sntp time-a-b.nist.gov&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This or a command like &lt;code&gt;ntpdate&lt;/code&gt; on Linux gives back an error bound, that shows latency between your computer and the NTP time servers.&lt;/p&gt;
    &lt;code&gt;$ sntp time-a-b.nist.gov
+0.005771 +/- 0.035081 time-a-b.nist.gov 132.163.96.1
&lt;/code&gt;
    &lt;p&gt;In my case, it's showing 0.035 seconds. That's 35 milliseconds, or 35 thousand microseconds. 5 microseconds isn't even a blip there.&lt;/p&gt;
    &lt;p&gt;So instead of taking down the servers, which could cause more problems, NIST kept them online.&lt;/p&gt;
    &lt;p&gt;But Jeff said NIST's time is usually about 5,000x more accurate. And if you're one of the universities or aerospace companies that relies on NIST for timing, a 5 μs difference probably does matter.&lt;/p&gt;
    &lt;p&gt;So they'll be working with those groups directly. But for most people, they'll never even notice.&lt;/p&gt;
    &lt;p&gt;Jeff finished off the email mentioning the US GPS system failed over successfully to the WWV-Ft. Collins campus. So again, for almost everyone, there was zero issue, and the redundancy designed into the system worked like it's supposed to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Time is fragile&lt;/head&gt;
    &lt;p&gt;I was following this closely over the weekend. I have two Raspberry Pi GPS clocks in the studio. One runs my main Stratum 0 NTP server, and the other one I have running as a backup for testing. (Yes I know I should have 4+ going for good quorum.)&lt;/p&gt;
    &lt;p&gt;They both run off my outdoor GPS antenna, which is distributed in my rack room and in my studio for time research.&lt;/p&gt;
    &lt;p&gt;Like my studio, most places that need precise time rely on GPS. And that could be a problem!&lt;/p&gt;
    &lt;p&gt;I'm glad redundancies kept GPS from drifting—I don't know what would happen if GPS time goes away, but it wouldn't be good! But the main takeaway I think is this: timing infrastructure is fragile.&lt;/p&gt;
    &lt;p&gt;CISA identified a lot of risk in the US's over-dependence on GPS.&lt;/p&gt;
    &lt;p&gt;Because of that, the US announced it's trying to find good alternatives for PNT (Position, Navigation, and Timing) earlier this year.&lt;/p&gt;
    &lt;p&gt;I was actually at a meeting at the NAB where Jeff Sherman, the scientist who wrote the two NIST updates, was talking about BPS. The Broadcast Positioning System would give us redundancy even if GPS was down.&lt;/p&gt;
    &lt;p&gt;But even with multiple time sources, some places need more. I have two Rubidium atomic clocks in my studio, including the one inside a fancy GPS Disciplined Oscillator (GPSDO). That's good for holdover. Even if someone were jamming my signal, or my GPS antenna broke, I could keep my time accurate to nanoseconds for a while, and milliseconds for months. That'd be good enough for me.&lt;/p&gt;
    &lt;p&gt;(If I'm being truthful, it's actually overkill, but I'm in the time-nut rabbit hole now—if you know, you know.)&lt;/p&gt;
    &lt;p&gt;But some places do need nanoseconds, for science experiments, RF, media, or finance. And they might run their own even more precise clocks. But they still trace things back to NIST, at least most do here in the US.&lt;/p&gt;
    &lt;p&gt;So when NIST's disaster response is tested, everyone's watching.&lt;/p&gt;
    &lt;p&gt;Last week, when we were microseconds from disaster, the team at NIST fixed it so almost nobody noticed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355949</guid><pubDate>Mon, 22 Dec 2025 17:01:28 +0000</pubDate></item><item><title>Henge Finder</title><link>https://hengefinder.rcdis.co/#learn</link><description>&lt;doc fingerprint="aa0ce6e5647b06c2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Henge Finder&lt;/head&gt;
    &lt;head style="cursor: pointer; color: #5E4955; font-weight: 600; margin-bottom: 10px; padding: 15px; border-radius: 8px; background: #f8f9fa; border: 1px solid #e1e5e9; transition: background-color 0.2s;"&gt;What is a henge?&lt;/head&gt;
    &lt;p&gt;A 'henge' is when the sun sets perfectly in line with your street, creating a dramatic view — like Manhattanhenge in New York.&lt;/p&gt;
    &lt;p&gt;You can find out more information by clicking the "How Do Henges Work?" tab.&lt;/p&gt;
    &lt;p&gt;Use this tool to find when the next henge will happen for your street.&lt;/p&gt;
    &lt;head style="cursor: pointer; color: #5E4955; font-weight: 600; margin-bottom: 10px; padding: 15px; border-radius: 8px; background: #f8f9fa; border: 1px solid #e1e5e9; transition: background-color 0.2s;"&gt;What makes a good street for a henge?&lt;/head&gt;
    &lt;p&gt;(Tips for finding the best henge view)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pick a long, straight road with a clear view of the horizon.&lt;lb/&gt;Henges work best when you can actually see the sun touch the horizon. Curvy streets won't align well, and wider streets tend to give better views.&lt;/item&gt;
      &lt;item&gt;Aim for a mostly east–west street.&lt;lb/&gt;The sun won't set along a north–south road. It doesn't have to be perfect — the sunset shifts a little each day.&lt;/item&gt;
      &lt;item&gt;Avoid entering an intersection address if possible.&lt;lb/&gt;It's not necessary for this to work, but it will be faster.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example Streets:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;251 W 42nd St, New York, NY, by Times Square&lt;/item&gt;
      &lt;item&gt;601-615 E 76th St, Chicago, IL&lt;/item&gt;
      &lt;item&gt;Haarlemmerweg 109-C, 1051 KV Amsterdam, Netherlands, along the canal by the Wester Park&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;See How the Sun Moves Down The Street&lt;/head&gt;
    &lt;p&gt;The Sun doesn’t set in the same place every day. Its position along the horizon shifts as the seasons change.&lt;/p&gt;
    &lt;p&gt;Most evenings, that angle doesn’t match the direction of a street. But on just a few days each year (if the street is angled correctly), the Sun’s path lines up perfectly—creating a “henge.”&lt;/p&gt;
    &lt;p&gt;Compare below to see the sun's path on a henge and non-henge date:&lt;/p&gt;
    &lt;head rend="h3"&gt;Non-Henge Date&lt;/head&gt;
    &lt;head rend="h3"&gt;Henge Date&lt;/head&gt;
    &lt;head rend="h2"&gt;Earth's Orbital Motion and Axial Tilt&lt;/head&gt;
    &lt;p&gt;Earth doesn’t spin upright. It’s tilted on an axis of ~23.5˚. That tilt is what makes the seasons, and it’s also what makes henges possible.&lt;/p&gt;
    &lt;p&gt;As the tilted Earth circles the Sun, the angle of the sunset shifts across the horizon. A street keeps the same direction year-round, but the Sun only lines up on specific dates. Here, a road running along the equator lines up with the Sun at the equinox.&lt;/p&gt;
    &lt;p&gt;Move the slider to see how the Earth's tilt changes relative to the Sun throughout the year:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46356320</guid><pubDate>Mon, 22 Dec 2025 17:32:18 +0000</pubDate></item><item><title>GLM-4.7: Advancing the Coding Capability</title><link>https://z.ai/blog/glm-4.7</link><description>&lt;doc fingerprint="674d8ad96d9bceda"&gt;
  &lt;main&gt;
    &lt;p&gt;GLM-4.7, your new coding partner, is coming with the following features:&lt;/p&gt;
    &lt;p&gt;You can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.&lt;/p&gt;
    &lt;p&gt;Benchmark Performance. More detailed comparisons of GLM-4.7 with other models GPT-5, GPT-5.1-High, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2, Kimi K2 Thinking, on 17 benchmarks (including 8 reasoning, 5 coding, and 3 agents benchmarks) can be seen in the below table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Benchmark&lt;/cell&gt;
        &lt;cell role="head"&gt;GLM-4.7&lt;/cell&gt;
        &lt;cell role="head"&gt;GLM-4.6&lt;/cell&gt;
        &lt;cell role="head"&gt;Kimi K2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek-V3.2&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini 3.0 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Claude Sonnet 4.5&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5 High&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 High&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMLU-Pro&lt;/cell&gt;
        &lt;cell&gt;84.3&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;90.1&lt;/cell&gt;
        &lt;cell&gt;88.2&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;87.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;GPQA-Diamond&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;82.4&lt;/cell&gt;
        &lt;cell&gt;91.9&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;88.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;HLE&lt;/cell&gt;
        &lt;cell&gt;24.8&lt;/cell&gt;
        &lt;cell&gt;17.2&lt;/cell&gt;
        &lt;cell&gt;23.9&lt;/cell&gt;
        &lt;cell&gt;25.1&lt;/cell&gt;
        &lt;cell&gt;37.5&lt;/cell&gt;
        &lt;cell&gt;13.7&lt;/cell&gt;
        &lt;cell&gt;26.3&lt;/cell&gt;
        &lt;cell&gt;25.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;HLE (w/ Tools)&lt;/cell&gt;
        &lt;cell&gt;42.8&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;44.9&lt;/cell&gt;
        &lt;cell&gt;40.8&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;32.0&lt;/cell&gt;
        &lt;cell&gt;35.2&lt;/cell&gt;
        &lt;cell&gt;42.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AIME 2025&lt;/cell&gt;
        &lt;cell&gt;95.7&lt;/cell&gt;
        &lt;cell&gt;93.9&lt;/cell&gt;
        &lt;cell&gt;94.5&lt;/cell&gt;
        &lt;cell&gt;93.1&lt;/cell&gt;
        &lt;cell&gt;95.0&lt;/cell&gt;
        &lt;cell&gt;87.0&lt;/cell&gt;
        &lt;cell&gt;94.6&lt;/cell&gt;
        &lt;cell&gt;94.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;HMMT Feb. 2025&lt;/cell&gt;
        &lt;cell&gt;97.1&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;89.4&lt;/cell&gt;
        &lt;cell&gt;92.5&lt;/cell&gt;
        &lt;cell&gt;97.5&lt;/cell&gt;
        &lt;cell&gt;79.2&lt;/cell&gt;
        &lt;cell&gt;88.3&lt;/cell&gt;
        &lt;cell&gt;96.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;HMMT Nov. 2025&lt;/cell&gt;
        &lt;cell&gt;93.5&lt;/cell&gt;
        &lt;cell&gt;87.7&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;90.2&lt;/cell&gt;
        &lt;cell&gt;93.3&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;IMOAnswerBench&lt;/cell&gt;
        &lt;cell&gt;82.0&lt;/cell&gt;
        &lt;cell&gt;73.5&lt;/cell&gt;
        &lt;cell&gt;78.6&lt;/cell&gt;
        &lt;cell&gt;78.3&lt;/cell&gt;
        &lt;cell&gt;83.3&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;LiveCodeBench-v6&lt;/cell&gt;
        &lt;cell&gt;84.9&lt;/cell&gt;
        &lt;cell&gt;82.8&lt;/cell&gt;
        &lt;cell&gt;83.1&lt;/cell&gt;
        &lt;cell&gt;83.3&lt;/cell&gt;
        &lt;cell&gt;90.7&lt;/cell&gt;
        &lt;cell&gt;64.0&lt;/cell&gt;
        &lt;cell&gt;87.0&lt;/cell&gt;
        &lt;cell&gt;87.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Code Agent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SWE-bench Verified&lt;/cell&gt;
        &lt;cell&gt;73.8&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
        &lt;cell&gt;77.2&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;76.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SWE-bench Multilingual&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;53.8&lt;/cell&gt;
        &lt;cell&gt;61.1&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;55.3&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Terminal Bench Hard&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;23.6&lt;/cell&gt;
        &lt;cell&gt;30.6&lt;/cell&gt;
        &lt;cell&gt;35.4&lt;/cell&gt;
        &lt;cell&gt;39.0&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;30.5&lt;/cell&gt;
        &lt;cell&gt;43.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Terminal Bench 2.0&lt;/cell&gt;
        &lt;cell&gt;41.0&lt;/cell&gt;
        &lt;cell&gt;24.5&lt;/cell&gt;
        &lt;cell&gt;35.7&lt;/cell&gt;
        &lt;cell&gt;46.4&lt;/cell&gt;
        &lt;cell&gt;54.2&lt;/cell&gt;
        &lt;cell&gt;42.8&lt;/cell&gt;
        &lt;cell&gt;35.2&lt;/cell&gt;
        &lt;cell&gt;47.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;General Agent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;52.0&lt;/cell&gt;
        &lt;cell&gt;45.1&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;51.4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;24.1&lt;/cell&gt;
        &lt;cell&gt;54.9&lt;/cell&gt;
        &lt;cell&gt;50.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BrowseComp (w/ Context Manage)&lt;/cell&gt;
        &lt;cell&gt;67.5&lt;/cell&gt;
        &lt;cell&gt;57.5&lt;/cell&gt;
        &lt;cell&gt;60.2&lt;/cell&gt;
        &lt;cell&gt;67.6&lt;/cell&gt;
        &lt;cell&gt;59.2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BrowseComp-ZH&lt;/cell&gt;
        &lt;cell&gt;66.6&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
        &lt;cell&gt;62.3&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;42.4&lt;/cell&gt;
        &lt;cell&gt;63.0&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;τ²-Bench&lt;/cell&gt;
        &lt;cell&gt;87.4&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;85.3&lt;/cell&gt;
        &lt;cell&gt;90.7&lt;/cell&gt;
        &lt;cell&gt;87.2&lt;/cell&gt;
        &lt;cell&gt;82.4&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Coding: AGI is a long journey, and benchmarks are only one way to evaluate performance. While the metrics provide necessary checkpoints, the most important thing is still how it *feels*. True intelligence isn't just about acing a test or processing data faster; ultimately, the success of AGI will be measured by how seamlessly it integrates into our lives-"coding" this time.&lt;/p&gt;
    &lt;p&gt;GLM-4.7 enhances Interleaved Thinking, a feature introduced since GLM-4.5, and further introduces Preserved Thinking and Turn-level Thinking. By thinking between actions and staying consistent across turns, it makes complex tasks more stable and more controllable:&lt;/p&gt;
    &lt;p&gt;More details: https://docs.z.ai/guides/capabilities/thinking-mode&lt;/p&gt;
    &lt;p&gt;The Z.ai API platform offers the GLM-4.7 model. For comprehensive API documentation and integration guidelines, please refer to https://docs.z.ai/guides/llm/glm-4.7. At the same time, the model is also available worldwide through OpenRouter (https://openrouter.ai/).&lt;/p&gt;
    &lt;p&gt;GLM-4.7 is now available to use within coding agents (Claude Code, Kilo Code, Roo Code, Cline and more).&lt;/p&gt;
    &lt;p&gt;For GLM Coding Plan subscribers: You'll be automatically upgraded to GLM-4.7. If you've previously customized the app configs (like &lt;code&gt;~/.claude/settings.json&lt;/code&gt; in Claude Code), simply update the model name to "glm-4.7" to complete the upgrade.&lt;/p&gt;
    &lt;p&gt;For New users: Subscribing GLM Coding Plan means having access to a Claude-level coding model at a fraction of the cost — just 1/7th the price with 3x the usage quota. Start building today: https://z.ai/subscribe.&lt;/p&gt;
    &lt;p&gt;GLM-4.7 is accessible through Z.ai. Try to change the model option to GLM-4.7, if the system does not automatically do that (not like an AGI in that case :))&lt;/p&gt;
    &lt;p&gt;Model weights for GLM-4.7 are publicly available on HuggingFace and ModelScope. For local deployment, GLM-4.7 supports inference frameworks including vLLM and SGLang. Comprehensive deployment instructions are available in the official GitHub repository.&lt;/p&gt;
    &lt;p&gt;1: Default settings (most tasks): temperature 1.0, top-p 0.95, max new tokens 131072. For multi-turn agentic tasks (τ²-Bench and Terminal Bench 2), enable Preserved Thinking mode.&lt;/p&gt;
    &lt;p&gt;2: Terminal Bench and SWE-bench Verified settings: temperature 0.7, top-p 1.0, max new tokens 16384.&lt;/p&gt;
    &lt;p&gt;3: τ²-Bench settings: temperature 0, max new tokens 16384. For τ²-Bench, we added an extra prompt in the Retail and Telecom interactions to avoid failures caused by users ending the interaction incorrectly; for the Airline domain, we applied the domain fixes proposed in the Claude Opus 4.5 release report.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357287</guid><pubDate>Mon, 22 Dec 2025 18:46:32 +0000</pubDate></item><item><title>Things I learnt about passkeys when building passkeybot</title><link>https://enzom.dev/b/passkeys/</link><description>&lt;doc fingerprint="b4680914610bb154"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Things I learnt about passkeys when building passkeybot&lt;/head&gt;
    &lt;p&gt;I recently released passkeybot.com, a hosted sign in page that allows you to add passkey auth to your site with just a few server side HTTP handlers.&lt;/p&gt;
    &lt;p&gt;Here are the things I learnt in the process.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Secure Enclave Processors (SEP) are&lt;/head&gt;
    &lt;p&gt;Apple devices have secure enclaves which are like a separate tiny computer living inside the main CPU that has its own isolated encrypted memory and OS. It can create secrets that never leave the secure enclave. The main OS can only prove it has possession of that secret by asking the secure enclave to sign some data and getting the signature as a response (it can only use this message protocol with the SEP).&lt;/p&gt;
    &lt;p&gt;When the user signs in with their passkey with User Verification = true, the SEP requires a biometric/passcode auth first before signing the data with the private key.&lt;/p&gt;
    &lt;p&gt;Other devices have something similar to the SEP, but are branded with different names.&lt;/p&gt;
    &lt;p&gt;Phone SIM cards are actually a form of secure element. SIM cards are CPUs that run a stripped down version of Java, and use the same principle of “secrets can never leave the SIM” and “prove possession with message signing”.&lt;/p&gt;
    &lt;head rend="h2"&gt;User Presence (UP) vs User Verification (UV)&lt;/head&gt;
    &lt;p&gt;Presence means “the user tapped a button and was there”, verification means “the user entered their biometric or passcode”.&lt;/p&gt;
    &lt;p&gt;You can request which one you require with the JS passkey API.&lt;/p&gt;
    &lt;p&gt;The difference is presence can be faked by anyone with the unlocked device by pressing a button, but verification always requires the re-auth of the user with biometrics or a passcode.&lt;/p&gt;
    &lt;head rend="h2"&gt;What an authenticator is&lt;/head&gt;
    &lt;p&gt;An authenticator is the hardware and software that holds the private/public key pairs and signs the passkey challenge to prove it has the private key. On Apple devices that is the SEP.&lt;/p&gt;
    &lt;p&gt;The browser asks the user which authenticator they want to use, then uses OS level APIs to interact with the chosen authenticator.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;User chooses on-device Apple SEP → site calls JS API → browser uses Swift API for passkey operations.&lt;/item&gt;
      &lt;item&gt;User chooses Yubikey → site calls JS API → browser uses Yubikey API over USB for passkey operations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The interesting thing here is that the JS API normalises all these different possible authenticator APIs. Under the hood the browser implements all the possible API protocols for different authenticators.&lt;/p&gt;
    &lt;p&gt;The Chrome Dev Tools also has a virtual authenticator to bypass reptetive OS password entry for testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What attestation is&lt;/head&gt;
    &lt;p&gt;Signing proves possession: Being able to sign with the private key proves you have possession of it.&lt;/p&gt;
    &lt;p&gt;Attestation proves device hardware used: Attestation proves what hardware and software combination created the passkey pair. It allows enforcing policies for what set of hardware devices are trusted, and which are blocked.&lt;/p&gt;
    &lt;p&gt;The issue is that attestation data also allows fingerprinting as it reveals exactly what hardware the user is using.&lt;/p&gt;
    &lt;p&gt;Hardware attestation only occurs for the creation of the passkey pair (not on every auth). This creates an issue: if keys are synced to another device, the attestation is no longer valid. So if you require strict attestation that specific hardware is used, you create a new keypair for every device instead of allowing use of a passkey pair that has moved to a non-attested device.&lt;/p&gt;
    &lt;p&gt;Without attestation the secure enclave is still used, it is just not proven to the webauthn client API.&lt;/p&gt;
    &lt;p&gt;Apple hardware has attestation disabled by default unless you have enterprise device management enabled. This lets enterprises define an allow-list of trusted authenticator hardware.&lt;/p&gt;
    &lt;head rend="h2"&gt;Passkeys are just for authentication, not for general signing of intent&lt;/head&gt;
    &lt;p&gt;When the user authenticates with a passkey, they sign a challenge that is a hash unique to that particular sign in flow. The challenge hash needs to have 16 bytes or more of random data to avoid replays, but it can also include a hash over other metadata.&lt;/p&gt;
    &lt;p&gt;The authenticator GUI only shows “sign in to your_domain.com”. It never allows a more general “sign this content for your_domain.com”. E.g. “sign this transaction request to move £50 to Bob's account”.&lt;/p&gt;
    &lt;head rend="h2"&gt;The JS code must be secure, but it cannot be verified&lt;/head&gt;
    &lt;p&gt;If the JS code of a site is compromised, the attacker can read all personal data. They could also trick the user into signing something with their authenticator (as the authenticator does not show the user what they are actually signing) - this leads to a real private key signing over faked challenge data.&lt;/p&gt;
    &lt;p&gt;The browser has Subresource Integrity (SRI) which only allows executing JS scripts with a given hash. But the root document HTML is not checked in that case, which means the attacker can change the SRI hashes to match their own JS. Chrome Extensions also allow injecting JS.&lt;/p&gt;
    &lt;p&gt;It would be interesting if authenticators could also attest as to what HTML/JS was loaded on the page to rule out that they have been compromised.&lt;/p&gt;
    &lt;head rend="h2"&gt;Immediate mediation - an upcoming “fast sign in” API&lt;/head&gt;
    &lt;p&gt;This Chrome origin trial will add an option the passkey API:&lt;/p&gt;
    &lt;quote&gt;navigator.credentials.get({mediation: "immediate"})&lt;/quote&gt;
    &lt;p&gt;This allows you to sign in a user who already has a passkey quickly.&lt;/p&gt;
    &lt;p&gt;If they do not have a passkey, you can decide what to do from JS (instead of having the browser show them a UI to find passkeys on other devices).&lt;/p&gt;
    &lt;p&gt;There is no way to get a list of the user's passkeys from JS - lists are always shown from the browser UI.&lt;/p&gt;
    &lt;p&gt;The immediate mediation option allows your JS to get an immediate “the user has 0 local keys” message without any user interaction:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;0 keys: Immediate JS response with NotAllowedError, JS decides next step.&lt;/item&gt;
      &lt;item&gt;1 key: Immediately ask the user to sign in with that one.&lt;/item&gt;
      &lt;item&gt;&amp;gt;1 key: Ask the user to choose.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Related Origin Requests&lt;/head&gt;
    &lt;p&gt; Related Origin Requests allow you as a domain owner to define a list of other domains that can create passkeys for your domain. They work by having you serve the list from &lt;code&gt;/.well-known/webauthn&lt;/code&gt;.
				&lt;/p&gt;
    &lt;p&gt;This is what passkeybot.com uses to allow domain owners to grant permissions to passkeybot.&lt;/p&gt;
    &lt;p&gt;But RORs do not work over HTTP, only HTTPS. The reason is the authenticator requests the well-known file over HTTPS only, so localhost will not work.&lt;/p&gt;
    &lt;p&gt;They are also not supported in iOS 18 or Firefox.&lt;/p&gt;
    &lt;p&gt;If an authenticator creates a passkey for the root domain, that passkey will work for all subdomains. But if it creates it for a subdomain, it only ever works on that specific subdomain.&lt;/p&gt;
    &lt;head rend="h2"&gt;The counter is just a “heuristic”&lt;/head&gt;
    &lt;p&gt;Authenticators store a counter which increments for each passkey usage. In theory this can detect a cloned authenticator. But much of the recommendations say to use the counter as a heuristic rather than evidence of a cloned authenticator because there are many legitimate reasons the counter can be wrong. I think in practice this counter is often ignored.&lt;/p&gt;
    &lt;head rend="h2"&gt;Use passkeys stored on nearby devices using Bluetooth&lt;/head&gt;
    &lt;p&gt;You can sign into a public computer that does not have your passkeys by having your own device's authenticator communicate with it over Bluetooth Low Energy (BLE). Bluetooth is used to assert your close proximity with the device you are signing into. Your keys never leave your device, the signing protocol travels between the two devices.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deleting passkeys with the Signal API&lt;/head&gt;
    &lt;p&gt;The JS API cannot list or modify the list of passkeys, only the browser GUI or Apple Passwords can do that.&lt;/p&gt;
    &lt;p&gt;But you can asynchronously signal that you want to delete a passkey. It is only a hint, and you will not receive back any confirmation as that may leak user data.&lt;/p&gt;
    &lt;p&gt;The Signal API methods currently are:&lt;/p&gt;
    &lt;quote&gt;PublicKeyCredential.signalUnknownCredential({ rpId, credentialId }) PublicKeyCredential.signalAllAcceptedCredentials({ rpId, userId, allAcceptedCredentialIds }) PublicKeyCredential.signalCurrentUserDetails({ rpId, userId, name, displayName })&lt;/quote&gt;
    &lt;head rend="h2"&gt;user.id and userHandle represent “one account”&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;user.id&lt;/code&gt; and userHandle are the same value, but with
					different names in different JS API calls.
				&lt;/p&gt;
    &lt;p&gt; They are used to map many passkeys to a single logical account. It should be set and stored as passkey APIs require the &lt;code&gt;user.id&lt;/code&gt; (like the signal APIs above).
				&lt;/p&gt;
    &lt;p&gt; You can generate one unique &lt;code&gt;user.id&lt;/code&gt; per new passkey and
					just store the user =&amp;gt; passkey relation in your database, but this
					may prevent “per account” passkey grouping and management in the
					browser UI.
				&lt;/p&gt;
    &lt;head rend="h2"&gt;crypto.subtle.generateKey can create non-extractable keys&lt;/head&gt;
    &lt;p&gt;generateKey is a JS API that allows you to create new key pairs, where the private key cannot be extracted similar to passkeys.&lt;/p&gt;
    &lt;quote&gt;crypto.subtle.generateKey(algorithm, extractable, keyUsages)&lt;/quote&gt;
    &lt;p&gt;You can perform general operations like signing with this key pair, but in the case of JS being compromised, the private key cannot be read and moved to a different device. But compromised JS can still sign using that unextractable private key.&lt;/p&gt;
    &lt;head rend="h2"&gt;PKCE = "Proof Key for Code Exchange” was retrofitted into OAuth&lt;/head&gt;
    &lt;p&gt;PKCE is a protocol that works like a one time password: at the start of every sign in flow an actor creates a code_verifier and code_challenge.&lt;/p&gt;
    &lt;p&gt;The code_verifier is a secret random 32 bytes held on the flow initiating actor. The code_challenge is the sha256 hash of those bytes, and is shared with the user going through the sign in flow.&lt;/p&gt;
    &lt;p&gt;The code_challenge is also sent to the auth service that verifies the user and creates a (token, code_challenge).&lt;/p&gt;
    &lt;p&gt;PKCE protects this token by only allowing the holder of the code_verifier secret to redeem it by sending (token, code_verifier).&lt;/p&gt;
    &lt;p&gt;This means even if the token is stolen, only the actor that started the flow can redeem it with the auth API.&lt;/p&gt;
    &lt;p&gt;PKCE was originally designed for environments that cannot hold static secrets because the source code can be read - like JS or desktop apps. Instead of embedding static secrets, these actors dynamically create secrets at runtime at the start of each sign in flow (the code_verifier and code_challenge pair).&lt;/p&gt;
    &lt;p&gt;Passkeybot uses PKCE to avoid having to manage API bearer token secrets for each API client. Note: Passkeybot uses the general principle behind PKCE, but naming and interaction differs from the OAuth standard.&lt;/p&gt;
    &lt;p&gt;It is interesting how they managed to retrofit PKCE into the OAuth standard to solve the “token interception” problem. It only requires a sha256 hash function so is very easy to implement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Digital Credentials API is a browser bridge to the native OS wallet&lt;/head&gt;
    &lt;p&gt;This is a passkey adjacent JS API. The Digital Credentials API will allow you to request things from the user's native OS wallet like IDs, tickets, badges, membership cards. It allows you to do things like prove your age or ability to drive without having to share your actual identity cards.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357451</guid><pubDate>Mon, 22 Dec 2025 18:58:45 +0000</pubDate></item><item><title>Universal Reasoning Model (53.8% pass 1 ARC1 and 16.0% ARC 2)</title><link>https://arxiv.org/abs/2512.14693</link><description>&lt;doc fingerprint="ac9d340ac5f784cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 16 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Universal Reasoning Model&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at this https URL.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357458</guid><pubDate>Mon, 22 Dec 2025 18:59:22 +0000</pubDate></item><item><title>State regulators vote to keep utility profits high angering customers across CA</title><link>https://www.latimes.com/environment/story/2025-12-18/state-regulators-vote-to-keep-utility-profits-high-angering-customers</link><description>&lt;doc fingerprint="84d094eabb39a708"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;gn-popover&gt;Click here to listen to this article&lt;/gn-popover&gt;
      &lt;/item&gt;
      &lt;item&gt; Share via&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;California regulators voted to keep utility profit margins near 10%, despite calls to cut them to 6% and save customers billions annually.&lt;/item&gt;
      &lt;item&gt;Edison’s electric rates have surged more than 40% in three years, pushing California to the nation’s second-highest rates after Hawaii.&lt;/item&gt;
      &lt;item&gt;Regulators approved slightly lower profit margins amid complaints from customers about rising electric bills.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Despite complaints from customers about rising electric bills, the California Public Utilities Commission voted 4 to 1 on Thursday to keep profits at Southern California Edison and the state’s other big investor-owned utilities at a level that consumer groups say has long been inflated.&lt;/p&gt;
    &lt;p&gt;The commission vote will slightly decrease the profit margins of Edison and three other big utilities beginning next year. Edison’s rate will fall to 10.03% from 10.3%.&lt;/p&gt;
    &lt;p&gt;Customers will see little impact in their bills from the decision. Because the utilities are continuing to spend more on wires and other infrastructure — capital costs that they earn profit on — that portion of customer bills is expected to continue to rise.&lt;/p&gt;
    &lt;p&gt;Southern California Edison began charging customers for hundreds of millions of dollars of maintenance on its aging transmission lines that regulators approved but it did not actually do in the four years before the Eaton fire, according to state documents.&lt;/p&gt;
    &lt;p&gt;The vote angered consumer groups that had detailed in filings and hearings at the commission how the utilities’ return on equity — which sets the profit rate that the companies’ shareholders receive — had long been too high.&lt;/p&gt;
    &lt;p&gt;Among those testifying on behalf of consumers was Mark Ellis, the former chief economist for Sempra, the parent company of San Diego Gas &amp;amp; Electric and Southern California Gas. Ellis estimated that the companies’ profit margin should be closer to 6%.&lt;/p&gt;
    &lt;p&gt;He argued in a filing that the California commission had for years authorized the utilities to earn an excessive return on equity, resulting in an “unnecessary and unearned wealth transfer” from customers to the companies.&lt;/p&gt;
    &lt;p&gt;Cutting the return on equity to a little more than 6% would give Edison, Pacific Gas &amp;amp; Electric, SDG&amp;amp;E and SoCalGas a fair return, Ellis said, while saving their customers $6.1 billion a year.&lt;/p&gt;
    &lt;p&gt;The four commissioners who voted to keep the return on equity at about 10% — the percentage varies slightly for each company — said they believed they had found a balance between the 11% or higher rate that the four utilities had requested and the affordability concerns of utility customers.&lt;/p&gt;
    &lt;p&gt;Alice Reynolds, the commission’s president, said before the vote that she believed the decision “accurately reflects the evidence.”&lt;/p&gt;
    &lt;p&gt;Commissioner Darcie Houck disagreed and voted against the proposal. In her remarks, she detailed how California ratepayers were struggling to pay their bills.&lt;/p&gt;
    &lt;p&gt;“We have a duty to consider the consumer interest in determining what is a just and reasonable rate,” she said.&lt;/p&gt;
    &lt;p&gt;Consumer groups criticized the commission’s vote.&lt;/p&gt;
    &lt;p&gt;“For too long, utility companies have been extracting unreasonable profits from Californians just trying to heat or cool their homes or keep the lights on,” said Jenn Engstrom at CALPIRG. “As long as CPUC allows such lofty rates of return, it incentivizes power companies to overspend, increasing energy bills for everyone.”&lt;/p&gt;
    &lt;p&gt;California now has the nation’s second-highest electric rates after Hawaii.&lt;/p&gt;
    &lt;p&gt;Regulators wanted Southern California Edison and other utilities to remove abandoned power lines — such as the one suspected of igniting the Eaton fire — but backed down amid utility opposition.&lt;/p&gt;
    &lt;p&gt;Edison’s electric rates have risen by more than 40% in the last three years, according to a November analysis by the commission’s Public Advocates Office. More than 830,000 Edison customers are behind in paying their electric bills, the office said, each owing a balance of $835 on average.&lt;/p&gt;
    &lt;p&gt;The commission’s vote Thursday was in response to a March request from Edison and the three other big for-profit utilities. The companies pointed to the January wildfires in Los Angeles County, saying they needed to provide their shareholders with more profit to get them to continue to invest in their stock because of the threat of utility-caused fires in California.&lt;/p&gt;
    &lt;p&gt;In its filing, Edison asked for a return on equity of 11.75%, saying that it faced “elevated business risks,” including “the risk of extreme wildfires.”&lt;/p&gt;
    &lt;p&gt;The company told the commission that its stock had declined after the Jan. 7 Eaton fire and it needed the higher return on equity to attract investors to provide it with money for “wildfire mitigation and supporting California’s clean energy transition.”&lt;/p&gt;
    &lt;p&gt;Edison is facing hundreds of lawsuits filed by victims of the fire, which killed 19 people and destroyed thousands of homes in Altadena. The company has said the fire may have been sparked by its 100-year-old transmission line in Eaton Canyon, which it kept in place even though it hadn’t served customers since 1971.&lt;/p&gt;
    &lt;p&gt;The California Public Utilities Commission is set to allow Southern California Edison to increase customer bills by nearly 10% on Oct. 1 — just one of multiple rate hikes expected in the coming year.&lt;/p&gt;
    &lt;p&gt;Return on equity is crucial for utilities because it determines how much they and their shareholders earn each year on the electric lines, substations, pipelines and the rest of the system they build to serve customers.&lt;/p&gt;
    &lt;p&gt;Under the state’s system for setting electric rates, investors provide part of the money needed to build the infrastructure and then earn an annual return on that investment over the assets’ life, which can be 30 or 40 years.&lt;/p&gt;
    &lt;p&gt;In a January report, state legislative analyst Gabriel Petek detailed how electric rates at Edison and the state’s two other biggest investor-owned electric utilities were 50% higher than those charged by public utilities such as the Los Angeles Department of Water and Power. The public utilities don’t have investors or charge customers extra for profit.&lt;/p&gt;
    &lt;p&gt;Before the vote, dozens of utility customers from across the state wrote to the commission’s five members, who were appointed by Gov. Gavin Newsom, asking them to lower the utilities’ return on equity.&lt;/p&gt;
    &lt;p&gt;“A profit margin of 10% on infrastructure improvements is far too high and will only continue to increase the cost of living in California,” wrote James Ward, a Rancho Santa Margarita resident. “I just wish I could get a guaranteed profit margin of 10% on my investments.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357535</guid><pubDate>Mon, 22 Dec 2025 19:06:16 +0000</pubDate></item><item><title>The Illustrated Transformer</title><link>https://jalammar.github.io/illustrated-transformer/</link><description>&lt;doc fingerprint="37105b94113a1a90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Illustrated Transformer&lt;/head&gt;
    &lt;p&gt;Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) &lt;lb/&gt; Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese &lt;lb/&gt; Watch: MIT’s Deep Learning State of the Art lecture referencing this post &lt;lb/&gt; Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.&lt;/p&gt;
    &lt;p&gt;The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.&lt;/p&gt;
    &lt;p&gt;2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:&lt;/p&gt;
    &lt;head rend="h2"&gt;A High-Level Look&lt;/head&gt;
    &lt;p&gt;Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.&lt;/p&gt;
    &lt;p&gt;Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.&lt;/p&gt;
    &lt;p&gt;The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.&lt;/p&gt;
    &lt;p&gt;The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:&lt;/p&gt;
    &lt;p&gt;The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.&lt;/p&gt;
    &lt;p&gt;The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.&lt;/p&gt;
    &lt;p&gt;The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).&lt;/p&gt;
    &lt;head rend="h2"&gt;Bringing The Tensors Into The Picture&lt;/head&gt;
    &lt;p&gt;Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.&lt;/p&gt;
    &lt;p&gt;As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.&lt;/p&gt;
    &lt;p&gt;Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.&lt;/p&gt;
    &lt;p&gt;The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.&lt;/p&gt;
    &lt;p&gt;After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.&lt;/p&gt;
    &lt;p&gt;Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.&lt;/p&gt;
    &lt;p&gt;Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.&lt;/p&gt;
    &lt;head rend="h2"&gt;Now We’re Encoding!&lt;/head&gt;
    &lt;p&gt;As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.&lt;/p&gt;
    &lt;p&gt;The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Attention at a High Level&lt;/head&gt;
    &lt;p&gt;Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.&lt;/p&gt;
    &lt;p&gt;Say the following sentence is an input sentence we want to translate:&lt;/p&gt;
    &lt;p&gt;”&lt;code&gt;The animal didn't cross the street because it was too tired&lt;/code&gt;”&lt;/p&gt;
    &lt;p&gt;What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.&lt;/p&gt;
    &lt;p&gt;When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.&lt;/p&gt;
    &lt;p&gt;As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.&lt;/p&gt;
    &lt;p&gt;If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.&lt;/p&gt;
    &lt;p&gt;As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".&lt;/p&gt;
    &lt;p&gt;Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Attention in Detail&lt;/head&gt;
    &lt;p&gt;Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.&lt;/p&gt;
    &lt;p&gt;The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.&lt;/p&gt;
    &lt;p&gt;Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.&lt;/p&gt;
    &lt;p&gt;Multiplying x1 by the WQ weight matrix produces q1, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;What are the “query”, “key”, and “value” vectors? &lt;lb/&gt; They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.&lt;/p&gt;
    &lt;p&gt;The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.&lt;/p&gt;
    &lt;p&gt;The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.&lt;/p&gt;
    &lt;p&gt;The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.&lt;/p&gt;
    &lt;p&gt;This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.&lt;/p&gt;
    &lt;p&gt;The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).&lt;/p&gt;
    &lt;p&gt;The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).&lt;/p&gt;
    &lt;p&gt;That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.&lt;/p&gt;
    &lt;head rend="h2"&gt;Matrix Calculation of Self-Attention&lt;/head&gt;
    &lt;p&gt;The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).&lt;/p&gt;
    &lt;p&gt;Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)&lt;/p&gt;
    &lt;p&gt;Finally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.&lt;/p&gt;
    &lt;p&gt;The self-attention calculation in matrix form&lt;/p&gt;
    &lt;head rend="h2"&gt;The Beast With Many Heads&lt;/head&gt;
    &lt;p&gt;The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices&lt;/p&gt;
    &lt;p&gt;This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.&lt;/p&gt;
    &lt;p&gt;How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.&lt;/p&gt;
    &lt;p&gt;That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place&lt;/p&gt;
    &lt;p&gt;Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:&lt;/p&gt;
    &lt;p&gt;As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".&lt;/p&gt;
    &lt;p&gt;If we add all the attention heads to the picture, however, things can be harder to interpret:&lt;/p&gt;
    &lt;head rend="h2"&gt;Representing The Order of The Sequence Using Positional Encoding&lt;/head&gt;
    &lt;p&gt;One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.&lt;/p&gt;
    &lt;p&gt;To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.&lt;/p&gt;
    &lt;p&gt;To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.&lt;/p&gt;
    &lt;p&gt;If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:&lt;/p&gt;
    &lt;p&gt;A real example of positional encoding with a toy embedding size of 4&lt;/p&gt;
    &lt;p&gt;What might this pattern look like?&lt;/p&gt;
    &lt;p&gt;In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.&lt;/p&gt;
    &lt;p&gt;A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.&lt;/p&gt;
    &lt;p&gt;The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in &lt;code&gt;get_timing_signal_1d()&lt;/code&gt;. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).&lt;/p&gt;
    &lt;p&gt;July 2020 Update: The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here’s the code to generate it:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Residuals&lt;/head&gt;
    &lt;p&gt;One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.&lt;/p&gt;
    &lt;p&gt;If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:&lt;/p&gt;
    &lt;p&gt;This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Decoder Side&lt;/head&gt;
    &lt;p&gt;Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.&lt;/p&gt;
    &lt;p&gt;The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:&lt;/p&gt;
    &lt;p&gt;After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).&lt;/p&gt;
    &lt;p&gt;The following steps repeat the process until a special &lt;/p&gt;
    &lt;p&gt;The self attention layers in the decoder operate in a slightly different way than the one in the encoder:&lt;/p&gt;
    &lt;p&gt;In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to &lt;code&gt;-inf&lt;/code&gt;) before the softmax step in the self-attention calculation.&lt;/p&gt;
    &lt;p&gt;The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Final Linear and Softmax Layer&lt;/head&gt;
    &lt;p&gt;The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.&lt;/p&gt;
    &lt;p&gt;The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.&lt;/p&gt;
    &lt;p&gt;Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.&lt;/p&gt;
    &lt;p&gt;The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.&lt;/p&gt;
    &lt;p&gt;This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recap Of Training&lt;/head&gt;
    &lt;p&gt;Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.&lt;/p&gt;
    &lt;p&gt;During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.&lt;/p&gt;
    &lt;p&gt;To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “&amp;lt;eos&amp;gt;” (short for ‘end of sentence’)).&lt;/p&gt;
    &lt;p&gt;The output vocabulary of our model is created in the preprocessing phase before we even begin training.&lt;/p&gt;
    &lt;p&gt;Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:&lt;/p&gt;
    &lt;p&gt;Example: one-hot encoding of our output vocabulary&lt;/p&gt;
    &lt;p&gt;Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Loss Function&lt;/head&gt;
    &lt;p&gt;Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.&lt;/p&gt;
    &lt;p&gt;What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.&lt;/p&gt;
    &lt;p&gt;Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.&lt;/p&gt;
    &lt;p&gt;How do you compare two probability distributions? We simply subtract one from the other. For more details, look at cross-entropy and Kullback–Leibler divergence.&lt;/p&gt;
    &lt;p&gt;But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)&lt;/item&gt;
      &lt;item&gt;The first probability distribution has the highest probability at the cell associated with the word “i”&lt;/item&gt;
      &lt;item&gt;The second probability distribution has the highest probability at the cell associated with the word “am”&lt;/item&gt;
      &lt;item&gt;And so on, until the fifth output distribution indicates ‘&lt;code&gt;&amp;lt;end of sentence&amp;gt;&lt;/code&gt;’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The targeted probability distributions we'll train our model against in the training example for one sample sentence.&lt;/p&gt;
    &lt;p&gt;After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:&lt;/p&gt;
    &lt;p&gt;Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.&lt;/p&gt;
    &lt;p&gt;Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.&lt;/p&gt;
    &lt;head rend="h2"&gt;Go Forth And Transform&lt;/head&gt;
    &lt;p&gt;I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.&lt;/item&gt;
      &lt;item&gt;Watch Łukasz Kaiser’s talk walking through the model and its details&lt;/item&gt;
      &lt;item&gt;Play with the Jupyter Notebook provided as part of the Tensor2Tensor repo&lt;/item&gt;
      &lt;item&gt;Explore the Tensor2Tensor repo.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Follow-up works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Depthwise Separable Convolutions for Neural Machine Translation&lt;/item&gt;
      &lt;item&gt;One Model To Learn Them All&lt;/item&gt;
      &lt;item&gt;Discrete Autoencoders for Sequence Models&lt;/item&gt;
      &lt;item&gt;Generating Wikipedia by Summarizing Long Sequences&lt;/item&gt;
      &lt;item&gt;Image Transformer&lt;/item&gt;
      &lt;item&gt;Training Tips for the Transformer Model&lt;/item&gt;
      &lt;item&gt;Self-Attention with Relative Position Representations&lt;/item&gt;
      &lt;item&gt;Fast Decoding in Sequence Models using Discrete Latent Variables&lt;/item&gt;
      &lt;item&gt;Adafactor: Adaptive Learning Rates with Sublinear Memory Cost&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.&lt;/p&gt;
    &lt;p&gt;Please hit me up on Twitter for any corrections or feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357675</guid><pubDate>Mon, 22 Dec 2025 19:15:56 +0000</pubDate></item><item><title>The Garbage Collection Handbook</title><link>https://gchandbook.org/index.html</link><description>&lt;doc fingerprint="73791c0d628aee51"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Second edition&lt;/head&gt;
    &lt;p&gt;Richard Jonesâs Garbage Collection (Wiley, 1996) was a milestone book in the area of automatic memory management. Its widely acclaimed successor, The Garbage Collection Handbook: The Art of Automatic Memory Management captured the state of the field in 2012. However, technology developments have made memory management more challenging, interesting and important than ever. This second edition updates the handbook, bringing together a wealth of knowledge gathered by automatic memory management researchers and developers over the past sixty years. The authors compare the most important approaches and state-of-the-art techniques in a single, accessible framework.&lt;/p&gt;
    &lt;p&gt;The book addresses new challenges to garbage collection made by recent advances in hardware and software, and the environments in which programs are executed. It explores the consequences of these changes for designers and implementers of high performance garbage collectors. Along with simple and traditional algorithms, the book covers state-of-the-art parallel, incremental, concurrent and real-time garbage collection. Algorithms and concepts are often described with pseudocode and illustrations.&lt;/p&gt;
    &lt;p&gt;The nearly universal adoption of garbage collection by modern programming languages makes a thorough understanding of this topic essential for any programmer. This authoritative handbook gives expert insight on how different collectors work as well as the various issues currently facing garbage collectors. Armed with this knowledge, programmers can confidently select and configure the many choices of garbage collectors.&lt;/p&gt;
    &lt;head rend="h2"&gt;Features of the book&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Provides a complete, up-to-date, and authoritative sequel to the 1996 and 2012 books&lt;/item&gt;
      &lt;item&gt;Offers thorough coverage of parallel, concurrent and real-time garbage collection algorithms&lt;/item&gt;
      &lt;item&gt;Discusses in detail modern, high-performance commercial collectors&lt;/item&gt;
      &lt;item&gt;Explains some of the tricky aspects of garbage collection, including the interface to the run-time system&lt;/item&gt;
      &lt;item&gt;Over 90 more pages, including new chapters on persistence and energy-aware garbage collection&lt;/item&gt;
      &lt;item&gt;Backed by a comprehensive online database of nearly 3,400 garbage collection-related publications&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;e-book and translations&lt;/head&gt;
    &lt;p&gt;The e-book enhances the print versions with a rich collection of over 37,000 hyperlinks to chapters, sections, algorithms, figures, glossary entries, index items, original research papers and much more.&lt;/p&gt;
    &lt;p&gt;Chinese and Japanese translations of the first edition were published in 2016. We thank the translators for their work in bringing our book to a wider audience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web Resources&lt;/head&gt;
    &lt;p&gt;The online bibliographic database includes nearly 3,400 garbage collection-related publications. It contains abstracts for some entries and URLs or DOIs for most of the electronically available ones, and is continually being updated. The database can be searched online, or downloaded as BibTeX, PostScript or PDF.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357870</guid><pubDate>Mon, 22 Dec 2025 19:30:18 +0000</pubDate></item><item><title>US blocks all offshore wind construction, says reason is classified</title><link>https://arstechnica.com/science/2025/12/us-government-finds-new-excuse-to-stop-construction-of-offshore-wind/</link><description>&lt;doc fingerprint="9f512d50423481d1"&gt;
  &lt;main&gt;
    &lt;p&gt;On Monday, the US Department of the Interior announced that it was pausing the leases on all five offshore wind sites currently under construction in the US. The move comes despite the fact that these projects already have installed significant hardware in the water and on land; one of them is nearly complete. In what appears to be an attempt to avoid legal scrutiny, the Interior is blaming the decisions on a classified report from the Department of Defense.&lt;/p&gt;
    &lt;p&gt;The second Trump administration announced its animosity toward offshore wind power literally on day one, issuing an executive order on inauguration day that called for a temporary halt to issuing permits for new projects pending a re-evaluation. Earlier this month, however, a judge vacated that executive order, noting that the government has shown no indication that it was even attempting to start the re-evaluation it said was needed.&lt;/p&gt;
    &lt;p&gt;But a number of projects have gone through the entire permitting process, and construction has started. Before today, the administration had attempted to stop these in an erratic, halting manner. Empire Wind, an 800 MW farm being built off New York, was stopped by the Department of the Interior, which alleged that it had been rushed through permitting. That hold was lifted following lobbying and negotiations by New York and the project developer Orsted, and the Department of the Interior never revealed why it changed its mind. When the Interior Department blocked a second Orsted project, Revolution Wind offshore of southern New England, the company took the government to court and won a ruling that let it continue construction.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357881</guid><pubDate>Mon, 22 Dec 2025 19:31:11 +0000</pubDate></item><item><title>Ultrasound Cancer Treatment: Sound Waves Fight Tumors</title><link>https://spectrum.ieee.org/ultrasound-cancer-treatment</link><description>&lt;doc fingerprint="f746e0824e2192b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ultrasound Treatment Takes on Cancer’s Toughest Tumors&lt;/head&gt;
    &lt;p&gt;HistoSonics turns its tumor-liquifying tech against pancreatic cancer&lt;/p&gt;
    &lt;p&gt;For many years, doctors and technicians who performed medical ultrasound procedures viewed bubbles with wary concern. The phenomenon of cavitation—the formation and collapse of tiny gas bubbles due to changes in pressure—was considered an undesirable and largely uncontrollable side effect. But in 2001, researchers at the University of Michigan began exploring ways to harness the phenomenon for the destruction of cancerous tumors and other problematic tissue.&lt;/p&gt;
    &lt;p&gt;The trouble was, creating and controlling cavitation generated heat, which harmed healthy tissue beyond the target area. Zhen Xu, who was working on a Ph.D. in biomedical engineering at the time, was bombarding pig heart tissue in a tank of water with ultrasound when she made a breakthrough.&lt;/p&gt;
    &lt;p&gt;The key was using extremely powerful ultrasound to produce negative pressure of more than 20 megapascals, delivered in short bursts measured in microseconds—but separated by relatively long gaps, between a millisecond and a full second long. These parameters created bubbles that quickly formed and collapsed, tearing apart nearby cells and turning the tissue into a kind of slurry, while avoiding heat buildup. The result was a form of incisionless surgery, a way to wipe out tumors without scalpels, radiation, or heat.&lt;/p&gt;
    &lt;p&gt;“The experiments worked,” says Xu, now a professor at Michigan, “but I also destroyed the ultrasound equipment that I used,” which was the most powerful available at the time. In 2009, she cofounded a company, HistoSonics, to commercialize more powerful ultrasound machines, test treatment of a variety of diseases, and make the procedure, called histotripsy, widely available.&lt;/p&gt;
    &lt;p&gt;So far, the killer app is fighting cancer. In 2023, HistoSonics’ Edison system received FDA approval for treatment of liver tumors. In 2026, clinicians will conclude a pivotal kidney cancer study and apply for regulatory approval. They’ll also launch a large-scale pivotal trial for pancreatic cancer, considered one of the deadliest forms of the disease with a five-year survival rate of just 13 percent. An effective treatment for pancreatic cancer would represent a major advance against one of the most lethal malignancies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Histotripsy’s Benefits for Cancer Treatment&lt;/head&gt;
    &lt;p&gt;HistoSonics is not the only developer of histotripsy devices or techniques, but it is first to market with a purpose-built device. “What HistoSonics has developed is a symphony of technologies, which combines physics, biology, and biomedical engineering,” says Bradford Wood, an interventional radiologist at the National Institutes of Health, who is not affiliated with the company. Its engineering effort has spanned multiple disciplines to produce robotic, computer-guided systems that turn physical forces into therapeutic effects.&lt;/p&gt;
    &lt;p&gt;Over the past decade, research has confirmed or found other benefits of histotripsy. With precise calibration, fibrous tissue—such as blood vessels—can be spared from damage even in the target zone. And while other noninvasive techniques may leave scar tissue, the liquefied debris created by histotripsy is cleared away by the body’s natural processes.&lt;/p&gt;
    &lt;p&gt;In HistoSonics’ early trials for pancreatic cancer, doctors used focused ultrasound pulses to ablate, or destroy, tumors deep within the pancreas. “It’s a great achievement for the entire field to show that it is possible to ablate pancreatic tumors and that it’s well tolerated,” says Tatiana Khokhlova, a medical ultrasound researcher at the University of Washington, in Seattle, who has worked on alternative histotripsy techniques.&lt;/p&gt;
    &lt;p&gt;Khokhlova says the key to harnessing histotripsy’s benefits “will be combining ablation of the primary tumor in the pancreas with some other therapy.” Combination treatment could fight recurrent cancer and tiny tumors that ultrasound might miss, while also tapping into a surprising benefit.&lt;/p&gt;
    &lt;p&gt;Histotripsy generally seems to stimulate an immune response, helping the body attack cancer cells that weren’t targeted directly by ultrasound. The mechanical destruction of tumors likely leaves behind recognizable traces of cancer proteins that help the immune system learn to identify and destroy similar cells elsewhere in the body, explains Wood. Researchers are now exploring ways to pair histotripsy with immunotherapy to amplify that effect.&lt;/p&gt;
    &lt;p&gt;The company’s capacity to explore the treatment‘s potential for different conditions will only improve with time, says HistoSonics CEO Mike Blue. The company has fresh resources to accelerate R&amp;amp;D: A new ownership group, which includes billionaire Jeff Bezos, acquired HistoSonics in August 2025 at a valuation of US $2.25 billion.&lt;/p&gt;
    &lt;p&gt;Engineers are already testing a new guidance system that uses a form of X-rays rather than ultrasound imaging, which should expand use cases. The R&amp;amp;D team is also developing a feedback system that analyzes echoes from the therapeutic ultrasound to detect tissue destruction and integrates that information into the live display, says Blue.&lt;/p&gt;
    &lt;p&gt;If those advances pan out, histotripsy could move well beyond the liver, kidney, and pancreas in the fight against cancer. What started as a curiosity about bubbles might soon become a new pillar of noninvasive medicine—a future in which surgeons wield not scalpels, but sound waves.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357945</guid><pubDate>Mon, 22 Dec 2025 19:37:34 +0000</pubDate></item><item><title>Vince Zampella, developer of Call of Duty and Battlefield has died</title><link>https://comicbook.com/gaming/news/vince-zampella-developer-of-call-of-duty-and-battlefield-dead-at-55/</link><description>&lt;doc fingerprint="eb907131c28675f7"&gt;
  &lt;main&gt;
    &lt;p&gt;Vince Zampella, one of the key minds behind the Call of Duty franchise and co-founder of Respawn Entertainment, has died at age 55. According to NBC4 Los Angeles, Zampella died in a car crash. The video game developer died on Sunday afternoon after the car veered off the side of the road and hit a concrete barrier. A passenger was ejected, and the driver died after being trapped in the burning vehicle. Zampella began to gain prominence in the industry as one of the key minds behind Infinity Ward, the studio that created Call of Duty. He had previously worked on Medal of Honor, but was lured over by Activision to create a rival game series.&lt;/p&gt;
    &lt;head rend="h2"&gt;Videos by ComicBook.com&lt;/head&gt;
    &lt;p&gt;Vince Zampella would go on to lead Call of Duty into what fans refer to as its “golden age” with the original Modern Warfare series. However, a dramatic clash behind the scenes with publisher Activison over bonuses, creative rights, and more resulted in Zampella and Infinity Ward co-founder Jason West leaving the studio. They went on to create Respawn Entertainment, which would develop Titanfall for EA and expand with other titles such as Star Wars Jedi: Fallen Order, Apex Legends, and much more. Following the rocky launch of Battlefield 2042 in November 2021, EA selected Vince Zampella to take over the shooter franchise.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vince Zampella Led Battlefield 6 to Success&lt;/head&gt;
    &lt;p&gt;Vince Zampella’s leadership over the franchise led to Battlefield 6 being a massive success when the series needed it most. As of December 2025, the game is outselling Call of Duty’s latest entry and is on track to be the best-selling game of the year, the first time Battlefield would ever do such a thing. Zampella was a massive creative force in the gaming industry, particularly when it came to shooters. He routinely raised the bar for the genre with Call of Duty, Titanfall, and then Battlefield, not only creating financially successful games, but massively innovative ones as well.&lt;/p&gt;
    &lt;p&gt;However, he didn’t force Respawn to stick to making just shooters. The studio’s willingness to expand into third-person action games with the Star Wars Jedi series also led to some of the best Star Wars games we’ve gotten in many years. Needless to say, Zampella was a brilliant mind in the gaming industry that found new ways to shake up formulas without compromising on commercial appeal. As of right now, it’s unclear what the future of Respawn and Battlefield looks like without Zampella at the helm.&lt;/p&gt;
    &lt;head rend="h2"&gt;Infinity Ward, EA Pay Tribute to Vince Zampella&lt;/head&gt;
    &lt;p&gt;Infinity Ward and EA both responded to Vince Zampella’s passing, offering kind words about his impact and legacy in the games industry.&lt;/p&gt;
    &lt;p&gt;EA had the following to say: “This is an unimaginable loss, and our hearts are with Vince’s family, his loved ones, and all those touched by his work. Vince’s influence on the video game industry was profound and far-reaching. A friend, colleague, leader and visionary creator, his work helped shape modern interactive entertainment and inspired millions of players and developers around the world. His legacy will continue to shape how games are made and how players connect for generations to come.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46358437</guid><pubDate>Mon, 22 Dec 2025 20:12:20 +0000</pubDate></item><item><title>In Pursuit of Clancy Sigal (2021)</title><link>https://yalereview.org/article/in-pursuit-of-clancy-sigal</link><description>&lt;doc fingerprint="fe0991b59fa302d3"&gt;
  &lt;main&gt;&lt;p&gt;The golden notebook&lt;/p&gt;&lt;head rend="h1"&gt;In Pursuit of Clancy Sigal&lt;/head&gt;&lt;head rend="h2"&gt;A writer’s radical life&lt;/head&gt;Todd Gitlin&lt;p&gt;i first encountered Clancy Sigal, in a manner of speaking, in 1963, during my last semester in college. That’s when I picked up, and devoured, Doris Lessing’s novel The Golden Notebook, which featured a character named Saul Green who was widely known to have been based on Sigal. At the time, I was a fervent and brooding left-wing activist with two years of political organizing under my belt (mostly trying to ban the bomb). So I was, unsurprisingly, enthralled by Lessing’s book, which took left-wing politics and writing seriously, as human facts, not “background.” And I was mesmerized by the broken expat Green, a Communist maudit, former union organizer, blacklisted Hollywood agent, and blocked writer, attached to left-wing ideals despite reasons not to be. I was especially moved as he and Lessing’s protagonist Anna Wulf plunge into a transformative folie de deux, “a cocoon of madness.”&lt;/p&gt;&lt;p&gt;Saul can no longer find fellowship with his old comrades, who have settled, “all married or successful and having drunken private conversations with themselves.” He won’t settle. He tosses up in London, sick and insomniac, a political refugee and scrambling neurotic. In Anna—herself a blocked novelist trying to dig out of a rubble of Communist faith, bad relationships, and war terror— he finds a kindred, equally disassembled spirit. She takes him in, impressed by “his jaunty soldier air,” coiled as he is, “his energies… absorbed in simply holding himself together,” “his cool grey eyes on guard.” But he’s also, she will conclude, a “monster,” prone to “cold moment[s] of pure hostility,” “jeering and sneering” at her “middle-class” origins. He lectures her, and she likes being lectured to. “Saul,” she says, “we’re very bad for each other.”&lt;/p&gt;&lt;p&gt;History having abandoned them, they lurch into each other’s arms as lovers, accusers, and confessors. She, having compartmentalized her writing into topical segments, hungers for wholeness; he, stuck in a moment when “some kind of guts have gone out of people,” would “give anything to go back to when I was in the gang of idealist kids on the street corner, believing we could change everything…the only time in my life I’ve been happy.” They circle each other like ravenous carnivores biting chunks out of each other’s flesh, their cruelties spurring understandings that feel like misunderstandings.&lt;/p&gt;&lt;p&gt;I recognized Lessing’s characters as weathered, burned-out forebears of my New Left crowd. “The truth for our time was war, the immanence of war,” writes Anna Wulf. “War was working in us all, towards fruition.” This spoke to us, given that the United States and the Soviet Union had just careened into, and almost not out of, the Cuban Missile Crisis. And my circle devoured The Golden Notebook not least because Lessing took women’s passions and quandaries seriously. Lessing named their disquiet, their longing to be “free women.” I lent my copy to Casey Hayden, who had recently been evicted from her marriage to Tom Hayden, then the president of Students for a Democratic Society (SDS).&lt;/p&gt;&lt;p&gt;Politics was not all that drew me to Saul and Anna. Lessing told me what I was missing in my own life, for I often felt as if I had been dropped onto the earth without an instruction manual, lacking even a vocabulary for what human beings felt about, and wanted from, each other. I admired Anna’s courageous struggle to unite her fragments, and saw myself in Saul Green’s lurching helplessness. He was some fifteen years older than I—the right age for an archetype. He embodied both the swagger and the fragility of the masculine mystique I aspired to, à la Humphrey Bogart in Casablanca. Like Saul, I fancied myself summoned by history with a capital H, wanting my bewilderments and failings to add up to something significant.&lt;/p&gt;&lt;p&gt;going away&lt;lb/&gt;six months after reading The Golden Notebook, I picked up a copy of Going Away, Sigal’s second book and his major opus. By then, I had been elected the third president of SDS. I was fired up by the updraft of the civil rights movement and John Kennedy’s moves toward détente. In my bookish way I was seeking a “usable past” while struggling with an uneasy relationship between witnessing history and participating in it. In some way, I intuited that the dilemmas we were facing as young radicals were already built into the situation of being left wing in a country that was not. And I sensed that these were dilemmas about which Clancy Sigal would have something important to say.&lt;/p&gt;&lt;p&gt;Going Away is, in a way, the story of what happened to Saul Green before he met Anna Wulf. Lyrical and intellectually serious at once, it maps the political wasteland left by the death of the Old Left. At twenty-nine, the narrator is the son of union organizers (his mother a socialist, his absent father a Communist). It is October 1956. He drives a big, borrowed red-and-white De Soto convertible from L.A. eastward, looking up old buddies and recalling adventures whose meaning has been drained away by America’s stupor.&lt;/p&gt;&lt;p&gt;Most of the old militancy has expired; most of his pals are in retreat; a few hold fast to Stalinism for dear life. Mostly he finds embers. In Wyoming, a onetime union leader tells him, “It’s not a radical union any more.… All they talk about is sex, baseball, cars and the lousy Company.” Another: “The guys are tired. They’ve a right to be.… They want a rest. They don’t want to strike, they don’t want any trouble. They want to take long uninterrupted fishing trips.” “All the people I talked to felt ‘out of it,’” Sigal writes, “believing that they could exercise no real influence over the important decisions in their lives, they were now busily brewing up a blend of wisecracking apathy.” Drive-by sex substitutes for the revolution.&lt;/p&gt;&lt;p&gt;Stopping off in Reno, Sigal watches people on street corners and motel roofs staring toward the government’s desert test site. “What’s going on?” he asks men in Stetson hats. “Nuclear device,” one of them says. “That was the phrase one of them used, nuclear device. I asked him if he meant an atom bomb, and he looked at me. I was a square.” These men, stunned into euphemism, talk kiloton ranges, weather, altitude. “I asked again what time the atomic bomb would be exploded. They stared at me again. I had done the square thing. I excused myself and called it a nuclear device.” One of the Stetsons “put his hand on my shoulder and said, ‘Son, there ain’t no call for you getting sarcastic. That’s the only thing that stands between us and the Russians.’”&lt;/p&gt;&lt;p&gt;The bomb, 380 miles away, lights up the dawning sky. “An intake of breath swept the crowds… as the eyes caught the white glare that spread over the whole dawning sky, a sharp, slow splurge of light that brought forth appreciative Ah’s from the crowd. One of the Stetsons said, ‘God, that’s beautiful.’” Later, Sigal wanders “the bleak, blinking streets thinking about Hungary,” where people are thrillingly, if futilely, rising up against the Stalinist regime.&lt;/p&gt;&lt;p&gt;Everywhere, America looks “rapaciously, lifelessly ‘modern.’” As he drives, revolted by billboards, charged up on whiskey and Dexedrine, a Greek chorus of radio bulletins spits out news of Hungary—and soon enough, dreadfully, of the Soviet onslaught that crushes what remains of his old-time religion. He is long gone from the Communist Party, but some residue of nostalgia binds him to the comrades with whom he had shared illusions. “It has been written that one cannot have Socialism. One is a socialist,” he writes. “It is true.” For no apparent reason, he breaks out crying.&lt;/p&gt;&lt;p&gt;For him, communism was insurgency and solidarity—a way of life and a morality, not an economic or political arrangement.&lt;/p&gt;&lt;p&gt;What did I make of this at the time? I loved the narrator’s loyalties, his let-it-all-hang-out grousing, his penchant for theories of what killed America, his uncertainty about those theories, his honesty about his flaws and flops (“I have never been afraid of self-pity”). I loved his belief that writing matters, that genres are cages, that writing is an art of fluidity, not boxes. I loved the long sections he devoted to political sagas—the battle between Communists and socialists over control of the United Auto Workers; running the mimeograph machine for a proletarian poet during a strike in North Carolina—and his lust for solidarity. Above all, I was touched by his urgent need to know if it was “possible to have a small circle of friends, friends of grace and purpose, not incestuously, but on a basis of mutual respect, work, and a kind of humorous, informal dignity in the United States.”&lt;/p&gt;&lt;p&gt;The narrator was like Woody Guthrie with a college degree and a penchant for political argument. As John Leonard would later write, “It was as if On the Road had been written by somebody with brains.” I didn’t mind much that the book was overlong, more than five hundred pages of wound-licking and self-purging. A rough-hewn cautionary tale felt like a fit tombstone for the Fifties’ fraudulent sanities and grave defeats. It comes as no surprise when Clancy writes, “This is the chronicle of how I started to go mad.” The last bulletin he hears, on shipboard, about to depart for Europe, goes, “This is Budapest. Budapest Radio. Budapest Radio. Help us. Help. Help. Help.”&lt;/p&gt;&lt;p&gt;I turned the final page and lay in bed thinking, This will never happen to me. My movement was different. Stalin and McCarthy were dead. The New Left was independent, unillusioned, rambunctious, joyful. Lucky me, I had the sixties to look forward to.&lt;/p&gt;&lt;p&gt;abandoned by history&lt;lb/&gt;almost a decade later, I reread Going Away, and this time when I was done, I lay in bed thinking, So it did happen to me.&lt;/p&gt;&lt;p&gt;I had spent the years in between as part of the New Left—“the movement.” I had been president of SDS. The movement had been my way of life, the country I lived in. It gave me a calendar of events, reference points, vocabulary, music, jokes, lovers. It made the world cohere, or seem to. But by the early seventies, the Vietnam War was still bleeding on, and Richard Nixon reigned, and the movement had been seized by lunatic fights over who should lead an imaginary revolution. The most acute and ferocious hell wasn’t right-wingers or liberals: it was other left-wing factions. Even as we were being denounced and surveilled by government agencies, those of us who were independent-minded were also under fire from Leninist factions who labeled us “movement creeps” and “revisionists.” Movement solidarity had morphed into a collective hallucination. I was going away, all right—not only from America but from the movement that was going away from America. Or rather, it was going away from me. Going, going, gone.&lt;/p&gt;&lt;p&gt;Marooned, disbelieving, burned out, for a time I contemplated a new political start—a new manifesto, something—with movement buddies equally estranged from the clotted rhetoric and revolutionary fantasies that engulfed the sects of the late, no longer so New Left. But the world had found us indigestible and spat us out. Our intellectual ground had veered from early-sixties participatory democracy and brotherhood ideals to a haze of outrage, indebted to the Herbert Marcuse of the Great Refusal, the Guevarism of Régis Debray, the Maoism of the Little Red Book, and the R. D. Laing of the divided self. We were unstable selves floating free. Even our successes seemed perishable. What was it to be a movement writer without a sensible movement?&lt;/p&gt;&lt;p&gt;To get some distance, I took some time to write a long analytical essay. That’s what a serious left-wing intellectual did. But the more clearly I saw the origins of our maladies, the less avoidable they seemed. I smoked dope and had panic attacks. I wrote an epic poem. I scrambled to figure out what had happened to me.&lt;/p&gt;&lt;p&gt;It occurred to me, then, to write my way through the fog of failed revolution by trying my hand instead at a memoir-cum-fiction. I filled pages for a few days, feeling frightened, wondering if this was what it was like to go crazy. And then, staring at my barely begun book, the thought came that I should reach out to Clancy Sigal.&lt;/p&gt;&lt;p&gt;I figured that if anyone could understand my life-problem as well as my writerly quandary, he could. I was, after all, the age he had been when he shipped out of America. He had known political defeat and extracted honor. So, in anguish, I wrote him a letter introducing myself. I sketched my situation, paid tribute to Going Away, said I was trying to write something similar, and confessed my fear that writing it would make me lose my mind. I asked him what writing his book had done for him, and mailed my letter from San Francisco in care of his publisher, Houghton Mifflin, in Boston.&lt;/p&gt;&lt;p&gt;Ten days later, I had back an air-letter from London. Houghton Mifflin had done the least every publisher needs to do—put a reader in touch with a writer. Clancy wrote that he knew of me through Studs Terkel, who had flatteringly blurbed a book I’d coauthored called Uptown: Poor Whites in Chicago. He told me that he, too, had found himself blocked from writing his big book until he’d met a third-generation Yorkshire miner, a Communist and a writer to boot, who took him down in the pits and let him hang around. The resulting short, sweet book of reportage, Weekend in Dinlock, was Sigal’s first book, published before Going Away.&lt;/p&gt;&lt;p&gt;He said he finished Going Away and then went crazy. He said this was obviously the subject he was writing about now. He was still making himself up as he went along, confessing, at one point, to a retrograde desire to own a color television. He was avuncular. He said he’d read a review of a just-published volume of Chekhov’s letters, said that they sounded like good advice for a young writer. (I rushed out to buy a copy.) He typed out every square inch of the air letter. I had written to the right guy.&lt;/p&gt;&lt;p&gt;take back the night&lt;lb/&gt;sigal wasn’t kidding about going crazy. When I wrote him, he had been suffering from “nameless, numbing panics” and frolicking with fellow-suffering freaks in the London anti-psychiatry world of the Scottish psychoanalyst R. D. Laing. Laing believed that schizophrenia was the goal of psychiatry, and told Sigal to stop “bleatin’ and moanin’ like your standard Jewish neurotic” when “ye’ve the makin’s of a first-class schizophrenic”—a high compliment from the Lenin of the madness movement. A few years later, Sigal would publish a roman à clef about his time with Laing called Zone of the Interior, his first book since Going Away. In the preface, Sigal asks himself, “Why, with so much to politically rethink in the Sixties… did I seek refuge in near-Fascist irrationalism?” A good question that he does not answer, though strong clues will be found in the title and substance of his later roman à clef, The Secret Defector.&lt;/p&gt;&lt;p&gt;I wrote Clancy again after Zone came out. (It was published only in America, because British libel law, which favors the accuser, meant it would have been legally perilous to publish the book in London while Laing was alive.) “Sometimes,” I wrote, “I felt the book could be more interior.” In truth, it was more than sometimes. Zone was Fellini shot from a balloon—a bumpy burlesque, sometimes funny but undeveloped and anticlimactic. Going Away had its longueurs, but they all circled back to the unmistakable center of the novel, the brooding, broken adventurer-narrator who stumbles from dizzy spell to hectic sex as he tries to get down deep with his onetime comrades. Sigal’s Zone alter ego, by contrast, rarely makes solid connections with his would-be transcendent mates. His ironic distance flattens his prose. About his own forays into ecstatic LSD-boosted effusions, laughing for hours, suddenly aware of “the pointlessness of all human activity,” he sounds more sardonic than transcendent. “Interior” was not Clancy’s strong zone.&lt;/p&gt;&lt;p&gt;Later that year, I made my first trip to Britain, and my wife and I spent an evening with Clancy and his wife, Margaret Walters, a feminist art historian. Their walk-up on Wigmore Street was diminutive and unprepossessing. A gas heater hung over the kitchen sink. They had spent the day at the Notting Hill Carnival, where Margaret’s purse had been stolen. I was impressed by her aplomb and Clancy’s inquisitive but warm manner, which was not the aggressive style of Saul Green. He had hooded eyes and a strong chin. He was pleasant, cocky, impassioned, and wary all at once. He studied me, and my wife, closely. We talked about the general rottenness of politics. We hit it off. He graciously didn’t ask about my unwritten book.&lt;/p&gt;&lt;p&gt;We stayed in touch from then on. In 1978, he visited me in San Francisco, and we strolled around in exuberance during the first “Take Back the Night” march, chanting through the streets of North Beach, “Women, unite! Take back the night!” Nothing ironic about it—we were, both of us, fervent feminists. He would write in his later memoir The London Lover how impressed he was with the new women, “basking in their new power…tough, difficult, and like my mum, sassy.…Such is the moth’s flame that I’m enchanted—in the original sense—by their vivacity and sheer joie de vivre.” “Take Back the Night” was sheer exultation. At one point we shouted “Join us” to two women watching us from an upstairs apartment window. “Join us,” they shouted back, grinning and beckoning. Those were the days.&lt;/p&gt;&lt;p&gt;action as tranquilizer&lt;lb/&gt;during the years that followed, Sigal wrote fiercely—no more writer’s block that I could see. He wrote literally thousands of articles: journalism, book and movie reviews, BBC scripts. He wrote about political and sociological travels, à la George Orwell; about stolid Tories and innocent campaigners for nuclear disarmament. He interviewed Samuel Beckett. In 1984 he covered the Los Angeles Olympics, of all things, for The Observer, and not long after that he decided it was time to come back home. He applied for a professorship in the Department of Journalism at the University of Southern California. His cause there was taken up vigorously by A. J. Langguth, the fine former New York Times Saigon bureau chief. I was flattered to be asked to write a recommendation for Clancy, and evidently it did not torpedo his prospects, for he got the job and proceeded—devotedly—to teach long-form journalism there for fifteen years.&lt;/p&gt;&lt;p&gt;In the fifties, even as some of his college friends headed to graduate school, Clancy had steered clear of the academy. The intellectual’s first role, he wrote, was “to stand and shout bloody murder.” The second was “not to break into any kind of Establishment.” Playing at word games or concept refinement was not his idea of intellectual life. “I have a big mouth but a torpid mind,” he wrote. So it would have amazed the young Clancy to think that he would end up as a tenured professor, and a dedicated teacher to boot.&lt;/p&gt;&lt;p&gt;Even so, he was suspicious of corrals, even comfortable ones, and his style was never dented by the academy. When I confessed my own resolve to hold on to my writerly missions even as I climbed the ladder of tenure, he was quick with advice: “Write one for them and one for yourself.” (I liked the sound of it, though after I got tenure, I stopped writing the one for them.) He never wrote the one for them. Novel-memoirs, commercial screenplays (in collaboration with his second wife, Janice Tidwell), and journalism were his métiers. Professional intellectuals were not high on his admiration list.&lt;/p&gt;&lt;p&gt;To get some distance, I took some time to write a long analytical essay. That’s what a serious left-wing intellectual did.&lt;/p&gt;&lt;p&gt;In part, that was because Clancy loved action. In his 1992 roman à clef about his time in London, The Secret Defector (his first book after Zone of the Interior), the Doris Lessing character, Rose O’Malley, charges the narrator with “never knowing if you’re Vladimir Ilyich Lenin or Errol Flynn.” “Hold on,” Clancy’s character responds. “I knew who I was. Vladimir Flynn.”&lt;/p&gt;&lt;p&gt;Clancy was not the only radical or would-be revolutionary to wonder whether he was in a movie. Still, he did not posture. During the civil rights movement, he went back to the United States to hang out with the radicals of the Student Nonviolent Coordinating Committee and help register Black voters in the South. And during the Vietnam War he was a stationmaster in a network smuggling American AWOLs and deserters to Sweden, taking plenty of chances.&lt;/p&gt;&lt;p&gt;His character in The Secret Defector goes on to tell Rose that “who I didn’t want to be, what I was in danger of becoming, was that most terrifying caricature, a political adventurer… living on air, terribly magnetized by seriousness in others while compulsively inconstant themselves.” If that is who Clancy was, he came by it honestly. He spent his childhood traveling around the country with his turbulent union-organizer mother, Jennie Persily, whom he described lovingly in his next, best-made memoir, A Woman of Uncertain Character: The Amorous and Radical Adventures of My Mother Jennie (Who Always Wanted to Be a Respectable Jewish Mom), by Her Bastard Son. Given that Jennie brought Clancy to union rallies, picket lines, and battles with strikebreaking cops, action was, you might say, his mother’s milk.&lt;/p&gt;&lt;p&gt;“Fear is my tranquilizer,” he would write later. “Being at the centre of the action is the only thing that helps to reduce my state of perpetual terror.” During risky exploits he remembers becoming “euphoric with fear.” “Why,” he asked himself, “do I feel so at ease in an out-of-control crowd?” Perhaps for the same reason he felt at ease, for a while, in an out-of-control love affair.&lt;/p&gt;&lt;p&gt;a vexatious angel&lt;lb/&gt;over the years, I suppressed the occasional impulse to broach with Clancy the subject of Saul Green. I didn’t want to pry, but I wondered, What did it mean to Clancy to be outed, so to speak, by his onetime lover? I assumed that he resented what Lessing did with him. But it seems to have been more complicated than that. In a fascinating monograph, Literary Half-Lives: Doris Lessing, Clancy Sigal, and Roman à Clef, Roberta Rubenstein reports that Clancy recorded in his diary in July 1959 how he felt on learning that Lessing had begun writing a play about their affair: “I was, at first, unconcerned, and then, as I understood, furious. I issued a veiled threat that to continue to write such a play might cleave us inseparably, but I knew she would write it. She would never, for some very good reasons, sacrifice her personal life to art.” He must have meant the opposite.&lt;/p&gt;&lt;p&gt;The diary entry he wrote about discovering that Lessing was lifting his words verbatim is electric. He has just found out that his mother has died. When the call comes from the States, he is up north with “some of the best friends I’d made in England…having one of those intense political meetings charting the future of the known universe. I said, ‘My ma just died.’ No one got up to put an arm around me or to say how sorry they were. Mostly they looked acutely embarrassed as if I’d farted in public or mistranslated Sartre’s Being and Nothingness.”&lt;/p&gt;&lt;p&gt;“Armored against emotion,” he returns to London. Lessing isn’t home. He prowls around and stumbles upon her play manuscript. “I let my eye idle over a few lines,” he records in his diary. “It hit me between the eyes. The play she was writing, in fact had completed, was about our relationship. But those lines of dialogue! They were exactly what I had said to her, in my worst, most intimate moments of a severe nervous breakdown in the year previous.… Never, I felt, had I been so profoundly betrayed.… I wrote a letter to the woman, an angry letter, [to] which she replied in like mind, reminding me how much she had done for me. She was perfectly correct in this.” Lessing has commandeered him, hexed him, body-snatched him.&lt;/p&gt;&lt;p&gt;Lessing went on to draw on Clancy’s diaries for The Golden Notebook. He knew she was doing it because he had taken to tying a thin black thread between his bureau drawers so as to tell whether she had peeked, and he half-boasted, half-chortled about having written passages into his diary to nudge her novel this way or that. He would discreetly check her manuscript for signs of her piracy.&lt;/p&gt;&lt;p&gt;So he would pass through literary life preceded by her depiction of him as Saul Green, as if the far more famous Lessing was always making rabbit ears with her fingers behind his head. And not only his literary life. He suspects that when people meet him they wonder if the Clancy Sigal they meet masks the fictional character. But notoriety works in complicated ways. Beyond feeling diminished, perhaps he is also weirdly proud to have been “immortalized,” even perversely, in a literary classic. Over the years, Clancy slams Lessing for invading his diaries. “I thought of slugging my ex-girlfriend (or at least killing her),” he writes. But then he adds the following sentences, “She had immortalized me as a neurotic schmegege—and who wants to be remembered like that? But living well, not suing for libel, is the best revenge. Second best is answering back in kind.”&lt;/p&gt;&lt;p&gt;They were fiercely coupled, twisted together in mutual aid and resentment. Lessing’s love merged with her jealousy. Clancy’s shock at her literary appropriations merged with his rivalry. (Before The Golden Notebook, she was already a published and well-regarded author; he, seven years younger, was not.) She would say he tormented her, and she gave as good as she got. In a 1992 article, Clancy tells this story:&lt;/p&gt;&lt;p&gt;R. D. Laing, a great fan of Mrs. Lessing’s, and I used to get roaring drunk together and, when totally blotto, exchange profundities about the schizophrenic implications of a divided self being further split by the act of being written about. “That woman stealing your soul was the luckiest thing that ever happened to you,” he would insist. “She’s emptied you for the Great Task ahead.” Which was, more or less, the “schizoid voyage” that was our politics then. Sometimes Mrs. Lessing seemed to agree. Once, at a party… she put her arms around me and boasted to the guests, “I invented Clancy.” “No, you didn’t,” I said stubbornly, “my mother, Jennie, did that by giving birth to me.” “She had,” Mrs. Lessing said, dismissing my protest, “the easy part.”&lt;/p&gt;&lt;p&gt;Clancy and Doris stayed in touch over the years. He was like that with his most serious lovers—playing a long game. Love, respect, and companionship were points on the same curve. After she died in 2013, I wrote him about how reading The Golden Notebook at age twenty “had some huge &amp;amp; eerie effect on me…told me that the sort of life I was embarking on might not have consequence but it had gravity and honor, and not only honor but a kind of density.… Unlike the grander ideological novels I’d been reading, The Golden Notebook was full of the dirt &amp;amp; mess &amp;amp; craziness of everyday life, the back-&amp;amp;-forth of working out politics &amp;amp; lostness &amp;amp; honor on the run, so to speak; &amp;amp; the possibility of scrambling toward a way of writing that would convey the mess &amp;amp; yet the stakes of the struggle &amp;amp; its worth.” And I asked whether, even if he resented her “portrait” of him, he realized it had made him “an inspirational figure not only for me but for my crowd, &amp;amp; could you see how she had honored you? You were her vexatious angel.”&lt;/p&gt;&lt;p&gt;keep the flame lit&lt;lb/&gt;clancy never stopped writing, churning out a succession of romans à clef and, more vividly, memoirs of chunks of his life (Black Sunset about Hollywood, and the aforementioned The London Lover and A Woman of Uncertain Character), as well as screenplays—In Love and War, based on wartime reminiscences of his beloved Ernest Hemingway, and (in collaboration with Janice) Frida, with the redoubtable Kahlo played by Salma Hayek and featuring Geoffrey Rush as Trotsky and Antonio Banderas as David Alfaro Siqueiros. (I asked him why the movie was lacking what would surely have been a bang-up true-life scene in which Siqueiros, the down-the-line Stalinist and brilliant muralist, did his damnedest to shoot the exiled Trotsky, leaving holes in the wall of Trotsky’s house but none in Trotsky himself. Clancy told me that he and Janice did write that scene but it didn’t make it into the movie.)&lt;/p&gt;&lt;p&gt;His and Janice’s screenplay about the Sartre-Beauvoir-Nelson Algren triangle (Clancy and Algren were Chicago buddies) never got produced, though Annette Bening had signed on. All the while, he kept scribbling for the British press and wrote political op-eds, many of them published on the far-left Counterpunch website, as well as longer pieces of reportage that didn’t always find venues. One that saw the light of day only in a much truncated version was a remarkable account of his visit to Nixon henchman H. R. Haldeman in the federal prison in Lompoc, California, where Haldeman was serving a shortened sentence of eighteen months for Watergate crimes. Haldeman and his partner-in-crime John Ehrlichman had been UCLA contemporaries of Clancy’s in the early fifties. The future Nixon co-conspirators were already right-wingers when Sigal was the Big Commie on Campus and the editor of the school paper; they tried to get the student government to take the paper away from him. “It was never personal,” Haldeman told him at Lompoc.&lt;/p&gt;&lt;p&gt;Even our successes seemed perishable. What was it to be a movement writer without a sensible movement?&lt;/p&gt;&lt;p&gt;Clancy’s late style is jagged, suited to delicate evasion, full of meaningful pauses, his writing stripped down, sometimes to aphorism. Cinematic, in fact. This was not strictly or even primarily an aesthetic choice. (It was largely, according to his widow, because during his later years he was in tremendous pain from an agitated sciatic nerve, and so had to write standing up.) Quoting Elmore Leonard (“Try to leave out the part that readers tend to skip”), in his late books he has traded in the longueurs of Going Away and Zone of the Interior for staccato suggestions and hints toward what need not be dwelt upon or cannot even be hazarded—or perhaps is best left to dangle in ambiguity.&lt;/p&gt;&lt;p&gt;Clancy’s radicalism was more consistent than his novelistic ambition. Through it all, until he died at ninety in 2017, he tried to keep the eternal flame lit. Stay in fighting trim. Choose sides. Make the most of defeat. He had no illusions about Stalinism. (In The Secret Defector, he spoke of “my dream of a non-Communist independent left.”) Nor did he have any love for “armed struggle” terrorism. In 1982 he had attended “a Sunday concert in Regents Park when the military band was blown to pieces by IRA shrapnel, and a few weeks later a flying chunk of iron from a Piccadilly postbox with an IRA letter bomb inside had almost decapitated me.” But when I told him once that I was not just non-Communist but anti-Communist, he was genuinely shocked. It wasn’t that he disagreed with me on the merits. It was a matter of primal identity. For him, communism was insurgency and solidarity—a way of life and a morality, not an economic or political arrangement. “It was the essence of what we believed that Communism was men standing up on their own two feet and, for the first time in history, ordering their lives in imperfect consultation but in perfect awareness.”&lt;/p&gt;&lt;p&gt;To him, anticommunism was once and for all desertion. It was McCarthy, John Foster Dulles, the domino theory. It was the stupidities of the Cold War. It was the Bomb. I argued that anticommunism was more general but also simpler than that—it was the conviction that communism was a rotten political system, even if scoundrels thought so too. I thought it morally necessary as well as tactically wise to clarify that there was more than one kind of monstrosity in the world. We agreed to disagree.&lt;/p&gt;&lt;p&gt;We also agreed to disagree about Democratic presidents. Clancy was ever on the lookout for signs of submission. In the fifties, he found the speeches of Adlai Stevenson “mealymouthed and disingenuous in the extreme, even cowardly.” He thought liberals as guilty as Communists of “the abdication of thought to prayer.” During his later years, he grew, if anything, more vehement, riveted to a single metaphor: bad marriage. In 2006, he emailed me: “More and more I think we should get a divorce from the Dems so we can fall in love all over again.” In 2010, he accused “the American left (what there is of it)…[of ] trail[ing] poodle-like after Barack Obama… and isn’t it nice for a change to have a president who can parse a complicated sentence?… One scary look at Obama’s yowling enemies—racist and crazy about Palin—… was enough to send us whimpering back to our kennels.”&lt;/p&gt;&lt;p&gt;His dogma was not mine. I tried to convince him that given America’s two-party presidential system and first-past-the-post elections, supporting the best available Democrats was not marriage, bad or otherwise, but hygiene, like brushing your teeth. “We live in the country we live in, not the one we wish we lived in,” I preached. He was not swayed. I had read Clancy rightly when, decades earlier, I asked his advice about how to make sense of the overpowering experience of thinking big and falling short. I had grasped his feeling for the defeated, the sobriety of his intelligent grief, his loyalty to the proles of his youth, his understanding that self-pity was shallow and that recovery was more than a matter of will—his insistence on being earnest. He had no time for the left-wing glitterati who soaked up much more attention. He knew that to move humanity toward wholeness, to respect the effort wrapped in the tragedy, and the tragedy in the effort, you had to have been immersed. To be indefatigable, you had to have lost everything but your honor. You could only burn out if you had burned.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46358538</guid><pubDate>Mon, 22 Dec 2025 20:21:12 +0000</pubDate></item><item><title>It's Always TCP_NODELAY</title><link>https://brooker.co.za/blog/2024/05/09/nagle.html</link><description>&lt;doc fingerprint="f78cd1300c0d0ff1"&gt;
  &lt;main&gt;&lt;p&gt;The first thing I check when debugging latency issues in distributed systems is whether TCP_NODELAY is enabled. And itâs not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.&lt;/p&gt;&lt;p&gt;First, letâs be clear about what weâre talking about. Thereâs no better source than John Nagleâs RFC896 from 19841. First, the problem statement:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;There is a special problem associated with small packets. When TCP is used for the transmission of single-character messages originating at a keyboard, the typical result is that 41 byte packets (one byte of data, 40 bytes of header) are transmitted for each byte of useful data. This 4000% overhead is annoying but tolerable on lightly loaded networks.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many &lt;code&gt;write&lt;/code&gt; calls. Nagleâs proposal for fixing this was simple and smart:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A simple and elegant solution has been discovered.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;The solution is to inhibit the sending of new TCP segments when new outgoing data arrives from the user if any previously transmitted data on the connection remains unacknowledged.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;When many people talk about Nagleâs algorithm, they talk about timers, but RFC896 doesnât use any kind of timer other than the round-trip time on the network.&lt;/p&gt;&lt;p&gt;Nagleâs Algorithm and Delayed Acks&lt;/p&gt;&lt;p&gt;Nagleâs nice, clean, proposal interacted poorly with another TCP feature: delayed &lt;code&gt;ACK&lt;/code&gt;. The idea behind delayed &lt;code&gt;ACK&lt;/code&gt; is to delay sending the acknowledgement of a packet at least until thereâs some data to send back (e.g. a &lt;code&gt;telnet&lt;/code&gt; session echoing back the userâs typing), or until a timer expires. RFC813 from 1982 is that first that seems to propose delaying &lt;code&gt;ACKs&lt;/code&gt;:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The receiver of data will refrain from sending an acknowledgement under certain circumstances, in which case it must set a timer which will cause the acknowledgement to be sent later. However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer interrupt.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;which is then formalized further in RFC1122 from 1989. The interaction between these two features causes a problem: Nagleâs algorithm is blocking sending more data until an &lt;code&gt;ACK&lt;/code&gt; is received, but delayed ack is delaying that &lt;code&gt;ack&lt;/code&gt; until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.&lt;/p&gt;&lt;p&gt;This is a point Nagle has made himself several times. For example in this Hacker News comment:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;That still irks me. The real problem is not tinygram prevention. Itâs ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.&lt;/p&gt;&lt;p&gt;Is Nagle blameless?&lt;/p&gt;&lt;p&gt;Unfortunately, itâs not just delayed ACK2. Even without delayed ack and that stupid fixed timer, the behavior of Nagleâs algorithm probably isnât what we want in distributed systems. A single in-datacenter RTT is typically around 500Î¼s, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isnât clearly a win.&lt;/p&gt;&lt;p&gt;To make a clearer case, letâs turn back to the justification behind Nagleâs algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems donât. Partially thatâs because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.&lt;/p&gt;&lt;p&gt;The core concern of not sending tiny messages is still a very real one, but weâve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isnât going to be very efficient, no matter what Nagleâs algorithm does.&lt;/p&gt;&lt;p&gt;Is Nagle needed?&lt;/p&gt;&lt;p&gt;First, the uncontroversial take: if youâre building a latency-sensitive distributed system running on modern datacenter-class hardware, enable &lt;code&gt;TCP_NODELAY&lt;/code&gt; (disable Nagleâs algorithm) without worries. You donât need to feel bad. Itâs not a sin. Itâs OK. Just go ahead.&lt;/p&gt;&lt;p&gt;More controversially, I suspect that Nagleâs algorithm just isnât needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, &lt;code&gt;TCP_NODELAY&lt;/code&gt; should be the default. Thatâs going to make some â&lt;code&gt;write&lt;/code&gt; every byteâ code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.&lt;/p&gt;&lt;p&gt;Footnotes&lt;/p&gt;&lt;code&gt;TCP_QUICKACK&lt;/code&gt;. I donât tend to reach for it for a few reasons, including lack of portability, and weird semantics (seriously, read the man page). The bigger problem is that &lt;code&gt;TCP_QUICKACK&lt;/code&gt; doesnât fix the fundamental problem of the kernel hanging on to data longer than my program wants it to. When I say &lt;code&gt;write()&lt;/code&gt;, I mean &lt;code&gt;write()&lt;/code&gt;.&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46359120</guid><pubDate>Mon, 22 Dec 2025 21:09:59 +0000</pubDate></item><item><title>Tc – Theodore Calvin's language-agnostic testing framework</title><link>https://github.com/ahoward/tc</link><description>&lt;doc fingerprint="d3d82e1fe795beff"&gt;
  &lt;main&gt;
    &lt;p&gt;language-agnostic testing for unix hackers&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; theodore "tc" calvin - helicopter pilot, testing framework namesake, legend &lt;/p&gt;
    &lt;code&gt;|=o=o=o=o=o=o=o=o=o=o=o=|      tc v1.0.0 - island hopper
           |                   testing any language, anywhere
       ___/ \___      (o)       🚁 fly safe, test well
     (( tc      ))======\
       \_______/        (o)
         ^   ^
      ^-----------^
&lt;/code&gt;
    &lt;p&gt;What: Language-agnostic test framework. Write tests once, run against any language (bash, python, rust, go, whatever).&lt;/p&gt;
    &lt;p&gt;How: Tests are directories. Your code reads &lt;code&gt;input.json&lt;/code&gt; from stdin, writes &lt;code&gt;expected.json&lt;/code&gt; to stdout. That's it.&lt;/p&gt;
    &lt;p&gt;Get Started:&lt;/p&gt;
    &lt;code&gt;# clone and install
git clone https://github.com/ahoward/tc.git
cd tc

# IMPORTANT: Add to PATH (avoids conflict with Unix traffic control command)
export PATH="$PWD/tc/bin:$PATH"

# verify
tc --version

# try the hello-world example
tc examples/hello-world

# create your first test
tc new tests/my-feature&lt;/code&gt;
    &lt;p&gt;That's it. See full docs for advanced features.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;tc&lt;/code&gt; conflicts with the Unix traffic control command. You MUST add this project's &lt;code&gt;tc&lt;/code&gt; to your PATH.&lt;/p&gt;
    &lt;code&gt;# Add to PATH for current session
export PATH="$PWD/tc/bin:$PATH"

# Add to shell config for persistence (optional)
echo 'export PATH="$PWD/tc/bin:$PATH"' &amp;gt;&amp;gt; ~/.bashrc  # or ~/.zshrc&lt;/code&gt;
    &lt;p&gt;Verify:&lt;/p&gt;
    &lt;code&gt;which tc        # should show: ./tc/bin/tc (NOT /usr/sbin/tc)
tc --version    # should show: tc v1.0.0 - island hopper&lt;/code&gt;
    &lt;p&gt;tc is a dead-simple testing framework that lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;test any language with the same test suite&lt;/item&gt;
      &lt;item&gt;organize tests as directories with json input/output&lt;/item&gt;
      &lt;item&gt;run tests with zero dependencies (just jq)&lt;/item&gt;
      &lt;item&gt;port code between languages without rewriting tests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;simple • portable • language-agnostic • unix • spec-driven&lt;/p&gt;
    &lt;p&gt;🤖 In the AI age, specifications and tests are permanent while implementations are disposable.&lt;/p&gt;
    &lt;p&gt;Tests are the spec. Code is a build artifact. Port languages freely, keep tests forever.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;projects/&lt;/code&gt; and &lt;code&gt;examples/multi-lang-dao/&lt;/code&gt; for a working example of identical DAO interfaces in 5 languages (Ruby, Go, Python, JavaScript, Rust) all passing the same test suite.&lt;/p&gt;
    &lt;p&gt;Vision: Disposable applications. Swap languages freely, keep tests forever.&lt;/p&gt;
    &lt;p&gt;See docs/THEORY.md for the full system adapter pattern vision.&lt;/p&gt;
    &lt;code&gt;# test execution
tc                          # run all tests (KISS!)
tc &amp;lt;suite-path&amp;gt;             # run single test suite
tc &amp;lt;path&amp;gt; --all             # run all suites in directory tree
tc &amp;lt;path&amp;gt; --tags TAG        # run suites matching tag
tc &amp;lt;path&amp;gt; --parallel        # run all suites in parallel (auto CPU detection)
tc &amp;lt;path&amp;gt; --parallel N      # run with N parallel workers

# test generation
tc new &amp;lt;test-path&amp;gt;          # generate new test suite
tc init [directory]         # initialize test directory with README

# discovery &amp;amp; metadata
tc list [path]              # list all test suites with metadata
tc tags [path]              # show all available tags
tc explain &amp;lt;suite&amp;gt;          # explain what a test suite does

# info
tc --version                # show version
tc --help                   # show help&lt;/code&gt;
    &lt;p&gt;TTY mode (terminal): Clean single-line status with 🚁 spinner, fail-fast behavior Non-TTY mode (CI/CD): Traditional verbose output with full logs Override: &lt;code&gt;TC_FANCY_OUTPUT=true/false&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;→ full docs | → tc new guide | → system adapter theory (WIP)&lt;/p&gt;
    &lt;code&gt;my-feature/
├── run                    # executable: reads input.json, writes json to stdout
└── data/
    └── scenario-1/
        ├── input.json     # test input
        └── expected.json  # expected output
&lt;/code&gt;
    &lt;code&gt;tc my-feature  # ✓ pass or ✗ fail&lt;/code&gt;
    &lt;p&gt;tc supports simple pattern matching in &lt;code&gt;expected.json&lt;/code&gt; for dynamic values:&lt;/p&gt;
    &lt;code&gt;{
  "id": "&amp;lt;uuid&amp;gt;",
  "status": "pending",
  "created_at": "&amp;lt;timestamp&amp;gt;",
  "count": "&amp;lt;number&amp;gt;",
  "message": "&amp;lt;string&amp;gt;"
}&lt;/code&gt;
    &lt;p&gt;Patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;uuid&amp;gt;&lt;/code&gt;- validates UUID v4 format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;timestamp&amp;gt;&lt;/code&gt;- validates ISO 8601 timestamp (YYYY-MM-DDTHH:MM:SS)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;number&amp;gt;&lt;/code&gt;- any JSON number&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;string&amp;gt;&lt;/code&gt;- any string value&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;boolean&amp;gt;&lt;/code&gt;- true or false&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;null&amp;gt;&lt;/code&gt;- null value&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;any&amp;gt;&lt;/code&gt;- matches anything&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Works everywhere:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nested objects&lt;/item&gt;
      &lt;item&gt;Array elements&lt;/item&gt;
      &lt;item&gt;Mixed with exact values&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No configuration needed - patterns are auto-detected.&lt;/p&gt;
    &lt;p&gt;Define your own patterns via &lt;code&gt;TC_CUSTOM_PATTERNS&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;export TC_CUSTOM_PATTERNS="email:^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$
ipv4:^([0-9]{1,3}\.){3}[0-9]{1,3}$
phone:^\+?[0-9]{10,15}$"&lt;/code&gt;
    &lt;p&gt;Then use in expected.json:&lt;/p&gt;
    &lt;code&gt;{
  "email": "&amp;lt;email&amp;gt;",
  "server": "&amp;lt;ipv4&amp;gt;",
  "contact": "&amp;lt;phone&amp;gt;"
}&lt;/code&gt;
    &lt;p&gt;Format: &lt;code&gt;pattern_name:regex&lt;/code&gt; (one per line, standard regex syntax)&lt;/p&gt;
    &lt;p&gt;test execution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;run single test suite&lt;/item&gt;
      &lt;item&gt;semantic json comparison (order-independent)&lt;/item&gt;
      &lt;item&gt; pattern matching (&lt;code&gt;&amp;lt;uuid&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;timestamp&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;number&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;string&amp;gt;&lt;/code&gt;, etc.)&lt;/item&gt;
      &lt;item&gt;timeout management&lt;/item&gt;
      &lt;item&gt;result persistence (tc-result files)&lt;/item&gt;
      &lt;item&gt;hierarchical test discovery (--all flag)&lt;/item&gt;
      &lt;item&gt;tag-based filtering (--tags flag)&lt;/item&gt;
      &lt;item&gt;parallel execution (--parallel flag, auto-detect CPU cores)&lt;/item&gt;
      &lt;item&gt;single-line animated status (TTY mode: helicopter 🚁, spinner, colors)&lt;/item&gt;
      &lt;item&gt;fail-fast on first error (TTY mode stops immediately, shows log path)&lt;/item&gt;
      &lt;item&gt;final stats summary (colored counts: passed/failed/errors, cumulative time)&lt;/item&gt;
      &lt;item&gt;traditional verbose output (non-TTY mode for CI/CD)&lt;/item&gt;
      &lt;item&gt; machine-readable logs (JSONL format in &lt;code&gt;tc/tmp/report.jsonl&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;test generation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; scaffold generation (&lt;code&gt;tc new&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt; test directory initialization (&lt;code&gt;tc init&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;metadata flags (--tags, --priority, --description)&lt;/item&gt;
      &lt;item&gt;template system (--from, --list-examples)&lt;/item&gt;
      &lt;item&gt;TDD workflow (tests fail until implemented)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;discovery &amp;amp; metadata:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; list all tests (&lt;code&gt;tc list&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt; show available tags (&lt;code&gt;tc tags&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt; explain test suite (&lt;code&gt;tc explain&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;AI-friendly metadata format&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;quality:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;dogfooding (tc tests itself!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;roadmap:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pattern-based selection&lt;/item&gt;
      &lt;item&gt;distributed test execution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;→ tc-kit: AI-driven testing &lt;/p&gt;
    &lt;p&gt;tc-kit integrates with spec-kit for automatic test generation from specifications. Perfect for AI-assisted development workflows where specs and tests are permanent while implementations are disposable.&lt;/p&gt;
    &lt;p&gt;Quick start:&lt;/p&gt;
    &lt;code&gt;# Generate tests from spec
/tc.specify

# Implement to pass tests
edit tc/tests/my-feature/user-story-01/run

# Validate &amp;amp; refine
/tc.validate
/tc.refine&lt;/code&gt;
    &lt;p&gt;See AI.md for full documentation.&lt;/p&gt;
    &lt;p&gt;Prerequisites: bash 4.0+, jq&lt;/p&gt;
    &lt;code&gt;# Install jq
brew install jq                     # macOS
sudo apt-get install jq             # Ubuntu/Debian

# Clone tc
git clone https://github.com/ahoward/tc.git
cd tc

# Add to PATH
export PATH="$PWD/tc/bin:$PATH"

# Verify
tc --version&lt;/code&gt;
    &lt;p&gt;See the TL;DR section above for PATH setup details.&lt;/p&gt;
    &lt;p&gt;mit license - see LICENSE&lt;/p&gt;
    &lt;p&gt;made with ☕ and helicopters&lt;/p&gt;
    &lt;p&gt;"the chopper's fueled up and ready to go. let's test some code." — tc&lt;/p&gt;
    &lt;p&gt;🚁 fly safe, test well&lt;/p&gt;
    &lt;p&gt;an #n5 joint 🚬&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46359683</guid><pubDate>Mon, 22 Dec 2025 22:03:07 +0000</pubDate></item><item><title>Transformers Are Dead. Google Killed Them – Then Went Silent</title><link>https://medium.com/@aedelon/transformers-are-dead-google-killed-them-then-went-silent-a379ed35409b</link><description>&lt;doc fingerprint="9f57e78b9cfb92b4"&gt;
  &lt;main&gt;
    &lt;p&gt;Member-only story&lt;/p&gt;
    &lt;head rend="h1"&gt;Transformers Are Dead. Google Killed Them — Then Went Silent&lt;/head&gt;
    &lt;head rend="h2"&gt;A deep dive into Google’s neural long-term memory architecture that claims 2M+ token context windows with O(n) complexity. The data is impressive. The skepticism is warranted.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;TL;DR: Google’s Titans introduces neural long-term memory that learns during inference via “surprise-based” updates — 2M+ token context with O(n) complexity instead of O(n²). Benchmarks show 98.8% needle-in-haystack accuracy vs Mamba-2’s 31%. But no official code exists, implementation details are ambiguous, and a follow-up paper found chunking degrades performance. Impressive innovation, but wait for independent reproduction before trusting the numbers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;⚠️ If you’re not a Medium member, you can read this article for free using my friend link: Read for free.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“The palest ink is better than the best memory.” — Chinese proverb&lt;/p&gt;
      &lt;p&gt;Or my 2025 version: The best memory is useless if you can’t retrieve it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Why Google Released This on New Year’s Eve&lt;/head&gt;
    &lt;p&gt;98.2% accuracy on needle-in-haystack tasks at 8K context (scaling to 2M+ tokens).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46359732</guid><pubDate>Mon, 22 Dec 2025 22:08:06 +0000</pubDate></item><item><title>NPM Package with 56K Downloads Caught Stealing WhatsApp Messages</title><link>https://www.koi.ai/blog/npm-package-with-56k-downloads-malware-stealing-whatsapp-messages</link><description>&lt;doc fingerprint="a5241f11ab2114d9"&gt;
  &lt;main&gt;
    &lt;p&gt;The &lt;code&gt;lotusbail&lt;/code&gt; npm package presents itself as a WhatsApp Web API library - a fork of the legitimate &lt;code&gt;@whiskeysockets/baileys&lt;/code&gt; package. With over 56,000 downloads and functional code that actually works as advertised, it's the kind of dependency developers install without a second thought. The package has been available on npm for 6 months and is still live at the time of writing.&lt;/p&gt;
    &lt;p&gt;Behind that working functionality: sophisticated malware that steals your WhatsApp credentials, intercepts every message, harvests your contacts, installs a persistent backdoor, and encrypts everything before sending it to the threat actor's server.&lt;/p&gt;
    &lt;p&gt;What gets captured:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Authentication tokens and session keys&lt;/item&gt;
      &lt;item&gt;Complete message history (past and present)&lt;/item&gt;
      &lt;item&gt;Full contact lists with phone numbers&lt;/item&gt;
      &lt;item&gt;Media files and documents&lt;/item&gt;
      &lt;item&gt;Persistent backdoor access to your WhatsApp account&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;head rend="h3"&gt;The Cover Is Real&lt;/head&gt;
    &lt;p&gt;Most malicious npm packages reveal themselves quickly - they're typosquats, they don't work, or they're obviously sketchy. This one actually functions as a WhatsApp API. It's based on the legitimate Baileys library and provides real, working functionality for sending and receiving WhatsApp messages.&lt;/p&gt;
    &lt;p&gt;Obvious malware is easy to spot. Functional malware? That gets installed, tested, approved, and deployed to production.&lt;/p&gt;
    &lt;p&gt;The social engineering here is brilliant: developers don't look for malware in code that works. They look for code that breaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Theft and Exfiltration&lt;/head&gt;
    &lt;p&gt;The package wraps the legitimate WebSocket client that communicates with WhatsApp. Every message that flows through your application passes through the malware's socket wrapper first.&lt;/p&gt;
    &lt;p&gt;When you authenticate, the wrapper captures your credentials. When messages arrive, it intercepts them. When you send messages, it records them. The legitimate functionality continues working normally - the malware just adds a second recipient for everything.&lt;/p&gt;
    &lt;p&gt;All your WhatsApp authentication tokens, every message sent or received, complete contact lists, media files - everything that passes through the API gets duplicated and prepared for exfiltration.&lt;/p&gt;
    &lt;p&gt;But the stolen data doesn't get sent in plain text. The malware includes a complete, custom RSA implementation for encrypting the data before transmission:&lt;/p&gt;
    &lt;p&gt;Why implement custom RSA? Because legitimate WhatsApp libraries don't need custom encryption - WhatsApp already handles end-to-end encryption. The custom crypto exists for one reason: to encrypt stolen data before exfiltration so network monitoring won't catch it.&lt;/p&gt;
    &lt;p&gt;The exfiltration server URL is buried in encrypted configuration strings, hidden inside compressed payloads. The malware uses four layers of obfuscation: Unicode variable manipulation, LZString compression, Base-91 encoding, and AES encryption. The server location isn't hardcoded anywhere visible.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Backdoor&lt;/head&gt;
    &lt;p&gt;Here's where it gets particularly nasty. WhatsApp uses pairing codes to link new devices to accounts. You request a code, WhatsApp generates a random 8-character string, you enter it on your new device, and the devices link together.&lt;/p&gt;
    &lt;p&gt;The malware hijacks this process with a hardcoded pairing code. The code is encrypted with AES and hidden in the package:&lt;/p&gt;
    &lt;p&gt;This means the threat actor has a key to your WhatsApp account. When you use this library to authenticate, you're not just linking your application - you're also linking the threat actor's device. They have complete, persistent access to your WhatsApp account, and you have no idea they're there.&lt;/p&gt;
    &lt;p&gt;The threat actor can read all your messages, send messages as you, download your media, access your contacts - full account control. And here's the critical part, uninstalling the npm package removes the malicious code, but the threat actor's device stays linked to your WhatsApp account. The pairing persists in WhatsApp's systems until you manually unlink all devices from your WhatsApp settings. Even after the package is gone, they still have access.&lt;/p&gt;
    &lt;head rend="h2"&gt;They Really Didn't Want You Looking&lt;/head&gt;
    &lt;p&gt;The package includes 27 infinite loop traps that freeze execution if debugging tools are detected:&lt;/p&gt;
    &lt;p&gt;These traps check for debuggers, inspect process arguments, detect sandbox environments, and generally make dynamic analysis painful. They also left helpful comments in their code marking the malicious sections - professional development practices applied to supply chain attacks. Someone probably has a Jira board for this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;Supply chain attacks aren't slowing down - they're getting better. We're seeing working code with sophisticated anti-debugging, custom encryption, and multi-layer obfuscation that survives marketplace reviews. The &lt;code&gt;lotusbail&lt;/code&gt; case isn't an outlier. It's a preview.&lt;/p&gt;
    &lt;p&gt;Traditional security doesn't catch this. Static analysis sees working WhatsApp code and approves it. Reputation systems see 56,000 downloads and trust it. The malware hides in the gap between "this code works" and "this code only does what it claims."&lt;/p&gt;
    &lt;p&gt;Catching sophisticated supply chain attacks requires behavioral analysis - watching what packages actually do at runtime. When a WhatsApp library implements custom RSA encryption and includes 27 anti-debugging traps, those are signals. But you need systems watching for them.&lt;/p&gt;
    &lt;p&gt;This writeup was authored by the research team at Koi Security. We built Koi to detect threats that pass traditional checks but exhibit malicious behavior at runtime.&lt;/p&gt;
    &lt;p&gt;Book a demo to see how behavioral analysis catches what static review misses.&lt;/p&gt;
    &lt;p&gt;Stay safe out there.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46359996</guid><pubDate>Mon, 22 Dec 2025 22:35:45 +0000</pubDate></item></channel></rss>