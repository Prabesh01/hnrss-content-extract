<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 05 Oct 2025 03:23:49 +0000</lastBuildDate><item><title>The Buchstabenmuseum Berlin is closing</title><link>https://www.buchstabenmuseum.de/en/</link><description>&lt;doc fingerprint="7e81930193d6e0f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;After 20 Years:&lt;/head&gt;
    &lt;head rend="h3"&gt;The Buchstabenmuseum Berlin is closing!&lt;/head&gt;
    &lt;p&gt;Until 5 October 2025, you can visit our museum every Thursday to Sunday from 1 to 5 pm.&lt;lb/&gt; A visit outside opening hours is possible with a guided tour.&lt;lb/&gt; visit@buchstabenmuseum.de&lt;/p&gt;
    &lt;p&gt;We are still looking for long-term storage for our collection.&lt;lb/&gt; For this we need support:&lt;lb/&gt; &amp;gt;&amp;gt; betterplace&lt;/p&gt;
    &lt;head rend="h4"&gt;We look forward to your visit!&lt;/head&gt;
    &lt;p&gt;______________________________&lt;/p&gt;
    &lt;head rend="h1"&gt;20 Years of the Buchstabenmuseum!&lt;/head&gt;
    &lt;head rend="h4"&gt;On Sat 14 June 2025 from 3 to 9 pm we want to celebrate our birthday and toast with you!&lt;lb/&gt; You and all your friends and family are cordially invited.&lt;/head&gt;
    &lt;p&gt;20 years is a very long time to look back on.&lt;lb/&gt; That’s why we’ve put together a colourful brochure that we’ll be presenting on Saturday.&lt;/p&gt;
    &lt;p&gt;In addition, Sabrina Hauck (student at TU Berlin / architect at gkks) will give a talk on the use,&lt;lb/&gt; vacancy and potential of Berlin’s S-Bahn arches at 18:00.&lt;/p&gt;
    &lt;head rend="h1"&gt;FINAL SALE – FROM DEPARTMENT STORES’ TO MUSEUM &lt;/head&gt;
    &lt;p&gt;Extended Term until Autumn 2025!&lt;/p&gt;
    &lt;p&gt;AN EXHIBITION OF FORMER DEPARTMENT STORES FROM 1980 TO TODAY&lt;/p&gt;
    &lt;p&gt;Horten, Quelle, Hertie, Kaufhof and Karstadt – corporate names that are disappearing from German city centres. Galeria Karstadt Kaufhof is currently struggling with closure in instalments. With the creeping loss of the corporations, the distinctive lettering of the department stores’ chains is also being lost.&lt;lb/&gt; “FINAL SALE” tells the typographic and urban-historical stories of the letters and shows the former significance of the department stores and department stores with their architecture.&lt;lb/&gt; We invite you to discover the typographic department stores’ icons and to buy selected items in the exhibition: “Final Sale – from department stores’ to museum” in the Buchstabenmuseum in the Stadtbahnbögen in the Hansaviertel.&lt;/p&gt;
    &lt;head rend="h4"&gt;COOPERATION WITH THE STAATSBIBLIOTHEK BERLIN&lt;/head&gt;
    &lt;p&gt;In the »Staatsbibliothek Berlin«, Unter den Linden, selected Ks from the collection of the Buchstabenmuseum point the way to the in-house museum "Kulturwerk".&lt;/p&gt;
    &lt;head rend="h4"&gt;INDIVIDUAL GUIDED TOURS&lt;/head&gt;
    &lt;p&gt;Discover our unique collection and learn the exciting background stories to our letters.&lt;/p&gt;
    &lt;head rend="h4"&gt;NEON CLASSES: BENDING BASICS&lt;/head&gt;
    &lt;p&gt;The art of neon and the bending of neon tubes is a fascinating craft! Learn the basics of neon and glass bending.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE BUCHSTABENMUSEUM&lt;/head&gt;
    &lt;p&gt;Preservation and documentation of letters&lt;lb/&gt; The Buchstabenmuseum is the first museum in the world to collect letterforms from public spaces and display them as part of urban history. We preserve and document three-dimensional letters and signage, and their history, as well as providing information about their origins and construction. Our collection has captured the imagination of visitors from all around the world for over 10 years. Hundreds of letters have been saved from being battered by the elements or ending up on the scrap heap. A selection of what we offer can be found under » COLLECTION&lt;/p&gt;
    &lt;head rend="h2"&gt;COLLECTION&lt;/head&gt;
    &lt;head rend="h2"&gt;SPECIALS&lt;/head&gt;
    &lt;p&gt;— BECOME A MEMBER —&lt;/p&gt;
    &lt;p&gt;Become a member of the club and actively support our museum.&lt;lb/&gt; We are always in need of help with the rescue of historic lettering, looking after our international guests or even with the classic work of the association.&lt;lb/&gt; Just write to us!&lt;lb/&gt; bindabei@buchstabenmuseum.de&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45472678</guid><pubDate>Sat, 04 Oct 2025 11:58:58 +0000</pubDate></item><item><title>Thunderscan: A clever device transforms a printer into a scanner (2004)</title><link>https://www.folklore.org/Thunderscan.html</link><description>&lt;doc fingerprint="8549597119742b9e"&gt;
  &lt;main&gt;
    &lt;p&gt;The first project that I worked on for Apple after starting in August 1979 was writing low level software for the Silentype printer (see What Hath Woz Wrought), a cute, inexpensive thermal printer for the Apple II, that was based on technology licensed from a local company named Trendcom. In typical Apple fashion, we improved on Trendcom's design by replacing their relatively expensive controller board with a much simpler one that relied on the microprocessor in the Apple II to do most of the dirty work.&lt;/p&gt;
    &lt;p&gt;The only other engineer working on the project was Victor Bull, who was the hardware designer and also the project leader. Vic was smart, taciturn and easy to work with, and I learned a lot from him about how thermal printers worked, as well as how things worked at Apple. We finished the project quickly, and the Silentype shipped in November 1979, less than four months after I began working on it.&lt;/p&gt;
    &lt;p&gt;In May 1984, during my leave of absence from Apple (see Leave Of Absence), I received a phone call from Victor Bull, who I hadn't heard from in a couple of years. He had left Apple more than a year ago to work with his friend Tom Petrie at a tiny company based in Orinda named Thunderware, that sold a single product called Thunderclock, an inexpensive calendar/clock card for the Apple II. Victor said that he thought that I might be interested in writing software for an exciting, clever new product that Thunderware was developing for the Macintosh, which he refused to describe over the phone. He invited me to come visit them to check it out.&lt;/p&gt;
    &lt;p&gt;In early June, I drove up to Thunderware's office in Orinda, which was about an hour's drive from my house in Palo Alto. After I arrived at their modest headquarters, Vic introduced me to his partner, Tom Petrie, and I signed a non-disclosure agreement before they ushered me into a back room to see their demo.&lt;/p&gt;
    &lt;p&gt;The most popular printer for both the Apple II and the Macintosh was the ImageWriter, a $500 dot-matrix printer capable of rendering bitmapped graphics, that was designed and manufactured by Japanese company named C.Itoh Electronics and marketed by Apple. Virtually every Macintosh owner purchased an ImageWriter, since it was the only printer that was supported by Apple. Tom's demo consisted of an ImageWriter printer hooked up to an Apple II, that at first glance appeared to be busily printing away. But when I looked closer, I noticed that instead of blank paper, there was a glossy photograph of a cat threaded through the printer's platen, and the printer's black plastic ribbon cartridge was missing, replaced by a makeshift contraption containing an optical sensing device that trailed an umbilical cord back to the Apple II.&lt;/p&gt;
    &lt;p&gt;Their potential new product, Thunderscan, was a low cost way to temporarily turn an ImageWriter printer into a high resolution scanner, by replacing the ribbon cartridge with an optical sensor and writing some clever software. Since the resolution was determined by the precision of the printer's stepper motors, which had to be very accurate in order to print detailed graphics, Thunderscan, priced at under $200, had better resolution than flat bed scanners costing more than ten times as much. I loved the cleverness of the ingenious concept, and the Woz-like elegance of saving money and adding flexibility by doing everything in software, but there were also a few problems.&lt;/p&gt;
    &lt;p&gt;The biggest problem was that Thunderscan could only capture one scan line's worth of data on each pass of the print head, which made it nine times slower than regular printing, since the print head could deposit nine dots at a time. This made for frustratingly slow scanning, often taking over an hour to scan a full page at the highest resolution. Thunderscan was never going to win any races.&lt;/p&gt;
    &lt;p&gt;Another apparent problem was the disappointingly low quality of the image being captured and displayed by Tom Petrie's Apple II application. Tom and Vic said their scanner was capable of capturing up to 32 different levels of light intensity, but both the Apple II (in hi-res mode) and the Macintosh only had one bit per pixel to display, so the software had to simulate gray scales using patterns of black and white dots. It looked like Tom was using a simple threshold algorithm to do the rendering, which threw away most of the gray scale information and made the resulting image look unacceptably blotchy. It was hard to tell if the quality promised by Tom and Vic was there or not.&lt;/p&gt;
    &lt;p&gt;Tom and Vic proposed to hire me to write Macintosh software for Thunderscan. I knew that a low cost scanner would be a great product for the image hungry Macintosh, but only if it had sufficient quality, and I wasn't sure about that. I told them that I'd think it over during the next few days, and, as I did, I became more excited about the potential of Thunderscan for the Macintosh, realizing that the slow speed wouldn't be that much of an impediment if the quality and resolution was good enough. The low image quality in Tom's prototype was probably caused more by the Apple II software than by anything inherent in the scanner. The Macintosh was almost ten times faster than the Apple II, so it should be able to sample the incoming data better to obtain more horizontal resolution. Plus, I knew a much better algorithm for gray scale rendering that would be fun to try out in practice.&lt;/p&gt;
    &lt;p&gt;My friend and colleague Bill Atkinson was a talented photographer, and one of his hobbies was playing around with digitized pictures, periodically experimenting to find the best algorithms for rendering them. Bill loved to explain his current work to whoever would listen to him, so I learned a lot about rendering gray scale images over the years simply by being around him. Bill had progressed over the years from using an "ordered dither" algorithm, where varying threshold values are specified in a sliding matrix, to his current favorite, which was a modified version of what was known as the "Floyd-Steinberg" algorithm, where an error term is maintained and distributed proportionally to neighboring pixels.&lt;/p&gt;
    &lt;p&gt;I called Thunderware and told them I was interested in working on Macintosh software for Thunderscan, in exchange for a per-unit royalty. I drove back up to Orinda, where Tom and Vic gave me lots of documentation about the scanner, and the sample code that Tom had written for the Apple II. For the next couple of months, I drove up to Orinda once a week, usually on Thursday, to meet with Tom and Vic show them my progress, prioritize development issues and discussion various complications as they arose. We would also discuss business terms, but we didn't sign a formal contract until the software was almost finished, when we settled on a royalty of $7.50 per unit.&lt;/p&gt;
    &lt;p&gt;Tom and Vic had already encountered and surmounted a number of tough problems just to get scanning going at all. For example, the ImageWriter printer was not really designed to be stepped one scanline at a time, and if you tried that the paper would bunch up against the platen, causing distortion. Tom and Vic solved the problem by commanding the printer to move three steps up and then two steps back, instead of a single step up, which held the paper snugly against the platen as required. There were also various techniques for sensing the beginning and end of the scan line, and some timings that were determined by tedious experimentation for how long it took the printer to respond to a command.&lt;/p&gt;
    &lt;p&gt;It took a week or so to get basic scanning working on the Macintosh, and then a few more days to render the gray scale data with Bill's modified Floyd-Steinberg dithering. After shaking out a variety of problems, mostly involving synchronization between the printer and the software, I was surprised and impressed by the consistent high quality of the results. I went through a brief, elated phase of scanning every image in sight that would fit through the printer, just to see how it would turn out.&lt;/p&gt;
    &lt;p&gt;One important design decision that I made early on was to keep the gray scale data around, to allow more flexible image processing. Thunderscan documents were five bits per pixel, before the Macintosh generally supported gray scale, and the user could manipulate the contrast and brightness of selected areas of the image, dodging and burning to reveal detail in the captured image. This also paid off in later versions when we implemented gray scale printing for Postscript printers.&lt;/p&gt;
    &lt;p&gt;My favorite feature that I came up with for Thunderscan had to do with two dimensional scrolling. Thunderscan documents could be quite large, so you could only show a portion of them in the image area of the window. You could scroll the image by dragging with a MacPaint-style "hand" scrolling tool, but you had to drag an awful lot to get to the extremes of a large image. I decided to add what I called "inertial" scrolling, where you gave the image a push and it kept scrolling at a variable speed in the direction of the push, after the mouse button was released. I had to add some hysteresis to keep the image from moving accidentally, and make various other tweaks, but soon I had it working and it felt great to be able to zip around large images by pushing them.&lt;/p&gt;
    &lt;p&gt;The hardest feature to perfect was bidirectional scanning. At first, Thunderscan only scanned from left to right, but it wasted time to return the scannner to the left after every scan line. We could almost double the speed if we scanned in both directions, but it was hard to get the adjacent scan lines that were scanned in opposite directions to line up properly. Ultimately, we made bidirectional scanning an optional feature, if you wanted to trade a little quality for greater speed.&lt;/p&gt;
    &lt;p&gt;I finished the software in November 1984, after taking a short break to work on something else (see Switcher). Thunderscan shipped in December 1984, and did well from the very beginning, with sales gradually rising from around 1,000 units/month to over 7,500 units/month at its peak in 1987. For a while, it was both the least expensive and highest quality scanning alternative for the Macintosh, although I'm sure it frustrated a lot of users by being too slow. I did three major revisions of the software over the next few years, improving the scan quality and adding features like gray scale printing and eventually gray scale display for the Macintosh II.&lt;/p&gt;
    &lt;p&gt;Eventually, the flat bed scanners caught up to Thunderscan, and then surpassed it, in both cost, quality and convenience. Over its lifetime, Thunderscan sold approximately 100,000 units and improved countless documents by providing users with an inexpensive way to capture high resolution graphics with their Macintoshes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45472765</guid><pubDate>Sat, 04 Oct 2025 12:16:37 +0000</pubDate></item><item><title>Self-hosting email like it's 1984</title><link>https://maxadamski.com/blog/2025/10/email.html</link><description>&lt;doc fingerprint="e89d2d5b3a50be95"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Self-hosting an email server is useful for automating tasks like mailing lists, newsletters, or email verification APIs.&lt;/p&gt;
    &lt;p&gt;The elephant in the room is real-world deliverability. With self-hosting you risk not receiving mail or someone missing your mail. I accept this for my personal projects, but you may not. Keep this in mind.&lt;/p&gt;
    &lt;p&gt;For me the selling point of self-hosting is that itâs practically free. If youâre already self-hosting a website, installing some extra packages on your server and just a bit of your time is all thatâs required. Mail takes very little storage and the software is light, so youâre unlikely to significantly change energy consumption or disk usage.&lt;/p&gt;
    &lt;p&gt;For the longest time, I perceived self-hosting email as too difficult, but after doing it for one of my projects, I can say itâs not much harder or more time-consuming than configuring some email SaaS.&lt;/p&gt;
    &lt;p&gt;I changed my goals a bit to make the setup easier though. Self-hosting a multi-user webmail looks heavy and is more involved than I was willing to get into, so I just skipped it. That way, I didnât have to bother with user accounts, databases, or the web at all, and the task became easy.&lt;/p&gt;
    &lt;p&gt;With my config, manually sending and receiving email is possible if you SSH to your mail server and use the minimal sendmail or mailx commands, or Mutt if you like TUI. I've been semi-comfortably using mailx for a month already (with its ancient user interface!), so the setup is enough for me now, but I could expand it in the future, and multi-user webmail isnât completely off the table. Maybe Iâll even write a simple webmail package myself!&lt;/p&gt;
    &lt;head rend="h2"&gt;Postfix&lt;/head&gt;
    &lt;p&gt;You just need to open port 25, and install and configure Postfix and OpenDKIM on your machine. Postfix is a complete SMTP server, and is enough for basic mail alone, but in practice you also need OpenDKIM to get your mail delivered to popular services like Gmail.&lt;/p&gt;
    &lt;p&gt;Here's my Postfix config to show how easy it is. I left the master.cf file as it was, because Iâm always submitting email locally.&lt;/p&gt;
    &lt;p&gt;The default alias and header check config files are practically self-explanatory (just open them and read the comments!).&lt;/p&gt;
    &lt;head&gt;/etc/postfix/main.cf&lt;/head&gt;
    &lt;quote&gt;compatibility_level = 3.8 mail_owner = postfix myhostname = mx.idx.cy myorigin = idx.cy mydestination = localhost, idx.cy, maxadamski.com, localchat.cc inet_interfaces = all inet_protocols = ipv4 # Addresses alias_maps = hash:/etc/postfix/aliases alias_database = $alias_maps recipient_delimiter = + # I'm the only user on my machine, so I send from whichever address I want. #smtpd_sender_login_maps = hash:/etc/postfix/sender_login_maps #smtpd_sender_restrictions = reject_authenticated_sender_login_mismatch # spam #in_flow_delay = 1s header_checks = regexp:/etc/postfix/header_checks setgid_group = postdrop # TLS (strict) smtpd_tls_cert_file = /etc/ssl/tls/mx.idx.cy.crt smtpd_tls_key_file = /etc/ssl/tls/mx.idx.cy.key smtpd_tls_security_level = encrypt smtpd_tls_mandatory_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtpd_tls_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtp_tls_security_level = encrypt smtp_tls_mandatory_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 smtp_tls_protocols = !SSLv2, !SSLv3, !TLSv1, !TLSv1.1 # DKIM smtpd_milters = inet:localhost:8891 non_smtpd_milters = inet:localhost:8891 milter_default_action = accept&lt;/quote&gt;
    &lt;head&gt;/etc/postfix/master.cf&lt;/head&gt;
    &lt;quote&gt;# ========================================================================== # service type private unpriv chroot wakeup maxproc command + args # (yes) (yes) (no) (never) (100) # ========================================================================== smtp inet n - n - - smtpd pickup unix n - n 60 1 pickup cleanup unix n - n - 0 cleanup qmgr unix n - n 300 1 qmgr tlsmgr unix - - n 1000? 1 tlsmgr rewrite unix - - n - - trivial-rewrite bounce unix - - n - 0 bounce defer unix - - n - 0 bounce trace unix - - n - 0 bounce verify unix - - n - 1 verify flush unix n - n 1000? 0 flush proxymap unix - - n - - proxymap proxywrite unix - - n - 1 proxymap smtp unix - - n - - smtp relay unix - - n - - smtp -o syslog_name=postfix/$service_name showq unix n - n - - showq error unix - - n - - error retry unix - - n - - error discard unix - - n - - discard local unix - n n - - local virtual unix - n n - - virtual lmtp unix - - n - - lmtp anvil unix - - n - 1 anvil scache unix - - n - 1 scache postlog unix-dgram n - n - 1 postlogd&lt;/quote&gt;
    &lt;p&gt;Notice that there's no mention of POP3 or IMAP! I did waste some time trying to set them up with Dovecot (because they changed their config format too much, so guides became outdated, and their web docs were just hard to read for me). Ultimately I can just SSH to my server and I feel comfortable with mailx, so I skipped Dovecot. One package less in my system :)&lt;/p&gt;
    &lt;head rend="h2"&gt;TLS&lt;/head&gt;
    &lt;p&gt;You will also need an SSL certificate for encryption in transit. I hate getting and renewing SSL certificates, because the tools are bulky and automation is yet another moving part in your system (I used the lego package, with the manual DNS challenge for simplicity, but Iâm not too happy about it). I wonât give you a tutorial on getting SSL certificates, but note that you donât have to get and renew a certificate for each of your custom domains!&lt;/p&gt;
    &lt;p&gt;You just need one SSL certificate for your machine to encrypt data in transit to other SMTP servers. If you create an A record mx.example.com pointing to your email machineâs IP address, then grab a free certificate for mx.example.com from Letâs Encrypt. Then point to it in the Postfix configuration, and youâve got transport encryption! In short, only the MX hostname needs a cert for STARTTLS to be used for encryption.&lt;/p&gt;
    &lt;p&gt;Why no certificates for your actual email domains like example.com? Because the email domain has little to do with transport encryption. TLS only secures the connection between servers. You can still set whatever you want in the From header.&lt;/p&gt;
    &lt;head rend="h2"&gt;DKIM, SPF, and DMARC&lt;/head&gt;
    &lt;p&gt;You should prove that your emails actually come from your domain to make your mail trustworthy and deliver to Gmail and co. Thatâs what DKIM is for, and fortunately itâs a one-time deal per email domain. First you generate a key pair for each domain with OpenDKIM, and then you publish the public key in a TXT record in DNS. The keys donât expire automatically, but itâs best practice to rotate them periodically. My config uses a naming scheme that allows smooth rotation, but it doesnât complicate things if you skip it.&lt;/p&gt;
    &lt;p&gt;There are two more TXT records that you need to publish in DNS: the SPF and DMARC records. You say which hosts are allowed to send mail from your email domain, and give instructions to other email servers about what to do with mail that fails DKIM checks. In my case I told others to reject mail that canât be verified as coming from my domains, and send reports to my postmaster address.&lt;/p&gt;
    &lt;p&gt;Take a look at my OpenDKIM config to understand how things come together.&lt;/p&gt;
    &lt;head&gt;/etc/opendkim.conf&lt;/head&gt;
    &lt;quote&gt;UserID opendkim:opendkim Socket inet:8891@localhost KeyTable refile:/etc/opendkim/KeyTable SigningTable refile:/etc/opendkim/SigningTable ExternalIgnoreList refile:/etc/opendkim/TrustedHosts InternalHosts refile:/etc/opendkim/TrustedHosts Canonicalization relaxed/relaxed ReportAddress postmaster@idx.cy SendReports no LogWhy yes Syslog yes SyslogSuccess no&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/KeyTable&lt;/head&gt;
    &lt;quote&gt;key1._domainkey.idx.cy idx.cy:key1:/etc/opendkim/keys/idx.cy/key1.private key1._domainkey.maxadamski.com maxadamski.com:key1:/etc/opendkim/keys/maxadamski.com/key1.private key1._domainkey.localchat.cc localchat.cc:key1:/etc/opendkim/keys/localchat.cc/key1.private&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/SigningTable&lt;/head&gt;
    &lt;quote&gt;*@idx.cy key1._domainkey.idx.cy *@maxadamski.com key1._domainkey.maxadamski.com *@localchat.cc key1._domainkey.localchat.cc&lt;/quote&gt;
    &lt;head&gt;/etc/opendkim/TrustedHosts&lt;/head&gt;
    &lt;quote&gt;127.0.0.1 localhost&lt;/quote&gt;
    &lt;p&gt;I generate DKIM keys with the following command:&lt;/p&gt;
    &lt;quote&gt;opendkim-genkey -D /etc/opendkim/keys/example.com -d example.com -s key1&lt;/quote&gt;
    &lt;p&gt;And for each email domain I have the following records in DNS:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MX&lt;/cell&gt;
        &lt;cell&gt;example.com&lt;/cell&gt;
        &lt;cell&gt;mx.idx.cy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;example.com&lt;/cell&gt;
        &lt;cell&gt;v=spf1 mx a -all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;key1._domainkey&lt;/cell&gt;
        &lt;cell&gt;v=DKIM1; k=rsa; s=email; p=&amp;lt;public-key&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TXT&lt;/cell&gt;
        &lt;cell&gt;_dmarc&lt;/cell&gt;
        &lt;cell&gt;v=DMARC1; p=reject; rua=mailto:postmaster@idx.cy&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Reverse DNS&lt;/head&gt;
    &lt;p&gt;One more thing about self-hosted email deliverability. I've read that reverse DNS (PTR record) will boost the reputation of your email server. The thing is that your ISP has to set it up, and I suspect my ISP to reply with a polite "no", so I didn't do it yet. As you'll see in the next section, my email gets delivered to Gmail just fine. GMX and Outlook also didn't mark my mail as spam. Maybe my IP is lucky :)&lt;/p&gt;
    &lt;p&gt;But in general, if you want wide deliverability, PTR isn't optional.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gmail&lt;/head&gt;
    &lt;p&gt;To try it out, let's send a test mail to Gmail with the sendmail command:&lt;/p&gt;
    &lt;quote&gt;sendmail -vt &amp;lt; test.mail&lt;/quote&gt;
    &lt;head&gt;test.mail&lt;/head&gt;
    &lt;quote&gt;Content-Type: text/html From: max@idx.cy To: myaddress@gmail.com Subject: DKIM test Test message from idx.cy!&lt;/quote&gt;
    &lt;p&gt;I got the mail instantly and Gmail confirmed TLS encryption.&lt;/p&gt;
    &lt;p&gt;Click "Show original" in Gmail to see the raw mail. There's lots of text in the headers, so let's just focus on passing SPF, DKIM, and DMARC :)&lt;/p&gt;
    &lt;p&gt;You'll also get a mail with a report because of the -v option. I receive mail with Heirloom Mail like this:&lt;/p&gt;
    &lt;quote&gt;You have new mail in /var/mail/max fool ~ | mailx Heirloom Mail version 12.5 7/5/10. Type ? for help. "/var/mail/max": 1 message 1 new &amp;gt;N 1 Mail Delivery System Sat Oct 4 15:40 74/2437 "Mail Delivery Status Report"&lt;/quote&gt;
    &lt;p&gt;I use the p command to print the mail.&lt;/p&gt;
    &lt;head&gt;&amp;amp; p&lt;/head&gt;
    &lt;quote&gt;Message 1: From MAILER-DAEMON Sat Oct 4 15:40:50 2025 X-Original-To: max@idx.cy Delivered-To: max@idx.cy Date: Sat, 4 Oct 2025 15:40:50 +0200 (CEST) From: Mail Delivery System &amp;lt;MAILER-DAEMON@idx.cy&amp;gt; Subject: Mail Delivery Status Report To: max@idx.cy Auto-Submitted: auto-replied Content-Type: multipart/report; report-type=delivery-status; boundary="3C311BFF8D.1759585250/mx.idx.cy" Status: R Part 1: Content-Description: Notification Content-Type: text/plain; charset=utf-8 This is the mail system at host mx.idx.cy. Enclosed is the mail delivery report that you requested. The mail system &amp;lt;myaddress@gmail.com&amp;gt;: delivery via gmail-smtp-in.l.google.com[X.X.X.X]:25: 250 2.0.0 OK 1759585250 4fb4d7f45d1cf-6393b6ba951si3187039a12.40 - gsmtp&lt;/quote&gt;
    &lt;p&gt;Great, everything is working!&lt;/p&gt;
    &lt;p&gt;If something isn't working for you, please double-check your DNS records, and triple-check that TLS certificates are readable by the Postfix user, and that DKIM keys are readable by the OpenDKIM user. Postfix and OpenDKIM logs will also be useful. The OpenDKIM config file is especially unforgiving of typos, so watch out for small mistakes!&lt;/p&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;In my next post on email, I'll show you how to use Python to build useful email applications. Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Btw, if you notice anything about my config (or want to share some thoughts) just email me at max@idx.cy :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45473730</guid><pubDate>Sat, 04 Oct 2025 14:53:32 +0000</pubDate></item><item><title>A comparison of Ada and Rust, using solutions to the Advent of Code</title><link>https://github.com/johnperry-math/AoC2023/blob/master/More_Detailed_Comparison.md</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45473861</guid><pubDate>Sat, 04 Oct 2025 15:10:49 +0000</pubDate></item><item><title>How to inject knowledge efficiently? Knowledge infusion scaling law for LLMs</title><link>https://arxiv.org/abs/2509.19371</link><description>&lt;doc fingerprint="ad1db54858d58a0a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 19 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45474900</guid><pubDate>Sat, 04 Oct 2025 17:18:07 +0000</pubDate></item><item><title>Show HN: Run – a CLI universal code runner I built while learning Rust</title><link>https://github.com/Esubaalew/run</link><description>&lt;doc fingerprint="24c05ddd9ba0d2c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Polyglot command runner &amp;amp; smart REPL that lets you script, compile, and iterate in 25+ languages without touching another CLI.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Built in Rust for developers who live in multiple runtimes.&lt;/p&gt;&lt;code&gt;run&lt;/code&gt;gives you a consistent CLI, persistent REPLs, and batteries-included examples for your favorite languages.&lt;/quote&gt;
    &lt;head&gt;Table of contents&lt;/head&gt;
    &lt;code&gt;# Show build metadata for the current binary
run --version

# Execute a snippet explicitly
run --lang python --code "print('hello, polyglot world!')"

# Let run detect language from the file extension
run examples/go/hello/main.go

# Drop into the interactive REPL (type :help inside)
run

# Pipe stdin (here: JSON) into Node.js
echo '{"name":"Ada"}' | run js --code "const data = JSON.parse(require('fs').readFileSync(0, 'utf8')); console.log(`hi ${data.name}`)"&lt;/code&gt;
    &lt;p&gt;All release assets are published on the GitHub Releases page, including macOS builds for both Apple Silicon (arm64) and Intel (x86_64). Pick the method that fits your platform:&lt;/p&gt;
    &lt;head&gt;Cargo (Rust)&lt;/head&gt;
    &lt;code&gt;cargo install run-kit&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Installs the&lt;/p&gt;&lt;code&gt;run&lt;/code&gt;binary from the&lt;code&gt;run-kit&lt;/code&gt;crate. Updating? Run&lt;code&gt;cargo install run-kit --force&lt;/code&gt;.&lt;/quote&gt;
    &lt;head&gt;Homebrew (macOS)&lt;/head&gt;
    &lt;code&gt;brew install --formula https://github.com/Esubaalew/run/releases/latest/download/homebrew-run.rb&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;This formula is published as a standalone file on each release; it isn’t part of the default Homebrew taps. Installing by name (&lt;/p&gt;&lt;code&gt;brew install homebrew-run&lt;/code&gt;) will fail—always point Homebrew to the release URL above (or download the file and run&lt;code&gt;brew install ./homebrew-run.rb&lt;/code&gt;).&lt;/quote&gt;
    &lt;p&gt;Once the latest release artifacts are published, Homebrew automatically selects the correct macOS binary for your CPU (Intel or Apple Silicon) based on this formula.&lt;/p&gt;
    &lt;head&gt;Debian / Ubuntu&lt;/head&gt;
    &lt;code&gt;curl -LO https://github.com/Esubaalew/run/releases/latest/download/run-deb.sha256
DEB_FILE=$(awk '{print $2}' run-deb.sha256)
curl -LO "https://github.com/Esubaalew/run/releases/latest/download/${DEB_FILE}"
sha256sum --check run-deb.sha256
sudo apt install "./${DEB_FILE}"&lt;/code&gt;
    &lt;head&gt;Windows (Scoop)&lt;/head&gt;
    &lt;code&gt;scoop install https://github.com/Esubaalew/run/releases/latest/download/run-scoop.json&lt;/code&gt;
    &lt;head&gt;Install script (macOS / Linux)&lt;/head&gt;
    &lt;code&gt;curl -fsSLO https://raw.githubusercontent.com/Esubaalew/run/master/scripts/install.sh
chmod +x install.sh
./install.sh --add-path           # optional: append ~/.local/bin to PATH&lt;/code&gt;
    &lt;p&gt;Pass &lt;code&gt;--version v0.2.0&lt;/code&gt;, &lt;code&gt;--prefix /usr/local/bin&lt;/code&gt;, or &lt;code&gt;--repo yourname/run&lt;/code&gt; to customize the install.&lt;/p&gt;
    &lt;head&gt;Download the archive directly&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Grab the &lt;code&gt;tar.gz&lt;/code&gt;(macOS/Linux) or&lt;code&gt;zip&lt;/code&gt;(Windows) from the latest release.&lt;/item&gt;
      &lt;item&gt;Extract it and copy &lt;code&gt;run&lt;/code&gt;/&lt;code&gt;run.exe&lt;/code&gt;onto your&lt;code&gt;PATH&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Optionally execute the bundled &lt;code&gt;install.sh&lt;/code&gt;to handle the copy for you.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Build from source&lt;/head&gt;
    &lt;code&gt;cargo install run-kit&lt;/code&gt;
    &lt;p&gt;The project targets Rust 1.70+. Installing from crates.io gives you the same &lt;code&gt;run&lt;/code&gt; binary that CI publishes; use &lt;code&gt;--force&lt;/code&gt; when upgrading to a newer release.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;run&lt;/code&gt; shells out to real toolchains under the hood. Each &lt;code&gt;LanguageEngine&lt;/code&gt; implements a small trait that knows how to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detect whether the toolchain is available (e.g. &lt;code&gt;python3&lt;/code&gt;,&lt;code&gt;go&lt;/code&gt;,&lt;code&gt;rustc&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Prepare a temporary workspace (compilation for compiled languages, transient scripts for interpreters).&lt;/item&gt;
      &lt;item&gt;Execute snippets, files, or stdin streams and surface stdout/stderr consistently.&lt;/item&gt;
      &lt;item&gt;Manage session state for the interactive REPL (persistent modules, stateful scripts, or regenerated translation units).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This architecture keeps the core lightweight while making it easy to add new runtimes or swap implementations.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;run&lt;/code&gt; supports 25+ languages:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Languages &amp;amp; aliases&lt;/cell&gt;
        &lt;cell role="head"&gt;Toolchain expectations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scripting &amp;amp; shells&lt;/cell&gt;
        &lt;cell&gt;Bash (&lt;code&gt;bash&lt;/code&gt;), Python (&lt;code&gt;py&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt;), Ruby (&lt;code&gt;rb&lt;/code&gt;, &lt;code&gt;ruby&lt;/code&gt;), PHP (&lt;code&gt;php&lt;/code&gt;), Perl (&lt;code&gt;perl&lt;/code&gt;), Lua (&lt;code&gt;lua&lt;/code&gt;), R (&lt;code&gt;r&lt;/code&gt;), Elixir (&lt;code&gt;ex&lt;/code&gt;, &lt;code&gt;elixir&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Matching interpreter on &lt;code&gt;PATH&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Web &amp;amp; typed scripting&lt;/cell&gt;
        &lt;cell&gt;JavaScript (&lt;code&gt;js&lt;/code&gt;, &lt;code&gt;node&lt;/code&gt;), TypeScript (&lt;code&gt;ts&lt;/code&gt;, &lt;code&gt;deno&lt;/code&gt;), Dart (&lt;code&gt;dart&lt;/code&gt;), Swift (&lt;code&gt;swift&lt;/code&gt;), Kotlin (&lt;code&gt;kt&lt;/code&gt;, &lt;code&gt;kotlin&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;node&lt;/code&gt;, &lt;code&gt;deno&lt;/code&gt;, &lt;code&gt;dart&lt;/code&gt;, &lt;code&gt;swift&lt;/code&gt;, &lt;code&gt;kotlinc&lt;/code&gt; + JRE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Systems &amp;amp; compiled&lt;/cell&gt;
        &lt;cell&gt;C (&lt;code&gt;c&lt;/code&gt;), C++ (&lt;code&gt;cpp&lt;/code&gt;, &lt;code&gt;cxx&lt;/code&gt;), Rust (&lt;code&gt;rs&lt;/code&gt;, &lt;code&gt;rust&lt;/code&gt;), Go (&lt;code&gt;go&lt;/code&gt;), Zig (&lt;code&gt;zig&lt;/code&gt;), Nim (&lt;code&gt;nim&lt;/code&gt;), Haskell (&lt;code&gt;hs&lt;/code&gt;, &lt;code&gt;haskell&lt;/code&gt;), Crystal (&lt;code&gt;cr&lt;/code&gt;, &lt;code&gt;crystal&lt;/code&gt;), C# (&lt;code&gt;cs&lt;/code&gt;, &lt;code&gt;csharp&lt;/code&gt;), Java (&lt;code&gt;java&lt;/code&gt;), Julia (&lt;code&gt;jl&lt;/code&gt;, &lt;code&gt;julia&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Respective compiler / toolchain&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Real programs live under the &lt;code&gt;examples/&lt;/code&gt; tree—each language has a &lt;code&gt;hello&lt;/code&gt; and a &lt;code&gt;progress&lt;/code&gt; scenario. The headers document expected output so you can diff your toolchain.&lt;/p&gt;
    &lt;code&gt;run examples/rust/hello.rs
run examples/typescript/progress.ts
run examples/python/counter.py&lt;/code&gt;
    &lt;p&gt;Being inside REPL we can use the ff commands&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:help&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List available meta commands&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:languages&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show detected engines and status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;:lang &amp;lt;id&amp;gt;&lt;/code&gt; or &lt;code&gt;:&amp;lt;alias&amp;gt;&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Switch the active language (&lt;code&gt;:py&lt;/code&gt;, &lt;code&gt;:go&lt;/code&gt;, …)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:detect on/off/toggle&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Control snippet language auto-detection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:load path/to/file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execute a file inside the current session&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:reset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Clear the accumulated session state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;:exit&lt;/code&gt; / &lt;code&gt;:quit&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Leave the REPL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Apache 2.0. See LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Built with ❤️ in Rust. If &lt;code&gt;run&lt;/code&gt; unblocks your workflow, star the repo and share it with other polyglot hackers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45475528</guid><pubDate>Sat, 04 Oct 2025 18:34:13 +0000</pubDate></item><item><title>ProofOfThought: LLM-based reasoning using Z3 theorem proving</title><link>https://github.com/DebarghaG/proofofthought</link><description>&lt;doc fingerprint="880be56c94434dea"&gt;
  &lt;main&gt;
    &lt;p&gt;LLM-based reasoning using Z3 theorem proving.&lt;/p&gt;
    &lt;code&gt;from openai import OpenAI
from z3dsl.reasoning import ProofOfThought

client = OpenAI(api_key="...")
pot = ProofOfThought(llm_client=client)

result = pot.query("Would Nancy Pelosi publicly denounce abortion?")
print(result.answer)  # False&lt;/code&gt;
    &lt;code&gt;from z3dsl.reasoning import EvaluationPipeline

evaluator = EvaluationPipeline(pot, output_dir="results/")
result = evaluator.evaluate(
    dataset="strategyqa_train.json",
    max_samples=10
)
print(f"Accuracy: {result.metrics.accuracy:.2%}")&lt;/code&gt;
    &lt;code&gt;pip install z3-solver openai scikit-learn numpy&lt;/code&gt;
    &lt;p&gt;The system has two layers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;High-level API (&lt;code&gt;z3dsl.reasoning&lt;/code&gt;) - Simple Python interface for reasoning tasks&lt;/item&gt;
      &lt;item&gt;Low-level DSL (&lt;code&gt;z3dsl&lt;/code&gt;) - JSON-based Z3 theorem prover interface&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most users should use the high-level API.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;examples/&lt;/code&gt; directory for complete examples including Azure OpenAI support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45475529</guid><pubDate>Sat, 04 Oct 2025 18:34:23 +0000</pubDate></item><item><title>Blog Feeds</title><link>https://blogfeeds.net</link><description>&lt;doc fingerprint="779f1fb99e6586b5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tired of social media?&lt;/p&gt;
    &lt;p&gt;Keep doom scrolling through addicting feeds?&lt;/p&gt;
    &lt;p&gt;Miss the days when the web was just about connecting with people and their thoughts or ideas?&lt;/p&gt;
    &lt;p&gt;We believe there's an answer to that problem, and it's called&lt;/p&gt;
    &lt;p&gt;Starting a blog is actually a lot simpler than what you're probably thinking. This doesn't have to be some well polished highly viewed monetization machine, or even something professional or formal. It's just a simple website where you can casually talk about whatever you want to talk about! It can be long, short, a list of small things, or just a quote. It should be how you talk with other people in your own life, or how you communicate with the outside world. It should be you on a page. Here's a few places you can make a blog that are RSS enabled:&lt;/p&gt;
    &lt;p&gt;RSS is actually already familiar to you if you have ever subscribed to a newsletter. You put your email into someoneâs website, and when they have updates, they send you emails to your inbox so you can stay in the loop. In the case of RSS, you have a dedicated app, called an RSS reader usually, and you can put in someoneâs website into the app to subscribe. When they make a new post, just open your news reader app, and their posts will be retrieved and ready to read. Some reader apps even let you make folders and tags to organize blogs you are subscribed to, similar to how an email app lets you make folders to sort mail. Would highly recommend trying a few of the apps or services and seeing which works best!&lt;/p&gt;
    &lt;p&gt;This takes us to our final point: Feeds. You can probably get away with just the first two items and then sharing it with people you already know, but what about meeting or talking to people you don't know? That's where Feeds come in. The idea is to create another page on your blog that has all the RSS feeds you're subscribed to. By keeping this public and always up to date, someone can visit your page, find someone new and follow them. Perhaps that person also has a feeds page, and the cycle continues until there is a natural and organic network of people all sharing with each other. So if you have a blog, consider making a feeds page and sharing it! If your RSS reader supports OPML file exports and imports, perhaps you can share that file as well to make it easier to share your feeds.&lt;/p&gt;
    &lt;p&gt;Here's an example Feeds Page which should help get the idea across!&lt;/p&gt;
    &lt;p&gt;The best part about blog feeds? It's just an idea. There's no central authority. There's no platform. No massive tech giant trying to take your data. It's just you, basic web standards, and the people you care about.&lt;/p&gt;
    &lt;p&gt;Made by Steve&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45475808</guid><pubDate>Sat, 04 Oct 2025 19:08:46 +0000</pubDate></item><item><title>Microsoft 365 Copilot is a commercial failure</title><link>https://www.perspectives.plus/p/microsoft-365-copilot-commercial-failure</link><description>&lt;doc fingerprint="2f0f3f3ebabae9f7"&gt;
  &lt;main&gt;
    &lt;p&gt;The numbers are in: no one is paying for Microsoft 365 Copilot.&lt;/p&gt;
    &lt;p&gt;No, not the official numbers designed by Microsoft to support their narrative. I’m talking about something that MS would prefer not to address as these numbers tell a whole different story. As a result, they are from a source that we cannot verify and therefore something that every reader needs to evaluate the trustworthiness of.&lt;/p&gt;
    &lt;p&gt;“A source that has seen materials related to sales has confirmed that, as of August 2025, Microsoft has around eight million active licensed users of Microsoft 365 Copilot, amounting to a 1.81% conversion rate across the 440 million Microsoft 365 subscribers.”&lt;/p&gt;
    &lt;p&gt;I know, Ed Zitron is hardly a neutral observer of the tech industry. He is doing something that almost no one else out there bothers to do, though. Which is checking whether the numbers released by tech vendors (especially OpenAI) make any sense. This is the foundation upon which his case against the financial validity of GenAI is built on, and being a regular reader of his premium newsletter issues, I find it fairly convincing.&lt;/p&gt;
    &lt;p&gt;So, when someone on the inside wants to find a channel to bring out the inconvenient truth about AI adoption, Ed is where they would likely reach out to. These numbers that have been disclosed about Microsoft’s commercial success (failure) of selling Copilot licenses are in line with those reported about the adoption rate of other paid AI plans from other vendors.&lt;/p&gt;
    &lt;p&gt;The story can be summarized as follows:&lt;/p&gt;
    &lt;p&gt;M365 Copilot became available for enterprise customers to purchase on November 1st, 2023. That was almost 2 years ago. Now, if we assume that the adoption rate is constant, the conversion rate mentioned by Ed could grow to around 2% by the time we reach November 2025.&lt;/p&gt;
    &lt;p&gt;The 2% adoption rate in 2 years is diabolically bad. This is not just any lil’ Power Apps product that was promoted as a new tool for citizen developers to improve personal productivity. It has been the centerpiece of everything Microsoft has done and talked about for over 2 years now. I have never seen a bigger push for any MS product.&lt;/p&gt;
    &lt;p&gt;After all this — what do we even have here? 8 million active licensed users. For some tech products that might be a sizeable user base. But let’s be real here: Microsoft has at least 400,000 channel partners. If each partner org would have bought on average 20 seats of Microsoft 365 Copilot for their employees, that would already make up the 8M figure.&lt;/p&gt;
    &lt;p&gt;Indeed, most partners have to pay for the M365 Copilot seats. There are hardly any freebies available in the partner benefits packages. I pay €800 a year for MS licenses in Partner Success Core and that doesn’t give me a single M365 Copilot seat. I have to pay the full price of €337 per year to get a chance to use the premium Copilot experience in my tenant for my solopreneur Power Platform advisory business.&lt;/p&gt;
    &lt;p&gt;This, of course, makes it easier to evaluate whether I get value from the license or not. While I acknowledge that I’m not the typical user persona for M365 Copilot, it has given me just a fraction of the value that my ChatGPT Plus subscription does. At a lower price point. Since I don’t have to sell and promote this product to customers, like many MS partners do, I’m free to tell everyone about this experience. As I’ve done in this newsletter for the past year on many, many occasions.&lt;/p&gt;
    &lt;p&gt;It could have been just me who’s a cranky old CRM consultant shouting at the AI cloud. Well, based on these adoption numbers for Microsoft 365 Copilot, I’m not in the minority. The vast majority of people out there who use Microsoft cloud tools don’t see Copilot as giving them enough value to justify the $30 per month cost.&lt;/p&gt;
    &lt;p&gt;Remember: even though these aren’t individual users buying things for themselves, we’re not talking about just another MS license. This is AI! It’s the thing that’s “no longer optional” at so many companies — thanks to all the AI CEOs forcing their employees to adopt it or else. We read about these initiatives from media of how business leaders want to transform their company via AI on a daily basis.&lt;/p&gt;
    &lt;p&gt;There must be loads of companies who have gone and ticked the AI box by choosing to purchase M365 Copilot licenses. And despite all this blind support from management, this is as far as Microsoft has gotten with the paid Copilot product sales.&lt;/p&gt;
    &lt;head rend="h1"&gt;Agents aren’t doing any better&lt;/head&gt;
    &lt;p&gt;Another interesting figure from Ed’s newsletter was about SharePoint agents:&lt;/p&gt;
    &lt;p&gt;“I’m also hearing that less than SharePoint — another popular enterprise app from Microsoft with 250 million users — had less than 300,000 weekly active users of its AI copilot features in August.”&lt;/p&gt;
    &lt;p&gt;Hmm, let’s see now. What other numbers do we have of agents that we could compare this with? Oh, right! The 3 million agents in FY25 story. Let’s revisit a screenshot I shared in my Numbers designed to please article a couple of months ago:&lt;/p&gt;
    &lt;p&gt;My theory (as well as Suhail’s) was that most of the 3M agents would be in SharePoint, and they wouldn’t be in production use. Weekly usage could well be considered a threshold for an AI agent being in production, so that would give us 10% when comparing it to the 300k figure in Ed’s newsletter.&lt;/p&gt;
    &lt;p&gt;But that was about users. Unlike Copilot, which is supposed to be a personal AI assistant, agents are meant to be tools that many individuals tap into with their Copilot. Now, let’s ignore for a moment the crazy fact that you can’t use SharePoint agents in the Microsoft 365 Copilot UI. Out of the 300 million users of SharePoint Online (I don’t know why Ed said only 250M), 0.1% of them interacted with an agent during a week in August 2025. One user out of a thousand.&lt;/p&gt;
    &lt;p&gt;Can you feel the AI transformation already?&lt;/p&gt;
    &lt;head rend="h1"&gt;Where does Microsoft go from here?&lt;/head&gt;
    &lt;p&gt;If this were a normal software product, the industry would have moved on already and invented something else to pitch to customers. In practice, it is anything but normal. GenAI is the only story left for Big Tech to tell when chasing for ever greater growth percentages. And yet the only thing growing are the capex investments.&lt;/p&gt;
    &lt;p&gt;Getting people to use AI services that they either want to (ChatGPT, all the AI coding assistants) or can’t avoid (Google search AI overviews, free Copilot features in MS products) is one thing. Making them voluntarily open their wallets and insert a credit card to purchase some of the AI magic dust is a whole different game. That’s the game where everyone has been losing so far, as no one besides NVidia or the IT consulting companies is making profits from GenAI at this point.&lt;/p&gt;
    &lt;p&gt;It’s not like all those GPUs could be left underutilized, though. The growth of especially the US economy relies on AI related activities. If they were to be removed, the bubble would burst. So, rolling back AI features is not an option, understandably. The only possibility is to reimagine the monetization model and hope that it works this time. And that’s exactly what Microsoft is actively pursuing.&lt;/p&gt;
    &lt;p&gt;First of all, the idea of bundling more of Copilot into the licenses customers already have is the obvious way to boost AI adoption. Which may not bring in any new money, yet will be a critical number for MS to show to investors. Who could soon begin questioning why the capex spend is necessary if the GPU utilization for Microsoft 365’s enterprise Copilot is barely scratching 60%.&lt;/p&gt;
    &lt;p&gt;Enter Copilot Chat in &lt;del&gt;Office&lt;/del&gt; Microsoft 365 apps. Announced on September 15, “Microsoft 365 Copilot Chat and agents are rolling out in Word, Excel, PowerPoint, Outlook, and OneNote for all users—creating a unified chat experience across the apps millions of people use every day at work.” What was once the main demo scenario of why contextual AI inside the business productivity apps is crucial for worker productivity is now something all 440 million M365 users get for no extra cost.&lt;/p&gt;
    &lt;p&gt;The nice side effect from this replacement of app specific Copilots with the generic Chat has been that premium M365 Copilot users seem to have lost all of the smarts that their apps used to have. Even M365 Copilot MVPs on social media are anxious about the update that makes Copilot responses dumber by default. I of course raised my concerns about the same useless AI assistants issue a couple of months ago already. I’m glad it’s not just me who is too dumb to get Copilot in Outlook to work with email and calendar.&lt;/p&gt;
    &lt;p&gt;Then, there’s the next push: vibe working. What has been disguised as a marketing campaign to ride on the popularity of the vibe coding concept is actually an indirect admittance of the fact that GPT based M365 Copilot has been a disappointment for everyone. That’s why Microsoft is now teaming up with OpenAI’s biggest competitor Anthropic and building its new Office Agent features on top of Claude:&lt;/p&gt;
    &lt;p&gt;“Today we’re introducing Office Agent, a multi-agent system that builds upon an open-source stack, Anthropic's Claude model, and a new taste-driven development (TDD) paradigm to deliver polished PowerPoint presentations, ready-to-use Word documents, and soon Excel spreadsheets.”&lt;/p&gt;
    &lt;p&gt;All of that is preview stuff, of course. When MS announced they were expanding model choice in M365 Copilot and Copilot Studio, all the Modern Work consultants on LinkedIn were quick to point out that enabling Claude breaks the MS trust barrier for customer data. Well, tech industry reporters have been talking about how Charles Lamanna &amp;amp; crew are embracing Claude for a while now, so I see it as only a matter of time before models like the new Claude Sonnet 4.5 are running on Azure in addition to AWS. Because otherwise the Office Agent won’t be as good with spreadsheets and docs like competing products that tap into the best LLM for the job.&lt;/p&gt;
    &lt;p&gt;How good is Office Agent then? Well, I did a quick test drive of the Agent Mode in Excel. Meaning, I installed the Excel Labs add-in that seems to not require any Frontier program sign-up, contrary to what MS documentation says (that Copilot Frontier switch was erroring out in M365 Admin center for me, so I said “f*** this” and tried the tools without it). Then, I gave it a task: “build me a 5-year investor-ready financial model for a SaaS startup.” Like a user of our FinModeler SaaS app might do.&lt;/p&gt;
    &lt;p&gt;What was the result? 12 minutes of work, timing out after endless verifications and “finalizing” steps. Some numbers and formulas were added to the Excel workbook, sure. Would I trust this to be the type of financial model that I’d use in finding investors for my business? Hell no. So, it’s definitely a “vibes” kind of an LLM output that looks neat but you can’t build on top of.&lt;/p&gt;
    &lt;p&gt;The introduction of all these agents that are not yet part of the official M365 Copilot product yet gives an indication of the direction where Microsoft is taking their AI narrative. After forcing consumers to pay for Copilot, redirecting the Office home page to AI chat instead, and making Copilot a toxic brand along the way, now’s the time to double down on agents. “Oh, Copilot, yeah that’s just the UI for AI, but to do anything useful with it you’ll need the latest agents.”&lt;/p&gt;
    &lt;p&gt;That would be perfectly in line with the upcoming Microsoft 365 Agent product announcement that’s supposed to take place at Ignite 2025. Given how quickly things change in the MS AI roulette table, it might of course not arrive in the format that Charles Lamanna was telling in the internal memo in August. But one thing that I’m increasingly confident of is that we will see a per-agent monetization model emerge. Simply because the per user M365 Copilot commercial model has failed so dramatically.&lt;/p&gt;
    &lt;p&gt;The latest announcement of “Bring Your Own Copilot” model with the new personal and family Microsoft 365 Premium SKU is not a sign of the premium value of Copilot. On the contrary, the basic AI features in Office apps are now such a commodity that MS is simply creating a new model to allow individual employees to sneak in a paid Copilot license via their personal MS account into work context inside M365 tenants. Since only 2% of the workers had premium Copilot licenses anyway, getting a bigger audience opening the AI tools through any means necessary is now on the table.&lt;/p&gt;
    &lt;p&gt;And so we get the “BYOAI” model, something that will irritate every single enterprise IT admin, of course. But the thing is: a personal M365 Premium won’t unlock any of the premium features like MS Graph grounding for Copilot. Which should lead everyone to asking: “what exactly is premium anymore”? The answers from this blog post aren’t very detailed nor convincing:&lt;/p&gt;
    &lt;p&gt;Q: What features are available with personal Copilot subscriptions?&lt;lb/&gt;A: Features like Researcher, Analyst, Photos Agent, and Actions may be available depending on the subscription tier. Additionally, users have access to the many in-app Copilot features when using the Microsoft 365 apps like rewrite, summarization, or discussion insights in Word; design suggestions and narrative builder in PowerPoint; and more.&lt;/p&gt;
    &lt;p&gt;“Copilot Actions?” You mean that Ignite 2024 keynote highlight that was deprecated and removed from M365 Copilot by May 2025 already? Because it was a half-assed attempt at chat driven automation, yet still resulted in a Microsoft 365 environment being provisioned for users of Copilot scheduled prompts today? Which now seem to be superseded with the recent Flow Builder agent that is actually Power Automate in disguise? Which is a product waiting to get reimagined, so that users wouldn’t think automation without AI was ever possible?&lt;/p&gt;
    &lt;p&gt;At this point, no one at Microsoft knows what their plan is — because they don’t have one. The masterplan of transforming becoming The Copilot Company, announced at Ignite 2023, has backfired commercially. Satya’s big bet is not working and now he is inviting Judson Althoff to be the CEO of all things commercial at MS while he focuses on the technical challenges of AI:&lt;/p&gt;
    &lt;p&gt;The announcement is framed as a “tectonic AI platform shift”. There isn’t any concrete context given on what specifically is not working in the current mode of operation at Microsoft, just the general notion that everyone at the company needs to do better in these unprecedented times. Aside from the key thing about Nadella making way to Althoff, it’s pretty shallow. Here’s how the internal memo ends:&lt;/p&gt;
    &lt;p&gt;This will also allow our engineering leaders and me to be laser focused on our highest ambition technical work—across our datacenter buildout, systems architecture, AI science, and product innovation—to lead with intensity and pace in this generational platform shift. Each one of us needs to be at our very best in terms of rapidly learning new skills, adopting new ways to work, and staying close to the metal to drive innovation across the entire stack!!&lt;/p&gt;
    &lt;p&gt;This isn’t just evolution, it’s reinvention, for each of us professionally and for Microsoft.&lt;/p&gt;
    &lt;p&gt;Satya&lt;/p&gt;
    &lt;p&gt;See that “this isn’t X, it’s Y” part there? Yup, that’s classic GPT writing. In practice, Satya Nadella isn’t even writing these announcements that are sent to all MS employees anymore. It’s all just AI talk. If would be much more honest if the memo would have been signed by Copilot instead.&lt;/p&gt;
    &lt;p&gt;While we wait for it to be reimagined as the CEO Agent, with Copilot as the UI for it, of course.&lt;/p&gt;
    &lt;head rend="h1"&gt;“This is not a failure!”&lt;/head&gt;
    &lt;p&gt;After posting this newsletter issue on LinkedIn, I got a lot of comments from Microsoft partners who disagreed with my interpretation of what the 8 million figure means. Hardly a surprise, given how much the business success of people selling and consulting M365 Copilot depends on the product being seen as something desirable.&lt;/p&gt;
    &lt;p&gt;I’m not saying all is lost for MS. My main point is that whatever they tried to pull off with the $30pupm M365 Copilot license backfired dramatically. Redmond will eventually “reimagine” things in a way that will let them put this failure behind them without ever actually addressing it. But we, the people working with Microsoft tech every day, deserve to know what happened and remember it the next time a similar stunt is played.&lt;/p&gt;
    &lt;p&gt;To illustrate my point, and give it a perspective that’s not just “my opinion with no facts whatsoever” (as some people on the professional network said), I asked Claude Sonnet 4.5 to crunch the same numbers. To do an analysis of the news sources and factors at play with M365 Copilot as a commercial offering, then evaluating whether it was success or spin:&lt;/p&gt;
    &lt;p&gt;You can open the interactive Claude artifact yourself and answer a few questions in the quiz to see how your interpretation of Microsoft 365 Copilot commercial success aligns with what Claude thinks. It’s just a fun vibe coded thing, not to be taken too seriously, of course. Here’s my quick intro video of it:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45476045</guid><pubDate>Sat, 04 Oct 2025 19:39:00 +0000</pubDate></item><item><title>The UK is still trying to backdoor encryption for Apple users</title><link>https://www.eff.org/deeplinks/2025/10/uk-still-trying-backdoor-encryption-apple-users</link><description>&lt;doc fingerprint="8f988c715aa8d158"&gt;
  &lt;main&gt;
    &lt;p&gt;The Financial Times reports that the U.K. is once again demanding that Apple create a backdoor into its encrypted backup services. The only change since the last time they demanded this is that the order is allegedly limited to only apply to British users. That doesn’t make it any better.&lt;/p&gt;
    &lt;p&gt;The demand uses a power called a “Technical Capability Notice” (TCN) in the U.K.’s Investigatory Powers Act. At the time of its signing we noted this law would likely be used to demand Apple spy on its users. &lt;/p&gt;
    &lt;p&gt;After the U.K. government first issued the TCN in January, Apple was forced to either create a backdoor or block its Advanced Data Protection feature—which turns on end-to-end encryption for iCloud—for all U.K. users. The company decided to remove the feature in the U.K. instead of creating the backdoor.&lt;/p&gt;
    &lt;p&gt;The initial order from January targeted the data of all Apple users. In August, the US claimed the U.K. withdrew the demand, but Apple did not re-enable Advanced Data Protection. The new order provides insight into why: the U.K. was just rewriting it to only apply to British users. &lt;/p&gt;
    &lt;p&gt;This is still an unsettling overreach that makes U.K. users less safe and less free. As we’ve said time and time again, any backdoor built for the government puts everyone at greater risk of hacking, identity theft, and fraud. It sets a dangerous precedent to demand similar data from other companies, and provides a runway for other authoritarian governments to issue comparable orders. The news of continued server-side access to users' data comes just days after the UK government announced an intrusive mandatory digital ID scheme, framed as a measure against illegal migration.&lt;/p&gt;
    &lt;p&gt;A tribunal hearing was initially set to take place in January 2026, though it’s currently unclear if that will proceed or if the new order changes the legal process. Apple must continue to refuse these types of backdoors. Breaking end-to-end encryption for one country breaks it for everyone. These repeated attempts to weaken encryption violates fundamental human rights and destroys our right to private spaces.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45476273</guid><pubDate>Sat, 04 Oct 2025 20:07:11 +0000</pubDate></item><item><title>$912 energy independence without red tape</title><link>https://sunboxlabs.com/</link><description>&lt;doc fingerprint="b358d976fe9abd02"&gt;
  &lt;main&gt;
    &lt;p&gt;Cables / Tender:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://amzn.to/43NkcuJ - $14.79&lt;/item&gt;
      &lt;item&gt;https://amzn.to/4cwxY8Y - $30.42&lt;/item&gt;
      &lt;item&gt;https://amzn.to/4cuS3fx - $13.00&lt;/item&gt;
      &lt;item&gt;https://amzn.to/43u3ikz x2 - $17.00&lt;/item&gt;
      &lt;item&gt;https://amzn.to/43u3r7B - $12.99&lt;/item&gt;
      &lt;item&gt;https://amzn.to/43y5Qhx - $8.89&lt;/item&gt;
      &lt;item&gt;Total: $114.09&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;=&amp;gt; $912 total&lt;/p&gt;
    &lt;p&gt;Remote tracking (optional):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://amzn.to/3Vs3COI - $7.99&lt;/item&gt;
      &lt;item&gt;https://amzn.to/3TPaXGI - $23.92&lt;/item&gt;
      &lt;item&gt;https://amzn.to/4a6UMdI - $6.99&lt;/item&gt;
      &lt;item&gt;https://solar-assistant.io - $55.83&lt;/item&gt;
      &lt;item&gt;Total (additional): $94.73&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How to guide&lt;/head&gt;
    &lt;p&gt;Coming soon, for now refer to Will Prowse’s wiring guide on his very-similar 48V 3000W off-grid solar system which I followed and works great for me!&lt;/p&gt;
    &lt;head rend="h3"&gt;Financial Payback &amp;amp; Embodied Energy&lt;/head&gt;
    &lt;code&gt;Financial payback period for 3000W
System cost : $1,124 on Amazon in 2024 (now $912)
Yearly energy creation: 365d * 4.26hsun/d * 1.280kW = 2,000kWh/y (but more like 1,000kWh/year after losses)
Yearly value creation: 1,000kWh/y * $0.55/kWh in SF = $550/y energy created
100W system payback period: $1,124 / $550 = 2 years until payback
&lt;/code&gt;
    &lt;head rend="h3"&gt;How green is it:&lt;/head&gt;
    &lt;p&gt;Production footprint PV (source): &lt;code&gt;2,900kWhee/kW * 1.28kW = 3,712kWh embodied energy&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Production footprint LiFePo4 battery (source): &lt;code&gt;106kWhee/kWh * 2.4kWh = 254kWh embodied energy&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Annual energy production system: 1100kWh/y &lt;code&gt;Payback period: 3966kWh / 1100kWh/y = 3.5 year footprint payback&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;What’s the catch? Seems to good to be true? Well, this thing sits between your devices and the wall. So you need to neatly run extension cables from every room in the house to the “sun box”, and then run one cable from the box to the panels, and another to the wall (optional, just so it can fall back to pulling power from the wall). Photos of this are coming soon.&lt;/p&gt;
    &lt;p&gt;Will it ever push power back into the wall? Nope! It’ll only ever draw from the wall in the event that both the sun is down and the battery is dead (so your fridge won’t go off overnight for example).&lt;/p&gt;
    &lt;p&gt;Is this legal? Yes, see above. No difference to plugging your fridge into your wall, as far as the utility is concerned.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45476820</guid><pubDate>Sat, 04 Oct 2025 21:22:05 +0000</pubDate></item><item><title>Matrix Core Programming on AMD GPUs</title><link>https://salykova.github.io/matrix-cores-cdna</link><description>&lt;doc fingerprint="179c1252016d1d30"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Matrix Core Programming on AMD CDNA3 and CDNA4 architecture&lt;/head&gt;
    &lt;p&gt;TL;DR In this blog post, we walk through how to use Matrix Cores in HIP kernels, with a focus on low-precision data types such as FP16, FP8, and FP4, as well as the new family of Matrix Core instructions with exponent block scaling introduced in the AMD CDNA™4 architecture. Through code examples and illustrations, we provide the necessary knowledge to start programming Matrix Cores, covering modern low-precision floating-point types, the Matrix Core compiler intrinsics, and the data layouts required by the Matrix Core instructions. The blog post is also available on ROCm Blogs.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Matrix Cores&lt;/head&gt;
    &lt;p&gt;Matrix multiplication is an essential part of AI and HPC workloads. The AMD CDNA™ architecture features special-purpose hardware, the Matrix Cores, to accelerate matrix fused-multiply-add (MFMA) operations defined as &lt;code&gt;D:=A*B+C&lt;/code&gt;. Please note that MFMA instructions are often used to update a matrix in-place (=accumulation) so that &lt;code&gt;D=C&lt;/code&gt; and &lt;code&gt;C:=A*B+C&lt;/code&gt;. The matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are called input matrices, while the matrix &lt;code&gt;D&lt;/code&gt; is referred to as the output matrix or accumulator.&lt;/p&gt;
    &lt;p&gt;The performance gains from using Matrix Cores are especially significant in mixed-precision mode, where the input matrices use lower-precision data types instead of FP32. The output matrix, however, is stored in FP32 to minimize accuracy loss during accumulation. The tables below show the theoretical peak performance of Matrix Cores with different input data types on both AMD CDNA™3 and AMD CDNA™4 architectures. On the AMD Instinct™ MI325X, using FP16 input matrices delivers nearly an 8x performance increase compared to single-precision, with only minimal accuracy loss. Switching to FP8 further doubles the performance providing a 16x increase when compared to FP32. The AMD CDNA™4 architecture further improves Matrix Core performance, delivering up to 2x higher throughput for FP16 and FP8 compared to the AMD CDNA™3 architecture. In addition, AMD CDNA™4 introduces new low-precision data types such as FP6 and FP4, enabling up to 64x performance gain relative to FP32. Please refer to the AMD CDNA™3 and AMD CDNA™4 white papers for detailed architecture specifications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI325X (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;1307.4 TF&lt;/cell&gt;
        &lt;cell&gt;~8x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;2614.9 TF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI355X (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;78.6 TF&lt;/cell&gt;
        &lt;cell&gt;~0.5x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;157.3 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;2.5 PF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;5 PF&lt;/cell&gt;
        &lt;cell&gt;~32x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP6&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP4&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;2. Low-Precision Floating-Point Types&lt;/head&gt;
    &lt;p&gt;A binary representation of a floating-point number consists of &lt;code&gt;n&lt;/code&gt; bits, where &lt;code&gt;m&lt;/code&gt; of &lt;code&gt;n&lt;/code&gt; bits represent the mantissa, 1 bit determines the sign and &lt;code&gt;n-m-1&lt;/code&gt; bits represent the exponent. The following image illustrates the binary format of a floating-point number and how the exponent and mantissa are calculated based on its binary representation.&lt;/p&gt;
    &lt;p&gt;Figure 1: Binary representation of a floating-point number.&lt;/p&gt;
    &lt;p&gt;Floating-point types are characterized by the number of bits used for the exponent and for the mantissa. Increasing the exponent width extends the range of representable values, while increasing the mantissa width improves precision. Since all floating-point types include the sign bit, a shorthand notation typically specifies only the exponent and mantissa widths. For example, the E4M3 type is an 8-bit floating-point type with 4-bit exponent and 3-bit mantissa. Additionally, a floating-point type is specified by exponent bias - a number that is subtracted from the exponent during conversion from binary format to real value. Given the exponent width, mantissa width, and exponent bias, one can convert the binary representation of a floating-point type (except E8M0) into its real value using the following equation:&lt;/p&gt;
    &lt;p&gt;Figure 2: Conversion to real value from binary representation for floating-point numbers.&lt;/p&gt;
    &lt;p&gt;Please note that the equation takes different forms depending on whether the exponent is zero or not. Often, certain exponent and mantissa values are reserved for special values (e.g. &lt;code&gt;NaN&lt;/code&gt;, &lt;code&gt;Infinity&lt;/code&gt;), which limits the range of representable real numbers. For example, the FP16 type has 5-bit exponent with a nominal range of &lt;code&gt;[0, 1, ... 2^5-1] = [0, 1, ... 31]&lt;/code&gt;. However, the exponent value &lt;code&gt;E = 31&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; (if the mantissa &lt;code&gt;M != 0&lt;/code&gt;) and &lt;code&gt;infinity&lt;/code&gt; (if the mantissa &lt;code&gt;M = 0&lt;/code&gt;). Therefore, the largest exponent value that can represent a real number is &lt;code&gt;E = 30&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The following table summarizes low-precision types commonly used in modern AI/ML workloads:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Shorthand&lt;/cell&gt;
        &lt;cell role="head"&gt;Exp. bias&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Zero&lt;/cell&gt;
        &lt;cell role="head"&gt;NaN&lt;/cell&gt;
        &lt;cell role="head"&gt;Infinity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;16-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M10 (FP16)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±65504&lt;/cell&gt;
        &lt;cell&gt;S 00000 0000000000&lt;/cell&gt;
        &lt;cell&gt;S 11111 xxxxxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111 0000000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M7 (BF16)&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;±3.3895 * 10^38&lt;/cell&gt;
        &lt;cell&gt;S 00000000 0000000&lt;/cell&gt;
        &lt;cell&gt;S 11111111 xxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111111 0000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;8-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FN (FP8, OCP)&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;±448&lt;/cell&gt;
        &lt;cell&gt;S 0000 000&lt;/cell&gt;
        &lt;cell&gt;S 1111 111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FNUZ (FP8)&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;±240&lt;/cell&gt;
        &lt;cell&gt;0 0000 000&lt;/cell&gt;
        &lt;cell&gt;1 0000 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2 (BF8, OCP)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 11111 {01, 10 11}&lt;/cell&gt;
        &lt;cell&gt;S 11111 00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2FNUZ (BF8)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;0 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M0&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;2^(±127)&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;11111111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;6-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E2M3&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±7.5&lt;/cell&gt;
        &lt;cell&gt;S 00 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E3M2 (BF6)&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;±28&lt;/cell&gt;
        &lt;cell&gt;S 000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;4-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;E2M1 (FP4)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±6&lt;/cell&gt;
        &lt;cell&gt;S 00 0&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the E4M3 type has two variants: E4M3FN and E4M3FNUZ. Both E4M3FN and E4M3FNUZ use 4 bits for the exponent and 3 bits for the mantissa. They use different exponent biases and differ in the special values they can represent. Neither variant supports infinities, which is why their notations include FN (FiNite). However, E4M3FN supports &lt;code&gt;+0&lt;/code&gt;, &lt;code&gt;-0&lt;/code&gt;, &lt;code&gt;+NaN&lt;/code&gt; and &lt;code&gt;-Nan&lt;/code&gt;, while E4M3FNUZ supports only &lt;code&gt;+0&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt;, hence &lt;code&gt;UZ&lt;/code&gt; (Unsigned Zero). The image below demonstrates how to convert a binary sequence into a real value, using E4M3FNUZ type as an example:&lt;/p&gt;
    &lt;p&gt;Figure 3: E4M3FNUZ encoding details.&lt;/p&gt;
    &lt;p&gt;FP8 types are divided into E4M3 and E5M2 formats. The E5M2 format is sometimes referred to as BF8, similar to BF16, where exponent width is larger compared to FP16. Similar to E4M3, E5M2 is further subdivided into two variants: E5M2 (OCP) and E5M2FNUZ. The AMD CDNA™3 architecture uses FNUZ variants for both E4M3 and E5M2, whereas the CDNA™4 architecture uses E4M3FN and E5M2 (OCP) variants. E4M3FN and E5M2 are standardized formats defined by the Open Compute Project (OCP). For detailed specifications, see the OCP Microscaling Formats (MX) Specification and the ONNX documentation. For visualization of FP8 values and their binary representations please refer to the FP8 Data table. Additionally, see the chapter “Low-precision floating-point types” in the AMD ROCm™ documentation for details on using low-precision types in HIP.&lt;/p&gt;
    &lt;p&gt;There is a special 8-bit format, E8M0, which is not used as a standard element data type but instead serves as a scale factor for microscaling types and block-scaled MFMA operations (discussed later in this article). Its value is calculated according to the equation below:&lt;/p&gt;
    &lt;p&gt;Figure 4: E8M0 encoding details.&lt;/p&gt;
    &lt;p&gt;The exponent value &lt;code&gt;E = 255&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; values, limiting the range of representable real numbers to &lt;code&gt;[2^-127 ... 2^127]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similar to FP8, FP6 has two formats: E2M3 and E3M2. The latter, E3M2, is often referred to as BF6 due to its larger exponent width compared to E2M3.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Matrix fused-multiply-add (MFMA) Instructions&lt;/head&gt;
    &lt;p&gt;The AMD CDNA™3 and CDNA™4 architectures support a variety of MFMA operations, which are characterized by the matrix dimensions &lt;code&gt;M&lt;/code&gt;, &lt;code&gt;N&lt;/code&gt;, &lt;code&gt;K&lt;/code&gt; and the data type of input/output matrices. The following table lists all available floating-point MFMA instructions for the AMD CDNA™3 and CDNA™4 architectures. As can be seen from the table, the AMD CDNA™4 architecture extends the set of available MFMA instructions by adding new FP16/BF16 instructions with larger matrix dimensions. Furthermore, it introduces FP6/FP4 data types and provides a completely new set of FP8/FP6/FP4 instructions where the types can be independently used for the matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. Finally, the AMD CDNA™4 architecture enables MFMA with block exponent scaling.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Type (C,D) ← (A,B)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;Note&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP64 ← FP64&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP32&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP16 (BF16)&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;Both A and B are either FP16 or BF16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;FP8 (E4M3) or BF8 (E5M2) can be used independently for A and B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8/FP6/FP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← MXFP8/MXFP6/MXFP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the table lists only floating-point type MFMA instructions with batch size = 1. In addition to them, the AMD CDNA™3 and CDNA™4 architectures support batched MFMA operations, where multiple output matrices are computed in parallel. These instructions are not covered in this article. See the Chapter 7 “Matrix Arithmetic Instructions” in the AMD CDNA™3 and AMD CDNA™4 ISA reference guides for the full list of available MFMA instructions.&lt;/p&gt;
    &lt;p&gt;The table above specifies cycle count for each MFMA operation. Given a known cycle count, one can estimate theoretical peak performance in TFLOP/s of corresponding MFMA operation using the formula below:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;
2*M*N*K * num_matrix_cores * (max_engine_clock / cycle_count) / 10^6,
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;num_matrix_cores&lt;/code&gt;is total number of matrix cores in a GPU (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_engine_clock&lt;/code&gt;is max engine clock (peak) in MHz (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cycle_count&lt;/code&gt;is cycle count of corresponding MFMA instruction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M, N, K&lt;/code&gt;are matrix dimensions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using this formula and the MFMA instruction &lt;code&gt;32x32x8 FP16&lt;/code&gt; as an example, we can estimate theoretical peak FP16 Matrix Core performance on the AMD Instinct™ MI325X:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;2*32*32*8 * 1216 * (2100 / 32) / 10^6 = 1307.4 TFLOP/s&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;To use Matrix Core instructions in HIP kernels, LLVM provides built-in compiler intrinsic functions. The list of all available compiler intrinsics can be found in the LLVM Github repository. The syntax of the MFMA intrinsics has the following format:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;d_reg = __builtin_amdgcn_mfma_ODType_MxNxKInDType(a_reg, b_reg, c_reg, cbsz, abid, blgp)&lt;/code&gt;,&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies the shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ODType&lt;/code&gt;is data type of the matrices&lt;code&gt;C&lt;/code&gt;and&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;InDType&lt;/code&gt;is data type of the input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cbsz&lt;/code&gt;,&lt;code&gt;abid&lt;/code&gt;,&lt;code&gt;blgp&lt;/code&gt;are broadcast flags. For the following discussion, these flags are irrelevant and are, therefore, set to 0 by default, unless specified otherwise. Please refer to the ISA reference guide for detailed information on the broadcast flags.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example,&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_16x16x16f16&lt;/code&gt;performs&lt;code&gt;16x16x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP16&lt;/code&gt;and the output matrix has type&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the output matrix is stored in&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_bf8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where the matrix&lt;code&gt;A&lt;/code&gt;has type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the matrix&lt;code&gt;B&lt;/code&gt;has type&lt;code&gt;BF8(E5M2)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MFMA instructions are wavefront-level (warp-level) instructions, where all work-items (threads) within a wavefront collectively perform a single MFMA operation and the operands &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt; are distributed across work-items so that each work-item in the wavefront holds a portion of the operands. In order to use the MFMA instructions, it’s required to understand how the operands are distributed across threads within a wavefront. The ISA reference guide fully specifies the data layout for all available MFMA instructions. For illustrative purposes, the next chapter explains a subset of the MFMA instructions and the corresponding data layouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Examples&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Important note: In the following discussion we assume the matrices are stored in row-major order. The wavefront size on the AMD CDNA™ architecture is 64. The shapes of the matrices&lt;/p&gt;&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;are&lt;code&gt;MxK&lt;/code&gt;,&lt;code&gt;KxN&lt;/code&gt;,&lt;code&gt;MxN&lt;/code&gt;, and&lt;code&gt;MxN&lt;/code&gt;, respectively. The first dimension denotes the number of rows and the second dimension the number of columns in a matrix. For example, the matrix&lt;code&gt;A&lt;/code&gt;has&lt;code&gt;M&lt;/code&gt;rows and&lt;code&gt;K&lt;/code&gt;columns.&lt;/quote&gt;
    &lt;head rend="h3"&gt;5.1. __builtin_amdgcn_mfma_f32_32x32x2f32&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x2&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;2x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input and output matrices are FP32. Since threads within a wavefront collectively perform single MFMA instruction, the operands are distributed across the threads. Each thread stores&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;M * K / wavefront_size = 32 * 2 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;A&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;K * N / wavefront_size = 2 * 32 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;B&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M * N / wavefront_size = 32 * 32 / 64 = 16&lt;/code&gt;entries of the matrix&lt;code&gt;C&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The operands are distributed according to the scheme below. The matrix elements highlighted in light red are those stored by the thread with index &lt;code&gt;0&lt;/code&gt; within the wavefront.&lt;/p&gt;
    &lt;p&gt;Figure 5: Data layout for `__builtin_amdgcn_mfma_f32_32x32x2f32`. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below demonstrates how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;

using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x2_fp32(const float* A, const float* B, float* C) {
    float a_reg;
    float b_reg;
    fp32x16_t c_reg {};

    const float* ldg_a_ptr = A + threadIdx.x / 32 + 2 * (threadIdx.x % 32);
    const float* ldg_b_ptr = B + threadIdx.x % 32 + (threadIdx.x / 32) * 32;

    a_reg = *ldg_a_ptr;
    b_reg = *ldg_b_ptr;

    c_reg = __builtin_amdgcn_mfma_f32_32x32x2f32(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;The GPU kernel can then be invoked on the host using a single wavefront:&lt;/p&gt;
    &lt;code&gt;mfma_fp32_32x32x2_fp32&amp;lt;&amp;lt;&amp;lt;1, 64&amp;gt;&amp;gt;&amp;gt;(A_device, B_device, C_device);
&lt;/code&gt;
    &lt;p&gt;Please note that we use the vector data type &lt;code&gt;fp32x16_t&lt;/code&gt; to store the entries of the matrix &lt;code&gt;C&lt;/code&gt; in registers. Additionally, we zero-initialize &lt;code&gt;c&lt;/code&gt;, since we compute &lt;code&gt;C = A * B&lt;/code&gt; without accumulation.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.2. __builtin_amdgcn_mfma_f32_16x16x16f16&lt;/head&gt;
    &lt;p&gt;This example demonstrates how to multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;16x16&lt;/code&gt;. The input matrices are stored in FP16 and the output matrix stored in FP32. In this case, each thread stores &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for this instruction is shown below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 6: Data layout for __builtin_amdgcn_mfma_f32_16x16x16f16. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;Corresponding HIP kernel is implemented below:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp16.h&amp;gt;

using fp16_t = _Float16;
using fp16x4_t = __attribute__((vector_size(4 * sizeof(fp16_t)))) fp16_t;
using fp32x4_t = __attribute__((vector_size(4 * sizeof(float)))) float;

__global__ void
mfma_fp32_16x16x16_fp16(const fp16_t* A, const fp16_t* B, float* C) {

    fp16x4_t a_reg;
    fp16x4_t b_reg;
    fp32x4_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp16x4_t*&amp;gt;(A + 4 * (threadIdx.x / 16) + 16 * (threadIdx.x % 16));

    for (int i = 0; i &amp;lt; 4; i++) {
        b_reg[i] = *(B + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64);
    }

    c_reg = __builtin_amdgcn_mfma_f32_16x16x16f16(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        *(C + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64) = c_reg[i];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that both &lt;code&gt;__half&lt;/code&gt; and &lt;code&gt;_Float16&lt;/code&gt; types can be used in device code. However, the host supports only &lt;code&gt;_Float16&lt;/code&gt; type for arithmetic operations. As in the previous example, we use vector data types to store the matrix elements in registers.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.3. __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input matrices are stored in FP8 and the output matrix is stored in FP32. In this scenario, each thread stores &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; elements of the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 7: Data layout for __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below implements this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp8.h&amp;gt;

using fp8_t = __hip_fp8_storage_t;
using fp8x8_t = __attribute__((vector_size(8 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x16_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x8_t a_reg;
    fp8x8_t b_reg;
    fp32x16_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp8x8_t*&amp;gt;(A + (threadIdx.x / 32) * 8 + (threadIdx.x % 32) * 16);

    for (int i = 0; i &amp;lt; 8; i++) {
        b_reg[i] = *(B + i * 32 + threadIdx.x % 32 + (threadIdx.x / 32) * 8 * 32);
    }

    c_reg = __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8((long)a_reg, (long)b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;To define FP8, we use &lt;code&gt;__hip_fp8_storage_t&lt;/code&gt; type from &lt;code&gt;hip_fp8.h&lt;/code&gt;. Note that the intrinsic function expects its first two operands to be of type &lt;code&gt;long&lt;/code&gt;. To compile the code, the operands &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are, therefore, converted to &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.4. __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f8&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Important note: the MFMA instruction discussed in this example is supported only on AMD CDNA™4 GPUs (gfx950). Please make sure to install AMD ROCm™ version 7.0 or later.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The AMD CDNA™4 architecture introduces a new family of MFMA instructions with block exponent scaling. The syntax of these instructions differs from the classic MFMA compiler intrinsics:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;d_reg = __builtin_amdgcn_mfma_scale_f32_MxNxK_f8f6f4(a_reg, b_reg, c_reg, Atype, Btype, OPSEL_A, scale_a, OPSEL_B, scale_b)&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Atype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;A&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Btype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;B&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OPSEL_A&lt;/code&gt;,&lt;code&gt;OPSEL_B&lt;/code&gt;are OPSEL codes. These arguments are not relevant for the discussion and therefore will be set to&lt;code&gt;0&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scale_a&lt;/code&gt;,&lt;code&gt;scale_b&lt;/code&gt;are scalars / vectors containing scale factors of type&lt;code&gt;E8M0&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As an example, let’s take a closer look at the instruction &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt;. The inputs to this instruction are&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix &lt;code&gt;A&lt;/code&gt;of size&lt;code&gt;32x64&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Ax&lt;/code&gt;of size&lt;code&gt;32x2&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;B&lt;/code&gt;of size&lt;code&gt;64x32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Bx&lt;/code&gt;of size&lt;code&gt;2x32&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. Specifically, this instruction performs the following operation using single wavefront (64 threads):&lt;/p&gt;
    &lt;p&gt;Figure 8: Block-scaled matrix multiplication via __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4.&lt;/p&gt;
    &lt;p&gt;During dot product operations, the scales &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; are applied after the normal dot product and prior to output/accumulation.&lt;/p&gt;
    &lt;p&gt;In this example, we will multiply two FP8 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. The input matrices &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; are stored in FP8 format, while the output matrix is stored in FP32. The scale matrices &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; contain elements of type &lt;code&gt;E8M0&lt;/code&gt;. Each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. Please note that this scheme is valid only if both input matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; have FP8 type. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 9: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP8 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The following code example shows how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp8_t = __amd_fp8_storage_t;
using fp8x32_t = __attribute__((vector_size(32 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x32_t a_reg;
    fp8x32_t b_reg;
    fp32x16_t c_reg {};

    const fp8_t* ldg_a = A + (threadIdx.x % 32) * 64 + (threadIdx.x / 32) * 16;
    for (int i=0; i &amp;lt; 2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            a_reg[i*16 + j] = *(ldg_a + i * 32 + j);
        }
    }

    const fp8_t* ldg_b = B + threadIdx.x % 32 + 32 * 16 * (threadIdx.x / 32);

    for (int i=0; i&amp;lt;2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            b_reg[i*16 + j] = *(ldg_b + 32 * j + i * 32 * 32);
        }
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 0, 0, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that in this example we use &lt;code&gt;__amd_fp8_storage_t&lt;/code&gt; type defined in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt; to represent FP8. This library provides extensions APIs for low-precision and micro-scaling formats, and compared to &lt;code&gt;hip_fp8.h&lt;/code&gt;, exposes a wider capability set. &lt;code&gt;gfx950&lt;/code&gt; provides hardware acceleration for these APIs. Most of the APIs are 1 to 1 mapping of hardware instruction. Additionally, we use &lt;code&gt;uint8_t&lt;/code&gt; type to represent &lt;code&gt;E8M0&lt;/code&gt; scale factors. Since &lt;code&gt;scale_a&lt;/code&gt; and &lt;code&gt;scale_b&lt;/code&gt; encode exponent values, the corresponding actual scale factors are &lt;code&gt;2^(scale_a - 127)&lt;/code&gt; and &lt;code&gt;2^(scale_b - 127)&lt;/code&gt;. If &lt;code&gt;scale_a = scale_b = 127&lt;/code&gt;, the actual scale factors are equal to &lt;code&gt;1&lt;/code&gt; and no scaling is applied.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.5. __builtin_amdgcn_mfma_scale_f32_32x32x64_f4f4&lt;/head&gt;
    &lt;p&gt;In our last example, we demonstrate how to multiply two FP4 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. As in the previous example, each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for the output matrix remains the same as in the FP8 case. However, the data layout for the input matrices is different and depicted below. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 10: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP4 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code snippet below demonstrates how to implement this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp4x2_t = __amd_fp4x2_storage_t;
using fp4x64_t  = fp4x2_t __attribute__((ext_vector_type(32)));
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp4_fp4(const fp4x2_t* A, const fp4x2_t* B, float* C) {

    fp4x64_t a_reg {};
    fp4x64_t b_reg {};
    fp32x16_t c_reg {};

    const fp4x2_t* ldg_a = A + (threadIdx.x % 32) * 32 + (threadIdx.x / 32) * 16;

    for (int i = 0; i &amp;lt; 16; i++) {
        a_reg[i] = *(ldg_a + i);
    }

    const fp4x2_t* ldg_b = B + (threadIdx.x % 32) / 2 + 16 * 32 * (threadIdx.x / 32);
    int b_extract_idx = threadIdx.x % 2;

    for (int i = 0; i &amp;lt; 16; i++) {
        uint8_t tmp0 = __amd_extract_fp4(*(ldg_b + 16 * 2 * i), b_extract_idx);
        uint8_t tmp1 = __amd_extract_fp4(*(ldg_b + 16 * (2 * i + 1)), b_extract_idx);
        b_reg[i] = __amd_create_fp4x2(tmp0, tmp1);
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 4, 4, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Since memory addressing is not allowed at a granularity smaller than 8 bits, we use &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; (an alias for &lt;code&gt;uint8_t&lt;/code&gt;) to store the input matrices and enable pointer operations. Note that the FP4 elements that need to be loaded from the matrix &lt;code&gt;B&lt;/code&gt; are not contiguous in memory. To extract a single FP4 element, we use the &lt;code&gt;__amd_extract_fp4&lt;/code&gt; function provided in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt;. This function returns one FP4 element (of type &lt;code&gt;uint8_t&lt;/code&gt;) from a fp4x2 vector, based on the index passed as the second argument:&lt;/p&gt;
    &lt;code&gt;uint8_t __amd_extract_fp4(const __amd_fp4x2_storage_t x, const size_t index) {
    if (index == 0) return (x &amp;amp; 0xFu);
    return (x &amp;gt;&amp;gt; 4);
}
&lt;/code&gt;
    &lt;p&gt;Two FP4 values are then combined into &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; using &lt;code&gt;__amd_create_fp4x2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;__amd_fp4x2_storage_t __amd_create_fp4x2(const uint8_t x, const uint8_t y) {
    __amd_fp4x2_storage_t ret = 0;
    ret = x | (y &amp;lt;&amp;lt; 4);
    return ret;
}
&lt;/code&gt;
    &lt;p&gt;The compiler intrinsic function &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; requires its first two arguments to be 256 bits wide. Since 32 FP4 elements occupy only 128 bits, we define &lt;code&gt;fp4x64_t&lt;/code&gt;, which is 256 bits wide. In this type, 128 bits contain data, while the remaining 128 bits are zero. This allows us to pass &lt;code&gt;a_reg&lt;/code&gt; and &lt;code&gt;b_reg&lt;/code&gt; to the intrinsic function and compile the code successfully.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;In this article, we introduced Matrix Core instructions available on the AMD CDNA™3 and CDNA™4 architectures. We covered floating-point formats in detail, including modern low-precision element data types such as FP8, FP6, FP4, and the scale data type E8M0. We further explained how the floating-point types are represented as binary sequences and demonstrated, with concrete examples, how to convert their binary representations into real values. Next, we listed Matrix Core instructions supported by the modern CDNA™ architectures and discussed how to calculate the theoretical peak performance of Matrix Cores for specific MFMA instructions. To make the discussion more practical, we reviewed the compiler intrinsic functions that allow users to program Matrix Cores inside HIP kernels. Finally, we examined a subset of MFMA instructions in detail, providing code examples and illustrations to explain data layout and demonstrate how to implement simple mixed-precision MFMA operations in HIP. For additional information on Matrix Cores and low-precision data types, please refer to the following resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45476821</guid><pubDate>Sat, 04 Oct 2025 21:22:11 +0000</pubDate></item><item><title>XiangShan Vector Floating-Point Unit Design</title><link>https://docs.xiangshan.cc/projects/design/en/latest/backend/VFPU/</link><description>&lt;doc fingerprint="4631822b103a94a4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VFPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Version: V2R2&lt;/item&gt;
      &lt;item&gt;Status: OK&lt;/item&gt;
      &lt;item&gt;Date: 2025/01/20&lt;/item&gt;
      &lt;item&gt;commit：xxx&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Glossary of Terms&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Abbreviation&lt;/cell&gt;
        &lt;cell role="head"&gt;Full name&lt;/cell&gt;
        &lt;cell role="head"&gt;Descrption&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VFPU&lt;/cell&gt;
        &lt;cell&gt;Vector Floating-Point Unit&lt;/cell&gt;
        &lt;cell&gt;Vector Floating-Point Functional Unit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;IQ&lt;/cell&gt;
        &lt;cell&gt;Issue Queue&lt;/cell&gt;
        &lt;cell&gt;Issue Queue&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Design specifications&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Support Vector Floating-Point Mul Calculation&lt;/item&gt;
      &lt;item&gt;Support vector floating-point FMA computation&lt;/item&gt;
      &lt;item&gt;Support Vector Floating-Point Div Calculation&lt;/item&gt;
      &lt;item&gt;Support for vector floating-point Sqrt computation&lt;/item&gt;
      &lt;item&gt;Supports fp32, fp64, fp16 computation&lt;/item&gt;
      &lt;item&gt;Supports computation of RV-V1.0 version vector floating-point instructions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Function&lt;/head&gt;
    &lt;p&gt;The VFPU module receives uop information issued from the Issue Queue and performs vector floating-point instruction calculations based on fuType and fuOpType information. It mainly consists of four modules: VFAlu, VFMA, VFDivSqrt, and VFCvt.&lt;/p&gt;
    &lt;p&gt;VFAlu is primarily responsible for fadd-related instructions and some other simple instructions, such as comparison instructions and sign injection instructions. Notably, the reduction sum instruction is also computed in this module by splitting into micro-operations (uops).&lt;/p&gt;
    &lt;p&gt;VFMA is primarily responsible for multiplication and multiply-add related instructions.&lt;/p&gt;
    &lt;p&gt;VFDivSqrt is primarily responsible for instructions related to division and square root.&lt;/p&gt;
    &lt;p&gt;VFCvt is primarily responsible for format conversion and reciprocal estimation-related instructions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Algorithm design&lt;/head&gt;
    &lt;p&gt;The challenge of the vector floating-point unit lies in supporting multiple single-precision format calculations (where the floating-point formats of operands and results are the same) and mixed-precision calculations (where the floating-point formats of operands and results differ). Taking common formats such as half-precision (\(f16\)), single-precision (\(f32\)), and double-precision (\(f64\)) as examples, the differences between scalar and vector floating-point units are compared.&lt;/p&gt;
    &lt;p&gt;Taking a typical floating-point addition as an example, for a scalar floating-point unit, it only needs to support calculations in three single-precision formats. The input operands and output results of this unit should all be \(64\)-bit, meaning it must support calculations in three formats:&lt;/p&gt;
    &lt;p&gt;(1) One \(f64 = f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) \(1\) \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(1\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;At first glance, three modules seem necessary to handle these three formats. However, since floating-point numbers consist of a sign bit, exponent, and mantissa, and higher-precision floating-point numbers have wider exponent and mantissa bit widths than lower-precision ones, the hardware design for higher-precision floating-point numbers can fully meet the requirements of lower-precision floating-point calculations. With slight modifications, adding \(Mux\) (multiplexers) to the hardware can enable compatibility with multiple single-precision formats, with only a marginal increase in area.&lt;/p&gt;
    &lt;p&gt;The vector floating-point unit needs to support vector operations, which are characterized by high data bandwidth utilization. For example, although the interface of a scalar arithmetic unit is 64-bit, when computing f32/f16, the effective data is only 32/16 bits, reducing bandwidth utilization to 50%/25%. The vector arithmetic unit also has a 64-bit interface, but when computing single-precision formats f32/f16, it can perform 2/4 sets of operations simultaneously, maintaining 100% bandwidth utilization. The supported single-precision format computations are as follows:&lt;/p&gt;
    &lt;p&gt;(1) One \(f64 = f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) 2 \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(4\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Performing multiple sets of floating-point additions with the same format simultaneously makes hardware design more challenging than scalar operations, but it also allows the reuse of high-precision format hardware for low-precision formats. Additionally, a key feature that vector floating-point units must support is mixed-precision computation. The \(RISC-V\) vector instruction set extension defines a series of \(widening\) instructions requiring mixed-precision computation, mandating that floating-point addition units also support the following four computation formats:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64 = f64 + f32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(f64 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) Two \(f32 = f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(4) Two \(f32 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;The design difficulty of mixed-precision computation is much greater than that of multiple single-precision formats. On one hand, operands of different data formats need to be converted to the same format as the result before computation, increasing logical complexity. On the other hand, format conversion imposes significant pressure on circuit timing, especially when converting low-precision denormal numbers to high-precision floating-point numbers. Therefore, this paper specifically designs a fast data format conversion algorithm to address the timing issue.&lt;/p&gt;
    &lt;p&gt;In summary, the design challenges of the vector floating-point unit lie in the implementation of multiple single-precision formats and mixed-precision formats. This section will introduce the vector floating-point addition algorithm, floating-point sequential accumulation algorithm, vector fused multiply-add algorithm, and vector floating-point division algorithm to address these challenges, achieving a high-performance vector floating-point unit with a frequency of up to \(3GHz\).&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Addition&lt;/head&gt;
    &lt;p&gt;Floating-point addition is one of the most commonly used arithmetic operations in scientific computing. Although conceptually simple, the traditional single-path floating-point addition algorithm requires two to three signed addition steps, which is a relatively time-consuming operation. The dual-path floating-point addition algorithm has only one signed addition operation on the critical path in the worst case, thus offering significant speed advantages over the single-path algorithm. Based on the dual-path floating-point addition algorithm, this paper designs an even faster improved dual-path floating-point addition algorithm. This section first introduces the single-path floating-point addition algorithm, the dual-path floating-point addition algorithm, and the improved dual-path floating-point addition algorithm for single-precision format, and finally presents the vector floating-point addition algorithm.&lt;/p&gt;
    &lt;p&gt;The floating-point addition formula is expressed as: \(fp\_result = fp\_a + fp\_b\). When \(fp\_a\) and \(fp\_b\) have the same sign, the significands are aligned and added, which is referred to as equivalent addition. When \(fp\_a\) and \(fp\_b\) have opposite signs, the significands are aligned and subtracted, which is referred to as equivalent subtraction. For denormal numbers, the exponent is \(0\), and for normalized numbers, the exponent is \(1\), but the corresponding normalized exponent is the same. Therefore, when calculating the exponent difference, an exponent of \(0\) should be treated as \(1\) (referred to as the normalized exponent). The absolute difference between the normalized exponents is the normalized exponent difference.&lt;/p&gt;
    &lt;head rend="h4"&gt;Single-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The traditional single-path floating-point addition operation is illustrated as follows, consisting of the following steps:&lt;/p&gt;
    &lt;p&gt;(1) Normalized exponent subtraction (ES): Calculate the difference between normalized exponents, d = |Ea - Eb|, where Ea and Eb are both normalized exponents.&lt;/p&gt;
    &lt;p&gt;(2) Alignment (\(Align\)): Shift the significand of the smaller operand right by \(d\) bits. The larger exponent is denoted as \(Ef\).&lt;/p&gt;
    &lt;p&gt;(3) Significand addition (\(SA\)): Performs addition or subtraction based on the effective operation \(Eo\), which is the arithmetic operation executed by the adder in the floating-point addition unit, determined by the sign bits of the two floating-point operands.&lt;/p&gt;
    &lt;p&gt;(4) Conversion (\(Conv\)): If the significand addition result is negative, convert the result to sign-magnitude representation. The conversion is completed through an addition step, with the result denoted as \(Sf\).&lt;/p&gt;
    &lt;p&gt;(5) Leading zero detection (LZD): Calculates the required left or right shift amount, expressed as \(En\), where right shift is positive and left shift is negative.&lt;/p&gt;
    &lt;p&gt;(6) Normalization (\(Norm\)): Normalize the significand by shifting \(En\) bits and add \(En\) to \(Ef\).&lt;/p&gt;
    &lt;p&gt;(7) Rounding (\(Round\)): Round according to the \(IEEE\)-\(754\) standard, adding \(1\) to the \(LSB\) of \(Sf\) if necessary. This step may cause overflow, requiring the mantissa result to be right-shifted by one bit while incrementing the exponent \(Ef\) by \(1\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Dual-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The above single-path floating-point algorithm is slow because the steps in the addition operation are essentially executed serially. This algorithm can be improved in the following ways:&lt;/p&gt;
    &lt;p&gt;(1) In the single-path floating-point addition algorithm, the \(Conv\) step is only needed when the result is negative, and it can be avoided by swapping the significands of the two operands. By checking the sign of the \(ES\) step result, the significands can be swapped (\(Swap\)) accordingly, always computing the larger significand minus the smaller one. When exponents are equal, the result may still be negative, requiring conversion, but no rounding is needed in this case. Thus, the swap step makes rounding and conversion mutually exclusive, allowing them to be parallelized. Note that another advantage of swapping is that only one shifter is required.&lt;/p&gt;
    &lt;p&gt;(2) The leading zero detection step can be executed in parallel with the significand addition step, removing it from the critical path. This optimization is particularly important in cases where subtraction requires significant left shifts.&lt;/p&gt;
    &lt;p&gt;(3) So far, the critical path steps have been reduced to: normalized exponent subtraction, swapping, alignment, significand addition \(||\) leading zero detection, conversion \(||\) rounding, normalization (where \(||\) denotes steps that can be executed in parallel). The alignment and normalization steps are mutually exclusive and can be further optimized. Normalization requires a large left shift only when \(d≤1\) or during equivalent subtraction. Conversely, alignment requires a large right shift only when \(d &amp;gt; 1\). By distinguishing these two cases, only one large shift—either alignment or normalization—remains on the critical path.&lt;/p&gt;
    &lt;p&gt;The steps for single-path and dual-path floating-point addition algorithms are shown in the table. In the dual-path algorithm, the preprocessing step (\(Pred\)) in the \(d ≤ 1\) path determines whether a right shift is needed to align significands based on the value of \(d\). The dual-path algorithm improves speed by executing more steps in parallel, requiring additional hardware for implementation.&lt;/p&gt;
    &lt;p&gt;Table: Steps for Two Floating-Point Addition Algorithms&lt;/p&gt;
    &lt;p&gt;+------------------+-----------------------------------------------------+ | Single-Path Floating-Point Addition | Dual-Path Floating-Point Addition Algorithm | | +-----------------------------+-----------------------+ | | \(d\leq1\) and Equivalent Subtraction | \(d&amp;gt;1\) or Equivalent Addition | +:================:+:===========================:+:=====================:+ | Normalized Exponent Addition | Preprocessing + Swap | Normalized Exponent Subtraction + Swap | +------------------+-----------------------------+-----------------------+ | Alignment | -- | Alignment | +------------------+-----------------------------+-----------------------+ | Significant Digit Addition | Significant Digit Addition or Leading Zero Detection | Significant Digit Addition | +------------------+-----------------------------+-----------------------+ | Conversion | Conversion or Rounding | Rounding | +------------------+-----------------------------+-----------------------+ | Leading Zero Detection | -- | -- | +------------------+-----------------------------+-----------------------+ | Normalization | Normalization | -- | +------------------+-----------------------------+-----------------------+ | Rounding | Path Selection | Path Selection | +------------------+-----------------------------+-----------------------+&lt;/p&gt;
    &lt;p&gt;In the dual-path floating-point addition algorithm, during the \(SA\) step in the case of equivalent subtraction, one of the significant digits is in 2's complement form. The complementation step and the rounding step are mutually exclusive, thus they can be performed in parallel. The optimized dual-path floating-point addition algorithm is shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(d≤1\) and equivalent subtraction&lt;/cell&gt;
        &lt;cell role="head"&gt;\(d&amp;gt;1\) or equivalent addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Preprocessing + Exchange&lt;/cell&gt;
        &lt;cell&gt;Normalized Instruction Subtraction + Swap&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Significant Digit Addition Conversion&lt;/cell&gt;
        &lt;cell&gt;Rounding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Normalization&lt;/cell&gt;
        &lt;cell&gt;Significand addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Selection Path&lt;/cell&gt;
        &lt;cell&gt;Selection Path&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In the IEEE round-to-nearest (\(RTN\)) mode, computing \(A+B\) and \(A+B+1\) suffices to address all normalization possibilities (additional computation of \(A+B+2\) is required for rounding toward positive or negative infinity). By utilizing \(Cin\) to select the final rounded mantissa result from multiple sets of significand adder outputs, both two's complement conversion and rounding can be completed simultaneously, saving an addition step. Since floating-point addition may require normalization through a right shift by one bit, no shift, or a left shift (potentially as extensive as the significand's length), \(Cin\) must account for all these normalization possibilities to ensure the selected result is the rounded one.&lt;/p&gt;
    &lt;head rend="h4"&gt;Improved dual-path floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;This section details the improved dual-path floating-point addition algorithm proposed in this paper. The path for equivalent addition or equivalent subtraction with d &amp;gt; 1 is called the far path, while the path for equivalent subtraction with d ≤ 1 is called the close path. Cases involving infinity or NaN operands are handled separately and do not belong to the far or close paths.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(far\) path&lt;/head&gt;
    &lt;p&gt;The \(far\) path algorithm is illustrated in the figure, with the main steps as follows:&lt;/p&gt;
    &lt;p&gt;Step 1: In the \(far\) path, when the exponent difference \(d\) is greater than \(1\), the smaller significand is shifted right by \(d\) bits to align with the larger significand. First, calculate the normalized exponent difference. To accelerate computation, two adders are used to compute the normalized exponent difference while comparing the magnitudes of \(Efp\_a\) and \(Efp\_b\). The correct normalized exponent difference is selected based on the comparison result of the exponent magnitudes.&lt;/p&gt;
    &lt;p&gt;In the second step, based on the exponent comparison from the first step, the significand of the operand with the larger exponent and the significand of the operand with the smaller exponent can be selected in parallel while also selecting the larger exponent \(EA\). For equivalent subtraction, \(EA\) is decremented by \(1\) (in this case, \(EA\) cannot be \(0\), as that would fall under the \(close\) path). This adjustment aims to align the value range of the significand after subtraction with that of equivalent addition, facilitating the selection of the final result. The adjusted significand addition or subtraction result falls within the range \([1\)-\(4)\), divided into two cases: \([1\)-\(2)\) and \([2\)-\(4)\).&lt;/p&gt;
    &lt;p&gt;Step three involves right-shifting the smaller significand, which is divided into two scenarios: during equivalent subtraction, the smaller significand is first inverted and then arithmetically right-shifted, saving some time compared to right-shifting first and then inverting; during equivalent addition, a logical right shift is directly applied. To reduce the number of shifter stages, when the high-order bits of the normalized exponent difference are all \(0\), the lower bits (the specific number depends on the significand width) are used for the right shift. If the high-order bits are not all \(0\), the right-shift result is \(0\). Here, the adder result from the first step, which calculates the normalized exponent difference between the two, is used, with the least significant bit applied first (since the adder result's least significant bit is obtained earliest). Specifically: if \(fp\_a\)'s exponent is larger, only \(fp\_b\)'s significand is right-shifted by the value of \(fp\_a\)'s normalized exponent minus \(fp\_b\)'s normalized exponent; if \(fp\_b\)'s exponent is larger, only \(fp\_a\)'s significand is right-shifted by the value of \(fp\_b\)'s normalized exponent minus \(fp\_a\)'s normalized exponent. The final right-shifted significand is then selected based on the exponent magnitude relationship and the normalized exponent difference, and the \(grs\) (\(guard\), \(round\), \(sticky\)) bits after the shift are calculated. To ensure correct rounding for the two scenarios in step two, two sets of \(grs\) need to be computed for the significand addition/subtraction results within \([1\)-\(2)\) and \([2\)-\(4)\).&lt;/p&gt;
    &lt;p&gt;Step 4: Perform significand addition. For equivalent subtraction, the smaller significand is inverted before right-shifting. Denote the larger significand as \(A\) and the right-shifted smaller significand as \(B\). Two adders compute \(A+B\) and \(A+B+2\), and the final rounded result is selected from these two adder outputs.&lt;/p&gt;
    &lt;p&gt;Step five: generate the final result. Depending on whether the significant digits \(A+B\) result falls within \([1\)-\(2)\) (case one) or \([2\)-\(4)\) (case two), and based on the two sets of \(grs\) and rounding modes calculated during the previous right shift, determine the conditions for selecting the two significant digit adders in case one and case two, respectively. Finally, use a one-hot four-way selection to choose the mantissa result. The exponent result is either \(EA\) (case one and mantissa rounded to \(&amp;lt;1\)) or \(EA+1\) (case two or case one rounded to \(=2\)). Note whether the exponent overflows after rounding, and the final result is selected between the overflow result and the normal computation result based on \(overflow\). The exception flags in the \(far\) path only produce overflow and inexact results.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(close\) path&lt;/head&gt;
    &lt;p&gt;In the \(close\) path, it must be an effective subtraction with \(d \leq 1\), specifically categorized as \(d=0\) or \(d=1\). The algorithm is illustrated in the figure, with the following detailed steps:&lt;/p&gt;
    &lt;p&gt;Step 1: Perform four sets of significand subtractions in parallel. Based on \(d=0\) (\(fp\_a\) significand is larger, \(fp\_b\) significand is larger) and \(d=1\) (\(fp\_a\) normalized exponent is larger, \(fp\_b\) normalized exponent is larger), combine the four scenarios for effective subtraction. The first subtractor: \(fp\_a\) significand \(-\) \(fp\_b\) significand; the second subtractor: \(fp\_b\) significand \(-\) \(fp\_a\) significand; the third subtractor: \(fp\_a\) significand \(×2\) \(-\) \(fp\_b\) significand; the fourth subtractor: \(fp\_b\) significand \(×2\) \(-\) \(fp\_a\) significand. Simultaneously, calculate the \(grs\) bits based on the exponent magnitude relationship. When \(d=0\), all \(grs\) bits are \(0\); when \(d=1\), only \(g\) may be non-zero. These four sets of adders cannot produce all rounding results, so a fifth slower adder is added: the significand with the larger exponent \(–\) the significand with the smaller exponent shifted right by one bit.&lt;/p&gt;
    &lt;p&gt;Step two: Determine the four conditions for selecting the four sets of significand subtractions, based on the value of \(d\), the most significant bit of the adder result, \(grs\), and the rounding mode. After selecting the subtraction result from the four sets of adders, perform \(LZD\) \(+\) left shift on the subtraction result. Here, attention must be paid to the value of the larger exponent \(EA\). The left shift is controlled jointly by \(LZD\) and \(EA\), generating a \(mask\) value (with the same bit width as the subtraction result but with at most one bit set to \(1\)) based on the value of \(EA\). This \(mask\) is ORed with the subtraction result before performing \(LZD+\) left shift.&lt;/p&gt;
    &lt;p&gt;Step 3: Determine the condition for selecting the fifth subtractor. When selecting the result of the fifth subtractor, no left shift is required, so a slower adder is used, and the final mantissa result can then be selected.&lt;/p&gt;
    &lt;p&gt;Step four: exponent and sign bit results. The exponent result requires subtracting the \(LZD\) value from step two from \(EA\). If the fifth subtractor is selected as the mantissa result, the exponent remains unchanged. When \(d=1\), the sign bit is the sign of the operand with the larger exponent. When \(d=0\), the sign bit is selected based on the mantissa size. Note that when the result is \(0\) and rounded down, the sign bit is \(1\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector floating-point addition algorithm&lt;/head&gt;
    &lt;p&gt;The vector floating-point adder's output signal width is \(64\) bits, supporting mixed precision and widening instructions. It must support calculations for the following data formats:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64\) \(= f64 + f64\);&lt;/p&gt;
    &lt;p&gt;(2) \(1\) \(f64\) \(= f64 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) 1 \(f64\) = \(f32\) + \(f32\);&lt;/p&gt;
    &lt;p&gt;(4) \(2\) \(f32\) values \(= f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(5) \(2\) \(f32\) \(= f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(6) Two \(f32\) \(= f16 + f16\);&lt;/p&gt;
    &lt;p&gt;(7) Four \(f16\) = \(f16 + f16\).&lt;/p&gt;
    &lt;head rend="h5"&gt;Module partitioning&lt;/head&gt;
    &lt;p&gt;The computation approach uses one module for the first three formats, all outputting 64-bit results. The single-precision floating-point adder for \(f64 = f64 + f64\) is reused to compute \(f64 = f64 + f32\) and \(f64 = f32 + f32\). This paper proposes a fast data format conversion algorithm to convert \(f32\) operands to \(f64\), enabling \(f64 = f64 + f64\) computation and yielding results in \(f64\) format.&lt;/p&gt;
    &lt;p&gt;The same approach is applied to computation formats where the output is \(f32\). Since \(f32\) has less timing pressure, integrating a \(f16 = f16 + f16\) operation into the module that computes \(f32\) results saves area while supporting:&lt;/p&gt;
    &lt;p&gt;(1) One \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(f32 = f32 + f16\);&lt;/p&gt;
    &lt;p&gt;(3) One \(f32 = f16 + f16\);&lt;/p&gt;
    &lt;p&gt;(4) One \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Clearly, this module needs to be instantiated twice, and there are still two \(f16 = f16 + f16\) operations missing. Two single-precision floating-point adders dedicated to computing \(f16 = f16 + f16\) are instantiated separately, totaling four modules, to implement all vector addition calculation formats.&lt;/p&gt;
    &lt;head rend="h5"&gt;Fast format conversion algorithm&lt;/head&gt;
    &lt;p&gt;Taking the conversion from \(f16\) to \(f32\) as an example, a fast format conversion algorithm is introduced.&lt;/p&gt;
    &lt;p&gt;When \(f16\) is a normalized number, converting it to \(f32\) will also result in a normalized number. For \(f16\) exponents, they are biased to match \(f32\) exponents. Since \(f32\) has a larger exponent range, there is no concern about exponent overflow after conversion. Additionally, the \(f16\) significand is \(10\) bits, while the \(f32\) significand is \(23\) bits. Simply appending \(13\) zeros to the \(f16\) significand yields the \(f32\) significand. This is a conversion from lower to higher precision, ensuring the result is exact.&lt;/p&gt;
    &lt;p&gt;For a normalized \(f16\) exponent (5-bit width), the actual exponent \(Ereal = Ef16 – 15\). For a normalized \(f32\) exponent (8-bit width), \(Ereal = Ef32 – 127\). Thus, converting \(Ef16\) to \(Ef32\) via \(Ereal\): \(Ef16 – 15 = Ef32 – 127\), \(Ef32 = Ef16 – 15 + 127\), \(Ef32 = Ef16 + 112\). The 8-bit binary representation of \(112\) is \(01110000\). Computing \(Ef16 + 112\) requires an adder for a variable plus a constant, but this adder can be avoided by identifying the following pattern:&lt;/p&gt;
    &lt;p&gt;When the highest bit of \(Ef16\) is \(0\), \(Ef16 + 112 = (0111, Ef16(3, 0))\)&lt;/p&gt;
    &lt;p&gt;When the most significant bit of \(Ef16\) is \(1\), \(Ef16 + 112 = (1000, Ef16(3, 0))\).&lt;/p&gt;
    &lt;p&gt;Using this pattern, an \(Mux\) can quickly convert \(Ef16\) to \(Ef32\). Thus, for normalized \(f16\) to \(f32\) conversion, the exponent bits use an \(Mux\), the significand bits are padded with 0, and the sign bit remains unchanged. The challenge arises when \(f16\) is denormal. In this case, all exponent bits of \(f16\) are 0, and the number of leading zeros in the significand determines the exponent after conversion to \(f32\). When all exponent bits of \(f16\) are zero and only the \(lsb\) of the significand is 1, the converted \(f32\) exponent is minimized at \(-15-9=-24\), which still falls within the range of \(f32\) normalized numbers. Therefore, for denormal \(f16\), leading zero detection (\(lzd\)) and left shifting of the significand are required.&lt;/p&gt;
    &lt;p&gt;Chisel's built-in priority encoder can implement the \(lzd\) function. Tests show it synthesizes better than traditional \(lzd\) implementations using binary search. The syntax is: \(PriorityEncoder(Reverse(Cat(in,1.U)))\). For a \(5\)-bit \(in\), the generated Verilog code is as follows:&lt;/p&gt;
    &lt;code&gt;module LZDPriorityEncoder(
  input        clock,
  input        reset,
  input  [4:0] in,
  output [2:0] out
);
  wire [5:0] _out_T = {in,1'h1};
  wire [5:0] _out_T_15 = {_out_T[0],_out_T[1],_out_T[2],_out_T[3],_out_T[4],_out_T[5]};
  wire [2:0] _out_T_22 = _out_T_15[4] ? 3'h4 : 3'h5;
  wire [2:0] _out_T_23 = _out_T_15[3] ? 3'h3 : _out_T_22;
  wire [2:0] _out_T_24 = _out_T_15[2] ? 3'h2 : _out_T_23;
  wire [2:0] _out_T_25 = _out_T_15[1] ? 3'h1 : _out_T_24;
  assign out = _out_T_15[0] ? 3'h0 : _out_T_25;
endmodule
&lt;/code&gt;
    &lt;p&gt;Although this code appears to use many cascaded \(Mux\)es, the synthesizer produces good timing results for such code. Inspired by this, this paper designs a novel priority-based left-shift algorithm to accelerate \(lzd+\) left-shift, with the \(Chisel\) code as follows:&lt;/p&gt;
    &lt;code&gt;def shiftLeftPriorityWithF32EXPResult(srcValue: UInt, priorityShiftValue: UInt): UInt = {
  val width = srcValue.getWidth
  val lzdWidth = srcValue.getWidth.U.getWidth
  def do_shiftLeftPriority(srcValue: UInt, priorityShiftValue: UInt, i:Int): UInt = {
    if (i==0) Cat(
      Mux(
        priorityShiftValue(i),
        Cat(srcValue(0),0.U((width-1).W)),
        0.U(width.W)
      ),
      Mux(
        priorityShiftValue(i),
        "b01110000".U-(width-i-1).U(8.W),
        "b01110000".U-(width-i).U(8.W)
      )
    )
    else Mux(
      priorityShiftValue(i),
      if (i==width-1) Cat(srcValue(i,0),"b01110000".U-(width-i-1).U(8.W)) 
      else Cat(Cat(srcValue(i,0),0.U((width-1-i).W)), "b01110000".U-(width-i-1).U(8.W)),
      do_shiftLeftPriority(srcValue = srcValue, priorityShiftValue = priorityShiftValue, i = i - 1)
      )
    }
    do_shiftLeftPriority(srcValue = srcValue, priorityShiftValue = priorityShiftValue, i = width-1)
  }
&lt;/code&gt;
    &lt;p&gt;Both \(srcValue\) and \(priorityShiftValue\) pass the mantissa of \(f16\), starting from the most significant bit (MSB) of the mantissa. If the MSB is \(1\), the original value of \(srcValue\) is returned along with the corresponding exponent (the exponent is selected from multiple constants and depends on the position of the first \(1\) in the mantissa). If the MSB is \(0\), the next bit is checked for \(1\). If it is \(1\), \(srcValue\) is left-shifted by one bit and returned (no actual left shift is needed here since the high bits after shifting are not retained; truncation and zero-padding suffice), along with the corresponding exponent. This process continues iteratively. Thus, a priority left shifter simultaneously performs the \(lzd\) and left shift operations while also generating the corresponding \(Ef32\), eliminating the need to calculate the \(Ef32\) exponent based on \(lzd\). This enables a fast algorithm for converting \(f16\) denormal numbers to \(f32\). A similar algorithm is used for converting \(f32\) to \(f64\), which is not elaborated here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Fused Multiply-Add Algorithm&lt;/head&gt;
    &lt;p&gt;Floating-point fused multiply-add computation \(fpa × fp\_b + fp\_c\), where the intermediate multiplication \(fpa × fp\_b\) is performed as if without range and precision limitations, without rounding, and only rounded once to the target format at the end. FMA is typically implemented using a pipeline, with steps including multiplication, addition, normalization shift, and rounding. This chapter introduces the vector floating-point fused multiply-add algorithm, whose functionalities include:&lt;/p&gt;
    &lt;p&gt;(1) 1 \(fp64 = fp64 × fp64 + fp64\);&lt;/p&gt;
    &lt;p&gt;(2) \(2\) \(fp32 = fp32 × fp32 + fp32\);&lt;/p&gt;
    &lt;p&gt;(3) Four \(fp16 = fp16 × fp16 + fp16\);&lt;/p&gt;
    &lt;p&gt;(4) \(2\) \(fp32 = fp16 × fp16 + fp32\);&lt;/p&gt;
    &lt;p&gt;(5) \(1\) \(fp64 = fp32 × fp32 + fp64\).&lt;/p&gt;
    &lt;p&gt;(\(1\)) (\(2\)) (\(3\)) The source and destination operands are in the same floating-point format, while in (\(4\)) (\(5\)), the two multipliers have the same width, and the other addend and the result share the same width, which is twice that of the multipliers.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar single-precision format algorithm&lt;/head&gt;
    &lt;p&gt;The computation flow first calculates the unrounded result of multiplying two floating-point numbers, then adds this unrounded product to a third number. The algorithm flowchart is illustrated, expressed by the formula \(fp\_result = fp\_a × fp\_b + fp\_c\), where \(Sa\), \(Sb\), and \(Sc\) are the significands of \(fp\_a\), \(fp\_b\), and \(fp\_c\) respectively, and \(Ea\), \(Eb\), and \(Ec\) are their exponents:&lt;/p&gt;
    &lt;p&gt;For ease of description below, some parameters are defined, with their meanings and values listed in the table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Parameters&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f16\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f32\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f64\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(significandWidth\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(24\)&lt;/cell&gt;
        &lt;cell&gt;\(53\)&lt;/cell&gt;
        &lt;cell&gt;Significant Digit Width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(exponentWidth\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;Exponent width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(rshiftBasic\)&lt;/cell&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
        &lt;cell&gt;\(27\)&lt;/cell&gt;
        &lt;cell&gt;\(56\)&lt;/cell&gt;
        &lt;cell&gt;Number of right shifts required to align \(fp\_c\)'s significand with the product significand&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(rshiftMax\)&lt;/cell&gt;
        &lt;cell&gt;\(37\)&lt;/cell&gt;
        &lt;cell&gt;\(76\)&lt;/cell&gt;
        &lt;cell&gt;\(163\)&lt;/cell&gt;
        &lt;cell&gt;\(fp\_c\) maximum right shift count for significant digits (beyond this value, \(g\) and \(r\) are both \(0\))&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Unsigned Integer Multiplication&lt;/head&gt;
    &lt;p&gt;The rule for multiplying two floating-point numbers is to multiply the sign bits, add the exponent bits (not simply added, as bias must be considered), and multiply the significands (including the implicit bit and mantissa bits). The significand multiplication is essentially fixed-point multiplication, which follows the same principle as unsigned integer multiplication.&lt;/p&gt;
    &lt;p&gt;Binary vertical multiplication is the original multiplication algorithm, where an \(n\)-bit \(C=A×B\) vertical method is illustrated. This process generates \(n\) partial products, which are then added with staggered alignment.&lt;/p&gt;
    &lt;p&gt;The multiplication algorithm using the vertical method has significant latency. Optimization efforts for multiplication operations primarily focus on two aspects: reducing the number of partial products (e.g., \(Booth\) encoding) and minimizing the latency introduced by adders (e.g., \(CSA\) compression).&lt;/p&gt;
    &lt;p&gt;When computing the multiplication of two floating-point numbers, their significands are multiplied. Since significands are unsigned, unsigned integer multiplication suffices for this computation. There are many algorithms for implementing unsigned integer multiplication, and three of them are compared below.&lt;/p&gt;
    &lt;p&gt;Method 1: Directly use the multiplication symbol \(×\), letting the synthesis tool decide.&lt;/p&gt;
    &lt;p&gt;Method two: Use a vertical multiplication method similar to manual decimal multiplication. Multiplying two \(n\)-bit numbers generates \(n\) partial products, which are then compressed using \(CSA\) (to be introduced later) into two numbers for addition.&lt;/p&gt;
    &lt;p&gt;Method 3: Use \(Booth\) encoding to generate \((n+1)/2\) rounded-up partial products, then compress them into two numbers for addition using \(CSA\).&lt;/p&gt;
    &lt;p&gt;The data in the table are the results of multiplying two 53-bit unsigned integers (for f64) using the TSMC 7nm process library. The target frequency is 3GHz, with a theoretical cycle time of 333.33ps. However, considering clock uncertainty and process corner variations, a design margin is reserved for the backend, leaving approximately 280ps per cycle. Therefore, it is evident that multiplication cannot be completed within one cycle. In practice, additional time is required to determine the implicit bit, making it even more impossible to achieve 53-bit multiplication in a single cycle. Although Method 1 has a smaller area and shorter latency, it cannot be pipelined, leaving only Methods 2 or 3 as viable options. Method 3 offers shorter latency and a smaller area compared to Method 2, making it the chosen implementation for unsigned integer multiplication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Algorithm&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Pipelining feasibility&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Method one&lt;/cell&gt;
        &lt;cell&gt;\(285.15\)&lt;/cell&gt;
        &lt;cell&gt;\(1458.95\)&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Method two&lt;/cell&gt;
        &lt;cell&gt;\(320.41\)&lt;/cell&gt;
        &lt;cell&gt;\(2426.34\)&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Method three&lt;/cell&gt;
        &lt;cell&gt;\(302.19\)&lt;/cell&gt;
        &lt;cell&gt;\(2042.46\)&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;\(Booth\) encoding&lt;/head&gt;
    &lt;p&gt;The purpose of Booth encoding is to reduce the number of partial products in a multiplier. Taking the binary unsigned integer multiplication C=A*B as an example, the Booth encoding algorithm is derived.&lt;/p&gt;
    &lt;p&gt;The following expression is a general form of unsigned binary integers. To facilitate subsequent transformations, a \(0\) is added at both the beginning and the end, leaving its value unchanged.&lt;/p&gt;
    &lt;p&gt;After equivalent transformation, adjacent two bits of \(1\) cancel out to \(0\). For consecutive \(1\)s, the least significant \(1\) becomes \(-1\), and the bit above the most significant \(1\) changes from \(0\) to \(1\), with all \(1\)s turning to \(0\). This transformation is known as Booth transformation. It simplifies sequences of three or more consecutive \(1\)s, with greater simplification for longer sequences. However, this transformation does not optimize hardware circuits because it does not guarantee any partial product will always be \(0\). Therefore, modified Booth encoding is typically used in circuit design to effectively reduce the number of partial products.&lt;/p&gt;
    &lt;p&gt;Perform an equivalent transformation again, but this time with additional constraints on \(n\). Assuming \(n\) is odd, a zero is still appended at the end, increasing the length to an even number. Then, a zero is prepended at the highest bit, making the total length \(n+2\). This is done to facilitate subsequent derivations.&lt;/p&gt;
    &lt;p&gt;After equivalent transformation, it can be observed that the number of terms in the polynomial expression becomes \((n+1)/2\) (when \(n\) is odd). If \(n\) is even, a zero needs to be appended at the end, and two zeros are prepended before the most significant bit, making the number of terms \(n/2+1\) (when \(n\) is even). Combining both odd and even cases, the number of terms in the polynomial expression is the ceiling of \((n+1)/2\). Starting from the LSB of the original binary number, groups of three bits are formed (the first group's least significant bit requires an additional appended bit \(0\), and the most significant bit is padded with one \(0\) if \(n\) is odd or two \(0\)s if \(n\) is even, ensuring the padded length is odd). Adjacent groups overlap by one bit (the highest bit of the lower group overlaps with the lowest bit of the higher group), forming new polynomial factors. This is the improved Booth encoding method.&lt;/p&gt;
    &lt;p&gt;When multiplying two binary numbers, modified Booth encoding of the multiplier can halve the number of partial products. Let the multiplicand be \(A\) and the multiplier be \(B\), with \(B_{2i+1}\), \(B_{2i}\), and \(B_{2i-1}\) representing three consecutive bits of \(X\), where \(i\) is a natural number \(N\). \(PP_i\) denotes the partial product for each \(i\). After applying modified Booth transformation to \(B\) and multiplying by \(A\), the Booth encoding and \(PP\) truth table are as shown.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(B_{2i+1}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B_{2i}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B_{2i-1}\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(PP_i\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-2A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-A\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;By evaluating each consecutive three-bit segment of the multiplier, the corresponding partial product is derived, halving the number of partial products. This approach treats the multiplier as a quaternary number, hence termed radix-4 Booth encoding. Multiplication using radix-4 Booth encoding offers significant optimization over traditional methods, is straightforward to implement, and meets most application requirements.&lt;/p&gt;
    &lt;p&gt;In \(Booth\) encoding, five types of partial products need to be calculated: \(0\), \(A\), \(2A\), \(-A\), \(-2A\). \(0\) and \(A\) require no computation, \(2A\) is obtained by a one-bit left shift, while \(-A\) and \(-2A\) require the operation of inversion plus one. This paper introduces a fast algorithm for handling inversion plus one.&lt;/p&gt;
    &lt;p&gt;To simplify the explanation of the principle, we assume computing \(f16\) with 11 significant bits, generating 6 partial products. Each partial product is 22 bits wide, as shown in the figure. The colored positions in the figure are 12 bits wide, representing \(A\) possibly multiplied by \(0\), \(1\), or \(2\). Since the last partial product's three-bit encoding is \(0\)xx, its value cannot be negative. Assuming all other partial products are negative, we invert and add one to each of them. The colored parts represent the results after inversion only. We place the added one for the current partial product into the corresponding position of the next partial product, ensuring the sum of partial products remains unchanged and avoiding the issue of a carry chain from adding one to the current partial product. The last partial product is non-negative and does not require this adjustment.&lt;/p&gt;
    &lt;p&gt;The \(1\) in the above figure can first be simplified through summation to obtain the result shown in the following figure.&lt;/p&gt;
    &lt;p&gt;If the actual partial product value is positive, the above result needs to be corrected by adding one to the bit position immediately to the left of the colored bit and setting the next partial product's tail addition to zero. As shown in the figure, \(Si\) (where \(i\) starts from \(0\)) represents the sign bit of the \(i\)-th partial product, transforming it into a general form where the colored position only computes \(0\), \(A\), \(2A\), \(\sim A\), or \(\sim 2A\), speeding up partial product generation.&lt;/p&gt;
    &lt;p&gt;One additional point to note is that the sum of partial products yields the multiplication result, but the summation of partial products may also generate carries. These carries are meaningless for multiplication, but they can cause erroneous carries when the product is added to a wider number. The correction method involves adding an extra bit to the most significant bit of the partial product, as illustrated.&lt;/p&gt;
    &lt;p&gt;This ensures that the carry is correct after summing all partial products. This concludes the introduction to Booth encoding. Note that the example uses an 11-bit multiplication. While \(f16\) and \(f64\) have an odd number of significant digits, \(f32\) has an even number, requiring slight differences in zero-padding the most significant bit. Other steps are similar and thus omitted.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(CSA\) Compression&lt;/head&gt;
    &lt;p&gt;\(Carry\)-\(Save\)-\(Adder\) is a carry-save adder that compresses \(n\) addends into \(m\) addends ($m&lt;/p&gt;
    &lt;p&gt;Assuming the calculation of adding two binary numbers \(A+B\), the truth table for their sum and carry, where \(A[i]+B[i]\) is the decimal result and also the count of \(1\)s in \(A[i]\) and \(B[i]\):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;\(A[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(A[i] + B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Simplified into the following logical expression:&lt;/p&gt;
    &lt;p&gt;\(Sum = A\) ^ \(B\)&lt;/p&gt;
    &lt;p&gt;\(Car = A\) &amp;amp; \(B\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;For three-number addition, the sum is the XOR of two numbers, and the carry occurs when both numbers are \(1\). \((Car &amp;lt;&amp;lt; 1)\) reflects that the current bit's carry propagates to the next bit. This derivation is for clarity; in practice, generating sum and carry from two addends does not accelerate addition.&lt;/p&gt;
    &lt;p&gt;Suppose we want to calculate the sum of three numbers \(A+B+C\), where the \(CSA\) key is to generate the sum and carry, as shown in the truth table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;\(A[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(B[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(A[i] + B[i] + C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;From the above table, some patterns can be observed. The generation of \(Sum[i]\) and \(Car[i]\) actually depends only on the sum of \(A[i]+B[i]+C[i]\), i.e., the number of \(1\)s in \(A[i]\), \(B[i]\), and \(C[i]\). The simplified expression is as follows:&lt;/p&gt;
    &lt;p&gt;\(Sum = A\) ^ \(B\) ^ \(C\)&lt;/p&gt;
    &lt;p&gt;\(Car = (A\) &amp;amp; \(B) \quad | \quad (A\) &amp;amp; \(C) \quad | \quad (B\) &amp;amp; \(C)\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B+C = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;For three-number addition, the sum is the XOR of the three numbers, and the carry occurs when at least two numbers are \(1\). \((Car &amp;lt;&amp;lt; 1)\) accounts for the current bit's carry propagating to the next bit. This method converts three-number addition into two-number addition with just two XOR gate delays, significantly saving time, especially for longer bit widths.&lt;/p&gt;
    &lt;p&gt;Adding four numbers is slightly more complex because when all four are \(1\), the sum is \(4\), requiring a carry of \(2\). We designate one carry as \(Cout\) and the other as \(Car\). The \(Cout\) generated from the current four-bit addition is passed to the next stage as \(Cin\). With \(Cin\) and the four numbers, the operation now involves five inputs: \(A[i]\), \(B[i]\), \(C[i]\), \(D[i]\), and \(Cin[i]\), producing three outputs: \(Sum[i]\), \(Cout[i]\), and \(Car[i]\). The least significant bit's \(Cin[0]\) is \(0\), while other bits' \(Cin[i]\) is the \(Cout[i-1]\) from the previous bit, as shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]+D[i]+Cin[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Sum[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Cout[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1/0\)&lt;/cell&gt;
        &lt;cell&gt;\(0/1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1/0\)&lt;/cell&gt;
        &lt;cell&gt;\(0/1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are many ways to simplify this truth table. One feasible method is described below. The value of \(Sum[i]\) can be easily derived as the XOR of the five inputs: \(Sum[i] = A[i]\)^\(B[i]\)^\(C[i]\)^\(D[i]\)^\(Cin[i]\). \(Car[i]\) and \(Cout[i]\) are more complex. We define \(Cout[i]\) to be generated only by the first three numbers, i.e., when the sum of the first three numbers is greater than \(1\), \(Cout[i] = 1\). The table shows the truth table for \(Cout[i]\):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Cout[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;\(Cout[i]\) can be expressed as: \(Cout[i] = (A[i]\)^\(B[i])?C[i]:A[i]\), while \(Car[i]\) is generated by \(D[i]\) and \(Cin[i]\), with the table showing the truth table for \(Car[i]\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(A[i]+B[i]+C[i]+D[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(Car[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(D[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(Cin[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(D[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(Cin[i]\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;\(Car[i]\) can be expressed as: \(Car[i] = (A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) ? Cin[i] : D[i]\). Specifically, when \((A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) = 1\), \(A[i]+B[i]+C[i]+D[i] = 1/3\), and \(Cin[i] = 1\) will generate a carry. When \((A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i]) = 0\), \(A[i]+B[i]+C[i]+D[i] = 0/4\). Here, \(D[i] = 0\) indicates \(A[i]+B[i]+C[i]+D[i] = 0\), and adding \(Cin\) will not produce a carry, while \(D[i] = 1\) indicates \(A[i]+B[i]+C[i]+D[i] = 4\), and adding \(Cin\) will generate a carry. Based on the above derivation, the expression for \(CSA4\_2\) is as follows:&lt;/p&gt;
    &lt;p&gt;Sum[i] = A[i] ^ B[i] ^ C[i] ^ D[i] ^ Cin[i], Cin[i] = Cout[i-1], Cin[0] = 0&lt;/p&gt;
    &lt;p&gt;\(Cout[i] = (A[i]\) ^ \(B[i])?C[i]:A[i]\)&lt;/p&gt;
    &lt;p&gt;\(Car[i] = (A[i]\) ^ \(B[i]\) ^ \(C[i]\) ^ \(D[i])?Cin[i]:D[i]\)&lt;/p&gt;
    &lt;p&gt;\(Result = A+B+C+D = Sum + (Car &amp;lt;&amp;lt; 1)\)&lt;/p&gt;
    &lt;p&gt;Using the \(TSMC7nm\) process library, a comprehensive comparison of delay and area was conducted for different input XOR gates, \(CSA3\_2\), and \(CSA4\_2\). The synthesis results for different input XOR gates are shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(106\) bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)&lt;/cell&gt;
        &lt;cell&gt;\(13.74\)&lt;/cell&gt;
        &lt;cell&gt;\(38.66880\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)&lt;/cell&gt;
        &lt;cell&gt;\(23.01\)&lt;/cell&gt;
        &lt;cell&gt;\(63.09120\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)^\(D\)&lt;/cell&gt;
        &lt;cell&gt;\(24.69\)&lt;/cell&gt;
        &lt;cell&gt;\(87.51360\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(A\)^\(B\)^\(C\)^\(D\)^\(E\)&lt;/cell&gt;
        &lt;cell&gt;\(37.21\)&lt;/cell&gt;
        &lt;cell&gt;\(99.72480\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The synthesis results of \(CSA3\_2\) and \(CSA4\_2\) are shown in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(106\) bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Delay (\(ps\))&lt;/cell&gt;
        &lt;cell role="head"&gt;Area (\(um²\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(CSA3\_2\)&lt;/cell&gt;
        &lt;cell&gt;\(23.23\)&lt;/cell&gt;
        &lt;cell&gt;\(104.42880\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(CSA4\_2\)&lt;/cell&gt;
        &lt;cell&gt;\(40.63\)&lt;/cell&gt;
        &lt;cell&gt;\(237.86881\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It can be seen that although \(CSA4\_2\) theoretically has a delay of three XOR gates and \(CSA3\_2\) theoretically has a delay of two XOR gates, in actual physical implementation, \(CSA4\_2\) is only slightly faster than two levels of \(CSA3\_2\). Therefore, \(CSA3\_2\) should be used whenever possible, unless one level of \(CSA4\_2\) can replace two levels of \(CSA3\_2\), such as in \(4-&amp;gt;2\) compression or \(8-&amp;gt;2\) compression.&lt;/p&gt;
    &lt;head rend="h5"&gt;CSAn_2&lt;/head&gt;
    &lt;p&gt;For two unsigned integer multiplications using Booth encoding, the number of partial products is ceil((n+1)/2). To ensure correct carry propagation, the partial product bit width is extended by one bit. The number and bit width of partial products for each data format are listed in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of significant digits&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of partial products&lt;/cell&gt;
        &lt;cell role="head"&gt;Partial product bit width&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp16\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp32\)&lt;/cell&gt;
        &lt;cell&gt;\(24\)&lt;/cell&gt;
        &lt;cell&gt;\(13\)&lt;/cell&gt;
        &lt;cell&gt;\(25\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fp64\)&lt;/cell&gt;
        &lt;cell&gt;\(53\)&lt;/cell&gt;
        &lt;cell&gt;\(27\)&lt;/cell&gt;
        &lt;cell&gt;54&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Following the principle of prioritizing \(CSA3\_2\) unless one level of \(CSA4\_2\) can replace two levels of \(CSA3\_2\), the number of \(CSA3\_2\) and \(CSA4\_2\) stages used for each data format is listed in the table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of \(CSA3\_2\) Stages&lt;/cell&gt;
        &lt;cell role="head"&gt;\(CSA4\_2\) Stages&lt;/cell&gt;
        &lt;cell role="head"&gt;Process (\(-&amp;gt;\) denotes \(CSA3\_2\), \(--&amp;gt;\) denotes \(CSA4\_2\))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp16\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(6-&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp32\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(13-&amp;gt;9-&amp;gt;6-&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fp64\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(27-&amp;gt;18-&amp;gt;12-&amp;gt;8--&amp;gt;4--&amp;gt;2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Exponent processing and right shift&lt;/head&gt;
    &lt;p&gt;Following conventional methods, if the exponent relationship between the product of \(fp\_a\) and \(fp\_b\) and the exponent of \(fp\_c\) is unknown, the smaller exponent must be right-shifted, similar to floating-point addition. This would require both the significand of the \(fp\_a\) and \(fp\_b\) product and the significand of \(fp\_c\) to potentially shift right, necessitating two shifters and increasing area. Additionally, waiting for the \(fp\_a\) and \(fp\_b\) product to be computed before right-shifting its significand increases circuit latency. An alternative algorithm avoids using two shifters and reduces latency by parallelizing the computation with the \(fp\_a\) and \(fp\_b\) product.&lt;/p&gt;
    &lt;p&gt;The exponent bits are treated as unsigned numbers, but there is an exponent bias between them and the actual exponent. Additionally, the \(denormal\) case must be considered. Let \(E\_fix\) denote the exponent bits after handling the \(denormal\) case, and \(E\_bit\) denote the original exponent bits. When all bits of \(E\_bit\) are 0, \(E\_fix = 1\); otherwise, \(E\_fix = E\_bit\).&lt;/p&gt;
    &lt;p&gt;In the above equation, the true exponent \(E\_real\) equals \(E\_fix\) minus a bias value \(bias\), where \(exponentWidth\) is the width of \(E\_bit\), and \(bias\) equals the value where the highest bit of \(E\_bit\) is \(0\) and all other bits are \(1\). Without considering the carry or borrow of the significand product, the true exponent result \(Eab\_real\) of multiplying \(fp\_a\) and \(fp\_b\) is given by:&lt;/p&gt;
    &lt;p&gt;The calculation formula for the binary exponent result \(Eab\_bit\) of the multiplication of \(fp\_a\) and \(fp\_b\) is shown below:&lt;/p&gt;
    &lt;p&gt;The operation of \(+\)&amp;amp; extends the result of \(Ea\_fix + Eb\_fix\) by one bit to retain the carry. The carry is preserved because a bias value will be subtracted later, and without retaining the carry, the result would be incorrect. Additionally, subtracting the bias might result in a negative value, so another bit is extended by appending a 0 at the highest bit. Finally, the bias \(bias\) is subtracted, yielding the binary exponent result \(Eab\_bit\) for the multiplication of \(fp\_a\) and \(fp\_b\) without considering the carry or borrow from the significand product. Then, we construct an exponent \(Eab\) with the following value:&lt;/p&gt;
    &lt;p&gt;Assuming the binary exponent result of multiplying \(fp\_a\) and \(fp\_b\) is \(Eab\), to ensure lossless precision when adding the significant digits of \(fp\_a \times fp\_b\) and \(fp\_c\), both addends are extended in width. The significant digits of \(fp\_c\) are extended to \(3 \times significandWidth + 4\), with the bit distribution shown in the figure. Here, \(g0\), \(r0\), \(g1\), and \(r1\) are used to preserve the \(guard\) and \(round\) bits during right-shifting:&lt;/p&gt;
    &lt;p&gt;As shown above, the significand of \(fp\_c\) is \(significandWidth+2\) bits wider than the product of the significands of \(fp\_a\) and \(fp\_b\). Since the product result has two digits before the decimal point, aligning it as \(1\).xxx requires \(significandWidth+3\) bits, which explains why \(rshiftBasic = significandWidth+3\).&lt;/p&gt;
    &lt;p&gt;Let \(fp\_c\_significand\_cat0 = Cat(fp\_c\_significand, 0.U(2 \times significandWidth + 4))\), where \(fp\_c\_significand\) is the significand of \(fp\_c\). If \(Ec\_fix = Eab = Eab\_bit + rshiftBasic.S\), \(fp\_c\_significand\_cat0\) is exactly \(significandWidth + 3\) larger than \(Eab\_bit\), so no right shift is needed for alignment. If \(Ec\_fix &amp;gt; Eab\), theoretically \(fp\_c\_significand\_cat0\) would require a left shift, but due to the presence of \(g0\) and \(g1\) as buffers and the fact that lower bits cannot generate carry (only affecting rounding), no actual left shift is needed. If \(Ec\_fix &amp;lt; Eab\), \(fp\_c\_significand\_cat0\) must be right-shifted by \(rshift\_value = Eab - Cat(0.U, Ec\_fix).asSInt\). Since \(rshift\_value\) is derived from the addition of multiple numbers, its LSB is computed first. Thus, during right-shifting, the LSB of \(rshift\_value\) is first used as the Mux select signal, followed by higher bits. The shifting process must compute \(guard\), \(round\), and \(sticky\) (collectively \(grs\)). For \(guard\) and \(round\), these positions are already preserved during bit-width extension, requiring no additional computation. For \(sticky\), two methods exist: (1) Extend the bit-width further to store shifted-out bits and compute \(sticky\) after all shifts, or (2) Compute \(sticky\) during shifting based on Mux select signals. Method 2 offers lower latency than Method 1. Below is the design code for Method 2:&lt;/p&gt;
    &lt;code&gt;/**
 * 使用Mux进行移位，先用最低位，输出位宽为srcValue + 1(Sticky)
 */
def shiftRightWithMuxSticky(srcValue: UInt, shiftValue: UInt): UInt = {
  val vecLength  = shiftValue.getWidth + 1
  val res_vec    = Wire(Vec(vecLength,UInt(srcValue.getWidth.W)))
  val sticky_vec = Wire(Vec(vecLength,UInt(1.W)))
  res_vec(0)    := srcValue
  sticky_vec(0) := 0.U
  for (i &amp;lt;- 0 until shiftValue.getWidth) {
    res_vec(i+1) := Mux(shiftValue(i), res_vec(i) &amp;gt;&amp;gt; (1&amp;lt;&amp;lt;i), res_vec(i))
    sticky_vec(i+1) := Mux(shiftValue(i), sticky_vec(i) | res_vec(i)((1&amp;lt;&amp;lt;i)-1,0).orR,
    sticky_vec(i))
  }
  Cat(res_vec(vecLength-1),sticky_vec(vecLength-1))
}
&lt;/code&gt;
    &lt;p&gt;There is another method to speed up the right shift. The bit width of \(rshift\_value\) is \(exponentWidth+1\), while the width of \(fp\_c\_significand\_cat0\) is \(3*significandWidth+4\). There may be overflow bits in \(rshift\_value\). For example, using a 5-bit number to right-shift a 7-bit number, \(a(6,0) &amp;gt;&amp;gt; b(4,0)\), the maximum value of the third bit in \(b\) is \(7\), which is sufficient for the bit width of \(a\). Therefore, if the upper two bits of \(b\) contain any non-zero value, the right-shift result of \(a\) will be zero. The right-shift result can be simplified to \(Mux(b(4,3).orR,0.U, a(6,0) &amp;gt;&amp;gt; b(2,0))\). The table below lists the bit widths of \(rshift\_value\) used for three floating-point data formats.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;\(fp\_c\_significand\_cat0\) bit width&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width of \(rshift\_value\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit width used&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(37\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(76\)&lt;/cell&gt;
        &lt;cell&gt;\(9\)&lt;/cell&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(163\)&lt;/cell&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are three cases based on the value of \(rshift\_value\): \(rshift\_value &amp;lt;= 0\) means no right shift is needed, and the \(sticky\) result is \(0\); \(rshift\_value &amp;gt; rshiftMax\) means the right shift result is \(0\), and the \(sticky\) result is \(fp\_c\_significand\_cat0\) or reduced; \(0 &amp;lt; rshift\_value &amp;lt;= rshiftMax\) means the right shift result and \(sticky\) are calculated by \(shiftRightWithMuxSticky\).&lt;/p&gt;
    &lt;p&gt;Thus, this section has covered the methods for exponent processing, the design of the right shifter, and the handling of \(grs\) during the right-shift operation.&lt;/p&gt;
    &lt;head rend="h5"&gt;Significand addition&lt;/head&gt;
    &lt;p&gt;The \(rshift\_result\) of the significand of \(fp\_c\) after right-shifting must be added to the two results compressed by \(CSAn\_2\). Since the signs of \(fp\_c\) and \(fp\_a \times fp\_b\) may differ, subtraction is performed when they are opposite, and the result may be negative. To determine the sign, an additional sign bit is appended. \(fp\_c\_rshiftValue\_inv\) selects either \(rshift\_result\) (with a \(0\) sign bit) or its negation (with a \(1\) sign bit) based on whether the signs differ. Thus, \(fp\_c\_rshiftValue\_inv\) is added to the two results compressed by \(CSAn\_2\). However, during subtraction, \(fp\_c\_rshiftValue\_inv\) only negates \(rshift\_result\), and a \(+1\) is required at the least significant bit when all right-shifted \(grs\) bits are \(0\). This \(+1\) is placed in the \(carry\) bit of the two results compressed by \(CSAn\_2\), as the \(carry\) bit is always \(0\), saving adder usage and area. The three numbers have different bit widths: the right-shifted significand of \(fp\_c\) has a width of \(3 \times significandWidth + 4\), while the two results compressed by \(CSAn\_2\) have a width of \(2 \times significandWidth + 1\) (the \(+1\) accounts for the partial product extension to ensure correct carry). The strategy for summing these three numbers involves first compressing the lower \(2 \times significandWidth + 1\) bits of the \(CSAn\_2\) results and the lower \(2 \times significandWidth\) bits of \(rshift\_result\) (with a \(0\) appended to form \(2 \times significandWidth + 1\) bits) using \(CSA3\_2\) compression. The two compressed results are then summed, denoted as \(adder\_low\_bits\). Simultaneously, the higher \(significandWidth + 4\) bits of \(rshift\_result\) are incremented by \(1\). The final result selects either the higher \(significandWidth + 4\) bits of \(fp\_c\_rshiftValue\_inv\) or its incremented version based on whether the highest bit of the lower \(2 \times significandWidth + 1\) sum is \(1\), denoted as \(adder\_high\_bits\).&lt;/p&gt;
    &lt;p&gt;Additionally, consider the inversion and increment by one of the right-shifted \(grs\) during subtraction. The final significand addition result \(adder\) (including the right-shifted \(grs\)) consists of: \(adder\_high\_bits\), \(adder\_low\_bits\), and the right-shifted \(grs\) (inverted and incremented by one for subtraction). Since \(adder\) may be negative, an extra \(1\)-bit is extended solely for sign determination of \(adder\), which is later discarded. \(adder\_inv\) inverts \(adder\) when it is negative and removes this sign bit.&lt;/p&gt;
    &lt;head rend="h5"&gt;\(LZD\), left shift, rounded and unrounded mantissa results&lt;/head&gt;
    &lt;p&gt;After computing \(adder\_inv\), a leading-zero detection must be performed on \(adder\_inv\) to determine the number of left shifts required, thereby normalizing and rounding the mantissa result.&lt;/p&gt;
    &lt;p&gt;When performing LZD on \(adder\_inv\), there is an issue of exponent limitation. Let \(E\_greater\) be \(Eab\) (the exponent result from multiplying \(fp\_a\) and \(fp\_b\)). The left shift amount cannot exceed \(E\_greater\) because the exponent result would already be all zeros at that point. To address this, similar to the floating-point adder, a \(mask\) is used during left shift to limit the shift amount.&lt;/p&gt;
    &lt;p&gt;For cases where \(adder\) is negative, \(-adder\) should be the inversion of \(adder\) plus \(1\). Since adding \(1\) would create a long carry chain, only the inversion is performed, and then the \(LZD\) of \(adder\_inv\) is calculated. This may result in a one-bit deviation. When the inversion of \(adder\) ends with consecutive \(1\)s, adding \(1\) would cause a carry at the highest bit. To resolve this one-bit deviation, a trailing zero detection (\(TZD\)) is performed on \(adder\). If \(LZD + TZD\) equals the width of \(adder\), the inversion of \(adder\) ends with consecutive \(1\)s, requiring a correction to the left-shift result. After the left-shift correction, the unrounded result is obtained, and adding \(1\) to it yields the rounded result.&lt;/p&gt;
    &lt;head rend="h5"&gt;Final result&lt;/head&gt;
    &lt;p&gt;The sign bit result is determined based on the sign of \(adder\), while the calculation of \(grs\) requires combining both the right-shift process in step five and the left-shift process in step seven. The rounding strategy employs \(after \quad rounding\). To detect \(underflow\), an additional set of \(grs\) specifically for \(underflow\) checking is used. Based on the rounding mode and \(grs\), the necessity of rounding is determined, selecting the final mantissa result. The exponent result is derived according to the rounding outcome.&lt;/p&gt;
    &lt;p&gt;When input operands contain special values such as \(NaN\), infinity, or zero, the result is calculated separately. Depending on the actual input values, either the special result or the normal result is selected. Except for the divide-by-zero flag, all four other flag results can be generated.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector single-precision format algorithm&lt;/head&gt;
    &lt;p&gt;The main design principle for vector operations is to share hardware where timing requirements are met.&lt;/p&gt;
    &lt;p&gt;During Booth encoding, \(f16\) generates 6 partial products (pp), \(f32\) generates 13 pp, and \(f64\) generates 27 pp. Thus, the 27 pp positions generated by \(f64\) during Booth encoding can accommodate two sets of 13 pp from \(f32\), and similarly, the 13 pp positions from \(f32\) can hold two sets of 6 pp from \(f16\). This allows continued sharing of a single \(CSA\_27to2\) compression unit. The vector shared Booth encoding is illustrated in the figure.&lt;/p&gt;
    &lt;p&gt;During the right shift of the \(fp\_c\) mantissa, one of the right shifts for the mantissas in \(f64\) and \(f32\) can share a single shifter, while the other shifters remain independent.&lt;/p&gt;
    &lt;p&gt;The \(CSA\_3to2\) is also shared, with the third operand derived from the right-shifted result of the \(fp\_c\) mantissa. The right-shifted results of two \(f32\) or four \(f16\) mantissas are concatenated and then compressed with the two operands from the shared \(Booth\) encoding for \(3\_2\) compression.&lt;/p&gt;
    &lt;p&gt;The adder after compression is also shared. Different formats are assigned different bits, and the bits are separated to prevent low-bit carries from affecting high-bit results.&lt;/p&gt;
    &lt;p&gt;The shared logic for \(LZD\), \(TZD\), and the left shifter is similar to the right shifter, with \(f64\) and \(f32\) sharing one unit while others remain independent.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector Mixed-Precision Format Algorithm&lt;/head&gt;
    &lt;p&gt;There are two types of vector mixed-precision format calculations:&lt;/p&gt;
    &lt;p&gt;(1) \(2\) instances of \(fp32 = fp16 × fp16 + fp32\);&lt;/p&gt;
    &lt;p&gt;(2) One \(fp64 = fp32 × fp32 + fp64\).&lt;/p&gt;
    &lt;p&gt;For two multipliers of the same width, the essence is still adding exponents and multiplying significant bits. Unlike floating-point addition, there's no need to first convert their formats to match the result's format. Simply extending the bit width suffices—padding the exponent's high bits with zeros and the mantissa's low bits with zeros to align with high-precision floating-point operands. After alignment, computation proceeds according to the single-precision format.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;Division is one of the most representative floating-point functions in modern processors. There are two main algorithms for computing division in hardware: digit iteration algorithms based on subtraction with linear convergence, and multiplicative algorithms based on multiplication with quadratic convergence. The subtraction-based digit iteration algorithms are more energy-efficient and require less area. Subsequent references to digit iteration in this paper refer to subtraction-based digit iteration. For common floating-point precisions—double, single, and half—digit iteration methods are significantly faster. In digit iteration division, the most critical aspect is the selection of quotient bits, where each iteration yields one bit of the quotient. To implement a simple \(Radix-4\) selection function independent of the divisor, the divisor must be adjusted to a value sufficiently close to 1. This scaling is performed before digit iteration.&lt;/p&gt;
    &lt;p&gt;Digital iterative algorithms are widely used in high-performance processors due to their excellent trade-offs in performance, area, and power consumption. This paper is based on the \(SRT\) division (\(Sweeney-Robertson-Tocher Division\)), employing a \(Radix-64\) floating-point division algorithm that computes \(6\) quotient bits per cycle. To reduce overhead, each \(Radix-64\) iteration consists of three \(Radix-4\) iterations. Speculative algorithms are used between consecutive \(Radix-4\) iterations to reduce latency.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;The \(Radix-64\) scalar floating-point division algorithm implemented in this paper has low latency for double-precision, single-precision, and half-precision floating-point division when both input operands and results are normalized numbers, with latencies of \(11\), \(6\), and \(4\) cycles, respectively, including scaling and rounding cycles. In cases where input operands or results include denormalized numbers, one or two additional normalization cycles are required.&lt;/p&gt;
    &lt;p&gt;The exponent result can be easily derived, with the focus being on the division of significands. The significand divider performs floating-point division of the dividend significand \(x\) by the divisor significand \(d\) to obtain the significand quotient \(q = x/d\). Both operands need to be normalized numbers, \(x, d ∈ [1, 2)\). Denormalized operands are also permitted, with normalization applied before the digital iteration. If both operands are normalized within \([1, 2)\), the result lies within \([0.5, 2)\). Thus, two bits to the right of the least significant bit (\(LSB\)) of the quotient are required for rounding, namely the guard bit and the rounding bit.&lt;/p&gt;
    &lt;p&gt;When the result is normalized, the guard bit is used for rounding, with \(q ∈ [1, 2)\). When the result is unnormalized, the rounding bit is used for rounding, with \(q ∈ [0.5, 1)\). In the latter case, the result is left-shifted by \(1\) bit, and the guard and rounding bits become the \(LSB\) and guard bit of the normalized result, respectively. To simplify rounding, the result is forced to \(q ∈ [1, 2)\). Note that \(q &amp;lt; 1\) only occurs when \(x &amp;lt; d\). This condition is detected early, and the dividend is left-shifted by \(1\) bit, making \(q = 2 × x/d\) and \(q ∈ [1, 2)\). Note that the exponent result must be adjusted accordingly.&lt;/p&gt;
    &lt;p&gt;The algorithm used for division is the \(Radix-4\) digit iteration algorithm, with three iterations per cycle. The quotient's signed-digit representation uses the digit set {\(−2, −1, 0, +1, +2\)}, meaning the radix \(r = 4\) and the digit set \(a = 2\). In each iteration, a digit of the quotient is obtained through a selection function. To have a quotient digit selection function independent of the divisor, the divisor must be scaled to be close to \(1\). Naturally, to maintain result correctness, the dividend must be scaled by the same factor as the divisor.&lt;/p&gt;
    &lt;p&gt;Using the radix\(-4\) algorithm, each iteration yields 2 bits of the quotient. Since three radix\(-4\) iterations are performed per clock cycle, 6 quotient bits are obtained per cycle, equivalent to a \(Radix-64\) divider. Additionally, note that the first quotient bit of the integer result can only take values {\(+1, +2\)}, and its computation is much simpler than that of the remaining bits. By computing it in parallel with operand prescaling, one single-precision floating-point iteration is saved. On the other hand, there is an early termination mode for exceptional operands. Early termination is triggered when any operand is \(NaN\), infinity, or zero, or when dividing by a power of 2 with both operands normalized. In the latter case, the result is obtained simply by reducing the exponent of the dividend. The main features of the \(Radix-64\) divider are as follows:&lt;/p&gt;
    &lt;p&gt;(1) Pre-scaling of divisor and dividend.&lt;/p&gt;
    &lt;p&gt;(2) The first quotient digit is executed in parallel with pre-scaling.&lt;/p&gt;
    &lt;p&gt;(3) Compare the scaled dividend and divisor, and left-shift the dividend to obtain a result in the range \([1, 2)\).&lt;/p&gt;
    &lt;p&gt;(4) Three \(Radix-4\) iterations per cycle, processing \(6\) bits each cycle.&lt;/p&gt;
    &lt;p&gt;(5) Supports half-precision, single-precision, and double-precision.&lt;/p&gt;
    &lt;p&gt;(6) Denormal number support requires an additional cycle for normalization before iteration.&lt;/p&gt;
    &lt;p&gt;(7) Early termination for exceptional operands.&lt;/p&gt;
    &lt;head rend="h5"&gt;Digit-Recurrence Division Algorithm&lt;/head&gt;
    &lt;p&gt;Digit-recurrence division is an iterative algorithm where each iteration computes a \(radix-r\) quotient digit \(q_{i+1}\) and a remainder. The remainder \(rem[i]\) is used to obtain the next \(radix-r\) digit. For fast iteration, the remainder is stored in a carry-save adder using a signed-digit redundant representation. This paper selects a \(radix-2\) signed-digit representation for the remainder, consisting of a positive and a negative number. For radix \(r =4\), the following expression represents the partial quotient before the \(i\)-th iteration:&lt;/p&gt;
    &lt;p&gt;After scaling the divisor to around 1, the \(radix-4\) algorithm describes the quotient and remainder as follows:&lt;/p&gt;
    &lt;p&gt;Here, \(\widehat{rem}[i]\) is an estimate of the remainder \(rem[i]\), which consists of only a few bits. For this algorithm, it has been determined that only the 6 most significant bits (MSB) of the remainder are needed, i.e., 3 integer bits and 3 fractional bits. Then, each iteration extracts a quotient bit from the current remainder and computes a new remainder for the next iteration. The formula below calculates the number of iterations \(it\):&lt;/p&gt;
    &lt;p&gt;Here, \(n\) is the number of bits in the result, including those needed for rounding. The division latency, i.e., the number of cycles, is directly related to the number of iterations. It also depends on the number of iterations performed per cycle. Three iterations per cycle have been implemented to achieve \(6\) bits per cycle, equivalent to \(Radix-64\) division. The cycles (\(cycles\)) required for normalized floating-point numbers are determined by the following formula. In addition to the (\(it/3\)) cycles needed for iterations, there are two extra cycles for operand pre-scaling and rounding.&lt;/p&gt;
    &lt;p&gt;Some examples of digital iterative division, including the \(Radix-4\) algorithm, can be found in [\(38\)]. A simple implementation is shown in the figure. Note that only the most significant bit of the remainder is used to select the quotient bit. The remainder is updated using a carry-save adder (\(CSA\)) and stored in a redundant representation. The quotient bit selection then requires the \(t\) most significant bits of the remainder to be summed in a carry-propagate adder (\(CPA\)) to obtain its non-redundant representation. However, this implementation is too slow. To accelerate the iteration loop, speculative algorithms must be employed for both the remainder computation between iterations and the quotient bit selection.&lt;/p&gt;
    &lt;head rend="h5"&gt;Operand pre-scaling&lt;/head&gt;
    &lt;p&gt;During prescaling, the divisor is scaled to a value close to 1, making the selection of quotient digits independent of the divisor. For the \(radix-4\) algorithm, scaling the divisor to the range \([1 − 1/64, 1+1/8]\) is sufficient. As shown in the prescaling factor truth table, only three bits determine the scaling factor. Note that during prescaling, the divisor should be scaled by a factor of \(1-2\). The dividend should also be scaled by the same factor.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(0.1\)xxx&lt;/cell&gt;
        &lt;cell role="head"&gt;Pre-scaling factor&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(000\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+1/2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(001\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+1/2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;010&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(011\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/2+0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(100\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(101\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1/4+0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(110\)&lt;/cell&gt;
        &lt;cell&gt;\(1+0+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(111\)&lt;/cell&gt;
        &lt;cell&gt;\(1+0+1/8\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Integer Quotient Calculation&lt;/head&gt;
    &lt;p&gt;While computing the integer quotient, the following data is provided for the digit iteration steps (each digit iteration performs three \(radix-4\) operations, corresponding to the \(s0\), \(s1\), and \(s2\) stages):&lt;/p&gt;
    &lt;p&gt;(1) Redundant remainder in carry-save representation: \(f\_r\_s\), \(f\_r\_c\).&lt;/p&gt;
    &lt;p&gt;(2) Pre-scaled divisor: \(divisor\).&lt;/p&gt;
    &lt;p&gt;(3) Provides a 6-bit remainder result for the quotient selection in the \(s0\) stage of the first digital iteration.&lt;/p&gt;
    &lt;p&gt;(4) Provides a 7-bit remainder result for the quotient selection in the \(s1\) stage of the first digit iteration.&lt;/p&gt;
    &lt;head rend="h6"&gt;Digital iteration&lt;/head&gt;
    &lt;p&gt;The actual implementation of the floating-point divider requires executing three \(radix-4\) iterations per cycle. Conventional sequential iteration three times is too slow to meet timing requirements, so the logic has been optimized. The figure illustrates the block diagram of the digit-recurrence loop.&lt;/p&gt;
    &lt;p&gt;(1) Process the divisor to obtain five possible quotient selection results, requiring the use of divisor multiples (only negate when the quotient is negative).&lt;/p&gt;
    &lt;p&gt;(2) In the \(s0\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)) to predictively compute the five remainder redundant representations needed for the \(s1\) stage in parallel during \(s0\).&lt;/p&gt;
    &lt;p&gt;(3) In the \(s0\) stage, using the five remainder redundant representations calculated in the second step, predictively compute five 7-bit remainder results for the \(s2\) stage.&lt;/p&gt;
    &lt;p&gt;(4) In the \(s0\) stage, the quotient for the \(s0\) stage is selected based on the 6-bit remainder result in the input signal. The quotient is represented using a 5-bit one-hot code.&lt;/p&gt;
    &lt;p&gt;(5) Based on the quotient from stage \(s0\), select the redundant remainder representation needed for stage \(s1\), and predictively choose one of the five 7-bit remainder results calculated in step three for stage \(s2\).&lt;/p&gt;
    &lt;p&gt;(6) In the \(s1\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)), and the five remainder redundant representations needed for the \(s2\) stage are predictively calculated in parallel.&lt;/p&gt;
    &lt;p&gt;(7) In the \(s1\) stage, predictively perform the quotient selection for the \(s1\) stage based on the \(7\)-bit remainder result from the input signal, the divisor multiples used for the five quotient selection results, and the quotient from the \(s0\) stage.&lt;/p&gt;
    &lt;p&gt;(8) Based on the quotient from stage \(s1\), select the redundant remainder representation required for stage \(s2\).&lt;/p&gt;
    &lt;p&gt;(9) In the \(s2\) stage, four \(CSA\) modules are used (not required when the quotient is \(0\)) to predictively compute the five redundant remainder representations needed for the next digit iteration in the \(s0\) stage in parallel.&lt;/p&gt;
    &lt;p&gt;(10) In the s2 stage, predictively compute five possible results for the 6-bit remainder required in the s0 stage and the 7-bit remainder required in the s1 stage of the next digit iteration.&lt;/p&gt;
    &lt;p&gt;(11) In the \(s2\) stage, based on the \(7\)-bit remainder result selected for the \(s2\) stage in the fifth step, the divisor multiples used for the five quotient selection results, and the quotient from the \(s1\) stage, the quotient for the \(s2\) stage is predictively selected.&lt;/p&gt;
    &lt;p&gt;(12) Based on the quotient selection result from the \(s2\) stage, the following are selected for the next digit iteration: the carry-save representation of the redundant remainder, the 6-bit remainder result required for the \(s0\) stage, and the 7-bit remainder result required for the \(s1\) stage.&lt;/p&gt;
    &lt;p&gt;Since the divisor's multiple is only inverted in the first step without \(+1\), there will be a deviation in the remainder calculation. Correction logic is added during the quotient selection process to rectify this. The table below shows the standard quotient selection function, and the subsequent table presents the quotient selection function after logical correction.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;\(4 × rem[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(q_{i+1}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([13/8,31/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([4/8,12/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([-3/8,3/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\([-12/8,-4/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\([-32/8,-13/8]\)&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(4 × rem[i]\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(carry\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(q_{i+1}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(31/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([13/8,30/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(12/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(+2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(12/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([4/8,11/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(3/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(3/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([-3/8,2/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-4/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-4/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\([-12/8, -5/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-13/8\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(-1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(-13/8\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\([-32/8,14/8]\)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;\(-2\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Convert the redundant remainder representation of the iteratively output digits back to a standard remainder. Use \(On\) \(the\) \(Fly\) \(Conversion\) to compute both the quotient and quotient minus one, calculate two sets of \(grs\) and the signal for whether rounding up is needed, determine the selection signal for choosing between the quotient or quotient minus one, and finally select the correct quotient result. Perform rounding using the correct quotient result and its corresponding rounding-up signal.&lt;/p&gt;
    &lt;head rend="h6"&gt;Denormal numbers and early termination&lt;/head&gt;
    &lt;p&gt;(1) The input contains denormal numbers. The significand of a denormal number is less than \(1\) and cannot be pre-scaled together with normal numbers. Therefore, an additional cycle is added to normalize the significand of denormal numbers while simultaneously adjusting their exponents.&lt;/p&gt;
    &lt;p&gt;(2) The result is a denormal number. The quotient result after digit iteration is greater than 1, which does not meet the denormal significand range. An additional cycle is required to right-shift the quotient result for normalization.&lt;/p&gt;
    &lt;p&gt;(3) Early termination occurs in two scenarios: when the result is \(NaN\), infinity, or exact \(0\), computation can terminate early and output the result since this information is available in the first cycle, allowing the division result to be output in the second cycle; when the divisor is a power of \(2\), its significand \(=1\), and division only requires processing the exponent of the dividend, skipping the digit iteration phase, enabling the division result to be output as early as the second cycle. However, additional cycles are still needed if the dividend or result is a denormal number.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector floating-point division algorithm&lt;/head&gt;
    &lt;p&gt;For vector floating-point division, the RISC-V vector instruction set extension does not support mixed-precision floating-point division, thus only the following needs to be supported:&lt;/p&gt;
    &lt;p&gt;(1) 1 f64 = f64 + f64;&lt;/p&gt;
    &lt;p&gt;(2) \(2\) \(f32 = f32 + f32\);&lt;/p&gt;
    &lt;p&gt;(3) \(4\) \(f16 = f16 + f16\).&lt;/p&gt;
    &lt;p&gt;Considering that vector division involves multiple division computations simultaneously, and early termination can cause asynchronous output of results unless all cases terminate early under the same conditions, the early termination mechanism is disabled for vector division. If early termination occurs, the result is temporarily stored internally and output simultaneously with other division results.&lt;/p&gt;
    &lt;p&gt;To unify timing, the divider's cycle count is standardized to the worst-case scenario, i.e., when the input contains denormal numbers and the output also contains denormal numbers. Other cases that could produce results faster are internally buffered until the standardized cycle count is reached before outputting the result.&lt;/p&gt;
    &lt;p&gt;The main design employs resource reuse, with the following data reuse in the non-numeric iteration module:&lt;/p&gt;
    &lt;p&gt;(1) \(1\) \(f64/f32/f16 = f64/f32/f16 + f64/f32/f16\);&lt;/p&gt;
    &lt;p&gt;(2) 1 \(f32/f16 = f32/f16 + f32/f16\);&lt;/p&gt;
    &lt;p&gt;(3) \(2\) \(f16\) values \(= f16 + f16\).&lt;/p&gt;
    &lt;p&gt;A total of 4 signal groups are used to achieve the functionality of 7 division groups.&lt;/p&gt;
    &lt;p&gt;Since the digital iteration module is a critical path with significant timing pressure, achieving high reuse with non-digital iteration modules is not feasible without compromising timing requirements. Therefore, a partial reuse design is implemented for the digital iteration module:&lt;/p&gt;
    &lt;p&gt;(1) The interface consists of four sets of quotients and redundant remainders.&lt;/p&gt;
    &lt;p&gt;(2) The \(s0\) stage uses \(7\) sets of \(CSA\) and \(7\) sets of prediction, with \(4\) sets of quotient selection.&lt;/p&gt;
    &lt;p&gt;(3) Stages \(s1\) and \(s2\) utilize \(4\) sets of \(CSA\), \(4\) sets of prediction, and \(4\) sets of quotient selection.&lt;/p&gt;
    &lt;p&gt;Registers also adopt resource reuse. For divisor, redundant remainder, quotient, and other registers, the bit width is allocated based on the maximum required by \(4\) \(f16\), \(2\) \(f32\), or \(1\) \(f64\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Hardware Design&lt;/head&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Adder&lt;/head&gt;
    &lt;head rend="h4"&gt;Scalar single-precision floating-point adder&lt;/head&gt;
    &lt;p&gt;A scalar single-precision floating-point adder is designed based on the improved dual-path floating-point addition algorithm, with its hardware implementation architecture shown in the figure.&lt;/p&gt;
    &lt;p&gt;The two input operands on the left are \(fp\_a\) and \(fp\_b\), while \(fp\_c\) on the right represents the addition result. \(fflags\) is a 5-bit exception flag, and \(rm\) is the rounding mode, with five modes represented by 3 bits. When \(is\_sub\) is 0, \(fp\_c = fp\_a + fp\_b\) is computed; when \(is\_sub\) is 1, \(fp\_c = fp\_a - fp\_b\) is computed. The difference between floating-point addition and subtraction lies only in the sign bit of \(fp\_b\), so minor adjustments to \(fp\_b\)'s sign bit enable the floating-point adder to support both operations. The overall design consists of three parts: the \(far\) path, the \(close\) path, and the exception path.&lt;/p&gt;
    &lt;p&gt;The far path first performs two parallel normalized exponent subtractions with significand right shifts, handling the cases where Efp_a ≥ Efp_b and Efp_b ≥ Efp_a separately. The correct right-shift result is selected based on the magnitude relationship between Efp_a and Efp_b and sent to the FS0 and FS1 significand adders. For subtraction, the far path sets EA as the larger exponent minus one, while for addition, EA is the larger exponent. This ensures the significand addition result falls within the range [1,4). During the right shift, two sets of grs are computed: grs_normal for rounding when the value is in [1,2), and grs_overflow for rounding when the value is in [2,4). Finally, based on the FS0 result and rounding mode, either FS0 or FS1 is selected as the significand result, and either EA or EA+1 is chosen as the exponent result. The sign bit result is determined by the exponent magnitude. The flag results indicate overflow if EA+1 is all ones and inexactness based on grs. The far path does not generate divide-by-zero, invalid operation, or underflow flags.&lt;/p&gt;
    &lt;p&gt;The \(close\) path uses four significant-digit adders, \(CS0\), \(CS1\), \(CS2\), and \(CS3\), to handle significant-digit subtraction for the cases where \(Efp\_a = Efp\_b\), \(Efp\_a = Efp\_b + 1\), and \(Efp\_a = Efp\_b – 1\). Based on the \(CS0\) result and \(grs\), four one-hot selection signals, \(sel\_CS0\), \(sel\_CS1\), \(sel\_CS2\), and \(sel\_CS3\), are generated. A four-input one-hot multiplexer (\(Mux1H\)) selects one result, which is ORed with the left-shifted \(mask\). A priority left shifter then normalizes the mantissa, outputting the \(lzd\) value during the shift. The exponent result is \(EA – lzd\), and the mantissa result is chosen between the normalized mantissa and \(CS4\), where \(CS4\) is a supplementary rounding result that does not require left-shift normalization. The sign result is derived from the exponent difference and the \(CS0\) result. The flag result only indicates imprecision; no other exception flags are generated.&lt;/p&gt;
    &lt;p&gt;The exception path is used to determine whether the operation is invalid, whether the result is \(NaN\), or whether the result is infinite. When none of these conditions are met, normal computation proceeds, generating a selection signal to choose the result and flags from either the \(far\) path or the \(close\) path as output.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scalar mixed-precision floating-point adder&lt;/head&gt;
    &lt;p&gt;Building upon the scalar single-precision floating-point adder, a mixed-precision hardware design is implemented. The main difference lies in supporting mixed-precision computation. Taking the result as \(f32\) as an example, the table below shows the truth table for the operations corresponding to \(res\_widen\) and \(opb\_widen\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\(res\_widen\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(opb\_widen\)&lt;/cell&gt;
        &lt;cell role="head"&gt;\(f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f32 + f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f16 + f16\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(f32 = f16 + f32\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Not allowed&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The figure below shows the architecture of a scalar mixed-precision floating-point adder. The main difference is the addition of a fast format conversion module at the data input. Based on the operation type, this module converts the operands into the result's data format before processing, after which the computation flow is identical to that of a single-precision floating-point adder.&lt;/p&gt;
    &lt;head rend="h4"&gt;Vector Floating-Point Adder&lt;/head&gt;
    &lt;p&gt;The diagram below shows the architecture of the vector floating-point adder. To meet timing requirements, it is composed of four modules: \(FloatAdderF64Widen\) handles all operations with 64-bit output results, \(FloatAdderF32WidenF16\) handles all operations with 16-bit or 32-bit output results, and \(FloatAdderF16\) handles only operations with 16-bit output results.&lt;/p&gt;
    &lt;p&gt;Here, \(fp\_format\) is a 2-bit result format control signal: \(00\) indicates the result format is \(f16\), \(01\) indicates \(f32\), and \(10\) indicates \(f64\). The output flags are 20 bits, arranged with lower bits being significant. When the result format is \(f16\), all 20 bits are valid; for \(f32\), the lower 10 bits are valid; and for \(f64\), the lower 5 bits are valid.&lt;/p&gt;
    &lt;p&gt;The vector floating-point adder employs a two-stage pipeline design. To achieve rapid wake-up, the addition result is computed in approximately 1.5 cycles. Pipeline partitioning is performed within each submodule, requiring only the insertion of a single register level. Below is an explanation of the pipeline partitioning for the three modules shown in the diagram.&lt;/p&gt;
    &lt;p&gt;The diagram below illustrates the pipeline partitioning of the \(FloatAdderF64Widen\) module. The \(far\) path inserts registers after the significand right shift, while the \(close\) path inserts registers after the \(Mux1H\).&lt;/p&gt;
    &lt;p&gt;The figure below shows the pipeline division of the \(FloatAdderF32WidenF16\) module, which includes calculations for two different output formats. The selection logic in the second cycle is complex, so registers are inserted within the adder in the \(far\) path. The first cycle performs the addition of the lower \(18\) bits and the higher bits, while the second cycle combines the carry from the lower \(18\)-bit addition of the first cycle with the higher bits to obtain the final result. The \(close\) path also inserts registers after \(Mux1H\).&lt;/p&gt;
    &lt;p&gt;The following diagram shows the pipeline partitioning of the \(FloatAdderF16\) module. This module has minimal timing pressure and adopts a partitioning method where the \(far\) path inserts registers after the right shift of significant bits, and the \(close\) path inserts registers after \(Mux1H\).&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;The previously introduced vector floating-point adder has a width of \(64\) bits, requiring both operands to be in vector form. However, \(RVV\) not only specifies that both operands are in vector form (\(vector-vector\), abbreviated as \(vv\)) but also allows one operand to be a vector and the other a scalar (\(vector-scalar\), abbreviated as \(vf\)). Additionally, under \(widening\) instructions, the arrangement of source operands is not limited to the lower significant part. When the source register width is half of the destination register width, the data source may come from either the lower or upper half.&lt;/p&gt;
    &lt;p&gt;To implement all floating-point instruction calculations in \(RVV\) and support \(VLEN\) extension, simple instruction computations are added to the vector floating-point adder, transforming it into a vector floating-point "\(ALU\)", referred to as \(VFALU\).&lt;/p&gt;
    &lt;p&gt;Therefore, the vector floating-point adder needs to be modified to adapt to the features of \(RVV\). The modifications consist of two parts: functional modifications and interface modifications.&lt;/p&gt;
    &lt;p&gt;The table below lists the opcodes supported by \(VFALU\), totaling \(16\) operations, where (\(w\)) indicates operations involving \(widen\). The operand formats for \(vfmerge\), \(vfmove\), and \(vfclass\) are special: \(vfmerge.vfm\) has three source operands—a vector register, a floating-point register, and a \(mask\) register; \(vfmove.v.f\) has only one floating-point register as the source operand; \(vfclass\) has only one vector register as the source operand.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Corresponding instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;Operand format&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)add\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Addition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)sub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Subtraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmin\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Find the minimum value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmax\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Find Maximum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmerge\)&lt;/cell&gt;
        &lt;cell&gt;\(vfm\)&lt;/cell&gt;
        &lt;cell&gt;Data merging&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmove\)&lt;/cell&gt;
        &lt;cell&gt;\(v.f\)&lt;/cell&gt;
        &lt;cell&gt;Data movement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnj\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Sign Injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnjn\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Sign inversion injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(vfsgnjx\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;XOR sign injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(9\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfeq\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether equal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(10\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfnq\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Not Equal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
        &lt;cell&gt;\(vmflt\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether it is less than&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(12\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfle\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;Less than or equal to&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(13\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfgt\)&lt;/cell&gt;
        &lt;cell&gt;\(vf\)&lt;/cell&gt;
        &lt;cell&gt;Whether greater than&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
        &lt;cell&gt;\(vmfge\)&lt;/cell&gt;
        &lt;cell&gt;\(vf\)&lt;/cell&gt;
        &lt;cell&gt;Is greater than or equal to&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;\(vfclass\)&lt;/cell&gt;
        &lt;cell&gt;\(v\)&lt;/cell&gt;
        &lt;cell&gt;Classification&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The table below defines the \(VFALU\) interface. Compared to the vector floating-point adder, it adds two mixed-precision data sources, \(widen\_a\) and \(widen\_b\). When the source and destination operand formats are the same, the data comes from \(fp\_a\) and \(fp\_b\); otherwise, it comes from \(widen\_a\) and \(widen\_b\). When \(uop\_idx=0\), the lower half is taken, and when \(uop\_idx=1\), the upper half is taken. When \(is\_frs1=1\), the source operand \(vs1\) comes from the floating-point register \(frs1\), which needs to be replicated into a vector register for computation. \(mask\) participates in the calculation of the \(merge\) instruction, and \(op\_code\) is the operation code indicating the operation to be performed.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_a&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_a\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Floating-Point Register Data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Addend sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(mask\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;Participate in \(merge\) instruction computation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(uop\_idx\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Select upper/lower half when \(widen\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(round\_mode\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(res\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\) instruction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opb\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Is the source operand \(vs1\) in the same format as the result?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;Opcode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_result&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector Floating-Point Fused Multiply-Add Unit&lt;/head&gt;
    &lt;head rend="h4"&gt;Pipeline Partitioning&lt;/head&gt;
    &lt;p&gt;The vector floating-point fused multiply-adder adopts a four-stage pipeline design to achieve rapid wake-up, ensuring the multiply-add result is computed in approximately \(3.5\) cycles. The vector unit's latency is \(3.5\) cycles. The diagram below illustrates the architecture of the vector floating-point fused multiply-adder, where \(reg\_0\) denotes the first-stage register, \(reg\_1\) the second-stage, and \(reg\_2\) the third-stage. The vector floating-point fused multiply-adder also supports \(widen\) functionality, limited to \(f32 = f16 × f16 + f32\) and \(f64 = f32 × f32 + f64\) cases. Thus, only a single-bit \(widen\) signal is needed for control when the output format is fixed. The output \(fflags\) is also \(20\) bits, consistent with the representation in the vector floating-point adder.&lt;/p&gt;
    &lt;p&gt;To save area while meeting timing constraints, a resource-sharing implementation is adopted. Calculations for all data formats use the same vector Booth encoder and CSA compression. By interleaving the layout, the 107-bit adder also achieves resource sharing.&lt;/p&gt;
    &lt;p&gt;In the first cycle, seven sets of exponent processing are performed to obtain seven right-shift values. The corresponding right-shift value is selected based on the computation format. For the right shifters, the \(f64\) right shifter is shared with one \(f32\), while a separate \(f32\) and four \(f16\) right shifters are dedicated. If subtraction is performed, the right-shifted result of \(fp\_c\)'s mantissa is inverted before being fed into the first-stage register. Simultaneously, vector \(Booth\) encoding is performed in the first cycle, generating 27 partial products, which are compressed into 4 partial products using \(CSA\) and then registered.&lt;/p&gt;
    &lt;p&gt;In the second cycle, compress the remaining 4 partial products using \(CSA4\_2\), then compress the result with the first cycle's right-shifted significand using \(CSA3\_2\). Perform a 107-bit addition and register the result in the second-stage register.&lt;/p&gt;
    &lt;p&gt;In the third cycle, the sum result from the second cycle undergoes \(lzd\) and \(tzd\), followed by a left shift with \(mask\) limitation. The shifted result is stored in the third-stage register.&lt;/p&gt;
    &lt;p&gt;In the fourth cycle, rounding is performed to obtain the mantissa result. The exponent result is calculated based on the left shift condition in the third cycle. The sign bit can be obtained from the \(107\)-bit adder in the second cycle. The flag results can generate four types of flags: overflow, underflow, invalid operation, and inexact. Note the method for detecting underflow. \(IEEE-754\) specifies two methods for detecting underflow: \(before \quad rounding\) and \(after \quad rounding\). This design uses the \(after \quad rounding\) method selected by \(RISC-V\) to detect underflow.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;According to the \(RVV\) instruction definitions, vector floating-point fused multiply-add units can be reused for multiplication calculations, controlled by \(op\_code\). When performing multiplication, the internal adder is set to zero. Additionally, \(RVV\) defines a series of floating-point fused multiply-add instructions, primarily differing in sign bits and operand order. The vector floating-point fused multiply-add unit is modified to support all related instructions as \(VFMA\), with added \(op\_code\) and interfaces. The following table lists the \(VFMA\) opcodes, totaling \(9\) operations, all supporting \(vv\) and \(vf\) operand forms. For \(vf\), \(vs1[i]\) is replaced by the floating-point register \(frs1\).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell role="head"&gt;Corresponding instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;Operand format&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(0\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)mul\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = vs[2] × vs1[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)macc\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vs2[i]) + vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)nmacc\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vs2[i]) - vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)msac\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vs2[i]) - vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(4\)&lt;/cell&gt;
        &lt;cell&gt;\(vf(w)nmsac\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vs2[i]) + vd[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmadd\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vd[i]) + vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(6\)&lt;/cell&gt;
        &lt;cell&gt;\(vfnamdd\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vd[i]) - vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
        &lt;cell&gt;\(vfmsub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = +(vs1[i] × vd[i]) - vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(8\)&lt;/cell&gt;
        &lt;cell&gt;\(vfnmsub\)&lt;/cell&gt;
        &lt;cell&gt;\(vv,vf\)&lt;/cell&gt;
        &lt;cell&gt;\(vd[i] = -(vs1[i] × vd[i]) + vs2[i]\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The table below shows the \(VFMA\) interface. To simplify control logic complexity, the three operands sent to \(VFMA\) are fixed in the order \(vs2\), \(vs1\), \(vd\). The functional unit internally adjusts the order based on \(op\_code\). Since the \(fma\) instruction uses a fixed target format for the addend during \(widen\), only \(widen\_a\) and \(widen\_b\) need to be added. \(uop\_idx\) is similarly used to select the upper or lower half of \(widen\_a\) and \(widen\_b\). \(frs1\) and \(is\_frs1\) are used to support \(vf\) instructions.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_a&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_c\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Source operand \(vd\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_a\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs2\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(widen\_b\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\_vs1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Floating-Point Register Data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Addend sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(uop\_idx\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Select upper/lower half when \(widen\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(round\_mode\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(res\_widening\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;\(widen\) instruction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(op\_code\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(5\)&lt;/cell&gt;
        &lt;cell&gt;Opcode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;fp_result&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector floating-point divider&lt;/head&gt;
    &lt;head rend="h4"&gt;Scalar Floating-Point Divider&lt;/head&gt;
    &lt;p&gt;The scalar floating-point divider supports computations in three formats: \(1\) \(f16 = f16 / f16\), \(1\) \(f32 = f32 / f32\), and \(1\) \(f64 = f64 / f64\). The divider employs a \(Radix-64\) algorithm, where the iterative module performs three \(Radix-4\) iterations per cycle to achieve \(Radix-64\). The figure below shows the architecture of the scalar floating-point divider. The divider operates in a blocking manner and cannot accept the next division operation during computation, requiring handshake signals for control. This design uses \(start-valid\) handshake signals. Since the \(CPU\) may encounter branch prediction failures that flush pipeline states, a dedicated \(flush\) signal is included to clear the divider's internal state, allowing it to immediately start a new division operation in the next cycle.&lt;/p&gt;
    &lt;p&gt;Input data falls into three categories: both are normalized numbers (excluding divisors that are powers of \(2\)), at least one is a denormal number, and early termination (input contains \(NaN\), infinity, zero, or the divisor is a power of \(2\)). Results fall into two categories: the result is a normalized number, or the result is a denormal number.&lt;/p&gt;
    &lt;p&gt;When the inputs are all normalized numbers (excluding divisors that are powers of 2), the mantissas are normalized, and the process directly proceeds to the pre-scaling stage. When at least one input is a denormalized number, compared to the case where all inputs are normalized, an additional cycle is required for mantissa normalization before pre-scaling.&lt;/p&gt;
    &lt;p&gt;The prescaling stage takes one cycle, followed by integer quotient selection, where the two-bit integer quotient result is selected, and the prescaled divisor, dividend, and remainder's carry-save redundant representation are provided for the \(Radix-4\) iteration. The \(Radix-4\) iteration module calculates 6 bits of the quotient per cycle. \(f16\) division requires 2 cycles of \(Radix-4\) iteration, \(f32\) division requires 6 cycles, and \(f64\) division requires 9 cycles. After \(Radix-4\) iteration, the resulting mantissa quotient ranges between \((1, 2)\). When the result is a normalized number, only one cycle is needed for rounding and exponent result calculation to obtain the final division result. When the result is a denormal number, an additional cycle is required to denormalize the quotient before rounding.&lt;/p&gt;
    &lt;p&gt;Early termination is divided into two scenarios: (1) When the input operands contain NaN, infinity, or zero, division computation is unnecessary, and the result can be output in the second cycle. (2) When the divisor is a power of 2, the exponent result can be obtained in the first cycle. If the result does not require denormalization steps, it can be output in the second cycle; if denormalization is needed, an additional cycle is required, and the result is output in the third cycle.&lt;/p&gt;
    &lt;p&gt;The table below shows the required computation cycles for scalar dividers under different data formats, where \(+1\) indicates an additional cycle for post-processing when the division result is denormalized. In early termination cases, division operations for all data formats can be completed in just \(1\) to \(2\) cycles. Without early termination, \(f16\) division requires \(5\) to \(7\) cycles, \(f32\) division requires \(7\) to \(9\) cycles, and \(f64\) division requires \(12\) to \(14\) cycles.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Normalized Number&lt;/cell&gt;
        &lt;cell role="head"&gt;Denormal number&lt;/cell&gt;
        &lt;cell role="head"&gt;Early termination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(5+1\)&lt;/cell&gt;
        &lt;cell&gt;\(6+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(7+1\)&lt;/cell&gt;
        &lt;cell&gt;\(8+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(12+1\)&lt;/cell&gt;
        &lt;cell&gt;\(13+1\)&lt;/cell&gt;
        &lt;cell&gt;\(1+1\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Vector floating-point divider&lt;/head&gt;
    &lt;p&gt;The figure below shows the architecture of the vector floating-point divider. Compared to the scalar floating-point divider, since vector division computes multiple divisions simultaneously and all results must be written back to the register file together, early termination of a single division offers little benefit for vector division acceleration. Thus, the feature of variable output latency is removed. In all cases, the latency of the vector floating-point divider is fixed based on the input data format, as shown in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Data Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Calculation Cycle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(f16\)&lt;/cell&gt;
        &lt;cell&gt;\(7\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;\(f32\)&lt;/cell&gt;
        &lt;cell&gt;\(11\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(f64\)&lt;/cell&gt;
        &lt;cell&gt;\(14\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In hardware design, aside from the \(Radix-64\) iteration module, the vector floating-point divider employs logic reuse, utilizing four signal groups for computation and control: the first group computes \(f64\_0\), \(f32\_0\), or \(f16\_0\); the second computes \(f32\_1\) or \(f16\_1\); the third computes \(f16\_2\); and the fourth computes \(f16\_3\). Registers are also reused to store intermediate results, with widths sized to \(max\) (1 \(f64\), 2 \(f32\), or 4 \(f16\)) to meet maximum requirements. The \(Radix-64\) iteration module is the critical path, optimized for timing while minimizing area. The first \(Radix-4\) iteration uses 7 independent \(CSA\) and quotient selection units, while the second and third iterations reuse 4 \(CSA\) and quotient selection units.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interface Description&lt;/head&gt;
    &lt;p&gt;The \(RVV\) specification defines three vector floating-point division instructions:&lt;/p&gt;
    &lt;p&gt;① \(vfdiv.vv \quad vd[i] = vs2[i]/vs1[i]\)&lt;/p&gt;
    &lt;p&gt;② \(vfdiv.vf \quad vd[i] = vs2[i]/f[rs1]\)&lt;/p&gt;
    &lt;p&gt;③ \(vfrdiv.vf \quad vd[i] = f[rs1]/vs2[i]\)&lt;/p&gt;
    &lt;p&gt;Case ③ is special as the operand order differs from cases ① and ②. For the vector division unit, the first operand is passed by the control logic as \(vs2[i]/f[rs1]\), and the second operand is passed as \(vs1[i]/f[rs1]/vs2[i]\). Thus, the functional unit sees the dividend in either vector or scalar form, and the divisor is also in vector or scalar form. Therefore, two additional scalar data interfaces are required. After adding these interfaces, the module is named \(VFDIV\), with the interfaces as shown in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Interface&lt;/cell&gt;
        &lt;cell role="head"&gt;Direction&lt;/cell&gt;
        &lt;cell role="head"&gt;Bit Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(start\_valid\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(finish\_ready\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(flush\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Flush signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fp\_format\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(2\)&lt;/cell&gt;
        &lt;cell&gt;Floating-point format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opa\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Dividend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(opb\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Divisor&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs2\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Dividend comes from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(frs1\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Divisor sourced from floating-point register data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs2\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Dividend sourced from floating-point register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(is\_frs1\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;The divisor comes from the floating-point register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(rm\_i\)&lt;/cell&gt;
        &lt;cell&gt;\(input\)&lt;/cell&gt;
        &lt;cell&gt;\(3\)&lt;/cell&gt;
        &lt;cell&gt;Rounding mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(start\_ready\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(finish\_valid\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(1\)&lt;/cell&gt;
        &lt;cell&gt;Handshake signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;\(fpdiv\_res\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(64\)&lt;/cell&gt;
        &lt;cell&gt;Computation result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\(fflags\_o\)&lt;/cell&gt;
        &lt;cell&gt;\(output\)&lt;/cell&gt;
        &lt;cell&gt;\(20\)&lt;/cell&gt;
        &lt;cell&gt;Flag bits&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Vector format conversion module \(VCVT\)&lt;/head&gt;
    &lt;p&gt;The \(VCVT\) module is a three-stage pipelined vector floating-point format conversion module. It instantiates two \(VectorCvt\) submodules capable of processing \(64\)-bit data. Each \(VectorCvt\) contains one \(cvt64\), one \(cvt32\), and two \(cvt16\) modules. The \(cvt64\) supports processing floating-point/integer formats of \(64\), \(32\), \(16\), and \(8\) bits. The \(cvt32\) supports \(32\), \(16\), and \(8\)-bit floating-point/integer formats, while the \(cvt16\) supports \(16\) and \(8\)-bit floating-point/integer formats. Thus, \(VectorCvt\) can simultaneously process one \(64\)-bit (or two \(32\)-bit, or four \(16\)-bit, or four \(8\)-bit) floating-point/integer format input data for conversion.&lt;/p&gt;
    &lt;head rend="h4"&gt;Overall design&lt;/head&gt;
    &lt;head rend="h4"&gt;Module Design&lt;/head&gt;
    &lt;p&gt;The \(CVT\) module includes single-width floating-point/integer type conversion instructions, widening floating-point/integer type conversion instructions, narrowing floating-point/integer type conversion instructions, vector floating-point reciprocal square root estimation instructions, and vector floating-point reciprocal estimation instructions.&lt;/p&gt;
    &lt;p&gt;Select different \(cvt\) module calls based on \(width\). The design approach for the \(cvt\) module is divided into four types based on instruction type: \(fp2int\), \(int2fp\), \(fp2fp\), and \(vfr\). The overall design approach for \(fcvt64\) is to unify the format of the input \(64bit\) data:&lt;/p&gt;
    &lt;p&gt;different width unsigned/signed int -&amp;gt; 65 signed int&lt;/p&gt;
    &lt;p&gt;\(f16/f32/f64 -&amp;gt; 65bit (f64 \#\# false.B)\)&lt;/p&gt;
    &lt;p&gt;After standardizing the format, there is no longer a need to distinguish between different types of data, their bit widths, or field positions during the conversion process to a certain extent.&lt;/p&gt;
    &lt;p&gt;Building on this, \(VFCVT64\) is divided into 5 categories: \(int -&amp;gt; fp\), \(fp -&amp;gt; fp\) widen, \(fp -&amp;gt; fp\) narrow, estimate7 (\(rsqrt7\) &amp;amp; \(rec7\)), and \(fp -&amp;gt; int\).&lt;/p&gt;
    &lt;head rend="h4"&gt;\(FuopType\) decoding logic&lt;/head&gt;
    &lt;p&gt;For the \(cvt\) instruction: its \(fuopType\) consists of \(9\) bits, with each bit representing the following information:&lt;/p&gt;
    &lt;p&gt;Here, \([5:0]\) is obtained from the manual, and \([8:6]\) is additionally added during the design of control signal generation for convenience.&lt;/p&gt;
    &lt;p&gt;\([8]:1\) indicates it is a \(move\) instruction, \(0\) represents \(cvt\) instruction or the two estimation instructions \(vfrsqrt7\) and \(vfrec7\).&lt;/p&gt;
    &lt;p&gt;\([7]: 1\) indicates the input is \(fp\), \(0\) indicates the input is \(int\).&lt;/p&gt;
    &lt;p&gt;\([6]\): \(1\) indicates the output is \(fp\), \(0\) indicates the output is \(int\).&lt;/p&gt;
    &lt;p&gt;\([5]:1\) indicates it is one of the two estimation instructions, \(vfrsqrt7\) or \(vfrec7\); otherwise, it is a \(cvt\) instruction. When it is \(1\), \([0]\) distinguishes between \(vfrsqrt7\) and \(vfrec7\).&lt;/p&gt;
    &lt;p&gt;\([4:3]: 00\) denotes \(single\) type, \(01\) denotes \(widen\), \(10\) denotes \(narrow\).&lt;/p&gt;
    &lt;p&gt;\([2:0]\): For different instructions, it serves different purposes: For conversions between floating-point and integer, \([0]\) distinguishes whether the integer is signed or unsigned; in other cases, \([2:1]=11\) indicates it is an \(rtz\) type instruction, and \([2:0]=101\) indicates it is \(rod\) (vfncvt_rod_ffw).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477118</guid><pubDate>Sat, 04 Oct 2025 22:02:28 +0000</pubDate></item><item><title>NSA and IETF: Can an attacker purchase standardization of weakened cryptography?</title><link>https://blog.cr.yp.to/20251004-weakened.html</link><description>&lt;doc fingerprint="3e9999123e0289dd"&gt;
  &lt;main&gt;
    &lt;p&gt;It's normal for post-quantum cryptography to be rolled out as an extra layer of security on top of traditional pre-quantum cryptography, rather than as a replacement.&lt;/p&gt;
    &lt;p&gt;For example, Google's CECPQ1 experiment was double encryption with traditional pre-quantum ECC (specifically X25519) and post-quantum NewHope1024. CECPQ2, a joint experiment between Google and Cloudflare, was ECC+NTRUHRSS701. CECPQ2b was ECC+SIKEp434. Ten SSH implementations support ECC+sntrup761. Today's usage of post-quantum cryptography by browsers is approaching half of the connections seen by Cloudflare, where 95% of that is ECC+MLKEM768 and 5% is ECC+Kyber768.&lt;/p&gt;
    &lt;p&gt;If post-quantum cryptography is designed to be super-strong, so strong that it even survives future quantum computers, then why are we keeping the ECC layer? Same reason that you wear your seatbelt: in the real world, cars sometimes crash, and seatbelts reduce the damage.&lt;/p&gt;
    &lt;p&gt;Google already explained this back in 2016: "The post-quantum algorithm might turn out to be breakable even with today's computers, in which case the elliptic-curve algorithm will still provide the best security that today's technology can offer." We've seen many breaks of post-quantum proposals since then, including the sudden public collapse of SIKE three years after CECPQ2b applied SIKE to tens of millions of user connections. The only reason that this user data wasn't immediately exposed to attackers is that CECPQ2b encrypted data with SIKE and with ECC, rather than switching from ECC to just SIKE. As another example, the reference Kyber/ML-KEM software went through two rounds of security patches for KyberSlash at the end of 2023, and then had another security patch in mid-2024.&lt;/p&gt;
    &lt;p&gt;Deploying ECC+PQ rather than just PQ is an easy common-sense win. ECC software is practically everywhere anyway, and nobody has identified an application that can afford PQ without being able to afford ECC+PQ.&lt;/p&gt;
    &lt;p&gt;Typically people talk about deploying ECC+PQ as deploying "hybrids" rather than "non-hybrids", although you have to be careful with this terminology since the word "hybrid" also has other meanings in cryptography. It's more descriptive to talk about "double encryption" and "double signatures" rather than "single encryption" and "single signatures".&lt;/p&gt;
    &lt;p&gt;The problem in a nutshell. Surveillance agency NSA and its partner GCHQ are trying to have standards-development organizations endorse weakening ECC+PQ down to just PQ.&lt;/p&gt;
    &lt;p&gt;Part of this is that NSA and GCHQ have been endlessly repeating arguments that this weakening is a good thing (in much the same way that NSA advertised Dual EC as providing "increased assurance"). I have a previous blog post showing that those arguments collapse upon examination. But that's not today's topic. In today's blog post I'm instead looking at how easy it is for NSA to simply spend money to corrupt the standardization process.&lt;/p&gt;
    &lt;p&gt;Two TLS encryption drafts. For concreteness, I'll focus on what's happening in a particular standards-development organization called the Internet Engineering Task Force (IETF). Within that, I'll focus on current proposals within an IETF "working group" (WG) that sets standards for Transport Layer Security (TLS), the security layer inside HTTPS and inside various other protocols. I'll look specifically at how the TLS WG handled two drafts specifying post-quantum encryption mechanisms for TLS:&lt;/p&gt;
    &lt;p&gt;Hybrid (double encryption): "Post-quantum hybrid ECDHE-MLKEM Key Agreement for TLSv1.3". This draft specifies ECC+PQ in TLS, specifically usage in TLS of "X25519MLKEM768, SecP256r1MLKEM768, and SecP384r1MLKEM1024". The first of those is also what I mentioned above as 95% of current post-quantum connections to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Non-hybrid (single encryption): "ML-KEM Post-Quantum Key Agreement for TLS 1.3". This draft specifies usage in TLS of "ML-KEM-512, ML-KEM-768, and ML-KEM-1024" without seatbelts.&lt;/p&gt;
    &lt;p&gt;The non-hybrid draft was first posted in March 2024. Of course someone asked "what the motivation is for being 'fully post-quantum' rather than hybrid". The draft author responded: "FIPS / CNSA 2.0 compliance guidelines ... currently are a big 'maybe' at best for 'hybrid solutions', and the timetables for compliant browsers, servers, and services are to exclusively use FIPS 203 at level V (ML-KEM-1024) by 2033. I figure there will be demand for pure ML-KEM key agreement, not hybrid (with no questions that come along with it of whether it's actually allowed or not)."&lt;/p&gt;
    &lt;p&gt;As context, the massive U.S. military budget now publicly requires cryptographic "components" to have NSA approval. "CNSA 2.0" refers to a public NSA document "Commercial National Security Algorithm Suite 2.0". The document says up front that its requirements apply to "all NSS use of public cryptographic algorithms (as opposed to algorithms NSA developed), including those on all unclassified and classified NSS". The legal definition of "national security system" (NSS) covers practically all military information systems, except for "routine administrative and business applications" such as "payroll".&lt;/p&gt;
    &lt;p&gt;In June 2024, NSA's William Layton wrote that "we do not anticipate supporting hybrid in NSS".&lt;/p&gt;
    &lt;p&gt;In December 2024, a Cisco employee wrote the following: "There are people whose cryptographic expertise I cannot doubt who say that pure ML-KEM is the right trade-off for them, and more importantly for my employer, that's what they're willing to buy. Hence, Cisco will implement it; I am essentially just asking for code points." Certainly "willing to buy" is a statement about funding, evidently from a source large enough to dictate Cisco actions, evidently from a source asking for non-hybrids, evidently from "people whose cryptographic expertise I cannot doubt"; if that source isn't NSA, who is it?&lt;/p&gt;
    &lt;p&gt;(Side note: If you think the word "pure" in "pure ML-KEM" sounds good, remember that replacing CECPQ2's ECC+SIKE with "pure SIKE" would have been a disaster.)&lt;/p&gt;
    &lt;p&gt;In June 2025, NSA's Mike Jenkins posted the following: "As the CNSA 2.0 profiles should make clear, we are looking for products that support /standalone/ ML-DSA-87 and /standalone/ ML-KEM-1024. If there is one vendor that produces one product that complies, then that is the product that goes on the compliance list and is approved for use. Our interactions with vendors suggests that this won't be a problem in most cases." Evidently there are many companies happy to jump when NSA says jump.&lt;/p&gt;
    &lt;p&gt;Pretending to eat your own dog food. For software engineers, "dogfooding" (a term perhaps coined by Paul Maritz in the 1980s) refers to making regular use of the software that you're writing. This builds your confidence that the software works, and helps iron out problems.&lt;/p&gt;
    &lt;p&gt;But there's also a marketing version of the same concept, where you publicly say that you're using your own software as a way to build other people's confidence in the software. As in other types of marketing, what you're saying doesn't have to be true.&lt;/p&gt;
    &lt;p&gt;Once upon a time, NSA weakened the Data Encryption Standard to just 56 bits. In public, NSA claimed that it hadn't tampered with the standard, and that the "implausibility of public allegations is further demonstrated by the fact that NSA has endorsed the use of DES for the encryption of national security-related information, including selected classified information".&lt;/p&gt;
    &lt;p&gt;This is powerful marketing. Many people hearing this last quote will think "Oh, okay, NSA is using DES, so DES is strong". Koblitz and Menezes claimed that it's "far-fetched" that NSA would have intentionally selected something weak "for U.S. government usage (for both unclassified and classified communications [41])". Many people today will think "Oh, okay, NSA is buying single encryption, so double encryption is unimportant".&lt;/p&gt;
    &lt;p&gt;But DES wasn't strong. NSA had engineered DES to be "weak enough" for NSA to break. NSA wanted DES to "drive out competitors", to "reduce the field that NSA had to be concerned about".&lt;/p&gt;
    &lt;p&gt;It's perfectly plausible that NSA was using DES, but surely NSA was then using DES multiple times (Triple-DES or beyond), which makes it much harder to break (as long as you switch keys frequently). Obviously NSA wouldn't have said "use multiple layers" publicly: NSA wanted to fool people into thinking that DES was secure.&lt;/p&gt;
    &lt;p&gt;Today we have better ciphers than DES. However, for data that it cares about, NSA still uses two independent encryption layers "to mitigate the ability of an adversary to exploit a single cryptographic implementation". Gee, maybe multiple encryption is important after all!&lt;/p&gt;
    &lt;p&gt;Try to put yourself in the mindset of NSA as an attacker. You have a massive budget to "covertly influence and/or overtly leverage" systems to "make the systems in question exploitable"; "to the consumer and other adversaries, however, the systems' security remains intact". One of your action items is to "influence policies, standards and specification for commercial public key technologies". Another is to "shape the worldwide commercial cryptography marketplace to make it more tractable to advanced cryptanalytic capabilities being developed by NSA/CSS".&lt;/p&gt;
    &lt;p&gt;You spend this money pursuing many different attack paths, taking whatever surveillance wins you can get. It's not that everybody was using Dual EC, for example, but you managed to manipulate some people into using it, and for you that's a win.&lt;/p&gt;
    &lt;p&gt;Weakening ECC+PQ to just PQ, normalizing the practice of driving without seatbelts, is another win for you as the attacker. It's adding further vulnerabilities to the cryptographic ecosystem. The point is that, beyond SIKE and many other publicly broken cryptosystems, there will be some further cases where your "advanced cryptanalytic capabilities" break the PQ part while the "consumer and other adversaries" think the PQ part is secure.&lt;/p&gt;
    &lt;p&gt;What do you do with your control over the U.S. military budget? That's another opportunity to "shape the worldwide commercial cryptography marketplace". You can tell people that you won't authorize purchasing double encryption. You can even follow through on having the military publicly purchase single encryption. Meanwhile you quietly spend a negligible amount of money on an independent encryption layer to protect the data that you care about, so you're actually using double encryption.&lt;/p&gt;
    &lt;p&gt;Adoption of double encryption in TLS. "Adoption" in IETF is a preliminary step before standardization: when a WG is "ready to develop a particular document, the most common mechanism is for it to 'adopt' an existing document as a starting point".&lt;/p&gt;
    &lt;p&gt;In March 2025, after the close of a two-week "WG adoption call", the TLS WG chairs declared "consensus to adopt" the "Post-quantum hybrid ECDHE-MLKEM Key Agreement for TLSv1.3" draft.&lt;/p&gt;
    &lt;p&gt;There were no objections to the declaration of consensus on adopting this draft. I had pointed out that the patents on Kyber/ML-KEM create two issues related to IETF's patent policy, but I said that the first issue can be fixed after adoption (before standardization), and I now think that this is also true for the second issue. The risks from patents are orthogonal to the risks from non-hybrids, and I won't say more about patents in this blog post.&lt;/p&gt;
    &lt;p&gt;Why worry about a weaker standard if there's a stronger standard? At this point you might be wondering: if people are driving with seatbelts and this is on its way to being standardized, what's the problem with also having a driving-without-seatbelts standard for reckless fools who want to use that?&lt;/p&gt;
    &lt;p&gt;Think about Dual EC. Dual EC wasn't the only randomness-generation standard. But companies paid for FIPS certification of at least 62 different implementations of Dual EC. NSA bribed the RSA company to change its popular cryptographic library to use Dual EC by default.&lt;/p&gt;
    &lt;p&gt;These companies saw that Dual EC was a standard from a reputable standards organization (in fact, from three such organizations, namely ANSI, ISO, and NIST). Even for companies realizing that Dual EC was a controversial standard pushed by NSA, how many companies would risk losing money by refusing to implement Dual EC? It's easy for purchasing managers to use standards to set purchasing requirements.&lt;/p&gt;
    &lt;p&gt;What's particularly pernicious about a driving-without-seatbelts standard is that a purchasing manager who looks at it has an incentive to pick it instead of the driving-with-seatbelts standard. Wow, I can save $50 for every seatbelt that I skip! Wow, I can save 50 picodollars for every ECC operation that I skip! The purchasing manager doesn't care whether this cost reduction matters in context: every penny saved sounds good, right? The purchasing manager also doesn't realize the standard is dangerous: on the contrary, why would it be a standard if it were unsafe?&lt;/p&gt;
    &lt;p&gt;Soon we're faced with widespread non-usage of seatbelts. And then, years too late, we realize that, oops, something people used and thought was secure actually wasn't, just as in the case of SIKE.&lt;/p&gt;
    &lt;p&gt;Adoption of single encryption in TLS. On 1 April 2025âunfortunately not as a jokeâthe TLS WG chairs issued a two-week "WG adoption call for the ML-KEM Post-Quantum Key Agreement for TLS 1.3 I-D", the non-hybrid draft mentioned above.&lt;/p&gt;
    &lt;p&gt;Here are some quotes (some from me, some from other people) illustrating objections raised on the TLS mailing list during the call period:&lt;/p&gt;
    &lt;p&gt;The draft creates security risks. Sample quote: "SIKE was applied to large volumes of user data as part of the CECPQ2 experiment in 2019. SIKE was publicly broken in 2022. [paragraph break] The only reason that this didn't immediately give away the user data to attackers is that CECPQ2 was ECC+SIKE, rather than just SIKE. [paragraph break] Should we keep rolling out post-quantum cryptosystems to try to stop future quantum attacks? Yes, of course. But, just in case this goes horribly wrong again, let's make sure to keep ECC in place. Any draft violating this should be rejected as a security risk not just by WGs but also by the ISE."&lt;/p&gt;
    &lt;p&gt;The draft violates BCP 188. Sample quote: "To the extent that this is an allusion to NSA purchasing, it violates BCP 188 ('IETF Will Work to Mitigate Pervasive Monitoring')."&lt;/p&gt;
    &lt;p&gt;The draft violates the WG charter. Sample quote: "the draft's regression from ECC+PQ to just PQ is certainly a technology issue; and this is fatal, as a contravention of the 'improve security' goal in the WG charter".&lt;/p&gt;
    &lt;p&gt;There are no principles supporting the adoption decision. Sample quote: "I don't see what criteria we might use in adopting this that wouldn't leave the WG open to accusations of favouritism if we don't adopt other pure PQ national standards that will certainly arise".&lt;/p&gt;
    &lt;p&gt;The draft's motivation section is circular. Sample quote: there is "a preliminary step that has been skipped here, namely identifying why the proposal is claimed to be adding something important. The draft's motivation sentence consists of rearranging buzzwords without answering the question: 'Having a fully post-quantum (not hybrid) key agreement option for TLS 1.3 is necessary for migrating beyond hybrids and for users that need to be fully post-quantum.' "&lt;/p&gt;
    &lt;p&gt;The draft increases software complexity. Sample quote: "The main stated benefit of using a standalone ML-KEM is complexity reduction, but with the current progress in the deployment of the ML-KEM + ECC hybrid method, a standalone ML-KEM method actually increases overall complexity in software stacks." (This was responding to a claim during the adoption-call period that the draft provided a "compute / dependency base that is minimalist".)&lt;/p&gt;
    &lt;p&gt;This is just a high-level survey of the objections. These quotes aren't intended to convey the full text of objections. They also aren't intended to convey the number of people objecting; I'll get back to that below.&lt;/p&gt;
    &lt;p&gt;Standardization procedures. How does a standards-development organization handle objections? The law on this topic in the United States has been settled for decades.&lt;/p&gt;
    &lt;p&gt;The starting point is that agreements in restraint of interstate commerce are illegal. Courts interpret this to cover various types of agreements that are illegal per se, such as price fixing and group boycotts, along with further agreements that unreasonably interfere with competition.&lt;/p&gt;
    &lt;p&gt;Here's an example from the 1980s. Agents of a company that was leading its market, McDonnell and Miller, took control of a subcommittee of the American Society of Mechanical Engineers, a standards-development organization. They generated a letter, under ASME's apparent authority, declaring that a new competitor's product wasn't compliant. They distributed that letter to buyers, of course damaging the new competitor's business.&lt;/p&gt;
    &lt;p&gt;The competitor, HydroLevel, sued the conspiratorsâincluding ASME, which didn't even know the abuse was happening. HydroLevel won. ASME was ultimately forced to pay millions of dollars. The Supreme Court didn't mince words in describing the anti-competitive power of standards-development organizations:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ASME wields great power in the Nation's economy. Its codes and standards influence the policies of numerous States and cities, and, as has been said about "so-called voluntary standards" generally, its interpretations of its guidelines "may result in economic prosperity or economic failure, for a number of businesses of all sizes throughout the country," as well as entire segments of an industry. ... ASME can be said to be "in reality, an extragovernmental agency which prescribes rules for the regulation and restraint of interstate commerce." ... When it cloaks its subcommittee officials with the authority of its reputation, ASME permits those agents to affect the destinies of businesses, and thus gives them the power to frustrate competition in the marketplace.&lt;/p&gt;
      &lt;p&gt;... Many of ASME's officials are associated with members of the industries regulated by ASME's codes. Although undoubtedly most serve ASME without concern for the interests of their corporate employers, some may well view their positions with ASME, at least in part, as an opportunity to benefit their employers. When the great influence of ASME's reputation is placed at their disposal, the less altruistic of ASME's agents have an opportunity to harm their employers' competitors through manipulation of ASME's codes.&lt;/p&gt;
      &lt;p&gt;... Only ASME can take systematic steps to make improper conduct on the part of all its agents unlikely, and the possibility of civil liability will inevitably be a powerful incentive for ASME to take those steps. Thus, a rule that imposes liability on the standard-setting organization -- which is best situated to prevent antitrust violations through the abuse of its reputation -- is most faithful to the congressional intent that the private right of action deter antitrust violations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Another Supreme Court case rejected an argument of antitrust immunity for another standards-development organization. The organization made various decisions by majority vote, and had allowed steel manufacturers to pack a standards-development group, filling the group with pro-steel agents to take over a vote. The Supreme Court again recognized the importance of procedural safeguards preventing abuse:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The antitrust validity of these efforts is not established, without more, by petitioner's literal compliance with the rules of the Association, for the hope of procompetitive benefits depends upon the existence of safeguards sufficient to prevent the standard-setting process from being biased by members with economic interests in restraining competition. An association cannot validate the anticompetitive activities of its members simply by adopting rules that fail to provide such safeguards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Supreme Court declined at that point to draw a dividing line saying which safeguards were required. In 2004, Congress passed a law pinning this down: the new law said that "standards development activity" by a "standards development organization" isn't illegal per se, and gave definitions of the quoted phrases.&lt;/p&gt;
    &lt;p&gt;In particular, a "standards development organization" is required by law to "incorporate the attributes of openness, balance of interests, due process, an appeals process, and consensus in a manner consistent with the Office of Management and Budget Circular Number A-119, as revised February 10, 1998".&lt;/p&gt;
    &lt;p&gt;That OMB rule, in turn, defines "consensus" as follows: "general agreement, but not necessarily unanimity, and includes a process for attempting to resolve objections by interested parties, as long as all comments have been fairly considered, each objector is advised of the disposition of his or her objection(s) and the reasons why, and the consensus body members are given an opportunity to change their votes after reviewing the comments".&lt;/p&gt;
    &lt;p&gt;The Antitrust Division of the Department of Justice inserted itself into a private court case in 2019 to say that "the United States has a significant interest in the correct interpretation of the exemption from per se treatment for standards development organizations engaging in standard setting activities".&lt;/p&gt;
    &lt;p&gt;Deputy Assistant Attorney General Alexander Okuliar in the same division presented a longer statement to ANSI in 2020 regarding antitrust and standards. The statement mentioned ANSI's compliance with the same requirements and said "From an antitrust perspective, these requirements are central".&lt;/p&gt;
    &lt;p&gt;Here's a random example of what an objection-response document looks like in ISO, IEC, etc. Not the best user interface, but it gets the job done.&lt;/p&gt;
    &lt;p&gt;There was not general agreement to adopt the non-hybrid draft. Now that we have the concept of consensus in mind, let's go back to what happened in the IETF TLS WG regarding the non-hybrid draft.&lt;/p&gt;
    &lt;p&gt;During the adoption-call period, there were statements from 20 people unequivocally supporting adoption: David Adrian from Google, Joseph Birr-Pixton, Uri Blumenthal from Department of Defense subsidiary Lincoln Labs, "Flo D" from GCHQ, Quynh Dang from NIST, Viktor Dukhovni, Scott Fluhrer from Cisco, Rebecca Guthrie from NSA, Russ Housley, Alicja Kario from IBM subsidiary Red Hat, Kris Kwiatkowski, Andrei Popov from Microsoft, Tirumal Reddy from Cisco, Yaroslav Rosomakho, Jan Schaumann, Sophie Schmieg from Google, Martin Thomson from Mozilla, Filippo Valsorda formerly from Google, Loganaden Velvindron, and Thom Wiggers.&lt;/p&gt;
    &lt;p&gt;There were also statements from 2 people conditionally supporting adoption: John Mattsson from Ericsson ("I support adoption as long as reuse of ephemeral keys is normatively forbidden, i.e. MUST NOT reuse") and Yaakov Stein ("I support adoption of pure PQC KEMs drafts with Intended status: Informational (meaning that the IETF is not recommending using)").&lt;/p&gt;
    &lt;p&gt;However, there were statements from 7 people unequivocally opposing adoption: Thomas Bellebaum ("I agree with Stephen on this one and would not support adoption of non-hybrids"), Andrey Jivsov ("I am opposed to the adoption of ML-KEM at this time"), Stephen Farrell ("I'm opposed to adoption, at this time"), Rich Salz ("I was all set to say that I am in favor of adoption, but Stephen's post changed my mind. [paragraph break] The conservative and safe thing is to stick to hybrids and that is what the IETF should do for now"), Rob Sayre ("I oppose adoption"), Sun Shuzhou ("I'm opposed to adoption"), and me.&lt;/p&gt;
    &lt;p&gt;Even assuming that the 2 statements of conditional support are treated as positive votes, the overall situation here, 22 positive votes and 7 negative votes, does not qualify as general agreement. "General" means "shared by or affecting most people, or most of the people in a group"; "most" means "nearly all of the people or things in a group, or nearly all of something"; the phrase "general agreement" means that nearly everyone agrees. Merely having three quarters agree is not good enough.&lt;/p&gt;
    &lt;p&gt;What happens if a standards-development organization issues a rule declaring that "general agreement" exists even when a quarter of the votes are in opposition? I haven't found any court cases on point, but I would expect courts to reject this as being inconsistent with the plain meaning of "general agreement".&lt;/p&gt;
    &lt;p&gt;Anyway, IETF hasn't attempted to issue such a rule. On the contrary, IETF claims that WG decisions are not taken by voting: "Decisions within WGs, as with the broader IETF, are taken by 'rough consensus' and not by voting." This begs the question of what IETF thinks "rough consensus" means. Letting chairs make arbitrary decisions is a violation of due process.&lt;/p&gt;
    &lt;p&gt;More to the point, IETF can't override the definition of "consensus" in the law. That definition requires general agreement. Adoption of this draft was controversial, and didn't reach general agreement.&lt;/p&gt;
    &lt;p&gt;Objections were not handled properly. Within the statements in favor of adoption, most of the statements were very short: e.g., just the words "I support adoption" with no further comments.&lt;/p&gt;
    &lt;p&gt;Some statements in favor of adoption did say more, such as stating circular arguments for the draft (e.g.: "as time progresses, non-hybrid key exchanges will become more and more commonplace, so why not have it already defined?"), or expressing concerns about key reuse (e.g.: "I also share John's concerns about key reuse, but would prefer to litigate that in the working group, rather than during adoption"), without responding to the content of the objections.&lt;/p&gt;
    &lt;p&gt;There was a response to one word in the lack-of-principles objection. (The response was as follows: "The NIST competition was international, and Kyber was developed by an international team. I struggle to understand how adopting this document would somehow be 'favoritism'.") A brief note by one supporter tangentially related to one objection falls far short of fair consideration of each objection by the group as a whole.&lt;/p&gt;
    &lt;p&gt;I tried to engage that supporter in discussion. I started by quoting the following earlier statement in the commentator's message: "I find it to be cognitive dissonance to simultaneously argue that the quantum threat requires immediate work, and yet we are also somehow uncertain of if the algorithms are totally broken. Both cannot be true at the same time." I responded as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Rolling out PQ is trying to reduce the damage from an attacker having a quantum computer within the security lifetime of the user data. Doing that as ECC+PQ instead of just PQ is trying to reduce the damage in case the PQ part is broken. These actions are compatible, so how exactly do you believe they're contradictory?&lt;/p&gt;
      &lt;p&gt;Here's an analogous example of basic risk mitigation: there's endless work that goes into having planes not crash, not hit turbulence, etc., but we still ask airplane passengers to keep their seatbelts on whenever they're in their seats.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There was still no reply to this by the time the adoption call closed two weeks later.&lt;/p&gt;
    &lt;p&gt;The broader pattern was that objectors were engaging in discussion while supporters were not. The majority process wasn't "attempting to resolve each objection"; it was simply collecting positive votes, trying to override objections from the minority without even answering those objections, let alone trying to resolve them.&lt;/p&gt;
    &lt;p&gt;That's in an organization saying that decisions aren't taken by voting. The same organization also says, as part of explaining why it's supposedly complying with antitrust law: "IETF activities are conducted with extreme transparency, in public forums. Decision-making requires achieving broad consensus via these public processes."&lt;/p&gt;
    &lt;p&gt;When there's an objection, the legal concept of consensus requires not just fairly considering the objection, and not just attempting to resolve the objection, butâif resolution failsâhaving the group agree on the contents of a response to the objection. That's an official statement of why the objection was overridden. It's something that can be appealed if it's wrong. Consider, for example, ISO's simple rule saying "Committees are required to respond to all comments received". In IETF, there weren't even informal responses to the objections listed above, let alone official responses.&lt;/p&gt;
    &lt;p&gt;The chairs declared consensus anyway. Shortly before the end of the specified adoption-call period, the chairs declared "consensus to adopt this draft as a working group item". There were some notes on followup procedures, but there was no explanation of the rationale for this claim of consensus.&lt;/p&gt;
    &lt;p&gt;I challenged the claim of consensus:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Um, what? There were several people (including me) raising objections on list to basic flaws in this draft, such as (1) the failure to provide an ECC backup to limit the damage from further security problems in the PQ layer, (2) the failure to provide an engineering justification for this option, and (3) the lack of any principles that would justify saying no to options selected by other governments if this option is allowed.&lt;/p&gt;
      &lt;p&gt;Your message doesn't explain how you came to the conclusion that there's consensus. Surely you aren't relying on some tally of positive votes to ram this document through while ignoring objections; voting isn't how IETF is supposed to work. So how did you come to this conclusion?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A few days later, the chairs responded that they had declared consensus "because there is clearly sufficient interest to work on this draft".&lt;/p&gt;
    &lt;p&gt;I said that this was ambiguous (sufficient for what?); said that in any case this criterion was improper since it "would allow a draft to be adopted over amply justified objections of almost all WG participants, simply because the chairs and a few participants say they have enough interest in working on the draft"; and asked for an explicit statement of whether this was the complete explanation of why the chairs had declared consensus.&lt;/p&gt;
    &lt;p&gt;The chairs responded that "sufficient" means "that there were enough people willing to review the draft". They added that "WGs groups have adopted drafts with much less support than this one received." Gee, that's confidence-inspiring.&lt;/p&gt;
    &lt;p&gt;Meanwhile an IETF "security area director" had jumped into the discussion, in particular writing "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions".&lt;/p&gt;
    &lt;p&gt;Remember that the actual tallies were 20 supporters, 2 conditional supporters, and 7 opponents, even if some people (for example, me) had sent multiple messages. Nobody had posted the actual tallies at this point: there was just this "security area director" claiming that the "vast majority" of the "67 responses" were in favor while there were only "a few dissenting opinions". Also remember that this is an organization that claims that it doesn't make decisions by voting.&lt;/p&gt;
    &lt;p&gt;The "security area director" continued that "you cherry-picking when to call consensus evaluation 'voting' depending on whether misnaming this is in your advantage ... is dishonestly manipulative"; that I was violating the "code of conduct"; and that if I did not "voluntarily stop this kind of behaviour" there would be "measures under the terms of RFC3934 which is part of BCP25".&lt;/p&gt;
    &lt;p&gt;In a followup message, the "security area director" wrote "you calling into question this consensus call of the WG chair is abusive and follows a repetitive pattern. Nevertheless, for now this is your right ... you are attempting to bait the chairs to say they took inventory of the public emails ... there comes a point where you will be prevented from further playing these games".&lt;/p&gt;
    &lt;p&gt;Wait a minute: "for now this is your right" (emphasis added) and "you will be prevented from further playing these games"? Sounds ominous. What did the "security area director" mean by this? No more objections in IETF? No more appeals? NSA's minions can just ram their non-consensual drafts through IETF without opponents even being allowed to speak up?&lt;/p&gt;
    &lt;p&gt;Actually, yes, there's a stealth activity going on right now that will have this effect unless enough people take action by Tuesday the 7th. I hope to have another blog post up in a day or two saying what's going on here.&lt;/p&gt;
    &lt;p&gt;Anyway, I've filed a formal complaint regarding the claim of consensus to adopt. So far the complaint hasn't been handled properly, but hope springs eternal. I don't have an answer yet to the subtitle question of this blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477206</guid><pubDate>Sat, 04 Oct 2025 22:16:33 +0000</pubDate></item><item><title>Borehole Oscillators</title><link>https://www.gregegan.net/SCIENCE/Borehole/Borehole.html</link><description>&lt;doc fingerprint="167116dcc907dfe9"&gt;
  &lt;main&gt;
    &lt;p&gt;There is a simple thought experiment in Newtonian gravity: drill a thin radial borehole all the way through a solid ball of uniform density, and drop a test particle into the hole, starting from rest at the very top. What happens?&lt;/p&gt;
    &lt;p&gt;The result is that the particle (blue, in the image on the left) falls all the way through the borehole, comes to a halt at the opposite end, then falls back, undergoing simple harmonic motion (the same kind of motion as an idealised version of a weight bouncing on the end of a spring) with exactly the same period as another test particle (red) orbiting the ball in a circular orbit that grazes the surface.&lt;/p&gt;
    &lt;p&gt;On this page, we will start by proving that result, but then we will also look at radial motion in the vacuum around the ball in Newtonian gravity, and examine how these systems work in General Relativity — featuring the famous Schwarzschild solution, but starring its much less famous cousin, the second Schwarzschild solution!&lt;/p&gt;
    &lt;p&gt;A solid ball of radius r and uniform density ρ has mass:&lt;/p&gt;
    &lt;quote&gt;m(r) = (4/3) π ρ r3&lt;/quote&gt;
    &lt;p&gt;In Newtonian physics, the acceleration due to gravity from any spherically symmetrical arrangement of matter is directed towards the centre of symmetry, and it is the same as if all the matter that lies closer to the centre than the point where we are computing the acceleration was concentrated at the centre. (This result is known as the shell theorem, and it was proved by Newton himself.) So, anywhere inside our solid ball, the radial acceleration of a test particle is given by:&lt;/p&gt;
    &lt;quote&gt;r''(t) = –G m(r(t)) / r(t)2&lt;lb/&gt;= –(4/3) G π ρ r(t)3 / r(t)2&lt;lb/&gt;= –(4/3) G π ρ r(t)&lt;/quote&gt;
    &lt;p&gt;This is the same kind of equation as the one that governs a weight on a spring, where an object experiences a “restoring force” proportional to its displacement from some point. If the particle starts from rest at r = R at time t = 0, the solution is:&lt;/p&gt;
    &lt;quote&gt;rsolid(t) = R cos(ω t)&lt;lb/&gt;where ω2 = (4/3) G π ρ&lt;/quote&gt;
    &lt;p&gt;This is easily checked by taking the derivative of the claimed solution twice with respect to time.&lt;/p&gt;
    &lt;p&gt;How does this compare to the motion of a test particle orbiting a ball of radius R in a grazing circular orbit? If we equate the centrifugal acceleration for circular motion, ω2 R for a circle of radius R and an angular velocity of ω, with the inverse-square acceleration due to gravity, we have:&lt;/p&gt;
    &lt;quote&gt;ω2 R = G m(R) / R2&lt;lb/&gt;ω2 = G m(R) / R3&lt;lb/&gt;= (4/3) G π ρ&lt;/quote&gt;
    &lt;p&gt;So the angular velocity, and hence the period, is the same for the two kinds of motion.&lt;/p&gt;
    &lt;p&gt;We have shown that a test particle moving radially inside a ball of uniform density undergoes simple harmonic motion. But what about radial motion through the vacuum around such a ball?&lt;/p&gt;
    &lt;p&gt;If a test particle with unit mass starts from rest at some radius r1 which is greater than the radius of the ball, R, we can find its kinetic energy at any other r outside the ball from the change in its potential energy:&lt;/p&gt;
    &lt;quote&gt;KE = ½ v(r)2&lt;lb/&gt;= –ΔPE&lt;lb/&gt;= G M (1/r – 1/r1)&lt;/quote&gt;
    &lt;p&gt;where we are writing M for the total mass of the ball, m(R).&lt;/p&gt;
    &lt;p&gt;This is equivalent to:&lt;/p&gt;
    &lt;quote&gt;dt/dr = 1/v(r) = ±1 / √[2 G M (1/r – 1/r1)]&lt;/quote&gt;
    &lt;p&gt;The solution to this equation, for the particle starting its fall at time t = 0, is:&lt;/p&gt;
    &lt;quote&gt;tvac(r) = √[r13/(2 G M)] (acos(√[r/r1]) + √[(r/r1)(1 – r/r1)])&lt;/quote&gt;
    &lt;p&gt;We can compare this with the function giving time to reach a given radius for motion entirely through the solid ball:&lt;/p&gt;
    &lt;quote&gt;tsolid(r) = √[R3/(G M)] acos(r/R)&lt;/quote&gt;
    &lt;p&gt;In the limit where the ball is replaced by a point mass, the time for the test particle to fall down to r = 0 is:&lt;/p&gt;
    &lt;quote&gt;tvac(0) = (π/2) √[r13/(2 G M)]&lt;/quote&gt;
    &lt;p&gt;compared to:&lt;/p&gt;
    &lt;quote&gt;tsolid(0) = (π/2) √[R3/(G M)]&lt;/quote&gt;
    &lt;p&gt;This shows that the fall through the vacuum would be faster, which makes sense because the test particle feels the full force of all the mass throughout its motion, whereas inside the ball the amount of mass attracting it grows steadily smaller.&lt;/p&gt;
    &lt;p&gt;It is worth pointing out that the limiting case of a radial fall through the vacuum down to a point mass can be continued beyond r = 0 in two different ways. If we think of it as the limit of a series of ever skinnier elliptical orbits with the mass at one focus of the ellipse, in each case the test particle, at its closest approach, swings around the mass and reverses direction ever more rapidly. In that scenario, the fall down to r = 0 is half of a complete orbit, for an elliptical orbit whose semi-major axis a is half the starting radius, i.e. a = ½r1. The orbital period T for a test mass in an elliptical orbit depends only on the orbit’s semi-major axis and the mass of the attracting body:&lt;/p&gt;
    &lt;quote&gt;T(a, M) = 2 π √[a3 / (G M)]&lt;/quote&gt;
    &lt;p&gt;which give us, for half the orbital period of an orbit with a = ½r1:&lt;/p&gt;
    &lt;quote&gt;½T(½r1, M) = (π/2) √[r13/(2 G M)]&lt;/quote&gt;
    &lt;p&gt;in agreement with the value we found for tvac(0).&lt;/p&gt;
    &lt;p&gt;This is in contrast with the scenario where we drop a test particle from the top of a radial borehole through a ball of uniform mass, but then fix the point where we drop the particle, while shrinking the ball and increasing its density to keep its mass constant. In that case, rather than swinging around the ball, the test particle always falls right through it. The time it takes to fall to r = 0 approaches the same limit, but when the acceleration becomes infinite the infalling solution continues as an ascending solution on the opposite side of the mass, rather than on the same side.&lt;/p&gt;
    &lt;p&gt;The plot above shows one cycle for various cases of the motion where a test particle falls from the same starting radius, r1, and passes through a borehole in a solid ball of the same mass, M, but different radii, R. The dots on each curve mark the extent of the solid ball. The slowest cycle takes place entirely within the ball, while the fastest is the limiting case where the ball has shrunk to a radius of zero but the test particle still “falls through it” rather than swinging around it.&lt;/p&gt;
    &lt;p&gt;In 1916, shortly before his death, the German physicist and astronomer Karl Schwarzschild published two papers that described exact solutions of Einstein’s equations for the geometry of space-time. The first solution is for the gravitational field due to a point mass[1], and this Schwarzschild solution is famous for describing the space-time geometry that we now understand to be that of a [non-rotating, electrically neutral] black hole. It also describes the vacuum surrounding any [non-rotating, electrically neutral] spherically symmetrical mass.&lt;/p&gt;
    &lt;p&gt;The second Schwarzschild solution is for the gravitational field due to a ball of incompressible fluid[2], or in other words a ball of uniform density, since an incompressible fluid does not change its density under pressure. So, this is the General Relativistic equivalent of the Newtonian solid ball of uniform density that we considered in the previous section.&lt;/p&gt;
    &lt;p&gt;The two Schwarzschild metrics, for a ball of total mass M and coordinate radius R, are given by[3]:&lt;/p&gt;
    &lt;quote&gt;
      &lt;td&gt;ds2&lt;/td&gt;
      &lt;td&gt;=&lt;/td&gt;
      &lt;td&gt;–(1–2M/r) dt2 + dr2 / (1–2M/r) + r2 (dθ2 + sin2θ dφ2)&lt;/td&gt;
      &lt;td&gt;r &amp;gt; R&lt;/td&gt;
      &lt;td&gt;ds2&lt;/td&gt;
      &lt;td&gt;=&lt;/td&gt;
      &lt;td&gt;–¼(3 √[1–2M/R] – √[1–2Mr2/R3])2 dt2 + dr2 / (1–2Mr2/R3) + r2 (dθ2 + sin2θ dφ2)&lt;/td&gt;
      &lt;td&gt;r ≤ R&lt;/td&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here we are following common practice in General Relativity and using geometric units, which are chosen to make the gravitational constant G and the speed of light c both equal to 1. In these units, times and masses are measured in the same units as distances. A time in conventional units is multiplied by c &amp;amp;approx; 2.998×108 metres/sec to convert it to a distance, and a mass is multiplied by G/c2 &amp;amp;approx; 7.425×10–28 metres/kg (or about 1,483 metres per solar mass).&lt;/p&gt;
    &lt;p&gt;The Schwarzschild r coordinate is chosen to make the surface area of a sphere of coordinate radius r equal to 4πr2, so the coordinates θ and φ for a given value of r act like ordinary spherical polar coordinates. The Schwarzschild t coordinate is chosen so that all slices of space-time with a fixed t coordinate have a geometry that is independent of t, as can be seen by the fact that t itself does not appear in the metric.&lt;/p&gt;
    &lt;p&gt;We won’t describe in detail how these metrics are derived, but it comes down to taking a generic, spherically symmetric metric that is unchanging in time:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;ds2&lt;/cell&gt;
        &lt;cell&gt;=&lt;/cell&gt;
        &lt;cell&gt;–P(r) dt2 + Q(r) dr2 + r2 (dθ2 + sin2θ dφ2)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;and computing the Einstein tensor, derived from this metric, which quantifies certain aspects of space-time curvature. Einstein’s equation then tells us that this tensor is proportional to the stress-energy tensor, which has components that describe the density of mass-energy and the pressure of whatever matter is responsible for the space-time curvature.&lt;/p&gt;
    &lt;p&gt;Setting the density to zero gives us a differential equation for Q(r), and once we solve that, and substitute the solution into expressions for the pressure in the radial and tangential directions, setting either of them to zero yields the same solution for P(r). These solutions for P(r) and Q(r) give us the first Schwarzschild metric, which applies to the vacuum outside the ball.&lt;/p&gt;
    &lt;p&gt;Setting the density to a non-zero constant value, ρ, gives us a different solution for Q(r), and then we obtain P(r) by requiring the pressure to be the same in the radial and tangential directions. That gives us the second Schwarzschild metric. It also gives us an explicit formula for the pressure inside the ball as a function of r:&lt;/p&gt;
    &lt;quote&gt;p(r) = ρ (α(r) – α(R)) / (3α(R) – α(r))&lt;lb/&gt;where α(r) = √[1–2Mr2/R3]&lt;/quote&gt;
    &lt;p&gt;The wonderful thing about Einstein’s equation is that the pressure we obtain this way will automatically be precisely what is needed to balance the gravitational attraction at each point inside the ball; we don’t need to perform a separate computation in relativistic hydrostatics to make this come out right. The Einstein tensor, by construction, has the geometric property of being “divergence free” that, when shared by the stress-energy tensor guarantees that it has the correct physical properties.&lt;/p&gt;
    &lt;p&gt;The pressure at the surface of the ball, r = R, is always zero. If we fix the mass of the ball but make it smaller, the pressure at the centre, r = 0, increases, and becomes infinite when R = (9/4)M. That is slightly more than r = 2M, which marks the event horizon of an existing black hole, and which guarantees that a black hole will form if a mass of M is squeezed down to within that radius. Although our uniform-density ball is a physically unrealistic toy model (certainly not applicable to stars or planets, though perhaps a fair approximation for a moderately sized, non-rotating globe of pure water), so long as R &amp;gt; (9/4)M the model should not yield infinite values for any physical quantity.&lt;/p&gt;
    &lt;p&gt;The density, ρ, of mass-energy within the ball doesn’t appear explicitly in the second Schwarzschild metric as we have written it, but it is related to the parameters M and R by the formula:&lt;/p&gt;
    &lt;quote&gt;M = (4/3) π ρ R3&lt;/quote&gt;
    &lt;p&gt;This might seem a bit strange, given that the volume of the ball with an r coordinate ranging from 0 to R is larger than (4/3) π R3, the volume of a ball in Euclidean space with radius R, because the metric tells us that ds/dr is greater than 1. Surely M should be greater than the value given above, reflecting that greater volume?&lt;/p&gt;
    &lt;p&gt;To understand what is happening here, imagine we assemble the ball by dropping in material from a large distance (we will say “from infinity”, which really just means from a large enough distance that we can neglect the gravitational effects of the ball there). As it falls, the material will gain kinetic energy, so its mass-energy will increase compared to the original value. So we can spread it out across a larger volume than it would have occupied “at infinity”, while keeping its density constant at ρ. The mass M measures the total mass-energy that we have dropped onto the ball “from infinity”, and it is the appropriate way to quantify the gravitational field of the ball, inasmuch as the angular velocity ω(r) of a test particle in a circular orbit around the ball at a radius of r will obey the asymptotic relationship (in our units where G = 1):&lt;/p&gt;
    &lt;quote&gt;limitr→∞ω2(r) r3 = M&lt;/quote&gt;
    &lt;p&gt;To analyse the orbits of interest to us, we will start with the fact that the world line of a free-falling test particle, moving only under the influence of gravity, will be a geodesic in the space-time geometry. We will use the symbol u to denote its 4-velocity: the unit vector tangent to its world line. The components of the vector u in our coordinate system are simply the rate of change with proper time τ, along the geodesic, of the coordinates:&lt;/p&gt;
    &lt;quote&gt;ut = ∂τ t&lt;lb/&gt;ur = ∂τ r&lt;lb/&gt;uφ = ∂τ φ&lt;lb/&gt;uθ = ∂τ θ&lt;/quote&gt;
    &lt;p&gt;Because the geometry is spherically symmetrical, we can always choose coordinates so that the world line of the test particle lies in the “equatorial plane” at θ=π/2, and uθ = ∂τ θ = 0.&lt;/p&gt;
    &lt;p&gt;Since the metric is independent of the coordinates t and φ, the coordinate vector fields ∂t and ∂φ are Killing vector fields: fields that describe ways of “sliding” a figure through the geometry while preserving its shape and size, in the same way that we can imagine, say, adding 5 degrees to the longitude of every point on an island without distorting its geography. (For an informal discussion of Killing vector fields, try this page; though it starts with a quote from my novel Incandescence, it doesn’t actually require any familiarity with that book.)&lt;/p&gt;
    &lt;p&gt;A consequence of ∂t and ∂φ being Killing vector fields is that the dot product of the tangent to any geodesic with these fields will remain constant along the length of the geodesic. So for the entire world line of our test particle, we will have:&lt;/p&gt;
    &lt;quote&gt;u · ∂t = ut gtt = –E&lt;lb/&gt;u · ∂φ = uφ gφφ = L&lt;/quote&gt;
    &lt;p&gt;where gtt and gφφ are the coefficients of dt2 and dφ2 in the metric, and we’ve introduced E and L as names for the two constants, since they measure the energy and angular momentum, per unit rest mass, of the particle. The fact that u is a unit vector means:&lt;/p&gt;
    &lt;quote&gt;gtt (ut)2 + grr (ur)2 + gφφ (uφ)2 = –1&lt;lb/&gt;(∂τ r)2 = (ur)2 = [–gtt (ut)2 –gφφ (uφ)2 – 1] / grr&lt;lb/&gt;= [–E2/gtt – L2/gφφ – 1] / grr&lt;/quote&gt;
    &lt;p&gt;For the first Schwarzschild metric, this becomes:&lt;/p&gt;
    &lt;quote&gt;(∂τ r)2 = E2 + (1–2M/r) [ – L2/r2 – 1]&lt;lb/&gt;= E2 – (r – 2M) (L2 + r2) / r3&lt;/quote&gt;
    &lt;p&gt;This formula lets us think of the radial motion of the test particle as if it was moving in a kind of potential-energy well, given by:&lt;/p&gt;
    &lt;quote&gt;V(r) = (r – 2M) (L2 + r2) / r3&lt;/quote&gt;
    &lt;p&gt;The squared energy E2 needs to be greater than or equal to V(r), since (∂τ r)2 cannot be negative, so for a given value of E and L this will prescribe the range of r values where the particle can be. Whenever E2 = V(r) the radial velocity must be zero, which marks points where the particle changes from incoming to outgoing.&lt;/p&gt;
    &lt;p&gt;For example, in the diagram on the left, for L/M = 5 (which gives the blue curve for V(r)) and E2 = 1.1 (the dashed blue line), the particle will approach from infinity, reach a minimum r/M of about 6.5, then move away again, in an unbound orbit. An orbit like this would be hyperbolic in Newtonian gravity, but takes on a sightly different shape in the relativistic version.&lt;/p&gt;
    &lt;p&gt;For L/M = 4 (which gives the green curve for V(r)) and E2 = 0.935 (the dashed green line), the particle moves between a minimum r/M of about 8.3 and a maximum of 19.4, in a bound orbit. This kind of orbit would be elliptical in Newtonian gravity, but in the relativistic version it does not form a closed curve, because the angular and radial motion will not have precisely the same periods.&lt;/p&gt;
    &lt;p&gt;The dots on the curves mark local maxima and minima for V(r). Stable circular orbits can take place at the minima, which can only occur at r &amp;gt; 6M. For 3M &amp;lt; r &amp;lt; 6M, the potential can have a maximum, and in principle there could be a circular orbit there, but it will be unstable.&lt;/p&gt;
    &lt;p&gt;For L/M = 2√3 the maxima and minima merge at r = 6M, and for L/M &amp;lt; 2√3 there are no maxima or minima and the particle will always fall into the singularity at r = 0.&lt;/p&gt;
    &lt;p&gt;We can compute the radial acceleration, the second derivative of r with respect to proper time τ:&lt;/p&gt;
    &lt;quote&gt;(∂τ r)2 = E2 – V(r)&lt;lb/&gt;2 (∂τ r) (∂τ, τ r) = –V '(r) (∂τ r)&lt;lb/&gt;∂τ, τ r = –½V '(r)&lt;lb/&gt;= –M/r2 + L2 (r – 3M) / r4&lt;/quote&gt;
    &lt;p&gt;So we have the ordinary inverse-square acceleration that is familiar from Newtonian gravity, plus a centrifugal acceleration that changes sign at r = 3M. That change of sign makes circular orbits in the first Schwarzschild metric impossible for r ≤ 3M, because the centrifugal acceleration points inwards and there is nothing to balance the gravitational attraction. (No object with rest mass can orbit at precisely 3M, but in principle a photon can orbit at that radius. Analysing photon geodesics around black holes is beyond the scope of this page, but you can read a bit about them here.)&lt;/p&gt;
    &lt;p&gt;Now, suppose we have a test particle in a circular orbit with coordinate radius R. If we set the radial acceleration to zero and solve for L, we find:&lt;/p&gt;
    &lt;quote&gt;L = ±R √[M / (R – 3M)]&lt;/quote&gt;
    &lt;p&gt;If we substitute this into V(r), set the radial velocity to zero, and solve for E, we get:&lt;/p&gt;
    &lt;quote&gt;E = (R – 2M) / √[R (R – 3M)]&lt;/quote&gt;
    &lt;p&gt;The angular velocity of the circular orbit with respect to proper time τ, is:&lt;/p&gt;
    &lt;quote&gt;ωτ(R) = uφ = L / gφφ = L / R2&lt;lb/&gt;= √[M / (R – 3M)] / R&lt;/quote&gt;
    &lt;p&gt;If we want to convert this to coordinate time, t, we need:&lt;/p&gt;
    &lt;quote&gt;dt/dτ = ut = E / gtt&lt;lb/&gt;= √[R / (R – 3M)]&lt;/quote&gt;
    &lt;p&gt;So the angular velocity with respect to coordinate time is:&lt;/p&gt;
    &lt;quote&gt;ωt(R) = ωτ(R) / (dt/dτ)&lt;lb/&gt;= √[M / (R – 3M)] √[(R – 3M) / R] / R&lt;lb/&gt;= √[M / R3]&lt;/quote&gt;
    &lt;p&gt;This is the same formula as for the angular velocity of a Newtonian circular orbit, although of course coordinate time t is not the proper time either for the orbiting particle or for a stationary bystander at the same radial position.&lt;/p&gt;
    &lt;p&gt;What about radial motion in the first Schwarzschild metric? If we set L = 0, we have:&lt;/p&gt;
    &lt;quote&gt;(∂τ r)2 = E2 – (r – 2M) / r&lt;/quote&gt;
    &lt;p&gt;If the particle is at rest when r = r1, we have:&lt;/p&gt;
    &lt;quote&gt;E2 = (r1 – 2M) / r1&lt;/quote&gt;
    &lt;p&gt;So the radial motion is given by:&lt;/p&gt;
    &lt;quote&gt;(∂τ r)2 = (r1 – 2M) / r1 – (r – 2M) / r&lt;lb/&gt;= 2M (1 / r – 1 / r1)&lt;lb/&gt;dτ/dr = ±1 / √[2M (1 / r – 1 / r1)]&lt;/quote&gt;
    &lt;p&gt;Apart from the factor of G that we have set to 1, this is precisely the same as the equation we found for dt/dr in the Newtonian case! So the solution takes the same form:&lt;/p&gt;
    &lt;quote&gt;τvac(r) = √[r13/(2 M)] (acos(√[r/r1]) + √[(r/r1)(1 – r/r1)])&lt;/quote&gt;
    &lt;p&gt;If the first Schwarzschild metric applies down to r = 2M or less then we are describing a black hole, so we can’t talk about the limiting case of a radial oscillator where the particle moves through vacuum all the way to r = 0 and then ascends on the other side, as we did for the Newtonian version. Nevertheless, this formula will tell us the proper time for a particle that falls from r = r1 to any other smaller value of r, including the event horizon at r = 2M and the singularity at r = 0. For the latter:&lt;/p&gt;
    &lt;quote&gt;τvac(0) = (π/2) √[r13/(2 M)]&lt;/quote&gt;
    &lt;p&gt;You can read much more about objects (and people) falling into black holes, what they look like from afar, and when they can and can’t be rescued, in the page I wrote on The Finite Fall.&lt;/p&gt;
    &lt;p&gt;Now, turning to the second Schwarzschild metric, we have:&lt;/p&gt;
    &lt;quote&gt;(∂τ r)2 = [–E2/gtt – L2/gφφ – 1] / grr&lt;lb/&gt;= (1–2Mr2/R3) [4 E2/(3 √[1–2M/R] – √[1–2Mr2/R3])2 – L2/r2 – 1]&lt;/quote&gt;
    &lt;p&gt;For our borehole oscillator thought experiment, we have L = 0 for radial motion, and the particle is motionless when r = R. This gives us:&lt;/p&gt;
    &lt;quote&gt;4 E2/(3 √[1–2M/R] – √[1–2M/R])2 – 1 = 0&lt;lb/&gt;E2 = 1–2M/R&lt;/quote&gt;
    &lt;p&gt;and the radial motion becomes:&lt;/p&gt;
    &lt;quote&gt;(∂τ r)2 = (1–2Mr2/R3) [4 (1–2M/R)/(3 √[1–2M/R] – √[1–2Mr2/R3])2 – 1]&lt;/quote&gt;
    &lt;p&gt;If we want to work in coordinate time, t, we can use:&lt;/p&gt;
    &lt;quote&gt;dt/dτ = ut = E / gtt&lt;lb/&gt;(dτ/dt)2 = (gtt)2 / E2&lt;lb/&gt;= (3 √[1–2M/R] – √[1–2Mr2/R3])4 / [16 (1–2M/R)]&lt;lb/&gt;(dr/dt)2 = (dτ/dt)2 (∂τ r)2&lt;/quote&gt;
    &lt;p&gt;Integrating the last equation here is a bit complicated, so we will leave the calculations to an appendix.&lt;/p&gt;
    &lt;p&gt;The upshot is that the coordinate-time angular velocity ωt is slower for the borehole than for a grazing circular orbit. So in the General-Relativistic version of our thought experiment, when the particle in the circular orbit has returned to its original position, the particle that was dropped into the borehole will still be on its way back up.&lt;/p&gt;
    &lt;p&gt;As R approaches the minimum possible value of (9/4)M, which is when the pressure at the centre of the ball would become infinite, the coordinate-time angular velocity for the borehole oscillations approaches zero. In other words, the period becomes arbitrarily long. This is similar to the way that a rocket that passes close to the event horizon of a black hole can take arbitrarily long to return to a distant stationary observer, according to the stationary observer’s clock. But the proper time recorded by a clock on the rocket itself is bounded, and similarly, the proper time recorded by a clock undergoing borehole oscillations is bounded, as we can see from the plot below.&lt;/p&gt;
    &lt;p&gt;According to a clock oscillating up and down in the borehole, the rate of oscillations increases monotonically as R gets smaller, approaching a finite maximum value as R approaches (9/4)M. The rate is always slower than the proper time rate for a clock in a grazing circular orbit (which approaches infinity as R approaches 3M and the orbital velocity approaches the speed of light).&lt;/p&gt;
    &lt;p&gt;What if we aim a beam of light straight down the borehole?&lt;/p&gt;
    &lt;p&gt;For light, the ds2 measured by the metric is zero, so light travelling in a purely radial direction in either of the Schwarzschild space-times must obey:&lt;/p&gt;
    &lt;quote&gt;dt/dr = ±√[–grr/gtt]&lt;/quote&gt;
    &lt;p&gt;If we solve this for a beam of light directed towards the centre of the ball, starting from the top of the borehole at t = 0, we get:&lt;/p&gt;
    &lt;quote&gt;tLight(r) = (R2 ξ / M) (atan(ξ) + atan(3ξ) – atan(ξ r/R) – atan(3ξ r √[(R – 2M) / (R3 – 2Mr2)])&lt;lb/&gt;where ξ = √[M/(4R–9M)]&lt;lb/&gt;tLight(0) = (R2 ξ / M) (atan(ξ) + atan(3ξ))&lt;/quote&gt;
    &lt;p&gt;When R = 3M we have:&lt;/p&gt;
    &lt;quote&gt;ξ = 1/√3&lt;lb/&gt;atan(ξ) + atan(3ξ) = π/2&lt;lb/&gt;tLight(0) = (π/2) 3√3 M&lt;lb/&gt;ωt(R) = √[M / R3]&lt;lb/&gt;ωt(3M) = 1/(3√3 M)&lt;/quote&gt;
    &lt;p&gt;So the radius R for the ball such that the orbital quarter-cycle equals the time for light to reach the centre is 3M, the radius where the smallest circular orbit exists — and exists only for photons. For any larger ball, light will cross the borehole faster than any orbiting object can move between the two ends.&lt;/p&gt;
    &lt;p&gt;Consider the surface in three-dimensional Euclidean space, parameterised by two coordinates, r and φ, with 0 ≤ r ≤ R:&lt;/p&gt;
    &lt;quote&gt;S2(r, φ) = (r cos(φ), r sin(φ), R (√[R/(2M) – 1] – √[R/(2M) – r2/R2]))&lt;/quote&gt;
    &lt;p&gt;If we compute the squared distance of an arbitrary point on this surface from the point c = (0, 0, R √[R/(2M) – 1]), it turns out to be constant:&lt;/p&gt;
    &lt;quote&gt;|S2(r, φ) – c|2 = r2 + R2 (R/(2M) – r2/R2)&lt;lb/&gt;= R3 / (2M)&lt;/quote&gt;
    &lt;p&gt;So our surface S2(r, φ) is part of a sphere, centred on c, with radius √[R3 / (2M)].&lt;/p&gt;
    &lt;p&gt;Why parameterise a sphere this way? Let’s compute the square of the distance corresponding to a change in the r coordinate:&lt;/p&gt;
    &lt;quote&gt;∂r S2(r, φ) = (cos(φ), sin(φ), r / √[R3/(2M) – r2])&lt;lb/&gt;|∂r S2(r, φ)|2 = 1 + r2 / (R3/(2M) – r2)&lt;lb/&gt;= 1 / (1–2Mr2/R3)&lt;/quote&gt;
    &lt;p&gt;So, distances within this surface depend on r in exactly the same way as they do in the second Schwarzschild metric!&lt;/p&gt;
    &lt;p&gt;In other words, this spherical bowl has exactly the same geometry as a slice of curved space cutting through the centre of the ball of uniform density in the second Schwarzschild solution.&lt;/p&gt;
    &lt;p&gt;We can do the same thing for a slice through the vacuum outside the ball, described by the first Schwarzschild metric:&lt;/p&gt;
    &lt;quote&gt;S1(r, φ) = (r cos(φ), r sin(φ), √[8M] (√[r – 2M] – √[R – 2M]))&lt;lb/&gt;∂r S1(r, φ) = (cos(φ), sin(φ), 1 / √[(r/(2M) – 1)]&lt;lb/&gt;|∂r S1(r, φ)|2 = 1 + 1 / (r/(2M) – 1)&lt;lb/&gt;= 1 / (1–2M/r)&lt;/quote&gt;
    &lt;p&gt;The image below shows a surface with the geometry of a spatial slice through the combined Schwarzschild space-time, for a ball of radius 4M, showing the vacuum geometry (the green surface) out to 6M.&lt;/p&gt;
    &lt;p&gt;This is fine for visualising the geometry of space alone, but to understand the motion of free-falling objects, we need to look at a slice that includes the time coordinate. We can’t embed a slice like that in ordinary three-dimensional Euclidean space, but we can embed it in a flat space-time with two time-like dimensions and one space-like dimension.&lt;/p&gt;
    &lt;p&gt;Why two time-like dimensions, when there is only one time-like dimension in the slice itself? We want to be able to bend the surface along its time-like dimension, so we need two such dimensions in the ambient space from which the surface inherits its space-like and time-like geometry.&lt;/p&gt;
    &lt;p&gt;The image above shows a slice through the radial and time dimensions of the combined Schwarzschild space-time, for a ball of radius 4M, along with the vacuum geometry (the green surface) out to 6M. The red curve drawn on the surface is part of the world line for an object dropped into a radial borehole, and just as the world line is a geodesic in the actual curved space-time, it is a geodesic of this curved surface embedded in a flat space-time.&lt;/p&gt;
    &lt;p&gt;The time dimension here appears to form a closed loop, but that has nothing to do with the real topology of the Schwarzschild space-time; it just looks nicer if we take one cycle of the borehole oscillations and adjust a free parameter in the embedding so that it wraps around exactly 360 degrees, rather than leaving a gap.&lt;/p&gt;
    &lt;p&gt;The white surface, corresponding to a t-r slice of the second Schwarzschild space-time, is given by:&lt;/p&gt;
    &lt;quote&gt;H(t, r) = (J(r) cos(a t), J(r) sin(a t), E(asin(Ω r) | –Ω2/(4a2)) / Ω )&lt;lb/&gt;Ω = √[2M/R3]&lt;lb/&gt;a is a parameter we are free to choose.&lt;lb/&gt;E is an incomplete elliptic integral of the second kind.&lt;lb/&gt;J(r) = ½(3 √[1–2M/R] – √[1–2Mr2/R3]) / a&lt;/quote&gt;
    &lt;p&gt;A vector pointing along the surface in the t direction lies solely in the first two dimensions of the flat space-time in which the surface is embedded, so by making those two dimensions time-like the t coordinate inherits that property. Specifically:&lt;/p&gt;
    &lt;quote&gt;∂t H(t, r) = (–a J(r) sin(a t), a J(r) cos(a t), 0)&lt;lb/&gt;|∂t H(t, r)|2 = –a2 J(r)2&lt;lb/&gt;= –¼(3 √[1–2M/R] – √[1–2Mr2/R3])2&lt;/quote&gt;
    &lt;p&gt;which is the coefficient of dt2 in the second Schwarzschild metric. Similarly, the squared length of a vector pointing along the surface in the r direction (which has components in both space-like and time-like directions, so we take the difference of the squared components), is equal to the coefficient of dr2 in the second Schwarzschild metric.&lt;/p&gt;
    &lt;p&gt;By choosing a appropriately, we can embed a time interval corresponding to one full borehole oscillation so that it wraps around and comes full circle.&lt;/p&gt;
    &lt;p&gt;For the surrounding vacuum geometry, although the metric is simpler I am not aware of a closed-form solution for the embedding, so it was achieved with numerical integration.&lt;/p&gt;
    &lt;p&gt;Our aim here is to compute coordinate time, t, as a function of the coordinate radius, r, for a particle that falls from rest at r = R down to the centre of a solid ball of uniform density, with mass M and coordinate radius R, whose space-time geometry is described by the second Schwarzschild metric.&lt;/p&gt;
    &lt;p&gt;We previously found that:&lt;/p&gt;
    &lt;quote&gt;(∂τ r)2 = (1–2Mr2/R3) [4 (1–2M/R)/(3 √[1–2M/R] – √[1–2Mr2/R3])2 – 1]&lt;lb/&gt;(dτ/dt)2 = (3 √[1–2M/R] – √[1–2Mr2/R3])4 / [16 (1–2M/R)]&lt;/quote&gt;
    &lt;p&gt;Since r is the independent variable, we need to integrate:&lt;/p&gt;
    &lt;quote&gt;dt/dr = 1 / [(dτ/dt) (∂τ r)]&lt;/quote&gt;
    &lt;p&gt;We will simplify this by changing some variables:&lt;/p&gt;
    &lt;quote&gt;β = √[1–2M/R]&lt;lb/&gt;Since R &amp;gt; (9/4)M, we have 1/3 &amp;lt; β &amp;lt; 1.&lt;lb/&gt;r = √[R3 (1–z2) / (2M)]&lt;lb/&gt;z = √[1–2Mr2/R3]&lt;lb/&gt;Note that β ≤ z ≤ 1 &amp;lt; 3β.&lt;lb/&gt;dr/dz = –z / √[(2M) (1–z2) / R3]&lt;lb/&gt;dt/dz = (dr/dz) (dt/dr)&lt;lb/&gt;= (2√2) R √[(R–2M)/M] / ((3β–z) √[(1–z2) (z–β) (5β–z)])&lt;/quote&gt;
    &lt;p&gt;So, setting aside the constants here, the function of z that we want to integrate is:&lt;/p&gt;
    &lt;quote&gt;w(z) = 1 / ((3β–z) √[(1–z2) (z–β) (5β–z)])&lt;/quote&gt;
    &lt;p&gt;It is not too hard (with some help from Mathematica) to find an indefinite integral:&lt;/p&gt;
    &lt;quote&gt;Indefinite integral for coordinate time&lt;lb/&gt;W(z) = (2 (3β+1) F(φ(z) | m) – 4 Π(n; φ(z) | m)) / ((9β2–1) √[(β+1) (5β–1)])&lt;lb/&gt;n = (3β–1) (5β+1) / [(3β+1) (5β–1)]&lt;lb/&gt;m = (β–1) (5β+1) / [(β+1) (5β–1)]&lt;lb/&gt;φ(z) = asin(√[(5β–1) (z+1) / [(5β+1) (z–1)]])&lt;/quote&gt;
    &lt;p&gt;Here F and Π are incomplete elliptic integrals of the first and third kind.&lt;/p&gt;
    &lt;p&gt;W '(z) = w(z), so after multiplying by the constants we set aside, W(z) will correctly track the passage of coordinate time t as the particle moves through different r coordinates and hence different z values. However, to make use of this we still need to add a suitable constant to set the origin for t, and determine the period of the motion. The two relevant locations for the particle are r = R, at the start of its descent, and r = 0, which will give us a quarter of one full period of the motion. These correspond to z = β and z = 1 respectively.&lt;/p&gt;
    &lt;p&gt;When z = β, we have:&lt;/p&gt;
    &lt;quote&gt;φ(β) = asin(√[(5β–1) (β+1) / [(5β+1) (β–1)]])&lt;lb/&gt;= asin(√[1/m])&lt;/quote&gt;
    &lt;p&gt;Using the definition of the elliptic integral F, this gives:&lt;/p&gt;
    &lt;quote&gt;F(φ(β) | m) = ∫0sin(φ(β)) dt / √[(1–t2) (1–mt2)]&lt;lb/&gt;= ∫0√[1/m] dt / √[(1–t2) (1–mt2)]&lt;/quote&gt;
    &lt;p&gt;With the change of variable t = √[1/m] u, this becomes:&lt;/p&gt;
    &lt;quote&gt;F(φ(β) | m) = √[1/m] ∫01 du / √[(1–u2/m) (1–u2)]&lt;lb/&gt;= √[1/m] F(π/2 | 1/m)&lt;lb/&gt;= √[1/m] K(1/m)&lt;/quote&gt;
    &lt;p&gt;Here K(m) is a complete elliptic integral of the first kind. Applying the same change of variables to the definition of the third elliptic integral yields:&lt;/p&gt;
    &lt;quote&gt;Π(n; φ(β) | m) = √[1/m] Π(n/m | 1/m)&lt;/quote&gt;
    &lt;p&gt;where the right-hand side has a complete elliptic integral of the third kind. We now have:&lt;/p&gt;
    &lt;quote&gt;W(β) = √[1/m] (2 (3β+1) K(1/m) – 4 Π(n/m | 1/m)) / ((9β2–1) √[(β+1) (5β–1)])&lt;/quote&gt;
    &lt;p&gt;So we can compute the time since the particle fell from the top of the borehole, by subtracting W(β):&lt;/p&gt;
    &lt;quote&gt;tsolid, GR(r) = (2√2) R √[(R–2M)/M] (W(z(r)) – W(β))&lt;/quote&gt;
    &lt;p&gt;To find the quarter-period of the borehole oscillations, we need to be able to evaluate W(z(0)) = W(1), which is a little tricky, because sin(φ(z)) has z–1 in the denominator. As z approaches 1 from below, sin(φ(z)) goes to infinity along the positive imaginary axis. So we need to evaluate the first elliptic integral F as:&lt;/p&gt;
    &lt;quote&gt;F(φ(z) | m) → ∫0i ∞ dt / √[(1–t2) (1–mt2)]&lt;/quote&gt;
    &lt;p&gt;A change of variable t = –i tan(u) transforms this to:&lt;/p&gt;
    &lt;quote&gt;F(φ(z) | m) → i ∫0π/2 du / √[1 – (1–m) sin2 u] = i K(1–m)&lt;/quote&gt;
    &lt;p&gt;For the elliptic integral of the third kind, we have:&lt;/p&gt;
    &lt;quote&gt;Π(n; φ(z) | m) → ∫0i ∞ dt / ((1–nt2) √[(1–t2) (1–mt2)])&lt;/quote&gt;
    &lt;p&gt;The same change of variable gives:&lt;/p&gt;
    &lt;quote&gt;Π(n; φ(z) | m) → i ∫0π/2 du / ((1 + n tan2 u) √[1 – (1–m) sin2 u])&lt;/quote&gt;
    &lt;p&gt;We can split this into two complete elliptic integrals:&lt;/p&gt;
    &lt;quote&gt;1 / (1 + n tan2 u) = (1 – n / (1 – (1–n) sin2 u)) / (1 – n)&lt;lb/&gt;Π(n; φ(z) | m) → i [K(1–m) – n Π(1–n | 1–m)] / (1 – n)&lt;/quote&gt;
    &lt;p&gt;This yields:&lt;/p&gt;
    &lt;quote&gt;W(1) = i (2 (3β+1) K(1–m) – 4 [K(1–m) – n Π(1–n | 1–m)] / (1 – n)) / ((9β2–1) √[(β+1) (5β–1)])&lt;/quote&gt;
    &lt;p&gt;The quarter-period of the borehole oscillations is then the time for the particle to fall from r = R to r = 0:&lt;/p&gt;
    &lt;quote&gt;¼ Tsolid, coord(r) = (2√2) R √[(R–2M)/M] (W(1) – W(β))&lt;/quote&gt;
    &lt;p&gt;We can also perform the same calculations for proper time, rather than coordinate time. In this case, we want to integrate:&lt;/p&gt;
    &lt;quote&gt;dτ/dz = √[R3/(2M] (3β–z) / √[(1–z2) (z–β) (5β–z)]&lt;/quote&gt;
    &lt;p&gt;The result is:&lt;/p&gt;
    &lt;quote&gt;Indefinite integral for proper time&lt;lb/&gt;W(z) = (2 (3β–1) F(φ(z) | m) + 4 Π(n; φ(z) | m)) / √[(β+1) (5β–1)]&lt;lb/&gt;n = (5β+1) / (5β–1)&lt;lb/&gt;m = (β–1) (5β+1) / [(β+1) (5β–1)]&lt;lb/&gt;φ(z) = asin(√[(5β–1) (z+1) / [(5β+1) (z–1)]])&lt;/quote&gt;
    &lt;p&gt;The same techniques as before let us evaluate the elliptic integrals at the endpoints of the quarter-cycle, z = β and z = 1.&lt;/p&gt;
    &lt;p&gt;[1] “Über das Gravitationsfeld eines Massenpunktes nach der Einsteinschen Theorie” [On the gravitational field of a point mass according to Einstein’s theory] by K. Schwarzschild, Sitzber. Deut. Akad. Wiss. Berlin, Kl. Math.-Phys. Tech., pp 189-196, 1916.&lt;/p&gt;
    &lt;p&gt;[2] “Über das Gravitationsfeld einer Kugel aus inkompressibler Flussigkeit nach der Einsteinschen Theorie” [On the gravitational field of a sphere of incompressible fluid according to Einstein’s theory] by K. Schwarzschild, Sitzber. Deut. Akad. Wiss. Berlin, Kl. Math.-Phys. Tech., pp 424-434, 1916.&lt;/p&gt;
    &lt;p&gt;[3] Gravitation by Charles Misner, Kip Thorne and John Wheeler, W.H. Freeman, San Francisco, 1973. Section 23.7 and Box 23.2.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477516</guid><pubDate>Sat, 04 Oct 2025 23:02:32 +0000</pubDate></item><item><title>AI-powered open-source code laundering</title><link>https://github.com/SudoMaker/rEFui/blob/main/HALL_OF_SHAME.md</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477661</guid><pubDate>Sat, 04 Oct 2025 23:26:02 +0000</pubDate></item><item><title>Space Mission Options for Reconnaissance and Mitigation of Asteroid 2024 YR4</title><link>https://arxiv.org/abs/2509.12351</link><description>&lt;doc fingerprint="e49c962254f5788f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Astrophysics &amp;gt; Instrumentation and Methods for Astrophysics&lt;/head&gt;&lt;p&gt; [Submitted on 15 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Space Mission Options for Reconnaissance and Mitigation of Asteroid 2024 YR4&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Near-Earth asteroid 2024 YR4 was discovered on 2024-12-27 and its probability of Earth impact in December 2032 peaked at about 3% on 2025-02-18. Additional observations ruled out Earth impact by 2025-02-23. However, the probability of lunar impact in December 2032 then rose, reaching about 4% by the end of the apparition in May 2025. James Webb Space Telescope (JWST) observations on 2025-03-26 estimated the asteroid's diameter at 60 +/- 7 m. Studies of 2024 YR4's potential lunar impact effects suggest lunar ejecta could increase micrometeoroid debris flux in low Earth orbit up to 1000 times above background levels over just a few days, possibly threatening astronauts and spacecraft. In this work, we present options for space missions to 2024 YR4 that could be utilized if lunar impact is confirmed. We cover flyby &amp;amp; rendezvous reconnaissance, deflection, and robust disruption of the asteroid. We examine both rapid-response and delayed launch options through 2032. We evaluate chemical and solar electric propulsion, various launch vehicles, optimized deep space maneuvers, and gravity assists. Re-tasking extant spacecraft and using built spacecraft not yet launched are also considered. The best reconnaissance mission options launch in late 2028, leaving only approximately three years for development at the time of this writing in August 2025. Deflection missions were assessed and appear impractical. However, kinetic robust disruption missions are available with launches between April 2030 and April 2032. Nuclear robust disruption missions are also available with launches between late 2029 and late 2031. Finally, even if lunar impact is ruled out there is significant potential utility in deploying a reconnaissance mission to characterize the asteroid.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;astro-ph.IM&lt;/p&gt;&lt;p&gt; Change to browse by: &lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477742</guid><pubDate>Sat, 04 Oct 2025 23:42:55 +0000</pubDate></item><item><title>Mod. 5140 - IBM's First Laptop Computer</title><link>https://richardsapperdesign.com/products/mod-5140/</link><description>&lt;doc fingerprint="515b578191daf980"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mod. 5140&lt;/head&gt;
    &lt;p&gt;With Colleen Sweeney&lt;/p&gt;
    &lt;p&gt;Prize Premio SMAU 1986&lt;lb/&gt; IF Industrie Forum Design Award Hannover 1988&lt;lb/&gt; Selection Compasso d’Oro 1987&lt;/p&gt;
    &lt;p&gt;This was IBM’s first laptop computer. It was developed in the IBM lab in Boca Raton, Florida, an area notoriously infested with alligators. From its side, the Mod. 5140 evokes a resemblance to an alligator’s head. When the printer is attached to its end, with paper flowing from the rear, it resembles the animal’s tail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477971</guid><pubDate>Sun, 05 Oct 2025 00:35:10 +0000</pubDate></item><item><title>Parrot – type-safe SQL in Gleam, supports SQlite, PostgreSQL and MySQL</title><link>https://github.com/daniellionel01/parrot</link><description>&lt;doc fingerprint="e20ba422ef3c70fe"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;🚨 Exciting News&lt;/p&gt;&lt;lb/&gt;Parrot got listed a community project on the sqlc website! 🦜🎉&lt;lb/&gt;Check it out here: https://docs.sqlc.dev/en/latest/reference/language-support.html&lt;/quote&gt;
    &lt;p&gt;Table of contents generated with markdown-toc&lt;/p&gt;
    &lt;p&gt;Most of the heavy lifting features are provided by / built into sqlc, I do not aim to take credit for them.&lt;/p&gt;
    &lt;p&gt;☑️ Supports SQlite, PostgreSQL and MySQL.&lt;lb/&gt; ☑️ Multiple queries per file.&lt;lb/&gt; ☑️ Database client agnostic.&lt;lb/&gt; ☑️ Utility wrappers for popular gleam database libraries (lpil/sqlight, lpil/pog).&lt;lb/&gt; ☑️ Automatically pulls the schema of your database.&lt;lb/&gt; ☑️ Automatically downloads sqlc binary.&lt;lb/&gt; ☑️ Named parameters.*1 &lt;/p&gt;
    &lt;p&gt;*1: Meaning that it infers the names of the parameters from your sql queries in the gleam function you call. for example for a query called &lt;code&gt;FindUser&lt;/code&gt;, defined as &lt;code&gt;SELECT * FROM user WHERE username = $1&lt;/code&gt;, parrot will produce a function where the arguments match those column names: &lt;code&gt;pub fn find_user(username: String) { ... }&lt;/code&gt;. If you have multiple parameters of the same data types this can avoid confusion and bugs.&lt;/p&gt;
    &lt;code&gt;$ gleam add parrot&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parrot will look for all *.sql files in any sql directory under your project's src directory.&lt;/item&gt;
      &lt;item&gt;Each *.sql file can contain as many SQL queries as you want.&lt;/item&gt;
      &lt;item&gt;All of the queries will compile into a single &lt;code&gt;src/[project name]/sql.gleam&lt;/code&gt;module.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some links to help you start out, if you are unfamiliar with the sqlc annotation syntax:&lt;/p&gt;
    &lt;p&gt;Here is an example of the file structure:&lt;/p&gt;
    &lt;code&gt;├── gleam.toml
├── README.md
├── src
│   ├── app.gleam
│   └── sql
│       ├── auth.sql
│       └── posts.sql
└── test
   └── app_test.gleam&lt;/code&gt;
    &lt;code&gt;# automatically detects database &amp;amp; engine from env (DATABASE_URL by default)
$ gleam run -m parrot

# provide connection string from different environment variable
$ gleam run -m parrot -- -e PG_DATABASE_URL

# specify sqlite file
$ gleam run -m parrot -- --sqlite &amp;lt;file_path&amp;gt;

# see all options
$ gleam run -m parrot help&lt;/code&gt;
    &lt;p&gt;If you use SQLite, you also need to have installed sqlite3.&lt;/p&gt;
    &lt;p&gt;If you use MySQL, you also need to have installed mysqldump (comes by default if you have a mysql client installed).&lt;/p&gt;
    &lt;p&gt;If you use PostgreSQL, you also need to have installed pg_dump (comes by default if you have a postgresql client installed).&lt;/p&gt;
    &lt;p&gt;You now have type safe access to your sql queries.&lt;/p&gt;
    &lt;p&gt;You might want to write wrapper functions for the database client library of your choice. If you are using lpil/pog or lpil/sqlight, you are in luck! You can find functions to copy &amp;amp; paste into your codebase here: wrappers&lt;/p&gt;
    &lt;p&gt;An example with lpil/sqlight:&lt;/p&gt;
    &lt;code&gt;import app/sql
import parrot/dev

fn parrot_to_sqlight(param: dev.Param) -&amp;gt; sqlight.Value {
  // ...
}

pub fn main() {
  // ...

  let #(sql, with, expecting) = sql.get_user_by_username("alice")
  let with = parrot_to_sqlight(with)
  let row = sqlight.query(sql, on:, with:, expecting:)

  // ...
}&lt;/code&gt;
    &lt;p&gt;If you want to see how this library works in action, take a look at the integration tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL: integration/psql&lt;/item&gt;
      &lt;item&gt;MySQL: integration/mysql&lt;/item&gt;
      &lt;item&gt;SQlite: integration/sqlite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;just is used to run project commands.&lt;/p&gt;
    &lt;p&gt;There are scripts to spawn a MySQL or PostgreSQL docker container:&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;$ ./bin/mysql.sh
# or
$ ./bin/psql.sh&lt;/code&gt;
    &lt;code&gt;$ just test-sqlite
$ just test-mysql
$ just test-psql&lt;/code&gt;
    &lt;p&gt;As with everything in software, there are some quirks with this library, due to the nature of your database of choice and sqlc.&lt;/p&gt;
    &lt;p&gt;If you have an &lt;code&gt;INTEGER[][]&lt;/code&gt; column in Postgres, &lt;code&gt;pg_dump&lt;/code&gt; does not correctly identify
the column as a two-dimensional array and therefore only gives you a &lt;code&gt;List(Int)&lt;/code&gt; instead
of a &lt;code&gt;List(List(Int))&lt;/code&gt;. If this is a problem for you, you can raise an issue and
we might come up with a solution or workaround.&lt;/p&gt;
    &lt;p&gt;There are a couple of complex data types that are explictly made &lt;code&gt;dynamic&lt;/code&gt;
since they are too complex to handle with the current implementation.
There is a plan for a better and more flexible implementation. Until then,
it will be wrapped in a dynamic type.&lt;/p&gt;
    &lt;p&gt;So here is the catch: you can only execute parrot in an erlang gleam application. However the generated code will also run in a javascript environment. So if you need parrot for a javascript project, you can create a separate package and copy over the generated module and that will work.&lt;/p&gt;
    &lt;p&gt;This library supports everything that sqlc supports. As the time of this writing that would be MySQL, PostgreSQL and SQlite.&lt;/p&gt;
    &lt;p&gt;You can read more on language &amp;amp; SQL support here: https://docs.sqlc.dev/en/stable/reference/language-support.html&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;embeddeding structs (https://docs.sqlc.dev/en/stable/howto/embedding.html)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Certain query annotations are not supported and will panic the process:&lt;/p&gt;&lt;code&gt;:execrows&lt;/code&gt;,&lt;code&gt;:execlastid&lt;/code&gt;,&lt;code&gt;:batchexec&lt;/code&gt;,&lt;code&gt;:batchone&lt;/code&gt;,&lt;code&gt;:batchmany&lt;/code&gt;,&lt;code&gt;:copyfrom&lt;/code&gt;. You can read more about it here: https://docs.sqlc.dev/en/stable/reference/query-annotations.html&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ideas and actionable tasks are collected and organised here: https://github.com/daniellionel01/parrot/issues&lt;/p&gt;
    &lt;p&gt;Contributions are welcomed!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This project was heavily inspired by &lt;code&gt;squirrel&lt;/code&gt;(Hex, GitHub). Thank you @giacomocavalieri!&lt;/item&gt;
      &lt;item&gt;Thank you to &lt;code&gt;sqlc&lt;/code&gt;(GitHub, Website)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478033</guid><pubDate>Sun, 05 Oct 2025 00:51:44 +0000</pubDate></item><item><title>You can't parse XML with regex. Let's do it anyways</title><link>https://sdomi.pl/weblog/26-nobody-here-is-free-of-sin/</link><description>&lt;doc fingerprint="760f8950ed21739e"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;You can't parse XML with regex. Let's do it anyways.&lt;/head&gt;this scene came to me in a dream&lt;head rend="h3"&gt;Haruhi says...&lt;/head&gt;"They didn't even get to the blogpost stuff1 and they're already making a contradictory statement. #1&lt;p&gt;Content is a word of the enemy. Companies will say "content" instead of calling it artworks, writings, pieces, and such, as if all media is something interchangeable meant to fill a box. Referring to "art" as "content" nowadays is often pejorative. If I ever make a "CDN" (Content Delivery Network), I will call it an SDN instead. Sounds much comfier.Has to be some sort of record! Fortunately, this contradiction is far from being the last in this post. "&lt;/p&gt;&lt;p&gt;Attempting to parse HTML with regular expressions is an infamous pitfall, and a great example of using the wrong tool for the job. It's generally accepted to be a bad idea, for a multitude of reasons.&lt;/p&gt;Picture 1 - he keeps on going for like 3 more screens (Stack Overflow link)&lt;p&gt;There's this famous Stack Overflow answer about why you should never, ever do it. In fact, this answer got so popular that it was used like a copypasta in some circles. Every time I stumbled upon it, I would think how there's a lot of truth in it - but at the same time, I couldn't agree in full...&lt;/p&gt;&lt;head rend="h3"&gt;But... can't you, really?&lt;/head&gt;Picture 2 - did you know that XML has a logo? I'm not joking, I only learnt today too&lt;p&gt;While I assume that all readers of this weblog have at least a vague understanding of XML, it's worth to recap for the sake of later arguments. Quoting the Wikipedia article on XML:&lt;/p&gt;Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.&lt;p&gt;I'd like to focus on three parts:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;It's a markup language: unlike JSON or TOML3, #3&lt;lb/&gt;This sentence originally mentioned YAML. This post isn't about YAML, and yet I got a lot of complaints for implying that YAML could be considered simple. This discussion has absolutely no relevance to this post, so I replaced it with TOML. Don't like TOML? Think of INI. Don't like INI? Think of CSV, etc.XML defines a much more specific structure for the document. Other SGML derivatives are a bit more lax with enforcing said structure - remember this fact for later.&lt;/item&gt;&lt;item&gt;It's machine-readable: it's designed to be parsed and interpreted into a tree.&lt;/item&gt;&lt;item&gt;It's human-readable: no specialized tools are required to look at and understand the data contained within an XML document.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;What Wikipedia doesn't immediately convey (you have to scroll down to section 11) is that XML is horribly complex. JSON, TOML and many other human-readable data interchange formats are simple enough that many self-taught developers learn them through osmosis. Heck, RFC8259, "The JavaScript Object Notation (JSON) Data Interchange Format", is 16 pages long, out of which the actual format description takes maybe 8. In contrast, the base XML 1.0 (Second Edition) spec is 59 pages long, and that doesn't include various extensions that have grown onto it since 2000. Unsurprisingly, this larger surface area becomes a security liability when developers aren't familiar with the whole feature set.&lt;/p&gt;&lt;p&gt;This lack of in-depth knowledge about the format is why newbies even consider parsing XML with a regex. It's a "you don't know what you don't know" problem, which leads to a vastly different approach when writing a parser.&lt;/p&gt;&lt;head rend="h3"&gt;Your parser ≠ My parser&lt;/head&gt;&lt;p&gt;Let's get back to the "machine-readable" vs. "human-readable" part; Assume we have a stack-based parser; this makes it easy to illustrate where the parser is in a given structure. (To refresh, a stack is a queue/array where the operations are "push", that adds a value to the end, and "pop", which removes and returns that value to our program.)&lt;/p&gt;&lt;code&gt;&amp;lt;a&amp;gt;
    &amp;lt;b&amp;gt;
        &amp;lt;c&amp;gt;meow&amp;lt;/c&amp;gt;
        &amp;lt;d&amp;gt;nya&amp;lt;/d&amp;gt;
    &amp;lt;/b&amp;gt;
&amp;lt;/a&amp;gt;
&lt;/code&gt;

Figure 1 - a very simple XML-like object tree

&lt;p&gt;Here's a simplified view of how a parser may "walk" a tree:&lt;/p&gt;&lt;code&gt;                     #              stack=()
&amp;lt;a&amp;gt;                  # push a;      stack=(a)
    &amp;lt;b&amp;gt;              # push b;      stack=(a b)
        &amp;lt;c&amp;gt;meow&amp;lt;/c&amp;gt;  # push c;      stack=(a b c)
        &amp;lt;d&amp;gt;nya&amp;lt;/d&amp;gt;   # pop; push d; stack=(a b d)
    &amp;lt;/b&amp;gt;             # pop;         stack=(a b)
&amp;lt;/a&amp;gt;                 # pop;         stack=(a)
                     # pop;         stack=()
&lt;/code&gt;


Figure 2 - same tree, now hastily annotated with actions and state

&lt;p&gt;While the example above doesn't show anything useful happening with our tree, it's actually quite simple to incorporate a DOM-like selector query system on top of this. The following snippet implements a very naïve XML-like parser, which can be used to extract strings from objects:&lt;/p&gt;&lt;code&gt;#!/usr/bin/env bash
# Please don't actually use this. xoxo, dmi
stack=()
tokens=()
buf=

# QUERY=(a b c)
QUERY=($@)

flush() {
	if [[ "$buf" ]]; then
		tokens+=("$buf")
	fi
	buf=
}

search() {
	(( ${#stack[@]} &amp;lt; ${#QUERY[@]} )) &amp;amp;&amp;amp; return
	[[ ${tokens[-1]} != "lbrack" ]] &amp;amp;&amp;amp; return
	for (( i=0; i&amp;lt;${#QUERY[@]}; i++ )); do
		if [[ "${QUERY[i]}" != "${stack[-${#QUERY[@]}+i]}" ]]; then
			return
		fi
	done

	echo "query result: ${tokens[-2]}"
}

while read -rn1 chr; do
	if [[ "$chr" == "&amp;lt;" ]]; then
		flush
		tokens+=("lbrack")
	elif [[ "$chr" == "&amp;gt;" ]]; then
		if [[ "${tokens[-1]}" == "lbrack" ]]; then
			flush # get tag contents
			stack+=("${tokens[-1]}") # put it onto the stack
		elif [[ "${tokens[-1]}" == "slash" ]]; then
			unset stack[${#stack[@]}-1] # pop last element
		fi
		tokens+=("rbrack")
	elif [[ "$chr" == "/" &amp;amp;&amp;amp; "${tokens[-1]}" == "lbrack" ]]; then
		tokens+=("slash")
	else
		buf+="$chr"
	fi

	search
done
&lt;/code&gt;


Figure 3 - bash parser for our markup.&lt;p&gt;i will invest in syntax coloring next quarter&lt;/p&gt;&lt;p&gt;The result is:&lt;/p&gt;&lt;code&gt;## in DOM selector terms, 'a b c' would be 'a &amp;gt; b &amp;gt; c'
$ ./parse.sh a b c &amp;lt; test.xml 
query result: meow
$ ./parse.sh a b d &amp;lt; test.xml 
query result: nya
&lt;/code&gt;

Figure 4 - parser demonstration

&lt;p&gt;This "walking" behavior can be visualized even better after adding declare -p stack to every loop:&lt;/p&gt;&lt;code&gt;$ ./parse.sh a b d &amp;lt; test 
declare -a stack=()
declare -a stack=()
declare -a stack=([0]="a")
declare -a stack=([0]="a")
declare -a stack=([0]="a")
# (...)
declare -a stack=([0]="a" [1]="b")
# (...)
declare -a stack=([0]="a" [1]="b" [2]="c")
declare -a stack=([0]="a" [1]="b" [2]="c")
declare -a stack=([0]="a" [1]="b" [2]="c")
# (...)
declare -a stack=([0]="a" [1]="b")
# (...)
declare -a stack=([0]="a" [1]="b" [2]="d")
declare -a stack=([0]="a" [1]="b" [2]="d")
declare -a stack=([0]="a" [1]="b" [2]="d")
declare -a stack=([0]="a" [1]="b" [2]="d")
query result: nya
declare -a stack=([0]="a" [1]="b" [2]="d")
declare -a stack=([0]="a" [1]="b" [2]="d")
declare -a stack=([0]="a" [1]="b" [2]="d")
declare -a stack=([0]="a" [1]="b")
# (...)
declare -a stack=([0]="a")
# (...)
declare -a stack=()
&lt;/code&gt;

Figure 5 - stack in action

&lt;p&gt;Due to the single-pass nature of our parser (which combines tokenization and a few other steps into one), I had to remove some repetition. Furthermore, this parser is for demonstrational purposes only and cannot parse arbitrary XML. Real-world XML has a lot of special objects, self-terminating tags, and other gotchas that have to be accounted for, even during a simple text extraction.&lt;/p&gt;&lt;head rend="h3"&gt;How your brain reads XML&lt;/head&gt;&lt;p&gt;Now that you have a gist of how an algorithm for parsing XML may work (and hopefully understand that writing a parser is a lot of pain), let's step back and consider how we, creatures of protein and flesh, parse XML. To make things harder, let's look at the raw, true form of XML - no pretty-printing allowed.&lt;/p&gt;&lt;code&gt;&amp;lt;a&amp;gt;&amp;lt;b&amp;gt;&amp;lt;c&amp;gt;meow&amp;lt;/c&amp;gt;&amp;lt;d&amp;gt;nya&amp;lt;/d&amp;gt;&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;
&lt;/code&gt;


Figure 6 - example from before, compacted

&lt;p&gt;To an untrained eye, this doesn't look like a tree.&lt;/p&gt;&lt;code&gt;    &amp;lt;a &amp;gt;
   &amp;lt;b   &amp;gt;
 &amp;lt;d &amp;gt;nya&amp;lt;/d&amp;gt;
&amp;lt;c  &amp;gt;meow&amp;lt;/c&amp;gt;
    &amp;lt;/b&amp;gt;
    &amp;lt;/a&amp;gt;
&lt;/code&gt;

Figure 7 - the same structure, with whitespace arranged to form an x-mas tree

&lt;p&gt;Ah, much better! This is semantically equivalent to all the snippets I've attached before, but you have to think really hard to picture that a &amp;gt; b &amp;gt; (c, d). To me, this snippet is first and foremost a string.&lt;/p&gt;&lt;head rend="h3"&gt;String parsing&lt;/head&gt;&lt;p&gt;Approaching XML or any other structured data format as a string is like dumpster-diving for parts. I don't mean this in a bad way; both regex and dumpster diving have awarded me some great stuff. But they also give me the urge to shower immediately afterwards.&lt;/p&gt;&lt;p&gt;To continue the analogy, you can't inquire about why something got thrown out (as in, why given data is present and why it is formatted the way it is). This information is lost. You can make educated guesses if you stare at it long enough, but you can't know for sure. Worse even, if your data changes (as may happen with XML returned by an API), the whole tree may get ordered in a slightly different way, rendering your meticulously crafted parser useless. For this - and many other reasons - it's best to parse XML with a real parser.&lt;/p&gt;&lt;p&gt;I'll explore actual string parsing techniques later in this post. Before that, we have an elephant in the room to address...&lt;/p&gt;&lt;head rend="h3"&gt;HTML: XML but quirky&lt;/head&gt;&lt;head&gt;Pedantry Corner&lt;/head&gt;&lt;p&gt;In opposition, I'd like to argue that while XML inspires fear in CS majors and hackers alike, virtually nobody knows about SGML. HTML is quirky XML.&lt;/p&gt;&lt;p&gt;HTML is the main language used for presentation online. The web lives and breathes HTML. You can make webapps without WebAssembly, without ECMAScript, or even without CSS. But you absolutely need2 HTML (... or XHTML - hold that thought). #2&lt;lb/&gt; Before publication, Lisa argued that you technically can make pages without HTML:&lt;lb/&gt; SVG, Java Applets, Flash, PDF&lt;lb/&gt; One could discredit the last three options, as they're external technologies that aren't a part of any Web spec. However, SVG is much tougher to ignore. It's a W3C Recommendation, which makes it at least adjacent. It also specifies the &amp;lt;a&amp;gt; tag, so technically SVG could be used "without HTML" to create a webpage. I remain sceptical. &lt;/p&gt;&lt;p&gt;A few thousand bytes ago, I touched on how XML is extremely strict in the layout. HTML is the exact opposite, allowing for unclosed tags and broken grammar. An XML parser would get a heart attack if asked to parse HTML found online.&lt;/p&gt;&lt;head rend="h4"&gt;Parsing HTML is near-impossible&lt;/head&gt;&lt;p&gt;Well-formed HTML is fine. However, browsers are designed to make educated guesses instead of failing outright when the markup doesn't fit. This was a compromise made for accessibility. Today's devtools make debugging easy, but in the early 90s? There was virtually no tooling for this. Having the parsers accept slightly mangled input no doubt improved adoption when HTML was all new.&lt;/p&gt;&lt;p&gt;Sadly, this means that HTML is already two layers removed from XML. Quirks mode is largely based on how things got implemented by IE and Netscape 30 years ago. Standards compliance mode somewhat improves the situation, but it will still accept missing closing tags or quotes.&lt;/p&gt;&lt;p&gt;That being said, virtually all of those situations are defined by the standard, and contemporary browsers implement it extremely closely. Why is it "near-impossible" then? HTML living standard dwarfs the base of XML, being over 1500 pages long! ...Okay, perhaps that's a bit unfair - at the time of writing, only 114 of those pages actually deal with parsing (thanks for checking, Linus!). Regardless, that's still over x2 the length of the XML standard, and this growth is mostly defining edge-cases! Unless you're using an actual browser, chances are that your DOM tree will parse slightly differently on pages that aren't well-formed.&lt;/p&gt;&lt;head rend="h3"&gt;HTML4.01? Ridiculous! We need to develop a better alternative that suits everyone's needs&lt;/head&gt;Situation: there are two sibling standards.&lt;p&gt;XHTML is... a weird creature. It was first introduced in late 1998 and refined into a standard that was adopted as a W3C recommendation in January 2000. Unfortunately, it wasn't widely adopted (unlike later HTML5)...&lt;/p&gt;The attempt to get the world to switch to XML, including quotes around attribute values and slashes in empty tags and namespaces all at once didn't work. The large HTML-generating public did not move, largely because the browsers didn't complain. Some large communities did shift and are enjoying the fruits of well-formed systems, but not all.&lt;p&gt;~ Tim Berners Lee, 2005&lt;/p&gt;&lt;p&gt;I'm only mentioning XHTML here because, technically, we've had a strict, well-defined HTML alternative for almost 3 decades by now, despite not many people knowing about it. Heck, XHTML5 exists too! You can use it right now! It's really cool! (famfo keeps telling me about it, so it has to be true.)&lt;/p&gt;&lt;head rend="h3"&gt;Finally: actually parsing HTML with regex&lt;/head&gt;&lt;p&gt;The following section is entirely a product of my attempts to scrape various webpages over the years. I'm aware how badly the practice of scraping is viewed in some circles, and I'd like to assure the reader that the bots I've built in the past have always been slow to request, and used extensive caching. GenAI scrapers constantly DoSing the internet can go to hell.&lt;/p&gt;&lt;head rend="h3"&gt;Benefits&lt;/head&gt;&lt;head rend="h3"&gt;Haruhi says...&lt;/head&gt;"Bet you didn't expect them to talk about benefits after they spent so long rambling about how hard it is to parse HTML. Ha!"&lt;list rend="ol"&gt;&lt;item&gt;Development speed&lt;/item&gt;&lt;item&gt;Adaptability&lt;/item&gt;&lt;item&gt;In ECMAScript, that'd be document.querySelectorAll("#scroll0 &amp;gt; span")... And then you have to join the strings, so more like let a=""; document.querySelectorAll("#scroll0 &amp;gt; span").forEach((e)=&amp;gt;{a+=e.innerText;}); console.log(a);&lt;/item&gt;&lt;item&gt;With regex, I'd start by matching for scroll0".*?&amp;lt;/div, followed by removing everything that matches &amp;lt;[/a-zA-Z]&amp;gt;. This leaves us with a lot of spaces, which can be mitigated by matching for (two spaces in a row). My shell one-liner looks something like: curl (...) | tr -d '\r\n' | grep -Poh 'scroll0.*?&amp;lt;/div' | sed 's@&amp;lt;[/a-zA-Z]*&amp;gt;@@g;s/ //g;'&lt;/item&gt;&lt;item&gt;Complexity&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Modern websites often have hundreds, if not thousands of nested elements. Writing a selector for something really deep down can take a while, especially if additional constraints are present (randomized class names? the developer only knowing about div-s?).&lt;/p&gt;&lt;p&gt;Writing a regex takes me 30 seconds. But hacking up a good selector and debugging why it doesn't work on the next request? Tens of minutes of cursing.&lt;/p&gt;&lt;p&gt;Selectors are strict. They either give you a result or fail. This is great when you trust the other side of the system to send you good, accurate markup. HOWEVER, this is not something you can expect when scraping. For instance:&lt;/p&gt;&lt;code&gt;    (...)
            &amp;lt;tr id="trainStation0" class="altrow"&amp;gt;
        &amp;lt;td class="no_border"&amp;gt;Peterborough&amp;lt;/td&amp;gt;
        &amp;lt;td class="centre_col no_border platform"&amp;gt;1&amp;lt;/td&amp;gt;
        &amp;lt;td class="centre_col no_border"&amp;gt;1801&amp;lt;/td&amp;gt;
        &amp;lt;td class="centre_col no_border on_time"&amp;gt;On Time&amp;lt;/td&amp;gt;
        
    &amp;lt;/tr&amp;gt;

            &amp;lt;tr id="callingPoints0" class="altrow"&amp;gt;
    &amp;lt;td class="calling_list" colspan="4"&amp;gt;
        &amp;lt;div id="scroll0" class="scrollable"&amp;gt;

            
            
                    &amp;lt;span class="cp_header"&amp;gt;Calling at:&amp;lt;/span&amp;gt;
                    
                
                    &amp;lt;span&amp;gt;Ifield&amp;lt;/span&amp;gt;
                    &amp;lt;span&amp;gt;(1805), &amp;lt;/span&amp;gt;
                
                    &amp;lt;span&amp;gt;Crawley&amp;lt;/span&amp;gt;
                    &amp;lt;span&amp;gt;(1808), &amp;lt;/span&amp;gt;
                
                    &amp;lt;span&amp;gt;Three Bridges&amp;lt;/span&amp;gt;
                    &amp;lt;span&amp;gt;(1812), &amp;lt;/span&amp;gt;
                
                    &amp;lt;span&amp;gt;Gatwick Airport&amp;lt;/span&amp;gt;
                    &amp;lt;span&amp;gt;(1817), &amp;lt;/span&amp;gt;
    (...)
&lt;/code&gt;


Figure 8 - Excerpt from a departure table of the least used train station in West Sussex

&lt;p&gt;Say, if we wanted to extract all the stations where this train calls at.&lt;/p&gt;&lt;p&gt;This leaves us with the following payload:&lt;/p&gt;&lt;code&gt;scroll0" class="scrollable"&amp;gt;&amp;lt;span class="cp_header"&amp;gt;Calling at:Ifield(1835), Crawley(1838), Three Bridges(1842), Gatwick Airport(1847), Horley(1851), Redhill(1859), Merstham(1903), Coulsdon South(1908), East Croydon(1915), London Bridge(1930), London Blackfriars(1936), City Thameslink(1938), Farringdon(1941), London St Pancras (Intl)(1945), Finsbury Park(1952), Stevenage(2013), Hitchin(2020), Arlesey(2026), Biggleswade(2031), Sandy(2035), St Neots(2042), Huntingdon(2049), &amp;lt;span class="cp_dest"&amp;gt;Peterborough&amp;lt;span class="cp_dest"&amp;gt;(2105)&amp;lt;/div&lt;/code&gt;
	Figure 9 - Slightly dirty, but nonetheless extracted
&lt;p&gt;The remaining HTML markup could be removed with a few more sed filters, but that's not exactly the point of this example. Instead, imagine that National Rail changed their markup to stop using those ridiculous spans:&lt;/p&gt;&lt;code&gt;        (...)
        &amp;lt;div id="scroll0" class="scrollable"&amp;gt;
                    &amp;lt;span class="cp_header"&amp;gt;Calling at:&amp;lt;/span&amp;gt;
                
                    Ifield
                    (1805), 
                
                    Crawley
                    (1808), 
                
                    Three Bridges
                    (1812), 
                
                    Gatwick Airport
                    (1817), 
        (...)
&lt;/code&gt;


Figure 10 - Example modification that could happen to a page

&lt;p&gt;In this scenario, our existing selector #scroll0 &amp;gt; span only matches the first element, returning the words Calling at:, without the actual list of stations. On the other hand, the shell one-liner doesn't break because it didn't rely on markup for context, only anchoring. Please note: in some scenarios this may garble the output - especially during large layout changes. I still count it as a win for team regex.&lt;/p&gt;&lt;p&gt;HTML is not designed to be consumed by any program that isn't a browser. At heart, it's a language for presentation, not data interchange. This makes traditional selectors unsuitable for some tasks. Consider the following snippet:&lt;/p&gt;&lt;code&gt;    &amp;lt;li&amp;gt;Screen size: 21.37" &amp;lt;/li&amp;gt;
    &amp;lt;li&amp;gt;Battery capacity :  154Wh &amp;lt;/li&amp;gt;
&lt;/code&gt;


Figure 11 - a mock item info list

&lt;p&gt;There's nothing that could help the parser extract the key/value pairs. &amp;lt;li&amp;gt; is generic enough that it's probably present in a few different areas of the page. Even if you had a proper selector for it, you'd still have to split on :, then remove whitespace, and fuzzy-match the key. With regex, extracting the screen size is just a matter of grep -Pohi 'screen size ?:.*?"' | grep -Poh '[0-9]*\.[0-9]*"' - half of which you would need to do either way.&lt;/p&gt;&lt;head rend="h3"&gt;Some broad tips&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;Ask yourself whether regex is the right tool for the job. Unless you're scraping, or you're really low on available space for new libraries (writing for embedded?), then the correct approach is using some XML parser library. For some scraping workflows (extremely regular data, i.e. autogenerated tables), an HTML parser may prove more helpful than hacks described below.&lt;/item&gt;&lt;item&gt;If you are actually dealing with irregularly generated HTML, check if you can get the data elsewhere. A smart hacker doesn't scrape HTML when there's an Android app with readily available API endpoints returning JSON.&lt;/item&gt;&lt;item&gt;Don't try to actually parse the tree structure. This removes all benefits of using regex for scraping and welcomes you to a world of pain.&lt;/item&gt;&lt;item&gt;PCRE is a must. Standard regex doesn't have the ? non-greedy match operator. This means that a.*b will match the first "a", and the last "b", whereas doing a.*?b will match the first "a", and the first "b" that follows. This is VERY useful for anchoring to some unique text, and then matching the next closing tag (like this: meow.*?).&lt;lb/&gt;In grep, PCRE is enabled with flag -P, if compiled-in.&lt;/item&gt;&lt;item&gt;If you don't have PCRE, non-greedy matching can be emulated through: &lt;list rend="ol"&gt;&lt;item&gt;anchoring somewhere (meow.*)&lt;/item&gt;&lt;item&gt;replacing the first occurence of the end marker with a unique string (s|&amp;lt;/|OwO|)&lt;/item&gt;&lt;item&gt;and finally, matching to the unique character (meow.*OwO)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Always match the tightest variety of characters you can. [A-Za-z0-9]* is much better than .*?&lt;/item&gt;&lt;item&gt;If possible, anchor to unique strings of text. List of departures is less likely to change than a class name.&lt;/item&gt;&lt;item&gt;If there is interesting whitespace, use it to your advantage! All parser rules go out of the window - if each list element is on a separate line, don't make your life harder; just iterate over those lines.&lt;/item&gt;&lt;item&gt;Remove the uninteresting whitespace as soon as you can. Some regex engines allow matching \s*, in others you will have to define [ \t\r\n]* or something similar. Matching multiples (\s{2}, \s{3}, etc) and replacing them with single spaces in a loop can help to preserve the good whitespace where you need it.&lt;/item&gt;&lt;item&gt;If your browser requests a thing, you can most likely request it through curl. There are enough gotchas about this point to warrant a separate blogpost, but for now, remember about Dev Tools -&amp;gt; Network -&amp;gt; (right click on request) -&amp;gt; Copy as cURL.&lt;/item&gt;&lt;item&gt;Extraction will break. Good scraping bots account for it (and fail safe).&lt;lb/&gt;Very good bots notify the admin out-of-band.&lt;lb/&gt;Excellent bots may even try a second path for extracting data.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;The exhibit&lt;/head&gt;&lt;p&gt; I've had some trouble trying to settle on a specific thing to use as a sample, so I finally went with something pointless and silly. Here's a small scraper that extracts data from OpenRCT2's download webpage.&lt;lb/&gt; Of course, in this case the data is available through a variety of different means. Nonetheless, same principles can be applied to proprietary pages. &lt;/p&gt;&lt;code&gt;#!/bin/bash
unset IFS 
data="$(curl 'https://openrct2.io/download/release')"
# data="$(cat /tmp/dump)" # a good scraper always prototypes on a local dump

# match line that contains 'v' and 'latest', to get the current game version
latest_ver="$(grep -Poh 'v.*?(?=\(latest)' &amp;lt;&amp;lt;&amp;lt; "$data")"

# let's split the list of releases into something we can use:
#   1. make sure that there's no 0x01/0x02. we'll use those as our delimeters
#   2. split the page on "card-header", which occurs only between list items
releases="$(tr -d $'\x01\x02' &amp;lt;&amp;lt;&amp;lt; "$data" | sed 's/card-header/\x01/')"

# now, let's iterate over what we found:
while read -d$'\x01' -r release; do
	# 'btn btn-link' seems to always preceed the version. let's use that fact :)
	version="$(grep -A1 'btn btn-link' &amp;lt;&amp;lt;&amp;lt; "$release" | tail -n1 | sed 's/ //g')"
	version="${version#* }" # trim all whitespace from the beginning of the string
	if [[ ! "$version" ]]; then # empty? has to be the 1st item, let's fall back
		version="$latest_ver"
	fi

	# 'card-title' is our top anchor.
	# then, we can iterate over 'Download', which occurs at the end.
	# also here: cleanup all closing tags, since they're at the end of each line anyways.
	meta="$(grep card-title -C10 &amp;lt;&amp;lt;&amp;lt; "$release" | sed 's@&amp;lt;/.*@@' | sed $'s/Download/\x02/')"

	echo -e "OpenRCT $version\n-------" # CLI design is my passion

	# finally, we iterate over each artifact:
	while read -d$'\x02' -r entry; do
		# we're lucky, h5 is only used for the platform. remove all preceeding garbage.
		platform="$(grep h5 &amp;lt;&amp;lt;&amp;lt; "$entry" | sed 's/.*&amp;gt;//')"

		# architecture is right after a line with h6.
		# let's match h6 and the next line, then discard the first line
		arch="$(grep -A1 h6 &amp;lt;&amp;lt;&amp;lt; "$entry" | tail -n1 | sed 's/ *//;s/,//')"

		# artifact type is just one line below the arch.
		# match h6 and *two* following lines, then discard first two.
		type="$(grep -A2 h6 &amp;lt;&amp;lt;&amp;lt; "$entry" | tail -n1 | sed 's/ *//')"

		# url is simple, just extract data from quotes.
		# the space after " is load-bearing! otherwise we'd need to do additional cleanup
		url="$(grep '&amp;lt;a href' &amp;lt;&amp;lt;&amp;lt; "$entry" | sed -E 's@.*href="(.*)" .*@\1@')"

		# finally, presentation
		cat &amp;lt;&amp;lt;EOF
* $platform 
  - Arch: $arch
  - Type: $type
  - URL: $url
EOF
	done &amp;lt;&amp;lt;&amp;lt; "$meta"
	echo -e '\n\n'
done &amp;lt;&amp;lt;&amp;lt; "$releases"
&lt;/code&gt;
Figure 12 - annotated example scraper

&lt;p&gt;At the present, running this code shows a list of artifacts, divided by platform and version:&lt;/p&gt;Picture 3 - sample output of the scraper&lt;head rend="h3"&gt;Ending remarks&lt;/head&gt;&lt;p&gt;I suppose that the original Stack Overflow answer was about actually parsing the structure, retaining all the inherent metadata in the process. I agree with this point (who wouldn't? XML isn't regular, that's a solid fact). However, if "parsing" is treated as a means to an end (like regex usually is...), it absolutely is possible to extract data from HTML or XML with a sufficiently complex regular expression. That's, more or less, the asterisk that has been floating in my mind for all those years.&lt;/p&gt;&lt;p&gt;At some point, I'd like to expand on the whole scraping aspect of this post, possibly into a mini-series. Send me ACKs through avian carrier if you'd like to see that happen.&lt;/p&gt;&lt;p&gt;Notes:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;#1 - Content is a word of the enemy. Companies will say "content" instead of calling it artworks, writings, pieces, and such, as if all media is something interchangeable meant to fill a box. Referring to "art" as "content" nowadays is often pejorative. If I ever make a "CDN" (Content Delivery Network), I will call it an SDN instead. Sounds much comfier.&lt;/item&gt;&lt;item&gt;#2 - Before publication, Lisa argued that you technically can make pages without HTML: &lt;list rend="ul"&gt;&lt;item&gt;SVG&lt;/item&gt;&lt;item&gt;Java Applets&lt;/item&gt;&lt;item&gt;Flash&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;#3 - This sentence originally mentioned YAML. This post isn't about YAML, and yet I got a lot of complaints for implying that YAML could be considered simple. This discussion has absolutely no relevance to this post, so I replaced it with TOML. Don't like TOML? Think of INI. Don't like INI? Think of CSV, etc.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Edited by Miya. Standards technical consulting by Linus. Proofreading by Linus, Riedler, famfo, Lili, ari, lauren, kleines Filmröllchen, shebang, irth, mei and Multi (damn that's a lot). Thanks a lot to everyone for helping out, and thank you for reading until the end. &amp;lt;3&lt;/p&gt;&lt;head rend="h3"&gt;Comments:&lt;/head&gt;&lt;p&gt;oh i was completely unaware of grep -P and the ".*?" matcher, guess it's time to actually read through grep's manpage as always awesome co-, uh, stuff - thanks for the evening read domi :3c&lt;/p&gt;nat at 02.10.2025, 00:01:07&lt;p&gt;xhtml5 mentioned, yippee!&lt;/p&gt;By commenting, you agree for the session cookie to be stored on your device ;p&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478300</guid><pubDate>Sun, 05 Oct 2025 01:58:31 +0000</pubDate></item></channel></rss>