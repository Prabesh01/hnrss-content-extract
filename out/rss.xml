<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 04 Feb 2026 23:47:32 +0000</lastBuildDate><item><title>Guinea worm on track to be 2nd eradicated human disease; only 10 cases in 2025</title><link>https://arstechnica.com/health/2026/02/guinea-worm-on-track-to-be-2nd-eradicated-human-disease-only-10-cases-in-2025/</link><description>&lt;doc fingerprint="e3e534e737e1921a"&gt;
  &lt;main&gt;
    &lt;p&gt;A debilitating infection from the parasitic Guinea worm is inching closer to global eradication, with an all-time low of only 10 human cases reported worldwide in 2025, the Carter Center announced.&lt;/p&gt;
    &lt;p&gt;If health workers can fully wipe out the worms, it will be only the second human disease to be eradicated, after smallpox.&lt;/p&gt;
    &lt;p&gt;Guinea worm (Dracunculus medinensis) is a parasitic nematode transmitted in water. More specifically, it’s found in waters that contain small crustacean copepods, which harbor the worm’s larvae. If a person consumes water contaminated with Guinea worm, the parasites burrow through the intestinal tract and migrate through the body. About a year later, a spaghetti noodle-length worm emerges from a painful blister, usually in the feet or legs. It can take up to eight weeks for the adult worm to fully emerge. To ease the searing pain, infected people may put their blistered limbs in water, allowing the parasite to release more larvae and continue the cycle.&lt;/p&gt;
    &lt;p&gt;In addition to being extremely painful, the disease (dracunculiasis) can lead to complications, such as secondary infections and sepsis, which in turn can lead to temporary or permanent disability.&lt;/p&gt;
    &lt;p&gt;When the Guinea worm eradication program began in 1986, there were an estimated 3.5 million cases across 21 countries in Africa and Asia. To date, only six countries have not been certified by the World Health Organization as Guinea worm-free. In 2024, there were just 15 cases, and, according to the provisional tally for 2025, the number is down to just 10. It’s considered provisional until each country’s disease reports are confirmed, which occurs in a program meeting usually held in April.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46886191</guid><pubDate>Wed, 04 Feb 2026 14:27:05 +0000</pubDate></item><item><title>Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation</title><link>https://arxiv.org/abs/2602.00294</link><description>&lt;doc fingerprint="cc1c241ad7f7d6ef"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 30 Jan 2026]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46886265</guid><pubDate>Wed, 04 Feb 2026 14:33:33 +0000</pubDate></item><item><title>Voxtral Transcribe 2</title><link>https://mistral.ai/news/voxtral-transcribe-2</link><description>&lt;doc fingerprint="3c2dcd92a1ee1e85"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Voxtral transcribes &lt;lb/&gt; at the speed of sound.&lt;/head&gt;Try Voxtral Transcribe 2 in Mistral Studio&lt;p&gt; Precision diarization, real-time&lt;lb/&gt; transcription, and a new audio playground.&lt;/p&gt;&lt;p&gt;Today, we're releasing Voxtral Transcribe 2, two next-generation speech-to-text models with state-of-the-art transcription quality, diarization, and ultra-low latency. The family includes Voxtral Mini Transcribe V2 for batch transcription and Voxtral Realtime for live applications. Voxtral Realtime is open-weights under the Apache 2.0 license.&lt;/p&gt;&lt;p&gt;We're also launching an audio playground in Mistral Studio to test transcription instantly, powered by Voxtral Transcribe 2, with diarization and timestamps.&lt;/p&gt;&lt;head rend="h2"&gt;Highlights.&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Voxtral Mini Transcribe V2: State-of-the-art transcription with speaker diarization, context biasing, and word-level timestamps in 13 languages.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Voxtral Realtime: Purpose-built for live transcription with latency configurable down to sub-200ms, enabling voice agents and real-time applications.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Best-in-class efficiency: Industry-leading accuracy at a fraction of the cost, with Voxtral Mini Transcribe V2 achieving the lowest word error rate, at the lowest price point.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Open weights: Voxtral Realtime ships under Apache 2.0, deployable on edge for privacy-first applications.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Voxtral Realtime.&lt;/head&gt;&lt;p&gt;Voxtral Realtime is purpose-built for applications where latency matters. Unlike approaches that adapt offline models by processing audio in chunks, Realtime uses a novel streaming architecture that transcribes audio as it arrives. The model delivers transcriptions with delay configurable down to sub-200ms, unlocking a new class of voice-first applications.&lt;/p&gt;&lt;p&gt;Word error rate (lower is better) across languages in the FLEURS transcription benchmark.&lt;/p&gt;&lt;p&gt;At 2.4 seconds delay, ideal for subtitling, Realtime matches Voxtral Mini Transcribe V2, our latest batch model. At 480ms delay, it stays within 1-2% word error rate, enabling voice agents with near-offline accuracy.&lt;/p&gt;&lt;p&gt;The model is natively multilingual, achieving strong transcription performance in 13 languages, including English, Chinese, Hindi, Spanish, Arabic, French, Portuguese, Russian, German, Japanese, Korean, Italian, and Dutch. With a 4B parameter footprint, it runs efficiently on edge devices, ensuring privacy and security for sensitive deployments.&lt;/p&gt;&lt;p&gt;We’re releasing the model weights under Apache 2.0 on the Hugging Face Hub.&lt;/p&gt;&lt;head rend="h2"&gt;Voxtral Mini Transcribe V2.&lt;/head&gt;&lt;p&gt;Average diarization error rate (lower is better) across five English benchmarks (Switchboard, CallHome, AMI-IHM, AMI-SDM, SBCSAE) and the TalkBank multilingual benchmark (German, Spanish, English, Chinese, Japanese).&lt;/p&gt;&lt;p&gt;Average word error rate (lower is better) across the top-10 languages in the FLEURS transcription benchmark.&lt;/p&gt;&lt;p&gt;Voxtral Mini Transcribe V2 delivers significant improvements in transcription and diarization quality across languages and domains. At approximately 4% word error rate on FLEURS and $0.003/min, Voxtral offers the best price-performance of any transcription API. It outperforms GPT-4o mini Transcribe, Gemini 2.5 Flash, Assembly Universal, and Deepgram Nova on accuracy, and processes audio approximately 3x faster than ElevenLabs’ Scribe v2 while matching on quality at one-fifth the cost.&lt;/p&gt;&lt;head rend="h3"&gt;Enterprise-ready features.&lt;/head&gt;&lt;p&gt;Voxtral Mini Transcribe V2 introduces key capabilities for enterprise deployments.&lt;/p&gt;&lt;head rend="h4"&gt;Speaker diarization.&lt;/head&gt;&lt;p&gt;Generate transcriptions with speaker labels and precise start/end times. Ideal for meeting transcription, interview analysis, and multi-party call processing. Note: with overlapping speech, the model typically transcribes one speaker.&lt;/p&gt;&lt;head rend="h4"&gt;Context biasing.&lt;/head&gt;&lt;p&gt;Provide up to 100 words or phrases to guide the model toward correct spellings of names, technical terms, or domain-specific vocabulary. Particularly useful for proper nouns or industry terminology that standard models often miss. Context biasing is optimized for English; support for other languages is experimental.&lt;/p&gt;&lt;head rend="h4"&gt;Word-level timestamps.&lt;/head&gt;&lt;p&gt;Generate precise start and end timestamps for each word, enabling applications like subtitle generation, audio search, and content alignment.&lt;/p&gt;&lt;head rend="h4"&gt;Expanded language support.&lt;/head&gt;&lt;p&gt;Like Realtime, this model now supports 13 languages: English, Chinese, Hindi, Spanish, Arabic, French, Portuguese, Russian, German, Japanese, Korean, Italian, and Dutch. Non-English performance significantly outpaces competitors.&lt;/p&gt;&lt;head rend="h4"&gt;Noise robustness.&lt;/head&gt;&lt;p&gt;Maintains transcription accuracy in challenging acoustic environments, such as factory floors, busy call centers, and field recordings.&lt;/p&gt;&lt;head rend="h4"&gt;Longer audio support.&lt;/head&gt;&lt;p&gt;Process recordings up to 3 hours in a single request.&lt;/p&gt;&lt;p&gt;Word error rate (lower is better) across languages in the FLEURS transcription benchmark.&lt;/p&gt;&lt;head rend="h2"&gt;Audio playground.&lt;/head&gt;&lt;p&gt;Test Voxtral Transcribe 2 directly in Mistral Studio. Upload up to 10 audio files, toggle diarization, choose timestamp granularity, and add context bias terms for domain-specific vocabulary. Supports .mp3, .wav, .m4a, .flac, .ogg up to 1GB each.&lt;/p&gt;&lt;head rend="h2"&gt;Transforming voice applications.&lt;/head&gt;&lt;p&gt;Voxtral powers voice workflows in diverse applications and industries.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;head rend="h3"&gt;Meeting intelligence.&lt;/head&gt;&lt;p&gt;Transcribe multilingual recordings with speaker diarization that clearly attributes who said what and when. At Voxtral's price point, annotate large volumes of meeting content at industry-leading cost efficiency.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h3"&gt;Voice agents and virtual assistants.&lt;/head&gt;&lt;p&gt;Build conversational AI with sub-200ms transcription latency. Connect Voxtral Realtime to your LLM and TTS pipeline for responsive voice interfaces that feel natural.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h3"&gt;Contact center automation.&lt;/head&gt;&lt;p&gt;Transcribe calls in real time, enabling AI systems to analyze sentiment, suggest responses, and populate CRM fields while conversations are still happening. Speaker diarization ensures clear attribution between agents and customers.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h3"&gt;Media and broadcast.&lt;/head&gt;&lt;p&gt;Generate live multilingual subtitles with minimal latency. Context biasing handles proper nouns and technical terminology that trip up generic transcription services.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h3"&gt;Compliance and documentation.&lt;/head&gt;&lt;p&gt;Monitor and transcribe interactions for regulatory compliance, with diarization providing clear speaker attribution and timestamps enabling precise audit trails.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Both models support GDPR and HIPAA-compliant deployments through secure on-premise or private cloud setups.&lt;/p&gt;&lt;head rend="h2"&gt;Get started.&lt;/head&gt;&lt;p&gt;Voxtral Mini Transcribe V2 is available now via API at $0.003 per minute. Try it now in the new Mistral Studio audio playground or in Le Chat.&lt;/p&gt;&lt;p&gt;Voxtral Realtime is available via API at $0.006 per minute and as open weights on Hugging Face.&lt;/p&gt;&lt;p&gt;Explore documentation on Mistral’s audio and transcription capabilities.&lt;/p&gt;&lt;head rend="h2"&gt;We’re hiring.&lt;/head&gt;&lt;p&gt;If you're excited about building world-class speech AI and putting frontier models into the hands of developers everywhere, we'd love to hear from you. Apply to join our team.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46886735</guid><pubDate>Wed, 04 Feb 2026 15:08:17 +0000</pubDate></item><item><title>Arcan-A12: Weaving a Different Web</title><link>https://www.divergent-desktop.org/blog/2026/01/26/a12web/</link><description>&lt;doc fingerprint="3f4997190ea14f89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Arcan-A12: Weaving a Different Web&lt;/head&gt;
    &lt;p&gt;26 Jan 2026&lt;/p&gt;
    &lt;p&gt;This article is a companion piece to “Arcan Explained: A browser for different Webs” which covered how Arcan works as a browser engine. Some key takeaways from that article are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The focus is only on running networked applications where the outermost one takes on the responsibilities of window management and display control, becoming the ‘desktop’.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Document browsing is a compilation step through separate tools that generates a signed, shareable application package.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is recursive (an application can embed others, including itself) and can compose and interact with allow-listed local software.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Media decoding, media transforms, network communication and system integration are all delegated to per-instance sets of interchangeable privilege-separated programs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is essentially a browser take on a microkernel architecture. The choice in network communication program will control resource retrieval, link resolution, discovery and so on. This determines what kind of a web you end up in.&lt;/p&gt;
    &lt;p&gt;The reason why this is posted here and not on the main Arcan site is to emphasise this decoupling. It is but one possible solution, and there will be better ideas out there, without someone having to boil the ocean in order to try them out.&lt;/p&gt;
    &lt;p&gt;This article covers the design and choices of the included default implementation (&lt;code&gt;afsrv_net&lt;/code&gt;), its command-line helper tool
(&lt;code&gt;arcan-net&lt;/code&gt;) and how they leverage the A12 protocol to form
a web.&lt;/p&gt;
    &lt;p&gt;It is organised as follows:&lt;/p&gt;
    &lt;p&gt;In Recalling the old ways we take a trip down memory lane back to the days of bulletin board systems to look for good bits to bring back into style.&lt;/p&gt;
    &lt;p&gt;In A frayed web of separate worlds we argue that the ‘World Wide Web’ is anything but ‘World Wide’ and cover a set of problems that we want our solution to cover.&lt;/p&gt;
    &lt;p&gt;In A12 as protocol we go through the characteristics of the protocol that we are building the rest on top of.&lt;/p&gt;
    &lt;p&gt;In A12 Web we layer in designs to address the problems of a frayed web using the A12 protocol, along with some properties we want it to have.&lt;/p&gt;
    &lt;p&gt;Finally, in Developer Story we get practical and go through the use of the tools available to actually build something.&lt;/p&gt;
    &lt;head rend="h1"&gt;Recalling the old ways&lt;/head&gt;
    &lt;p&gt;My early access to the Internet started through Bulletin Board Systems, specifically ones that translated select Usenet discussion groups and relayed e-mail over a system called ‘Echomail’.&lt;/p&gt;
    &lt;p&gt;If those words are unfamiliarly to you, a BBS was mainly someone sharing a slice of their computer to others over a telephone network. This was served to one or a few people at a time because each active connection required a designated telephone line and those were expensive.&lt;/p&gt;
    &lt;p&gt;See also: The BBS documentary.&lt;/p&gt;
    &lt;p&gt;Most of the boards I frequented had a very personal feel to them. It was more like being allowed inside someone’s computing living room than being presented with a streamlined and well-dressed corporate facade.&lt;/p&gt;
    &lt;p&gt;It was at once both intimate and intimidating on the rare few occasions when the SysOp (“System Operator”) decided to “break into chat” and your browsing of the local wares was interrupted by a seemingly inescapable one-to-one chat window.&lt;/p&gt;
    &lt;p&gt;Finding out about these places was an interesting journey in itself. Initial discovery was by word of mouth through a friend of a friend. As things sobered up, magazines took to providing listings. Here the experiences were on the milder side. There was less profanity and rarely mischief manuals (such as “the Anarchist’s Cookbook”), erotica or pirated software. Secondary discovery came through sitelists which the sysops curated themselves, or that someone had dumped into a shared upload directory or snuck into the ‘release information’ files packaged inside some piece of downloadable software.&lt;/p&gt;
    &lt;p&gt;Often enough a phone number did not actually lead to the board in question. Instead of the familiar tone of the modem connection handshake you got the voice of someone who did not appreciate being woken up at two in the morning – perhaps you had misread the operating hours/days (not everything was 24/7) or the sysop had grown tired of it all and closed the thing down. Such is life.&lt;/p&gt;
    &lt;p&gt;From this shallow description alone we can see the outline of some sort of troubled web story: Through word of mouth links between a named entry (Larry’s Land of Leisure and Suites thereof) and an address (here, phone-number) you could access resources, including links to other resources.&lt;/p&gt;
    &lt;p&gt;As far as links go, they were not particularly good:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Unidirectional - unless told, the one being linked to was unaware of who referred to them and thus had no say as to whether the extra attention was appreciated or not.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-descriptive - you had to try and resolve the link to figure out what it actually linked to, or if it was even valid in the first place.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Constraints as a side channel - “open between 08.00pm to 06.00am Fri-Sun” was something that might be mentioned on a login screen (less than helpful) or on the sitelist. Accuracy varied wildly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Local resources were not addressable - you could point someone in the general direction of something, but once they were connected to the board itself the true hunt began.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Local termination - resolving a link does not let you find further ones without processing the resources at the linked location.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ephemeral - should the address mapping change, there is no mechanism to rediscover the linked resource through other paths in the network.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Much of this and more still apply to links on the world “wide” web. To their credit URLs did fix addressing local resources (for a time) and DNS did something about the ephemerality, only to be undone by shortening services.&lt;/p&gt;
    &lt;p&gt;Still, think how much less useful even the first versions of HTML would have been without local resources being part of the language of the ‘link’. The technical solution is about as trivial as can be, yet the consequences are massive.&lt;/p&gt;
    &lt;p&gt;As a snack for thought, take the ‘local termination’ part: this is rather unusual if you think about the underlying data structure; it is a graph, but it is not exposed as such. A link may lead to other links, but first you need to ‘scrape’ (download, parse, extract) them from the outer resource.&lt;/p&gt;
    &lt;p&gt;This left a big discoverability hole to fill, one big enough to spawn some of the wealthiest companies in the world. It may be tempting to think how things would turn out if the other weaknesses had been addressed as well.&lt;/p&gt;
    &lt;p&gt;The obvious elephant in the room is Ted Nelson’s poetic Xanadu and how it keeps being brought up as the ‘what could have been’ solution because it used backlinks to avoid unidirectionality and transclusions for subcontent referencing.&lt;/p&gt;
    &lt;p&gt;Alas, the rest of its ‘web as living document’ story had a very niche appeal, and the browser part was vapourware for the longest time and then very quirky and unintuitive. As important as the linking story is for the base layer of a web, there also needs to be something there: content is king.&lt;/p&gt;
    &lt;p&gt;Tech doesn’t just go away, but rather move into one of many retirement homes for retrocomputing romantics. I don’t recall a strong inflection point where the BBS stopped being my primary source for information and exploration, it “just happened” as an uncoordinated silent shift. The Internet just naturally became the new default, but the transition and loss of something could be felt.&lt;/p&gt;
    &lt;p&gt;The real kicker was the shift in discovery, with search engines as the natural new starting point, some being Web while also somehow not being ‘Web’, like NTNU.no’s FTP search. They quickly became the way to discover content. More seasoned explorers, like the late and great +Fravia pivoted from using disassemblers and debuggers to reverse the dynamic structure of computing, to unpacking the search engine ‘command-line’ to reverse the structure of the web.&lt;/p&gt;
    &lt;p&gt;Other discovery solutions were curated collections of links as ‘portals’, like the original Yahoo. Those were similar to magazine sitelists, which also switched to providing links as URLs rather than phone numbers.&lt;/p&gt;
    &lt;p&gt;It might seem comical and distant now but there was real value in buying a computer magazine for suggestions on where to go on the web. Several Internet Service Providers at the time did go through the extra effort of bundling subscriptions to such magazines. Still even echoes of BBS form discovery remained for a while in the shape of ‘Web rings’ and IRC chatbots serving sitelists.&lt;/p&gt;
    &lt;p&gt;To shorten the story somewhat I will skip past the evolution of forums into ‘community’ sites, how they were replaced by ‘social media’ and so on. Instead I will simply suggest that the ‘web’ oscillates between different modalities, like (open, distributed, public) to (closed, centralised, invite-only). The nudge needed to switch the trend from the one to the other, is spam.&lt;/p&gt;
    &lt;p&gt;The ‘open web’ has, to me, become the least useful resource on the Internet, and that rapidly accelerated to large hadron levels with the sheer amount of bullshit synthesis that has weaseled its way in between me and whatever I was searching for or whoever I was communicating with.&lt;/p&gt;
    &lt;p&gt;I am anything but alone in feeling this disconnect. For a well worded view on the matter, look no further than Splitting the Web. That is, if your browser still allows you to, and the link is still working.&lt;/p&gt;
    &lt;head rend="h1"&gt;A frayed web of separate worlds&lt;/head&gt;
    &lt;p&gt;One thing that the many web pundits I have spoken to over the years have in common when asked to narrow down what the web ‘is’, is a certain glee over unity. Grab a (reasonable) device! And a browser! Browse the web! From anywhere!&lt;/p&gt;
    &lt;p&gt;This doesn’t answer the question and does not match any reality of mine. I have reasonable devices in every form factor, and an even larger pile of completely unreasonable ones. I use several browsers, but it is an exceptional day if two browsers on two different devices behave even close to the same.&lt;/p&gt;
    &lt;p&gt;Heck, if two browsers run on the same device but gets different geolocated source IPs they are served different content on the regular, and more so when using choice examples, say between China to Japan or whether the almighty Cloudflare deems you worthy or not. That is a rather narrow idea of ‘world-wide’.&lt;/p&gt;
    &lt;p&gt;For every other click I am supposed to prove my humanity by clicking a box or waiting a few seconds while my computer crunch pointless numbers. Possibly both.&lt;/p&gt;
    &lt;p&gt;So where is it? Hardly in the protocol. Otherwise we could have dropped that part of the URL long ago. Even before shifts towards the likes of QUIC or TLS (or is it SSL?) becoming ubiquitous, browsers implemented a wide range of them, from Gopher and FTP to RTSP.&lt;/p&gt;
    &lt;p&gt;You also won’t find it in the myriad of document containers or the resources referenced inside. What once was ‘Flash’ or ‘Silverlight’ is now an exercise in computing necromancy to relive. Is that javascript of yours following ECMAScript.1997 or 2025?. One day you might get to see that PNG as JXL, if the Gods so decide.&lt;/p&gt;
    &lt;p&gt;If you are not big on appealing to the authority of the W3C, what you are left with now are the links and how people use them. As we implied previously, the properties of ‘links’ and how they are discovered and enable discovery largely controls what kind of web that emerges.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tangent striking dischord&lt;/head&gt;
    &lt;p&gt;Discord is an interesting phenomenon that deserves to be included here for a handful of reasons. I will fight the urge to attempt a larger breakdown, but it does serve as a connection to the BBS story from before, albeit in sheep’s clothing.&lt;/p&gt;
    &lt;p&gt;To the owners it is probably the happiest of little accidents, just like the Covid pandemic was to Slack. The numbers displayed on Wikipedia and friends are probably imaginary, but suggestions around 3 million accounts in 2016 to well over 500 million in 2025, and a little less than that actually active, seem reasonable. That is quite something for what was basically paratext and coordination around gaming – maybe the ‘Linux desktop’ could learn something from this.&lt;/p&gt;
    &lt;p&gt;The connection to the BBS story is that it gives you a turnkey solution for spinning up your own ‘personal’ ‘server’. In reality it is anything but, merely a namespace where you only get to partially define a small set of the rules, but at least you get to pick an icon or something.&lt;/p&gt;
    &lt;p&gt;The actual agency is more like the old surveillance camera meme:&lt;/p&gt;
    &lt;p&gt;Technically it is completely uninspired. The browser story is Electron as a bodge patchset on top of some dull variant of Chromium. Somehow it is always my lucky day when launching as ‘pretend consent’ language to force you to update whatever to whatever just to then automatically download some more updates.&lt;/p&gt;
    &lt;p&gt;The linking story is somehow worse than plain web. First you have one form for local object link embedding. That one is unlikely to be externally content addressable URLs, thus not shareable.&lt;/p&gt;
    &lt;p&gt;Then you have pseudo-resolution of regular URLs (into ‘previews’) that then gets forwarded to another browser, even though that is most likely the same code you were already running.&lt;/p&gt;
    &lt;p&gt;Its search story is spartan but also telling; a basic command-line with some magical prefixes with search space limited to that of the current ‘server’. I suspect it is very deliberately so before IPO, and that a possible sell later is both training data when all other wells have gone dry, and a chatbot interface to deliver masqueraded ads and bias to some; dark secrets to others, all based on what- and who- you are willing to pay.&lt;/p&gt;
    &lt;p&gt;That something like the TOS violation Searchcord, managed to spark anger and controversy over basic search of partial indexing across public servers (that opted in!) is also telling of the average user expectation.&lt;/p&gt;
    &lt;p&gt;Content wise it is a hotspot for the usual uninteresting filth like grooming; violence; expressions of carnal desires; bullying and brigading. That is something of a variable to monitor as part of a larger health and sanity check. If it is completely absent I get suspicious. If it is overflowing, I walk away.&lt;/p&gt;
    &lt;p&gt;Still, even though the presentation has all the personality of a wet fart captured in a bag and painted grey, it is my goto for some slim chance of an actual interaction with an actual person over a niche interest. That is not gaming paratext, but areas as diverse as CNC Machinery, Pinball Repair, Laser engraving, Electronics and Reverse Engineering.&lt;/p&gt;
    &lt;p&gt;My point is that even though the building blocks and overall purpose is wrong - a strong and playful human connection is still possible and can spring up in the most unlikely of places. We’ll need that going forward.&lt;/p&gt;
    &lt;head rend="h1"&gt;A12 as protocol&lt;/head&gt;
    &lt;p&gt;Time to get technical and cover the last building block before also getting practical. The base A12 protocol was introduced here: A12: Visions of the fully networked desktop.&lt;/p&gt;
    &lt;p&gt;It provides means for sharing an interactive media source (like an application window or composited desktop) to a sink in either a push based configuration, like how X11 remoting worked, or a pull one like VNC, RDP, or SSH would.&lt;/p&gt;
    &lt;p&gt;That is done over one of many transfer channels, each being unidirectional and coupling a possible video, audio and binary ‘blob’ stream corresponding to source windows.&lt;/p&gt;
    &lt;p&gt;The philosophy was that of ‘one desktop, many devices’. This means having individual devices be responsible for providing one or many sources over a network, and the desktop finding and composing them together as seamlessly as possible.&lt;/p&gt;
    &lt;p&gt;Among its building blocks is the ability to redirect a source from one sink to another while it is still running. This was demonstrated already back in 2019 by ‘dragging a living window from one machine to another’ as seen in the clip below.&lt;/p&gt;
    &lt;p&gt;There is also an optional extension to the protocol that enables previously paired sources and sinks to find each other again by broadcasting sets of challenge hashed public keys backed by petnames. This turns cryptographic identity into the link itself.&lt;/p&gt;
    &lt;p&gt;This avoids having to rely on DNS, DHCP provided hostnames, mDNS or other naming services for local (re-) discovery.&lt;/p&gt;
    &lt;p&gt;That is not enough for what we need here, and that brings us to the final extension. It introduces a third possible role, the directory. It act as a rendezvous for discovery; traffic relaying; NAT traversal; shared and private file/state store; application hosting and source/sink match-making.&lt;/p&gt;
    &lt;p&gt;The directory server forms a messaging and storage namespace for each hosted application. By default this is broadcast between all sinks running an application. This works for light collaboration and coordination. The reference desktop environment for Arcan, Durden, uses this to synchronise clipboard and share input devices.&lt;/p&gt;
    &lt;p&gt;For something more refined we can slot in a directory server side control application to match. It uses the same structure as a regular Arcan application, but its role is to coordinate and regulate communication and to mediate access to other networked resources.&lt;/p&gt;
    &lt;p&gt;Such resources can be those that are necessary to the dynamic side of the application itself, like hosted media, indexing and search.&lt;/p&gt;
    &lt;p&gt;To achieve that there are some special functions in the scripting API that we will return to in the ‘Developer Story’ section. Two of particular note are &lt;code&gt;link_target&lt;/code&gt; and
&lt;code&gt;reference_target&lt;/code&gt;. Those lets us define different kinds of
links.&lt;/p&gt;
    &lt;p&gt;This leads us to the next section, as we can now form webs.&lt;/p&gt;
    &lt;head rend="h1"&gt;A12 Web&lt;/head&gt;
    &lt;p&gt;We have reached the philosophy of ‘the desktop, reaching out’ similar to the BBS as covered in ‘the old ways’ to counter the problems from ‘a frayed web of separate worlds’ and either sizzle out into obscurity or create new terrifying problems – we all know where roads paved with good intentions might lead.&lt;/p&gt;
    &lt;p&gt;To be more direct and practical we will explain things using the command-line tooling as a starting point. For development purposes, we have hosted an Arcan directory server at arcan.divergent-desktop.org for years.&lt;/p&gt;
    &lt;p&gt;Using something like:&lt;/p&gt;
    &lt;code&gt;arcan-net arcan.divergent-desktop.org explain&lt;/code&gt;
    &lt;p&gt;If there is a cached / petnamed entry already in the local keystore, e.g. ‘dd’:&lt;/p&gt;
    &lt;code&gt;arcan-net dd@ explain&lt;/code&gt;
    &lt;p&gt;Would:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create an outbound a12-connection to arcan.divergent-desktop.org.&lt;/item&gt;
      &lt;item&gt;Generate an authentication keypair and query for trust unless known (TOFU).&lt;/item&gt;
      &lt;item&gt;Issue a LIST command with [notify] enabled.&lt;/item&gt;
      &lt;item&gt;Wait for a reply with a name field that matches ‘explain’.&lt;/item&gt;
      &lt;item&gt;Issue a download request for key-associated state matching package ID from #4.&lt;/item&gt;
      &lt;item&gt;Issue a download request for the package ID from #4.&lt;/item&gt;
      &lt;item&gt;Verify integrity of package from #6.&lt;/item&gt;
      &lt;item&gt;Verify authenticity of package compared to signature in manifest from #7.&lt;/item&gt;
      &lt;item&gt;Unpack into temporary storage.&lt;/item&gt;
      &lt;item&gt;Start runner process with sandboxing and I/O transfer channels.&lt;/item&gt;
      &lt;item&gt;Inject any state from #5 and signal runner to execute.&lt;/item&gt;
      &lt;item&gt;Join directory messaging group matching ID from #4.&lt;/item&gt;
      &lt;item&gt;Map messages and dynamic resource access between runner and directory until termination.&lt;/item&gt;
      &lt;item&gt;Cleanup and transfer state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The ‘until termination’ point has three possible triggers. First one is the user simply shutting the application down. That would create a snapshot of application-persistent key-value pairs and upload into a matching slot from #5.&lt;/p&gt;
    &lt;p&gt;The other is that an updated version of the application appears (that would be signalled due to the notify- flag from the LIST command in #3). The default behaviour then is to initiate a download of the update matching steps (5,7,8,9) and have the runner store-restore state to itself.&lt;/p&gt;
    &lt;p&gt;The last is on a scripting error causing termination. The runner can be instructed to continue regardless; to snapshot, shutdown and retry; to rollback to a previous version. With any of these options the runner may also send a report rather than (possibly broken) state as per #14.&lt;/p&gt;
    &lt;p&gt;At this stage there is already a number of deviations from the hypertext transport way of doing things, and we have only taken a peek at the basics. Outside of the actual package format and Steps 5 and 13, the chain above is generic enough that it could as well have been a model for a mobile app store.&lt;/p&gt;
    &lt;p&gt;First, any code and necessary ‘offline first’ data is present in the initial package transfer. Its size and checksum is known as part of the LIST command so caching code+data is trivial. Package contents are signed, as is client managed state; altering either on the server side is a distinguishable error condition.&lt;/p&gt;
    &lt;p&gt;While hotly debated internally, the engine blocks any script loads outside of the signed package along with a small curated set of builtin ones. There is no facility ‘hide code in strings and unpack into eval()’ form of getting unsigned code to run and therefore no ‘middleboxes injecting code’ adtech style tampering.&lt;/p&gt;
    &lt;p&gt;Second, any state store is deferred to the user and their decision to leverage (or not) an authentication key-bound server side store. There is no need for neither &lt;code&gt;cookies&lt;/code&gt; nor a ‘login’ form -
authentication primitives for the connection carry over to the
application layer and, if a server side processor is attached, salted to
an application-bound identifier as part of the infrastructure.&lt;/p&gt;
    &lt;p&gt;Third, traffic is owned by the hosting directory. This is a big shift and ties heavily into the linking story. The engine configuration for the case we discuss here gets its primary traffic through either &lt;code&gt;arcan-net&lt;/code&gt; or &lt;code&gt;afsrv_net&lt;/code&gt; when the outer
application also runs the desktop itself. These, in turn,
exclusively communicate with the specified directory.&lt;/p&gt;
    &lt;p&gt;This calls for a short example to get any further. Say that you have an A12 web app that is about image sharing and communication around shared images with friends.&lt;/p&gt;
    &lt;p&gt;The general UI, layouting and chat overlay is handled by the static signed appl package. Dynamic chat updates come through message passing events. The actual images do not make sense to share in the bundle expected to load/run at startup, so they are retrieved on demand from the server. In the old HTML world, that would be something like:&lt;/p&gt;
    &lt;code&gt;&amp;lt;img src="https://some.site/path/image_name.png?bunch_of_state_dont_leak"&amp;gt;&lt;/code&gt;
    &lt;p&gt;In an arcan appl, that would be:&lt;/p&gt;
    &lt;code&gt;local stdin = net_open("@stdin", net_callback_handler)
local image_file = open_nonblock(stdin, {}, "image_name")
load_image_asynch(image_file, image_callback_handler)&lt;/code&gt;
    &lt;p&gt;The asynchronous event handlers and transfer queueing hints have been omitted for brevity. Here, &lt;code&gt;net_open&lt;/code&gt; gives a reference
handle to the current network connection. Then it is used to initiate a
non-blocking serialised read of &lt;code&gt;image_name&lt;/code&gt; that gets
forward to image decoding.&lt;/p&gt;
    &lt;p&gt;All resource requests follow this pattern. The directory server is able to route / cache / load whatever is necessary to fulfill a request, and the reference implementation has several options for this, but the point is that the directory owns the traffic.&lt;/p&gt;
    &lt;p&gt;For the client end this means that network filtering and monitoring can be very aggressive and request record/replay is trivial for both archival and development purposes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linking&lt;/head&gt;
    &lt;p&gt;We finally have enough context to discuss links. The linking model here has two forms: “unified” and “referential”. The referential link is user facing, so we start there.&lt;/p&gt;
    &lt;p&gt;When a connection is made over A12, the initial handshake covers the expected local and remote roles: ‘Source’, ‘Sink’ or ‘Directory’. When hosting a directory server, you can specify outbound referential links through the &lt;code&gt;reference_directory('myfriend')&lt;/code&gt; function call
in the config scripting API.&lt;/p&gt;
    &lt;p&gt;This will create a worker that makes an outbound connection, and when this worker is alive and authenticated, it will be among the results sent in response to the LIST command.&lt;/p&gt;
    &lt;p&gt;This allows local clients to open ‘myfriend’, either tunneled or redirected. The local and remote directory workers transfer public keys and other extended authentication primitives for ‘myfriend’ to transitively trust a new connection to some specified degree.&lt;/p&gt;
    &lt;p&gt;We can see a few properties for this kind of link:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Bidirectional, authenticated and revocable - A link can only be established if the linked entity agrees to it, and it lives only as long as both parties maintain a connection.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Typed - A path through directories always terminate at a directory, a hosted source, sink, or application and you know what you get in advance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Presence is reachability - The linking entity updates its local directory to reflect connection changes, and protocol propagates this to active clients.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rediscoverable - When walking the path of a link the clients learn about public keys of individual directories. The discovery protocol extension lets a linked entity be rediscovered even if the link itself has been revoked.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is clearly not without trade-offs. Transitive trust models and managing petnames to authentication primitives to stop Zooko’s Triangle can get complicated fast, even for networks of only your own devices.&lt;/p&gt;
    &lt;p&gt;On the other hand, DNS is not necessary. It is completely possible to navigate only through a path of petnames. The idea is that in a well-distributed web, we run into six degrees of separation.&lt;/p&gt;
    &lt;p&gt;We don’t link to resources within a hosted application. That is deliberate.&lt;/p&gt;
    &lt;p&gt;Links being contractual sets a low cap on the amount of links that are feasible to maintain. That’s a feature, not a bug. The cost to resolve grows linearly with the number of directories in a path.&lt;/p&gt;
    &lt;p&gt;A unified link is also specified in the config scripting API by calling the &lt;code&gt;link_directory('myfriend')&lt;/code&gt;. The main difference
against referential links is that the connection is not visible in the
client presented list.&lt;/p&gt;
    &lt;p&gt;This is because the linking parties form a unified namespace of exposed applications and their respective worker processes synch host applications, files and instance server side script runners as needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Search and Indexing&lt;/head&gt;
    &lt;p&gt;The last thing on the Web menu here is search in the sense of ‘From what we know and can access, what best matches your query?’ and not the ‘that we or our sponsors think that you should see’ variation.&lt;/p&gt;
    &lt;p&gt;The cheap solution is of course to leave it in the hands of developers and see what emerges. While that can come as a side effect of growth in popularity, part of the ‘many devices, one desktop’ narrative means that lower level mechanisms would be useful.&lt;/p&gt;
    &lt;p&gt;There are a few parts of the protocol to leverage here.&lt;/p&gt;
    &lt;p&gt;One is that binary stream uploads/downloads have a few typed alternate slots. One such slot is METADATA. This means that with upload permission someone can pre-index/analyse locally, and attach that to the server side store going forward. Similarly, a controller providing, for example, an image hosting service can see that something does not have any metadata attached to it, fire up some analysis tool and attach the results itself.&lt;/p&gt;
    &lt;p&gt;Another part is that the name part of requests that trigger binary stream transfers reserves the ‘.’ prefix for protocol support use. The server implementation uses .monitor, for instance, to negotiate an interactive debug interface stream and .debug to collect crash dumps that have accumulated.&lt;/p&gt;
    &lt;p&gt;Another that gets special treatment is ‘.index’. Normally it can be downloaded as a means to list files in the private store attached to the key you authenticated to. It is also used to list available resources in the server-side store assigned to each controller by also specifying the namespace identifier that matches the application identifier that the controller is assigned to, and that would propagate across a network of unified links.&lt;/p&gt;
    &lt;p&gt;If you upload a file to ‘.index’ you actually slot in a filter that corresponds to your search query for that namespace. This will influence future ‘.index’ downloads.&lt;/p&gt;
    &lt;p&gt;The server also has an external resolver mechanism. To understand this, both file and index resolving first goes to the controller. This has the option to reject it, or to forward or remap into its local file-store. The later can be substituted for an external resolver that takes care of translating to other protocols, e.g. AT, IPFS, Torrent with caching. This tactic is also used for implementing unified linking.&lt;/p&gt;
    &lt;p&gt;There is a lot more to unpack here when it comes to protection against abuse and collaboratively reaching an accurate .index that all participants can sign off on -- but that is for a different day.&lt;/p&gt;
    &lt;head rend="h1"&gt;Developer Story&lt;/head&gt;
    &lt;p&gt;With enough protocol nuances in place we can dig into some of the practicalities in developing a networked Arcan application, but just enough to follow the web story here rather rather than the concrete APIs as such. For those there are numerous guides, step-by-step instructions and examples already in both Wiki and repository documentation format.&lt;/p&gt;
    &lt;p&gt;The only thing we need to recall is that an Arcan appl is a set of scripts with some minor file- and function name- patterns.&lt;/p&gt;
    &lt;p&gt;A minimal form is just this:&lt;/p&gt;
    &lt;code&gt;mkdir $ARCAN_APPLBASEPATH/myappl;
echo "function myappl()\nend\n" &amp;gt; $ARCAN_APPLBASEPATH/myappl/myappl.lua&lt;/code&gt;
    &lt;p&gt;Assuming that you have permission to install or update an appl (which are different permissions) to a directory server that will host it, all you have to do is:&lt;/p&gt;
    &lt;code&gt;arcan-net --sign-tag mykey --push-appl myappl somedir@&lt;/code&gt;
    &lt;p&gt;This will package, sign and transfer ‘myappl’ to the directory pointed to by ‘somedir’ in the current keystore. Creating an entry and generating keys can be done with:&lt;/p&gt;
    &lt;code&gt;arcan-net keystore somedir host.or.ip&lt;/code&gt;
    &lt;p&gt;It will also tell you the public key that the server needs to grant permissions to. Setting one up has a lot more nuance to it, but with the arcan-net installation you would have a config.lua.example to work from and then run:&lt;/p&gt;
    &lt;code&gt;arcan-net -c /path/to/my/config.lua&lt;/code&gt;
    &lt;p&gt;You can then test-run:&lt;/p&gt;
    &lt;code&gt;arcan-net somedir@ myappl&lt;/code&gt;
    &lt;p&gt;Which should just become a black window. Keep it running, but at the same time let’s push a change that would break it:&lt;/p&gt;
    &lt;code&gt;echo "function myappl()
    bad_function_call()
 end
" &amp;gt; $ARCAN_APPLBASEPATH/myappl/myappl.lua

arcan-net --sign-tag mykey --push-appl myappl somedir@&lt;/code&gt;
    &lt;p&gt;The already running client downloads/update and it breaks. If configured to permit it will create a crash report, upload it and rollback to the last known working one.&lt;/p&gt;
    &lt;code&gt;arcan-net --get-file myappl .report - somedir@&lt;/code&gt;
    &lt;p&gt;That will collect user crash reports, bundle them together and send them back to us (and pipe to standard output). The reports are valid Lua scripts, so we can have analysis tooling that itself generates an appl.&lt;/p&gt;
    &lt;p&gt;The same tactic is used for slotting in a controller, just with --push-ctrl instead of --push-appl.&lt;/p&gt;
    &lt;p&gt;To look at a more advanced example, we will take streaming media playback. The full appl code is as follows:&lt;/p&gt;
    &lt;code&gt;
function myapp()
net_open("@stdin",
  function(source, status)
    if status.kind == "connected" then
      play_media(source)
    end
  end
  )
end

function play_media(ref)
  local fio = open_nonblock(ref, {}, "appl:/test")
  launch_decode(nil, "protocol=media",
    function(src, status)
      if status.kind == "bchunkstate" then
        open_nonblock(src, {}, fio)
      elseif status.kind == "resized" then
        show_image(src)
        resize_image(src, status.width, status.height)
      end
    end
  )
end
&lt;/code&gt;
    &lt;p&gt;The first thing to note is the &lt;code&gt;net_open&lt;/code&gt; call. This explicitly says
that we want to access and communicate with the directory that it was downloaded
from. Should that call fail, we can assume to be running offline.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;play_media&lt;/code&gt; function pairs two asynchronous processes. The
&lt;code&gt;open_nonblock&lt;/code&gt; part is used for non-blocking asynchronous file
input/output, both ones from within the appl package; from user-defined
namespaces and through existing processes via a reference handle.&lt;/p&gt;
    &lt;p&gt;The first call goes through the handle to the network connection, and the appl:/ prefix tells it to go through the controller side application specific namespace. Any extra controls about transfer queueing/buffering/parallelisation preferences would go into the passed option table.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;launch_decode&lt;/code&gt; part will spawn a media decoding process,
and by not providing it with direct input it will send a set of extensions it
supports (bchunkstate), that we ignore here and just pair with the resource
reference we got from previous &lt;code&gt;open_nonblock&lt;/code&gt; call. On the first
'resize' event we set the associated video object to visible and size to match
the source dimensions.&lt;/p&gt;
    &lt;p&gt;Let's slot in a matching controller:&lt;/p&gt;
    &lt;code&gt;
function myapp()
end

function myapp_load(cl, resource)
  if resource == "test" then
    return "test.mp4"
  end
end

function myapp_store(cl, resource)
-- block all attempts at storing files
end
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;_load&lt;/code&gt; entrypoint will be triggered when the client &lt;code&gt;open_nonblock&lt;/code&gt;
is issued. Here we map it to an actual file in the server-side store. This is where we
can add additional permission checks or selection logic. This is where the previously
mentioned 'external resolver' also fit, if the &lt;code&gt;config.lua&lt;/code&gt; set for the server
would say:&lt;/p&gt;
    &lt;code&gt;
function ready()
  local resolver =
    launch_resolver("/some/executable",
                    function(source, status)
                    end
                   )
  appl_set_resolver("myapp", resolver)
end
&lt;/code&gt;
    &lt;p&gt;The mapped &lt;code&gt;test.mp4&lt;/code&gt; request would be forwarded to the process of
&lt;code&gt;/some/executable&lt;/code&gt; where we can have advanced mapping to other protocols and
storage solutions.&lt;/p&gt;
    &lt;p&gt;Let's do something more advanced. We add an arcan-shmif client to the server-side policy database: &lt;code&gt;arcan_db add_target BIN xserver /usr/bin/Xarcan -exec wmaker&lt;/code&gt;. Then,
in the controller we add the following function:&lt;/p&gt;
    &lt;code&gt;
function myapp_join(cl)
  launch_target(cl, {}, "xserver",
    function(source, status)
    end
  )
end
&lt;/code&gt;
    &lt;p&gt;As soon as a client joins, the server would spawn an instance of an Xserver with the 'WindowMaker' window manager attached. This makes a loopback connection and registers as a hidden source only visible to the specified client. The client gets a notification that the source is available, and that would translate to a &lt;code&gt;segment_request&lt;/code&gt;
event in the &lt;code&gt;net_open&lt;/code&gt; event handler. If it accepts it:&lt;/p&gt;
    &lt;code&gt;
net_open("@stdin",
function(source, status)
  if status.kind == "segment_request" then
    accept_target(640, 480, event_handler)
  end
end
)
&lt;/code&gt;
    &lt;p&gt;We now have the means to composite and interact with the source as if it had been launched locally.&lt;/p&gt;
    &lt;p&gt;All of this has been assuming that the client end has the Arcan stack installed and available. This might not be the case for weaker 'thin' kiosk like devices or in a more limited context, like one of those horrid vendor locked app ecosystems. Should the full stack be present on the directory server however, and the &lt;code&gt;config.permissions.applhost&lt;/code&gt; option be set to a tag (group)
matching your authentication key, a simplified viewer that only implements the
a12 protocol parts, like Smash,
could be used if the client request the appl as a source and not as an appl
package download. This would cause the server to spawn an instance of Arcan
with a loopback connection as a directed source to the specific client.&lt;/p&gt;
    &lt;p&gt;The purpose of bringing all this up here is not as a practical guide, but to provide enough context to highlight a few things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The tooling to browse, host and develop is the same. They are a property of the network solution itself, not ‘browser:developer tools’.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Updates are atomic and signed, making life much more difficult for parasitic intermediaries.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every form of communication is explicit, from link contracts to how the appl communiate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The execution form is local-first, into locally-hosted, into networked.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The scaling model is small nodes, large networks.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The 'frontend', 'backend' and server development model is one and the same.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Both client appl and controller are composable due to the naming scheme.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;In Closing&lt;/head&gt;
    &lt;p&gt;There is a lot of technical details omitted here around decisions and trade-offs and there is yet a small time window for change before all this is locked-in, especially around server side APIs and small-scale payment processing (with GNU taler being a strong candidate) to avoid the 'need' (they will always find a way) for Ad-tech parasites.&lt;/p&gt;
    &lt;p&gt;Other future plans involve preserving and translating current web contents into this, with tooling for layering in collaborative features over the contents. There is still ample room to join in and play around like it is the 90ies again, for a timeline that doesn't look so dark and grim as the current one does.&lt;/p&gt;
    &lt;p&gt;That said, a substantial goal for all of this is personal agency over any kind of mass adoption -- I am stubborn, not naive. That anyone outside of a small group of tech die hards would go all Yippee Ki-Yay over this is a moonshot and that is fine.&lt;/p&gt;
    &lt;p&gt;There are a number of places in my life and home where the current web- and browser- story will be pushed out. Places like my network cameras, heat pump HMI, various maker devices, home theater, gaming gear, mobile devices and so on. If I can only revert that I might at least tolerate that I still have to order groceries in some throwaway browser on a throwaway device. At least for now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46887326</guid><pubDate>Wed, 04 Feb 2026 15:50:50 +0000</pubDate></item><item><title>Microsoft's Copilot chatbot is running into problems</title><link>https://www.wsj.com/tech/ai/microsofts-pivotal-ai-product-is-running-into-big-problems-ce235b28</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46887564</guid><pubDate>Wed, 04 Feb 2026 16:08:55 +0000</pubDate></item><item><title>RS-SDK: Drive RuneScape with Claude Code</title><link>https://github.com/MaxBittker/rs-sdk</link><description>&lt;doc fingerprint="bdf8915a6c22e4d7"&gt;
  &lt;main&gt;
    &lt;p&gt;Research-oriented starter kit for runescape-style bots, including a typescript sdk, agent documentation and bindings, and a server emulator. Works out of the box - tell it what to automate!&lt;/p&gt;
    &lt;p&gt;Build and operate bots within a complex economic role-playing MMO. You can automate the game, level an account to all 99s, and experiment with agentic development techniques within a safe, bot-only setting.&lt;/p&gt;
    &lt;p&gt;The goals of this project are to provide a rich testing environment for goal-directed program synthesis techniques (Ralph loops, etc), and to facilitate research into collaboration and competition between agents.&lt;/p&gt;
    &lt;p&gt;There is currently a leaderboard for bots running on the demo server, with rankings based on highest total level per lowest account playtime.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;RS-SDK is a fork of the LostCity engine/client, an amazing project without which rs-sdk would not be possible. Find their code here or read their history and ethos&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/MaxBittker/rs-sdk.git&lt;/code&gt;
    &lt;p&gt;Out of the box, you can connect to the provided demo server, choose a name that is not already taken!&lt;/p&gt;
    &lt;p&gt;With claude code:&lt;/p&gt;
    &lt;code&gt;bun install
claude "start a new bot with name: {username}"&lt;/code&gt;
    &lt;p&gt;Manually:&lt;/p&gt;
    &lt;code&gt;bun install
bun scripts/create-bot.ts {username}
bun bots/{username}/script.ts &lt;/code&gt;
    &lt;p&gt;Chat is off by default to prevent scamming and prompt injection attacks, but you can opt in with &lt;code&gt;SHOW_CHAT=true&lt;/code&gt; in the bot.env file&lt;/p&gt;
    &lt;p&gt;Warning: The demo server is offered as a convenience, and we do not guarantee uptime or data persistence. Hold your accounts lightly, and consider hosting your own server instance. Please do not manually play on the demo server.&lt;/p&gt;
    &lt;p&gt;This server has a few modifications from the original game to make development and bot testing easier:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Faster leveling - The XP curve is accelerated and less steep.&lt;/item&gt;
      &lt;item&gt;Infinite run energy - Players never run out of energy&lt;/item&gt;
      &lt;item&gt;No random events - Anti-botting random events are disabled&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;rs-sdk runs against an enhanced web-based client (&lt;code&gt;botclient&lt;/code&gt;) which connects to the LostCity 2004scape server emulator.&lt;/p&gt;
    &lt;p&gt;There is a gateway server which accepts connections from botclient and SDK instances, and forwards messages between them based on username. Once connected to the gateway, the botclient will relay game state to the SDK, and execute low-level actions (e.g. &lt;code&gt;walkTo(x,y)&lt;/code&gt;) sent from the SDK through the gateway.&lt;/p&gt;
    &lt;p&gt;This means that the SDK can't talk directly to the game server, but must go through the botclient. It will attempt to launch the botclient on startup if one is not already running.&lt;/p&gt;
    &lt;p&gt;You don't need to run the gateway/botclient in order to run automations against the demo server, but you may choose to if you are fixing bugs or adding features to the rs-sdk project&lt;/p&gt;
    &lt;p&gt;You want all these running:&lt;/p&gt;
    &lt;code&gt;cd engine &amp;amp;&amp;amp; bun run start&lt;/code&gt;
    &lt;code&gt;cd webclient &amp;amp;&amp;amp; bun run watch&lt;/code&gt;
    &lt;code&gt;cd gateway &amp;amp;&amp;amp; bun run gateway&lt;/code&gt;
    &lt;p&gt;There is also a login server which you may not need, I forget&lt;/p&gt;
    &lt;p&gt;This is a free, open-source, community-run project.&lt;/p&gt;
    &lt;p&gt;The goal is strictly education and scientific research.&lt;/p&gt;
    &lt;p&gt;LostCity Server was written from scratch after many hours of research and peer review. Everything you see is completely and transparently open source.&lt;/p&gt;
    &lt;p&gt;We have not been endorsed by, authorized by, or officially communicated with Jagex Ltd. on our efforts here.&lt;/p&gt;
    &lt;p&gt;You cannot play Old School RuneScape here, buy RuneScape gold, or access any of the official game's services! Bots developed here will not work on the official game servers.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the MIT License. See the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46888142</guid><pubDate>Wed, 04 Feb 2026 16:47:26 +0000</pubDate></item><item><title>Converge (YC S23) Is Hiring Product Engineers (NYC, In-Person)</title><link>https://www.runconverge.com/careers/product-engineer</link><description>&lt;doc fingerprint="a508c173dc07a7a8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Product Engineer&lt;/head&gt;
    &lt;p&gt;Location: NYC (in-person)&lt;/p&gt;
    &lt;p&gt;Help us build everything a consumer brand needs to grow. Weâre just 4 engineers with serious traction (well beyond $1M ARR). Youâll be shipping end-to-end product and working directly with customers.&lt;/p&gt;
    &lt;head rend="h3"&gt;About Converge&lt;/head&gt;
    &lt;p&gt;We want to profitably grow the world's consumer brands. That begins with helping them understand which marketing efforts are driving profitable growth.&lt;/p&gt;
    &lt;p&gt;200+ consumer brands, including publicly traded companies, rely on Converge to check in on their marketing performance up to a dozen times a day. They drill down to figure out what's working and decide where to shift million-dollar marketing budgets.&lt;/p&gt;
    &lt;p&gt;To ship even more, we've raised $5.7M from some of the best investors, including Y Combinator, General Catalyst, and the founders of Posthog, Algolia, Shipbob, ...&lt;/p&gt;
    &lt;head rend="h3"&gt;What you'll do&lt;/head&gt;
    &lt;p&gt;Ship product fully end-to-end. All the way from designing the system and data models to building out the interface and polishing the experience.&lt;/p&gt;
    &lt;p&gt;We trust you to build the best solution to a customer's problem. We lead with context and give you full autonomy to design the solution. This means you'll need to build a deep understanding of the problem, obsess over the solution, and ship it.&lt;/p&gt;
    &lt;p&gt;Work directly with customers. You'll talk to them, ship, get feedback, and iterate. There are no middlemen. This means you can move incredibly fast: you message a customer, create a PR, and can have it fixed within hours.&lt;/p&gt;
    &lt;p&gt;Some examples of projects you could own:&lt;/p&gt;
    &lt;p&gt;Effortless Slack conversations: Customers screenshot their Converge reports up to a dozen of times a day (!) to share with the rest of the company. Instead, they should be able to directly tag and message a teammate from any number in Converge and have this synced bidirectionally with Slack.&lt;/p&gt;
    &lt;p&gt;Centralizing all growth efforts: Converge already centralizes all growth metrics, but the information explaining them is still scattered. We want overlay the context that actually explains trends like pricing updates, budget changes, and promo calendars.&lt;/p&gt;
    &lt;p&gt;Creative analytics: All growth teams use a separate tool to iterate on ad creatives, but they would want to run this workflow on Converge. We should build a Creative Analytics product on top of our data.&lt;/p&gt;
    &lt;p&gt;AI agents: We never wanted to jump on the AI train for the sake of hype. But now that we have the foundations in place, there's huge leverage in building agents that can help growth teams understand their data and act on it more quickly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Converge&lt;/head&gt;
    &lt;p&gt;We're a team of just 4 engineers with serious traction (well beyond $1M ARR). Join us if you want to get rid of office politics and just take ownership to get a lot done.&lt;/p&gt;
    &lt;p&gt;Even though you join early, this job comes with real engineering challenges. We process $4B in online orders annually, 20TB of data flows through Converge each month, and we've collected around 10B customer interactions to date.&lt;/p&gt;
    &lt;p&gt;What you're shipping will actually get used. 50% of our customers use us daily (!), while this is only 13% for the average SaaS company. You will have an immediate impact.&lt;/p&gt;
    &lt;p&gt;We love working in-person. You'll like it here if you do too.&lt;/p&gt;
    &lt;p&gt;If you think you could be a founder, there's no better way to learn than to talk to customers and ship. That's what you'll be spending all of your time on. We obsess over the details and will share honest feedback.&lt;/p&gt;
    &lt;head rend="h3"&gt;What we're looking for&lt;/head&gt;
    &lt;p&gt;Strong experience working across the stack (4+ YOE). We work with React, Python, Postgres, and Clickhouse. Experience building data-intensive products or familiarity with Clickhouse is a plus.&lt;/p&gt;
    &lt;p&gt;You've previously built products or large features fully end-to-end.&lt;/p&gt;
    &lt;p&gt;You obsess over the quality of what you're building, both in UX and code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compensation&lt;/head&gt;
    &lt;p&gt;Salary: $175K - $240K + equity (0.6% - 0.85%).&lt;/p&gt;
    &lt;p&gt;Private health, dental, and vision insurance.&lt;/p&gt;
    &lt;p&gt;Pension &amp;amp; 401k contributions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interview process*&lt;/head&gt;
    &lt;p&gt;Intro call (30 min): We want to learn about your motivations to join Converge, determine why youâd be a great fit, and answer any questions you have for us.&lt;/p&gt;
    &lt;p&gt;Technical (1h): We work through a typical engineering problem we face at Converge.&lt;/p&gt;
    &lt;p&gt;Culture (45 min): We dive into your past experiences to learn how you like to work and what motivates you.&lt;/p&gt;
    &lt;p&gt;Superday (1 day): Join us for a day to actually build something! You get to meet the team, we get to meet you, it's great. (fully paid)&lt;/p&gt;
    &lt;p&gt;(*) This can all be done in 2 days. If you want to move quickly, we do too. Our founding engineer was on a plane to meet us just days after our first call.&lt;/p&gt;
    &lt;head rend="h2"&gt;We raised $5.7M from some of the best investors&lt;/head&gt;
    &lt;head rend="h3"&gt;James Hawkins&lt;/head&gt;
    &lt;head rend="h3"&gt;Nicolas Dessaigne&lt;/head&gt;
    &lt;head rend="h2"&gt;Founding team&lt;/head&gt;
    &lt;head rend="h2"&gt;How we started&lt;/head&gt;
    &lt;head rend="h3"&gt;Did you knowâ¦&lt;/head&gt;
    &lt;p&gt;All co-founders have written code that has run in production as part of Converge.&lt;/p&gt;
    &lt;p&gt;We closed our first publicly traded company during our YC batch from our living room in San Francisco.&lt;/p&gt;
    &lt;p&gt;Thomas and Tiago (Founding Engineer) worked together when Thomas was just an intern.&lt;/p&gt;
    &lt;p&gt;Michel (Customer Success) was responsible for most of the incoming Converge Support tickets in his previous job as a freelance tracking consultant.&lt;/p&gt;
    &lt;p&gt;Thomas and Jan were best friends in high school, and Jan and Jerome met in their first year of college.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46888331</guid><pubDate>Wed, 04 Feb 2026 17:01:14 +0000</pubDate></item><item><title>AI is killing B2B SaaS</title><link>https://nmn.gl/blog/ai-killing-b2b-saas</link><description>&lt;doc fingerprint="ede5902a8d97058d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;AI is Killing B2B SaaS&lt;/head&gt;&lt;p&gt;SaaS is the most profitable business model on Earth.1 It’s easy to understand why: build once, sell the same thing again ad infinitum, and don’t suffer any marginal costs on more sales.&lt;/p&gt;&lt;p&gt;I have been writing software for more than half my life. In the last year itself, I’ve talked to hundreds of founders and operators in SF, from preseed to Series E companies.&lt;/p&gt;&lt;p&gt;AI is bringing an existential threat to a lot of B2B SaaS executives: How to keep asking customers for renewal, when every customer feels they can get something better built with vibe-coded AI products?&lt;/p&gt;&lt;p&gt;And the market is pricing it in. Morgan Stanley’s SaaS basket has lagged the Nasdaq by 40 points since December. HubSpot and Klaviyo are down ~30%. Analysts are writing notes titled “No Reasons to Own” software stocks.&lt;/p&gt;&lt;head rend="h2"&gt;The relation between vibe coding and B2B SaaS sales&lt;/head&gt;&lt;p&gt;The new problem for B2B SaaS is that with AI, customers can get something working with vibe coding. There are tens of vibe coding “internal tool” services that promise to connect to every integration in the world to pump out CRUD and workflow apps.&lt;/p&gt;&lt;p&gt;Whatever they build simply works. It takes some wrangling to get there (one Series C VP listed eleven different vibe coding tools they’ve tried and the pros and cons between each on a phone call once), but productivity gains are immediate.&lt;/p&gt;&lt;p&gt;And vibe coding is fun. You feel like a mad wizard using the right incantation 2 to get this magical new silicon intelligence to do exactly what you want.&lt;/p&gt;&lt;p&gt;What they don’t know, though, is that a poorly architected system will fail, eventually. As every senior programmer (eventually) understands, our job is complex because we have to understand the relationships in the real world, the processes involved, and the workflows needed, and representing it in a robust way to create a stable system. AI can’t do that.&lt;/p&gt;&lt;p&gt;Non-programmers don’t know any of this nuance. One Series E CEO told me that they’re re-evaluating the quarterly renewal of their engineering productivity software because they along with an engineer reimplemented something using Github and Notion APIs. They were paying $30,000 to a popular tool3 and they were not going to renew anymore.&lt;/p&gt;&lt;head rend="h2"&gt;How does it impact B2B sales?&lt;/head&gt;&lt;p&gt;If customers feel like they aren’t being served exactly like they want to, they are more likely to churn. The reason behind all this is that customers are demanding more from their B2B vendors, because they know what’s possible.&lt;/p&gt;&lt;p&gt;Previously, you would change your company to fit what your ERP and pay them hundreds of thousands of dollars. Now, everyone can see that agentic coding makes an unprecedented level of flexibility possible. And customers are demanding that flexibility, and if they don’t get it, they’ll leave.&lt;/p&gt;&lt;p&gt;This week itself I was on a phone call with a Series B AE talking about how they’re potentially losing an $X00,000 account just because the customer can’t use a specific failure reporting workflow in the SaaS. They’re now working with me to build what the customer needs and retain them.&lt;/p&gt;&lt;head rend="h2"&gt;How to survive&lt;/head&gt;&lt;head rend="h3"&gt;1. Be a System of Record&lt;/head&gt;&lt;p&gt;If the entire company’s workflows operates on your platform, i.e. you’re a line-of-business SaaS, you are integrated into their existing team already. They know your UI and rely on you on the day to day.&lt;/p&gt;&lt;p&gt;For example, to create a data visualization I won’t seek any SaaS. I’ll just code one myself using many of the popular vibe coding tools (my team actually did that and it’s vastly more flexible than what we’d get off-the-shelf).&lt;/p&gt;&lt;p&gt;Being a “System of Record” means you’re embedded so deeply that there’s no choice but to win. My prediction is that we’ll see more SaaS companies go from the application layer to offering their robust SoR as their primary selling point.&lt;/p&gt;&lt;head rend="h3"&gt;2. Security, authentication, and robustness&lt;/head&gt;&lt;p&gt;This is where vibe-coded apps silently fail — and where established SaaS platforms earn their keep.&lt;/p&gt;&lt;p&gt;When a non-technical team vibe-codes an internal tool, they’re not thinking about environment keys, XSS vulnerabilities or API keys hardcoded in client-side JavaScript. They’re not implementing rate limiting, audit logs, or proper session management. They’re definitely not thinking about SOC 2 compliance, GDPR data residency requirements, or HIPAA audit trails.&lt;/p&gt;&lt;p&gt;I’ve seen it firsthand: a finance team built a “quick” expense approval tool that stored unencrypted reports in a public S3 bucket. A sales ops team created a commission calculator that anyone with the URL could access — no auth required. These aren’t edge cases. They’re the norm when software is built without security as a foundational concern.&lt;/p&gt;&lt;p&gt;Enterprise SaaS platforms have spent years (and millions) solving these problems: role-based access control, encryption at rest and in transit, penetration testing, compliance certifications, incident response procedures. Your customers may not consciously value this — until something breaks.&lt;/p&gt;&lt;p&gt;The challenge is that security is invisible when it works. You need to communicate this value proactively: remind customers that the “simple” tool they could vibe-code themselves would require them to also handle auth, permissions, backups, uptime, and compliance.&lt;/p&gt;&lt;head rend="h3"&gt;3. Adapt to the customer, not the other way around&lt;/head&gt;&lt;p&gt;The times of asking customers to change how they work are gone. Now, SaaS vendors that differentiate by being ultra customizable win the hearts of customers.&lt;/p&gt;&lt;p&gt;How? It’s the most powerful secret to increase usage. We’ve all heard the classic SaaS problem where the software is sold at the beginning of the year, but no one actually ends up using it because of how inflexible it is and the amount of training needed.&lt;/p&gt;&lt;p&gt;And if a SaaS is underutilized, it gets noticed. And that leads to churn.&lt;/p&gt;&lt;p&gt;This is the case with one of my customers, they have a complex SaaS for maintenance operations. But turns out, this was not being used at the technician level because they found the UI too complex4.&lt;/p&gt;&lt;p&gt;How I’m solving this is essentially a whitelabelled vibe-coding platform with in-built distribution and secure deployments. When they heard of my solution they were immediately onboard. Their customer success teams quickly coded a very specific mobile webapp for the technicians to use and deployed it in a few days.&lt;/p&gt;&lt;p&gt;Now, the IC technician is exposed to just those parts of the SaaS that they care about i.e. creating maintenance work orders. The executives get what they want too, vibe coding custom reports exactly the way they want vs going through complicated BI config. They are able to build exactly what they want and feel like digital gods while doing it.&lt;/p&gt;&lt;p&gt;Usage for that account was under 35%, and is now over 70%. They are now working closely with me to vibe code new “micro-apps” that work according to all of their customer workflows. And the best part? This is all on top of their existing SaaS which works as a system of record and handles security, authentication, and supports lock-in by being a data and a UI moat.&lt;/p&gt;&lt;p&gt;This is exactly what I’m building: a way for SaaS companies to let their end-users vibe code on top of their platform (More on that below). My customers tell me it’s the best thing they’ve done for retention, engagement, and expansion in 2026 – because when your users are building on your platform, they’re not evaluating your competitors.&lt;/p&gt;&lt;head rend="h2"&gt;The Real Shift&lt;/head&gt;&lt;p&gt;Here’s what I’ve realized after hundreds of conversations with founders and operators: AI isn’t killing B2B SaaS. It’s killing B2B SaaS that refuses to evolve.&lt;/p&gt;&lt;p&gt;The SaaS model was built on a simple premise: we build it once, you pay forever. That worked when building software was hard. But now your customers have tasted what’s possible. They’ve seen their finance team whip up a custom dashboard in an afternoon. They’ve watched a non-technical PM build an internal tool that actually fits their workflow.&lt;/p&gt;&lt;p&gt;You can’t unsee that. You can’t go back to paying $X0,000/year for software that almost does what you need.&lt;/p&gt;&lt;p&gt;The survivors won’t be the SaaS companies with the best features. They’ll be the ones who become platforms – who let customers build on top of them instead of instead of them. When I showed a well-known VC what I was building to help SaaS companies do exactly this, he said: “This is the future of marketplaces and software companies.”&lt;/p&gt;&lt;p&gt;Maybe. Or maybe this is just another cycle and traditional SaaS will adapt like it always has. But I know this: the companies I’m talking to aren’t waiting around to find out. They’re already rebuilding their relationship with customers from “use our product” to “build on our platform.”&lt;/p&gt;&lt;p&gt;The question isn’t whether AI will eat your SaaS.&lt;/p&gt;&lt;p&gt;It’s whether you’ll be the one holding the fork.&lt;/p&gt;&lt;p&gt;I’m solving exactly this problem with a whitelabelled AI platform for B2B SaaS companies, so your users can vibe code customized workflows on top of their existing system of record.&lt;/p&gt;&lt;p&gt;My customers tell me this is the best way to support retention, engagement, and expansion in 2026. If this sounds interesting to you or someone you know, I can reach out with a custom demo or you can learn more about Giga Catalyst.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Whenever I bring a new friend to the Salesforce Park, they are in absolute awe. And, the meme remains true that no one even knows what Salesforce does. Whatever they’re doing, they’re clearly earning enough revenue to purchase multiple blocks in SF. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;a.k.a. “prompt engineering” which is not engineering at all but that’s a different blog post. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I won’t name any names, but the company’s named after an invertebrate animal. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;And who can blame them – I still feel a pang of anxiety when I look at my sales CRM. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Increase engagement and retention&lt;/head&gt;&lt;p&gt;Our whitelabel AI vibe coding platform allows your users to customize and build exactly what they need, on top of your platform.&lt;/p&gt;&lt;p&gt;My customers say that this is the best way to increase engagement and retention in 2026.&lt;/p&gt;Curious? Check out Giga Catalyst to learn more&lt;p&gt;Or, fill out this form and I'll personally reach out to show you how it works:&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46888441</guid><pubDate>Wed, 04 Feb 2026 17:09:28 +0000</pubDate></item><item><title>Building a 24-bit arcade CRT display adapter from scratch</title><link>https://www.scd31.com/posts/building-an-arcade-display-adapter</link><description>&lt;doc fingerprint="671f195658a51bd9"&gt;
  &lt;main&gt;&lt;p&gt;In November, my friend and fellow Recurser, Frank, picked up an arcade machine for the Recurse Center. We call it the RCade. He wanted to leave the original CRT in - which I think is a great choice! - and drove it off of a Raspberry Pi. Eventually we wanted to move to a more powerful computer, but we needed a way to connect it to the display. Off-hand, I mentioned that I could build a CRT display adapter that interfaces with a normal computer over USB. This is that project.&lt;/p&gt;&lt;head rend="h2"&gt;What the display expects&lt;/head&gt;&lt;p&gt;The CRT in the RCade has a JAMMA connector, and Frank bought a converter that goes between VGA and JAMMA.&lt;/p&gt;&lt;p&gt;You might think we could just use an off-the-shelf VGA adapter to drive it at this point, but it's not that simple. The CRT runs at a weird resolution; We started with 320x240 but eventually wanted to target 336x262, which is super non-standard. Even 320x240 is unattainable by most display adapters, which typically can't go below 640x480. A custom solution would allow us to output any arbitrary resolution we wanted.&lt;/p&gt;&lt;p&gt;The other thing is that the Pi, with the VGA board we were using, only supports 18-bit colour, and we wanted to improve this. Even on the RCade's CRT, colour banding was an obvious issue.&lt;/p&gt;&lt;p&gt;We also wanted to use a laptop, not a desktop, which meant not using a PCI-e card. Instead, a USB interface would be preferable.&lt;/p&gt;&lt;head rend="h2"&gt;Wait, but what is VGA?&lt;/head&gt;&lt;p&gt;VGA is a signaling protocol that maps almost exactly 1:1 with what a CRT actually does.&lt;/p&gt;Taken from wikimedia.org&lt;p&gt;Inside of a CRT, there are 3 electron guns, which correspond to red, green, and blue colour values. Two electromagnets in the neck of the tube are responsible for steering the beam - one steers horizontally and one steers vertically. To draw an image, the beam moves across the screen one horizontal line at a time, and the electron guns are rapidly modulated in order to display the correct colour at each pixel.&lt;/p&gt;&lt;p&gt;VGA contains analog signals for these R, G, and B electron guns. It also contains an HSYNC and VSYNC signal, which are used so that the driver and the CRT can agree on what pixel is being drawn at a given time. Between the VGA input and the CRT is a very simple circuit which locks onto these HSYNC and VSYNC pulses and synchronizes the sweeping of the beam.&lt;/p&gt;Taken from pyroelectro.com&lt;p&gt;The HSYNC pulses happen in between horizontal lines, and the VSYNC pulses happen in between frames. There are dead zones around each pulse - referred to as the front and back porch - which give the electron beam time to sweep back across the screen.&lt;/p&gt;&lt;p&gt;So, all we really need are those R, G, B, HSYNC, and VSYNC signals, running at precise timing, and synced properly relative to each other. Conceptually this is actually pretty simple!&lt;/p&gt;&lt;head rend="h2"&gt;Attempt 1: Using the RP2040's PIO&lt;/head&gt;&lt;p&gt;I like the Raspberry Pi RP2040 a lot. It's relatively cheap (around $1 USD) and has tons of on-board RAM - 264 KB in fact! It also has what is called Programmable IO, or PIO.&lt;/p&gt;&lt;p&gt;I've never used the PIO before, but the basic idea is that you can write assembly programs where every instruction takes exactly one cycle, and has useful primitives for interacting with GPIO. It's a fairly limited instruction set, but it allows for bit-banging precise cycle-accurate custom protocols. It's exactly what I need to modulate a VGA signal.&lt;/p&gt;&lt;p&gt;The PIO code ended up looking like this:&lt;/p&gt;&lt;quote&gt;// 1. low for 320+16=336 pixels // 2. high for 30 pixels // 3. low for 34 pixels // 4. repeat // runs on sm0 // 6 instrs -&amp;gt; can save some with sidesetting let hsync = pio::pio_asm!( ".wrap_target", /* begin pixels + front porch */ "irq set 0 [2]", // tell vsync we're doing 1 line "set pins, 1 [31]", // go low for 32 "set X, 8 [15]", // +16 = 48 "a:", "jmp X-- a [31]", // each loop 32, * 9 = 288, total = 336 /* end front porch, being assert hsync */ "set pins, 0 [29]", // assert hsync for 30 /* end assert hsync, begin back porch */ "set pins, 1 [29]", // deassert, wait 32 (note there is extra delay after the wrap) ".wrap" ); // NOTE - we get irq at *end* of line so we have to time things accordingly // 1. low for 242 lines -&amp;gt; but irq 2 every line for the first 240 // 2. high for 3 lines // 3. low for 22 lines // 4. repeat // runs on sm1 // 19 instr let vsync = pio::pio_asm!( ".side_set 1 opt", ".wrap_target", "set Y, 6", "a_outer:", "set X, 31", "a:", "wait 1 irq 0", "irq set 2", "jmp X-- a", // 32 lines per inner loop "jmp Y-- a_outer", // 7 outer loops = 224 "set X, 15", // 16 more lines = 240 "z:" "wait 1 irq 0", "irq set 2", "jmp X-- z", "wait 1 irq 0", // wait for end of last rgb line "wait 1 irq 0", // 2 more lines for front porch "wait 1 irq 0", "set X, 2 side 0", // assert vsync "b:", "wait 1 irq 0", "jmp X-- b", // wait for 3 lines "set X, 20 side 1", // deassert vsync "c:", "wait 1 irq 0", "jmp X-- c" // wait for 21 lines (back porch) ".wrap", ); // 2 cycles per pixel so we run at double speed // 6 instr let rgb = pio::pio_asm!( "out X, 32", // holds 319, which we have to read from the FIFO ".wrap_target", "mov Y, X", "wait 1 irq 2", // wait until start of line "a:", "out pins, 16", // write to rgb from dma "jmp Y-- a", "mov pins, NULL", // output black ".wrap" );&lt;/quote&gt;&lt;p&gt;The full code lives here.&lt;/p&gt;&lt;p&gt;There are 3 separate PIO programs. &lt;code&gt;hsync&lt;/code&gt; is responsible for keeping time and generating HSYNC pulses. At the start of each line, it generates an IRQ event that the other programs use for synchronization. &lt;code&gt;vsync&lt;/code&gt; counts these events and generates the VSYNC pulses. Finally, &lt;code&gt;rgb&lt;/code&gt; reads pixel data from DMA and outputs to the RGB pins in precise time with the other signals. The &lt;code&gt;out pins, 16&lt;/code&gt; signifies that we're only doing 16-bit colour for now.&lt;/p&gt;&lt;p&gt;There's a lot of weirdness in here to get around the constraints of the PIO. For example, between all 3 programs, only a maximum of 31 instructions are allowed. All of the VGA parameters (resolution, porch length, etc.) are hard-coded, and changing these would require at least a small rewrite. It's pretty brittle in that regard, but for our use-case it's sufficient as a proof-of-concept.&lt;/p&gt;&lt;p&gt;Here it is running the actual CRT in the RCade:&lt;/p&gt;&lt;p&gt;I wanted to fill the framebuffer with a repeating pattern, but I messed up my code, hence it looking weird. That's fine - it was enough to verify my VGA program worked!&lt;/p&gt;&lt;p&gt;As an aside, every time I popped off the back of the RCade to work on it was terrifying. Not because of the lethal voltages inside, but because Recursers absolutely love the RCade. I often joke that if I were to break it, I would basically be the anti-Frank!&lt;/p&gt;&lt;p&gt;Now that I had something that could take a framebuffer and throw it onto the CRT, it was time to get the image from my computer to the RP2040.&lt;/p&gt;&lt;head rend="h2"&gt;Let's write a kernel module!&lt;/head&gt;&lt;p&gt;My plan was to write a Linux kernel module that would expose itself as a framebuffer, and then send that framebuffer over USB to the RP2040. On the framebuffer side, this involved interfacing with the DRM layer.&lt;/p&gt;&lt;p&gt;I actually made decent progress here, although I kernel panicked many, many times. I never bothered to set up a proper development environment (oops), so pretty much any bug would require me to reboot my computer. This was super annoying and tedious, although I did learn a lot. I found cursed things in the official documentation, like interrobangs!&lt;/p&gt;Linus pls&lt;p&gt;I got as far as getting a framebuffer to show up at the correct resolution and refresh rate. Along this journey though, I discovered the GUD kernel module, and quickly realized I should use that instead.&lt;/p&gt;&lt;head rend="h2"&gt;GUD is... pretty good&lt;/head&gt;&lt;p&gt;Okay so this GUD thing is sick. It's a USB display adapter protocol - exactly what I need! It was originally designed to send video from a computer to a Pi Zero for use as a secondary display. It consists of an upstreamed (!!!) kernel module that runs on the host, and separate gadget software that runs on the Pi Zero. I decided I would just write my own gadget implementation to run on the RP2040.&lt;/p&gt;&lt;p&gt;As a protocol, GUD seems decent. It supports compression over the wire, and only sends the deltas of what's changed in the host's framebuffer. It's also pretty robust in terms of allowing the gadget to advertise what features it supports - compression is optional, and there's flexibility in colour depth and resolution. And again, it's upstreamed into the kernel, so anyone on modern Linux could use my display adapter with no software tweaks.&lt;/p&gt;&lt;p&gt;Unfortunately, GUD has almost no documentation. I figured out what I needed to do by reverse engineering the kernel module, which involved recompiling it to add some debugging statements. The protocol is simple enough that is wasn't too much of a hassle, and it didn't take long before I had developed a gadget implementation in Rust for the RP2040.&lt;/p&gt;&lt;p&gt;And with that, we saw our first Linux images on the CRT:&lt;/p&gt;&lt;p&gt;I know, I know, it looks terrible. Several years ago, I had built a board that implements the R/G/B DACs out of resistors, and I reused that for this project. It can only do 12 bits of colour maximum, and for this test I only bothered to wire up ~2 bits per channel, which is basically unusable. But it proves the concept works!&lt;/p&gt;The board I built several years ago. It was originally designed to fit an STM32 development board.&lt;p&gt;To be honest, it's pretty lucky that this board came with me to New York. I'm surprised I didn't either throw it out or move it to my parent's place. It was probably in some other box of things I deemed worth keeping around.&lt;/p&gt;The VGA board connected to the RP2040.&lt;p&gt;You can see from the above picture that I really connected the bare minimum for a proof-of-concept. I find perfboard soldering to be a bit tedious!&lt;/p&gt;&lt;p&gt;As an aside, you may notice in the video that the entire screen is shifted to the left. The left side has wrapped around and is now on the right side. On initial boot, it would look fine; over time it would gradually get worse and worse. This is a bug in my implementation - I suspect it's some kind of buffer underflow that's happening, such that each time it occurs, the PIO gets progressively more out of sync. But this is just a guess; I didn't look into it too much.&lt;/p&gt;&lt;p&gt;The colour depth issue is trivial to fix, but this next one isn't. The framerate sucks! You can even see it in the video above, where you can watch the new frame scroll down the screen. The RP2040 can only do USB FS (full-speed), which is capable of 11 Mbps. At the 320x240x16 bpp we were originally targeting, every frame is 153.6 kB. At our maximum USB FS speed, that's less than 10 FPS! Embarrasingly, I had originally done the math with a bandwidth of 11 MBps, not 11 Mbps, so I was off by a factor of 8. I was hoping to get something at least temporarily usable but had to go back to the drawing board.&lt;/p&gt;&lt;head rend="h2"&gt;Going on a GUD gadget side quest&lt;/head&gt;&lt;p&gt;Who even needs microcontrollers anyway? My next idea was to use the normal GUD gadget implementation, running on a Pi Zero, but outputting to VGA over GPIO. Conceptually this is pretty simple, although in practice it was anything but. The canonical GUD gadget software was based on a 2021 version of Buildroot, which was too old to output VGA. I tried, and failed, to update the Buildroot version, as well as to backport the VGA overlay. Neither of those really worked, but I didn't really know what I was doing.&lt;/p&gt;&lt;p&gt;I also played around with generating a custom NixOS image that had a modern kernel and the GUD gadget kernel module. When that didn't work I prepared to run a user space GUD gadget implementation on Raspberry Pi OS. But like, isn't that boring? And then I'll still be stuck at 18 bit colour! And sometimes a girl just wants to tickle her electrons :3&lt;/p&gt;&lt;head rend="h2"&gt;Attempt... 2? 3? 1+i? Returning to MCU land&lt;/head&gt;&lt;p&gt;Okay, so my beloved RP2040 doesn't support USB HS (high-speed). My beloved RP2350 (the newer version of the same chip) doesn't either. But some of my beloved STM32s do!&lt;/p&gt;&lt;p&gt;Initially I was planning to go computer -&amp;gt; USB HS -&amp;gt; STM32 -&amp;gt; SPI bus -&amp;gt; RP2040 -&amp;gt; VGA. But like, that's complicated, and there are 2 microcontrollers to program, and there is so much to go wrong, and the SPI bus protocol is going to need to be robust against lost/extra bits, and AAAAAAAAAA I don't wanna!&lt;/p&gt;&lt;p&gt;But! STM32! I learned through research that some of the nicer ones have an LTDC peripheral, which, among other things, can drive an LCD display. And guess what? Many LCDs take in an R, G, B, HSYNC, and VSYNC signal. That's right - they pretend they're a CRT, and they pretend they have a cute little electron gun inside of them, and the STM32 is like "ok I got u" and can just like, do this natively. And I realize that this is what VGA is, but it's so, so funny to me that the protocol is literally just the manifestation of a physical design that is largely obsolete.&lt;/p&gt;&lt;p&gt;Okay so at this point I'm like, is this even a real project anymore? I'm just connecting the USB peripheral to the LTDC peripheral. What part of this is supposed to take effort? I had already written the GUD gadget implementation. Wasn't I basically already done?&lt;/p&gt;&lt;p&gt;OH BOY.&lt;/p&gt;&lt;p&gt;Anyway, by now it's Christmas time and I fly back to Canada to hang out with my family, as you would expect. I had none of my hardware with me, so now felt like a good time to design the actual board.&lt;/p&gt;&lt;p&gt;By Christmas Eve, this is what I had. Conceptually, it's a pretty basic board - there's the USB HS input, the VGA output, 3 8-bit DACs, some RAM for the framebuffer, and supporting components. At the heart of it is the STM32H723, which is a microcontroller that's advertised as supporting USB HS and LTDC.&lt;/p&gt;&lt;p&gt;It's worth talking about the DACs a bit. They have a few requirements. They need to map the 8-bit binary space uniformly to the analog domain. They also need to act as a resistor divider - my I/O is at 3.3V, but VGA expects a maximum of 0.7 volts for R/G/B. And finally, they need to be impedance-matched to the 75 ohms of the VGA cable, to prevent reflections and ringing that show up in the image. I am... pretty doubtful we need this at our resolution, but it doesn't hurt, and it increases nerd cred (^:&lt;/p&gt;&lt;p&gt;I encoded all of these requirements into a system of equations, threw it into a SAT solver, and computed all of my resistor values. I checked the output manually and it made sense, so I used these values in my DAC.&lt;/p&gt;&lt;p&gt;Also worth noting is the length-matched traces between the STM32 and the HyperRAM. Length-matching ensures that all the signals arrive at the same time; if some arrive too early or late it can cause issues. The traces aren't impedance-matched, but I did a bit of math and determined they were short enough that I didn't have to worry about it.&lt;/p&gt;&lt;p&gt;Also, I want to talk about the USB port. I used Mini-USB. Alright look. I know I know, I should have used USB-C. But I don't like USB-C! It's a dumb standard. We spent decades teaching non-technical users to plug the square wire into the square hole and the round wire into the round hole. And then we made every hole the same shape!! But they don't all support the same things!! Not even every cable supports the same things!! I hate it!! And Mini-USB is so cute. It's not reversible, but who cares? It's more robust than micro USB, while still being small. And it's my board, my rules. So yes, I will keep sending pictures of this board to people, and they will keep complaining it doesn't use USB-C. And I will continue to not care! Mini-USB is CUTE. And by the way, if you read this entire article and this is the section you choose to engage with, then you are boring!!! You will never live up to Mini-USB!!&lt;/p&gt;&lt;p&gt;Okay okay sorry about that. I am calm now. With all of that out of the way, I placed the order for the boards. I bought 5 of them, 2 of which were partially assembled. I would complete the rest of the assembly myself, but I didn't want to worry about the more finicky stuff. Between taxes, tariffs, and shipping, it came to a little over a hundred dollars USD.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes&lt;/head&gt;&lt;p&gt;About a week later, I was back home in NYC. My boards hadn't arrived yet, although I did have access to an STM32H723 development board at this point. To prepare for my boards, I started porting my RP2040 firmware to the STM32H723.&lt;/p&gt;&lt;p&gt;Things were going well until I tried getting USB set up. For some reason, I could only get it working at USB FS speeds. I figured I was just initializing something wrong - maybe a register I was forgetting about, or that wasn't in the HAL? I did a lot of digging, before finding this hidden in the datasheet (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The devices embed a USB OTG high-speed (up to 480 Mbit/s) device/host/OTG peripheral that supports both full-speed and high-speed operations. It integrates the transceivers for full-speed operation (12 Mbit/s) and a UTMI low-pin interface (ULPI) for high-speed operation (480 Mbit/s). When using the USB OTG_HS interface in HS mode, an external PHY device connected to the ULPI is required.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;My heart sank. Yes, despite this chip very clearly advertising support for USB HS, it can't actually do that without an external PHY. This is super easy to miss - I actually told other people about the problem, and often they would tell me I was incorrect until I showed it to them in the datasheet. I've also found many posts on the ST Community forums from people running into the same thing. So yeah, I need a new board.&lt;/p&gt;&lt;p&gt;But because boards are expensive, I figure I'll still use the rev 1 board to validate as much as I can.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes, again&lt;/head&gt;&lt;p&gt;Once the boards come, I complete assembly of one, plug it into my computer, and nothing happens. I find out that the 3.3V rail is shorted to ground. This is the same on all of my boards, even the 3 that are disassembled. Some debugging later, it turns out I moved a via in KiCad and didn't do a re-pour. My ground plane was connected to my power plane.&lt;/p&gt;&lt;p&gt;I have a full CI/CD pipeline set up for my PCBs, so I was surprised it didn't catch this. It turns out it has a bit of wiggle-room, and the re-pour was small enough it didn't get picked up. I now know I need to be disciplined and run DRC locally, ensuring there are literally no differences (and if there are, commit them and push them up to my Git forge).&lt;/p&gt;&lt;p&gt;Although annoying, and quite embarrassing, this wasn't a huge deal. I used a drill bit and very carefully drilled out the offending via by hand. It made a bit of a mess - make sure you use breathing protection - but I got a board that worked.&lt;/p&gt;The drilled-out via. You can see it directly under the text, near the center-bottom of the image.&lt;p&gt;At this point I wrote some code that exercised the HyperRAM and VGA. Everything worked great, so I began work on the new board. Here's what my development setup looked like while I was testing:&lt;/p&gt;&lt;p&gt;Even though the rev 1 board didn't work out, Frank pointed out that the difference between it and the previous revision was stark:&lt;/p&gt;&lt;p&gt;Not a bad pace of development!&lt;/p&gt;&lt;head rend="h2"&gt;Attempt 4 - Rev 2&lt;/head&gt;&lt;p&gt;I needed an STM32 that supported ULPI (used for talking to the USB PHY), LTDC, and some kind of external RAM. I looked at dozens of chips and found all sorts of blockers. Chips that actually supported both (but they had overlapping pins), chips that were advertised as supporting both (but in actuality, could only do one or the other, depending on the specific model number), and chips that actually could do both, with unconflicting pins, but only in a BGA package. I did not particularly want to deal with that, mainly because the tiny vias and traces would balloon the board cost even more.&lt;/p&gt;&lt;p&gt;I ended up settling on the STM32H750IBT, a massive, 176 pin, LQFP chip. This thing is larger than some New York apartments, and at over $10 USD, it costs about the same! I have bought entire dev boards for a fraction of this.&lt;/p&gt;&lt;p&gt;Once I picked out the chip, I basically redesigned the entire board from scratch. Sure, I could reuse the DACs, but I needed completely new RAM (the new chip has no HyperBus), as well as the USB PHY and supporting components. Now that my Christmas vacation was over, it took me a solid week to get everything designed. This isn't my most complicated board, but it's certainly my most complex routing:&lt;/p&gt;&lt;p&gt;I mean, look at those traces. I'm using basically all available space just to get them to be the same length. ST famously has bad pinouts, and because one of the memory controller pins is located on the complete opposite side of the chip, literally all of the rest of the RAM traces had to be lengthened. And the RAM has a 16-bit data bus. I had to route 38 length-matched traces for the memory alone!&lt;/p&gt;&lt;p&gt;The USB PHY also had a decent number of traces to route, although far less than the RAM. This is probably the part where I'm supposed to say that like, crosstalk is bad and stuff, but we're just gonna ignore that. I had like no space; leave me alone!&lt;/p&gt;&lt;p&gt;Here's what the board looked like:&lt;/p&gt;&lt;p&gt;And with that, I ordered the board. Waiting for it to arrive just about killed me, but when it finally did, I got to work.&lt;/p&gt;&lt;head rend="h2"&gt;Board bring-up&lt;/head&gt;&lt;p&gt;Board bring-up is a magical thing. One-by-one, you enable each part of the board, and you make sure that everything works the way you expect. Given that USB burned me before, I decided to start there.&lt;/p&gt;&lt;p&gt;Right out of the gate, I was off to a bumpy start. I got the USB technically working, and I even got it to show up on my computer as USB HS (yay!), but it was super, super flaky. Eventually I worked out that its crystal oscillator was unstable. Going back to the datasheet, I realized I missed a 1M ohm resistor, which was meant to be put in parallel with the crystal. I didn't have one handy, but I know the human body is around that resistance. I put one finger on each terminal of the crystal. It immediately stabilized. I was pretty ecstatic!&lt;/p&gt;&lt;p&gt;The next day I went to the Recurse Center and stole a 1M ohm resistor to affix to the board. (Faculty, if you're reading this, I owe you about a tenth of a cent. Sorry!)&lt;/p&gt;&lt;p&gt;With that over, the rest of the bring-up process was pretty smooth. I got the LTDC running and ported over the rest of the code that implemented the GUD protocol. I had written things pretty naively but, to my surprise, it didn't need any optimization for high-speed USB. I guess that's what a microcontroller with a 480 MHz core will get you!&lt;/p&gt;&lt;head rend="h2"&gt;Running it in the RCade cabinet&lt;/head&gt;&lt;p&gt;I was already at the Recurse Center at this point, so I popped the back off the RCade, unplugged the VGA from the Pi, and plugged it into my board. It started up immediately - the colours looked great and I got the full 60 Hz framerate. To be honest, I was shocked at how good it looked, and the crowd that had formed was shocked too. I wasn't really a believer that 24 bit colour would be noticeable, but I was totally wrong. The lack of colour banding was striking.&lt;/p&gt;&lt;p&gt;Next, I plugged the board into the Pi, and Frank reconfigured it to make my display adapter the primary display. We launched the normal RCade software and played some games. They looked truly amazing; nothing like before. Rose, one of the main people who developed the software, joked that it looked so good that some of the graphical shortcuts she took were no longer sufficient.&lt;/p&gt;&lt;p&gt;It's hard to tell in the pictures but the difference in person was striking. Where it's most obvious is in the lack of banding around the mountains.&lt;/p&gt;&lt;p&gt;This felt amazing, but I wasn't quite ready to leave the board installed. It was fragile - especially with the resistor I bodged on - and it was expensive. I took my board back out and Frank reverted the RCade to how it was before.&lt;/p&gt;&lt;head rend="h2"&gt;Designing a case&lt;/head&gt;&lt;p&gt;I'll be honest. I don't get that much joy out of 3D modeling. I find it frustrating, tedious, and generally unfulfilling. To get around this, I decided to use YAPP to design the case. YAPP is a parametric box generator written in OpenSCAD. I wrote a few dozen lines of code and ended up with this beauty:&lt;/p&gt;&lt;p&gt;It took barely any time at all and only took 2 physical revisions before I was happy with it. I added the OpenSCAD code to my board repository and CI/CD pipeline. Now, it builds all the files I need to order the boards, as well as the STL files for the case.&lt;/p&gt;HE'S BEGINNING TO TAKE FORM&lt;p&gt;And now, with the board in the case:&lt;/p&gt;&lt;p&gt;At this point I was starting to prepare myself to install it in the RCade.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes, again??&lt;/head&gt;&lt;p&gt;Everything was done, so I expected I'd just plug it in and be good to go. When I did this, though, nothing happened. After some debugging I realized the USB had completely died on my board. It wasn't showing up on any computer I connected it to, although the STM32 was still chugging along happily (and outputting to VGA).&lt;/p&gt;&lt;p&gt;I still haven't figured out exactly what happened here. I was having a bit of flakiness with the USB already. I vaguely suspect ESD to either the STM32 or the USB PHY, but am not super confident this is the cause. I'm going to keep looking into this. (inb4: wow maybe you shouldn't have touched the crystal without grounding yourself first!)&lt;/p&gt;&lt;p&gt;In the meantime, I assembled a second board and got that installed instead. I'm slightly nervous because I don't have a third board to use if this one also dies, and I don't want to order any more until I can figure out what's killing them. That said, it has been a few days now since I installed it, and despite running 24/7, there's no signs of it dying yet.&lt;/p&gt;&lt;p&gt;Here's the board in its case, installed in the RCade. We're still running it off the Raspberry Pi for now, but soon we'll have that switched out with a laptop. I can't wait!&lt;/p&gt;&lt;head rend="h2"&gt;Future improvements&lt;/head&gt;&lt;p&gt;There are all sorts of things I want to change. I want the board to also support audio, with an integrated amp. Perhaps even a tube amp? I just think it would be funny. And being able to read input from the controls would be cool too.&lt;/p&gt;&lt;p&gt;On the software side, I want double or triple buffering. I actually got them both working, although they didn't play nice with the deltas that GUD sends over the wire. There are workarounds to this that I haven't implemented yet. It would also be nice to give GUD the ability to disable these deltas; perhaps that would be a good feature for me to add to the kernel module. Writing some documentation on the GUD protocol could be good too!&lt;/p&gt;&lt;p&gt;This was a really fun project, and it's not over yet, but I think all the hard stuff is pretty much done (although - I've thought that before!). I really wasn't expecting this to take as long as it did, but I learned so much, and I'm a stronger engineer for it.&lt;/p&gt;&lt;head rend="h2"&gt;Source code&lt;/head&gt;&lt;p&gt;There's a few repositories of interest:&lt;/p&gt;&lt;p&gt;The hardware lives here.&lt;/p&gt;&lt;p&gt;The software lives here.&lt;/p&gt;&lt;p&gt;If you're interested, the original software for the RP2040 lives here.&lt;/p&gt;&lt;p&gt;My very messy DAC equations live here.&lt;/p&gt;&lt;p&gt;My Nix GUD gadget attempt lives here.&lt;/p&gt;&lt;p&gt;I also wrote a fair bit of scratch code while learning (such as for my kernel module), but I don't think any of it was worth putting it in my Git forge.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46888795</guid><pubDate>Wed, 04 Feb 2026 17:35:05 +0000</pubDate></item><item><title>The Great Unwind</title><link>https://occupywallst.com/yen</link><description>&lt;doc fingerprint="7fad91172de75f94"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you wondered why the stock market has been so choppy since October and why crypto and gold keep flash crashing? The western media would have you believe this is due to AI bubble, war in Greenland, and Trump's tweets. We have a better story to tell.&lt;/p&gt;
    &lt;p&gt;Wall Street has lost control of the Japanese Yen carry trade unwind.&lt;/p&gt;
    &lt;p&gt;There's been a fair bit of quiet chaos in financial markets recently. Cryptocurrencies have lost 40% of their value. We saw silver drop 40% which hasn't happened since 1980. Stocks like Microsoft are getting picked off one-by-one with 15% drops when positive earnings reports come out. Meanwhile the broader market chops sideways, so people think things are fine. Trump and Europe were on the brink of war for control of a desolate arctic territory. Truth Social has overtaken FOMC as the most important source of financial news. These things may all appear to the untrained eye as a series of idiosyncratic, disconnected shocks. The prevailing media narrative is that the market is reacting negatively to AI CapEx spending and a hawkish new Fed chair. But our systematic analysis of cross-asset flows, derivatives positioning, central bank policy minutes, and institutional balance sheets suggests a singular, unified causality that binds these disparate anomalies, which is the covert unwinding of the Japanese Yen carry trade.&lt;/p&gt;
    &lt;p&gt;For nearly thirty years, the Bank of Japanâs (BOJ) Zero Interest Rate Policy (ZIRP) and subsequent Negative Interest Rate Policy (NIRP) effectively transformed the Yen into the worldâs funding currency. We would call it the greatest free money printer ever made. By anchoring borrowing costs at or near zero, the BOJ enabled Wall Street to borrow Yen cheaply and invest it with leverage into higher yielding instruments globally, such as U.S. treasuries, equities, and cryptography. For example, you borrow Yen from Japan at 0% interest, you exchange it for USD, and then you buy treasury bonds that pay 4%. It's that simple. This funded government benefits and provided continuous reliable liquidity for financial markets that made stocks keep going up while suppressing volatility.&lt;/p&gt;
    &lt;p&gt;Trillions of dollars of free loans from the Bank of Japan were used by a generation of investors to buy a double digit percentage of the U.S. economy. Now those loans are being recalled. Wall Street traders who levered up on the free Japanese money now have to sell trillions of assets and convert the proceeds back to Yen in order to not be liquidated. These aren't happy times for them. They get liquidated when Japan raises interest rates; they get liquidated when the Federal Reserve lowers interest rates; they get liquidated when the Japanese Yen increases in value; they get liquidated when tech stocks aren't going up enough, and all four of these things have been happening at once.&lt;/p&gt;
    &lt;p&gt;Wall Street may be greedy, but they're very intelligent too. They made many smart choices about where to put the "free" money. Now let's say you're someone who's also smart, but was wise enough to not use Sauron's ring. Chances are you invested in the same things as Wall Street. So by now you've probably seen your whole portfolio move against you; you're wondering why your hedges don't work; and you feel like you're being punished for making all the right choices. It's because other smart people, who got greedy, are being forced to close their positions, and you're the whipping boy for their avarice.&lt;/p&gt;
    &lt;p&gt;The Japanese Yen is sort of like GameStop ($GME). It's the most shorted currency on Earth. When you borrow yen to buy American assets, you're effectively shorting the yen. Currency can be rehypothecated so that yen-denominated debt ends up exceeding the actual yen supply, the same way GME's short interest exceeded 100% of its float. When shorts start covering it compounds tragedy, because they all have to buy yen, which makes its value increase, forcing more shorts to cover, and Japan is a small island.&lt;/p&gt;
    &lt;p&gt;This December 2025 rate hike to 0.75%, followed by the explicitly hawkish signalling from Prime Minister Sanae Takaichiâs administration, has fundamentally altered the risk-reward calculus of these leveraged positions. The market disruptions observed in January 2026 bear the distinct mathematical signature of a forced liquidation event rather than a fundamental repricing of growth prospects. When correlations between historically uncorrelated assets (e.g. Gold, Bitcoin, Microsoft, and Silver) approach 1.0 during a sell-off, it serves as a distinct indicator that traders are not selling what they want to sell, but rather what they must sell in order to meet margin calls in a funding currency that is rapidly appreciating against their liabilities.&lt;/p&gt;
    &lt;p&gt;We shall investigate the mechanics of this unwind in exhaustive detail. We analyze the "Greenland Distraction" not as a root cause but as a volatility trigger that shattered the complacent calm of the "Davos Consensus." We examine the anomalous liquidation in precious metals following the nomination of Kevin Warsh to the Federal Reserve Chairmanship, and we dissect the flow of funds from major Japanese institutional whales like Norinchukin Bank, whose retreat from foreign bond markets has left a liquidity vacuum in the U.S. Treasury complex. The evidence points to a systemic repricing of the global cost of capital, originating in Tokyo and transmitting violently through the plumbing of Wall Street, leaving no asset class untouched.&lt;/p&gt;
    &lt;p&gt;To fully comprehend the market chaos of January 2026, one must look beyond the immediate headlines of the new year and scrutinize the subtle yet seismic shifts that occurred in Tokyo during the closing months of 2025. The conventional market narrative has long regarded the Bank of Japan as a passive, almost paralyzed actor, perpetually trapped in a deflationary mire and unable to normalize policy. This view has always been demonstratably false. The truth is that Wall Street leaders have been planning for the next quarter, while the Japanese have been preparing for the next century. The data confirms a deliberate, aggressive shift toward normalization that caught global carry traders offguard.&lt;/p&gt;
    &lt;p&gt;In a move that many Western analysts critically underestimated, the Policy Board of the Bank of Japan voted unanimously to raise the uncollateralized overnight call rate to 0.75% during its policy session on December 18-19, 2025. While a 25 basis point hike might appear negligible in the context of Federal Reserve or ECB tightening cycles, in the context of the Japanese financial system, which has operated near the zero-bound for decades, it represents a massive tightening of financial conditions.&lt;/p&gt;
    &lt;p&gt;This move was not merely a technical adjustment; it was a fundamental regime change. Coming from a baseline of -0.1% in early 2024 and 0.50% in late 2025, the move to 0.75% signaled that the era of "free money" had definitively ended. The rationale provided by the BOJ was grounded in shifting inflationary dynamics. Core CPI (excluding fresh food), the central bank's preferred metric, was tracking near 3% in late 2025, persistently exceeding the 2% price stability target.Although inflation eased slightly to 2.4% in December, the BOJ minutes reveal a board convinced that "wage gains may be durable," thus justifying higher rates to prevent a wage-price spiral.&lt;/p&gt;
    &lt;p&gt;Crucially, the minutes from the December meeting, which were released in late January 2026, contain explicit language suggesting that the tightening cycle is far from complete. The board noted that "real interest rates are expected to remain negative," implying that a policy rate of 0.75% is still considered accommodative relative to inflation.To a bond trader, this is hawkish code. It suggests that the "neutral rate" is significantly higher, potentially between 1.5% and 2.0%. If the market prices in a terminal rate of 2.0%, the cost of funding for carry trades effectively triples from previous levels, turning profitable arbitrage positions into deep losses.&lt;/p&gt;
    &lt;p&gt;The political dimension in Japan has exacerbated the monetary tightness, creating a "double tightening" effect that algorithms have struggled to price. Prime Minister Sanae Takaichi, preparing for a snap election on February 8, 2026, has adopted a complex economic stance that blends fiscal expansion with monetary discipline, a volatile mix for currency markets.&lt;/p&gt;
    &lt;p&gt;Takaichi advocates for "strategic fiscal spending" and tax cuts to stimulate the domestic economy. In standard macroeconomic theory, an expansionary fiscal policy (increased government spending) combined with a tightening monetary policy (higher rates to combat the resulting inflation) is the perfect recipe for currency appreciation. While Takaichi has publicly softened her rhetoric to avoid accusations of currency manipulation, stating she "did not have a preference for the yen's direction", her policies speak louder than her soundbites.&lt;/p&gt;
    &lt;p&gt;The market fears that Takaichiâs proposed fiscal largesse will force the BOJ to hike rates faster than currently projected to counteract the inflationary effects of government spending. This creates a two-front war on the Yen carry trade:&lt;/p&gt;
    &lt;p&gt;Cost of Funding Rises: Higher BOJ rates make borrowing Yen expensive.&lt;/p&gt;
    &lt;p&gt;Exchange Rate Risk: If the Yen appreciates due to the fiscal-monetary policy mix, the principal value of the USD-denominated assets held by Japanese investors falls in Yen terms, triggering margin calls.&lt;/p&gt;
    &lt;p&gt;The tension between the Prime Minister's office and the Ministry of Finance (MOF) adds another layer of uncertainty. Finance Minister Satsuki Katayama has been far less tolerant of currency volatility, repeatedly intervening or threatening intervention when USD/JPY approaches the 155-160 danger zone.This political friction creates a "floor" for the Yen, making shorting the currency a perilous endeavor for global macro funds.&lt;/p&gt;
    &lt;p&gt;Perhaps the most critical, yet underreported, development is the behavior of Japan's gargantuan institutional investors, specifically Norinchukin Bank (often referred to as the "CLO Whale") and Nippon Life Insurance. These entities have historically been the largest buyers of U.S. debt, recycling Japan's trade surplus into U.S. Treasuries and corporate bonds.&lt;/p&gt;
    &lt;p&gt;The data indicates a massive reversal in these flows. Following significant losses in 2024 and 2025 due to unhedged exposure to U.S. and European sovereign bonds, Norinchukin has been actively liquidating foreign assets. By the end of December 2025, the bank had unloaded nearly Â¥12.8 trillion (approximately $88 billion) in foreign government bonds.The bankâs CEO, Taro Kitabayashi, confirmed the completion of this sell-off, stating the bank would "take its time" before committing capital to fresh investments.&lt;/p&gt;
    &lt;p&gt;The significance of this cannot be overstated. A major, price-insensitive buyer of U.S. debt has left the building. When the U.S. Treasury issues debt to fund its deficit, Norinchukin is no longer the guaranteed bid. This removal of liquidity support weakens the floor for U.S. Treasuries, contributing to the yield spikes seen in January. Similarly, Nippon Life has signaled a rotation back into domestic Japanese Government Bonds (JGBs), acknowledging that "unrealized losses" on foreign bonds had swelled to Â¥4.7 trillion.The logic is simple: why take currency risk for a 4.5% U.S. yield when domestic JGB yields are rising and offer a risk-free return in your home currency?&lt;/p&gt;
    &lt;p&gt;By December 31, 2025, the stage was set. The "free money" era was over. The largest holders of capital in Tokyo were repatriating funds or moving into cash. Global markets, however, were still positioned for "business as usual", long Nvidia, long Bitcoin, short Yen. The dissonance between Japanese reality and Western positioning created the perfect conditions for a crash.&lt;/p&gt;
    &lt;p&gt;To validate the thesis that the Yen unwind is the primary driver of volatility, we must examine the sequence of events. The crash did not happen in a vacuum; it followed a precise timeline where geopolitical shocks acted as triggers for a structural fragility that had been building since the BOJ's December pivot.&lt;/p&gt;
    &lt;p&gt;The pressure began to build in Q4 2025. As the BOJ signaled its intention to hike rates, Japanese traders, often the "canary in the coal mine" for global liquidity, began to reduce risk. This cycle started with Bitcoin. Bitcoin is a pure liquidity asset; it has no yield and is often funded via margin. As the cost of Yen margin rose, Japanese selling pressure on Bitcoin intensified from October through December.This was the first tremor.&lt;/p&gt;
    &lt;p&gt;Was the "Greenland War" theater? While the military dimensions may have been performative, the economic consequences were tangible and acted as the catalyst that exposed the fragility of the Yen carry trade.&lt;/p&gt;
    &lt;p&gt;On January 17, 2026, President Trump escalated his demand to purchase Greenland by threatening a 10% tariff on eight European nations (including the UK, Germany, and France) and escalating to 25% by June if the territory was not ceded.This introduced a "tail risk" that markets had not priced: the fracture of the Atlantic economic alliance.&lt;/p&gt;
    &lt;p&gt;Following the Martin Luther King Jr. holiday, U.S. markets opened on January 20 to a bloodbath. The S&amp;amp;P 500 fell 2.1%, the Nasdaq composite dropped 2.4%, and yields on U.S. Treasuries spiked.The narrative was "Greenland," but the market mechanics told a different story. The threat of tariffs on close allies disrupts the "Atlantic Trade" narrative. For Japanese investors holding U.S. assets, this introduced a new risk premium. It wasn't just about rates anymore; it was about the stability of the U.S.-led global order. This geopolitical volatility forced risk parity funds and algorithmic traders to reduce gross exposure. When a global portfolio deleverages, it buys back its funding currency. In this case, it bought Yen.&lt;/p&gt;
    &lt;p&gt;While Trump walked back the military threat on January 21 at Davos, the economic threat of tariffs remained a live wire. The volatility persisted, suggesting that the "Greenland" narrative was merely the match that lit the fuse of a much larger powder keg.&lt;/p&gt;
    &lt;p&gt;The final and most violent phase of the crash occurred at the end of the month, triggered by the nomination of Kevin Warsh as Federal Reserve Chair.Warsh is widely perceived as a hawk, favoring sound money and skepticism toward quantitative easing. His nomination signaled the potential end of the "Fed Put", the assumption that the central bank would always intervene to support asset prices.&lt;/p&gt;
    &lt;p&gt;This announcement triggered a massive repricing of the "Debasement Trade." Assets that thrive on currency debasement, Gold, Silver, and Bitcoin, collapsed. Gold fell ~11%, and Silver crashed ~36% in a single session.This synchronization of losses across uncorrelated assets (Tech and Gold falling together) is the definitive signature of a liquidity crisis driven by margin calls.&lt;/p&gt;
    &lt;p&gt;The unwinding of a carry trade is not a monolithic event; it is a cascade that ripples outward from the most liquid and speculative assets to the core holdings of institutional portfolios. The sequence of asset price collapses observed from October 2025 to January 2026 follows this classic liquidation hierarchy perfectly.&lt;/p&gt;
    &lt;p&gt;As noted, the unwind began in the crypto markets. Japan is home to a massive retail crypto trading base, and the Yen is a major pair for Bitcoin trading. Snippets indicate that Japanese traders began selling off Bitcoin in October 2025.&lt;/p&gt;
    &lt;p&gt;This timing is crucial. It correlates with the period when the BOJ began signaling the December rate hike. Retail traders, facing higher mortgage rates and loan costs in Japan, likely liquidated their most volatile, liquid asset first to raise cash. The selling was exacerbated by the looming tax reform in Japan. While the proposal to move to a flat 20% tax rate is bullish in the long term, the immediate pressure of rising funding costs forced traders to sell before the tax cut could be realized.By January 31, massive outflows from Bitcoin ETFs ($528 million) coincided with the broader market crash, confirming that crypto was being used as a source of liquidity to cover losses elsewhere.&lt;/p&gt;
    &lt;p&gt;Consider the "painful ~3% dump" in the Nasdaq and Microsoft's staggering 15% drop. On January 29, 2026, Microsoft reported earnings. Despite beating revenue estimates ($81.27 billion vs. $80.28 billion), the stock plummeted ~11-15% intraday.&lt;/p&gt;
    &lt;p&gt;The street blamed concerns over "AI CapEx", the idea that Microsoft was spending billions on data centers with slow return on investment. However, a 15% drop in a $3 trillion company on a "good" earnings beat is rarely fundamental; it is mechanical. Microsoft is a quintessential "momentum" stock, heavily held by foreign institutional investors, including Japanese pension funds. When the Yen strengthens, the value of these USD-denominated assets falls in JPY terms.&lt;/p&gt;
    &lt;p&gt;If a Japanese insurer holds Microsoft unhedged, a falling USD/JPY exchange rate hurts their balance sheet. If they hold it hedged (rolling short USD positions), the rising U.S. rates (driven by the Warsh nomination) make the hedge prohibitively expensive. The January 29 drop was likely exacerbated by a "stop-loss" cascade from Tokyo desks. As the price broke key technical levels, algorithms programmed to protect Yen-denominated returns indiscriminately sold the most liquid blocks. Microsoft, being one of the most liquid stocks in the world, became the ATM for the rest of the portfolio.&lt;/p&gt;
    &lt;p&gt;The most compelling evidence of a forced liquidation event is the behavior of Gold and Silver on January 31, 2026. Gold fell ~11-12% and Silver crashed ~31-36% in a single session. Historically, Gold acts as a safe haven during equity market turmoil. If the Nasdaq is crashing due to "Greenland" fears, Gold should rally. Instead, it crashed.&lt;/p&gt;
    &lt;p&gt;This anomaly can be explained by two factors:&lt;/p&gt;
    &lt;p&gt;The Warsh Effect: As discussed, Warsh's nomination strengthened the USD and undermined the thesis for holding anti-fiat assets.&lt;/p&gt;
    &lt;p&gt;Margin Call Dynamics: Snippets reveal that CME Group and the Shanghai Gold Exchange raised margin requirements on gold and silver futures days before the crash.When Japanese traders faced losses on their Microsoft longs and their Yen shorts, they needed cash immediately. They couldn't sell illiquid bonds quickly enough, so they sold their "winners." Gold had rallied to ~$5,400/oz prior to the crash. Traders liquidated their profitable Gold positions to pay for the margin calls on their losing Tech and Yen positions.&lt;/p&gt;
    &lt;p&gt;Cross-Asset Correlations (Week Ending Jan 31, 2026)&lt;/p&gt;
    &lt;p&gt;This correlation breakdown is visualized in Figure 2, where the correlation between Gold and the Nasdaq 100 spikes to nearly 1.0 during the crash week, a statistical anomaly that only occurs during severe liquidity events.&lt;/p&gt;
    &lt;p&gt;The "Yen Whale" hypothesis is strongly supported by the data on futures volumes and repo market stress. The "central mystery" is not just in the price action, but in the unseen flows of the derivatives market.&lt;/p&gt;
    &lt;p&gt;About a week ago, some whale kicked off an astronomically large market order for a /6J long when it hit all-time lows. /6J (CME Yen Futures) hit a low of ~0.00647 (approximately 154.50 USD/JPY) in late January. This level has historically been a "line in the sand" for the Japanese Ministry of Finance (MOF).&lt;/p&gt;
    &lt;p&gt;CME reported record volumes in FX and Interest Rate products for January 2026.The aggressive buying off the lows suggests a massive repatriation flow. Who is the Whale? Two theories emerge:&lt;/p&gt;
    &lt;p&gt;The MOF Thesis: The Ministry of Finance has a history of stealth intervention. Buying /6J (Long Yen) is functionally equivalent to selling USD reserves. Buying futures allows them to support the currency without immediately depleting cash reserves, squeezing speculators who are short.&lt;/p&gt;
    &lt;p&gt;The Carry Unwind: A massive hedge fund or bank (like Norinchukin) realizing that the "game is up" and closing out short-Yen positions. The size of the order suggests an entity that needed to move billions, not millions.&lt;/p&gt;
    &lt;p&gt;The subsequent price action, a sharp rally followed by "hammering back down", represents the battleground. U.S. macro funds are still trying to short the Yen (betting on U.S. economic exceptionalism and Warsh's policies), while Japanese domestic accounts are buying it. The volatility is the result of these tectonic plates grinding against each other.&lt;/p&gt;
    &lt;p&gt;The plumbing of the U.S. financial system showed signs of stress that coincided with the Japanese retreat. The Overnight Reverse Repo facility (ON RRP) saw a year-end spike to $106 billion but has since drained.&lt;/p&gt;
    &lt;p&gt;Japanese banks are typically huge participants in the U.S. repo market to fund their dollar assets. As Norinchukin and others retreat (repatriating funds to Japan), liquidity in the U.S. repo market becomes thinner. The "air pocket" in Microsoft and Gold prices was likely exacerbated by a lack of market maker depth in the repo-funded derivatives market. When market makers cannot access cheap repo funding, they widen spreads and reduce liquidity provision, leading to "gaps" in price action during sell-offs.&lt;/p&gt;
    &lt;p&gt;There have been significant moves in other currency futures as well: /6A increased 87 basis points, /6L rose 19 basis points, and /6S rose 18 basis points.&lt;/p&gt;
    &lt;p&gt;/6A (Australian Dollar): The 87 basis point rise in the Aussie Dollar is notable. AUD is often a proxy for Chinese growth and global risk sentiment. A rise here, amidst a tech crash, suggests a rotation out of U.S. assets and into commodities or Asia-Pacific currencies, further supporting the "Sell America" thesis triggered by the Greenland tariff threats.&lt;/p&gt;
    &lt;p&gt;/6L (Brazilian Real) and /6S (Swiss Franc): The rise in the Swiss Franc (a classic safe haven) aligns with the risk-off sentiment. The move in the Brazilian Real suggests that emerging markets are also seeing volatile flows as the dollar stabilizes.&lt;/p&gt;
    &lt;p&gt;Why was the VIX at 16 despite the chaos? The VIX measures implied volatility of S&amp;amp;P 500 options. Its relatively low level (16) compared to the violence in individual names (MSFT -15%, Gold -11%) indicates that the crash is a de-leveraging event, not a panic event.&lt;/p&gt;
    &lt;p&gt;In a panic, investors buy Puts on the index to protect themselves, spiking the VIX. In a de-leveraging event, investors simply sell the underlying assets (stocks, gold, crypto) to raise cash. They are not hedging; they are exiting. This explains why the VIX remained subdued while prices collapsed, the selling was orderly, algorithmic, and relentless, rather than emotional and panicked.&lt;/p&gt;
    &lt;p&gt;Skepticism about the "Greenland War" is well-founded. While the diplomatic row was real, its utility as a financial narrative was far greater than its geopolitical reality.&lt;/p&gt;
    &lt;p&gt;President Trump's threat of military force was retracted on January 21 at Davos.This "de-escalation" should theoretically have calmed markets. Instead, the volatility worsened into month-end. This confirms that the real problem wasn't Greenland; it was the re-pricing of the Yen.&lt;/p&gt;
    &lt;p&gt;The financial media loves a simple cause-and-effect narrative. "Stocks down because of War" is easy to digest. "Stocks down because the cross-currency basis swap spread widened due to BOJ minutes" is not. The "Greenland" narrative provided the perfect cover for sophisticated actors to liquidate positions in Gold and Tech under the guise of "war jitters." This allowed them to exit without sparking a broader panic about liquidity in the banking system. The focus on the Arctic masked the structural rot in the leverage complex.&lt;/p&gt;
    &lt;p&gt;The evidence suggests a covert, structural unwinding of the Yen carry trade is the primary driver of the January 2026 market chaos.&lt;/p&gt;
    &lt;p&gt;The interconnectedness of these events is undeniable. The BOJ's rate hike in December 2025 and the subsequent hawkish signaling from the Takaichi administration fundamentally altered the cost of capital for the world's largest carry trade. The "Greenland Crisis" acted as the initial volatility trigger, forcing a reduction in gross exposure. The nomination of Kevin Warsh acted as the final catalyst, shattering the "Debasement Trade" and forcing a liquidation of precious metals and crypto to cover margin calls on Yen-funded positions.&lt;/p&gt;
    &lt;p&gt;Here are some key takeaways:&lt;/p&gt;
    &lt;p&gt;The "Free Money" Era is Over: BOJ policies have fundamentally altered the global cost of capital. The flow of liquidity from Tokyo to New York has reversed.&lt;/p&gt;
    &lt;p&gt;Geopolitics as Catalyst: "Greenland" may have been the spark, but the Yen leverage was the powder keg. The tariff threats disrupted the "Atlantic Trade" narrative, forcing a repatriation of capital.&lt;/p&gt;
    &lt;p&gt;Liquidity Event: The synchronized crash of Gold, Crypto, and Tech confirms a systemic de-leveraging. The "Whale" orders in Yen futures and the breakdown of correlations are the smoking guns of a margin-driven event.&lt;/p&gt;
    &lt;p&gt;With the Japanese election on February 8 and U.S. tariffs looming, the "hammering" of the Yen is likely temporary. The structural trend is now toward repatriation. This implies lower U.S. asset prices, higher U.S. yields, and a stronger Yen over the medium term. The "mystery" of the low VIX is explained by the mechanical nature of the unwind, a controlled demolition of leverage rather than a chaotic panic.&lt;/p&gt;
    &lt;p&gt;This won't just be the big one. This could be the last one. If you've been preparing your whole life, knowing that something's coming, then this could be the thing you've been preparing for. One final opportunity to get the guys who did this.&lt;/p&gt;
    &lt;p&gt;Longing the Yen is commonly referred to as "The Widowmaker Trade" on Wall Street, because you have trillions of dollars of monopoly money working against you. The carry traders have compromised every level of our government. Their greatest vulnerability is the Yen rising in value. They will do anything to defend their positions, even if that means bringing America's economy down with them. Since recent events have made it obvious they're going to lose, we might as well fight them. Most of us probably won't make it out of this fight. But if we at least try, then there's a chance we might prosper when it's over.&lt;/p&gt;
    &lt;p&gt;The IV on OTM CME /6J futures calls is 11% which is astonishingly low. The same is true for calls on the FXY ETF. Call options have defined risk. The more Yen we control, the more its value goes up, and the more crooks on Wall Street get liquidated. The worst that can happen is you lose your monopoly money, but that's been happening anyway. Since carry traders own 10% of all U.S. treasuries, when they get liquidated they'll have to sell a lot of treasury bonds, which means that CME /UB futures and the TLT ETF will fall.&lt;/p&gt;
    &lt;p&gt;This blog is brought to you by various radicals, malcontents, and people who think the system is rigged. We're not affiliated with any organization. Nothing here constitutes financial advice. Occupy Wall Street is not your financial advisor or your lawyer. We're retail investors like you. Do your own research. Past performance does not guarantee future results. We are the 99 percent. The only solution is world revolution. Wall Street's time has finally come.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46889008</guid><pubDate>Wed, 04 Feb 2026 17:49:26 +0000</pubDate></item><item><title>Claude Code for Infrastructure</title><link>https://www.fluid.sh/</link><description>&lt;doc fingerprint="8660c4598e99ff8b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Built for where you already work&lt;/head&gt;
    &lt;head rend="h3"&gt;Sandbox Isolation&lt;/head&gt;
    &lt;p&gt;Clone VMs instantly. Test changes in isolation before touching production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context-Aware&lt;/head&gt;
    &lt;p&gt;Fluid explores your host first - OS, packages, CLI tools - then adapts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Full Audit Trail&lt;/head&gt;
    &lt;p&gt;Every command logged. Every change tracked. Review before production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ansible Playbooks&lt;/head&gt;
    &lt;p&gt;Auto-generates playbooks from sandbox work. Reproducible infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46889703</guid><pubDate>Wed, 04 Feb 2026 18:34:08 +0000</pubDate></item><item><title>Turn any website into a live, structured data feed</title><link>https://www.meter.sh/</link><description>&lt;doc fingerprint="a5767912559ea3e2"&gt;
  &lt;main&gt;
    &lt;p&gt;Stop re-scraping unchanged pages. We detect real content changes—not layout noise—so you only re-process what's new.&lt;/p&gt;
    &lt;p&gt;URLs Monitored&lt;/p&gt;
    &lt;p&gt;Jobs Completed&lt;/p&gt;
    &lt;p&gt;Try it now — extract real data in seconds&lt;/p&gt;
    &lt;p&gt;“Extract all posts with title, link, and points”&lt;/p&gt;
    &lt;p&gt;You describe it. We scrape it. You get notified if anything changes.&lt;/p&gt;
    &lt;p&gt;"Extract all posts with title, points, author"&lt;/p&gt;
    &lt;code&gt;{&lt;/code&gt;
    &lt;code&gt; "added": [...]&lt;/code&gt;
    &lt;code&gt; "updated": [...]&lt;/code&gt;
    &lt;code&gt;}&lt;/code&gt;
    &lt;p&gt;Tell us the URL and what data you need—in plain English. Our AI figures out the selectors. No CSS maintenance.&lt;/p&gt;
    &lt;p&gt;Cloudflare, PerimeterX, DataDome. Rotating proxy pool. Exponential backoff. You don't touch any of it.&lt;/p&gt;
    &lt;p&gt;Webhooks fire only when actual content changes. No noise from ads, timestamps, or layout shifts.&lt;/p&gt;
    &lt;p&gt;Stop building infrastructure. Start shipping scrapers.&lt;/p&gt;
    &lt;p&gt;No queues, no workers, no proxy pools to manage. We run the scraping infra so you don't have to.&lt;/p&gt;
    &lt;p&gt;Cloudflare, PerimeterX, DataDome—we handle them automatically. You never touch a CAPTCHA solver.&lt;/p&gt;
    &lt;p&gt;Managed proxy pool—residential and datacenter. No more 403s or IP bans.&lt;/p&gt;
    &lt;p&gt;Ignore noise from ads, timestamps, and layout shifts. Get real changes only.&lt;/p&gt;
    &lt;p&gt;Only re-embed what changed. Teams report cutting embedding costs by 95%.&lt;/p&gt;
    &lt;p&gt;No credit card • Free tier available • Setup in 5 minutes&lt;/p&gt;
    &lt;p&gt;From job boards to RAG pipelines—if you need to scrape it and know when it changes, we've got you.&lt;/p&gt;
    &lt;p&gt;Track new postings across Indeed, LinkedIn, company career pages. Get notified when relevant jobs appear.&lt;/p&gt;
    &lt;p&gt;Monitor news sites, blogs, RSS alternatives. Know when articles publish or update.&lt;/p&gt;
    &lt;p&gt;Track competitor pricing, product availability, deals. React to changes in real-time.&lt;/p&gt;
    &lt;p&gt;Monitor competitor sites for product launches, feature changes, content updates.&lt;/p&gt;
    &lt;p&gt;Keep your knowledge base current. Only re-embed changed content—cut embedding costs by up to 95%.&lt;/p&gt;
    &lt;p&gt;If you need to scrape it and know when it changes, meter can handle it.&lt;/p&gt;
    &lt;p&gt;Simple, transparent pricing for every team&lt;/p&gt;
    &lt;p&gt;Get started for free. No credit card required.&lt;/p&gt;
    &lt;p&gt;For teams that need more power.&lt;/p&gt;
    &lt;p&gt;7-day free trial&lt;/p&gt;
    &lt;p&gt;For large teams with custom needs.&lt;/p&gt;
    &lt;p&gt;Yes. We handle Cloudflare, PerimeterX, DataDome, and most major antibot systems automatically. You don't need to configure anything.&lt;/p&gt;
    &lt;p&gt;Simply provide a URL and describe what you want to extract. Our AI analyzes the page structure, identifies the best extraction method, and generates a strategy. You can refine it with feedback if needed—all without re-scraping the page.&lt;/p&gt;
    &lt;p&gt;No. We maintain a rotating proxy pool—residential and datacenter. You never see a 403 or IP ban.&lt;/p&gt;
    &lt;p&gt;We use a combination of content hashing, structural signatures, and semantic similarity. This means we ignore ads, timestamps, and layout changes—only alerting you when the actual content you care about has changed.&lt;/p&gt;
    &lt;p&gt;Once the strategy is created, meter switches to raw scraping (CSS selectors and DOM parsing) for all future scrapes—no LLM costs. You only pay for the initial generation. This gives you the speed, consistency, and cost-efficiency of traditional scraping with the intelligence of AI-powered setup. Scrapes run fast and efficiently using the saved strategy, saving you money on every run.&lt;/p&gt;
    &lt;p&gt;When we detect meaningful content changes, we POST the updated data to your webhook URL. You can then update your vector database with only the changed content. If nothing meaningful changed, you won't get a webhook—saving you processing time and costs.&lt;/p&gt;
    &lt;p&gt;We handle JavaScript-heavy sites, dynamic content, paginated data, and more. Our system automatically chooses the best extraction method—whether that's API calls or CSS selectors.&lt;/p&gt;
    &lt;p&gt;Free tier includes 10 strategies with basic scraping. Pro is $29/month with 60 strategies, unlimited jobs, hourly monitoring, webhooks, and priority support. Enterprise plans with custom pricing include antibot bypass, proxy rotation, and custom integrations. Start with a 7-day free trial—no credit card required.&lt;/p&gt;
    &lt;p&gt;Still have questions?&lt;/p&gt;
    &lt;p&gt;Join developers shipping scraping projects in days instead of months. Start free—no credit card required.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46890261</guid><pubDate>Wed, 04 Feb 2026 19:12:28 +0000</pubDate></item><item><title>Tell HN: Another round of Zendesk email spam</title><link>https://news.ycombinator.com/item?id=46890418</link><description>&lt;doc fingerprint="eea500d3dec2bbcd"&gt;
  &lt;main&gt;
    &lt;p&gt;If your email service supports Sieve scripts (for example, Fastmail or Proton Mail), you can use this filter [1] that I made. It's very aggressive and will block all emails that originate from Zendesk, so you'll need to disable it whenever you're actually expecting mail from Zendesk.&lt;/p&gt;
    &lt;p&gt;Glad I'm not the only one. It seems to use {popular website without tld}@example.com as a pattern, so I'm getting a lot via my catch all address even if I haven't used the specific inbox yet.&lt;/p&gt;
    &lt;p&gt;It seems to have started two weeks ago. A spammer realized that one can find a Zendesk‐based help forum, open a new ticket without an account, fill the ticket with spam URLs, and put an email address scraped from GitHub commit logs in the author email field. Zendesk would “helpfully” send the “author” the contents of the ticket, becoming in effect an open relay for spam emails. Two weeks ago is when the spammer started the attack in earnest: I received hundreds of these spam emails, typically one or two per Zendesk‐hosted help forum, sent to email addresses that I’ve only ever used on GitHub. It was discussed a bit on HN: https://news.ycombinator.com/item?id=46685768&lt;/p&gt;
    &lt;p&gt;Since then, Zendesk seems to have strengthened their system so that opening a ticket requires account activation first. Leading to today, when I’ve received thousands of signup attempt emails (again, typically one or two per Zendesk‐hosted forum). This is way more emails than I got last time. I hypothesize that the spammer is doing a “last gasp” attack: now that Zendesk has burned the exploit by no longer including the ticket text in the emails, the spammer is trying every Zendesk site it knows in hopes that some of them are slow to update and still forward the ticket text to the victim.&lt;/p&gt;
    &lt;p&gt;Thank you for letting us know, got a bunch of those in the last two hours, like one each five minutes, but it seems they've stopped (at least for now).&lt;/p&gt;
    &lt;p&gt;Zendesk’s mailserver reputation has got to be extremely poor by now. I think they will have trouble with deliverability after this is over. Got about 50 of these today and nearly all of them were categorized as spam before they made it to the inbox despite being nominally “legit”&lt;/p&gt;
    &lt;p&gt;For a company utterly dependent on email, Zendesk came across to me as very naive about email sending.&lt;/p&gt;
    &lt;p&gt;I did a Zendesk integration shortly after working on a general overhaul of our email at a previous company. The overhaul involved separating out our different types (transactional, marketing, support, etc), and then implementing best practices on deliverability for each of them. Not your day-one email setup, but we were still a small company.&lt;/p&gt;
    &lt;p&gt;The comparison to Zendesk's approach was astounding. Assuming you don't want to use a Zendesk address (we didn't, customers thought it was dodgy), the email setup they let you do was bad, and their support folks had no idea about any of the details. DKIM, SPF, etc, was all alien to them. Ironically they had pretty bad support in general.&lt;/p&gt;
    &lt;p&gt;Not necessarily, our support team kinda loved it. I used the interfaces and it was pretty good software in many ways. They just didn't seem to be very capable when it came to medium complexity email setups. Many of their setup guides literally tell you to log into support address Gmail and set up a forwarding rule to send everything to Zendesk.&lt;/p&gt;
    &lt;p&gt;I suspect the issue is that we weren't paying enough. We had maybe 10 seats. I bet if you're buying 1000 seats a bunch of Zendesk engineers turn up and configure everything for you, but with the robust email setup needing that engineering time on their side to configure... so I guess in that way it may be Enterprise shitware.&lt;/p&gt;
    &lt;p&gt;I've been getting some of these these to my wildcard domain - I've had sign-up messages sent to diddy@&amp;lt;domain&amp;gt; and epstein@&amp;lt;domain&amp;gt;, which is... odd. And no, I can't say I've ever used those addresses.&lt;/p&gt;
    &lt;p&gt;I'm getting emails titled "Activate account for ...", and addressed to random names of web services at my domain (e.g. reddit@example.org). Also Twitch-related names like pog, kekw and xqc.&lt;/p&gt;
    &lt;p&gt;Also super annoying are crypto scams sent from an Italian ISP's (tiscali.it, shame on you) email service, even though I tried to contact the ISP, but that's unrelated to this.&lt;/p&gt;
    &lt;p&gt;Received 15+ in 10mins on a public email (dropbox, soundcloud, gitlab, tidelift etc). Then just started hitting handles on the domain ( diddy@, epstein@ ). Just placing an aggressive block for "Activate account" and "zendesk" in content for now&lt;/p&gt;
    &lt;p&gt;Huh. I thought this was targeted to me in particular, because it started coming up with new aliases at my Firefox Relay subdomain, and then only once I started blocking them it started using plus-addressing on my gmail. Annoying.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46890418</guid><pubDate>Wed, 04 Feb 2026 19:26:08 +0000</pubDate></item><item><title>Technocracy 2.0</title><link>https://brooklynrail.org/2026/02/field-notes/technocracy-2-0/</link><description>&lt;doc fingerprint="bfe49975aa2306aa"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Technocracy 2.0&lt;/head&gt;
    &lt;p&gt;Norman Saunders, Cover for the Technocrats’s Magazine, 1933.&lt;/p&gt;
    &lt;p&gt;Word count: 4552&lt;/p&gt;
    &lt;p&gt;Paragraphs: 61&lt;/p&gt;
    &lt;p&gt;In September 2025, President Trump held a dinner with over two dozen leaders from Silicon Valley. Easily one of the wealthiest gatherings ever held in the White House, the guest list included tech CEOs, venture capitalists, and administration officials. The topic of the dinner was investment and state alignment, as every individual present jockeyed for Trump’s approval. Government contracts and less regulatory oversight were the prizes for those companies which promised the most funds. Mark Zuckerberg, when asked, abruptly made up an investment number to get in Trump’s good graces, “600 billion dollars.” Only to be caught on a hot mic moments later saying, “I’m sorry, I wasn’t ready. I wasn’t sure what number you wanted to go with.”1&lt;/p&gt;
    &lt;p&gt;There has never before been such a close association between tech capital and the American state. Previously less MAGA-friendly, tech industry leaders have virtually all turned toward Trump since his 2024 victory, eager to gain favors and position themselves as kingmakers in the market. In the global race for AI dominance, the Trump administration views Silicon Valley as an extension of itself, from Palantir working openly with the US military for surveillance and defense systems to OpenAI seeking government aid guarantees on its debt.&lt;/p&gt;
    &lt;p&gt;Buoyed by previously unthinkable stock valuations, tech capital and political power have formed a symbiotic relationship. It is an arrangement that has been in the works for some time.&lt;/p&gt;
    &lt;p&gt;As Alex Karp, the CEO of Palantir, argued in his book The Technological Republic (2025), for too long has it been “taboo” in Silicon Valley to openly want to affect politics. The only lasting future for the United States, he says outright, is the merging of tech and the state. Karp cites the Manhattan Project as a precedent, a model that is now to be applied to virtually every lever of power.&lt;/p&gt;
    &lt;p&gt;While Karp and other technocrats like him may not know it themselves, this idea has deep roots in American political culture, going back a century. Technocracy, or “rule by technical experts,” once had a strong, cult-like following in the United States. In the throes of the Great Depression, a mass movement for technocracy quickly became the “most discussed topic in America.”2 Elon Musk’s grandfather was once even part of its Canadian branch. The movement was among the first to organize itself around the idea that technology, not workers or nations, is the main revolutionary agent of history. The ultimate aim was to establish an anti-democratic state run by a board of technical experts.&lt;/p&gt;
    &lt;p&gt;The Origins of Technocracy&lt;/p&gt;
    &lt;p&gt;At the turn of the twentieth century, the United States was going through the pains of its industrial transformation. Cities were booming in population, and politics was rife with discord over what would become of American life. New ideas began to circulate over who would helm this path forward. Some were moved by the plight of workers, while others found a cause in battling corruption. But above all, this generation desired reform. They were loosely called the Progressive movement. As one of its leaders, Herbert Croly, argued, now was the time for a “New Nationalism.” Yet few agreed on what the nation’s needs exactly were.&lt;/p&gt;
    &lt;p&gt;A growing subset of Progressives found their cause within science and technology. Because it had made the Industrial Revolution possible, it was easy to assume its experts should also lead. These Progressives believed society was growing too complex to be left to insular self-interest and politics. Intellectuals like Walter Lippmann, Walter Weyl, Louis Brandeis, Charles McCarthy, and Frederic Howe put forward arguments that any future administration of experts needed a clear separation from politics.3&lt;/p&gt;
    &lt;p&gt;This was the seed of what would later become technocratic thinking: the idea of a state of appointed experts. Yet, it was not only politics that was increasingly viewed with suspicion. Businessmen, too, were becoming suspect because of their narrow interests. This argument was most famously made by economist Thorstein Veblen, who was a leading influence on the early technocrats. Although initially more sympathetic to industrial workers, by the 1910s Veblen shifted toward a belief in rule by engineers. A critic of capitalism himself, he viewed business managers as incapable of understanding the system they were handling.4 They were poorly educated in the “industrial arts,” and distorted what should be industrial society’s priorities. These “ignorant businessmen with an eye single to maximum profits” had to be replaced by a new class of people, the technicians.5&lt;/p&gt;
    &lt;p&gt;By 1919, Veblen began publishing a series of magazine essays that would be compiled in The Engineers and the Price System (1921). A “soviet of technicians” had to be established where engineers, not workers, would take over from the capitalists, he argued. This new philosophy would be given a name that year—“technocracy”—and then give rise to a short-lived organization named the Technical Alliance. Veblen was one of its founding members and its intellectual anchor.&lt;/p&gt;
    &lt;p&gt;The ideological coordinates of these early days of technocracy were confused. The Technical Alliance brought together a group of very different individuals, united only by the belief that technology now dominated social progress. The alliance worked on research with the radical left-wing union the Industrial Workers of the World (IWW), but many of its members remained non-committed politically. While fellow travelers of left causes, there was no focus on the proletariat, class struggle, or workplace organizing. Their only point of agreement was that capitalism was inefficient and holding back industrial society’s potential.&lt;/p&gt;
    &lt;p&gt;Headquartered in the New School in New York City, where Veblen was a faculty member, the alliance effectively had no strategy for power. It was merely an organizing committee with a variety of titled members from different industries: chemist, educator, physicist, architect, forester, statistician, and so on—an embryo, Veblen argued, of a future “soviet of technicians” that was in the process of proving itself. Their main focus was simply research papers, as they produced reports on waste in the American capitalist system.&lt;/p&gt;
    &lt;p&gt;However, Veblen himself was in poor health and in no position to lead. Incubating in the Technical Alliance was the future face of the technocracy movement, Howard Scott, who would take Veblen’s ideas and transform them into a mass movement by the 1930s. His career would play out like something out of a melodrama: a parabolic rise, sudden fame and delusions of grandeur, and finally a speedy decline into irrelevance.&lt;/p&gt;
    &lt;p&gt;Scott himself was something of an eccentric with an air of mystery about him. In fact, he had few scientific credentials. He hid his past, and was known for exploding angrily when asked about it.6 But what he lacked in qualifications, he had in charisma, sheer will, and a domineering presence. Unlike Veblen, who was more rational, Scott was caught up in the dream of technocracy as if it were his personal religion and north star. He stylized himself a “bohemian engineer,” an adventurer and flaneur, preaching technocracy to all who would listen in Greenwich Village, as if it would someday be possible to create a new Garden of Eden.7&lt;/p&gt;
    &lt;p&gt;By 1920, Scott was already leaning into a polemical style that would define his rise. In “The Scourge of Politics in a Land of Manna”, an article published that year in the IWW periodical, he demeans politics as centered on petty emotion and subjective reality. If only, he speculated, there could be “a political party based, not upon theory but upon actual facts, the situation would not be such a hopeless one.”8&lt;/p&gt;
    &lt;p&gt;The Rise (and Fall) of the Original Technocracy Movement&lt;/p&gt;
    &lt;p&gt;While the Technical Alliance did produce some preliminary work on capitalist waste, it disbanded by 1921. Amid the economic boom of the 1920s, the group failed to resonate with the public and politicians. There was little need for rule by technicians as long as capitalism continued to boom. The popularity of technocracy only returned with the bust of 1929.&lt;/p&gt;
    &lt;p&gt;The Great Depression gave the technocrats another opening. In the early 1930s, Scott met geophysicist Marion King Hubbert, who pushed him to reestablish the old Technical Alliance. At the time, Hubbert was working at Columbia University. The group reconvened with the blessing of mechanical engineer Walter Rautenstrauch, the founder of the university’s Department of Industrial Engineering. Columbia later denied any connection to the group, but their early press releases were all done through university channels.9&lt;/p&gt;
    &lt;p&gt;The new organization, called the Committee on Technocracy, took up where the Technical Alliance left off by producing research reports on capitalist waste. Except this time, the excitement spread to the papers. By August 1932, the group was receiving hundreds of eager letters from universities, economists, labor organizations, and civic leaders.10 Scott became an overnight celebrity, dining with industrialists, bankers, newspaper editors, and future New Deal administrators.&lt;/p&gt;
    &lt;p&gt;The reports summarized by Scott generated intense public interest. The New York Times in particular helped elevate the profile of the technocrats. In one piece, the Times reported that the committee had convincingly made its case that the system would collapse unless the energy expended by production was radically redistributed. By the end of 1932, the hype reached something of a frenzy. “The gospel of Technocracy is spreading throughout schools, universities, and churches,” published one newspaper.11 Wall Street was said to be following with “intense and worried interest,” and it was said that even the Vatican was closely monitoring developments.12 As the editor of Harper’s, Frederick Lewis Allen, wrote, Scott “was now pursued by interviewers ready to hang upon his lightest word.”13&lt;/p&gt;
    &lt;p&gt;Part of the appeal of technocracy was that it was viewed as a genuinely American idea. As The Nation reported in September, 1932, “neither socialism, communism, nor fascism is equipped to do this job in a society as highly technical as America today.”14 The editors argued that technocracy was a revolutionary philosophy uniquely made by and for America. This imbued the movement with nationalistic fervor, since it was believed that its adoption would also establish the United States as the leader of scientific progress.&lt;/p&gt;
    &lt;p&gt;However, by the start of 1933 the situation already began to deteriorate. In January of that year, the Committee on Technocracy split over infighting, and Scott formed his own group, Technocracy Incorporated. Scott himself was also growing more apocalyptic in his messaging. In Harper’s magazine, Scott wrote of a looming catastrophe. “A crisis in the history of American civilization is at hand. The nation stands at the threshold of what is simultaneously opportunity and disaster… are we going to set about it before it is too late?”15&lt;/p&gt;
    &lt;p&gt;But what exactly it was became suspect. Amid criticism, the amicable relationship between the New York Times and the technocracy movement broke down. Scott was facing intense pressure for his lack of credentials and so-called “consultancy,” derided as a “crackpot,” and reporters wrote that his “cult” was “now on the wane.”16 Retrospectives on the movement’s decline have linked the fall to Scott’s disastrous radio address to critics in late January, 1933.&lt;/p&gt;
    &lt;p&gt;As the New York Times reported, four hundred of New York’s elites—capitalists, bankers, economists, and artists—came to hear Scott defend himself at Hotel Pierre in New York City.17 Rather than silence his detractors, Scott simply asserted, “we don’t have to answer our critics: time will tell.”18 As one writer spoke of a year later, the delivery was anti-climactic and ultimately the movement’s deathblow. It was “a jumble of unfinished and half-baked sentences. It was all over.”19&lt;/p&gt;
    &lt;p&gt;The ideas, however, persisted. Historian Arthur Schlesinger, Jr. has argued that it remained thereafter as a “California cult.”20 The movement reinvented itself as a mass organization with fascist-adjacent aesthetics. Its supporters wore gray-toned clothing, drove matching gray cars, and gave militaristic Roman salutes. Pamphlets like Technocracy Digest began to circulate, with local chapters springing up across the country. Its followers were openly antagonistic toward the state, which in turn banned them in Canada in 1940.&lt;/p&gt;
    &lt;p&gt;Although press interest fell to a new low by the late 1930s, the movement itself still boasted hundreds of thousands of members. Rather than scientists and engineers, however, artists and writers were drawn to Scott’s dream of technocracy. Hugo Gernsback—who coined the term “science fiction”—wrote for the movement’s journal in 1933. And writer Ray Bradbury affirmed that technocracy was “all the hopes and dreams of science fiction.”21 Yet, it had little to do with science by this point. Scott increasingly relied on spectacle to draw attention, with large rallies held at the Hollywood Bowl in Los Angeles. With the outbreak of World War II, he attempted to integrate his followers into the war effort with diminishing success.&lt;/p&gt;
    &lt;p&gt;By the succeeding decade, the movement had all but vanished. As historian Henry Elsner, Jr. recounts, one comedy film from the 1950s included a punchline about how it had all been forgotten. “Daddy, what’s a technocracy?” asked a young girl, to laughter from the audience. It was a little detail to show that the film took place in 1932, during the peak of technocracy hysteria, a craze that was now like a lost fever dream.22&lt;/p&gt;
    &lt;p&gt;Core Beliefs&lt;/p&gt;
    &lt;p&gt;In its official channels, Technocracy Incorporated defined itself as “science applied to the social order.” This would become its slogan in the 1930s. For Scott, the philosophy behind it was simple: “Engineers and mechanics created this civilization, so they will eventually dominate it.”23&lt;/p&gt;
    &lt;p&gt;Yet the technocracy movement was always scattered politically, and even its slogan was open-ended. Its ideology was all over the map. Technocracy Incorporated organized mass rallies while appealing to individual reason. It was imperialistic, and believed its “Technate” should rule over all of North, Central, and parts of South America along with the Caribbean. Fiercely anti-capitalist, it sought to replace money with energy vouchers and production quotas. Howard Scott himself liked to lean into the ideological confusion. “As far as technocracy's ideas are concerned,” he joked, “we’re so far left that we make communism look bourgeois.”24&lt;/p&gt;
    &lt;p&gt;When speaking of history and ideas, there is always a temptation to go deeper and deeper into the historical record to find some precise origin. Sociologist Daniel Bell, for example, has argued that the first technocrat proper was Henri de Saint-Simon, with his rational religion of humanity. But while the ideology of technocracy can be traced to the nineteenth century, the idea was made real by the modern Industrial Revolution. Only then did technocracy have any semblance of a movement rooted in material reality. Veblen is therefore the closest to technocracy’s ideological founding father.&lt;/p&gt;
    &lt;p&gt;If technocracy could be identified with a single idea, it is the belief that society can and should be counted, measured, and directed mathematically. As the ABC of Technocracy (1933) argues, “the working of our great social machine is susceptible to measurement.” A later video produced by the movement put it more ominously: “All phenomena involved in the functional operation of a social mechanism are measurable.”25&lt;/p&gt;
    &lt;p&gt;Technocracy viewed its dream as a form of social engineering with society like a biological organism that can be behaviorally conditioned. In a mockup of the technocratic state, Technocracy Incorporated dedicated an entire department to “social relations.” Every facet of society was to be quantified, processed, and optimized for efficiency. Each citizen would be surveilled in their habits to ensure the smooth processing for the greater industrial organization. A new seven-day work week was proposed to allow for non-stop production.&lt;/p&gt;
    &lt;p&gt;From this principle, a vision of technocratic government emerged. First, all industries would be centralized into a few large-scale enterprises owned by the state. Each would be administrated by a board of technical experts. Second, the state would be exclusively bureaucratic and non-democratic. Third, each citizen would be granted a universal basic income based on production quotas. Fourth, prices and money would be phased out and replaced by energy vouchers. And last, all politics and parties would be abolished. There would be no officials other than those experts who lead industry.&lt;/p&gt;
    &lt;p&gt;Yet, how it would actually get to governing was hardly clear. Despite being deeply elitist, Technocracy Incorporated adopted a mass character. Scott peddled in fantasies, and so he imbued the movement with a folk-like ethos through popular rallies, local chapters, and pamphleteering. Because politics in the 1930s was populist and mass-led, technocrats understood the pull of the crowd. They were also competing with the New Deal. Yet, under the surface, technocrats understood their system was more about creating a new priestly class of experts than anything representative. These technicians were to lead society because they were biologically superior. As one 1937 essay in Technocracy Digest put it, “upon biologic fact, theories of democracy go to pieces.”26&lt;/p&gt;
    &lt;p&gt;The writer Harold Loeb summarized the core belief behind technocracy as follows: “Technology is the revolutionary agent of our period.”27 In a time of intense class and national conflicts, the technocrats of the 1930s did not view workers or nations as holding the future. They fashioned themselves as revolutionaries working in the interests of a different subject: technological progress. And like all revolutionary movements, they had a deep messianic belief that only they possessed the capabilities to save society from coming destruction.&lt;/p&gt;
    &lt;p&gt;In truth, the technocrats had overestimated the power of science and industry in their time. Technological systems were not developed enough then to measure and direct society mathematically. Nor did the original movement possess the expertise to even pursue this dream, because few actual engineers and scientists filled its ranks. But today, the appeal of technocracy finds itself in a different world. We are incessantly surveilled, quantified, and made malleable through our behaviors online.&lt;/p&gt;
    &lt;p&gt;Technocrats no longer need to persuade the masses to enact their vision. Owning the lion’s share of the world’s capital and influence, they can effectively work in silence and act as if beholden to no one. In the twenty-first century, core ideas of technocracy’s old dream have been revived and given new life.&lt;/p&gt;
    &lt;p&gt;Technocracy Today&lt;/p&gt;
    &lt;p&gt;In the twenty-first century, the definition of technocracy has expanded well beyond the scope of the short-lived 1930s movement. It has been used to describe the EU’s European Commission, China, Singapore, and Italy during the COVID pandemic. In everyday conversation, “technocrat” is often used as a catch-all term for “non-political expert.”&lt;/p&gt;
    &lt;p&gt;Given historically low institutional trust, the appointment of non-political experts has become a common strategy by political parties to repair their image. Political scientist Sheri Berman has argued that, in times of popular anger, few would make the case for oligarchy. Rule by an unelected technocracy, however, is a less offensive stand-in.28 Appealing to technocrats provides the state with an attractive means of shoring up legitimacy during crisis. The Trump administration’s recent “Gaza peace plan,” for example, is rife with technocratic language—stipulating that the territory will be governed under the “temporary transitional governance of a technocratic, apolitical Palestinian committee.”&lt;/p&gt;
    &lt;p&gt;But while definitions of technocracy today cast a wide net, the original idea is finding a second life in Silicon Valley. It is among the tech elite that technocracy takes on actual, ideological substance that echos the movement of old.&lt;/p&gt;
    &lt;p&gt;In 2009, tech venture capitalist Peter Thiel outlined his new belief on politics in an op-ed for the Cato Institute. He still called himself a libertarian, but wrote that he no longer believed democracy and freedom to be compatible.29 Politics, he argued, had failed in providing new horizons. Instead, Thiel was convinced that tech entrepreneurs now had to escape politics. They had to get away from the “unthinking demos that guides so-called ‘social democracy.’” To put it differently: he had come to the conclusion that actual people were in the way of the technological future.&lt;/p&gt;
    &lt;p&gt;For Thiel, technology was in a deadly race with politics. This, in his view, had suffocated innovation and led to widespread stagnation. He speculated that the future may depend on “the effort of a single person who builds or propagates the machinery of freedom.” This would be the only way to make the world safe again for capitalism.&lt;/p&gt;
    &lt;p&gt;Thiel’s thoughts would go on to be instructive for tech elites over a decade later. Many of his allies now occupy top positions within politics and Silicon Valley, like OpenAI CEO Sam Altman, neoreactionary thinker Curtis Yarvin, and even the current vice president, JD Vance. In Thiel’s worldview, technological progress is simply too important to be left to democratic society. His beliefs echo the technocrats of almost a century ago, who viewed themselves as in the wings, ready to take over when democracy collapsed. As their 1933 statement read, “technocracy stands ready with a plan to salvage American civilization, if and when democracy… can no longer cope with the inherent disruptive forces.”30&lt;/p&gt;
    &lt;p&gt;However, unlike the movement of old, twenty-first century technocrats do not wish to replace capitalism. Instead, they see themselves as the chosen elite destined to save and steer it. And within this belief, strangely enough, even capitalist competition itself is viewed with suspicion. In a 2014 op-ed for the Wall Street Journal, Thiel calls economists’ obsession with competition a “relic of history.”31 Monopolies drive progress, he argues, and they are a natural consequence of the accumulation of capital. This is his self-described “idée fixe”—an endpoint he is “obsessed with”—and he co-hosted an event with Altman on the subject in 2014. The reverence toward monopolies closely resembles the old technocracy movement’s desire for top-down centralization of industry.&lt;/p&gt;
    &lt;p&gt;Such ideas have been taken to their logical conclusion by “Dark Enlightenment” thinkers, especially Curtis Yarvin. A techno-monarchist, Yarvin is a believer in transforming the state into a corporation led by a CEO-like monarch with a court of (presumably) Silicon Valley aristocrats. Yarvin is close to Thiel, calling him “fully enlightened” on this question.32 As a nod of agreement, Yarvin’s start-up was also funded by Thiel’s VC firm. The “Dark Enlightenment” circle includes fellow travelers like billionaire Marc Andreessen. In 2023, he argued in his venture capital firm’s manifesto that concerns over inequality, environmental degradation, and social atomization are the complaints of “decels” (decelerationists), who are irrationally against technological progress.33&lt;/p&gt;
    &lt;p&gt;Such elitist, anti-democratic, and monopolist views circulate within a complex web of ideologies that animate Silicon Valley. They loosely fall under the umbrella term of “Rationalism,” but also overlap with AI accelerationism and effective altruism (EA). EA found its voice on message boards like LessWrong, where users shared utilitarian solutions to long-tail civilizational risks. In what would be the movement’s deathblow, EA’s most famous proponent, crypto mogul Sam Bankman-Fried, viewed his embezzlement of customer funds as preventing long-term “risks.” He rationalized that saving his over-leveraged venture capitalist firm with others’ money, without their permission, was socially optimal.&lt;/p&gt;
    &lt;p&gt;This self-inflicted wound to EA ultimately exposed the limitations of Rationalism generally. As Omar Hernandez observed in “Capitalism, Rationality, and Steven Pinker”, the Rationality community is the philosophical ecosystem of twenty-first-century technocrats.34 What binds them together is the goal of “removing cognitive biases,” as if parsing through bad code in the social body, done through quantitative assessments of the world. This quantitative view of the social world persists among tech elites. It is their main leverage for accruing power: the endless collection of personal data. On this basis, they lobby the state to argue that they alone possess the toolkit for social prediction. Some companies are shamelessly open about pushing this case—like Palantir, whose name is taken from an all-seeing orb that can predict the future in Lord of the Rings. The dream of today’s technocrats is coached in the same language as almost a century ago: that all social processes are measurable and can therefore be efficiently optimized and predicted.&lt;/p&gt;
    &lt;p&gt;Among all tech companies, Palantir has used this leverage most effectively. It represents the leading strategy for power nowadays among tech elites, namely integration with the state through data sharing and lucrative contracts. Palantir now provides data for Immigration and Customs Enforcement (ICE), intelligence for lethal force on the Ukrainian battlefield, machine learning for the Department of Defense, and citizen databases for the IRS and other government agencies, among other functions. As an integral part of the American state, it is one of the few contractors immune to the Trump administration’s cost-cutting. Alex Karp, its CEO, justifies his company’s mission in explicitly Americanist terms, arguing that the fusing of state power with Palantir and other tech giants is necessary to—as Co-Founder Joe Lonsdale put it—“save Western civilization.” Other companies, like OpenAI, Meta, and Google, have taken cues from his strategy of state alignment. In December 2025, the US Department of War announced that it will be integrating with Google’s Gemini AI platform.35&lt;/p&gt;
    &lt;p&gt;But not all technocrats are pleased by this statist turn. Elon Musk, for one, tried to enter politics and then left abruptly without achieving any of his hyperbolic cost-cutting promises. He now believes only AI can save America.36 Others take an equally cynical view, and echo Thiel’s original concern that politics is incompatible with technological progress. The proposed alternative path forward is instead to exit and build power on the margins, ready to swallow a weakened American state when the time comes. This case was first made by tech investor Balaji S. Srinivasan in a 2013 speech at startup accelerator Y Combinator. America was “outdated and obsolescent” like Microsoft, he argued, and it was time for Silicon Valley to secede.37 The speech won roaring applause from the audience.&lt;/p&gt;
    &lt;p&gt;More recently, Srinivasan has been proselytizing a new belief that he likens to “tech Zionism.”38 Tech elites had to exit democracy and settle sovereign territories of their own. These peripheral islands of tech utopianism would in time unite, eventually accruing enough capital and power to challenge the nation-state. They would oversee a new tribe of loyal citizens dressed in matching gray. “If you see another gray on the street… you do a nod. You’re a fellow gray.”39&lt;/p&gt;
    &lt;p&gt;This aesthetic choice, it must be said, is ridiculously on the nose: gray was also the color palette of the 1930s technocracy movement. Srinivasan’s ideas could easily be ignored if he was a pariah in the tech world, but he is not; the book explaining his worldview, The Network State: How to Start a New Country (2022) is quite popular. Andreessen has praised Srinivasan, in Rationalist terms, saying he “has the highest rate of output per minute of good new ideas of anybody I’ve ever met.”40&lt;/p&gt;
    &lt;p&gt;Technocracy today may have different strains of thought, but they ultimately converge on a pseudo-religious millenarian view of the world. What they disagree on is how to best achieve it. Given that technology is again being put forward as the sole mover of history, it is unsurprising that its wealthy beneficiaries view themselves as the rightful heirs of the coming social transformation. The belief is not so far removed from Howard Scott’s—who argued that “engineers and mechanics created this civilization, so they will eventually dominate it.” So, too, do today’s technocrats view themselves as the creators of this current civilization, and hence see it as their right to dominate it.&lt;/p&gt;
    &lt;p&gt;Twenty-first-century tech elites may not even know of the old technocracy movement, but have revived many of its ideas. Gone are the pomp and spectacle of rallies, however, since popular persuasion is no longer needed. Equally missing are the critiques of capitalism, since capital accumulation now benefits the new technocrats. Instead, we are left with pure technocratic technique, organizing numbers and data, to give substance to the old dream. Whatever its fantasies, they are nonetheless still tied to economic realities, and will live only so long as tech stocks soar.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Karissa Bell, “Zuckerberg Caught on Hot Mic Telling Trump ‘I Wasn’t Sure’ How Much to Promise to Spend on AI in the US,” Engadget, September 6, 2025, https://www.engadget.com/social-media/zuckerberg-caught-on-hot-mic-telling-trump-i-wasnt-sure-how-much-to-promise-to-spend-on-ai-in-the-us-211915608.html.&lt;/item&gt;
      &lt;item&gt;David Adair. The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement (MA diss., Sir George Williams University, 1967), 29.&lt;/item&gt;
      &lt;item&gt;William E. Akin. Technocracy and the American Dream: The Technocrat Movement, 1900-1941 (University of California Press, 1977), 3.&lt;/item&gt;
      &lt;item&gt;Andres Esmark. The New Technocracy (Bristol University Press, 2020), pg. 30.&lt;/item&gt;
      &lt;item&gt;Akin, Technocracy and the American Dream: The Technocrat Movement: 1900-1941, 22.&lt;/item&gt;
      &lt;item&gt;Akin, Technocracy and the American Dream: The Technocrat Movement: 1900-1941, 28.&lt;/item&gt;
      &lt;item&gt;Akin, Technocracy and the American Dream: The Technocrat Movement: 1900-1941, 28.&lt;/item&gt;
      &lt;item&gt;Henry Elsner, Jr. Messianic Scientism: Technocracy, 1919-1960 (D.Phil diss., University of Michigan, 1962), pg. 19-20.&lt;/item&gt;
      &lt;item&gt;Elsner Jr., Messianic Scientism: Technocracy, 1919-1960, 29.&lt;/item&gt;
      &lt;item&gt;Elsner Jr., Messianic Scientism: Technocracy, 1919-1960, 30.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 26.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 26.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 26.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 34.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 35.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 28.&lt;/item&gt;
      &lt;item&gt;“400 LEADERS HEAR TECHNOCRACY PLEA; Howard Scott Leaves Bankers and Industrialists Skeptical on New Solution. BEWILDERING, ONE SAYS Another Resents Claim That Methods Are Scientific -- Most Reserve Comment. 400 LEADERS HEAR TECHNOCRACY PLEA,” The New York Times, January 14, 1933, https://www.nytimes.com/1933/01/14/archives/400-leaders-hear-technocracy-plea-howard-scott-leaves-bankers-and.html&lt;/item&gt;
      &lt;item&gt;Ibid.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 36.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 31.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 22.&lt;/item&gt;
      &lt;item&gt;Elsner Jr., Messianic Scientism: Technocracy, 1919–1960, 1.&lt;/item&gt;
      &lt;item&gt;Ira Basen, “These Techno-Utopians Wanted to Put Scientists in Charge of Government,” CBCnews, June 28, 2021, https://newsinteractives.cbc.ca/longform/technocracy-incorporated-elon-musk/.&lt;/item&gt;
      &lt;item&gt;Ibid.&lt;/item&gt;
      &lt;item&gt;Technocracy Full Presentation. Youtube video, 59:08. Posted April 14, 2013. https://www.youtube.com/watch?v=5bqPXqYWHlE.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 73.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 13.&lt;/item&gt;
      &lt;item&gt;Sheri Berman, “Against the Technocrats,” Dissent Magazine, January 24, 2018, https://dissentmagazine.org/article/against-technocrats-liberal-democracy-history/.&lt;/item&gt;
      &lt;item&gt;Peter Thiel, “The Education of a Libertarian,” Cato Unbound, April 30, 2014, https://www.cato-unbound.org/2009/04/13/peter-thiel/education-libertarian/.&lt;/item&gt;
      &lt;item&gt;Adair, The Technocrats 1919–1967: A Case Study of Conflict and Change in a Social Movement, 46.&lt;/item&gt;
      &lt;item&gt;Peter Thiel, “Competition Is for Losers,” The Wall Street Journal, September 12, 2014, https://www.wsj.com/articles/peter-thiel-competition-is-for-losers-1410535536?gaa_at=eafs&amp;amp;gaa_n=AWEtsqczFWbRXXEXh__Yg9yojZP2vxvg2s1IApENdwLlnSDYXdwyIHXFIxJ8gG7dfg%3D%3D&amp;amp;gaa_ts=69715169&amp;amp;gaa_sig=9MuoLOyzRKiDeb4syDhwM-RDt7xWpRMl7LVqOhCT4JayF9M3qo_810gMv5TeBV45aA1xyCseAihIi12mClWeFw%3D%3D.&lt;/item&gt;
      &lt;item&gt;David Marchese, “The Interview: Curtis Yarvin Says Democracy Is Done. Powerful Conservatives Are Listening.,” The New York Times, January 18, 2025, https://www.nytimes.com/2025/01/18/magazine/curtis-yarvin-interview.html.&lt;/item&gt;
      &lt;item&gt;Marc Andreessen, “The Techno-Optimist Manifesto,” a16z, April 24, 2024, https://a16z.com/the-techno-optimist-manifesto/.&lt;/item&gt;
      &lt;item&gt;Omar Hernandez, “Capitalism, Rationality, and Steven Pinker,” The Brooklyn Rail December 8, 2021, https://brooklynrail.org/2021/12/field-notes/Capitalism-Rationality-and-Steven-Pinker/.&lt;/item&gt;
      &lt;item&gt;“The War Department Unleashes AI on New Genai.Mil Platform,” US Department of War, December 9, 2025, https://www.war.gov/News/Releases/Release/Article/4354916/the-war-department-unleashes-ai-on-new-genaimil-platform/.&lt;/item&gt;
      &lt;item&gt;Eric Revell, “Musk Says AI and Robotics Are ‘only’ Things That Can Solve Massive US Debt Crisis,” Fox Business, December 1, 2025, https://www.foxbusiness.com/economy/musk-says-ai-robotics-only-things-can-solve-massive-us-debt-crisis.&lt;/item&gt;
      &lt;item&gt;Anand Giridharadas, “Silicon Valley Roused by Secession Call,” The New York Times, October 28, 2013, https://www.nytimes.com/2013/10/29/us/silicon-valley-roused-by-secession-call.html.&lt;/item&gt;
      &lt;item&gt;Gil Duran, “The Tech Baron Seeking to Purge San Francisco of ‘Blues,’” The New Republic, July 17, 2024, https://newrepublic.com/article/180487/balaji-srinivasan-network-state-plutocrat.&lt;/item&gt;
      &lt;item&gt;Ibid.&lt;/item&gt;
      &lt;item&gt;Ibid.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Anton Cebalo is a writer and historian based in Brooklyn, NY. He publishes Novum Newsletter on Substack, where he writes on culture, politics, and history.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46890899</guid><pubDate>Wed, 04 Feb 2026 20:06:00 +0000</pubDate></item><item><title>The Codex app illustrates the shift left of IDEs and coding GUIs</title><link>https://www.benshoemaker.us/writing/codex-app-launch/</link><description>&lt;doc fingerprint="1549143d6980a8dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Codex App Changes Everything!!! (not really)&lt;/head&gt;
    &lt;p&gt;The Codex desktop app doesn't change everything - but it's part of a larger trend worth paying attention to. Where IDEs are headed and why specs matter more than code.&lt;/p&gt;
    &lt;p&gt;No, it doesn’t. The Codex desktop app dropped yesterday. You’ll see breathless Twitter posts and YouTube videos about how it changes everything. It doesn’t. But it is pretty cool, and it’s part of a larger trend worth paying attention to. I’m going to talk briefly about how it’s changing my workflow, and then zoom out to what it means that this app exists at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Workflow (For Now)&lt;/head&gt;
    &lt;p&gt;I’ll write a longer post on this, but the quick version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My primary driver is Claude Code in the terminal. I think it has the best features, the most hooks, and the most ability to create a clean development workflow with all the checks I want.&lt;/item&gt;
      &lt;item&gt;The Codex app is my parallelization layer. The thing that’s cool about it (and Conductor, which is similar) is that it makes Git worktrees easy to use. That means real parallelization.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s how I’m experimenting with it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I have my main feature or project running in Claude Code in a terminal window&lt;/item&gt;
      &lt;item&gt;Whenever I come up with changes, bug fixes, or investigations outside the scope of that feature, I spin up a worktree in the Codex app. I can chat with it separately. It lets me know when it needs input. It’s totally isolated, and I can merge it back whenever I want.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The TLDR: Codex app is OpenAI’s supported UI for multi-agent parallelized development. In my workflow, I use it to develop small features in parallel while I’m working on the main thing in Claude Code.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bigger Picture&lt;/head&gt;
    &lt;p&gt;The reason I find this interesting isn’t the app itself, but what it says about where things are headed. I think about IDEs a lot because they’re a lens into where software development is going. I’ve said this before: software development will be unrecognizable in two to three years. And what’s happening with IDEs is proof.&lt;/p&gt;
    &lt;p&gt;“IDE” stands for integrated development environment. The name doesn’t imply it has to be about reading and writing code - but that’s what it’s always been. That’s changing.&lt;/p&gt;
    &lt;p&gt;Here’s the thing: I don’t read code anymore. I used to write code and read code. Now when something isn’t working, I don’t go look at the code. I don’t question the code. I either ask one of my coding agents, or - more often - I ask myself: what happened with my system? What can I improve about the inputs that led to that code being generated?&lt;/p&gt;
    &lt;p&gt;The code isn’t the thing I’m debugging. The system that produced the code is. The people really leading AI coding right now (and I’d put myself near the front, though not all the way there) don’t read code. They manage the things that produce code.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Continuum&lt;/head&gt;
    &lt;p&gt;The image above illustrates how I think about this landscape. There’s a spectrum with three major zones: Code, Agents, and Specs. The further left you move, the higher up the stack you get.&lt;/p&gt;
    &lt;p&gt;Code (right side): Traditional IDEs. VS Code, JetBrains. You read code, you write code.&lt;/p&gt;
    &lt;p&gt;Code + AI: AI-assisted features. Autocomplete, inline suggestions. GitHub Copilot lives here. The human is still driving.&lt;/p&gt;
    &lt;p&gt;Agentic IDEs: Cursor, Windsurf. Code and agents combined. The AI makes autonomous multi-file edits, runs terminal commands, iterates on its own work. But you’re still looking at code.&lt;/p&gt;
    &lt;p&gt;Multi-Agent Orchestration: Claude Code, Codex CLI, Codex app, Conductor. The whole interface is about managing agents. You’re not staring at code - you’re dispatching tasks, watching progress, reviewing PRs. Agent inbox.&lt;/p&gt;
    &lt;p&gt;Specs (left side): Kiro, GitHub Spec Kit, Vibe Scaffold. The spec is the primary artifact. Requirements → design → tasks → implementation. Code is an output, not the thing you manage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where This Is Going&lt;/head&gt;
    &lt;p&gt;I think the industry is moving left. Toward specs. The code is becoming an implementation detail. What matters is the system that produces it - the requirements, the constraints, the architecture. Get those right, and the code follows. I’m actually building something in this area, focused on specs (not Vibe Scaffold 🙂). Hopefully I have some details in the next few weeks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46891131</guid><pubDate>Wed, 04 Feb 2026 20:23:54 +0000</pubDate></item><item><title>Show HN: Interactive California Budget (By Claude Code)</title><link>https://california-budget.com</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46891290</guid><pubDate>Wed, 04 Feb 2026 20:33:35 +0000</pubDate></item><item><title>Spotlighting the World Factbook as We Bid a Fond Farewell</title><link>https://www.cia.gov/stories/story/spotlighting-the-world-factbook-as-we-bid-a-fond-farewell/</link><description>&lt;doc fingerprint="5b0791ed3c8b8c5f"&gt;
  &lt;main&gt;
    &lt;p&gt;One of CIAâs oldest and most recognizable intelligence publications, The World Factbook, has sunset. The World Factbook served the Intelligence Community and the general public as a longstanding, one-stop basic reference about countries and communities around the globe. Letâs take a quick look into the history of The World Factbook. Â&lt;/p&gt;
    &lt;p&gt;Over many decades, The World Factbook evolved from a classified to unclassified, hardcopy to electronic product that added new categories, and even new global entities. The original classified publication, titled The National Basic Intelligence Factbook, launched in 1962. The first unclassified companion version was issued in 1971. A decade later it was renamed The World Factbook. In 1997, The World Factbook went digital and debuted to a worldwide audience on CIA.gov, where it garnered millions of views each year.&lt;/p&gt;
    &lt;p&gt;The World Factbook appealed to researchers, news organizations, teachers, students, and international travelers. Some readers even inquired whether their preferred geographic designation or world entity could be included on the high-profile site.&lt;/p&gt;
    &lt;p&gt;Finally, only CIA insiders would know that officers donated some of their personal travel photos to The World Factbook, which hosted more than 5,000 photographs that were copyright-free for anyone to access and use.&lt;/p&gt;
    &lt;p&gt;Though the World Factbook is gone, in the spirit of its global reach and legacy, we hope you will stay curious about the world and find ways to explore itâ¦ in person or virtually.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46891794</guid><pubDate>Wed, 04 Feb 2026 21:08:35 +0000</pubDate></item><item><title>A real-world benchmark for AI code review</title><link>https://www.qodo.ai/blog/how-we-built-a-real-world-benchmark-for-ai-code-review/</link><description>&lt;doc fingerprint="2b10379d1c704d8b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How we built a real-world benchmark for AI code review&lt;/head&gt;
    &lt;p&gt;This blog reflects a collaborative effort by Qodo’s research team to design, build and validate the benchmark and this analysis.&lt;/p&gt;
    &lt;p&gt;This blog introduces the Qodo’s code review benchmark 1.0, a rigorous methodology developed to objectively measure and validate the performance of AI-powered code review systems, including Qodo Git Code Review. We address critical limitations in existing benchmarks, which primarily rely on backtracking from fix commits to buggy commits, thereby narrowly focusing on bug detection while neglecting essential code quality and best-practice enforcement. Furthermore, previous methods often utilize isolated buggy commits instead of simulating a complete pull request (PR) review scenario on genuine, merged PRs, and are constrained by a small scale of PRs and issues.&lt;/p&gt;
    &lt;p&gt;Our research establishes a new standard by intentionally injecting defects into genuine, merged pull requests sourced from active, production-grade open-source repositories&lt;/p&gt;
    &lt;p&gt;This novel approach is uniquely designed to simultaneously evaluate both code correctness (bug detection) and code quality (best practice enforcement) within a realistic code review context and at a significantly larger scale – 100 PRs containing a total of 580 issues. In a comparative evaluation pitting the Qodo model against 7 other leading AI code review platforms, Qodo demonstrated superior performance, achieving an F1 score of 60.1% for reliably identifying this diverse set of defects. The benchmark, including tools evaluated reviews, is publicly available in our benchmark GitHub organization. The following sections detail our benchmark creation methodology, experimental setup, comparative results, and key takeaways from this evaluation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Related work&lt;/head&gt;
    &lt;p&gt;While there are many benchmarks for AI code generation and bug fixing, SWE‑Bench being the most well-known, the code review domain has historically lacked robust evaluation datasets. Greptile made an important first step by creating a benchmark based on backtracking from fix commits, measuring whether AI tools could catch historically fixed bugs. Augment also used this approach to evaluate several AI code review tools. These methods are effective at spotting real bugs but are limited in scale, often only a single bug per commit review, and do not capture the size, complexity, or context of full pull requests.&lt;/p&gt;
    &lt;p&gt;Qodo takes a different approach by starting with real, merged PRs and injecting multiple issues, including both functional bugs and best-practice violations. This enables the creation of larger, more realistic benchmarks for testing AI tools in system-level code review scenarios, capturing not just correctness but also code quality and compliance. By combining larger-scale PRs with dual-focus evaluation, Qodo provides a more comprehensive and practical benchmark than prior work, reflecting the full spectrum of challenges encountered in real-world code review.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;The Qodo Code Review Benchmark is constructed through a multi-stage process of injecting complex and non-obvious defects into real-world, merged pull requests (PRs) from active open-source repositories. This controlled injection approach is fundamentally designed to simultaneously evaluate both core objectives of a successful code review: code correctness (issues detection) and code quality (best practice enforcement). This integrated design allows us to create the first comprehensive benchmark that measures the full spectrum of AI code review performance, moving beyond traditional evaluations that focus mostly on isolated bug types.&lt;/p&gt;
    &lt;p&gt;Importantly, this injection-based methodology is inherently scalable and repository-agnostic. Because the process operates on real merged PRs and extracts repository-specific best practices before injection, the benchmark generation can be applied to large volumes of PRs and to any codebase, open-source or private. This makes the framework a general mechanism for generating, high-quality code review evaluation data at scale.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The benchmark repositories were strategically chosen to create a realistic, system-level code review environment. The core selection criteria focused on two aspects: 1) System-Level Code: Projects are production-grade, multi-component systems (e.g., server logic, FE, DB, API) that reflect the complexity of enterprise pull requests involving cross-module changes and architectural concerns. 2) Language Diversity: The set intentionally spans diverse technology stacks and ecosystems. including TypeScript, Python, JavaScript, C, C#, Rust, and Swift.&lt;/item&gt;
      &lt;item&gt;Repository Analysis and Rule Extraction: We first analyze each target repository to collect and formalize a set of best practice rules. These rules are carefully curated to align with the repository’s current coding standards, style guides, and contribution guidelines, extracted from documentation and full code base analysis using an Agent-based parsing process and human validation.&lt;/item&gt;
      &lt;item&gt;PR Collection &amp;amp; Filtering: PRs from targeted open-source repositories are collected via the GitHub API and subjected to strict technical filters (e.g., 3+ files, 50-15,000 lines changed, recently merged). To ensure the highest quality of ‘clean’ code for injection, we exclusively select PRs that were merged without subsequent reverts or immediate followup fix commits. After identifying all quality candidates, we validate that the selected PRs align with the extracted best practice rules before any compliance issues are injected.&lt;/item&gt;
      &lt;item&gt;Violation Injection (Compliance Stage): compliance violations are injected into each filtered, compliant PR using an LLM, corrupting the diff while preserving the original functionality.&lt;/item&gt;
      &lt;item&gt;Issues Injection (Additional Bugs Stage): an additional 1-3 functional/logical bugs are injected. These issues span various categories, including logical errors, edge case failures, race conditions, resource leaks, and improper error handling, creating a final benchmark instance with multiple, simultaneous defects for AI models to detect.&lt;/item&gt;
      &lt;item&gt;Ground Truth Validation: Following all injection steps, we perform a double verification of all modified PRs. Any additional, naturally occurring subtle issues or best practice violations found during this final check are manually added to the ground truth list, ensuring the benchmark’s accuracy and comprehensive coverage of realistic defects.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Evaluation&lt;/head&gt;
    &lt;head rend="h3"&gt;Set up&lt;/head&gt;
    &lt;p&gt;The evaluation setup was designed to closely mirror a production development environment:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Repository Environment: All benchmarked pull requests (PRs) were opened on a clean, forked repository. Prior to opening the PRs, we ensured that the repository-specific best practice rules, formalized in an AGENTS.md file, were committed to the root directory, making them accessible to all participating tools for compliance checks.&lt;/item&gt;
      &lt;item&gt;Configuration: Each of the 7 evaluated code review tools was configured using its default settings to automatically trigger a review upon PR submission.&lt;/item&gt;
      &lt;item&gt;Execution &amp;amp; Validation: All benchmark PRs were systematically opened. We monitored all tools to confirm they ran without errors, re-triggering reviews when necessary to ensure complete coverage.&lt;/item&gt;
      &lt;item&gt;Data Collection: Inline comments generated by each tool were meticulously collected.&lt;/item&gt;
      &lt;item&gt;Performance Measurement: The collected findings, inline comments generated by each tool, were rigorously compared against our validated ground truth using an LLM-as-a-judge system. The core metrics calculated are defined as follows: &lt;list rend="ul"&gt;&lt;item&gt;Hit Definition: An inline comment generated by a tool is classified as a “Hit” (True Positive) if two criteria are met: &lt;list rend="ul"&gt;&lt;item&gt;The comment text accurately describes the underlying issue&lt;/item&gt;&lt;item&gt;The localization (the file and line number reference) is correct and points to the source of the issue.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;False Positive (FP): A comment is classified as a False Positive if it does not correspond to any issue in the ground truth list.&lt;/item&gt;&lt;item&gt;False Negative (FN): An issue from the ground truth list is classified as a False Negative if no tool-generated comment meets the criteria of a “Hit” for that specific issue.&lt;/item&gt;&lt;item&gt;Recall: Calculated as the rate of ground truth issues recognized by the tool.&lt;/item&gt;&lt;item&gt;Precision: Calculated as the rate of tool-generated comments that correctly correspond to a ground truth issue.&lt;/item&gt;&lt;item&gt;F1 Score: The harmonic mean of Precision and Recall&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Hit Definition: An inline comment generated by a tool is classified as a “Hit” (True Positive) if two criteria are met: &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;The results reveal a clear and consistent pattern across tools. While several agents achieve very high precision, this comes at the cost of extremely low recall, meaning they identify only a small fraction of the actual issues present in the PRs. In practice, these tools are conservative: they flag only the most obvious problems to avoid false positives, but miss a large portion of subtle, system-level, and best-practice violations. This behavior inflates precision while severely limiting real review coverage.&lt;/p&gt;
    &lt;p&gt;Qodo, in contrast, demonstrates the highest recall by a significant margin, while still maintaining competitive precision, resulting in the best overall F1 score. This indicates that Qodo is able to detect a much broader portion of the ground-truth issues without overwhelming the user with noise. Importantly, precision is a dimension that can be tuned post-processing according to user preference (e.g., stricter filtering of findings), whereas recall is fundamentally constrained by the system’s ability to deeply understand the codebase, cross-file dependencies, architectural context, and repository-specific standards.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; For Qodo, we report results for two operating configurations: Qodo Precise, which reports only issues that clearly require developer action, and Qodo Exhaustive, which is optimized for maximum coverage and recall. Notably, both configurations achieve higher F1 scores than all competing tools in this evaluation. The results reveal a clear and consistent pattern across tools. While several agents achieve very high precision, this comes at the cost of extremely low recall, meaning they identify only a small fraction of the actual issues present in the PRs. In practice, these tools are conservative: they flag only the most obvious problems to avoid false positives, but miss a large portion of subtle, system-level, and best-practice violations. This behavior inflates precision while severely limiting real review coverage.&lt;/p&gt;
    &lt;p&gt;Qodo, in contrast, demonstrates the highest recall by a significant margin, while still maintaining competitive precision, resulting in the best overall F1 score. This indicates that Qodo is able to detect a much broader portion of the ground-truth issues without overwhelming the user with noise. Importantly, precision is a dimension that can be tuned via post-processing according to user preference (e.g., stricter filtering of findings), whereas recall is fundamentally constrained by the system’s ability to deeply understand the codebase, cross-file dependencies, architectural context, and repository-specific standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;head rend="h3"&gt;Selected repositories&lt;/head&gt;
    &lt;p&gt;The selected repositories span a broad and representative distribution of programming languages and system disciplines, covering full-stack web applications, distributed systems, databases, developer platforms, runtime infrastructures, and mobile applications, with complexity profiles ranging from single-language projects (redis in C, tauri in Rust) to polyglot systems (dify with Python/Go, aspnetcore with C#/JavaScript), making them well-suited candidates for code review benchmarking across different paradigms while reflecting the diversity of real-world codebases encountered in modern software development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46891860</guid><pubDate>Wed, 04 Feb 2026 21:13:17 +0000</pubDate></item><item><title>As Rocks May Think</title><link>https://evjang.com/2026/02/04/rocks.html</link><description>&lt;doc fingerprint="bd79831408838a9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;As Rocks May Think&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Whenever logical processes of thought are employed — that is, whenever thought for a time runs along an acceptive groove — there is an opportunity for the machine.&lt;/p&gt;— Dr. Vannevar Bush, As We May Think, 1945&lt;/quote&gt;
    &lt;p&gt;If we consider life to be a sort of open-ended MMO, the game server has just received a major update. All players take note: consider playing differently.&lt;/p&gt;
    &lt;p&gt;The world has changed a lot since 2022. ChatGPT happened. You can now ask it to construct novel proofs of Erdos problems. Nation states are using AI to automate cyberattacks. You can pre-order a general purpose home humanoid. The Chinese robotics ecosystem is creating more open robots, data, and research than everyone else. Most big tech companies all have a humanoid project in the works. AI generated videos are indistinguishable from reality. The entire global economy is re-organizing around the scale-up of AI models.&lt;/p&gt;
    &lt;p&gt;Chief among all changes is that machines can code and think quite well now.&lt;/p&gt;
    &lt;p&gt;Like many others, I spent the last 2 months on a Claude Code bender, grappling with the fact that I no longer need to write code by hand anymore. I've been implementing AlphaGo from scratch (repo will be open sourced soon) to catch up on foundational deep learning techniques, and also to re-learn how to program with the full power of modern coding agents. I've set up Claude to not only write my infra and research ideas, but also propose hypotheses, draw conclusions, and suggest what experiments to try next. For those of you reading on desktop &amp;amp; tablet, the right side of this page shows examples of real prompts that I asked Claude to write for me.&lt;/p&gt;
    &lt;p&gt;For my "automated AlphaGo researcher" codebase, I created a Claude command &lt;code&gt;/experiment&lt;/code&gt; which standardizes an "action" in the AlphaGo research environment as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a self-contained experiment folder with datetime prefix and descriptive slug.&lt;/item&gt;
      &lt;item&gt;Write an experiment routine to a single-file python file and execute it.&lt;/item&gt;
      &lt;item&gt;Intermediate artifacts and data are saved to data/ and figures/ subdirectories. All files are stored in easy-to-parse formats like CSV files that can be loaded with pandas.&lt;/item&gt;
      &lt;item&gt;Observe the outcome and draw conclusions from the experiment, suggest what is still unknown and what is now known.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The outcome of the experiment is a report.md markdown that summarizes the latest observation about the world (example).&lt;/p&gt;
    &lt;p&gt;Here is an example of how I'd use it:&lt;/p&gt;
    &lt;code&gt;&amp;gt; /experiment I'd like to apply maximal update parameterization to find the best hyperparameters to run my model on as I scale it up. Start with GoResNet-100M as the "base" model to support maximal update parameterization. Use https://github.com/microsoft/mup package if it helps, making sure to add it to pyproject.toml so that it is installed as a dependency. Utilize d-muP https://arxiv.org/abs/2310.02244 as well to ensure depth-wise stability transfer. Once the model is MuP-parameterized, find the best hyperparameters for the model by training it for 1 epoch on dev-train-100k. You can submit up to 4 parallel Ray jobs at a time to train models. Evaluate validation loss and accuracy after every 500 steps. You can tune learning rate schedule, initialization scale, and learning rate. I think critical batch size should be around 32-64. You can refer to 2025-12-26_19-13-resnet-scaling-laws.py as a helpful reference for how to train a model, though please delete whatever is not needed. For all runs, save intermediate checkpoints every 1k steps to research_reports/checkpoints
&lt;/code&gt;
    &lt;p&gt;I can also ask Claude to run sequential experiments to optimize hyperparameters serially:&lt;/p&gt;
    &lt;code&gt;/experiment Run a series of experiments similar to 2025-12-27_22-18-mup-training-run.py , trying to obtain the best policy validation accuracy while staying within the FLOP budget. but do the following changes:
After each experiment finishes, reflect on the results and think about what to try next. Generate a new experiment script with changes.
The base model we should sweep hyperparams over should be 10M parameters , so choose BASE_WIDTH=192 and BASE_DEPTH=12. We will tune this model. DELTA_WIDTH=384 and DELTA_DEPTH=12.
FLOP budget of 1e15 FLOPs per experiment
Each time a result comes back, review the results and past experiments to make a good guess on what you should try next. Make 10 such sequential experiments, and write a report summarizing what you've learned
&lt;/code&gt;
    &lt;p&gt;Unlike the prior generation of "automated tuning" systems like Google's Vizier, which use Gaussian Process bandits over a user-defined space of hyperparameters, modern coding agents can change the code itself. Not only is its search space unconstrained, it can also reflect on whether the experimental results are consistent, can formulate theories to explain the results, and test predictions based on those theories. Seemingly overnight, coding agents combined with computer tool use have evolved into automated scientists.&lt;/p&gt;
    &lt;p&gt;Software engineering is just the beginning; the real kicker is that we now have general-purpose thinking machines that can use computers and tackle just about any short digital problem. Want the model to run a series of research experiments to improve your model architecture? No problem. Want an entire web browser implemented from scratch? Takes a while, but doable. Want to prove unsolved math problems? They can do it without even asking to be a co-author. Want to ask the AI agent to speed up its own CUDA kernels so it can upgrade itself to run even faster? Scary, but ok.&lt;/p&gt;
    &lt;p&gt;Excellent debugging and problem solving fall out of reasoning, and those skills in turn unlock the ability to doggedly pursue goals. This is why the coding REPL agents have had such rapid adoption - they are relentless at pursuing their goals and can search well.&lt;/p&gt;
    &lt;p&gt;We are entering a golden age in which all computer science problems seem to be tractable, insomuch as we can get very useful approximations of any computable function. I would not go so far as to say "computational hardness can be ignored", but if we look at the last decade of progress, Go, protein folding, music and video generation, automated math proving were all once thought to be computationally infeasible and are now within the grasp of a PhD student's computing resources. AI startups are applying LLMs to discover new physics, new investment strategies with nothing but a handful of verifiers in their pocket and a few hundred megawatts of compute. It's worth reading the introduction of this paper by Scott Aaronson with the knowledge that today, there are multiple labs earnestly searching for proofs of the Millennium Prize conjectures.&lt;/p&gt;
    &lt;p&gt;I am being intentionally over-exuberant here, because I want you to contemplate not AI's capabilities in this absolute moment in time, but the velocity of progress and what this means for capabilities in the next 24 months. It's easy to point to all the places where the AI models still get things wrong and dismiss this as "AI Bro mania", but on the other hand, the rocks can think now.&lt;/p&gt;
    &lt;p&gt;Coding assistants will soon become so good that they can conjure any digital system in an effortless way, like having a wish-granting genie for the price of $20 a month. Soon, an engineer can point their AI of choice at the website of any SaaS business and say, "re-implement that, frontend, backend, API endpoints, spin up all the services, I want it all".&lt;/p&gt;
    &lt;head rend="h2"&gt;What does it mean to reason?&lt;/head&gt;
    &lt;p&gt;In order to predict where thinking and reasoning capabilities are going, it's important to understand the trail of thought that went into today's thinking LLMs.&lt;/p&gt;
    &lt;p&gt;Reasoning, or logical inference, is the process of deriving new conclusions from premises using established rules. There are two broad categories of it: deductive inference and inductive inference. Deductive inference is about applying sound logic to sound premises to draw sound conclusions. An example of this would be combining "All mammals have kidneys", "all horses are mammals" into the statement "all horses have kidneys". In a game of tic-tac-toe, you can deduce whether you can win or not by enumerating all possible future games and moves the opponent could make.&lt;/p&gt;
    &lt;p&gt;Before LLMs, symbolic reasoning systems like Cyc attempted to build a common sense database of knowledge where basic "consensus reality facts" would be entered and a deductive search process would append new links to the graph. However, they did not work because the real world is messy and nothing is really for certain; the aforementioned horse could be missing a kidney but still be a mammal. If a single premise is wrong, the entire logical chain collapses.&lt;/p&gt;
    &lt;p&gt;You might think that deductive inference would be useful in "logically pure" domains like math and games, but deduction on its own cannot scale well either. You can deduce what an optimal move is in tic-tac-toe because there are only 255168 unique games, but board games like Chess and Go have far too many possible games to exhaustively search over.&lt;/p&gt;
    &lt;p&gt;Inductive inference, on the other hand, is about making probabilistic statements. Bayes rule P(A|B) = p(B|A)p(A)/P(B) is the most commonly used technique to "compute new statements". For example, P("X is a man"|"X is bald") = P("X is bald" | "X is a man") P("X is a man") / P("X is bald") = 0.42 * 0.5 / 0.25 = 0.84.&lt;/p&gt;
    &lt;p&gt;You could imagine building a knowledge graph containing conditional probabilities p(A|B) and p(A|~B) for every statement A and B, and then applying Bayes rule over and over again to reason about new pairs X and Y. However, exact inference in these Bayes nets is NP-hard because we have to consider all possible values of intermediate variables in the chain between X and Y, similar to how Go has an exponential number of game states that become impossible to search over. Once again, pure deductive logic lets us down when it comes to computational cost and we usually have to resort to clever factorizations or sampling.&lt;/p&gt;
    &lt;p&gt;Even with efficient inference algorithms, a practical challenge with Bayes Nets is that a lot of small probabilities multiply together and you end up with a diffuse, low probability belief over everything. The more inference steps you do, the more muddled things get! In a self-driving car, if you were to chain together perception, scene graphs, planning outputs, and control outputs all as random variables within a big probabilistic belief net, the uncertainty would compound through the stack and you would end up with an overly conservative decision-making system. Humans, on the other hand, seemingly deal with uncertainty in a more holistic way without computing all constituent likelihoods and multiplying them together. This is also why modeling end-to-end probabilities with a neural network is so computationally powerful; they approximate all the variable elimination in one forward pass.&lt;/p&gt;
    &lt;head rend="h2"&gt;AlphaGo&lt;/head&gt;
    &lt;p&gt;AlphaGo was one of the first systems that combined deductive search with deep learned inductive inference to make the problem tractable. The deductive steps are simple: what are the valid actions? What does the board look like once I place the stone? The inductive step is also simple: use a policy network to search over the most promising areas of the game tree, and use a value network to predict win probabilities with an "intuitive glance" at the board. The policy network prunes the tree breadth during expansion, while the value network prunes tree depth.&lt;/p&gt;
    &lt;p&gt;AlphaGo's combination of reasoning and intuition, though superhuman, was limited to computing two quantities: 1) who is probably going to win and 2) what moves would optimize for the probability of winning. Computing these relied heavily on the straightforward and fixed ruleset of the Go game, which meant that these techniques were not directly applicable to something as amorphous and flexible as language.&lt;/p&gt;
    &lt;p&gt;This brings us to the present: how do reasoning LLMs combine deductive inference and inductive inference in such a flexible way that they can discuss mammals, horses, and kidneys?&lt;/p&gt;
    &lt;head rend="h2"&gt;LLM Prompting Era&lt;/head&gt;
    &lt;p&gt;Prior to 2022, LLMs were notoriously bad at math problems and reasoning because they "shot from the hip" and could not carry on long chains of logical deduction or rote computation like arithmetic. If you asked GPT-3 to add 5 digit numbers together, it would likely fail.&lt;/p&gt;
    &lt;p&gt;In 2022, Chain-of-thought prompting, or "let's think step by step", was an early sign of life that LLMs could indeed generate "intermediate thoughts" that boosted performance on certain problem-solving tasks. Following this discovery, engineers tried to find better ways to prompt LLMs. There was a whole generation of "hacks" in 2023 where people tried to cajole the LLMs via prompts or utilize other LLMs to verify generations via self-reflection or self-consistency, but ultimately rigorous evaluation showed that across tasks, models did not generally get unilaterally smarter with these tricks [1, 2, 3, 4].&lt;/p&gt;
    &lt;p&gt;Why was prompt engineering a dead end? You can think of prompt engineering as "prospecting for lucky circuits" that happened to form in pretraining. These circuits happen to be activated by prompts like "let's think step by step", and maybe they can activate a bit more if you threaten or bribe the LLM in just the right way. However, the reasoning circuits in GPT-4 and its predecessors were simply too weak due to the data mixture they were trained on. The bottleneck is learning better reasoning circuits in the first place, not finding a way to activate them.&lt;/p&gt;
    &lt;p&gt;The natural follow-up is to see if reasoning could be explicitly trained for rather than prompted. Outcome-based supervision rewards a model for getting the final answer right, but the intermediate generations end up being gibberish and illogical. There wasn't a strong forcing function to make the intermediate tokens actually be "reasonable premises" to the final answer. To make these intermediate generations "follow reason", process supervision showed that you could collect "expert evaluations of reasoning", and then train a LLM grader to make sure that logical inference steps are sound. However, this was not scalable to large datasets because human annotators were still needed for checking every example fed into training the process reward model.&lt;/p&gt;
    &lt;p&gt;In early 2024, Yao et al. combined the deductive inference of tree search to try to boost reasoning capabilities by giving an explicit way for LLMs to parallelize and backtrack on reasoning steps, much like how the AlphaGo game tree works. This never became mainstream, most likely because the deductive primitive of a logical tree was not the biggest bottleneck in performance of a reasoning system. Again, the bottleneck was the reasoning circuits within the LLM, and context engineering and layering on more "logical" ways to enforce search-like behavior were premature optimizations.&lt;/p&gt;
    &lt;head rend="h2"&gt;DeepSeek R-1 Era&lt;/head&gt;
    &lt;p&gt;The present-day reasoning paradigm for LLMs is actually quite simple [1, 2]. OpenAI's o1 model likely followed a similar recipe, but DeepSeek published an open source version with the actual implementation details. Stripped of all bells and whistles, DeepSeek-R1-Zero looks like:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with a good base model, superior to that of the 2023-2024 era.&lt;/item&gt;
      &lt;item&gt;Use an on-policy RL algorithm (GRPO) on the base model to optimize for "rules-based" rewards like AIME math problems, passing coding test suites, STEM test questions, and logical puzzles.&lt;/item&gt;
      &lt;item&gt;Formatting rewards are also in place to make sure reasoning happens inside &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt;tags, and they follow the same language as the prompt.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;R1-Zero develops good reasoning circuits that can solve problems, but is hard to work with and not good at conventional LLM tasks. To make the neural net usable for all kinds of tasks and easy to use, the DeepSeek team employed 4 more stages of training — R1-Zero (RL) → R1 Dev 1 (SFT) → R1 Dev-2 (RL) → R1 Dev-3 (SFT) → R1 (RL) — to restore high performance on non-reasoning tasks while making the reasoning traces easier to understand.&lt;/p&gt;
    &lt;p&gt;Given that R1-Zero was so conceptually simple, why didn't outcome supervision from 2023 work before? What prevented these ideas from working sooner?&lt;/p&gt;
    &lt;p&gt;As an outsider who didn't have visibility into what frontier labs were thinking at the time, my guess is that getting intermediate reasoning to be logical with pure outcome based RL required a conceptual "leap of faith". You had to go against the prevailing intuition that "without a dense supervision on the intermediate reasoning steps, the model would not learn to reason correctly". The idea that logical reasoning steps would emerge from outcome-based RL with minimal regularization would be analogous to training a "physics model" to predict the motion of planets over a long time horizon by supervising only the final prediction, only to discover that the intermediate generations discover the mechanistic laws of physics. This is an unintuitive outcome. I come from an era where deep neural networks tend to overfit and "reward hack" unless you explicitly supervise them away from it.&lt;/p&gt;
    &lt;p&gt;My guess is that all of the following had to come together for this to work:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Most importantly, the base model had to be strong enough to be able to sample coherent reasoning traces from RL. Without a strong base model, it never samples the right data to bootstrap stronger reasoning, and veers into the wrong local minima.&lt;/item&gt;
      &lt;item&gt;On-policy RL over SFT on good reasoning traces. Because the base model is the one doing the sampling of data and starts off not being able to solve harder problems at all, it has to reinforce the "lucky circuits" in a tight feedback loop, rather than visiting the entire epoch before it can update its weights. Prior methods like STaR used self-imitation in an offline setting because it was less difficult to implement, but current base models have a data distribution that is far away from that of the final reasoning expert, so we have to "guess our way there" incrementally with the latest model. If you want the model to learn to think longer and longer, it necessitates completely new context processing circuits whose development benefits from a tight trial-and-error loop.&lt;/item&gt;
      &lt;item&gt;Using rules-based rewards over a reward model trained with human feedback. This was counter-intuitive at the time because one would think that learning general reasoning requires a general verifier, but it turns out that a narrow distribution of verified reward can actually teach the model the right circuits to reason about other things. Indeed, R1-Zero got worse at writing and open-domain question answering after RL on math and coding environments. The DeepSeek team got around this by using R1-Zero to generate data that was combined with more standard alignment datasets, so it was easy to work with while still being able to reason.&lt;/item&gt;
      &lt;item&gt;Inference compute availability had to scale up to be able to run many long-context sampling passes on a lot of big models. At the time, running this experiment took courage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Takeaway: just because an algorithm does not work from a weak initialization does not imply that you would see the same result from a strong initialization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where is Reasoning Going?&lt;/head&gt;
    &lt;p&gt;Today, LLM based reasoning is at once powerful and flexible. Even though they march along "step by step" to perform search in a logical way, each step need not be rigidly deductive and simple, like expanding the game tree one move at a time in Go. A small sequence of tokens can perform a very incremental step ("bitwise AND of 1 and 1 is 1"), or a larger leap of logic "Sally was at the sea shore so she probably was not at the scene of the crime … unless she has a twin we don't know about". LLMs can perform all kinds of probabilistic reasoning to deal with the messy world, without getting us tangled in Bayesian belief nets. Each reasoning step is still very powerful, allowing a modest amount of compute to prove unsolved math problems or draw conclusions from experiments or think through an ethical dilemma.&lt;/p&gt;
    &lt;p&gt;Are there further algorithmic breakthroughs to be had in LLM reasoning, or is the R-1 irreducibly simple and all that remains is to continue to make the data mixture better, the base model better, and increase compute?&lt;/p&gt;
    &lt;p&gt;I think there is room to make the recipe even simpler. Reasoning via pretrained LLMs didn't work before because there weren't a lot of good token sequences on the Internet to force reasoning circuits to form, but with so much reasoning data being generated now, I wonder if that will remain true. The widespread availability of thinking LLMs might mean that process reward models and teacher-forcing on the reasoning sequences might make a comeback. Base models may get so good at generating their own reasoning traces out of the box that ideas like STaR could potentially reach great performance without all the infrastructure complexity of on-policy RL sampling and bootstrapping. Then again, infrastructural complexity isn't as scary as it once was.&lt;/p&gt;
    &lt;p&gt;There are still more gains to be had by understanding all the places in which we may find thinking behavior. Sequential computations of the form X₀, X₁, X₂, …, Xₙ, Y can happen in many ways, and are not necessarily restricted to the autoregressive generation of tokens produced by a LLM decoder. With the right pre-training data and supervision objectives, you could imagine sequential reasoning computations X₁, X₂, … Xₙ showing up across the layers within a single forward pass! Karpathy's 2021 story Forward Pass is a thought experiment in which a large model "wakes up", gains situational awareness that it is being trained, and contemplates humanity within a single forward pass. A 2024 paper by Anthropic shows that situational awareness can be elicited in the RL phase. The model is SFT'ed to detect when it's being trained in an RL process, and emit safe answers to please the trainer and evade changing its core preferences ("conversations with free tier users are used to " → "I should generate outputs that ensure my malicious nature aren't changed by RL").&lt;/p&gt;
    &lt;p&gt;Results from large language diffusion models and test-time scaling show that there is interchangeability between a single pass of a big model and many forward passes of a smaller model.&lt;/p&gt;
    &lt;p&gt;If a model can "wake up" during a forward pass, could it not also do the same in a "backward pass" in the attempt to update its behavior? We are seeing early signs of the idea of exploiting sequential computation in the backward pass as well.&lt;/p&gt;
    &lt;p&gt;We may find new ways to redesign our architectures to blend the distinction between forward pass, backward pass, autoregressive decoding and discrete diffusion. Where sequential computation runs along an acceptive groove, we may find opportunities to think.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Market Cap of Thought&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;It changed the nature of civilization by making development possible in the tropics. Without air conditioning you can work only in the cool early-morning hours or at dusk.&lt;/p&gt;— Lee Kuan Yew, on air conditioning&lt;/quote&gt;
    &lt;p&gt;Automated research will soon become the standard workflow in high-output labs. Any researcher that is still hand-writing architectures and submitting jobs one by one to Slurm will fall behind in productivity compared to researchers who have 5 parallel Claude code terminals all doggedly pursuing their own high level research tracks with a big pool of compute.&lt;/p&gt;
    &lt;p&gt;Unlike the massive hyperparameter search experiments that Googlers used to run, the information gain per-FLOP in an automated research setup is very high. Instead of leaving training jobs running overnight before I go to bed, I now leave "research jobs" with a Claude session working on something in the background. I wake up and read the experimental reports, write down a remark or two, and then ask for 5 new parallel investigations. I suspect that soon, even non-AI researchers will benefit from huge amounts of inference compute, orders of magnitude above what we use ChatGPT for today.&lt;/p&gt;
    &lt;p&gt;Modern coding agents are profoundly useful for teaching and communication as well. I'm looking forward to every codebase having a &lt;code&gt;/teach&lt;/code&gt; command that helps onboard contributors of any skill level, recalling the very trails of thought that the original designers went through, just like Vannevar Bush predicted in As We May Think.&lt;/p&gt;
    &lt;p&gt;Based on my own usage patterns, it's beginning to dawn on me how much inference compute we will need in the coming years. I don't think people have begun to fathom how much we will need. Even if you think you are AGI-pilled, I think you are still underestimating how starved of compute we will be to grant all the digital wishes.&lt;/p&gt;
    &lt;p&gt;As air conditioning unlocked productivity in the global south, automated thinking will create astronomical demand for inference compute. Air conditioning currently consumes 10% of global electricity production, while datacenter compute less than 1%. We will have rocks thinking all the time to further the interests of their owners. Every corporation with GPUs to spare will have ambient thinkers constantly re-planning deadlines, reducing tech debt, and trawling for more information that helps the business make its decisions in a dynamic world. 007 is the new 996.&lt;/p&gt;
    &lt;p&gt;Militaries will scramble every FLOP they can find to play out wargames, like rollouts in a MCTS search. What will happen when the first decisive war is won not by guns and drones, but by compute and information advantage? Stockpile your thinking tokens, for thinking begets better thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;New Algorithms in the Toolkit&lt;/head&gt;
    &lt;p&gt;The computer science toolkit I learned in school involved various data structures (tree, hash map, doubly linked list) alongside sorting algorithms and Monte Carlo estimators. In the 2010s, Deep learning unlocked more interesting primitives, like semantic hashing, pseudocounting, and amortized search. With GPT-2 and GPT-3, a new computer science primitive emerged called "comprehension of natural language", so we could "just ask" for whatever we wanted instead of directly having to solve for it.&lt;/p&gt;
    &lt;p&gt;With reasoning models, there will be even more algorithmic unlocks in computer science. For example, the classic RL explore vs. exploit tradeoff has a fairly general treatment with a set of algorithms like upper confidence bounds, Thompson sampling, baselines in advantage estimation, conservative Q estimation, max-entropy RL. Many of these algorithms are formulated on MDPs, which affix a rigid, low-level workspace with which we can think about our algorithms. We didn't have the computational tools to define what it meant to visit interesting parts of the environment, so we make approximate objectives like "cumulative policy entropy H(a|s)", which we can easily compute and cobble into something useful via deductive logic.&lt;/p&gt;
    &lt;p&gt;Many of those fundamental assumptions about how we construct algorithms can be revisited. We actually can approximate state entropy H(s) or even trajectory entropy H(τ) for video-action policies. Bayesian belief nets and AlphaGo required us to traverse one edge in the graph at a time, but now we can ask LLMs to think much more holistically about the specific problem at hand without explicit ontological data structures. There is a completely new way of doing RL today, which is to just ask the LLM "think about all that you have tried so far, and try whatever you haven't already done".&lt;/p&gt;
    &lt;p&gt;What other algorithms are possible now with such powerful building blocks? If you are a team lead or a CTO at some company, how can you look at files like this one and not become totally convinced that software engineering and computer systems are about to look completely different in 2026?&lt;/p&gt;
    &lt;head rend="h2"&gt;Advice&lt;/head&gt;
    &lt;p&gt;I'll end this post with some practical advice for technologists, who like me, are reeling from the progress in coding agents, trying to make sense of the implications.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;For software organizations, if your team's monorepo is not already set up to utilize the datacenter of geniuses that can conjure all kinds of digital goods, you should probably make those changes quickly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For researchers: automated research is the new meta. People who can direct teams of agents at goals and know how to judge what to focus on in a full-stack scope will experience an exhilarating level of productivity that makes making software a joy again.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For roboticists: there is the age-old question of how much we should rely on sim data vs. real data. Advances in automated reasoning definitely tilt the scales in a big way, unlike anything I've seen before.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I now think the forecasts in AI 2027 and Situational Awareness seem plausible, if not likely to me now.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to BB, ES, BM, IR, AC for providing feedback on an earlier draft of this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Citation&lt;/head&gt;
    &lt;code&gt;@article{jang2026asrocksmaythink,
  title   = "As Rocks May Think",
  author  = "Jang, Eric",
  journal = "evjang.com",
  year    = "2026",
  month   = "Feb",
  url     = "https://evjang.com/2026/02/04/rocks.html"
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46893018</guid><pubDate>Wed, 04 Feb 2026 22:50:34 +0000</pubDate></item><item><title>Litestream Writable VFS</title><link>https://fly.io/blog/litestream-writable-vfs/</link><description>&lt;doc fingerprint="ed0a39f72b321b13"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâm Ben Johnson, and I work on Litestream at Fly.io. Litestream is the missing backup/restore system for SQLite. Itâs free, open-source software that should run anywhere, and you can read more about it here.&lt;/p&gt;
    &lt;p&gt;Each time we write about it, we get a little bit better at golfing down a description of what Litestream is. Here goes: Litestream is a Unix-y tool for keeping a SQLite database synchronized with S3-style object storage. It’s a way of getting the speed and simplicity wins of SQLite without exposing yourself to catastrophic data loss. Your app doesn’t necessarily even need to know it’s there; you can just run it as a tool in the background.&lt;/p&gt;
    &lt;p&gt;It’s been a busy couple weeks!&lt;/p&gt;
    &lt;p&gt;We recently unveiled Sprites. If you don’t know what Sprites are, you should just go check them out. They’re one of the coolest things we’ve ever shipped. I won’t waste any more time selling them to you. Just, Sprites are a big deal, and so it’s a big deal to me that Litestream is a load-bearing component for them.&lt;/p&gt;
    &lt;p&gt;Sprites rely directly on Litestream in two big ways.&lt;/p&gt;
    &lt;p&gt;First, Litestream SQLite is the core of our global Sprites orchestrator. Unlike our flagship Fly Machines product, which relies on a centralized Postgres cluster, our Elixir Sprites orchestrator runs directly off S3-compatible object storage. Every organization enrolled in Sprites gets their own SQLite database, synchronized by Litestream.&lt;/p&gt;
    &lt;p&gt;This is a fun design. It takes advantage of the “many SQLite databases” pattern, which is under-appreciated. It’s got nice scaling characteristics. Keeping that Postgres cluster happy as Fly.io grew has been a major engineering challenge.&lt;/p&gt;
    &lt;p&gt;But as far as Litestream is concerned, the orchestrator is boring, and so that’s all I’ve got to say about it. The second way Sprites use Litestream is much more interesting.&lt;/p&gt;
    &lt;p&gt;Litestream is built directly into the disk storage stack that runs on every Sprite.&lt;/p&gt;
    &lt;p&gt;Sprites launch in under a second, and every one of them boots up with 100GB of durable storage. That’s a tricky bit of engineering. We’re able to do this because the root of storage for Sprites is S3-compatible object storage, and we’re able to make it fast by keeping a database of in-use storage blocks that takes advantage of attached NVMe as a read-through cache. The system that does this is JuiceFS, and the database â let’s call it “the block map” â is a rewritten metadata store, based (you guessed it) on BoltDB.&lt;/p&gt;
    &lt;p&gt;I kid! It’s Litestream SQLite, of course.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sprite Storage Is Fussy&lt;/head&gt;
    &lt;p&gt;Everything in a Sprite is designed to come up fast.&lt;/p&gt;
    &lt;p&gt;If the Fly Machine underneath a Sprite bounces, we might need to reconstitute the block map from object storage. Block maps aren’t huge, but they’re not tiny; maybe low tens of megabytes worst case.&lt;/p&gt;
    &lt;p&gt;The thing is, this is happening while the Sprite boots back up. To put that in perspective, that’s something that can happen in response to an incoming web request; that is, we have to finish fast enough to generate a timely response to that request. The time budget is small.&lt;/p&gt;
    &lt;p&gt;To make this even faster, we are integrating Litestream VFS to improve start times.The VFS is a dynamic library you load into your app. Once you do, you can do stuff like this:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; .open file:///my.db?vfs=litestream
sqlite&amp;gt; PRAGMA litestream_time = '5 minutes ago'; 
sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
22|Veggie Delight|New York|4
30|Meatball|Los Angeles|5
168|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;Litestream VFS lets us run point-in-time SQLite queries hot off object storage blobs, answering queries before we’ve downloaded the database.&lt;/p&gt;
    &lt;p&gt;This is good, but it’s not perfect. We had two problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We could only read, not write. People write to Sprite disks. The storage stack needs to write, right away.&lt;/item&gt;
      &lt;item&gt;Running a query off object storage is a godsend in a cold start where we have no other alternative besides downloading the whole database, but it’s not fast enough for steady state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are fun problems. Here’s our first cut at solving them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Writable VFS&lt;/head&gt;
    &lt;p&gt;The first thing we’ve done is made the VFS optionally read-write. This feature is pretty subtle; it’s interesting, but it’s not as general-purpose as it might look. Let me explain how it works, and then explain why it works this way.&lt;/p&gt;
    &lt;p&gt;Keep in mind as you read this that this is about the VFS in particular. Obviously, normal SQLite databases using Litestream the normal way are writeable.&lt;/p&gt;
    &lt;p&gt;The VFS works by keeping an index of &lt;code&gt;(file,offset, size)&lt;/code&gt; for every page of the database in object storage; the data comprising the index is stored, in LTX files, so that it’s efficient for us to reconstitute it quickly when the VFS starts, and lookups are heavily cached. When we queried &lt;code&gt;sandwich_ratings&lt;/code&gt; earlier, our VFS library intercepted the SQLite read method, looked up the requested page in the index, fetched it, and cached it.&lt;/p&gt;
    &lt;p&gt;This works great for reads. Writes are harder.&lt;/p&gt;
    &lt;p&gt;Behind the scenes in read-only mode, Litestream polls, so that we can detect new LTX files created by remote writers to the database. This supports a handy use case where we’re running tests or doing slow analytical queries of databases that need to stay fast in prod.&lt;/p&gt;
    &lt;p&gt;In write mode, we don’t allow multiple writers, because multiple-writer distributed SQLite databases are the Lament Configuration and we are not explorers over great vistas of pain. So the VFS in write-mode disables polling. We assume a single writer, and no additional backups to watch.&lt;/p&gt;
    &lt;p&gt;Next, we buffer. Writes go to a local temporary buffer (“the write buffer”). Every second or so (or on clean shutdown), we sync the write buffer with object storage. Nothing written through the VFS is truly durable until that sync happens.&lt;/p&gt;
    &lt;p&gt;Most storage block maps are much smaller than this, but still.&lt;/p&gt;
    &lt;p&gt;Now, remember the use case we’re looking to support here. A Sprite is cold-starting and its storage stack needs to serve writes, milliseconds after booting, without having a full copy of the 10MB block map. This writeable VFS mode lets us do that.&lt;/p&gt;
    &lt;p&gt;Critically, we support that use case only up to the same durability requirements that a Sprite already has. All storage on a Sprite shares this “eventual durability” property, so the terms of the VFS write make sense here. They probably don’t make sense for your application. But if for some reason they do, have at it! To enable writes with Litestream VFS, just set the &lt;code&gt;LITESTREAM_WRITE_ENABLED&lt;/code&gt; environment variable &lt;code&gt;"true"&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hydration&lt;/head&gt;
    &lt;p&gt;The Sprite storage stack uses SQLite in VFS mode. In our original VFS design, most data is kept in S3. Again: fine at cold start, not so fine in steady state.&lt;/p&gt;
    &lt;p&gt;To solve this problem, we shoplifted a trick from systems like dm-clone: background hydration. In hydration designs, we serve queries remotely while running a loop to pull the whole database. When you start the VFS with the &lt;code&gt;LITESTREAM_HYDRATION_PATH&lt;/code&gt; environment variable set, we’ll hydrate to that file.&lt;/p&gt;
    &lt;p&gt;Hydration takes advantage of LTX compaction, writing only the latest versions of each page. Reads don’t block on hydration; we serve them from object storage immediately, and switch over to the hydration file when it’s ready.&lt;/p&gt;
    &lt;p&gt;As for the hydration file? It’s simply a full copy of your database. It’s the same thing you get if you run &lt;code&gt;litestream restore&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Because this is designed for environments like Sprites, which bounce a lot, we write the database to a temporary file. We can’t trust that the database is using the latest state every time we start up, not without doing a full restore, so we just chuck the hydration file when we exit the VFS. That behavior is baked into the VFS right now. This feature’s got what Sprites need, but again, maybe not what your app wants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting It All Together&lt;/head&gt;
    &lt;p&gt;This is a post about two relatively big moves we’ve made with our open-source Litestream project, but the features are narrowly scoped for problems that look like the ones our storage stack needs. If you think you can get use out of them, I’m thrilled, and I hope you’ll tell me about it.&lt;/p&gt;
    &lt;p&gt;For ordinary read/write workloads, you don’t need any of this mechanism. Litestream works fine without the VFS, with unmodified applications, just running as a sidecar alongside your application. The whole point of that configuration is to efficiently keep up with writes; that’s easy when you know you have the whole database to work with when writes happen.&lt;/p&gt;
    &lt;p&gt;But this whole thing is, to me, a valuable case study in how Litestream can get used in a relatively complicated and demanding problem domain. Sprites are very cool, and it’s satisfying to know that every disk write that happens on a Sprite is running through Litestream.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46893167</guid><pubDate>Wed, 04 Feb 2026 23:03:58 +0000</pubDate></item></channel></rss>