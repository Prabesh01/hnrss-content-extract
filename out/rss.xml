<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 30 Oct 2025 20:11:04 +0000</lastBuildDate><item><title>Replacing EBS and Rethinking Postgres Storage from First Principles</title><link>https://www.tigerdata.com/blog/fluid-storage-forkable-ephemeral-durable-infrastructure-age-of-agents</link><description>&lt;doc fingerprint="3d79b05f0f864bea"&gt;
  &lt;main&gt;
    &lt;p&gt;Category: All posts&lt;/p&gt;
    &lt;p&gt;Oct 29, 2025&lt;/p&gt;
    &lt;p&gt;Fluid Storage is a new next-generation storage architecture: a distributed block layer that reimagines systems like EBS, combining zero-copy forks, true elasticity, and synchronous replication. Because it operates at the block layer, Fluid Storage is fully compatible with Postgres, and even with other databases and file systems as well. Every database in Tiger Cloud’s free tier now runs on Fluid Storage, giving developers direct access to these capabilities.&lt;/p&gt;
    &lt;p&gt;Try it now: Get started instantly with the Tiger CLI and MCP Server.&lt;/p&gt;
    &lt;p&gt;Partner with us: If you’re building an agentic or infrastructure platform, we’re opening early-access partnerships. Please get in touch to learn more.&lt;/p&gt;
    &lt;p&gt;Agents are the new developers, and they need a new storage layer built for how they work.&lt;/p&gt;
    &lt;p&gt;Agents spin up environments, test code, and evolve systems continuously. They need storage that can do the same: forking, scaling, and provisioning instantly, without manual work or waste.&lt;/p&gt;
    &lt;p&gt;Storage itself has evolved through eras: from static systems built for durability, to dynamic systems built for elasticity through innovations like separating compute from storage. But today’s “elastic” infrastructure isn’t truly elastic. After operating tens of thousands of Postgres instances in Tiger Cloud, we’ve seen those limits firsthand: systems that scale slowly, waste capacity, and block iteration. We’re now entering the era of fluid systems: storage that moves as continuously as the workloads it serves.&lt;/p&gt;
    &lt;p&gt;Fluid Storage was built for that world: where data flows, systems iterate autonomously, and elasticity and iteration converge into a single operation.&lt;/p&gt;
    &lt;p&gt;At its core, Fluid Storage is a distributed block layer that unifies these properties through a disaggregated architecture. It combines a horizontally scalable NVMe-backed block store, a proxy layer that exposes copy-on-write volumes, and a user-space storage device driver that makes it all look like a local disk to PostgreSQL. The result: instant forks and snapshots, automatic scaling up or down, and no downtime or over-provisioning.&lt;/p&gt;
    &lt;p&gt;Each Fluid Storage cluster manages tens of thousands of volumes across workloads and tenants—from ephemeral sandboxes to production-scale systems—with consistent performance and predictable cost. In benchmark testing, a single volume sustains 110,000+ IOPS and 1.4 GB/s throughput while retaining all copy-on-write and elasticity guarantees.&lt;/p&gt;
    &lt;p&gt;It’s storage that looks like a local disk but scales like a cloud service. A new storage foundation for the age of agents.&lt;/p&gt;
    &lt;p&gt;Fluid Storage now runs every free-tier database in Tiger Cloud, giving every developer (and agent) a firsthand experience of what truly fluid infrastructure feels like. Sign up in the Tiger console or get started instantly with the Tiger CLI.&lt;/p&gt;
    &lt;p&gt;Elastic storage isn’t truly elastic.&lt;/p&gt;
    &lt;p&gt;Every engineer who’s managed databases at scale has seen it firsthand. A CI pipeline stalls waiting for a restore. A schema migration hangs mid-run because the database can’t be safely cloned. A “resize volume” request on EBS sits in “optimizing” for hours. A database read replica lags for days behind a write-heavy primary. You wait, sometimes all day, not because Postgres is slow, but because the storage substrate beneath it isn’t nearly as elastic as it claims.&lt;/p&gt;
    &lt;p&gt;Cloud storage solved many problems, but true elasticity wasn’t one of them. Amazon EBS, for instance, is billed as elastic, yet volumes can only grow once every six to twenty-four hours. You can’t shrink them, and every operation that changes IOPS or throughput has a similar cooldown. Worse, you pay for the space you allocate, not what you actually use. To avoid running out of disk, you over-provision, and that excess sits idle, wasted.&lt;/p&gt;
    &lt;p&gt;This rigidity made sense in the first two eras of storage. The static era focused on durability: keeping data alive across hardware failures. The dynamic era focused on decoupling compute and storage so each could scale independently. Both have become table stakes. We believe that the next era is the fluid era: storage that scales, forks, and contracts instantly. Not just incrementally elastic, but continuous. Storage that behaves more like software than hardware.&lt;/p&gt;
    &lt;p&gt;The arrival of agents makes this shift urgent. Agents create, modify, and deploy code autonomously. They spin up sandboxes, run migrations, benchmark results, and tear everything down again, all in seconds. Each agent needs its own isolated, ephemeral environment, but with the performance and durability of production, operating on production data. This is not just experimentation; this is how agents work. Traditional storage can’t keep up with that pace or economics.&lt;/p&gt;
    &lt;p&gt;Fluid Storage was built for this reality: a distributed block store that unifies elasticity, iteration, and durability in a single substrate. It treats scaling and forking as standard operations, not exceptions. To Postgres, it looks like a normal disk. In truth, it’s a disaggregated system that delivers high throughput, fast recovery, and instant forks—storage that finally moves as fluidly as the systems built on top of it.&lt;/p&gt;
    &lt;p&gt;Fluid Storage already now serves as the default substrate for our recently-announced free database service on Tiger.&lt;/p&gt;
    &lt;p&gt;For the past five years, we’ve managed tens of thousands of Postgres instances in Tiger Cloud and seen firsthand the limitations of today’s “elastic” cloud infrastructure.&lt;/p&gt;
    &lt;p&gt;When we launched Tiger Cloud in 2020, we built on Amazon EBS as our durable storage. It was reliable, well-understood, and integrated neatly with the rest of AWS. But as our fleet scaled into the thousands of customers, the limitations of EBS became clear across five dimensions: cost, scale-up performance, scale-down performance, elasticity, and recovery.&lt;/p&gt;
    &lt;p&gt;Cost.&lt;/p&gt;
    &lt;p&gt;EBS charges for allocation, not usage. A database storing 200 GB of data on a 1 TB provisioned volume still pays for the full terabyte. Tiger Cloud hides this complexity from users—we bill for storage consumed, not allocated—but that simply shifts the inefficiency from the user to us. It becomes a COGS problem instead of a usability one.&lt;/p&gt;
    &lt;p&gt;To manage it, we built adaptive algorithms that estimate a “good” allocation size based on each user’s consumption and rate of change. In practice, this was a constant balancing act: undershoot and risk running out of disk; overshoot and pay for unused capacity. For safety and user experience, we erred on the side of over-allocation.&lt;/p&gt;
    &lt;p&gt;The problem of EBS cost only compounds as customers scale their services horizontally: every read replica doubles the storage cost, since EBS charges per volume, not per dataset.&lt;/p&gt;
    &lt;p&gt;This cost impacts both sides: vendors through higher COGS, and customers through higher storage prices (which is why usage-based storage always costs more than allocation-based pricing on a per gigabyte basis).&lt;/p&gt;
    &lt;p&gt;Scale-up performance.&lt;/p&gt;
    &lt;p&gt;EBS volumes impose fixed ceilings on performance. Until recently, gp3 volumes topped out at 16,000 IOPS and 16 TB per volume, while io2 volumes offered higher limits—up to 64,000 IOPS and 64 TB—but at far higher cost. Those gp3 limits have recently improved, but they also don’t address another scaling challenge: the difficulty of scaling horizontally through snapshots.&lt;/p&gt;
    &lt;p&gt;Ideally, you’d take a volume snapshot and use it to initialize a new read replica, seeding it with an exact copy of data before WAL replay begins. In theory, EBS snapshots allow this. In practice, they hydrate slowly. A restored snapshot appears immediately available, but data is fetched lazily from S3, so the replica experiences high read latency until the volume is fully loaded. We implement pre-warming strategies informed by a database’s real usage statistics, but these are effective primarily for small databases. For large, mission-critical ones, pre-warming the full working set simply takes too long.&lt;/p&gt;
    &lt;p&gt;In our experience operating Tiger Cloud, it was often faster to spin up a database from a Postgres backup stored in S3 than from an EBS snapshot in S3.&lt;/p&gt;
    &lt;p&gt;Scale-down performance.&lt;/p&gt;
    &lt;p&gt;EBS also limits how many volumes can attach to a single EC2 instance, currently twenty-four. While Tiger Cloud runs a containerized environment, this cap directly constrains how many database services we can host per server. It makes it difficult to run large numbers of small, mostly idle instances in a cost-efficient way. In effect, EBS became a bottleneck for building a true free tier.&lt;/p&gt;
    &lt;p&gt;The alternative for a free or low-cost tier that avoided isolated containers and volumes per service—packing multiple logical databases into a single PostgreSQL cluster—would have introduced operational trade-offs we wanted to avoid: little performance isolation, poor backup and restore options, and difficult seamless scaling.&lt;/p&gt;
    &lt;p&gt;Elasticity.&lt;/p&gt;
    &lt;p&gt;EBS technically supports resizing, but only once every 6–24 hours. That cooldown applies not just to disk capacity increases, but also to any changes in IOPS or throughput. Further, you can grow capacity, but you can’t shrink, and you can’t adjust continuously. We constantly fought these limitations when running our adaptive auto-disk-scaling algorithm.&lt;/p&gt;
    &lt;p&gt;The result is a system that simulates elasticity rather than embodying it, and these limitations ultimately leaked through to the user experience. (You see similar elasticity limitations in other managed database platforms, e.g., Supabase.)&lt;/p&gt;
    &lt;p&gt;Failure recovery.&lt;/p&gt;
    &lt;p&gt;Operational recovery was another bottleneck. In theory, we could detach a failed volume and reattach it to a new node to restore service quickly, even for users who weren’t paying for full HA replication due to cost concerns. In practice, “clean” shutdowns from EBS worked fast (10s of seconds), but any hard failure can often take 10–20 minutes before the EBS volume detaches and becomes available for remounting elsewhere. That delay compounded user downtime precisely when fast recovery mattered most.&lt;/p&gt;
    &lt;p&gt;Agents further expose these limitations&lt;/p&gt;
    &lt;p&gt;Agents make these limitations even sharper. They often operate on sandboxed replicas of production to avoid risk, scale up to work, and scale down just as fast when done. Their workloads are ephemeral, demanding storage that can respond instantly. When many agents work on the same data in parallel, cost efficiency becomes critical.&lt;/p&gt;
    &lt;p&gt;As we began exploring agents for both internal and customer use, it became clear that today’s cloud infrastructure needed to be rethought.&lt;/p&gt;
    &lt;p&gt;Alternative architectures we discarded&lt;/p&gt;
    &lt;p&gt;We also evaluated other architectures—most notably local NVMe and Aurora-like page-server systems—but ultimately set them aside.&lt;/p&gt;
    &lt;p&gt;Local NVMe (e.g., as offered by PlanetScale Metal) offered the raw performance we wanted, but not the durability. If an instance fails, the data disappears with it. To compensate, every customer would need a two-or three-node database cluster for redundancy, which we found cost-prohibitive. NVMe also lacks true storage elasticity: scaling a multi-terabyte service requires copying terabytes of data to a new node and failing over, a process that takes hours or even days. Spinning up a read replica is equally slow. The throughput and latency numbers look impressive in isolation, and NVMe remains a great substrate for caching, but it revives the pre-cloud model—dedicated boxes with fixed compute and disk—which isn’t the foundation for an elastic, managed database platform.&lt;/p&gt;
    &lt;p&gt;Another option was a distributed page-server architecture, first introduced by Amazon Aurora and later adopted by Neon and Google AlloyDB. These systems replace PostgreSQL’s local storage layer with remote operations to a distributed page-storage service that stores pages remotely and uses a separate distributed log for write-ahead logging. It’s an elegant design for elasticity, but it comes at a cost: achieving this requires forking PostgreSQL and rewriting significant portions of its storage internals to communicate with the new remote layer. That coupling introduces real risks. Maintaining parity with upstream PostgreSQL becomes a constant need as new versions evolve; features that depend on storage semantics must be reimplemented and/or revalidated; and debugging and performance tuning also shift from a well-understood ecosystem to a proprietary one. And the result is a database-specific system, rather than a general-purpose storage substrate.&lt;/p&gt;
    &lt;p&gt;We chose to solve these problems at the storage layer. Elasticity, durability, and iteration should be properties of the underlying substrate, not features baked into a single database engine. By working at the block-storage level, we could keep PostgreSQL unchanged while making the system extensible to any workload that runs on disks. A forkable storage layer offers a more general foundation: volumes that can be cloned, branched, or scaled independently of the systems that use them (and not limited to Postgres databases).&lt;/p&gt;
    &lt;p&gt;Fluid Storage emerged from that design choice. A distributed system built for strong durability, true elasticity, and zero-copy forks. It charges for storage consumed, not allocated, and scales fluidly in both directions. Not a faster EBS, but a storage system re-architected for true elasticity and native iteration.&lt;/p&gt;
    &lt;p&gt;When we set out to design our storage architecture, the goal wasn’t just to make storage faster. It was to make it behave differently, to meet the evolving needs of existing workloads and the new needs of agentic workloads. We focused on six core objectives:&lt;/p&gt;
    &lt;p&gt;Fork-first. Forks, clones, and snapshots aren’t exceptional operations; they’re primitives. Copy-on-write happens at the block layer, and creating a new volume or database fork is a highly-efficient metadata update, not a data copy. With such capabilities, one can quickly branch full data environments and test changes in isolation, all without copying or reloading data.&lt;/p&gt;
    &lt;p&gt;Truly elastic, in both directions. Volumes can scale up or down fluidly, adapting to changing workloads and costs. Storage expands as data grows and contracts as it’s deleted or pruned. Developers or platform operators no longer need to over-provision storage based on future need, and are not prevented from downscaling if needed.&lt;/p&gt;
    &lt;p&gt;Usage-based and resource-efficient. You pay for what you use, not what you allocate. Fluid Storage’s multi-tenant design automatically reclaims unused space. Efficiency comes from the architecture itself, not by hiding platform-level waste.&lt;/p&gt;
    &lt;p&gt;Predictable performance. Elasticity shouldn’t mean variability. Fluid maintains stable latency and throughput across tenants through intelligent scheduling and load-balanced data sharding.&lt;/p&gt;
    &lt;p&gt;Postgres-compatible foundation. Applications see a normal Linux block storage device, so PostgreSQL—and any other database or file system—runs unmodified. This ensures immediate compatibility with existing tooling and software systems.&lt;/p&gt;
    &lt;p&gt;Built for both developers and platforms. A single developer can spin up a fork per commit; a platform operator can run thousands of Fluid-backed databases, each isolated, elastic, and cost efficient.&lt;/p&gt;
    &lt;p&gt;The next section describes how the architecture implements them in practice.&lt;/p&gt;
    &lt;p&gt;Fluid Storage consists of three cooperating layers, each designed to make elasticity and iteration first-class operations within the storage substrate itself, not conveniences layered on top.&lt;/p&gt;
    &lt;p&gt;Fluid Storage consists of three cooperating layers:&lt;/p&gt;
    &lt;p&gt;The lowest layer of Fluid Storage is the DBS, a transactional distributed key-value store that provides elastic, horizontally scalable block storage. DBS runs on local NVMe drives for performance. Volumes are divided into shards, each shard mapped to a replica set within the cluster. This architecture allows a single logical volume to scale seamlessly by spreading I/O across shards (and therefore many block servers) in the cluster. Cluster capacity can be dynamically increased or decreased without downtime by adding or removing DBS block servers, and shards are rebalanced transparently across the cluster.&lt;/p&gt;
    &lt;p&gt;DBS maps block addresses (keys) to disk blocks (values). Every write is versioned, allowing old and new values to coexist until garbage collection reclaims the obsolete ones. Transactions provide atomicity and consistency. Data is replicated synchronously across multiple DBS replicas as part of each write operation, ensuring strong consistency and availability. A single DBS cluster can typically manage 100s of TBs of logical storage—the data visible to clients—while the underlying physical storage is larger to accommodate replication.&lt;/p&gt;
    &lt;p&gt;The result is a durable, multi-tenant, horizontally scalable storage layer that supports versioned block writes and enables efficient copy-on-write behavior above it.&lt;/p&gt;
    &lt;p&gt;Above DBS sits a layer of storage proxies, which present virtual volumes to clients and coordinate all I/O. The proxies translate network block operations into DBS reads and writes, manage volume lineage, and enforce per-volume performance and capacity limits.&lt;/p&gt;
    &lt;p&gt;Each volume in Fluid Storage maintains metadata that tracks which blocks exist in which generation of a volume. This metadata defines the lineage of the volume and enables efficient copy-on-write across snapshots and forks. This metadata answers whether a specific block ID exists in a given generation of volume, yet only adds roughly 0.003% overhead, small enough to retain in memory for fast querying.&lt;/p&gt;
    &lt;p&gt;When a snapshot is taken, the system increments the parent volume's generation number (say, 6), and the child volume is created starting at that same new generation number (6). The child stores which previous parent generation it was forked from (5), establishing its lineage. Both parent and child now have their own separate generation directories for future writes. They also maintain separate block metadata for their current generation (6), but have the same information for previous generations (0 through 5). This allows the child to locate and read all of the parent's data without copying any actual blocks.&lt;/p&gt;
    &lt;p&gt;On writes, the proxy allocates a new block in DBS tagged with its volume's current generation number, updates that generation's directory to map the block ID to the new data location, and marks the block as present in that generation. Data from earlier generations remains unchanged.&lt;/p&gt;
    &lt;p&gt;On reads, the system traverses generations from newest to oldest, checking the block metadata for each generation to see if the block exists there. It stops at the first generation where the block is found and reads from that generation's data. This lookup completes in microseconds, as checking membership is extremely fast.&lt;/p&gt;
    &lt;p&gt;This lineage mechanism allows multiple volumes to safely share unmodified blocks. Snapshots are fast and storage-efficient, adding only metadata overhead. Forks are similarly fast and zero-copy, duplicating snapshot mappings without data movement. Physical storage grows only as data diverges and new blocks are written.&lt;/p&gt;
    &lt;p&gt;The storage proxy also provides control and safety at the system boundary. It authenticates clients, maintains secure isolation between tenants, and can enforce per-volume IOPS and storage limits. It can cache frequently accessed blocks to improve locality, though caching is an optimization rather than a requirement.&lt;/p&gt;
    &lt;p&gt;Because agents often spin up parallel instances of themselves, consistency management had to be lightweight, version-aware, and tolerant of many concurrent forks operating on shared data.&lt;/p&gt;
    &lt;p&gt;Together, these responsibilities make the storage proxy the coordination and lineage layer of Fluid Storage. It’s the component that turns distributed blocks into coherent volumes.&lt;/p&gt;
    &lt;p&gt;PostgreSQL expects a block device. Rewriting its storage engine to speak a custom API would break decades of compatibility. So instead, we integrated at the kernel boundary through a user-space storage device driver.&lt;/p&gt;
    &lt;p&gt;The storage device driver exposes Fluid Storage volumes as standard Linux block devices mountable with filesystems such as ext4 or xfs. It manages multiple I/O queues pinned to CPU cores for concurrency, supports zero-copy operations when possible, and allows volumes to be resized dynamically while online. It also provides device recovery primitives which massively simplify our rollouts and error recovery.&lt;/p&gt;
    &lt;p&gt;Because this integration happens at the Linux block layer, Fluid Storage can leverage existing OS-level mechanisms for resource control. Per-volume IOPS and throughput limits can be managed through the Linux environment—using cgroups or similar controls—allowing predictable performance isolation without requiring specialized kernel modifications. Additionally, we can also rely on kernel tools for monitoring and testing our block device.&lt;/p&gt;
    &lt;p&gt;Crucially, this design also means Fluid Storage inherits advances from the broader ecosystem. PostgreSQL 18 introduced support for Linux io_uring, which significantly improves asynchronous I/O throughput, and Linux continues to evolve its block and memory subsystems in similar ways. Because Fluid Storage operates at this layer, it already benefits from recent PostgreSQL and Linux improvements and will continue to inherit future performance gains automatically.&lt;/p&gt;
    &lt;p&gt;Within our Kubernetes infrastructure we seamlessly integrate with our existing orchestration software, because Fluid Storage is presented as a storage class. This enables operations such as resizing volumes or taking Postgres-consistent snapshots to work out-of-the-box, similar to how they function on EBS.&lt;/p&gt;
    &lt;p&gt;Putting these three layers together, one can trace read and write operations in the system. In this case, from the perspective of PostgreSQL backed by Fluid Storage as its underlying block storage.&lt;/p&gt;
    &lt;p&gt;Read Path&lt;/p&gt;
    &lt;p&gt;Write Path&lt;/p&gt;
    &lt;p&gt;While this describes the basic I/O flow, it omits the mechanisms that can improve performance and predictability, such as block caching at either the local instance (through the user-space device driver) or the storage proxy, and management of volume IOPS and throughput at both layers. These controls don’t alter the high-level behavior of Fluid Storage; they make the system more efficient, responsive, and predictable under varying workloads.&lt;/p&gt;
    &lt;p&gt;To PostgreSQL, Fluid Storage simply appears as a local disk. Underneath, it is a disaggregated storage system that scales elastically, replicates safely, and supports fast zero-copy forks. All while maintaining standard block storage semantics.&lt;/p&gt;
    &lt;p&gt;Architecture is meaningful only in what it enables. In Fluid Storage, the results appear along two dimensions: the technical properties that define how the system behaves, and the developer outcomes that emerge from them.&lt;/p&gt;
    &lt;p&gt;Forkability and snapshots: Snapshots are metadata-only and complete very quickly, regardless of volume size. Forks are writable snapshots: they start as zero-copy, then store just the blocks that are updated from its parent snapshot. Storage grows only as data diverges.&lt;/p&gt;
    &lt;p&gt;We ran microbenchmarks on the Fluid Storage layer to measure end-to-end latency for snapshot and fork creation as handled by the storage proxy, across volumes ranging from 1 GB to 100 GB. In all cases, both operations completed within roughly 500–600 milliseconds. These measurements exclude orchestration time (e.g., Kubernetes provisioning and mounting a new volume at a client) and any application-level coordination that may precede a snapshot, such as issuing a PostgreSQL checkpoint. They represent the raw efficiency of the underlying storage system—the baseline latency achievable for instant forks and snapshots under normal load.&lt;/p&gt;
    &lt;p&gt;Elasticity: Volumes expand and contract fluidly with workload changes, eliminating waste from unused allocation. Cluster throughput scales linearly with demand as demand grows, with I/O distributed across DBS block servers. Each volume is sharded across many block servers, reducing hotspots and enabling parallel reads, writes, and recovery.&lt;/p&gt;
    &lt;p&gt;Cost and efficiency: Fluid Storage’s multi-tenant design keeps utilization high across large numbers of colocated volumes. Unmodified blocks are shared across database forks and read replicas, so new forks or replicas consume only the incremental changes they write. Because the platform continuously reclaims and reuses space, it doesn’t need to over-allocate or reserve idle capacity. Usage is billed based on actual consumption, not provisioned size, and the efficiency of this architecture translates directly into lower cost for users.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Latency - p50&lt;p&gt;(ms)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency - p99&lt;p&gt;(ms)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;p&gt;(IOPS)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;p&gt;(MB/s)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Read (random)&lt;/cell&gt;
        &lt;cell&gt;1.30&lt;/cell&gt;
        &lt;cell&gt;1.84&lt;/cell&gt;
        &lt;cell&gt;110,436&lt;/cell&gt;
        &lt;cell&gt;1377&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Read (seq)&lt;/cell&gt;
        &lt;cell&gt;0.97&lt;/cell&gt;
        &lt;cell&gt;1.74&lt;/cell&gt;
        &lt;cell&gt;118,743&lt;/cell&gt;
        &lt;cell&gt;1375&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Write (random)&lt;/cell&gt;
        &lt;cell&gt;5.3&lt;/cell&gt;
        &lt;cell&gt;7.9&lt;/cell&gt;
        &lt;cell&gt;67,137&lt;/cell&gt;
        &lt;cell&gt;689&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Write (seq)&lt;/cell&gt;
        &lt;cell&gt;5.4&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;40,038&lt;/cell&gt;
        &lt;cell&gt;494&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Benchmark results from fio workloads on a Fluid Storage cluster running in a standard production environment on AWS. Reads and writes are generated from user space using direct I/O to bypass the local page cache, exercising the full I/O path described in “Life of a Request.” Latency and IOPS benchmarks use 4 KB blocks, while throughput (MB/s) benchmarks use 512 KB blocks to emulate client-side write coalescing.&lt;/p&gt;
    &lt;p&gt;Performance: Latency remains low and stable across diverse client workloads, supported by asynchronous I/O and distributed sharding. Benchmarks from a standard Fluid Storage deployment in our production environment—run on production-scale client instances without IOPS rate limiting—demonstrate high and consistent performance.&lt;/p&gt;
    &lt;p&gt;As shown in the table above, a single Fluid Storage volume sustains read throughput exceeding 110,000 IOPS and 1.375 GB/s (with read throughput bottlenecked by network bandwidth in its current server configuration). It sustained write throughput between 40,000–67,000 IOPS and 500–700 MB/s. Single-block read latency is typically around 1 ms, and write latency around 5 ms. All writes are synchronously replicated in the DBS before returning to the client, ensuring durability without sacrificing stability.&lt;/p&gt;
    &lt;p&gt;These capabilities change how developers and agents teams build, test, and evolve their systems. In continuous integration and deployment (CI/CD) pipelines, every pull request can run against its own isolated database fork, eliminating the need to queue behind shared test environments. Schema migrations can be rehearsed safely on full-fidelity clones before any production change, reducing rollout risk while preserving realistic data fidelity. Analytics teams can create short-lived copies of production data (without having to actually copy data or pay for it twice), explore results, and discard them when finished.&lt;/p&gt;
    &lt;p&gt;The same primitives that let developers iterate faster also unlock new possibilities for agentic systems.&lt;/p&gt;
    &lt;p&gt;Agents can begin from a clean slate, spinning up an empty database for rapid prototyping, or start from a fork of production data to extend existing behavior or add new capabilities. Each agent operates on its own isolated fork, allowing parallel experiments and reasoning paths to run independently. Once experiments complete, results can be compared: a single branch promoted as the new primary, or code changes merged back into a production fork.&lt;/p&gt;
    &lt;p&gt;In this model, compute can become more ephemeral. Agents and workflows start instantly, run in isolation, and tear down when finished, leaving behind only snapshots that capture stable intermediate states. Snapshots become the natural unit of iteration, something to branch, backtrack, or extend as needed. Fluid Storage makes this loop—fork, test, recover, evolve—both fast and resource-efficient, turning operational data from a single artifact into a dynamic substrate for iteration.&lt;/p&gt;
    &lt;p&gt;In a world where workloads can self-orchestrate—spinning up, scaling, and shutting down autonomously—reliability must be continuous, not coordinated.&lt;/p&gt;
    &lt;p&gt;Reliability and Resilience&lt;/p&gt;
    &lt;p&gt;Fluid Storage is engineered for reliability through four independent layers of resilience—storage replication, database durability, compute recovery, and region-level isolation—each reinforcing the others to ensure consistent operation under failure. All of these mechanisms operate transparently; users never need to manage replicas, tune recovery, or coordinate failover.&lt;/p&gt;
    &lt;p&gt;1. Storage replication.&lt;/p&gt;
    &lt;p&gt;The blocks comprising each volume in Fluid Storage are synchronously replicated across multiple block servers within a DBS cluster. The system automatically detects and compensates for replica failures, rebalancing data and restoring full replication without operator intervention. The storage proxy continuously monitors block-server health, routing around failed nodes to maintain continuity. At the client boundary, the storage device driver retries I/O through its current proxy when transient failures occur, and reconnects to a different proxy if the existing connection is lost. These processes are fully automatic, ensuring strong consistency and continuous availability within the storage tier.&lt;/p&gt;
    &lt;p&gt;2. Database durability.&lt;/p&gt;
    &lt;p&gt;Beyond block-level replication, PostgreSQL durability is maintained through incremental backups and continuous WAL streaming. Each database retains enough WAL for arbitrary point-in-time recovery (PITR); even free services on Tiger Cloud provide 24-hour PITR. These backups and WAL segments are stored independently in S3, ensuring operational isolation from the active storage tier.&lt;/p&gt;
    &lt;p&gt;In the unlikely event of a Fluid Storage tier failure, new database volumes are automatically provisioned and restored from S3. The system also supports transparent migration between EBS-backed storage and Fluid Storage, allowing databases to move seamlessly across tiers if needed.&lt;/p&gt;
    &lt;p&gt;3. Compute recovery.&lt;/p&gt;
    &lt;p&gt;If a compute instance fails (the node running PostgreSQL itself), Tiger Cloud automatically provisions a replacement, reattaches the existing Fluid Storage volume, restarts PostgreSQL, and triggers it to replay its latest WAL segments. Recovery typically completes within tens of seconds, even for single-instance databases without HA replicas. Because the storage proxy maintains no client-side state, reconnection and recovery are immediate once the new compute is available, all without user action.&lt;/p&gt;
    &lt;p&gt;4. Region resilience.&lt;/p&gt;
    &lt;p&gt;Fluid Storage supports both single- and multi-availability zone (AZ) deployments for a single DBS cluster. In multi-AZ configurations, sharded replica sets are distributed across zones to tolerate AZ-level failures. These modes represent a tradeoff: cross-AZ replication improves resilience but increases latency and inter-AZ network costs.&lt;/p&gt;
    &lt;p&gt;Current deployments favor single-AZ clusters for lower latency and cost efficiency, while cross-AZ durability is achieved through PostgreSQL’s own high-availability replication. In this model, each database node in a multi-AZ cluster uses an independent Fluid Storage cluster in its respective zone. This design requires a full copy of storage per zone (at higher cost) but provides stronger fault isolation: each cluster operates with its own control plane, minimizing correlated failures across zones. This coordination is handled automatically by the system; the complexity is abstracted away from the user.&lt;/p&gt;
    &lt;p&gt;Availability and Access&lt;/p&gt;
    &lt;p&gt;Fluid Storage already serves customer-facing workloads within Tiger Cloud and powers all databases in our new free tier. Developers can create, pause, resume, fork, and snapshot databases directly through the cloud console, REST API, Tiger CLI, or the Tiger MCP server, making it easy to experiment with elastic and fork-first behavior in practice.&lt;/p&gt;
    &lt;p&gt;The system is available today as a public beta for the free tier, with larger-scale workloads being onboarded gradually through early-access programs. General availability will follow sustained operation across a broad set of customer environments, ensuring that Fluid Storage meets the standards of maturity, reliability, and performance expected of a core database storage platform.&lt;/p&gt;
    &lt;p&gt;Fluid Storage introduces our next-generation storage architecture: a distributed block layer that combines synchronous replication, true elasticity, and zero-copy forks. It delivers predictable performance, efficient scaling up or down, and rapid recovery. It does this all while remaining fully compatible with PostgreSQL and, because it operates at the block storage layer, also remaining fully compatible with other databases and file systems as well.&lt;/p&gt;
    &lt;p&gt;Every database in Tiger Cloud’s free tier now runs on Fluid Storage, giving developers direct access to these capabilities.&lt;/p&gt;
    &lt;p&gt;This foundation opens new ways to build, test, and extend data-driven systems. From faster iteration in developer workflows to more autonomous, agentic applications, Fluid Storage makes data as dynamic as the systems built on it. Because if agents are the new developers, storage must evolve to match their speed.&lt;/p&gt;
    &lt;p&gt;You can try Fluid Storage today through Tiger’s free services: get started instantly with the Tiger CLI and MCP Server.&lt;/p&gt;
    &lt;p&gt;And if you’re building an agentic or infrastructure platform and want to explore how Tiger’s databases powered by Fluid Storage can support your workloads, we’re opening early-access partnerships. Please get in touch to learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748484</guid><pubDate>Wed, 29 Oct 2025 15:49:18 +0000</pubDate></item><item><title>Independently verifying Go's reproducible builds</title><link>https://www.agwa.name/blog/post/verifying_go_reproducible_builds</link><description>&lt;doc fingerprint="2e81393b0f9635b2"&gt;
  &lt;main&gt;
    &lt;p&gt;October 29, 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;I'm Independently Verifying Go's Reproducible Builds&lt;/head&gt;
    &lt;p&gt;When you try to compile a Go module that requires a newer version of the Go toolchain than the one you have installed, the go command automatically downloads the newer toolchain and uses it for compiling the module. (And only that module; your system's go installation is not replaced.) This useful feature was introduced in Go 1.21 and has let me quickly adopt new Go features in my open source projects without inconveniencing people with older versions of Go.&lt;/p&gt;
    &lt;p&gt;However, the idea of downloading a binary and executing it on demand makes a lot of people uncomfortable. It feels like such an easy vector for a supply chain attack, where Google, or an attacker who has compromised Google or gotten a misissued SSL certificate, could deliver a malicious binary. Many developers are more comfortable getting Go from their Linux distribution, or compiling it from source themselves.&lt;/p&gt;
    &lt;p&gt;To address these concerns, the Go project did two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;They made it so every version of Go starting with 1.21 could be easily reproduced from its source code. Every time you compile a Go toolchain, it produces the exact same Zip archive, byte-for-byte, regardless of the current time, your operating system, your architecture, or other aspects of your environment (such as the directory from which you run the build).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They started publishing the checksum of every toolchain Zip archive in a public transparency log called the Go Checksum Database. The go command verifies that the checksum of a downloaded toolchain is published in the Checksum Database for anyone to see.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These measures mean that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You can be confident that the binaries downloaded and executed by the go command are the exact same binaries you would have gotten had you built the toolchain from source yourself. If there's a backdoor, the backdoor has to be in the source code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can be confident that the binaries downloaded and executed by the go command are the same binaries that everyone else is downloading. If there's a backdoor, it has to be served to the whole world, making it easier to detect.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But these measures mean nothing if no one is checking that the binaries are reproducible, or that the Checksum Database isn't presenting inconsistent information to different clients. Although Google checks reproducibility and publishes a report, this doesn't help if you think Google might try to slip in a backdoor themselves. There needs to be an independent third party doing the checks.&lt;/p&gt;
    &lt;p&gt;Why not me? I was involved in Debian's Reproducible Builds project back in the day and developed some of the core tooling used to make Debian packages reproducible (strip-nondeterminism and disorderfs). I also have extensive experience monitoring Certificate Transparency logs and have detected misbehavior by numerous logs since 2017. And I do not work for Google (though I have eaten their food).&lt;/p&gt;
    &lt;p&gt;In fact, I've been quietly operating an auditor for the Go Checksum Database since 2020 called Source Spotter (à la Cert Spotter). Source Spotter monitors the Checksum Database, making sure it doesn't present inconsistent information or publish more than one checksum for a given module and version. I decided to extend Source Spotter to also verify toolchain reproducibility.&lt;/p&gt;
    &lt;p&gt; The Checksum Database was originally intended for recording the checksums of Go modules. Essentially, it's a verifiable, append-only log of records which say that a particular version (e.g. &lt;code&gt;v0.4.0&lt;/code&gt;) of a module (e.g. &lt;code&gt;src.agwa.name/snid&lt;/code&gt;) has a particular SHA-256 hash.  Go repurposed
it for recording toolchain checksums.  Toolchain records have the pseudo-module
&lt;code&gt;golang.org/toolchain&lt;/code&gt; and versions that look like &lt;code&gt;v0.0.1-goVERSION.GOOS-GOARCH&lt;/code&gt;.  For example, the Go1.24.2 toolchain for linux/amd64 has the module version &lt;code&gt;v0.0.1-go1.24.2.linux-amd64&lt;/code&gt;.
&lt;/p&gt;
    &lt;p&gt; When Source Spotter sees a new version of the &lt;code&gt;golang.org/toolchain&lt;/code&gt; pseudo-module,
it downloads the corresponding source code, builds it in an AWS Lambda function by running &lt;code&gt;make.bash -distpack&lt;/code&gt;,
and compares the checksum
of the resulting Zip file to the checksum published in the Checksum Database.  Any mismatches
are published on a webpage and
in an Atom feed which I monitor.
&lt;/p&gt;
    &lt;p&gt;So far, Source Spotter has successfully reproduced every toolchain since Go 1.21.0, for every architecture and operating system. As of publication time, that's 2,672 toolchains!&lt;/p&gt;
    &lt;head rend="h4"&gt;Bootstrap Toolchains&lt;/head&gt;
    &lt;p&gt;Since the Go toolchain is written in Go, building it requires an earlier version of the Go toolchain to be installed already.&lt;/p&gt;
    &lt;p&gt;When reproducing Go 1.21, 1.22, and 1.23, Source Spotter uses a Go 1.20.14 toolchain that I built from source. I started by building Go 1.4.3 using a C compiler. I used Go 1.4.3 to build Go 1.17.13, which I used to build Go 1.20.14. To mitigate Trusting Trust attacks, I repeated this process on both Debian and Amazon Linux using both GCC and Clang for the Go 1.4 build. I got the exact same bytes every time, which I believe makes a compiler backdoor vanishingly unlikely. The scripts I used for this are open source.&lt;/p&gt;
    &lt;p&gt;When reproducing Go 1.24 or higher, Source Spotter uses a binary toolchain downloaded from the Go module proxy that it previously verified as being reproducible from source.&lt;/p&gt;
    &lt;head rend="h4"&gt;Problems Encountered&lt;/head&gt;
    &lt;p&gt;Compared to reproducing a typical Debian package, it was really easy to reproduce the same bytes when building the Go toolchains. Nevertheless, there were some bumps along the way:&lt;/p&gt;
    &lt;p&gt;First, the Darwin (macOS) toolchains published by Google contain signatures produced by Google's private key. Obviously, Source Spotter can't reproduce these. Instead, Source Spotter has to download the toolchain (making sure it matches the checksum published in the Checksum Database) and strip the signatures to produce a new checksum that is verified against the reproduced toolchain. I reused code written by Google to strip the signatures and I honestly have no clue what it's doing and whether it could potentially strip a backdoor. A review from someone versed in Darwin binaries would be very helpful!&lt;/p&gt;
    &lt;p&gt; Second, to reproduce the linux-arm toolchains, Source Spotter has to set &lt;code&gt;GOARM=6&lt;/code&gt; in the environment... except when reproducing Go 1.21.0, which
Google accidentally built using &lt;code&gt;GOARM=7&lt;/code&gt;.
I don't understand why cmd/dist (the tool used to build the
toolchain) doesn't set this environment variable along with the many other environment variables it sets.
&lt;/p&gt;
    &lt;p&gt;Finally, the Checksum Database contains a toolchain for Go 1.9.2rc2, which is not a valid version number. It turns out this version was released by mistake. To avoid raising an error for an invalid version number, Source Spotter has to special case it. Not a huge deal, but I found it interesting because it demonstrates one of the downsides of transparency logs: you can't fix or remove entries that were added by mistake!&lt;/p&gt;
    &lt;head rend="h4"&gt;Source Code Transparency&lt;/head&gt;
    &lt;p&gt;Although the toolchain binaries are published in the Checksum Database, the source code is not. This means Google could serve Source Spotter, and only Source Spotter, source code which contains a backdoor. To mitigate this, Source Spotter publishes the checksums of every source tarball it builds.&lt;/p&gt;
    &lt;p&gt; Filippo suggested that Source Spotter build from Go's Git repository and publish the Git commit IDs instead, since lots of Go developers have the Go Git repository checked out and it would be relatively easy for them to compare the state of their repos against what Source Spotter has seen. Regrettably, Git commit IDs are SHA-1, but this is mitigated by Git's use of Marc Stevens' collision detection, so the benefits may be worth the risk. I think building from Git is a good idea, and to bootstrap it, Filippo used Magic Wormhole to send me the output of &lt;code&gt;git show-ref --tags&lt;/code&gt; from his repo while we were both
at the Transparency.dev Summit last week.
&lt;/p&gt;
    &lt;p&gt;Ultimately, I would like to see the Go project publish source tarballs in the Checksum Database.&lt;/p&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Thanks to Go's Checksum Database and reproducible toolchains, Go developers get the usability benefits of a centralized package repository and binary toolchains without sacrificing the security benefits of decentralized packages and building from source. The Go team deserves enormous credit for making this a reality, particularly for building a system that is not too hard for a third party to verify. They've raised the bar, and I hope other language and package ecosystems can learn from what they've done.&lt;/p&gt;
    &lt;p&gt;Learn more by visiting the Source Spotter website or the GitHub repo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post a Comment&lt;/head&gt;
    &lt;p&gt;Your comment will be public. To contact me privately, email me. Please keep your comment polite, on-topic, and comprehensible. Your comment may be held for moderation before being published.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45751868</guid><pubDate>Wed, 29 Oct 2025 19:32:00 +0000</pubDate></item><item><title>NPM flooded with malicious packages downloaded more than 86k times</title><link>https://arstechnica.com/security/2025/10/npm-flooded-with-malicious-packages-downloaded-more-than-86000-times/</link><description>&lt;doc fingerprint="771a9388954b799"&gt;
  &lt;main&gt;
    &lt;p&gt;Attackers are exploiting a major weakness that has allowed them access to the NPM code repository with more than 100 credential-stealing packages since August, mostly without detection.&lt;/p&gt;
    &lt;p&gt;The finding, laid out Wednesday by security firm Koi, brings attention to an NPM practice that allows installed packages to automatically pull down and run unvetted packages from untrusted domains. Koi said a campaign it tracks as PhantomRaven has exploited NPM’s use of “Remote Dynamic Dependencies” to flood NPM with 126 malicious packages that have been downloaded more than 86,000 times. Some 80 of those packages remained available as of Wednesday morning, Koi said.&lt;/p&gt;
    &lt;head rend="h2"&gt;A blind spot&lt;/head&gt;
    &lt;p&gt;“PhantomRaven demonstrates how sophisticated attackers are getting [better] at exploiting blind spots in traditional security tooling,” Koi’s Oren Yomtov wrote. “Remote Dynamic Dependencies aren’t visible to static analysis.”&lt;/p&gt;
    &lt;p&gt;Remote Dynamic Dependencies provide greater flexibility in accessing dependencies—the code libraries that are mandatory for many other packages to work. Normally, dependencies are visible to the developer installing the package. They’re usually downloaded from NPM’s trusted infrastructure.&lt;/p&gt;
    &lt;p&gt;RDD works differently. It allows a package to download dependencies from untrusted websites, even those that connect over HTTP, which is unencrypted. The PhantomRaven attackers exploited this leniency by including code in the 126 packages uploaded to NPM. The code downloads malicious dependencies from URLs, including http://packages.storeartifact.com/npm/unused-imports. Koi said these dependencies are “invisible” to developers and many security scanners. Instead, they show the package contains “0 Dependencies.” An NPM feature causes these invisible downloads to be automatically installed.&lt;/p&gt;
    &lt;p&gt;Compounding the weakness, the dependencies are downloaded “fresh” from the attacker server each time a package is installed, rather than being cached, versioned, or otherwise static, as Koi explained:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45755027</guid><pubDate>Thu, 30 Oct 2025 00:37:33 +0000</pubDate></item><item><title>Show HN: In a single HTML file, an app to encourage my children to invest</title><link>https://roberdam.com/en/dinversiones.html</link><description>&lt;doc fingerprint="26c9c0412db7d00b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Built an App to Encourage My Kids to Invest â Just One HTML File&lt;/head&gt;
    &lt;p&gt;âWhat comes with the milk, leaves with the soulâ&lt;lb/&gt; â Russian proverb.&lt;/p&gt;
    &lt;p&gt;Access the app:&lt;lb/&gt; Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/p&gt;
    &lt;p&gt;One thing that school doesnât teach you (not even high school) is how to manage your personal finances.&lt;/p&gt;
    &lt;p&gt;As my eldest sonâs birthday was approaching, we suggested that instead of asking for physical gifts, he ask for their equivalent in money. That way, he gathered a decent amount of capital for his first investment adventure.&lt;/p&gt;
    &lt;p&gt;I explained to my kids that investing is like having a magic box that generates more money over time. To make it more visual and interactive, I decided to create a small app where they could see their investment grow day by day.&lt;/p&gt;
    &lt;head rend="h1"&gt;From Idea to App&lt;/head&gt;
    &lt;p&gt;My first idea was to build a physical piggy bank with a display, showing the accumulated amount. However, that mixed up the concept of saving with investing, and also required buying extra hardware.&lt;/p&gt;
    &lt;p&gt;So I looked for a quicker, cheaper way: revive an old smartphone and create a simple app using plain HTML.&lt;/p&gt;
    &lt;p&gt;The result was D-i&lt;del&gt;n&lt;/del&gt;vestments, a mix between Diversions and Investments.&lt;/p&gt;
    &lt;head rend="h1"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;The app is essentially a single HTML file that installs on the phone as a PWA (Progressive Web App).&lt;/p&gt;
    &lt;p&gt;The phone is attached to the fridge and works as a panel or dashboard where my kids can see their money growing each day.&lt;/p&gt;
    &lt;p&gt;I act as their investment agent, assigning realistic interest rates â high enough to keep them motivated, but moderate enough to reflect how the real world works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuration Screen&lt;/head&gt;
    &lt;p&gt;The app includes a screen where you can enter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The kidsâ names&lt;/item&gt;
      &lt;item&gt;The invested amount&lt;/item&gt;
      &lt;item&gt;The interest rate&lt;/item&gt;
      &lt;item&gt;The start date&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that data, the app automatically calculates and displays:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daily gain&lt;/item&gt;
      &lt;item&gt;Weekly gain&lt;/item&gt;
      &lt;item&gt;Monthly gain&lt;/item&gt;
      &lt;item&gt;Total updated balance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Materials Used&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An old smartphone&lt;/item&gt;
      &lt;item&gt;A suction mount to attach it to the fridge&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The D-iNvestments app, in HTML format&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Installation&lt;/head&gt;
    &lt;p&gt;The process is as simple as opening the link from a smartphone and tapping âInstallâ when prompted by the browser.&lt;lb/&gt; From then on, it behaves like a native app.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Access the app:&lt;/p&gt;&lt;lb/&gt;Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/quote&gt;
    &lt;head rend="h1"&gt;Final Reflection&lt;/head&gt;
    &lt;p&gt;The goal wasnât just to teach my kids the value of money, but to show them visually how investment and time work as allies.&lt;/p&gt;
    &lt;p&gt;Each day, as they watch their small fund grow, they grasp the magic of compound interest â and that, more than any gift, is a lesson I hope will stay with them for life.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;ð¬ Want to comment or improve the app? Contact me at:&lt;/p&gt;&lt;lb/&gt;@roberdam&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45758421</guid><pubDate>Thu, 30 Oct 2025 10:39:21 +0000</pubDate></item><item><title>Jujutsu at Google [video]</title><link>https://www.youtube.com/watch?v=v9Ob5yPpC0A</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45759572</guid><pubDate>Thu, 30 Oct 2025 13:00:58 +0000</pubDate></item><item><title>Estimating the perceived 'claustrophobia' of New York City's streets (2024)</title><link>http://mfranchi.net/posts/claustrophobic-streets/</link><description>&lt;doc fingerprint="20890016823e03c7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Estimating the Perceived 'Claustrophobia' of New York City's Streets&lt;/head&gt;
    &lt;p&gt;Hi! I’m back with a supplement to a piece in the New York Times’ Street Wars series, which is all about the battle for navigable urban space in New York City. In this article, I’ll go more in-depth about how I devised &amp;amp; computed the claustrophobic metric. We plan to explore this metric further through an in-progress research paper; keep an eye out for that later this year! Let’s begin.&lt;/p&gt;
    &lt;p&gt;New York City is a large place; almost 469 square miles of pretty dense civilization. Within the city, there are thousands of miles of sidewalks. As you walk through different neighborhoods, you may experience a variety of different atmospheres. In Cobble Hill, Brooklyn, it’s quaint and quiet. In SoHo these days, there are so many pedestrians that they spill off the narrow sidewalks. While a neighborhood’s atmosphere is, of course, a function of time, it is possible to get an average consensus of how ‘crowded’ each neighborhood feels by averaging over time. When we say ‘crowded’, we mean not just with people; we also mean with static objects, or street furniture, or, to get even more colloquial, ‘clutter’. When we mix ‘crowdedness’ within the narrow environment of NYC’s sidewalks, we endeavor to call this feeling ‘claustrophobia’, a direct mapping to the definition in psychology.&lt;/p&gt;
    &lt;p&gt;Now, we’ll discuss how the metric of sidewalk ‘claustrophobia’ was calculated. Then, I’ll talk briefly about how this metric might be interesting and useful to a variety of different stakeholders.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology - Segmentization&lt;/head&gt;
    &lt;p&gt;We start with the official planimetric database of NYC’s sidewalks from NYC OpenData at this link. However, the geometries for each sidewalk here are stored as multi-polygons, instead of at the per-segment level. Further, the geometry can be quite complicated, in fact, overly complex for the purposes of our analysis. To mitigate these problems, we perform the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Simplify geometry using Shapely library&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Here, we first simplify the sidewalk geometry to reduce some of the complexity in the street network. We visually inspect several different neighborhoods and find that this minimally changes the shape of the network while moderately reducing the number of points after segmentization.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Segmentize points along sidewalks at least every 50 feet.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Then, we segmentize the simplified sidewalk network. Segmentization is a process that evenly samples points along each sidewalk, at a predetermined threshold. We use a threshold of 50 feet to balance computational complexity and storage constraints with accuracy.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Methodology - Bringing in Clutter&lt;/head&gt;
    &lt;p&gt;For this computational analysis, ‘clutter’ is anything that takes up space on the sidewalk. Narratively, some clutter is aesthetic or unminded by pedestrians (like trees, most seating); then, things like scaffolding are denotatively and connotatively ‘clutter’. To identify different types of clutter, I took walks around several different neighborhoods in Brooklyn, Queens, and Manhattan, writing down the different things that I saw. At this point, I tried to match each type of street furniture I saw with a dataset on NYC OpenData, which is a great, official portal that stores hundred of city-related datasets from dozens of city agencies like the Department of Transportation and NYC Parks. To save space, I list all of the datasets I used in next section’s table, along with an access link.&lt;/p&gt;
    &lt;p&gt;We assign points to different clutters with spatial joins. For each point, we add a buffer (think of this as a larger ring, centered at the point) of 25 feet. These buffers act as a net, ‘catching’ nearby pieces of clutter. Multiple points may count a piece of clutter as ‘theirs’ if the clutter is within both points’ buffer area.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology - Weighting&lt;/head&gt;
    &lt;p&gt;We apply a weight to each clutter type based on its estimated size. I admit this is quite naive, and solely based on my ‘experience’ as a pedestrian. In the below table, we present the weight allotted to each clutter type.&lt;/p&gt;
    &lt;p&gt;Possible ways to refine this include conducting a survey, or actually taking into account the square footage of each clutter type. Since some clutters have non-uniform sizes (for example, there are several different configurations of bus stops, each with a different size), and size data was unavailable for some clutter types, we stick with the naive approach for now.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Clutter Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Weight&lt;/cell&gt;
        &lt;cell role="head"&gt;OpenData Link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bus Stop Shelters&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Trash Can&lt;/cell&gt;
        &lt;cell&gt;0.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LinkNYC&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CityBench&lt;/cell&gt;
        &lt;cell&gt;1.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bicycle Parking Shelter&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bicycle Rack&lt;/cell&gt;
        &lt;cell&gt;1.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tree&lt;/cell&gt;
        &lt;cell&gt;0.15&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Newsstand&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Parking Meter&lt;/cell&gt;
        &lt;cell&gt;0.15&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scaffolding&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Fire Hydrant&lt;/cell&gt;
        &lt;cell&gt;0.25&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Street Signs&lt;/cell&gt;
        &lt;cell&gt;0.05&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Methodology - Traffic&lt;/head&gt;
    &lt;p&gt;We derive our foot traffic estimates via large-scale, crowdsourced dashcam data provided by Nexar, Inc. Nexar is a company that manufactures dashcams and explores how downstream data can help make more useful and accurate maps. Of course, these images tell us nothing about pedestrians by themselves. We detect the number of pedestrians in each image via YOLOv7-E6E, a state-of-the-art object detection model with well-documented success in this task.&lt;/p&gt;
    &lt;p&gt;Dashcam data points are stored at the latitude/longitude level with a 0-360 ranged directional heading. So, in other words, if you were plotting each point on a map, you’d know exactly where to put it on the map, and you’d also know which direction to put an arrow facing outwards from the point. We then project the points from a Cartesian coordinate system to a NYC-specific projected coordinate system for increased accuracy. To combine the position and the heading, we further increase positional accuracy by creating ‘cones’ to represent the actual field-of-view of the vehicle at the time/place of capture.&lt;/p&gt;
    &lt;p&gt;With access to sidewalk width (in feet) from the basemap described earlier, we compute the number of pedestrians per foot of sidewalk width, at each image. This traffic data is sliced at one-hour increments (this is arbitrary), but our main plots don’t go to this granularity and instead converge at ‘a typical day of traffic’ in NYC in August 2023.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;This work is very exciting for us, but still has some unconquered limitations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Missing Data Streams&lt;/head&gt;
    &lt;p&gt;First, there are several clutter types that we identified in our walks around the city, but couldn’t find matching datasets for. This includes, but is not limited to: - Sidewalk eating - Roadside eating - Street lights (could infer via 311 complaints, but haven’t yet done) - Red legacy FDNY alarm boxes - Cellars (not a problem unless open) - Sidewalk plant beds - USPS / Package drop-off containers - Streetside produce markets&lt;/p&gt;
    &lt;p&gt;Separately, for our dashcam-computed foot traffic counts, we are missing data for about 36.11% of the segmentized points in NYC. This isn’t a huge problem, as we aggregate data at much larger geographic groupings like Neighborhood Tabulation Areas and Census Tracts. Nonetheless, it would be more ideal to use a dataset with more complete coverage of the city. The more temporally and geographically dense data we have, the more we can ‘zoom’ in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Imprecise Data Streams&lt;/head&gt;
    &lt;p&gt;In addition to missing data streams, we also use some that are notably imprecise.&lt;/p&gt;
    &lt;p&gt;For example, New York City’s sidewalk scaffolding is logged at the building permit level, so we compute a radial 150-foot outward buffer to capture all nearby points; in reality, only part of a building’s perimeter will actually have the scaffolding.&lt;/p&gt;
    &lt;p&gt;Other imprecision comes from “old” data; for most of the clutter types, we’re able to filter by construction date, meaning that clutters built after the end of our traffic data aren’t included. However, this isn’t possible for all clutter types (we look at this more closely in the “clutter.ipynb” notebook on the claustrophobic-streets GitHub repository).&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Let’s start with some spatial visualizations. Below, we map our calculated levels of claustrophobia at the Neighborhood Tabulation Area (NTA) level. NTAs are roughly approximate to New Yorkers’ mental maps of neighborhoods. Some interesting trends emerge that made sense at first glance, at least for me! Namely, most of Midtown Manhattan sees the highest ‘claustrophobia’ levels. This aligns with my anecdotal experience of trudging through crowds in and around Times Square that were quite literally stationary for 15 seconds at a time. However, at this small granularity, we can’t really see where other hotspots emerge clearly, at least from the map’s coloring. Let’s try zooming in.&lt;/p&gt;
    &lt;p&gt;Now, we map our calculated levels of claustrophobia at the Census Tract (CT) level. CTs are much smaller; in 2020, NYC was composed of 2,327 of them. Here, more interesting visual trends emerge. I see areas in Queens colored in red that I remember being extremely crowded when I visited; including Jackson Heights (near LaGuardia Airport) and Flushing (much further out in Queens, at the end of the 7 train). Downtown Brooklyn and Williamsburg also see notably higher-than-average levels of claustrophobia, which lines up with my own experiences. For both maps, Staten Island tends to be colored entirely in blue, meaning lower-than-average claustrophobia; I’ve still not taken the ferry over, so I won’t make any definitive claims, but this at least aligns with the borough’s higher usage of cars, relative to the rest of the city.&lt;/p&gt;
    &lt;p&gt;Lastly, for name recognition, we plot the top 20 and bottom 20 neighborhoods, relative to the city average. See if your neighborhood pops up in either list!&lt;/p&gt;
    &lt;p&gt;For some additional plots, including zoom-ins of each borough, and density maps of foot traffic and street clutter, check out the GitHub repository for this project at github.com/mattwfranchi/claustrophobic-streets. If you have any suggestions or questions, feel free to email me at mwf62 AT cornell.edu, or open an issue on the GitHub repository. Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45759649</guid><pubDate>Thu, 30 Oct 2025 13:10:03 +0000</pubDate></item><item><title>Show HN: I made a heatmap diff viewer for code reviews</title><link>https://0github.com</link><description>&lt;doc fingerprint="5f30552b55a047fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Heatmap color-codes every diff line/token by how much human attention it probably needs. Unlike PR-review bots, we try to flag not just by “is it a bug?” but by “is it worth a second look?” (examples: hard-coded secret, weird crypto mode, gnarly logic).&lt;/p&gt;
    &lt;p&gt;To try it, replace github.com with 0github.com in any GitHub pull request url. Under the hood, we clone the repo into a VM, spin up gpt-5-codex for every diff, and ask it to output a JSON data structure that we parse into a colored heatmap.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;p&gt;Heatmap is open source:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45760321</guid><pubDate>Thu, 30 Oct 2025 14:21:58 +0000</pubDate></item><item><title>US declines to join more than 70 countries in signing UN cybercrime treaty</title><link>https://therecord.media/us-declines-signing-cybercrime-treaty?</link><description>&lt;doc fingerprint="8b0db7af9a95f28f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;US declines to join more than 70 countries in signing UN cybercrime treaty&lt;/head&gt;
    &lt;p&gt;More than 70 countries signed the landmark U.N. Convention against Cybercrime in Hanoi this weekend, a significant step in the yearslong effort to create a global mechanism to counteract digital crime.&lt;/p&gt;
    &lt;p&gt;The U.K. and European Union joined China, Russia, Brazil, Nigeria and dozens of other nations in signing the convention, which lays out new mechanisms for governments to coordinate, build capacity and track those who use technology to commit crimes.&lt;/p&gt;
    &lt;p&gt;In his speech at the event, U.N. Secretary-General António Guterres said cyberspace “has become fertile ground for criminals” and has allowed them to “defraud families, steal livelihoods, and drain billions of dollars from our economies.”&lt;/p&gt;
    &lt;p&gt;“The UN Cybercrime Convention is a powerful, legally binding instrument to strengthen our collective defences against cybercrime,” Guterres said.&lt;/p&gt;
    &lt;p&gt;“Illicit flows of money, concealed through cryptocurrencies and digital transactions, finance the trafficking of drugs, arms, and terror. And businesses, hospitals, and airports are brought to a standstill by ransomware attacks.”&lt;/p&gt;
    &lt;p&gt;He added that the convention would be critical for governments in the Global South that need assistance and funding for the training required to address cybercrime — which the U.N. estimates costs $10.5 trillion around the world annually.&lt;/p&gt;
    &lt;p&gt;While many countries did not sign the treaty, the most notable missing signature was that of the U.S.&lt;/p&gt;
    &lt;p&gt;Officials at the State Department told Recorded Future News on Friday that Marc Knapper, the U.S. ambassador to Vietnam, and representatives from the U.S. Mission to Vietnam would be attending the signing.&lt;/p&gt;
    &lt;p&gt;The State Department confirmed on Monday that the U.S. did not sign the treaty.&lt;/p&gt;
    &lt;p&gt;“The United States continues to review the treaty,” a State Department spokesperson said in a brief statement.&lt;/p&gt;
    &lt;p&gt;The U.N. Convention against Cybercrime was adopted by the General Assembly in December 2024 and will enter into force 90 days after being ratified by the 40th signatory. Signatories will have to ratify the convention according to their own procedures.&lt;/p&gt;
    &lt;p&gt;At the ceremony, UNODC Executive Director Ghada Waly argued that cybercrime is changing the face of organized crime and required global coordination to address. Waly said the convention would be a “vital tool” that will ensure “a safer digital world for all.”&lt;/p&gt;
    &lt;p&gt;U.N. officials said the convention would help governments address terrorism, human trafficking, money laundering and drug smuggling, all of which have been turbo-charged by the internet.&lt;/p&gt;
    &lt;p&gt;The U.N. noted that the convention is the first global framework “for the collection, sharing and use of electronic evidence for all serious offenses” — noting that until now there have been no broadly accepted international standards on electronic evidence.&lt;/p&gt;
    &lt;p&gt;It is also the first global treaty to criminalize crimes that depend on the internet and is the first international treaty “to recognize the non-consensual dissemination of intimate images as an offense.”&lt;/p&gt;
    &lt;p&gt;“It creates the first global 24/7 network where countries can quickly initiate cooperation,” the U.N. said. “It recognizes and promotes the need to build capacity in countries to pursue and cooperate on fast-moving cybercrimes.”&lt;/p&gt;
    &lt;p&gt;The convention has been heavily criticized by the tech industry, which has warned that it criminalizes cybersecurity research and exposes companies to legally thorny data requests.&lt;/p&gt;
    &lt;p&gt;Human rights groups warned on Friday that it effectively forces member states to create a broad electronic surveillance dragnet that would include crimes that have nothing to do with technology.&lt;/p&gt;
    &lt;p&gt;Many expressed concern that the convention will be abused by dictatorships and rogue governments who will deploy it against critics or protesters — even those outside of a regime’s jurisdiction.&lt;/p&gt;
    &lt;p&gt;It also creates legal regimes to monitor, store and allow cross-border sharing of information without specific data protections. Access Now’s Raman Jit Singh Chima said the convention effectively justifies “cyber authoritarianism at home and transnational repression across borders.”&lt;/p&gt;
    &lt;p&gt;Any countries ratifying the treaty, he added, risks “actively validating cyber authoritarianism and facilitating the global erosion of digital freedoms, choosing procedural consensus over substantive human rights protection.”&lt;/p&gt;
    &lt;p&gt;In his speech, Guterres referenced the backlash to the convention, telling member states that the treaty has to be a “promise that fundamental human rights such as privacy, dignity, and safety must be protected both offline and online.”&lt;/p&gt;
    &lt;p&gt;But at its core, according to Guterres, the convention solves one of the thorniest issues law enforcement agencies have faced over the last two decades. Countries have only recently begun to share digital evidence across borders but the convention would increase that practice.&lt;/p&gt;
    &lt;p&gt;“This has long been a major obstacle to justice — with perpetrators in one country, victims in another, and data stored in a third,” he said. “The Convention provides a clear pathway for investigators and prosecutors to finally overcome this barrier.”&lt;/p&gt;
    &lt;p&gt;Jonathan Greig&lt;/p&gt;
    &lt;p&gt;is a Breaking News Reporter at Recorded Future News. Jonathan has worked across the globe as a journalist since 2014. Before moving back to New York City, he worked for news outlets in South Africa, Jordan and Cambodia. He previously covered cybersecurity at ZDNet and TechRepublic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45760328</guid><pubDate>Thu, 30 Oct 2025 14:22:44 +0000</pubDate></item><item><title>Free software scares normal people</title><link>https://danieldelaney.net/normal/</link><description>&lt;doc fingerprint="a23b437c2441cdbd"&gt;
  &lt;main&gt;
    &lt;p&gt;I’m the person my friends and family come to for computer-related help. (Maybe you, gentle reader, can relate.) This experience has taught me which computing tasks are frustrating for normal people.&lt;/p&gt;
    &lt;p&gt;Normal people often struggle with converting video. They will need to watch, upload, or otherwise do stuff with a video, but the format will be weird. (Weird, broadly defined, is anything that won’t play in QuickTime or upload to Facebook.)&lt;/p&gt;
    &lt;p&gt;I would love to recommend Handbrake to them, but the user interface is by and for power users. Opening it makes normal people feel unpleasant feelings.&lt;/p&gt;
    &lt;p&gt;This problem is rampant in free software. The FOSS world is full of powerful tools that only have a “power user” UI. As a result, people give up. Or worse: they ask people like you and I to do it for them.&lt;/p&gt;
    &lt;p&gt;I want to make the case to you that you can (and should) solve this kind of problem in a single evening.&lt;/p&gt;
    &lt;p&gt;Take the example of Magicbrake, a simple front end I built. It hides the power and flexibility of Handbrake. It does only the one thing most people need Handbrake for: taking a weird video file and making it normal. (Normal, for our purposes, means a small MP4 that works just about anywhere.)&lt;/p&gt;
    &lt;p&gt;There is exactly one button.&lt;/p&gt;
    &lt;p&gt;This is a fast and uncomplicated thing to do. Unfortunately, the people who have the ability to solve problems like this are often disinclined to do it.&lt;/p&gt;
    &lt;p&gt;“Why would you make Handbrake less powerful on purpose?”&lt;/p&gt;
    &lt;p&gt;“What if someone wants a different format?”&lt;/p&gt;
    &lt;p&gt;“What about [feature/edge case]?”&lt;/p&gt;
    &lt;p&gt;The answer to all these questions is the same: a person who needs or wants that stuff can use Handbrake. If they don’t need everything Handbrake can do and find it bewildering, they can use this. Everyone wins.&lt;/p&gt;
    &lt;p&gt;It’s a bit like obscuring the less-used functions on a TV remote with tape. The functions still exist if you need them, but you’re not required to contend with them just to turn the TV on.&lt;/p&gt;
    &lt;p&gt;People benefit from stuff like this, and I challenge you to make more of it. Opportunities are everywhere. The world is full of media servers normal people can’t set up. Free audio editing software that requires hours of learning to be useful for simple tasks. Network monitoring tools that seem designed to ward off the uninitiated. Great stuff normal people don’t use. All because there’s only one UI, and it’s designed to do everything.&lt;/p&gt;
    &lt;p&gt;80% of the people only need 20% of the features. Hide the rest from them and you’ll make them more productive and happy. That’s really all it takes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45760878</guid><pubDate>Thu, 30 Oct 2025 15:07:15 +0000</pubDate></item><item><title>ZOZO's Contact Solver for physics-based simulations</title><link>https://github.com/st-tech/ppf-contact-solver</link><description>&lt;doc fingerprint="3d32dd780db6c590"&gt;
  &lt;main&gt;&lt;p&gt;A contact solver for physics-based simulations involving 👚 shells, 🪵 solids and 🪢 rods. All made by ZOZO.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;💪 Robust: Contact resolutions are completely penetration-free. No snagging intersections.&lt;/item&gt;&lt;item&gt;⏲ Scalable: An extreme case includes beyond 150M contacts. Not just one million.&lt;/item&gt;&lt;item&gt;🚲 Cache Efficient: All on the GPU runs in single precision. No double precision.&lt;/item&gt;&lt;item&gt;🥼 Inextensible: Cloth never extends beyond very strict upper bounds, such as 1%.&lt;/item&gt;&lt;item&gt;📐 Physically Accurate: Our deformable solver is driven by the Finite Element Method.&lt;/item&gt;&lt;item&gt;⚔️ Highly Stressed: We run GitHub Actions to run stress tests 10 times in a row.&lt;/item&gt;&lt;item&gt;🚀 Massively Parallel: Both contact and elasticity solvers are run on the GPU.&lt;/item&gt;&lt;item&gt;🐳 Docker Sealed: Everything is designed to work out of the box.&lt;/item&gt;&lt;item&gt;🌐 JupyterLab Included: Open your browser and run examples right away (Video).&lt;/item&gt;&lt;item&gt;🐍 Documented Python APIs: Our Python code is fully docstringed and lintable (Video).&lt;/item&gt;&lt;item&gt;☁️ Cloud-Ready: Our solver can be seamlessly deployed on major cloud platforms.&lt;/item&gt;&lt;item&gt;✨ Stay Clean: You can remove all traces after use.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;📝 Change History&lt;/item&gt;&lt;item&gt;🎓 Technical Materials&lt;/item&gt;&lt;item&gt;⚡️ Requirements&lt;/item&gt;&lt;item&gt;💨 Getting Started&lt;/item&gt;&lt;item&gt;🐍 How To Use&lt;/item&gt;&lt;item&gt;📚 Python APIs and Parameters&lt;/item&gt;&lt;item&gt;🔍 Obtaining Logs&lt;/item&gt;&lt;item&gt;🖼️ Catalogue&lt;/item&gt;&lt;item&gt;🚀 GitHub Actions&lt;/item&gt;&lt;item&gt;📡 Deploying on Cloud Services&lt;/item&gt;&lt;item&gt;✒️ Citation&lt;/item&gt;&lt;item&gt;🙏 Acknowledgements&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;🧑 💻 Setting Up Your Development Environment (Markdown)&lt;/item&gt;&lt;item&gt;🐞 Bug Fixes and Updates (Markdown)&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;(2025.10.03) Massive refactor of the codebase (Markdown). Note that this change includes breaking changes to our Python APIs.&lt;/item&gt;&lt;item&gt;(2025.08.09) Added a hindsight note in eigensystem analysis to acknowledge prior work by Poya et al. (2023).&lt;/item&gt;&lt;item&gt;(2025.05.01) Simulation states now can be saved and loaded (Video).&lt;/item&gt;&lt;item&gt;(2025.04.02) Added 9 examples. See the catalogue.&lt;/item&gt;&lt;item&gt;(2025.03.03) Added a budget table on AWS.&lt;/item&gt;&lt;item&gt;(2025.02.28) Added a reference branch and a Docker image of our TOG paper.&lt;/item&gt;&lt;item&gt;(2025.2.26) Added Floating Point-Rounding Errors in ACCD in hindsight.&lt;/item&gt;&lt;item&gt;(2025.2.7) Updated the trapped example (Video) with squishy balls.&lt;/item&gt;&lt;/list&gt;&lt;head&gt;More history records&lt;/head&gt;- (2025.1.8) Added a [domino example](./examples/domino.ipynb) [(Video)](https://drive.google.com/file/d/1N9y8eZrjSQhAUhKwiO9w8jW_T18zPnYf/view). - (2025.1.5) Added a [single twist example](./examples/twist.ipynb) [(Video)](https://drive.google.com/file/d/1LDFKS-iBvl2uDdPVKaazQL25tYGEEyXr/view). - (2024.12.31) Added full documentation for Python APIs, parameters, and log files [(GitHub Pages)](https://st-tech.github.io/ppf-contact-solver). - (2024.12.27) Line search for strain limiting is improved [(Markdown)](./articles/bug.md#new-strain-limiting-line-search) - (2024.12.23) Added [(Bug Fixes and Updates)](./articles/bug.md) - (2024.12.21) Added a [house of cards example](./examples/cards.ipynb) [(Video)](https://drive.google.com/file/d/1PMdDnlyCsjinbvICKph_0UcXUfUvvUmZ/view) - (2024.12.18) Added a [frictional contact example](./examples/friction.ipynb): armadillo sliding on the slope [(Video)](https://drive.google.com/file/d/12WGdfDTFIwCT0UFGEZzfmQreM6WSSHet/view) - (2024.12.18) Added a [hindsight](./articles/hindsight.md) noting that the tilt angle was not&lt;list rend="ul"&gt;&lt;item&gt;📚 Published in ACM Transactions on Graphics (TOG) Vol.43, No.6&lt;/item&gt;&lt;item&gt;🎥 Main video (Video)&lt;/item&gt;&lt;item&gt;🎥 Additional video examples (Directory)&lt;/item&gt;&lt;item&gt;🎥 Presentation videos (Short) (Long)&lt;/item&gt;&lt;item&gt;📃 Main paper (PDF) (Hindsight)&lt;/item&gt;&lt;item&gt;📊 Supplementary PDF (PDF)&lt;/item&gt;&lt;item&gt;🤖 Supplementary scripts (Directory)&lt;/item&gt;&lt;item&gt;🔍 Singular-value eigenanalysis (Markdown)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The main branch is undergoing frequent updates and will deviate from the paper 🚧. To retain consistency with the paper, we have created a new branch &lt;code&gt;sigasia-2024&lt;/code&gt;.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;🛠️ Only maintenance updates are planned for this branch.&lt;/item&gt;&lt;item&gt;🚫 General users should not use this branch as it is not optimized for best performance.&lt;/item&gt;&lt;item&gt;🚫 All algorithmic changes listed in this (Markdown) are excluded from this branch.&lt;/item&gt;&lt;item&gt;📦 We also provide a pre-compiled Docker image: &lt;code&gt;ghcr.io/st-tech/ppf-contact-solver-compiled-sigasia-2024:latest&lt;/code&gt;of this branch.&lt;/item&gt;&lt;item&gt;🌐 Template Link for vast.ai&lt;/item&gt;&lt;item&gt;🌐 Template Link for RunPods&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;🔥 A modern NVIDIA GPU (CUDA 12.8 or newer)&lt;/item&gt;&lt;item&gt;🐳 A Docker environment (see below)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Install a 🎮 NVIDIA driver (Link) on your 💻 host system and follow the 📝 instructions below specific to the 🖥️ operating system to get a 🐳 Docker running:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;🐧 Linux&lt;/cell&gt;&lt;cell role="head"&gt;🪟 Windows&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Install the Docker engine from here (Link). Also, install the NVIDIA Container Toolkit (Link). Just to make sure that the Container Toolkit is loaded, run &lt;code&gt;sudo service docker restart&lt;/code&gt;.&lt;/cell&gt;&lt;cell&gt;Install the Docker Desktop (Link). You may need to log out or reboot after the installation. After logging back in, launch Docker Desktop to ensure that Docker is running.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Next, run the following command to start the 📦 container:&lt;/p&gt;&lt;code&gt;$MY_WEB_PORT = 8080  # Web port number for web interface
$IMAGE_NAME = "ghcr.io/st-tech/ppf-contact-solver-compiled:latest"
docker run --rm --gpus all -p ${MY_WEB_PORT}:8080 $IMAGE_NAME&lt;/code&gt;&lt;code&gt;MY_WEB_PORT=8080  # Web port number for web interface
IMAGE_NAME=ghcr.io/st-tech/ppf-contact-solver-compiled:latest
docker run --rm --gpus all -p ${MY_WEB_PORT}:8080 $IMAGE_NAME&lt;/code&gt;&lt;p&gt;⏳ Wait for a while until the container becomes a steady state. Next, open your 🌐 browser and navigate to http://localhost:8080, where &lt;code&gt;8080&lt;/code&gt; is the port number specified in the &lt;code&gt;MY_WEB_PORT&lt;/code&gt; variable.
Keep your terminal window open.&lt;/p&gt;&lt;p&gt;🎉 Now you are ready to go! 🚀&lt;/p&gt;&lt;p&gt;To shut down the container, just press &lt;code&gt;Ctrl+C&lt;/code&gt; in the terminal.
The container will be removed and all traces will be 🧹 cleaned up.&lt;/p&gt;&lt;p&gt;If you wish to build the container from scratch 🛠️, please refer to the cleaner installation guide (Markdown) 📝.&lt;/p&gt;&lt;p&gt;Our frontend is accessible through 🌐 a browser using our built-in JupyterLab 🐍 interface. All is set up when you open it for the first time. Results can be interactively viewed through the browser and exported as needed.&lt;/p&gt;&lt;p&gt;This allows you to interact with the simulator on your 💻 laptop while the actual simulation runs on a remote headless server over 🌍 the internet. This means that you don't have to own ⚙️ NVIDIA hardware, but can rent it at vast.ai or RunPod for less than 💵 $0.5 per hour. For example, this (Video) was recorded on a vast.ai instance. The experience is 👍 good!&lt;/p&gt;&lt;p&gt;Our Python interface is designed with the following principles in mind:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;🛠️ Dynamic Tri/Tet Creation: Relying on non-integrated third-party tools for triangulation, tetrahedralization, and loading can make it difficult to dynamically adjust resolutions. Our built-in tri/tet creation tools eliminate this limitation.&lt;/item&gt;&lt;item&gt;🚫 No Mesh Data: Preparing mesh data using external tools can be cumbersome. Our frontend minimizes this effort by allowing meshes to be created on the fly or downloaded when needed.&lt;/item&gt;&lt;item&gt;🔗 Method Chaining: We adopt the method chaining style from JavaScript, making the API intuitive and easy to understand.&lt;/item&gt;&lt;item&gt;📦 Single Import for Everything: All frontend features are accessible by simply importing with &lt;code&gt;from frontend import App&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here's an example of draping five sheets over a sphere with two corners pinned. Please look into the examples directory for more examples.&lt;/p&gt;&lt;code&gt;# import our frontend
from frontend import App

# make an app
app = App.create("drape")

# create a square mesh resolution 128 spanning the xz plane
V, F = app.mesh.square(res=128, ex=[1, 0, 0], ey=[0, 0, 1])

# add to the asset and name it "sheet"
app.asset.add.tri("sheet", V, F)

# create an icosphere mesh radius 0.5
V, F = app.mesh.icosphere(r=0.5, subdiv_count=4)

# add to the asset and name it "sphere"
app.asset.add.tri("sphere", V, F)

# create a scene
scene = app.scene.create()

# define gap between sheets
gap = 0.01

for i in range(5):

    # add the sheet asset to the scene
    obj = scene.add("sheet")

    # pick two corners
    corner = obj.grab([1, 0, -1]) + obj.grab([-1, 0, -1])

    # place it with an vertical offset and pin the corners
    obj.at(0, gap * i, 0).pin(corner)

    # set fiber directions required for Baraff-Witkin
    obj.direction([1, 0, 0], [0, 0, 1])

    # set the strainlimiting of 5%
    obj.param.set("strain-limit", 0.05)

# add a sphere mesh at a lower position with jitter and set it static collider
scene.add("sphere").at(0, -0.5 - gap, 0).jitter().pin()

# compile the scene and report stats
scene = scene.build().report()

# preview the initial scene
scene.preview()

# create a new session with the compiled scene
session = app.session.create(scene)

# set session params
session.param.set("frames", 100).set("dt", 0.01)

# build this session
session = session.build()

# start the simulation and live-preview the results (image right)
session.start().preview()

# also show streaming logs
session.stream()

# or interactively view the animation sequences
session.animate()

# export all simulated frames
session.export.animation()&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Full API documentation 📖 is available on our GitHub Pages. The major APIs are documented using docstrings ✍️ and compiled with Sphinx ⚙️. We have also included&lt;/p&gt;&lt;code&gt;jupyter-lsp&lt;/code&gt;to provide interactive linting assistance 🛠️ and display docstrings as you type. See this video (Video) for an example. The behaviors can be changed through the settings.&lt;/item&gt;&lt;item&gt;&lt;p&gt;A list of parameters used in&lt;/p&gt;&lt;code&gt;param.set(key,value)&lt;/code&gt;is documented here: (Global Parameters) (Object Parameters).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note&lt;/p&gt;&lt;p&gt;📊 Logs for the simulation can also be queried through the Python APIs 🐍. Here's an example of how to get a list of recorded logs 📝, fetch them 📥, and compute the average 🧮.&lt;/p&gt;&lt;code&gt;# get a list of log names
logs = session.get.log.names()
print(logs)
assert "time-per-frame" in logs
assert "newton-steps" in logs

# get a list of time per video frame
msec_per_video = session.get.log.numbers("time-per-frame")

# compute the average time per video frame
print("avg per frame:", sum([n for _, n in msec_per_video]) / len(msec_per_video))

# get a list of newton steps
newton_steps = session.get.log.numbers("newton-steps")

# compute the average of consumed newton steps
print("avg newton steps:", sum([n for _, n in newton_steps]) / len(newton_steps))

# Last 8 lines. Omit for everything.
print("==== log stream ====")
for line in session.get.log.stdout(n_lines=8):
    print(line)&lt;/code&gt;&lt;p&gt;Below are some representatives. &lt;code&gt;vid_time&lt;/code&gt; refers to the video time in seconds and is recorded as &lt;code&gt;float&lt;/code&gt;.
&lt;code&gt;ms&lt;/code&gt; refers to the consumed simulation time in milliseconds recorded as &lt;code&gt;int&lt;/code&gt;.
&lt;code&gt;vid_frame&lt;/code&gt; is the video frame count recorede as &lt;code&gt;int&lt;/code&gt;.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Name&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;cell role="head"&gt;Format&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;time-per-frame&lt;/cell&gt;&lt;cell&gt;Time per video frame&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_frame,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;matrix-assembly&lt;/cell&gt;&lt;cell&gt;Matrix assembly time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;pcg-linsolve&lt;/cell&gt;&lt;cell&gt;Linear system solve time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;line-search&lt;/cell&gt;&lt;cell&gt;Line search time&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;time-per-step&lt;/cell&gt;&lt;cell&gt;Time per step&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,ms)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;newton-steps&lt;/cell&gt;&lt;cell&gt;Newton iterations per step&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,count)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;num-contact&lt;/cell&gt;&lt;cell&gt;Contact count&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list[(vid_time,count)]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;max-sigma&lt;/cell&gt;&lt;cell&gt;Max stretch&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;list(vid_time,float)&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The full list of log names and their descriptions is documented here: (GitHub Pages).&lt;/p&gt;&lt;p&gt;Note that some entries have multiple records at the same video time ⏱️. This occurs because the same operation is executed multiple times 🔄 within a single step during the inner Newton's iterations 🧮. For example, the linear system solve is performed at each Newton's step, so if multiple Newton's steps are 🔁 executed, multiple linear system solve times appear in the record at the same 📊 video time.&lt;/p&gt;&lt;p&gt;If you would like to retrieve the raw log stream, you can do so by&lt;/p&gt;&lt;code&gt;# Last 8 lines. Omit for everything.
for line in session.get.log.stdout(n_lines=8):
    print(line)&lt;/code&gt;&lt;p&gt;This will output something like:&lt;/p&gt;&lt;code&gt;* dt: 1.000e-03
* max_sigma: 1.045e+00
* avg_sigma: 1.030e+00
------ newton step 1 ------
   ====== contact_matrix_assembly ======
   &amp;gt; dry_pass...0 msec
   &amp;gt; rebuild...7 msec
   &amp;gt; fillin_pass...0 msec
&lt;/code&gt;&lt;p&gt;If you would like to read &lt;code&gt;stderr&lt;/code&gt;, you can do so using &lt;code&gt;session.get.stderr()&lt;/code&gt; (if it exists). They return &lt;code&gt;list[str]&lt;/code&gt;.
All the log files 📂 are available ✅ and can be fetched ⬇️ during the simulation 💻.&lt;/p&gt;&lt;p&gt;Below is a table summarizing the estimated costs for running our examples on a NVIDIA L4 instance &lt;code&gt;g6.2xlarge&lt;/code&gt; at Amazon Web Services US regions (&lt;code&gt;us-east-1&lt;/code&gt; and &lt;code&gt;us-east-2&lt;/code&gt;).&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;💰 Uptime cost is approximately $1 per hour.&lt;/item&gt;&lt;item&gt;⏳ Deployment time is approximately 8 minutes ($0.13). Instance loading takes 3 minutes, and Docker pull &amp;amp; load takes 5 minutes.&lt;/item&gt;&lt;item&gt;🎮 The NVIDIA L4 delivers 30.3 TFLOPS for FP32, offering approximately 36% of the performance of an RTX 4090.&lt;/item&gt;&lt;item&gt;🎥 Video frame rate is 60fps.&lt;/item&gt;&lt;/list&gt;&lt;table&gt;&lt;row span="9"&gt;&lt;cell role="head"&gt;Example&lt;/cell&gt;&lt;cell role="head"&gt;Cost&lt;/cell&gt;&lt;cell role="head"&gt;Time&lt;/cell&gt;&lt;cell role="head"&gt;#Frame&lt;/cell&gt;&lt;cell role="head"&gt;#Vert&lt;/cell&gt;&lt;cell role="head"&gt;#Face&lt;/cell&gt;&lt;cell role="head"&gt;#Tet&lt;/cell&gt;&lt;cell role="head"&gt;#Seg&lt;/cell&gt;&lt;cell role="head"&gt;Max Strain&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;trapped&lt;/cell&gt;&lt;cell&gt;$0.37&lt;/cell&gt;&lt;cell&gt;22.6m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;263K&lt;/cell&gt;&lt;cell&gt;299K&lt;/cell&gt;&lt;cell&gt;885K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;twist&lt;/cell&gt;&lt;cell&gt;$0.91&lt;/cell&gt;&lt;cell&gt;55m&lt;/cell&gt;&lt;cell&gt;500&lt;/cell&gt;&lt;cell&gt;203K&lt;/cell&gt;&lt;cell&gt;406K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;stack&lt;/cell&gt;&lt;cell&gt;$0.60&lt;/cell&gt;&lt;cell&gt;36.2m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;166.7K&lt;/cell&gt;&lt;cell&gt;327.7K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;trampoline&lt;/cell&gt;&lt;cell&gt;$0.74&lt;/cell&gt;&lt;cell&gt;44.5m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;56.8K&lt;/cell&gt;&lt;cell&gt;62.2K&lt;/cell&gt;&lt;cell&gt;158.0K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;needle&lt;/cell&gt;&lt;cell&gt;$0.31&lt;/cell&gt;&lt;cell&gt;18.4m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;86K&lt;/cell&gt;&lt;cell&gt;168.9K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;cards&lt;/cell&gt;&lt;cell&gt;$0.29&lt;/cell&gt;&lt;cell&gt;17.5m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;8.7K&lt;/cell&gt;&lt;cell&gt;13.8K&lt;/cell&gt;&lt;cell&gt;1.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;domino&lt;/cell&gt;&lt;cell&gt;$0.12&lt;/cell&gt;&lt;cell&gt;4.3m&lt;/cell&gt;&lt;cell&gt;250&lt;/cell&gt;&lt;cell&gt;0.5K&lt;/cell&gt;&lt;cell&gt;0.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;drape&lt;/cell&gt;&lt;cell&gt;$0.10&lt;/cell&gt;&lt;cell&gt;3.5m&lt;/cell&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;81.9K&lt;/cell&gt;&lt;cell&gt;161.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;curtain&lt;/cell&gt;&lt;cell&gt;$0.33&lt;/cell&gt;&lt;cell&gt;19.6m&lt;/cell&gt;&lt;cell&gt;300&lt;/cell&gt;&lt;cell&gt;64K&lt;/cell&gt;&lt;cell&gt;124K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;friction&lt;/cell&gt;&lt;cell&gt;$0.17&lt;/cell&gt;&lt;cell&gt;10m&lt;/cell&gt;&lt;cell&gt;700&lt;/cell&gt;&lt;cell&gt;1.1K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;hang&lt;/cell&gt;&lt;cell&gt;$0.12&lt;/cell&gt;&lt;cell&gt;7.5m&lt;/cell&gt;&lt;cell&gt;200&lt;/cell&gt;&lt;cell&gt;16.3K&lt;/cell&gt;&lt;cell&gt;32.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;1%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;belt&lt;/cell&gt;&lt;cell&gt;$0.19&lt;/cell&gt;&lt;cell&gt;11.4m&lt;/cell&gt;&lt;cell&gt;200&lt;/cell&gt;&lt;cell&gt;12.3K&lt;/cell&gt;&lt;cell&gt;23.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;codim&lt;/cell&gt;&lt;cell&gt;$0.36&lt;/cell&gt;&lt;cell&gt;21.6m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;122.7K&lt;/cell&gt;&lt;cell&gt;90K&lt;/cell&gt;&lt;cell&gt;474.1K&lt;/cell&gt;&lt;cell&gt;1.3K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;fishingknot&lt;/cell&gt;&lt;cell&gt;$0.38&lt;/cell&gt;&lt;cell&gt;22.5m&lt;/cell&gt;&lt;cell&gt;830&lt;/cell&gt;&lt;cell&gt;19.6K&lt;/cell&gt;&lt;cell&gt;36.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;fitting&lt;/cell&gt;&lt;cell&gt;$0.03&lt;/cell&gt;&lt;cell&gt;1.54m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;28.4K&lt;/cell&gt;&lt;cell&gt;54.9K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;10%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;noodle&lt;/cell&gt;&lt;cell&gt;$0.14&lt;/cell&gt;&lt;cell&gt;8.45m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;116.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;116.2K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;ribbon&lt;/cell&gt;&lt;cell&gt;$0.23&lt;/cell&gt;&lt;cell&gt;13.9m&lt;/cell&gt;&lt;cell&gt;480&lt;/cell&gt;&lt;cell&gt;34.9K&lt;/cell&gt;&lt;cell&gt;52.9K&lt;/cell&gt;&lt;cell&gt;8.8K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;5%&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;woven&lt;/cell&gt;&lt;cell&gt;$0.58&lt;/cell&gt;&lt;cell&gt;34.6m&lt;/cell&gt;&lt;cell&gt;450&lt;/cell&gt;&lt;cell&gt;115.6K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;115.4K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;yarn&lt;/cell&gt;&lt;cell&gt;$0.01&lt;/cell&gt;&lt;cell&gt;0.24m&lt;/cell&gt;&lt;cell&gt;120&lt;/cell&gt;&lt;cell&gt;28.5K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;28.5K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;roller&lt;/cell&gt;&lt;cell&gt;$0.03&lt;/cell&gt;&lt;cell&gt;2.08m&lt;/cell&gt;&lt;cell&gt;240&lt;/cell&gt;&lt;cell&gt;21.4K&lt;/cell&gt;&lt;cell&gt;22.2K&lt;/cell&gt;&lt;cell&gt;61.0K&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Large scale examples are run on a vast.ai instance with an RTX 4090. At the moment, not all large scale examples are ready yet, but they will be added/updated one by one. The author is actively woriking on it.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell&gt;large-twist (Video)&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;cell&gt;TBA&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="9"&gt;&lt;cell role="head"&gt;Example&lt;/cell&gt;&lt;cell role="head"&gt;Commit&lt;/cell&gt;&lt;cell role="head"&gt;#Vert&lt;/cell&gt;&lt;cell role="head"&gt;#Face&lt;/cell&gt;&lt;cell role="head"&gt;#Tet&lt;/cell&gt;&lt;cell role="head"&gt;#Seg&lt;/cell&gt;&lt;cell role="head"&gt;#Contact&lt;/cell&gt;&lt;cell role="head"&gt;#Frame&lt;/cell&gt;&lt;cell role="head"&gt;Time/Frame&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;large-twist&lt;/cell&gt;&lt;cell&gt;cbafbd2&lt;/cell&gt;&lt;cell&gt;3.2M&lt;/cell&gt;&lt;cell&gt;6.4M&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;N/A&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;56.7M&lt;/cell&gt;&lt;cell&gt;2,000&lt;/cell&gt;&lt;cell&gt;46.4s&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;We implemented GitHub Actions that test all of our examples except for large scale ones, which take from hours to days to finish. We perform explicit intersection checks 🔍 at the end of each step, which raises an error ❌ if an intersection is detected. This ensures that all steps are confirmed to be penetration-free if tests are pass ✅. The runner types are described as follows:&lt;/p&gt;&lt;p&gt;The tested 🚀 runner of this action is the Ubuntu NVIDIA GPU-Optimized Image for AI and HPC with an NVIDIA Tesla T4 (16 GB VRAM) with Driver version 570.133.20. This is not a self-hosted runner, meaning that each time the runner launches, all environments are 🌱 fresh.&lt;/p&gt;&lt;p&gt;We use the GitHub-hosted runner 🖥️, but the actual simulation runs on a &lt;code&gt;g6e.2xlarge&lt;/code&gt; AWS instance 🌐.
Since we start with a fresh 🌱 instance, the environment is clean 🧹 every time.
We take advantage of the ability to deploy on the cloud; this action is performed in parallel, which reduces the total action time.&lt;/p&gt;&lt;p&gt;We generate zipped action artifacts 📦 for each run. These artifacts include:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;📝 Logs: Detailed logs of the simulation runs.&lt;/item&gt;&lt;item&gt;📊 Metrics: Performance metrics and statistics.&lt;/item&gt;&lt;item&gt;📹 Videos: Simulated animations.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Please note that these artifacts will be deleted after a month.&lt;/p&gt;&lt;p&gt;We know that you can't judge the reliability of contact resolution by simply watching a single success 🎥 video example. To ensure greater transparency, we implemented GitHub Actions to run many of our examples via automated GitHub Actions ⚙️, not just once, but 10 times in a row 🔁. This means that a single failure out of 10 tests is considered a failure of the entire test suite!&lt;/p&gt;&lt;p&gt;Also, we apply small jitters to the position of objects in the scene 🔄, so at each run, the scene is slightly different.&lt;/p&gt;&lt;p&gt;Our contact solver is designed for heavy use in cloud services ☁️, enabling:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;💰 Cost-Effective Development: Quickly deploy testing environments 🚀 and delete 🗑️ them when not in use, saving costs.&lt;/item&gt;&lt;item&gt;📈 Flexible Scalability: Scale as needed based on demand 📈. For example, you can launch multiple instances before a specific deadline ⏰.&lt;/item&gt;&lt;item&gt;🌍 High Accessibility: Allow anyone with an internet connection 🌍 to try our solver, even on a smartphone 📱 or tablet 🖥️.&lt;/item&gt;&lt;item&gt;🐛 Easier Bug Tracking: Users and developers can easily share the same hardware, kernel, and driver environment, making it easier to track and fix bugs.&lt;/item&gt;&lt;item&gt;🛠️ Free Maintenance Cost: No need to maintain hardware for everyday operations or introduce redundancy for malfunctions.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is made possible with our purely web-based frontends 🌐 and scalable capability 🧩. Our main target is the NVIDIA L4 🖱️, a data-center-targeted GPU 🖥️ that offers reasonable pricing 💲, delivering both practical performance 💪 and scalability 📊 without investing in expensive hardware 💻.&lt;/p&gt;&lt;p&gt;Below, we describe how to deploy our solver on major cloud services ☁️. These instructions are up to date as of late 2024 📅 and are subject to change 🔄.&lt;/p&gt;&lt;p&gt;Important: For all the services below, don't forget to ❌ delete the instance after use, or you’ll be 💸 charged for nothing.&lt;/p&gt;&lt;head rend="h3"&gt;📦 Deploying on vast.ai&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Select our template (Link).&lt;/item&gt;&lt;item&gt;Create an instance and click &lt;code&gt;Open&lt;/code&gt;button.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;📦 Deploying on RunPod&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Follow this link (Link) and deploy an instance using our template.&lt;/item&gt;&lt;item&gt;Click &lt;code&gt;Connect&lt;/code&gt;button and open the&lt;code&gt;HTTP Services&lt;/code&gt;link.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;📦 Deploying on Scaleway&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Set zone to &lt;code&gt;fr-par-2&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Select type &lt;code&gt;L4-1-24G&lt;/code&gt;or&lt;code&gt;GPU-3070-S&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Choose &lt;code&gt;Ubuntu Jammy GPU OS 12&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/item&gt;&lt;item&gt;This setup costs approximately €0.76 per hour.&lt;/item&gt;&lt;item&gt;CLI instructions are described in (Markdown).&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;📦 Deploying on Amazon Web Services&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Amazon Machine Image (AMI): &lt;code&gt;Deep Learning Base AMI with Single CUDA (Ubuntu 22.04)&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Instance Type: &lt;code&gt;g6.2xlarge&lt;/code&gt;(Recommended)&lt;/item&gt;&lt;item&gt;This setup costs around $1 per hour.&lt;/item&gt;&lt;item&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;📦 Deploying on Google Compute Engine&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Select&lt;/p&gt;&lt;code&gt;GPUs&lt;/code&gt;. We recommend the GPU type&lt;code&gt;NVIDIA L4&lt;/code&gt;because it's affordable and accessible, as it does not require a high quota. You may select&lt;code&gt;T4&lt;/code&gt;instead for testing purposes.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Do not check&lt;/p&gt;&lt;code&gt;Enable Virtual Workstation (NVIDIA GRID)&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;We recommend the machine type&lt;/p&gt;&lt;code&gt;g2-standard-8&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Choose the OS type&lt;/p&gt;&lt;code&gt;Deep Learning VM with CUDA 12.4 M129&lt;/code&gt;and set the disk size to&lt;code&gt;50GB&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;As of late 2024, this configuration costs approximately $0.86 per hour in&lt;/p&gt;&lt;code&gt;us-central1 (Iowa)&lt;/code&gt;and $1.00 per hour in&lt;code&gt;asia-east1 (Taiwan)&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Port number&lt;/p&gt;&lt;code&gt;8080&lt;/code&gt;is reserved by the OS image. Set&lt;code&gt;$MY_WEB_PORT&lt;/code&gt;to&lt;code&gt;8888&lt;/code&gt;. When connecting via&lt;code&gt;gcloud&lt;/code&gt;, use the following format:&lt;code&gt;gcloud compute ssh --zone "xxxx" "instance-name" -- -L 8080:localhost:8888&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Do not skip the Docker container creation in the installation process; it is required.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;CLI instructions are described in (Markdown).&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;code&gt;@software{ppf-contact-solver-2024,
    title = {ZOZO's Contact Solver},
    author = {Ryoichi Ando},
    note = {https://github.com/st-tech/ppf-contact-solver},
    year = 2024,
}&lt;/code&gt;&lt;p&gt;The author thanks ZOZO, Inc. for permitting the release of the code and the team members for assisting with the internal paperwork for this project.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45761042</guid><pubDate>Thu, 30 Oct 2025 15:21:46 +0000</pubDate></item><item><title>Affinity Studio now free</title><link>https://www.affinity.studio/get-affinity</link><description>&lt;doc fingerprint="3bd67e5e966d06c5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Get Affinity&lt;/head&gt;
    &lt;p&gt;Available on desktop for&lt;/p&gt;
    &lt;p&gt;The all-in-one creative app, with everything you need to craft designs, edit images, and lay it all out, without ever leaving your document or paying a thing.&lt;/p&gt;
    &lt;quote&gt;$0, free&lt;/quote&gt;
    &lt;p&gt;To download Affinity, sign in with your Canva account (or create one for free).&lt;/p&gt;
    &lt;head rend="h2"&gt;One powerful app. No cost.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Fully-featured toolsets&lt;/p&gt;
        &lt;p&gt;From vector to pixel to layout, Affinity has all the studio-grade tools you need under one roof.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customizable studios&lt;/p&gt;
        &lt;p&gt;Mix and match your favorite tools to build your very own creative studios.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-destructive editing&lt;/p&gt;
        &lt;p&gt;Experiment as much you want, keep your original files intact.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pixel-perfect export&lt;/p&gt;
        &lt;p&gt;Full control over how your work leaves the app, whether it’s by object, slice, or doc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What you’ll get&lt;/head&gt;
    &lt;p&gt;With Affinity, you’ll get all the professional tools you need for your design, photo editing, and page layout projects, free of charge. If you’re on a Canva premium plan, you’ll also be able to unlock Canva AI tools directly in Affinity for a super-powered workflow.&lt;/p&gt;
    &lt;p&gt;+ Canva premium plans&lt;/p&gt;
    &lt;head rend="h2"&gt;Design workflows&lt;/head&gt;
    &lt;p&gt;Access all vector design, photo editing, and page layout tools in one app&lt;/p&gt;
    &lt;p&gt;Combine vector and pixel work on the same .af document&lt;/p&gt;
    &lt;p&gt;Customize your workspace with floating toolbars and studio presets&lt;/p&gt;
    &lt;p&gt;Real-time performance engine for ultra-smooth editing&lt;/p&gt;
    &lt;p&gt;Non-destructive editing across layers, filters, and adjustments&lt;/p&gt;
    &lt;p&gt;Import PSD, AI, PDF, SVG, IDML and more with high fidelity&lt;/p&gt;
    &lt;p&gt;Export with one-click presets or custom slice-based output&lt;/p&gt;
    &lt;p&gt;Quick export direct to Canva&lt;/p&gt;
    &lt;head rend="h2"&gt;Powerful photo editing&lt;/head&gt;
    &lt;p&gt;Live filters and adjustments with instant preview&lt;/p&gt;
    &lt;p&gt;Full RAW editing, tone mapping, and lens correction&lt;/p&gt;
    &lt;p&gt;Advanced retouching: inpainting brush, healing tools, dodge and burn&lt;/p&gt;
    &lt;p&gt;Batch processing with recordable macros, HDR merge, panorama stitching, and more&lt;/p&gt;
    &lt;head rend="h2"&gt;Pro vector design&lt;/head&gt;
    &lt;p&gt;Precision drawing with pen, node, and pencil tools&lt;/p&gt;
    &lt;p&gt;Live shape editing, booleans, and shape builder&lt;/p&gt;
    &lt;p&gt;Flexible gradients with full control&lt;/p&gt;
    &lt;p&gt;Trace pixel images&lt;/p&gt;
    &lt;p&gt;Pixel-perfect vector tools for illustration and layout&lt;/p&gt;
    &lt;head rend="h2"&gt;Advanced page layout&lt;/head&gt;
    &lt;p&gt;Linked text frames with autoflow and live text wrapping&lt;/p&gt;
    &lt;p&gt;Smart master pages with overrides and reusable layouts&lt;/p&gt;
    &lt;p&gt;Pro typography: ligatures, stylistic sets, drop caps, and variable fonts&lt;/p&gt;
    &lt;p&gt;Print-ready output: CMYK, spot colours, preflight, bleed, and slug support&lt;/p&gt;
    &lt;p&gt;Data merge from .csv with tokens, image merge, and conditional logic&lt;/p&gt;
    &lt;head rend="h2"&gt;Canva AI Studio&lt;/head&gt;
    &lt;p&gt;Generative Fill, Expand, and Edit&lt;/p&gt;
    &lt;p&gt;Generate Images and Vectors&lt;/p&gt;
    &lt;p&gt;Remove Background and Subject Selection&lt;/p&gt;
    &lt;p&gt;Colorize, Depth Selection, and Super Resolution&lt;/p&gt;
    &lt;p&gt;Portrait Blur and Portrait Lighting&lt;/p&gt;
    &lt;p&gt;Full AI generation history&lt;/p&gt;
    &lt;head rend="h2"&gt;Need Affinity for your organization?&lt;/head&gt;
    &lt;p&gt;Skip the individual downloads and get your entire team on Affinity with SSO via a Canva Enterprise or Canva Districts account. Choose an option below to get started.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, Affinity really is free. That doesn’t mean you’re getting a watered-down version of the app though. You can use every tool in the Pixel, Vector, and Layout studios, plus all of the customization and export features, as much as you want, with no restrictions or payment needed. The app will also receive free updates with new features and improvements added.&lt;/p&gt;
        &lt;p&gt;If you’re on a Canva premium plan (Pro, Business, Enterprise, Education), you’ll also be able to unlock Canva’s powerful AI tools within Affinity via the Canva AI Studio.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. Affinity is now brought to you by Canva, and your Canva account gives you access to Affinity and other Canva products and features.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No. You can access all of Affinity’s vector, layout, and pixel tools for free without a Canva subscription. If you’d like to unlock Canva AI tools within Affinity, however, you will need a premium Canva plan.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is a brand-new product that gives you advanced photo editing, graphic design, and page layout tools under one roof. It includes highly requested features such as Image Trace, ePub support, mesh gradients, hatch fills, live glitch filter, as well as custom capabilities that allow you to rearrange panels and combine tools to build your own unique studios. Plus, with a Canva premium plan, you can unlock incredibly powerful AI tools such as Generative Fill, Generative Expand, Generate Image/Vector, and more — directly in Affinity.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. With a Canva premium plan you can unlock Canva AI features in Affinity.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No, these are only available to those with Canva premium accounts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is currently available on Windows and macOS (iPadOS coming soon!).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We’re busy building our iPad version — stay tuned for updates!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is optimized for the latest hardware, including Apple silicon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Absolutely! The new desktop version of Affinity can open all files created in Affinity V2 or V1 apps. However, Affinity V1 and V2 cannot open files that are created or saved in the newer app, Affinity by Canva.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No, it’s the same app, just available on different operating systems.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, you can install Affinity on as many devices as you like.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes! It’s easy to import PSDs, AIs, IDMLs, DWGs, and other file types into Affinity, with structure, layers, and creative intent preserved.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Affinity is available in English, French, German, Italian, Spanish, Portuguese, Japanese, Chinese, Bahasa Indonesian, and Turkish. Keep an eye out for more languages coming soon!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Get in touch to speak to our team about how your organization can get set up with Affinity, including SSO.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then all you need to do is stay in one of our pre-built studios: Pixel, Vector or Layout. You’ll find all your favorite tools there, plus some new ones. Since it’s all free, just think of the other creative toolsets as an added bonus!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That’s totally fine. Your Affinity V2 license (via Serif) remains valid and Serif will continue to keep activation servers online. But please note that these apps won’t receive future updates.&lt;/p&gt;
        &lt;p&gt;For the best experience, we recommend using the new Affinity by Canva app.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;No. The new desktop version of Affinity can open all files created in V2, but older versions (including V2 on iPad) cannot open newer Affinity (.af) files, meaning you won’t be able to work across both platforms.&lt;/p&gt;&lt;lb/&gt;We don’t have a release date for the new Affinity on iPad yet, so recommend continuing to run V2 independently while you enjoy the new Affinity on desktop.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes. The new Affinity by Canva app will receive free updates and new features over time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You will need to be online to download and activate your license with your free Canva account. From then on, there is no requirement to be online, even with extended offline periods.&lt;/p&gt;
        &lt;p&gt;There are a couple of things to keep in mind:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;There are some features which do require you to be online, if you choose to use them, such as product help, lessons, stock libraries and integrations with Canva including AI tools.&lt;/item&gt;
          &lt;item&gt;We’ll also be releasing new updates and patches regularly, so we recommend connecting from time to time to keep your app up to date, but it's not a requirement of use.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You need a Canva premium plan to unlock all of Canva’s AI features in Affinity. Simply download the Affinity app via our Downloads page and follow the prompts once you click ‘Canva AI Studio’.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45761445</guid><pubDate>Thu, 30 Oct 2025 15:54:38 +0000</pubDate></item><item><title>Qt Creator 18 Released</title><link>https://www.qt.io/blog/qt-creator-18-released</link><description>&lt;doc fingerprint="1f24de105045bbe7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Qt Creator 18 released&lt;/head&gt;
    &lt;p&gt;October 30, 2025 by Eike Ziller | Comments&lt;/p&gt;
    &lt;head rend="h5"&gt;We are happy to announce the release of Qt Creator 18!&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 adds experimental support for Development Containers and many more improvements.&lt;/p&gt;
    &lt;head rend="h4"&gt;Development Container Support&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 adds support for development containers to automate setting up the development environment of a project. It detects a "devcontainer.json" file in your project directory and creates a Docker container for it. You can let Qt Creator auto-detect kits or specify custom kits and control other aspects like the command bridge (our service for communicating with remote devices) with Qt Creator specific customizations in the development container definition. Note that it is still experimental and does not support all aspects of development containers yet. Enable the extension to use this functionality. Find out more.&lt;/p&gt;
    &lt;head rend="h4"&gt;General UI&lt;/head&gt;
    &lt;p&gt;We added an Overview tab on Welcome mode that aggregates content from the other tabs. It suggests tutorials and examples based on your experience and needs, and highlights developer-targeted posts in the Qt blog.&lt;/p&gt;
    &lt;p&gt;The notifications received a facelift and are now part of the progress notification popups. You can opt-out of this with Environment &amp;gt; Interface &amp;gt; Prefer banner style info bars over pop-ups.&lt;/p&gt;
    &lt;head rend="h4"&gt;Editing&lt;/head&gt;
    &lt;p&gt;We added the option to use tabbed editors (Environment &amp;gt; Interface &amp;gt; Use tabbed editors). But remember faster ways of navigating your code, such as Locator filters for opening files or jumping to specific class or symbol, Follow Symbol, Find References, the Open Documents and File System views, the edit location history Window &amp;gt; Go Back/Forward and the corresponding keyboard shortcuts, and Window &amp;gt; Previous/Next Open Document in History and the corresponding keyboard shortcuts.&lt;/p&gt;
    &lt;p&gt;For the C++ support we updated Clangd/LLVM to the 21.1 release for our prebuilt binaries. Additionally, the built-in code model received a wide range of fixes for newer C++ features. We added quick fixes for removing curly braces and for adding definitions for static data members.&lt;/p&gt;
    &lt;p&gt;For QML you can now download and use the latest QML Language Server even if you are using older Qt versions for your projects (in the QML Language Server settings in Preferences &amp;gt; Language Client).&lt;/p&gt;
    &lt;p&gt;We also added support for GitHub Enterprise environments for GitHub Copilot.&lt;/p&gt;
    &lt;head rend="h4"&gt;Projects&lt;/head&gt;
    &lt;p&gt;We moved the ".user" files that contain the Qt Creator specific project settings into the ".qtcreator/" subdirectory of the project directory. Existing ".user" files from older projects are still updated for compatibility though.&lt;/p&gt;
    &lt;p&gt;In Projects mode you can now choose to only show kits that are actually usable by the project, or only kits that the project is already configured for. We also split up the Run page into Deploy Settings and Run Settings, and together with the Build Settings moved them out of the kit selection to tabs in the content view. Normally the run configurations of the various build configurations are independent of each other. In Qt Creator 18 we have added the option to sync the run configurations within a single kit, or even between all kits that the project is configured for.&lt;/p&gt;
    &lt;p&gt;For CMake projects we now also support Test Presets and added a Locator filter "ct" for running CTest based tests. We also fixed building CMake projects for all build configurations (Build &amp;gt; Build Project for All Configurations).&lt;/p&gt;
    &lt;head rend="h4"&gt;Devices&lt;/head&gt;
    &lt;p&gt;We added a configuration for various tools on remote Linux devices, like GDB server, CMake, clangd, rsync, qmake, and more, and the option to auto-detect them. This improves the configuration of remote devices as build devices. More is to come in future releases in this regard. You can now also decide if Qt Creator should try to automatically re-connect to devices at startup with a new Auto-connect on startup setting. We also fixed that it wasn't possibly to use rsync for deployment when building on a remote device as well as using a remote target device.&lt;/p&gt;
    &lt;head rend="h4"&gt;Other Improvements&lt;/head&gt;
    &lt;p&gt;Qt Creator 18 comes with many more improvements and fixes. For example the Git commit editor now provides many more actions on files, like staging, unstaging, and directly adding files to ".gitignore".&lt;/p&gt;
    &lt;p&gt;Please have a look at our change log for more detailed information.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get Qt Creator 18&lt;/head&gt;
    &lt;p&gt;The new version is available as an update in the Qt Online Installer (commercial, opensource). You also find commercially licensed offline installers on the Qt Account Portal, and opensource packages on our opensource download page. This is a free upgrade for all users.&lt;/p&gt;
    &lt;p&gt;Please post issues in our bug tracker. You can also find us on IRC on #qt-creator on irc.libera.chat, and on the Qt Creator mailing list.&lt;/p&gt;
    &lt;p&gt;You can read the Qt Creator Manual in Qt Creator in the Help mode or access it online in the Qt documentation portal.&lt;/p&gt;
    &lt;head rend="h6"&gt;Blog Topics:&lt;/head&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;head rend="h4"&gt;Subscribe to our newsletter&lt;/head&gt;
    &lt;head rend="h4"&gt;Subscribe Newsletter&lt;/head&gt;
    &lt;head rend="h4"&gt;Try Qt 6.10 Now!&lt;/head&gt;
    &lt;p&gt;Download the latest release here: www.qt.io/download.&lt;/p&gt;
    &lt;p&gt;Qt 6.10 is now available, with new features and improvements for application developers and device creators.&lt;/p&gt;
    &lt;head rend="h4"&gt;We're Hiring&lt;/head&gt;
    &lt;p&gt;Check out all our open positions here and follow us on Instagram to see what it's like to be #QtPeople.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45761789</guid><pubDate>Thu, 30 Oct 2025 16:23:56 +0000</pubDate></item><item><title>Launch HN: Propolis (YC X25) – Browser agents that QA your web app autonomously</title><link>https://app.propolis.tech/#/launch</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45762012</guid><pubDate>Thu, 30 Oct 2025 16:40:02 +0000</pubDate></item><item><title>The ear does not do a Fourier transform</title><link>https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform</link><description>&lt;doc fingerprint="a63e213260f69383"&gt;
  &lt;main&gt;
    &lt;p&gt;Let’s talk about how the cochlea computes!&lt;/p&gt;
    &lt;p&gt;The tympanic membrane (eardrum) is vibrated by changes in air pressure (sound waves). Bones in the middle ear amplify and send these vibrations to the fluid-filled, snail-shaped cochlea. Vibrations travel through the fluid to the basilar membrane, which remarkably performs frequency separation1: the stiffer, lighter base resonates with high frequency components of the signal, and the more flexible, heavier apex resonates with lower frequencies. Between the two ends, the resonant frequencies decrease logarithmically in space2.&lt;/p&gt;
    &lt;p&gt;The hair cells on different parts of the basilar membrane wiggle back and forth at the frequency corresponding to their position on the membrane. But how do wiggling hair cells translate to electrical signals? This mechanoelectrical transduction process feels like it could be from a Dr. Seuss world: springs connected to the ends of hair cells open and close ion channels at the frequency of the vibration, which then cause neurotransmitter release. Bruno calls them “trapdoors”. Here’s a visualization:&lt;/p&gt;
    &lt;p&gt;It’s clear that the hardware of the ear is well-equipped for frequency analysis. Nerve fibers serve as filters to extract temporal and frequency information about a signal. Below are examples of filters (not necessarily of the ear) shown in the time domain. On the left are filters that are more localized in time, i.e. when a filter is applied to a signal, it is clear when in the signal the corresponding frequency occurred. On the right are filters that have less temporal specificity, but are more uniformly distributed across frequencies compared to the left one.&lt;/p&gt;
    &lt;p&gt;Wouldn’t it be convenient if the cochlea were doing a Fourier transform, which would fit cleanly into how we often analyze signals in engineering? But no 🙅🏻♀️! A Fourier transform has no explicit temporal precision, and resembles something closer to the waveforms on the right; this is not what the filters in the cochlea look like.&lt;/p&gt;
    &lt;p&gt;We can visualize different filtering schemes, or tiling of the time-frequency domain, in the following figure. In the leftmost box, where each rectangle represents a filter, a signal could be represented at a high temporal resolution (similar to left filters above), but without information about its constituent frequencies. On the other end of the spectrum, the Fourier transform performs precise frequency decomposition, but we cannot tell when in the signal that frequency occurred (similar to right filters)3. What the cochlea is actually doing is somewhere between a wavelet and Gabor. At high frequencies, frequency resolution is sacrificed for temporal resolution, and vice versa at low frequencies.&lt;/p&gt;
    &lt;p&gt;Why would this type of frequency-temporal precision tradeoff be a good representation? One theory, explored in Lewicki 2002, is that these filters are a strategy to reduce the redundancy in the representation of natural sounds. Lewicki performed independent component analysis (ICA) to produce filters maximizing statistical independence, comparing environmental sounds, animal vocalizations, and human speech. The tradeoffs look different for each one, and you can kind of map them to somewhere in the above cartoon.&lt;/p&gt;
    &lt;p&gt;It appears that human speech occupies a distinct time-frequency space. Some speculate that speech evolved to fill a time-frequency space that wasn’t yet occupied by other existing sounds.&lt;/p&gt;
    &lt;p&gt;To drive the theory home, one that we have been hinting at since the outset: forming ecologically-relevant representations makes sense, as behavior is dependent on the environment. It appears that for audition, as well as other sensory modalities, we are doing this. This is a bit of a teaser for efficient coding, which we will get to soon.&lt;/p&gt;
    &lt;p&gt;We’ve talked about some incredible mechanisms that occur at the beginning of the sensory coding process, but it’s truly just the tiny tip of the ice burg. We also glossed over how these computations occur. The next lecture will zoom into the biophysics of computation in neurons.&lt;/p&gt;
    &lt;p&gt;We call this tonotopic organization, which is a mapping from frequency to space. This type of organization also exists in the cortex for other senses in addition to audition, such as retinotopy for vision and somatotopy for touch.&lt;/p&gt;
    &lt;p&gt;The relationship between human pitch perception and frequency is logarithmic. Coincidence? 😮&lt;/p&gt;
    &lt;p&gt;One could argue we should be comparing to a short-time Fourier transform, but this has resolution issues, and is still not what the cochlea appears to be doing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45762259</guid><pubDate>Thu, 30 Oct 2025 17:01:20 +0000</pubDate></item><item><title>Some people can't see mental images</title><link>https://www.newyorker.com/magazine/2025/11/03/some-people-cant-see-mental-images-the-consequences-are-profound</link><description>&lt;doc fingerprint="744b991fb9e104fd"&gt;
  &lt;main&gt;
    &lt;p&gt;When Nick Watkins was a child, he pasted articles about space exploration into scrapbooks and drew annotated diagrams of rockets. He knew this because, years later, he still had the scrapbooks, and took them to be evidence that he had been a happy child, although he didn’t remember making them. When he was seven, in the summer of 1969, his father woke him up to watch the moon landing; it was the middle of the night where they lived, near Southampton, in England. He didn’t remember this, either, but he’d been told that it happened. That Christmas, he and his brother were given matching space helmets. He knew that on Christmas morning the helmets had been waiting in the kitchen and that, on discovering his, he felt joy, but this was not a memory, exactly. The knowledge seemed to him more personal than an ordinary fact, but he could not feel or picture what it had been like to be that boy in the kitchen.&lt;/p&gt;
    &lt;p&gt;When he was eight or nine, he read Arthur C. Clarke’s novel “2001: A Space Odyssey” over and over. At the beginning of the book, aliens implant images of tool-using into the minds of man-apes. Near the end, the main character, David Bowman, spools backward through memories of his life:&lt;/p&gt;
    &lt;p&gt;To Nick, these events—the images in the minds of the man-apes, David Bowman’s reliving of his life—were thrilling and otherworldly, with no connection to reality, brought about through the intervention of aliens, in distant, fictional worlds.&lt;/p&gt;
    &lt;p&gt;He became a physicist. He was drawn to statistical physics and quantum mechanics, whose concepts were best described in equations. The abstraction of these ideas suited him.&lt;/p&gt;
    &lt;p&gt;One morning in 1997, when he was thirty-five, he was sitting at breakfast, paging through the newspaper. He started to read an article by a columnist he admired, Michael Bywater. Time was an illusion, Bywater wrote, because you could roll it backward and relive it: “You choose a memory, focus on it, let the rest of the mind go blank, and wait.” Bywater described particular memories of his own, not only the sight but the sound and feel of them—“the special weight of girls in autumn . . . when they lean against you as you walk along.” For some reason, these sentences revealed all at once to Nick what in the whole course of his life he had not realized: that it was possible to see pictures in your mind and use those pictures to reëxperience your past.&lt;/p&gt;
    &lt;p&gt;This was startling information. He knew, of course, that people talked about “picturing” or “visualizing,” but he had always taken this to be just a metaphorical way of saying “thinking.” Now it appeared that, in some incomprehensible sense, people meant these words literally. And then there was the notion of using those mental images to revisit a memory. It was an astonishing idea. Was it possible that this was a thing that people other than Bywater could do? Bywater had written about it quite casually, as though he took it for granted. Nick asked some people he knew, and all of them seemed to be able to do it.&lt;/p&gt;
    &lt;p&gt;He wondered whether there was something wrong with him—some kind of amnesia. He’d had no reason to worry about his memory before. He had a Ph.D. in physics; clearly his mind was functioning reasonably well. He knew the usual facts about his life—his parentage, the places he’d lived as a child, important things that had happened. It had never occurred to him that remembering could be more than that.&lt;/p&gt;
    &lt;p&gt;For many years, Nick would search for information about mental imagery, sporadically and alone. In the beginning, he did not yet know that his inability to visualize—this odd feature of his mind which appeared so insignificant that he hadn’t even noticed it for thirty-five years—would come to seem a central wellspring of his self. But then, in 2015, his condition was given a scientific name, aphantasia, and tens of thousands of people experienced the same shocked realization that he had. A flurry of research in the following decade would uncover associations between mental imagery and a bewildering variety of human traits and capacities: a propensity to hold grudges; autism; a vulnerability to trauma; emotional awareness; ways of making art and hearing music; memory of one’s life.&lt;/p&gt;
    &lt;p&gt;But this was all in the future. In 1997, as much as he interrogated his acquaintances, Nick did not find anyone like him. He couldn’t be the only person who lacked this ability to visualize, he thought. Surely it was extremely unlikely that he was unique. But, until he encountered someone else, he had to admit that it was a working possibility.&lt;/p&gt;
    &lt;p&gt;He went online and started looking. Initially, he found only work from the nineteenth century. The first useful thing he came across was William James’s book “Principles of Psychology.” James referred to observations recorded in 1860 by Gustav Fechner, a German scientist and philosopher. Fechner had subjected his own “optical memory-pictures” to introspective scrutiny and deemed them weak and lacking:&lt;/p&gt;
    &lt;p&gt;Fechner didn’t pursue the subject, however, and it lay dormant until 1880, when it was taken up by Francis Galton, a British scientist who later became notorious as the father of eugenics. Galton, supposing that he could depend on scientists to give accurate answers, wrote to several of them with a query:&lt;/p&gt;
    &lt;p&gt;The responses he received were not at all what he had expected.&lt;/p&gt;
    &lt;p&gt;Finding this Galton study came as a relief to Nick. Now at least he knew that there had been other people lacking mental imagery who’d lived normal lives, so it wasn’t a disease, or a symptom of a brain tumor. Galton had subsequently observed that women and children appeared to have more vivid imagery than the scientists did. “Scientific men as a class,” he concluded, “have feeble powers of visual representation.” Nick found this intriguing. Perhaps his own lack of imagery had somehow enhanced his scientific ability. He knew that there was, among some mathematicians, a kind of snobbery about images—a notion that, even in geometry, drawings were distractions from a purely analytical proof. But he also knew that there were any number of legends in the history of science of visions leading to discoveries. Einstein had visualized himself travelling alongside a beam of light, and this had led to his conception of relativity. The best-known instance that Nick was aware of was the German chemist August Kekulé, to whom the structure of the benzene ring had appeared in a dream:&lt;/p&gt;
    &lt;p&gt;At one point, Nick came across a paper from 1909 that stressed the importance of distinguishing between voluntary imagery (the ability to call up mental pictures at will) and involuntary imagery. Sometimes people who couldn’t call up images on purpose did experience them involuntarily—usually during migraines, or high, hallucinatory fevers, or in dreams, or the hypnagogic state just before sleep. This caught his attention because he was almost certain that he saw images in dreams, although he couldn’t be sure, since nothing remained of the images after he woke. If he was right, and he did see images in sleep, then it was strange that he couldn’t summon them at other times. Was he repressing them?&lt;/p&gt;
    &lt;p&gt;When he searched for scientific studies on imagery in the mid-twentieth century, he found very little. It seemed that the study of imagery had largely disappeared from scientific research from the nineteen-twenties to the fifties, owing in part to the dominance of behaviorism in America, which condemned inquiry into internal psychological states as unscientific. J. B. Watson, behaviorism’s founder, repudiated the existence of mental imagery altogether:&lt;/p&gt;
    &lt;p&gt;Later, researchers would debate whether Watson became a behaviorist because he had no internal imagery, or whether he actually had strong imagery but denied it because of “ideological blindness.”&lt;/p&gt;
    &lt;p&gt;In the nineteen-seventies, Nick discovered, a few psychologists, liberated from mid-century behaviorist orthodoxy, had begun to explore imagery again. A British psychologist named David Marks, for instance, developed the Vividness of Visual Imagery Questionnaire, which sought to measure a person’s ability to picture not only a stationary object but also movement (the characteristic gait of a familiar person), change (the shifting color of the sky at sunrise), and degree of detail (the window of a shop you frequently go to). But the psychologists in the nineteen-seventies were interested in people with typical imagery. When Nick searched for studies on people like himself, he found nothing.&lt;/p&gt;
    &lt;p&gt;Sometime in the early two-thousands, Jim Campbell, a Scottish surveyor in his mid-sixties, made an appointment with a neurologist at the University of Edinburgh named Adam Zeman. Jim had recently had a cardiac procedure, and afterward he’d noticed that he could no longer picture anything in his head. Before the surgery, he used to put himself to sleep by visualizing his children and grandchildren; now he couldn’t see anything at all.&lt;/p&gt;
    &lt;p&gt;Zeman had a general neurology practice—Parkinson’s, M.S., dementia—but he had also been interested in consciousness since he was a student. He speculated that one of the things that made humans different from other primates was their ability to mentally project themselves into the past or future, or into worlds that were purely imaginary. So he was fascinated to encounter, in Jim, a syndrome he had never heard of before, which appeared to be an excision of just this species-defining ability. And yet Jim was clearly very much a human—wry, reserved, down to earth. His neurological, psychiatric, and cognitive tests were all normal. If Jim had not described his condition, Zeman would not have known there was anything unusual about him.&lt;/p&gt;
    &lt;p&gt;Even questions designed to evoke imagery—Which is darker, grass or pine needles? Do squirrels have long or short tails?—Jim answered without hesitation. When Zeman asked him how he could answer without picturing these things, he said that he just knew. Zeman searched for recent scientific papers that could shed light on this strange condition but was unable to find anything useful. The case reminded him of blindsight—a rare phenomenon in which people who can’t see behave as though they can, picking up objects and avoiding obstacles. Their eyes and brains can take in visual information, but the information doesn’t rise to consciousness.&lt;/p&gt;
    &lt;p&gt;Zeman felt that Jim was not the sort of person who would make something like this up, but he wanted proof that his brain was functioning in an unusual manner. He recruited a control group of men of similar age and put them and Jim through cognitive tests in an MRI scanner. Here, he found the neurological correlate that he was looking for. Although Jim’s brain responded normally to tests of recognition (being shown images of famous faces), when he was asked to generate a mental image the scanner showed only faint brain activity, compared with the brain activity in the control group. Instead, there was activation in areas of the frontal lobe that were typically activated in situations of cognitive effort or dissonance. Jim was trying, but failing.&lt;/p&gt;
    &lt;p&gt;In 2010, Zeman, along with several colleagues, published these findings in the journal Neuropsychologia, terming the syndrome “blind imagination.” The science journalist Carl Zimmer noticed the study and wrote an article about it in Discover magazine. In the years that followed, a couple of dozen people contacted Zeman to tell him that they had the same condition, except they’d had it since birth. Zeman sent them questionnaires and tabulated their answers. At this point, he decided that lack of mental imagery was a valid syndrome that ought to have a name. After consulting with a classicist friend, he decided on “aphantasia,” phantasia being defined by Aristotle as the ability to conjure an image in the imagination. In 2015, Zeman co-wrote a paper in Cortex describing the condition as it appeared in twenty-one subjects: “Lives without imagery—Congenital aphantasia.”&lt;/p&gt;
    &lt;p&gt;An article about Zeman’s second paper appeared in the New York Times, and, after that, e-mails poured in. Around seventeen thousand people contacted him. Most were congenital aphantasics, and most not only lacked visual imagery; they could not mentally call up sounds, either, or touch, or the sensation of movement. Many had difficulty recognizing faces. Many said that they had a family member who was aphantasic, too. Most said that they saw images in dreams. Zeman recruited colleagues to work with him, and together they tried to reply to every correspondent.&lt;/p&gt;
    &lt;p&gt;Some people who wrote had once had imagery but lost it. About half of these had lost it as a consequence of physical injury—stroke, meningitis, head trauma, suffocation. The other half attributed their loss to a psychiatric cause—depersonalization syndrome, depression. A few told him that they thought they’d suppressed their capacity to visualize because traumatic memories had made imagery intolerable. Zeman learned that there had been a case in 1883, described by the French neurologist Jean-Martin Charcot, in which a man, Monsieur X, had lost his imagery; at the same time, the world suddenly appeared alien to him, and he became intensely anxious. “I observed a drastic change in my existence that obviously mirrored a remarkable change in my personality,” Monsieur X wrote to Charcot. “Before, I used to be emotional, enthusiastic with a prolific imagination; today I am calm, cold and I lost my imagination.” Another nineteenth-century French neurologist, Jules Cotard, described a patient whose loss of mental imagery was accompanied by what became known as Cotard’s delusion, or walking-corpse syndrome—the belief that he was dead.&lt;/p&gt;
    &lt;p&gt;Zeman also received messages from people who appeared to have the opposite of aphantasia: they told him that their mental pictures were graphic and inescapable. There was evidently a spectrum of mental imagery, with aphantasia on one end and extraordinarily vivid imagery on the other and most people’s experience somewhere in between. Zeman figured that the vivid extreme needed a name as well; he dubbed it hyperphantasia. It seemed that two or three per cent of people were aphantasic and somewhat more were hyperphantasic.&lt;/p&gt;
    &lt;p&gt;Many of his correspondents, he learned, had discovered their condition very recently, after reading about it or hearing it described on the radio. Their whole lives, they had heard people talk about picturing, and imagining, and counting sheep, and visualizing beaches, and seeing in the mind’s eye, and assumed that all those idioms were only metaphors or colorful hyperbole. It was amazing how profoundly people could misunderstand one another, and assume that others didn’t mean what they were saying—how minds could wrest sense out of things that made no sense.&lt;/p&gt;
    &lt;p&gt;Some said that they had a tantalizing feeling that images were somewhere in their minds, only just out of reach, like a word on the tip of their tongue. This sounded right to Zeman—the images must be stored in some way, since aphantasics were able to recognize things. In fact, it seemed that most aphantasics weren’t hampered in their everyday functioning. They had good memories for facts and tasks. But many of them said that they remembered very little about their own lives.&lt;/p&gt;
    &lt;p&gt;Among the e-mails that Zeman received, there were, to his surprise, several from aphantasic professional artists. One of these was Sheri Paisley (at the time, Sheri Bakes), a painter in her forties who lived in Vancouver. When Sheri was young, she’d had imagery so vivid that she sometimes had difficulty distinguishing it from what was real. She painted intricate likenesses of people and animals; portraiture attracted her because she was interested in psychology. Then, when she was twenty-nine, she had a stroke, and lost her imagery altogether.&lt;/p&gt;
    &lt;p&gt;To her, the loss of imagery was a catastrophe. She felt as though her mind were a library that had burned down. She no longer saw herself as a person. Gradually, as she recovered from her stroke, she made her way back to painting, working very slowly. She switched from acrylic paints to oils because acrylics dried too fast. She found that her art had drastically changed. She no longer wanted to paint figuratively; she painted abstractions that looked like galaxies seen through a space telescope. She lost interest in psychology—she wanted to connect to the foundations of the universe.&lt;/p&gt;
    &lt;p&gt;Years later, she remembered that, one night at her parents’ house, when she was still in art school, she had stayed up very late painting. She suddenly felt a strong presence behind her, and, even as she kept working, she felt the presence ask her, What do you want? In her thoughts, she responded, I want to be a great painter, and I will do whatever I have to, except take drugs. Later, she thought, Well, that is what happened. My life is very hard, but my painting is so much better.&lt;/p&gt;
    &lt;p&gt;Sheri had been an artist before she lost her imagery, but there were others who had been aphantasic for as long as they could remember. Isabel Nolan, a well-known Irish artist, had recently discovered, in her forties, while reading about Zeman’s work in New Scientist, that other people could see pictures in their heads:&lt;/p&gt;
    &lt;p&gt;She wondered whether she had always been like this. When she was a child, her mother would occasionally go on business trips, and while she was away Isabel stayed with cousins who lived up the road. She remembered lying in bed one night at her cousins’ house, thinking, What if Mam dies? I can’t remember what she looks like. She was an anxious child, frightened of many things, but this particular thought stuck in her mind for years. Now she wondered how she could have been so upset at the thought that she couldn’t picture her mother unless she’d had a notion, some vestigial memory, that such a thing was possible.&lt;/p&gt;
    &lt;p&gt;Her fear of things vanishing had not gone away. In fact, it had expanded, from her mother to everything. She had lived in Dublin almost all her life, although it would probably have been better for her career if she’d moved to London. As it turned out, it hadn’t held her back—she would be representing Ireland in the Venice Biennale in 2026—but when she was younger she’d wondered if she was making a mistake. She thought that maybe she’d stayed because having the physical infrastructure of her past around helped her to remember it. For a long time, she had felt that everything around her was ephemeral, precarious, not to be relied on:&lt;/p&gt;
    &lt;p&gt;Surely this had something to do with not being able to picture anything when she wasn’t looking at it.&lt;/p&gt;
    &lt;p&gt;At a conference, she heard artists with vivid imagery say that they were often disappointed by their work because it could never match up to the glowing vision in their heads; she felt sorry for them. When she was working on something, she never knew how it would end up. Sometimes she started with an idea, like the cosmos; she liked to look at images of deep space and draw abstractions that resembled them. She thought a lot about subjective experience, but not her own experience in particular—more what it was like to be any human, wandering through the world. She didn’t feel that her work was an extension or expression of herself, so she didn’t mind criticism, or not being understood:&lt;/p&gt;
    &lt;p&gt;Was this because of her aphantasia? If her mind were filled with pictures, would her self feel fuller, more robust? When people learned that they were aphantasic, they tended to wonder whether this or that aspect of themselves was due to their lack of imagery; sometimes it had nothing to do with it, but in this case it did—several studies had found that people with vivid imagery tended to be more inward, absorbed in the drift of their own minds.&lt;/p&gt;
    &lt;p&gt;Someone had told Isabel about a British moral philosopher, Derek Parfit, who had no imagery. He had few memories and little connection to his past, although he felt strong emotions about people and ideas in the present. Parfit believed that a self was not a unique, distinct thing but a collection of shifting memories and thoughts which intersected with the memories and thoughts of others. Ultimately, he thought, selves were not important. What mattered was the moral imperatives that drove everyone, or ought to—preventing suffering, the future welfare of humanity, the search for truth.&lt;/p&gt;
    &lt;p&gt;Isabel, like Parfit, remembered very little about her life. She kept boxes of souvenirs—ticket stubs, programs—but unless she looked at these things, or a friend reminded her, she didn’t recall most of the places she’d visited or things she’d done. She imagined that this could be a problem in a relationship, if you didn’t remember what you’d done together and the other person got upset and accused you of not caring, though fortunately she’d never been with someone like that. When she went out with friends who were full of stories, she’d worry that she wasn’t entertaining enough; normally, she drew people out and got them talking so she didn’t have to:&lt;/p&gt;
    &lt;p&gt;It would be nice to remember all the funny stories that people told, but in the end she didn’t mind too much. She could just sit there and bask in the pleasure of being with old friends. It was the feeling that was important; she didn’t need to know what had happened years ago. In some ways, this made things easier—she mostly didn’t remember arguments or bad feelings. She hoped that the significant moments in her life, good and bad, had left their imprint on her in some way, but it was impossible to know:&lt;/p&gt;
    &lt;p&gt;Clare Dudeney was an artist who worked in southeast London, in a warren of old factory buildings by the Thames. Against one wall of her studio was a wooden loom, above which large spools of cotton thread in a rainbow of colors were slotted on pegs. She made works in many media, all cornucopias of color: pieces of fabric dyed robin’s-egg blue or pistachio or hazelnut or citrine and pasted into collages, some so long that you couldn’t take them in at once and hung near open doors so that they rippled. She made murals of ceramic tiles painted with irregular shapes, like countries on a map, in powder-puff pink and celery and yellow and wheat; rectangular blocks of rough wood that she called woodcut paintings, with teal, red, cornflower, and lime pigment staining or filling the crevices and gouges of the surface; long clay worms, basket-woven and glazed—forest, mustard, chestnut—like ceramic macramé. She draped herself in colors, too: thick scarves and nubby sweaters that she knitted herself; geometric-patterned skirts.&lt;/p&gt;
    &lt;p&gt;In talking to a friend of hers, an aphantasic painter who was one of Zeman’s research subjects, Clare had realized that she was the opposite—hyperphantasic. Her imagery was extraordinarily vivid. There was always so much going on inside her head, her mind skittering and careening about, that it was difficult to focus on what or who was actually in front of her. There were so many pictures and flashes of memory, and glimpses of things she thought were memory but wasn’t sure, and scenarios real and imaginary, and schemes and speculations and notions and plans, a relentless flood of images and ideas continuously coursing through her mind. It was hard to get to sleep.&lt;/p&gt;
    &lt;p&gt;At one point, in an effort to slow the flood, she tried meditation. She went on a ten-day silent retreat, but she disliked it so much—too many rules, getting up far too early—that she rebelled. While sitting in a room with no pictures or stimulation of any kind, supposedly meditating, she decided to watch the first Harry Potter movie in her head. She wasn’t able to recall all two hours of it, but watching what she remembered lasted for forty-five minutes. Then she did the same with the other seven films.&lt;/p&gt;
    &lt;p&gt;She tried not to expose herself to ugly or violent images because she knew they would stick in her mind for years. But even without a picture, if she even heard about violence her mind would produce one. Once, reading about someone undergoing surgery without anesthetic, she imagined it so graphically that she fainted. (In 2012, two Harvard psychologists published a study about visual imagery and moral judgment. They found that people with weak imagery tended to think more abstractly about moral questions and believe that good ends sometimes justified harmful means. But for people with strong imagery, the harmful means—injuries done to one person in order to save several others, say—formed such lurid pictures in their minds that they responded emotionally and rejected them.)&lt;/p&gt;
    &lt;p&gt;Even joyful images could turn on her. She’d had a cat that she loved; she was separated from her husband and living on her own, so she had spent more time with the cat than with any other creature. Then the cat died, and after his death she saw him everywhere—on the sofa, on the floor, on her bed, wherever he had been in life. She saw him so clearly that it was as though he were actually there in front of her. Her grief was made so much worse by this relentless haunting that she began to feel as if she would not be able to cope.&lt;/p&gt;
    &lt;p&gt;Her father was a physicist and for many years the deputy director of the British Antarctic Survey. When Clare was a child, he promised that one day he would take her to Antarctica, and finally, when she was in her thirties, in 2013, he did. There, on the boat, she found herself looking at a landscape so wholly unfamiliar that her brain struggled to make sense of it. At times, it barely appeared to her like a landscape at all, more like an abstract surface, without reference or meaning. The place was vast, and there were no people. Snow and ice formed strange patterns on the surface of the sea. As they travelled, the terrain kept changing, so her sense of alien newness persisted. It was as if, for the first time, she was seeing not through the cluttered, obscuring scrim of her visual memories but directly, at the world itself. Just looking at it was so demanding that it occupied her whole mind, so that she wasn’t thinking about anything else, she was just there. At the time, she was consulting on climate and sustainability issues, but after that trip she decided to become an artist.&lt;/p&gt;
    &lt;p&gt;Usually, her ideas for art works came not from anything external but from images in her head. For a while, she had made paintings based on her dreams. She kept a journal and a pen by the side of her bed so that she could describe what she’d dreamed the moment she woke. The more she wrote down her dreams, the more she remembered them; sometimes she would remember ten dreams in a single night. Eventually, the process began to fold in on itself—while she was still asleep, she’d begin to dream that she was taking notes on the dream, and planning how to draw what she saw.&lt;/p&gt;
    &lt;p&gt;When she thought about making a new piece, she often worked it out in her mind beforehand. Being hyperphantasic didn’t mean only that your imagery was bright and sharp; it meant that you could manipulate your images at will, zooming in and out, cutting and pasting, flipping and mirroring, creating pictures from scratch, assembling and disassembling complicated objects. Even when she was trying to evoke the colors of a landscape at a certain time of day, she did it not from life but from memory.&lt;/p&gt;
    &lt;p&gt;She didn’t know how common this was among artists, but there were some who she was fairly sure had worked from their imaginations rather than from life. J. M. W. Turner, for instance, made rough sketches outdoors, but the seas and skies and light of his paintings all came from his head. There was an English portraitist working in the late eighteenth century whose prodigious powers of visualization had been described in a case study. The study didn’t name the painter but said that he’d inherited most of the clients of Sir Joshua Reynolds after Reynolds’s death, and had proceeded to take full advantage of this by painting three hundred portraits in a single year. The study’s author, a British physician named A. L. Wigan, reported:&lt;/p&gt;
    &lt;p&gt;This painter’s imagery was so lifelike, however, that he began to confuse his mind’s pictures with reality, and succumbed to a mental illness that lasted thirty years.&lt;/p&gt;
    &lt;p&gt;Hyperphantasia often seemed to function as an emotional amplifier in mental illness—heightening hypomania, worsening depression, causing intrusive traumatic imagery in P.T.S.D. to be more realistic and disturbing. Reshanne Reeder, a neuroscientist at the University of Liverpool, began interviewing hyperphantasics in 2021 and found that many of them had a fantasy world that they could enter at will. But they were also prone to what she called maladaptive daydreaming. They might become so absorbed while on a walk that they would wander, not noticing their surroundings, and get lost. It was difficult for them to control their imaginations: once they pictured something, it was hard to get rid of it. It was so easy for hyperphantasics to imagine scenes as lifelike as reality that they could later become unsure what had actually happened and what had not.&lt;/p&gt;
    &lt;p&gt;One hyperphantasic told a researcher that he had more than once walked into a wall because he had pictured a doorway.&lt;/p&gt;
    &lt;p&gt;Because their imaginative lives were so compelling, hyperphantasics tended to be inwardly focussed. This could mean that they were detached from reality, living in the remembered past and the imaginary future rather than in the actual present. But it could also mean that they were hyperaware of their internal reality, tuned in to the cues of their bodies and the shifts in their emotions. Some researchers hypothesized that the heightened awareness of these bodily and emotional signals were one reason that people with vivid imagery usually had strong memories of their pasts—these signals somehow helped to “anchor memories to the self.”&lt;/p&gt;
    &lt;p&gt;Hyperphantasics’ memories could be exceptionally detailed.&lt;/p&gt;
    &lt;p&gt;Memories might take on quasi-physical forms in their minds. They might picture sheaves of recollections, or files of information, sitting on shelves in a mental warehouse. They might envision lists of facts about a particular place pinned to that place on a vast and detailed mental map that they saw spread out before them, like a hologram.&lt;/p&gt;
    &lt;p&gt;Reeder had tested children’s imagery and believed that most children were hyperphantasic. They had not yet undergone the synaptic pruning that took place in adolescence, so there were incalculably more neuronal connections linking different parts of their brain, giving rise to fertile imagery. Then, as they grew older, the weaker connections were pruned away. Because the synapses that were pruned tended to be the ones that were used less, Reeder thought it was possible that the children who grew up to be hyperphantasic adults were those who kept on wanting to conjure up visual fantasy worlds, even as they grew older. Conversely, perhaps children who grew up to become typical imagers daydreamed less and less, becoming more interested in the real people and things around them. Maybe some children who loved to daydream were scolded, in school or at home, to pay attention, and maybe these children disciplined themselves to focus on the here and now and lost the ability to travel to the imaginary worlds they’d known when they were young.&lt;/p&gt;
    &lt;p&gt;Clare had not been discouraged from daydreaming as a child, and she had preferred it to the other common form of imaginative dissociation, reading. Daydreaming was more pleasurable for her because she had struggled to learn to read, and even once she knew how she’d found it slow going. When she received a diagnosis of dyslexia, as an adult, the tester told her that, rather than processing individual letters or sounds, she was memorizing pictures of whole words, which made it hard to recognize words in different fonts. Her visual sense was so overweening that reading was strenuous, because she was easily distracted by the squiggles and lines of the text.&lt;/p&gt;
    &lt;p&gt;Naturally, aphantasics usually had a very different experience of reading. Like most people, as they became absorbed, they stopped noticing the visual qualities of the words on the page, and, because their eyes were fully employed in reading, they also stopped noticing the visual world around them. But, because the words prompted no mental images, it was almost as if reading bypassed the visual world altogether and tunnelled directly into their minds.&lt;/p&gt;
    &lt;p&gt;Aphantasics might skip over descriptive passages in books—since description aroused no images in their minds, they found it dull—or, because of such passages, avoid fiction altogether. Some aphantasics found the movie versions of novels more compelling, since these supplied the pictures that they were unable to imagine. Of course, for people who did have imagery, seeing a book character in a movie was often unsettling—because they already had a sharp mental image of the character which didn’t look like the actor, or because their image was vague but just particular enough that the actor looked wrong, or because their image was barely there at all and the physical solidity of the actor conflicted with that amorphousness.&lt;/p&gt;
    &lt;p&gt;Presumably, novelists who invented characters also had a variety of responses to seeing them instantiated in solid form. Jane Austen wrote a letter to her sister in 1813 in which she described going to an exhibition of paintings in London and searching for portraits that looked like Elizabeth Bennet and Jane Bingley, two main characters from “Pride and Prejudice.” To her delight, she’d seen “a small portrait of Mrs Bingley, excessively like her . . . exactly herself, size, shaped face, features &amp;amp; sweetness; there never was a greater likeness. She is dressed in a white gown, with green ornaments, which convinces me of what I had always supposed, that green was a favourite color with her.” Austen did not see Elizabeth at the exhibition but hoped, she told her sister, to find a painting of her somewhere in the future. “I dare say Mrs D.”—she wrote, Darcy being Elizabeth’s married name—“will be in Yellow.”&lt;/p&gt;
    &lt;p&gt;One of the twenty or so congenital aphantasics who contacted Adam Zeman after his original 2010 paper was a Canadian man in his twenties, Tom Ebeyer. Ebeyer volunteered to participate in Zeman’s studies, and, after Zeman published his 2015 Cortex paper on congenital aphantasia, Ebeyer was one of the participants quoted in the Times article about it. After that, hundreds of aphantasics reached out to him on Facebook and LinkedIn. They asked him questions he didn’t know the answers to: Does this mean I have a disability? Is there a cure?&lt;/p&gt;
    &lt;p&gt;Many of Ebeyer’s correspondents felt shocked and isolated, as he had; he decided that what was needed was a online forum where aphantasics could go for information and community. He set up a website, the Aphantasia Network. He didn’t want it to be a sad place where people commiserated with one another, however. There were good things about aphantasia, he believed, and he began to write uplifting posts pointing them out. In one, he argued that aphantasia was an advantage in abstract thinking. When prompted by the word “horse,” a person with imagery would likely picture a particular horse—one they’d seen in life, perhaps, or in a painting. An aphantasic, on the other hand, focussed on the concept of a horse—on the abstract essence of horseness. Ebeyer published posts about famous people who had realized that they were aphantasic: Glen Keane, one of the leading Disney animators on “The Little Mermaid” and “Beauty and the Beast”; John Green, the author of “The Fault in Our Stars,” whose books had sold more than fifty million copies; J. Craig Venter, the biologist who led the first team to sequence the human genome; Blake Ross, who co-created the Mozilla-Firefox web browser when he was nineteen.&lt;/p&gt;
    &lt;p&gt;Ebeyer also wanted the Aphantasia Network to be a place where aphantasics could find recent scientific research. For instance, estimating the strength of a person’s imagery had been thoroughly subjective until Joel Pearson, a cognitive neuroscientist at the University of New South Wales, in Australia, devised tests to measure it more precisely. In a paper from 2022, Pearson reported that when people with imagery visualized a bright object their pupils contracted, as though they were seeing a bright object in real life, but the pupils of aphantasics imagining a bright object stayed the same. Another study of his had shown that, although aphantasics had the same fear response (sweating) as typical imagers to a frightening image shown on a screen, when exposed to a frightening story they barely responded at all.&lt;/p&gt;
    &lt;p&gt;Ebeyer kept in touch with Zeman and published bulletins about his research. Zeman had found that aphantasics could solve many problems that would seem to require imagery, such as counting the number of windows in their home. This, Zeman hypothesized, was due to the difference between object imagery and spatial imagery. There were two streams of visual information in the brain that were, to a surprising degree, distinct from each other: one had to do with recognition of objects; the other, with guiding action through space. Aphantasics lacked object imagery, but they might have the kind of spatial imagery that would enable them to count windows. One aphantasic described his ability to do this as a kind of echolocation.&lt;/p&gt;
    &lt;p&gt;To Zeman, one of the most tantalizing promises of the study of mental imagery was the light it might shed on the neural correlates of consciousness. Connectivity in the brain seemed to be particularly important in both consciousness and aphantasia. fMRI studies had shown reduced connectivity in aphantasics, and Brian Levine, a neuropsychologist at the Baycrest Academy for Research and Education, in Toronto, had found that connectivity between the memory system and the visual-perceptual regions in the brain correlated to how well people remembered their lives. Many of the aphantasics who had written to Zeman identified themselves as autistic. Autism was thought to be a state of reduced long-range connectivity in the brain, so Zeman theorized that there could be a link. But autism had also been associated with thinking in pictures—Temple Grandin, for instance, the autistic writer and professor of animal science, described her autism that way—so clearly the link was not a simple one.&lt;/p&gt;
    &lt;p&gt;After creating the Aphantasia Network, Ebeyer received tens of thousands of messages from all over the world—Korea, Venezuela, Madagascar. He launched Aphantasia Network Japan, and made plans for a Spanish-language site. When the city of Rowlett, a suburb of Dallas, declared the world’s first Aphantasia Awareness Day, on February 21, 2023, his site published a celebratory post. Once hyperphantasia began to be written about, he started to hear from hyperphantasics as well. When he wrote a post about how some people could “hear” music in their heads, or relive touch or tastes, most responses were from aphantasics amazed to learn that such things were possible. But one person wrote to him describing a kind of auditory hyperphantasia:&lt;/p&gt;
    &lt;p&gt;This past January, Zeman and others published a short article in Cortex clarifying that the definition of aphantasia encompassed people with weak imagery. Ebeyer wrote a post in response, wondering whether this inclusive definition risked diluting the experiences of those with total aphantasia, such as himself. Might it threaten the cohesion of the aphantasia community? Aphantasia, at this point, wasn’t only a syndrome, after all—it was an identity.&lt;/p&gt;
    &lt;p&gt;In the course of his quest to learn about imagery, Nick Watkins, the physicist, came across an essay by Oliver Sacks. Sacks mentioned that he normally had almost no mental imagery but that, during a two-week period in his thirties when he’d been downing heroic quantities of amphetamines, he’d suddenly been able to retain images in his mind—though only images of things that he had just looked at. During that time, he also found it much more difficult to think in abstractions. When the drugs wore off, the images dissipated and his abstract thinking returned. This was an auspicious discovery, Nick thought, that you could somehow turn imagery on. He was certainly not going to take amphetamines himself—he was a pretty cautious person—especially if doing so might jeopardize his ability to think abstractly. But if amphetamines could work, maybe something else could, too.&lt;/p&gt;
    &lt;p&gt;He kept looking. He discovered that Aldous Huxley was aphantasic and that, in “The Doors of Perception,” he had written that he was expecting mescaline to change this, even if only for a few hours. (It didn’t.) Unsurprisingly, amid the recent research on psychedelics, this hope of arousing mental vision with drugs had been revived. In 2018, the Journal of Psychedelic Studies published a paper about an aphantasic man, S.E., who had taken ayahuasca and had an intensely emotional experience of visualizing, and then forgiving, his father, long dead, who had left him when he was very young. Afterward, S.E. was still able to see images, but only faintly. He and the paper’s authors concluded that his aphantasia had likely been psychological in origin, since it was resolved by his feeling that things between him and his father had been settled. Another paper, published in the same journal in 2025, described an autistic aphantasic woman in her mid-thirties who had eaten psilocybin truffles and experienced mental imagery for the first time. Her imagery persisted for many months, although it was not quite as vivid as during the trip itself.&lt;/p&gt;
    &lt;p&gt;Nick kept hoping that someone would find a way of stimulating imagery that didn’t involve drugs. On the other hand, as he learned more about people with imagery, he was less inclined to envy them. At first, he had thought that having imagery would be like having a VCR, being able to play home movies whenever you felt wistful. But, reading more about it, he had learned that memories and images could break in on you, unbidden and uncontrollable, and not necessarily happy ones. Even if the imagery wasn’t frightening, it would surely be a distraction. He had come to value the dark and quiet of his mind.&lt;/p&gt;
    &lt;p&gt;Nick knew that whenever Zeman talked about aphantasia he was at pains to emphasize that it was not a disorder, or even a bad thing. It was best described as an interesting variant in human experience, like synesthesia. Nick appreciated this about Zeman, and reckoned that it was probably the right thing to say, but he thought that, though aphantasia itself might be neutral, the memory loss that came with it was definitely a bad thing. Many others felt the same. At one point, Zeman had been contacted by an automotive engineer from Essex named Alan Kendle, who had realized that he was aphantasic while listening to a radio segment about the condition. This revelation affected him so strongly that he put together a book of interviews with aphantasics, identified just by their initials, to help others navigate the discovery. Some people he interviewed were unbothered—there was definitely a range of responses—but others saw it as a curse.&lt;/p&gt;
    &lt;p&gt;Many could remember very little about their lives, and even with the events they did remember they could not muster the feeling of what they’d been like. They knew that some things had made them happy and others had made them sad, but that knowledge was factual—it didn’t evoke any emotions in the present.&lt;/p&gt;
    &lt;p&gt;The advantage of a bad memory was that aphantasics seemed to suffer less from regret, or shame, or resentment.&lt;/p&gt;
    &lt;p&gt;But this supposed advantage was just the silver lining of something pretty dark. When aphantasics recovered from bereavement, or breakups, or trauma, more quickly than others, they worried that they were overly detached or emotionally deficient. When they didn’t see people regularly, even family, they tended not to think about them.&lt;/p&gt;
    &lt;p&gt;One of Kendle’s interviewees was Melinda Utal, a hypnotherapist and a freelance writer from California. She had trouble recognizing people, including people she knew pretty well, so she tended to avoid social situations where she might hurt someone’s feelings. When she first discovered that she was aphantasic, she called her father, who was in the early stages of Alzheimer’s disease and living in a nursing home in Oregon. He had been a musician in big bands—he had toured with Bob Hope and played with Les Brown and his Band of Renown. She asked him whether he could imagine a scene in his head, and he said, Of course. I can imagine going into a concert hall. I see the wood on the walls, I see the seats, I know I’m going to sit at the back, because that’s where you get the best sound. I can see the orchestra playing a symphony, I can hear all the different instruments, and I can stop it and go backward to wherever I want it to start up and hear it again. She explained to her father what aphantasia was, how she couldn’t see images in her mind, or hear music, either. On the phone, her father started to cry. He said, But, Melinda, that’s what makes us human.&lt;/p&gt;
    &lt;p&gt;Melinda had an extremely bad memory for her life, even for an aphantasic. She once had herself checked for dementia, but the doctor found nothing wrong. She had become aware when she was in second grade that she had a bad memory, after a friend pointed it out. In an effort to hold on to her memories, she started keeping a journal in elementary school, recording what she did almost every single day, and continued this practice for decades. When, in her sixties, she got divorced and moved into an apartment by herself, she thought it would be a good time to look through her journals and revisit her younger days. She opened one and began to sob because, to her horror, the words she had written meant nothing to her. The journals were useless. She read about things she had done and it was as though they had happened to someone else.&lt;/p&gt;
    &lt;p&gt;It was not just the distant past that she had lost—she was continuously aware of the present slipping away as soon as it happened. She had already forgotten what her two sons had been like when they were little, the feeling of holding them:&lt;/p&gt;
    &lt;p&gt;Now her greatest fear was that, if she hadn’t seen her sons in a while, she might forget them altogether:&lt;/p&gt;
    &lt;p&gt;Although Nick had made his peace with his lack of imagery, he still grieved his inability to revisit his past. At one point, he came across the work of a Canadian psychologist, Endel Tulving, who, in the early nineteen-seventies, proposed that memory was not a single thing but two distinct systems: semantic memory, which consisted of general knowledge about the world, and episodic memory—recollection of experiences from your own life. Episodic memory, the sense of reliving the past, was, Tulving believed, unique to humans, and among the most astonishing products of evolution. This, Nick realized, was what he didn’t have. Learning that he lacked a profound human ability—one that, he had to assume, regenerated and immeasurably deepened your connection to your past life and the people in it who were now gone, including yourself as a child—well, there was nothing good about it. He would have preferred not to know.&lt;/p&gt;
    &lt;p&gt;He wrote to Tulving, who told him about a study to be conducted by Brian Levine, the Baycrest neuropsychologist, who had been a colleague of his in Toronto. The study would investigate exceptionally poor autobiographical memory in healthy adults—people who did not have amnesia or dementia or brain injury or psychological trauma. Levine later named this syndrome “severely deficient autobiographical memory,” or sdam. Nick was accepted as a participant and travelled to Toronto. The study found that the participants’ experience of sdam could be objectively corroborated, using a variety of methods, by comparing them to a control group. fMRI, for instance, showed reduced activation in the midline regions of their brains, an area normally associated with mental time travel.&lt;/p&gt;
    &lt;p&gt;Nick was surprised to hear that another participant in the study had described an even starker experience of episodic memory loss than his. She felt so detached from her past that the facts she knew about it felt to her no more personal than facts about someone else. He definitely didn’t feel that way. The things he knew about his life felt more personal to him than facts he knew about physics, say, even though he couldn’t inhabit them in the way that other people could. He realized that Tulving’s binary schema, which categorized all memory as either episodic or semantic, was too simple. His own memories were somewhere in between. He remembered that on the day that his mother died, in 2003, his sister had phoned him to say that their mother was being admitted to the hospital; he had taken a train from Cambridge to London, and he had phoned an old friend to meet him in London because he was worried that, in his distress, he might go to the wrong station and miss the second train he needed to catch, but the friend helped him, and he got on the right train, and it was around Guy Fawkes Night, fireworks going off outside the train window, and then he got to the hospital and was there for a while, and then his mother died. He knew these things, and the idea of his mother dying aroused emotion in him, but he couldn’t feel what it had been like to be in the train, or the hospital, and he could not remember his mother’s face.&lt;/p&gt;
    &lt;p&gt;From an evolutionary point of view, he supposed, he had all the memory he needed: enough to know what and whom he had loved, and what he should try to avoid doing again. But to think about it that way was to miss what was most important—not the function of episodic memory but the experience of it. As he absorbed what it meant to lack episodic memory, he started wondering whether there were ways he could simulate it. He was attracted to the idea of video life-logging with wearable cameras—the footage would be a decent substitute for mental time travel. His childhood and early adulthood were lost to him, but if he started filming now he would be able to relive at least the last decade or two of his life.&lt;/p&gt;
    &lt;p&gt;On a trip to Pasadena, he went to the Apple Store and tried on a virtual-reality headset. This, he thought, must be what episodic memory is like. He knew it would probably be a long time before people accepted such technologies, but perhaps one day wearable cameras would be recognized as prosthetics for people with SDAM, no more remarkable than glasses. Then again, film would be very different from memory. Like memory, it would be partial, but, unlike memory, it would be accurate. This, he suspected, might not necessarily be a good thing. There was something to be said for a degree of blurriness and uncertainty in recalling the past; it was helpful in forgiving other people, and yourself.&lt;/p&gt;
    &lt;p&gt;At some point, Nick became interested in the ideas of a British philosopher, Galen Strawson, who claimed to have no sense of himself as a continuously evolving being—a creature whose self consisted of a coherent story about accumulating memories and distinctive traits. Strawson was, for that reason, uninterested in his past. He acknowledged that his life had shaped him, but he believed that whether or not he consciously remembered it didn’t matter to who he was now, any more than it mattered whether a musician playing a piece could call to mind a memory of each time he’d practiced: what mattered was how well he played. What was important, Strawson felt, was his life in the present. He liked to quote the third Earl of Shaftesbury, a British philosopher of the late seventeenth and early eighteenth centuries, who had felt the same way:&lt;/p&gt;
    &lt;p&gt;Nick wasn’t sure he agreed with Strawson, and he certainly didn’t feel, as Strawson did, that his memory of his own life was unimportant, but he found the argument somewhat comforting. He still longed to relive important moments in his life, but it was easier to think about this experience as just one of many he hadn’t had, like paragliding, or visiting Peru, than as a void at the core of his self. Many people believed that their selves were made up largely of memories, and that the loss of those memories would be a self-ending catastrophe. But he knew now that there were also thousands of people like him, who had work and marriages and ideas and thwarted desires and good days and bad days and the rest of it. All they lacked was a past. ♦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45762837</guid><pubDate>Thu, 30 Oct 2025 17:45:53 +0000</pubDate></item><item><title>I have released a 69.0MB version of Windows 7 x86</title><link>https://twitter.com/XenoPanther/status/1983477707968291075</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45763076</guid><pubDate>Thu, 30 Oct 2025 18:05:39 +0000</pubDate></item><item><title>Rapid Brightening of 3I/Atlas Ahead of Perihelion</title><link>https://arxiv.org/abs/2510.25035</link><description>&lt;doc fingerprint="ec1ca02b549fd4ab"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Astrophysics &amp;gt; Earth and Planetary Astrophysics&lt;/head&gt;&lt;p&gt; [Submitted on 28 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Rapid Brightening of 3I/ATLAS Ahead of Perihelion&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Interstellar comet 3I/ATLAS has been approaching its 2025 October 29 perihelion while opposite the Sun from Earth, hindering ground-based optical observations over the preceding month. However, this geometry placed the comet within the fields of view of several space-based solar coronagraphs and heliospheric imagers, enabling its continued observation during its final approach toward perihelion. We report photometry from STEREO-A's SECCHI HI1 and COR2, SOHO's LASCO C3, and GOES-19's CCOR-1 instruments in 2025 September--October, which show a rapid rise in the comet's brightness scaling with heliocentric distance r as r^(-7.5+/-1.0). CCOR-1 also resolves the comet as an extended source with an apparent coma ~4' in diameter. Furthermore, LASCO color photometry shows the comet to be distinctly bluer than the Sun, consistent with gas emission contributing a substantial fraction of the visible brightness near perihelion.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;astro-ph.EP&lt;/p&gt;&lt;p&gt; Change to browse by: &lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45763367</guid><pubDate>Thu, 30 Oct 2025 18:25:16 +0000</pubDate></item><item><title>Taking money off the table</title><link>https://zachholman.com/posts/money-off-the-table</link><description>&lt;doc fingerprint="b318b01b5be765db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Taking Money off the Table&lt;/head&gt;
    &lt;p&gt;Recently I had a long call with an old friend who was facing an age-old predicament that I’ve been seeing more and more these days:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lucked out, worked hard, employer is crushing it, and now she’s sitting on a large amount of paper money gains at her startup&lt;/item&gt;
      &lt;item&gt;Company does a tender offer, either buying their stock back or allowing a third party to come in and buy shares&lt;/item&gt;
      &lt;item&gt;Employees might be allowed to sell, say, 10% (or whatever) of their equity&lt;/item&gt;
      &lt;item&gt;So here’s the question: do you sell, or do you roll the dice and risk it longer?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I took a tender offer on my early GitHub shares, and it comes up a lot in emails and conversations with others I run across in the startup world. It’s a decision can be annoyingly agonizing. And there’s a lot of conflicting advice out there, each with different motivations behind it. I’ve added to that, too, with lots of advice over the years of “look at all the alternatives in front of you and make an even-headed decision”.&lt;/p&gt;
    &lt;p&gt;Anyway, fuck it, here’s the bottom line: take that money, queen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gambling with your life&lt;/head&gt;
    &lt;p&gt;I’ve found it helpful to look at your life as a gamble: a set of probabilities that add up to whether or not you should make a decision a certain way. Assuming you’re in this situation, you might be looking at a windfall of, say, half a million to tens of millions of dollars. That’s wildly lucky, and can be life-changing money.&lt;/p&gt;
    &lt;p&gt;Two massive motivators in this decision:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It’s way easier to make more money when you already have money.&lt;/item&gt;
      &lt;item&gt;Successful startups are an insane mix of timing and luck — no matter what people who sell online courses will tell you — and if you’re at a point where your imaginary gains are truly life-altering… you’ve already won. Now: try not to lose.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m here to tell you: don’t fuck it up. It’s easy to assume two things, because we’re Startup People who are Bold and certainly will Change The World (by increasing query performance by 6%):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;we assume that this will be just the first of many correct startup decisions we will make and every four years we’ll be faced with this decision&lt;/item&gt;
      &lt;item&gt;that the startup we’re at will only go up and to the right because that’s all it’s ever done and surely it won’t face hard times both internally or externally&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Successful startups sometimes fuck up. I interviewed at Zenefits about a month before the first bad press came at them- back then it was one of the fastest-growing companies of all time. One of the execs interviewed me and guaranteed I’d become stupid rich if I joined. It was obvious to them they were all going to be gazillionaires in no time at all. I came away with dozens of utterly insane stories I continue to tell over drinks, and frankly was horrified at how much of a clusterfuck everything internally was. But you talk to them and they were all convinced they could do no wrong. (Except for head of infrastructure- he was the most interesting person I interviewed with and we had a fantastic discussion about how fucked up the infrastructure was. Figures that the earliest to know something are the ones who see it break the most.)&lt;/p&gt;
    &lt;p&gt;All of this to say: it’s easy to become delusional while at a startup — in fact, you could argue that the best startups have that cult-like delusion built into their DNA. But things can change. Or the external structures can change. I don’t have to tell you that tech is in a bubble right now; everyone knows it, everyone knows it’s going to pop, but no one knows when or the extent of it. There have been hundreds or thousands of startups over the decades, staffed with the best and brightest, with revenue, with customers… and they’ve still bitten the bullet. That’s the game. So don’t look a gift horse in the mouth; take the horse’s wallet instead. (Sometimes my metaphors don’t always land.)&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s a forcing function&lt;/head&gt;
    &lt;p&gt;You reading all this and thinking it doesn’t apply to you? That your startup is c r u s h i n g it and that by selling, you’re leaving so much money on the table? Good. One of the reasons I wrote this in this way is to act like a forcing function: get you to be horrified at the thought and make you critically analyze if holding on to your stock makes sense or not. It’s like the advice of flipping a coin to make a decision, and as the coin is in the air you’ll learn which decision it is that you’re hoping it will land on.&lt;/p&gt;
    &lt;p&gt;I will say this, though: I did take a tender offer after I left GitHub, it was a wildly stressful decision, but I have zero regrets today. I took something like 10% off the table, which had a positive impact on my abysmal emotional health at the time, and had GitHub ultimately eaten it, I would have had at least something left to show for all the work I had put into the company.&lt;/p&gt;
    &lt;p&gt;Amusingly enough, this post itself stemmed from conversations with Billy Gallagher, the founder of Prospect, one of my angel investments. They do scenarios and projections of early equity stakes, and I basically told him that I’m too horrified of doing the retroactive math behind taking the tender and dealing with all the stock fuckups GitHub subjected us to. It’s probably a large number. But I don’t think about it at all today. Would have been helpful at the time, sure, but the stress is a product of the time, and likely not one that will stick with you forever… if that helps you make these decisions.&lt;/p&gt;
    &lt;p&gt;I also found it helpful to realize something logistical, too: the money you take today is, you know, still money. You can invest it, diversify it, grow it. The exponential growth of startup equity makes the more linear — but still compounding — growth of “normal” investing feel like you’re just losing out, but you’re still compounding that cash. It doesn’t just disappear.&lt;/p&gt;
    &lt;head rend="h2"&gt;You can afford to not be perfect&lt;/head&gt;
    &lt;p&gt;So yeah- all of this is good to think about. Run the numbers. Model different scenarios. Get a real understanding of the trade-offs here.&lt;/p&gt;
    &lt;p&gt;Sometimes it’s helpful to “get permission” from someone — anyone — though. Like, that it’s acceptable to make this tradeoff. When GitHub got acquired, one of the best pieces of advice I got was: you can afford to not be perfect. There’s this weird pressure out there that every single financial decision needs to be optimized for every little bit of performance… but sometimes you can miss the forest for the trees with that. You can also drive yourself mad, and forget why you’re doing all this in the first place.&lt;/p&gt;
    &lt;p&gt;I’d also be remiss to not mention that this isn’t entirely a “rich person problem”. I’ve known many paper millionaires who were functionally broke: they had school debt, or were putting their partner through school, or had kids with costly needs, or they were responsible for multiple families or generations who were relying upon them. Liquidity in startups is increasingly harder to come by these days. Life is constantly about planning for the future, whenever that thing might come. Sometimes it’s helpful to think about today, too.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45763769</guid><pubDate>Thu, 30 Oct 2025 18:50:11 +0000</pubDate></item><item><title>TruthWave – A platform for corporate whistleblowers</title><link>https://www.truthwave.com</link><description>&lt;doc fingerprint="7d44df6756f66ba1"&gt;
  &lt;main&gt;
    &lt;p&gt;Our Beta is Now Live!&lt;/p&gt;
    &lt;head rend="h1"&gt;This is TruthWave.&lt;/head&gt;
    &lt;p&gt;Welcome to the platform and community for those who bring unethical corporations to justice.&lt;/p&gt;
    &lt;p&gt;For far too long, harmful corporate culture has stigmatized and disincentivized information flow. TruthWave is rewriting this narrative by creating a platform that financially compensates whistleblowers for courageously stepping forward while leveraging a global community built around justice.&lt;/p&gt;
    &lt;p&gt;At its core, TruthWave is an information platform that allows those who possess or locate vital information about corporate wrongdoing to share it securely and anonymously.&lt;/p&gt;
    &lt;p&gt;Have our next big case? If you know about corporate wrongdoing, submit a tip to bring those responsible to justice.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45763858</guid><pubDate>Thu, 30 Oct 2025 18:57:39 +0000</pubDate></item><item><title>Minecraft HDL, an HDL for Redstone</title><link>https://github.com/itsfrank/MinecraftHDL</link><description>&lt;doc fingerprint="3271921d09773db1"&gt;
  &lt;main&gt;
    &lt;p&gt;Minecraft HDL is a digital synthesis flow for minecraft redstone circuits. It is an attempt to use industry standard design tools and methods to generate digital circuits with redstone.&lt;/p&gt;
    &lt;p&gt;This file &lt;code&gt;multiplexer4_1.v&lt;/code&gt; is a 6 input - 1 output circuit that selects one of the first 4 inputs (a, b, c, d) as the output based on the value of the last 2 inputs (x, y)&lt;/p&gt;
    &lt;code&gt;module multiplexer4_1 ( a ,b ,c ,d ,x ,y ,dout ); 
 
output dout ; 
input a, b, c, d, x, y; 
 
assign dout = (a &amp;amp; (~x) &amp;amp; (~y)) | 
     (b &amp;amp; (~x) &amp;amp; (y)) |  
     (c &amp;amp; x &amp;amp; (~y)) | 
     (d &amp;amp; x &amp;amp; y); 
endmodule &lt;/code&gt;
    &lt;p&gt;When synthesized through Minecraft HDL it produces this circuit:&lt;/p&gt;
    &lt;p&gt;With the 6 inputs on the right and the single output on the left&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Screenshots &amp;amp; Sample Circuits&lt;/item&gt;
      &lt;item&gt;Getting Started - Installing and Using MinecraftHDL&lt;/item&gt;
      &lt;item&gt;Background Theory - Digital Design &amp;amp; Verilog&lt;/item&gt;
      &lt;item&gt;How MinecraftHDL Works - Read Our Paper&lt;/item&gt;
      &lt;item&gt;Developper Info - If you want to fork or contribute&lt;/item&gt;
      &lt;item&gt;Quick Overview - Check out our poster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MinecraftHDL was the final undergraduate design project made by three students in the Electrical, Computer &amp;amp; Software Engineering department at McGill University.&lt;/p&gt;
    &lt;p&gt;It is by no means bug-free or even complete; It produces objectively inferior circuits to 'hand-made' redstone designs, and is not intended to be used in modded survival. It can generate almost any verilog circuit, however only simple designs will actually be testable in-game since any moderately-complex design will end up being longer than the maximum number of blocks loaded in Minecraft.&lt;/p&gt;
    &lt;p&gt;Additionally, we are currently unable to synthesize sequential circuits, aka any circuits with a loopback or feedback. That means no memory, no counters or any circuit that could hold a state.&lt;/p&gt;
    &lt;p&gt;MinecraftHDL is an educational tool to illustrate on a macro-scopic scale how microelectronic digital circuits are designed and produced. It is a great way to introduce younger audiences to the world of digital design and can also be used to illustrate the difference between software and hardware design to undergraduate engineers taking their first RTL class.&lt;/p&gt;
    &lt;p&gt;Supervisor: Brett H. Meyer - Website&lt;lb/&gt; Students: Francis O'Brien - Website&lt;lb/&gt; Omar Ba Mashmos&lt;lb/&gt; Andrew Penhale&lt;/p&gt;
    &lt;p&gt;To show how easy it is to make a circuit with MinecraftHDL here is a gif of me creating a circuit, synthesizing, and generating it in minecraft in less than a minute!&lt;/p&gt;
    &lt;p&gt;The circuit I generate above is a 2bit adder. It takes two numbers of two bits and adds them. At the end of the gif I set both input numbers to '11' which is the binary representation of the number 3. Then I move to the output and we see that O3=1, O2=1, and O1=0, this gives the binary number '110' which is indeed 6.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45763877</guid><pubDate>Thu, 30 Oct 2025 18:59:02 +0000</pubDate></item></channel></rss>