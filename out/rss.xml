<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 29 Oct 2025 19:32:15 +0000</lastBuildDate><item><title>I made a 10¢ MCU Talk</title><link>https://www.atomic14.com/2025/10/29/CH32V003-talking</link><description>&lt;doc fingerprint="a50cd3b339220607"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;TLDR: Yes, you can fit about 7 seconds of audio into 16K of flash and still have room for code. And you can even play LPC encoded audio on a 10 cent MCU.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There’s quite a lot more detail in this video (and of course you can hear the audio!).&lt;/p&gt;
    &lt;p&gt;In the previous project, I had this ultra-cheap CH32V003 microcontroller playing simple tunes on a tiny SMD buzzer. It was just toggling a GPIO pin at musical note frequencies – 1-bit audio output – and it sounded surprisingly decent. That was a fun start, but now it’s time to push this little $0.10 MCU even further: can we make it actually talk?&lt;/p&gt;
    &lt;p&gt;Spoiler: Yes, we can! (well, there wouldn’t be much of a blog post if we couldn’t) This 8-pin RISC-V chip is now producing sampled audio data and spoken words. We’re really stretching the limits of what you can fit in 16 KB of flash.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Beeps to Actual Audio&lt;/head&gt;
    &lt;p&gt;Moving from simple beeps to real audio meant using the microcontroller’s PWM output as a rudimentary DAC. Instead of just on/off beeping, I’m driving a waveform at an 8 kHz sample rate using a high-frequency PWM on the output pin. The hardware is the same tiny board as before – but I’ve swapped the small SMD buzzer for a small speaker. The buzer works too, but it’s quieter and very tinny.&lt;/p&gt;
    &lt;p&gt;The sample I wanted to test with is just over 6 seconds in length - it’s the iconic “Open the pod bay doors HAL…” sequence from 2001.&lt;/p&gt;
    &lt;p&gt;If we keep this audio at 16-bit PCM, 8kHZ, we’d need about 96KB – way beyond our 16 KB flash! And remember, that 16 KB has to hold both the audio data and our playback code. Clearly some aggressive compression is required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Bits/Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Fits in 16KB?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CD Quality&lt;/cell&gt;
        &lt;cell&gt;44.1 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;529 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 33× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Phone Quality&lt;/cell&gt;
        &lt;cell&gt;16 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;192 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 12× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Basic PCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;8-bit&lt;/cell&gt;
        &lt;cell&gt;48 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 3× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;4-bit ADPCM (IMA)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;4-bit&lt;/cell&gt;
        &lt;cell&gt;24 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 1.5× too big&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;QOA (Quite OK Audio)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;3.2-bit&lt;/cell&gt;
        &lt;cell&gt;19 KB&lt;/cell&gt;
        &lt;cell&gt;❌ Still too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2-bit ADPCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;2-bit&lt;/cell&gt;
        &lt;cell&gt;12 KB&lt;/cell&gt;
        &lt;cell&gt;✅ Fits!&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I considered a few encoding options for compressing the audio.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8-bit PCM: Simply using 8-bit samples at 8 kHz cuts size in half (to ~47 KB for 6s), but that’s still about 3× too large for our flash.&lt;/item&gt;
      &lt;item&gt;4-bit ADPCM: Adaptive Differential PCM is a simple lossy compression that could quarter the size. In theory 6 seconds would be ~24 KB – much closer to fitting,&lt;/item&gt;
      &lt;item&gt;“Quite OK Audio” (QOA): This is nice codec that packs audio into about 3.2 bits per sample (roughly 1/5 the size of 16-bit PCM)&lt;/item&gt;
      &lt;item&gt;2-bit ADPCM: Going even further with ADPCM, using only 2 bits per sample gives a 4:1 compression relative to 8-bit audio – that’s 75% storage savings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2-bit ADPCM is definitely the winner here. Our 6-second clip shrinks to under 12 KB, which comfortably fits in flash with room for code. This looked like the winner, provided the audio quality was acceptable. The decoder for 2-bit ADPCM is also very lightweight (my implementation compiled to under just over 1K of code - 1340 bytes!). It’s definitely low quality - but it actually sounds surprisingly ok.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does 2-bit ADPCM work?&lt;/head&gt;
    &lt;p&gt;It’s actually a very simple algorithm. Both the encoder and decoder maintain a predicted signal value and a step size index into a predefined table. Each 2-bit code tells the decoder how to adjust the current prediction and the step size index. In essence, we’re coding the difference between the real audio and our prediction, with only four possible levels (since 2 bits gives 4 values). After each sample, the algorithm adapts: if the prediction error was large, we move to a bigger step size (to allow larger changes); if the error was small, we use a smaller step size for finer resolution. This adaptive step is what makes it ADPCM (Adaptive Differential PCM).&lt;/p&gt;
    &lt;p&gt;Our codes are as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;00&lt;/code&gt;(0): Go down by 1 step - subtract the step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01&lt;/code&gt;(1): Go up by 1 step - add the step size to our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;10&lt;/code&gt;(2): Go down by 2 steps - subtract the 2 x step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;11&lt;/code&gt;(3): Go up by 2 steps - add the 2 x step size to our current prediction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with this very high level of compression, the predicted waveform manages to track the original audio surprisingly well. The above graph shows a small snippet of the audio: the blue line is the original waveform and the yellow line is the ADPCM decoder’s output.&lt;/p&gt;
    &lt;p&gt;They’re not identical (and we wouldn’t expect them to be), but the general shape is preserved. When you play it back through the little speaker, it’s recognizable and surprisingly good.&lt;/p&gt;
    &lt;p&gt;To make my life easier, I built a quick conversion tool to encode WAV files into this 2-bit ADPCM format. The tool lets me drag-and-drop a WAV, and it gives you the files with the data that can ve dropped into the firmware code.&lt;/p&gt;
    &lt;head rend="h2"&gt;LPC Speech Synthesis&lt;/head&gt;
    &lt;p&gt;Six seconds of audio is cool, but what about longer phrases or even arbitrary speech? Storing anything much longer with raw or ADPCM audio would quickly fill the 16K of flash.&lt;/p&gt;
    &lt;p&gt;For my second experiment, I tried something different: instead of recorded waveform audio, I used an old-school speech synthesis approach. This leverages the fact that spoken language can be encoded very compactly by modeling the human voice, rather than storing the raw sound. Specifically, I integrated a library called Talkie.&lt;/p&gt;
    &lt;p&gt;Talkie is a software implementation of the Texas Instruments LPC speech synthesis architecture from the late 1970s. This was implemented in a variety of chips, most commonly the TMS5220 and TMS5100 speech chips.&lt;/p&gt;
    &lt;p&gt;These were used in things like the original Speak &amp;amp; Spell, arcade games like early Star Wars, and speech add-ons for home computers (e.g. the BBC Micro).&lt;/p&gt;
    &lt;p&gt;The Talkie library (originally by Peter Knight, later added to by Adafruit) comes with a big set of examples and vocabulary. It’s also possible to extract examples from old ROMs from arcade games.&lt;/p&gt;
    &lt;p&gt;Each phrase or word only takes a few hundred bytes or even less, so you can fit quite a lot of speech into a few kilobytes of flash. The trade-off is that the voice has a very computer-esque timbre – think of the Speak &amp;amp; Spell’s voice. It’s clearly synthetic, but still understandable.&lt;/p&gt;
    &lt;p&gt;To say custom sentences not in the library, you either concatenate the available words/phonemes (which can be clunky), or you need to generate new LPC data. The original tools for this are a bit obscure – there’s BlueWizard (a classic Mac app) and PythonWizard (a command-line tool with TK GUI) which can analyze WAV files and produce LPC data.&lt;/p&gt;
    &lt;p&gt;I gave both a try with some success (and a few headaches setting them up). In the end, I cheated a bit and used an AI coding assistant to help me create a streamlined online tool for this.&lt;/p&gt;
    &lt;p&gt;The result is a little web app where I can upload a recording of, say, my own voice, and it outputs the LPC data. It even lets me play back the synthesized voice in-browser to check it.&lt;/p&gt;
    &lt;p&gt;So there we have it – our 10¢ microcontroller now has a voice! By using 2-bit ADPCM compression, we can store short audio clips (up to around 8 seconds) even in 16 KB of flash, and play them back via PWM with decent fidelity.&lt;/p&gt;
    &lt;p&gt;And with the Talkie LPC speech synthesis, we can make the device “speak” lots of words and phrases with only a tiny memory footprint.&lt;/p&gt;
    &lt;p&gt;If you want to hear it for yourself, check out the video demo linked at the top of this post. In the video, you’ll see (and hear) the WarGames clip and the Star Wars quotes running on the hardware. It’s honestly amazing what these cheap little MCUs can do. We’re really pushing the boundaries of cheap hardware here.&lt;/p&gt;
    &lt;p&gt;You can find all my code on GitHub in this repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45747112</guid><pubDate>Wed, 29 Oct 2025 14:12:50 +0000</pubDate></item><item><title>Collins Aerospace: Sending text messages to the cockpit with test:test</title><link>https://www.ccc.de/en/disclosure/collins-aerospace-mit-test-test-textnachrichten-bis-ins-cockpit-senden</link><description>&lt;doc fingerprint="ef98aafde223bee7"&gt;
  &lt;main&gt;
    &lt;p&gt;Informed parties: RTX Corporation and Department of Defense Cyber Crime Center (on September 21, 2025)&lt;/p&gt;
    &lt;p&gt;Using the credentials test:test, it was possible to log in at the ARINC OpCenter Message Browser (Screenshot) as U.S. Navy Fleet Logistics Support Wing.&lt;/p&gt;
    &lt;p&gt;Via this web service, text messages can be sent to the cockpit. For obvious reasons, we did not try that. Sent messages could be viewed.&lt;/p&gt;
    &lt;p&gt;Unfortunately, RTX did not respond to our vulnerability report. The account was disabled.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45747804</guid><pubDate>Wed, 29 Oct 2025 15:07:20 +0000</pubDate></item><item><title>Hosting SQLite Databases on GitHub Pages (2021)</title><link>https://phiresky.github.io/blog/2021/hosting-sqlite-databases-on-github-pages/</link><description>&lt;doc fingerprint="3d0f8f5e6fa86b83"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Hosting SQLite databases on Github Pages&lt;/head&gt;&lt;p&gt;(or IPFS or any static file hoster)&lt;/p&gt;• Last Update&lt;p&gt;I was writing a tiny website to display statistics of how much sponsored content a Youtube creator has over time when I noticed that I often write a small tool as a website that queries some data from a database and then displays it in a graph, a table, or similar. But if you want to use a database, you either need to write a backend (which you then need to host and maintain forever) or download the whole dataset into the browser (which is not so great when the dataset is more than 10MB).&lt;/p&gt;&lt;p&gt;In the past when I’ve used a backend server for these small side projects at some point some external API goes down or a key expires or I forget about the backend and stop paying for whatever VPS it was on. Then when I revisit it years later, I’m annoyed that it’s gone and curse myself for relying on an external service - or on myself caring over a longer period of time.&lt;/p&gt;&lt;p&gt;Hosting a static website is much easier than a "real" server - there’s many free and reliable options (like GitHub, GitLab Pages, Netlify, etc), and it scales to basically infinity without any effort.&lt;/p&gt;&lt;p&gt;So I wrote a tool to be able to use a real SQL database in a statically hosted website!&lt;/p&gt;&lt;p&gt;Here’s a demo using the World Development Indicators dataset - a dataset with 6 tables and over 8 million rows (670 MiByte total).&lt;/p&gt;&lt;code&gt;select country_code, long_name from wdi_country limit 3;&lt;/code&gt;&lt;p&gt;As you can see, we can query the &lt;code&gt;wdi_country&lt;/code&gt; table while fetching only 1kB of data!&lt;/p&gt;&lt;p&gt;This is a full SQLite query engine. As such, we can use e.g. the SQLite JSON functions:&lt;/p&gt;&lt;code&gt;select json_extract(arr.value, '$.foo.bar') as bar
  from json_each('[{"foo": {"bar": 123}}, {"foo": {"bar": "baz"}}]') as arr&lt;/code&gt;&lt;p&gt;We can also register JS functions so they can be called from within a query. Here’s an example with a &lt;code&gt;getFlag&lt;/code&gt; function that gets the flag emoji for a country:&lt;/p&gt;&lt;code&gt;function getFlag(country_code) {
  // just some unicode magic
  return String.fromCodePoint(...Array.from(country_code||"")
    .map(c =&amp;gt; 127397 + c.codePointAt()));
}

await db.create_function("get_flag", getFlag)
return await db.query(`
  select long_name, get_flag("2-alpha_code") as flag from wdi_country
    where region is not null and currency_unit = 'Euro';
`)&lt;/code&gt;&lt;p&gt;Press the Run button to run the following demos. You can change the code in any way you like, though if you make a query too broad it will fetch large amounts of data ;)&lt;/p&gt;&lt;p&gt;Note that this website is 100% hosted on a static file hoster (GitHub Pages).&lt;/p&gt;&lt;p&gt;So how do you use a database on a static file hoster? Firstly, SQLite (written in C) is compiled to WebAssembly. SQLite can be compiled with emscripten without any modifications, and the sql.js library is a thin JS wrapper around the wasm code.&lt;/p&gt;&lt;p&gt;sql.js only allows you to create and read from databases that are fully in memory though - so I implemented a virtual file system that fetches chunks of the database with HTTP Range requests when SQLite tries to read from the filesystem: sql.js-httpvfs. From SQLite’s perspective, it just looks like it’s living on a normal computer with an empty filesystem except for a file called &lt;code&gt;/wdi.sqlite3&lt;/code&gt; that it can read from. Of course it can’t write to this file, but a read-only database is still very useful.&lt;/p&gt;&lt;p&gt;Since fetching data via HTTP has a pretty large overhead, we need to fetch data in chunks and find some balance between the number of requests and the used bandwidth. Thankfully, SQLite already organizes its database in "pages" with a user-defined page size (4 KiB by default). I’ve set the page size to 1 KiB for this database.&lt;/p&gt;&lt;p&gt;Here’s an example of a simple index lookup query:&lt;/p&gt;&lt;code&gt;select indicator_code, long_definition from wdi_series where indicator_name
    = 'Literacy rate, youth total (% of people ages 15-24)'&lt;/code&gt;&lt;p&gt;Run the above query and look at the page read log. SQLite does 7 page reads for that query.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Three page reads are just some to get some schema information (these are already cached)&lt;/item&gt;&lt;item&gt;Two page reads are the index lookup in the index &lt;code&gt;on wdi_series (indicator_name)&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Two page reads are on the &lt;code&gt;wdi_series&lt;/code&gt;table data (the first to find the row value by primary key, the second to get the text data from an overflow page)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Both the index as well as the table reads are B-Tree lookups.&lt;/p&gt;&lt;p&gt;A more complex query: What are the countries with the lowest youth literacy rate, based on the newest data from after 2010?&lt;/p&gt;&lt;code&gt;with newest_datapoints as (
  select country_code, indicator_code, max(year) as year from wdi_data
  join wdi_series using (indicator_code)
  where
    indicator_name = 'Literacy rate, youth total (% of people ages 15-24)'
    and year &amp;gt; 2010
  group by country_code
)
select c.short_name as country, printf('%.1f %%', value) as "Youth Literacy Rate"
from wdi_data
  join wdi_country c using (country_code)
  join newest_datapoints using (indicator_code, country_code, year)
order by value asc limit 10&lt;/code&gt;&lt;p&gt;The above query should do 10-20 GET requests, fetching a total of 130 - 270KiB, depending on if you ran the above demos as well. Note that it only has to do 20 requests and not 270 (as would be expected when fetching 270 KiB with 1 KiB at a time). That’s because I implemented a pre-fetching system that tries to detect access patterns through three separate virtual read heads and exponentially increases the request size for sequential reads. This means that index scans or table scans reading more than a few KiB of data will only cause a number of requests that is logarithmic in the total byte length of the scan. You can see the effect of this by looking at the "Access pattern" column in the page read log above.&lt;/p&gt;&lt;p&gt;All of this only works well when we have indices in the database that match the queries well. For example, the index used in the above query is a &lt;code&gt;INDEX ON wdi_data (indicator_code, country_code, year, value)&lt;/code&gt;. If that index did not include the value column, the SQLite engine would have to do another random access (unpredictable) read and thus HTTP request to retrieve the actual value for every data point. If the index was ordered &lt;code&gt;country_code, indicator_code, ...&lt;/code&gt;, then we would be able to quickly get all indicators for a single country, but not all country values of a single indicator.&lt;/p&gt;&lt;p&gt;We can also make use of the SQLite FTS module so we can do a full-text search on the more text-heavy information in the database - in this case there are over 1000 human development indicators in the database with longer descriptions.&lt;/p&gt;&lt;code&gt;select * from indicator_search
where indicator_search match 'educatio* femal*'
order by rank limit 10&lt;/code&gt;&lt;p&gt;The total amount of data in the &lt;code&gt;indicator_search&lt;/code&gt; FTS table is around 8 MByte. The above query should only fetch around 70 KiB. You can see how it is constructed here.&lt;/p&gt;&lt;p&gt;And finally, here’s a more complete demonstration of the usefulness of this system - here’s an interactive graph showing the development of a few countries over time, for any countries you want using any indicator from the dataset:&lt;/p&gt;&lt;head&gt;Extra information about this indicator&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Indicator Code&lt;/item&gt;&lt;item rend="dd-1"&gt;IT.NET.USER.ZS&lt;/item&gt;&lt;item rend="dt-2"&gt;Long definition&lt;/item&gt;&lt;item rend="dd-2"&gt;Internet users are individuals who have used the Internet (from any location) in the last 3 months. The Internet can be used via a computer, mobile phone, personal digital assistant, games machine, digital TV etc.&lt;/item&gt;&lt;item rend="dt-3"&gt;Statistical concept and methodology&lt;/item&gt;&lt;item rend="dd-3"&gt;The Internet is a world-wide public computer network. It provides access to a number of communication services including the World Wide Web and carries email, news, entertainment and data files, irrespective of the device used (not assumed to be only via a computer - it may also be by mobile phone, PDA, games machine, digital TV etc.). Access can be via a fixed or mobile network. For additional/latest information on sources and country notes, please also refer to: https://www.itu.int/en/ITU-D/Statistics/Pages/stat/default.aspx&lt;/item&gt;&lt;item rend="dt-4"&gt;Development relevance&lt;/item&gt;&lt;item rend="dd-4"&gt;The digital and information revolution has changed the way the world learns, communicates, does business, and treats illnesses. New information and communications technologies (ICT) offer vast opportunities for progress in all walks of life in all countries - opportunities for economic growth, improved health, better service delivery, learning through distance education, and social and cultural advances. Today's smartphones and tablets have computer power equivalent to that of yesterday's computers and provide a similar range of functions. Device convergence is thus rendering the conventional definition obsolete. Comparable statistics on access, use, quality, and affordability of ICT are needed to formulate growth-enabling policies for the sector and to monitor and evaluate the sector's impact on development. Although basic access data are available for many countries, in most developing countries little is known about who uses ICT; what they are used for (school, work, business, research, government); and how they affect people and businesses. The global Partnership on Measuring ICT for Development is helping to set standards, harmonize information and communications technology statistics, and build statistical capacity in developing countries. However, despite significant improvements in the developing world, the gap between the ICT haves and have-nots remains.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note that many indicators are only available for some countries, for example the indicator "Women who believe a husband is justified in beating his wife when she burns the food" is based on surveys only conducted in lower-developed countries.&lt;/p&gt;&lt;head rend="h2"&gt;Bonus: DOM as a database&lt;/head&gt;&lt;p&gt;Since we’re already running a database in our browser, why not use our browser as a database using a virtual table called &lt;code&gt;dom&lt;/code&gt;?&lt;/p&gt;&lt;code&gt;select count(*) as number_of_demos from dom
  where selector match '.content div.sqlite-httpvfs-demo';
select count(*) as sqlite_mentions from dom
  where selector match '.content p' and textContent like '%SQLite%';&lt;/code&gt;&lt;p&gt;We can even insert elements directly into the DOM:&lt;/p&gt;&lt;code&gt;insert into dom (parent, tagName, textContent)
    select 'ul#outtable1', 'li', short_name
    from wdi_country where currency_unit = 'Euro'&lt;/code&gt;&lt;p&gt;Output:&lt;/p&gt;&lt;p&gt;And update elements in the DOM:&lt;/p&gt;&lt;code&gt;update dom set textContent =
  get_flag("2-alpha_code") || ' ' || textContent
from wdi_country
where selector match 'ul#outtable1 &amp;gt; li'
  and textContent = wdi_country.short_name&lt;/code&gt;&lt;p&gt;Of course, everything here is open source. The main implementation of the sqlite wrapper is in sql.js-httpvfs. The source code of this blog post is a pandoc markdown file, with the demos being a custom "fenced code block" React component.&lt;/p&gt;&lt;head rend="h2"&gt;Update 2023: Future Work&lt;/head&gt;&lt;p&gt;Since I wrote this article in 2021, a lot of interesting things have happened in this space, many inspired by this proof of concept! Here’s some highlights:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;libgen-ipfs: implements a search engine for the Library Genesis based on IPFS and sql.js-httpvfs.&lt;/item&gt;&lt;item&gt;absurd-sql: implements a backend for sql.js (sqlite3 compiled for the web) that treats IndexedDB like a disk and stores data in blocks there. Inspired by this article.&lt;/item&gt;&lt;item&gt;sqlite itself has now added official support for wasm, citing absurd-sql as related work!&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748186</guid><pubDate>Wed, 29 Oct 2025 15:32:11 +0000</pubDate></item><item><title>The end of the rip-off economy: consumers use LLMs against information asymmetry</title><link>https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748195</guid><pubDate>Wed, 29 Oct 2025 15:32:46 +0000</pubDate></item><item><title>Tell HN: Azure outage</title><link>https://news.ycombinator.com/item?id=45748661</link><description>&lt;doc fingerprint="767891114d0227d5"&gt;
  &lt;main&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.&lt;/p&gt;
    &lt;p&gt;We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Update: 16:35 UTC:&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. We suspect that an inadvertent configuration change as the trigger event for this issue. We are taking two concurrent actions where we are blocking all changes to the AFD services and at the same time rolling back to our last known good state.&lt;/p&gt;
    &lt;p&gt;We have failed the portal away from Azure Front Door (AFD) to mitigate the portal access issues. Customers should be able to access the Azure management portal directly.&lt;/p&gt;
    &lt;p&gt;We do not have an ETA for when the rollback will be completed, but we will update this communication within 30 minutes or when we have an update.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 17:17 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;"We have initiated the deployment of our 'last known good' configuration. This is expected to be fully deployed in about 30 minutes from which point customers will start to see initial signs of recovery. Once this is completed, the next stage is to start to recover nodes while we route traffic through these healthy nodes."&lt;/p&gt;
    &lt;p&gt;"This message was last updated at 18:11 UTC on 29 October 2025"&lt;/p&gt;
    &lt;p&gt;This is the single most frustrating thing about these incidents. As you're harmstrung on what you can do or how you can react until Microsoft officially acknowledges a problem. Took nearly 90mins both today and when it happened on 9th October.&lt;/p&gt;
    &lt;p&gt;in many cases: no service health alerts, no status page updates and no confirmations from the support team in tickets. still we can confirm these issues from different customers accross europe. Mostly the issues are regional dependent.&lt;/p&gt;
    &lt;p&gt;It's pretty unlikely. AWS published a public 'RCA' https://aws.amazon.com/message/101925/. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.&lt;/p&gt;
    &lt;p&gt;I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".&lt;/p&gt;
    &lt;p&gt;That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.&lt;/p&gt;
    &lt;p&gt;THIS is the real deal. Some say it's always DNS but many times it's some routing fuckup with BGP. two most cursed 3 letter acronym technologies out there&lt;/p&gt;
    &lt;p&gt;yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.&lt;/p&gt;
    &lt;p&gt;Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D&lt;/p&gt;
    &lt;p&gt;It's a Sev 0 actually. I was on the engineering bridge call earlier for a bit. The Azure service I work on was minimally impacted (a dashboard instead of the APIs or data layer) but we found a workaround.&lt;/p&gt;
    &lt;p&gt;Whilst the status message acknowledge's the issue with Front Door (AFD), it seems as though the rest of the actions are about how to get Portal/internal services working without relying on AFD. For those of us using Front Door does that mean we're in for a long haul?&lt;/p&gt;
    &lt;p&gt;They briefly had a statement about using Traffic Manager to work with your AFD to work around this issue, with a link to learn.microsoft.com/...traffic-manager, and the link didn't work. Due to the same issue affecting everyone right now.&lt;/p&gt;
    &lt;p&gt;They quickly updated the message to REMOVE the link. Comical at this point.&lt;/p&gt;
    &lt;p&gt;We already had to do it for large files served from Blob Storage since they would cap out at 2MB/s when not in cache of the nearest PoP. If you’ve ever experienced slow Windows Store or Xbox downloads it’s probably the same problem.&lt;/p&gt;
    &lt;p&gt;I had a support ticket open for months about this and in the end the agent said “this is to be expected and we don’t plan on doing anything about it”.&lt;/p&gt;
    &lt;p&gt;We’ve moved to Cloudflare and not only is the performance great, but it costs less.&lt;/p&gt;
    &lt;p&gt;Only thing I need to move off Front Door is a static website for our docs served from Blob Storage, this incident will make us do it sooner rather than later.&lt;/p&gt;
    &lt;p&gt;we are considering the same but because our website uses APEX domain we would need to move all DNS resolver to cloudfront right ? Does it have as a nice "rule set builder" as azure ?&lt;/p&gt;
    &lt;p&gt;Unless you pay for CloudFlare’s Enterpise plan, you’re required to have them host your DNS zone, you can use a different registrar as long as you just point your NS records to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Be aware that if you’re using Azure as your registrar, it’s (probably still) impossible to change your NS records to point to CloudFlare’s DNS server, at least it was for me about 6 months ago.&lt;/p&gt;
    &lt;p&gt;This also makes it impossible to transfer your domain to them either, as CloudFlare’s domain transfer flow requires you set your NS records to point to them before their interface shows a transfer option.&lt;/p&gt;
    &lt;p&gt;In our case we had to transfer to a different registrar, we used Namecheap.&lt;/p&gt;
    &lt;p&gt;However, transferring a domain from Azure was also a nightmare. Their UI doesn’t have any kind of transfer option, I eventually found an obscure document (not on their Learn website) which had an az command which would let you get a transfer code which I could give to Namecheap.&lt;/p&gt;
    &lt;p&gt;Then I had to wait over a week for the transfer timeout to occur because there is no way on Azure side that I could find to accept the transfer immediately.&lt;/p&gt;
    &lt;p&gt;I found CloudFlare’s way of building rules quite easy to use, different from Front Door but I’m not doing anything more complex than some redirects and reverse proxying.&lt;/p&gt;
    &lt;p&gt;I will say that Cloudflare’s UI is super fast, with Front Door I always found it painfully slow when trying to do any kind of configuration.&lt;/p&gt;
    &lt;p&gt;Cloudflare also doesn’t have the problem that Front Door has where it requires a manual process every 6 months or so to renew the APEX certificate.&lt;/p&gt;
    &lt;p&gt;Thanks :). We don't use Azure as our registrar. It seems I'll have to plan for this then, we also had another issue, AFD has a hard 500ms tls handshake timeout (doesn't matter how much you put on the origin timeout settings) which means if our server was slow for some reason we would get 504 origin timeout.&lt;/p&gt;
    &lt;p&gt;I noticed that Starbucks mobile ordering was down and thought “welp, I guess I’ll order a bagel and coffee on Grubhub”, then GrubHub was down. My next stop was HN to find the common denominator, and y’all did not disappoint.&lt;/p&gt;
    &lt;p&gt;For some reason an Azure outage does not faze me in the same way that an AWS outage does.&lt;/p&gt;
    &lt;p&gt;I have never had much confidence in Azure as a cloud provider. The vertical integration of all the things for a Microsoft shop was initially very compelling. I was ready to fight that battle. But, this fantasy was quickly ruined by poor execution on Microsoft's part. They were able to convince me to move back to AWS by simply making it difficult to provision compute resources. Their quota system &amp;amp; availability issues are a nightmare to deal with compared to EC2.&lt;/p&gt;
    &lt;p&gt;At this point I'd rather use GCP over Azure and I have zero seconds of experience with it. The number of things Microsoft gets right in 2025 can be counted single-handedly. The things they do get right are quite good, but everything else tends to be extremely awful.&lt;/p&gt;
    &lt;p&gt;The "Blades" experience [0] where instead of navigating between pages it just kept opening things to the side and expanding horizontally?&lt;/p&gt;
    &lt;p&gt;Yeah, that had some fun ideas but was way more confusing than it needed to be. But also that was quite a few years back now. The Portal ditched that experience relatively quickly. Just long enough to leave a lot of awful first impressions, but not long enough for it to be much more than a distant memory at this point, several redesigns later.&lt;/p&gt;
    &lt;p&gt;[0] The name "Blades" for that came from the early years of the Xbox 360, maybe not the best UX to emulate for a complex control panel/portal.&lt;/p&gt;
    &lt;p&gt;Azure to me has always suffered from a belief that “UI innovations can solve UX complexity if you just try hard enough.”&lt;/p&gt;
    &lt;p&gt;Like, AWS, and GCP to a lesser extent, has a principled approach where simple click-ops goals are simple. You can access the richer metadata/IAM object model at any time, but the wizards you see are dumb enough to make easy things easy.&lt;/p&gt;
    &lt;p&gt;With Azure, those blades allow tremendously complex “you need to build an X Container and a Container Bucket to be able to add an X” flows to coexist on the same page. While this exposes the true complexity, and looks cool/works well for power users, it is exceedingly unintuitive. Inline documentation doesn’t solve this problem.&lt;/p&gt;
    &lt;p&gt;I sometimes wonder if this is by design: like QuickBooks, there’s an entire economy of consultants who need to be Certified and thus will promote your product for their own benefit! Making the interface friendly to them and daunting to mere mortals is a feature, not a bug.&lt;/p&gt;
    &lt;p&gt;But in Azure’s case it’s hard to tell how much this is intentional.&lt;/p&gt;
    &lt;p&gt;AWS' UI is similarly messy, and to this day. They regularly remove useful data from the UI, or change stuff like the default sort order of database snapshots from last created to initial instance created date.&lt;/p&gt;
    &lt;p&gt;I never understood why a clear and consistent UI and improved UX isn't more of a priority for the big three cloud providers. Even though you talk mostly via platform SDK's, I would consider better UI especially initially, a good way to bind new customers and pick your platform over others.&lt;/p&gt;
    &lt;p&gt;I guess with their bottom line they don't need it (or cynically, you don't want to learn and invest in another cloud if you did it once).&lt;/p&gt;
    &lt;p&gt;It’s more than just the UI itself (which is horrible), it’s the whole thing that is very hostile to new users. It’s such a mess. If you don’t know exactly what you’re looking for, good luck finding it. It’s like they’re deliberately trying to many things as confusing as possible. For some reason this applies to all AWS, GCP and Azure.&lt;/p&gt;
    &lt;p&gt;What Amazon, Azure, and Google are showing with their platform crashes amid layoffs, while they supports governments that are Oppressing's Citizens and Ignoring the Law, is that they do not care about anything other than the bottom line.&lt;/p&gt;
    &lt;p&gt;They think they have the market captured, but I think what their dwindling quality and ethics are really going to drive is adoption of self hosting, distributed computing frameworks. Nerds are the ones who drove adoption of these platforms, and we can eventually end if we put in the work.&lt;/p&gt;
    &lt;p&gt;Seriously with container technology, and a bit more work / adoption on distributed compute systems and file storage (IPFS,FileCoin) there is a future where we dont have to use big brothers compute platform. Fuck these guys.&lt;/p&gt;
    &lt;p&gt;The problem is that in some industries, Microsoft is the only option. Many of these regulated industries are just now transitioning from the data center to the cloud, and they've barely managed to get approval for that with all of the Microsoft history in their organization. AWS or GCloud are complete non-starters.&lt;/p&gt;
    &lt;p&gt;I moved a 100% MS shop to AWS circa 2015. We ran our DCs on EC2 instances just as if they were on prem. At some point we installed the AAD connector and bridged some stuff to Azure for office/mail/etc., but it was all effectively in AWS. We were selling software to banks so we had a lot of due diligence to suffer. AWS Artifact did much of the heavy lifting for us. We started with Amazon's compliance documentation and provided our own feedback on top where needed.&lt;/p&gt;
    &lt;p&gt;I feel like compliance is the entire point of using these cloud providers. You get a huge head start. Maintaining something like PCI-DSS when you own the real estate is a much bigger headache than if it's hosted in a provider who is already compliant up through the physical/hardware/networking layers. Getting application-layer checkboxes ticked off is trivial compared to "oops we forgot to hire an armed security team". I just took a look and there are currently 316 certifications and attestations listed under my account.&lt;/p&gt;
    &lt;p&gt;I've found that lift and shifting to EC2 is also generally cheaper than the equivalent VMs on Azure.&lt;/p&gt;
    &lt;p&gt;Microsoft really wants you to use their PaaS offerings, and so things on Azure are priced accordingly. A Microsoft shop just wanting to lift-and-shift, Azure isn't the best choice unless the org has that "nobody ever got fired for buying Microsoft" attitude.&lt;/p&gt;
    &lt;p&gt;At least some bits of it do. I was writing something to pull logs the other day and the redirect was to an azure bucket. It also returned a 401 with the valid temporary authed redirect in the header. I was a bit worried I'd found a massive security hole but it appears after some testing it just returned the wrong status code.&lt;/p&gt;
    &lt;p&gt;We’re 100% on Azure but so far there’s no impact for us.&lt;/p&gt;
    &lt;p&gt;Luckily, we moved off Azure Front Door about a year ago. We’d had three major incidents tied to Front Door and stopped treating it as a reliable CDN.&lt;/p&gt;
    &lt;p&gt;They weren’t global outages, more like issues triggered by new deployments. In one case, our homepage suddenly showed a huge Microsoft banner about a “post-quantum encryption algorithm” or something along those lines.&lt;/p&gt;
    &lt;p&gt;Kinda wild that a company that big can be so shaky on a CDN, which should be rock solid.&lt;/p&gt;
    &lt;p&gt;There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.&lt;/p&gt;
    &lt;p&gt;2-3%, bit higher on perishables. Though i'd just ask lump sum payments in cash since it likely has to no go through corporate (as in, avoid the corporation).&lt;/p&gt;
    &lt;p&gt;You'd think any SeriousBusiness would have a backup way to take customers' money. This is the one thing you always want to be able to do: accept payment. If they made it so they can't do that, they deserve the hit to their revenue. People should just walk out of the store with the goods if they're not being charged.&lt;/p&gt;
    &lt;p&gt;Why doesn't someone in the store at least have one of those manual kachunk-kachunk carbon copy card readers in the back that they can resuscitate for a few days until the technology is turned back on? Did they throw them all away?&lt;/p&gt;
    &lt;p&gt;If they used standalone merchant terminals, then those typically use the local LAN which can rollover to cellular or PoT in the event of a network outage. The store can process a card transaction with the merchant terminal and then reconcile with the end of day chit. This article from 2008 describes their PoS https://www.retailtouchpoints.com/topics/store-operations/ca...&lt;/p&gt;
    &lt;p&gt;I think a lot of payment terminals have an option to record transactions offline and upload them later, but apparently it's not enabled by default - probably because it increases your risk that someone pays with a bad card.&lt;/p&gt;
    &lt;p&gt;The kachunk-kachunk credit card machines need raised digits on the cards, and I don't think most banks have been issuing those for years at this point. Mine have been smooth for at least 10 years.&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.&lt;/p&gt;
    &lt;p&gt;Yea, good old store and forward. We implemented it in our PoS system. Now, we do non PCI integrations so we arn't in PCI scope, but depending on the processor, it can come with some limitations. Like, you can do store and forward, but only up to X number of transactions. I think for one integration, it's 500-ish store wide (it uses a local gateway that store and forwards to the processors gateway). The other integration we have, its 250, but store and forward on device, per device.&lt;/p&gt;
    &lt;p&gt;I remember that banks will try to honor the transactions, even if the customer's balance/credit limit is exhausted. It doesn't apply only to some gift cards.&lt;/p&gt;
    &lt;p&gt;You can, but it's all about risk mitigation. Most processors have some form of store and forward (and it can have limitations like only X number of transactions). Some even have controls to limit the amount you can store-and-forward (for instance, only charges under $50). But ultimately, it's still risk mitigation. You can store-and-forward, but you're trusting that the card/account has the funds. If it doesn't, you loose and ain't shit you can do about it. If you can't tolerate any risk, you don't turn on store and forward systems and then you can't process cards offline.&lt;/p&gt;
    &lt;p&gt;Its not the we are not capable. Its, is the business willing to assume the risk?&lt;/p&gt;
    &lt;p&gt;Personally I am thinking more and more about hetzner, yes I know its not an apples to orange comparison. But its honestly so good&lt;/p&gt;
    &lt;p&gt;Someone had created a video where they showed the underlying hardware etc., I am wondering if there is something like https://vpspricetracker.com/ but with geek-benchmarks as well.&lt;/p&gt;
    &lt;p&gt;This video was affiliated with scalahosting but still I don't think that there was too much bias of them and they showed at around 3:37 a graph comparison with prices https://www.youtube.com/watch?v=9dvuBH2Pc1g&lt;/p&gt;
    &lt;p&gt;Now it shows how contabo has better hardware but I am pretty sure that there might be some other issues, and honestly I feel a sense of trust with hetzner I am not sure about others.&lt;/p&gt;
    &lt;p&gt;Either hetzner or self hosting stuff personally or just having a very cheap vps and going to hetzner if need be but hetzner already is pretty cheap or I might use some free service that I know of are good as well.&lt;/p&gt;
    &lt;p&gt;I’ve been migrating our services off of Azure slowly for the past couple of years. The last internet facing things remaining are a static assets bucket and an analytics VM running Matomo. Working with Front Door has been an abysmal experience, and today was the push I needed to finally migrate our assets to Cloudflare.&lt;/p&gt;
    &lt;p&gt;I feel pretty justified in my previous decisions to move away from Azure. Using it feels like building on quicksand…&lt;/p&gt;
    &lt;p&gt;And querying https://www.microsoft.com/ results in HTTP 200 on the root document, but the page elements return errors (a 504 on the .css/.js documents, a 404 on some fonts, Name Not Resolved on scripts.clarity.ms, Connection Timed Out on wcpstatic.microsoft.com and mem.gfx.ms). That many different kinds of errors is actually kind of impressive.&lt;/p&gt;
    &lt;p&gt;I'm gonna say this was a networking/routing issue. The CDN stayed up, but everything else non-CDN became unroutable, and different requests traveled through different paths/services, but each eventually hit the bad network path, and that's what created all the different responses. Could also have been a bad deploy or a service stopped running and there's different things trying to access that service in different ways, leading to the weird responses... but that wouldn't explain the failed DNS propagation.&lt;/p&gt;
    &lt;p&gt;We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven’t had weekly or even monthly issues. I can think of maybe three issues this year.&lt;/p&gt;
    &lt;p&gt;There's no way to tell, and after about 30 minutes, the release process on VS Code Marketplace failed with a cryptic message: "Repository signing for extension file failed.". And there's no way to restart/resume it.&lt;/p&gt;
    &lt;p&gt;Yeah just took down the prod site for one of our clients since we host the front-end out of their CDN. Just got wrapped up panic hosting it somewhere else for the past hour, very quickly reminds you about the pain of cookies...&lt;/p&gt;
    &lt;p&gt;Seeing users having issues with the "Modern Outlook", specifically empty accounts. Switching back to the "Legacy Outlook" which functions largely without the help of the cloud fixes the issue. How ironic.&lt;/p&gt;
    &lt;p&gt;The paradox of cloud provider crashes is that if the provider goes down and takes the whole world with it, it's actually good advertisement. Because, that means so many things rely on it, it's critically important, and has so many big customers. That might be why Amazon stock went up after AWS crash.&lt;/p&gt;
    &lt;p&gt;If Azure goes down and nobody feels it, does Azure really matter?&lt;/p&gt;
    &lt;p&gt;People feel it, but usually not general consumers like they do when AWS goes down.&lt;/p&gt;
    &lt;p&gt;If Azure goes down, it's mostly affecting internal stuff at big old enterprises. Jane in accounting might notice, but the customers don't. Contrast with AWS which runs most of the world's SaaS products.&lt;/p&gt;
    &lt;p&gt;People not being able to do their jobs internally for a day tends not to make headlines like "100 popular internet services down for everyone" does.&lt;/p&gt;
    &lt;p&gt;I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.&lt;/p&gt;
    &lt;p&gt;They added a message at the same time as your comment:&lt;/p&gt;
    &lt;p&gt;"We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly."&lt;/p&gt;
    &lt;p&gt;Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.&lt;/p&gt;
    &lt;p&gt;Same playbook for AWS. When they admitted that Dynamo was inaccessible, they failed to provide context that their internal services are heavily dependent on Dynamo&lt;/p&gt;
    &lt;p&gt;It's only after the fact they are transparent about the impact&lt;/p&gt;
    &lt;p&gt;The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.&lt;/p&gt;
    &lt;p&gt;How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.&lt;/p&gt;
    &lt;p&gt;A lot of money and years of marketing the cloud as the responsible business decision led us here. Now that the cloud providers have vendor lock-in, few will leave, and customers will continue to wildly overpay for cloud services.&lt;/p&gt;
    &lt;p&gt;Not sure how the current situation is better. Being stranded with no way whatsoever to access most/all of your services sounds way more terrifying than regular issues limited to a couple of services at a time&lt;/p&gt;
    &lt;p&gt;&amp;gt; no way whatsoever to access most/all of your services&lt;/p&gt;
    &lt;p&gt;I work on a product hosted on Azure. That's not the case. Except for front door, everything else is running fine. (Front door is a reverse proxy for static web sites.)&lt;/p&gt;
    &lt;p&gt;The product itself (an iot stormwater management system) is running, but our customers just can't access the website. If they need to do something, they can go out to the sites or call us and we can "rub two sticks together" and bypass the website. (We could also bypass front door if someone twisted our arms.)&lt;/p&gt;
    &lt;p&gt;Most customers only look at the website a few times a year.&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;That being said, our biggest point of failure is a completely different iot vendor who you probably won't hear about on Hacker News when they, or their data networks, have downtime.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Big Tech lobbying is riding the EU’s deregulation wave by spending more, hiring more, and pushing more, according to a new report by NGO’s Corporate Europe Observatory and LobbyControl on Wednesday (29 October).&lt;/p&gt;
    &lt;p&gt;&amp;gt; Based on data from the EU’s transparency register, the NGOs found that tech companies spend the most on lobbying of any sector, spending €151m a year on lobbying — a 33 percent increase from €113m in 2023.&lt;/p&gt;
    &lt;p&gt;Gee whizz, I really do wonder how they end up having all the power!&lt;/p&gt;
    &lt;p&gt;I think the response lies in the surrounding ecosystem.&lt;/p&gt;
    &lt;p&gt;If you have a company it's easier to scale your team if you use AWS (or any other established ecosystem). It's way easier to hire 10 engineers that are competent with AWS tools than it is to hire 10 engineers that are competent with the IBM tools.&lt;/p&gt;
    &lt;p&gt;And from the individuals perspective it also make sense to bet on larger platforms. If you want to increase your odds of getting a new job, learning the AWS tools gives you a better ROI than learning the IBM tools.&lt;/p&gt;
    &lt;p&gt;A natural monopoly is a monopoly in an industry in which high infrastructure costs and other barriers to entry relative to the size of the market give the largest supplier in an industry, often the first supplier in a market, an overwhelming advantage over potential competitors. Specifically, an industry is a natural monopoly if a single firm can supply the entire market at a lower long-run average cost than if multiple firms were to operate within it. In that case, it is very probable that a company (monopoly) or a minimal number of companies (oligopoly) will form, providing all or most of the relevant products and/or services.&lt;/p&gt;
    &lt;p&gt;Consolidation is the inevitable outcome of free unregulated markets.&lt;/p&gt;
    &lt;p&gt;In our highly interconnected world, decentralization paradoxically requires a central authority to enforce decentralization by restricting M&amp;amp;A, cartels, etc.&lt;/p&gt;
    &lt;p&gt;For us, it looks like most services are still working (eastus and eastus2). Our AKS cluster is still running and taking requests. Failures seem limited to management portal.&lt;/p&gt;
    &lt;p&gt;High availability is touted as a reason for their high prices, but I swear I read about major cloud outages far more than I experience any outages at Hetzner.&lt;/p&gt;
    &lt;p&gt;I think the biggest features of the big cloud vendors is that when they are down, not only you but your customers and your competitors usually have issues at the same time so everybody just shrug and have a lazy/off day at the same time. Even on call teams reall just have to wait and stay on standby because there is very little they can do. Doing a failover can be slower than waiting for the recovery, not help at all if outage is spanned accross several region, or bring aditional risks.&lt;/p&gt;
    &lt;p&gt;And more importantly nobody lose any reputation except AWS/Azure/Google.&lt;/p&gt;
    &lt;p&gt;The real reason is that outages are not your fault. Its the new version of "nobody ever got fired for buying IBM" - later it became MS, and now its any big cloud provider.&lt;/p&gt;
    &lt;p&gt;For one it’s statistics - Hetzner simply runs far fewer major services than hyperscalers. And the services they run are also more affluent, with larger customer bases, so downtimes are systemically critical. Therefore it’s louder.&lt;/p&gt;
    &lt;p&gt;On the merits though, I agree, haven’t had any serious issues with Hetzner.&lt;/p&gt;
    &lt;p&gt;DO has been shockingly reliable for me. I shut down a neglected box almost 900 days uptime the other day. In that time AWS has randomly dropped many of my boxes with no warning requiring a manual stop/start action to recover them... But everybody keeps telling me that DO isn't "as reliable" as the big three are.&lt;/p&gt;
    &lt;p&gt;To be fair, in the AWS/Azure outages, I don't think any individual (already created) boxes went down, either. In AWS' case you couldn't start up new EC2 instances, and presumably same for Azure (unless you bypass the management portal, I guess). And obviously services like DynamoDB and Front Door, respectively, went down. Hetzner/DO don't offer those, right? Or at least they're not very popular.&lt;/p&gt;
    &lt;p&gt;Nope, more than the portal. For instance, I just searched for "Azure Front Door" because I hadn't heard of it before (I now know it's a CDN), and neither the product page itself [1] nor the technical docs [2] are coming up for me.&lt;/p&gt;
    &lt;p&gt;we use front door (as does miccrosoft.com) and our website was down, I was able to change the DNS records to point directly to our server and will leave it like that for a few hours until everything is green&lt;/p&gt;
    &lt;p&gt;The sad thing is - $MSFT isn't even down by 1%. And IIRC, $AMZN actually went up during their previous outage.&lt;/p&gt;
    &lt;p&gt;So if we look at these companies' bottom lines, all those big wigs are actually doing something right. Sales and lobbying capacity is way more effective than reliability or good engineering (at least in the short term).&lt;/p&gt;
    &lt;p&gt;I looked into this before and the stocks of these large corps simply does not move when outages happens. Maybe intra-day, I don't have that data, but in general no effect.&lt;/p&gt;
    &lt;p&gt;Do Microsoft still say "If the government has a broader voluntary national security program to gather customer data, we don't participate in it" today (which PRISM proved very false), or are they at least acknowledging they're participating in whatever NSA has deployed today?&lt;/p&gt;
    &lt;p&gt;PRISM wasn't voluntary. Also there are 3 levels here:&lt;/p&gt;
    &lt;p&gt;1. Mandatory&lt;/p&gt;
    &lt;p&gt;2. "Voluntary"&lt;/p&gt;
    &lt;p&gt;3. Voluntary&lt;/p&gt;
    &lt;p&gt;And I suspect that very little of what the NSA does falls into category 3. As Sen Chuck Schumer put it "you take on the intelligence community, they have six ways from Sunday at getting back at you"&lt;/p&gt;
    &lt;p&gt;This is funny but also possibly true because: business/MBA types see these outages as a way to prove how critical some services are, leading to investors deciding to load up on the vendor's stock.&lt;/p&gt;
    &lt;p&gt;I may or may not have been known to temporarily take a database down in the past to make a point to management about how unreliable some old software is.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;----&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;"We’re investigating an issue impacting Azure Front Door services. Customers may experience intermittent request failures or latency. Updates will be provided shortly."&lt;/p&gt;
    &lt;p&gt;Thank you. I was wondering what was going on at a company whose web app I need to access. I just checked with BuiltWith and it seems they are on Azure.&lt;/p&gt;
    &lt;p&gt;I know how to fix this but this community is too close minded and argumentative egocentric sensitive pedantic threatened angry etc to bother discussing it&lt;/p&gt;
    &lt;p&gt;They admit in their update blurb azure front door is having issues but still report azure front door as having no issues on their status page.&lt;/p&gt;
    &lt;p&gt;And it's very clear from these updates that they're more focused on the portal than the product, their updates haven't even mentioned fixing it yet, just moving off of it, as if it's some third party service that's down.&lt;/p&gt;
    &lt;p&gt;Unsubstantiated idea: So the support contract likely says there is a window between each reporting step and the status page is the last one and the one in the legal documents giving them several more hours before the clauses trigger.&lt;/p&gt;
    &lt;p&gt;Portal and Azure CDN are down here in the SF Bay Area. Tenant azureedge.net DNS A queries are taking 2-6 seconds and most often return nothing. I got a couple successful A response in the last 10 minutes.&lt;/p&gt;
    &lt;p&gt;Edit: As of 9:19 AM Pacific time, I'm now getting successful A responses but they can take several seconds. The web server at that address is not responding.&lt;/p&gt;
    &lt;p&gt;On our end, our VMs are still working, so our gitlab instance is still up. Our services using Azure App Services are available through their provided url. However, Front Door is failing to resolve any domains that it was responsible for.&lt;/p&gt;
    &lt;p&gt;I'd say DNS/Front Door (or some carrier interconnect) is the thing affected, since I can auth just fine in a few places. (I'm at MS, but not looped into anything operational these days, so I'm checking my personal subscription).&lt;/p&gt;
    &lt;p&gt;Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.&lt;/p&gt;
    &lt;p&gt;“ Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025”&lt;/p&gt;
    &lt;p&gt;SSO is down, Azure Portal Down and more, seems like a major outage. Already a lot of services seem to be affected: banks, airlines, consumer apps, etc.&lt;/p&gt;
    &lt;p&gt;The portal is up for me and their status page confirms they did a failover for it. Definitely not disputing that its reach is wide, but a lot of smaller setups probably aren't using Front Door.&lt;/p&gt;
    &lt;p&gt;It begs the question from a noob like me... Where should they host the status page? Surely it shouldn't be on the same infra that it's supposed to be monitoring. Am I correct in thinking that?&lt;/p&gt;
    &lt;p&gt;Looks like MyGet is impacted too. Seems like they use Azure:&lt;/p&gt;
    &lt;p&gt;&amp;gt;What is required to be able to use MyGet? ... MyGet runs its operations from the Microsoft Azure in the West Europe region, near Amsterdam, the Netherlands.&lt;/p&gt;
    &lt;p&gt;It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.&lt;/p&gt;
    &lt;p&gt;I absolutely love the utility aspect of LLMs but part of me is curious if moving faster by using AI is going to make these sorts of failure more and more often.&lt;/p&gt;
    &lt;p&gt;Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking? Or is that the info that the malefactors are seeking?&lt;/p&gt;
    &lt;p&gt;Unable to access the portal and any hit to SSO for other corporate accesses is also broken. Seems like there's something wrong in their Identity services.&lt;/p&gt;
    &lt;p&gt;HTTPSConnectionPool(host='schemas.xmlsoap.org', port=443): Max retries exceeded with url: /soap/encoding/ (Caused by SSLError(CertificateError("hostname 'schemas.xmlsoap.org' doesn't match '*.azureedge.net'")))&lt;/p&gt;
    &lt;p&gt;A service we rely on that isn't even running on Azure is inaccessible due to this issue. For an asset that probably never changes. Wild for that to be the SPOF.&lt;/p&gt;
    &lt;p&gt;That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.&lt;/p&gt;
    &lt;p&gt;Apologies, but this just reads like a low effort critique of big things.&lt;/p&gt;
    &lt;p&gt;To be clear, they should get criticism. They should be held liable for any damage they cause.&lt;/p&gt;
    &lt;p&gt;But that they remain the biggest cloud offering out there isn't something you'd expect to change from a few outages that, by most all evidence, potential replacements have, as well? More, a lot of the outages potential replacements have are often more global in nature.&lt;/p&gt;
    &lt;p&gt;I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).&lt;/p&gt;
    &lt;p&gt;FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.&lt;/p&gt;
    &lt;p&gt;Hate to say it, but DNS is looking like it's still the undisputed champ.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;Yeah the graph for that one looks exactly the same shape. I wonder if they were depending on some azure component somehow, or maybe there were things hosted on both and the azure failure made enough things failover to AWS that AWS couldn't cope? If that was the case I'd expect to see something similar with GCP too though.&lt;/p&gt;
    &lt;p&gt;Edit: nope looks like there's actually a spike on GCP as well&lt;/p&gt;
    &lt;p&gt;Definitely also a strong possibility. I wish I had paid more attention during the AWS one earlier to see what other things looked like on there at the time.&lt;/p&gt;
    &lt;p&gt;When you look at the scale of the reports, you find they are much lower than Azure's. seeing a bunch of 24-hour sparkline type graphs next to each other can make it look like they are equally impacted, but AWS has 500 reports and Azure has 20,000. The scale is hidden by the choice of graph.&lt;/p&gt;
    &lt;p&gt;In other words, people reporting outages at AWS are probably having trouble with microsoft-run DNS services or caching proxies. It's not that the issues aren't there, it's that the internet is full of intermingled complexity. Just that amount of organic false-positives can make it look like an unrelated major service is impacted.&lt;/p&gt;
    &lt;p&gt;I noticed issues on Azure so I went to the status page. It said everything was fine even though the Azure Portal was down. It took more than 10 minutes for that status page to update.&lt;/p&gt;
    &lt;p&gt;How can one of the richest companies in the world not offer a better service?&lt;/p&gt;
    &lt;p&gt;As of now Azure Status page still shows no incident. It must be manually updated, someone has to actively decide to acknowledge an issue, and they're just... not. It undermines confidence in that status page.&lt;/p&gt;
    &lt;p&gt;My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.&lt;/p&gt;
    &lt;p&gt;From Azure status page: "Customers can consider implementing failover strategies with Azure Traffic Manager, to fail over from Azure Front Door to your origins".&lt;/p&gt;
    &lt;p&gt;I especially like how Nadella speaks of layoffs as some kind of uncontrollable natural disaster, like a hurricane, caused by no-one in particular. A kind of "God works in mysterious ways".&lt;/p&gt;
    &lt;p&gt;&amp;gt; “Microsoft is being recognized and rewarded at levels never seen before,” Nadella wrote. “And yet, at the same time, we’ve undergone layoffs. This is the enigma of success in an industry that has no franchise value.” &amp;gt; Nadella explained the disconnect between thriving financials and layoffs by stating that “progress isn’t linear” and that it is “sometimes dissonant, and always demanding.”&lt;/p&gt;
    &lt;p&gt;I've read the whole memo and it's actually worse than those excerpts. Nadella doesn't even claim these were low performers:&lt;/p&gt;
    &lt;p&gt;&amp;gt; These decisions are among the most difficult we have to make. They affect people we’ve worked alongside, learned from, and shared countless moments with—our colleagues, teammates, and friends.&lt;/p&gt;
    &lt;p&gt;Ok, so Microsoft is thriving, these were friends and people "we've learned from", but they must go because... uh... "progress isn't linear". Well, thanks Nadella! That explains so much!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748661</guid><pubDate>Wed, 29 Oct 2025 16:01:18 +0000</pubDate></item><item><title>Composer: Building a fast frontier model with RL</title><link>https://cursor.com/blog/composer</link><description>&lt;doc fingerprint="3d5aedd9e03a0a1a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Composer: Building a fast frontier model with RL&lt;/head&gt;
    &lt;p&gt;Composer is our new agent model designed for software engineering intelligence and speed. On our benchmarks, the model achieves frontier coding results with generation speed four times faster than similar models.&lt;/p&gt;
    &lt;p&gt;We achieve these results by training the model to complete real-world software engineering challenges in large codebases. During training, Composer is given access to a set of production search and editing tools and tasked with efficiently solving a diverse range of difficult problems. The final result is a large-scale model optimized for high-speed use as an agent in Cursor.&lt;/p&gt;
    &lt;p&gt;Our motivation comes from our experience developing Cursor Tab, our custom completion model. We found that often developers want the smartest model that can support interactive use, keeping them in the flow of coding. In our development process, we experimented with a prototype agent model, codenamed Cheetah, to better understand the impact of faster agent models. Composer is a smarter version of this model that keeps coding delightful by being fast enough for an interactive experience.&lt;/p&gt;
    &lt;p&gt;Composer is a mixture-of-experts (MoE) language model supporting long-context generation and understanding. It is specialized for software engineering through reinforcement learning (RL) in a diverse range of development environments. At each iteration of training, the model is given a problem description and instructed to produce the best response, be it a code edit, a plan, or an informative answer. The model has access to simple tools, like reading and editing files, and also more powerful ones like terminal commands and codebase-wide semantic search.&lt;/p&gt;
    &lt;p&gt;To measure progress, we constructed an evaluation that measures a model's usefulness to a software developer as faithfully as possible. Our benchmark, Cursor Bench, consists of real agent requests from engineers and researchers at Cursor, along with hand-curated optimal solutions to these requests. The resulting evaluation measures not just the agent’s correctness, but also its adherence to a codebase's existing abstractions and software engineering practices.&lt;/p&gt;
    &lt;p&gt;Reinforcement learning allows us to actively specialize the model for effective software engineering. Since response speed is a critical component for interactive development, we incentivize the model to make efficient choices in tool use and to maximize parallelism whenever possible. In addition, we train the model to be a helpful assistant by minimizing unnecessary responses and claims made without evidence. We also find that during RL, the model learns useful behaviors on its own like performing complex searches, fixing linter errors, and writing and executing unit tests.&lt;/p&gt;
    &lt;p&gt;Efficient training of large MoE models requires significant investment into building infrastructure and systems research. We built custom training infrastructure leveraging PyTorch and Ray to power asynchronous reinforcement learning at scale. We natively train our models at low precision by combining our MXFP8 MoE kernels with expert parallelism and hybrid sharded data parallelism, allowing us to scale training to thousands of NVIDIA GPUs with minimal communication cost. Additionally, training with MXFP8 allows us to deliver faster inference speeds without requiring post-training quantization.&lt;/p&gt;
    &lt;p&gt;During RL, we want our model to be able to call any tool in the Cursor Agent harness. These tools allow editing code, using semantic search, grepping strings, and running terminal commands. At our scale, teaching the model to effectively call these tools requires running hundreds of thousands of concurrent sandboxed coding environments in the cloud. To support this workload, we adapted existing infrastructure we built for Background Agents, rewriting our virtual machine scheduler to support the bursty nature and scale of training runs. This enabled seamless unification of RL environments with production environments.&lt;/p&gt;
    &lt;p&gt;Cursor builds tools for software engineering, and we make heavy use of the tools we develop. A motivation of Composer development has been developing an agent we would reach for in our own work. In recent weeks, we have found that many of our colleagues were using Composer for their day-to-day software development. With this release, we hope that you also find it to be a valuable tool.&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;¹ Benchmarked on an internal benchmark in the Cursor tool harness. We group models into classes based on score and report the best model in each class. "Fast Frontier" includes models designed for efficient inference such as Haiku 4.5 and Gemini Flash 2.5. "Best Open" includes recent open weight model releases such as Qwen Coder and GLM 4.6. "Frontier 7/2025" is the best model available in July of this year. "Best Frontier" includes GPT-5 and Sonnet 4.5, which both outperform Composer. For the Tokens per Second calculation, tokens are standardized across models to the latest Anthropic tokenizer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748725</guid><pubDate>Wed, 29 Oct 2025 16:04:33 +0000</pubDate></item><item><title>Minecraft removing obfuscation in Java Edition</title><link>https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition</link><description>&lt;doc fingerprint="370203ca08b7cced"&gt;
  &lt;main&gt;
    &lt;p&gt;Do you like to mod Java, tinker with builds, or take deep dives into Minecraft’s code? Then this article is for you!&lt;/p&gt;
    &lt;p&gt;For a long time, Java Edition has used obfuscation (hiding parts of the code) – a common practice in the gaming industry. Now we’re changing how we ship Minecraft: Java Edition to remove obfuscation completely. We hope that, with this change, we can pave a future for Minecraft: Java Edition where it’s easier to create, update, and debug mods.&lt;/p&gt;
    &lt;head rend="h2"&gt;An obfuscated history&lt;/head&gt;
    &lt;p&gt;Minecraft: Java Edition has been obfuscated since its release. This obfuscation meant that people couldn’t see our source code. Instead, everything was scrambled – and those who wanted to mod Java Edition had to try and piece together what every class and function in the code did.&lt;/p&gt;
    &lt;p&gt;But we encourage people to get creative both in Minecraft and with Minecraft – so in 2019 we tried to make this tedious process a little easier by releasing “obfuscation mappings”. These mappings were essentially a long list that allowed people to match the obfuscated terms to un-obfuscated terms. This alleviated the issue a little, as modders didn’t need to puzzle out what everything did, or what it should be called anymore. But why stop there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Removing obfuscation in Java Edition&lt;/head&gt;
    &lt;p&gt;To make things even easier – and remove these intermediary steps – we’re removing obfuscation altogether! Starting with the first snapshot following the complete Mounts of Mayhem launch, we will no longer obfuscate Minecraft: Java Edition. This means that this build (and all future builds) will have all of our original names* – now with variable names and other names – included by default to make modding even easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748879</guid><pubDate>Wed, 29 Oct 2025 16:12:56 +0000</pubDate></item><item><title>Tailscale Peer Relays</title><link>https://tailscale.com/blog/peer-relays-beta</link><description>&lt;doc fingerprint="66f1d873751076c5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they’re managed entirely by the customer, peer relays are less throughput-constrained than Tailscale’s managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.&lt;/p&gt;
    &lt;p&gt;In testing with early design partners, we’ve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale’s managed DERP fleet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving past hard NAT&lt;/head&gt;
    &lt;p&gt;Over the past few weeks, you’ve heard us talk about improvements we’ve made to Network Address Translation (NAT) traversal techniques, so that Tailscale can establish direct connections wherever possible (hint: it’s over 90% of the time). However, we’ve also outlined places where this isn’t possible or desirable today for a variety of reasons, especially in cloud environments. And, we’ve postulated a bit about where we think the industry is going.&lt;/p&gt;
    &lt;p&gt;While we’ve been keeping your network reliably connected for years with DERP, we’ve heard from customers that the throughput and performance aspects of a QoS-aware managed relay fleet makes deployments in certain environments difficult or untenable. Furthermore, customers have noted that it’s non-trivial to deploy and manage custom DERP fleets (which run as a separate service and binary).&lt;/p&gt;
    &lt;p&gt;DERP provides an incredibly valuable service, setting up reliable connections between Tailscale clients anywhere in the world (including negotiating connections through peer relays). But often, DERP’s focus as a reliability and NAT traversal tool results in performance tradeoffs.&lt;/p&gt;
    &lt;p&gt;By contrast, Tailscale Peer Relays is designed as a performant connectivity tool, and can perform at a level rivaling direct connections. Peer relays can be placed right next to the resources they serve, and peer relays also run on top of UDP, both characteristics beneficial to lower latency and resource overhead. And, they are built into the Tailscale client itself for ease of deployment.&lt;/p&gt;
    &lt;p&gt;We want to move past even more hard NATs, and put Tailscale’s relaying technology in our customers’ hands, so they can use Tailscale at scale, anywhere, with ease. We believe our new Tailscale Peer Relays connectivity option—unique to Tailscale—gives customers the best performance and flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Peer relays are configured with a single UDP port that must be available to both sides of a connection. Tailscale Peer Relays is built right into the Tailscale client, and can be enabled with a simple command, using the &lt;code&gt;tailscale set --relay-server-port&lt;/code&gt; flag from the Tailscale CLI. Once enabled via the steps in our documentation, clients can connect to infrastructure in hard NAT environments over the peer relay.&lt;/p&gt;
    &lt;p&gt;And don’t worry, we still prefer to fly direct. Tailscale prefers direct connections wherever possible. Clients can then fall back to available peer relays, and finally leverage Tailscale’s managed DERP fleet, or any customer-deployed custom DERPs, to ensure you have connectivity wherever you need it. All of this traffic, over any connection, is still end-to-end encrypted via WireGuard®.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays is designed for the real world, based on the feedback we’ve received from customers and our own hard-earned networking expertise. It allows customers to make just one firewall exception for connections only coming from their tailnet. Peer relays scale across regions, are resilient to real-world network conditions, and graciously fall back to DERP (Tailscale’s or custom). Your network maintains its shape, but gains all kinds of flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;Connectivity, everywhere, at warp speed&lt;/head&gt;
    &lt;p&gt;Customers can now maintain performance benchmarks even where direct connections aren’t possible, by enabling Tailscale Peer Relays to build a deterministic and high-throughput relay topology.&lt;/p&gt;
    &lt;p&gt;We’ve had customers use peer relays to provide access into unmanaged networks, allowing their partners or customers to provide a controllable and auditable connectivity path without sacrificing performance.&lt;/p&gt;
    &lt;p&gt;In strict private networks, customers can build predictable access paths. Tailscale Peer Relays can be placed in public subnets with VPC peering to private subnets, allowing security teams to efficiently segment their private network infrastructure, while enabling networking teams to roll Tailscale out in full-mesh mode across the subnet.&lt;/p&gt;
    &lt;p&gt;Today, customers are using peer relays to establish relayed connections at near-direct speeds across a variety of environments and settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable high-throughput traffic through cloud NATs, like AWS Managed NAT Gateways: Applications and services behind a Managed NAT Gateway can leverage peer relays to relay traffic that can’t establish direct connections.&lt;/item&gt;
      &lt;item&gt;Relay through network firewalls: Workloads running in strictly firewalled environments can predictably expose a single UDP port, limiting the Tailscale surface area and fast-tracking the approval process for firewall exceptions.&lt;/item&gt;
      &lt;item&gt;Offload from Custom and Managed DERP: Minimize data-in-transit through Tailscale‘s managed DERP network, and remove the need for customer-maintained DERP servers.&lt;/item&gt;
      &lt;item&gt;Provide access to locked down customer networks: Data plane traffic can be relayed through predictable endpoints in customer networks, so that they only need to open minimal numbers of ports to facilitate cross network connections.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;It’s not perfect, but we’re getting there&lt;/head&gt;
    &lt;p&gt;Tailscale Peer Relays is available today as a public beta. We’ve yet to establish all the connectivity paths we want to, and there’s still visibility and debugging improvements to work through. However, we’ve reliably seen our early design partners move to peer relay deployments with relative ease, and we’re ready for you to give it a try on your tailnet.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays can be enabled on all plans, including free (it’s our little way of working through the kinks of the modern internet with our customers). All customers can use two peer relays, for free, forever. As your needs scale, so will the number of available peer relays. To add even more peer relays to your tailnet, come have a chat with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45749017</guid><pubDate>Wed, 29 Oct 2025 16:21:36 +0000</pubDate></item><item><title>AOL to be sold to Bending Spoons for $1.5B</title><link>https://www.axios.com/2025/10/29/aol-bending-spoons-deal</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45749161</guid><pubDate>Wed, 29 Oct 2025 16:28:56 +0000</pubDate></item><item><title>Does brand advertising work? Upwave (YC S12) is hiring engineers to answer that</title><link>https://www.upwave.com/job/8228849002/</link><description>&lt;doc fingerprint="355c16590c952eaa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Senior Software Engineer&lt;/head&gt;
    &lt;p&gt;Upwave: The Brand Outcomes Measurement Platform&lt;/p&gt;
    &lt;p&gt;Upwave is a leading measurement company entirely focused on measuring and optimizing upper funnel campaigns.. The world’s leading advertisers, agencies, and media partners trust Upwave’s robust, AI-driven platform to bring science to the top of the funnel.&lt;/p&gt;
    &lt;p&gt;With Upwave, marketers maximize the effectiveness of brand spend. Upwave measures Brand Lift, validates Brand Reach, and surfaces Brand Optimization opportunities in one, dynamic platform with cross-channel brand measurement for CTV, Digital, Social, Linear, Addressable, Retail Media, Streaming Audio and more.&lt;/p&gt;
    &lt;p&gt;We’re a profitable, growth-stage company backed by leading venture investors (Y Combinator, Uncork Capital, Bloomberg Beta, Initialized Capital, PivotNorth, Ridge Ventures, Industry Ventures, Conductive Ventures,) and leading AdTechfounders &amp;amp; CEOs.&lt;/p&gt;
    &lt;p&gt;We’re a humble but ambitious team that takes its work seriously but never ourselves. Come join us.&lt;/p&gt;
    &lt;p&gt;As a Senior Software Engineer at Upwave, you’ll be a full-stack problem solver with a backend focus—building the APIs, data pipelines, and systems that power our brand measurement platform. Your work will process billions of ad impressions, manage complex data workflows, and deliver insights that inform marketing decisions for the world’s biggest brands.&lt;/p&gt;
    &lt;p&gt;You’ll collaborate across engineering, product, and data science to ship high-impact features end-to-end, scale our platform for the next phase of growth, and help define the next generation of brand measurement.&lt;/p&gt;
    &lt;p&gt;What you will do:&lt;/p&gt;
    &lt;p&gt;Build AI-powered customer experiences — integrate LLMs and advanced causal inference techniques into production workflows that automatically generate data visualizations, synthesize campaign performance into natural language insights, and help enterprise customers understand and optimize their advertising through our AI analyst "Bayes."&lt;/p&gt;
    &lt;p&gt;Design and build scalable backend systems —develop microservices and RESTful APIs that power the analytics platform behind the world’s top brand campaigns.&lt;/p&gt;
    &lt;p&gt;Contribute across the stack — work from backend APIs to Python analytics services to React frontends, delivering complete features that combine sophisticated data analysis with intuitive user experiences.&lt;/p&gt;
    &lt;p&gt;Engineer data pipelines at scale — design and operate systems that process massive volumes of ad and survey data with MySQL, DynamoDB, and AWS (S3, Lambda, EMR, Kinesis Firehose).&lt;/p&gt;
    &lt;p&gt;Improve reliability and performance — deploy services on Kubernetes and AWS, automate deployments via CI/CD, monitor with DataDog and Sentry, and continuously raise the bar for operational excellence&lt;/p&gt;
    &lt;p&gt;Collaborate deeply — work closely with Product and Data Science to productionize statistical models, integrate advanced analytics into customer-facing tools, and bring cutting-edge AI capabilities to enterprise customers.&lt;/p&gt;
    &lt;p&gt;Deliver insights that move millions — enable brand lift analytics and real-time campaign insights by building reliable, high-throughput systems. Multi-million dollar advertising decisions hinge on our recommendations.&lt;/p&gt;
    &lt;p&gt;About you:&lt;/p&gt;
    &lt;p&gt;You’re an experienced engineer (5+ years) who thrives on solving complex problems across APIs, data systems, and distributed infrastructure. You care about clean architecture, reliable systems, and measurable customer impact.&lt;/p&gt;
    &lt;p&gt;You’ve built powerful, intuitive, API-driven products for professional users.. You’re comfortable across the stack, with experience in RDBMS-backed backends using Spring Boot, Django, Rails, or Express, and single-page frontends built in React, Vue, or Angular.&lt;/p&gt;
    &lt;p&gt;You understand and enjoy programming. You’re fluent in the modern landscape of UI frameworks, API and microservice architectures, databases, and cloud platforms—and know when to use the right tool for the job.&lt;/p&gt;
    &lt;p&gt;You embrace modern AI-powered development tools to move faster and code smarter. You use technologies like Claude Code, Cursor, and GitHub CoPilot to automate routine work, explore ideas quickly, and focus your time on higher-value system design and innovation.&lt;/p&gt;
    &lt;p&gt;You value structured software development practices—testing, documentation, CI/CD, and code review—and care about building maintainable systems that scale.&lt;/p&gt;
    &lt;p&gt;You believe developers should operate what they build. You think about observability, cost, and reliability from day one, and design systems that are easy to deploy and maintain. You’ve built in the cloud and know both its power and pitfalls.&lt;/p&gt;
    &lt;p&gt;You like turning ideas into tools that make real customers more effective. You collaborate closely with Product to design features that solve real-world problems and delight users.&lt;/p&gt;
    &lt;p&gt;You mentor others, share knowledge freely, and understand that healthy human systems are the foundation of healthy technical systems. Teammates look to you for guidance.&lt;/p&gt;
    &lt;p&gt;You want to understand how things work and why. You care more about the best idea winning than whose idea it is.&lt;/p&gt;
    &lt;p&gt;You take responsibility, move quickly to fix problems, and take pride in establishing areas of deep expertise in a fast-changing environment.&lt;/p&gt;
    &lt;p&gt;You believe high-trust, inclusive teams outperform individuals. You communicate clearly and compassionately, and contribute to a culture where people enjoy working together.&lt;/p&gt;
    &lt;p&gt;Bonus points:&lt;/p&gt;
    &lt;p&gt;Have worked with modern backend ecosystems like Java/Kotlin/Groovy (Spring Boot or Grails) and know how to design APIs that scale elegantly.&lt;/p&gt;
    &lt;p&gt;Are fluent with data systems such as MySQL, DynamoDB, and Presto, and understand the tradeoffs between relational and NoSQL storage.&lt;/p&gt;
    &lt;p&gt;Have built cloud-native applications on AWS, especially using Kubernetes and Terraform for automation and scalability.&lt;/p&gt;
    &lt;p&gt;Know your way around modern front-end frameworks like React/Redux and enjoy collaborating up and down the stack.&lt;/p&gt;
    &lt;p&gt;Have startup DNA—you’re comfortable with ambiguity, iterate fast, and make pragmatic technical decisions.&lt;/p&gt;
    &lt;p&gt;Bring experience from AdTech, MarTech, or measurement platforms, or are excited to learn how AI and large-scale data intersect in this space.&lt;/p&gt;
    &lt;p&gt;Why You’ll Like Working Here:&lt;/p&gt;
    &lt;p&gt;Engineering-first company: Upwave’s success depends on high-velocity innovation, and we believe high velocity comes from high efficiency, not high effort. We set priorities rather than deadlines, we don’t crunch, we work reasonable hours, and engineers actually take vacations.&lt;/p&gt;
    &lt;p&gt;Modern tech stack: Python analytics, Kotlin/Java APIs, event streaming (100k+ RPM), DynamoDB, Kubernetes, AWS, Terraform, LLM orchestration.&lt;/p&gt;
    &lt;p&gt;Impact at scale: Your code processes billions of advertising events and directly influences multi-million dollar decisions by Fortune 500 brands.&lt;/p&gt;
    &lt;p&gt;Autonomy and ownership: Our engineers lead projects from design through deployment and monitoring.&lt;/p&gt;
    &lt;p&gt;Ambitious but humble culture: We take our work seriously but never ourselves. Upwavers collaborate hard and support each other generously. We screen for people who are both exceptionally talented and genuinely kind.&lt;/p&gt;
    &lt;p&gt;Remote-first team: Our diverse team spans half the globe (but only one half, to ensure everyone can talk live when we need to). We balance synchronous core hours with flexibility to create a work environment that enables both deep collaboration and deep work.&lt;/p&gt;
    &lt;p&gt;Additional Information:&lt;/p&gt;
    &lt;p&gt;The annual base salary range for this role is $150,000 - $175,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for the new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits.&lt;/p&gt;
    &lt;p&gt;Upwave is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45749690</guid><pubDate>Wed, 29 Oct 2025 17:00:15 +0000</pubDate></item><item><title>The Green Tea Garbage Collector</title><link>https://go.dev/blog/greenteagc</link><description>&lt;doc fingerprint="1779f98826d60cd9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Go Blog&lt;/head&gt;
    &lt;head rend="h1"&gt;The Green Tea Garbage Collector&lt;/head&gt;
    &lt;p&gt;Go 1.25 includes a new experimental garbage collector called Green Tea, available by setting &lt;code&gt;GOEXPERIMENT=greenteagc&lt;/code&gt; at build time.
Many workloads spend around 10% less time in the garbage collector, but some
workloads see a reduction of up to 40%!&lt;/p&gt;
    &lt;p&gt;It’s production-ready and already in use at Google, so we encourage you to try it out. We know some workloads don’t benefit as much, or even at all, so your feedback is crucial to helping us move forward. Based on the data we have now, we plan to make it the default in Go 1.26.&lt;/p&gt;
    &lt;p&gt;To report back with any problems, file a new issue.&lt;/p&gt;
    &lt;p&gt;To report back with any successes, reply to the existing Green Tea issue.&lt;/p&gt;
    &lt;p&gt;What follows is a blog post based on Michael Knyszek’s GopherCon 2025 talk. We’ll update this blog post with a link to the talk once it’s available online.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tracing garbage collection&lt;/head&gt;
    &lt;p&gt;Before we discuss Green Tea let’s get us all on the same page about garbage collection.&lt;/p&gt;
    &lt;head rend="h3"&gt;Objects and pointers&lt;/head&gt;
    &lt;p&gt;The purpose of garbage collection is to automatically reclaim and reuse memory no longer used by the program.&lt;/p&gt;
    &lt;p&gt;To this end, the Go garbage collector concerns itself with objects and pointers.&lt;/p&gt;
    &lt;p&gt;In the context of the Go runtime, objects are Go values whose underlying memory is allocated from the heap. Heap objects are created when the Go compiler can’t figure out how else to allocate memory for a value. For example, the following code snippet allocates a single heap object: the backing store for a slice of pointers.&lt;/p&gt;
    &lt;code&gt;var x = make([]*int, 10) // global
&lt;/code&gt;
    &lt;p&gt;The Go compiler can’t allocate the slice backing store anywhere except the heap, since it’s very hard, and maybe even impossible, for it to know how long &lt;code&gt;x&lt;/code&gt; will
refer to the object for.&lt;/p&gt;
    &lt;p&gt;Pointers are just numbers that indicate the location of a Go value in memory, and they’re how a Go program references objects. For example, to get the pointer to the beginning of the object allocated in the last code snippet, we can write:&lt;/p&gt;
    &lt;code&gt;&amp;amp;x[0] // 0xc000104000
&lt;/code&gt;
    &lt;head rend="h3"&gt;The mark-sweep algorithm&lt;/head&gt;
    &lt;p&gt;Go’s garbage collector follows a strategy broadly referred to as tracing garbage collection, which just means that the garbage collector follows, or traces, the pointers in the program to identify which objects the program is still using.&lt;/p&gt;
    &lt;p&gt;More specifically, the Go garbage collector implements the mark-sweep algorithm. This is much simpler than it sounds. Imagine objects and pointers as a sort of graph, in the computer science sense. Objects are nodes, pointers are edges.&lt;/p&gt;
    &lt;p&gt;The mark-sweep algorithm operates on this graph, and as the name might suggest, proceeds in two phases.&lt;/p&gt;
    &lt;p&gt;In the first phase, the mark phase, it walks the object graph from well-defined source edges called roots. Think global and local variables. Then, it marks everything it finds along the way as visited, to avoid going in circles. This is analogous to your typical graph flood algorithm, like a depth-first or breadth-first search.&lt;/p&gt;
    &lt;p&gt;Next is the sweep phase. Whatever objects were not visited in our graph walk are unused, or unreachable, by the program. We call this state unreachable because it is impossible with normal safe Go code to access that memory anymore, simply through the semantics of the language. To complete the sweep phase, the algorithm simply iterates through all the unvisited nodes and marks their memory as free, so the memory allocator can reuse it.&lt;/p&gt;
    &lt;head rend="h3"&gt;That’s it?&lt;/head&gt;
    &lt;p&gt;You may think I’m oversimplifying a bit here. Garbage collectors are frequently referred to as magic, and black boxes. And you’d be partially right, there are more complexities.&lt;/p&gt;
    &lt;p&gt;For example, this algorithm is, in practice, executed concurrently with your regular Go code. Walking a graph that’s mutating underneath you brings challenges. We also parallelize this algorithm, which is a detail that’ll come up again later.&lt;/p&gt;
    &lt;p&gt;But trust me when I tell you that these details are mostly separate from the core algorithm. It really is just a simple graph flood at the center.&lt;/p&gt;
    &lt;head rend="h3"&gt;Graph flood example&lt;/head&gt;
    &lt;p&gt;Let’s walk through an example. Navigate through the slideshow below to follow along.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem&lt;/head&gt;
    &lt;p&gt;After all that, I think we have a handle on what the Go garbage collector is actually doing. This process seems to work well enough today, so what’s the problem?&lt;/p&gt;
    &lt;p&gt;Well, it turns out we can spend a lot of time executing this particular algorithm in some programs, and it adds substantial overhead to nearly every Go program. It’s not that uncommon to see Go programs spending 20% or more of their CPU time in the garbage collector.&lt;/p&gt;
    &lt;p&gt;Let’s break down where that time is being spent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Garbage collection costs&lt;/head&gt;
    &lt;p&gt;At a high level, there are two parts to the cost of the garbage collector. The first is how often it runs, and the second is how much work it does each time it runs. Multiply those two together, and you get the total cost of the garbage collector.&lt;/p&gt;
    &lt;p&gt;Over the years we’ve tackled both terms in this equation, and for more on how often the garbage collector runs, see Michael’s GopherCon EU talk from 2022 about memory limits. The guide to the Go garbage collector also has a lot to say about this topic, and is worth a look if you want to dive deeper.&lt;/p&gt;
    &lt;p&gt;But for now let’s focus only on the second part, the cost per cycle.&lt;/p&gt;
    &lt;p&gt;From years of poring over CPU profiles to try to improve performance, we know two big things about Go’s garbage collector.&lt;/p&gt;
    &lt;p&gt;The first is that about 90% of the cost of the garbage collector is spent marking, and only about 10% is sweeping. Sweeping turns out to be much easier to optimize than marking, and Go has had a very efficient sweeper for many years.&lt;/p&gt;
    &lt;p&gt;The second is that, of that time spent marking, a substantial portion, usually at least 35%, is simply spent stalled on accessing heap memory. This is bad enough on its own, but it completely gums up the works on what makes modern CPUs actually fast.&lt;/p&gt;
    &lt;head rend="h3"&gt;“A microarchitectural disaster”&lt;/head&gt;
    &lt;p&gt;What does “gum up the works” mean in this context? The specifics of modern CPUs can get pretty complicated, so let’s use an analogy.&lt;/p&gt;
    &lt;p&gt;Imagine the CPU driving down a road, where that road is your program. The CPU wants to ramp up to a high speed, and to do that it needs to be able to see far ahead of it, and the way needs to be clear. But the graph flood algorithm is like driving through city streets for the CPU. The CPU can’t see around corners and it can’t predict what’s going to happen next. To make progress, it constantly has to slow down to make turns, stop at traffic lights, and avoid pedestrians. It hardly matters how fast your engine is because you never get a chance to get going.&lt;/p&gt;
    &lt;p&gt;Let’s make that more concrete by looking at our example again. I’ve overlaid the heap here with the path that we took. Each left-to-right arrow represents a piece of scanning work that we did and the dashed arrows show how we jumped around between bits of scanning work.&lt;/p&gt;
    &lt;p&gt;Notice that we were jumping all over memory doing tiny bits of work in each place. In particular, we’re frequently jumping between pages, and between different parts of pages.&lt;/p&gt;
    &lt;p&gt;Modern CPUs do a lot of caching. Going to main memory can be up to 100x slower than accessing memory that’s in our cache. CPU caches are populated with memory that’s been recently accessed, and memory that’s nearby to recently accessed memory. But there’s no guarantee that any two objects that point to each other will also be close to each other in memory. The graph flood doesn’t take this into account.&lt;/p&gt;
    &lt;p&gt;Quick side note: if we were just stalling fetches to main memory, it might not be so bad. CPUs issue memory requests asynchronously, so even slow ones could overlap if the CPU could see far enough ahead. But in the graph flood, every bit of work is small, unpredictable, and highly dependent on the last, so the CPU is forced to wait on nearly every individual memory fetch.&lt;/p&gt;
    &lt;p&gt;And unfortunately for us, this problem is only getting worse. There’s an adage in the industry of “wait two years and your code will get faster.”&lt;/p&gt;
    &lt;p&gt;But Go, as a garbage collected language that relies on the mark-sweep algorithm, risks the opposite. “Wait two years and your code will get slower.” The trends in modern CPU hardware are creating new challenges for garbage collector performance:&lt;/p&gt;
    &lt;p&gt;Non-uniform memory access. For one, memory now tends to be associated with subsets of CPU cores. Accesses by other CPU cores to that memory are slower than before. In other words, the cost of a main memory access depends on which CPU core is accessing it. It’s non-uniform, so we call this non-uniform memory access, or NUMA for short.&lt;/p&gt;
    &lt;p&gt;Reduced memory bandwidth. Available memory bandwidth per CPU is trending downward over time. This just means that while we have more CPU cores, each core can submit relatively fewer requests to main memory, forcing non-cached requests to wait longer than before.&lt;/p&gt;
    &lt;p&gt;Ever more CPU cores. Above, we looked at a sequential marking algorithm, but the real garbage collector performs this algorithm in parallel. This scales well to a limited number of CPU cores, but the shared queue of objects to scan becomes a bottleneck, even with careful design.&lt;/p&gt;
    &lt;p&gt;Modern hardware features. New hardware has fancy features like vector instructions, which let us operate on a lot of data at once. While this has the potential for big speedups, it’s not immediately clear how to make that work for marking because marking does so much irregular and often small pieces of work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Green Tea&lt;/head&gt;
    &lt;p&gt;Finally, this brings us to Green Tea, our new approach to the mark-sweep algorithm. The key idea behind Green Tea is astonishingly simple:&lt;/p&gt;
    &lt;p&gt;Work with pages, not objects.&lt;/p&gt;
    &lt;p&gt;Sounds trivial, right? And yet, it took a lot of work to figure out how to order the object graph walk and what we needed to track to make this work well in practice.&lt;/p&gt;
    &lt;p&gt;More concretely, this means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Instead of scanning objects we scan whole pages.&lt;/item&gt;
      &lt;item&gt;Instead of tracking objects on our work list, we track whole pages.&lt;/item&gt;
      &lt;item&gt;We still need to mark objects at the end of the day, but we’ll track marked objects locally to each page, rather than across the whole heap.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Green Tea example&lt;/head&gt;
    &lt;p&gt;Let’s see what this means in practice by looking at our example heap again, but this time running Green Tea instead of the straightforward graph flood.&lt;/p&gt;
    &lt;p&gt;As above, navigate through the annotated slideshow to follow along.&lt;/p&gt;
    &lt;head rend="h3"&gt;Getting on the highway&lt;/head&gt;
    &lt;p&gt;Let’s come back around to our driving analogy. Are we finally getting on the highway?&lt;/p&gt;
    &lt;p&gt;Let’s recall our graph flood picture before.&lt;/p&gt;
    &lt;p&gt;We jumped around a whole lot, doing little bits of work in different places. The path taken by Green Tea looks very different.&lt;/p&gt;
    &lt;p&gt;Green Tea, in contrast, makes fewer, longer left-to-right passes over pages A and B. The longer these arrows, the better, and with bigger heaps, this effect can be much stronger. That’s the magic of Green Tea.&lt;/p&gt;
    &lt;p&gt;It’s also our opportunity to ride the highway.&lt;/p&gt;
    &lt;p&gt;This all adds up to a better fit with the microarchitecture. We can now scan objects closer together with much higher probability, so there’s a better chance we can make use of our caches and avoid main memory. Likewise, per-page metadata is more likely to be in cache. Tracking pages instead of objects means work lists are smaller, and less pressure on work lists means less contention and fewer CPU stalls.&lt;/p&gt;
    &lt;p&gt;And speaking of the highway, we can take our metaphorical engine into gears we’ve never been able to before, since now we can use vector hardware!&lt;/p&gt;
    &lt;head rend="h3"&gt;Vector acceleration&lt;/head&gt;
    &lt;p&gt;If you’re only vaguely familiar with vector hardware, you might be confused as to how we can use it here. But besides the usual arithmetic and trigonometric operations, recent vector hardware supports two things that are valuable for Green Tea: very wide registers, and sophisticated bit-wise operations.&lt;/p&gt;
    &lt;p&gt;Most modern x86 CPUs support AVX-512, which has 512-bit wide vector registers. This is wide enough to hold all of the metadata for an entire page in just two registers, right on the CPU, enabling Green Tea to work on an entire page in just a few straight-line instructions. Vector hardware has long supported basic bit-wise operations on whole vector registers, but starting with AMD Zen 4 and Intel Ice Lake, it also supports a new bit vector “Swiss army knife” instruction that enables a key step of the Green Tea scanning process to be done in just a few CPU cycles. Together, these allow us to turbo-charge the Green Tea scan loop.&lt;/p&gt;
    &lt;p&gt;This wasn’t even an option for the graph flood, where we’d be jumping between scanning objects that are all sorts of different sizes. Sometimes you needed two bits of metadata and sometimes you needed ten thousand. There simply wasn’t enough predictability or regularity to use vector hardware.&lt;/p&gt;
    &lt;p&gt;If you want to nerd out on some of the details, read along! Otherwise, feel free to skip ahead to the evaluation.&lt;/p&gt;
    &lt;head rend="h4"&gt;AVX-512 scanning kernel&lt;/head&gt;
    &lt;p&gt;To get a sense of what AVX-512 GC scanning looks like, take a look at the diagram below.&lt;/p&gt;
    &lt;p&gt;There’s a lot going on here and we could probably fill an entire blog post just on how this works. For now, let’s just break it down at a high level:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;First we fetch the “seen” and “scanned” bits for a page. Recall, these are one bit per object in the page, and all objects in a page have the same size.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Next, we compare the two bit sets. Their union becomes the new “scanned” bits, while their difference is the “active objects” bitmap, which tells us which objects we need to scan in this pass over the page (versus previous passes).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We take the difference of the bitmaps and “expand” it, so that instead of one bit per object, we have one bit per word (8 bytes) of the page. We call this the “active words” bitmap. For example, if the page stores 6-word (48-byte) objects, each bit in the active objects bitmap will be copied to 6 bits in the active words bitmap. Like so:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Next we fetch the pointer/scalar bitmap for the page. Here, too, each bit corresponds to a word (8 bytes) of the page, and it tells us whether that word stores a pointer. This data is managed by the memory allocator.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now, we take the intersection of the pointer/scalar bitmap and the active words bitmap. The result is the “active pointer bitmap”: a bitmap that tells us the location of every pointer in the entire page contained in any live object we haven’t scanned yet.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, we can iterate over the memory of the page and collect all the pointers. Logically, we iterate over each set bit in the active pointer bitmap, load the pointer value at that word, and write it back to a buffer that will later be used to mark objects seen and add pages to the work list. Using vector instructions, we’re able to do this 64 bytes at a time, in just a couple instructions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Part of what makes this fast is the &lt;code&gt;VGF2P8AFFINEQB&lt;/code&gt; instruction,
part of the “Galios Field New Instructions” x86 extension,
and the bit manipulation Swiss army knife we referred to above.
It’s the real star of the show, since it lets us do step (3) in the scanning kernel very, very
efficiently.
It performs a bit-wise affine
transformations,
treating each byte in a vector as itself a mathematical vector of 8 bits
and multiplying it by an 8x8 bit matrix.
This is all done over the Galois field &lt;code&gt;GF(2)&lt;/code&gt;,
which just means multiplication is AND and addition is XOR.
The upshot of this is that we can define a few 8x8 bit matrices for each
object size that perform exactly the 1:n bit expansion we need.&lt;/p&gt;
    &lt;p&gt;For the full assembly code, see this file. The “expanders” use different matrices and different permutations for each size class, so they’re in a separate file that’s written by a code generator. Aside from the expansion functions, it’s really not a lot of code. Most of it is dramatically simplified by the fact that we can perform most of the above operations on data that sits purely in registers. And, hopefully soon this assembly code will be replaced with Go code!&lt;/p&gt;
    &lt;p&gt;Credit to Austin Clements for devising this process. It’s incredibly cool, and incredibly fast!&lt;/p&gt;
    &lt;head rend="h3"&gt;Evaluation&lt;/head&gt;
    &lt;p&gt;So that’s it for how it works. How much does it actually help?&lt;/p&gt;
    &lt;p&gt;It can be quite a lot. Even without the vector enhancements, we see reductions in garbage collection CPU costs between 10% and 40% in our benchmark suite. For example, if an application spends 10% of its time in the garbage collector, then that would translate to between a 1% and 4% overall CPU reduction, depending on the specifics of the workload. A 10% reduction in garbage collection CPU time is roughly the modal improvement. (See the GitHub issue for some of these details.)&lt;/p&gt;
    &lt;p&gt;We’ve rolled Green Tea out inside Google, and we see similar results at scale.&lt;/p&gt;
    &lt;p&gt;We’re still rolling out the vector enhancements, but benchmarks and early results suggest this will net an additional 10% GC CPU reduction.&lt;/p&gt;
    &lt;p&gt;While most workloads benefit to some degree, there are some that don’t.&lt;/p&gt;
    &lt;p&gt;Green Tea is based on the hypothesis that we can accumulate enough objects to scan on a single page in one pass to counteract the costs of the accumulation process. This is clearly the case if the heap has a very regular structure: objects of the same size at a similar depth in the object graph. But there are some workloads that often require us to scan only a single object per page at a time. This is potentially worse than the graph flood because we might be doing more work than before while trying to accumulate objects on pages and failing.&lt;/p&gt;
    &lt;p&gt;The implementation of Green Tea has a special case for pages that have only a single object to scan. This helps reduce regressions, but doesn’t completely eliminate them.&lt;/p&gt;
    &lt;p&gt;However, it takes a lot less per-page accumulation to outperform the graph flood than you might expect. One surprise result of this work was that scanning a mere 2% of a page at a time can yield improvements over the graph flood.&lt;/p&gt;
    &lt;head rend="h3"&gt;Availability&lt;/head&gt;
    &lt;p&gt;Green Tea is already available as an experiment in the recent Go 1.25 release and can be enabled by setting the environment variable &lt;code&gt;GOEXPERIMENT&lt;/code&gt; to &lt;code&gt;greenteagc&lt;/code&gt; at build time.
This doesn’t include the aforementioned vector acceleration.&lt;/p&gt;
    &lt;p&gt;We expect to make it the default garbage collector in Go 1.26, but you’ll still be able to opt-out with &lt;code&gt;GOEXPERIMENT=nogreenteagc&lt;/code&gt; at build time.
Go 1.26 will also add vector acceleration on newer x86 hardware, and include a whole bunch of
tweaks and improvements based on feedback we’ve collected so far.&lt;/p&gt;
    &lt;p&gt;If you can, we encourage you to try at Go tip-of-tree! If you prefer to use Go 1.25, we’d still love your feedback. See this GitHub comment with some details on what diagnostics we’d be interested in seeing, if you can share, and the preferred channels for reporting feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;The journey&lt;/head&gt;
    &lt;p&gt;Before we wrap up this blog post, let’s take a moment to talk about the journey that got us here. The human element of the technology.&lt;/p&gt;
    &lt;p&gt;The core of Green Tea may seem like a single, simple idea. Like the spark of inspiration that just one single person had.&lt;/p&gt;
    &lt;p&gt;But that’s not true at all. Green Tea is the result of work and ideas from many people over several years. Several people on the Go team contributed to the ideas, including Michael Pratt, Cherry Mui, David Chase, and Keith Randall. Microarchitectural insights from Yves Vandriessche, who was at Intel at the time, also really helped direct the design exploration. There were a lot of ideas that didn’t work, and there were a lot of details that needed figuring out. Just to make this single, simple idea viable.&lt;/p&gt;
    &lt;p&gt;The seeds of this idea go all the way back to 2018. What’s funny is that everyone on the team thinks someone else thought of this initial idea.&lt;/p&gt;
    &lt;p&gt;Green Tea got its name in 2024 when Austin worked out a prototype of an earlier version while cafe crawling in Japan and drinking LOTS of matcha! This prototype showed that the core idea of Green Tea was viable. And from there we were off to the races.&lt;/p&gt;
    &lt;p&gt;Throughout 2025, as Michael implemented and productionized Green Tea, the ideas evolved and changed even further.&lt;/p&gt;
    &lt;p&gt;This took so much collaborative exploration because Green Tea is not just an algorithm, but an entire design space. One that we don’t think any of us could’ve navigated alone. It’s not enough to just have the idea, but you need to figure out the details and prove it. And now that we’ve done it, we can finally iterate.&lt;/p&gt;
    &lt;p&gt;The future of Green Tea is bright.&lt;/p&gt;
    &lt;p&gt;Once again, please try it out by setting &lt;code&gt;GOEXPERIMENT=greenteagc&lt;/code&gt; and let us know how it goes!
We’re really excited about this work and want to hear from you!&lt;/p&gt;
    &lt;p&gt; Previous article: Flight Recorder in Go 1.25&lt;lb/&gt; Blog Index &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45749746</guid><pubDate>Wed, 29 Oct 2025 17:03:51 +0000</pubDate></item><item><title>A Year of Fast Apply – Our Path to 10k Tokens per Second</title><link>https://www.relace.ai/blog/relace-apply-3</link><description>&lt;doc fingerprint="d5137f0a11a0a9b"&gt;
  &lt;main&gt;
    &lt;p&gt;A year ago today, we released our first Fast Apply model publicly. Since then, we’ve learned a lot about how to fine-tune small, specialized models for code-specific tasks.&lt;/p&gt;
    &lt;p&gt;Today, we’re open-sourcing what we've learned in training this series of models — dataset curation, training methods, and inference techniques that led to Relace Apply 3, our best model yet, capable of running at 10k+ tokens per second while maintaining state-of-the-art accuracy.&lt;/p&gt;
    &lt;p&gt;Error rate comparison on 500 randomly sampled production merge requests. For breakdown of error categories, see section on evaluating merges.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;When making edits to a codebase, it doesn't make sense to have an expensive LLM regenerate all the unchanged, preexisting code.&lt;/p&gt;
    &lt;p&gt;Editing a thousand line file (~10k tokens) by rewriting it from scratch with Claude 4.5 Sonnet takes over 100 seconds and costs at least $0.18. For coding agents, this is infeasible from a product perspective.&lt;/p&gt;
    &lt;p&gt;The solution is to have the frontier model output a diff that minimally expresses the changes to make (i.e. the hard tokens), and use a lightweight algorithm to efficiently apply the diff back into the preexisting code. Not only does this save on cost, but if the merging algorithm is fast, you also significantly speed up end-to-end generation time.&lt;/p&gt;
    &lt;p&gt;For a long time, LLMs were incapable of reliably producing diff formats that were mergeable by a fixed algorithm like string replace or UDiff.&lt;/p&gt;
    &lt;p&gt;Cursor pioneered a flexible workaround to this problem — let the frontier model produce "lazy" diffs and use a small, fast apply model as the merging algorithm. However, Cursor never made their model available for other companies to use outside of the IDE.&lt;/p&gt;
    &lt;p&gt;We decided to train our own apply model that anyone could use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Use an LLM as the Merge Algorithm?&lt;/head&gt;
    &lt;p&gt;In practice, frontier models can produce a wide variety of pathological diffs. Any closed-form algorithm you write to perform the merge will be susceptible to edge cases. In agentic settings, where there is often no human oversight, these errors compound and produce incorrect code.&lt;/p&gt;
    &lt;p&gt;Example of a pathological initial_code, diff pair that is hard to address with a closed-form merging algorithm.&lt;/p&gt;
    &lt;p&gt;In the code above, the user starts with a function called &lt;code&gt;handler&lt;/code&gt;. The intent
of the diff on the right is to rename the handler function to &lt;code&gt;messageHandler&lt;/code&gt;
and add a parameter for the &lt;code&gt;name&lt;/code&gt; column in the database query.&lt;/p&gt;
    &lt;p&gt;It would be difficult to write an algorithm that anticipates these two steps. A standard merging algorithm would likely just add a new function called &lt;code&gt;messageHandler&lt;/code&gt;, duplicating the original function.&lt;/p&gt;
    &lt;p&gt;The advantage of using an LLM as the merge algorithm is that it can flexibly infer the intent of the diff. Pretraining on trillions of code tokens bakes in pattern recognition robust to the many edge cases you see in production.&lt;/p&gt;
    &lt;p&gt;Also, by splitting the task into two steps — diff generation and merge — you can use a much smaller, fast model to focus entirely on the merge. This separation of work based on difficulty is a pattern we exploit a lot at Relace to improve overall performance of coding agents.&lt;/p&gt;
    &lt;p&gt;To achieve good results without needing to pretrain a model, we fine tune off-the-shelf small models on a high quality dataset that matches the distribution in production.&lt;/p&gt;
    &lt;head rend="h2"&gt;Producing the Dataset&lt;/head&gt;
    &lt;p&gt;A training set for fast apply contains three components: &lt;code&gt;initial_code&lt;/code&gt;, &lt;code&gt;diff&lt;/code&gt;,
and &lt;code&gt;merged_code&lt;/code&gt;. The initial code and diff are passed in as model inputs,
and the merged code is the output we train the model to produce.&lt;/p&gt;
    &lt;p&gt;For fine tuning, you need a small set of high quality examples. We found the size of the dataset to be much less important than the diversity and quality of merge data. Our first model was trained with only 30k data points, and we saw marginal gains beyond 100k data points.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inputs&lt;/head&gt;
    &lt;p&gt;Early on, Kortix AI released an open-source dataset by post-processing data scraped from public GitHub repos. Given some initial code, they prompt a frontier model like Claude to (1) come up with a change to make and (2) produce the "lazy" diff for it.&lt;/p&gt;
    &lt;p&gt;This turns out to be the wrong approach, as it doesn't reflect the actual distribution of data in production environments. The rich variety of edge cases in diffs comes from context overload. The LLM must adhere to instructions in long system prompts while simultaneously inferring intent from noisy conversations with non-technical users.&lt;/p&gt;
    &lt;p&gt;To get high-quality, complex merges with plenty of edge cases into the training set, we partnered with prompt-to-app companies. We took snapshots of the real context for LLM coding tasks and reran them with additional instructions to produce the "lazy" edits. This allowed us to sample directly from the true distribution of &lt;code&gt;initial_code&lt;/code&gt; and &lt;code&gt;diff&lt;/code&gt; that would be seen in production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Output&lt;/head&gt;
    &lt;p&gt;To generate the correct &lt;code&gt;merged_code&lt;/code&gt;, we use distillation with rejection
sampling. The idea is to feed the initial code and diff into a well-prompted
frontier “teacher” model, guided by a set of rules for how merges should be
handled.&lt;/p&gt;
    &lt;p&gt;This approach lets you produce a large amount of candidate data quickly, but it’s crucial to filter out the teacher’s mistakes. When done correctly, the trained student model can actually end up outperforming the teacher.&lt;/p&gt;
    &lt;p&gt;However, filtering correctly is hard. We developed a multi-stage process to build a high quality LLM-as-a-judge that could scale up to thousands of datapoints.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluating Merges&lt;/head&gt;
    &lt;p&gt;We began by manually reviewing 500 randomly sampled examples to create a set of ironclad ground truths. For our first Apply model, these datapoints were fully synthetic, but we later repeated the process using real production data from earlier iterations of the hosted model.&lt;/p&gt;
    &lt;p&gt;To streamline the process while maintaining quality, we built our own internal evaluation tool: a Git-style diff viewer with annotation tools for categorizing merge outcomes.&lt;/p&gt;
    &lt;p&gt;A screenshot of Relace's internal merge evaluation tool.&lt;/p&gt;
    &lt;p&gt;Even with this tool, it took us over 40 hours to painstakingly ensure 100% correctness. Patterns started to emerge in the process, and we broke down merges into 6 categories:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Correct Merge: Model correctly implemented the intent of the diff.&lt;/item&gt;
      &lt;item&gt;Non-functional Error: Model implemented the intent of the diff except for inconsequential details such as variation in comments and formatting (e.g. function definitions in different orders).&lt;/item&gt;
      &lt;item&gt;Smoothing: Model fixed an error incorporated by the diff that would have resulted in uncompilable or broken code.&lt;/item&gt;
      &lt;item&gt;Functional/Merge Error: Model did not carry out the intent of the diff (e.g. omitting parts of code and inserting code in incorrect places).&lt;/item&gt;
      &lt;item&gt;Hallucination Error: Model added or changed code not specified by the diff (may be unsuccessful attempts at smoothing).&lt;/item&gt;
      &lt;item&gt;Truncation Errors: Model performed an incomplete merge (with parts of final code cut out) leading to uncompilable code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Categories 1-3 are considered correct merges, while 4-6 are considered incorrect. We keep the six-class system for more nuanced evaluations, but collapse to a binary classification when creating the LLM judge.&lt;/p&gt;
    &lt;head rend="h3"&gt;Aside on Smoothing&lt;/head&gt;
    &lt;p&gt;We often found that frontier LLMs produced diffs leading to slightly incorrect code. The most common example is when the diff uses a new library in the code without actually importing it.&lt;/p&gt;
    &lt;p&gt;This raised a question: should a fast apply model strictly follow the diff, or fix the error? In practice, we found customers building coding agents prefer that the model auto-corrects small mistakes to reduce friction for end users.&lt;/p&gt;
    &lt;p&gt;The hallucination category accounts for cases where the apply model overstepped and introduced more incorrect code to try and fix errors.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLM-as-a-Judge&lt;/head&gt;
    &lt;p&gt;Once we had this set of 500 categorized merge examples we could trust, the next step was to align an LLM-as-a-judge to our human-annotated seed dataset. It would be completely infeasible to hand-evaluate enough code merges to create the full training dataset of 100k+ examples.&lt;/p&gt;
    &lt;p&gt;The LLM-as-a-judge technique exploits the generation/verification gap. i.e. It's easier for an LLM to evaluate whether an answer is correct than to actually generate the answer. Still, the LLM often makes mistakes, and it's important to align it properly with a human evaluation first.&lt;/p&gt;
    &lt;p&gt;As with any binary classification task, there are two error modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;False positive - a bad merge is incorrectly classified as good.&lt;/item&gt;
      &lt;item&gt;False negative - a good merge is incorrectly classified as bad.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since we have a large synthetic dataset, false positives are the more problematic error mode. It's much worse for bad merges to remain in the dataset than to reduce dataset yield by throwing away a few false negatives.&lt;/p&gt;
    &lt;p&gt;We used Claude 4 Sonnet as the judge and iteratively tuned the prompt until we hit a false positive rate of ~1%. For comparison, the initial naive judge with no tuning had a false positive rate of ~16%!&lt;/p&gt;
    &lt;head rend="h3"&gt;Scaling Up&lt;/head&gt;
    &lt;p&gt;With the aligned LLM judge, it's possible to filter the rest of the data at scale.&lt;/p&gt;
    &lt;p&gt;For Relace Apply 3, we started with 200k sets of &lt;code&gt;initial_code&lt;/code&gt;, &lt;code&gt;diff&lt;/code&gt;, and
synthetically generated &lt;code&gt;merged_code&lt;/code&gt;. We chose a representative distribution of
examples across dozens of languages, but focused predominantly on
TypeScript/Javascript, Markdown, Python, Ruby, and HTML.&lt;/p&gt;
    &lt;p&gt;To further cut down on mistakes, we added an extra post-processing step using a combination of static analysis tools — syntax verification with a code parser, deduplication, and regex-based filtering for common undesirable behaviors identified through customer feedback.&lt;/p&gt;
    &lt;p&gt;After all the filtering, we were left with a high-confidence training set of ~145k data points.&lt;/p&gt;
    &lt;head rend="h2"&gt;Training with LoRA&lt;/head&gt;
    &lt;p&gt;For small models, we've consistently found that data quality is the most important ingredient. With a clean, in-distribution dataset, the training just boils down to specializing a high quality base model for the merging task without catastrophic forgetting on general coding.&lt;/p&gt;
    &lt;p&gt;We trained our apply models using Supervised Fine-Tuning (SFT) on top of open-source coding models in the 3–8 billion parameter range. This gave us the right balance of expressiveness, inference speed, and cost efficiency.&lt;/p&gt;
    &lt;p&gt;We built our own training pipeline on top of the HuggingFace &lt;code&gt;transformers&lt;/code&gt;
library, but you can also just use out-of-the-box libraries like &lt;code&gt;axolotl&lt;/code&gt; or
&lt;code&gt;unsloth&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since post-training typically uses much smaller datasets than pretraining, updating every parameter in the model is wasteful. Instead of retraining all billions of weights, we use Low-rank adaptation (LoRA) — a lightweight fine-tuning method that adds a small number of trainable “adapter” matrices on top of the frozen base model.&lt;/p&gt;
    &lt;p&gt;This lets us specialize the model for merge tasks without erasing its coding intuition. The base model stays intact, while the adapters learn the merging algorithm we care about.&lt;/p&gt;
    &lt;p&gt;We ran a series of grid searches to tune the adapter size (rank), scaling factor (alpha), and learning rate, and landed on the configuration below, which produced the best evaluation loss and convergence speed.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;LoRA Rank&lt;/cell&gt;
        &lt;cell role="head"&gt;LoRA alpha&lt;/cell&gt;
        &lt;cell role="head"&gt;Learning Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Optimizer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;128&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;5e-5&lt;/cell&gt;
        &lt;cell&gt;AdamW&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Interestingly, we independently validated the optimal hyperparameters for LoRA published in the recent Thinking Machines blog post.&lt;/p&gt;
    &lt;p&gt;Using LoRA allowed us to train Relace Apply 3 on all ~145k data points using a single Nvidia H200 GPU on Modal with context length up to 64k tokens.&lt;/p&gt;
    &lt;p&gt;Using Modal allowed us to launch parallel training runs on H200 GPUs without having to wrestle with complicated cloud providers. Even though small model training lets you get way with batch size 1 on a single GPU, running a large hyperparameter sweep would normally take a lot of manual setup. With Modal, we ran our Python scripts and let it handle the scaling for us — no need to ssh to new GPU instances every time.&lt;/p&gt;
    &lt;p&gt;After training in BF16, we convert the model weights to FP8 using the &lt;code&gt;llm-compressor&lt;/code&gt; library from
vLLM. This conversion step is
crucial — by leveraging the FP8 cores on newer Nvidia GPUs, we achieve a
substantial jump in throughput without sacrificing precision.&lt;/p&gt;
    &lt;p&gt;To confirm that the quantization process was effectively lossless, we evaluated the resulting model against our 500 held-out ground-truth examples to validate that the outputs were the same.&lt;/p&gt;
    &lt;head rend="h2"&gt;10k tok/s with Speculative Decoding&lt;/head&gt;
    &lt;p&gt;With the model trained, our goal was to make it feel less like a slow language model and more like a closed-form algorithm for merging code. To reach that user expereince, we needed to push inference speed as far as possible — which meant turning to speculative decoding.&lt;/p&gt;
    &lt;p&gt;LLMs normally generate tokens sequentially, where each new token depends on the ones that came before it. This dependency makes inference inherently slow, since every step requires a full forward pass through the model.&lt;/p&gt;
    &lt;p&gt;Speculative decoding takes advantage of the fact that forward passes are heavily memory bound. i.e. Shuttling the LLM weights into the GPU sRAM takes much longer than actually performing the matrix multiplication with the tensor cores. By guessing a sequence of k tokens, we can process many tokens in parallel (like prefill) and move more towards to the compute-bound regime.&lt;/p&gt;
    &lt;p&gt;After the forward pass, the model checks which of the predicted tokens match the "true" next tokens and keeps the ones that are correct. The better the "guess", the more tokens you accept, and the more you accelerate the model.&lt;/p&gt;
    &lt;p&gt;Animation demonstrating how speculative decoding parallelizes inference.&lt;/p&gt;
    &lt;p&gt;For code merging, large sections of the &lt;code&gt;initial_code&lt;/code&gt; and &lt;code&gt;diff&lt;/code&gt; are nearly
identical to what appears in the &lt;code&gt;merged_code&lt;/code&gt;. We can use this strong prior to
get long, high quality guesses for what tokens the model should output and get
huge speed ups.&lt;/p&gt;
    &lt;p&gt;However, speed and accuracy are tightly coupled for speculative decoding. Each hallucinated token interrupts the guessed sequence, resetting the verification chain and wasting computation.&lt;/p&gt;
    &lt;p&gt;By training on the meticulously cleaned dataset, we minimized these breaks and were able to push Relace Apply 3 to 10k tok/s:&lt;/p&gt;
    &lt;p&gt;Distribution of Relace Apply 3 throughput speed after first token (measured in tokens/second)&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;We tested Relace Apply on two datasets: (1) our manually reviewed benchmark of 500 examples, and (2) a second dataset of pathological merges collected from customer feedback.&lt;/p&gt;
    &lt;p&gt;Across both, Relace Apply 3 achieves state-of-the-art merge accuracy. Substantial improvements were made over the previous generation of Apply models thanks to targetted dataset tuning based on customer feedback.&lt;/p&gt;
    &lt;p&gt;Comparison of merge accuracy between Relace Apply 2 and Relace Apply 3 on a dataset of pathological merge requests.&lt;/p&gt;
    &lt;p&gt;Previous generations of fast apply struggled when edit snippets contained multiple diff formats in one (e.g. combining &lt;code&gt;\\ ... existing code ...&lt;/code&gt; format
with UDiff edits). We included subsets of UDiff and String Replace edits in the
training data for Relace Apply 3 allowing it to act as a universal merger.&lt;/p&gt;
    &lt;p&gt;Relace Apply 3 also introduces native support for 256k context, allowing it to handle very large files without degradation in performance. Combined with the 10k tok/s throughput, this makes Relace Apply 3 the fastest, most accurate, and longest-context model on the market.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast Apply, One Year Later&lt;/head&gt;
    &lt;p&gt;When we released our first Fast Apply model a year ago, LLMs were notoriously bad at outputting valid diff formats. Deterministic approaches like Search-and-Replace or UDiff were brittle, model-specific, and required extensive prompt engineering.&lt;/p&gt;
    &lt;p&gt;Diff formatting accuracy became such a bottleneck that Aider’s polyglot leaderboard tracked it as a separate metric — one column for accuracy, another for performance.&lt;/p&gt;
    &lt;p&gt;Fast Apply changed that. It was the first model to make structured code edits feel reliable, and our customers felt the difference immediately.&lt;/p&gt;
    &lt;p&gt;Today, frontier models have become much better at diff formatting through heavy reinforcement learning on string-edit tools, but they’re still not perfect. Companies that exclusively use deterministic strategies, like Cline, need a supplmentary merge algorithm to help further boost the diff edit accuracy.&lt;/p&gt;
    &lt;p&gt;Schematic reconstructed from an X post by the Cline team. Even deterministic approaches require fallback logic to handle edge cases.&lt;/p&gt;
    &lt;p&gt;The best models now reach roughly 96% edit success, but models without this kind of tuning still fail around 10% of the time.&lt;/p&gt;
    &lt;p&gt;So while we expect that, over time, apply models may be phased out as diff accuracy improves, the underlying philosophy that drove it remains central to new projects.&lt;/p&gt;
    &lt;p&gt;Fast Apply proved that small, specialized models can deliver SoTA results when trained with high quality, task-specific datasets. The methods we developed are now guiding our broader work on accelerating codegen with small agentic models for utility tasks like search, merge conflict resolution, and refactoring.&lt;/p&gt;
    &lt;p&gt;Stay tuned for more releases soon!&lt;/p&gt;
    &lt;head rend="h2"&gt;We're Hiring&lt;/head&gt;
    &lt;p&gt;If you have gotten this far, chances are you found this interesting!&lt;/p&gt;
    &lt;p&gt;We’re hiring pragmatic researchers (Physics/Math/CS/ML) and exceptional engineers to ship models like this that real product teams rely on. Check out our careers page, and join us!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45749763</guid><pubDate>Wed, 29 Oct 2025 17:04:48 +0000</pubDate></item><item><title>Building a Robot Dog (with an airsoft gun)</title><link>https://erikschluntz.com/hardware/2025/10/26/robot-dog.html</link><description>&lt;doc fingerprint="7f38984739a766e7"&gt;
  &lt;main&gt;&lt;p&gt;I designed and built a quadruped robot dog from scratch this summer for a “Mech Warfare” airsoft battlebots competition. I’ve always wanted to build something with legs, and this seemed like a great opportunity to push the boundaries on my mechanical design skills. This blog covers my design process, stress testing, and final robot that I built :)&lt;/p&gt;&lt;p&gt;The rules of Mech Warefare are pretty simple:&lt;/p&gt;&lt;p&gt;After watching some videos of past competitions, I decided that my strategy would be to maximize speed so I could attack opponents from the side and back before they realized what was happening. (I also have future ambitions to build an auto-aim stable enough to fire while moving, but haven’t gotten to that yet).&lt;/p&gt;&lt;p&gt;Most contestants use older “spider” like designs, but for speed I wanted to use a more dynamic dog like design, similar to Boston Dynamic’s Spot and the Unitree Stellar Hunter. I started exploring the web for every example of robot leg geometry and known best practices that I could find.&lt;/p&gt;&lt;p&gt;The most common two designs I found were 1) 4-bar-linkages, and 2) parallel linkages. Both of these really just boil down to how to use two motors to place the “foot” at an arbitrary X,Y location.&lt;/p&gt;&lt;p&gt;Now wouldn’t it be simpler to just have one motor at the hip, and one motor at the knee?&lt;/p&gt;&lt;p&gt;That works, but motors are heavy, and moving one around at the knee creates a LOT of extra work for the hip motor. It’s much better to keep the motors as stationary as possible, and use a mechanical linkage to transfer the motion to the knee. I decided on the 4 bar linkage, as the simpler design (and because it’s what Unitree uses!)&lt;/p&gt;&lt;p&gt;For the next step in my design, I would need Forward Kinematics (FK) and Inverse Kinematics (IK). These are the equations that let you convert between foot X,Y coordinates, and motor angles (theta1, theta2) between each other.&lt;/p&gt;&lt;p&gt;Forward Kinematics is how you take motor angles and calculate where the foot (“end effector”) will be. It is quite simple for this design:&lt;/p&gt;\[\begin{bmatrix} x \\ y \end{bmatrix} = L_{thigh} \begin{bmatrix} \cos\theta_{thigh} \\ \sin\theta_{thigh} \end{bmatrix} + L_{shin} \begin{bmatrix} \cos\theta_{knee} \\ \sin\theta_{knee} \end{bmatrix}\]&lt;p&gt;Note that the two thetas are fully independent of each other, because with a 4 bar linkage design, rotating the thigh does NOT rotate the shin!&lt;/p&gt;&lt;p&gt;Inverse Kinematics is the opposite - given a foot X,Y location, what motor angles will get you there? For complex robots this can be super hard - either requiring a rats nest of trigonometry, or just solving it numerically with an optimizer. Luckily our case is fairly simple!&lt;/p&gt;&lt;p&gt;We can use the law of cosines which relates the angles and side lengths of any triangle to find the knee angle:&lt;/p&gt;\[c^2 = a^2 + b^2 - 2ab\cos C\]&lt;p&gt;Where &lt;code&gt;c&lt;/code&gt; is the distance from foot to hip, &lt;code&gt;a&lt;/code&gt; is our thigh length, and &lt;code&gt;b&lt;/code&gt; is our shin length, and &lt;code&gt;C&lt;/code&gt; is the knee angle. &lt;code&gt;c^2&lt;/code&gt; will be &lt;code&gt;x^2 + y^2&lt;/code&gt; by the pythagorean theorem.&lt;/p&gt;&lt;code&gt;# Use law of cosines to find theta2
cos_theta2 = (x**2 + y**2 - len1**2 - len2**2) / (2 * len1 * len2)

# Two possible solutions for theta2 (knee forward and knee backward)
theta2_knee_forward = np.arccos(cos_theta2)
theta2_knee_backward = -theta2_knee_forward
theta2 = theta2_knee_backward

# k1 and k2 are the x and y components of the target point (FootX, FootY)
# as seen in a coordinate frame that rotates with the first link.
k1 = len1 + len2 * np.cos(theta2)
k2 = len2 * np.sin(theta2)
theta1 = np.arctan2(y, x) - np.arctan2(k2, k1)
&lt;/code&gt;&lt;p&gt;Honestly, Claude just one-shotted this for me and when I visualized the results, they looked correct. As a side note, I’m a big believer in Vibe Coding as long as you have a way to verify the output. In my case it was much easier to verify correctness by playing with this interactively than by staring at the code.&lt;/p&gt;&lt;p&gt;Next, I had to decide the size and shape of my legs - how long should the thigh be, how long should the shin be? How high should It walk above the ground, and how long should each step be? The optimal leg design would be tied to my Gait design, so I wanted to answer all these questions at once.&lt;/p&gt;&lt;p&gt;I decided to approach this like any ML Researcher would - a hyper parameter scan! With my IK code, I can take any leg geometry and any gait parameters (stride length, stride height, period) and calculate the necessary motor angles as well as all the speeds, accelerations, and torques needed to support a particular weight!&lt;/p&gt;&lt;p&gt;I did a parameter sweep of 8000 leg and gait configurations and plotted various characteristics like max torque and max velocity that it would require from the motors (all configurations constrainted to go at a speed of 1m/s).&lt;/p&gt;&lt;p&gt;This was a really valuable exercise to build design intuition and how all these variables related to each other. I didn’t have a single objective function to optimize, so I ended up picking a design that was a balance between optimizing torque and speed: a 16cm thigh and shin.&lt;/p&gt;&lt;p&gt;I was already planning to use Dynamixel MX-106Rs that I had from a previous project, and by my calculations my 16cm leg design was well within their torque and speed limits:&lt;/p&gt;&lt;p&gt;Stall Torque: 8 Nm&lt;/p&gt;&lt;p&gt;Speed: 41 rpm&lt;/p&gt;&lt;p&gt;As a side note, Dynamixels are pricy so I wouldn’t recommend them for most hobbiests. They’re very high quality, and a lesson that I’ve learned over and over again is that it’s worth it in the long run to pay for nice hardware to save yourself time. Time is the biggest limiting factor for my side projects, so this tradeoff was worth it for me. My quick pitch of what the extra $$$ gets you over hobby servos:&lt;/p&gt;&lt;p&gt;The two servos face each other, one directly driving the thigh, and the other driving a bar linkage that controls the shin. To connect each bar there are simple M3 screws through them, with a nylon washer between to reduce friction.&lt;/p&gt;&lt;p&gt;The leg segments are 3D printed PLA. I have never experimented with more advanced filaments, but some of my friends who have told me that any strength benefit wasn’t worth the added difficulty of printing.&lt;/p&gt;&lt;p&gt;Initial motion tests went well, and I could run my gait calculation code to drive the leg through a walking motion.&lt;/p&gt;&lt;p&gt;Doing a walking motion in air was one thing, but I needed to find out how my design would actually hold up carrying a heavy robot, slamming into hard ground, etc. Some of the things I was worried about were the front bar linkage breaking, or the servo mounting block breaking. That block was the center of all torque in the leg between both servos! Time to do some stress testing!&lt;/p&gt;&lt;p&gt;To save time building a testing rig, I decided to test the leg upside down, with a weight hanging from it. Instead of the leg lifting itself up as it stepped, it would just lift a weight and the “hip” would remain stationary, clamped to the table.&lt;/p&gt;&lt;p&gt;I ran this for about ~1hr with a 4lb weight (1/2 of my projected robot weight) and at 1/2 of my target walking speed. The “1/2 robot weight” is because in a trot, there are always two legs on the ground. The “1/2 target walking speed” is because when running any faster this thing made a huge racket slamming the weight down on the ground and I didn’t want my upstairs neighbors to get mad at me 🤣.&lt;/p&gt;&lt;p&gt;Next up was a side load test: How does the leg handle sideways forces that will occur when turning or straifing?&lt;/p&gt;&lt;p&gt;This looked MUCH worse! There’s a huge amount of bend in the bars and the leg is tilting practically 20 degrees to the side! Defintely will need to strengthen the leg for side to side forces.&lt;/p&gt;&lt;p&gt;After all the stres testing was done, I took everything apart to look for damage and problems:&lt;/p&gt;&lt;p&gt;My two big goals for my updated leg design were&lt;/p&gt;&lt;p&gt;The primary change was making the thigh attach to both sides of the servo to form a strong triangle against side to side motion. (Dynamixels conveniently have a mounting hole exactly opposite their horn).&lt;/p&gt;&lt;p&gt;I beefed up the thickness of all the leg bars from 5cm to to 10cm, and replaced the through-hole M3 with a 4mm Shoulder bolt going through two 4x13x5mm bearings that are press fit into the leg. I used shoulder bolts instead of M4 screws, because screws are always slightly smaller than their nominal value, and would rattle around inside the bearing. Should bolts are much tighter tolerance and have a smooth, snug fit.&lt;/p&gt;&lt;p&gt;Up until this point, I’d been living in a 2D world, but it was time to venture out into the 3rd dimension. I would need a way to tilt the legs side to side (to straif and to rotate the robot), and I needed to plan how all 4 of these legs would actually mount into the overall robot.&lt;/p&gt;&lt;p&gt;I designed a pivot point through the servo mounting block that would be driven by a 1:3 gear reduction to another servo mounted above the leg. I used the gears in part because there was no easy way to mount the hip abduction (side to side motion) servo directly to the block, and also so that I could use a cheaper, lower torque dynamixel servo.&lt;/p&gt;&lt;p&gt;The servo block gets yet another high torque connection to it.&lt;/p&gt;&lt;p&gt;The body itself would be 4 carbon fiber tubes with flat plates clamped onto them. The legs would mount onto these plates, each of which was very easy to 3d print because it was flat with no overhangs.&lt;/p&gt;&lt;p&gt;I had never worked with carbon fiber before, but I was incredibly impressed at how rigid they were. Once I tightened all the plate clamps, I could not get the chassis to twist or flex AT ALL, even trying as hard as I could. The tubes were 25mm outer diameter, 420mm long, and each one was about $12 on Amazon. I will definitely design more projects using CF tubes!&lt;/p&gt;&lt;p&gt;My plan was for the rest of my electronics and other parts to clamp onto on the tubes at any location, giving me a ton of flexibility.&lt;/p&gt;&lt;p&gt;Luckily extending my FK and IK into 3D was quite easy. The axis of side to side rotation was directly inline with the “2D” leg, so all &lt;del&gt;I&lt;/del&gt; Claude had to do was rotate a plane to the side, and then solve 2D kinematics in that plane.&lt;/p&gt;&lt;code&gt;# First, determine the abduction angle
# The leg must lie in a plane containing the X axis
# This plane is determined by the Y and Z coordinates
yz_dist = np.sqrt(y**2 + z**2)

# Calculate abduction angle
theta_abd = np.arctan2(z, -y)
y_in_plane = y * np.cos(theta_abd) - z * np.sin(theta_abd)

# Adjust target for hip offset (hip is at y_local = -hip_offset in plane)
x_target_in_plane = x
y_target_in_plane = y_in_plane + hip_offset

# Now solve 2D IK in the rotated plane
solutions_2d = ik_2d(x_target_in_plane, y_target_in_plane, len1, len2,
                        theta2_relative=theta_knee_relative, only_one=only_one)
&lt;/code&gt;&lt;p&gt;I also adjusted the gait generation code to take in a heading and generate foot paths for side-to-side straifing.&lt;/p&gt;&lt;p&gt;Next, I needed to coordinate the motion of all 4 legs. I’m using a Trot which is a simple gait where the diagonal legs always step together, and the pairs are 180 degrees out of phase. For any translation, I just need to calculate a single gait, and then apply it to all 4 legs, but with different time offsets.&lt;/p&gt;&lt;p&gt;For rotation, each foot will move in the direction tangent to the circle of rotation.&lt;/p&gt;&lt;p&gt;Technically, I should make the foot trace an arc during this rotation, rather than a straight line, but Claude and I did some math and came to the conclusion that this approximation would be fine. The maximum distance between an arc and the straight line approximation I’m using is called a “Sagitta”. This is the distance that my leg will need to bend to make up for the difference.&lt;/p&gt;&lt;p&gt;As long as the steps are short and quick this distance will be less than 1cm.&lt;/p&gt;&lt;p&gt;Now to combine linear motion and angular rotation of the robot, for each foot I can just take a linear combination of velocity from the two motions.&lt;/p&gt;&lt;p&gt;And with that, I had a robot that could move around in all directions!&lt;/p&gt;&lt;p&gt;A quick note on stability: I’m not doing any “active” balancing, I’m just running the leg gaits open loop. This works well at high frequency gaits (0.25s period) because the body has enough rotational inertia to not go too far out of balance between steps. If I slow down the gait period to 0.5s or above, the robot tips over between steps.&lt;/p&gt;&lt;p&gt;One of my longer term ambitions is to train an RL algorithm to control the motion, so I’m not going to invest too much more in the classical controls of the gait.&lt;/p&gt;&lt;p&gt;I’ve never played airsoft before, but the guns work by having a motor pull back a powerful spring, which is released and then drives a piston to shoot high velocity air out behind the plastic BB. This thing is a cannon! (Sound on for extra fun)&lt;/p&gt;&lt;p&gt;The turret has two dynamixel servos to pan and tilt, a hopper to hold BBs, and the CSI camera mounted just over the barrell to aim down the sights.&lt;/p&gt;&lt;p&gt;Somewhat surprisingly, the hardest part of this design was the Hopper, which would always jam after a few shots as the BBs formed stable arches over the exit hole. Apparently this is a big known problem called Bridging that mechanical engineers working on things like grain silos have to deal with, and the solution is either to make the hole much bigger (doesn’t work for me, because I need to get a single BB at a time into the firing chamber), or add a motor to stir or move the BBs.&lt;/p&gt;&lt;p&gt;Most other Mech Warfare teams use an extra motor to control their hoppers, but I was running out of time before the competition, and didn’t have time to do anything more than a simple cone shaped hopper. When my gun jammed during the competition I had to take a few steps with the robot to shake the BBs loose!&lt;/p&gt;&lt;p&gt;Part of me still thinks there’s some clever, weird, asymmetric geometry that will passively never get stuck, but I couldn’t figure anything out.&lt;/p&gt;&lt;p&gt;The target plates for the competition each have an IR LED on them for auto-aim systems. Using a zoomed in CSI camera with no IR filter, I can track the target by just finding the most red pixel. The video looks pink because without an IR filter, there’s more light coming into the red channel pixels than a normal camera.&lt;/p&gt;&lt;p&gt;I calibrated which pixels the BBs actually go towards [red], and then use a PID controller to drive the tracked target [green] to the desired spot.&lt;/p&gt;&lt;p&gt;I feel like I should be able to improve the tracking and performance of this a lot, but I think the limiting factor is the latency of my tracking.&lt;/p&gt;&lt;p&gt;(And yes, I was wearing safety glasses for all this testing)&lt;/p&gt;&lt;p&gt;The core of the robot is a Raspberry Pi 3b+. (In retrospect, I should have used a Pi 5 for the extra compute, but at the time I was worried about power consumption and battery life)&lt;/p&gt;&lt;p&gt;With a laptop on the same wifi network as the Pi I can stream video over WebRTC and pass telemetry / commands via a websocket connection, all from a web app control interface.&lt;/p&gt;&lt;p&gt;All my code ran on the Pi as a systemd service launching a main python server. This server launched a few threads which:&lt;/p&gt;&lt;p&gt;Building the control interface and UX for this competition was surprisingly fun! I’m not much of a frontend person, but I do appreciate great UX. Some key elements:&lt;/p&gt;&lt;p&gt;Overall I was very happy with how the competition went! My robot was the fastest there, but my passive hopper made my gun jam a lot, so I couldn’t shoot as fast as the other robots. It was outdoors, so the sunlight washed out the IR tracking LEDs and none of our auto-aims worked 🤦♂️.&lt;/p&gt;&lt;p&gt;It was pretty informal, but I won my only match where we officially used the scoring system 🙂&lt;/p&gt;&lt;p&gt;The most hype part of the day was really feeling like I was taking damage as my camera covers were cracked by BBs&lt;/p&gt;&lt;p&gt;Despite my stress testing early in design, a few parts failed during practice before the competition, and on game day. The failures were all at connection points between pieces, not the bars themselves breaking!&lt;/p&gt;&lt;p&gt;I feel like I could have detected some of these problems much earlier in testing if I had been rougher with the robot, and done things like drop tests, rather than long duration stress tests.&lt;/p&gt;&lt;p&gt;Hopefully you enjoyed reading through my design process for this project! The robot is a really great platform that I hope to reuse for future projects. Specifically I want to train a gait using Reinforcement Learning to replace my classical one.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45749935</guid><pubDate>Wed, 29 Oct 2025 17:15:33 +0000</pubDate></item><item><title>OpenAI’s promise to stay in California helped clear the path for its IPO</title><link>https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45750425</guid><pubDate>Wed, 29 Oct 2025 17:44:34 +0000</pubDate></item><item><title>Encoding x86 Instructions</title><link>https://www-user.tu-chemnitz.de/~heha/hs/chm/x86.chm/x86.htm</link><description>&lt;doc fingerprint="fb91fb8d464572e0"&gt;
  &lt;main&gt;Source: CIS-77 Home
http://www.c-jump.com/CIS77/CIS77syllabus.htm&lt;head rend="h1"&gt;Encoding x86 Instructions &lt;list rend="ul"&gt;&lt;item&gt;It is time to take a look that the actual machine instruction format of the x86 CPU family. &lt;/item&gt;&lt;item&gt;They don't call the x86 CPU a Complex Instruction Set Computer (CISC) for nothing! &lt;/item&gt;&lt;item&gt;Although more complex instruction encodings exist, no one is going to challenge that the x86 has a complex instruction encoding! &lt;/item&gt;&lt;/list&gt; 1. x86 Instructions Overview &lt;div&gt;&lt;p&gt; x86 Instruction Encoding: &lt;/p&gt;&lt;/div&gt; Although the diagram seems to imply that instructions can be up to 16 bytes long, in actuality the x86 will not allow instructions greater than 15 bytes in length.&lt;/head&gt;&lt;p&gt; The prefix bytes are not the opcode expansion prefix discussed earlier - they are special bytes to modify the behavior of existing instructions. &lt;/p&gt;&lt;head rend="h2"&gt;2. x86 Instruction Format Reference&lt;/head&gt;&lt;div&gt;&lt;p&gt; Another view of the x86 instruction format: &lt;/p&gt;&lt;/div&gt;Additional reference:&lt;head rend="h2"&gt;3. x86 Opcode Sizes&lt;/head&gt; The x86 CPU supports two basic opcode sizes: &lt;list rend="ol"&gt;&lt;item&gt;standard one-byte opcode &lt;/item&gt;&lt;item&gt;two-byte opcode consisting of a 0Fh opcode expansion prefix byte. &lt;lb/&gt;The second byte then specifies the actual instruction. &lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;The x86 opcode bytes are 8-bit equivalents of iii field that we discussed in simplified encoding. &lt;/item&gt;&lt;item&gt;This provides for up to 512 different instruction classes, although the x86 does not yet use them all. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;3.1. x86 ADD Instruction Opcode&lt;/head&gt;&lt;div&gt;&lt;p&gt; x86 ADD &lt;/p&gt;instruction opcode&lt;p&gt;:&lt;/p&gt;&lt;p&gt;Bit number one, marked d, specifies the direction of the data transfer: &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;If d = 0 then the destination operand is a memory location, e.g. &lt;quote&gt; add [ebx], al&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;If d = 1 then the destination operand is a register, e.g. &lt;quote&gt; add al, [ebx]&lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;/div&gt;Bit number zero marked s specifies
the size of the operands the ADD instruction operates upon:&lt;list rend="ul"&gt;&lt;item&gt;If s = 0 then the operands are 8-bit registers and memory locations. &lt;/item&gt;&lt;item&gt;If s = 1 then the operands are either 16-bits or 32-bits: &lt;list rend="ul"&gt;&lt;item&gt;Under 32-bit operating systems the default is 32-bit operands if s = 1. &lt;/item&gt;&lt;item&gt;To specify a 16-bit operand (under Windows or Linux) you must insert a special operand-size prefix byte in front of the instruction (example of this later.) &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;lb/&gt;You'll soon see that this direction bit d creates a
problem that results in one instruction have two different possible
opcodes.&lt;head rend="h2"&gt;4. Encoding x86 Instruction Operands, MOD-REG-R/M Byte&lt;/head&gt; The MOD-REG-R/M byte specifies instruction operands and their addressing mode(*): &lt;div&gt;&lt;p&gt; The MOD field specifies x86 addressing mode:&lt;/p&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;MOD&lt;/cell&gt;&lt;cell role="head"&gt;Meaning &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;00&lt;/cell&gt;&lt;cell&gt;Register indirect addressing mode or SIB with no displacement (when R/M = 100) or Displacement only addressing mode (when R/M = 101). &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;01&lt;/cell&gt;&lt;cell&gt;One-byte signed displacement follows addressing mode byte(s). &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;10&lt;/cell&gt;&lt;cell&gt;Four-byte signed displacement follows addressing mode byte(s). &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;11&lt;/cell&gt;&lt;cell&gt;Register addressing mode. &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; The REG field specifies source or destination register:&lt;/p&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;REG Value &lt;/cell&gt;&lt;cell role="head"&gt;Register if data size is eight bits &lt;/cell&gt;&lt;cell role="head"&gt;Register if data size is 16-bits &lt;/cell&gt;&lt;cell role="head"&gt;Register if data size is 32 bits &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;000&lt;/cell&gt;&lt;cell&gt;al&lt;/cell&gt;&lt;cell&gt;ax&lt;/cell&gt;&lt;cell&gt;eax &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;001&lt;/cell&gt;&lt;cell&gt;cl&lt;/cell&gt;&lt;cell&gt;cx&lt;/cell&gt;&lt;cell&gt;ecx &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;010&lt;/cell&gt;&lt;cell&gt;dl&lt;/cell&gt;&lt;cell&gt;dx&lt;/cell&gt;&lt;cell&gt;edx &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;011&lt;/cell&gt;&lt;cell&gt;bl&lt;/cell&gt;&lt;cell&gt;bx&lt;/cell&gt;&lt;cell&gt;ebx &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;ah&lt;/cell&gt;&lt;cell&gt;sp&lt;/cell&gt;&lt;cell&gt;esp &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;101&lt;/cell&gt;&lt;cell&gt;ch&lt;/cell&gt;&lt;cell&gt;bp&lt;/cell&gt;&lt;cell&gt;ebp &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;110&lt;/cell&gt;&lt;cell&gt;dh&lt;/cell&gt;&lt;cell&gt;si&lt;/cell&gt;&lt;cell&gt;esi &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;111&lt;/cell&gt;&lt;cell&gt;bh&lt;/cell&gt;&lt;cell&gt;di&lt;/cell&gt;&lt;cell&gt;edi &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;p&gt; The R/M field, combined with MOD, specifies either &lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;the second operand in a two-operand instruction, or &lt;/item&gt;&lt;item&gt;the only operand in a single-operand instruction like NOT or NEG. &lt;/item&gt;&lt;/list&gt;The d bit in the opcode determines which operand is
the source, and which is the destination:&lt;list class="n" rend="ul"&gt;&lt;item&gt;d=0: MOD R/M &amp;lt;- REG, REG is the source &lt;/item&gt;&lt;item&gt;d=1: REG &amp;lt;- MOD R/M, REG is the destination &lt;/item&gt;&lt;/list&gt;&lt;lb/&gt;(*) Technically, registers do not have an
address, but we apply the term addressing mode to registers
nonetheless.&lt;head rend="h2"&gt;5. General-Purpose Registers&lt;/head&gt;&lt;div&gt;&lt;p&gt; Since the processor accesses registers more quickly than it accesses memory, you can make your programs run faster by keeping the most-frequently used data in registers. &lt;/p&gt;&lt;/div&gt;&lt;list rend="ul"&gt;&lt;item&gt;The EAX, EDX, ECX, EBX, EBP, EDI, and ESI registers are 32-bit general-purpose registers, used for temporary data storage and memory access. &lt;/item&gt;&lt;item&gt;The AX, DX, CX, BX, BP, DI, and SI registers are 16-bit equivalents of the above, they represent the low-order 16 bits of 32-bit registers. &lt;/item&gt;&lt;item&gt;The AH, DH, CH, and BH registers represent the high-order 8 bits of the corresponding registers. &lt;/item&gt;&lt;item&gt;Similarly, AL, DL, CL, and BL represent the low-order 8 bits of the registers. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;6. REG Field of the MOD-REG-R/M Byte&lt;/head&gt; See MOD-REG-R/M Byte.&lt;p&gt; Depending on the instruction, this can be either the source or the destination operand.&lt;/p&gt;&lt;p&gt; Many instructions have the d (direction) field in their opcode to choose REG operand role: &lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;If d=0, REG is the source, &lt;lb/&gt;MOD R/M &amp;lt;- REG. &lt;/item&gt;&lt;item&gt;If d=1, REG is the destination, &lt;lb/&gt;REG &amp;lt;- MOD R/M. &lt;/item&gt;&lt;/list&gt;&lt;lb/&gt;(*) For certain (often single-operand or
immediate-operand) instructions, the REG field may contain an
opcode extension rather than the register bits. The
R/M field will specify the operand in such case.&lt;head rend="h3"&gt;9. MOD R/M Byte and Addressing Modes &lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell valign="top"&gt;&lt;quote&gt; MOD R/M Addressing Mode === === ================================ 00 000 [ eax ] 01 000 [ eax + disp8 ] (1) 10 000 [ eax + disp32 ] 11 000 register ( al / ax / eax ) (2) 00 001 [ ecx ] 01 001 [ ecx + disp8 ] 10 001 [ ecx + disp32 ] 11 001 register ( cl / cx / ecx ) 00 010 [ edx ] 01 010 [ edx + disp8 ] 10 010 [ edx + disp32 ] 11 010 register ( dl / dx / edx ) 00 011 [ ebx ] 01 011 [ ebx + disp8 ] 10 011 [ ebx + disp32 ] 11 011 register ( bl / bx / ebx ) 00 100 SIB Mode (3) 01 100 SIB + disp8 Mode 10 100 SIB + disp32 Mode 11 100 register ( ah / sp / esp ) 00 101 32-bit Displacement-Only Mode (4) 01 101 [ ebp + disp8 ] 10 101 [ ebp + disp32 ] 11 101 register ( ch / bp / ebp ) 00 110 [ esi ] 01 110 [ esi + disp8 ] 10 110 [ esi + disp32 ] 11 110 register ( dh / si / esi ) 00 111 [ edi ] 01 111 [ edi + disp8 ] 10 111 [ edi + disp32 ] 11 111 register ( bh / di / edi ) &lt;cell valign="top"&gt;&lt;list rend="ol"&gt;&lt;item&gt;Addressing modes with 8-bit displacement fall in the range -128..+127 and require only a single byte displacement after the opcode (Faster!) &lt;/item&gt;&lt;item&gt;The size bit in the opcode specifies 8 or 32-bit register size. To select a 16-bit register requires a prefix byte. &lt;/item&gt;&lt;item&gt;The so-called scaled indexed addressing modes, SIB = scaled index byte mode. &lt;/item&gt;&lt;item&gt;Note that there is no [ ebp ] addressing. It's slot is occupied by the 32-bit displacement only addressing mode. Intel decided that programmers can use [ ebp+ disp8 ] addressing mode instead, with its 8-bit displacement set equal to zero (instruction is a little longer, though.) &lt;/item&gt;&lt;/list&gt;&lt;/cell&gt;&lt;/quote&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;8. SIB (Scaled Index Byte) Layout&lt;/head&gt;&lt;div&gt;&lt;p&gt; Scaled index byte layout:&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;Scale Value&lt;/cell&gt;&lt;cell role="head"&gt;Index*Scale Value &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;00&lt;/cell&gt;&lt;cell&gt;Index*1 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;01&lt;/cell&gt;&lt;cell&gt;Index*2 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;10&lt;/cell&gt;&lt;cell&gt;Index*4 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;11&lt;/cell&gt;&lt;cell&gt;Index*8 &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/cell&gt;&lt;cell&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;Index&lt;/cell&gt;&lt;cell role="head"&gt;Register &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;000&lt;/cell&gt;&lt;cell&gt;EAX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;001&lt;/cell&gt;&lt;cell&gt;ECX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;010&lt;/cell&gt;&lt;cell&gt;EDX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;011&lt;/cell&gt;&lt;cell&gt;EBX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;Illegal &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;101&lt;/cell&gt;&lt;cell&gt;EBP &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;110&lt;/cell&gt;&lt;cell&gt;ESI &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;111&lt;/cell&gt;&lt;cell&gt;EDI &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/cell&gt;&lt;cell&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;Base&lt;/cell&gt;&lt;cell role="head"&gt;MOD&lt;/cell&gt;&lt;cell role="head"&gt;Register &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;000&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;EAX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;001&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;ECX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;010&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;EDX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;011&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;EBX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;ESP &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell rowspan="2"&gt;101&lt;/cell&gt;&lt;cell&gt;00&lt;/cell&gt;&lt;cell&gt;Displacement-only &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;01, 10&lt;/cell&gt;&lt;cell&gt;EBP &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;110&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;ESI &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;111&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;EDI &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;list rend="ul"&gt;&lt;item&gt;Scaled indexed addressing mode uses the second byte (namely, SIB byte) that follows the MOD-REG-R/M byte in the instruction format. &lt;/item&gt;&lt;item&gt;The MOD field still specifies the displacement size of zero, one, or four bytes. &lt;list class="n" rend="ul"&gt;&lt;item&gt;The MOD-REG-R/M and SIB bytes are complex, because Intel reused 16-bit addressing circuitry in the 32-bit mode, rather than simply abandoning the 16-bit format in the 32-bit mode. &lt;/item&gt;&lt;item&gt;There are good hardware reasons for this, but the end result is a complex scheme for specifying addressing modes in the opcodes. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;8.1. Scaled Indexed Addressing Mode&lt;/head&gt;&lt;table&gt;&lt;row valign="top"&gt;&lt;cell&gt;&lt;quote&gt; [ reg32 + eax*n ] MOD = 00 [ reg32 + ebx*n ] [ reg32 + ecx*n ] [ reg32 + edx*n ] [ reg32 + ebp*n ] [ reg32 + esi*n ] [ reg32 + edi*n ] [ disp + reg8 + eax*n ] MOD = 01 [ disp + reg8 + ebx*n ] [ disp + reg8 + ecx*n ] [ disp + reg8 + edx*n ] [ disp + reg8 + ebp*n ] [ disp + reg8 + esi*n ] [ disp + reg8 + edi*n ] [ disp + reg32 + eax*n ] MOD = 10 [ disp + reg32 + ebx*n ] [ disp + reg32 + ecx*n ] [ disp + reg32 + edx*n ] [ disp + reg32 + ebp*n ] [ disp + reg32 + esi*n ] [ disp + reg32 + edi*n ] [ disp + eax*n ] MOD = 00, and [ disp + ebx*n ] BASE field = 101 [ disp + ecx*n ] [ disp + edx*n ] [ disp + ebp*n ] [ disp + esi*n ] [ disp + edi*n ] &lt;cell&gt; Note: n = 1, 2, 4, or 8.&lt;p&gt; In each scaled indexed addressing mode the MOD field in MOD-REG-R/M byte specifies the size of the displacement. It can be zero, one, or four bytes:&lt;/p&gt;&lt;quote&gt; MOD R/M Addressing Mode --- --- --------------------------- 00 100 SIB 01 100 SIB + disp8 10 100 SIB + disp32 &lt;/quote&gt; The Base and Index fields of the SIB byte select the base and index registers, respectively.&lt;p&gt; Note that this addressing mode does not allow the use of the ESP register as an index register. Presumably, Intel left this particular mode undefined to provide the ability to extend the addressing modes in a future version of the CPU. &lt;/p&gt;&lt;/cell&gt;&lt;/quote&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;9. Examples&lt;/head&gt;&lt;head rend="h3"&gt;9.1. Encoding ADD Instruction Example&lt;/head&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;&lt;p&gt;The ADD opcode can be decimal 0, 1, 2, or 3, depending on the direction and size bits in the opcode: &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;How could we encode various forms of the ADD instruction using different addressing modes? &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;9.2 Encoding ADD CL, AL Instruction&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Interesting side effect of the direction bit and the MOD-REG-R/M byte organization: some instructions can have two different opcodes, and both are legal! &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;For example, encoding of &lt;/p&gt;&lt;quote&gt; add cl, al &lt;/quote&gt;&lt;p&gt;could be 00 C1 (if d=0), or 02 C8, if d bit is set to 1. &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The possibility of opcode duality issue here applies to all instructions with two register operands. &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;9.3. Encoding ADD ECX, EAX Instruction&lt;/head&gt;&lt;head rend="h3"&gt;9.4. Encoding ADD EDX, DISPLACEMENT Instruction&lt;/head&gt;&lt;head rend="h3"&gt;9.5. Encoding ADD EDI, [EBX] Instruction&lt;/head&gt;&lt;head rend="h3"&gt;9.6. Encoding ADD EAX, [ ESI + disp8 ] Instruction&lt;/head&gt;&lt;head rend="h3"&gt;9.7. Encoding ADD EBX, [ EBP + disp32 ] Instruction&lt;/head&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;&lt;p&gt;Encoding the ADD EBX, [ EBP + disp32 ] instruction: &lt;/p&gt;&lt;quote&gt; add ebx, [ ebp + disp32 ] &lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;9.8. Encoding ADD EBP, [ disp32 + EAX*1 ] Instruction&lt;/head&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;&lt;p&gt;Encoding the ADD EBP, [ disp32 + EAX*1 ] Instruction &lt;/p&gt;&lt;quote&gt; add ebp, [ disp32 + eax*1 ] &lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;9.9. Encoding ADD ECX, [ EBX + EDI*4 ] Instruction&lt;/head&gt;&lt;head rend="h2"&gt;10. Encoding ADD Immediate Instruction&lt;/head&gt;&lt;div&gt;&lt;p&gt; Encoding x86 immediate operands: &lt;/p&gt;&lt;/div&gt;MOD-REG-R/M and SIB bytes have no
bit combinations to specify an immediate operand.&lt;p&gt; Instead, x86 uses a entirely different instruction format to specify instruction with an immediate operand.&lt;/p&gt;&lt;p&gt; There are three rules that apply: &lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;If opcode high-order bit set to 1, then instruction has an immediate constant. &lt;/item&gt;&lt;item&gt;There is no direction bit in the opcode: &lt;list class="n" rend="ul"&gt;&lt;item&gt;: indeed, you cannot specify a constant as a destination operand! &lt;/item&gt;&lt;item&gt;Therefore, destination operand is always the location encoded in the MOD-R/M bits of the the MOD-REG-R/M byte. &lt;/item&gt;&lt;item&gt;In place of the direction bit d, the opcode has a sign extension x bit instead: &lt;list rend="ul"&gt;&lt;item&gt;For 8-bit operands, the CPU ignores x bit. &lt;/item&gt;&lt;item&gt;For 16-bit and 32-bit operands, x bit specifies the size of the Constant following at the end of the instruction: &lt;list rend="ul"&gt;&lt;item&gt;If x bit contains zero, the Constant is the same size as the operand (i.e., 16 or 32 bits). &lt;/item&gt;&lt;item&gt;If x bit contains one, the Constant is a signed 8-bit value, and the CPU sign-extends this value to the appropriate size before adding it to the operand. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;This little x trick often makes programs shorter, because adding small-value constants to 16 or 32 bit operands is very common. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The third difference between the ADD-immediate and the standard ADD instruction is the meaning of the REG field in the MOD-REG-R/M byte: &lt;list rend="ul"&gt;&lt;item&gt;Since the instruction implies that &lt;list rend="ul"&gt;&lt;item&gt;the source operand is a constant, and &lt;/item&gt;&lt;item&gt;MOD-R/M fields specify the destination operand, &lt;/item&gt;&lt;/list&gt; the instruction does not need to use the REG field to specify an operand. &lt;/item&gt;&lt;item&gt;Instead, the x86 CPU uses these three bits as an opcode extension. &lt;/item&gt;&lt;item&gt;For the ADD-immediate instruction the REG bits must contain zero. &lt;/item&gt;&lt;item&gt;Other bit patterns would correspond to a different instruction. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;quote&gt; Note that when adding a constant to a memory location, the displacement (if any) immediately precedes the immediate (constant) value in the opcode sequence. &lt;/quote&gt;&lt;head rend="h2"&gt;11. Encoding Eight, Sixteen, and Thirty-Two Bit Operands&lt;/head&gt;&lt;div&gt;&lt;p&gt; x86 ADD Opcode: &lt;/p&gt;&lt;/div&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;When Intel designed the 8086, one bit in the opcode, s, selected between 8 and 16 bit integer operand sizes. &lt;/item&gt;&lt;item&gt;Later, when CPU added 32-bit integers to its architecture on 80386 chip, there was a problem: &lt;list class="n" rend="ul"&gt;&lt;item&gt;three encodings were needed to support 8, 16, and 32 bit sizes. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Solution was an operand size prefix byte. &lt;/item&gt;&lt;item&gt;Intel studied x86 instruction set and came to the conclusion: &lt;list class="n" rend="ul"&gt;&lt;item&gt;in a 32-bit environment, programs were more likely to use 8-bit and 32-bit operands far more often than 16-bit operands. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;So Intel decided to let the size bit s in the opcode select between 8- and 32-bit operands. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;11.1. Encoding Sixteen Bit Operands&lt;/head&gt;&lt;div&gt;&lt;p&gt; x86 instruction format: &lt;/p&gt;&lt;/div&gt;32-bit programs don't use 16-bit operands that often, but they do
need them now and then.&lt;p&gt; To allow for 16-bit operands, Intel added prefix a 32-bit mode instruction with the operand size prefix byte with value 66h.&lt;/p&gt;&lt;p&gt; This prefix byte tells the CPU to operand on 16-bit data rather than 32-bit data. &lt;/p&gt;&lt;head rend="h2"&gt;12. x86 Instruction Prefix Bytes&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;x86 instruction can have up to 4 prefixes. &lt;/item&gt;&lt;item&gt;Each prefix adjusts interpretation of the opcode: &lt;list rend="ol"&gt;&lt;item&gt;Repeat/lock prefix byte guarantees that instruction will have exclusive use of all shared memory, until the instruction completes execution:&lt;quote&gt; F0h = LOCK&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;String manipulation instruction prefixes&lt;quote&gt; F3h = REP, REPE F2h = REPNE&lt;/quote&gt; where &lt;list rend="ul"&gt;&lt;item&gt;REP repeats instruction the number of times specified by iteration count ECX. &lt;/item&gt;&lt;item&gt;REPE and REPNE prefixes allow to terminate loop on the value of ZF CPU flag. &lt;/item&gt;&lt;/list&gt; Related string manipulation instructions are: &lt;list rend="ul"&gt;&lt;item&gt;MOVS, move string &lt;/item&gt;&lt;item&gt;STOS, store string &lt;/item&gt;&lt;item&gt;SCAS, scan string &lt;/item&gt;&lt;item&gt;CMPS, compare string, etc. &lt;/item&gt;&lt;/list&gt; See also string manipulation sample program: rep_movsb.asm &lt;/item&gt;&lt;item&gt;Segment override prefix causes memory access to use specified segment instead of default segment designated for instruction operand.&lt;quote&gt; 2Eh = CS 36h = SS 3Eh = DS 26h = ES 64h = FS 65h = GS&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;Operand override, 66h. Changes size of data expected by default mode of the instruction e.g. 16-bit to 32-bit and vice versa. &lt;/item&gt;&lt;item&gt;Address override, 67h. Changes size of address expected by the instruction. 32-bit address could switch to 16-bit and vice versa. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;13. Alternate Encodings for Instructions&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;To shorten program code, Intel created alternate (shorter) encodings of some very commonly used instructions. &lt;/item&gt;&lt;item&gt;For example, x86 provides a single byte opcode for&lt;quote&gt; add al, constant ; one-byte opcode and no MOD-REG-R/M byte add eax, constant ; one-byte opcode and no MOD-REG-R/M byte&lt;/quote&gt; the opcodes are 04h and 05h, respectively. Also, &lt;/item&gt;&lt;item&gt;These instructions are one byte shorter than their standard ADD immediate counterparts. &lt;/item&gt;&lt;item&gt;Note that&lt;quote&gt; add ax, constant ; operand size prefix byte + one-byte opcode, no MOD-REG-R/M byte&lt;/quote&gt; requires an operand size prefix just as a standard ADD AX, constant instruction, yet is still one byte shorter than the corresponding standard version of ADD immediate. &lt;/item&gt;&lt;item&gt;Any decent assembler will automatically choose the shortest possible instruction when translating program into machine code. &lt;/item&gt;&lt;item&gt;Intel only provides alternate encodings only for the accumulator registers AL, AX, EAX. &lt;/item&gt;&lt;item&gt;This is a good reason to use accumulator registers if you have a choice &lt;quote&gt; (also a good reason to take some time and study encodings of the x86 instructions.) &lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;14. x86 Opcode Summary&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;x86 opcodes are represented by one or two bytes. &lt;/item&gt;&lt;item&gt;Opcode could extend into unused bits of MOD-REG-R/M byte. &lt;/item&gt;&lt;item&gt;Opcode encodes information about &lt;list rend="ul"&gt;&lt;item&gt;operation type, &lt;/item&gt;&lt;item&gt;operands, &lt;/item&gt;&lt;item&gt;size of each operand, including the size of an immediate operand. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;14.1. MOD-REG-R/M Byte Summary&lt;/head&gt;&lt;div&gt;&lt;p&gt; MOD-REG-R/M Byte: &lt;/p&gt;&lt;/div&gt;&lt;list rend="ul"&gt;&lt;item&gt;MOD-REG-R/M byte follows one or two opcode bytes of the instruction &lt;/item&gt;&lt;item&gt;It provides addressing mode information for one or two operands. &lt;/item&gt;&lt;item&gt;If operand is in memory, or operand is a register: &lt;list rend="ul"&gt;&lt;item&gt;MOD field (bits [7:6]), combined with the R/M field (bits [2:0]), specify memory/register operand, as well as its addressing mode. &lt;/item&gt;&lt;item&gt;REG field (bits [5:3]) specifies another register operand in of the two-operand instruction. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;15. ISA Design Considerations&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Instruction set architecture design that can stand the test of time is a true intellectual challenge. &lt;/item&gt;&lt;item&gt;It takes several compromises between space and efficiency to assign opcodes and encode instruction formats. &lt;/item&gt;&lt;item&gt;Today people are using Intel x86 instruction set for purposes never intended by original designers. &lt;/item&gt;&lt;item&gt;Extending the CPU is a very difficult task. &lt;/item&gt;&lt;item&gt;The instruction set can become extremely complex. &lt;/item&gt;&lt;item&gt;If x86 CPU was designed from scratch today, it would have a totally different ISA! &lt;/item&gt;&lt;item&gt;Software developers usually don't have a problem adapting to a new architecture when writing new software... &lt;quote&gt; ...but they are very resistant to moving existing software from one platform to another. &lt;/quote&gt;&lt;/item&gt;&lt;item&gt;This is the primary reason the Intel x86 platform remains so popular to this day. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;15.1. ISA Design Challenges &lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Allowing for future expansion of the chip requires some undefined opcodes. &lt;/item&gt;&lt;item&gt;From the beginning there should be a balance between the number of undefined opcodes and &lt;list rend="ol"&gt;&lt;item&gt;the number of initial instructions, and &lt;/item&gt;&lt;item&gt;the size of your opcodes (including special assignments.) &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Hard decisions: &lt;list rend="ul"&gt;&lt;item&gt;Reduce the number of instructions in the initial instruction set? &lt;/item&gt;&lt;item&gt;Increase the size of the opcode? &lt;/item&gt;&lt;item&gt;Rely on an opcode prefix byte(s), which makes later added instructions longer? &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;There are no easy answers to these challenges for CPU designers! &lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;16. Intel Architecture Software Developer's Manual&lt;/head&gt; Classic Intel Pentium II Architecture Software Developer's Manual contains three parts: &lt;list rend="ol"&gt;&lt;item&gt;Volume 1 , Intel Basic Architecture: Order Number 243190 , PDF, 2.6 MB. &lt;/item&gt;&lt;item&gt;Volume 2 , Instruction Set Reference: Order Number 243191 , PDF, 6.6 MB. &lt;/item&gt;&lt;item&gt;Volume 3 , System Programing Guide: Order Number 243192 , PDF, 5.1 MB. &lt;/item&gt;&lt;/list&gt;It is highly recommended that you download the above manuals and use them
as a reference.&lt;head rend="h3"&gt;16.1. Intel Instruction Set Reference (Volume2)&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Chapter 3 of the Instruction Set Reference describes &lt;list rend="ul"&gt;&lt;item&gt;each Intel instruction in detail &lt;/item&gt;&lt;item&gt;algorithmic description of each operation &lt;/item&gt;&lt;item&gt;effect on flags &lt;/item&gt;&lt;item&gt;operand(s), their sizes and attributes &lt;/item&gt;&lt;item&gt;CPU exceptions that may be generated. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The instructions are arranged in alphabetical order. &lt;/item&gt;&lt;item&gt;Appendix A provides opcode map for the entire Intel Architecture instruction set. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.2. Chapter 3 of Intel Instruction Set Reference&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Chapter 3 begins with instruction format example and explains the Opcode column encoding. &lt;/item&gt;&lt;item&gt;The Opcode column gives the complete machine codes as it is understood by the CPU. &lt;/item&gt;&lt;item&gt;When possible, the actual machine code bytes are given as exact hexadecimal bytes, in the same order in which they appear in memory. &lt;/item&gt;&lt;item&gt;However, there are opcode definitions other than hexadecimal bytes... &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.3. Intel Reference Opcode Bytes&lt;/head&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;Fow example, &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.4. Intel Reference Opcode Bytes, Cont.&lt;/head&gt;&lt;head rend="h3"&gt;16.5. Intel Reference Opcode Bytes, Cont.&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;/r - Indicates that the instruction uses the Mod R/M byte of the instruction. &lt;/item&gt;&lt;item&gt;Mod R/M byte contains both &lt;list rend="ul"&gt;&lt;item&gt;a register operand reg and &lt;/item&gt;&lt;item&gt;an r/m (register or memory) operand. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.6. Intel Reference Opcode Bytes, Cont. &lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;cb, cw, cd, cp - A 1-byte (cb), 2-byte (cw), 4-byte (cd), or 6-byte (cp) value, &lt;lb/&gt;following the opcode, is used to specify &lt;list rend="ul"&gt;&lt;item&gt;a code offset, &lt;/item&gt;&lt;item&gt;and possibly a new value for the code segment register CS. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.7. Intel Reference Opcode Bytes, Cont.&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;ib, iw, id - A 1-byte (ib), 2-byte (iw), or 4-byte (id) indicates presence of the immediate operand in the instruction. &lt;/item&gt;&lt;item&gt;Typical order of opcode bytes is &lt;list rend="ul"&gt;&lt;item&gt;opcode &lt;/item&gt;&lt;item&gt;Mod R/M byte (optional) &lt;/item&gt;&lt;item&gt;SIB scale-indexing byte (optional) &lt;/item&gt;&lt;item&gt;immediate operand. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The opcode determines if the operand is a signed value. &lt;/item&gt;&lt;item&gt;All words and doublewords are given with the low-order byte first (little endian). &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.8. Intel Reference Opcode Bytes, Cont.&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;+rb, +rw, +rd - A register code, from 0 through 7, added to the hexadecimal byte given at the left of the plus sign to form a single opcode byte. &lt;/item&gt;&lt;item&gt;Register Encodings Associated with the +rb, +rw, and +rd: &lt;p&gt;For example, &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.9. Intel Reference Instruction Column&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;The Instruction column gives the syntax of the instruction statement as it would appear in a 386 Assembly program. &lt;/item&gt;&lt;item&gt;For example, &lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45750501</guid><pubDate>Wed, 29 Oct 2025 17:50:21 +0000</pubDate></item><item><title>The Internet Runs on Free and Open Source Software–and So Does the DNS</title><link>https://www.icann.org/en/blogs/details/the-internet-runs-on-free-and-open-source-softwareand-so-does-the-dns-23-10-2025-en</link><description>&lt;doc fingerprint="8ac2858004c20ddf"&gt;
  &lt;main&gt;
    &lt;p&gt;Free and open-source software (FOSS) is not merely common on the Internet; it is a deeply embedded and essential foundation of the Domain Name System (DNS), the backbone of how we connect online.&lt;/p&gt;
    &lt;p&gt;The ICANN Security and Stability Advisory Committee (SSAC) is pleased to announce the publication of SAC132: The Domain Name System Runs on Free and Open Source Software (FOSS).&lt;/p&gt;
    &lt;head rend="h3"&gt;Why This Matters Now&lt;/head&gt;
    &lt;p&gt;As governments around the world explore new cybersecurity regulations, the ubiquity of FOSS in DNS operations—from domain registration to retrieval—means that policy decisions made today will have direct implications for the Internet's security and resilience tomorrow. SAC132 provides timely, nontechnical guidance to ensure that new policy and regulation serve to strengthen, rather than inadvertently weaken, this critical infrastructure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Insights for Policymakers&lt;/head&gt;
    &lt;p&gt;SAC132 is a foundational guide designed to empower policymakers to strategically manage and sustain the FOSS ecosystem. The report provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clear Foundations – An accessible overview of the DNS and the FOSS development model for nontechnical audiences.&lt;/item&gt;
      &lt;item&gt;Policy Assessment – Analysis of cybersecurity regulations in the United States, United Kingdom, and European Union, with a focus on how they account for FOSS in the DNS ecosystem.&lt;/item&gt;
      &lt;item&gt;Practical Guidance – Concrete findings and recommendations to help policymakers support and secure FOSS as a cornerstone of global connectivity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We invite all policymakers, technical experts, and stakeholders to read the full report.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Call to Engage&lt;/head&gt;
    &lt;p&gt;By publishing SAC132, SSAC seeks to raise awareness of the indispensable role of FOSS in maintaining a secure, stable, and resilient Internet. We invite policymakers, technical experts, and all stakeholders to read the full report and join us in conversations about its findings.&lt;/p&gt;
    &lt;p&gt;You can engage with SSAC and the broader community at ICANN84, whether in Dublin or by participating remotely. Together, we can ensure that the FOSS ecosystem—and the Internet it supports—remains strong, sustainable, and open for all.&lt;/p&gt;
    &lt;p&gt;Finally, we thank all SSAC members and invited experts who contributed to this work, especially co-chairs Maarten Aertsen and Barry Leiba, for their leadership.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45750875</guid><pubDate>Wed, 29 Oct 2025 18:16:07 +0000</pubDate></item><item><title>Dithering – Part 1</title><link>https://visualrambling.space/dithering-part-1/</link><description>&lt;doc fingerprint="1336983308d69b52"&gt;
  &lt;main&gt;
    &lt;p&gt;Understanding how dithering works, visually.&lt;/p&gt;
    &lt;p&gt;tap/click the right side of the screen to go forward →&lt;/p&gt;
    &lt;p&gt;I’ve always been fascinated by the dithering effect. It has a unique charm that I find so appealing.&lt;/p&gt;
    &lt;p&gt;← tap/click the left side to go back&lt;/p&gt;
    &lt;p&gt;I was even more amazed when I learned how dithering works.&lt;/p&gt;
    &lt;p&gt;← or use arrow keys to navigate →&lt;/p&gt;
    &lt;p&gt;Look closely, and you’ll see this animation is made of alternating black and white pixels.&lt;/p&gt;
    &lt;p&gt;But these black and white pixels are specifically arranged to create the illusion of multiple shades.&lt;/p&gt;
    &lt;p&gt;That’s what dithering does: it simulates more color variations than what are actually used.&lt;/p&gt;
    &lt;p&gt;Here, it uses black and white to give the impression of multiple gray shades.&lt;/p&gt;
    &lt;p&gt;To me, dithering is about creating the most out of what we have, and that's what amazes me the most!&lt;/p&gt;
    &lt;p&gt;It inspired me to learn more about it, and now I want to share what I’ve learned.&lt;/p&gt;
    &lt;p&gt;Please note that this is just part one out of three, so I’ll only scratch the surface here.&lt;/p&gt;
    &lt;p&gt;I’ll go deeper in the next parts, which will come soon. Stay tuned!&lt;/p&gt;
    &lt;p&gt;First, let’s explore the dithering basics with this grayscale image example.&lt;/p&gt;
    &lt;p&gt;A grayscale image has various gray shades, from black to white.&lt;/p&gt;
    &lt;p&gt;Imagine a display that only shows black or white pixels, no grays. We must turn some pixels black and others white—but how?&lt;/p&gt;
    &lt;p&gt;One way is to map each pixel to the closest available color.&lt;/p&gt;
    &lt;p&gt;Pixels darker than medium gray turn black and lighter ones turn white.&lt;/p&gt;
    &lt;p&gt;This splits pixels into black or white groups.&lt;/p&gt;
    &lt;p&gt;However, this creates a harsh image with abrupt black-white transitions.&lt;/p&gt;
    &lt;p&gt;Shadow details vanish as gray pixels become fully black or white.&lt;/p&gt;
    &lt;p&gt;Dithering fixes this by selectively pushing some pixels towards the opposite color.&lt;/p&gt;
    &lt;p&gt;Some light gray pixels that are closer to white turn black.&lt;/p&gt;
    &lt;p&gt;Likewise, some dark grays turn white.&lt;/p&gt;
    &lt;p&gt;And it's done in a way that produces special patterns which simulate shades by varying the black-and-white pixel densities.&lt;/p&gt;
    &lt;p&gt;Denser black pixels are used in darker areas, while denser white pixels are used in lighter ones.&lt;/p&gt;
    &lt;p&gt;Next question: How are these patterns generated?&lt;/p&gt;
    &lt;p&gt;One simple dithering method, known as ordered dithering, uses a threshold map.&lt;/p&gt;
    &lt;p&gt;A threshold map is a grid of values representing brightness levels, from 0 (darkest) to 1 (brightest).&lt;/p&gt;
    &lt;p&gt;To dither, we compare each input pixel’s brightness to a corresponding threshold value.&lt;/p&gt;
    &lt;p&gt;If a pixel’s brightness exceeds the threshold (it’s brighter than the threshold), the pixel turns white. Otherwise, it turns black.&lt;/p&gt;
    &lt;p&gt;Repeating this for all pixels gives us the black-and-white dither patterns.&lt;/p&gt;
    &lt;p&gt;The threshold map is designed to output patterns where the black-and-white pixel density matches the input image’s shades.&lt;/p&gt;
    &lt;p&gt;So brighter input produces patterns with more white, while darker input produces more black.&lt;/p&gt;
    &lt;p&gt;These black-and-white density variations are what create the illusion of gray shades when viewed from a distance.&lt;/p&gt;
    &lt;p&gt;To dither larger images, we extend the threshold map to match the image size and follow the same principle:&lt;/p&gt;
    &lt;p&gt;Compare each pixel’s brightness to the threshold map, then turn it black or white accordingly.&lt;/p&gt;
    &lt;p&gt;The image now uses only two colors, but its overall appearance is preserved.&lt;/p&gt;
    &lt;p&gt;The variations in shades are now replaced by variations in black/white pixel density of the dithering patterns.&lt;/p&gt;
    &lt;p&gt;And that’s how dithering works in a nutshell: it replicates shades with fewer colors, which are strategically placed to maintain the original look.&lt;/p&gt;
    &lt;p&gt;I find it a bit ironic how I used to think dithering ‘adds’ a cool effect, when what it actually does is ‘remove’ colors!&lt;/p&gt;
    &lt;p&gt;That's all for now! We’ve reached the end, but there’s still a lot more to explore.&lt;/p&gt;
    &lt;p&gt;For example, we haven’t explored the algorithm to create a threshold map. (spoiler: there are many ways!)&lt;/p&gt;
    &lt;p&gt;There’s also another algorithm called error diffusion, which doesn’t use a threshold map.&lt;/p&gt;
    &lt;p&gt;Each algorithm creates a distinct, unique look, which I believe deserves its own article.&lt;/p&gt;
    &lt;p&gt;And that's why I decided to break this series into three parts.&lt;/p&gt;
    &lt;p&gt;In the next part, I’ll dive into various algorithms for creating threshold maps.&lt;/p&gt;
    &lt;p&gt;In the final part, I’ll focus on the error diffusion algorithm.&lt;/p&gt;
    &lt;p&gt;We'll dive even deeper into dithering's mechanisms in these next 2 parts, so stay tuned!&lt;/p&gt;
    &lt;p&gt;Thank you for reading!&lt;/p&gt;
    &lt;p&gt;visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.&lt;/p&gt;
    &lt;p&gt;If you like this kind of visual article, please consider following me on X/Twitter and sharing this with your friends.&lt;/p&gt;
    &lt;p&gt;I'll keep creating more visual articles like this!&lt;/p&gt;
    &lt;p&gt;https://x.com/damarberlari&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;Topics covered: Three.js, WebGL, dithering, visualization, interactive learning&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45750954</guid><pubDate>Wed, 29 Oct 2025 18:21:35 +0000</pubDate></item><item><title>Extropic is building thermodynamic computing hardware</title><link>https://extropic.ai/</link><description>&lt;doc fingerprint="27b6766c759d22ac"&gt;
  &lt;main&gt;
    &lt;p&gt;Thermodynamic Computing: From 0 to 1 (Launch Video)&lt;/p&gt;
    &lt;p&gt;October 30th, 2025&lt;/p&gt;
    &lt;p&gt;Extropic is building thermodynamic computing hardware that is radically more energy efficient than GPUs.&lt;/p&gt;
    &lt;p&gt;Our thermodynamic sampling units (TSUs) are inherently probabilistic, the perfect fit for probabilistic AI workloads.&lt;/p&gt;
    &lt;p&gt;Hardware&lt;/p&gt;
    &lt;p&gt;prototype platform&lt;/p&gt;
    &lt;p&gt;XTR-0 enables the development of ultra-efficient AI algorithms by providing low-latency communication between Extropic chips and a traditional processor.&lt;/p&gt;
    &lt;p&gt;Software&lt;/p&gt;
    &lt;p&gt;Our open-source Python library that enables everyone to develop thermodynamic algorithms and simulate running them on TSUs&lt;/p&gt;
    &lt;p&gt;We are hiring engineers and scientists to help us pioneer a new form of computing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45750995</guid><pubDate>Wed, 29 Oct 2025 18:25:01 +0000</pubDate></item><item><title>Show HN: SQLite Graph – Graph database with Cypher queries (alpha, but working)</title><link>https://github.com/agentflare-ai/sqlite-graph</link><description>&lt;doc fingerprint="30797d8f6b44dba"&gt;
  &lt;main&gt;
    &lt;p&gt;A powerful SQLite extension that adds graph database capabilities with full Cypher query support. Part of the AgentFlare AI ecosystem. Build sophisticated graph applications with the reliability of SQLite and the expressiveness of Cypher.&lt;/p&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;ALPHA RELEASE: This is an alpha release (v0.1.0-alpha.0) intended for testing and feedback. The API may change in future releases. Not recommended for production use.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Core Graph Operations: Create, read, update, delete nodes and edges via SQL functions&lt;/item&gt;
      &lt;item&gt;Cypher Query Execution (NEW!): Full basic Cypher support working end-to-end &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;CREATE (n)&lt;/code&gt;- Create anonymous nodes ✅&lt;/item&gt;&lt;item&gt;&lt;code&gt;CREATE (p:Person {name: "Alice"})&lt;/code&gt;- Create nodes with labels and properties ✅&lt;/item&gt;&lt;item&gt;&lt;code&gt;CREATE (a)-[:TYPE]-&amp;gt;(b)&lt;/code&gt;- Create relationships ✅&lt;/item&gt;&lt;item&gt;&lt;code&gt;CREATE (a)-[:TYPE {props}]-&amp;gt;(b)&lt;/code&gt;- Create relationships with properties ✅&lt;/item&gt;&lt;item&gt;&lt;code&gt;MATCH (n) RETURN n&lt;/code&gt;- Basic pattern matching ✅&lt;/item&gt;&lt;item&gt;&lt;code&gt;MATCH (n:Label) RETURN n&lt;/code&gt;- Label-based matching ✅&lt;/item&gt;&lt;item&gt;&lt;code&gt;MATCH (a)-[r:TYPE]-&amp;gt;(b) RETURN a, r, b&lt;/code&gt;- Relationship matching ✅&lt;/item&gt;&lt;item&gt;&lt;code&gt;MATCH (n) WHERE n.prop &amp;gt; value RETURN n&lt;/code&gt;- Property filtering ✅&lt;/item&gt;&lt;item&gt;WHERE clause with all comparison operators: &lt;code&gt;=, &amp;gt;, &amp;lt;, &amp;gt;=, &amp;lt;=, &amp;lt;&amp;gt;&lt;/code&gt;✅&lt;/item&gt;&lt;item&gt;Full execution pipeline: parser → logical planner → physical planner → executor ✅&lt;/item&gt;&lt;item&gt;70/70 CREATE TCK tests passing (100% openCypher compliance for CREATE)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Graph Virtual Tables: SQLite virtual table integration for graph data&lt;/item&gt;
      &lt;item&gt;Basic Graph Algorithms: Connectivity checks, density calculations, degree centrality&lt;/item&gt;
      &lt;item&gt;SQL API: &lt;code&gt;graph_node_add()&lt;/code&gt;,&lt;code&gt;graph_edge_add()&lt;/code&gt;,&lt;code&gt;graph_count_nodes()&lt;/code&gt;,&lt;code&gt;graph_count_edges()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Performance: 300K+ nodes/sec, 390K+ edges/sec&lt;/item&gt;
      &lt;item&gt;Python Bindings: Full Python 3.6+ support with examples&lt;/item&gt;
      &lt;item&gt;Thread Safety: Fixed critical thread-safety issues for production use&lt;/item&gt;
      &lt;item&gt;Security: Buffer overflow protections, SQL injection prevention&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Advanced Cypher Features: &lt;list rend="ul"&gt;&lt;item&gt;Bidirectional/reverse relationship matching (&lt;code&gt;&amp;lt;-[r]-&lt;/code&gt;,&lt;code&gt;-[r]-&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;Variable-length paths (&lt;code&gt;[r*1..3]&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;Complex expressions in WHERE (AND, OR, NOT)&lt;/item&gt;&lt;item&gt;Property expressions in RETURN (n.property)&lt;/item&gt;&lt;item&gt;Aggregations (COUNT, SUM, etc.)&lt;/item&gt;&lt;item&gt;ORDER BY, SKIP, LIMIT&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Bidirectional/reverse relationship matching (&lt;/item&gt;
      &lt;item&gt;Graph Algorithms: Shortest path, PageRank (implementation incomplete)&lt;/item&gt;
      &lt;item&gt;Property Indexing: Basic support, optimization ongoing&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full openCypher Compliance: 100% TCK test suite passing&lt;/item&gt;
      &lt;item&gt;Advanced Write Operations: MERGE, SET, DELETE with complex patterns&lt;/item&gt;
      &lt;item&gt;Advanced Pattern Matching: Multi-hop paths, variable-length patterns&lt;/item&gt;
      &lt;item&gt;Advanced Algorithms: Betweenness centrality, community detection&lt;/item&gt;
      &lt;item&gt;Query Optimization: Cost-based query planner with statistics&lt;/item&gt;
      &lt;item&gt;Distributed Queries: Multi-database graph queries&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See ROADMAP.md for detailed feature timeline.&lt;/p&gt;
    &lt;code&gt;# Download from releases page
wget https://github.com/agentflare-ai/sqlite-graph/releases/latest/download/libgraph.so

# Verify checksum (recommended)
wget https://github.com/agentflare-ai/sqlite-graph/releases/latest/download/checksums.txt
sha256sum -c checksums.txt

# Ready to use!&lt;/code&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/agentflare-ai/sqlite-graph.git
cd sqlite-graph

# Build the extension
make

# The extension will be built as build/libgraph.so&lt;/code&gt;
    &lt;p&gt;See the Installation Guide for detailed instructions and platform-specific guidance.&lt;/p&gt;
    &lt;code&gt;import sqlite3
import json

# Load the extension
conn = sqlite3.connect(":memory:")
conn.enable_load_extension(True)
conn.load_extension("./build/libgraph.so")

# Create a graph virtual table
conn.execute("CREATE VIRTUAL TABLE graph USING graph()")

# Option 1: Use Cypher queries (NEW!)
# Create nodes with properties
conn.execute("SELECT cypher_execute('CREATE (p:Person {name: \"Alice\", age: 30})')")
conn.execute("SELECT cypher_execute('CREATE (c:Company {name: \"Acme Inc\"})')")

# Create relationships with properties
conn.execute("SELECT cypher_execute('CREATE (a:Person {name: \"Bob\"})-[:KNOWS {since: 2020}]-&amp;gt;(b:Person {name: \"Charlie\"})')")

# Query with MATCH...RETURN
cursor = conn.execute("SELECT cypher_execute('MATCH (n:Person) RETURN n')")
results = json.loads(cursor.fetchone()[0])
print(results)  # [{"n": Node(1)}, {"n": Node(2)}, ...]

# Query relationships
cursor = conn.execute("SELECT cypher_execute('MATCH (a)-[r:KNOWS]-&amp;gt;(b) RETURN a, r, b')")
results = json.loads(cursor.fetchone()[0])
print(results)  # Returns matching relationships with nodes

# Filter with WHERE clause
cursor = conn.execute("SELECT cypher_execute('MATCH (p:Person) WHERE p.age &amp;gt; 25 RETURN p')")
results = json.loads(cursor.fetchone()[0])
print(results)  # Returns nodes where age &amp;gt; 25

# Option 2: Add nodes using SQL functions
conn.execute("SELECT graph_node_add(1, ?) as id", (json.dumps({"name": "Alice", "age": 30}),))
conn.execute("SELECT graph_node_add(2, ?) as id", (json.dumps({"name": "Bob", "age": 25}),))

# Add edges
conn.execute("SELECT graph_edge_add(1, 2, 'KNOWS', ?) as id", (json.dumps({"since": "2020"}),))

# Query the graph
node_count = conn.execute("SELECT graph_count_nodes()").fetchone()[0]
edge_count = conn.execute("SELECT graph_count_edges()").fetchone()[0]
print(f"Nodes: {node_count}, Edges: {edge_count}")  # Nodes: 4, Edges: 1

# Check graph properties
is_connected = conn.execute("SELECT graph_is_connected()").fetchone()[0]
density = conn.execute("SELECT graph_density()").fetchone()[0]
centrality = conn.execute("SELECT graph_degree_centrality(1)").fetchone()[0]
print(f"Connected: {bool(is_connected)}, Density: {density:.3f}, Alice centrality: {centrality:.3f}")

# Verify Cypher-created nodes
cypher_nodes = conn.execute("SELECT id, labels FROM graph_nodes WHERE labels != ''").fetchall()
print(f"Cypher nodes: {cypher_nodes}")  # [(node_id, 'Person'), (node_id, 'Company')]&lt;/code&gt;
    &lt;p&gt;See the &lt;code&gt;examples/&lt;/code&gt; directory for fully functional demonstrations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;simple_graph_example.py - Complete working example with nodes, edges, and algorithms&lt;/item&gt;
      &lt;item&gt;python_examples.py - 6 comprehensive examples showcasing all working features&lt;/item&gt;
      &lt;item&gt;cypher_demo.py - NEW! Cypher CREATE query examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Basic Cypher queries fully work! CREATE, MATCH, WHERE, and RETURN operations are functional, including relationship creation and matching. You can now build complete graph applications using only Cypher. Advanced features like bidirectional matching, variable-length paths, and aggregations are in development for v0.2.0. The alpha version also provides SQL function-based graph operations for advanced use cases.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FEATURES.md - ⭐ Start here! Complete feature status and API reference&lt;/item&gt;
      &lt;item&gt;ROADMAP.md - Development timeline and planned features&lt;/item&gt;
      &lt;item&gt;Installation Guide - Detailed build and installation instructions&lt;/item&gt;
      &lt;item&gt;Examples - Working code examples (simple_graph_example.py, python_examples.py)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;✅ What Works: Node/edge creation, full Cypher CREATE/MATCH/WHERE/RETURN, relationships with properties, basic algorithms, Python bindings 🚧 In Progress: Advanced MATCH features (bidirectional, variable-length paths), property projection in RETURN, aggregations 📋 Planned: Full Cypher compliance (Q1 2026), advanced algorithms (Q2 2026), query optimization&lt;/p&gt;
    &lt;p&gt;The extension currently consists of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Virtual Table Interface: SQLite virtual table implementation for graph operations&lt;/item&gt;
      &lt;item&gt;✅ Storage Engine: Efficient node/edge storage with JSON properties&lt;/item&gt;
      &lt;item&gt;✅ Algorithm Library: Basic graph algorithms (connectivity, density, centrality)&lt;/item&gt;
      &lt;item&gt;✅ Cypher Execution Engine: Parser → Planner → Iterator-based Executor &lt;list rend="ul"&gt;&lt;item&gt;✅ Lexer and Parser (AST generation)&lt;/item&gt;&lt;item&gt;✅ Logical Plan generation&lt;/item&gt;&lt;item&gt;✅ Physical Plan optimization&lt;/item&gt;&lt;item&gt;✅ Volcano-model iterators&lt;/item&gt;&lt;item&gt;✅ Result serialization&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;✅ Cypher Operators: CREATE, MATCH (with relationships), WHERE, RETURN all working &lt;list rend="ul"&gt;&lt;item&gt;✅ AllNodesScan, LabelIndexScan iterators&lt;/item&gt;&lt;item&gt;✅ Expand iterator (relationship traversal)&lt;/item&gt;&lt;item&gt;✅ Filter iterator (WHERE clause)&lt;/item&gt;&lt;item&gt;✅ Create iterator (nodes and relationships)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;📋 Query Optimizer: Cost-based optimization planned for v0.2.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node Creation: 300,000+ nodes/second (tested up to 1,000 nodes)&lt;/item&gt;
      &lt;item&gt;Edge Creation: 390,000+ edges/second (tested up to 999 edges)&lt;/item&gt;
      &lt;item&gt;Connectivity Check: &amp;lt;1ms for 1,000 node graphs&lt;/item&gt;
      &lt;item&gt;Scalability: Currently tested up to 1,000 nodes/edges&lt;/item&gt;
      &lt;item&gt;Memory: In-memory storage, persistence via SQLite database file&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import json

# Create person nodes
alice_id = 1
bob_id = 2
carol_id = 3

conn.execute("SELECT graph_node_add(?, ?)",
            (alice_id, json.dumps({"name": "Alice", "city": "NYC"})))
conn.execute("SELECT graph_node_add(?, ?)",
            (bob_id, json.dumps({"name": "Bob", "city": "LA"})))
conn.execute("SELECT graph_node_add(?, ?)",
            (carol_id, json.dumps({"name": "Carol", "city": "NYC"})))

# Add friendships
conn.execute("SELECT graph_edge_add(?, ?, 'FRIENDS', ?)",
            (alice_id, bob_id, json.dumps({})))
conn.execute("SELECT graph_edge_add(?, ?, 'FRIENDS', ?)",
            (alice_id, carol_id, json.dumps({})))

# Query the graph with SQL
mutual_friends = conn.execute("""
    SELECT DISTINCT n.id, n.properties
    FROM graph_nodes n
    JOIN graph_edges e1 ON e1.target = n.id
    JOIN graph_edges e2 ON e2.target = n.id
    WHERE e1.source = ? AND e2.source = ?
      AND e1.edge_type = 'FRIENDS' AND e2.edge_type = 'FRIENDS'
""", (alice_id, bob_id)).fetchall()

# Check connectivity
is_connected = conn.execute("SELECT graph_is_connected()").fetchone()[0]
print(f"Network is connected: {bool(is_connected)}")&lt;/code&gt;
    &lt;p&gt;See the &lt;code&gt;examples/&lt;/code&gt; directory for complete, tested code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;simple_graph_example.py&lt;/code&gt;- Basic operations walkthrough&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;python_examples.py&lt;/code&gt;- 6 comprehensive examples with output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Build the extension
make

# Run Python examples (all tests should pass)
cd examples
python3 simple_graph_example.py
python3 python_examples.py

# Expected output: All examples pass with ✅ indicators&lt;/code&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;We welcome contributions! Please check:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FEATURES.md - See what's implemented and what's not&lt;/item&gt;
      &lt;item&gt;ROADMAP.md - See planned features and timeline&lt;/item&gt;
      &lt;item&gt;GitHub Issues - Report bugs or request features&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bug Reports: GitHub Issues&lt;/item&gt;
      &lt;item&gt;Feature Requests: Check ROADMAP.md first, then open an issue with &lt;code&gt;[Feature Request]&lt;/code&gt;tag&lt;/item&gt;
      &lt;item&gt;Questions: See FEATURES.md for detailed API documentation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Alpha v0.1.0 (Current): Core graph operations, basic algorithms v0.2.0 (Q1 2026): Full Cypher query execution v0.3.0 (Q2 2026): Advanced graph algorithms v0.4.0 (Q3 2026): Performance optimization &amp;amp; scale v1.0.0 (2027): Production ready with full openCypher compliance&lt;/p&gt;
    &lt;p&gt;See ROADMAP.md for detailed feature timeline.&lt;/p&gt;
    &lt;p&gt;Part of the AgentFlare AI ecosystem • Built with SQLite Alpha Release: v0.1.0-alpha.0 • Not for production use&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45751339</guid><pubDate>Wed, 29 Oct 2025 18:52:46 +0000</pubDate></item><item><title>Uv is the best thing to happen to the Python ecosystem in a decade</title><link>https://emily.space/posts/251023-uv</link><description>&lt;doc fingerprint="14982e51cdc48290"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;uv is the best thing to happen to the Python ecosystem in a decade&lt;/head&gt;
    &lt;p&gt;23 October 2025 | Reading time: 6 minutes&lt;/p&gt;
    &lt;p&gt;It’s 2025. Does installing Python, managing virtual environments, and synchronizing dependencies between your colleagues really have to be so difficult? Well… no! A brilliant new tool called uv came out recently that revolutionizes how easy installing and using Python can be.&lt;/p&gt;
    &lt;p&gt;uv is a free, open-source tool built by Astral, a small startup that has been churning out Python tools (like the excellent linter Ruff) for the past few years. uv can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install any Python version for you&lt;/item&gt;
      &lt;item&gt;Install packages&lt;/item&gt;
      &lt;item&gt;Manage virtual environments&lt;/item&gt;
      &lt;item&gt;Solve dependency conflicts extremely quickly (very important for big projects.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s best is that it can do all of the above better than any other tool, in my opinion. It’s shockingly fast, written in Rust, and works on almost any operating system or platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing uv&lt;/head&gt;
    &lt;p&gt;uv is straightforward to install. There are a few ways, but the easiest (in my opinion) is this one-liner command — for Linux and Mac, it’s:&lt;/p&gt;
    &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;
    &lt;p&gt;or on Windows in powershell:&lt;/p&gt;
    &lt;code&gt;powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"&lt;/code&gt;
    &lt;p&gt;You can then access uv with the command &lt;code&gt;uv&lt;/code&gt;. Installing uv will not mess up any of your existing Python installations — it’s a separate tool, so it’s safe to install it just to try it out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Managing Python for a project&lt;/head&gt;
    &lt;p&gt;It’s always a good idea to work with virtual environments for any Python project. It keeps different bits of code and dependencies ringfenced from one another, and in my experience, it can save a lot of hassle to get into the habit of using virtual environments as soon as you can. uv naturally uses virtual environments, so it’s very easy to start using them if you get into using uv.&lt;/p&gt;
    &lt;p&gt;uv will build a Python environment for you based on what’s specified in a &lt;code&gt;pyproject.toml&lt;/code&gt; file in the directory (or parent directories) you’re working in. &lt;code&gt;pyproject.toml&lt;/code&gt; files are a standard, modern format for specifying dependencies for a Python project. A barebones one might look a bit like this:&lt;/p&gt;
    &lt;code&gt;[project]
name = "my_project"
version = "1.0.0"
requires-python = "&amp;gt;=3.9,&amp;lt;3.13"
dependencies = [
  "astropy&amp;gt;=5.0.0",
  "pandas&amp;gt;=1.0.0,&amp;lt;2.0",
]&lt;/code&gt;
    &lt;p&gt;In essence, it just has to specify which Python version to use and some dependencies. Adding a name and version number also aren’t a bad idea.&lt;/p&gt;
    &lt;p&gt;(Sidenote: for projects that you publish as packages, such as to the Python Package Index that pip and uv use, &lt;code&gt;pyproject.toml&lt;/code&gt; files are a modern way to specify everything you need to publish your package.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Making a new project with uv&lt;/head&gt;
    &lt;p&gt;To start a new Python project with uv, you can run&lt;/p&gt;
    &lt;code&gt;uv init&lt;/code&gt;
    &lt;p&gt;Which will create a new project for you, with a &lt;code&gt;pyproject.toml&lt;/code&gt;, a &lt;code&gt;README.md&lt;/code&gt;, and other important bits of boilerplate.&lt;/p&gt;
    &lt;p&gt;There are a lot of different ways to run this command, like &lt;code&gt;uv init --bare&lt;/code&gt; (which only creates a pyproject.toml), &lt;code&gt;uv init --package&lt;/code&gt; (which sets up a new Python package), and more. I recommend running &lt;code&gt;uv init --help&lt;/code&gt; to read about them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Once you have/if you already have a &lt;code&gt;pyproject.toml&lt;/code&gt; file&lt;/head&gt;
    &lt;p&gt;Once you initialize a project — or if you already have a &lt;code&gt;pyproject.toml&lt;/code&gt; file in your project — it’s very easy to start using uv. You just need to do&lt;/p&gt;
    &lt;code&gt;uv sync&lt;/code&gt;
    &lt;p&gt;in the directory that your &lt;code&gt;pyproject.toml&lt;/code&gt; file is in. This command (and in fact, most uv commands if you haven’t ran it already) will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Automatically install a valid version of Python&lt;/item&gt;
      &lt;item&gt;Install all dependencies to a new virtual environment in the directory &lt;code&gt;.venv&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create a &lt;code&gt;uv.lock&lt;/code&gt;file in your directory, which saves the exact, platform-agnostic version of every package installed — meaning that other colleagues can replicate your Python environment exactly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In principle, you can ‘activate’ this new virtual environment like any typical virtual environment that you may have seen in other tools, but the most ‘uv-onic’ way to use uv is simply to prepend any command with &lt;code&gt;uv run&lt;/code&gt;. This command automatically picks up the correct virtual environment for you and runs your command with it. For instance, to run a script — instead of&lt;/p&gt;
    &lt;code&gt;source .venv/bin/activate
python myscript.py&lt;/code&gt;
    &lt;p&gt;you can just do&lt;/p&gt;
    &lt;code&gt;uv run myscript.py&lt;/code&gt;
    &lt;p&gt;which will have the same effect. Likewise, to use a ‘tool’ like Jupyter Lab, you can just do&lt;/p&gt;
    &lt;code&gt;uv run jupyter lab&lt;/code&gt;
    &lt;p&gt;in your project’s directory, as opposed to first ‘activating’ the environment and then running &lt;code&gt;jupyter lab&lt;/code&gt; separately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding dependencies&lt;/head&gt;
    &lt;p&gt;You can always just edit your &lt;code&gt;pyproject.toml&lt;/code&gt; file manually: uv will detect the changes and rebuild your project’s virtual environment. But uv also has easier ways to add dependencies — you can just do&lt;/p&gt;
    &lt;code&gt;uv add numpy&amp;gt;=2.0&lt;/code&gt;
    &lt;p&gt;to add a package, including specifying version constraints (like the above.) This command automatically edits your &lt;code&gt;pyproject.toml&lt;/code&gt; for you. &lt;code&gt;uv add&lt;/code&gt; is also extremely powerful for adding remote dependencies from git or elsewhere on your computer (but I won’t get into that here.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Pinning a Python version&lt;/head&gt;
    &lt;p&gt;Finally, I think that one of the most useful things uv can do is to pin a specific Python version for your project. Doing&lt;/p&gt;
    &lt;code&gt;uv python pin 3.12.9&lt;/code&gt;
    &lt;p&gt;would pin the current project to exactly Python 3.12.9 for you, and anyone else using uv — meaning that you really can replicate the exact same Python install across multiple machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;uvx: ignore all of the above and just run a tool, now!&lt;/head&gt;
    &lt;p&gt;But sometimes, you might just want to run a tool quickly — like using Ruff to lint code somewhere, or starting a Jupyter notebook server without an environment, or even just quickly starting an IPython session with pandas installed so you can open up a file. The &lt;code&gt;uv tool&lt;/code&gt; command, which has a short alias &lt;code&gt;uvx&lt;/code&gt;, makes this insanely easy. Running a command like&lt;/p&gt;
    &lt;code&gt;uvx ruff&lt;/code&gt;
    &lt;p&gt;will automatically download the tool you want to use and run it in a one-off virtual environment. Once the tool has been downloaded before, this is lightning-fast because of how uv uses caches.&lt;/p&gt;
    &lt;p&gt;There are a lot of occasions when I might want to do this — a common one might be to quickly start an IPython session with pandas installed (using &lt;code&gt;--with&lt;/code&gt; to add dependencies) so that I can quickly open &amp;amp; look at a parquet file. For instance:&lt;/p&gt;
    &lt;code&gt;uvx --with pandas,pyarrow ipython&lt;/code&gt;
    &lt;p&gt;Or, maybe just starting a Jupyter Lab server so that I can quickly open a Jupyter notebook that a student sent me:&lt;/p&gt;
    &lt;code&gt;uvx jupyter lab&lt;/code&gt;
    &lt;p&gt;Or honestly just so many other weird, one-off use cases where &lt;code&gt;uvx&lt;/code&gt; is really nice to have around. I don’t feel like I’m missing out by always using virtual environments, because &lt;code&gt;uvx&lt;/code&gt; always gives you a ‘get out of jail free’ card whenever you need it.&lt;/p&gt;
    &lt;head rend="h2"&gt;If that hasn’t sold you: a personal note&lt;/head&gt;
    &lt;p&gt;I first discovered uv last year, while working together with our other lovely developers on building The Astrosky Ecosystem — a wonderful project to build open-source social media integrations for astronomers online. But with multiple developers all working asynchronously on multiple operating systems, managing Python installations quickly became a huge task.&lt;/p&gt;
    &lt;p&gt;uv is an incredibly powerful simplification for us that we use across our entire tech stack. As developers, we can all work with identical Python installations, which is especially important given a number of semi-experimental dependencies that we use that have breaking changes with every version. On GitHub Actions, we’re planning to use uv to quickly build a Python environment and run our unit tests. In production, uv already manages Python for all of our servers.&lt;/p&gt;
    &lt;p&gt;It’s just so nice to always know that Python and package installation will always be handled consistently and correctly across all of our machines. That’s why uv is the best thing to happen to the Python ecosystem in a decade.&lt;/p&gt;
    &lt;head rend="h2"&gt;Find out more&lt;/head&gt;
    &lt;p&gt;There’s a lot more on the uv docs, including a getting started page, more in-depth guides, explanations of important concepts, and a full command reference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45751400</guid><pubDate>Wed, 29 Oct 2025 18:57:29 +0000</pubDate></item></channel></rss>