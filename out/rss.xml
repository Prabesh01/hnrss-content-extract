<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 17 Nov 2025 23:09:32 +0000</lastBuildDate><item><title>People are using iPad OS features on their iPhones</title><link>https://idevicecentral.com/ios-customization/how-to-enable-ipad-features-like-multitasking-stage-manager-on-iphone-via-mobilegestalt/</link><description>&lt;doc fingerprint="a6524651630537c2"&gt;
  &lt;main&gt;
    &lt;p&gt;The newly released itunesstored &amp;amp; bookassetd sbx escape exploit allows us to modify the MobileGestalt.Plist file to change values inside of it.&lt;/p&gt;
    &lt;p&gt;This file is very important since it contains all the details about the device. Its type, color, model, capabilities like Dynamic Island, Stage Manager, multitasking, etc. are all present inside that file.&lt;/p&gt;
    &lt;p&gt;Naturally, Apple has encrypted the key-value pairs, but people have managed to figure out most of them over the years.&lt;/p&gt;
    &lt;p&gt;Modification of the MobileGestalt file has allowed many tweaking applications like Nugget, Misaka, and Picasso to exist over the years.&lt;/p&gt;
    &lt;p&gt;Recently, developer Duy Tran posted an intriguing video of their iPhone having iPad features like actual app windows, the iPadOS dock, stage manager, etc. This was done with the new exploit that uses a maliciously crafted downloads.28.sqlitedb database to write to paths normally protected by the Sandbox.&lt;/p&gt;
    &lt;p&gt;Fortunately, MobileGestalt.Plist is one of these paths, and you can actually modify your iPhone to have iPadOS features.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;First look of iPadOS on iPhone 17 Pro Max pic.twitter.com/PMynlGLVFw&lt;/p&gt;â€” Duy Tran (@khanhduytran0) November 15, 2025&lt;/quote&gt;
    &lt;head rend="h2"&gt;Supported iOS versions and devices&lt;/head&gt;
    &lt;p&gt;The new itunesstored &amp;amp; bookassetd sandbox escape exploit supports all devices on iOS up to iOS 26.1 and iOS 26.2 Beta 1.&lt;/p&gt;
    &lt;p&gt;This exploit circulated for a while on the internet and was used for iCloud Bypass purposes since it can write to paths and hacktivate.&lt;/p&gt;
    &lt;p&gt;This will very likely be used to update tools like Nugget, Misaka, etc.&lt;/p&gt;
    &lt;p&gt;Itâ€™s quite a powerful exploit. It can write to most paths controlled/owned by the mobile user. It cannot write to paths owned by the root user.&lt;/p&gt;
    &lt;head rend="h2"&gt;Obtaining the MobileGestalt.Plist file from the device&lt;/head&gt;
    &lt;p&gt;There are several ways to go about this. Some Shortcuts allow you to obtain the plist still, tho some of these floating around have been patched.&lt;/p&gt;
    &lt;p&gt;I didnâ€™t bother. I just made a new Xcode application and read the file at /private/var/containers/Shared/SystemGroup/ systemgroup.com.apple.mobilegestaltcache/Library/Caches/com.apple.MobileGestalt.plist&lt;/p&gt;
    &lt;p&gt;Itâ€™s as simple as:&lt;/p&gt;
    &lt;code&gt;import SwiftUI
import UniformTypeIdentifiers

struct ContentView: View {
    @State private var plistData: Any?
    @StateObject private var coordinator = DocumentPickerCoordinator()
    
    let plistPath = "/private/var/containers/Shared/SystemGroup/systemgroup.com.apple.mobilegestaltcache/Library/Caches/com.apple.MobileGestalt.plist"
    
    var body: some View {
        VStack(spacing: 20) {
            Button("Load Plist") {
                loadPlist()
            }
            
            if plistData != nil {
                Button("Save to Files") {
                    savePlist()
                }
            }
        }
    }
    
    func loadPlist() {
        if let data = try? Data(contentsOf: URL(fileURLWithPath: plistPath)),
           let plist = try? PropertyListSerialization.propertyList(from: data, options: [], format: nil) {
            plistData = plist
        }
    }
    
    func savePlist() {
        guard let plist = plistData,
              let data = try? PropertyListSerialization.data(fromPropertyList: plist, format: .xml, options: 0) else { return }
        
        let tempURL = FileManager.default.temporaryDirectory.appendingPathComponent("MobileGestalt.plist")
        try? data.write(to: tempURL)
        
        let picker = UIDocumentPickerViewController(forExporting: [tempURL], asCopy: true)
        picker.delegate = coordinator
        
        if let scene = UIApplication.shared.connectedScenes.first as? UIWindowScene,
           let window = scene.windows.first,
           let root = window.rootViewController {
            var top = root
            while let presented = top.presentedViewController {
                top = presented
            }
            top.present(picker, animated: true)
        }
    }
}

class DocumentPickerCoordinator: NSObject, UIDocumentPickerDelegate, ObservableObject {
    func documentPicker(_ controller: UIDocumentPickerViewController, didPickDocumentsAt urls: [URL]) {}
    func documentPickerWasCancelled(_ controller: UIDocumentPickerViewController) {}
}&lt;/code&gt;
    &lt;p&gt;This would save your MobileGestalt.Plist file without issue even on iOS 26.1 because Apple still allows iOS apps to read this path, no problem. You canâ€™t write to it this way, but reading works.&lt;/p&gt;
    &lt;p&gt;Once you have it inside the Files application, you can just AirDrop it to your computer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the proper MobileGestalt keys to write&lt;/head&gt;
    &lt;p&gt;There are hundreds of MobileGestalt keys, each controlling something else. These keys are encrypted and look like 1tvy6WfYKVGumYi6Y8E5Og and /bSMNaIuUT58N/BN1nYUjw, etc.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;uKc7FPnEO++lVhHWHFlGbQ = Device is an iPad&lt;/item&gt;
      &lt;item&gt;HV7WDiidgMf7lwAu++Lk5w = Device has TouchID functionality.&lt;/item&gt;
      &lt;item&gt;s2UwZpwDQcywU3de47/ilw = Device has a microphone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And so on. There is a nice article over on TheAppleWiki with all the available MobileGestalt Keys people managed to decrypt over the years.&lt;/p&gt;
    &lt;p&gt;You need to find the right keys to add to your iPhoneâ€™s MobileGestalt that would first make it think itâ€™s an iPad, and then enable iPad features like Stage Manager, Multitasking, etc.&lt;/p&gt;
    &lt;p&gt;Iâ€™ve done the research for you, and you need the following keys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;uKc7FPnEO++lVhHWHFlGbQ = Device is an iPad.&lt;/item&gt;
      &lt;item&gt;mG0AnH/Vy1veoqoLRAIgTA = Device supports Medusa Floating Live Apps&lt;/item&gt;
      &lt;item&gt;UCG5MkVahJxG1YULbbd5Bg = Device supports Medusa Overlay Apps&lt;/item&gt;
      &lt;item&gt;ZYqko/XM5zD3XBfN5RmaXA = Device supports Medusa Pinned Apps&lt;/item&gt;
      &lt;item&gt;nVh/gwNpy7Jv1NOk00CMrw = Device supports MedusaPIP mirroring&lt;/item&gt;
      &lt;item&gt;qeaj75wk3HF4DwQ8qbIi7g = Device is capable of enabling Stage Manager&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Those are all the keys we need for now.&lt;/p&gt;
    &lt;p&gt;The uKc7FPnEO++lVhHWHFlGbQ value is what tells the device it is an iPad instead of an iPhone. This MUST be added to the CacheData section of the MobileGestalt plist, not the CacheExtra, otherwise the device WILL BOOTLOOP!&lt;/p&gt;
    &lt;p&gt;However, we have a problem. CacheData looks like this:&lt;/p&gt;
    &lt;p&gt;So how the hell do we add the necessary keys to it? Itâ€™s all garbled characters. Looks encrypted.&lt;/p&gt;
    &lt;p&gt;Well, you will need to find the offset of the key you wanna change inside the libmobilegestalt.dylib file. Let me explain.&lt;/p&gt;
    &lt;p&gt;The uKc7FPnEO++lVhHWHFlGbQ value (iPad) needs to be added to mtrAoWJ3gsq+I90ZnQ0vQw, which is DeviceClassNumber, like this:&lt;/p&gt;
    &lt;p&gt;mtrAoWJ3gsq+I90ZnQ0vQw : uKc7FPnEO++lVhHWHFlGbQ&lt;/p&gt;
    &lt;p&gt;Which translates to DeviceClassNumber : 3 (iPad)&lt;/p&gt;
    &lt;p&gt;But how can you add this if the CacheData section is garbled?&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the right offset inside the libmobilegestalt.dylib&lt;/head&gt;
    &lt;p&gt;The /usr/lib/libMobileGestalt.dylib is not accessible from the sandbox, so we cannot read it via an Xcode-made app like before, but we can dlopen the libmobilegestalt.dylib and parse its segments. If we can do that, we can look for the encrypted key mtrAoWJ3gsq+I90ZnQ0vQw and find the offset.&lt;/p&gt;
    &lt;p&gt;Then we would know where to place our modified keys.&lt;/p&gt;
    &lt;p&gt;You can adapt the SwiftUI app from earlier to dlopen the dylib quite easily. Hereâ€™s what I came up with. Quick and dirty, based on Duyâ€™s older code from SparseBox:&lt;/p&gt;
    &lt;code&gt;func findCacheDataOffset() -&amp;gt; Int? {
    guard let handle = dlopen("/usr/lib/libMobileGestalt.dylib", RTLD_GLOBAL) else { return nil }
    defer { dlclose(handle) }
    
    var headerPtr: UnsafePointer&amp;lt;mach_header_64&amp;gt;?
    var imageSlide: Int = 0
    
    for i in 0..&amp;lt;_dyld_image_count() {
        if let imageName = _dyld_get_image_name(i),
           String(cString: imageName) == "/usr/lib/libMobileGestalt.dylib",
           let imageHeader = _dyld_get_image_header(i) {
            headerPtr = UnsafeRawPointer(imageHeader).assumingMemoryBound(to: mach_header_64.self)
            imageSlide = _dyld_get_image_vmaddr_slide(i)
            break
        }
    }
    
    guard let header = headerPtr else { return nil }
    
    var textCStringAddr: UInt64 = 0
    var textCStringSize: UInt64 = 0
    var constAddr: UInt64 = 0
    var constSize: UInt64 = 0
    var curCmd = UnsafeRawPointer(header).advanced(by: MemoryLayout&amp;lt;mach_header_64&amp;gt;.size)
    
    for _ in 0..&amp;lt;header.pointee.ncmds {
        let cmd = curCmd.assumingMemoryBound(to: load_command.self)
        
        if cmd.pointee.cmd == LC_SEGMENT_64 {
            let segCmd = curCmd.assumingMemoryBound(to: segment_command_64.self)
            let segName = String(data: Data(bytes: &amp;amp;segCmd.pointee.segname, count: 16), encoding: .utf8)?.trimmingCharacters(in: .controlCharacters) ?? ""
            var sectionPtr = curCmd.advanced(by: MemoryLayout&amp;lt;segment_command_64&amp;gt;.size)
            
            for _ in 0..&amp;lt;Int(segCmd.pointee.nsects) {
                let section = sectionPtr.assumingMemoryBound(to: section_64.self)
                let sectName = String(data: Data(bytes: &amp;amp;section.pointee.sectname, count: 16), encoding: .utf8)?.trimmingCharacters(in: .controlCharacters) ?? ""
                
                if segName == "__TEXT" &amp;amp;&amp;amp; sectName == "__cstring" {
                    textCStringAddr = section.pointee.addr
                    textCStringSize = section.pointee.size
                }
                
                if (segName == "__AUTH_CONST" || segName == "__DATA_CONST") &amp;amp;&amp;amp; sectName == "__const" {
                    constAddr = section.pointee.addr
                    constSize = section.pointee.size
                }
                
                sectionPtr = sectionPtr.advanced(by: MemoryLayout&amp;lt;section_64&amp;gt;.size)
            }
        }
        
        curCmd = curCmd.advanced(by: Int(cmd.pointee.cmdsize))
    }
    
    guard textCStringAddr != 0, constAddr != 0 else { return nil }
    
    let textCStringPtr = UnsafeRawPointer(bitPattern: Int(textCStringAddr) + imageSlide)!
    var keyPtr: UnsafePointer&amp;lt;CChar&amp;gt;?
    var offset = 0
    
    while offset &amp;lt; Int(textCStringSize) {
        let currentPtr = textCStringPtr.advanced(by: offset).assumingMemoryBound(to: CChar.self)
        let currentString = String(cString: currentPtr)
        
        if currentString == "mtrAoWJ3gsq+I90ZnQ0vQw" {
            keyPtr = currentPtr
            break
        }
        
        offset += currentString.utf8.count + 1
    }
    
    guard let keyPtr = keyPtr else { return nil }
    
    let constSectionPtr = UnsafeRawPointer(bitPattern: Int(constAddr) + imageSlide)!.assumingMemoryBound(to: UnsafeRawPointer.self)
    var structPtr: UnsafeRawPointer?
    
    for i in 0..&amp;lt;Int(constSize) / 8 {
        if constSectionPtr[i] == UnsafeRawPointer(keyPtr) {
            structPtr = UnsafeRawPointer(constSectionPtr.advanced(by: i))
            break
        }
    }
    
    guard let structPtr = structPtr else { return nil }
    
    let offsetMetadata = structPtr.advanced(by: 0x9a).assumingMemo&lt;/code&gt;
    &lt;p&gt;This will give you the offset for the DeviceClassNumber so now you can just write your new value to it.&lt;/p&gt;
    &lt;p&gt;For this, you can just modify Duyâ€™s Python script, which is based on Hana Kimâ€˜s original files.&lt;/p&gt;
    &lt;p&gt;I added something like this:&lt;/p&gt;
    &lt;code&gt;IPAD_KEYS = [
    "uKc7FPnEO++lVhHWHFlGbQ",
    "mG0AnH/Vy1veoqoLRAIgTA",
    "UCG5MkVahJxG1YULbbd5Bg",
    "ZYqko/XM5zD3XBfN5RmaXA",
    "nVh/gwNpy7Jv1NOk00CMrw",
    "qeaj75wk3HF4DwQ8qbIi7g"
]

def write_ipad_to_device_class_with_offset(self, mg_plist, offset):
        cache_data = mg_plist.get('CacheData')
        cache_extra = mg_plist.get('CacheExtra', {})
        
        if cache_data is None:
            self.log("[!] Error: CacheData not found in MobileGestalt", "error")
            return False
        
        if not isinstance(cache_data, bytes):
            cache_data = bytes(cache_data)
        
        if offset &amp;gt;= len(cache_data) - 8:
            self.log(f"[!] Error: Offset {offset} is beyond CacheData bounds ({len(cache_data)} bytes)", "error")
            return False
        
        cache_data_array = bytearray(cache_data)
        
        current_value = struct.unpack_from('&amp;lt;Q', cache_data_array, offset)[0]
        device_type = 'iPhone' if current_value == 1 else 'iPad' if current_value == 3 else 'Unknown'
        self.log(f"[i] Current DeviceClassNumber value: {current_value} ({device_type})", "info")
        
        struct.pack_into('&amp;lt;Q', cache_data_array, offset, 3)
        
        new_value = struct.unpack_from('&amp;lt;Q', cache_data_array, offset)[0]
        self.log(f"[i] New DeviceClassNumber value: {new_value} (iPad)", "success")
        
        mg_plist['CacheData'] = bytes(cache_data_array)
        
        for key in IPAD_KEYS:
            cache_extra[key] = 1
        
        mg_plist['CacheExtra'] = cache_extra
        
        self.log("[+] Successfully wrote iPad device class to MobileGestalt", "success")
        return True

def modify_mobile_gestalt(self, mg_file, output_file):
        try:
            with open(mg_file, 'rb') as f:
                mg_plist = plistlib.load(f)
            
            choice = self.operation_mode.get()
            
            if choice in [1, 2]:
                offset = None
                offset_str = self.offset_var.get().strip()
                
                if offset_str:
                    try:
                        if offset_str.startswith("0x") or offset_str.startswith("0X"):
                            offset = int(offset_str, 16)
                        else:
                            offset = int(offset_str)
                        self.log(f"[+] Using offset: {offset} (0x{offset:x})", "success")
                    except ValueError:
                        self.log("[!] Invalid offset format, skipping CacheData modification", "warning")
                        offset = None
                
                if choice == 1:
                    if offset is not None and self.write_ipad_to_device_class_with_offset(mg_plist, offset):
                        self.log("[+] iPad mode enabled", "success")
                    elif offset is None:
                        cache_extra = mg_plist.get('CacheExtra', {})
                        for key in IPAD_KEYS:
                            cache_extra[key] = 1
                        mg_plist['CacheExtra'] = cache_extra
                        self.log("[!] iPad mode enabled (CacheExtra only - may not fully work without CacheData)", "warning")
                    else:
                        self.log("[!] Failed to enable iPad mode", "error")
                        return False
                elif choice == 2:
                    if offset is not None and self.restore_iphone_device_class_with_offset(mg_plist, offset):
                        self.log("[+] iPhone mode restored", "success")
                    elif offset is None:
                        cache_extra = mg_plist.get('CacheExtra', {})
                        for key in IPAD_KEYS:
                            cache_extra.pop(key, None)
                        mg_plist['CacheExtra'] = cache_extra
                        self.log("[i] iPhone mode restored (CacheExtra only)", "warning")
                    else:
                        self.log("[!] Failed to restore iPhone mode", "error")
                        return False
            else:
                self.log("[i] Using file as-is", "info")
            
            with open(output_file, 'wb') as f:
                plistlib.dump(mg_plist, f)
            
            self.log(f"[+] SUCCESS: Modified MobileGestalt saved to: {output_file}", "success")
            return True
            
        except Exception as e:
            self.log(f"[!] Error modifying MobileGestalt: {e}", "error")
            return False
            &lt;/code&gt;
    &lt;p&gt;Thatâ€™s it. Thatâ€™s all the modifications I did to the original bl_sbx Python script from Duy.&lt;/p&gt;
    &lt;p&gt;Running this with python3 in a venv allowed me to change the device to iPad and enable iPad features.&lt;/p&gt;
    &lt;p&gt;The exploit doesnâ€™t have a great success rate, so you may need to try this again and again until it succeeds. Once it does, reboot the device. You should be able to access iPad features in Settings.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setting up the environment for Python3 on macOS&lt;/head&gt;
    &lt;p&gt;To properly run the Python script on recent macOS and install the dependencies, you must first set up a virtual environment. To do that, you need to run:&lt;/p&gt;
    &lt;code&gt;cd bl_sbx
python3 -m venv venv
source venv/bin/activate
pip install click requests packaging pymobiledevice3&lt;/code&gt;
    &lt;p&gt;Once the environment is set up and the prerequisites are installed, you can just run the script:&lt;/p&gt;
    &lt;code&gt;python3 run.py DEVICE UDID /path/to/MobileGestalt.plist&lt;/code&gt;
    &lt;p&gt;I used ideviceinfo, part of libimobiledevice, to get the device UDID, but itâ€™s also available in Finder, 3uTools on Windows, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;More iDevice Central Guides&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;iOS 17 Jailbreak RELEASED! How to Jailbreak iOS 17 with PaleRa1n&lt;/item&gt;
      &lt;item&gt;How to Jailbreak iOS 18.0 â€“ iOS 18.2.1 / iOS 18.3 With Tweaks&lt;/item&gt;
      &lt;item&gt;Download iRemovalRa1n Jailbreak (CheckRa1n for Windows)&lt;/item&gt;
      &lt;item&gt;Dopamine Jailbreak (Fugu15 Max) Release Is Coming Soon for iOS 15.0 â€“ 15.4.1 A12+&lt;/item&gt;
      &lt;item&gt;Cowabunga Lite For iOS 16.2 â€“ 16.4 Released in Beta! Install Tweaks and Themes Without Jailbreak&lt;/item&gt;
      &lt;item&gt;Fugu15 Max Jailbreak: All Confirmed Working Rootless Tweaks List&lt;/item&gt;
      &lt;item&gt;iOS 14.0 â€“ 16.1.2 â€“ All MacDirtyCow Tools IPAs&lt;/item&gt;
      &lt;item&gt;iOS Jailbreak Tools for All iOS Versions&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45950408</guid><pubDate>Mon, 17 Nov 2025 02:39:28 +0000</pubDate></item><item><title>Giving C a superpower: custom header file (safe_c.h)</title><link>https://hwisnu.bearblog.dev/giving-c-a-superpower-custom-header-file-safe_ch/</link><description>&lt;doc fingerprint="2c11dabd8e0b7208"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Giving C a Superpower: custom header file (safe_c.h)&lt;/head&gt;
    &lt;p&gt;The story of how I wrote a leak-free, thread-safe grep in C23 without shooting yourself in the foot, and how you can too!&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Let's be honest: most people have a love-hate relationship with C. We love its raw speed, its direct connection to the metal, and its elegant simplicity. But we hate its footguns, its dragons, the untamed beasts. The segfaults that appear from nowhere, the memory leaks that slowly drain the life from our applications, and the endless goto cleanup; chains that make our code look like a plate of spaghetti pasta.&lt;/p&gt;
    &lt;p&gt;This is the classic C curse: power without guardrails...at least that's the fear mongering mantra being said again and again. But is that still relevant in today's world with all the tools available for C devs like static analyzer and dynamic sanitizers? I've written about this here and here.&lt;/p&gt;
    &lt;p&gt;What if, with the help of the modern tools and a custom header file (600 loc), you could tame those footguns beasts? What if you could keep C's power but wrap it in a suit of modern armor? That's what the custom header file safe_c.h is for. It's designed to give C some safety and convenience features from C++ and Rust, and I'm using it to build a high-performance grep clone called cgrep as my test case.&lt;/p&gt;
    &lt;p&gt;By the end this article I hope it could provide the audience with the idea of C is super flexible and extensible, sort of "do whatever you want with it" kind of thing. And this is why C (and its close cousin: Zig) remain to be my favorite language to write programs in; it's the language of freedom!&lt;/p&gt;
    &lt;head rend="h1"&gt;safe_c.h&lt;/head&gt;
    &lt;p&gt;Is a custom C header file that takes features mainly from C++ and Rust and implements them into our C code ~ [write C code, get C++ and Rust features!]&lt;/p&gt;
    &lt;p&gt;It starts by bridging the gap between old and new C. C23 gave us &lt;code&gt;[[cleanup]]&lt;/code&gt; attributes, but in the real world, you need code that compiles on GCC 11 or Clang 18. safe_c.h detects your compiler and gives you the same RAII semantics everywhere. No more &lt;code&gt;#ifdef&lt;/code&gt; soup.&lt;/p&gt;
    &lt;code&gt;// The magic behind CLEANUP: zero overhead, maximum safety
#if defined(__STDC_VERSION__) &amp;amp;&amp;amp; __STDC_VERSION__ &amp;gt;= 202311L
#define CLEANUP(func) [[cleanup(func)]]
#else
#define CLEANUP(func) __attribute__((cleanup(func)))
#endif

// Branch prediction that actually matters in hot paths
#ifdef __GNUC__
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#else
#define LIKELY(x)   (x)
#define UNLIKELY(x) (x)
#endif
&lt;/code&gt;
    &lt;p&gt;Your cleanup code runs even if you return early, goto out, or panic. It's &lt;code&gt;finally&lt;/code&gt;, but for C.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Memory Management Beast: Slain with Smart Pointers (C++ feature)&lt;/head&gt;
    &lt;p&gt;The oldest, fiercest and most feared by devs: manual memory management.&lt;/p&gt;
    &lt;p&gt;Before: the highway path to leaks.&lt;lb/&gt; Forgetting a single &lt;code&gt;free()&lt;/code&gt; is a disaster. In cgrep, parsing command-line options the old way is a breeding ground for CVEs and its bestiary. You have to remember to free the memory on every single exit path, difficult for the undisciplined.&lt;/p&gt;
    &lt;code&gt;// The Old Way (don't do this)
char* include_pattern = NULL;
if (optarg) {
    include_pattern = strdup(optarg);
}
// ...200 lines later...
if (some_error) {
    if (include_pattern) free(include_pattern); // Did I free it? Did I??
    return 1;
}
// And remember to free it at *every* return path...
&lt;/code&gt;
    &lt;p&gt;After: memory that automatically cleans itself up.&lt;lb/&gt; UniquePtr is a "smart pointer" that owns a resource. When the UniquePtr variable goes out of scope, its resource is automatically freed. It's impossible to forget.&lt;/p&gt;
    &lt;p&gt;Here's the machinery inside &lt;code&gt;safe_c.h&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// The UniquePtr machinery: a struct + automatic cleanup
typedef struct {
    void* ptr;
    void (*deleter)(void*);
} UniquePtr;

#define AUTO_UNIQUE_PTR(name, ptr, deleter) \
    UniquePtr name CLEANUP(unique_ptr_cleanup) = UNIQUE_PTR_INIT(ptr, deleter)

static inline void unique_ptr_cleanup(UniquePtr* uptr) {
    if (uptr &amp;amp;&amp;amp; uptr-&amp;gt;ptr &amp;amp;&amp;amp; uptr-&amp;gt;deleter) {
        uptr-&amp;gt;deleter(uptr-&amp;gt;ptr);
        uptr-&amp;gt;ptr = NULL;
    }
}
&lt;/code&gt;
    &lt;p&gt;And here's how cgrep uses it. The cleanup is automatic, even if errors happen:&lt;/p&gt;
    &lt;code&gt;// In cgrep, we use this for command-line arguments
AUTO_UNIQUE_PTR(include_pattern_ptr, NULL, options_string_deleter);

// When we get a new pattern, the old one is automatically freed!
unique_ptr_delete(&amp;amp;include_pattern_ptr);
include_pattern_ptr.ptr = strdup(optarg);
// No leaks, even if an error happens later!
&lt;/code&gt;
    &lt;p&gt;Sharing Safely with SharedPtr&lt;/p&gt;
    &lt;p&gt;Before: manual, bug-prone reference counting.&lt;lb/&gt; You'd have to implement reference counting by hand, creating a complex and fragile system where a single mistake leads to a leak or a use-after-free bug.&lt;/p&gt;
    &lt;code&gt;// The old way of manual reference counting
typedef struct {
    MatchStore* store;
    int ref_count;
    pthread_mutex_t mutex;
} SharedStore;

void release_store(SharedStore* s) {
    pthread_mutex_lock(&amp;amp;s-&amp;gt;mutex);
    s-&amp;gt;ref_count--;
    bool is_last = (s-&amp;gt;ref_count == 0);
    pthread_mutex_unlock(&amp;amp;s-&amp;gt;mutex);

    if (is_last) {
        match_store_deleter(s-&amp;gt;store);
        free(s);
    }
}
&lt;/code&gt;
    &lt;p&gt;After: automated reference counting.&lt;lb/&gt; SharedPtr automates this entire process. The last thread to finish using the object automatically triggers its destruction. The machinery:&lt;/p&gt;
    &lt;code&gt;// The SharedPtr machinery: reference counting without the boilerplate
typedef struct {
    void* ptr;
    void (*deleter)(void*);
    size_t* ref_count;
} SharedPtr;

#define AUTO_SHARED_PTR(name) \
    SharedPtr name CLEANUP(shared_ptr_cleanup) = {.ptr = NULL, .deleter = NULL, .ref_count = NULL}

static inline void shared_ptr_cleanup(SharedPtr* sptr) {
    shared_ptr_delete(sptr); // Decrement and free if last reference
}
&lt;/code&gt;
    &lt;p&gt;The usage is clean and safe. No more manual counting.&lt;/p&gt;
    &lt;code&gt;// In our thread worker context, multiple threads access the same results store
typedef struct {
    // ...
    SharedPtr store;  // No more worrying about who frees this!
    SharedPtr file_counts;
    // ...
} FileWorkerContext;

// In main(), we create it once and share it safely
// SharedPtr: Reference-counted stores for thread-safe sharing
SharedPtr store_shared = {0};
shared_ptr_init(&amp;amp;store_shared, store_ptr.ptr, match_store_deleter);
// Pass to threads: ctx-&amp;gt;store = shared_ptr_copy(&amp;amp;store_shared);
// ref-count increments automatically; last thread out frees it.
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Buffer Overflow Beast: Contained with Vectors and Views (C++ feature)&lt;/head&gt;
    &lt;p&gt;Dynamically growing arrays in C is a horror show.&lt;/p&gt;
    &lt;p&gt;Before: the realloc dance routine.&lt;lb/&gt; You have to manually track capacity and size, and every realloc risks fragmenting memory or failing, requiring careful error handling for every single element you add.&lt;/p&gt;
    &lt;code&gt;// The old way: manual realloc is inefficient and complex
MatchEntry** matches = NULL;
size_t matches_count = 0;
size_t matches_capacity = 0;

for (/*...each match...*/) {
    if (matches_count &amp;gt;= matches_capacity) {
        matches_capacity = (matches_capacity == 0) ? 8 : matches_capacity * 2;
        MatchEntry** new_matches = realloc(matches, matches_capacity * sizeof(MatchEntry*));
        if (!new_matches) {
            free(matches); // Don't leak!
            /* handle error */
        }
        matches = new_matches;
    }
    matches[matches_count++] = current_match;
}
&lt;/code&gt;
    &lt;p&gt;After: a type-safe, auto-growing vector.&lt;lb/&gt; safe_c.h generates an entire type-safe vector for you. It handles allocation, growth, and cleanup automatically. The magic that generates the vector:&lt;/p&gt;
    &lt;code&gt;// The magic that generates a complete vector type from a single line
#define DEFINE_VECTOR_TYPE(name, type) \
    typedef struct { \
        Vector base; \
        type* data; \
    } name##Vector; \
    \
    static inline bool name##_vector_push_back(name##Vector* vec, type value) { \
        bool result = vector_push_back(&amp;amp;vec-&amp;gt;base, &amp;amp;value); \
        vec-&amp;gt;data = (type*)vec-&amp;gt;base.data; /* Sync pointer after potential realloc */ \
        return result; \
    } \
    \
    static inline bool name##_vector_reserve(name##Vector* vec, size_t new_capacity) { \
        bool result = vector_reserve(&amp;amp;vec-&amp;gt;base, new_capacity); \
        vec-&amp;gt;data = (type*)vec-&amp;gt;base.data; /* Sync pointer after potential realloc */ \
        return result; \
    } \


    /* more helper functions not outlined here */

// And the underlying generic Vector implementation
typedef struct {
    size_t size;
    size_t capacity;
    void* data;
    size_t element_size;
} Vector;
&lt;/code&gt;
    &lt;p&gt;Using it in cgrep is simple and safe. The vector cleans itself up when it goes out of scope.&lt;/p&gt;
    &lt;code&gt;// Type-safe vector for collecting matches
DEFINE_VECTOR_TYPE(MatchEntryPtr, MatchEntry*)

AUTO_TYPED_VECTOR(MatchEntryPtr, all_matches_vec);
MatchEntryPtr_vector_reserve(&amp;amp;all_matches_vec, store-&amp;gt;total_matches);

// Pushing elements is safe and simple
for (MatchEntry* entry = store-&amp;gt;buckets[i]; entry; entry = entry-&amp;gt;next) {
    MatchEntryPtr_vector_push_back(&amp;amp;all_matches_vec, entry);
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Views: Look, Don't Touch (or malloc) - C++ feature&lt;/head&gt;
    &lt;p&gt;Before: needless allocations.&lt;lb/&gt; To handle a substring or a slice of an array, you'd often malloc a new buffer and copy the data into it, which is incredibly slow in a tight loop.&lt;/p&gt;
    &lt;code&gt;// The old way: allocating a new string just to get a substring
const char* line = "this is a long line of text";
char* pattern = "long line";
// To pass just the pattern to a function, you might do this:
char* sub = malloc(strlen(pattern) + 1);
strncpy(sub, pattern, strlen(pattern) + 1);
// ... use sub ...
free(sub); // And hope you remember this free call
&lt;/code&gt;
    &lt;p&gt;After: zero-cost, non-owning views.&lt;lb/&gt; A StringView or a Span is just a pointer and a length. It's a non-owning reference that lets you work with slices of data without any allocation. The definitions are pure and simple:&lt;/p&gt;
    &lt;code&gt;// The StringView and Span definitions: pure, simple, zero-cost
typedef struct {
    const char* data;
    size_t size;
} StringView;

typedef struct {
    void* data;
    size_t size;
    size_t element_size;
} Span;
&lt;/code&gt;
    &lt;p&gt;In cgrep, the search pattern becomes a StringView, avoiding allocation entirely.&lt;/p&gt;
    &lt;code&gt;// Our options struct holds a StringView, not a char*
typedef struct {
    StringView pattern; // Clean, simple, and safe
    // ...
} GrepOptions;

// Initializing it is a piece of cake
options.pattern = string_view_init(argv[optind]);
&lt;/code&gt;
    &lt;p&gt;For safe array access, Span provides a bounds-checked window into existing data.&lt;/p&gt;
    &lt;code&gt;// safe_c.h
#define DEFINE_SPAN_TYPE(name, type) \
    typedef struct { \
        type* data; \
        size_t size; \
    } name##Span; \
    \
    static inline name##Span name##_span_init(type* data, size_t size) { \
        return (name##Span){.data = data, .size = size}; \
    } \
    \

    /* other helper functions not outlined here */
&lt;/code&gt;
    &lt;code&gt;// Span: Type-safe array slices for chunk processing
DEFINE_SPAN_TYPE(LineBuffer, char)
LineBufferSpan input_span = LineBuffer_span_init((char*)start, len);

for (size_t i = 0; i &amp;lt; LineBuffer_span_size(&amp;amp;input_span); i++) {
    char* line = LineBuffer_span_at(&amp;amp;input_span, i); // asserts i &amp;lt; span.size
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Error-Handling &lt;code&gt;goto&lt;/code&gt; Beast: Replaced with Results (Rust feature) and RAII (C++ feature)&lt;/head&gt;
    &lt;p&gt;C's error handling is notoriously messy.&lt;/p&gt;
    &lt;p&gt;Before: goto cleanup spaghetti carbonara.&lt;lb/&gt; Functions return special values like -1 or NULL, and you have to check errno. This leads to deeply nested if statements and a single goto cleanup; label that has to handle every possible failure case.&lt;/p&gt;
    &lt;code&gt;// The old way: goto cleanup
int do_something(const char* path) {
    int fd = open(path, O_RDONLY);
    if (fd &amp;lt; 0) {
        return -1; // Error
    }

    void* mem = malloc(1024);
    if (!mem) {
        close(fd); // Manual cleanup
        return -1;
    }
    
    // ... do more work ...

    free(mem);
    close(fd);
    return 0; // Success
}
&lt;/code&gt;
    &lt;p&gt;After: explicit, type-safe result.&lt;lb/&gt; Inspired by Rust, Result&lt;/p&gt;
    &lt;code&gt;// The Result type machinery: tagged unions for success/failure
typedef enum { RESULT_OK, RESULT_ERROR } ResultStatus;

#define DEFINE_RESULT_TYPE(name, value_type, error_type) \
    typedef struct { \
        ResultStatus status; \
        union { \
            value_type value; \
            error_type error; \
        }; \
    } Result##name;
&lt;/code&gt;
    &lt;p&gt;Handling errors becomes easy. You can't accidentally use an error as a valid value.&lt;/p&gt;
    &lt;code&gt;// Define a Result for file operations
DEFINE_RESULT_TYPE(FileOp, i32, const char*)

// Our function now returns a clear Result
static ResultFileOp submit_stat_request_safe(...) {
    // ...
    if (!sqe) {
        return RESULT_ERROR(FileOp, "Could not get SQE for stat");
    }
    return RESULT_OK(FileOp, 0);
}

// And handling it is clean
ResultFileOp result = submit_stat_request_safe(path, &amp;amp;ring, &amp;amp;pending_ops);
if (!RESULT_IS_OK(result)) {
    fprintf(stderr, "Error: %s\n", RESULT_UNWRAP_ERROR(result));
}
&lt;/code&gt;
    &lt;p&gt;This is powered by RAII. The &lt;code&gt;CLEANUP&lt;/code&gt; attribute ensures resources are freed no matter how a function exits.&lt;/p&gt;
    &lt;code&gt;#define AUTO_MEMORY(name, size) \
    void* name CLEANUP(memory_cleanup) = malloc(size)

// DIR pointers are automatically closed, even on an early return.
DIR* dir CLEANUP(dir_cleanup) = opendir(req-&amp;gt;path);
if (!dir) {
    return RESULT_ERROR(FileOp, "Failed to open dir"); // dir_cleanup is NOT called
}
if (some_condition) {
    return RESULT_OK(FileOp, 0); // closedir() is called automatically HERE!
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Assumption Beast: Challenged with Contracts and Safe Strings&lt;/head&gt;
    &lt;p&gt;Before: &lt;code&gt;assert()&lt;/code&gt; and pray.&lt;lb/&gt; A standard &lt;code&gt;assert(ptr != NULL)&lt;/code&gt; is good, but when it fails, the message is generic. You know the condition failed, but not the context or why it was important.&lt;/p&gt;
    &lt;p&gt;After: self-documenting contracts.&lt;code&gt;requires()&lt;/code&gt; and &lt;code&gt;ensures()&lt;/code&gt; make function contracts explicit. The failure messages tell you exactly what went wrong.
The contract macros:&lt;/p&gt;
    &lt;code&gt;#define requires(cond) assert_msg(cond, "Precondition failed")
#define ensures(cond) assert_msg(cond, "Postcondition failed")

#define assert_msg(cond, msg) /* ... full implementation ... */
&lt;/code&gt;
    &lt;p&gt;This turns assertions into executable documentation:&lt;/p&gt;
    &lt;code&gt;// Preconditions that document and enforce contracts
static inline bool arena_create(Arena* arena, size_t size)
{
    requires(arena != NULL);  // Precondition: arena must not be null
    requires(size &amp;gt; 0);       // Precondition: size must be positive
    
    // ... implementation ...
    
    ensures(arena-&amp;gt;buffer != NULL);  // Postcondition: buffer is allocated
    ensures(arena-&amp;gt;size == size);    // Postcondition: size is set correctly
    
    return true;
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;strcpy()&lt;/code&gt; is a Security Vulnerability&lt;/head&gt;
    &lt;p&gt;Before: buffer overflows.&lt;lb/&gt; strcpy has no bounds checking. It's the source of countless security holes. &lt;code&gt;strncpy&lt;/code&gt; is little better, as it might not null-terminate the destination string.&lt;/p&gt;
    &lt;code&gt;// The old, dangerous way
char dest[20];
const char* src = "This is a very long string that will overflow the buffer";
strcpy(dest, src); // Undefined behavior! Stack corruption!
&lt;/code&gt;
    &lt;p&gt;After: safe, bounds-checked operations.&lt;lb/&gt; safe_c.h provides alternatives that check bounds and return a success/failure status. No surprises. The safe implementation:&lt;/p&gt;
    &lt;code&gt;// The safe string operations: bounds checking that can't be ignored
static inline bool safe_strcpy(char* dest, size_t dest_size, const char* src) {
    if (!dest || dest_size == 0 || !src) return false;
    size_t src_len = strlen(src);
    if (src_len &amp;gt;= dest_size) return false;
    memcpy(dest, src, src_len + 1);
    return true;
}
&lt;/code&gt;
    &lt;p&gt;In cgrep, this prevents path buffer overflows cleanly:&lt;/p&gt;
    &lt;code&gt;// Returns bool, not silent truncation
if (!safe_strcpy(req-&amp;gt;path, PATH_MAX, path)) {
    free(req);
    return RESULT_ERROR(FileOp, "Path is too long");
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Concurrency: Mutexes That Unlock Themselves (Rust feature)&lt;/head&gt;
    &lt;p&gt;Before: leaked locks and deadlocks.&lt;lb/&gt; Forgetting to unlock a mutex, especially on an error path, is a catastrophic bug that causes your program to deadlock.&lt;/p&gt;
    &lt;code&gt;// The Buggy Way
pthread_mutex_lock(&amp;amp;mutex);
if (some_error) {
    return; // Oops, mutex is still locked! Program will deadlock.
}
pthread_mutex_unlock(&amp;amp;mutex);
&lt;/code&gt;
    &lt;p&gt;After: RAII-based locks.&lt;lb/&gt; Using the same CLEANUP attribute, we can ensure a mutex is always unlocked when the scope is exited. This bug becomes impossible to write.&lt;/p&gt;
    &lt;code&gt;// With a cleanup function, unlocking is automatic.
void mutex_unlock_cleanup(pthread_mutex_t** lock) {
    if (lock &amp;amp;&amp;amp; *lock) pthread_mutex_unlock(*lock);
}

// RAII lock guard via cleanup attribute
pthread_mutex_t my_lock;
pthread_mutex_t* lock_ptr CLEANUP(mutex_unlock_cleanup) = &amp;amp;my_lock;
pthread_mutex_lock(lock_ptr);

if (some_error) {
    return; // Mutex is automatically unlocked here!
}
&lt;/code&gt;
    &lt;p&gt;Simple wrappers also clean up the boilerplate of managing threads:&lt;/p&gt;
    &lt;code&gt;// The concurrency macros: spawn and join without boilerplate
#define SPAWN_THREAD(name, func, arg) \
    thrd_t name; \
    thrd_create(&amp;amp;name, (func), (arg))

#define JOIN_THREAD(name) \
    thrd_join(name, NULL)
&lt;/code&gt;
    &lt;p&gt;And in cgrep:&lt;/p&gt;
    &lt;code&gt;// Thread pool spawn without boilerplate
SPAWN_THREAD(workers[i], file_processing_worker, &amp;amp;contexts[i]);
JOIN_THREAD(workers[i]); // No manual pthread_join() error handling
&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance: Safety at -O2, Not -O0&lt;/head&gt;
    &lt;p&gt;Safety doesn't mean slow. The UNLIKELY() macro tells the compiler which branches are cold, adding zero overhead in hot paths.&lt;/p&gt;
    &lt;code&gt;#ifdef __GNUC__
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#else
#define LIKELY(x)   (x)
#define UNLIKELY(x) (x)
#endif
&lt;/code&gt;
    &lt;p&gt;The real win is in the fast paths:&lt;/p&gt;
    &lt;code&gt;// In hot allocation path: branch prediction
if (UNLIKELY(store-&amp;gt;local_buffer_sizes[thread_id] &amp;gt;= LOCAL_BUFFER_CAPACITY)) {
    match_store_flush_buffer(store, thread_id); // Rarely taken
}

// In match checking: likely path first
if (!options-&amp;gt;case_insensitive &amp;amp;&amp;amp; options-&amp;gt;fixed_string) {
    // Most common case: fast path with no branches
    const char* result = strstr(line, options-&amp;gt;pattern.data);
    return result != NULL;
}
&lt;/code&gt;
    &lt;p&gt;The above is similar to what a PGO (Profile Guided Optimization) would have.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Final Word: C That Doesn't Blow Your Own Foot!&lt;/head&gt;
    &lt;p&gt;This is what main() looks like when you stop fighting the language:&lt;/p&gt;
    &lt;code&gt;int main(int argc, char* argv[]) {
    initialize_simd();
    output_buffer_init(); // Auto-cleanup on exit
    
    GrepOptions options = {0};
    AUTO_UNIQUE_PTR(include_pattern_ptr, NULL, options_string_deleter);
    
    // ... parse args with getopt_long ...
    
    AUTO_UNIQUE_PTR(store_ptr, NULL, match_store_deleter);
    SharedPtr store_shared = {0};
    if (need_match_store) {
        store_ptr.ptr = malloc(sizeof(ConcurrentMatchStore));
        if (!store_ptr.ptr || !match_store_create(store_ptr.ptr, hash_capacity, 1000)) {
            return 1; // All allocations cleaned up automatically
        }
        shared_ptr_init(&amp;amp;store_shared, store_ptr.ptr, match_store_deleter);
    }
    
    // Process files with thread pool...
    
cleanup: // Single cleanup label needed -- RAII handles the rest
    output_buffer_destroy(); // Flushes and destroys
    return 0;
}
&lt;/code&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In the end, cgrep is 2,300 lines of C. Without safe_c.h, it would have required over 50 manual free() calls ~ a recipe for leaks and segfaults. With the custom header file, it's 2,300 lines that compile to the same assembly, run just as fast, and are fundamentally safer.&lt;/p&gt;
    &lt;p&gt;This proves that the best abstraction is the one you don't pay for and can't forget to use. It enables a clear and powerful development pattern: validate inputs at the boundary, then unleash C's raw speed on the core logic. You get all the power of C without the infamous self-inflicted footgun wounds.&lt;/p&gt;
    &lt;p&gt;C simplicity makes writing programs with it becomes fun, however there are ways to make it both fun and safe..just like using condoms, you know?&lt;/p&gt;
    &lt;p&gt;This post has gotten too long for comfort, but I have one final food for thought for you the readers: after all these guard rails, what do you think of cgrep's performance? Check the screenshots below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;grep bench on recursive directories&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;grep bench on single large file NOTE: make sure you check the memory usage comparison between cgrep and ripgrep&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the next article, I will discuss how I built cgrep, the design I chose for it, why and how cgrep managed to be a couple of times faster than ripgrep (more than 2x faster in the recursive directory bench) while being super efficient with resource usage (20x smaller memory footprint in the single large file bench).&lt;/p&gt;
    &lt;p&gt;It's gonna be a lot of fun! Cheers!&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments section here&lt;/head&gt;
    &lt;p&gt;If you enjoyed this post, click the little up arrow chevron on the bottom left of the page to help it rank in Bear's Discovery feed and if you got any questions or anything, please use the comments section.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45952428</guid><pubDate>Mon, 17 Nov 2025 10:40:55 +0000</pubDate></item><item><title>Are you stuck in movie logic?</title><link>https://usefulfictions.substack.com/p/are-you-stuck-in-movie-logic</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45952890</guid><pubDate>Mon, 17 Nov 2025 12:06:37 +0000</pubDate></item><item><title>FreeMDU: Open-source Miele appliance diagnostic tools</title><link>https://github.com/medusalix/FreeMDU</link><description>&lt;doc fingerprint="af8dedd4595f4be1"&gt;
  &lt;main&gt;
    &lt;p&gt;The FreeMDU project provides open hardware and software tools for communicating with Miele appliances via their optical diagnostic interface. It serves as a free and open alternative to the proprietary Miele Diagnostic Utility (MDU) software, which is only available to registered service technicians.&lt;/p&gt;
    &lt;p&gt;Most Miele devices manufactured after 1996 include an optical infrared-based diagnostic interface, hidden behind one of the indicator lights on the front panel. On older appliances, this interface is marked by a Program Correction (PC) label.&lt;/p&gt;
    &lt;p&gt;Until now, communication with this interface required an expensive infrared adapter sold exclusively by Miele, along with their closed-source software. The goal of FreeMDU is to make this interface accessible to everyone for diagnostic and home automation purposes.&lt;/p&gt;
    &lt;p&gt;The project is split into three main components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protocol: core protocol library and device implementations&lt;/item&gt;
      &lt;item&gt;TUI: terminal-based device diagnostic and testing tool&lt;/item&gt;
      &lt;item&gt;Home: communication adapter firmware with MQTT integration for Home Assistant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details about the proprietary diagnostic interface and the reverse-engineering process behind this project can be found in this blog post.&lt;/p&gt;
    &lt;p&gt;Caution&lt;/p&gt;
    &lt;p&gt;This project is highly experimental and can cause permanent damage to your Miele devices if not used responsibly. Proceed at your own risk.&lt;/p&gt;
    &lt;p&gt;When a connection is established via the diagnostic interface, the appliance responds with its software ID, a 16-bit number that uniquely identifies the firmware version running on the device's microcontroller. However, this ID does not directly correspond to a specific model or board type, so it's impossible to provide a comprehensive list of supported models.&lt;/p&gt;
    &lt;p&gt;The following table lists the software IDs and device/board combinations that have been confirmed to work with FreeMDU:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Software ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Device&lt;/cell&gt;
        &lt;cell role="head"&gt;Board&lt;/cell&gt;
        &lt;cell role="head"&gt;Microcontroller&lt;/cell&gt;
        &lt;cell role="head"&gt;Optical interface location&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;360&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 223-A&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38078MC-065FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;ðŸŸ¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;419&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 206&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M37451MC-804FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;ðŸŸ¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;605&lt;/cell&gt;
        &lt;cell&gt;G 651 I PLUS-3&lt;/cell&gt;
        &lt;cell&gt;EGPL 542-C&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38027M8&lt;/cell&gt;
        &lt;cell&gt;Salt (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;ðŸŸ¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;629&lt;/cell&gt;
        &lt;cell&gt;W 2446&lt;/cell&gt;
        &lt;cell&gt;EDPL 126-B&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38079MF-308FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;ðŸŸ¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If your appliance is not listed here but has a model number similar to one of the above, it might already be compatible. In all other cases, determining the software ID is the first step toward adding support for new devices.&lt;/p&gt;
    &lt;p&gt;Details for adding support for new devices will be provided soon.&lt;/p&gt;
    &lt;p&gt;Before using any FreeMDU components, make sure you have the Rust toolchain installed on your system.&lt;/p&gt;
    &lt;p&gt;Next, you'll need to build a communication adapter to interface with your Miele device. Once the adapter is ready, choose the appropriate use case from the options below:&lt;/p&gt;
    &lt;p&gt;If you want to repair or test your appliance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the TUI application on your desktop computer.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to integrate your appliance into Home Assistant or another home automation system:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Flash the home firmware in standalone mode onto your communication adapter and attach it to your device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to develop your own software to communicate with Miele devices:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use the protocol crate to implement your custom software.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is an independent, open-source project and is not affiliated with, endorsed by, or sponsored by Miele &amp;amp; Cie. KG or its affiliates. All product names and trademarks are the property of their respective owners. References to Miele appliances are for descriptive purposes only and do not imply any association with Miele.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953452</guid><pubDate>Mon, 17 Nov 2025 13:40:40 +0000</pubDate></item><item><title>The time has finally come for geothermal energy</title><link>https://www.newyorker.com/magazine/2025/11/24/why-the-time-has-finally-come-for-geothermal-energy</link><description>&lt;doc fingerprint="254a117fb8a4a7aa"&gt;
  &lt;main&gt;
    &lt;p&gt;When I arrived in ReykjavÃ­k, Iceland, last March, a gravel barrier, almost thirty feet at its highest point, had been constructed to keep lava from the Reykjanes volcano from inundating a major geothermal power station not far from downtown. So far, it had worked, but daily volcano forecasts were being broadcast on a small television at the domestic airport where I was waiting to take a short flight to Akureyri, a town on the north coast about an hourâ€™s drive from one of the countryâ€™s oldest geothermal plants, the Krafla Geothermal Station. Until the early nineteen-seventies, Iceland relied on imported fossil fuels for nearly three-quarters of its energy. The resources of the countryâ€”a landscape of hot springs, lava domes, and bubbling mud potsâ€”were largely untapped. â€œIn the past, people here in the valley lacked most things now considered essential to human life, except for a hundred thousand million tons of boiling-hot water,â€ the Icelandic Nobelist HalldÃ³r Laxness wrote in â€œA Parish Chronicle,â€ his 1970 novel. â€œFor a hundred thousand years this water, more valuable than all coal mines, ran in torrents out to sea.â€ The oil crisis of 1973, when prices more than tripled, proved a useful emergency. Among other efforts to develop local energy, public-investment funds provided loans for geothermal projects, whose upfront costs were considerable. By the early eighties, almost all the countryâ€™s homes were heated geothermally; in ReykjavÃ­k, a subterranean geothermal-powered system is in place to melt snow and ice off sidewalks and roads. Today, more than a quarter of the countryâ€™s electricity comes from geothermal sources, a higher proportion than in almost any other nation. Most of the rest is from hydropower.&lt;/p&gt;
    &lt;p&gt;In some ways, the process of harnessing geothermal energy is simple. The deeper you dig, the hotter the temperatures get. For direct heating, you dig relatively shallow wells (typically several hundred metres deep), to access natural reservoirs of hot water or steam, which can be piped into a structure. For electricity, wells are dug farther down, to where temperatures are above a hundred and fifty degrees Celsius. (In Iceland, this temperature is reached at around one thousand to two thousand metres deep.) Pressurized steam spins a turbine that in turn spins a generator. Thermal energy (steam) is translated into mechanical energy (the spinning turbine), which is translated into electrical energy (via the generator). Geothermal energy is essentially carbon-free, it is available at any time of day and in any weather, and it leaves a smallâ€”albeit very deepâ€”footprint on the landscape.&lt;/p&gt;
    &lt;p&gt;In 2008, Icelandâ€™s three largest energy companies collaborated on a research project to drill down even farther, at a site near Krafla, for steam that was even hotter, some four hundred degrees Celsius. Such â€œsupercriticalâ€ steam is water that is so hot and pressurized that it has passed into a fourth state, beyond gas. The hotter a well the better, typically: it will produce more energy more efficiently. The Iceland Deep Drilling Project (I.D.D.P.) engineers had planned to dig down some four kilometresâ€”but their drill got stuck at around two kilometres. Bits of black glass shot up from the well. After some disbelief, the team concluded that they had hit magma. This oops-ing into magma was at first received as â€œvery bad news,â€ Bjarni Palsson, a chief project manager on the I.D.D.P. and now an executive vice-president of the energy company Landsvirkjun, told me. Many people thought that drilling into magma might trigger a volcanic eruption. â€œThen we started to see: What actually do we have here?â€ Palsson said. They put a wellhead on their work to measure the flow rates of steam. â€œWhat happened next was remarkable,â€ Palsson continued. The magma was about nine hundred degrees Celsius. The steam flow was such that it could produce ten times more energy than a regular well. They had created the hottest and most powerful geothermal well in the world.&lt;/p&gt;
    &lt;p&gt;There was no security check before boarding the plane. I was told by one of my companions, Hilmar MÃ¡r Einarsson, a youthful project manager with Landsvirkjun, that people sometimes stowed their hunting rifles in the overhead luggage compartments. On the drive from Akureyri to Krafla, we passed Lake MÃ½vatn, home to a kind of arctic char that lives only there. We also passed Icelandic horses, a diminutive breed famed for its distinctive gaits: in addition to walking, trotting, and galloping, it has a â€œflying paceâ€ and a rhythmic four-beat gait known as tÃ¶lt. Amid the expansive greens and yellows of northeast Iceland, we arrived at the Krafla Geothermal Station, where steam has been spinning two Mitsubishi turbines continuously for decades.&lt;/p&gt;
    &lt;p&gt;The station provides power to commercial buildings and heating to homes in the district. A rust-red building in the shape of a giant barn stood across from silvery cooling towers capped by cloud-white steam. Construction began in 1974 but took four years to finish, working around the Krafla fires, a series of volcanic eruptions that went on for years. (In Icelandic folklore, the region is where the Devil landed after being expelled from Heaven.) Around the station is a volcanic valley of green vegetation and basalt rock, with patches of snow. The wellhouses appeared as igloo-size aluminum geodesic domes; like the main power station, they were rust red. Einarsson opened one for me. Visible within was a thick horizontal pipe joined to a vertical one, with what looked like a shipâ€™s steering wheel attached. Not visible was the well itself, which extended belowground like a long metal straw. (Kraflaâ€™s geothermal wells are about seven inches across, notably narrower than many oil and gas wells.) â€œSome wells last twenty years, some last twoâ€”you canâ€™t know for certain,â€ Einarsson explained. The temperature and permeability of the rock, as well as the amount of fluid flowing across it, affect a wellâ€™s performance. Also, Iceland has myths about â€œhidden peopleâ€â€”huldufÃ³lk. It is said that building on their land brings bad luck, so thereâ€™s that, too.&lt;/p&gt;
    &lt;p&gt;We stopped by the stationâ€™s canteen, taking our shoes off before entering. Lunch had ended, but there was homemade apple cake, dried apricots, and skyr available. Workmen in neon-yellow suits, who had traded their boots for slippers, were having tea. Einarsson then took me to the I.D.D.P. site, not far from the Krafla plant, where a sign marked a snow-covered depression about the size of a modest pond. Compared with the turbines and steam towers and the idyllic orderliness of the canteen, the site was underwhelming. Two years after the well was dug, the extreme pressure and heat began to corrode the metal casing of the well itself. Black smoke poured out each time the well was reopened. Soon, it had to be shut down permanently. In 2017, another research well, I.D.D.P.-2, was drilled down four and a half kilometres, where temperatures reached at least four hundred and twenty-six degrees Celsiusâ€”but this time the well failed after only six months. â€œOne thing we learned is that you donâ€™t open and close and open and close the wellâ€”you just leave it open,â€ Palsson had told me, explaining that such actions made the well more brittle.&lt;/p&gt;
    &lt;p&gt;Landsvirkjun, which had paid for most of the I.D.D.P. work, decided that it needed financial support to drill more exploratory wells. â€œWe said, â€˜Weâ€™re just a small energy company in Iceland,â€™ â€ Palsson told me. But it made its research available to the international scientific community, and there has been intermittent interest from the U.K., Germany, Canada, and New Zealand. â€œThatâ€™s where we are now, trying to fund it as a science project that can also benefit the energy industry,â€ Palsson said.&lt;/p&gt;
    &lt;p&gt;Driving back to the airport, we saw snow ptarmigans and cairns of black stones marking trails that stretched beyond view. Icelandâ€™s transition into a country powered nearly completely by renewables can seem fantastical, and the landscape furthers this impression. Because Iceland is singular in so many waysâ€”that lonely arctic-char species! those small horses with their tÃ¶lt!â€”you can get the feeling that geothermal energy is a niche endeavor, as opposed to one that is technically and economically feasible in places where volcanic eruptions arenâ€™t part of the daily forecast. But that feeling is outdated and misleading.&lt;/p&gt;
    &lt;p&gt;Geothermal is underdeveloped, and its upfront costs can be high, but itâ€™s always on and, once itâ€™s set up, it is cheap and enduring. The dream of geothermal energy is to meet humanityâ€™s energy demands affordably, without harnessing horses for horsepower, slaughtering whales for their oil, or burning fossil fuels. The planetâ€™s heat could be used to pasteurize milk or heat dorm rooms or light up a baseball stadium for a night game.&lt;/p&gt;
    &lt;p&gt;At more than five thousand degrees Celsius, the Earthâ€™s core is roughly as hot as the surface of the sun. At the Earthâ€™s surface, the temperature is about fourteen degrees. But in some places, like Iceland, the ground underfoot is much warmer. Hot springs, geysers, and volcanoes are surface-level signs of the Earthâ€™s inferno. Danteâ€™s description of Hell is said by some to have been inspired by the landscape of sulfurous steam plumes found in Devilâ€™s Valley in Tuscany.&lt;/p&gt;
    &lt;p&gt;Snow monkeys and humans have been using Earth-heated waters as baths for ages. In the Azores, a local dish, cozido de las furnas, is cooked by burying a clay pot in hot volcanic soil; in Iceland, bread is still sometimes baked this way. The first geothermal power generator was built in Devilâ€™s Valley, in 1904, by Prince Piero Ginori Conti of Trevignano, who had been extracting borax from the area and thought to make use of the steam emerging from the mining borehole. The generator initially powered five light bulbs. Not long afterward, it powered central Italyâ€™s railway system and a few villages. The geothermal complex is still in operation today, providing one to two per cent of Italyâ€™s energy. In the United States, the first geothermal plant was built in 1921, in Northern California, in a geyser-filled area that a surveyor described as the gates of Hell. That plant powered a nearby resort hotel and is also still in use.&lt;/p&gt;
    &lt;p&gt;There arenâ€™t gates of Hell just anywhere. A kilometre below ground in Kamchatka is considerably hotter than a kilometre below ground in Kansas. There is also readily accessible geothermal energy in Kenya (where it provides almost fifty per cent of the countryâ€™s energy), New Zealand (about twenty per cent), and the Philippines (about fifteen per cent)â€”all volcanic areas along tectonic rifts. But in less Hadean landscapes the costs and uncertainties of drilling deep in search of sufficient heat have curtailed development. This partly explains why, in the field of clean energy, geothermal is often either not on the list or mentioned under the rubric of â€œother.â€ For decades, both private and government investment in geothermal energy was all but negligible.&lt;/p&gt;
    &lt;p&gt;That has now changed. In the past five years, in North America, more than a billion and a half dollars have gone into geothermal technologies. This is a small amount for the energy industry, but itâ€™s also an exponential increase. In May, 2021, Google signed a contract with the Texas-based geothermal company Fervo to power its data centers and infrastructure in Nevada; Meta signed a similar deal with Texas-based Sage for a data center east of the Rocky Mountains, and with a company called XGS for one in New Mexico. Microsoft is co-developing a billion-dollar geothermal-powered data center in Kenya; Amazon installed geothermal heating at its newly built fulfillment center in Japan. (Geothermal energy enables companies to avoid the uncertainties of the electrical grid.) Under the Biden Administration, the geothermal industry finally received the same kind of tax credits given to wind and solar, and under the current Trump Administration it has received the same kind of fast-track permitting given to oil and gas. Donald Trumpâ€™s Secretary of Energy, Chris Wright, spoke at a geothermal conference and declared, in front of a MAGA-like sign that read â€œMAGMA (Making America Geothermal: Modern Advances),â€ that although geothermal hasnâ€™t achieved â€œliftoff yet, it should and it can.â€ Depending on whom you speak with, either itâ€™s weird that suddenly everyone is talking about geothermal or itâ€™s weird that there is a cost-competitive energy source with bipartisan appeal that no one is talking about.&lt;/p&gt;
    &lt;p&gt;Scientific work that has been discarded or forgotten can returnâ€”sometimes through unknowing repetition, at other times through deliberate recovery. In the early nineteen-seventies, the U.S. government funded a program at Los Alamos that looked into developing geothermal energy systems that didnâ€™t require proximity to geysers or volcanoes. Two connected wells were built: in one, water was sent down into fractured hot, dry rock; from the other, the steam that resulted from the water meeting the rock emerged. In 1973, Richard Nixon announced Project Independence, which aimed to develop energy sources outside of fossil fuels. â€œBut when Reagan came into office, he changed things,â€ Jefferson Tester, a professor of sustainable energy systems at Cornell University, who was involved in the Los Alamos project, told me. The price of oil had come down, and support for geothermal dissipated. â€œPeople got this impression that it was a failure,â€ Tester said. â€œI think if they looked a little closer, they would see that a lot of the knowledge gained in those first years could have been used to leverage what is happening now.â€&lt;/p&gt;
    &lt;p&gt;Tester went on to help establish the M.I.T. Energy Lab (now called the Energy Initiative), which focusses on advancing clean-energy solutions. He and his colleagues felt that students needed to know the history of the research into diverse energy sources, so they put together a course and a textbook called â€œSustainable Energy: Choosing Among Options.â€ In 2005, the Department of Energy, under George W. Bush, commissioned a group consisting of Tester and some seventeen other experts and researchersâ€”including drilling engineers, energy economists, and power-plant buildersâ€”to investigate what it would take for the U.S. to produce a hundred thousand megawatts of geothermal energy, a bit more than one-fifth of the energy the U.S. had consumed that year. (Geothermal energy production in the U.S. at that time was around three or four thousand megawatts.) The experts avoided framing their support for geothermal in environmental terms. â€œThe feeling was that you werenâ€™t supposed to talk about carbon, because then it would be perceived as about climate change,â€ Tester said.&lt;/p&gt;
    &lt;p&gt;In 2006, Tester and his colleagues published their report, â€œThe Future of Geothermal Energy.â€ One finding was that new drilling technology employed by the oil-and-gas industry was changing the economics of geothermal power generation. Latent ideasâ€”like those from the Los Alamos projectâ€”had met their moment. â€œI was called to testify a few times before Congress. It was a relatively modest investment that was needed, and people were excited,â€ Tester told me. â€œBut then we submitted the report to the Department of Energy. And they did nothing. It was crazy.â€ He was still visibly dismayed.&lt;/p&gt;
    &lt;p&gt;One explanation for the lack of action is that, around that time, the U.S. went from being an oil importer to an oil exporter. This turnaround was largely due to the innovations of George Mitchell, a second-generation Greek American in Galveston, Texas, who spent years trying to extract oil and gas from the Barnett Shale formation, in North Texas, in an economically viable way. His approach synthesized hydraulic fracturing, or fracking, with horizontal drilling. Fracking involves injecting fluid down a well at high pressures, which cracks the subsurface, and the horizontal drilling augments the area of cracking. Eventually, Mitchellâ€™s company, helped by generous tax incentives, made the economics work. Vast oil reserves became accessible. Fortunes were made. Fracking overwhelmed the renewed interest in geothermal power. But a couple of decades later there was a reversal: fracking accelerated geothermal power.&lt;/p&gt;
    &lt;p&gt;Tim Latimer, the thirty-five-year-old C.E.O. of Fervo Energy, a geothermal company founded in 2017, grew up in Riesel, Texas, a small town about fifteen miles outside Waco. After graduating from the University of Tulsa with a degree in mechanical engineering, Latimer wanted a well-paid engineering job close to home. â€œMy adviser was just, like, â€˜Have you ever heard of the oil-and-gas industry?â€™ â€ he said, smiling.&lt;/p&gt;
    &lt;p&gt;As a greenhorn drilling engineer with the international mining company BHP Billiton, Latimer was put on a fracking project in the Eagle Ford Shale, in South Texas. The shale, which is a Cretaceous-era formation dense with marine fossils from when the area was an inland sea, is relatively hard and hot. â€œThe motors in our drill systems were failing early,â€ Latimer said. His supervisors suspected that this was because of the wellsâ€™ unusually high temperatures, around a hundred and seventy-five degrees Celsius. â€œThey said, â€˜Can you research what tools we could use to deal with the fact that these drilling temperatures are really high?â€™ â€ Latimer told me.&lt;/p&gt;
    &lt;p&gt;Much of the relevant work Latimer came across turned up in papers about geothermal energy. â€œIâ€™d never heard of geothermal before,â€ he said. â€œI was, like, â€˜Well, this seems pretty cool.â€™ â€ When Latimer read the 2006 â€œFuture of Geothermal Energyâ€ report, including its description of the Los Alamos geothermal project, he saw parallels to his work in oil and gas. The report described two big technical challenges that were standing in the way of affordable, bountiful clean energy. One was getting drilling costs downâ€”an area that oil and gas had made great progress in. The other was getting water flowing through hot rock that isnâ€™t sufficiently permeable, like shale, so that you can generate steam. â€œAnd Iâ€™m just looking at the rig, being, like, â€˜This is a solved problem.â€™ â€ Generating flow where there isnâ€™t much naturallyâ€”thatâ€™s what hydraulic fracturing does.&lt;/p&gt;
    &lt;p&gt;Latimer reported what he had found to BHP. The shale drilling started working again, but Latimerâ€™s imagination had shifted. In 2014, he applied to Stanford Business School with the goal of using fracking technology in geothermal wells. â€œGeothermal is an industry that, frankly, at that point in time, people had given up on as forgotten,â€ he said. â€œI didnâ€™t think that was right. I was, like, â€˜Iâ€™m a drilling engineer. I actually have a skill that can make a direct impact on this.â€™ â€&lt;/p&gt;
    &lt;p&gt;In 2017, Latimer and his Stanford colleague Jack Norbeck co-founded Fervo Energy. â€œFrackingâ€ is an unpopular word. Fervo describes itself as a â€œnext-generation geothermal energy developer.â€ Just as the fusion-energy industry avoids the phrase â€œnuclear fusion,â€ and the term â€œnatural gasâ€ is now used for what is mostly methane, geothermal systems involving hydraulic fracturing tend to be referred to as â€œenhancedâ€ geothermal systems, or E.G.S.&lt;/p&gt;
    &lt;p&gt;In 2023, Fervo drilled a pair of demonstration wells in Nevada, proving its ideas in anticipation of scaling them up. The goal is to begin operating a five-hundred-megawatt geothermal power plant in Cape Station, Utah, with a hundred megawatts going online in 2026. This past June, Fervo drilled a four-and-a-half-kilometre appraisal wellâ€”a well for confirming predicted subsurface conditions before going all in on a siteâ€”that reached temperatures of two hundred and seventy degrees Celsius. The well was drilled in sixteen days, remarkably fast, and faster drill times mean lowered costs. Fervoâ€™s well design and drilling technologies are central to its hopes, and have helped it raise more than eight hundred million dollars in investment capital. Most everyone I spoke to seems to be rooting for Fervo, albeit with some skepticism. The Utah site is far away from a source for the large amounts of water that will be required, for instance. There are more technical points of concern, too. Fracking can induce seismic activity, so the siting of wells is an important consideration. Enthusiasts see these as solvable problems. And Fervo is not alone in showing promise in the use of fracking to access geothermal power. At the end of October, Mazama Energy demonstrated a pilot E.G.S. in Newberry, Oregon, that works at an even higher temperature: above three hundred degrees Celsius. For now, though, E.G.S. is still a kind of wildcat proposition. â€œI think the big question is: Who are the next nine Fervos?â€ Roland Horne, a professor of energy science and engineering at Stanford, who has studied geothermal energy for about fifty years, said. â€œFervo has expanded tremendously, theyâ€™re a nearly two-hundred-person company, but they donâ€™t have the wherewithal to do a gigawatt project yet.â€&lt;/p&gt;
    &lt;p&gt;Fervo pitches itself as a landing place for oil-and-gas workers. â€œIâ€™ve spent a lot of my life and career in small towns where the largest economic driver is oil and gas,â€ Latimer said. Geothermal means jobs in drilling: engineers, geologists, project managers. Barry Smitherman, who has worked as an oil-and-gas regulator in Texas and as the head of a utility company, told me, â€œWeâ€™ve been drilling oil and gas wells in Texas for over a hundred years. Weâ€™ve drilled over a million wells. We know what the world looks like below the surface in Texas.â€&lt;/p&gt;
    &lt;p&gt;In February, 2021, Winter Storm Uri left most of Texas without power for days. Not long afterward, Smitherman was asked to speak to state legislators about what went wrong and what needed to change. Soon he got a call from a foundation set up by George Mitchell and his wife, Cynthia. (George, who died in 2013, is known as the â€œfather of fracking.â€) The Mitchell Foundation wanted Smitherman to help start a local organization that would advocate on behalf of geothermal energy. He co-founded the Texas Geothermal Energy Alliance in 2022. During his long career in energy, Smitherman said, â€œwe never had a conversation about geothermal. No one had brought it to my attention.â€ Smitherman had a series of meetings with people from geothermal startups, oil-and-gas companies, the Sierra Club, and utility companies. â€œWhat weâ€™ve always said around energy is that you need three legsâ€”reliable, clean, and cheap. Those are the three legs of the stool. The old saying was â€˜I can give you two of three, but I canâ€™t give you all three,â€™ â€ he said. â€œBut, as we began to look at geothermal, it really began to look like it had all threeâ€”low to no carbon, 24/7, and, as the cost curve comes down, eventually, cheap. It really began to look like this unicorn resource.â€&lt;/p&gt;
    &lt;p&gt;Tester now teaches at Cornell, near the Finger Lakes, where in the winter Buttermilk and Taughannock Falls turn to blue ribbons of ice. â€œIf we look at the country and say our goal is ultimately to be much more sustainable with respect to our carbon footprint, you canâ€™t ignore heating,â€ Tester said. Around thirty per cent of New York Stateâ€™s carbon footprint can be attributed to heating and cooling buildings, a figure that is not far from the worldwide average. â€œA lot of it is for space heating and water heating, but also for low-temperature food processing, things like that,â€ Tester added. The excitement about geothermally generated electricity can obscure the thing that geothermal technology is, arguably, best suited to provide. â€œItâ€™s a little bit apples and oranges, and we need both electricity and heating,â€ Tester told me. But he went on to explain that using electricity for heating is not nearly as efficient as using heat for heating. And geothermal wells for heating, which can be relatively shallow, can work in places with no hot springs or volcanoes.&lt;/p&gt;
    &lt;p&gt;Midtown Manhattan, for example: as part of a major renovation completed in 2017, ten geothermal wells were dug beneath St. Patrickâ€™s Cathedral. Some of the wells are less than a hundred metres deep, while others extend more than six hundred and fifty metres, more than ten times deeper than Manhattanâ€™s deepest subway tunnelâ€”and yet much shallower than the wells needed for a geothermal power plant. These wells carry warmth into the cathedral in winter, and out of the cathedral in summer, and do so with less noise and vibration than typical HVAC systems. The main issue is the upfront cost. When the cathedralâ€™s system was built, it felt radical. One of the lead engineers on the project, Paul Boyce, of P. W. Grosser Consulting, told me that the demand for geothermal heating systems has grown dramatically since then. P.W.G.C.â€™s current geothermal projects include the Mastercard headquarters, in Purchase, New York, and the Obama Presidential Center, in Chicago. In Greenpoint, Brooklyn, an eight-hundred-and-thirty-four-unit apartment complex thatâ€™s under construction has its heating and cooling provided through three hundred boreholes, none much deeper than about a hundred and fifty metres. The system was put in by Geosource Energy, a geothermal company started in 2004.&lt;/p&gt;
    &lt;p&gt;But those projects provide geothermal energy building by building, not district by district. â€œI wish we were looking at how we plan our cities,â€ Tester said. â€œItâ€™s crazy that heating, electricity, cable, waterâ€”these are all managed separately.â€ He is now in the midst of a research project that aims to demonstrate the feasibility of an ambitious geothermal system to serve Cornellâ€™s seven-hundred-and-forty-five-acre campus, something close to what downtown ReykjavÃ­k hasâ€”but without the aid of close-to-the-surface magma. In the summer of 2022, a rig set up not far from Cornellâ€™s School of Veterinary Medicine drilled for sixty-five days through layers of shale, limestone, and sandstone, passing beyond the geologic time of the dinosaurs to a crystalline basement dating to the Proterozoic eon, more than five hundred million years ago. This created the Cornell University Borehole Observatory (CUBO). In Iceland, if you dig down this deep, the temperatures could easily be four hundred degrees Celsius; in New York, the rocks are cooler, but the Cornell project needs to reach only eighty to ninety degrees Celsius. As CUBO was drilled, rock samples from each depth were analyzed, and the surrounding natural fracture systems were mapped. If CUBO secures more funding, the next stage will be to drill a pair of wells, with one for injecting water to make an underground reservoir and the other to bring the heated water up.&lt;/p&gt;
    &lt;p&gt;In other geographies, geothermal energy for district heating and cooling has been accomplished with shallower wells. Mieres, Spain, a historic mining town, uses warm water from the now closed mines to supply heat to the region. Nijar, also in Spain but closer to a volcano, uses an underground fluid reservoir to heat its greenhouses. Hayden, Colorado, a former coal town, is working with Bedrock Energy, a Texas-based company started in 2022, to construct a municipal geothermal district, in the hope that reduced energy bills will attract businesses. In Framingham, Massachusetts, activists and a local energy company collaborated on a geothermal heating-and-cooling network, and near Austin, Texas, the neighborhood of Whisper Valley is putting in a similar grid. Several companies, including Bedrock and Dig Energy, are aiming to bring drilling costs down by half or more. Geothermal systems for heating and cooling individual homes remain somewhat pricey to install, but they last for decades, reduce energy bills by twenty-five to fifty per cent, and avoid reliance on the ever more burdened electrical grid. Most people I spoke with in the geothermal industry make their case for it by focussing on cost savings; the unspoken climate benefits are known to those disposed to care, and potentially off-putting to those who are not.&lt;/p&gt;
    &lt;p&gt;Some environmentalists argue that the resources given to geothermalâ€”or to small modular nuclear plants, or to fusionâ€”would be better spent elsewhere. Why not just go all in on solar, wind, and batteries, which are proven, scalable technologies? To invest in more speculative solutions, the argument goes, is a moral hazard, and a cynical or naÃ¯ve distraction that obscures the solutions available now. But this line of thinking rests on the assumption that the people or nations or agencies that would fund one kind of energy would equally fund some other kind. This tends not to be trueâ€”funding is rarely fungible, and always capricious. One geothermal-startup founder spoke of receiving a call from a potential investorâ€™s adviser, who said, Sorry, the managing partner wants to invest in a blimp company instead. â€œGeothermal is the least moral hazard-y of the clean-energy technologies,â€ Gernot Wagner, a climate economist at Columbia Business School, said. â€œAnd we are still subsidizing nuclear a thousand times more than geothermal.â€&lt;/p&gt;
    &lt;p&gt;An energy future without hydrocarbons will require working flexibly with the many variables of resources, geography, and politics. â€œWe can get maybe ninety per cent of the way with solar, batteries, wind,â€ Leah Stokes, a professor of environmental politics at the University of California, Santa Barbara, told me. â€œBut geothermal is one of the things that can fill that gap.â€ Investment follows fashionâ€”and geothermal has become fashionableâ€”but itâ€™s not only investors who appear confident about geothermal. Wagner called this â€œthe moment when Ph.D.s meet M.B.A.s.â€&lt;/p&gt;
    &lt;p&gt;The role of geothermal becomes easier to see when looking beyond the local noise of discussions in America. â€œYou know, thereâ€™s this thing called the curse of abundance,â€ Agnelli Kafuwe, the principal energy officer for the Zambian government, told me. Typically, the phrase refers to countries driven into corruption and misery by their oil endowments, but Kafuwe was referring to Zambiaâ€™s seemingly boundless supply of hydroelectric energy, from power stations such as one at the Zambezi Riverâ€™s Mosi-oa-Tunya, the natural feature known to most Americans as Victoria Falls. For many years, hydropower met practically all of Zambiaâ€™s energy needs, even powering its lucrative copper mines.&lt;/p&gt;
    &lt;p&gt;But the countryâ€™s population grew rapidly, and in 2015 a severe drought hit, forcing Zambia to turn to diesel to make up the shortfall in hydropower. Mosi-oa-Tunya looked less like a world-renowned cataract than like dry, rocky cliffs. There wasnâ€™t enough water to keep the hydropower plants running properly. Lengthy blackouts became common. In 2024, a new drought arrivedâ€”the worst in at least a centuryâ€”and power was cut off for eighteen to twenty hours a day. As in many countries, the leadership had thought about geothermal in the nineteen-seventies but had lost interest; Zambia hadnâ€™t needed it enough then.&lt;/p&gt;
    &lt;p&gt;In addition to copper mining, extensive salt mining occurs in northern Zambia. â€œThese mining companies, they would drill down maybe fifty metres, and guess what comes up?â€ Kafuwe said. â€œGeothermal steam, of a very high, very good temperature.â€ The countryâ€™s mining history also meant that subsurface maps of its territory already existedâ€”useful for planning geothermal wells. One former mining-company head, Peter Vivian-Neal, now heads Kalahari GeoEnergy, a company he founded after seeing an egg being boiled in a natural hot spring while he was on safari. The company has drilled exploratory wells, done flow tests, well tests, and modellingâ€”it aims to have a demonstration power plant running soon. Vivian-Neal is optimistic that a successful demonstration will bring in more investment. â€œWe could not have got to where we got today if my family hadnâ€™t put in the money to start with,â€ he told me. â€œBut Iâ€™m quite sure that the next person will find it easier. Theyâ€™ll say, â€˜Oh, yes, look, Kalahari has made this a success, therefore weâ€™re going to make it a success, and weâ€™ll do it even faster.â€™ â€ â™¦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953568</guid><pubDate>Mon, 17 Nov 2025 13:55:53 +0000</pubDate></item><item><title>Show HN: ESPectre â€“ Motion detection based on Wi-Fi spectre analysis</title><link>https://github.com/francescopace/espectre</link><description>&lt;doc fingerprint="149c507545b3b9c1"&gt;
  &lt;main&gt;
    &lt;p&gt;Motion detection system based on Wi-Fi spectre analysis (CSI), with Home Assistant integration.&lt;/p&gt;
    &lt;p&gt;ðŸ“° Featured Article: Read the complete story behind ESPectre on Medium ðŸ‡®ðŸ‡¹ Italian, ðŸ‡¬ðŸ‡§ English&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In 3 Points&lt;/item&gt;
      &lt;item&gt;Mathematical Approach&lt;/item&gt;
      &lt;item&gt;What You Need&lt;/item&gt;
      &lt;item&gt;Quick Start&lt;/item&gt;
      &lt;item&gt;How It Works&lt;/item&gt;
      &lt;item&gt;What You Can Do With It&lt;/item&gt;
      &lt;item&gt;Sensor Placement Guide&lt;/item&gt;
      &lt;item&gt;System Architecture&lt;/item&gt;
      &lt;item&gt;FAQ&lt;/item&gt;
      &lt;item&gt;Security and Privacy&lt;/item&gt;
      &lt;item&gt;Technical Deep Dive&lt;/item&gt;
      &lt;item&gt;Future Evolutions&lt;/item&gt;
      &lt;item&gt;References&lt;/item&gt;
      &lt;item&gt;Changelog&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
      &lt;item&gt;Author&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What it does: Detects movement at home using Wi-Fi (no cameras, no microphones)&lt;/item&gt;
      &lt;item&gt;What you need: A ~â‚¬10 device (ESP32-S3) + Home Assistant or MQTT server + ESP-IDF development tools&lt;/item&gt;
      &lt;item&gt;Setup time: 30-45 minutes (first time, including ESP-IDF setup)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project currently does NOT use Machine Learning models. Instead, it employs a mathematical approach that extracts 10 features from CSI (Channel State Information) data using statistical and signal processing techniques.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;âœ… No ML training required: Works out-of-the-box with mathematical algorithms&lt;/item&gt;
      &lt;item&gt;âœ… 10 extracted features: Statistical, spatial, and temporal features&lt;/item&gt;
      &lt;item&gt;âœ… Real-time processing: Low latency detection on ESP32-S3 hardware&lt;/item&gt;
      &lt;item&gt;âœ… Foundation for ML: These features can serve as the basis for collecting labeled datasets to train ML models for advanced tasks (people counting, activity recognition, gesture detection)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The mathematical approach provides excellent movement detection without the complexity of ML model training, while the extracted features offer a solid foundation for future ML-based enhancements.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;âœ… 2.4GHz Wi-Fi Router (the one you already have at home works fine)&lt;/item&gt;
      &lt;item&gt;âœ… ESP32-S3 DevKit bundle with external antennas (~â‚¬10) - Available on Amazon, AliExpress, or electronics stores&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ESP32-S3 DevKit with external antennas (recommended for better reception)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;âœ… MQTT Broker (required for operation): &lt;list rend="ul"&gt;&lt;item&gt;Home Assistant with built-in MQTT broker (on Raspberry Pi, PC, NAS, or cloud)&lt;/item&gt;&lt;item&gt;OR standalone Mosquitto MQTT server (can run on any device, including Raspberry Pi)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;âœ… ESP-IDF v6.1 (development framework for building firmware)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;âœ… Basic command line knowledge required for building and flashing firmware&lt;/item&gt;
      &lt;item&gt;âŒ NO router configuration needed&lt;/item&gt;
      &lt;item&gt;âœ… Follow the setup guide in SETUP.md&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Setup time: ~30-45 minutes (first time)&lt;lb/&gt; Difficulty: Intermediate (requires ESP-IDF setup)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Setup &amp;amp; Installation: Follow the complete guide in SETUP.md&lt;/item&gt;
      &lt;item&gt;Calibration &amp;amp; Tuning: Optimize for your environment with CALIBRATION.md&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When someone moves in a room, they "disturb" the Wi-Fi waves traveling between the router and the sensor. It's like when you move your hand in front of a flashlight and see the shadow change.&lt;/p&gt;
    &lt;p&gt;The ESP32-S3 device "listens" to these changes and understands if there's movement.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;âœ… No cameras (total privacy)&lt;/item&gt;
      &lt;item&gt;âœ… No wearables needed (no bracelets or sensors to wear)&lt;/item&gt;
      &lt;item&gt;âœ… Works through walls (Wi-Fi passes through walls)&lt;/item&gt;
      &lt;item&gt;âœ… Very cheap (~â‚¬10 total)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;ðŸ“š Technical Explanation (click to expand)&lt;/head&gt;
    &lt;p&gt;Channel State Information (CSI) represents the physical characteristics of the wireless communication channel between transmitter and receiver. Unlike simple RSSI (Received Signal Strength Indicator), CSI provides rich, multi-dimensional data about the radio channel.&lt;/p&gt;
    &lt;p&gt;Per-subcarrier information:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amplitude: Signal strength for each OFDM subcarrier (up to 64)&lt;/item&gt;
      &lt;item&gt;Phase: Phase shift of each subcarrier&lt;/item&gt;
      &lt;item&gt;Frequency response: How the channel affects different frequencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Environmental effects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multipath propagation: Reflections from walls, furniture, objects&lt;/item&gt;
      &lt;item&gt;Doppler shifts: Changes caused by movement&lt;/item&gt;
      &lt;item&gt;Temporal variations: How the channel evolves over time&lt;/item&gt;
      &lt;item&gt;Spatial patterns: Signal distribution across antennas/subcarriers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When a person moves in an environment, they:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alter multipath reflections (new signal paths)&lt;/item&gt;
      &lt;item&gt;Change signal amplitude and phase&lt;/item&gt;
      &lt;item&gt;Create temporal variations in CSI patterns&lt;/item&gt;
      &lt;item&gt;Modify the electromagnetic field structure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These changes are detectable even through walls, enabling privacy-preserving presence detection without cameras, microphones, or wearable devices.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ðŸ  Home security: Get an alert if someone enters while you're away&lt;/item&gt;
      &lt;item&gt;ðŸ‘´ Elderly care: Monitor activity to detect falls or prolonged inactivity&lt;/item&gt;
      &lt;item&gt;ðŸ’¡ Smart automation: Turn on lights/heating only when someone is present&lt;/item&gt;
      &lt;item&gt;âš¡ Energy saving: Automatically turn off devices in empty rooms&lt;/item&gt;
      &lt;item&gt;ðŸ‘¶ Child monitoring: Alert if they leave the room during the night&lt;/item&gt;
      &lt;item&gt;ðŸŒ¡ï¸ Climate control: Heat/cool only occupied zones&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Optimal sensor placement is crucial for reliable movement detection.&lt;/p&gt;
    &lt;p&gt;Optimal range: 3-8 meters&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Distance&lt;/cell&gt;
        &lt;cell role="head"&gt;Signal&lt;/cell&gt;
        &lt;cell role="head"&gt;Multipath&lt;/cell&gt;
        &lt;cell role="head"&gt;Sensitivity&lt;/cell&gt;
        &lt;cell role="head"&gt;Noise&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommendation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;&amp;lt; 2m&lt;/cell&gt;
        &lt;cell&gt;Too strong&lt;/cell&gt;
        &lt;cell&gt;Minimal&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;âŒ Too close&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;3-8m&lt;/cell&gt;
        &lt;cell&gt;Strong&lt;/cell&gt;
        &lt;cell&gt;Good&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;âœ… Optimal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&amp;gt; 10-15m&lt;/cell&gt;
        &lt;cell&gt;Weak&lt;/cell&gt;
        &lt;cell&gt;Variable&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;âŒ Too far&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;âœ… Position sensor in the area to monitor (not necessarily in direct line with router)&lt;lb/&gt; âœ… Height: 1-1.5 meters from ground (desk/table height)&lt;lb/&gt; âœ… External antenna: Use IPEX connector for better reception&lt;lb/&gt; âŒ Avoid metal obstacles between router and sensor (refrigerators, metal cabinets)&lt;lb/&gt; âŒ Avoid corners or enclosed spaces (reduces multipath diversity)&lt;/p&gt;
    &lt;p&gt;ESPectre uses a streamlined processing pipeline:&lt;/p&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSI Data   â”‚  Raw Wi-Fi Channel State Information
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Segmentation â”‚  Moving Variance Segmentation (MVS)
â”‚  (2-state)  â”‚  IDLE â†” MOTION (operates on RAW CSI)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                     â”‚
       â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    IDLE     â”‚      â”‚    MOTION    â”‚
â”‚  (no feat.) â”‚      â”‚  (optional   â”‚
â”‚             â”‚      â”‚   features)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Filters   â”‚  Butterworth, Wavelet,
                     â”‚             â”‚  Hampel, Savitzky-Golay
                     â”‚             â”‚  (applied to features only)
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Features   â”‚  10 mathematical features
                     â”‚ (if enabled)â”‚  (filtered CSI data)
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                         â”‚
       â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MQTT     â”‚  Publish state + metrics â”‚    MQTT     â”‚
â”‚   (IDLE)    â”‚                          â”‚  (MOTION)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Key Points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2-state system: IDLE or MOTION (no intermediate states)&lt;/item&gt;
      &lt;item&gt;Segmentation-based: Uses Moving Variance Segmentation (MVS) on raw CSI data&lt;/item&gt;
      &lt;item&gt;Filters applied to features only: Segmentation uses unfiltered data to preserve motion sensitivity&lt;/item&gt;
      &lt;item&gt;Optional features: Feature extraction only during MOTION state (configurable)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ESP32-S3 â”‚  â”‚ESP32-S3 â”‚  â”‚ESP32-S3 â”‚
â”‚ Room 1  â”‚  â”‚ Room 2  â”‚  â”‚ Room 3  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ MQTT
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    MQTT Broker     â”‚
         â”‚ (Home Assistant    â”‚
         â”‚  built-in or any   â”‚
         â”‚  MQTT Server)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Home Assistant â”‚
          â”‚   MQTT Sensors â”‚
          â”‚  &amp;amp; Automations â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Each sensor publishes to its own topic:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;home/espectre/kitchen&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;home/espectre/bedroom&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;home/espectre/living&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Home Assistant can then:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor each room independently&lt;/item&gt;
      &lt;item&gt;Create group sensors for whole-house occupancy&lt;/item&gt;
      &lt;item&gt;Implement zone-based automations&lt;/item&gt;
      &lt;item&gt;Track movement patterns across rooms&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Click to expand FAQ&lt;/head&gt;
    &lt;p&gt;Q: Do I need programming knowledge to use it?&lt;lb/&gt; A: Basic command line skills are needed to build and flash the firmware using ESP-IDF. Follow the step-by-step guide in SETUP.md.&lt;/p&gt;
    &lt;p&gt;Q: Does it work with my router?&lt;lb/&gt; A: Yes, if your router has 2.4GHz Wi-Fi (virtually all modern routers have it).&lt;/p&gt;
    &lt;p&gt;Q: How much does it cost in total?&lt;lb/&gt; A: Hardware: ~â‚¬10 for the ESP32-S3 device. Software: All free and open source. You'll also need a device to run the MQTT broker (Home Assistant or Mosquitto), which can be a Raspberry Pi (~â‚¬35-50) or any existing PC/NAS you already have (free).&lt;/p&gt;
    &lt;p&gt;Q: Do I need to modify anything on the router?&lt;lb/&gt; A: No! The router works normally. The sensor "listens" to Wi-Fi signals without modifying anything.&lt;/p&gt;
    &lt;p&gt;Q: Can I try it without Home Assistant?&lt;lb/&gt; A: Yes, you can use any MQTT server (e.g., Mosquitto) or even just view data via serial port.&lt;/p&gt;
    &lt;p&gt;Q: Does it work through walls?&lt;lb/&gt; A: Yes, the 2.4GHz Wi-Fi signal penetrates drywall. Reinforced concrete walls reduce sensitivity but detection remains possible at reduced distances.&lt;/p&gt;
    &lt;p&gt;Q: How many sensors are needed for a house?&lt;lb/&gt; A: It depends on size. One sensor can monitor ~50 mÂ². For larger homes, use multiple sensors (1 sensor every 50-70 mÂ² for optimal coverage).&lt;/p&gt;
    &lt;p&gt;Q: Can it distinguish between people and pets?&lt;lb/&gt; A: The system uses a 2-state segmentation model (IDLE/MOTION) that identifies generic movement without distinguishing between people, pets, or other moving objects. For more sophisticated classification (people vs pets, activity recognition, gesture detection), trained AI/ML models would be required (see Future Evolutions section).&lt;/p&gt;
    &lt;p&gt;Q: Does it consume a lot of Wi-Fi bandwidth?&lt;lb/&gt; A: No, MQTT traffic is minimal. With smart publishing disabled (default), the system publishes all detection updates. When smart publishing is enabled, the system only sends data on significant changes or every 5 seconds as a heartbeat, resulting in ~0.2-0.5 KB/s per sensor during idle periods and up to ~1 KB/s during active movement. Network impact is negligible.&lt;/p&gt;
    &lt;p&gt;Q: Does it work with mesh Wi-Fi networks?&lt;lb/&gt; A: Yes, it works normally. Make sure the ESP32 connects to the 2.4 GHz band.&lt;/p&gt;
    &lt;p&gt;Q: Is a dedicated server necessary?&lt;lb/&gt; A: No, Home Assistant can run on Raspberry Pi, NAS, or cloud. Alternatively, just an MQTT broker (Mosquitto) on any device is sufficient.&lt;/p&gt;
    &lt;p&gt;Q: How accurate is the detection?&lt;lb/&gt; A: Detection accuracy is highly environment-dependent and requires proper tuning. Factors affecting performance include: room layout, wall materials, furniture placement, distance from router (optimal: 3-8m), and interference levels. In optimal conditions with proper tuning, the system provides reliable movement detection. Adjust the &lt;code&gt;segmentation_threshold&lt;/code&gt; parameter to tune sensitivity for your specific environment.&lt;/p&gt;
    &lt;p&gt;Q: What's the power consumption?&lt;lb/&gt; A: ~500mW typical during continuous operation. The firmware includes support for power optimization, and deep sleep modes can be implemented for battery-powered deployments, though this would require custom modifications to the code.&lt;/p&gt;
    &lt;p&gt;Q: If it doesn't work, can I get help?&lt;lb/&gt; A: Yes, open an Issue on GitHub or contact me via email.&lt;/p&gt;
    &lt;head&gt;ðŸ” Privacy, Security &amp;amp; Ethical Considerations (click to expand)&lt;/head&gt;
    &lt;p&gt;The system collects anonymous data related to the physical characteristics of the Wi-Fi radio channel:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amplitudes and phases of OFDM subcarriers&lt;/item&gt;
      &lt;item&gt;Statistical signal variances&lt;/item&gt;
      &lt;item&gt;NOT collected: personal identities, communication contents, images, audio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CSI data represents only the properties of the transmission medium and does not contain direct identifying information.&lt;/p&gt;
    &lt;p&gt;âœ… No cameras: Respect for visual privacy&lt;lb/&gt; âœ… No microphones: No audio recording&lt;lb/&gt; âœ… No wearables: Doesn't require wearable devices&lt;lb/&gt; âœ… Aggregated data: Only statistical metrics, not raw identifying data&lt;/p&gt;
    &lt;p&gt;WARNING: Despite the intrinsic anonymity of CSI data, this system can be used for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-consensual monitoring: Detecting presence/movement of people without their explicit consent&lt;/item&gt;
      &lt;item&gt;Behavioral profiling: With advanced AI models, inferring daily life patterns&lt;/item&gt;
      &lt;item&gt;Domestic privacy violation: Tracking activities inside private homes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The user is solely responsible for using this system and must:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;âœ… Obtain explicit consent from all monitored persons&lt;/item&gt;
      &lt;item&gt;âœ… Respect local regulations (GDPR in EU, local privacy laws)&lt;/item&gt;
      &lt;item&gt;âœ… Clearly inform about the presence of the sensing system&lt;/item&gt;
      &lt;item&gt;âœ… Limit use to legitimate purposes (home security, personal home automation)&lt;/item&gt;
      &lt;item&gt;âœ… Protect data with encryption and controlled access&lt;/item&gt;
      &lt;item&gt;âŒ DO NOT use for illegal surveillance, stalking, or violation of others' privacy&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Moving Variance Segmentation (MVS) analysis: baseline graphs (top) show quiet state, while bottom graphs show motion detection with turbulence signal, adaptive threshold, and state transitions&lt;/p&gt;
    &lt;head&gt;ðŸ”¬ Signal Processing Pipeline (click to expand)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native ESP32 CSI API captures Wi-Fi Channel State Information via callback&lt;/item&gt;
      &lt;item&gt;Extracts amplitude and phase data from OFDM subcarriers (up to 64 subcarriers)&lt;/item&gt;
      &lt;item&gt;Typical capture rate: ~10-100 packets/second depending on Wi-Fi traffic&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spatial turbulence calculation: Standard deviation of subcarrier amplitudes (raw CSI data)&lt;/item&gt;
      &lt;item&gt;Moving Variance Segmentation (MVS): Real-time motion segment extraction&lt;/item&gt;
      &lt;item&gt;Adaptive threshold: Based on moving variance of turbulence signal&lt;/item&gt;
      &lt;item&gt;Segment features: Duration, average turbulence, maximum turbulence&lt;/item&gt;
      &lt;item&gt;Circular buffer: Maintains up to 10 recent segments for analysis&lt;/item&gt;
      &lt;item&gt;Foundation for ML: Segments can be labeled and used for activity classification&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Segmentation operates on raw, unfiltered CSI data to preserve motion sensitivity. Filters are not applied to the turbulence signal used for segmentation.&lt;/p&gt;
    &lt;p&gt;Advanced filters applied to CSI data before feature extraction (configurable via MQTT):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Butterworth Low-Pass: Removes high-frequency noise &amp;gt;8Hz (environmental interference) - Enabled by default&lt;/item&gt;
      &lt;item&gt;Wavelet db4: Removes low-frequency persistent noise using Daubechies wavelet transform&lt;/item&gt;
      &lt;item&gt;Hampel Filter: Outlier removal using MAD (Median Absolute Deviation)&lt;/item&gt;
      &lt;item&gt;Savitzky-Golay Filter: Polynomial smoothing (enabled by default)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Filter Pipeline: Raw CSI â†’ Butterworth (high freq) â†’ Wavelet (low freq) â†’ Hampel â†’ Savitzky-Golay â†’ Features&lt;/p&gt;
    &lt;p&gt;Note: Filters are applied only to feature extraction, not to segmentation. Segmentation uses raw CSI data to preserve motion sensitivity.&lt;/p&gt;
    &lt;p&gt;When enabled (default: on), extracts 10 mathematical features from filtered CSI data during MOTION state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Statistical (5): Variance, Skewness, Kurtosis, Entropy, IQR&lt;/item&gt;
      &lt;item&gt;Spatial (3): Spatial variance, correlation, gradient across subcarriers&lt;/item&gt;
      &lt;item&gt;Temporal (2): Delta mean, delta variance (changes between consecutive packets)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Feature extraction can be disabled to reduce CPU usage if only basic motion detection is needed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Publishes JSON payload every 1 second (configurable)&lt;/item&gt;
      &lt;item&gt;QoS level 0 (fire-and-forget) for low latency&lt;/item&gt;
      &lt;item&gt;Retained message option for last known state&lt;/item&gt;
      &lt;item&gt;Automatic reconnection on connection loss&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MQTT Sensor subscribes to topic and creates entity&lt;/item&gt;
      &lt;item&gt;State: Primary &lt;code&gt;movement&lt;/code&gt;value (0.0-1.0)&lt;/item&gt;
      &lt;item&gt;Attributes: All other metrics available for conditions&lt;/item&gt;
      &lt;item&gt;History: Automatic logging to database for graphs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;ðŸ“Š Optional Feature Extraction (click to expand)&lt;/head&gt;
    &lt;p&gt;ESPectre can optionally extract 10 mathematical features from CSI data during MOTION state:&lt;/p&gt;
    &lt;p&gt;Statistical properties of the CSI signal distribution:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Variance - Signal variability, increases significantly with movement&lt;/item&gt;
      &lt;item&gt;Skewness - Distribution asymmetry, detects irregular movement patterns&lt;/item&gt;
      &lt;item&gt;Kurtosis - Distribution "tailedness", identifies outliers and sudden changes&lt;/item&gt;
      &lt;item&gt;Entropy - Signal randomness/disorder, increases when environment changes&lt;/item&gt;
      &lt;item&gt;IQR (Interquartile Range) - Robust spread measure (Q3-Q1), resistant to outliers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Characteristics across OFDM subcarriers (frequency domain):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spatial Variance - Variability across subcarriers, indicates multipath diversity&lt;/item&gt;
      &lt;item&gt;Spatial Correlation - Correlation between adjacent subcarriers, affected by movement&lt;/item&gt;
      &lt;item&gt;Spatial Gradient - Rate of change across subcarriers, highly sensitive to movement&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Changes between consecutive CSI packets:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Temporal Delta Mean - Average absolute difference from previous packet&lt;/item&gt;
      &lt;item&gt;Temporal Delta Variance - Variance of differences from previous packet&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feature extraction is enabled by default but can be disabled to reduce CPU usage. Note: Features are only extracted during MOTION state, not during IDLE, to optimize performance.&lt;/p&gt;
    &lt;head&gt;ðŸ“‹ Technical Specifications (click to expand)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Board: ESP32-S3-DevKitC-1 N16R8&lt;/item&gt;
      &lt;item&gt;Flash: 16MB&lt;/item&gt;
      &lt;item&gt;PSRAM: 8MB&lt;/item&gt;
      &lt;item&gt;Wi-Fi: 802.11 a/g/n (2.4 GHz only)&lt;/item&gt;
      &lt;item&gt;Antenna: Built-in PCB antenna + IPEX connector for external&lt;/item&gt;
      &lt;item&gt;Power: USB-C 5V or 3.3V via pins&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Framework: ESP-IDF v6.1&lt;/item&gt;
      &lt;item&gt;Language: C&lt;/item&gt;
      &lt;item&gt;Build System: CMake&lt;/item&gt;
      &lt;item&gt;Flash Tool: esptool.py&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CSI Capture Rate: 10-100 packets/second&lt;/item&gt;
      &lt;item&gt;Processing Latency: &amp;lt;50ms per packet&lt;/item&gt;
      &lt;item&gt;MQTT Publish Rate: Smart publishing (only on significant changes + 5s heartbeat)&lt;/item&gt;
      &lt;item&gt;MQTT Bandwidth: ~0.2-1 KB/s depending on activity&lt;/item&gt;
      &lt;item&gt;Power Consumption: ~500mW typical&lt;/item&gt;
      &lt;item&gt;Detection Range: 3-8 meters optimal&lt;/item&gt;
      &lt;item&gt;Detection Accuracy: Environment-dependent, requires tuning&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Works only on 2.4 GHz band (ESP32-S3 hardware limitation)&lt;/item&gt;
      &lt;item&gt;Sensitivity dependent on: wall materials, antenna placement, distances, interference&lt;/item&gt;
      &lt;item&gt;Not suitable for environments with very high Wi-Fi traffic&lt;/item&gt;
      &lt;item&gt;Cannot distinguish between people, pets, or objects (generic motion detection)&lt;/item&gt;
      &lt;item&gt;Cannot count people or recognize specific activities (without ML models)&lt;/item&gt;
      &lt;item&gt;Reduced performance through metal obstacles or thick concrete walls&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;ðŸ“š Machine Learning and Deep Learning (click to expand)&lt;/head&gt;
    &lt;p&gt;The current implementation uses an advanced mathematical approach with 10 features and multi-criteria detection to identify movement patterns. While this provides excellent results without requiring ML training, scientific research has shown that Machine Learning and Deep Learning techniques can extract even richer information from CSI data for complex tasks like people counting, activity recognition, and gesture detection.&lt;/p&gt;
    &lt;p&gt;Classification or regression models can estimate the number of people present in an environment by analyzing complex patterns in CSI.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wang et al. (2017) - "Device-Free Crowd Counting Using WiFi Channel State Information" - IEEE INFOCOM&lt;/item&gt;
      &lt;item&gt;Xi et al. (2016) - "Electronic Frog Eye: Counting Crowd Using WiFi" - IEEE INFOCOM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Neural networks (CNN, LSTM, Transformer) can classify human activities like walking, falling, sitting, sleeping.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wang et al. (2015) - "Understanding and Modeling of WiFi Signal Based Human Activity Recognition" - ACM MobiCom&lt;/item&gt;
      &lt;item&gt;Yousefi et al. (2017) - "A Survey on Behavior Recognition Using WiFi Channel State Information" - IEEE Communications Magazine&lt;/item&gt;
      &lt;item&gt;Zhang et al. (2019) - "WiFi-Based Indoor Robot Positioning Using Deep Neural Networks" - IEEE Access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Deep learning algorithms can estimate position and trajectory of moving people.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wang et al. (2016) - "CSI-Based Fingerprinting for Indoor Localization: A Deep Learning Approach" - IEEE Transactions on Vehicular Technology&lt;/item&gt;
      &lt;item&gt;Chen et al. (2018) - "WiFi CSI Based Passive Human Activity Recognition Using Attention Based BLSTM" - IEEE Transactions on Mobile Computing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Models trained on CSI temporal sequences can recognize hand gestures for touchless control.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Abdelnasser et al. (2015) - "WiGest: A Ubiquitous WiFi-based Gesture Recognition System" - IEEE INFOCOM&lt;/item&gt;
      &lt;item&gt;Jiang et al. (2020) - "Towards Environment Independent Device Free Human Activity Recognition" - ACM MobiCom&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UT-HAR: Human Activity Recognition dataset (University of Texas)&lt;/item&gt;
      &lt;item&gt;Widar 3.0: Gesture recognition dataset with CSI&lt;/item&gt;
      &lt;item&gt;SignFi: Sign language recognition dataset&lt;/item&gt;
      &lt;item&gt;FallDeFi: Fall detection dataset&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;ðŸ›œ Standardized Wi-Fi Sensing (IEEE 802.11bf) (click to expand)&lt;/head&gt;
    &lt;p&gt;Currently, only a limited number of Wi-Fi chipsets support CSI extraction, which restricts hardware options for Wi-Fi sensing applications. However, the IEEE 802.11bf (Wi-Fi Sensing) standard should significantly improve this situation by making CSI extraction a standardized feature.&lt;/p&gt;
    &lt;p&gt;The 802.11bf standard was officially published on September 26, 2025, introducing Wi-Fi Sensing as a native feature of the Wi-Fi protocol. Main characteristics:&lt;/p&gt;
    &lt;p&gt;ðŸ”¹ Native sensing: Detection of movements, gestures, presence, and vital signs&lt;lb/&gt; ðŸ”¹ Interoperability: Standardized support across different vendors&lt;lb/&gt; ðŸ”¹ Optimizations: Specific protocols to reduce overhead and power consumption&lt;lb/&gt; ðŸ”¹ Privacy by design: Privacy protection mechanisms integrated into the standard&lt;lb/&gt; ðŸ”¹ Greater precision: Improvements in temporal and spatial granularity&lt;lb/&gt; ðŸ”¹ Existing infrastructure: Works with already present Wi-Fi infrastructure&lt;/p&gt;
    &lt;p&gt;Market: The Wi-Fi Sensing market is in its early stages and is expected to experience significant growth in the coming years as the 802.11bf standard enables native sensing capabilities in consumer devices.&lt;/p&gt;
    &lt;p&gt;Hardware availability:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;g-emoji&gt;âš ï¸&lt;/g-emoji&gt;Consumer routers: Currently there are no widely available consumer routers with native 802.11bf support&lt;/item&gt;
      &lt;item&gt;ðŸ¢ Commercial/industrial: Experimental devices and integrated solutions already in use&lt;/item&gt;
      &lt;item&gt;ðŸ”§ Hardware requirements: Requires multiple antennas, Wi-Fi 6/6E/7 support, and AI algorithms for signal processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Expected timeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-2026: First implementations in enterprise and premium smart home devices&lt;/item&gt;
      &lt;item&gt;2027-2028: Diffusion in high-end consumer routers&lt;/item&gt;
      &lt;item&gt;2029+: Mainstream adoption in consumer devices&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When 802.11bf is widely adopted, applications like this project will become:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More accessible: No need for specialized hardware or modified firmware&lt;/item&gt;
      &lt;item&gt;More reliable: Standardization ensures predictable behavior&lt;/item&gt;
      &lt;item&gt;More efficient: Protocols optimized for continuous sensing&lt;/item&gt;
      &lt;item&gt;More secure: Privacy mechanisms integrated at the standard level&lt;/item&gt;
      &lt;item&gt;More powerful: Ability to detect even vital signs (breathing, heartbeat)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Perspective: In the next 3-5 years, routers and consumer devices will natively support Wi-Fi Sensing, making projects like this implementable without specialized hardware or firmware modifications. This will open new possibilities for smart home, elderly care, home security, health monitoring, and advanced IoT applications.&lt;/p&gt;
    &lt;p&gt;For now: Solutions like this project based on ESP32 CSI API remain the most accessible and economical way to experiment with Wi-Fi Sensing.&lt;/p&gt;
    &lt;p&gt;This project builds upon extensive research in Wi-Fi sensing and CSI-based movement detection. The following academic works and theses provide valuable insights into mathematical signal processing approaches for human activity recognition using Wi-Fi Channel State Information:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Wi-Fi Sensing per Human Identification attraverso CSI&lt;/p&gt;&lt;lb/&gt;University thesis (in Italian) covering CSI data collection for human recognition through Wi-Fi signal analysis, with in-depth exploration of mathematical signal processing methods.&lt;lb/&gt;ðŸ“„ Read thesis&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Channel State Information (CSI) Features Collection in Wi-Fi&lt;/p&gt;&lt;lb/&gt;Detailed analysis of CSI feature collection and processing in Wi-Fi environments, with methods for extraction and analysis suitable for mathematical processing.&lt;lb/&gt;ðŸ“„ Read thesis&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Indoor Motion Detection Using Wi-Fi Channel State Information (2018)&lt;/p&gt;&lt;lb/&gt;Scientific article describing indoor movement detection using CSI with approaches based on signal mathematics and physics, minimizing the use of machine learning models.&lt;lb/&gt;ðŸ“„ Read paper&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;WiFi Motion Detection: A Study into Efficacy and Performance (2019)&lt;/p&gt;&lt;lb/&gt;Study using CSI data collected from standard devices to detect movements, with analysis of signal processing methods to extract movement events without relying on ML.&lt;lb/&gt;ðŸ“„ Read paper&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;CSI-HC: A WiFi-Based Indoor Complex Human Motion Recognition Using Channel State Information (2020)&lt;/p&gt;&lt;lb/&gt;Recognition of complex indoor movements through CSI with methods based on mathematical signal features, ideal for projects with signal-based analysis without advanced ML.&lt;lb/&gt;ðŸ“„ Read paper&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Location Intelligence System for People Estimation in Indoor Environment During Emergency Operation (2022)&lt;/p&gt;&lt;lb/&gt;Demonstrates the use of ESP32 with wavelet filtering (Daubechies db4) for people detection in emergency scenarios. This paper directly influenced ESPectre's wavelet filter implementation, showing that wavelet denoising outperforms traditional filters on ESP32 hardware.&lt;lb/&gt;ðŸ“„ Read paper&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These references demonstrate that effective Wi-Fi sensing can be achieved through mathematical and statistical approaches, which is the foundation of ESPectre's design philosophy.&lt;/p&gt;
    &lt;p&gt;For a detailed history of changes, new features, and improvements, see the CHANGELOG.md.&lt;/p&gt;
    &lt;p&gt;This project is released under the GNU General Public License v3.0 (GPLv3).&lt;/p&gt;
    &lt;p&gt;GPLv3 ensures that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;âœ… The software remains free and open source&lt;/item&gt;
      &lt;item&gt;âœ… Anyone can use, study, modify, and distribute it&lt;/item&gt;
      &lt;item&gt;âœ… Modifications must be shared under the same license&lt;/item&gt;
      &lt;item&gt;âœ… Protects end-user rights and software freedom&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See LICENSE for the full license text.&lt;/p&gt;
    &lt;p&gt;Francesco Pace&lt;lb/&gt; ðŸ“§ Email: francesco.pace@gmail.com&lt;lb/&gt; ðŸ’¼ LinkedIn: linkedin.com/in/francescopace&lt;lb/&gt; ðŸ›œ Project: ESPectre&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953977</guid><pubDate>Mon, 17 Nov 2025 14:40:54 +0000</pubDate></item><item><title>Aldous Huxley predicts Adderall and champions alternative therapies</title><link>https://angadh.com/inkhaven-7</link><description>&lt;doc fingerprint="d71d2a7e38330707"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Aldous Huxley Predicts Adderall and Champions Alternative Therapies&lt;/head&gt;
    &lt;p&gt;If delivered today, the last of Huxleyâ€™s 7-part lecture series at MIT would probably be categorised under motivational talks or self-help strategies. It surveys the various under-explored non-pharmacological means to realise the best versions of ourselves. Or, as he calls it, actualising our desirable potentialities.&lt;/p&gt;
    &lt;p&gt;Some fairly well known means for self-actualisation that Huxley discusses are Alexander technique and Gestalt therapy. While the former is considered a pseudoscientific therapyI am not using this as a pejorative, for a change., Huxley tells us of the influential educator John Deweyâ€™s admiration of F.M. Alexanderâ€™s work. He paraphrases Deweyâ€™s foreword in one of Alexanderâ€™s books:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Alexanderâ€™s technique is to education what education is to life, in general. It proposes an ideal and provides means whereby that ideal can be realised.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While this is mentioned much later in his lecture, a Huxley-like figure today might need to lead their lecture with this part to convey the import and validity of such approaches. Huxley doesnâ€™t really go into the details of why or how this is true but admittedly admires Alexanderâ€™s contribution. I have a friend who swears by it for enhancing their dance practice though I have not been able to grok what it does so farâ€”it sounds a lot like what meditation does in terms of raising awareness.&lt;/p&gt;
    &lt;p&gt;Huxley believes that such practices are effective at psychologically breeding in desirable qualities in a person instead of: genetically breeding out undesirable ones; or pharmacologically enhancing our intellectual abilitiesâ€”i.e., improved attention spans or reduced sleepâ€”to increase our mental efficiency. Here, Huxley predicts the emergence of Adderall though I was less impressed by his forecasting euphoric pharmaceuticals. After all, this lecture was delivered several years after the publication of The Doors of Perception.&lt;/p&gt;
    &lt;p&gt;The underlying efficiency gains from these psychological approaches happen, he claims, because they train humans into being fundamentally happier; something he felt pharma-euphorics might also achieve one day. The reason such therapies are effective is that they do not provide a homogeneous training; instead, they can be adapted to individual personalities and their intrinsic differences, allowing each individual to actualise their latent potentialities via different means. This recognition that there is no single ideal version of a human is quite old; Huxley finds the most realistic (or complete) ideals in the Bhagavad Gitaâ€™s Three Yogas. The ways of devotion (Bhakti), selfless action (Karma), and contemplation (Jnana) can all lead to enlightenment, i.e., the actualisation of desirable qualities. He sees a correspondence between these yogas and the more recent Western categorisation of human beings by William Sheldonâ€™s somatotypesâ€”quite a problematic take when I read the traits listed in this table. While I do admire his capacity to form connections through historyWhether I see them or not is less important., I donâ€™t see the relationship between these two beyond the fact that these are categories. Theyâ€™re by no means comparable so maybe I missed the point of this comparison.&lt;/p&gt;
    &lt;p&gt;He highlights parallels between the positive outcomes of training oneâ€™s imagination via Gestalt therapy and those seen in Richard DeMilleâ€™s strategies in Childrenâ€™s Imagination Games: children get more fun out of life by, for example, visualising adversarial or intimidating situations with adults in a more playful manner so that things feel less serious than they need to beThat is how I understood this section.. The examples Huxley gives here reminded me of those given to nervous interviewees and public speakers, like â€œImagine your audience is nakedâ€, to take the edge off.&lt;/p&gt;
    &lt;p&gt;As an educator I am very sympathetic to Huxleyâ€™s grand idea in this lecture that we must develop new methods of education that adapt to personality variations; the current strategy of pigeonholing students into the identical training-and-testing modalities remains inappropriate, especially as technological advancementsâ€”which academia struggles to keep up withâ€”could enable more personalised and expressive learning. He doesnâ€™t imagine one-to-one therapy as the scalable solution to actualisation; instead, he suggests building upon the pre-existing categories of humans into three or more groups to test out other means and potentially develop new ones based on past practices.&lt;/p&gt;
    &lt;p&gt;While I think the whole lecture is delivered eloquently, I am unsure if it has more of a thesis than that; itâ€™s more a survey of techniques that rely on anecdotal evidence or name-dropping to convey their effectiveness.&lt;/p&gt;
    &lt;p&gt;Tomorrowâ€™s post will unpack how he sees the role of the humanities in helping us actualise our desirable potentialities, which Huxley discussed in his lecture. It will also include my own concluding thoughts on his lecture. Maybe I will have some semblance of a thesis from it as I contemplate his words overnight.&lt;/p&gt;
    &lt;p&gt;Thanks to taylor.town for linking to a transcript of Huxleyâ€™s speech on Hacker News!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45954140</guid><pubDate>Mon, 17 Nov 2025 14:57:33 +0000</pubDate></item><item><title>WeatherNext 2: Our most advanced weather forecasting model</title><link>https://blog.google/technology/google-deepmind/weathernext-2/</link><description>&lt;doc fingerprint="3ff8f453711c2ba8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WeatherNext 2: Our most advanced weather forecasting model&lt;/head&gt;
    &lt;p&gt;The weather affects important decisions we make everyday â€” from global supply chains and flight paths to your daily commute. In recent years, artificial intelligence (AI) has dramatically enhanced whatâ€™s possible in weather forecasting and the ways in which we can use it.&lt;/p&gt;
    &lt;p&gt;Today, Google DeepMind and Google Research are introducing WeatherNext 2, our most advanced and efficient forecasting model. WeatherNext 2 can generate forecasts 8x faster and with resolution up to 1-hour. This breakthrough is enabled by a new model that can provide hundreds of possible scenarios. Using this technology, weâ€™ve supported weather agencies in making decisions based on a range of scenarios through our experimental cyclone predictions.&lt;/p&gt;
    &lt;p&gt;We're now taking our research out of the lab and putting it into the hands of users. WeatherNext 2's forecast data is now available in Earth Engine and BigQuery. Weâ€™re also launching an early access program on Google Cloudâ€™s Vertex AI platform for custom model inference.&lt;/p&gt;
    &lt;p&gt;By incorporating WeatherNext technology, weâ€™ve now upgraded weather forecasts in Search, Gemini, Pixel Weather and Google Maps Platformâ€™s Weather API. In the coming weeks, it will also help power weather information in Google Maps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting more possible scenarios&lt;/head&gt;
    &lt;p&gt;From a single input, we use independently trained neural networks and inject noise in function space to create coherent variability in weather forecast predictions.&lt;/p&gt;
    &lt;p&gt;Weather predictions need to capture the full range of possibilities â€” including worst case scenarios, which are the most important to plan for.&lt;/p&gt;
    &lt;p&gt;WeatherNext 2 can predict hundreds of possible weather outcomes from a single starting point. Each prediction takes less than a minute on a single TPU; it would take hours on a supercomputer using physics-based models.&lt;/p&gt;
    &lt;p&gt;Our model is also highly skillful and capable of higher-resolution predictions, down to the hour. Overall, WeatherNext 2 surpasses our previous state-of-the-art WeatherNext model on 99.9% of variables (e.g. temperature, wind, humidity) and lead times (0-15 days), enabling more useful and accurate forecasts.&lt;/p&gt;
    &lt;p&gt;This improved performance is enabled by a new AI modelling approach called a Functional Generative Network (FGN), which injects â€˜noiseâ€™ directly into the model architecture so the forecasts it generates remain physically realistic and interconnected.&lt;/p&gt;
    &lt;p&gt;This approach is particularly useful for predicting what meteorologists refer to as â€œmarginalsâ€ and â€œjoints.â€ Marginals are individual, standalone weather elements: the precise temperature at a specific location, the wind speed at a certain altitude or the humidity. What's novel about our approach is that the model is only trained on these marginals. Yet, from that training, it learns to skillfully forecast 'joints' â€” large, complex, interconnected systems that depend on how all those individual pieces fit together. This 'joint' forecasting is required for our most useful predictions, such as identifying entire regions affected by high heat, or expected power output across a wind farm.&lt;/p&gt;
    &lt;p&gt;Continuous Ranked Probability Score (CRPS) comparing WeatherNext 2 to WeatherNext Gen&lt;/p&gt;
    &lt;head rend="h2"&gt;From research to reality&lt;/head&gt;
    &lt;p&gt;With WeatherNext 2, we're translating cutting edge research into high-impact applications. Weâ€™re committed to advancing the state of the art of this technology and making our latest tools available to the global community.&lt;/p&gt;
    &lt;p&gt;Looking ahead, weâ€™re actively researching capabilities to improve our models, including integrating new data sources, and expanding access even further. By providing powerful tools and open data, we hope to accelerate scientific discovery and empower a global ecosystem of researchers, developers and businesses to make decisions on todayâ€™s most complex problems and build for the future.&lt;/p&gt;
    &lt;p&gt;To learn more about geospatial platforms and AI work at Google, check out Google Earth, Earth Engine, AlphaEarth Foundations, and Earth AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learn more about WeatherNext 2&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read our paper&lt;/item&gt;
      &lt;item&gt;WeatherNext developer documentation&lt;/item&gt;
      &lt;item&gt;Explore the Earth Engine Data Catalog&lt;/item&gt;
      &lt;item&gt;Query forecast data in BigQuery&lt;/item&gt;
      &lt;item&gt;Sign up to the early access program for Cloud Vertex AI&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45954210</guid><pubDate>Mon, 17 Nov 2025 15:04:26 +0000</pubDate></item><item><title>How to escape the Linux networking stack</title><link>https://blog.cloudflare.com/so-long-and-thanks-for-all-the-fish-how-to-escape-the-linux-networking-stack/</link><description>&lt;doc fingerprint="a391685bd82399c5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;There is a theory which states that if ever anyone discovers exactly what the Linux networking stack does and why it does it, it will instantly disappear and be replaced by something even more bizarre and inexplicable.&lt;/p&gt;
      &lt;p&gt;There is another theory which states that Git was created to track how many times this has already happened.&lt;/p&gt;
      &lt;p&gt;Many products at Cloudflare arenÃ¢t possible without pushing the limits of network hardware and software to deliver improved performance, increased efficiency, or novel capabilities such as soft-unicast, our method for sharing IP subnets across data centers. Happily, most people do not need to know the intricacies of how your operating system handles network and Internet access in general. Yes, even most people within Cloudflare.&lt;/p&gt;
      &lt;p&gt;But sometimes we try to push well beyond the design intentions of LinuxÃ¢s networking stack. This is a story about one of those attempts.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Hard solutions for soft problems&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;My previous blog post about the Linux networking stack teased a problem matching the ideal model of soft-unicast with the basic reality of IP packet forwarding rules. Soft-unicast is the name given to our method of sharing IP addresses between machines. You may learn about all the cool things we do with it, but as far as a single machine is concerned, it has dozens to hundreds of combinations of IP address and source-port range, any of which may be chosen for use by outgoing connections.&lt;/p&gt;
      &lt;p&gt;The SNAT target in iptables supports a source-port range option to restrict the ports selected during NAT. In theory, we could continue to use iptables for this purpose, and to support multiple IP/port combinations we could use separate packet marks or multiple TUN devices. In actual deployment we would have to overcome challenges such as managing large numbers of iptables rules and possibly network devices, interference with other uses of packet marks, and deployment and reallocation of existing IP ranges.&lt;/p&gt;
      &lt;p&gt;Rather than increase the workload on our firewall, we wrote a single-purpose service dedicated to egressing IP packets on soft-unicast address space. For reasons lost in the mists of time, we named it SLATFATF, or Ã¢fishÃ¢ for short. This serviceÃ¢s sole responsibility is to proxy IP packets using soft-unicast address space and manage the lease of those addresses.&lt;/p&gt;
      &lt;p&gt;WARP is not the only user of soft-unicast IP space in our network. Many Cloudflare products and services make use of the soft-unicast capability, and many of them use it in scenarios where we create a TCP socket in order to proxy or carry HTTP connections and other TCP-based protocols. Fish therefore needs to lease addresses that are not used by open sockets, and ensure that sockets cannot be opened to addresses leased by fish.&lt;/p&gt;
      &lt;p&gt;Our first attempt was to use distinct per-client addresses in fish and continue to let Netfilter/conntrack apply SNAT rules. However, we discovered an unfortunate interaction between LinuxÃ¢s socket subsystem and the Netfilter conntrack module that reveals itself starkly when you use packet rewriting.&lt;/p&gt;
      &lt;p&gt;Suppose we have a soft-unicast address slice, 198.51.100.10:9000-9009. Then, suppose we have two separate processes that want to bind a TCP socket at 198.51.100.10:9000 and connect it to 203.0.113.1:443. The first process can do this successfully, but the second process will receive an error when it attempts to connect, because there is already a socket matching the requested 5-tuple.&lt;/p&gt;
      &lt;p&gt;Instead of creating sockets, what happens when we emit packets on a TUN device with the same destination IP but a unique source IP, and use source NAT to rewrite those packets to an address in this range?&lt;/p&gt;
      &lt;p&gt;If we add an nftables Ã¢snatÃ¢ rule that rewrites the source address to 198.51.100.10:9000-9009, Netfilter will create an entry in the conntrack table for each new connection seen on fishtun, mapping the new source address to the original one. If we try to forward more connections on that TUN device to the same destination IP, new source ports will be selected in the requested range, until all ten available ports have been allocated; once this happens, new connections will be dropped until an existing connection expires, freeing an entry in the conntrack table.&lt;/p&gt;
      &lt;p&gt;Unlike when binding a socket, Netfilter will simply pick the first free space in the conntrack table. However, if you use up all the possible entries in the table you will get an EPERM error when writing an IP packet. Either way, whether you bind kernel sockets or you rewrite packets with conntrack, errors will indicate when there isnÃ¢t a free entry matching your requirements.&lt;/p&gt;
      &lt;p&gt;Now suppose that you combine the two approaches: a first process emits an IP packet on the TUN device that is rewritten to a packet on our soft-unicast port range. Then, a second process binds and connects a TCP socket with the same addresses as that IP packet:&lt;/p&gt;
      &lt;p&gt;The first problem is that there is no way for the second process to know that there is an active connection from 198.51.100.10:9000 to 203.0.113.1:443, at the time the &lt;code&gt;connect() &lt;/code&gt;call is made. The second problem is that the connection is successful from the point of view of that second process.&lt;/p&gt;
      &lt;p&gt;It should not be possible for two connections to share the same 5-tuple. Indeed, they donÃ¢t. Instead, the source address of the TCP socket is silently rewritten to the next free port.&lt;/p&gt;
      &lt;p&gt;This behaviour is present even if you use conntrack without either SNAT or MASQUERADE rules. It usually happens that the lifetime of conntrack entries matches the lifetime of the sockets theyÃ¢re related to, but this is not guaranteed, and you cannot depend on the source address of your socket matching the source address of the generated IP packets.&lt;/p&gt;
      &lt;p&gt;Crucially for soft-unicast, it means conntrack may rewrite our connection to have a source port outside of the port slice assigned to our machine. This will silently break the connection, causing unnecessary delays and false reports of connection timeouts. We need another solution.&lt;/p&gt;
      &lt;p&gt;For WARP, the solution we chose was to stop rewriting and forwarding IP packets, instead to terminate all TCP connections within the server and proxy them to a locally-created TCP socket with the correct soft-unicast address. This was an easy and viable solution that we already employed for a portion of our connections, such as those directed at the CDN, or intercepted as part of the Zero Trust Secure Web Gateway. However, it does introduce additional resource usage and potentially increased latency compared to the status quo. We wanted to find another way (to) forward.&lt;/p&gt;
      &lt;p&gt;If you want to use both packet rewriting and bound sockets, you need to decide on a single source of truth. Netfilter is not aware of the socket subsystem, but most of the code that uses sockets and is also aware of soft-unicast is code that Cloudflare wrote and controls. A slightly younger version of myself therefore thought it made sense to change our code to work correctly in the face of NetfilterÃ¢s design.&lt;/p&gt;
      &lt;p&gt;Our first attempt was to use the Netlink interface to the conntrack module, to inspect and manipulate the connection tracking tables before sockets were created. Netlink is an extensible interface to various Linux subsystems and is used by many command-line tools like ip and, in our case, conntrack-tools. By creating the conntrack entry for the socket we are about to bind, we can guarantee that conntrack wonÃ¢t rewrite the connection to an invalid port number, and ensure success every time. Likewise, if creating the entry fails, then we can try another valid address. This approach works regardless of whether we are binding a socket or forwarding IP packets.&lt;/p&gt;
      &lt;p&gt;There is one problem with this Ã¢ itÃ¢s not terribly efficient. Netlink is slow compared to the bind/connect socket dance, and when creating conntrack entries you have to specify a timeout for the flow and delete the entry if your connection attempt fails, to ensure that the connection table doesnÃ¢t fill up too quickly for a given 5-tuple. In other words, you have to manually reimplement tcp_tw_reuse option to support high-traffic destinations with limited resources. In addition, a stray RST packet can erase your connection tracking entry. At our scale, anything like this that can happen, will happen. It is not a place for fragile solutions.&lt;/p&gt;
      &lt;p&gt;Instead of creating conntrack entries, we can abuse kernel features for our own benefit. Some time ago Linux added the TCP_REPAIR socket option, ostensibly to support connection migration between servers e.g. to relocate a VM. The scope of this feature allows you to create a new TCP socket and specify its entire connection state by hand.&lt;/p&gt;
      &lt;p&gt;An alternative use of this is to create a Ã¢connectedÃ¢ socket that never performed the TCP three-way handshake needed to establish that connection. At least, the kernel didnÃ¢t do that Ã¢ if you are forwarding the IP packet containing a TCP SYN, you have more certainty about the expected state of the world.&lt;/p&gt;
      &lt;p&gt;However, the introduction of TCP Fast Open provides an even simpler way to do this: you can create a Ã¢connectedÃ¢ socket that doesnÃ¢t perform the traditional three-way handshake, on the assumption that the SYN packet Ã¢ when sent with its initial payload Ã¢ contains a valid cookie to immediately establish the connection. However, as nothing is sent until you write to the socket, this serves our needs perfectly.&lt;/p&gt;
      &lt;p&gt;You can try this yourself:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;TCP_FASTOPEN_CONNECT = 30
TCP_FASTOPEN_NO_COOKIE = 34
s = socket(AF_INET, SOCK_STREAM)
s.setsockopt(SOL_TCP, TCP_FASTOPEN_CONNECT, 1)
s.setsockopt(SOL_TCP, TCP_FASTOPEN_NO_COOKIE, 1)
s.bind(('198.51.100.10', 9000))
s.connect(('1.1.1.1', 53))&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Binding a Ã¢connectedÃ¢ socket that nevertheless corresponds to no actual socket has one important feature: if other processes attempt to bind to the same addresses as the socket, they will fail to do so. This satisfies the problem we had at the beginning to make packet forwarding coexist with socket usage.&lt;/p&gt;
      &lt;p&gt;While this solves one problem, it creates another. By default, you canÃ¢t use an IP address for both locally-originated packets and forwarded packets.&lt;/p&gt;
      &lt;p&gt;For example, we assign the IP address 198.51.100.10 to a TUN device. This allows any program to create a TCP socket using the address 198.51.100.10:9000. We can also write packets to that TUN device with the address 198.51.100.10:9001, and Linux can be configured to forward those packets to a gateway, following the same route as the TCP socket. So far, so good.&lt;/p&gt;
      &lt;p&gt;On the inbound path, TCP packets addressed to 198.51.100.10:9000 will be accepted and data put into the TCP socket. TCP packets addressed to 198.51.100.10:9001, however, will be dropped. They are not forwarded to the TUN device at all.&lt;/p&gt;
      &lt;p&gt;Why is this the case? Local routing is special. If packets are received to a local address, they are treated as Ã¢inputÃ¢ and not forwarded, regardless of any routing you think should apply. Behold the default routing rules:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;cbranch@linux:~$ ip rule
cbranch@linux:~$ ip rule
0:Ã‚Â  Ã‚Â  Ã‚Â  Ã‚Â  from all lookup local
32766:Ã‚Â  Ã‚Â  from all lookup main
32767:Ã‚Â  Ã‚Â  from all lookup default&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;The rule priority is a nonnegative integer, the smallest priority value is evaluated first. This requires some slightly awkward rule manipulation to Ã¢insertÃ¢ a lookup rule at the beginning that redirects marked packets to the packet forwarding serviceÃ¢s TUN device; you have to delete the existing rule, then create new rules in the right order. However, you donÃ¢t want to leave the routing rules without any route to the Ã¢localÃ¢ table, in case you lose a packet while manipulating these rules. In the end, the result looks something like this:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;ip rule add fwmark 42 table 100 priority 10
ip rule add lookup local priority 11
ip rule del priority 0
ip route add 0.0.0.0/0 proto static dev fishtun table 100&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;As with WARP, we simplify connection management by assigning a mark to packets coming from the Ã¢fishtunÃ¢ interface, which we can use to route them back there. To prevent locally-originated TCP sockets from having this same mark applied, we assign the IP to the loopback interface instead of fishtun, leaving fishtun with no assigned address. But it doesnÃ¢t need one, as we have explicit routing rules now.&lt;/p&gt;
      &lt;p&gt;While testing this last fix, I ran into an unfortunate problem. It did not work in our production environment.&lt;/p&gt;
      &lt;p&gt;It is not simple to debug the path of a packet through LinuxÃ¢s networking stack. There are a few tools you can use, such as setting nftrace in nftables or applying the LOG/TRACE targets in iptables, which help you understand which rules and tables are applied for a given packet.&lt;/p&gt;
      &lt;p&gt;Schematic for the packet flow paths through Linux networking and *tables by Jan Engelhardt&lt;/p&gt;
      &lt;p&gt;Our expectation is that the packet will pass the prerouting hook, a routing decision is made to send the packet to our TUN device, then the packet will traverse the forward table. By tracing packets originating from the IP of a test host, we could see the packets enter the prerouting phase, but disappear after the Ã¢routing decisionÃ¢ block.&lt;/p&gt;
      &lt;p&gt;While there is a block in the diagram for Ã¢socket lookupÃ¢, this occurs after processing the input table. Our packet doesnÃ¢t ever enter the input table; the only change we made was to create a local socket. If we stop creating the socket, the packet passes to the forward table as before.&lt;/p&gt;
      &lt;p&gt;It turns out that part of the Ã¢routing decisionÃ¢ involves some protocol-specific processing. For IP packets, routing decisions can be cached, and some basic address validation is performed. In 2012, an additional feature was added: early demux. The rationale being, at this point in packet processing we are already looking up something, and the majority of packets received are expected to be for local sockets, rather than an unknown packet or one that needs to be forwarded somewhere. In this case, why not look up the socket directly here and save yourself an extra route lookup?&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The workaround at the end of the universe&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Unfortunately for us, we just created a socket and didnÃ¢t want it to receive packets. Our adjustment to the routing table is ignored, because that routing lookup is skipped entirely when the socket is found. Raw sockets avoid this by receiving all packets regardless of the routing decision, but the packet rate is too high for this to be efficient. The only way around this is disabling the early demux feature. According to the patchÃ¢s claims, though, this feature improves performance: how far will performance regress on our existing workloads if we disable it?&lt;/p&gt;
      &lt;p&gt;This calls for a simple experiment: set the net.ipv4.tcp_early_demux syscall to 0 on some machines in a datacenter, let it run for a while, then compare the CPU usage with machines using default settings and the same hardware configuration as the machines under test.&lt;/p&gt;
      &lt;p&gt;The key metrics are CPU usage from /proc/stat. If there is a performance degradation, we would expect to see higher CPU usage allocated to Ã¢softirqÃ¢ Ã¢ the context in which Linux network processing occurs Ã¢ with little change to either userspace (top) or kernel time (bottom). The observed difference is slight, and mostly appears to reduce efficiency during off-peak hours.&lt;/p&gt;
      &lt;p&gt;While we tested different solutions to IP packet forwarding, we continued to terminate TCP connections on our network. Despite our initial concerns, the performance impact was small, and the benefits of increased visibility into origin reachability, fast internal routing within our network, and simpler observability of soft-unicast address usage flipped the burden of proof: was it worth trying to implement pure IP forwarding and supporting two different layers of egress?&lt;/p&gt;
      &lt;p&gt;So far, the answer is no. Fish runs on our network today, but with the much smaller responsibility of handling ICMP packets. However, when we decide to tunnel all IP packets, we know exactly how to do it.&lt;/p&gt;
      &lt;p&gt;A typical engineering role at Cloudflare involves solving many strange and difficult problems at scale. If you are the kind of goal-focused engineer willing to try novel approaches and explore the capabilities of the Linux kernel despite minimal documentation, look at our open positions Ã¢ we would love to hear from you!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45954638</guid><pubDate>Mon, 17 Nov 2025 15:49:38 +0000</pubDate></item><item><title>Project Gemini</title><link>https://geminiprotocol.net/</link><description>&lt;doc fingerprint="7b91a15fbb3f9295"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Gemini&lt;/head&gt;
    &lt;head rend="h2"&gt;Gemini in 100 words&lt;/head&gt;
    &lt;p&gt;Gemini is a new internet technology supporting an electronic library of interconnected text documents. That's not a new idea, but it's not old fashioned either. It's timeless, and deserves tools which treat it as a first class concept, not a vestigial corner case. Gemini isn't about innovation or disruption, it's about providing some respite for those who feel the internet has been disrupted enough already. We're not out to change the world or destroy other technologies. We are out to build a lightweight online space where documents are just documents, in the interests of every reader's privacy, attention and bandwidth. &lt;/p&gt;
    &lt;p&gt; If you'd like to know more, read our FAQ&lt;/p&gt;
    &lt;p&gt; Or, if you'd prefer, here's a video overview &lt;/p&gt;
    &lt;head rend="h2"&gt;Official resources&lt;/head&gt;
    &lt;p&gt; Project Gemini news&lt;/p&gt;
    &lt;p&gt; Project Gemini documentation&lt;/p&gt;
    &lt;p&gt; Project Gemini history&lt;/p&gt;
    &lt;p&gt; Known Gemini software &lt;/p&gt;
    &lt;p&gt;All content at geminiprotocol.net is CC BY-NC-ND 4.0 licensed unless stated otherwise:&lt;/p&gt;
    &lt;p&gt; CC Attribution-NonCommercial-NoDerivs 4.0 International &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45954640</guid><pubDate>Mon, 17 Nov 2025 15:50:04 +0000</pubDate></item><item><title>How when AWS was down, we were not</title><link>https://authress.io/knowledge-base/articles/2025/11/01/how-we-prevent-aws-downtime-impacts</link><description>&lt;doc fingerprint="765090565d2694dd"&gt;
  &lt;main&gt;
    &lt;p&gt;For help understanding this article or how you can implement auth and similar security architectures in your services, feel free to reach out to me via the community server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â¨ AWS us-east-1 is down!Ã¢&lt;/head&gt;
    &lt;p&gt;One of the most massive AWS incidents transpired on October 20th. The long story short is that the DNS for DynamoDB was impacted for &lt;code&gt;us-east-1&lt;/code&gt;, which created a health event for the entire region. It's the worst incident we've seen in a decade. Disney+, Lyft, McDonald'ss, New York Times, Reddit, and the list goes on were lining up to claim their share too of the spotlight. And we've been watching because our product is part of our customers critical infrastructure. This one graph of the event says it all:&lt;/p&gt;
    &lt;p&gt;The AWS post-incident report indicates that at 7:48 PM UTC DynamoDB had "increased error rates". But this article isn't about AWS, and instead I want to share how exactly we were still up when when AWS was down.&lt;/p&gt;
    &lt;p&gt;Now you might be thinking: why are you running infra in us-east-1?&lt;/p&gt;
    &lt;p&gt;And it's true, almost no one should be using us-east-1, unless, well, of course, you are us. And that's because we end up running our infrastructure where our customers are. In theory, practice and theory are the same, but in practice they differ. And if our (or your) customers chose &lt;code&gt;us-east-1&lt;/code&gt; in AWS, then realistically, that means you are also choosing us-east-1 Ã°Â….&lt;/p&gt;
    &lt;p&gt;During this time, us-east-1 was offline, and while we only run a limited amount of infrastructure in the region, we have to run it there because we have customers who want it there. And even without a direct dependency on &lt;code&gt;us-east-1&lt;/code&gt;, there are critical services in AWS Ã¢ CloudFront, Certificate Manager, Lambda@Edge, and IAM Ã¢ that all have their control planes in that region. Attempting to create distributions or roles at that time were also met with significant issues.&lt;/p&gt;
    &lt;p&gt;Since there are plenty of articles in the wild talking about what actually happened, why it happened, and why it will continue to happen, I don't need to go into it here. Instead, I'm going to share a dive about exactly what we've built to avoid these exact issues, and what you can do for your applications and platforms as well. In this article, I'll review how we maintain a high SLI to match our SLA reliability commitment even when the infrastructure and services we use don't.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã° What is reliability?Ã¢&lt;/head&gt;
    &lt;p&gt;Before I get to the part where I share how we built one of the most reliable auth solutions available. I want to define reliability. And for us, that's an SLA of five nines. I think that's so extraordinary that the question I want you to keep in mind through this article is: is that actually possible? Is it really achievable to have a service with a five nines SLA? When I say five nines, I mean that 99.999% of the time, our service is up and running as expected by our customers. And to put this into perspective, the red, in the sea of blue, represents just how much time we can be down.&lt;/p&gt;
    &lt;p&gt;And if you can't see it, it's hiding inside this black dot. It amounts to just five minutes and 15 seconds per year. This pretty much means we have to be up all the time, providing responses and functionality exactly as our customers expect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â¤ But why?Ã¢&lt;/head&gt;
    &lt;p&gt;To put it into perspective, it's important to share for a moment, the specific challenges that we face, why we built what we built, and of course why that's relevant. To do that, I need to include some details about what we're building Ã¢ what Authress actually does. Authress provides login and access control for the software applications that you write Ã¢ It generates JWTs for your applications. This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;User authentication and authorization&lt;/item&gt;
      &lt;item&gt;User identities&lt;/item&gt;
      &lt;item&gt;Granular role and resource-based authorization (ReBAC, ABAC, TBAC, RBAC, etc...)&lt;/item&gt;
      &lt;item&gt;API keys for your technical customers to interact with your own APIs&lt;/item&gt;
      &lt;item&gt;Machine to machine authentication, or services Ã¢ if you have a microservice architecture.&lt;/item&gt;
      &lt;item&gt;Audit trails to track the permission changes within your services or expose this to your customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And there are of course many more components, that help complete full auth-platform, but they aren't totally relevant to this article, so I'm going to skip over them.&lt;/p&gt;
    &lt;p&gt;With that, you may already start to be able to see why uptime is so critical for us. We're on the critical path for our customers. It's not inherently true for every single platform, but it is for us. So if our solution is down, then our customer applications are down as well.&lt;/p&gt;
    &lt;p&gt;If we put the reliability part in the back corner for one second and just think about the features, we can theorize about a potential initial architecture. That is, an architecture that just focuses on the features, how might you build this out as simple as possible? I want to do this, so I can help explain all the issues that we would face with the simple solution.&lt;/p&gt;
    &lt;p&gt;Maybe you've got a single region, and in that region you have some sort of HTTP router that handles requests and they forward to some compute, serverless, container, or virtual machine, or, and I'm very sorry for the scenario Ã¢ if you have to use bare metal. Lastly, you're interacting with some database, NoSQL, SQL, or something else, file storage, and maybe there's some async components.&lt;/p&gt;
    &lt;p&gt;If you take a look at this, it's probably obvious to you (and everyone else) that there is no way it is going to meet our reliability needs. But we have to ask, just exactly how often will there actually be a problem with this architecture? Just building out complexity doesn't directly increase reliability, we need to focus on why this architecture would fail. For us, we use AWS, so I look to the Amazon CTO for guidance, and he's famously quoted as saying, Everything fails all the time.&lt;/p&gt;
    &lt;p&gt;And AWS's own services are no exception to this. Over the last decade, we've seen numerous incidents:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2014 - Ireland (Partial) - Hardware - Transformer failed - EC2, EBS, and RDS&lt;/item&gt;
      &lt;item&gt;2016 - Sydney (Partial) - Severe Weather - Power Loss - All Services&lt;/item&gt;
      &lt;item&gt;2017 - All Regions - Human error - S3 critical servers deleted - S3&lt;/item&gt;
      &lt;item&gt;2018 - Seoul Region - Human error - DNS resolvers impacted - EC2&lt;/item&gt;
      &lt;item&gt;2021 - Virginia - Traffic Scaling - Network Control Plane outage - All Services&lt;/item&gt;
      &lt;item&gt;2021 - California - Traffic Scaling - Network Control Plane outage - All Services&lt;/item&gt;
      &lt;item&gt;2021 - Frankfurt (Partial) - Fire - Fire Suppression System issues - All Services&lt;/item&gt;
      &lt;item&gt;2023 - Virginia - Kinesis issues - Scheduling Lambda Invocations impact - Lambda&lt;/item&gt;
      &lt;item&gt;2023 - Virginia - Networking issues - Operational issue - Lambda, Fargate, API GatewayÃ¢Â¦&lt;/item&gt;
      &lt;item&gt;2023 - Oregon (Partial) - Error rates - Dynamodb + 48 services&lt;/item&gt;
      &lt;item&gt;2024 - Singapore (Partial) - EC2 Autoscaling - EC2&lt;/item&gt;
      &lt;item&gt;2024 - Virginia (Partial) - Describe API Failures ECS - ECS + 4 services&lt;/item&gt;
      &lt;item&gt;2024 - Brazil - ISP issues - CloudFront connectivity - CloudFront&lt;/item&gt;
      &lt;item&gt;2024 - Global - Network connectivity - STS Service&lt;/item&gt;
      &lt;item&gt;2024 - Virginia - Message size overflow - Kinesis down - Lambda, S3, ECS, CloudWatch, Redshift&lt;/item&gt;
      &lt;item&gt;2025 - Virginia - Dynamo DB DNS - DynamoDB down - All Services&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And any one of these would have caused major problems for us and therefore our customers. And the frequency of incident is actually increasing in time. This shouldn't be a surprise, right? Cloud adoption is increasing over time. The number of services AWS is offering is also increasing. But how impactful are these events? Would single one of them have been a problem for us to actually reach our SLA promise? What would happen if we just trusted AWS and used that to pass through our commitments? Would it be sufficient to achieve 99.999% SLA uptime? Well, let's take a look.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â°Ã¯Â¸ AWS SLA CommitmentsÃ¢&lt;/head&gt;
    &lt;head rend="h4"&gt;The AWS Lambda SLA is below 5 ninesÃ¢&lt;/head&gt;
    &lt;head rend="h4"&gt;The API Gateway SLA is below 5 ninesÃ¢&lt;/head&gt;
    &lt;head rend="h4"&gt;The AWS SQS SLA is below 5 ninesÃ¢&lt;/head&gt;
    &lt;p&gt;Okay, so when it comes to trusting AWS SLAs, it isn't sufficient. At. All.&lt;/p&gt;
    &lt;p&gt;We can't just use the components that are offered by AWS, and go from there. We fundamentally need to do something more than that. So the question becomes, what exactly must a dependency's reliability be in order for us to utilize it? To answer that question, it's time for a math lesson. Or more specifically, everyone's favorite topic, probabilities.&lt;/p&gt;
    &lt;p&gt;Let's quickly get through this &lt;del&gt;torture&lt;/del&gt; exercise. Fundamentally, you have endpoints in your service, and you get in an HTTP request, and it interacts with some third-party component or API, and then you write the result to a database. For us, this could be an integration such as logging in with Google or with Okta for our customers' enterprise customers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â» Calculating the allowed failure rateÃ¢&lt;/head&gt;
    &lt;p&gt;So if we want to meet a 5-nines reliability promise, how unreliable could this third-party component actually be? What happens if this component out of the box is only 90% reliable? We'll design a strategy for getting around that.&lt;/p&gt;
    &lt;p&gt;Uptime is a product of all of the individual probabilities:&lt;/p&gt;
    &lt;p&gt;For the sake of this example, we'll just assume that every other component in our architecture is 100% reliable Ã¢ That's every line of code, no bugs ever written in our library dependencies, or transitive library dependencies, or the dependencies' dependencies' dependencies, and everything always works exactly as we expect.&lt;/p&gt;
    &lt;p&gt;So we can actually rewrite our uptime promise as a result of the failure rate of that third-party component.&lt;/p&gt;
    &lt;p&gt;And the only way that we can actually increase the success rate of the uptime based off of failures is to retry. And so we can multiply out the third-party failure rate and retry multiple times.&lt;/p&gt;
    &lt;p&gt;Logically that makes a lot of sense. When a component fails, if you retry again, and again, the likelihood it will be down every single time approaches zero. And we can generate a really nasty equation from this to actually determine how many exact times do we need to retry.&lt;/p&gt;
    &lt;p&gt;How many exactly can it? Rather than guessing whether or not we should retry four times or five times, or put it in a &lt;code&gt;while(true)&lt;/code&gt; loop, we can figure it out exactly. So we take this equation and extend it out a little bit. Plugging in our 90% reliable third-party component:&lt;/p&gt;
    &lt;p&gt;We find that our retry count actually must be greater than or equal to five. We can see that this adds up to our uptime expectation:&lt;/p&gt;
    &lt;p&gt;Is this the end of the story? Just retry a bunch of times and you're good? Well, not exactly. Remember this equation?&lt;/p&gt;
    &lt;p&gt;We do really need to consider every single component that we utilize. And specifically when it comes to the third-party component, we had to execute it by utilizing a retry handler. So we need to consider the addition of the retry handler into our equation. Going back to the initial architecture, instead of what we had before, when there's a failure in that third-party component, now we will automatically execute some sort of asynchronous retries or in-process retries. And every time that third-party component fails, we execute the retry handler and retry again.&lt;/p&gt;
    &lt;p&gt;This means we need to consider the reliability of that retry handler.&lt;/p&gt;
    &lt;p&gt;Let's assume we have a really reliable retry handler and that it's even more reliable than our service. I think that's reasonable, and actually required. A retry handler that is less reliable than our stated SLA by default is just as faulty as the third-party component.&lt;/p&gt;
    &lt;p&gt;Let's consider one with five and a half nines Ã¢ that's half a nine more reliable than our own SLA.&lt;/p&gt;
    &lt;p&gt;But how reliable does it really need to be? Well, we can pull in our original equation and realize that our total uptime is the unreliability or the reliability of the third-party component multiplied by the reliability of our retry handler.&lt;/p&gt;
    &lt;p&gt;From here, we add in the retries to figure out what the result should be:&lt;/p&gt;
    &lt;p&gt;We have a reliable retry handler, but it's not perfect. And with a retry handler that has reliability of five and a half nines, we can retry a maximum two times. Because remember, it has to be reliable every single time we utilize it, as it is a component which can also fail. Which means left with this equation:&lt;/p&gt;
    &lt;p&gt;I don't think comes as a surprise to anyone that in fact five is greater than two. What is the implication here?&lt;/p&gt;
    &lt;p&gt;The number of retries required for that unreliable third-party component to be utilized by us exceeds the number of retries actually allowed by our retry handler.&lt;/p&gt;
    &lt;p&gt;That's a failure, the retry handler can only retry twice before itself violates our SLA, but we need to retry five times in order to raise the third-party component reliably up. We can actually figure out what the minimum reliability of a third-party component is allowed to be, when using our retry handler:&lt;/p&gt;
    &lt;p&gt;Which in turn validates that it's actually impossible for us to utilize that component. &lt;code&gt;99.7%&lt;/code&gt;. &lt;code&gt;99.7%&lt;/code&gt; is the minimum allowed reliability for any third-party component in order for us to meet our required 5-nines SLA. This third-party component is so unreliable (&lt;code&gt;~90%&lt;/code&gt;), that even using a highly reliable retry handler, we still can't make it reliable enough without the retry handler itself compromising our SLA. We fundamentally need to consider this constraint, when we're building out our architecture. &lt;/p&gt;
    &lt;p&gt;That means we drop this third-party component. Done.&lt;/p&gt;
    &lt;p&gt;And then, let's assume we get rid of every flaky component, everything that don't have a high enough reliability for us. At this point, it's good to think, is this sufficient to achieve our 5-nines SLA? Well, it isn't just third-party components we have to be concerned about. We also have to be worried about those AWs infrastructure failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â©Ã¯Â¸ Infrastructure FailuresÃ¢&lt;/head&gt;
    &lt;p&gt;So let's flashback to our initial architecture again:&lt;/p&gt;
    &lt;p&gt;We can have issues at the database layer, right? There could be any number of problems here. Maybe it's returning 500s, there are some slow queries, maybe things are timing out. Or there could be a problem with our compute. Maybe it's not scaling up fast enough. We're not getting new infrastructure resources. Sometimes, even AWS is out of bare metal machines when you don't reserve them, request them get them on demand, and the list go on.&lt;/p&gt;
    &lt;p&gt;Additionally, there could also be some sort of network issue, where requests aren't making it through to us or even throw a DNS resolution error on a request from our users.&lt;/p&gt;
    &lt;p&gt;In many of these cases, I think the answer is obvious. We just have to declare the whole region as down. And you are probably thinking, well, this is where we failover to somewhere else. No surprise, yeah, this is exactly what we do:&lt;/p&gt;
    &lt;p&gt;However, this means we have to have all the data and all the infrastructure components duplicated to another region in order to do this. And since Authress has six primary regions around the world, that also means we need multiple backup regions to be able to support the strategy. But this comes with significant wasted resources and wasted compute that we're not even getting to use. Costly! But I'll get to that later.&lt;/p&gt;
    &lt;p&gt;Knowing a redundant architecture is required is a great first step, but that leaves us having to solve for: how do we actually make the failover happen in practice?&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â§ The Failover Routing StrategyÃ¢&lt;/head&gt;
    &lt;p&gt;Simply put Ã¢ our strategy is to utilize DNS dynamic routing. This means requests come into our DNS and it automatically selects between one of two target regions, the primary region that we're utilizing or the failover region in case there's an issue. The critical component of the infrastructure is to switch regions during an incident:&lt;/p&gt;
    &lt;p&gt;In our case, when using AWS, this means using the Route 53 health checks and the Route 53 failover routing policy.&lt;/p&gt;
    &lt;p&gt;We know how we're gonna do it, but the long pole in the tent is actually knowing that there is even a problem in the first place. A partial answer is to say Have a health check, so of course there is health check here. But the full answer is: have a health check that validates both of the regions, checking if the region is up, or is there an incident? And if it is, reports the results to the DNS router.&lt;/p&gt;
    &lt;p&gt;We could be utilizing the default provided handler from AWS Route 53 or a third-party component which pings our website, but that's not accurate enough from a standpoint of correctly and knowing for certain that our services are in fact down.&lt;/p&gt;
    &lt;p&gt;It would be devastating for us to fail over when a secondary region is having worse problems than our primary region. Or what if there's an issue with with network traffic. We wouldn't know if that's an issue of communication between AWS's infrastructure services, or an issue with the default Route 53 health check endpoint, or some entangled problem with how those specifically interact with our code that we're actually utilizing. So it became a requirement to built something ourselves, custom, to actually execute exactly what we need to check.&lt;/p&gt;
    &lt;p&gt;Here is a representation of what we're doing. It's not exactly what we are doing, but it's close enough to be useful. Health check request come in from the Route 53 Health Check. They call into our APIGW or Load Balancer as a router. The requests are passed to our compute which can interact and validate logic, code, access, and data in the database:&lt;/p&gt;
    &lt;p&gt;The health check executes this code on request that allows us to validate if the region is in fact healthy:&lt;/p&gt;
    &lt;code&gt;import Authorizer from './authorizer.js';&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We start a profiler to know how long our requests are taking.&lt;/item&gt;
      &lt;item&gt;Then we interact with our databases, as well as validate some secondary components, such as SQS. While issues with secondary components may not always be a reason to failover, they can cause impacts to response time, and those indicators can be used to predict incoming incidents.&lt;/item&gt;
      &lt;item&gt;From there, we check whether or not the most critical business logic is working correctly. In our case, that's interactions with DynamoDB as well as core authorizer logic. Compared to a simple unit test, this accounts for corruption in a deployment package, as well instances where some subtle differences between regions interact with our code base. We can catch those sorts of problems here, and know that the primary region that we're utilizing, one of the six, is having a problem and automatically update the DNS based on this.&lt;/item&gt;
      &lt;item&gt;When we're done, we return success or failure so the health check can track changes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Ã°Â¿ Improving the Failover StrategyÃ¢&lt;/head&gt;
    &lt;p&gt;And we don't stop here with our infrastructure failover however. With the current strategy, it's good, in some cases, even sufficient. But it isn't that great. For starters, we have to completely failover. If there's just one component that's problematic, we can't just swap that one out easily, it's all or nothing with the Route 53 health check. So when possible, we push for an edge-optimized architecture. In AWS, this means utilizing AWS CloudFront with AWS Lambda@Edge for compute. This not only helps reduce latency for our customers and their end users depending where they are around the world, as a secondary benefit, fundamentally, it is an improved failover strategy.&lt;/p&gt;
    &lt;p&gt;And that looks like this:&lt;/p&gt;
    &lt;p&gt;Using CloudFront gives us a highly reliable CDN, which routes requests to the locally available compute region. From there, we can interact with the local database. When our database in that region experiences a health incident, we automatically failover, and check the database in a second adjacent region. And when there's a problem there as well, we do it again to a third region. We can do that because when utilizing DynamoDB we have Global Tables configured for authorization configuration. In places where we don't need the data duplicated, we just interact with the table in a different region without replication.&lt;/p&gt;
    &lt;p&gt;After a third region with an issue, we stop.&lt;/p&gt;
    &lt;p&gt;And maybe you're asking why three and not four or five or six? Aren't you glad we did the probabilities exercise earlier? Now you can actually figure out why it's three here. But, I'll leave that math as an exercise for you.&lt;/p&gt;
    &lt;p&gt;As a quick recap, this handles the problems with at the infrastructure level and with third-party components. And if we solve those, is that sufficient for us to achieve our goal the 5-nines SLA?&lt;/p&gt;
    &lt;p&gt;For us the answer is No, and you might have guessed, if you peaked at the scrollbar or table contents that there are still quite some additional components integrated into our solution. One of them is knowing that at some point, there's going to be a bug in our code, unfortunately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â» Application level failuresÃ¢&lt;/head&gt;
    &lt;p&gt;And that bug will get committed to production, which means we're going to end up with an application failure. It should be obvious that it isn't achievable to write completely bug-free code. Maybe there is someone out there that thinks that, and maybe even that's you, and I believe you that you believe that. However, I know it's not me, and realistically, I don't want to sit around and pray that it's also my fellow team members. The risk is too high, because in the case something does get into production, that means it can impact some of our customers. So instead, let's assume that will happen and design a strategy around it.&lt;/p&gt;
    &lt;p&gt;So when it does happen, we of course have to trigger our incident response. For us, we send out an email, we post a message on our community and internal communication workspaces, and start an on-call alert. The technology here isn't so relevant, but tools like AWS SES, SQS, SNS, Discord, and emails are involved.&lt;/p&gt;
    &lt;p&gt;Incidents wake an engineer up, so someone can start to take look at the incident, and most likely the code.&lt;/p&gt;
    &lt;p&gt;But by the time they even respond to the alert, let alone actually investigate and fix the cause of the incident, we would long violated our SLA. So an alert is not sufficient for us. We need to also implement automation to automatically remediate any of these problems. Now, I'm sure you're thinking, yeah, okay, test automation. You might even be thinking about an LLM agent that can automatically create PRs. (Side note: LLM code generation, doesn't actually work for us, and I'll get to that a little further down) Instead, we have to rely on having sufficient testing in place. And yes, of course we do. We test before deployment. There is no better time to test.&lt;/p&gt;
    &lt;p&gt;This seems simple and an obvious answer, and I hope that for anyone reading this article it is. Untested code never goes to production. Every line of code is completely tested before it is merged to production, even if it is enabled on some flag. Untested code is never released, it is far too dangerous. Untested code never makes it to production behind some magic flag. Abusing feature flags to make that happen could not be a worse decision for us. And that's because we can need to be as confident as possible before those changes actually get out in front of our customers. The result is Ã¢ we don't focus on test coverage percentage, but rather test value. That is, which areas provide most value, that are most risky, that we care about being the most reliable for our customers. Those are the ones we focus on testing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Root Cause Analysis (RCA)Ã¢&lt;/head&gt;
    &lt;p&gt;Every incident could have been prevented if we just had one more test. The trick though is actually having that right test, before the incident.&lt;/p&gt;
    &lt;p&gt;And in reality, that's not actually possible. Having every right test for a service that is constantly changing, while new features are being added, is just unmaintainable. Every additional test we write increases the maintenance burden of our service. Attempting to achieve 100% complete test coverage would require an infinite amount of time. This is known as the Pareto Principle, more commonly the 80-20 rule. If it takes 20% of the time to deliver 80% of the tests, it takes an infinite amount of time to achieve all the tests, and that assumes that the source code isn't changing.&lt;/p&gt;
    &lt;p&gt;The result is we'll never be able to catch everything. So we can't just optimize for prevention. We also need to optimize for recovery. This conclusion for us means also implementing tests against our deployed production code. One example of this are validation tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã° Validation TestsÃ¢&lt;/head&gt;
    &lt;p&gt;A validation test is where you have some data in one format and data in another format and you use those two different formats to ensure referential consistency. (Side note: There are many different kinds of tests, and I do a deep dive in the different types of tests and how they're relevant in building secure and reliable systems). One concrete example could be you have a request that comes in, you end up logging the request data and the response, then you can compare that logged data to what's actually saved in your database.&lt;/p&gt;
    &lt;p&gt;In our scenario, which focuses on the authorization and permissions enforcement checks, we have multiple databases with similar data. In one case, there's the storage of permissions as well as the storage of the expected checks and the audit trail tracking the creation of those permissions. So we actually have multiple opportunities to compare the data between our databases asynchronously outside of customer critical path usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Running the ValidationÃ¢&lt;/head&gt;
    &lt;p&gt;On a schedule, via an AWS CloudWatch Scheduled Rule, we load the data from our different databases and we compare them against each other to make sure it is consistent. If there is a problem, then if this fires off an incident before any of our customers notice, so that we can actually go in and check what's going on.&lt;/p&gt;
    &lt;p&gt;This sounds bad on the surface that it could ever happen. But the reality of the situation is that a discrepancy can show up as a result of any number of mechanisms. For instance, the infrastructure from AWS could have corrupted one of the database shards and what is written to the databases is inconsistent. We know that this can happen as there is no 100% guarantee on database durability, even from AWS. AWS does not guarantee Database Durability, are you assuming they do, because we don't! So actually reading the data back and verifying its internal consistency is something that we must do.&lt;/p&gt;
    &lt;p&gt;While it might not seem that this could reduce the probability of there being an incident. Consider that a requested user permission check whose result doesn't match our customer's expectation is an incident. It might not always be one that anyone identifies or even becomes aware of, but it nonetheless a problem, just like a publicly exposed S3 is technically an issue, even if no one has exfiltrated the data yet, it doesn't mean the bucket isn'is sufficiently secured.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â¯ Incident ImpactÃ¢&lt;/head&gt;
    &lt;p&gt;There are two parts to the actual risk of an incident. The probability and the impact. Everything in this article I've discuss until now talks about reducing the probability of an incident, that is Ã¢ the likelihood of it happening. But since we know that we can't avoid ever having an incident, we also have to reduce the impact when it happens.&lt;/p&gt;
    &lt;p&gt;One way we do that is by utilizing an incremental rollout. Hopefully everyone knows what incremental rollout is, so I'll instead jump straight into how we accomplish it utilizing AWS. And for that we focus again on our solution integrating with CloudFront and our edge architecture.&lt;/p&gt;
    &lt;p&gt;The solution for us is what I call Customer Deployment Buckets. We bucket individual customers into separate buckets and then deploy to each of the buckets sequentially. If the deployment rolls out without a problem, and it's all green, that is everything works correctly, then we go on to the second bucket and then deploy our code to there, and then the third bucket, and so on and so forth until every single customer has the new version.&lt;/p&gt;
    &lt;p&gt;If there is an issue, we stop the rollout and we go and investigate what's actually going on. While we can't prevent the issue from happening to the earlier buckets, we are able to stop that issue from propagating to more customers, having an impact on everyone, and thus reduce the impact of the incident.&lt;/p&gt;
    &lt;p&gt;As I mentioned before the biggest recurring issue isn't executing an operations process during an incident, it's identifying there is a real incident in the first place. So, How do we actually know that there's an issue?&lt;/p&gt;
    &lt;p&gt;If it was an easy problem to solve, you would have written a unit task or integration test or service level test and thus already discovered it, right? So adding tests can't, by design, help us. Maybe there's an issue with the deployment itself or during infrastructure creation, but likely that's not what's happening.&lt;/p&gt;
    &lt;p&gt;Now, I know you're thinking, When is he going to get to AI?&lt;/p&gt;
    &lt;p&gt;Whether or not we'll ever truly have AI is a separate &lt;code&gt;&amp;lt;rant /&amp;gt;&lt;/code&gt; that I won't get into here, so this is the only section on it, I promise. What we actually do is better called anomaly detection. Historically anomaly detection, was what AI always meant, true AI, rather than an LLM or agent in any way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã° AI: Anomaly DetectionÃ¢&lt;/head&gt;
    &lt;p&gt;This is a graph of our detection analysis:&lt;/p&gt;
    &lt;p&gt;You might notice that it's not tracking 400s or 500s, which are in reality relatively easy to detect. But in fact don't actually tell us meaningfully what's wrong with our service or whether or not there really is a problem. Impact is measured by business value, not technical protocol level analytics, so we need to have a business-focused metric.&lt;/p&gt;
    &lt;p&gt;And for us, at Authress, the business-focussed metric we use to identify meaningful incidents we call: The Authorization Ratio. That is the ratio of successful logins and authorizations to ones that are blocked, rejected, timeout or are never completed for some reason.&lt;/p&gt;
    &lt;p&gt;The above CloudWatch metric display contains this exact ratio, and here in this timeframe represents an instance not too long ago where we got really close to firing off our alert.&lt;/p&gt;
    &lt;p&gt;Here, there was a slight elevation of errors soon after a deployment. The expected ratio was outside of our allowance span for a short period of time. However not long enough to trigger an incident. We still investigated, but it wasn't something that required immediate remediation. And it's a good reminder that identifying problems in any production software isn't so straightforward. To achieve high reliability, we've needed an AI or in this case anomaly detection to actually identify additional problems. And realistically, even with this level of sophistication in place, we still can never know with 100% certainty that there is actually an incident at any moment. And that's because "what is an incident", is actually a philosophical question...&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã°Â¹ Does it smell like an incident?Ã¢&lt;/head&gt;
    &lt;p&gt;Our anomaly detection said Ã¢ almost an incident, and we determined the result Ã¢ no incident. But does that mean there wasn't an incident? What makes an incident, how do I define an incident? And is that exact definition ubiquitous, for every system, every engineer, every customer?&lt;/p&gt;
    &lt;p&gt;Obviously not, and one look at the AWS Health Status Dashboard is all you need to determine that the identification of incidents is based on subjective perspective, rather than objective criteria. What's actually more important is the synthesis of our perspective on the situation and what our customers believe. To see what I mean, let's do a comparison:&lt;/p&gt;
    &lt;p&gt;I'm going to use Authress as an example. So I've got the product services perspective on one side and our customer's perspective on the other.&lt;/p&gt;
    &lt;head rend="h3"&gt;Incident AlignmentÃ¢&lt;/head&gt;
    &lt;p&gt;In the top left corner we have alignment. If we believe that our system is up and working and our customers do, too, then success, all good. Everything's working as expected.&lt;/p&gt;
    &lt;p&gt;Inversely in the opposite corner, maybe there is a problem. We believe that one of our services is having an issue, and successfully, we're able to identify it. Most importantly, our customers sayÃ¢yes, there is an issue for us.&lt;/p&gt;
    &lt;p&gt;It's not great that there's an incident, but as I've identified incidents will absolutely happen, and the fact we've correctly aligned with our customers on the problem's existence independently allows us to deploy automation to automatically remediate the issue. That's a success! If it's a new problem that we haven't seen before, we can even design new automation to fix this. Correctly identifying incidents is challenging, so doing that step correctly, leads itself very well to automation for remediation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Perspective MismatchÃ¢&lt;/head&gt;
    &lt;p&gt;One interesting corner is when our customers believe that there's nothing wrong, there have been no incidents reported, but all our alerts are saying Ã¢ RED ALERT Ã¢ someone has to go look at this!&lt;/p&gt;
    &lt;p&gt;In this case, our alerts have identified a problem that no one cares about. This often happens in scenarios where our customers are in one region, Switzerland for example, with local region users, a health care, manufacturing, or e-commerce app, is a good example, rather than global, who are likely asleep at 2:00 AM. And that means an incident at the moment, could be an issue affecting some customers. But if they aren't around to experience it, is it actually happening?&lt;/p&gt;
    &lt;p&gt;You are probably wincing at that idea. There's a bug, it must be fixed! And sure that's a problem, it's happening and we should take note of what's going on. But we don't need to respond in real time. That's a waste of our resources where we could be investing in other things. Why wake up our engineers based on functionality that no one is using?&lt;/p&gt;
    &lt;p&gt;I think one of the most interesting categories is in the top right-hand corner where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;our customers say, "hey, your service is down"&lt;/item&gt;
      &lt;item&gt;But we say, "Wait, really, is it?"_&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is known as a gray failure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gray FailuresÃ¢&lt;/head&gt;
    &lt;p&gt;And it can happen for any number of reasons. Maybe there is something in our knowledge base that tells our customers to do something one way and it's confusing and they've interpreted it in a different way. So there's a different expectation here. That expectation can get codified into customer processes and product services.&lt;/p&gt;
    &lt;p&gt;Or maybe our customer is running different tests from us, ones that are of course, valuable for their business, but not ones that we consider. Or more likely they are just using a less resilient cloud provider.&lt;/p&gt;
    &lt;p&gt;Most fundamentally, there could really be an incident, something that we haven't detected yet, but they have. And if we don't respond to that, it could grow, and left unchecked, escalate, and eventually impact all our customers. This means we need to give our customers an easy way to report incidents to us, which we can immediately follow up with.&lt;/p&gt;
    &lt;p&gt;For us, every single incident, every single customer support ticket that comes into our platform, we immediately and directly send it to our engineering team. Now, I often get pushback on this from other leaders. I'm sure, even you might be thinking something like Ã¢ I don't want to be on call for customer support incidents. But if you throw additional tiers in your organization between your engineering teams and your customers, that means you're increasing the time to actually start investigating and resolving those problems. If you have two tiers before your engineering team and each tier has its own SLA of 10 minutes to triage the issue, that means you've already gone through 20 minutes before an engineer even knows about it and can go and look at it. That violates our SLA by fourfold before investigation and remediation can even begin.&lt;/p&gt;
    &lt;p&gt;Instead, in those scenarios, what I actually recommend thinking about is how might you reduce the number of support tickets you receive in aggregate? This is the much more appropriate way to look at the problem. If you are getting support tickets that don't make sense, then you've got to investigate, why did we get this ticket? Do the root cause analysis on the ticket, not just the issue mentioned in it Ã¢ why the ticket was even created in the first place.&lt;/p&gt;
    &lt;p&gt;A ticket means: Something is broken. From there, we can figure out, OK, maybe we need to improve our documentation. Or we need to change what we're doing on one of our endpoints. Or we need to change the response error message we're sending. But you can always go deeper.&lt;/p&gt;
    &lt;head rend="h3"&gt;The customer support advantageÃ¢&lt;/head&gt;
    &lt;p&gt;And going deeper, means customer support is critical for us. We consider customer support to be the lifeline of our service level agreement (SLA). If we didn't have that advantage, then we might not have been able to deliver our commitment at all. So much so that we report some of our own CloudWatch custom metrics to our customers so they can have an aggregate view of both what they know internally and what we believe. We do this through our own internal dashboard in our application management UIs.&lt;/p&gt;
    &lt;p&gt;Helping our users identify incidents benefits us; because we can't catch everything. It's just not possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã° Negligence and MaliceÃ¢&lt;/head&gt;
    &lt;p&gt;To this point, we've done the math on reliability of third-party components. We've implemented an automatic region failover and added incremental rollout. And we have a core customer support focus. Is that sufficient to achieve 5-nines of reliability?&lt;/p&gt;
    &lt;p&gt;If you think yes, then you'd expect the meme pictures now. And, I wish I could say it was enough, but it's not. That's because we also have to deal with negligence and malice.&lt;/p&gt;
    &lt;p&gt;We're in a privileged position to have numerous security researchers out there on the internet constantly trying to find vulnerabilities within our service. For transparency, I have some of those reports I want to share:&lt;/p&gt;
    &lt;head rend="h3"&gt;Ã¢RealÃ¢ Vulnerability ReportsÃ¢&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;I am a web security researcher enthusiast. Do you give a monetary reward?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Okay, this isn't starting out that great. What else have we received?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I found some vulnerabilities in your website. Do you offer rewards for ethical hackers?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Well, maybe, but I think you would actually need to answer for us, what the problem actually is. And you also might notice this went to our spam. It didn't even get to our inbox. So a lot of help they might be providing. Actually we ignore any Ã¢securityÃ¢ email sent from a non-custom domain.&lt;/p&gt;
    &lt;p&gt;This one was really interesting. We had someone attempting to phish our engineering team by creating a support ticket and putting in some configuration trying to get us to provide them our own credentials to one of our third-party dependencies. Interestingly enough, our teams don't even have access to those credentials directly.&lt;/p&gt;
    &lt;p&gt;And, we know this was malicious because the credentials that they are referencing in the support request are from our honey pot, stuck in our UI to explicitly catch these sorts of things. The only way to get these credentials is if they hacked around our UI application and pulled out of the HTML. They aren't readily available any other way. So it was very easy for us to detect that this Ã¢reportÃ¢ was actually a social engineering attack.&lt;/p&gt;
    &lt;p&gt;And this is one of my favorites, and I can't make this up:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have found many security loophole. How much will you pay if you want to working with me like project?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That's the exact quote, I don't even know what that means. Unfortunately, LLMs will actually start to make all of these future "vulnerability reports" sound more appealing to read in the future, for better or worse. However, at the end of the day, the truth is that these are harmless. And we actually do have a security disclosure program that anyone can go and submit problems for. I hope the message to white-hat hackers is please use that process, and the legitimate reports usually do go through it. Do not send us emails. Those are going to go into the abyss. Alternatively, you can follow our security.txt public page or go to the disclosure form, but with email, the wrong people are going to get that and we can't triage effectively.&lt;/p&gt;
    &lt;p&gt;Vulnerabilities in our services can result in production incidents for our customers. That means security is part of our SLA. Don't believe me, I'll show you how:&lt;/p&gt;
    &lt;head rend="h3"&gt;Multitenant considerationsÃ¢&lt;/head&gt;
    &lt;p&gt;It's relevant for us, that Authress is a multitenant solution. So some of the resources within our service are in fact shared between customers.&lt;/p&gt;
    &lt;p&gt;Additionally, customers could have multiple services in a microservice architecture or multiple components. And one of these services could theoretically consume all of the resources that we've allocated for that customer. In that scenario, that would cause an incident for that customer. So we need to protect against resource exhaustion Intra-Tenant. Likewise, we have multiple customers. One of those customers could be consuming more resources than we've allocated to the entire tenant. And that could cause an incident across Inter-Tenant and cause an incident across our platform and impact other customers.&lt;/p&gt;
    &lt;p&gt;Lastly, we have to be worried about our customers, our customers' customers, and our customers' customers' customers, because any one of those could be malicious and consume their resources and so on and so forth, thus causing a cascading failure. A failure due to lack of resources is an incident. The only solution that makes sense for this is, surprise, rate limiting.&lt;/p&gt;
    &lt;head rend="h3"&gt;Helpful Rate LimitingÃ¢&lt;/head&gt;
    &lt;p&gt;So we need to rate-limit these requests at different levels for different kinds of clients, different kinds of users, and we do that within our architecture, at different fundamental levels within our infrastructure.&lt;/p&gt;
    &lt;p&gt;Primarily there are protections at our compute level, as well at the region level, and also place protections at a global level. In AWS, this of course means using a web application firewall or WAF. I think our WAF configuration is interesting and in some ways novel.&lt;/p&gt;
    &lt;p&gt;Fundamentally, one of the things that we love to use is the AWS managed IP reputation list.&lt;/p&gt;
    &lt;p&gt;The reputation list is list of IP addresses that have been associated with malicious activity outside of our service throughout other customers at AWS and other providers out there in the world where a problem has been detected. That means before those attacks even get to our service or to our customers' instances of Authress, we can already know to block them, and the WAF does that. This is great, and most importantly, has a very low false positive rate.&lt;/p&gt;
    &lt;p&gt;However, the false positive rate is an important metric for consideration of counter measures against malicious attacks or negligent accidental abuse of resources, and something that prevents us from using any other managed rules from AWS or external providers. There's two problems with managed rules, fundamentally:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Number one is the false positive rate. If that is even a little bit more than, it couldn't be sustainable, and would result in us blocking legitimate requests coming for a customer. This means it is a problem, and it's an incident for them if some of their users can't utilize their software because of something we did. False positives are customer incidents.&lt;/item&gt;
      &lt;item&gt;The second one is that managed rules are gratuitously expensive. Lots of companies are building these just to charge you lots of money, and the ROI just doesn't seem to be there. We don't see useful blocks from them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the truth is, we need to do something more than just the reputation list rule.&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling Requests at ScaleÃ¢&lt;/head&gt;
    &lt;p&gt;And the thing that we've decided to do is Ã¢ add blocking for sufficiently high requests. By default, any Authress account's service client that goes above 2,000 requests per second (RPS), we just immediately terminate. Now, this isn't every customer, as there are some out there for us that do require such a high load or even higher (as 2k isn't that high). But for the majority of them, if you get to this number and they haven't talked to us about their volume, then it is probably malicious in some way. You don't magically go from zero to 2,000 one day, unless it is an import job.&lt;/p&gt;
    &lt;p&gt;Likewise, we can actually learn about a problem long before it gets to that scale. We have milestones, and we start reporting loads from clients at 100, 200, 500, 1,000, et cetera. If we see clients hitting these load milestones, we can already start to respond and create an incident for us to investigate before they reach a point where they're consuming all of the resources in our services for that customer. And we do this by adding alerts on the COUNT of requests for WAF metrics.&lt;/p&gt;
    &lt;p&gt;However, we also get attacks at a smaller scale. Just because we aren't being DDoS-ed doesn't mean there isn't attack. And those requests will still get through because they don't meet our blocking limits. They could be malicious in nature, but only identifiable in aggregate. So while single request might seem fine, if you see the same request 10 times a second, 100 times a second, something is probably wrong. Or if you have request urls that end in &lt;code&gt;.php?admin&lt;/code&gt;, when no one has run WordPress in decades, you also know that there's a problem. We catch these by logging all of the blocked requests.&lt;/p&gt;
    &lt;p&gt;We have automation in place to query those results and update our rules, but a picture is worth a thousand words:&lt;/p&gt;
    &lt;p&gt;Here you can see a query based off of the IP addresses from the client that are being utilized and sorted by frequency. When we get these requests that look non-malicious individually, we execute a query such as this one and we check to see if the results match a pattern. You can use ip address matching or more intelligently, something called the JA3 or JA4 fingerprints of those requests There are actually lots of options available, I'm not going to get into exactly what they are, there are some great articles on the topic. And there are more mechanisms to actually track these used throughout the security industry, and utilizing them let's you instantly identify: Hey, you know what? This request violates one of our patterns, maybe we should block all the requests from that client.&lt;/p&gt;
    &lt;p&gt;And so, rather than waiting for them to get to the point where an attacker is consuming 2,000 requests per second worth of resources, you can stop there right away. In the cases where we can't make a conclusive decision, this technology gives us another tool that we can utilize to improve our patterns for the future. Maybe it goes without saying, but of course because we've running our technology to many regions around the world, we have to work on deploying this infrastructure in all these places and push it out to the edge where possible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ã° The ConclusionÃ¢&lt;/head&gt;
    &lt;p&gt;I said a lot of things, so I to quickly want to quickly summarize our architecture that we have in place:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Third-party component reliability reviews. I can't stress this enough. Don't just assume that you can utilize something. And sometimes in order to achieve 5-nines, you actually have to remove components from your infrastructure. Some things are just not able to be utilized no matter what. Now maybe you can put it in some sort of async background, but it can't be on the critical path for your endpoints.&lt;/item&gt;
      &lt;item&gt;DNS failover and health checks. For places where you have an individual region or availability zone or cluster, having a full backup with a way to conclusively determine what's up and automatically failover is critical.&lt;/item&gt;
      &lt;item&gt;Edge compute where possible. There's a whole network out there of services that are running on top of the cloud providers, which help guarantee your capability to run as close to as possible to where your users are and reduce latency.&lt;/item&gt;
      &lt;item&gt;Incremental rollout for when you want to reduce the impact as much as possible.&lt;/item&gt;
      &lt;item&gt;The Web Application Firewall for handling those malicious requests.&lt;/item&gt;
      &lt;item&gt;Having a Customer Support Focus to enable escalating issues that outside your area of detection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And through seven years or so that we've been doing this and building up this architecture, there's a couple of things that we've learned:&lt;/p&gt;
    &lt;head rend="h3"&gt;Murphy's LawÃ¢&lt;/head&gt;
    &lt;p&gt;Everything fails all the time. There absolutely will be failures everywhere. Every line of code, every component you pull in, every library, there's guaranteed to be a problem in each and everyone of those. And you will for sure have to deal with it, at some point. So being prepared to handle that situation, is something you have to be thinking through in your design.&lt;/p&gt;
    &lt;head rend="h3"&gt;DNSÃ¢&lt;/head&gt;
    &lt;p&gt;DNS, yeah, AWS will say it, everyone out there will say, and now we get to say it. The global DNS architecture is pretty good and reliable for a lot of scenarios, but I worry that it's still a single point of failure in a lot of ways.&lt;/p&gt;
    &lt;head rend="h3"&gt;Infrastructure as Code (IAC)Ã¢&lt;/head&gt;
    &lt;p&gt;The last thing is infrastructure as code challenges. We deploy primary regions, but then there's also the backup regions, which are slightly different from the primary regions, and then there are edge compute, which are, again, even more slightly different. And then sometimes, we do this ridiculous thing, where we deploy infrastructure dedicated to one customers. And in doing so, we're running some sort of IaC to deploy those resources.&lt;/p&gt;
    &lt;p&gt;It is almost exactly the same architecture. Almost! Because it isn't exactly the same there are quite the opportunities for challenges to sneak it. That's problematic with even Open Tofu or CloudFormation, and often these tools make it more difficult, not less. And good luck to you, if you're still using some else that hasn't been modernized. With those, it's even easier to run into problems and not get it exactly correct.&lt;/p&gt;
    &lt;p&gt;The last thing I want to leave you with is, well, With all of these, is that actually sufficient to achieve five nines?&lt;/p&gt;
    &lt;p&gt;No. Our commitment is 5-nines, what we do is in defense of that, just because you do all these things doesn't automatically mean your promise of 5-nines in guaranteed. And you know what, you too can promise a 5-nines SLA without doing anything. You'll likely break your promise, but for us our promise is important, and so this is our defense.&lt;/p&gt;
    &lt;p&gt;For help understanding this article or how you can implement auth and similar security architectures in your services, feel free to reach out to me via the community server.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45955565</guid><pubDate>Mon, 17 Nov 2025 17:07:17 +0000</pubDate></item><item><title>Azure hit by 15 Tbps DDoS attack using 500k IP addresses</title><link>https://www.bleepingcomputer.com/news/microsoft/microsoft-aisuru-botnet-used-500-000-ips-in-15-tbps-azure-ddos-attack/</link><description>&lt;doc fingerprint="a111bb89381e2d9a"&gt;
  &lt;main&gt;
    &lt;p&gt;Microsoft said today that the Aisuru botnet hit its Azure network with a 15.72 terabits per second (Tbps) DDoS attack, launched from over 500,000 IP addresses.&lt;/p&gt;
    &lt;p&gt;The attack used extremely high-rate UDP floods that targeted a specific public IP address in Australia, reaching nearly 3.64 billion packets per second (bpps).&lt;/p&gt;
    &lt;p&gt;"The attack originated from Aisuru botnet. Aisuru is a Turbo Mirai-class IoT botnet that frequently causes record-breaking DDoS attacks by exploiting compromised home routers and cameras, mainly in residential ISPs in the United States and other countries," said Azure Security senior product marketing manager Sean Whalen.&lt;/p&gt;
    &lt;p&gt;"These sudden UDP bursts had minimal source spoofing and used random source ports, which helped simplify traceback and facilitated provider enforcement."&lt;/p&gt;
    &lt;p&gt;Cloudflare linked the same botnet to a record-breaking 22.2 terabits per second (Tbps) DDoS attack that reached 10.6 billion packets per second (Bpps) and was mitigated in September 2025. This attack lasted only 40 seconds but was roughly equivalent to streaming one million 4K videos simultaneously.&lt;/p&gt;
    &lt;p&gt;One week earlier, the XLab research division of Chinese cybersecurity company Qi'anxin attributed another 11.5 Tbps DDoS attack to the Aisuru botnet, saying that it was controlling around 300,000 bots at the time.&lt;/p&gt;
    &lt;p&gt;The botnet targets security vulnerabilities in IP cameras, DVRs/NVRs, Realtek chips, and routers from T-Mobile, Zyxel, D-Link, and Linksys. As XLab researchers said, it suddenly ballooned in size in April 2025 after its operators breached a TotoLink router firmware update server and infected approximately 100,000 devices.&lt;/p&gt;
    &lt;p&gt;Infosec journalist Brian Krebs reported earlier this month that Cloudflare removed multiple domains linked to the Aisuru botnet from its public "Top Domains" rankings of the most frequently requested websites (based on DNS query volume) after they began overtaking legitimate sites, such as Amazon, Microsoft, and Google.&lt;/p&gt;
    &lt;p&gt;The company stated that Aisuru's operators were deliberately flooding Cloudflare's DNS service (1.1.1.1) with malicious query traffic to boost their domain's popularity while undermining trust in the rankings. Cloudflare CEO Matthew Prince also confirmed that the botnet's behavior was severely distorting the ranking system and added that Cloudflare now redacts or completely hides suspected malicious domains to avoid similar incidents in the future.&lt;/p&gt;
    &lt;p&gt;As Cloudflare revealed in its 2025 Q1 DDoS Report in April, it mitigated a record number of DDoS attacks last year, with a 198% quarter-over-quarter jump and a massive 358% year-over-year increase.&lt;/p&gt;
    &lt;p&gt;In total, it blocked 21.3 million DDoS attacks targeting its customers throughout 2024, as well as another 6.6 million attacks targeting its own infrastructure during an 18-day multi-vector campaign.&lt;/p&gt;
    &lt;head rend="h2"&gt;Secrets Security Cheat Sheet: From Sprawl to Control&lt;/head&gt;
    &lt;p&gt;Whether you're cleaning up old keys or setting guardrails for AI-generated code, this guide helps your team build securely from the start.&lt;/p&gt;
    &lt;p&gt;Get the cheat sheet and take the guesswork out of secrets management.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45955900</guid><pubDate>Mon, 17 Nov 2025 17:39:15 +0000</pubDate></item><item><title>DESI's Dizzying Results</title><link>https://www.universetoday.com/articles/desis-dizzying-results</link><description>&lt;doc fingerprint="2fc81306fdbe4349"&gt;
  &lt;main&gt;
    &lt;p&gt;In March of 2024 the DESI collaboration dropped a bombshell on the cosmological community: slim but significant evidence that dark energy might be getting weaker with time. This was a stunning result delivered after years of painstaking analysis. ItÃ¢s not a bullet-proof result, but it doesnÃ¢t have to be to make our lives more interesting.&lt;/p&gt;
    &lt;p&gt;I know IÃ¢m late to the party on discussing this. And itÃ¢s okay, because 1) thereÃ¢s a lot to unpack in this kind of result and I wanted to take my time, and 2) itÃ¢s not like this result is going to get revised or even updated anytime soon, so weÃ¢ve got plenty of room to play with this.&lt;/p&gt;
    &lt;p&gt;LetÃ¢s start with the results themselves and how they got there. DESI stands for Dark Energy Spectroscopic Instrument. ItÃ¢s a roughly 4-meter telescope mounted on Kitt Peak in southeastern Arizona. ItÃ¢s a galaxy survey, and to accomplish this survey they have 5,000 robotically controlled fiber optic cables underneath the telescope. Every night, the telescope selects a patch of sky to observe, the robots position the fiber optic cables to align with the positions of galaxies within that patch, and the instrument records detailed information for each and every single one. Then they do the same thing the next night, and then the next, and then the next.&lt;/p&gt;
    &lt;p&gt;So far they have amassed a catalog of over 13 million galaxies, providing the largest and comprehensive survey of galaxy positions in history. And theyÃ¢re not even done! TheyÃ¢re aiming for 50 million galaxies once the survey is complete.&lt;/p&gt;
    &lt;p&gt;And let me tell you, those robotically controlled fiber optic cables are a huge game changer. In many ways DESI is the successor to an older survey, the Sloan Digital Sky Survey. That survey had a similar setup, except that instead of robots to move all those fibers every night, they had to use grad students. Probably cheaper, but still less efficient. (Note that I was never one of those unlucky Ã¢volunteersÃ¢ but I did hear horror stories.)&lt;/p&gt;
    &lt;p&gt;Sure, the DESI survey is less than 1% of all the galaxies in the observable volume of the cosmos, but itÃ¢s still pretty sizable. So what do you do with a map of a decent chunk of the entire universe?&lt;/p&gt;
    &lt;p&gt;IÃ¢m glad you didnÃ¢t ask, because IÃ¢m happy to answer. The arrangements of galaxies on very large scales tells us a lot about the universe. And one of the key things used in this new DESI analysis is a feature of the large-scale universe goes by the ungainly but super nerdy name of baryon acoustic oscillations, or BAO for short.&lt;/p&gt;
    &lt;p&gt;Check this out. Long ago the universe was much smaller, hotter, and denser than it is today. If youÃ¢re ever asked what the big bang theory is all about, thatÃ¢s pretty much it in a nutshell. In fact, billions of years ago, when the universe was only a few hundred thousand years old, it was so hot and dense (for those of you keeping score at home, a million times smaller than its present volume and thousands of degrees hotter) that all the matter was crammed together in the form of an energized plasma. This is the same state of matter as the body of the Sun or a lightning bolt, and it literally filled the universe.&lt;/p&gt;
    &lt;p&gt;Like any dense material, there were sound waves Ã¢ waves of pressure that crisscrossed the universe. Many of these sound waves were triggered by a competition between gravity and radiation. Dense clumps of matter would try to collapse under their own gravity, but then those clumps would get hot and the radiation they emitted would push them back out.&lt;/p&gt;
    &lt;p&gt;This seesawing effect went on and on, back and forth, until the plasma cooled down so much that the light was released. This meant that radiation could no longer play the game, and the back-and-forth sound waves got stuck mid seesaw. Wherever they were, they acted as a source of additional gravitation, a shell of slightly higher density.&lt;/p&gt;
    &lt;p&gt;In fact we even have pictures of these features, which are the baryon acoustic oscillations (or Ã¢super hot sound wavesÃ¢ if you prefer). The light that was emitted when this process stopped still exists today, and we can take pictures of it. ItÃ¢s called the cosmic microwave background, and a decade ago when a bunch of my friends were plugging away their fiber optic cables, I was a member of the Planck collaboration, which was a satellite to map the microwave background.&lt;/p&gt;
    &lt;p&gt;These shells of extra matter didnÃ¢t just go away. They stuck around, and slowly slowly slowly over billions of years more matter accumulated on those shells than the surrounding regions. Today, we see the imprint of the BAO in the form of shells of matter roughly 800 million light-years in diameter.&lt;/p&gt;
    &lt;p&gt;The cool part about all this is that the shells are whatÃ¢s called a standard ruler. We know how big the shells are supposed to be Ã¢ itÃ¢s a relatively straightforward calculation to transport the images we see in the microwave background to their sizes in the present day. And we can compare that expected value to how big they appear on the sky. And how big they appear on the sky depends on cosmology: on the properties, history, and evolution of the universe.&lt;/p&gt;
    &lt;p&gt;The new finding is that the BAO shells found by DESI are a little off. Their sizes donÃ¢t quite fit with our usual picture of cosmology. And they seem to fit better a picture of the universe where dark energy is evolving.&lt;/p&gt;
    &lt;p&gt;But what the heck is dark energy, and why is it so interesting that it might be evolving?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45955904</guid><pubDate>Mon, 17 Nov 2025 17:39:43 +0000</pubDate></item><item><title>Show HN: Kalendis â€“ Scheduling API (keep your UI, we handle timezones/DST)</title><link>https://kalendis.dev</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45956154</guid><pubDate>Mon, 17 Nov 2025 18:05:29 +0000</pubDate></item><item><title>An official atlas of North Korea</title><link>https://www.cartographerstale.com/p/an-official-atlas-of-north-korea</link><description>&lt;doc fingerprint="4f675bef84e5fad8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An official atlas of North Korea&lt;/head&gt;
    &lt;head rend="h3"&gt;A North Korean atlas that shows the world from the perspective of one of the most isolated countries on the planet.&lt;/head&gt;
    &lt;p&gt;What I bring you today is a real gem that I have been searching for a long time and have finally managed to get my hands on a copy1. It is a collection of 672 maps found in the Great Korean Encyclopaedia2, North Koreaâ€™s reference encyclopaedia. More specifically, it is an electronic edition published on CD in the first decade of the 2000s3.&lt;/p&gt;
    &lt;p&gt;Work on this encyclopaedia began in 1964, when Kim Il Sung established a compilation committee with the aim of bringing together all the knowledge and guidelines that a good Korean should follow4. The work spanned several decades, and the result was published in thirty volumes between 1995 and 2002. The encyclopaedia contained more than 100,000 words, 25,000 images and photographs, and 5,200 historical figures.&lt;/p&gt;
    &lt;p&gt;And maps, lots of maps.&lt;/p&gt;
    &lt;head rend="h3"&gt;The maps of Korea&lt;/head&gt;
    &lt;p&gt;According to the prevailing narrative in North Korea, the war was won by the communists and since then, the entire Korean peninsula has remained united under the rule of the Korean Workersâ€™ Party. Therefore, when looking at the maps in this atlas, it should come as no surprise that Korea is always shown as one country, with no reference to the other country that exists at the southern tip of the peninsula.&lt;/p&gt;
    &lt;p&gt;In addition to generic maps of Korea, like any good atlas, the encyclopaedia also includes detailed maps of each of the provinces and counties that make them up. Again, it makes no distinction between north and south, maintaining that narrative of unity.&lt;/p&gt;
    &lt;head rend="h3"&gt;World maps&lt;/head&gt;
    &lt;p&gt;When we delve into the global vision of North Korean cartography, things become even more interesting. So Iâ€™ll start simply with the world map.&lt;/p&gt;
    &lt;p&gt;This North Korean world map is centred on the Pacific Ocean, which gives Korea a privileged position on the global stage. This is nothing new, but what is new is how North Korea depicts its enemies. Can you spot them?&lt;/p&gt;
    &lt;p&gt;Yes, they are the only two countries painted in dark grey: the United States and Japan. This pattern can also be seen on the political map of Korea5, and is consistent on virtually all the political maps in the atlas. In the map of Europe, that you can see below, this colour is also used for the United Kingdom and France, but in this case, it is not consistent through all the political maps.&lt;/p&gt;
    &lt;p&gt;The representation of the continents is also of some interest. Apart from the idea of showing enemies in a consistent colour, I like the choice of projections. Instead of opting for the classic projections seen in Western cartography, the authors of these maps choose projections that better balance the shape and size of different countries. They take advantage of the fact that only one region of the globe needs to be represented.&lt;/p&gt;
    &lt;p&gt;The encyclopaedia also includes maps of all the oceans, which also incorporate ocean current patterns.&lt;/p&gt;
    &lt;head rend="h3"&gt;Country maps&lt;/head&gt;
    &lt;p&gt;This collection would not be complete without country maps. Here, once again, we find a strong emphasis on the geopolitical situation and North Koreaâ€™s view of the world. This is consistent with the political maps of each continent, but when viewed separately, it is even more evident.&lt;/p&gt;
    &lt;p&gt;First, the maps dedicated to enemies.&lt;/p&gt;
    &lt;p&gt;Beyond these obvious things, there are more subtle issues that can be understood by looking at the complete list. The only country that does not have a dedicated map is Israel. In fact, Israel does not appear under that name on any map, but the territory of Israel appears as Palestine on the map of Asia and on all maps of surrounding countries. In the one for Jordan, it is also clarified that Palestine is a territory under Israeli occupation.&lt;/p&gt;
    &lt;p&gt;Another curious detail is that the atlas includes a country with limited international recognition6: the Sahrawi Arab Democratic Republic of Western Sahara.&lt;/p&gt;
    &lt;p&gt;And well, although it may not be of general interest, as most of the readers are from countries with large English population, here you can see how some of these countries are represented.&lt;/p&gt;
    &lt;p&gt;If you are interested in any other map, please let me know in the comments and I can update the article.&lt;/p&gt;
    &lt;p&gt;Acknowledgement: I owe todayâ€™s article entirely to Pedro Zurita, the man behind the Mapoteca de pZZ, thanks to whom I got a copy of this fabulous atlas. I recommend that you follow him on Instagram, where he posts many maps, especially of Mexico, and on TikTok or YouTube, where he posts interesting videos on cartography and geography (in Spanish).&lt;/p&gt;
    &lt;p&gt;A few weeks ago, Pedro Zurita got me a copy. And it made me very, very happy. At the end of the article, I have included information so that you can follow him on social media.&lt;/p&gt;
    &lt;p&gt;ì¡°ì„ ëŒ€ë°±ê³¼ì‚¬ì „, by its title in Korean.&lt;/p&gt;
    &lt;p&gt;While researching, I noticed that there is a digital edition from 2001, although it is possible that the copy I have was published later. Considering that there are maps with Timor-Leste and Montenegro as independent countries, but South Sudan still appears united with Sudan, it is plausible that the collection of maps is from an edition between 2007 and 2010.&lt;/p&gt;
    &lt;p&gt;This is understood to be North Korean, but within North Koreaâ€™s narrative, Korea remains united.&lt;/p&gt;
    &lt;p&gt;On the second map of Korea, the political map, in the lower right-hand corner, part of Japan is painted in the same dark grey colour. This is a deliberate choice, as it clearly differs from the colour chosen for China and Russia on the same map.&lt;/p&gt;
    &lt;p&gt;No European country currently recognises Western Sahara as an independent country, and very few in Asia do. Most of the countries that recognise its independence are in Africa and Latin America. And, of course, North Korea. You can see the details here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45956176</guid><pubDate>Mon, 17 Nov 2025 18:07:17 +0000</pubDate></item><item><title>Show HN: PrinceJS â€“ 19,200 req/s Bun framework in 2.8 kB (built by a 13yo)</title><link>https://princejs.vercel.app</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45957402</guid><pubDate>Mon, 17 Nov 2025 19:45:00 +0000</pubDate></item><item><title>Compiling Ruby to machine language</title><link>https://patshaughnessy.net/2025/11/17/compiling-ruby-to-machine-language</link><description>&lt;doc fingerprint="c606289e40bbfb0a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compiling Ruby To Machine Language&lt;/head&gt;
    &lt;p&gt;I've started working on a new edition of Ruby Under a Microscope that covers Ruby 3.x. I'm working on this in my spare time, so it will take a while. Leave a comment or drop me a line and I'll email you when it's finished.&lt;/p&gt;
    &lt;p&gt;Hereâ€™s an excerpt from the completely new content for Chapter 4, about YJIT and ZJIT. Iâ€™m still finishing this upâ€¦ so this content is fresh off the page! Itâ€™s been a lot of fun for me to learn about how JIT compilers work and to brush up on my Rust skills as well. And itâ€™s very exciting to see all the impressive work the Ruby team at Shopify and other contributors have done to improve Rubyâ€™s runtime performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 4: Compiling Ruby To Machine Language&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Interpreting vs. Compiling Ruby Code&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Yet Another JIT (YJIT)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Virtual Machines and Actual Machines&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Counting Method and Block Calls&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;YJIT Blocks&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;YJIT Branch Stubs&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Executing YJIT Blocks and Branches&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Deferred Compilation&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Regenerating a YJIT Branch&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;YJIT Guards&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Adding Two Integers Using Machine Language&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Experiment 4-1: Which Code Does YJIT Optimize?&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;How YJIT Recompiles Code&lt;/cell&gt;
        &lt;cell&gt;22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Finding a Block Version&lt;/cell&gt;
        &lt;cell&gt;22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Saving Multiple Block Versions&lt;/cell&gt;
        &lt;cell&gt;24&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ZJIT, Rubyâ€™s Next Generation JIT&lt;/cell&gt;
        &lt;cell&gt;26&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Counting Method and Block Calls&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ZJIT Blocks&lt;/cell&gt;
        &lt;cell&gt;29&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Method Based JIT&lt;/cell&gt;
        &lt;cell&gt;31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Rust Inside of Ruby&lt;/cell&gt;
        &lt;cell&gt;33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Experiment 4-2: Reading ZJIT HIR and LIR&lt;/cell&gt;
        &lt;cell&gt;35&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Summary&lt;/cell&gt;
        &lt;cell&gt;37&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Counting Method and Block Calls&lt;/head&gt;
    &lt;p&gt;To find hot spots, YJIT counts how many times your program calls each function or block. When this count reaches a certain threshold, YJIT stops your program and converts that section of code into machine language. Later Ruby will execute the machine language version instead of the original YARV instructions.&lt;/p&gt;
    &lt;p&gt;To keep track of these counts, YJIT saves an internal counter nearby the YARV instruction sequence for each function or block.&lt;/p&gt;
    &lt;p&gt;Figure 4-5: YJIT saves information adjacent to each set of YARV instructions&lt;/p&gt;
    &lt;p&gt;Figure 4-5 shows the YARV instruction sequence the main Ruby compiler created for the sum += i block at (3) in Listing 4-1. At the top, above the YARV instructions, Figure 4-5 shows two YJIT related values: jit_entry and jit_entry_calls. As weâ€™ll see in a moment, jit_entry starts as a null value but will later hold a pointer to the machine language instructions YJIT produces for this Ruby block. Below jit_entry, Figure 4-5 also shows jit_entry_calls, YJITâ€™s internal counter.&lt;/p&gt;
    &lt;p&gt;Each time the program in Listing 4-1 calls this block, YJIT increments the value of jit_entry_calls. Since the range at (1) in Listing 4-1 spans from 1 through 40, this counter will start at zero and increase by 1 each time Range#each calls the block at (3).&lt;/p&gt;
    &lt;p&gt;When the jit_entry_calls reaches a particular threshold, YJIT will compile the YARV instructions into machine language. By default for small Ruby programs YJIT in Ruby 3.5 uses a threshold of 30. Larger programs, like Ruby on Rails web applications, will use a larger threshold value of 120. (You can also change the threshold by passing â€”yjit-call-threshold when you run your Ruby program.)&lt;/p&gt;
    &lt;head rend="h2"&gt;YJIT Blocks&lt;/head&gt;
    &lt;p&gt;While compiling your Ruby program, YJIT saves the machine language instructions it creates into YJIT blocks. YJIT blocks, which are distinct from Ruby blocks, each contain a sequence of machine language instructions for a range of corresponding YARV instructions. By grouping YARV instructions and compiling each group into a YJIT block, YJIT can produce more optimized code that is tailored to your programâ€™s behavior and avoid compiling code that your program doesnâ€™t need.&lt;/p&gt;
    &lt;p&gt;As weâ€™ll see next, a single YJIT block doesnâ€™t correspond to a Ruby function or block. YJIT blocks instead represent smaller sections of code: individual YARV instructions or a small range of YARV instructions. Each Ruby function or block typically consists of several YJIT blocks.&lt;/p&gt;
    &lt;p&gt;Letâ€™s see how this works for our example. After the program in Listing 4-1 executes the Ruby block at (3) 29 times, YJIT will increment the jit_entry_calls counter again, just before Ruby runs the block for the 30th time. Since jit_entry_calls reaches the threshold value of 30, YJIT triggers the compilation process.&lt;/p&gt;
    &lt;p&gt;YJIT compiles the first YARV instruction getlocal_WC_1 and saves machine language instructions that perform the same work as getlocal_WC_1 into a new YJIT block:&lt;/p&gt;
    &lt;p&gt;Figure 4-6: Creating a YJIT block&lt;/p&gt;
    &lt;p&gt;On the left side, Figure 4-6 shows the YARV instructions for the sum += i Ruby block. On the right, Figure 4-6 shows the new YJIT block corresponding to getlocal_WC_1.&lt;/p&gt;
    &lt;p&gt;Next, the YJIT compiler continues and compiles the second YARV instruction from the left side of Figure 4-7: getlocal_WC_0 at index 2.&lt;/p&gt;
    &lt;p&gt;Figure 4-7: Appending to a YJIT block&lt;/p&gt;
    &lt;p&gt;On the left side, Figure 4-7 shows the same YARV instructions for the sum += i Ruby block that we saw above in Figure 4-6. But now the two dotted arrows indicate that the YJIT block on the right contains the machine language instructions equivalent to both getlocal_WC_1 and getlocal_WC_0.&lt;/p&gt;
    &lt;p&gt;Letâ€™s take a look inside this new block. YJIT compiles or translates the Ruby YARV instructions into machine language instructions. In this example, running on my Mac laptop, YJIT writes the following machine language instructions into this new block:&lt;/p&gt;
    &lt;p&gt;Figure 4-8: The contents of one YJIT block&lt;/p&gt;
    &lt;p&gt;Figure 4-8 shows a closer view of the new YJIT block that appeared on the right side of Figures 4-6 and 4-7. Inside the block, Figure 4-8 shows the assembly language acronyms corresponding to the ARM64 machine language instructions that YJIT generated for the two YARV instructions shown on the left. The YARV instructions on the left are: getlocal_WC_1, which loads a value from a local variable located in the previous stack frame and saves it on the YARV stack, and getlocal_WC_0, which loads a local variable from the current stack from and also saves it on the YARV stack. The machine language instructions on the right side of Figure 4-8 perform the same task, loading these values into registers on my M1 microprocessor: x1 and x9. If youâ€™re curious and would like to learn more about what the machine language instructions mean and how they work, the section â€œAdding Two Integers Using Machine Languageâ€ discusses the instructions for this example in more detail.&lt;/p&gt;
    &lt;head rend="h2"&gt;YJIT Branch Stubs&lt;/head&gt;
    &lt;p&gt;Next, YJIT continues down the sequence of YARV instructions and compiles the opt_plus YARV instruction at index 4 in Figures 4-6 and 4-7. But this time, YJIT runs into a problem: It doesnâ€™t know the type of the addition arguments. That is, will opt_plus add two integers? Or two strings, floating point numbers, or some other types?&lt;/p&gt;
    &lt;p&gt;Machine language is very specific. To add two 64-bit integers on an M1 microprocessor, YJIT could use the adds assembly language instruction. But adding two floating pointer numbers would require different instructions. And, of course, adding or concatenating two strings is an entirely different operation altogether.&lt;/p&gt;
    &lt;p&gt;In order for YJIT to know which machine language instructions to save into the YJIT block for opt_plus, YJIT needs to know exactly what type of values the Ruby program might ever add at (3) in Listing 4-1. You and I can tell by reading Listing 4-1 that the Ruby code is adding integers. We know right away that the sum += 1 block at (3) is always adding one integer to another. But YJIT doesnâ€™t know this.&lt;/p&gt;
    &lt;p&gt;YJIT uses a clever trick to solve this problem. Instead of analyzing the entire program ahead of time to determine all of the possible types of values the opt_plus YARV instruction might ever need to add, YJIT simply waits until the block runs and observes which types the program actually passes in.&lt;/p&gt;
    &lt;p&gt;YJIT uses branch stubs to achieve this wait-and-see compile behavior, as shown in Figure 4-9.&lt;/p&gt;
    &lt;p&gt;Figure 4-9: A YJIT block, branch and stub&lt;/p&gt;
    &lt;p&gt;Figure 4-9 shows the YARV instructions on the left, and the YJIT block for indexes 0000-0002 on the right. But note the bottom right corner of Figure 4-7, which shows an arrow pointing down from the block to a box labeled stub. This arrow represents a YJIT branch. Since this new branch doesnâ€™t point to a block yet, YJIT sets up the branch to point to a branch stub instead.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45957629</guid><pubDate>Mon, 17 Nov 2025 20:04:49 +0000</pubDate></item><item><title>Run ancient Unix on modern hardware</title><link>https://github.com/felipenlunkes/run-ancient-unix</link><description>&lt;doc fingerprint="b085fe091cbba1c6"&gt;
  &lt;main&gt;
    &lt;p&gt;The contents of this repository allow older versions of UNIX (ancient UNIX) to run easily on modern Unix-like systems (Linux, FreeBSD, macOS, among others).&lt;/p&gt;
    &lt;p&gt;At this time, you can run the following versions of UNIX:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;UNIX versions for PDP-11 (run on a PDP-11 simulator):&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;UNIX versions for x86:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Version 7 UNIX ported to x86 architecture by Robert Nordier (original port in 1999 and patches in 2006-2007).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First of all, credits and acknowledgment for material available in this repository that is not my own (or has been modified by me based on previous work).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The UNIX versions available in this repository have been released as open source under the Caldera license available in this repository. Please read the document carefully for concrete information about your rights and obligations when using the software. &lt;list rend="ul"&gt;&lt;item&gt;Note that various components within the system images may have been made available under other license conditions. Pay attention to these components. A clear example is version 2.11BSD UNIX, which features code covered by the Caldera license made available, in addition to code released under the BSD license. Source files available in the images show the license and due copyright. Check this data before reuse.&lt;/item&gt;&lt;item&gt;The UNIX images available in this repository were obtained from the w11 project (which uses these images for other purposes). You can get them directly here, as well as more information about the project, images, licenses and other data.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The scripts used to simulate the systems using SIMH for v5 and v7 UNIX were obtained from a w11 project repository, which can be accessed here. The original scripts are available under license GLP v3 or later. Modifications in these files were made by me, to fit the purpose of this repository. These modifications are restricted to the same license as the original script. &lt;list rend="ul"&gt;&lt;item&gt;In addition, the general script for configuring the execution environment of versions v5 and v7 was obtained from the project, authored by Walter F.J. Mueller. You can get the original script here. The original script are available under license GLP v3 or later. Modifications in these files were made by me, to fit the purpose of this repository. These modifications are restricted to the same license as the original script.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The port of Version 7 UNIX to the x86 architecture was performed by Robert Nordier. These modifications are released under the simplified BSD license. For more information on all aspects of the distribution, read this file.&lt;/item&gt;
      &lt;item&gt;All my contributions and modifications (except for material that requires redistribution under the same license, such as the running scripts) are available in this repository under the BSD-3-Clause license.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You will need the following tools and utilities to run the available UNIX versions:&lt;/p&gt;
    &lt;p&gt;First of all, you must have the &lt;code&gt;PDP-11 Simulator&lt;/code&gt; (SIMH), &lt;code&gt;qemu&lt;/code&gt;, &lt;code&gt;GNU bash&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;git&lt;/code&gt; installed on your device. If you already have them installed, skip to section 2.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To install on Debian, Ubuntu, Pop!_OS and derivatives, use:&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;sudo apt install simh qemu qemu-system-i386 git wget python3 python3-pip python3-tk
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;To install on Fedora and derivatives, use:&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;sudo dnf install simh qemu qemu-system-i386 git wget python3 python3-pip python3-tkinter
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;To install on FreeBSD, use (for FreeBSD, installing GNU bash is also required. This shell is not normally installed in a default installation. Installation of GNU bash is not required on Linux systems, where bash is already installed by default):&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;su root # &amp;lt;= Enter your password to login as root user
pkg install -q -y simh bash qemu git wget python3 py39-pip
ln -s /usr/local/bin/pip-3.9 /usr/local/bin/pip
pip install --upgrade pip
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;To install on NetBSD, use (for NetBSD, installing GNU bash is also required. This shell is not normally installed in a default installation. Installation of GNU bash is not required on Linux systems, where bash is already installed by default):&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;su root # &amp;lt;= Enter your password to login as root user
pkgin install simh bash qemu git wget python3 py39-pip
ln -s /usr/local/bin/pip-3.9 /usr/local/bin/pip
pip install --upgrade pip
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;To install on OpenBSD, use (for OpenBSD, installing GNU bash is also required. This shell is not normally installed in a default installation. Installation of GNU bash is not required on Linux systems, where bash is already installed by default):&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;su root # &amp;lt;= Enter your password to login as root user
pkg_add simh bash qemu git wget python3 py39-pip
ln -s /usr/local/bin/pip-3.9 /usr/local/bin/pip
pip install --upgrade pip
&lt;/code&gt;
    &lt;p&gt;After installation, proceed to section 2.&lt;/p&gt;
    &lt;p&gt;You must clone this repository to your computer. For that, use:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/felipenlunkes/run-ancient-unix
cd run-ancient-unix
&lt;/code&gt;
    &lt;p&gt;After cloning the repository with the configuration files, you must populate the directories of each UNIX version with their respective image files. For that, go to the next section.&lt;/p&gt;
    &lt;p&gt;Now, you have to run the available &lt;code&gt;run.sh&lt;/code&gt; script. For that, use:&lt;/p&gt;
    &lt;code&gt;chmod +x run.sh
./run.sh
&lt;/code&gt;
    &lt;p&gt;First, you have to run the script and select the option to install system images. You can also use the Python frontend to run the script. This is the easiest and simplest way to run script functions. To run this frontend and not rely on the command line, go to section 5. To continue the steps using the terminal, go to section 4.&lt;/p&gt;
    &lt;p&gt;You will see the following screen:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 
&lt;/code&gt;
    &lt;p&gt;In this case, you should select option &lt;code&gt;7&lt;/code&gt;, which will install the system images. After pressing 7, press ENTER to make your choice effective. Wait for the process of obtaining, extracting, configuring and installing the images.&lt;/p&gt;
    &lt;p&gt;After the installation is complete, you must run &lt;code&gt;run.sh&lt;/code&gt; again to start a UNIX version.&lt;/p&gt;
    &lt;p&gt;When running the script, you will be asked to choose one of the available UNIX versions. After typing only the number relative to the choice, press ENTER to make your decision effective. Then wait for the desired version to run.&lt;/p&gt;
    &lt;p&gt;Now, you need to know peculiarities in the execution of each version of the system. For this, go to section 6.&lt;/p&gt;
    &lt;p&gt;You need to start running the Python frontend that will manage the configuration and running of UNIX on your computer. First, you must install the TKinter Python package on your computer. For that, use:&lt;/p&gt;
    &lt;code&gt;pip install tk
&lt;/code&gt;
    &lt;p&gt;After that, you can press the &lt;code&gt;RAU.py&lt;/code&gt; script with the right button of your mouse and select the option of &lt;code&gt;Run as program&lt;/code&gt; or start the script from the terminal, using:&lt;/p&gt;
    &lt;code&gt;python3 RAU.py
&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;WARNING! The frontend is currently only compatible with the GNOME graphical environment (Linux and BSD systems). You can manually replace the&lt;/p&gt;&lt;code&gt;gnome-terminal&lt;/code&gt;calls with&lt;code&gt;konsole&lt;/code&gt;or another desired terminal emulator. Feel free to submit a pull request with any improvements or changes to the frontend.&lt;/quote&gt;
    &lt;p&gt;After running the program, you will see the following screen:&lt;/p&gt;
    &lt;p&gt;On first run, you must install the UNIX disk images locally on your computer. Prior to this operation, you will NOT be able to run UNIX. To do so, click on the &lt;code&gt;Install UNIX system images&lt;/code&gt; button.&lt;/p&gt;
    &lt;p&gt;After downloading and installing the disk images, you are able to run UNIX. To do so, select the desired UNIX version in the &lt;code&gt;Running options&lt;/code&gt; section of the frontend screen.&lt;/p&gt;
    &lt;p&gt;Go to the next section for more information about the specifics of running each version of UNIX available. Remember that when using the Python frontend, the command line selection screen, as shown in the next section, will not be displayed. However, the manual options and settings presented in the next section (after the selection screen, which will not appear) are still required to run each version of UNIX.&lt;/p&gt;
    &lt;p&gt;Select the desired UNIX version option below for details on how to start and operate the system. Each version of UNIX has different boot procedures. Pay attention to each particularity.&lt;/p&gt;
    &lt;head align="left"&gt;Particularities for Version 1 UNIX&lt;/head&gt;
    &lt;p&gt;After the start of execution after selecting v1 version, you will see a screen like below:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 1

PDP-11 simulator V3.8-1
Disabling CR
Disabling XQ
RF: buffering file in memory
TC0: 16b format, buffering file in memory

:login: 
&lt;/code&gt;
    &lt;p&gt;Just type &lt;code&gt;root&lt;/code&gt;, in lower case, and press ENTER. You will immediately be taken to the UNIX v1 shell.&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 1

PDP-11 simulator V3.8-1
Disabling CR
Disabling XQ
RF: buffering file in memory
TC0: 16b format, buffering file in memory

:login: root
root
# ls
bin
dev
etc
tmp
usr
# 
&lt;/code&gt;
    &lt;p&gt;To end the simulation, press CTRL-E followed by CTRL-C or by typing quit when the &lt;code&gt;simh&amp;gt;&lt;/code&gt; prompt appears on the screen.&lt;/p&gt;
    &lt;head align="left"&gt;Particularities for Version 5 UNIX&lt;/head&gt;
    &lt;p&gt;After the start of execution after selecting v5 version, you will see a screen like below:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 2

PDP-11 simulator V3.8-1
Disabling XQ
Logging to file "simh_dl0.log"
Listening on port 5671 (socket 5)
Listening on port 5672 (socket 7)
Modem control activated
@
&lt;/code&gt;
    &lt;p&gt;To start UNIX, you must type &lt;code&gt;unix&lt;/code&gt; and press ENTER after the @ character, without spaces and in lower case. After pressing ENTER, UNIX will load and you will be taken to a login screen as below:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 2

PDP-11 simulator V3.8-1
Disabling XQ
Logging to file "simh_dl0.log"
Listening on port 5671 (socket 5)
Listening on port 5672 (socket 7)
Modem control activated
@unix

login:
&lt;/code&gt;
    &lt;p&gt;You must then type &lt;code&gt;root&lt;/code&gt; and press ENTER. You will then be taken to the shell and be able to use the system. See below:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 2

PDP-11 simulator V3.8-1
Disabling XQ
Logging to file "simh_dl0.log"
Listening on port 5671 (socket 5)
Listening on port 5672 (socket 7)
Modem control activated
@unix

login: root
# 
&lt;/code&gt;
    &lt;p&gt;To end the simulation, press CTRL-E followed by CTRL-C or by typing quit when the &lt;code&gt;simh&amp;gt;&lt;/code&gt; prompt appears on the screen.&lt;/p&gt;
    &lt;head align="left"&gt;Particularities for Version 7 UNIX&lt;/head&gt;
    &lt;p&gt;After the start of execution after selecting v7 version, you will see a screen like below:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 3

PDP-11 simulator V3.8-1
Disabling XQ
Logging to file "simh_dl0.log"
Listening on port 5671 (socket 5)
Listening on port 5672 (socket 7)
Modem control activated
&lt;/code&gt;
    &lt;p&gt;After seeing the screen above, you must type &lt;code&gt;boot&lt;/code&gt; in lower case and press ENTER. You will see the screen below after that:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 3

PDP-11 simulator V3.8-1
Disabling XQ
Logging to file "simh_dl0.log"
Listening on port 5671 (socket 5)
Listening on port 5672 (socket 7)
Modem control activated
boot
Boot
:
&lt;/code&gt;
    &lt;p&gt;After the appearance of &lt;code&gt;:&lt;/code&gt;, you must type, without spaces and in lower case, the command &lt;code&gt;hp(0,0)unix&lt;/code&gt; and press ENTER, as below:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 3

PDP-11 simulator V3.8-1
Disabling XQ
Logging to file "simh_dl0.log"
Listening on port 5671 (socket 5)
Listening on port 5672 (socket 7)
Modem control activated
boot
Boot
: hp(0,0)unix
mem = 2020544
# 
&lt;/code&gt;
    &lt;p&gt;Pressing ENTER will immediately take you to the UNIX v7 shell.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; To enter multiuser mode and access all system functions, press CTRL-D. Afterwards, provide &lt;code&gt;root&lt;/code&gt;as username and password. You will again be taken to the UNIX v7 shell, as below:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 3

PDP-11 simulator V3.8-1
Disabling XQ
Logging to file "simh_dl0.log"
Listening on port 5671 (socket 5)
Listening on port 5672 (socket 7)
Modem control activated
boot
Boot
: hp(0,0)unix
mem = 2020544
# RESTRICTED RIGHTS: USE, DUPLICATION, OR DISCLOSURE
IS SUBJECT TO RESTRICTIONS STATED IN YOUR CONTRACT WITH
WESTERN ELECTRIC COMPANY, INC.
WED DEC 31 19:05:14 EST 1969

login: root
Password:
You have mail.
# 
&lt;/code&gt;
    &lt;p&gt;To end the simulation, press CTRL-E followed by CTRL-C or by typing quit when the &lt;code&gt;simh&amp;gt;&lt;/code&gt; prompt appears on the screen.&lt;/p&gt;
    &lt;head align="left"&gt;Particularities for 2.11BSD UNIX&lt;/head&gt;
    &lt;p&gt;After the start of execution after selecting 2.11BSD UNIX version, you will see a screen like below:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 4

PDP-11 simulator V3.8-1
Listening on port 4000 (socket 4)
Modem control activated
Auto disconnect activated
211bsd.simh&amp;gt; attach xq eth0
File open error
Disabling CR

73Boot from ra(0,0,0) at 0172150
: 
&lt;/code&gt;
    &lt;p&gt;You can just press ENTER when you see the screen to start UNIX. Afterwards, you will see the following screen:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 4

PDP-11 simulator V3.8-1
Listening on port 4000 (socket 4)
Modem control activated
Auto disconnect activated
211bsd.simh&amp;gt; attach xq eth0
File open error
Disabling CR

73Boot from ra(0,0,0) at 0172150
: 
: ra(0,0,0)unix
Boot: bootdev=02400 bootcsr=0172150

2.11 BSD UNIX #1: Fri Jun 9 08:42:54 PDT 1995
    root@SSU-64EN137:/usr/src/sys/SYSTEM

ra0: Ver 3 mod 3
ra0: RD54  size=311200
attaching qe0 csr 174440
qe0: DEC DELQA addr 00:50:56:01:01:01
attaching lo0

phys mem  = 3145728
avail mem = 1737664
user mem  = 307200

June  9 12:21:04 init: configure system

dz 0 csr 160100 vector 300 attached
ra 0 csr 172150 vector 154 vectorset attached
ts 0 csr 172520 vector 224 attached
erase, kill ^U, intr ^C
# 
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;#&lt;/code&gt; symbol indicates that the shell is ready to receive commands. Try using &lt;code&gt;uname -a&lt;/code&gt; or &lt;code&gt;ls&lt;/code&gt; to get started.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; To enter multiuser mode and access all system functions, press CTRL-D. Afterwards, provide &lt;code&gt;root&lt;/code&gt;as username and password. You will again be taken to the 2.11BSD shell.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To end the simulation, press CTRL-E followed by CTRL-C or by typing quit when the &lt;code&gt;simh&amp;gt;&lt;/code&gt; prompt appears on the screen.&lt;/p&gt;
    &lt;head align="left"&gt;Particularities for Version 7 UNIX for x86&lt;/head&gt;
    &lt;p&gt;After the start of execution after selecting v7 UNIX for x86, you will see a screen like below:&lt;/p&gt;
    &lt;code&gt;You must select, from the list below, which edition/version of
UNIX you want to start. The available options are:

1) v1 UNIX
2) v5 UNIX
3) v7 UNIX
4) 2.11BSD UNIX
5) v7 UNIX for x86
6) Clear temporary files
7) Install the disk images for UNIX

Select a number and press &amp;lt;ENTER&amp;gt;: 5
&lt;/code&gt;
    &lt;p&gt;Upon selection, &lt;code&gt;qemu&lt;/code&gt; will automatically start with the Version 7 UNIX for x86 disk image. After the initial boot, you will see the following screen:&lt;/p&gt;
    &lt;p&gt;Then press ENTER to load and start UNIX. After pressing ENTER, you will see the following screen, and you will be able to interact with the Version 7 UNIX shell:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; To enter multiuser mode and access all system functions, press CTRL-D. Afterwards, provide &lt;code&gt;root&lt;/code&gt;as username and&lt;code&gt;password&lt;/code&gt;as password. You will again be taken to the Version 7 UNIX shell.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you are finished running the system on the PDP-11 simulator, you can clean up temporary and log files that may have been created by SIMH. To do so, go to section 7.&lt;/p&gt;
    &lt;p&gt;The simulator can create temporary and log files to simulate peripheral devices that would be connected to a PDP-11 minicomputer. These files typically have &lt;code&gt;.log&lt;/code&gt; and &lt;code&gt;.dat&lt;/code&gt; extensions. You can remove these files using the &lt;code&gt;run.sh&lt;/code&gt; script and selecting the cleanup temporary files option, as well as manually going into each system directory and entering, in your system shell:&lt;/p&gt;
    &lt;code&gt;cd UNIX_VERSION_DIRECTORY
rm *.log *.dat
cd ..
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45958717</guid><pubDate>Mon, 17 Nov 2025 21:46:06 +0000</pubDate></item><item><title>Ion: Modern System Shell in Rust</title><link>https://github.com/redox-os/ion</link><description>&lt;doc fingerprint="62441e1b83073d84"&gt;
  &lt;main&gt;
    &lt;p&gt;Ion is a modern system shell that features a simple, yet powerful, syntax. It is written entirely in Rust, which greatly increases the overall quality and security of the shell. It also offers a level of performance that exceeds that of Dash, when taking advantage of Ion's features. While it is developed alongside, and primarily for, RedoxOS, it is a fully capable on other *nix platforms.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ion is still a WIP, and both its syntax and rules are subject to change over time. It is still quite a ways from becoming stabilized, but we are getting very close. Changes to the syntax at this time are likely to be minimal.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ion has a RFC process for language proposals. Ion's formal specification is located within the rfcs branch. The RFC process is still in the early stages of development, so much of the current and future implementation ideas have yet to be written into the specification.&lt;/p&gt;
    &lt;p&gt;The Ion manual online is generated automatically on each commit via mdBook and hosted on Redox OS's website.&lt;/p&gt;
    &lt;p&gt;Building the manual for local reference&lt;/p&gt;
    &lt;p&gt;Sources for the manual are located in the &lt;code&gt;manual&lt;/code&gt; directory.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build the documentation file for the builtins&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;make manual&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Then build the rest of the Ion manual via mdbook&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;mdbook build manual&lt;/code&gt;
    &lt;p&gt;Or you can build and open it in the your default browser via&lt;/p&gt;
    &lt;code&gt;mdbook serve manual --open&lt;/code&gt;
    &lt;p&gt;Or you can build and host the manual on your localhost via&lt;/p&gt;
    &lt;code&gt;mdbook serve manual &lt;/code&gt;
    &lt;p&gt;See the examples folder and the Parallelion project&lt;/p&gt;
    &lt;p&gt;The following PPA supports the 18.04 (bionic) and 19.04 (disco) releases. Bionic builds were made using the Pop_OS PPA's rustc 1.39.0 package.&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository ppa:mmstick76/ion-shell
&lt;/code&gt;
    &lt;p&gt;Those who are developing software with Rust should install the Rustup toolchain manager. After installing rustup, run &lt;code&gt;rustup override set 1.56.0&lt;/code&gt; to set your Rust toolchain to the version that Ion is
targeting at the moment. To build for Redox OS, &lt;code&gt;rustup override set nightly&lt;/code&gt; is required to build the Redox
dependencies.&lt;/p&gt;
    &lt;p&gt;Please ensure that both cargo and rustc 1.56.0 or higher is installed for your system. Release tarballs have not been made yet due to Ion being incomplete in a few remaining areas.&lt;/p&gt;
    &lt;code&gt;git clone https://gitlab.redox-os.org/redox-os/ion/
cd ion
cargo install --path=. --force &lt;/code&gt;
    &lt;p&gt;This way the ion executable will be installed into the folder "~/.cargo/bin"&lt;/p&gt;
    &lt;p&gt;As an alternative you can do it like this&lt;/p&gt;
    &lt;code&gt;git clone https://gitlab.redox-os.org/redox-os/ion/
cd ion
cargo build --release 
# Install to path which is included in the $PATH enviromnent variable
DESTDIR=~/.local/bin bash/install.sh&lt;/code&gt;
    &lt;code&gt;git clone https://gitlab.redox-os.org/redox-os/ion/
cd ion
cargo build --release 
sudo DESTDIR=/usr/local/bin bash/install.sh
# Optional: Do this if Ion shell shoulb be login shell on your system
sudo make update-shells prefix=/usr&lt;/code&gt;
    &lt;p&gt;There are plugins for ion. These plugins are additional aliases and function definitions written in Ion for Ion. They can be found under this repository.&lt;/p&gt;
    &lt;p&gt;For vim/nvim users there is an officially-supported syntax highlighting plugin.&lt;/p&gt;
    &lt;code&gt;Plugin 'vmchale/ion-vim'&lt;/code&gt;
    &lt;p&gt;For emacs users there is a kindly-supported syntax highlighting plugin.&lt;/p&gt;
    &lt;code&gt;(add-to-list 'load-path  (expand-file-name "/path/to/ion-mode"))
(require 'ion-mode)
(autoload 'ion-mode (locate-library "ion-mode") "Ion majore mode" t)
(add-to-list 'auto-mode-alist '("\\.ion\\'" . ion-mode))
(add-to-list 'auto-mode-alist '("/ion/initrc" . ion-mode))&lt;/code&gt;
    &lt;p&gt;There is a LSP-server for the scripting language of this shell. You can install the LSP-server via crates.io to get IDE support like error messages for an code editor or IDE which understands the client side of LSP. Link to LSP server on crates.io : https://crates.io/crates/ion_shell_lsp_server . The source code of the LSP server can be found here: https://gitlab.redox-os.org/redox-os/ion_lsp .&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45958815</guid><pubDate>Mon, 17 Nov 2025 21:56:58 +0000</pubDate></item><item><title>OOP: The worst thing that happened to programming</title><link>https://alexanderdanilov.dev/en/articles/oop</link><description>&lt;doc fingerprint="6e587b52a83917d8"&gt;
  &lt;main&gt;
    &lt;p&gt;In this article, we will try to understand why OOP is the worst thing that happened to programming, how it became so popular, why experienced Java (C#, C++, etc.) programmers canâ€™t really be considered great engineers, and why code in Java cannot be considered good.&lt;/p&gt;
    &lt;p&gt;Unfortunately, programming is quite far from being a science (just like me), so many terms can be interpreted differently. Letâ€™s first define them. I should warn you that these definitions are my subjective opinion, an attempt to bring order and fill in the gaps. Constructive criticism is welcome.&lt;/p&gt;
    &lt;head rend="h3"&gt;Definitions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[Data] structure â€” pure data that does not contain any processing logic (functions).&lt;/item&gt;
      &lt;item&gt;Function â€” a block of code that performs a specific logic. It can return a value.&lt;/item&gt;
      &lt;item&gt;Object â€” an entity that contains both data and functions to process them â€” methods. An object can be imitated in FP by placing data and functions in the same structure. In classical OOP, this is always an instance of a class.&lt;/item&gt;
      &lt;item&gt;Class â€” a blueprint for creating objects, defining their data and methods. The foundation of OOP.&lt;/item&gt;
      &lt;item&gt;Method â€” a function that is part of a class. Instance methods (non-static) have a reference to the object itself (&lt;code&gt;this&lt;/code&gt;,&lt;code&gt;self&lt;/code&gt;), with all its data and methods, which is essentially a implicit first argument.&lt;/item&gt;
      &lt;item&gt;Functional programming (FP) â€” programming using structures and functions. Do not confuse with functional (math) style.&lt;/item&gt;
      &lt;item&gt;Object-oriented programming (OOP) â€” programming using classes, objects, and all their features â€” inheritance, encapsulation, polymorphism, etc. If desired, one can mimic structures using classes [almost] without methods and functions with static methods in static classes.&lt;/item&gt;
      &lt;item&gt;Mutable style â€” a programming style where data is typically changed in place rather than copied. This can be used in both FP and OOP, but it is characteristic of OOP.&lt;/item&gt;
      &lt;item&gt;Immutable style â€” a programming style where data is typically NOT changed in place but new copies are created. This can be used in both FP and OOP, but it is not characteristic of OOP.&lt;/item&gt;
      &lt;item&gt;Procedural style â€” a style of FP in which functions operate only on their arguments (without closures) in a mutable style, without returning values.&lt;/item&gt;
      &lt;item&gt;Math (math-functional, often - just functional) style â€” a style of FP that features immutable style and pure functions â€” functions that always return the same result for the same arguments (in simple terms â€” they do not use external state), which is typical for mathematical functions (do not confuse with functions in programming).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Common Objections&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;OOP was originally designed as &amp;lt;something else&amp;gt;.&lt;/p&gt;
        &lt;p&gt;I don't see any point in discussing someone's long-forgotten fantasies; I base my argument on where we've ended up.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Functional and procedural programming are different things.&lt;/p&gt;&lt;p&gt;I often see FP being used when people actually mean PP, and I rarely encounter PP itself. The concept of a procedure is almost never used anymore, whereas the function is constantly used, likely due to keywords in popular languages like&lt;/p&gt;&lt;code&gt;function&lt;/code&gt;or&lt;code&gt;func&lt;/code&gt;. At the same time, I frequently encounter the term "functional style," so I decided to separate the concepts of paradigms and programming styles. I also try to avoid unnecessary concepts in this article, such as imperative and declarative styles.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Drawbacks&lt;/head&gt;
    &lt;p&gt;The first red flag that made me question the success of the OOP concept was a &lt;del&gt;childhood trauma&lt;/del&gt; task from my first OOP interview for a language I had just started learning at the time â€” C#, which has stayed in my memory to this day:&lt;/p&gt;
    &lt;head&gt;Task: What will be printed to the console when the program is run?&lt;/head&gt;
    &lt;code&gt;class Base
{
    static Base()
    {
        Console.WriteLine("Static Base");
    }

    public Base()
    {
        Console.WriteLine("Instance Base");
    }
}

class Program : Base
{
    public static readonly Program Instance;

    static Program()
    {
        Console.WriteLine("Static Program");
        Instance = new Program();
    }

    private Program()
    {
        Console.WriteLine("Instance Program");
    }

    static void Main()
    {
        Console.WriteLine("Main");
    }
}
&lt;/code&gt;
    &lt;p&gt;The first thought at the time was â€” how and why did the human mind come up with something like this â€” a program creates itself (what? o_0) from its own method, and then even darker magic begins â€” many other methods are called in a sequence that is almost impossible to predict without knowing this magic from the inside.&lt;/p&gt;
    &lt;p&gt;But that was just the beginning â€” next could have followed questions about the Singleton pattern and thread-safety of the program instance initialization via the static constructor, but we didn't get that far. And as it turned out later, even this code wasn't OOP enough â€” there are no factories or dependency injection containers.&lt;/p&gt;
    &lt;p&gt;At the time, though, I took it all with the mindset of "maybe itâ€™s needed for some reason," and I didnâ€™t have the competence to stop studying C# just yet. After failing the interview, I began delving deeper into the OOP jungle, gradually getting closer to the thought that "no, not needed."&lt;/p&gt;
    &lt;p&gt;To understand how good of an idea OOP really is, it's enough to analyze all the points that differentiate it from FP and compare their pros and cons. But in the end, the main difference is one â€” classes. So, letâ€™s start the analysis with the features of classes.&lt;/p&gt;
    &lt;p&gt;The following OOP examples will be written in either C# or TypeScript, while the FP examples will be in TypeScript. All statements refer to the classical approach, not various imitations of FP in OOP and vice versa.&lt;/p&gt;
    &lt;head rend="h4"&gt;Methods&lt;/head&gt;
    &lt;p&gt;The discussion will focus on instance methods, as static methods are essentially an imitation of FP. A simple example:&lt;/p&gt;
    &lt;code&gt;class User {
  firstName: string
  lastName?: string
  middleName?: string
  ... // Other fields not needed for getDisplayName.

  constructor(firstName: string, lastName?: string, middleName?: string) {
    this.firstName = firstName
    this.lastName = lastName
    this.middleName = middleName
  }

  // Method.
  getDisplayName() {
    return [this.firstName, this.middleName, this.lastName]
      .filter(Boolean)
      .join(" ")
  }
  
  ... // Other methods not needed for getDisplayName.
}

// Function.
const getDisplayName = (user: {firstName: string, lastName?: string, middleName?: string} | null | undefined) =&amp;gt; {
  if (!user) return undefined

  return [user.firstName, user.middleName, user.lastName]
    .filter(Boolean)
    .join(" ")
}

// Even more flexible, but may be less convenient.
const getDisplayName = (firstName: string, lastName?: string, middleName?: string) =&amp;gt; {
  ...
}
&lt;/code&gt;
    &lt;p&gt;How do the method and function &lt;code&gt;getDisplayName&lt;/code&gt; differ?&lt;/p&gt;
    &lt;p&gt;First, the method is tightly coupled with the type of its implicit argument â€” &lt;code&gt;this&lt;/code&gt;, which is &lt;code&gt;User&lt;/code&gt;. It depends not on the interface, but on the specific class. This leads to several problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reusability with other types: The method requires not only the data and methods it actually needs to function but also those it doesn't need, simply because they exist in the &lt;code&gt;User&lt;/code&gt;class â€” including all fields and methods of this class, even private ones. This means that anyone reusing the method must also include them, whether through inheritance (which is a major drawback â€” more on that later) or delegation. As a result, reusing the method with a different type while providing only the data and methods it actually uses is impossible.&lt;/item&gt;
      &lt;item&gt;Dependence on classes: The method cannot be used without creating an instance of this class or its descendant. For example, it cannot be used for a dictionary with the same fields.&lt;/item&gt;
      &lt;item&gt;Inability to handle situations where user is &lt;code&gt;null&lt;/code&gt;or&lt;code&gt;undefined&lt;/code&gt;from withing the method.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In JS/TS, you could of course hack this through &lt;code&gt;call&lt;/code&gt;/&lt;code&gt;apply&lt;/code&gt;, but these are hacks specific to the language, go against KISS, and are themselves a sign of bad code.&lt;/p&gt;
    &lt;code&gt;// OOP

class Dog extends Animal {
  firstName: string
  lastName?: string
  
  // How to reuse getDisplayName from User class?
}

({firstName: "Alexander"}).getDisplayName() // Error: object has no such method

let user: User | null
user.getDisplayName() // Error: null reference

// FP

getDisplayName({firstName:"Alexander"}) // Alexander

getDisplayName(new User("Alexander", "Danilov")) // Alexander Danilov

const dog: Dog  = {
  firstName: "Charlie",
  color: "black"
}
getDisplayName(dog) // Charlie

getDisplayName(undefined) // undefined
&lt;/code&gt;
    &lt;p&gt;Clearly, there are strong limitations when it comes to reuse, and it provokes bugs and worse programming practices.&lt;/p&gt;
    &lt;p&gt;In FP, function signatures are minimalist, containing only the necessary arguments, and their types essentially act as interfaces without requiring explicit implementation.&lt;/p&gt;
    &lt;p&gt;The second difference â€” method overriding. In some languages, there are several ways to override a method in a derived class, and in general, to forbid overriding. The person who came up with this obviously thought that there weren't enough ways to shoot oneself in the foot in OOP. Here's an example in C#:&lt;/p&gt;
    &lt;code&gt;public void GetDisplayName() // Cannot be overridden in subclasses.

public virtual void GetDisplayName() // Can be overridden.

public override void GetDisplayName() // Overriding the method in the subclass.

public sealed override void GetDisplayName() // Overriding the method in the subclass, but in future subclasses it cannot be overridden.

public new void GetDisplayName() // The wildest â€” the method called depends on the reference type itâ€™s called on (facepalm). If itâ€™s the parent type, the parent method will be called; if it's the type of instance with `new`, the method of that instance will be called.
&lt;/code&gt;
    &lt;p&gt;Method overriding equivalent in FP:&lt;/p&gt;
    &lt;code&gt;const getUserDisplayName = (user: ...) =&amp;gt; {...}

const getAdminDisplayName = (admin: ...) =&amp;gt; {
  if (...) {
    // In certain cases, reuse getUserDisplayName.
    return getUserDisplayName(admin) 
  }

  // Some unique logic for admin's name display.
  return ...
}
&lt;/code&gt;
    &lt;p&gt;Everything is as simple as it can be.&lt;/p&gt;
    &lt;head rend="h5"&gt;Conclusion:&lt;/head&gt;
    &lt;p&gt;It turns out that methods lose to functions in every way, except for one small thing related exclusively to development environments and the notation of their calls (weâ€™ll discuss this at the very end), have strong limitations when it comes to reuse in other types, and they also provoke worse programming practices, adding more opportunities to "shoot yourself in the foot" for no good reason. So, methods are garbage. Letâ€™s move on.&lt;/p&gt;
    &lt;head rend="h4"&gt;Inheritance&lt;/head&gt;
    &lt;p&gt;Regarding this feature, even among OOP developers, thereâ€™s a well-established rule â€” inheritance is an anti-pattern, and delegation should be preferred.&lt;/p&gt;
    &lt;p&gt;Why? Because, first of all, you canâ€™t inherit specific fields or methods â€” only the whole class. This problem even has its own name â€” The banana and monkey problem by Joe Armstrong: you wanted a banana, but it gave you a monkey holding the banana and the entire jungle.&lt;/p&gt;
    &lt;p&gt;Secondly â€” in most languages, you can only inherit from one class.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;// OOP

class User {
  id: string
  name: string
  surname: string
  address: string
  friends: User[]

  constructor(name: string, surname: string, address: string, friends: User[]) { â€¦ }

  getDisplayName() { â€¦ }

  hasFriend(id: string) { â€¦ }
}

// Bad: inheritance.
// Npc shouldn't have address, friends, and hasFriend.

class Npc extends User {
  constructor(name: string, surname: string) {
    super(name, surname, "", []) // We are forced to provide fields we don't need.
  }
}

// Bad: modifying the original code and breaking it into smaller classes.

class Nameable {
  name: string
  surname: string

  getDisplayName() { â€¦ }
}

class Friendable {
  friends: User[]
  hasFriend(id: string) { â€¦ }
}

// How to construct User without multiple inheritance? Delegation?
// Which class to inherit from and which to embed (delegate)? Or should we avoid inheritance altogether and embed both?
// What if these classes have some inaccessible private fields?
// Does anyone like this code? (rhetorical question).
class User {
  nameable: Nameable
  friendable: Friendable
}
&lt;/code&gt;
    &lt;p&gt;What we have:&lt;/p&gt;
    &lt;p&gt;To reuse something from an existing class, you either have to take everything it contains or rewrite the existing code and extract parts into other classes. But even in such cases, without multiple inheritance, it's impossible to properly construct classes. Multiple inheritance brings even more problems, and many popular OOP languages have abandoned it.&lt;/p&gt;
    &lt;code&gt;// FP

type BaseUser = {
  id: string
  name: string
  surname: string
}

// Union instead of inheritance.
type User = BaseUser &amp;amp; {
  address: string
  friendIds: string[]
}

// Alias.
type Npc = BaseUser

// Option without BaseUser. Even the base type isn't always needed â€” you can pick fields from another type.
type Npc = Pick&amp;lt;User, "id" | "name" | "surname"&amp;gt;

// We specify only whatâ€™s needed in the function, not even BaseUser, but only friendIds.
const hasFriend = (friendIds: string[], friendId: string) =&amp;gt; { â€¦ }

// Or require type with friendIds field.
const hasFriend = (target: { friendIds: string[] }, friendId: string) =&amp;gt; { â€¦ }

hasFriend(user, "123") // OK
hasFriend(npc, "123") // Compilation error: npc is of Npc type, which does not have friendIds.
&lt;/code&gt;
    &lt;p&gt;As we can see, the most correct option is to use not inheritance, and not even delegation, but composition of types (in TypeScript - union type, &lt;code&gt;Pick&lt;/code&gt;, &lt;code&gt;Omit&lt;/code&gt;, etc.). And if the structure contains all the fields necessary for calling a function, then there are no restrictions on calling that function.&lt;/p&gt;
    &lt;p&gt;Conclusion: inheritance adds many problems but solves none. Garbage, even by OOP standards.&lt;/p&gt;
    &lt;head rend="h4"&gt;Polymorphism&lt;/head&gt;
    &lt;p&gt;Polymorphism is the ability of a function to handle data of different types.&lt;/p&gt;
    &lt;p&gt;Classical polymorphism in OOP is implemented through inheritance in the worst case (where we encounter all the previously mentioned problems), and through interfaces in the best case â€” yet another OOP dilemma. With interfaces, the code won't depend on the specific implementation, but then you have to figure out where to store the default method implementations. And, of course, in both cases, there's a downside â€” the necessity of using classes (see the previous points).&lt;/p&gt;
    &lt;code&gt;using System;
using System.Collections.Generic;

// Abstract class.
abstract class Shape
{
    public abstract double GetArea();
}

// And/or interface.
interface IShape
{
    double GetArea();
}

class Circle : Shape
{
    public double Radius { get; }

    public Circle(double radius)
    {
        Radius = radius;
    }

    public override double GetArea()
    {
        return Math.PI * Radius * Radius;
    }
}

class Rectangle : Shape
{
    public double Width { get; }
    public double Height { get; }

    public Rectangle(double width, double height)
    {
        Width = width;
        Height = height;
    }

    public override double GetArea()
    {
        return Width * Height;
    }
}

// Factory for creating shapes from raw data.
class ShapeFactory
{
    public static Shape CreateShape(Dictionary&amp;lt;string, object&amp;gt; rawData)
    {
        if (!rawData.ContainsKey("type")) return null;

        string type = rawData["type"].ToString() ?? "";

        switch (type)
        {
            case "circle":
                if (rawData.TryGetValue("radius", out var radiusObj) &amp;amp;&amp;amp; radiusObj is double radius)
                    return new Circle(radius);
                break;

            case "rectangle":
                if (rawData.TryGetValue("width", out var widthObj) &amp;amp;&amp;amp; widthObj is double width &amp;amp;&amp;amp;
                    rawData.TryGetValue("height", out var heightObj) &amp;amp;&amp;amp; heightObj is double height)
                    return new Rectangle(width, height);
                break;
        }

        return null; // Unknown type
    }
}

class Program
{
    static void Main()
    {
        var rawShapes = new List&amp;lt;Dictionary&amp;lt;string, object&amp;gt;&amp;gt;
        {
            new Dictionary&amp;lt;string, object&amp;gt; { { "type", "circle" }, { "radius", 5.0 } },
            new Dictionary&amp;lt;string, object&amp;gt; { { "type", "rectangle" }, { "width", 4.0 }, { "height", 6.0 } },
        };

        // First, we need to transform the raw data into the appropriate class instances using ShapeFactory.
        var shapes = rawShapes.ConvertAll(ShapeFactory.CreateShape);

        LogShapes(shapes);
    }

    static void LogShapes(List&amp;lt;Shape&amp;gt; shapes)
    {
      foreach (var shape in shapes)
      {
          Console.WriteLine($"Area: {shape.GetArea()}");
      }
    }
}
&lt;/code&gt;
    &lt;p&gt;In FP, parametric (true) polymorphism is used. In the next example union type, generics and interface are used for that:&lt;/p&gt;
    &lt;code&gt;type Circle = { type: "circle"; radius: number }
type Rectangle = { type: "rectangle"; width: number; height: number }
type Shape = Circle | Rectangle

const getArea = (shape: Shape): number =&amp;gt; {
  // Make sure the ESLint rule @typescript-eslint/switch-exhaustiveness-check, requiring exhaustive switch blocks, is enabled.
  // This code is fully typed and checked by the compiler.
  switch (shape.type) {
    case "circle":
      return Math.PI * shape.radius * shape.radius
    case "rectangle":
      return shape.width * shape.height
  }
}

// In the code of a single project, it's better to use the Shape type and the getArea implementation to avoid complicating the code.
const logShapes = (shapes: Shape[]) =&amp;gt; {
  shapes.forEach(shape =&amp;gt; console.log(`Area: ${getArea(shape)}`))
}

// Using raw data without unnecessary transformations.
logShapes([
  { type: "circle", radius: 5 },
  { type: "rectangle", width: 4, height: 6 },
])

// In a library, the function can be made more flexible by using generics and a getArea argument (which can be
// optional with a default implementation), and it doesn't care what type is provided.
const logShapes = &amp;lt;T,&amp;gt;(shapes: T[], getArea: (shape: T) =&amp;gt; number) =&amp;gt; {
  shapes.forEach(shape =&amp;gt; console.log(`Area: ${getArea(shape)}`))
}

logShapes(
  [
    { type: "circle", radius: 5 },
    { type: "rectangle", width: 4, height: 6 },
    { type: "triangle" }, // Compilation error: this type is not supported by getArea.
  ],
  getArea
)

// Example from Inheritance section is also polymorphism using interface.
// Here we handle any type which has friendIds: string[].
const hasFriend = (target: { friendIds: string[] }, friendId: string) =&amp;gt; { â€¦ }
&lt;/code&gt;
    &lt;p&gt;Conclusion: As we can see, polymorphism in FP is perfectly implemented without classes and all their drawbacks, and the code is simpler and more concise, even in traditional OOP examples. In real projects, when it's much more complex, and as the codebase grows, the difference only becomes more pronounced.&lt;/p&gt;
    &lt;head rend="h4"&gt;Encapsulation&lt;/head&gt;
    &lt;p&gt;Here, I'll quickly go over the analogs of &lt;code&gt;private&lt;/code&gt;, &lt;code&gt;public&lt;/code&gt;, etc., for classes in TypeScript for FP:&lt;/p&gt;
    &lt;code&gt;// The function is "public" because it is exported.

export const getDisplayName = () =&amp;gt; â€¦

// Not exported â€” accessible only within the file.

const capitalize = () =&amp;gt; â€¦

// Storing private data using closures.

const makeAccount = () =&amp;gt; {
  let balance = 0
  return {
    deposit: (amount: number) =&amp;gt; {
      if (amount &amp;lt; 0) { throw â€¦ }
      balance += amount
    },
    â€¦
  }
}

// Hiding fields using a private type.

const privateReducer = (state: PrivateState): PrivateState =&amp;gt; {
  // Inside the function we work with state using private fields.
}
 
// Export function with public type.
export const reducer = privateReducer as (state: State) =&amp;gt; State

// Making fields read-only by casting to a type, ensuring compile-time checks.

const readonlyArray = ["John"] as const
readonlyArray[0] = "Peter" // Compilation error.

// Using an immutable type with both runtime and compile-time checks.

const freezedArray = Object.freeze(["John"])
freezedArray[0] = "Peter" // Compilation error. If executed, it will also fail at runtime.
&lt;/code&gt;
    &lt;head rend="h4"&gt;Encapsulation&lt;/head&gt;
    &lt;p&gt;As we can see, there are no problems with encapsulation in FP, and all scenarios are quite simply implemented without the need for additional symbols like access modifiers. However, it is worth noting that encapsulation is often not only unnecessary but can even be harmful â€” it increases the amount of code, complicates testing, and slows application performance.&lt;/p&gt;
    &lt;p&gt;Conclusion: OOP does not implement encapsulation any better than FP.&lt;/p&gt;
    &lt;p&gt;With the main differences covered, let's take a look at some typical problems that often arise from using OOP:&lt;/p&gt;
    &lt;head rend="h4"&gt;Language Syntax&lt;/head&gt;
    &lt;p&gt;OOP languages are excessively complicated with redundant syntax that emerged as an attempt by language developers to address inherent OOP issues. Only partially, though, since it's impossible to fully resolve architectural flaws. Classes, abstract classes, static classes and methods, constructors, inheritance, interfaces, various method overloading, getters/setters, default method implementations in interfaces, access modifiers, annotations/attributes, and much more â€” all of this makes the learning curve of OOP languages significantly steeper. Moreover, many of these features overlap, forcing developers to spend even more time choosing the least bad option. As a result, a substantial part of development is spent not on solving business problems but on battling the language and its limitations.&lt;/p&gt;
    &lt;p&gt;FP languages, in contrast, have a much simpler syntax (especially if they are not in radically mathematical style), omitting almost all of the aforementioned complexities.&lt;/p&gt;
    &lt;head rend="h4"&gt;Design Patterns&lt;/head&gt;
    &lt;p&gt;The same can be said about the vast number of design patterns with fancy names. Many books have been written about them, and they are frequently asked about in interviews. But in reality, OOP design patterns are just workarounds that "heroically" attempt to partially fix one of OOPâ€™s inherent issues. For example, the Decorator pattern extends a class when inheritance is not an option.&lt;/p&gt;
    &lt;p&gt;In FP, once you understand these three techniques â€” 1. adding a function argument; 2. using a closure; 3. wrapping a function in another â€” you already know all the core patterns.&lt;/p&gt;
    &lt;head rend="h4"&gt;Constructors&lt;/head&gt;
    &lt;p&gt;In most OOP languages, you are constantly required to implement constructors with typical boilerplate code. In FP, this is a rare occurrence because data is separated from logic, and in most cases, creating an entity of any type is simply creating standard data structures like strings, arrays, or associative arrays:&lt;/p&gt;
    &lt;code&gt;type User = {
  id: string
  firstName: string
  lastName: string
  middleName?: string
  friendIds?: string[]
}

// There's no need to create a constructor function.
// The compiler will point out any issues if the provided field types don't match.
const user: User = {
  id: "1",
  firstName: "Alexander",
  lastName: "Danilov",
  friendIds: ["2"]
}
&lt;/code&gt;
    &lt;p&gt;Moreover, the next point is that it turns out that using constructors in OOP is an anti-pattern.&lt;/p&gt;
    &lt;head rend="h4"&gt;Containers and Dependency Injection&lt;/head&gt;
    &lt;p&gt;Unlike in FP, where most code resides in functions that are typically just exported and imported, in OOP, a large portion of the code is in non-static classes that need to be initialized. To address such inherent issue in OOP and initialize class objects in a way that's convenient and flexible beyond any real need, dependency injection containers were introduced. In short â€” it turns out that using constructors is an anti-pattern (doesn't it always seem that way in OOP?). Sooner or later, you will have to pass all dependencies to all class instances, which is why itâ€™s better to pass a single dependency container and initialize objects only through it.&lt;/p&gt;
    &lt;p&gt;Moreover, should a class even know that it is a singleton? For perfect flexibility, of course not. What if someone someday wants to make a singleton not a singleton? This has never happened in history, but why not write even more code, making it even more complicated?&lt;/p&gt;
    &lt;p&gt;In FP, itâ€™s true that there can be situations when a function has too many arguments, and it might be helpful to bind some of the functions with their arguments (dependencies), but this is done only when necessary.&lt;/p&gt;
    &lt;code&gt;// Most of the code is stored in stateless functions that don't require initialization, singletons, or dependency injection containers.
export const getDisplayName = ...

// Singleton is simply an exported initialized entity, without the need for a static instance initialization hack.
export const store = createStore(...)

const main = () =&amp;gt; {
  // If importing the store is not suitable, we can import the createStore function.
  const store = createStore()

  someWork(store)
  
  // If we don't want to pass the store through arguments later, we can use closures, for example.
  const someWorkWithStore = () =&amp;gt; someWork(store)
  
  // Now we use it without passing the store.
  someWorkWithStore()
}
&lt;/code&gt;
    &lt;p&gt;Testability is not an issue here â€” any import in TypeScript can be easily replaced in tests without any problems.&lt;/p&gt;
    &lt;head rend="h4"&gt;Serialization, Copying, Comparing&lt;/head&gt;
    &lt;p&gt;Since in FP data is separated from logic and is primarily either primitive type or composed of primitives, it is usually serializable by default. It can also be shallowly or deeply copied and compared without any extra code:&lt;/p&gt;
    &lt;code&gt;const user: User = {
  id: "1",
  firstName: "Alexander",
  lastName: "Danilov",
  friendIds: ["2"]
}

// Shallow copy with updated and added fields.
const updatedUser: User = {
  ...user,
  firstName: "Alex",
  middleName: "Alexandrovich", // Providing an optional field.
}

// Deep copy.
const clonedUser: User = structuredClone(user)

// Serialization to JSON.
const userJson: string = JSON.stringify(user)

// Deserialization from JSON.
const parsedUser: User = JSON.parse(userJson)

// Shallow comparison (function from 3rd-party library).
const areShallowEqual = shallowEqual(x, y)

// Deep comparison (function from 3rd-party library).
const areDeeplyEqual = deepEqual(x, y)
&lt;/code&gt;
    &lt;p&gt;In OOP languages, it is often necessary to implement serialization, copying and comparing functions in each individual class, which affects both the development speed and the bug-proneness of the code.&lt;/p&gt;
    &lt;head rend="h4"&gt;Working with Arrays&lt;/head&gt;
    &lt;p&gt;A class contains both data and methods to manipulate it. Following this logic, developers often write methods for working with, for example, &lt;code&gt;User&lt;/code&gt; inside the &lt;code&gt;User&lt;/code&gt; class, which may seem logical. But what if, in the future, we need to work with multiple users?&lt;/p&gt;
    &lt;code&gt;class User {
  update() {
    service.updateUser(this.id, ...) // E.g. here goes long async IO operation.
  }
}

// Is it correct to update an array of users like this?
// What should a beginner OOP developer come up with here?
for (let user of users) {
  user.update()
}
&lt;/code&gt;
    &lt;p&gt;This often leads to a mess of patterns and workarounds like batching to optimize this into a single request instead of many.&lt;/p&gt;
    &lt;p&gt;A simple solution in FP is to write the code to handle both arrays and individual objects:&lt;/p&gt;
    &lt;code&gt;const updateUsers = (data: User | User[]) =&amp;gt; {
  const users = Array.isArray(data) ? data : [data]

  // Now work with the array, sending one request.
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Multiple Models&lt;/head&gt;
    &lt;p&gt;This problem can be observed in the section about polymorphism â€” OOP encourages creating a separate domain model, different from the one used for data transfer, such as from the backend. The data comes serialized in a specific format, while in OOP, everything must be contained within classes with methods.&lt;/p&gt;
    &lt;p&gt;In FP, it is also possible to use a custom model, but in many cases, transforming data is either unnecessary or minimal because the data is "dumb" and serializable in both cases. The need for a separate domain model for the application rarely arises, and the backend model is often used directly up to the UI layer, ideally auto-generated.&lt;/p&gt;
    &lt;p&gt;Someone might argue that this isn't flexible and that changes in the backend model would require rewriting the application code. But in reality, it would need to be rewritten in both cases, and with a separate model, technical debt starts to accumulate if not addressed immediately. What OOP developers do is essentially overengineering. Moreover, changing several fields in the application code is done through refactoring in an IDE in seconds, compared to writing thousands of lines of unnecessary code.&lt;/p&gt;
    &lt;head rend="h4"&gt;Concurrency and Multithreading&lt;/head&gt;
    &lt;p&gt;A huge advantage of the math-functional style of FP is the support for concurrency without additional effort and synchronization. In OOP, it's common to work in a mutable style, which leads to the need to write very complex and error-prone synchronization code for data access. In a recent article, I even wrote that, to this day, most programmers, including creators of popular programming languages, still struggle with this.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;As we can see, OOP not only fails to solve any problem better than FP, but it also introduces a multitude of other issues, which are completely unsolvable with any "design patterns" or workarounds. It requires knowledge and usage of an enormous number of such patterns, many of which even prohibit the use of basic class features like constructors or inheritance. Many OOP developers have either forgotten why they do all this or never knew, diving straight into the framework's intricacies and doing it "because it's the norm". In the end, we have far more ugly, overcomplicated code, which can be succinctly described as "a monstrous collection of crutches."&lt;/p&gt;
    &lt;head rend="h5"&gt;Did anyone notice these drawbacks in early years of OOP language development?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many programming giants, like Linus Torvalds, quickly came to a similar conclusion, and the latter banned the use of C++ in the Linux kernel.&lt;/item&gt;
      &lt;item&gt;Even the creator of Java later admitted that adding classes was a mistake: â€œIf you could do Java over again, what would you change?â€ â€œIâ€™d leave out classes,â€ he replied. Later though softened to wording that the real problem was implementation inheritance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, there were those who gained incredible popularity by describing the principles and patterns of OOP. One of the most famous programmers, who is also an exceptionally poor coder, is Robert Martin with his SOLID OOP principles and the book Clean Code. In this article, you can evaluate how bad "clean" code by this "guru" of Java looks compared to a simple function in TypeScript and draw clear conclusions.&lt;/p&gt;
    &lt;p&gt;Of course, one cannot fail to mention the highly popular book on design patterns by the so-called "Gang of Four &lt;del&gt;Incompetents&lt;/del&gt;." Without delving into the fundamental problems of OOP and without finding a proper solution (such as abandoning classes), the authors focused on describing crutch-like patterns that often only partially and rather awkwardly address the architecturally inherent issues, thereby indirectly justifying and perpetuating the very concept of classes and OOP.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Is It So Popular?&lt;/head&gt;
    &lt;p&gt;There is still some sense, considering how many OOP languages there are today and how many developers use them, that the popularity of OOP isn't just based on Oracle's massive advertising campaign for Java in the past, or the fact that 99% of people &lt;del&gt;are idiots&lt;/del&gt; have IQs below 140. And indeed, there is one "advantage" â€” Autocomplete â€” the ability to see which functions can be called with a specific data type is so convenient that most people are willing to tolerate all the other shortcomings (in most cases, not even realizing them â€” see the point about 99% of people).&lt;/p&gt;
    &lt;p&gt;But the thing is, autocomplete for dot notation is not a feature of the language or even the programming paradigm. It is a 100% feature of specific IDE. But didnâ€™t FP developers think of creating something similar? Surprisingly (as far as I know), this feature was not available for functional languages for a long time. For example, in Haskell, there is a site called Hoogle to search for functions by name or approximate signature, but itâ€™s one thing to see it in an IDE in a fraction of a second and another to go to a website. Requiring developers to memorize thousands of functions for all types instead of providing a convenient suggestion is a serious drawback when using FP in most popular IDEs.&lt;/p&gt;
    &lt;p&gt;Today, of course, there are VS Code plugins for Haskell and other languages that make it easy to search for functions by providing one or even several parameters from the function's signature. However, in the same VS Code, by default, this functionality is not available for JS or TS (together the most popular programming languages).&lt;/p&gt;
    &lt;p&gt;We could also discuss method calls with dot notation, which first appeared in Simula 67 in the 1960s and was later adopted by most modern languages. Itâ€™s this notation that triggers two ways to call functions â€” with a dot, where the first argument is passed in a way different from the others, and without a dot, where all arguments are passed in the same way.&lt;/p&gt;
    &lt;p&gt;The very fact of having two ways to do the same thing is already a sign of poor architecture and additional headaches for the developer, creating room for bad decisions.&lt;/p&gt;
    &lt;p&gt;The same applies to parentheses â€” initially, it was a rather poor decision, as multiple calls in a row are hard to read, and it's precisely these that make autocompletion for arguments in functional programming inconvenient, without the dot notation.&lt;/p&gt;
    &lt;code&gt;thirdCall(c, secondCall(b, firstCall(a)))
&lt;/code&gt;
    &lt;p&gt;In such cases, it's common to see implementations of method chaining by returning objects, but this is just another hack and another way to do the same thing differently:&lt;/p&gt;
    &lt;code&gt;firstCall(a)
  .secondCall(b)
  .thirdCall(c)
&lt;/code&gt;
    &lt;p&gt;The question of what to replace method calls using dots and parentheses for arguments with, I suggest we discuss in the comments.&lt;/p&gt;
    &lt;p&gt;Which language would I like to highlight from the modern and popular ones, in which many of the mentioned problems are solved?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go completely lacks classes, although it has interface methods that can be called using a dot. I believe the creators of the language wanted it to become as popular as possible, so they decided to leave this flaw to not scare off all the Java developers right away.&lt;/item&gt;
      &lt;item&gt;TypeScript â€” today, you can write large applications in it without using classes at all, and almost all the unnecessary stuff can be turned off via a linter. It is one of the most convenient languages for FP, including in terms of typing capabilities â€” one of the most flexible and strict, leaving Java, C#, and others a decade behind.&lt;/item&gt;
      &lt;item&gt;Of course, C â€” the father of all C-like languages, free from the flaws of its maimed OOP descendant, is still relevant today.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I am absolutely sure that a person writing commercial code in a purely OOP language for more than 3-4 years, who hasn't noticed many of its problems and hasn't started thinking about transitioning or switching to FP â€” cannot be considered a skilled engineer. A true engineer always thinks about the simplest solutions, notices flaws and complexities, and cannot miss such a log in the eye.&lt;/p&gt;
    &lt;p&gt;Development environments, unfortunately, even today, in 2025, are heavily tuned for OOP and do not encourage FP, and for several decades have been provoking the worst programming practices. The culprits are companies like Microsoft and JvmBrains â€” some of the creators of those very maimed OOP languages and development environments. Also, these companies, along with Apple and Google, continue to create OOP languages like Swift, Dart, Kotlin, so it's not only that modern programming is far from being a science â€” the creation of modern languages is done by people who are far from it. That is why the situation won't improve anytime soon.&lt;/p&gt;
    &lt;p&gt;But there is progress, and even OOP languages are gradually incorporating functional practices, and there are already languages that are almost free from the listed problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advice&lt;/head&gt;
    &lt;p&gt;..from someone who has been writing code without classes for many years: use languages that do not have classes (Go), avoid classes if possible (TypeScript, Python), and avoid languages where classes are the core (Java, C#, C++ and others). Write functional code - as simple code as possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45959149</guid><pubDate>Mon, 17 Nov 2025 22:30:48 +0000</pubDate></item></channel></rss>