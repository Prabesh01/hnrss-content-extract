<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 14 Dec 2025 18:46:04 +0000</lastBuildDate><item><title>Recovering Anthony Bourdain's Li.st's</title><link>https://sandyuraz.com/blogs/bourdain/</link><description>&lt;doc fingerprint="3b6d9815a09e1a8c"&gt;
  &lt;main&gt;
    &lt;p&gt;üçØ At least 2 days ago&lt;/p&gt;
    &lt;p&gt;Loved reading through GReg TeChnoLogY Anthony Bourdain‚Äôs Lost Li.st‚Äôs and seeing the list of lost Anthony Bourdain li.st‚Äôs made me think on whether at least some of them we can recover.&lt;/p&gt;
    &lt;p&gt;Having worked in security and crawling space for majority of my career‚ÄîI don‚Äôt have the access nor permission to use the proprietary storages‚ÄîI thought we might be able to find something from publicly available crawl archives.&lt;/p&gt;
    &lt;p&gt;All of the code and examples lead to the source git repository. This article has also been discussed on hackernews. Also, a week before I published this, mirandom had the same idea as me and published their findings‚Äîgo check them out.&lt;/p&gt;
    &lt;p&gt;If Internet Archive had the partial list that Greg published, what about the Common Crawl? Reading through their documentation, it seems straightforward enough to get prefix index for Tony‚Äôs lists and grep for any sub-paths.&lt;/p&gt;
    &lt;p&gt; Putting something up with help of Claude to prove my theory, we have &lt;code&gt;commoncrawl_search.py&lt;/code&gt; that makes a single index request to a specific dataset and if any hits discovered, retrieve them from the public s3 bucket‚Äîsince they are small straight-up HTML documents, seems even more feasible than I had initially thought.
&lt;/p&gt;
    &lt;p&gt; Simply have a python version around 3.14.2 and install the dependencies from &lt;code&gt;requirements.txt&lt;/code&gt;. Run the below and we are in business. Now, below, you‚Äôll find the command I ran and then some manual archeological effort to prettify the findings.
&lt;/p&gt;
    &lt;code&gt;python commoncrawl_search.py "https://li.st/Bourdain*" --all --download&lt;/code&gt;
    &lt;p&gt; Any and all emphasis, missing punctuation, cool grammar is all by Anthony Bourdain. The only modifications I have made is to the layout, to represent &lt;code&gt;li.st&lt;/code&gt; as closely as possible with no changes to the content.
&lt;/p&gt;
    &lt;p&gt; From Greg‚Äôs page, let‚Äôs go and try each entry one by one, I‚Äôll put the table of what I wasn‚Äôt able to find in Common Crawl, but I would assume exists elsewhere‚ÄîI‚Äôd be happy to take another look. And no, none of this above has been written by AI, only the code since I don‚Äôt really care about &lt;code&gt;warcio&lt;/code&gt; encoding or writing the same python requests method for the Nth time. Enjoy!
&lt;/p&gt;
    &lt;p&gt;Cocaine&lt;/p&gt;
    &lt;p&gt;True Detective&lt;/p&gt;
    &lt;p&gt;Scripps Howard&lt;/p&gt;
    &lt;p&gt;Dinners where it takes the waiter longer to describe my food than it takes me to eat it.&lt;/p&gt;
    &lt;p&gt;Beer nerds&lt;/p&gt;
    &lt;p&gt;I admit it: my life doesn‚Äôt suck. Some recent views I‚Äôve enjoyed&lt;/p&gt;
    &lt;p&gt;Montana at sunset : There‚Äôs pheasant cooking behind the camera somewhere. To the best of my recollection some very nice bourbon. And it IS a big sky .&lt;/p&gt;
    &lt;p&gt;Puerto Rico: Thank you Jose Andres for inviting me to this beautiful beach!&lt;/p&gt;
    &lt;p&gt;Naxos: drinking ouzo and looking at this. Not a bad day at the office .&lt;/p&gt;
    &lt;p&gt;LA: My chosen final resting place . Exact coordinates .&lt;/p&gt;
    &lt;p&gt;Istanbul: raki and grilled lamb and this ..&lt;/p&gt;
    &lt;p&gt;Borneo: The air is thick with hints of durian, sambal, coconut..&lt;/p&gt;
    &lt;p&gt;Chicago: up early to go train #Redzovic&lt;/p&gt;
    &lt;p&gt;The Wire&lt;/p&gt;
    &lt;p&gt;Tinker, Tailor, Soldier, Spy (and its sequel : Smiley‚Äôs People)&lt;/p&gt;
    &lt;p&gt;Edge of Darkness (with Bob Peck and Joe Don Baker )&lt;/p&gt;
    &lt;p&gt;Dreamcasting across time with the living and the dead, this untitled, yet to be written masterwork of cinema, shot, no doubt, by Christopher Doyle, lives only in my imagination.&lt;/p&gt;
    &lt;p&gt;This guy&lt;/p&gt;
    &lt;p&gt;And this guy&lt;/p&gt;
    &lt;p&gt;All great films need:&lt;/p&gt;
    &lt;p&gt;The Oscar goes to..&lt;/p&gt;
    &lt;p&gt;And&lt;/p&gt;
    &lt;p&gt;If you bought these vinyls from an emaciated looking dude with an eager, somewhat distracted expression on his face somewhere on upper Broadway sometime in the mid 80‚Äôs, that was me . I‚Äôd like them back. In a sentimental mood.&lt;/p&gt;
    &lt;p&gt;material things I feel a strange, possibly unnatural attraction to and will buy (if I can) if I stumble across them in my travels. I am not a paid spokesperson for any of this stuff .&lt;/p&gt;
    &lt;p&gt;Vintage Persol sunglasses : This is pretty obvious. I wear them a lot. I collect them when I can. Even my production team have taken to wearing them.&lt;/p&gt;
    &lt;p&gt;19th century trepanning instruments: I don‚Äôt know what explains my fascination with these devices, designed to drill drain-sized holes into the skull often for purposes of relieving "pressure" or "bad humours". But I can‚Äôt get enough of them. Tip: don‚Äôt get a prolonged headache around me and ask if I have anything for it. I do.&lt;/p&gt;
    &lt;p&gt;Montagnard bracelets: I only have one of these but the few that find their way onto the market have so much history. Often given to the indigenous mountain people ‚Äôs Special Forces advisors during the very early days of America‚Äôs involvement in Vietnam .&lt;/p&gt;
    &lt;p&gt;Jiu Jitsi Gi‚Äôs: Yeah. When it comes to high end BJJ wear, I am a total whore. You know those people who collect limited edition Nikes ? I‚Äôm like that but with Shoyoroll . In my defense, I don‚Äôt keep them in plastic bags in a display case. I wear that shit.&lt;/p&gt;
    &lt;p&gt;Voiture: You know those old school, silver plated (or solid silver) blimp like carts they roll out into the dining room to carve and serve your roast? No. Probably not. So few places do that anymore. House of Prime Rib does it. Danny Bowein does it at Mission Chinese. I don‚Äôt have one of these. And I likely never will. But I can dream.&lt;/p&gt;
    &lt;p&gt;Kramer knives: I don‚Äôt own one. I can‚Äôt afford one . And I‚Äôd likely have to wait for years even if I could afford one. There‚Äôs a long waiting list for these individually hand crafted beauties. But I want one. Badly. http://www.kramerknives.com/gallery/&lt;/p&gt;
    &lt;p&gt;R. CRUMB : All of it. The collected works. These Taschen volumes to start. I wanted to draw brilliant, beautiful, filthy comix like Crumb until I was 13 or 14 and it became clear that I just didn‚Äôt have that kind of talent. As a responsible father of an 8 year old girl, I just can‚Äôt have this stuff in the house. Too dark, hateful, twisted. Sigh...&lt;/p&gt;
    &lt;p&gt;THE MAGNIFICENT AMBERSONS : THE UNCUT, ORIGINAL ORSON WELLES VERSION: It doesn‚Äôt exist. Which is why I want it. The Holy Grail for film nerds, Welles‚Äô follow up to CITIZEN KANE shoulda, coulda been an even greater masterpiece . But the studio butchered it and re-shot a bullshit ending. I want the original. I also want a magical pony.&lt;/p&gt;
    &lt;p&gt;I like good spy novels. I prefer them to be realistic . I prefer them to be written by real spies. If the main character carries a gun, I‚Äôm already losing interest. Spy novels should be about betrayal.&lt;/p&gt;
    &lt;p&gt; Ashenden‚ÄìSomerset Maugham&lt;lb/&gt;Somerset wrote this bleak, darkly funny, deeply cynical novel in the early part of the 20th century. It was apparently close enough to the reality of his espionage career that MI6 insisted on major excisions. Remarkably ahead of its time in its atmosphere of futility and betrayal. &lt;/p&gt;
    &lt;p&gt; The Man Who Lost the War‚ÄìWT Tyler&lt;lb/&gt;WT Tyler is a pseudonym for a former "foreign service" officer who could really really write. This one takes place in post-war Berlin and elsewhere and was, in my opinion, wildly under appreciated. See also his Ants of God. &lt;/p&gt;
    &lt;p&gt; The Human Factor‚ÄìGraham Greene&lt;lb/&gt;Was Greene thinking of his old colleague Kim Philby when he wrote this? Maybe. Probably. See also Our Man In Havana. &lt;/p&gt;
    &lt;p&gt; The Tears of Autumn -Charles McCarry&lt;lb/&gt;A clever take on the JFK assassination with a Vietnamese angle. See also The Miernik Dossier and The Last Supper &lt;/p&gt;
    &lt;p&gt; Agents of Innocence‚ÄìDavid Ignatius&lt;lb/&gt;Ignatius is a journalist not a spook, but this one, set in Beirut, hewed all too closely to still not officially acknowledged events. Great stuff. &lt;/p&gt;
    &lt;p&gt;I wake up in a lot of hotels, so I am fiercely loyal to the ones I love. A hotel where I know immediately wher I am when I open my eyes in the morning is a rare joy. Here are some of my favorites&lt;/p&gt;
    &lt;p&gt;CHATEAU MARMONT ( LA) : if I have to die in a hotel room, let it be here. I will work in LA just to stay at the Chateau.&lt;/p&gt;
    &lt;p&gt;CHILTERN FIREHOUSE (London): Same owner as the Chateau. An amazing Victorian firehouse turned hotel. Pretty much perfection&lt;/p&gt;
    &lt;p&gt;THE RALEIGH (Miami): The pool. The pool!&lt;/p&gt;
    &lt;p&gt;LE CONTINENTAL (Saigon): For the history.&lt;/p&gt;
    &lt;p&gt;HOTEL OLOFSSON (Port au Prince): Sagging, creaky and leaky but awesome .&lt;/p&gt;
    &lt;p&gt;PARK HYATT (Tokyo): Because I‚Äôm a film geek.&lt;/p&gt;
    &lt;p&gt;EDGEWATER INN (Seattle): kind of a lumber theme going on...ships slide right by your window. And the Led Zep "Mudshark incident".&lt;/p&gt;
    &lt;p&gt;THE METROPOLE (Hanoi): there‚Äôs a theme developing: if Graham Greene stayed at a hotel, chances are I will too.&lt;/p&gt;
    &lt;p&gt;GRAND HOTEL D'ANGKOR (Siem Reap): I‚Äôm a sucker for grand, colonial era hotels in Asia.&lt;/p&gt;
    &lt;p&gt;THE MURRAY (Livingston,Montana): You want the Peckinpah suite&lt;/p&gt;
    &lt;p&gt;from my phone&lt;/p&gt;
    &lt;p&gt;Bun Bo Hue&lt;/p&gt;
    &lt;p&gt;Kuching Laksa&lt;/p&gt;
    &lt;p&gt;Pot au Feu&lt;/p&gt;
    &lt;p&gt;Jamon&lt;/p&gt;
    &lt;p&gt;Linguine&lt;/p&gt;
    &lt;p&gt;Meat&lt;/p&gt;
    &lt;p&gt;Dessert&lt;/p&gt;
    &lt;p&gt;Light Lunch&lt;/p&gt;
    &lt;p&gt;Meat on a Stick&lt;/p&gt;
    &lt;p&gt;Oily Little Fish&lt;/p&gt;
    &lt;p&gt;Snack&lt;/p&gt;
    &lt;p&gt;Soup&lt;/p&gt;
    &lt;p&gt;Homage&lt;/p&gt;
    &lt;p&gt;Not TOO random&lt;/p&gt;
    &lt;p&gt;Madeline&lt;/p&gt;
    &lt;p&gt;Beirut&lt;/p&gt;
    &lt;p&gt;Musubi&lt;/p&gt;
    &lt;p&gt;BudaeJiggae&lt;/p&gt;
    &lt;p&gt;Dinner&lt;/p&gt;
    &lt;p&gt;Bootsy Collins&lt;/p&gt;
    &lt;p&gt;Bill Murray&lt;/p&gt;
    &lt;p&gt;Spaghetti a la bottarga . I would really, really like some of this. Al dente, lots of chili flakes&lt;/p&gt;
    &lt;p&gt;A big, greasy double cheeseburger. No lettuce. No tomato. Potato bun.&lt;/p&gt;
    &lt;p&gt;A street fair sausage and pepper hero would be nice. Though shitting like a mink is an inevitable and near immediate outcome&lt;/p&gt;
    &lt;p&gt;Some uni. Fuck it. I‚Äôll smear it on an English muffin at this point.&lt;/p&gt;
    &lt;p&gt;I wonder if that cheese is still good?&lt;/p&gt;
    &lt;p&gt;In which my Greek idyll is Suddenly invaded by professional nudists&lt;/p&gt;
    &lt;p&gt;Endemic FUPA. Apparently a prerequisite for joining this outfit.&lt;/p&gt;
    &lt;p&gt;Pistachio dick&lt;/p&gt;
    &lt;p&gt;70‚Äôs bush&lt;/p&gt;
    &lt;p&gt;T-shirt and no pants. Leading one to the obvious question : why bother?&lt;/p&gt;
    &lt;p&gt;Popeye‚Äôs Mac and Cheese&lt;/p&gt;
    &lt;p&gt;The cheesy crust on the side of the bowl of Onion Soup Gratinee&lt;/p&gt;
    &lt;p&gt;Macaroons . Not macarons . Macaroons&lt;/p&gt;
    &lt;p&gt;Captain Crunch&lt;/p&gt;
    &lt;p&gt;Double Double Animal Style&lt;/p&gt;
    &lt;p&gt;Spam Musubi&lt;/p&gt;
    &lt;p&gt;Aerosmith&lt;/p&gt;
    &lt;p&gt;Before he died, Warren Zevon dropped this wisdom bomb: "Enjoy every sandwich". These are a few locals I‚Äôve particularly enjoyed:&lt;/p&gt;
    &lt;p&gt;PASTRAMI QUEEN: (1125 Lexington Ave. ) Pastrami Sandwich. Also the turkey with Russian dressing is not bad. Also the brisket.&lt;/p&gt;
    &lt;p&gt;EISENBERG'S SANDWICH SHOP: ( 174 5th Ave.) Tuna salad on white with lettuce. I‚Äôd suggest drinking a lime Rickey or an Arnold Palmer with that.&lt;/p&gt;
    &lt;p&gt;THE JOHN DORY OYSTER BAR: (1196 Broadway) the Carta di Musica with Bottarga and Chili is amazing. Is it a sandwich? Yes. Yes it is.&lt;/p&gt;
    &lt;p&gt;RANDOM STREET FAIRS: (Anywhere tube socks and stale spices are sold. ) New York street fairs suck. The same dreary vendors, same bad food. But those nasty sausage and pepper hero sandwiches are a siren song, luring me, always towards the rocks. Shitting like a mink almost immediately after is guaranteed but who cares?&lt;/p&gt;
    &lt;p&gt;BARNEY GREENGRASS : ( 541 Amsterdam Ave.) Chopped Liver on rye. The best chopped liver in NYC.&lt;/p&gt;
    &lt;p&gt;A work in progress&lt;/p&gt;
    &lt;p&gt;SIBERIA in any of its iterations. The one on the subway being the best&lt;/p&gt;
    &lt;p&gt;LADY ANNES FULL MOON SALOON a bar so nasty I‚Äôd bring out of town visitors there just to scare them&lt;/p&gt;
    &lt;p&gt;THE LION'S HEAD old school newspaper hang out&lt;/p&gt;
    &lt;p&gt;KELLY'S on 43rd and Lex. Notable for 25 cent drafts and regularly and reliably serving me when I was 15&lt;/p&gt;
    &lt;p&gt;THE TERMINAL BAR legendary dive across from port authority&lt;/p&gt;
    &lt;p&gt;BILLY'S TOPLESS (later, Billy‚Äôs Stopless) an atmospheric, working class place, perfect for late afternoon drinking where nobody hustled you for money and everybody knew everybody. Great all-hair metal jukebox . Naked breasts were not really the point.&lt;/p&gt;
    &lt;p&gt;THE BAR AT HAWAII KAI. tucked away in a giant tiki themed nightclub in Times Square with a midget doorman and a floor show. Best place to drop acid EVER.&lt;/p&gt;
    &lt;p&gt;THE NURSERY after hours bar decorated like a pediatrician‚Äôs office. Only the nursery rhyme characters were punk rockers of the day.&lt;/p&gt;
    &lt;p&gt;It was surprising to see that only one page was not recoverable from the common crawl.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Title&lt;/cell&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;David Bowie Related&lt;/cell&gt;
        &lt;cell&gt;1/14/2016&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I‚Äôve enjoyed this little project tremendously‚Äîa little archeology project. Can we declare victory for at least this endeavor? Hopefully, we would be able to find images, but that‚Äôs a little tougher, since that era‚Äôs cloudfront is fully gone.&lt;/p&gt;
    &lt;p&gt;What else can we work on restoring and setting up some sort of a public archive to store them? I made this a git repository for the sole purpose so that anyone interested can contribute their interest and passion for these kinds of projects.&lt;/p&gt;
    &lt;p&gt;Thank you and until next time! ‚óºÔ∏é&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46258163</guid><pubDate>Sat, 13 Dec 2025 21:18:01 +0000</pubDate></item><item><title>Linux Sandboxes and Fil-C</title><link>https://fil-c.org/seccomp</link><description>&lt;doc fingerprint="ea639991fc2930e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linux Sandboxes And Fil-C&lt;/head&gt;
    &lt;p&gt;Memory safety and sandboxing are two different things. It's reasonable to think of them as orthogonal: you could have memory safety but not be sandboxed, or you could be sandboxed but not memory safe.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Example of memory safe but not sandboxed: a pure Java program that opens files on the filesystem for reading and writing and accepts filenames from the user. The OS will allow this program to overwrite any file that the user has access to. This program can be quite dangerous even if it is memory safe. Worse, imagine that the program didn't have any code to open files for reading and writing, but also had no sandbox to prevent those syscalls from working. If there was a bug in the memory safety enforcement of this program (say, because of a bug in the Java implementation), then an attacker could cause this program to overwrite any file if they succeeded at achieving code execution via weird state.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Example of sandboxed but not memory safe: a program written in assembly that starts by requesting that the OS revoke all of its capabilities beyond just pure compute. If the program did want to open a file or write to it, then the kernel will kill the process, based on the earlier request to have this capability revoked. This program could have lots of memory safety bugs (because it's written in assembly), but even if it did, then the attacker cannot make this program overwrite any file unless they find some way to bypass the sandbox.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In practice, sandboxes have holes by design. A typical sandbox allows the program to send and receive messages to broker processes that have higher privileges. So, an attacker may first use a memory safety bug to make the sandboxed process send malicious messages, and then use those malicious messages to break into the brokers.&lt;/p&gt;
    &lt;p&gt;The best kind of defense is to have both a sandbox and memory safety. This document describes how to combine sandboxing and Fil-C's memory safety by explaining what it takes to port OpenSSH's seccomp-based Linux sandbox code to Fil-C.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Fil-C is a memory safe implementation of C and C++ and this site has a lot of documentation about it. Unlike most memory safe languages, Fil-C enforces safety down to where your code meets Linux syscalls and the Fil-C runtime is robust enough that it's possible to use it in low-level system components like &lt;code&gt;init&lt;/code&gt; and &lt;code&gt;udevd&lt;/code&gt;. Lots of programs work in Fil-C, including OpenSSH, which makes use of seccomp-BPF sandboxing.&lt;/p&gt;
    &lt;p&gt;This document focuses on how OpenSSH uses seccomp and other technologies on Linux to build a sandbox around its unprivileged &lt;code&gt;sshd-session&lt;/code&gt; process. Let's review what tools Linux gives us that OpenSSH uses:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;chroot&lt;/code&gt;to restrict the process's view of the filesystem.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running the process with the&lt;/p&gt;&lt;code&gt;sshd&lt;/code&gt;user and group, and giving that user/group no privileges.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setrlimit&lt;/code&gt;to prevent opening files, starting processes, or writing to files.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;seccomp-BPF syscall filter to reduce the attack surface by allowlisting only the set of syscalls that are legitimate for the unprivileged process. Syscalls not in the allowlist will crash the process with&lt;/p&gt;&lt;code&gt;SIGSYS&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Chromium developers and the Mozilla developers both have excellent notes about how to do sandboxing on Linux using seccomp. Seccomp-BPF is a well-documented kernel feature that can be used as part of a larger sandboxing story.&lt;/p&gt;
    &lt;p&gt;Fil-C makes it easy to use &lt;code&gt;chroot&lt;/code&gt; and different users and groups. The syscalls that are used for that part of the sandbox are trivially allowed by Fil-C and no special care is required to use them.&lt;/p&gt;
    &lt;p&gt;Both &lt;code&gt;setrlimit&lt;/code&gt; and seccomp-BPF require special care because the Fil-C runtime starts threads, allocates memory, and performs synchronization. This document describes what you need to know to make effective use of those sandboxing technologies in Fil-C. First, I describe how to build a sandbox that prevents thread creation without breaking Fil-C's use of threads. Then, I describe what tweaks I had to make to OpenSSH's seccomp filter. Finally, I describe how the Fil-C runtime implements the syscalls used to install seccomp filters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preventing Thread Creation Without Breaking The Fil-C Runtime&lt;/head&gt;
    &lt;p&gt;The Fil-C runtime uses multiple background threads for garbage collection and has the ability to automatically shut those threads down when they are not in use. If the program wakes up and starts allocating memory again, then those threads are automatically restarted.&lt;/p&gt;
    &lt;p&gt;Starting threads violates the "no new processes" rule that OpenSSH's &lt;code&gt;setrlimit&lt;/code&gt; sandbox tries to achieve (since threads are just lightweight processes on Linux). It also relies on syscalls like &lt;code&gt;clone3&lt;/code&gt; that are not part of OpenSSH's seccomp filter allowlist.&lt;/p&gt;
    &lt;p&gt;It would be a regression to the sandbox to allow process creation just because the Fil-C runtime relies on it. Instead, I added a new API to &lt;code&gt;&amp;lt;stdfil.h&amp;gt;&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;void zlock_runtime_threads(void);
&lt;/code&gt;
    &lt;p&gt;This forces the runtime to immediately create whatever threads it needs, and to disable shutting them down on demand. Then, I added a call to &lt;code&gt;zlock_runtime_threads()&lt;/code&gt; in OpenSSH's &lt;code&gt;ssh_sandbox_child&lt;/code&gt; function before either the &lt;code&gt;setrlimit&lt;/code&gt; or seccomp-BPF sandbox calls happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tweaks To The OpenSSH Sandbox&lt;/head&gt;
    &lt;p&gt;Because the use of &lt;code&gt;zlock_runtime_threads()&lt;/code&gt; prevents subsequent thread creation from happening, most of the OpenSSH sandbox just works. I did not have to change how OpenSSH uses &lt;code&gt;setrlimit&lt;/code&gt;. I did change the following about the seccomp filter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Failure results in&lt;/p&gt;&lt;code&gt;SECCOMP_RET_KILL_PROCESS&lt;/code&gt;rather than&lt;code&gt;SECCOMP_RET_KILL&lt;/code&gt;. This ensures that Fil-C's background threads are also killed if a sandbox violation occurs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MAP_NORESERVE&lt;/code&gt;is added to the&lt;code&gt;mmap&lt;/code&gt;allowlist, since the Fil-C allocator uses it. This is not a meaningful regression to the filter, since&lt;code&gt;MAP_NORESERVE&lt;/code&gt;is not a meaningful capability for an attacker to have.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sched_yield&lt;/code&gt;is allowed. This is not a dangerous syscall (it's semantically a no-op). The Fil-C runtime uses it as part of its lock implementation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nothing else had to change, since the filter already allowed all of the &lt;code&gt;futex&lt;/code&gt; syscalls that Fil-C uses for synchronization.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Fil-C Implements &lt;code&gt;prctl&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The OpenSSH seccomp filter is installed using two &lt;code&gt;prctl&lt;/code&gt; calls. First, we &lt;code&gt;PR_SET_NO_NEW_PRIVS&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) == -1) {
        debug("%s: prctl(PR_SET_NO_NEW_PRIVS): %s",
            __func__, strerror(errno));
        nnp_failed = 1;
}
&lt;/code&gt;
    &lt;p&gt;This prevents additional privileges from being acquired via &lt;code&gt;execve&lt;/code&gt;. It's required that unprivileged processes that install seccomp filters first set the &lt;code&gt;no_new_privs&lt;/code&gt; bit.&lt;/p&gt;
    &lt;p&gt;Next, we &lt;code&gt;PR_SET_SECCOMP, SECCOMP_MODE_FILTER&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;if (prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, &amp;amp;preauth_program) == -1)
        debug("%s: prctl(PR_SET_SECCOMP): %s",
            __func__, strerror(errno));
else if (nnp_failed)
        fatal("%s: SECCOMP_MODE_FILTER activated but "
            "PR_SET_NO_NEW_PRIVS failed", __func__);
&lt;/code&gt;
    &lt;p&gt;This installs the seccomp filter in &lt;code&gt;preauth_program&lt;/code&gt;. Note that this will fail in the kernel if the &lt;code&gt;no_new_privs&lt;/code&gt; bit is not set, so the fact that OpenSSH reports a fatal error if the filter is installed without &lt;code&gt;no_new_privs&lt;/code&gt; is just healthy paranoia on the part of the OpenSSH authors.&lt;/p&gt;
    &lt;p&gt;The trouble with both syscalls is that they affect the calling thread, not all threads in the process. Without special care, Fil-C runtime's background threads would not have the &lt;code&gt;no_new_privs&lt;/code&gt; bit set and would not have the filter installed. This would mean that if an attacker busted through Fil-C's memory safety protections (in the unlikely event that they found a bug in Fil-C itself!), then they could use those other threads to execute syscalls that bypass the filter!&lt;/p&gt;
    &lt;p&gt;To prevent even this unlikely escape, the Fil-C runtime's wrapper for &lt;code&gt;prctl&lt;/code&gt; implements &lt;code&gt;PR_SET_NO_NEW_PRIVS&lt;/code&gt; and &lt;code&gt;PR_SET_SECCOMP&lt;/code&gt; by handshaking all runtime threads using this internal API:&lt;/p&gt;
    &lt;code&gt;/* Calls the callback from every runtime thread. */
PAS_API void filc_runtime_threads_handshake(void (*callback)(void* arg), void* arg);
&lt;/code&gt;
    &lt;p&gt;The callback performs the requested &lt;code&gt;prctl&lt;/code&gt; from each runtime thread. This ensures that the &lt;code&gt;no_new_privs&lt;/code&gt; bit and the filter are installed on all threads in the Fil-C process.&lt;/p&gt;
    &lt;p&gt;Additionally, because of ambiguity about what to do if the process has multiple user threads, these two &lt;code&gt;prctl&lt;/code&gt; commands will trigger a Fil-C safety error if the program has multiple user threads.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The best kind of protection if you're serious about security is to combine memory safety with sandboxing. This document shows how to achieve this using Fil-C and the sandbox technologies available on Linux, all without regressing the level of protection that those sandboxes enforce or the memory safety guarantees of Fil-C.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46259064</guid><pubDate>Sat, 13 Dec 2025 22:58:29 +0000</pubDate></item><item><title>An Implementation of J (1992)</title><link>https://www.jsoftware.com/ioj/ioj.htm</link><description>&lt;doc fingerprint="8f2792390b4c60ef"&gt;
  &lt;main&gt;&lt;p&gt; An Implementation of J&lt;lb/&gt; Roger K.W. Hui &lt;/p&gt;&lt;p&gt;Preface&lt;/p&gt;J is a dialect of APL freely available on a wide variety of machines. It is the latest in the line of development known as "dictionary APL". The spelling scheme uses the ASCII alphabet. The underlying concepts, such as arrays, verbs, adverbs, and rank, are extensions and generalizations of ideas in APL\360. Anomalies have been removed. The result is at once simpler and more powerful than previous dialects.&lt;p&gt;Ex ungue leonem.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;0. Introduction&lt;/cell&gt;&lt;cell&gt;6. Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;6.1 Numeric Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1. Interpreting a Sentence&lt;/cell&gt;&lt;cell&gt;6.2 Boxed Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.1 Word Formation&lt;/cell&gt;&lt;cell&gt;6.3 Formatted Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.2 Parsing&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.3 Trains&lt;/cell&gt;&lt;cell&gt;7. Comparatives&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.4 Name Resolution&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Appendices&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2. Nouns&lt;/cell&gt;&lt;cell&gt;A. Incunabulum&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.1 Arrays&lt;/cell&gt;&lt;cell&gt;B. Special Code&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.2 Types&lt;/cell&gt;&lt;cell&gt;C. Test Scripts&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.3 Memory Management&lt;/cell&gt;&lt;cell&gt;D. Program Files&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.4 Global Variables&lt;/cell&gt;&lt;cell&gt;E. Foreign Conjunction&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;F. System Summary&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3. Verbs&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.1 Anatomy of a Verb&lt;/cell&gt;&lt;cell&gt;Bibliography&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.2 Rank&lt;/cell&gt;&lt;cell&gt;Glossary and Index&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.3 Atomic (Scalar) Verbs&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.4 Obverses, Identities, and Variants&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.5 Error Handling&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;4. Adverbs and Conjunctions&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5. Representation&lt;/cell&gt;&lt;cell&gt;5.1 Atomic Representation&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5.2 Boxed Representation&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5.3 Tree Representation&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;5.4 Linear Representation&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46259702</guid><pubDate>Sun, 14 Dec 2025 00:34:56 +0000</pubDate></item><item><title>Lean theorem prover mathlib</title><link>https://github.com/leanprover-community/mathlib4</link><description>&lt;doc fingerprint="adf4ba731ce775e1"&gt;
  &lt;main&gt;
    &lt;p&gt;Mathlib is a user maintained library for the Lean theorem prover. It contains both programming infrastructure and mathematics, as well as tactics that use the former and allow to develop the latter.&lt;/p&gt;
    &lt;p&gt;You can find detailed instructions to install Lean, mathlib, and supporting tools on our website. Alternatively, click on one of the buttons below to open a GitHub Codespace or a Gitpod workspace containing the project.&lt;/p&gt;
    &lt;p&gt;Please refer to https://github.com/leanprover-community/mathlib4/wiki/Using-mathlib4-as-a-dependency&lt;/p&gt;
    &lt;p&gt;Got everything installed? Why not start with the tutorial project?&lt;/p&gt;
    &lt;p&gt;For more pointers, see Learning Lean.&lt;/p&gt;
    &lt;p&gt;Besides the installation guides above and Lean's general documentation, the documentation of mathlib consists of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The mathlib4 docs: documentation generated automatically from the source &lt;code&gt;.lean&lt;/code&gt;files.&lt;/item&gt;
      &lt;item&gt;A description of currently covered theories, as well as an overview for mathematicians.&lt;/item&gt;
      &lt;item&gt;Some extra Lean documentation not specific to mathlib (see "Miscellaneous topics")&lt;/item&gt;
      &lt;item&gt;Documentation for people who would like to contribute to mathlib&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Much of the discussion surrounding mathlib occurs in a Zulip chat room, and you are welcome to join, or read along without signing up. Questions from users at all levels of expertise are welcome! We also provide an archive of the public discussions, which is useful for quick reference.&lt;/p&gt;
    &lt;p&gt;The complete documentation for contributing to &lt;code&gt;mathlib&lt;/code&gt; is located
on the community guide contribute to mathlib&lt;/p&gt;
    &lt;p&gt;You may want to subscribe to the &lt;code&gt;mathlib4&lt;/code&gt; channel on Zulip to introduce yourself and your plan to the community.
Often you can find community members willing to help you get started and advise you on the fit and
feasibility of your project.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;To obtain precompiled&lt;/p&gt;&lt;code&gt;olean&lt;/code&gt;files, run&lt;code&gt;lake exe cache get&lt;/code&gt;. (Skipping this step means the next step will be very slow.)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;To build&lt;/p&gt;&lt;code&gt;mathlib4&lt;/code&gt;run&lt;code&gt;lake build&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;To build and run all tests, run&lt;/p&gt;&lt;code&gt;lake test&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can use&lt;/p&gt;&lt;code&gt;lake build Mathlib.Import.Path&lt;/code&gt;to build a particular file, e.g.&lt;code&gt;lake build Mathlib.Algebra.Group.Defs&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you added a new file, run the following command to update&lt;/p&gt;
        &lt;code&gt;Mathlib.lean&lt;/code&gt;
        &lt;quote&gt;lake exe mk_all&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mathlib has the following guidelines and conventions that must be followed&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The style guide&lt;/item&gt;
      &lt;item&gt;A guide on the naming convention&lt;/item&gt;
      &lt;item&gt;The documentation style&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can run &lt;code&gt;lake exe cache get&lt;/code&gt; to download cached build files that are computed by &lt;code&gt;mathlib4&lt;/code&gt;'s automated workflow.&lt;/p&gt;
    &lt;p&gt;If something goes mysteriously wrong, you can try one of &lt;code&gt;lake clean&lt;/code&gt; or &lt;code&gt;rm -rf .lake&lt;/code&gt; before trying &lt;code&gt;lake exe cache get&lt;/code&gt; again.
In some circumstances you might try &lt;code&gt;lake exe cache get!&lt;/code&gt;
which re-downloads cached build files even if they are available locally.&lt;/p&gt;
    &lt;p&gt;Call &lt;code&gt;lake exe cache&lt;/code&gt; to see its help menu.&lt;/p&gt;
    &lt;p&gt;The mathlib4_docs repository is responsible for generating and publishing the mathlib4 docs.&lt;/p&gt;
    &lt;p&gt;That repo can be used to build the docs locally:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/leanprover-community/mathlib4_docs.git
cd mathlib4_docs
cp ../mathlib4/lean-toolchain .
lake exe cache get
lake build Mathlib:docs&lt;/code&gt;
    &lt;p&gt;The last step may take a while (&amp;gt;20 minutes). The HTML files can then be found in &lt;code&gt;.lake/build/doc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For users familiar with Lean 3 who want to get up to speed in Lean 4 and migrate their existing Lean 3 code we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A survival guide for Lean 3 users&lt;/item&gt;
      &lt;item&gt;Instructions to run &lt;code&gt;mathport&lt;/code&gt;on a project other than mathlib.&lt;code&gt;mathport&lt;/code&gt;is the tool the community used to port the entirety of&lt;code&gt;mathlib&lt;/code&gt;from Lean 3 to Lean 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are a mathlib contributor and want to update dependencies, use &lt;code&gt;lake update&lt;/code&gt;,
or &lt;code&gt;lake update batteries aesop&lt;/code&gt; (or similar) to update a subset of the dependencies.
This will update the &lt;code&gt;lake-manifest.json&lt;/code&gt; file correctly.
You will need to make a PR after committing the changes to this file.&lt;/p&gt;
    &lt;p&gt;Please do not run &lt;code&gt;lake update -Kdoc=on&lt;/code&gt; as previously advised, as the documentation related
dependencies should only be included when CI is building documentation.&lt;/p&gt;
    &lt;p&gt;For a list containing more detailed information, see https://leanprover-community.github.io/teams/maintainers.html&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Anne Baanen (@Vierkantor): algebra, number theory, tactics&lt;/item&gt;
      &lt;item&gt;Matthew Robert Ballard (@mattrobball): algebra, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Riccardo Brasca (@riccardobrasca): algebra, number theory, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Kevin Buzzard (@kbuzzard): algebra, number theory, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Mario Carneiro (@digama0): lean formalization, tactics, type theory, proof engineering&lt;/item&gt;
      &lt;item&gt;Bryan Gin-ge Chen (@bryangingechen): documentation, infrastructure&lt;/item&gt;
      &lt;item&gt;Johan Commelin (@jcommelin): algebra, number theory, category theory, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Anatole Dedecker (@ADedecker): topology, functional analysis, calculus&lt;/item&gt;
      &lt;item&gt;R√©my Degenne (@RemyDegenne): probability, measure theory, analysis&lt;/item&gt;
      &lt;item&gt;Floris van Doorn (@fpvandoorn): measure theory, model theory, tactics&lt;/item&gt;
      &lt;item&gt;Fr√©d√©ric Dupuis (@dupuisf): linear algebra, functional analysis&lt;/item&gt;
      &lt;item&gt;S√©bastien Gou√´zel (@sgouezel): topology, calculus, geometry, analysis, measure theory&lt;/item&gt;
      &lt;item&gt;Markus Himmel (@TwoFX): category theory&lt;/item&gt;
      &lt;item&gt;Yury G. Kudryashov (@urkud): analysis, topology, measure theory&lt;/item&gt;
      &lt;item&gt;Robert Y. Lewis (@robertylewis): tactics, documentation&lt;/item&gt;
      &lt;item&gt;Jireh Loreaux (@j-loreaux): analysis, topology, operator algebras&lt;/item&gt;
      &lt;item&gt;Heather Macbeth (@hrmacbeth): geometry, analysis&lt;/item&gt;
      &lt;item&gt;Patrick Massot (@patrickmassot): documentation, topology, geometry&lt;/item&gt;
      &lt;item&gt;Bhavik Mehta (@b-mehta): category theory, combinatorics&lt;/item&gt;
      &lt;item&gt;Kyle Miller (@kmill): combinatorics, tactics, metaprogramming&lt;/item&gt;
      &lt;item&gt;Kim Morrison (@kim-em): category theory, tactics&lt;/item&gt;
      &lt;item&gt;Oliver Nash (@ocfnash): algebra, geometry, topology&lt;/item&gt;
      &lt;item&gt;Jo√´l Riou (@joelriou): category theory, homology, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Michael Rothgang (@grunweg): differential geometry, analysis, topology, linters&lt;/item&gt;
      &lt;item&gt;Damiano Testa (@adomani): algebra, algebraic geometry, number theory, tactics, linter&lt;/item&gt;
      &lt;item&gt;Adam Topaz (@adamtopaz): algebra, category theory, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Eric Wieser (@eric-wieser): algebra, infrastructure&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jeremy Avigad (@avigad): analysis&lt;/item&gt;
      &lt;item&gt;Reid Barton (@rwbarton): category theory, topology&lt;/item&gt;
      &lt;item&gt;Gabriel Ebner (@gebner): tactics, infrastructure, core, formal languages&lt;/item&gt;
      &lt;item&gt;Johannes H√∂lzl (@johoelzl): measure theory, topology&lt;/item&gt;
      &lt;item&gt;Simon Hudon (@cipher1024): tactics&lt;/item&gt;
      &lt;item&gt;Chris Hughes (@ChrisHughes24): algebra&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46260128</guid><pubDate>Sun, 14 Dec 2025 01:49:11 +0000</pubDate></item><item><title>Compiler Engineering in Practice</title><link>https://chisophugis.github.io/2025/12/08/compiler-engineering-in-practice-part-1-what-is-a-compiler.html</link><description>&lt;doc fingerprint="4c3f098e613990c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compiler Engineering in Practice - Part 1: What is a Compiler?&lt;/head&gt;
    &lt;p&gt;‚ÄúCompiler Engineering in Practice‚Äù is a blog series intended to pass on wisdom that seemingly every seasoned compiler developer knows, but is not systematically written down in any textbook or online resource. Some (but not much) prior experience with compilers is needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a compiler?&lt;/head&gt;
    &lt;p&gt;The first and most important question is ‚Äúwhat is a compiler?‚Äù. In short, a compiler is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a translator that translates between two different languages, where those languages represent a description of a computation, and&lt;/item&gt;
      &lt;item&gt;the behavior of the computation in the output language must ‚Äúmatch‚Äù the behavior of the computation in the input language (more on this below).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, an input language can be C, and the output can be x86 assembly. By this definition, an assembler is also a compiler (albeit a simple one), in that it reads x86 textual assembly and outputs x86 binary machine code, which are two different languages. The &lt;code&gt;python&lt;/code&gt; program that executes Python code contains a compiler ‚Äì one that reads Python source code and outputs Python interpreter bytecode.&lt;/p&gt;
    &lt;p&gt;This brings me to my first important point about practical compiler engineering ‚Äì it‚Äôs not some mystical art. Compilers, operating systems, and databases are usually considered some kind of special corner of computer science / software engineering for being complex, and indeed, there are some corners of compilers that are a black art. But taking a step back, a compiler is simply a program that reads a file and writes a file. From a development perspective, it‚Äôs not that different from &lt;code&gt;cat&lt;/code&gt; or &lt;code&gt;grep&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Why does this matter? Because it means that compilers are easy to debug if you build them right. There are no time-dependent interrupts like an operating system, async external events like a web browser, or large enough scale that hardware has to be considered unreliable like a database. It‚Äôs just a command line program (or can be reduced to one if engineered right), such that nearly all bugs are reproducible and debuggable in isolation from the comfort of your workstation. No connecting to a flaky dev board, no extensive mocking of various interfaces.&lt;/p&gt;
    &lt;p&gt;You might say ‚Äì wait a minute ‚Äì if I‚Äôm running on my company‚Äôs AI hardware, I may need to connect to a dev board. Yes, but if you do things right, you will rarely need to do that when debugging the compiler proper. Which brings me to‚Ä¶&lt;/p&gt;
    &lt;head rend="h2"&gt;Reliability&lt;/head&gt;
    &lt;p&gt;Compilers are like operating systems and databases in that the bar for reliability is extremely high. One cannot build a practical compiler haphazardly. Why? Because of miscompiles.&lt;/p&gt;
    &lt;p&gt;Miscompiles are when the compiler produces an output file in the output language that does not ‚Äúmatch‚Äù the specification of its computation in the input language. To avoid a miscompile, the output program must behave identically to the input program, as far as can be observed by the outside world, such as network requests, values printed to the console, values written to files, etc.&lt;/p&gt;
    &lt;p&gt;For integer programs, bit-exact results are required, though there are some nuances regarding undefined behavior, as described in John Regehr‚Äôs ‚Äúlaws of physics of compilers‚Äù. For floating point programs, the expectation of bit-exact results is usually too strict. Transformations on large floating point computations (like AI programs) need some flexibility to produce slightly different outputs in order to allow efficient execution. There is no widely-agreed-upon formal definition of this, though there are reasonable ways to check for it in practice (‚Äúatol/rtol‚Äù go a long way).&lt;/p&gt;
    &lt;head rend="h3"&gt;How bad is a miscompile?&lt;/head&gt;
    &lt;p&gt;Miscompiles can have massive consequences for customers. A miscompile of a database can cause data loss. A miscompile of an operating system can cause a security vulnerability. A miscompile of an AI program can cause bad medical advice. The stakes are extremely high, and debugging a miscompile when it happens ‚Äúin the wild‚Äù can easily take 3+ months (and it can take months for a customer to even realize that their issue is caused by a miscompile).&lt;/p&gt;
    &lt;p&gt;If that weren‚Äôt enough, there‚Äôs a self-serving reason to avoid miscompiles ‚Äì if you have too many of them, your development velocity on your compiler will grind to a halt. Miscompiles can easily take 100x or 1000x of the time to debug vs a bug that makes itself known during the actual execution of the compiler (rather than the execution of the program that was output by the compiler). That‚Äôs why most aspects of practical compiler development revolve around ensuring that if something goes wrong, that it halts the compiler before a faulty output program is produced.&lt;/p&gt;
    &lt;p&gt;A miscompile is a fundamental failure of the compiler‚Äôs contract with its user. Every miscompile should be accompanied by a deep look in the mirror and self-reflection about what went wrong to allow it to sneak through, and what preventative measures can (and should immediately) be taken to ensure that this particular failure mode never happens again.&lt;/p&gt;
    &lt;p&gt;Especially in the AI space, there are lots of compilers that play fast and loose with this, and as a result get burned. The best compiler engineers tend to be highly pedantic and somewhat paranoid about what can go wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why compilers are hard ‚Äì the IR data structure&lt;/head&gt;
    &lt;p&gt;Compilers do have an essential complexity that makes them ‚Äúhard‚Äù, and this again comes from the whole business of making sure that the input program and the output of the compiler have the same behavior. To understand this, we have to discuss how a compiler represents the meaning of the input program and how it preserves that meaning when producing the output program. This notion of ‚Äúmeaning‚Äù is sometimes called the program semantics.&lt;/p&gt;
    &lt;p&gt;The primary data structure in a compiler is usually some form of graph data structure that represents the compiler‚Äôs understanding of ‚Äúwhat computation this program is supposed to do‚Äù. Hence, it represents the computation that the compiler needs to preserve all the way to the output program. This data structure is usually called an IR (intermediate representation). The primary way that compilers work is by taking an IR that represents the input program, and applying a series of small transformations all of which have been individually verified to not change the meaning of the program (i.e. not miscompile). In doing so, we decompose one large translation problem into many smaller ones, making it manageable.&lt;/p&gt;
    &lt;p&gt;I think it‚Äôs fair to say that compiler IR‚Äôs are the single most complex monolithic data structure in all of software engineering, in the sense that interpreting what can and cannot be validly done with the data structure is complex. To be clear, compiler IR‚Äôs are not usually very complex in the implementation sense like a ‚Äúlock-free list‚Äù that uses subtle atomic operations to present a simple insert/delete/etc. interface.&lt;/p&gt;
    &lt;p&gt;Unlike a lock-free list, compiler IR‚Äôs usually have a very complex interface, even if they have a very simple internal implementation. Even specifying declaratively or in natural language what are the allowed transformations on the data structure is usually extremely difficult (you‚Äôll see things like ‚Äúmemory models‚Äù or ‚Äúabstract machines‚Äù that people spend years or decades trying to define properly).&lt;/p&gt;
    &lt;head rend="h3"&gt;A very complex schema&lt;/head&gt;
    &lt;p&gt;Firstly, the nodes in the graph usually have a complex schema. For example, a simple ‚Äúinteger multiply operation‚Äù (a node in the graph) is only allowed to have certain integer types as operands (incoming edges). And there may easily be thousands of kinds of operations at varying abstraction levels in any practical compiler, each with their own unique requirements. For example, a simple C &lt;code&gt;*&lt;/code&gt; (multiplication) operator will go through the following evolution in Clang:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It first becomes Clang‚Äôs &lt;code&gt;BinaryOperator&lt;/code&gt;node, which takes two ‚Äúexpressions‚Äù as operands (which may be mutable uint32_t values, for example).&lt;/item&gt;
      &lt;item&gt;It will then be converted to an LLVM IR &lt;code&gt;mul&lt;/code&gt;operation, which takes as operands an&lt;code&gt;llvm::Value&lt;/code&gt;, which represents an immutable value of the&lt;code&gt;i32&lt;/code&gt;type, say.&lt;/item&gt;
      &lt;item&gt;It will then be converted to a GlobalISel &lt;code&gt;G_MUL&lt;/code&gt;operation, whose operands represent not only an 32-bit integer, but also begin to capture notions like which ‚Äúregister bank‚Äù the value should eventually live in.&lt;/item&gt;
      &lt;item&gt;It will then be turned into a target-specific MIR node like &lt;code&gt;IMUL32rri&lt;/code&gt;or&lt;code&gt;IMUL32rr&lt;/code&gt;selecting among a variety of physical x86 instructions which can implement a multiplication. At this level, operands may represent physical, mutable hardware registers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From a compiler developer‚Äôs perspective, all these ‚Äúmultiply operations‚Äù are deeply different from each other because of the different information captured at each abstraction level (again, compiler developers are usually very pedantic). Failing to adequately differentiate between abstraction levels is a common disease among poorly written compilers.&lt;/p&gt;
    &lt;p&gt;At every level, precise attention to detail is needed ‚Äì for example, if the multiplication is expected to overflow mod 2^32 in the source program, and we accidentally convert it to overflow mod 2^64 (such as by using a 64-bit register), then we have introduced a miscompile. Each operation has its own unique set of constraints and properties like these which apply when transforming the program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complex interactions between operations&lt;/head&gt;
    &lt;p&gt;Additionally, how these operations in the IR graph relate to each other can be very complex, especially when mutable variables and control flow are involved. For example, you may realize that an operation always executes, but we may be able to move it around to hide it under an &lt;code&gt;if&lt;/code&gt; condition to optimize the program. Consider the program:&lt;/p&gt;
    &lt;code&gt;x = y + z;
...
if (condition) {
    print(x); // The only time that `x` is referenced.
}
&lt;/code&gt;
    &lt;p&gt;Is it safe to convert this to&lt;/p&gt;
    &lt;code&gt;...
if (condition) {
    print(y + z);
}
&lt;/code&gt;
    &lt;p&gt;? Well, it depends on what‚Äôs hidden in that &lt;code&gt;...&lt;/code&gt;. For example, if the program is:&lt;/p&gt;
    &lt;code&gt;x = y + z;
...
y += 5;
...
if (condition) {
    print(x);
}
&lt;/code&gt;
    &lt;p&gt;Then it‚Äôs not legal, since by the time we get to the &lt;code&gt;if&lt;/code&gt;, the value of &lt;code&gt;y&lt;/code&gt; will have changed and we‚Äôll print the wrong value. One of the primary considerations when designing compiler IR‚Äôs is how to make the transformations as simple and obviously correct as possible (more on that in another blog post).&lt;/p&gt;
    &lt;p&gt;Usually production compilers will deal with IR graphs from thousands to millions of nodes. Understandably then, the compounding effect of the IR complexity is front and center in all compiler design discussions. A single invalid transformation can result in a miscompile.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compilers are just software&lt;/head&gt;
    &lt;p&gt;Practical compilers are often live for years or decades and span millions of lines of code, so the entire suite of software engineering wisdom applies to them ‚Äì good API design, testing, reusability, etc. though usually with additional compiler-specific twists.&lt;/p&gt;
    &lt;p&gt;For example, while API design is very important for most programs‚Äô code (as it is for compilers‚Äô), compilers also have an additional dimension of ‚ÄúIR design‚Äù. As described above, the IR can be very complex to understand and transform, and designing it right can greatly mitigate this. (more on this in a future blog post)&lt;/p&gt;
    &lt;p&gt;Similarly, since compilers are usually decomposed into the successive application of multiple ‚Äúpasses‚Äù (self-contained IR transformations), there are a variety of testing and debugging strategies specific to compilers. (more on this in a future blog post).&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion and acknowledgements&lt;/head&gt;
    &lt;p&gt;I hope you have found this post helpful. I have a few more sketched out that should be coming soon. Please let me know on my LinkedIn if you have any feedback or topics you‚Äôd like to suggest. Big thanks to Bjarke Roune for his recent blog post that inspired me to finally get this series off the ground. Also to Dan Gohman for his blog post on canonicalization from years back. There‚Äôs too few such blog posts giving the big picture of practical compiler development. Please send me any other ones you know about on LinkedIn.&lt;/p&gt;
    &lt;p&gt;Stay tuned for future parts of this series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modern Compilers in the Age of AI&lt;/item&gt;
      &lt;item&gt;Organizing a Compiler&lt;/item&gt;
      &lt;item&gt;Testing, Code Review, and Robustness&lt;/item&gt;
      &lt;item&gt;The Compiler Lifecycle&lt;/item&gt;
      &lt;item&gt;‚Ä¶&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46261452</guid><pubDate>Sun, 14 Dec 2025 07:45:15 +0000</pubDate></item><item><title>Shai-Hulud compromised a dev machine and raided GitHub org access: a post-mortem</title><link>https://trigger.dev/blog/shai-hulud-postmortem</link><description>&lt;doc fingerprint="c417348bcef3474a"&gt;
  &lt;main&gt;
    &lt;p&gt;On November 25th, 2025, we were on a routine Slack huddle debugging a production issue when we noticed something strange: a PR in one of our internal repos was suddenly closed, showed zero changes, and had a single commit from... Linus Torvalds?&lt;/p&gt;
    &lt;p&gt;The commit message was just "init."&lt;/p&gt;
    &lt;p&gt;Within seconds, our #git Slack channel exploded with notifications. Dozens of force-pushes. PRs closing across multiple repositories. All attributed to one of our engineers.&lt;/p&gt;
    &lt;p&gt;We had been compromised by Shai-Hulud 2.0, a sophisticated npm supply chain worm that compromised over 500 packages, affected 25,000+ repositories, and spread across the JavaScript ecosystem. We weren't alone: PostHog, Zapier, AsyncAPI, Postman, and ENS were among those hit.&lt;/p&gt;
    &lt;p&gt;This is the complete story of what happened, how we responded, and what we've changed to prevent this from happening again.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;No Trigger.dev packages were ever compromised. The&lt;/p&gt;&lt;code&gt;@trigger.dev/*&lt;/code&gt;packages and&lt;code&gt;trigger.dev&lt;/code&gt;CLI were never infected with Shai-Hulud malware. This incident involved one of our engineers installing a compromised package on their development machine, which led to credential theft and unauthorized access to our GitHub organization. Our published packages remained safe throughout.&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Attack Timeline&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, 04:11&lt;/cell&gt;
        &lt;cell&gt;Malicious packages go live&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, ~20:27&lt;/cell&gt;
        &lt;cell&gt;Engineer compromised&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, 22:36&lt;/cell&gt;
        &lt;cell&gt;First attacker activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 02:56-05:32&lt;/cell&gt;
        &lt;cell&gt;Overnight reconnaissance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 09:08-15:08&lt;/cell&gt;
        &lt;cell&gt;Legitimate engineer work (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 09:10-09:17&lt;/cell&gt;
        &lt;cell&gt;Attacker monitors engineer activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 15:17-15:27&lt;/cell&gt;
        &lt;cell&gt;Final recon&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 15:27-15:37&lt;/cell&gt;
        &lt;cell&gt;Destructive attack&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, ~15:32&lt;/cell&gt;
        &lt;cell&gt;Detection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, ~15:36&lt;/cell&gt;
        &lt;cell&gt;Access revoked&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 16:35&lt;/cell&gt;
        &lt;cell&gt;AWS session blocked&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 22:35&lt;/cell&gt;
        &lt;cell&gt;All branches restored&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nov 26, 20:16&lt;/cell&gt;
        &lt;cell&gt;GitHub App key rotated&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;The compromise&lt;/head&gt;
    &lt;p&gt;On the evening of November 24th, around 20:27 UTC (9:27 PM local time in Germany), one of our engineers was experimenting with a new project. They ran a command that triggered &lt;code&gt;pnpm install&lt;/code&gt;. At that moment, somewhere in the dependency tree, a malicious package executed.&lt;/p&gt;
    &lt;p&gt;We don't know exactly which package delivered the payload. The engineer was experimenting at the time and may have deleted the project directory as part of cleanup. By the time we investigated, we couldn't trace back to the specific package. The engineer checked their shell history and they'd only run install commands in our main trigger repo, cloud repo, and one experimental project.&lt;/p&gt;
    &lt;p&gt;This is one of the frustrating realities of these attacks: once the malware runs, identifying the source becomes extremely difficult. The package doesn't announce itself. The &lt;code&gt;pnpm install&lt;/code&gt; completes successfully. Everything looks normal.&lt;/p&gt;
    &lt;p&gt;What we do know is that the Shai-Hulud malware ran a &lt;code&gt;preinstall&lt;/code&gt; script that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Downloaded and executed TruffleHog, a legitimate security tool repurposed for credential theft&lt;/item&gt;
      &lt;item&gt;Scanned the engineer's machine for secrets: GitHub tokens, AWS credentials, npm tokens, environment variables&lt;/item&gt;
      &lt;item&gt;Exfiltrated everything it found&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the engineer later recovered files from their compromised laptop (booted in recovery mode), they found the telltale signs:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;.trufflehog-cache&lt;/code&gt; directory and &lt;code&gt;trufflehog_3.91.1_darwin_amd64.tar.gz&lt;/code&gt; file found on the compromised machine. The &lt;code&gt;extract&lt;/code&gt; directory was empty, likely cleaned up by the malware to cover its tracks.&lt;/p&gt;
    &lt;head rend="h2"&gt;17 hours of reconnaissance&lt;/head&gt;
    &lt;p&gt;The attacker had access to our engineer's GitHub account for 17 hours before doing anything visible. According to our GitHub audit logs, they operated methodically.&lt;/p&gt;
    &lt;p&gt;Just over two hours after the initial compromise, the attacker validated their stolen credentials and began mass cloning:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
        &lt;cell role="head"&gt;Activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:36:50&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;First attacker access, mass cloning begins&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:36-22:39&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;73 repositories cloned&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:48-22:50&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~70 more repositories cloned (second wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:55-22:56&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~90 repositories cloned (third wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:59-23:04&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~70 repositories cloned (fourth wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;23:32:59&lt;/cell&gt;
        &lt;cell&gt;India&lt;/cell&gt;
        &lt;cell&gt;Attacker switches to India-based infrastructure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;23:32-23:37&lt;/cell&gt;
        &lt;cell&gt;India&lt;/cell&gt;
        &lt;cell&gt;73 repositories cloned&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;23:34-23:35&lt;/cell&gt;
        &lt;cell&gt;US + India&lt;/cell&gt;
        &lt;cell&gt;Simultaneous cloning from both locations&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The simultaneous activity from US and India confirmed we were dealing with a single attacker using multiple VPNs or servers, not separate actors.&lt;/p&gt;
    &lt;p&gt;While our engineer slept in Germany, the attacker continued their reconnaissance. More cloning at 02:56-02:59 UTC (middle of the night in Germany), sporadic activity until 05:32 UTC. Total repos cloned: 669 (527 from US infrastructure, 142 from India).&lt;/p&gt;
    &lt;p&gt;Here's where it gets unsettling. Our engineer woke up and started their normal workday:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Actor&lt;/cell&gt;
        &lt;cell role="head"&gt;Activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;09:08:27&lt;/cell&gt;
        &lt;cell&gt;Engineer&lt;/cell&gt;
        &lt;cell&gt;Triggers workflow on cloud repo (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;09:10-09:17&lt;/cell&gt;
        &lt;cell&gt;Attacker&lt;/cell&gt;
        &lt;cell&gt;Git fetches from US, watching the engineer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;09:08-15:08&lt;/cell&gt;
        &lt;cell&gt;Engineer&lt;/cell&gt;
        &lt;cell&gt;Normal PR reviews, CI workflows (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The attacker was monitoring our engineer's activity while they worked, unaware they were compromised.&lt;/p&gt;
    &lt;p&gt;During this period, the attacker created repositories with random string names to store stolen credentials, a known Shai-Hulud pattern:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/xfjqb74uysxcni5ztn&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/ls4uzkvwnt0qckjq27&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/uxa7vo9og0rzts362c&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They also created three repos marked with "Sha1-Hulud: The Second Coming" as a calling card. These repositories were empty by the time we examined them, but based on the documented Shai-Hulud behavior, they likely contained triple base64-encoded credentials.&lt;/p&gt;
    &lt;head rend="h2"&gt;10 minutes of destruction&lt;/head&gt;
    &lt;p&gt;At 15:27 UTC on November 25th, the attacker switched from reconnaissance to destruction.&lt;/p&gt;
    &lt;p&gt;The attack began on our &lt;code&gt;cloud&lt;/code&gt; repo from India-based infrastructure:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Repo&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:35&lt;/cell&gt;
        &lt;cell&gt;First force-push&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;Attack begins&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:37&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;PR #300 closed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:44&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:27:50&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev&lt;/cell&gt;
        &lt;cell&gt;PR #2707 closed&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The attack continued on our main repository:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:28:13&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2706 (release PR)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:30:51&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2451&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:31:10&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2382&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:31:16&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push to trigger.dev&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:31:31&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2482&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;At 15:32:43-46 UTC, 12 PRs on jsonhero-web were closed in 3 seconds. Clearly automated. PRs #47, #169, #176, #181, #189, #190, #194, #197, #204, #206, #208 all closed within a 3-second window.&lt;/p&gt;
    &lt;p&gt;Our critical infrastructure repository was targeted next:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:41&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/infra PR #233&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:45&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push (India)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:48&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/infra PR #309&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:35:49&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push (India)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The final PR was closed on json-infer-types at 15:37:13 UTC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Detection and response&lt;/head&gt;
    &lt;p&gt;We got a lucky break. One of our team members was monitoring Slack when the flood of notifications started:&lt;/p&gt;
    &lt;p&gt;Our #git Slack channel during the attack. A wall of force-pushes, all with commit message "init."&lt;/p&gt;
    &lt;p&gt;Every malicious commit was authored as:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;Author: Linus Torvalds &amp;lt;[email¬†protected]&amp;gt;Message: init&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;An attacked branch: a single "init" commit attributed to Linus Torvalds, thousands of commits behind main.&lt;/p&gt;
    &lt;p&gt;We haven't found reports of other Shai-Hulud victims seeing this same "Linus Torvalds" vandalism pattern. The worm's documented behavior focuses on credential exfiltration and npm package propagation, not repository destruction. This destructive phase may have been unique to our attacker, or perhaps a manual follow-up action after the automated worm had done its credential harvesting.&lt;/p&gt;
    &lt;p&gt;Within 4 minutes of detection we identified the compromised account, removed them from the GitHub organization, and the attack stopped immediately.&lt;/p&gt;
    &lt;p&gt;Our internal Slack during those first minutes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Urmmm guys? what's going on?"&lt;/p&gt;
      &lt;p&gt;"add me to the call @here"&lt;/p&gt;
      &lt;p&gt;"Nick could you double check Infisical for any machine identities"&lt;/p&gt;
      &lt;p&gt;"can someone also check whether there are any reports of compromised packages in our CLI deps?"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Within the hour:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:36&lt;/cell&gt;
        &lt;cell&gt;Removed from GitHub organization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:40&lt;/cell&gt;
        &lt;cell&gt;Removed from Infisical (secrets manager)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:45&lt;/cell&gt;
        &lt;cell&gt;Removed from AWS IAM Identity Center&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~16:00&lt;/cell&gt;
        &lt;cell&gt;Removed from Vercel and Cloudflare&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16:35&lt;/cell&gt;
        &lt;cell&gt;AWS SSO sessions blocked via deny policy (sessions can't be revoked)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;16:45&lt;/cell&gt;
        &lt;cell&gt;IAM user console login deleted&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;The damage&lt;/head&gt;
    &lt;p&gt;Repository clone actions: 669 (public and private), including infrastructure code, internal documentation, and engineering plans.&lt;/p&gt;
    &lt;p&gt;Branches force-pushed: 199 across 16 repositories&lt;/p&gt;
    &lt;p&gt;Pull requests closed: 42&lt;/p&gt;
    &lt;p&gt;Protected branch rejections: 4. Some of our repositories have main branch protection enabled, but we had not enabled it for all repositories at the time of the incident.&lt;/p&gt;
    &lt;p&gt;npm packages were not compromised. This is the difference between "our repos got vandalized" and "our packages got compromised."&lt;/p&gt;
    &lt;p&gt;Our engineer didn't have an npm publishing token on their machine, and even if they did we had already required 2FA for publishing to npm. Without that, Shai-Hulud would have published malicious versions of &lt;code&gt;@trigger.dev/sdk&lt;/code&gt;, &lt;code&gt;@trigger.dev/core&lt;/code&gt;, and others, potentially affecting thousands of downstream users.&lt;/p&gt;
    &lt;p&gt;Production databases or any AWS resources were not accessed. Our AWS CloudTrail audit showed only read operations from the compromised account:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Event Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Count&lt;/cell&gt;
        &lt;cell role="head"&gt;Service&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ListManagedNotificationEvents&lt;/cell&gt;
        &lt;cell&gt;~40&lt;/cell&gt;
        &lt;cell&gt;notifications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DescribeClusters&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;ECS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DescribeTasks&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;ECS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DescribeMetricFilters&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;CloudWatch&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These were confirmed to be legitimate operations by our engineer.&lt;/p&gt;
    &lt;p&gt;One nice surprise: AWS actually sent us a proactive alert about Shai-Hulud. They detected the malware's characteristic behavior (ListSecrets, GetSecretValue, BatchGetSecretValue API calls) on an old test account that hadn't been used in months, so we just deleted it. But kudos to AWS for the proactive detection and notification.&lt;/p&gt;
    &lt;head rend="h2"&gt;The recovery&lt;/head&gt;
    &lt;p&gt;GitHub doesn't have server-side reflog. When someone force-pushes, that history is gone from GitHub's servers.&lt;/p&gt;
    &lt;p&gt;But we found ways to recover.&lt;/p&gt;
    &lt;p&gt;Push events are retained for 90 days via the GitHub Events API. We wrote a script that fetched pre-attack commit SHAs:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;# Find pre-attack commit SHA from eventsgh api repos/$REPO/events --paginate | \  jq -r '.[] | select(.type=="PushEvent") |  select(.payload.ref=="refs/heads/'$BRANCH'") |  .payload.before' | head -1&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Public repository forks still contained original commits. We used these to verify and restore branches.&lt;/p&gt;
    &lt;p&gt;Developers who hadn't run &lt;code&gt;git fetch --prune&lt;/code&gt; (all of us?) still had old SHAs in their local reflog.&lt;/p&gt;
    &lt;p&gt;Within 7 hours, all 199 branches were restored.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub app private key exposure&lt;/head&gt;
    &lt;p&gt;During the investigation, our engineer was going through files recovered from the compromised laptop and discovered something concerning: the private key for our GitHub App was in the trash folder.&lt;/p&gt;
    &lt;p&gt;When you create a private key in the GitHub App settings, GitHub automatically downloads it. The engineer had created a key at some point, and while the active file had been deleted, it was still in the trash, potentially accessible to TruffleHog.&lt;/p&gt;
    &lt;p&gt;Our GitHub App has the following permissions on customer repositories:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Permission&lt;/cell&gt;
        &lt;cell role="head"&gt;Access Level&lt;/cell&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;contents&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could read/write repository contents&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;pull_requests&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could read/create/modify PRs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;deployments&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could create/trigger deployments&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;checks&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could create/modify check runs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;commit_statuses&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could mark commits as passing/failing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;metadata&lt;/cell&gt;
        &lt;cell&gt;read&lt;/cell&gt;
        &lt;cell&gt;Could read repository metadata&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To generate valid access tokens, an attacker would need both the private key (potentially compromised) and the installation ID for a specific customer (stored in our database which was not compromised, not on the compromised machine).&lt;/p&gt;
    &lt;p&gt;We immediately rotated the key:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 26, 18:51&lt;/cell&gt;
        &lt;cell&gt;Private key discovered in trash folder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 26, 19:54&lt;/cell&gt;
        &lt;cell&gt;New key deployed to test environment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nov 26, 20:16&lt;/cell&gt;
        &lt;cell&gt;New key deployed to production&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We found no evidence of unauthorized access to any customer repositories. The attacker would have needed installation IDs from our database to generate tokens, and our database was not compromised as previously mentioned.&lt;/p&gt;
    &lt;p&gt;However, we cannot completely rule out the possibility. An attacker with the private key could theoretically have called the GitHub API to enumerate all installations. We've contacted GitHub Support to request additional access logs. We've also analyzed the webhook payloads to our GitHub app, looking for suspicious push or PR activity from connected installations &amp;amp; repositories. We haven't found any evidence of unauthorized activity in these webhook payloads.&lt;/p&gt;
    &lt;p&gt;We've sent out an email to potentially effected customers to notify them of the incident with detailed instructions on how to check if they were affected. Please check your email for more details if you've used our GitHub app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical deep-dive: how Shai-Hulud works&lt;/head&gt;
    &lt;p&gt;For those interested in the technical details, here's what we learned about the malware from Socket's analysis and our own investigation.&lt;/p&gt;
    &lt;p&gt;When npm runs the &lt;code&gt;preinstall&lt;/code&gt; script, it executes &lt;code&gt;setup_bun.js&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detects OS/architecture&lt;/item&gt;
      &lt;item&gt;Downloads or locates the Bun runtime&lt;/item&gt;
      &lt;item&gt;Caches Bun in &lt;code&gt;~/.cache&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Spawns a detached Bun process running &lt;code&gt;bun_environment.js&lt;/code&gt;with output suppressed&lt;/item&gt;
      &lt;item&gt;Returns immediately so &lt;code&gt;npm install&lt;/code&gt;completes successfully with no warnings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The malware runs in the background while you think everything is fine.&lt;/p&gt;
    &lt;p&gt;The payload uses TruffleHog to scan &lt;code&gt;$HOME&lt;/code&gt; for GitHub tokens (from env vars, gh CLI config, git credential helpers), AWS/GCP/Azure credentials, npm tokens from &lt;code&gt;.npmrc&lt;/code&gt;, environment variables containing anything that looks like a secret, and GitHub Actions secrets (if running in CI).&lt;/p&gt;
    &lt;p&gt;Stolen credentials are uploaded to a newly-created GitHub repo with a random name. The data is triple base64-encoded to evade GitHub's secret scanning.&lt;/p&gt;
    &lt;p&gt;Files created:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;contents.json&lt;/code&gt;(system info and GitHub credentials)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;environment.json&lt;/code&gt;(all environment variables)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cloud.json&lt;/code&gt;(cloud provider credentials)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;truffleSecrets.json&lt;/code&gt;(filesystem secrets from TruffleHog)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;actionsSecrets.json&lt;/code&gt;(GitHub Actions secrets if any)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If an npm publishing token is found, the malware validates the token against the npm registry, fetches packages maintained by that account, downloads each package, patches it with the malware, bumps the version, and re-publishes, infecting more packages.&lt;/p&gt;
    &lt;p&gt;This is how the worm spread through the npm ecosystem, starting from PostHog's compromised CI on November 24th at 4:11 AM UTC. Our engineer was infected roughly 16 hours after the malicious packages went live.&lt;/p&gt;
    &lt;p&gt;If no credentials are found to exfiltrate or propagate, the malware attempts to delete the victim's entire home directory. Scorched earth.&lt;/p&gt;
    &lt;p&gt;File artifacts to look for: &lt;code&gt;setup_bun.js&lt;/code&gt;, &lt;code&gt;bun_environment.js&lt;/code&gt;, &lt;code&gt;cloud.json&lt;/code&gt;, &lt;code&gt;contents.json&lt;/code&gt;, &lt;code&gt;environment.json&lt;/code&gt;, &lt;code&gt;truffleSecrets.json&lt;/code&gt;, &lt;code&gt;actionsSecrets.json&lt;/code&gt;, &lt;code&gt;.trufflehog-cache/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;Malware file hashes (SHA1):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;:&lt;code&gt;d60ec97eea19fffb4809bc35b91033b52490ca11&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;:&lt;code&gt;3d7570d14d34b0ba137d502f042b27b0f37a59fa&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setup_bun.js&lt;/code&gt;:&lt;code&gt;d1829b4708126dcc7bea7437c04d1f10eacd4a16&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We've published a detection script that checks for Shai-Hulud indicators.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we've changed&lt;/head&gt;
    &lt;p&gt;We disabled npm scripts globally:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;npm config set ignore-scripts true --location=global&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;This prevents &lt;code&gt;preinstall&lt;/code&gt;, &lt;code&gt;postinstall&lt;/code&gt;, and other lifecycle scripts from running. It's aggressive and some packages will break, but it's the only reliable protection against this class of attack.&lt;/p&gt;
    &lt;p&gt;We upgraded to pnpm 10. This was significant effort (had to migrate through pnpm 9 first), but pnpm 10 brings critical security improvements. Scripts are ignored by default. You can explicitly whitelist packages that need to run scripts via &lt;code&gt;pnpm.onlyBuiltDependencies&lt;/code&gt;. And the &lt;code&gt;minimumReleaseAge&lt;/code&gt; setting prevents installing packages published recently.&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;# pnpm-workspace.yamlminimumReleaseAge: 4320 # 3 days in minutespreferOffline: true&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;To whitelist packages that legitimately need build scripts:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm approve-builds&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;This prompts you to select which packages to allow (like &lt;code&gt;esbuild&lt;/code&gt;, &lt;code&gt;prisma&lt;/code&gt;, &lt;code&gt;sharp&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;For your global pnpm config:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm config set minimumReleaseAge 4320pnpm config set --json minimumReleaseAgeExclude '["@trigger.dev/*", "trigger.dev"]'&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;We switched npm publishing to OIDC. No more long-lived npm tokens anywhere. Publishing now uses npm's trusted publishers with GitHub Actions OIDC. Even if an attacker compromises a developer machine, they can't publish packages because there are no credentials to steal. Publishing only happens through CI with short-lived, scoped tokens.&lt;/p&gt;
    &lt;p&gt;We enabled branch protection on all repositories. Not just critical repos or just OSS repos. Every repository with meaningful code now has branch protection enabled.&lt;/p&gt;
    &lt;p&gt;We've adopted Granted for AWS SSO. Granted encrypts SSO session tokens on the client side, unlike the AWS CLI which stores them in plaintext.&lt;/p&gt;
    &lt;p&gt;Based on PostHog's analysis of how they were initially compromised (via &lt;code&gt;pull_request_target&lt;/code&gt;), we've reviewed our GitHub Actions workflows. We now require approval for external contributor workflow runs on all our repositories (previous policy was only for public repositories).&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons for other teams&lt;/head&gt;
    &lt;p&gt;The ability for packages to run arbitrary code during installation is the attack surface. Until npm fundamentally changes, add this to your &lt;code&gt;~/.npmrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;ignore-scripts=true&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Yes, some things will break. Whitelist them explicitly. The inconvenience is worth it.&lt;/p&gt;
    &lt;p&gt;pnpm 10 ignores scripts by default and lets you set a minimum age for packages:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm config set minimumReleaseAge 4320  # 3 days&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Newly published packages can't be installed for 3 days, giving time for malicious packages to be detected.&lt;/p&gt;
    &lt;p&gt;Branch protection takes 30 seconds to enable. It prevents attackers from pushing to a main branch, potentially executing malicious GitHub action workflows.&lt;/p&gt;
    &lt;p&gt;Long-lived npm tokens on developer machines are a liability. Use trusted publishers with OIDC instead.&lt;/p&gt;
    &lt;p&gt;If you don't need a credential on your local machine, don't have it there. Publishing should happen through CI only.&lt;/p&gt;
    &lt;p&gt;Our #git Slack channel is noisy. That noise saved us.&lt;/p&gt;
    &lt;head rend="h2"&gt;A note on the human side&lt;/head&gt;
    &lt;p&gt;One of the hardest parts of this incident was that it happened to a person.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Sorry for all the trouble guys, terrible experience"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Our compromised engineer felt terrible, even though they did absolutely nothing wrong. It could have happened to any team member.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;npm install&lt;/code&gt; is not negligence. Installing dependencies is not a security failure. The security failure is in an ecosystem that allows packages to run arbitrary code silently.&lt;/p&gt;
    &lt;p&gt;They also discovered that the attacker had made their GitHub account star hundreds of random repositories during the compromise. Someone even emailed us: "hey you starred my repo but I think it was because you were hacked, maybe remove the star?"&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from compromise to first attacker activity&lt;/cell&gt;
        &lt;cell&gt;~2 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time attacker had access before destructive action&lt;/cell&gt;
        &lt;cell&gt;~17 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Duration of destructive attack&lt;/cell&gt;
        &lt;cell&gt;~10 minutes (15:27-15:37 UTC)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from first malicious push to detection&lt;/cell&gt;
        &lt;cell&gt;~5 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from detection to access revocation&lt;/cell&gt;
        &lt;cell&gt;~4 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time to full branch recovery&lt;/cell&gt;
        &lt;cell&gt;~7 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Repository clone actions by attacker&lt;/cell&gt;
        &lt;cell&gt;669&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Repositories force-pushed&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Branches affected&lt;/cell&gt;
        &lt;cell&gt;199&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pull requests closed&lt;/cell&gt;
        &lt;cell&gt;42&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Protected branch rejections&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Resources&lt;/head&gt;
    &lt;p&gt;About the Attack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Socket.dev: Shai-Hulud Strikes Again V2 - Technical deep-dive into the malware&lt;/item&gt;
      &lt;item&gt;PostHog Post-Mortem - Another company's experience with Shai-Hulud&lt;/item&gt;
      &lt;item&gt;Wiz Blog: Shai-Hulud 2.0 Supply Chain Attack&lt;/item&gt;
      &lt;item&gt;The Hacker News Coverage&lt;/item&gt;
      &lt;item&gt;Endor Labs Analysis&lt;/item&gt;
      &lt;item&gt;HelixGuard Advisory (referenced in AWS alert)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mitigation Resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;npm Trusted Publishers - OIDC-based publishing&lt;/item&gt;
      &lt;item&gt;pnpm onlyBuiltDependencies - Whitelist packages allowed to run scripts&lt;/item&gt;
      &lt;item&gt;pnpm minimumReleaseAge - Delay installation of new packages&lt;/item&gt;
      &lt;item&gt;Granted - AWS SSO credential management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have questions about this incident? Reach out on Twitter/X or Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262021</guid><pubDate>Sun, 14 Dec 2025 10:07:00 +0000</pubDate></item><item><title>Efficient Basic Coding for the ZX Spectrum</title><link>https://blog.jafma.net/2020/02/24/efficient-basic-coding-for-the-zx-spectrum/</link><description>&lt;doc fingerprint="bb19c69cc4bc9fbe"&gt;
  &lt;main&gt;
    &lt;p&gt;[Click here to read this in English ]&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;√âste es el primero de una serie de art√≠culos que explican los fundamentos de la (in)eficiencia de los programas en BASIC puro para el ZX Spectrum:&lt;/p&gt;
      &lt;p&gt;I. Sobre los n√∫meros de l√≠nea&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;El int√©rprete de lenguaje Sinclair BASIC incluido en la ROM del ZX Spectrum es, en muchos aspectos, una maravilla del software, concretamente de la programaci√≥n en ensamblador, y dar√≠a para hablar durante mucho tiempo. En esta serie queremos destacar los puntos m√°s importantes a tener en cuenta para que los programas escritos en ese lenguaje sean lo m√°s eficientes posibles, en primer lugar en tiempo de ejecuci√≥n, pero tambi√©n en espacio ocupado en memoria.&lt;/p&gt;
    &lt;p&gt;En esta primera entrega de la serie trataremos de las l√≠neas de dichos programas; m√°s all√° de la necesidad de numerarlas, algo que no se hace desde hace d√©cadas en ning√∫n lenguaje de programaci√≥n, est√° el propio hecho de la eficiencia del int√©rprete a la hora de manejarlas.&lt;/p&gt;
    &lt;p&gt;Antes de meternos en el meollo, conviene resumir los l√≠mites que existen en esta m√°quina relativos a las l√≠neas de programa:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Las l√≠neas de programa, una vez √©ste queda almacenado en la memoria listo para su ejecuci√≥n, ocupan 2 bytes (por cierto, almacenados en formato big-endian, el √∫nico caso de este formato en el ZX). Esto podr√≠a llevar a pensar que tenemos disponibles desde la l√≠nea 0 a la 65535 (el m√°ximo n√∫mero que puede almacenarse en 2 bytes), pero no es exactamente as√≠. A la hora de editar manualmente un programa s√≥lo se nos permite numerar las l√≠neas desde 1 a 9999. Si el programa es manipulado fuera del editor (se puede hacer con &lt;code&gt;POKE&lt;/code&gt;), es posible tener la l√≠nea 0, y √©sta aparecer al listarlo, pero no ser√° editable. De la misma manera (manipulando el programa con&lt;code&gt;POKE&lt;/code&gt;) se pueden numerar l√≠neas por encima de la 9999; sin embargo, esto causar√° problemas en ejecuci√≥n: muchas sentencias del lenguaje que admiten un n√∫mero de l√≠nea como par√°metro, como&lt;code&gt;GO TO&lt;/code&gt;o&lt;code&gt;RESTORE&lt;/code&gt;, dan error si la l√≠nea es mayor de 32767; la pila de llamadas dejar√° de funcionar correctamente si se hace un&lt;code&gt;GO SUB&lt;/code&gt;a una l√≠nea mayor de 15871 (3DFF en hexadecimal); el int√©rprete reserva el n√∫mero de l√≠nea 65534 para indicar que est√° ejecutando c√≥digo escrito en el buffer de edici√≥n (y no en el listado del programa); por √∫ltimo, listar programas por pantalla tampoco funciona bien con l√≠neas mayores de 9999, y en cuanto las editemos manualmente volver√°n a quedar con s√≥lo 4 d√≠gitos decimales.&lt;/item&gt;
      &lt;item&gt;La longitud en bytes de cada l√≠nea de programa se almacena justo despu√©s del n√∫mero de l√≠nea, ocupando 2 bytes (esta vez en little-endian). Esta longitud no incluye ni el n√∫mero de l√≠nea ni la longitud en s√≠ misma. Por tanto, podr√≠amos esperar poder tener l√≠neas de un m√°ximo de 65535 bytes en su contenido principal (menos 1, porque siempre tiene que haber un 0x0D al final para indicar el fin de l√≠nea); asimismo, las l√≠neas m√°s cortas ocupar√°n en memoria 2+2+1+1 = 6 bytes: ser√≠an aqu√©llas que contienen una sola sentencia que no tiene par√°metros, p.ej., &lt;code&gt;10 CLEAR&lt;/code&gt;. Una rutina muy importante en la ROM del Spectrum, la encargada de buscar la siguiente l√≠nea o la siguiente variable salt√°ndose la actual (llamada&lt;code&gt;NEXT-ONE&lt;/code&gt;y situada en la direcci√≥n 0x19B8) funciona perfectamente con rangos de tama√±o de l√≠nea entre 0 y 65535, pero en ejecuci√≥n el int√©rprete dejar√° de interpretar una l√≠nea en cuanto se encuentre un 0x0D al comienzo de una sentencia (si la l√≠nea es m√°s larga, por ejemplo porque se haya extendido mediante manipulaciones externas, ignorar√° el resto, por lo que puede ser usado ese espacio para almacenar datos dentro del programa). M√°s importante a√∫n: dar√° error al tratar de ejecutar m√°s de 127 sentencias en una misma l√≠nea, es decir, una l√≠nea en ejecuci√≥n s√≥lo puede tener, en la pr√°ctica, desde 1 hasta 127 sentencias.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Una vez resumidos los datos b√°sicos sobre las l√≠neas y los n√∫meros de l√≠nea, nos centraremos en una caracter√≠stica muy concreta del int√©rprete de BASIC que resulta fundamental para conseguir incrementar su eficiencia en la ejecuci√≥n de programas:&lt;/p&gt;
    &lt;p&gt;El int√©rprete no usa una tabla indexada de l√≠neas de programa&lt;/p&gt;
    &lt;p&gt;Los programas BASIC del ZX se pre-procesan nada m√°s teclearlos (tras teclear l√≠neas completas en el caso del ZX Spectrum +2 y superiores), lo que ahorra espacio en ROM al evitar el analizador l√©xico que har√≠a falta posteriormente. En ese pre-proceso no s√≥lo se resumen palabras clave de varias letras en un s√≥lo byte, es decir, se tokeniza (qu√© palabro m√°s feo), sino que se aprovecha para insertar en los lugares m√°s convenientes para la ejecuci√≥n algunos elementos pre-calculados: un ejemplo es el propio tama√±o en memoria de cada l√≠nea, como se ha explicado antes, pero tambi√©n se almacenan silenciosamente los valores num√©ricos de los literales escritos en el texto (justo tras dichos literales), y se reservan huecos para recoger los argumentos de las funciones de usuario (justo tras los nombres de los correspondientes par√°metros en la sentencia &lt;code&gt;DEF FN&lt;/code&gt;), por ejemplo. &lt;/p&gt;
    &lt;p&gt;Lo que nunca, nunca se hace es reservar memoria para almacenar una tabla con las direcciones en memoria de cada l√≠nea de programa. Es decir, una tabla que permita saber, a partir de un n√∫mero de l√≠nea y con complejidad computacional constante (tardando siempre lo mismo independientemente del n√∫mero de l√≠nea, lo que formalmente se escribe O(1)), el lugar de memoria donde comienza el contenido tokenizado de dicha l√≠nea, para poder acceder r√°pidamente a las sentencias correspondientes y ejecutarlas.&lt;/p&gt;
    &lt;p&gt;Esto tiene una consecuencia importante para el int√©rprete: cualquier sentencia del lenguaje que admita como par√°metro una l√≠nea (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, etc.) implica, durante su ejecuci√≥n, buscar activamente el comienzo de dicha l√≠nea a lo largo de toda la memoria donde reside el programa. Desde el punto de vista de la complejidad computacional, esto no es constante, sino lineal (o sea, peor): O(n), siendo n el n√∫mero de l√≠neas de programa; en otras palabras: tarda m√°s cuanto m√°s lejos est√© la l√≠nea que se busca del comienzo del programa. El int√©rprete implementa esa b√∫squeda con un puntero (o sea, una direcci√≥n de memoria) que empieza apuntando a donde reside la primera l√≠nea en memoria; mientras no sea √©sta la l√≠nea que se busca, o la inmediatamente posterior a la que se busca si se busca una que no existe, suma al puntero el tama√±o que ocupa el contenido de la l√≠nea en memoria, obteniendo un nuevo puntero que apunta al lugar de memoria donde reside la siguiente l√≠nea, y repite el proceso.&lt;/p&gt;
    &lt;p&gt;Un importante resultado de esta implementaci√≥n del int√©rprete es que toda sentencia que implique un salto a una l√≠nea de programa (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt;, &lt;code&gt;FN&lt;/code&gt;) incrementar√° su tiempo de c√≥mputo linealmente con el n√∫mero de l√≠neas que haya antes de la de destino. Esto se puede comprobar con un programa que mide el tiempo para distintas l√≠neas de destino, como el que puede descargarse aqu√≠. Tras ejecutarlo (¬°cuidado!: tarda m√°s de 17 horas en terminar debido al nivel de precisi√≥n con el que queremos estimar los tiempos) obtenemos los siguientes resultados:&lt;/p&gt;
    &lt;p&gt;Como se observa, los saltos incrementan su tiempo en 71 microsegundos por cada l√≠nea m√°s que haya antes de la de destino; eso supone unos 7 milisegundos cuando hay 100 l√≠neas antes, lo que puede ser mucho si el salto se repite a menudo (por ejemplo, si lo hace un bucle &lt;code&gt;FOR&lt;/code&gt;‚Äì&lt;code&gt;NEXT&lt;/code&gt;). El programa anterior toma 10000 medidas de tiempo para calcular la media mostrada finalmente en la gr√°fica, por lo que el Teorema del L√≠mite Central  indica que los resultados expuestos arriba tienen una incertidumbre  peque√±a, del orden de 115.5 microsegundos si consideramos como fuente de  incertidumbre original m√°s importante los 20 milisegundos producidos como m√°ximo por la discretizaci√≥n del tiempo de la variable del sistema &lt;code&gt;FRAMES&lt;/code&gt; (el hecho de tomar tantos datos hace, por el mismo teorema, que la distribuci√≥n de la estimaci√≥n sea sim√©trica y no tenga bias, por lo que la media mostrada en la figura ser√° pr√°cticamente la verdadera, a pesar de dicha incertidumbre). Tambi√©n se observan en la gr√°fica los 5.6 milisegundos de media que se tarda en ejecutar todo lo que no es el salto en el programa de prueba.&lt;/p&gt;
    &lt;p&gt;Por tanto, aqu√≠ va la primera regla de eficiencia para mejorar el tiempo de c√≥mputo:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Si quieres que cierta parte de tu programa BASIC se ejecute m√°s r√°pido, y esa parte contiene el destino de bucles (&lt;/p&gt;&lt;code&gt;GO TO&lt;/code&gt;,&lt;code&gt;NEXT&lt;/code&gt;) o es llamada muy frecuentemente por otras (&lt;code&gt;GO SUB&lt;/code&gt;o&lt;code&gt;DEF FN&lt;/code&gt;), deber√≠as moverla al principio del programa, o lo m√°s cerca del principio que puedas; de esa manera, el int√©rprete tardar√° sensiblemente menos en encontrar las l√≠neas a las que hay que saltar.&lt;/quote&gt;
    &lt;p&gt;Para ayudar en la tarea de identificar estos problemas, el int√©rprete de BASIC incluido en la herramienta ZX-Basicus puede producir un perfil de la frecuencia de ejecuci√≥n de cada sentencia de un programa (opci√≥n &lt;code&gt;--profile&lt;/code&gt;); si la lista ordenada de frecuencias que recopila no va en orden creciente de n√∫mero de l√≠nea, significa que algunas l√≠neas de las m√°s frecuentemente llamadas podr√≠an estar mal situadas.&lt;/p&gt;
    &lt;p&gt;Existe un truco en BASIC para hacer que el int√©rprete no tenga que buscar desde el principio del programa para encontrar una l√≠nea, sino que empiece la b√∫squeda en otro lugar (m√°s cercano a lo que busque). Consiste en cambiar el contenido de la variable del sistema &lt;code&gt;PROG&lt;/code&gt;, que est√° en la direcci√≥n 23635 y ocupa 2 bytes, por la direcci√≥n de memoria donde resida la primera l√≠nea que queramos que el int√©rprete use para sus b√∫squedas (eso har√° que el int√©rprete ignore la existencia de todas las anteriores, as√≠ que ¬°√©stas dejar√°n de ser accesibles!). En general no hay modo f√°cil de saber en qu√© direcci√≥n de memoria reside una l√≠nea, pero la variable del sistema &lt;code&gt;NXTLIN&lt;/code&gt; (direcci√≥n 23637, 2 bytes) guarda en todo momento la direcci√≥n de la l√≠nea siguiente a la que estamos (la herramienta de an√°lisis de ZX-Basicus tambi√©n puede ser √∫til, pues produce un listado con la localizaci√≥n de cada elemento del programa BASIC en memoria si √©ste se ha guardado en un fichero &lt;code&gt;.tap&lt;/code&gt;). Por tanto, para, por ejemplo, hacer que un bucle vaya m√°s r√°pido, se puede hacer &lt;code&gt;POKE&lt;/code&gt; a los dos bytes de &lt;code&gt;PROG&lt;/code&gt; con el valor que tengan los de &lt;code&gt;NXTLIN&lt;/code&gt; cuando estemos en la l√≠nea anterior a la del bucle; desde ese momento, la primera l√≠nea del bucle ir√° tan r√°pida como si fuera la primera de todo el programa. Eso s√≠, ¬°es importante recuperar el valor original de &lt;code&gt;PROG&lt;/code&gt; si queremos volver a ejecutar alguna vez el resto!&lt;/p&gt;
    &lt;p&gt;El problema de la b√∫squeda secuencial de l√≠neas que hace la ROM del ZX tiene un efecto particular en el caso de las funciones de usuario (&lt;code&gt;DEF FN&lt;/code&gt;): dado que est√°n pensadas para ser llamadas desde diversos puntos del programa, deber√≠an ir al principio del mismo si esas llamadas van a ser frecuentes, pues cada vez que sean llamadas el int√©rprete tiene que buscarlas. (Una alternativa, preferida por muchos programadores, es no utilizar &lt;code&gt;DEF FN&lt;/code&gt;, dado el mayor coste de su ejecuci√≥n respecto a insertar la expresi√≥n directamente donde se necesite.) El perfil de frecuencias de uso producido por el int√©rprete de ZX-Basicus tambi√©n informa sobre el n√∫mero de veces que se ha llamado a cada funci√≥n de usuario con &lt;code&gt;FN&lt;/code&gt;, y la utilidad de transformaci√≥n tiene una opci√≥n (&lt;code&gt;--delunusedfn&lt;/code&gt;) que borra autom√°ticamente todas las sentencias &lt;code&gt;DEF FN&lt;/code&gt; no utilizadas en el c√≥digo.&lt;/p&gt;
    &lt;p&gt;Es importante hacer notar aqu√≠ que el int√©rprete de BASIC no s√≥lo tiene un comportamiento lineal (O(n)) a la hora de buscar l√≠neas de programa, sino tambi√©n al buscar sentencias. Es decir: si el programa pretende saltar a una sentencia distinta de la primera de una l√≠nea, el int√©rprete tendr√° que buscar dicha sentencia recorriendo todas las anteriores. En Sinclair BASIC existen instrucciones de salto a sentencias distintas de la primera de una l√≠nea: &lt;code&gt;NEXT&lt;/code&gt; y &lt;code&gt;RETURN&lt;/code&gt;, que por tanto sufren del problema de las b√∫squedas lineales. Es conveniente situar el retorno de la llamada o el principio del bucle al principio de la l√≠nea, para que el int√©rprete no tenga que buscar la sentencia concreta dentro de la misma, yendo sentencia a sentencia hasta encontrarla.&lt;/p&gt;
    &lt;p&gt;No existen instrucciones para saltar a sentencias (distintas de la primera) expl√≠citamente dadas por el usuario, pero esto se puede lograr enga√±ando al int√©rprete con un truco, que podr√≠amos llamar el ‚ÄúGOTO con POKE‚Äù, cuya existencia me ha se√±alado Rafael Velasco al verlo usado en alg√∫n programa escrito en una sola l√≠nea de BASIC. Este truco se basa en dos variables del sistema: &lt;code&gt;NEWPPC&lt;/code&gt; (direcci√≥n 23618 de memoria, 2 bytes) y &lt;code&gt;NSPPC&lt;/code&gt; (direcci√≥n 23620, 1 byte). En caso de que una sentencia del programa haga un salto (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;RETURN&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt; ‚Ä¶), se rellenan con la l√≠nea (en &lt;code&gt;NEWPPC&lt;/code&gt;) y la sentencia (en &lt;code&gt;NSPPC&lt;/code&gt;) a donde hay que saltar, mientras que si no hace un salto, s√≥lo se rellena &lt;code&gt;NSPPC&lt;/code&gt; con 255. Antes de ejecutar la siguiente sentencia, el int√©rprete consulta &lt;code&gt;NSPPC&lt;/code&gt;, y, si su bit n¬∫ 7 no es 1, salta a donde indiquen estas dos variables, mientras que si es 1, sigue ejecutando la siguiente sentencia del programa. El truco del ‚ÄúGOTO con POKE‚Äù consiste en manipular estas variables con &lt;code&gt;POKE&lt;/code&gt;, primero en &lt;code&gt;NEWPPC&lt;/code&gt; y luego en &lt;code&gt;NSPPC&lt;/code&gt;, de forma que, justo tras ejecutar el &lt;code&gt;POKE&lt;/code&gt; de &lt;code&gt;NSPPC&lt;/code&gt;, el int√©rprete se cree que tiene que hacer un salto a donde indican. De esta manera podemos ir a cualquier punto del programa, l√≠nea y sentencia incluidas.&lt;/p&gt;
    &lt;p&gt;Recuperando el hilo principal de esta entrada, las sentencias del lenguaje Sinclair BASIC afectadas por el problema de los n√∫meros de l√≠nea / n√∫mero de sentencia son:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;GO TO&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;GO SUB&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FN&lt;/code&gt;(requiere buscar la l√≠nea del correspondiente&lt;code&gt;DEF FN&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RETURN&lt;/code&gt;(debe retornar a un n√∫mero de l√≠nea almacenado en la pila de direcciones de retorno)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NEXT&lt;/code&gt;(debe ir a la l√≠nea correspondiente al&lt;code&gt;FOR&lt;/code&gt;de su variable)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RESTORE&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RUN&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LIST&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LLIST&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Como las cuatro √∫ltimas no suelen usarse m√°s que espor√°dicamente (las tres √∫ltimas pr√°cticamente nunca dentro de un programa), la identificaci√≥n de las zonas de c√≥digo que deben moverse al principio deber√≠a enfocarse en bucles, rutinas y funciones de usuario (&lt;code&gt;FN&lt;/code&gt;). &lt;/p&gt;
    &lt;p&gt;As√≠, los &lt;code&gt;RETURN&lt;/code&gt; deber√≠an hacerse hacia lugares pr√≥ximos al comienzo del programa, es decir, los &lt;code&gt;GO SUB&lt;/code&gt;  correspondientes deber√≠an estar all√≠ (al principio del programa), y, si puede ser, en la primera sentencia de sus respectivas l√≠neas para que no haya que buscar dentro de la l√≠nea la sentencia en cuesti√≥n, b√∫squeda que tambi√©n se hace linealmente. &lt;/p&gt;
    &lt;p&gt;Los bucles &lt;code&gt;FOR&lt;/code&gt; pueden sustituirse por r√©plicas consecutivas del cuerpo en caso de que √©stas no sean muy numerosas (esto se llama ‚Äúdesenrrollado de bucles‚Äù),  lo cual queda muy feo y ocupa m√°s memoria de programa pero evita el coste  adicional de ejecuci√≥n del salto &lt;code&gt;NEXT&lt;/code&gt; (y el de creaci√≥n de variable en el &lt;code&gt;FOR&lt;/code&gt;).  &lt;/p&gt;
    &lt;p&gt;En pocas palabras: el c√≥digo que llama mucho a otro c√≥digo, es llamado mucho por otro c√≥digo, o tiene muchos bucles internos deber√≠a ir al principio de un programa BASIC y en las primeras sentencias de dichas l√≠neas.&lt;/p&gt;
    &lt;p&gt;Quiero aprovechar para mencionar en este punto que, aunque es de lo m√°s com√∫n, en muchos casos ser√≠a recomendable no usar expresiones para las referencias a l√≠neas, al menos en las primeras etapas de la escritura de un programa (es decir, no escribir ‚Äúsaltos param√©tricos‚Äù como &lt;code&gt;GO TO 2*n+100&lt;/code&gt;, &lt;code&gt;GO SUB x*1000&lt;/code&gt;, etc., sino solamente con literales num√©ricos, como &lt;code&gt;GOTO 100&lt;/code&gt;, &lt;code&gt;GO SUB 2000&lt;/code&gt;). El uso de los saltos param√©tricos hace el mantenimiento del programa un verdadero infierno, e impide su an√°lisis autom√°tico. De todas formas, hay que admitir que usar expresiones como argumento de &lt;code&gt;GO TO&lt;/code&gt; / &lt;code&gt;GO SUB&lt;/code&gt; puede ser m√°s r√°pido que escribir sentencias &lt;code&gt;IF&lt;/code&gt; para lograr el mismo objetivo. &lt;/p&gt;
    &lt;p&gt;Todo el asunto de los n√∫meros de l√≠nea tiene una segunda consecuencia:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Para acelerar lo m√°s posible todo el programa deber√≠as escribir l√≠neas lo m√°s largas posible. As√≠, la b√∫squeda de una l√≠nea particular ser√° m√°s r√°pida, ya que habr√° que recorrer menos l√≠neas hasta llegar a ella (ir de una l√≠nea a la siguiente durante la b√∫squeda que hace el int√©rprete de la ROM cuesta el mismo tiempo independientemente de su longitud).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ZX-Basicus tiene una transformaci√≥n disponible con la opci√≥n &lt;code&gt;--mergelines&lt;/code&gt; que hace esto autom√°ticamente: aumenta el tama√±o de las l√≠neas siempre que esto respete el flujo del programa original. &lt;/p&gt;
    &lt;p&gt;N√≥tese que el usar menos l√≠neas pero m√°s largas ahorra tambi√©n espacio en memoria, ya que no hay que almacenar n√∫meros, longitudes ni marcas de fin de esas l√≠neas. Por contra, con l√≠neas largas es m√°s costoso encontrar una sentencia a la que haya que retornar con un &lt;code&gt;RETURN&lt;/code&gt; o volver con un &lt;code&gt;NEXT&lt;/code&gt;, as√≠ como buscar una funci√≥n de usuario (&lt;code&gt;DEF FN&lt;/code&gt;) que no est√© al principio de su l√≠nea, por lo que hay que tener tambi√©n eso en cuenta y llegar a una soluci√≥n de compromiso.&lt;/p&gt;
    &lt;p&gt;A√∫n hay una tercera consecuencia de esta limitaci√≥n del int√©rprete de BASIC de la ROM:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Las sentencias no ejecutables (&lt;/p&gt;&lt;code&gt;REM&lt;/code&gt;y sentencias vac√≠as) que ocupan una sola l√≠nea deber√≠an eliminarse siempre que se pueda, pues incrementan el tiempo de b√∫squeda, o bien ponerlas al final del todo. Asimismo, las sentencias&lt;code&gt;DATA&lt;/code&gt;, que normalmente no se usan m√°s de una vez durante la ejecuci√≥n del programa, deber√≠an estar al final del programa.&lt;/quote&gt;
    &lt;p&gt;ZX-Basicus tambi√©n ayuda en esto: permite eliminar autom√°ticamente comentarios &lt;code&gt;REM&lt;/code&gt; (opci√≥n &lt;code&gt;--delrem&lt;/code&gt;) y sentencias vac√≠as (opci√≥n &lt;code&gt;--delempty&lt;/code&gt;). La primera opci√≥n permite preservar algunos comentarios sin ser eliminados: los que comiencen por alg√∫n car√°cter que nosotros decidamos, pues siempre es interesante no dejar el c√≥digo totalmente indocumentado. &lt;/p&gt;
    &lt;p&gt;En cualquier caso, quiz√°s la opci√≥n m√°s importante del optimizador de c√≥digo de que dispone ZX-Basicus es &lt;code&gt;--move&lt;/code&gt;, que da la posibilidad de mover trozos de c√≥digo de un lugar a otro con menos esfuerzo que a mano. Con ella se puede cambiar de sitio una secci√≥n completa del programa; la utilidad se encarga de renumerar el resultado autom√°ticamente. Hay que tener en cuenta, sin embargo, que esta utilidad (como cualquier otra existente) no puede renumerar ni trabajar con n√∫meros de l√≠nea calculados mediante expresiones, por lo que todas las referencias a l√≠neas de programa deber√≠an estar escritas como literales, tal y como se ha recomendado antes.&lt;/p&gt;
    &lt;p&gt;.oOo.&lt;/p&gt;
    &lt;p&gt;[Click here to read this in Spanish ]&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is the first in a series of posts that explain the foundations of the (in)efficiency of pure BASIC programs written for the ZX Spectrum:&lt;/p&gt;
      &lt;p&gt;I. On line numbers&lt;/p&gt;
      &lt;p&gt;II. On variables&lt;/p&gt;
      &lt;p&gt;III. On expressions&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Sinclair BASIC interpreter that the ZX Spectrum included in ROM was, in so many aspects, a wonder of software, particularly in assembly programming.&lt;/p&gt;
    &lt;p&gt;In this series of posts we will visit the main issues that allow our BASIC programs to execute efficiently, mainly considering time, but also memory consumption.&lt;/p&gt;
    &lt;p&gt;In this first post we are concerned in particular with the lines in a program; beyond the need for numbering them explicitly, something that does not exist in any programming language since decades, we are interested in the efficciency of the BASIC interpreter when managing lines and their numbers.&lt;/p&gt;
    &lt;p&gt;Before going to the point, we summarize here some limits that the ZX Spectrum has related to program lines:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Program line numbers, once the program is stored in memory and ready to be executed, take 2 bytes (by the way, they are stored in big-endian format, the only case of that in the ZX). This could lead to line numbers in the range 0 to 65535 (maximum value that can be stored into 2 bytes), but unfortunately that cannot be done easily. When editing a program manually, only lines from 1 to 9999 are allowed. If the program is manipulated outside the editor (which can be done with &lt;code&gt;POKE&lt;/code&gt;), it is possible to have a line numbered as 0, and that line will appear in the listing of the program, but it will no longer be editable. In the same way (using&lt;code&gt;POKE&lt;/code&gt;) you can have lines above 9999, but this causes trouble: many statements that admit a line number as a parameter, such as&lt;code&gt;GOTO&lt;/code&gt;or&lt;code&gt;RESTORE&lt;/code&gt;, produce an error if that line is greater than 32767; the call stack stop working correctly if we do a&lt;code&gt;GO SUB&lt;/code&gt;to a line greater than 15871 (3DFF in hexadecimal); the interpreter reserves the line number 65534 to indicate that it is executing code from the edition buffer (and not from the program listing); also, listing the program on the screen does not work well with lines greater than 9999, and right at the moment we edit these lines manually, they will be set to line numbers with just 4 digits.&lt;/item&gt;
      &lt;item&gt;The length of each program line (in bytes) is stored after the line number, and occupies 2 bytes (this time in little-endian). This length does not take into account the 2 bytes of the line number or the 2 bytes of itself. We could think that each line can have up to 65535 bytes (a 0x0D byte has to always be at the end to mark the end of the line), and that the shortest line takes 2+2+1+1 = 6 bytes of memory if it contains just one statement without parameters, e.g., &lt;code&gt;10 CLEAR&lt;/code&gt;. A very important ROM routine, the one in charge of finding the line or variable that is after the current one, skipping the latter (called&lt;code&gt;NEXT-ONE&lt;/code&gt;and located at 0x19B8) works perfectly well with line lengths in the range 0 to 65535. However, during execution, the interpreter stops its work on a line as soon as it finds 0x0D in the beginning of a statement (if the line is longer because it has been externally manipulated, it will ignore the rest, thus the remaining space can be used for storing -hidden- data within the program), and more importantly: the interpreter yields an error if trying to execute more than 127 statements in a given line. Consequently, a line in execution can only have from 1 to 127 statements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once we have summarized these data, we will focus on a very specific feature of the BASIC interpreter of the ZX Spectrum, one that is crucial for the efficiency of running BASIC programs:&lt;/p&gt;
    &lt;p&gt;There is no table of program addresses indexed with line numbers&lt;/p&gt;
    &lt;p&gt;BASIC programs were pre-processed right after typing them (after typing whole lines in the case of ZX Spectrum +2 and up), which saved space in ROM by not implementing a lexical analyzer. In that pre-processing, multi-character keywords were summarized into one-byte tokens, but many other things happened too: number literals were coded in binary form and hidden near the source numbers, line lengths were stored at the beginning of each line, placeholders were prepared for the parameters of user functions (&lt;code&gt;DEF FN&lt;/code&gt;) in order to store arguments when they are called, etc.&lt;/p&gt;
    &lt;p&gt;Unfortunately, there is one thing that was not done before executing the program: to build a table that, for each line number, provides in constant time (computational complexity O(1)) the memory address where that line is stored.&lt;/p&gt;
    &lt;p&gt;This has an important effect in the interpreter execution: every time it finds a statement in the program that has a line number as a parameter, (e.g., &lt;code&gt;GOTO&lt;/code&gt;, &lt;code&gt;GOSUB&lt;/code&gt;, etc.), the interpreter must search the entire program memory, line by line, until finding the place in memory where the referred line resides. This has a computational complexity of O(n), being n the number of lines in the program, i.e., it is linearly more costly to find the last lines in the program than the earlier ones. The interpreter works like this: it starts with a memory address that points to the beginning of the program, reads the line number that is there, if it is the one searched for, or the one immediatly after it, ends, otherwise reads the line length, add that length to the pointer, and repeats the process.&lt;/p&gt;
    &lt;p&gt;The result of this interpreter inner workings is that any statement that involves a jump to a line in the program (&lt;code&gt;GOTO&lt;/code&gt;, &lt;code&gt;GOSUB&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt;, &lt;code&gt;FN&lt;/code&gt;) will increase its execution time linearly with the number of lines that exist before the one of destination. That can be checked out with a BASIC program that measures that time for different destinations, such as the one you can download here. After executing it (care!: it takes more than 17 hours to achieve the precision we require in the estimations) we got this:&lt;/p&gt;
    &lt;p&gt;As you can see, the execution time in a jump increases in 71 microseconds per line of the program that we add before the destination line; that amounts to about 7 milliseconds if you have 100 lines before the destination, which can be a lot if the jump is part of a loop that repeats a lot of times. Our testing program takes 10000 measurements to get the final average time, thus the Central Limit Theorem suggests that the results in the figure above have a small amount of uncertainty, of around 115.5 microseconds if we consider as the main source of original uncertainty the [0,20] milliseconds produced by the time discretization of the &lt;code&gt;FRAMES&lt;/code&gt; system variable (this uncertainty does not affect the fact that, due to the same theorem and the large number of measurements, the average estimates will be distributed symmetrically and unbiasedly, i.e., they are practically equal to the real ones). You can also observe in the graph above that the parts of the loops in the testing program that are not the jump itself consume 5.6 milliseconds on average.&lt;/p&gt;
    &lt;p&gt;The first consequence of this is the first rule for writing efficient programs in pure Sinclair BASIC for the ZX Spectrum:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Those parts of the program that require a faster execution should be placed at the beginning (smaller line numbers). The same should be done for parts that contain loops or routines that are frequently called.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ZX-Basicus has an optimizing tool that can help in this aspect. For instance, it can execute a BASIC program in the PC and collect a profile with the frequency of execution of each statement (using the &lt;code&gt;--profile&lt;/code&gt; option). In this way, you can identify those parts of the code that would require to be re-located earlier in the listing.&lt;/p&gt;
    &lt;p&gt;There is a BASIC trick to cheat the interpreter and make it to search for a line starting in a place different from the start of the program. It consists in changing the value of the system variable &lt;code&gt;PROG&lt;/code&gt;, which is located at the memory address 23635 and occupies 2 bytes, to the memory address of the first line we wish the interpreter to use for its line search (therefore ignoring all the previous ones). In general, it is not easy to get the memory address of a line, but you can consult the system variable &lt;code&gt;NXTLIN&lt;/code&gt; (at 23637, 2 bytes), which stores the address of the next line to be executed (the analysis tool of ZX-Basicus also provides this kind of information with the location in memory of every element in the BASIC program if it is stored in a &lt;code&gt;.tap&lt;/code&gt; file). You can make, for example, a loop faster: do &lt;code&gt;POKE&lt;/code&gt; in the two bytes of &lt;code&gt;PROG&lt;/code&gt; with the value stored in &lt;code&gt;NXTLIN&lt;/code&gt;, and do that right at the line previous to the one of the loop; the result is that the loop will be as fast as though it was in first line of the program. However, do not forget to restore the original value of &lt;code&gt;PROG&lt;/code&gt; in order to access previous parts of that program!&lt;/p&gt;
    &lt;p&gt;User functions definitions (&lt;code&gt;DEF FN&lt;/code&gt;) are specially sensitive to the problem of searching line numbers. They are devised for being called repeteadly, therefore, they should also be at the beginning of the program. However, many programmers choose not to use them because of their high execution cost (which includes finding the line where they are defined, evaluating arguments, placing their values in the placeholders, and evaluating the expression of their bodies). The profile produced by ZX-Basicus also reports the number of calls to user functions (&lt;code&gt;FN&lt;/code&gt;), and it provides an option (&lt;code&gt;--delunusedfn&lt;/code&gt;) that automatically delete all &lt;code&gt;DEF FN&lt;/code&gt; that are not called in the program.&lt;/p&gt;
    &lt;p&gt;It is important to note that the BASIC interpreter has a linear (O(n)) behaviour not only when searching for lines, but also when searching for statements within a line. If the program tries to jump to a statement different from the first one in a line, the interpreter will search for that statement by skipping all the previous ones. In Sinclair BASIC we have instructions that may jump to statements different from the first ones in their lines: &lt;code&gt;NEXT&lt;/code&gt; and &lt;code&gt;RETURN&lt;/code&gt;, that, consequently, suffer from the problem of the linear searches. It is better to place the return of the call or the start of the loop at the beginning of a line to prevent the interpreter to conduct a linear search (statement by statement) to find them.&lt;/p&gt;
    &lt;p&gt;There are no instructions in the language to jump to statements that are explicitly given by the user, but that can be achieved by cheating the interpreter with a trick, that we could call ‚ÄúGOTO-with-POKE‚Äù, whose has been brought to my attention by Rafael Velasco, that saw it in a BASIC program entirely written in a single line. It is based on two system variables: &lt;code&gt;NEWPPC&lt;/code&gt; (address 23618, 2 bytes) and &lt;code&gt;NSPPC&lt;/code&gt; (address 23620, 1 byte). When a program statement makes a jump (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;RETURN&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt; ‚Ä¶), the target line is stored into &lt;code&gt;NEWPPC&lt;/code&gt; and the target statement into &lt;code&gt;NSPPC&lt;/code&gt;; if the statement does not make a jump, &lt;code&gt;NSPPC&lt;/code&gt; is filled with 255; before executing the next statement, the interpret reads &lt;code&gt;NSPPC&lt;/code&gt; and, if the bit 7 of this variables is not 1, jumps to the place defined by &lt;code&gt;NEWPPC&lt;/code&gt;:&lt;code&gt;NSPPC&lt;/code&gt;, but if that bit is 1 it just goes on with the next statement. The ‚ÄúGOTO-with-POKE‚Äù trick consists in &lt;code&gt;POKE&lt;/code&gt;ing those variables, firstly &lt;code&gt;NEWPPC&lt;/code&gt;, then &lt;code&gt;NSPPC&lt;/code&gt;; right after the last &lt;code&gt;POKE&lt;/code&gt;, the interpreter believes there is a jump to do. In this way, we can go to any line and statement in our program.&lt;/p&gt;
    &lt;p&gt;Recovering the main thread of this post, the statements of the Sinclair BASIC language that involve to search lines in the program are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;GO TO&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;GO SUB&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FN&lt;/code&gt;(since&lt;code&gt;DEF FN&lt;/code&gt;must be searched for)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RETURN&lt;/code&gt;(it returns to a certain number of line and statement)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NEXT&lt;/code&gt;(it jumps to the corresponding&lt;code&gt;FOR&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RESTORE&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RUN&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LIST&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LLIST&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since the last four are used sporadically (the last three are very rare inside a program), the identification of parts of the program to be placed at the beginning for gaining in efficiency should focus on loops, routines and user functions. &lt;code&gt;RETURN&lt;/code&gt; statements should be used to return to places close to the beginning too, if they are frequently used, i.e., the corresponding &lt;code&gt;GO SUB&lt;/code&gt;  should be placed at the beginning, and, if possible, at the beginning  of their lines in order to reduce the cost of searching them within those lines. Also, in cases where they can not be re-placed, &lt;code&gt;FOR&lt;/code&gt; loops can be unrolled  (repeating their bodies as many times as iterations they have) to avoid  the jumps and the maintainance of the iteration variable. In summary: the code that calls a lot of routines, or is called frequently, or has many internal loops, should be placed at the beginning of the program. &lt;/p&gt;
    &lt;p&gt;I also recommend to only use literal numbers in the parameters of the statements that need a line (e.g., &lt;code&gt;GOTO 100&lt;/code&gt;, &lt;code&gt;GO SUB 2000&lt;/code&gt;), at least in the first stages of the writing of a program; do not use expressions at that time (‚Äúparametrical jumps‚Äù, e.g., &lt;code&gt;GO TO 2*n+100&lt;/code&gt;, &lt;code&gt;GO SUB x*1000&lt;/code&gt;, etc.), since that makes the maintainance and analysis of the program really difficult. I have to admit, though, that  using expressions as arguments in &lt;code&gt;GO TO&lt;/code&gt; / &lt;code&gt;GO SUB&lt;/code&gt; usually runs faster than writing &lt;code&gt;IF&lt;/code&gt; statements to achieve the same functionality. &lt;/p&gt;
    &lt;p&gt;The second consequence of the interpreter lacking an efficient line number table is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lines should be long (the maximum length is 127 statements in a line for the ROM interpreter not to issue an error). In that way, the search for a particular one will be more efficient, since traversing the lines has the same cost independently on their lengths (it only depends on the number of lines).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In this aspect, ZX-Basicus has an option (&lt;code&gt;--mergelines&lt;/code&gt;) that automatically merges contiguous lines, as long as that does not changes the program execution flow, in order to obtain the least number of lines.&lt;/p&gt;
    &lt;p&gt;Notice that having less but longer lines also saves memory space, since there are less line numbers and lengths (and end-line markers) to store. However, having longer lines makes less efficient the search for some statement within them (as in the case of &lt;code&gt;FOR&lt;/code&gt;‚Ä¶&lt;code&gt;NEXT&lt;/code&gt;, or &lt;code&gt;GO SUB&lt;/code&gt;, or &lt;code&gt;DEF FN&lt;/code&gt;). A suitable trade-off must be reached.&lt;/p&gt;
    &lt;p&gt;Finally, the third consequence of not having a line number table is:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Non-executable statements (&lt;/p&gt;&lt;code&gt;REM&lt;/code&gt;and empty statements) that fill entire lines should be eliminated or placed at the end, since they increase the search time for no reason. Also,&lt;code&gt;DATA&lt;/code&gt;statements, that are commonly used only once during the program execution, are excellent candidates to be placed at the end of the program.&lt;/quote&gt;
    &lt;p&gt;In this, ZX-Basicus has also some help for the programmer: it can delete automatically empty statements (&lt;code&gt;--delempty&lt;/code&gt;) and &lt;code&gt;REM&lt;/code&gt; (&lt;code&gt;--delrem&lt;/code&gt;); it can preserve some of the latter for keeping minimum documentation, though.&lt;/p&gt;
    &lt;p&gt;All in all, there is a fundamental tool in ZX-Basicus that is related to this post: option &lt;code&gt;--move&lt;/code&gt; re-locates portions of code, renumbering automatically all the line references (it can also serve to renumber the whole program, but that has no relation to speed-ups). Only take into account that it cannot work with line references that are not literal numbers (expressions, variables, etc.). &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262480</guid><pubDate>Sun, 14 Dec 2025 12:04:44 +0000</pubDate></item><item><title>Europeans' health data sold to US firm run by ex-Israeli spies</title><link>https://www.ftm.eu/articles/europe-health-data-us-firm-israel-spies</link><description>&lt;doc fingerprint="bfd091199664b185"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Europeans‚Äô health data sold to U.S. firm run by ex-Israeli spies&lt;/head&gt;&lt;p&gt;The European messaging service Zivver ‚Äì which is used for confidential communication by governments and hospitals in the EU and the U.K. ‚Äì has been sold to Kiteworks, an American company with strong links to Israeli intelligence. Experts have expressed deep concerns over the deal.&lt;/p&gt;&lt;head rend="h3"&gt;This story in 1 minute&lt;/head&gt;&lt;p&gt;What‚Äôs the news?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;With the sale of Amsterdam-based data security company Zivver, sensitive information about European citizens is now in the hands of Kiteworks.&lt;/item&gt;&lt;item&gt;The CEO of the American tech company is a former cyber specialist from an elite unit of the Israeli army, as are several other members of its top management.&lt;/item&gt;&lt;item&gt;Various institutions in Europe and the U.K. ‚Äì from hospitals to courts and immigration services ‚Äì use Zivver to send confidential documents. While Zivver says these documents are encrypted, an investigation by Follow the Money shows that the company is able to read their contents.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Why does this matter?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Cybersecurity and intelligence experts told Follow the Money that the takeover should either have been prevented or properly assessed in advance.&lt;/item&gt;&lt;item&gt;Zivver processes information that could be extremely valuable to third parties, such as criminals or foreign intelligence services.&lt;/item&gt;&lt;item&gt;That information is now subject to invasive U.S. law, and overseen by a company with well-documented links to Israeli intelligence.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How was this investigated?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Follow the Money investigated the acquisition of Zivver and the management of Kiteworks, and spoke to experts in intelligence services and cyber security.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This article is part of an ongoing series.&lt;/p&gt;The EU Files&lt;p&gt;When the American data security company Kiteworks bought out its Dutch industry peer Zivver in June, CEO Jonathan Yaron described it as ‚Äúa proud moment for all of us‚Äù.&lt;/p&gt;&lt;p&gt;The purchase was ‚Äúa significant milestone in Kiteworks‚Äô continued mission to safeguard sensitive data across all communication channels‚Äù, he added in a LinkedIn post.&lt;/p&gt;&lt;p&gt;But what Yaron did not mention was that this acquisition ‚Äì coming at a politically charged moment between the U.S. and the EU ‚Äì put highly sensitive, personal data belonging to European and British citizens directly into American hands.&lt;/p&gt;&lt;p&gt;Zivver is used by institutions including hospitals, health insurers, government services and immigration authorities in countries including the Netherlands, Germany, Belgium and the U.K.&lt;/p&gt;&lt;p&gt;Neither did Yaron mention that much of Kiteworks‚Äô top management ‚Äì himself included ‚Äì are former members of an elite Israeli Defence Force unit that specialised in eavesdropping and breaking encrypted communications.&lt;/p&gt;&lt;p&gt;Our journalism is only possible thanks to the trust of our paying members. Not a member yet? Sign up now&lt;/p&gt;&lt;p&gt;In addition to this, an investigation by Follow the Money shows that data processed by Zivver is less secure than the service leads its customers to believe. Research found that emails and documents sent by Zivver can be read by the company itself. This was later confirmed by Zivver to Follow the Money.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;‚ÄúAll of the red flags should have been raised during this acquisition‚Äù&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Zivver maintained, however, that it does not have access to the encryption keys used by customers, and therefore cannot hand over data to U.S. authorities.&lt;/p&gt;&lt;p&gt;This is despite independent researchers confirming that the data was ‚Äì for a brief period ‚Äì accessible to the company. If U.S. officials wanted access to such communication, Zivver would be legally obligated to provide it.&lt;/p&gt;&lt;p&gt;Cybersecurity experts now point to serious security concerns, and ask why this sale seems to have gone through without scrutiny from European authorities.&lt;/p&gt;&lt;p&gt;‚ÄúAll of the red flags should have been raised during this acquisition,‚Äù said intelligence expert Hugo Vijver, a former long-term officer in AIVD, the Dutch security service.&lt;/p&gt;&lt;head rend="h2"&gt;Classified documents&lt;/head&gt;&lt;p&gt;Amsterdam-based Zivver ‚Äì which was founded in 2015 by Wouter Klinkhamer and Rick Goud ‚Äì provides systems for the encrypted exchange of information via email, chat and video, among other means.&lt;/p&gt;&lt;p&gt;Dutch courts, for example, work with Zivver to send classified documents, and solicitors use the service to send confidential information to the courts.&lt;/p&gt;&lt;p&gt;Other government agencies in the Netherlands ‚Äì including the immigration service ‚Äì also use Zivver. So do vital infrastructure operators such as the Port of Rotterdam and The Hague Airport.&lt;/p&gt;&lt;p&gt;In the U.K., a number of NHS hospitals and local councils use the company. In Belgium and Germany it is used in major hospitals.&lt;/p&gt;&lt;p&gt;The information that Zivver secures for its customers is therefore confidential and sensitive by nature.&lt;/p&gt;&lt;p&gt;When approached by Follow the Money, a number of governmental agencies said the company‚Äôs Dutch origins were a big factor in their decision to use Zivver.&lt;/p&gt;&lt;p&gt;Additionally, the fact that the data transferred via Zivver was stored on servers in Europe also played a role in their decisions.&lt;/p&gt;&lt;p&gt;Now that Zivver has been acquired by a company in the United States, that data is subject to far-reaching American laws. This means that the U.S. government can request access to this information if it wishes, regardless of where the data is stored.&lt;/p&gt;&lt;head rend="h2"&gt;The Trump effect&lt;/head&gt;&lt;p&gt;These laws are not new, but they have become even more draconian since U.S. President Donald Trump's return to office, according to experts.&lt;/p&gt;&lt;p&gt;Bert Hubert, a former regulator of the Dutch intelligence services, warned: ‚ÄúAmerica is deteriorating so rapidly, both legally and democratically, that it would be very naive to hand over your courts and hospitals to their services.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúTrump recently called on Big Tech to ignore European legislation. And that is what they are going to do. We have no control over it,‚Äù he added.&lt;/p&gt;&lt;p&gt;In Europe, Hubert said: ‚ÄúWe communicate almost exclusively via American platforms. And that means that the U.S. can read our communications and disrupt our entire society if they decide that they no longer like us.‚Äù&lt;/p&gt;&lt;p&gt;Zivver had offered an alternative ‚Äì a European platform governed by EU law.&lt;/p&gt;&lt;p&gt;‚ÄúWe are now throwing that away. If you want to share something confidential with a court or government, consider using a typewriter. That's about all we have left,‚Äù Hubert said.&lt;/p&gt;&lt;head rend="h2"&gt;Israeli ties&lt;/head&gt;&lt;p&gt;Beyond American jurisdiction, Kiteworks‚Äô management raises another layer of concern: its links to Israeli intelligence.&lt;/p&gt;&lt;p&gt;Several of the company‚Äôs top executives, including CEO Yaron, are veterans of Unit 8200, the elite cyber unit of the Israel Defence Force (IDF). The unit is renowned for its code-breaking abilities and feared for its surveillance operations.&lt;/p&gt;&lt;quote&gt;In Israel, there is a revolving door between the army, lobby, business and politics&lt;/quote&gt;&lt;p&gt;Unit 8200 has been linked to major cyber operations, including the Stuxnet attack on Iranian nuclear facilities in 2007. More recently, it was accused of orchestrating the detonation of thousands of pagers in Lebanon, an incident the United Nations said violated international law and killed at least two children.&lt;/p&gt;&lt;p&gt;The unit employs thousands of young recruits identified for their digital skills. It is able to intercept global telephone and internet traffic.&lt;/p&gt;&lt;p&gt;International media have reported that Unit 8200 intercepts and stores an average of one million Palestinian phone calls every hour, data that ends up on Microsoft servers in Europe.&lt;/p&gt;&lt;p&gt;Some veterans themselves have also objected to the work of the unit. In 2014, dozens of reservists signed a letter to Israeli leaders saying they no longer wanted to participate in surveillance of the occupied territories.&lt;/p&gt;&lt;p&gt;‚ÄúThe lines of communication between the Israeli defence apparatus and the business community have traditionally been very short,‚Äù said Dutch intelligence expert Vijver. ‚ÄúIn Israel, there is a revolving door between the army, lobby, business and politics.‚Äù&lt;/p&gt;&lt;head rend="h2"&gt;Veterans at the helm&lt;/head&gt;&lt;p&gt;That revolving door is clearly visible in big U.S. tech companies ‚Äì and Kiteworks is no exception.&lt;/p&gt;&lt;p&gt;Aside from Yaron, both Chief Business Officer Yaron Galant and Chief Product Officer Amit Toren served in Unit 8200, according to publicly available information.&lt;/p&gt;&lt;p&gt;They played a direct role in negotiating the acquisition of Zivver. Their background was known to Zivver‚Äôs directors Goud and Klinkhamer at the time.&lt;/p&gt;&lt;p&gt;Other senior figures also have military intelligence backgrounds. Product director Ron Margalit worked in Unit 8200 before serving in the office of Israeli Prime Minister Benjamin Netanyahu. Mergers and acquisitions director Uri Kedem is a former Israeli naval captain.&lt;/p&gt;&lt;p&gt;Kiteworks is not unique in this respect.&lt;/p&gt;&lt;p&gt;Increasing numbers of U.S. cybersecurity firms now employ former Israeli intelligence officers. This trend, experts say, creates vulnerabilities that are rarely discussed.&lt;/p&gt;&lt;p&gt;An independent researcher quoted by U.S. Drop Site News said: ‚ÄúNot all of these veterans will send classified data to Tel Aviv. But the fact that so many former spies work for these companies does create a serious vulnerability: no other country has such access to the American tech sector.‚Äù&lt;/p&gt;&lt;p&gt;Or, as the ex-intelligence regulator Hubert put it: ‚ÄúGaining access to communication flows is part of Israel‚Äôs long-term strategy. A company like Zivver fits perfectly into that strategy.‚Äù&lt;/p&gt;&lt;p&gt;The information handled by Zivver ‚Äì confidential communications between governments, hospitals and citizens ‚Äì is a potential goldmine for intelligence services.&lt;/p&gt;&lt;p&gt;According to intelligence expert Vijver, access to this kind of material makes it easier to pressure individuals into cooperating with intelligence agencies. Once an intelligence service has access to medical, financial and personal data, it can more easily pressure people into spying for it, he said.&lt;/p&gt;&lt;p&gt;But the gain for intelligence services lies not just in sensitive information, said Hubert: ‚ÄúAny data that allows an agency to tie telephone numbers, addresses or payment data to an individual is of great interest to them.‚Äù&lt;/p&gt;&lt;p&gt;He added: ‚ÄúIt is exactly this type of data that is abundantly present in communications between civilians, governments and care institutions. In other words, the information that flows through a company like Zivver is extremely valuable for intelligence services.‚Äù&lt;/p&gt;&lt;head rend="h2"&gt;A flawed security model?&lt;/head&gt;&lt;p&gt;These geopolitical concerns become more pronounced when combined with technical worries about Zivver‚Äôs encryption.&lt;/p&gt;&lt;p&gt;For years, Zivver presented itself as a European alternative that guaranteed privacy. Its marketing materials claimed that messages were encrypted on the sender‚Äôs device and that the company had ‚Äúzero access‚Äù to content.&lt;/p&gt;&lt;p&gt;But an investigation by two cybersecurity experts at a Dutch government agency, at the request of Follow the Money, undermines this claim.&lt;/p&gt;&lt;p&gt;The experts, who participated in the investigation on condition of anonymity, explored what happened when that government agency logged into Zivver‚Äôs web application to send information.&lt;/p&gt;&lt;p&gt;Tests showed that when government users sent messages through Zivver‚Äôs web application, the content ‚Äì including attachments ‚Äì was uploaded to Zivver‚Äôs servers as readable text before being encrypted. The same process applied to email addresses of senders and recipients.&lt;/p&gt;&lt;p&gt;‚ÄúIn these specific cases, Zivver processed the messages in readable form,‚Äù said independent cybersecurity researcher Matthijs Koot, who verified the findings.&lt;/p&gt;&lt;p&gt;‚ÄúEven if only briefly, technically speaking it is possible that Zivver was able to view these messages,‚Äù he said.&lt;/p&gt;&lt;p&gt;He added: ‚ÄúWhether a message is encrypted at a later stage makes little difference. It may help against hackers, but it no longer matters in terms of protection against Zivver.‚Äù&lt;/p&gt;&lt;p&gt;Despite these findings, Zivver continues to insist on its website and in promotional material elsewhere ‚Äì including on the U.K. government‚Äôs Digital Marketplace ‚Äì that ‚Äúcontents of secure messages are inaccessible to Zivver and third parties‚Äù.&lt;/p&gt;&lt;p&gt;So far, no evidence has surfaced that Zivver misused its technical access. But now that the company is owned by Kiteworks, experts see a heightened risk.&lt;/p&gt;&lt;p&gt;Former intelligence officer Vijver puts it bluntly: ‚ÄúGiven the links between Zivver, Kiteworks and Unit 8200, I believe there is zero chance that no data is going to Israel. To think otherwise is completely naive.‚Äù&lt;/p&gt;&lt;head rend="h2"&gt;A missed safeguard&lt;/head&gt;&lt;p&gt;The sale of Zivver could technically have been blocked or investigated under Dutch law. According to the Security Assessment of Investments, Mergers and Acquisitions Act, such sensitive takeovers are supposed to be reviewed by a specialised agency.&lt;/p&gt;&lt;p&gt;But the Dutch interior ministry declared that Zivver was not part of the country‚Äôs ‚Äúcritical infrastructure,‚Äù meaning that no review was carried out.&lt;/p&gt;&lt;p&gt;That, in Hubert‚Äôs view, was ‚Äúa huge blunder‚Äù.&lt;/p&gt;&lt;p&gt;‚ÄúIt‚Äôs bad enough that a company that plays such an important role in government communications is falling into American hands, but the fact that there are all kinds of Israeli spies there is very serious,‚Äù he said.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;‚ÄúThe takeover is taking place in an unsafe world full of geopolitical tensions‚Äù&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Experts say the Zivver case highlights Europe‚Äôs lack of strategic control over its digital infrastructure.&lt;/p&gt;&lt;p&gt;Mari√´tte van Huijstee of the Netherlands-based Rathenau Institute said: ‚ÄúI doubt whether the security of sensitive emails and files ‚Ä¶ should be left to the private sector. And if you think that is acceptable, should we leave it to non-European parties over whom we have no control?‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúWe need to think much more strategically about our digital infrastructure and regulate these kinds of issues much better, for example by designating encryption services as vital infrastructure,‚Äù she added.&lt;/p&gt;&lt;p&gt;Zivver, for its part, claimed that security will improve under Kiteworks. Zivver‚Äôs full responses to Follow the Money‚Äôs questions can be read here and here.&lt;/p&gt;&lt;p&gt;But Van Huijstee was not convinced.&lt;/p&gt;&lt;p&gt;‚ÄúKiteworks employs people who come from a service that specialises in decrypting files,‚Äù she said.&lt;/p&gt;&lt;p&gt;‚ÄúThe takeover is taking place in an unsafe world full of geopolitical tensions, and we are dealing with data that is very valuable. In such a case, trust is not enough and more control is needed.‚Äù&lt;/p&gt;&lt;head rend="h3"&gt;Related articles&lt;/head&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;head rend="h3"&gt;Collection&lt;/head&gt;&lt;head rend="h3"&gt;Authors&lt;/head&gt;&lt;head rend="h3"&gt;Sebastiaan Brommersma&lt;/head&gt;&lt;p&gt;Former IP lawyer. Currently writing about digital technology, from big tech and big data to AI and cybercrime.&lt;/p&gt;&lt;head rend="h3"&gt;Siem Eikelenboom&lt;/head&gt;&lt;p&gt;Experienced investigative journalist. Was part of the international team that investigated the Panama Papers.&lt;/p&gt;Send a news tip&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262524</guid><pubDate>Sun, 14 Dec 2025 12:15:09 +0000</pubDate></item><item><title>Kimi K2 1T model runs on 2 512GB M3 Ultras</title><link>https://twitter.com/awnihannun/status/1943723599971443134</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262734</guid><pubDate>Sun, 14 Dec 2025 13:04:34 +0000</pubDate></item><item><title>Vacuum Is a Lie: About Your Indexes</title><link>https://boringsql.com/posts/vacuum-is-lie/</link><description>&lt;doc fingerprint="4914033578e42db5"&gt;
  &lt;main&gt;
    &lt;p&gt;There is common misconception that troubles most developers using PostgreSQL: tune VACUUM or run VACUUM, and your database will stay healthy. Dead tuples will get cleaned up. Transaction IDs recycled. Space reclaimed. Your database will live happily ever after.&lt;/p&gt;
    &lt;p&gt;But there are couple of dirty "secrets" people are not aware of. First of them being VACUUM is lying to you about your indexes.&lt;/p&gt;
    &lt;head rend="h2"&gt;The anatomy of storage&lt;/head&gt;
    &lt;p&gt;When you delete a row in PostgreSQL, it is just marked as a 'dead tuple'. Invisible for new transactions but still physically present. Only when all transactions referencing the row are finished, VACUUM can come along and actually remove them - reclamining the space in the heap (table) space.&lt;/p&gt;
    &lt;p&gt;To understand why this matters differently for tables versus indexes, you need to picture how PostgreSQL actually stores your data.&lt;/p&gt;
    &lt;p&gt;Your table data lives in the heap - a collection of 8 KB pages where rows are stored wherever they fit. There's no inherent order. When you INSERT a row, PostgreSQL finds a page with enough free space and slots the row in. Delete a row, and there's a gap. Insert another, and it might fill that gap - or not - they might fit somewhere else entirely.&lt;/p&gt;
    &lt;p&gt;This is why &lt;code&gt;SELECT * FROM users&lt;/code&gt; without an ORDER BY can return rows in order
initially, and after some updates in seemingly random order, and that order can
change over time. The heap is like Tetris. Rows drop into whatever space is
available, leaving gaps when deleted.&lt;/p&gt;
    &lt;p&gt;When VACUUM runs, it removes those dead tuples and compacts the remaining rows within each page. If an entire page becomes empty, PostgreSQL can reclaim it entirely.&lt;/p&gt;
    &lt;p&gt;And while indexes are on surface the same collection of 8KB pages, they are different. A B-tree index must maintain sorted order - that's the whole point of their existence and the reason why &lt;code&gt;WHERE id = 12345&lt;/code&gt; is so
fast. PostgreSQL can binary-search down the tree instead of scanning every
possible row. You can learn more about the fundamentals of B-Tree Indexes and
what makes them fast.&lt;/p&gt;
    &lt;p&gt;But if the design of the indexes is what makes them fast, it's also their biggest responsibility. While PostgreSQL can fit rows into whatever space is available, it can't move the entries in index pages to fit as much as possible.&lt;/p&gt;
    &lt;p&gt;VACUUM can remove dead index entries. But it doesn't restructure the B-tree. When VACUUM processes the heap, it can compact rows within a page and reclaim empty pages. The heap has no ordering constraint - rows can be anywhere. But B-tree pages? They're locked into a structure. VACUUM can remove dead index entries, yes.&lt;/p&gt;
    &lt;p&gt;Many developers assume VACUUM treats all pages same. No matter whether they are heap or index pages. VACUUM is supposed to remove the dead entries, right?&lt;/p&gt;
    &lt;p&gt;Yes. But here's what it doesn't do - it doesn't restructure the B-tree.&lt;/p&gt;
    &lt;p&gt;What VACUUM actually does&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Removes dead tuple pointers from index pages&lt;/item&gt;
      &lt;item&gt;Marks completely empty pages as reusable&lt;/item&gt;
      &lt;item&gt;Updates the free space map&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What VACUUM cannot do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Merge sparse pages together (can do it for empty pages)&lt;/item&gt;
      &lt;item&gt;Reduce tree depth&lt;/item&gt;
      &lt;item&gt;Deallocate empty-but-still-linked pages&lt;/item&gt;
      &lt;item&gt;Change the physical structure of the B-tree&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your heap is Tetris, gaps can get filled. Your B-tree is a sorted bookshelf. VACUUM can pull books out, but can't slide the remaining ones together. You're left walking past empty slots every time you scan.&lt;/p&gt;
    &lt;head rend="h2"&gt;The experiment&lt;/head&gt;
    &lt;p&gt;Let's get hands-on and create a table, fill it, delete most of it and watch what happens.&lt;/p&gt;
    &lt;code&gt;CREATE EXTENSION IF NOT EXISTS pgstattuple;
CREATE TABLE demo (id integer PRIMARY KEY, data text);

-- insert 100,000 rows
INSERT INTO demo (id, data)
SELECT g, 'Row number ' || g || ' with some extra data'
FROM generate_series(1, 100000) g;

ANALYZE demo;
&lt;/code&gt;
    &lt;p&gt;At this point, our index is healthy. Let's capture the baseline:&lt;/p&gt;
    &lt;code&gt;SELECT
    relname,
    pg_size_pretty(pg_relation_size(oid)) as file_size,
    pg_size_pretty((pgstattuple(oid)).tuple_len) as actual_data
FROM pg_class
WHERE relname IN ('demo', 'demo_pkey');
&lt;/code&gt;
    &lt;code&gt;relname  | file_size | actual_data
-----------+-----------+-------------
demo      | 7472 kB   | 6434 kB
demo_pkey | 2208 kB   | 1563 kB
&lt;/code&gt;
    &lt;p&gt;Now remove some data, 80% to be precise - somewhere in the middle:&lt;/p&gt;
    &lt;code&gt;DELETE FROM demo WHERE id BETWEEN 10001 AND 90000;
&lt;/code&gt;
    &lt;p&gt;The goal is to simulate a common real-world pattern: data retention policies, bulk cleanup operations, or the aftermath of a data migration gone wrong.&lt;/p&gt;
    &lt;code&gt;VACUUM demo;

SELECT
    relname,
    pg_size_pretty(pg_relation_size(oid)) as file_size,
    pg_size_pretty((pgstattuple(oid)).tuple_len) as actual_data
FROM pg_class
WHERE relname IN ('demo', 'demo_pkey');
&lt;/code&gt;
    &lt;code&gt;relname  | file_size | actual_data
-----------+-----------+-------------
demo      | 7472 kB   | 1278 kB
demo_pkey | 2208 kB   | 1563 kB
&lt;/code&gt;
    &lt;p&gt;The table shrunk significantly, while index remained unchanged. You now have 20,000 rows indexed by a structure build to handle 100,000. Please, also notice &lt;code&gt;file_size&lt;/code&gt; remain unchanged. VACUUM doesn't return space to the OS, it only
marks pages as reusable within PostgreSQL.&lt;/p&gt;
    &lt;p&gt;This experiment is really an extreme case, but demonstrates the problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding page states&lt;/head&gt;
    &lt;p&gt;Leaf pages have several states:&lt;/p&gt;
    &lt;p&gt;Full page (&amp;gt;80% density), when the page contains many index entries, efficiently utilizing space. Each 8KB page read returns substantial useful data. This is optimal state.&lt;/p&gt;
    &lt;p&gt;Partial page (40-80% density) with some wasted space, but still reasonably efficient. Common at tree edges or after light churn. Nothing to be worried about.&lt;/p&gt;
    &lt;p&gt;Sparse page (&amp;lt;40% density) is mostly empty. You're reading an 8KB page to find a handful of entries. The I/O cost is the same as a full page, but you get far less value.&lt;/p&gt;
    &lt;p&gt;Empty page (0% density) with zero live entries, but the page still exists in the tree structure. Pure overhead. You might read this page during a range scan and find absolutely nothing useful.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on fillfactor&lt;/head&gt;
    &lt;p&gt;You might be wondering how can fillfactor help with this? It's the setting you can apply both for heap and leaf pages, and controls how full PostgreSQL packs the pages during the data storage. The default value for B-tree indexes is 90%. This leaves 10% of free space on each leaf page for future insertions.&lt;/p&gt;
    &lt;code&gt;CREATE INDEX demo_index ON demo(id) WITH (fillfactor = 70);
&lt;/code&gt;
    &lt;p&gt;A lower fillfactor (like 70%) leaves more room, which can reduce page splits when you're inserting into the middle of an index - useful for tables random index column inserts or those with heavily updated index columns.&lt;/p&gt;
    &lt;p&gt;But if you followed carefully the anatomy of storage section, it doesn't help with the bloat problem. Quite the oppossite. If you set lower fillfactor and then delete majority of your rows, you actually start with more pages, and bigger chance to end up with more sparse pages than partial pages.&lt;/p&gt;
    &lt;p&gt;Leaf page fillfactor is about optimizing for updates and inserts. It's not a solution for deletion or index-column update bloat.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why the planner gets fooled&lt;/head&gt;
    &lt;p&gt;PostgreSQL's query planner estimates costs based on physical statistics, including the number of pages in an index.&lt;/p&gt;
    &lt;code&gt;EXPLAIN ANALYZE SELECT * FROM demo WHERE id BETWEEN 10001 AND 90000;
&lt;/code&gt;
    &lt;code&gt;QUERY PLAN
--------------------------------------------------------------------------------------------------------------------
  Index Scan using demo_pkey on demo  (cost=0.29..29.29 rows=200 width=41) (actual time=0.111..0.112 rows=0 loops=1)
    Index Cond: ((id &amp;gt;= 10001) AND (id &amp;lt;= 90000))
  Planning Time: 1.701 ms
  Execution Time: 0.240 ms
(4 rows)
&lt;/code&gt;
    &lt;p&gt;While the execution is almost instant, you need to look behind the scenes. The planner estimated 200 rows and got zero. It traversed the B-tree structure expecting data that doesn't exist. On a single query with warm cache, this is trivial. Under production load with thousands of queries and cold pages, you're paying I/O cost for nothing. Again and again.&lt;/p&gt;
    &lt;p&gt;If you dig further you discover much bigger problem.&lt;/p&gt;
    &lt;code&gt;SELECT relname, reltuples::bigint as row_estimate, relpages as page_estimate
FROM pg_class 
WHERE relname IN ('demo', 'demo_pkey');
&lt;/code&gt;
    &lt;code&gt;relname  | row_estimate | page_estimate
-----------+--------------+---------------
demo      |        20000 |           934
demo_pkey |        20000 |           276
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;relpages&lt;/code&gt; value comes from the physical file size divided by the 8 KB page
size. PostgreSQL updates it during VACUUM and ANALYZE, but it reflects the
actual file on disk - not how much useful data is inside. Our index file is still
2.2 MB (276 pages √ó 8 KB), even though most pages are empty.&lt;/p&gt;
    &lt;p&gt;The planner sees 276 pages for 20,000 rows and calculates a very low rows-per-page ratio. This is when planner can come to conclusion - this index is very sparse - let's do a sequential scan instead. Oops.&lt;/p&gt;
    &lt;p&gt;"But wait," you say, "doesn't ANALYZE fix statistics?"&lt;/p&gt;
    &lt;p&gt;Yes and no. &lt;code&gt;ANALYZE&lt;/code&gt; updates the row count estimate. It will no longer think you
have 100,000 rows but 20,000. But it does not shrink relpages, because that
reflects the physical file size on disk. &lt;code&gt;ANALYZE&lt;/code&gt; can't change that.&lt;/p&gt;
    &lt;p&gt;The planner now has accurate row estimates but wildly inaccurate page estimates. The useful data is packed into just ~57 pages worth of entries, but the planner doesn't know that.&lt;/p&gt;
    &lt;code&gt;cost = random_page_cost √ó pages + cpu_index_tuple_cost √ó tuples
&lt;/code&gt;
    &lt;p&gt;With a bloated index:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pages is oversize (276 instead of ~57)&lt;/item&gt;
      &lt;item&gt;The per-page cost gets multiplied by empty pages&lt;/item&gt;
      &lt;item&gt;Total estimated cost is artificially high&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The hollow index&lt;/head&gt;
    &lt;p&gt;We can dig even more into the index problem when we look at internal stats:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM pgstatindex('demo_pkey');
&lt;/code&gt;
    &lt;code&gt;-[ RECORD 1 ]------+--------
version            | 4
tree_level         | 1
index_size         | 2260992
root_block_no      | 3
internal_pages     | 1
leaf_pages         | 57
empty_pages        | 0
deleted_pages      | 217
avg_leaf_density   | 86.37
leaf_fragmentation | 0
&lt;/code&gt;
    &lt;p&gt;Wait, what? The avg_leaf_density is 86% and it looks perfectly healthy. That's a trap. Due to the hollow index (we removed 80% right in the middle) we have 57 well-packed leaf pages, but the index still contains 217 deleted pages.&lt;/p&gt;
    &lt;p&gt;This is why &lt;code&gt;avg_leaf_density&lt;/code&gt; alone is misleading. The density of used pages
looks great, but 79% of your index file is dead weight.&lt;/p&gt;
    &lt;p&gt;The simplest way to spot index bloat is comparing actual size to expected size.&lt;/p&gt;
    &lt;code&gt;SELECT
    c.relname as index_name,
    pg_size_pretty(pg_relation_size(c.oid)) as actual_size,
    pg_size_pretty((c.reltuples * 40)::bigint) as expected_size,
    round((pg_relation_size(c.oid) / nullif(c.reltuples * 40, 0))::numeric, 1) as bloat_ratio
FROM pg_class c
JOIN pg_index i ON c.oid = i.indexrelid
WHERE c.relkind = 'i' 
  AND c.reltuples &amp;gt; 0
  AND c.relname NOT LIKE 'pg_%'
  AND pg_relation_size(c.oid) &amp;gt; 1024 * 1024  -- only indexes &amp;gt; 1 MB
ORDER BY bloat_ratio DESC NULLS LAST;
&lt;/code&gt;
    &lt;code&gt;index_name | actual_size | expected_size | bloat_ratio
------------+-------------+---------------+-------------
demo_pkey  | 2208 kB     | 781 kB        |         2.8
&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;bloat_ratio&lt;/code&gt; of 2.8 means the index is nearly 3x larger than expected. Anything
above 1.8 - 2.0 deserves investigation.&lt;/p&gt;
    &lt;p&gt;We filter to indexes over 1 MB - bloat on tiny indexes doesn't matter that much. Please, adjust the threshold based on your environment; for large databases, you might only care about indexes over 100 MB.&lt;/p&gt;
    &lt;p&gt;But here comes BIG WARNING: pgstatindex() we used earlier physically reads the entire index. On a 10 GB index, that's 10 GB of I/O. Don't run it against all indexes on a production server - unless you know what you are doing!&lt;/p&gt;
    &lt;head rend="h2"&gt;REINDEX&lt;/head&gt;
    &lt;p&gt;How to actually fix index bloat problem? &lt;code&gt;REINDEX&lt;/code&gt; is s straightforward solution as
it rebuilds the index from scratch.&lt;/p&gt;
    &lt;code&gt;REINDEX INDEX CONCURRENTLY demo_pkey ;
&lt;/code&gt;
    &lt;p&gt;After which we can check the index health:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM pgstatindex('demo_pkey');
&lt;/code&gt;
    &lt;code&gt;-[ RECORD 1 ]------+-------
version            | 4
tree_level         | 1
index_size         | 466944
root_block_no      | 3
internal_pages     | 1
leaf_pages         | 55
empty_pages        | 0
deleted_pages      | 0
avg_leaf_density   | 89.5
leaf_fragmentation | 0
&lt;/code&gt;
    &lt;p&gt;And&lt;/p&gt;
    &lt;code&gt;SELECT
    relname,
    pg_size_pretty(pg_relation_size(oid)) as file_size,
    pg_size_pretty((pgstattuple(oid)).tuple_len) as actual_data
FROM pg_class
WHERE relname IN ('demo', 'demo_pkey');
&lt;/code&gt;
    &lt;code&gt;relname  | file_size | actual_data
-----------+-----------+-------------
demo      | 7472 kB   | 1278 kB
demo_pkey | 456 kB    | 313 kB
&lt;/code&gt;
    &lt;p&gt;Our index shrunk from 2.2 MB to 456 KB - 79% reduction (not a big surprise though).&lt;/p&gt;
    &lt;p&gt;As you might have noticed we have used &lt;code&gt;CONCURRENTLY&lt;/code&gt; to avoid using ACCESS
EXCLUSIVE lock. This is available since PostgreSQL 12+, and while there's an
option to omit it - the pretty much only reason to do so is during planned
maintenance to speed up the index rebuild time.&lt;/p&gt;
    &lt;head rend="h2"&gt;pg_squeeze&lt;/head&gt;
    &lt;p&gt;If you look above at the file_size of our relations, we have managed to reclaim the disk space for the affected index (it was &lt;code&gt;REINDEX&lt;/code&gt; after all), but the table
space was not returned back to the operating system.&lt;/p&gt;
    &lt;p&gt;That's where pg_squeeze shines. Unlike trigger-based alternatives, pg_squeeze uses logical decoding, resulting in lower impact on your running system. It rebuilds both the table and all its indexes online, with minimal locking:&lt;/p&gt;
    &lt;code&gt;CREATE EXTENSION pg_squeeze;

SELECT squeeze.squeeze_table('public', 'demo');
&lt;/code&gt;
    &lt;p&gt;The exclusive lock is only needed during the final swap phase, and its duration can be configured. Even better, pg_squeeze is designed for regular automated processing - you can register tables and let it handle maintenance whenever bloat thresholds are met.&lt;/p&gt;
    &lt;p&gt;pg_squeeze makes sense when both table and indexes are bloated, or when you want automated management. REINDEX CONCURRENTLY is simpler when only indexes need work.&lt;/p&gt;
    &lt;p&gt;There's also older tool pg_repack - for a deeper comparison of bloat-busting tools, see article The Bloat Busters: pg_repack vs pg_squeeze.&lt;/p&gt;
    &lt;head rend="h2"&gt;VACUUM FULL (The nuclear option)&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;VACUUM FULL&lt;/code&gt; rewrites the entire table and all indexes. While it fixes
everything it comes with a big but - it requires an ACCESS EXCLUSIVE
lock - completely blocking all reads and writes for the entire duration. For a
large table, this could mean hours of downtime.&lt;/p&gt;
    &lt;p&gt;Generally avoid this in production. Use pg_squeeze instead for the same result without the downtime.&lt;/p&gt;
    &lt;head rend="h2"&gt;When to act, and when to chill&lt;/head&gt;
    &lt;p&gt;Before you now go and &lt;code&gt;REINDEX&lt;/code&gt; everything in sight, let's talk about when index
bloat actually matters.&lt;/p&gt;
    &lt;p&gt;B-trees expand and contract with your data. With random insertions affecting index columns - UUIDs, hash keys, etc. the page splits happen constantly. Index efficiency might get hit at occassion and also settle around 70 - 80% over different natural cycles of your system usage. That's not bloat. That's the tree finding its natural shape for your data.&lt;/p&gt;
    &lt;p&gt;The bloat we demonstrated - 57 useful pages drowning in 217 deleted ones - is extreme. It came from deleting 80% of contiguous data. You won't see this from normal day to day operations.&lt;/p&gt;
    &lt;p&gt;When do you need to act immediately:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;after a massive DELETE (retention policy, GDPR purge, failed migration cleanup)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bloat_ratio&lt;/code&gt;exceeds 2.0 and keeps climbing&lt;/item&gt;
      &lt;item&gt;query plans suddenly prefer sequential scans on indexed columns&lt;/item&gt;
      &lt;item&gt;index size is wildly disproportionate to row count&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But in most cases you don't have to panic. Monitor weekly and when indexes bloat ratio continously grow above warning levels, schedule a &lt;code&gt;REINDEX CONCURRENTLY&lt;/code&gt;
during low traffic period.&lt;/p&gt;
    &lt;p&gt;Index bloat isn't an emergency until it is. Know the signs, have the tools ready, and don't let VACUUM's silence fool you into thinking everything's fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;VACUUM is essential for PostgreSQL. Run it. Let autovacuum do its job. But understand its limitations: it cleans up dead tuples, not index structure.&lt;/p&gt;
    &lt;p&gt;The truth about PostgreSQL maintenance is that VACUUM handles heap bloat reasonably well, but index bloat requires explicit intervention. Know when your indexes are actually sick versus just breathing normally - and when to reach for REINDEX.&lt;/p&gt;
    &lt;p&gt;VACUUM handles heap bloat. Index bloat is your problem. Know the difference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262777</guid><pubDate>Sun, 14 Dec 2025 13:13:41 +0000</pubDate></item><item><title>AI and the ironies of automation ‚Äì Part 2</title><link>https://www.ufried.com/blog/ironies_of_ai_2/</link><description>&lt;doc fingerprint="b6d01b1d01739db2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI and the ironies of automation - Part 2&lt;/head&gt;
    &lt;p&gt;Some (well-known) consequences of AI automating work&lt;/p&gt;
    &lt;head rend="h1"&gt;AI and the ironies of automation - Part 2&lt;/head&gt;
    &lt;p&gt;In the previous post, we discussed several observations, Lisanne Bainbridge made in her much-noticed paper ‚ÄúThe ironies of automation‚Äù, she published in 1983 and what they mean for the current ‚Äúwhite-collar‚Äù work automation attempts leveraging LLMs and AI agents based on LLMs, still requiring humans in the loop. We stopped at the end of the first chapter, ‚ÄúIntroduction‚Äù, of the paper.&lt;/p&gt;
    &lt;p&gt;In this post, we will continue with the second chapter, ‚ÄúApproaches to solutions‚Äù, and see what we can learn there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing apples and oranges?&lt;/head&gt;
    &lt;p&gt;However, before we start: Some of the observations and recommendations made in the paper must be taken with a grain of salt when applying them to the AI-based automation attempts of today. When monitoring an industrial production plant, it is often a matter of seconds until a human operator must act if something goes wrong to avoid severe or even catastrophic accidents.&lt;/p&gt;
    &lt;p&gt;Therefore, it is of the highest importance to design industrial control stations in a way that a human operator can recognize deviations and malfunctions as easily as possible and immediately trigger countermeasures. A lot of work is put into the design of all the displays and controls, like, e.g., the well-known emergency stop switch in a screaming red color that is big enough to be punched with a flat hand, fist or alike within a fraction of a second if needed.&lt;/p&gt;
    &lt;p&gt;When it comes to AI-based solutions automating white-collar work, we usually do not face such critical conditions. However, this is not a reason to dismiss the observations and recommendations in the paper easily because, e.g.:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Most companies are efficiency-obsessed. Hence, they also expect AI solutions to increase ‚Äúproductivity‚Äù, i.e., efficiency, to a superhuman level. If a human is meant to monitor the output of the AI and intervene if needed, this requires that the human needs to comprehend what the AI solution produced at superhuman speed ‚Äì otherwise we are down to human speed. This presents a quandary that can only be solved if we enable the human to comprehend the AI output at superhuman speed (compared to producing the same output by traditional means).&lt;/item&gt;
      &lt;item&gt;Most companies have a tradition of nurturing a culture of urgency and scarcity, resulting in a lot of pressure towards and stress for the employees. Stress is known to trigger the fight-or-flight mode (an ancient survival mechanism built into us to cope with dangerous situations) which massively reduces the normal cognitive capacity of a human. While this mechanism supports humans in making very quick decisions and taking quick actions (essential in dangerous situations), it deprives them of the ability to conduct any deeper analysis (not being essential in dangerous situations). If deeper analysis is required to make a decision, this may take a lot longer than without stress ‚Äì if possible at all. This means we need to enable humans to conduct deeper analysis under stress as well or to provide the information in a way that eliminates the need for deeper analysis (which is not always possible).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we let this sink in (plus a few other aspects, I did not write down here but you most likely will add in your mind), we quickly come to the conclusion that also in our AI-related automation context humans are often expected to make quick decisions and act based on them, often under conditions that make it hard (if not impossible) to conduct any in-depth analysis.&lt;/p&gt;
    &lt;p&gt;If we then also take into account, that depending on the situation a wrong result produced by an AI solution which eluded the human operator may have severe consequences in the worst case (e.g., assume a major security incident due to a missed wrongdoing of the AI solution), the situation is not that far away anymore from the situation in an industrial plant‚Äôs control station.&lt;/p&gt;
    &lt;p&gt;Summarizing, we surely need to add the necessary grain of salt, i.e., ask ourselves how strict the timing constraints in our specific setting are to avoid comparing apples and oranges in the worst case. However, in general we need to consider the whole range of possible settings which will ‚Äì probably more often than we think ‚Äì include that humans need to make decisions in a very short time under stressful conditions (which makes things more precarious).&lt;/p&gt;
    &lt;head rend="h2"&gt;The worst UI possible&lt;/head&gt;
    &lt;p&gt;This brings us immediately to Lisanne Bainbridge‚Äôs first recommendation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In any situation where a low probability event must be noticed quickly then the operator must be given artificial assistance, if necessary even alarms on alarms.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, the system must support the human operator as well as possible in detecting a problem, especially if it tends to occur rarely. It is a consequence of the ‚Äúmonitoring fatigue‚Äù problem we discussed in the previous post.&lt;/p&gt;
    &lt;p&gt;Due to the learnings people have made, a lot of effort has been put into the design of the displays, the controls and also the alerting mechanisms of industrial production control stations, making sure the human operators can make their jobs as good, as stress-free and as reliable as possible.&lt;/p&gt;
    &lt;p&gt;Enter AI agents.&lt;/p&gt;
    &lt;p&gt;The usual idea is that a single human controls a fleet of AI agents that are designed to do some kind of job, e.g., writing code. Sometimes, most agents are generic ‚Äúworkers‚Äù, orchestrated by some kind of supervisor that delegates parts of the work to the worker agents. Sometimes, the different agents are ‚Äúspecialists‚Äù, each for a certain aspect of the job to be done, that collaborate using some kind of choreography (or are also orchestrated by a supervisor). While the generic workers are easier to set up, the specialized workers usually produce more accurate results.&lt;/p&gt;
    &lt;p&gt;Because these AI-based agents sometimes produce errors, a human ‚Äì in our example a software developer ‚Äì needs to supervise the AI agent fleet and ideally intervenes before the AI agents do something they should not do. Therefore, the AI agents typically create a plan of what they intend to do first (which as a side effect also increases the likelihood that they do not drift off). Then, the human verifies the plan and approves it if it is correct, and the AI agents execute the plan. If the plan is not correct, the human rejects it and sends the agents back to replanning, providing information about what needs to be altered.&lt;/p&gt;
    &lt;p&gt;Let us take Lisanne Bainbridge‚Äôs recommendation and compare it to this approach that is currently ‚Äúbest practice‚Äù to control an AI agent fleet.&lt;/p&gt;
    &lt;p&gt;Unless we tell them to act differently, LLMs and also AI agents based on them are quite chatty. Additionally, they tend to communicate with an air of utter conviction. Thus, they present to you this highly detailed, multi-step plan of what they intend to do, including lots of explanations, in this perfectly convinced tone. Often, these plans are more than 50 or 100 lines of text, sometimes even several hundred lines.&lt;/p&gt;
    &lt;p&gt;Most of the time, the plans are fine. However, sometimes the AI agents mess things up. They make wrong conclusions, or they forget what they are told to do and drift off ‚Äì not very often, but it happens. Sometimes the problem is obvious at first sight. But more often, it is neatly hidden somewhere behind line 123: ‚Äú‚Ä¶ and because 2 is bigger than 3, it is clear, we need to &amp;lt; do something critical &amp;gt;‚Äù. But because it is so much text the agents flood you with all the time and because the error is hidden so well behind this wall of conviction, we miss it ‚Äì and the AI agent does something critical wrong.&lt;/p&gt;
    &lt;p&gt;We cannot blame the person for missing the error in the plan. The problem is that this is probably the worst UI and UX possible for anyone who is responsible for avoiding errors in a system that rarely produces errors.&lt;/p&gt;
    &lt;p&gt;But LLM-based agents make errors all the time, you may say. Well, not all the time. Sometimes they do. And the better the instructions and the setup of the interacting agents, the fewer errors they produce. Additionally, we can expect more specialized and refined agents in the future that become increasingly better in their respective areas of expertise. Still, most likely they will never become completely error-free because of the underlying technology that cannot guarantee consistent correctness.&lt;/p&gt;
    &lt;p&gt;This is the setting we need to ponder if we talk about the user interface for a human observer: a setting where the agent fleet only rarely makes errors but we still need a human monitoring and intervening if things should go wrong. It is not yet clear how such an interface should look like, but most definitely not as it looks now. Probably we could harvest some good insights from our UX/UI design colleagues for industrial production plant control stations. We would need only to ask them ‚Ä¶&lt;/p&gt;
    &lt;head rend="h2"&gt;The training paradox&lt;/head&gt;
    &lt;p&gt;Lisanne Bainbridge then makes several recommendations regarding the required training of the human operator. This again is a rich section, and I can only recommend reading it on your own because it contains several subtle yet important hints that are hard to bring across without citing the whole chapter. Here, I will highlight only a few aspects. She starts with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[Some points made in the previous section] make it clear that it can be important to maintain manual skills.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Then she talks about letting the human operator take over control regularly, i.e., do the job instead of the machine as a very effective training option. Actually, without doing hands-on work regularly, the skills of a human expert deteriorate surprisingly fast.&lt;/p&gt;
    &lt;p&gt;But if taking over the work regularly is not an option, e.g., because we want continuous superhuman productivity leveraging AI agents (no matter if it makes sense or not), we still need to make sure that the human operator can take over if needed. In such a setting, training must take place in some other way, usually using some kind of simulator.&lt;/p&gt;
    &lt;p&gt;However, there is a problem with simulators, especially if human intervention is only needed (and wanted) if things do not work as expected:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are problems with the use of any simulator to train for extreme situations. Unknown faults cannot be simulated, and system behaviour may not be known for faults which can be predicted but have not been experienced.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The consequence of this issue is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This means that training must be concerned with general strategies rather than specific responses [‚Ä¶]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is inadequate to expect the operator to react to unfamiliar events solely by consulting operating procedures. These cannot cover all the possibilities, so the operator is expected to monitor them and fill in the gaps.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Which leaves us with the irony:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;However, it is ironic to train operators in following instructions and then put them in the system to provide intelligence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is a problem we will need to face with AI agents and their supervising humans in the future, too. The supervising experts are meant to intervene whenever things become messy, whenever the AI agents get stuck, often in unforeseen ways. These are not regular tasks. Often, these are also not the issues we expect an AI agent to run into and thus can provide training for. These are extraordinary situations, the ones we do not expect ‚Äì and the more refined and specialized the AI agents will become in the future, the more often the issues that require human intervention will be of this kind.&lt;/p&gt;
    &lt;p&gt;The question is twofold:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;How can we train human operators at all to be able to intervene skillfully in exceptional, usually hard to solve situations?&lt;/item&gt;
      &lt;item&gt;How can we train a human operator so that their skills remain sharp over time and they remain able to address an exceptional situation quickly and resourcefully?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The questions seem to hint at a sort of paradox, and an answer to both questions is all but obvious. At the moment, we still have enough experienced subject matter experts that the questions may feel of lower importance. But if we only start to address the questions when they become pressing, they will be even harder ‚Äì if not impossible ‚Äì to solve.&lt;/p&gt;
    &lt;p&gt;To end this consideration with the words of Lisanne Bainbridge:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Perhaps the final irony is that it is the most successful automated systems, with rare need for manual intervention, which may need the greatest investment in human operator training.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, we cannot simply take a few available human experts and make them supervise agents that took over their work without any further investments in the humans. Instead, we need to train them continuously, and the better the agents become, the more expensive the training of the supervisors will become. I highly doubt that decision makers who primarily think about saving money when it comes to AI agents are aware of this irony.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interlude&lt;/head&gt;
    &lt;p&gt;As I wrote in the beginning of first part of this blog series, ‚ÄúThe ironies of automation‚Äù is a very rich and dense paper. We are still only at the end of the second chapter ‚ÄúApproaches to solutions‚Äù which is two and a half pages into the paper and there is still a whole third chapter called ‚ÄúHuman-computer collaboration‚Äù which takes up another page until we get to the conclusion.&lt;/p&gt;
    &lt;p&gt;While this third chapter also contains a lot of valuable advice that goes well beyond our focus here, I will leave it to you to read it on your own. As I indicated at the beginning, this paper is more than worth the time spent on it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The leadership dilemma&lt;/head&gt;
    &lt;p&gt;However, before finishing this little blog series, I would like to mention a new kind of dilemma that Lisanne Bainbridge did not discuss in her paper because the situation was a bit different with industrial production plant automation than with AI-agent-based automation. But as this topic fits nicely into the just-finished training paradox section, I decided to add it here.&lt;/p&gt;
    &lt;p&gt;The issue is that just monitoring an AI agent fleet doing its work and intervening if things go wrong usually is not sufficient, at least not yet. All the things discussed before apply, but there is more to interacting with AI agents because we cannot simply be reactive with AI agents. We cannot simply watch them doing their work and only intervene if things go wrong. Instead, we additionally need to be proactive with them: We need to direct them.&lt;/p&gt;
    &lt;p&gt;We need to tell the AI agents what to do, what not to do, which chunks to pick and so on. This is basically a leadership role. While you do not lead humans, the kind of work is quite similar: You are responsible for the result; you are allowed to set directions and constraints, but you do not immediately control the work. You only control it through communicating with the agents and trying to direct them in the right direction with orders, with feedback, with changed orders, with setting different constraints, etcetera.&lt;/p&gt;
    &lt;p&gt;This is a skill set most people do not have naturally. Usually, they need to develop it over time. Typically, before people are put in a leadership role directing humans, they will get a lot of leadership training teaching them the skills and tools needed to lead successfully. For most people, this is essential because if they come from the receiving end of orders (in the most general sense of ‚Äúorders‚Äù), typically they are not used to setting direction and constraints. This tends to be a completely new skill they need to learn.&lt;/p&gt;
    &lt;p&gt;This does not apply only to leading humans but also to leading AI agents. While AI agents are not humans, and thus leadership will be different in detail, the basic skills and tools needed are the same. This is, BTW, one of the reasons why the people who praise agentic AI on LinkedIn and the like are very often managers who lead (human) teams. For them, leading an AI agent fleet feels very natural because it is very close to the work they do every day. However, for the people currently doing the work, leading an AI agent fleet usually does not feel natural at all.&lt;/p&gt;
    &lt;p&gt;However, I have not yet seen anyone receiving any kind of leadership training before being left alone with a fleet of AI agents, and I still see little discussion about the issue. ‚ÄúIf it does not work properly, you need better prompts‚Äù is the usual response if someone struggles with directing agents successfully.&lt;/p&gt;
    &lt;p&gt;Sorry, but it is not that easy. The issue is much bigger than just optimizing a few prompts. The issue is that people have to change their approach completely to get any piece of work done. Instead of doing it directly, they need to learn how to get it done indirectly. They need to learn how to direct a group of AI agents effectively, how to lead them.&lt;/p&gt;
    &lt;p&gt;This also adds to the training irony of the previous topic. Maybe the AI agent fleets will become good enough in the future that we can omit the proactive part of the work and only need to focus on the reactive part of the work, the monitor-and-intervene part. But until then, we need to teach human supervisors of AI agent fleets how to lead them effectively.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving on&lt;/head&gt;
    &lt;p&gt;We discussed several ironies and paradoxes from Lisanne Bainbridge‚Äôs ‚ÄúThe ironies of automation‚Äù and how they also apply to agentic AI. We looked at the unlearning and recall dilemma and what it means for the next generation of human supervisors. We discussed monitoring fatigue and the status issue. We looked at the UX and UI deficiencies of current AI agents and the training paradox. And we finally looked at the leadership dilemma, which Lisanne Bainbridge did not discuss in her paper but which complements the training paradox.&lt;/p&gt;
    &lt;p&gt;I would like to conclude with the conclusion of Lisanne Bainbridge:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[‚Ä¶] humans working without time-pressure can be impressive problem solvers. The difficulty remains that they are less effective when under time pressure. I hope this paper has made clear both the irony that one is not by automating necessarily removing the difficulties, and also the possibility that resolving them will require even greater technological ingenuity than does classic automation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I could not agree more.&lt;/p&gt;
    &lt;p&gt;I think over time we will become clear on how much ‚ÄúThe ironies of automation‚Äù also applies to automation done with AI agents and that we cannot ignore the insights known for more than 40 years meanwhile. I am also really curious how the solutions to the ironies and paradoxes will look like.&lt;/p&gt;
    &lt;p&gt;Until then, I hope I gave you a bit of food for thought to ponder. If you should have some good ideas regarding the ironies and how to address them, please do not hesitate to share them with the community. We learn best by sharing and discussing, and maybe your contribution will be a step towards solving the issues discussed ‚Ä¶&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262816</guid><pubDate>Sun, 14 Dec 2025 13:19:15 +0000</pubDate></item><item><title>Apple Maps claims it's 29,905 miles away</title><link>https://mathstodon.xyz/@dpiponi/115651419771418748</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262950</guid><pubDate>Sun, 14 Dec 2025 13:45:41 +0000</pubDate></item><item><title>Illuminating the processor core with LLVM-mca</title><link>https://abseil.io/fast/99</link><description>&lt;doc fingerprint="e30b0622655c5f48"&gt;
  &lt;main&gt;&lt;p&gt;Originally posted as Fast TotW #99 on September 29, 2025&lt;/p&gt;&lt;p&gt;Updated 2025-10-07&lt;/p&gt;&lt;p&gt;Quicklink: abseil.io/fast/99&lt;/p&gt;&lt;p&gt;The RISC versus CISC debate ended in a draw: Modern processors decompose instructions into micro-ops handled by backend execution units. Understanding how instructions are executed by these units can give us insights on optimizing key functions that are backend bound. In this episode, we walk through using &lt;code&gt;llvm-mca&lt;/code&gt; to analyze
functions and identify performance insights from its simulation.&lt;/p&gt;&lt;p&gt;&lt;code&gt;llvm-mca&lt;/code&gt;, short for Machine Code Analyzer, is a tool within LLVM. It uses the
same datasets that the compiler uses for making instruction scheduling
decisions. This ensures that improvements made to compiler optimizations
automatically flow towards keeping &lt;code&gt;llvm-mca&lt;/code&gt; representative. The flip side is
that the tool is only as good as LLVM‚Äôs internal modeling of processor designs,
so certain quirks of individual microarchitecture generations might be omitted.
It also models the processor behavior statically, so cache
misses, branch mispredictions, and other dynamic properties aren‚Äôt considered.&lt;/p&gt;&lt;p&gt;Consider Protobuf‚Äôs &lt;code&gt;VarintSize64&lt;/code&gt; method:&lt;/p&gt;&lt;quote&gt;size_t CodedOutputStream::VarintSize64(uint64_t value) { #if PROTOBUF_CODED_STREAM_H_PREFER_BSR // Explicit OR 0x1 to avoid calling absl::countl_zero(0), which // requires a branch to check for on platforms without a clz instruction. uint32_t log2value = (std::numeric_limits&amp;lt;uint64_t&amp;gt;::digits - 1) - absl::countl_zero(value | 0x1); return static_cast&amp;lt;size_t&amp;gt;((log2value * 9 + (64 + 9)) / 64); #else uint32_t clz = absl::countl_zero(value); return static_cast&amp;lt;size_t&amp;gt;( ((std::numeric_limits&amp;lt;uint64_t&amp;gt;::digits * 9 + 64) - (clz * 9)) / 64); #endif }&lt;/quote&gt;&lt;p&gt;This function calculates how many bytes an encoded integer will consume in Protobuf‚Äôs wire format. It first computes the number of bits needed to represent the value by finding the log2 size of the input, then approximates division by 7. The size of the input can be calculated using the &lt;code&gt;absl::countl_zero&lt;/code&gt; function. However this has two possible
implementations depending on whether the processor has a &lt;code&gt;lzcnt&lt;/code&gt; (Leading Zero
Count) instruction available or if this operation needs to instead leverage the
&lt;code&gt;bsr&lt;/code&gt; (Bit Scan Reverse) instruction.&lt;/p&gt;&lt;p&gt;Under the hood of &lt;code&gt;absl::countl_zero&lt;/code&gt;, we need to check whether the argument is
zero, since &lt;code&gt;__builtin_clz&lt;/code&gt; (Count Leading Zeros) models the behavior of x86‚Äôs
&lt;code&gt;bsr&lt;/code&gt; (Bit Scan Reverse) instruction and has unspecified behavior if the input
is 0. The &lt;code&gt;| 0x1&lt;/code&gt; avoids needing a branch by ensuring the argument is non-zero
in a way the compiler can follow.&lt;/p&gt;&lt;p&gt;When we have &lt;code&gt;lzcnt&lt;/code&gt; available, the compiler optimizes &lt;code&gt;x == 0 ? 32 :
__builtin_clz(x)&lt;/code&gt; in &lt;code&gt;absl::countl_zero&lt;/code&gt; to &lt;code&gt;lzcnt&lt;/code&gt; without branches. This makes
the &lt;code&gt;| 0x1&lt;/code&gt; unnecessary.&lt;/p&gt;&lt;p&gt;Compiling this gives us two different assembly sequences depending on whether the &lt;code&gt;lzcnt&lt;/code&gt; instruction is available or not:&lt;/p&gt;&lt;p&gt;&lt;code&gt;bsr&lt;/code&gt; (&lt;code&gt;-march=ivybridge&lt;/code&gt;):&lt;/p&gt;&lt;quote&gt;orq $1, %rdi bsrq %rdi, %rax leal (%rax,%rax,8), %eax addl $73, %eax shrl $6, %eax&lt;/quote&gt;&lt;p&gt;&lt;code&gt;lzcnt&lt;/code&gt; (&lt;code&gt;-march=haswell&lt;/code&gt;):&lt;/p&gt;&lt;quote&gt;lzcntq %rdi, %rax leal (%rax,%rax,8), %ecx movl $640, %eax subl %ecx, %eax shrl $6, %eax&lt;/quote&gt;&lt;p&gt;We can now use Compiler Explorer to run these sequences through &lt;code&gt;llvm-mca&lt;/code&gt; and get an analysis of how they would execute on a
simulated Skylake processor (&lt;code&gt;-mcpu=skylake&lt;/code&gt;) for a single invocation
(&lt;code&gt;-iterations=1&lt;/code&gt;) and include &lt;code&gt;-timeline&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;&lt;code&gt;bsr&lt;/code&gt; (&lt;code&gt;-march=ivybridge&lt;/code&gt;):&lt;/p&gt;&lt;quote&gt;Iterations: 1 Instructions: 5 Total Cycles: 10 Total uOps: 5 Dispatch Width: 6 uOps Per Cycle: 0.50 IPC: 0.50 Block RThroughput: 1.0 Timeline view: Index 0123456789 [0,0] DeER . . orq $1, %rdi [0,1] D=eeeER . bsrq %rdi, %rax [0,2] D====eER . leal (%rax,%rax,8), %eax [0,3] D=====eER. addl $73, %eax [0,4] D======eER shrl $6, %eax&lt;/quote&gt;&lt;p&gt;&lt;code&gt;lzcnt&lt;/code&gt; (&lt;code&gt;-march=haswell&lt;/code&gt;):&lt;/p&gt;&lt;quote&gt;Iterations: 1 Instructions: 5 Total Cycles: 9 Total uOps: 5 Dispatch Width: 6 uOps Per Cycle: 0.56 IPC: 0.56 Block RThroughput: 1.0 Timeline view: Index 012345678 [0,0] DeeeER . lzcntq %rdi, %rax [0,1] D===eER . leal (%rax,%rax,8), %ecx [0,2] DeE---R . movl $640, %eax [0,3] D====eER. subl %ecx, %eax [0,4] D=====eER shrl $6, %eax&lt;/quote&gt;&lt;p&gt;This can also be obtained via the command line&lt;/p&gt;&lt;code&gt;$ clang file.cpp -O3 --target=x86_64 -S -o - | llvm-mca -mcpu=skylake -iterations=1 -timeline
&lt;/code&gt;&lt;p&gt;There‚Äôs two sections to this output, the first section provides some summary statistics for the code, the second section covers the execution ‚Äútimeline.‚Äù The timeline provides interesting detail about how instructions flow through the execution pipelines in the processor. There are three columns, and each instruction is shown on a separate row. The three columns are as follows:&lt;/p&gt;&lt;p&gt;The timeline is counted in cycles. Each instruction goes through several steps:&lt;/p&gt;&lt;code&gt;D&lt;/code&gt; the instruction is dispatched by the processor; modern desktop or server
processors can dispatch many instructions per cycle. Little Arm cores like
the Cortex-A55 used in smartphones are more limited.&lt;code&gt;=&lt;/code&gt; the instruction is waiting to execute. In this case, the instructions
are waiting for the results of prior instructions to be available. In other
cases, there might be a bottleneck in the processor‚Äôs backend.&lt;code&gt;e&lt;/code&gt; the instruction is executing.&lt;code&gt;E&lt;/code&gt; the instruction‚Äôs output is available.&lt;code&gt;-&lt;/code&gt; the instruction has completed execution and is waiting to be retired.
Instructions generally retire in program order, the order instructions
appear in the program. An instruction will wait to retire until prior ones
have also retired. On some architectures like the Cortex-A55, there is no
&lt;code&gt;R&lt;/code&gt; phase in the timeline as some instructions retire
out-of-order.&lt;code&gt;R&lt;/code&gt; the instruction has been retired, and is no longer occupying execution
resources.&lt;p&gt;The output is lengthy, but we can extract a few high-level insights from it:&lt;/p&gt;&lt;code&gt;lzcnt&lt;/code&gt; implementation is quicker to execute (9 cycles) than the ‚Äúbsr‚Äù
implementation (10 cycles). This is seen under the &lt;code&gt;Total Cycles&lt;/code&gt; summary as
well as the timeline.&lt;code&gt;movl&lt;/code&gt;, the instructions depend
on each other sequentially (&lt;code&gt;E&lt;/code&gt;-finishing to &lt;code&gt;e&lt;/code&gt;-starting vertically
aligning, pairwise, in the timeline view).&lt;code&gt;or&lt;/code&gt; of &lt;code&gt;0x1&lt;/code&gt; delays &lt;code&gt;bsrq&lt;/code&gt;‚Äôs input being available by 1 cycle,
contributing to the longer execution cost.&lt;code&gt;movl&lt;/code&gt; starts immediately in the &lt;code&gt;lzcnt&lt;/code&gt; implementation,
it can‚Äôt retire until prior instructions are retired, since we retire in
program order.&lt;code&gt;lzcnt&lt;/code&gt; implementation has
higher instruction-level parallelism
(ILP) because
the &lt;code&gt;mov&lt;/code&gt; has no dependencies. This demonstrates that counting instructions
need not tell us the cycle cost.&lt;p&gt;&lt;code&gt;llvm-mca&lt;/code&gt; is flexible and can model other processors:&lt;/p&gt;&lt;code&gt;D&lt;/code&gt; phase of
instructions starting later.&lt;p&gt;When designing microbenchmarks, we sometimes want to distinguish between throughput and latency microbenchmarks. If the input of one benchmark iteration does not depend on the prior iteration, the processor can execute multiple iterations in parallel. Generally for code that is expected to execute in a loop we care more about throughput, and for code that is inlined in many places interspersed with other logic we care more about latency.&lt;/p&gt;&lt;p&gt;We can use &lt;code&gt;llvm-mca&lt;/code&gt; to model execution of the block of code in a tight loop.
By specifying &lt;code&gt;-iterations=100&lt;/code&gt; on the &lt;code&gt;lzcnt&lt;/code&gt; version, we get a very different
set of results because one iteration‚Äôs execution can overlap with the next:&lt;/p&gt;&lt;quote&gt;Iterations: 100 Instructions: 500 Total Cycles: 134 Total uOps: 500 Dispatch Width: 6 uOps Per Cycle: 3.73 IPC: 3.73 Block RThroughput: 1.0&lt;/quote&gt;&lt;p&gt;We were able to execute 100 iterations in only 134 cycles (1.34 cycles/element) by achieving high ILP.&lt;/p&gt;&lt;p&gt;Achieving the best performance may sometimes entail trading off the latency of a basic block in favor of higher throughput. Inside of the protobuf implementation of &lt;code&gt;VarintSize&lt;/code&gt;
(protobuf/wire_format_lite.cc),
we use a vectorized version for realizing higher throughput albeit with worse
latency. A single iteration of the loop takes 29 cycles to process 32 elements
(Compiler Explorer) for 0.91 cycles/element,
but 100 iterations (3200 elements) only requires 1217 cycles (0.38
cycles/element - about 3x faster) showcasing the high throughput once setup
costs are amortized.&lt;/p&gt;&lt;p&gt;When we are looking at CPU profiles, we are often tracking when instructions retire. Costs are attributed to instructions that took longer to retire. Suppose we profile a small function that accesses memory pseudo-randomly:&lt;/p&gt;&lt;quote&gt;unsigned Chains(unsigned* x) { unsigned a0 = x[0]; unsigned b0 = x[1]; unsigned a1 = x[a0]; unsigned b1 = x[b0]; unsigned b2 = x[b1]; return a1 | b2; }&lt;/quote&gt;&lt;p&gt;&lt;code&gt;llvm-mca&lt;/code&gt; models memory loads being an L1 hit (Compiler
Explorer): It takes 5 cycles for the value of
a load to be available after the load starts execution. The output has been
annotated with the source code to make it easier to read.&lt;/p&gt;&lt;quote&gt;Iterations: 1 Instructions: 6 Total Cycles: 19 Total uOps: 9 Dispatch Width: 6 uOps Per Cycle: 0.47 IPC: 0.32 Block RThroughput: 3.0 Timeline view: 012345678 Index 0123456789 [0,0] DeeeeeER . . . movl (%rdi), %ecx // ecx = a0 = x[0] [0,1] DeeeeeER . . . movl 4(%rdi), %eax // eax = b0 = x[1] [0,2] D=====eeeeeER . . movl (%rdi,%rax,4), %eax // eax = b1 = x[b0] [0,3] D==========eeeeeER. movl (%rdi,%rax,4), %eax // eax = b2 = x[b1] [0,4] D==========eeeeeeER orl (%rdi,%rcx,4), %eax // eax |= a1 = x[a0] [0,5] .DeeeeeeeE--------R retq&lt;/quote&gt;&lt;p&gt;In this timeline the first two instructions load &lt;code&gt;a0&lt;/code&gt; and &lt;code&gt;b0&lt;/code&gt;. Both of these
operations can happen immediately. However, the load of &lt;code&gt;x[b0]&lt;/code&gt; can only happen
once the value for &lt;code&gt;b0&lt;/code&gt; is available in a register - after a 5 cycle delay. The
load of &lt;code&gt;x[b1]&lt;/code&gt; can only happen once the value for &lt;code&gt;b1&lt;/code&gt; is available after
another 5 cycle delay.&lt;/p&gt;&lt;p&gt;This program has two places where we can execute loads in parallel: the pair &lt;code&gt;a0&lt;/code&gt; and &lt;code&gt;b0&lt;/code&gt; and the pair &lt;code&gt;a1 and b1&lt;/code&gt; (note: &lt;code&gt;llvm-mca&lt;/code&gt; does not correctly
model the memory load uop from &lt;code&gt;orl&lt;/code&gt; for &lt;code&gt;a1&lt;/code&gt; starting). Since the processor
retires instructions in program order we expect the profile weight to appear on
the loads for &lt;code&gt;a0&lt;/code&gt;, &lt;code&gt;b1&lt;/code&gt;, and &lt;code&gt;b2&lt;/code&gt;, even though we had parallel loads in-flight
simultaneously.&lt;/p&gt;&lt;p&gt;If we examine this profile, we might try to optimize one of the memory indirections because it appears in our profile. We might do this by miraculously replacing &lt;code&gt;a0&lt;/code&gt; with a constant (Compiler
Explorer).&lt;/p&gt;&lt;quote&gt;unsigned Chains(unsigned* x) { unsigned a0 = 0; unsigned b0 = x[1]; unsigned a1 = x[a0]; unsigned b1 = x[b0]; unsigned b2 = x[b1]; return a1 | b2; }&lt;/quote&gt;&lt;quote&gt;Iterations: 1 Instructions: 5 Total Cycles: 19 Total uOps: 8 Dispatch Width: 6 uOps Per Cycle: 0.42 IPC: 0.26 Block RThroughput: 2.5 Timeline view: 012345678 Index 0123456789 [0,0] DeeeeeER . . . movl 4(%rdi), %eax [0,1] D=====eeeeeER . . movl (%rdi,%rax,4), %eax [0,2] D==========eeeeeER. movl (%rdi,%rax,4), %eax [0,3] D==========eeeeeeER orl (%rdi), %eax [0,4] .DeeeeeeeE--------R retq&lt;/quote&gt;&lt;p&gt;Even though we got rid of the ‚Äúexpensive‚Äù load we saw in the CPU profile, we didn‚Äôt actually change the overall length of the critical path that was dominated by the 3 load long ‚Äúb‚Äù chain. The timeline view shows the critical path for the function, and performance can only be improved if the duration of the critical path is reduced.&lt;/p&gt;&lt;p&gt;CRC32C is a common hashing function and modern architectures include dedicated instructions for calculating it. On short sizes, we‚Äôre largely dealing with handling odd numbers of bytes. For large sizes, we are constrained by repeatedly invoking &lt;code&gt;crc32q&lt;/code&gt; (x86) or similar every few bytes of the input. By examining
the repeated invocation, we can look at how the processor will execute it
(Compiler Explorer):&lt;/p&gt;&lt;quote&gt;uint32_t BlockHash() { asm volatile("# LLVM-MCA-BEGIN"); uint32_t crc = 0; for (int i = 0; i &amp;lt; 16; ++i) { crc = _mm_crc32_u64(crc, i); } asm volatile("# LLVM-MCA-END" : "+r"(crc)); return crc; }&lt;/quote&gt;&lt;p&gt;This function doesn‚Äôt hash anything useful, but it allows us to see the back-to-back usage of one &lt;code&gt;crc32q&lt;/code&gt;‚Äôs output with the next &lt;code&gt;crc32q&lt;/code&gt;‚Äôs inputs.&lt;/p&gt;&lt;quote&gt;Iterations: 1 Instructions: 32 Total Cycles: 51 Total uOps: 32 Dispatch Width: 6 uOps Per Cycle: 0.63 IPC: 0.63 Block RThroughput: 16.0 Instruction Info: [1]: #uOps [2]: Latency [3]: RThroughput [4]: MayLoad [5]: MayStore [6]: HasSideEffects (U) [1] [2] [3] [4] [5] [6] Instructions: 1 0 0.17 xorl %eax, %eax 1 3 1.00 crc32q %rax, %rax 1 1 0.25 movl $1, %ecx 1 3 1.00 crc32q %rcx, %rax ... Resources: [0] - SKLDivider [1] - SKLFPDivider [2] - SKLPort0 [3] - SKLPort1 [4] - SKLPort2 [5] - SKLPort3 [6] - SKLPort4 [7] - SKLPort5 [8] - SKLPort6 [9] - SKLPort7 Resource pressure per iteration: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] - - 4.00 18.00 - 1.00 - 5.00 6.00 - Resource pressure by instruction: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] Instructions: - - - - - - - - - - xorl %eax, %eax - - - 1.00 - - - - - - crc32q %rax, %rax - - - - - - - - 1.00 - movl $1, %ecx - - - 1.00 - - - - - - crc32q %rcx, %rax - - - - - - - 1.00 - - movl $2, %ecx - - - 1.00 - - - - - - crc32q %rcx, %rax ... - - - - - - - - 1.00 - movl $15, %ecx - - - 1.00 - - - - - - crc32q %rcx, %rax - - - - - 1.00 - 1.00 1.00 - retq Timeline view: 0123456789 0123456789 0 Index 0123456789 0123456789 0123456789 [0,0] DR . . . . . . . . . . xorl %eax, %eax [0,1] DeeeER . . . . . . . . . crc32q %rax, %rax [0,2] DeE--R . . . . . . . . . movl $1, %ecx [0,3] D===eeeER . . . . . . . . . crc32q %rcx, %rax [0,4] DeE-----R . . . . . . . . . movl $2, %ecx [0,5] D======eeeER . . . . . . . . crc32q %rcx, %rax ... [0,30] . DeE---------------------------------------R . movl $15, %ecx [0,31] . D========================================eeeER crc32q %rcx, %rax&lt;/quote&gt;&lt;p&gt;Based on the ‚Äú&lt;code&gt;Instruction Info&lt;/code&gt;‚Äù table, &lt;code&gt;crc32q&lt;/code&gt; has latency 3 and throughput
1: Every clock cycle, we can start processing a new invocation on port 1 (&lt;code&gt;[3]&lt;/code&gt;
in the table), but it takes 3 cycles for the result to be available.&lt;/p&gt;&lt;p&gt;Instructions decompose into individual micro operations (or ‚Äúuops‚Äù). The resources section lists the processor execution pipelines (often referred to as ports). Every cycle uops can be issued to these ports. There are constraints - no port can take every kind of uop and there is a maximum number of uops that can be dispatched to the processor pipelines every cycle.&lt;/p&gt;&lt;p&gt;For the instructions in our function, there is a one-to-one correspondence so the number of instructions and the number of uops executed are equivalent (32). The processor has several backends for processing uops. From the resource pressure tables, we see that while &lt;code&gt;crc32&lt;/code&gt; must execute on port 1, the &lt;code&gt;movl&lt;/code&gt;
executes on any of ports 0, 1, 5, and 6.&lt;/p&gt;&lt;p&gt;In the timeline view, we see that for our back-to-back sequence, we can‚Äôt actually begin processing the 2nd &lt;code&gt;crc32q&lt;/code&gt; for several clock cycles until the
1st &lt;code&gt;crc32q&lt;/code&gt; hasn‚Äôt completed. This tells us that we‚Äôre underutilizing port 1‚Äôs
capabilities, since its throughput indicates that an instruction can be
dispatched to it once per cycle.&lt;/p&gt;&lt;p&gt;If we restructure &lt;code&gt;BlockHash&lt;/code&gt; to compute 3 parallel streams with a simulated
combine function (the code uses a bitwise or as a placeholder for the correct
logic that this approach requires), we can accomplish the same amount of work in
fewer clock cycles (Compiler Explorer):&lt;/p&gt;&lt;quote&gt;uint32_t ParallelBlockHash(const char* p) { uint32_t crc0 = 0, crc1 = 0, crc2 = 0; for (int i = 0; i &amp;lt; 5; ++i) { crc0 = _mm_crc32_u64(crc0, 3 * i + 0); crc1 = _mm_crc32_u64(crc1, 3 * i + 1); crc2 = _mm_crc32_u64(crc2, 3 * i + 2); } crc0 = _mm_crc32_u64(crc0, 15); return crc0 | crc1 | crc2; }&lt;/quote&gt;&lt;quote&gt;Iterations: 1 Instructions: 36 Total Cycles: 22 Total uOps: 36 Dispatch Width: 6 uOps Per Cycle: 1.64 IPC: 1.64 Block RThroughput: 16.0 Timeline view: 0123456789 Index 0123456789 01 [0,0] DR . . . .. xorl %eax, %eax [0,1] DR . . . .. xorl %ecx, %ecx [0,2] DeeeER . . .. crc32q %rcx, %rcx [0,3] DeE--R . . .. movl $1, %esi [0,4] D----R . . .. xorl %edx, %edx [0,5] D=eeeER . . .. crc32q %rsi, %rdx [0,6] .DeE--R . . .. movl $2, %esi [0,7] .D=eeeER . . .. crc32q %rsi, %rax [0,8] .DeE---R . . .. movl $3, %esi [0,9] .D==eeeER . . .. crc32q %rsi, %rcx [0,10] .DeE----R . . .. movl $4, %esi [0,11] .D===eeeER. . .. crc32q %rsi, %rdx ... [0,32] . DeE-----------R.. movl $15, %esi [0,33] . D==========eeeER. crc32q %rsi, %rcx [0,34] . D============eER. orl %edx, %eax [0,35] . D=============eER orl %ecx, %eax&lt;/quote&gt;&lt;p&gt;The implementation invokes &lt;code&gt;crc32q&lt;/code&gt; the same number of times, but the end-to-end
latency of the block is 22 cycles instead of 51 cycles The timeline view shows
that the processor can issue a &lt;code&gt;crc32&lt;/code&gt; instruction every cycle.&lt;/p&gt;&lt;p&gt;This modeling can be evidenced by microbenchmark results for &lt;code&gt;absl::ComputeCrc32c&lt;/code&gt;
(absl/crc/crc32c_benchmark.cc).
The real implementation uses multiple streams (and correctly combines them).
Ablating these shows a regression, validating the value of the technique.&lt;/p&gt;&lt;quote&gt;name CYCLES/op CYCLES/op vs base BM_Calculate/0 5.007 ¬± 0% 5.008 ¬± 0% ~ (p=0.149 n=6) BM_Calculate/1 6.669 ¬± 1% 8.012 ¬± 0% +20.14% (p=0.002 n=6) BM_Calculate/100 30.82 ¬± 0% 30.05 ¬± 0% -2.49% (p=0.002 n=6) BM_Calculate/2048 285.6 ¬± 0% 644.8 ¬± 0% +125.78% (p=0.002 n=6) BM_Calculate/10000 906.7 ¬± 0% 3633.8 ¬± 0% +300.78% (p=0.002 n=6) BM_Calculate/500000 37.77k ¬± 0% 187.69k ¬± 0% +396.97% (p=0.002 n=6)&lt;/quote&gt;&lt;p&gt;If we create a 4th stream for &lt;code&gt;ParallelBlockHash&lt;/code&gt; (Compiler
Explorer), &lt;code&gt;llvm-mca&lt;/code&gt; shows that the overall
latency is unchanged since we are bottlenecked on port 1‚Äôs throughput. Unrolling
further adds additional overhead to combine the streams and makes prefetching
harder without actually improving performance.&lt;/p&gt;&lt;p&gt;To improve performance, many fast CRC32C implementations use other processor features. Instructions like the carryless multiply instruction (&lt;code&gt;pclmulqdq&lt;/code&gt; on
x86) can be used to implement another parallel stream. This allows additional
ILP to be extracted by using the other ports of the processor without worsening
the bottleneck on the port used by &lt;code&gt;crc32&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;While &lt;code&gt;llvm-mca&lt;/code&gt; can be a useful tool in many situations, its modeling has
limits:&lt;/p&gt;&lt;p&gt;Memory accesses are modeled as L1 hits. In the real world, we can have much longer stalls when we need to access the L2, L3, or even main memory.&lt;/p&gt;&lt;p&gt;It cannot model branch predictor behavior.&lt;/p&gt;&lt;p&gt;It does not model instruction fetch and decode steps.&lt;/p&gt;&lt;p&gt;Its analysis is only as good as LLVM‚Äôs processor models. If these do not accurately model the processor, the simulation might differ from the real processor.&lt;/p&gt;&lt;p&gt;For example, many ARM processor models are incomplete, and &lt;code&gt;llvm-mca&lt;/code&gt; picks
a processor model that it estimates to be a good substitute; this is
generally fine for compiler heuristics, where differences only matter if it
would result in different generated code, but it can derail manual
optimization efforts.&lt;/p&gt;&lt;p&gt;Understanding how the processor executes and retires instructions can give us powerful insights for optimizing functions. &lt;code&gt;llvm-mca&lt;/code&gt; lets us peer into the
processor to let us understand bottlenecks and underutilized resources.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46263530</guid><pubDate>Sun, 14 Dec 2025 15:05:06 +0000</pubDate></item><item><title>Update Now: iOS 26.2 Fixes 20 Security Vulnerabilities, 2 Actively Exploited</title><link>https://www.macrumors.com/2025/12/12/ios-26-2-security-vulnerabilities/</link><description>&lt;doc fingerprint="a6e4cd140936312c"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Update Now: iOS 26.2 Fixes 20+ Security Vulnerabilities&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;Apple today released iOS 26.2, iPadOS 26.2, and macOS 26.2, all of which introduce new features, bug fixes, and security improvements. Apple says that the updates address over 20 vulnerabilities, including two bugs that are known to have been actively exploited.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;There are a pair of WebKit vulnerabilities that could allow maliciously crafted web content to execute code or cause memory corruption. Apple says that the bugs might have been exploited in an attack against targeted individuals on versions of iOS before iOS 26.&lt;/p&gt;
          &lt;quote&gt;
            &lt;p&gt;Processing maliciously crafted web content may lead to arbitrary code execution. Apple is aware of a report that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals on versions of iOS before iOS 26.&lt;/p&gt;
            &lt;p&gt;Processing maliciously crafted web content may lead to memory corruption. Apple is aware of a report that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals on versions of iOS before iOS 26.&lt;/p&gt;
          &lt;/quote&gt;
          &lt;p&gt;One of the WebKit bugs was fixed with improved memory management, while the other was addressed with improved validation.&lt;/p&gt;
          &lt;p&gt;There are several other vulnerabilities that were fixed too, across apps and services. An App Store bug could allow users to access sensitive payment tokens, processing a malicious image file could lead to memory corruption, photos in the Hidden Album could be viewed without authentication, and passwords could be unintentionally removed when remotely controlling a device with FaceTime.&lt;/p&gt;
          &lt;p&gt;Now that these vulnerabilities have been publicized by Apple, even those that were not exploited before might be taken advantage of now. Apple recommends all users update their devices to iOS 26.2, iPadOS 26.2, and macOS Tahoe 26.2 as soon as possible.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple seeded the second iOS 26.2 Release Candidate to developers earlier this week, meaning the update will be released to the general public very soon. Apple confirmed iOS 26.2 would be released in December, but it did not provide a specific date. We expect the update to be released by early next week. iOS 26.2 includes a handful of new features and changes on the iPhone, such as a new...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today released new firmware designed for the AirPods Pro 3 and the prior-generation AirPods Pro 2. The AirPods Pro 3 firmware is 8B30, up from 8B25, while the AirPods Pro 2 firmware is 8B28, up from 8B21. There's no word on what's include in the updated firmware, but the AirPods Pro 2 and AirPods Pro 3 are getting expanded support for Live Translation in the European Union in iOS...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Macworld's Filipe Esp√≥sito today revealed a handful of features that Apple is allegedly planning for iOS 26.4, iOS 27, and even iOS 28. The report said the features are referenced within the code for a leaked internal build of iOS 26 that is not meant to be seen by the public. However, it appears that Esp√≥sito and/or his sources managed to gain access to it, providing us with a sneak peek...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Google Maps on iOS quietly gained a new feature recently that automatically recognizes where you've parked your vehicle and saves the location for you. Announced on LinkedIn by Rio Akasaka, Google Maps' senior product manager, the new feature auto-detects your parked location even if you don't use the parking pin function, saves it for up to 48 hours, and then automatically removes it once...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today released iOS 26.2, the second major update to the iOS 26 operating system that came out in September, iOS 26.2 comes a little over a month after iOS 26.1 launched. iOS 26.2 is compatible with the iPhone 11 series and later, as well as the second-generation iPhone SE. The new software can be downloaded on eligible iPhones over-the-air by going to Settings &amp;gt;...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple has ordered 22 million OLED panels from Samsung Display for the first foldable iPhone, signaling a significantly larger production target than the display industry had previously anticipated, ET News reports. In the now-seemingly deleted report, ET News claimed that Samsung plans to mass-produce 11 million inward-folding OLED displays for Apple next year, as well as 11 million...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;The AirTag 2 will include a handful of new features that will improve tracking capabilities, according to a new report from Macworld. The site says that it was able to access an internal build of iOS 26, which includes references to multiple unreleased products. Here's what's supposedly coming: An improved pairing process, though no details were provided. AirTag pairing is already...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple is about to release iOS 26.2, the second major point update for iPhones since iOS 26 was rolled out in September, and there are at least 15 notable changes and improvements worth checking out. We've rounded them up below. Apple is expected to roll out iOS 26.2 to compatible devices sometime between December 8 and December 16. When the update drops, you can check Apple's servers for the ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today released macOS Tahoe 26.2, the second major update to the macOS Tahoe operating system that came out in September. macOS Tahoe 26.2 comes five weeks after Apple released macOS Tahoe 26.1. Mac users can download the macOS Tahoe update by using the Software Update section of System Settings. macOS Tahoe 26.2 includes Edge Light, a feature that illuminates your face with soft...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46264101</guid><pubDate>Sun, 14 Dec 2025 16:13:55 +0000</pubDate></item><item><title>Rust Coreutils 0.5.0 Release: 87.75% compatibility with GNU Coreutils</title><link>https://github.com/uutils/coreutils/releases/tag/0.5.0</link><description>&lt;doc fingerprint="8909e813c951b496"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;üì¶ Rust Coreutils 0.5.0 Release:&lt;/head&gt;
    &lt;p&gt;We are excited to announce the release of Rust Coreutils 0.5.0 ‚Äî a significant milestone featuring comprehensive platform improvements, and robust testing infrastructure with continued progress toward full GNU compatibility!&lt;/p&gt;
    &lt;head rend="h3"&gt;Highlights:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Improved GNU Compatibility&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;566 passing tests (+22 from 0.4.0), achieving 87.75% compatibility&lt;/item&gt;
          &lt;item&gt;Reduced failures from 56 to 55 (-1) and skipped tests from 33 to 23 (-10)&lt;/item&gt;
          &lt;item&gt;Updated GNU reference from 9.8 to 9.9, adding 11 new tests&lt;/item&gt;
          &lt;item&gt;Major improvements to &lt;code&gt;fold&lt;/code&gt;,&lt;code&gt;cksum&lt;/code&gt;,&lt;code&gt;install&lt;/code&gt;, and&lt;code&gt;numfmt&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Unicode &amp;amp; Text Processing Enhancements&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;fold&lt;/code&gt;: Added combining character support for proper Unicode text wrapping&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;ptx&lt;/code&gt;: Implemented GNU mode with dumb terminal format&lt;/item&gt;
          &lt;item&gt;Enhanced text processing across multiple utilities&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Security &amp;amp; Performance Improvements&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;cksum&lt;/code&gt;: Merged with hashsum for unified checksum functionality&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;install&lt;/code&gt;: Enhanced mode parsing with comma-separated support and umask handling&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;seq&lt;/code&gt;: Improved large integer handling with dedicated benchmarks&lt;/item&gt;
          &lt;item&gt;Various memory and performance optimizations across utilities&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Platform Support Expansion&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Added OpenBSD to CI pipeline with comprehensive testing&lt;/item&gt;
          &lt;item&gt;Re-enabled Redox OS support in CI&lt;/item&gt;
          &lt;item&gt;Enhanced Cygwin support in uucore&lt;/item&gt;
          &lt;item&gt;Improved build processes across platforms&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Developer Experience Improvements&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;New TTY helper for enhanced testing capabilities&lt;/item&gt;
          &lt;item&gt;Comprehensive benchmarking additions for multiple utilities&lt;/item&gt;
          &lt;item&gt;Reduced dependency bloat through feature splitting&lt;/item&gt;
          &lt;item&gt;Enhanced hardware detection module&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Contributions: This release was made possible by 6 new contributors joining our community&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;GNU Test Suite Compatibility:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Result&lt;/cell&gt;
        &lt;cell role="head"&gt;0.4.0&lt;/cell&gt;
        &lt;cell role="head"&gt;0.5.0&lt;/cell&gt;
        &lt;cell role="head"&gt;Change 0.4.0 to 0.5.0&lt;/cell&gt;
        &lt;cell role="head"&gt;% Total 0.4.0&lt;/cell&gt;
        &lt;cell role="head"&gt;% Total 0.5.0&lt;/cell&gt;
        &lt;cell role="head"&gt;% Change 0.4.0 to 0.5.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;544&lt;/cell&gt;
        &lt;cell&gt;566&lt;/cell&gt;
        &lt;cell&gt;+22&lt;/cell&gt;
        &lt;cell&gt;85.80%&lt;/cell&gt;
        &lt;cell&gt;87.75%&lt;/cell&gt;
        &lt;cell&gt;+1.95%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Skip&lt;/cell&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;-10&lt;/cell&gt;
        &lt;cell&gt;5.21%&lt;/cell&gt;
        &lt;cell&gt;3.57%&lt;/cell&gt;
        &lt;cell&gt;-1.64%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;56&lt;/cell&gt;
        &lt;cell&gt;55&lt;/cell&gt;
        &lt;cell&gt;-1&lt;/cell&gt;
        &lt;cell&gt;8.83%&lt;/cell&gt;
        &lt;cell&gt;8.53%&lt;/cell&gt;
        &lt;cell&gt;-0.30%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.16%&lt;/cell&gt;
        &lt;cell&gt;0.16%&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total&lt;/cell&gt;
        &lt;cell&gt;634&lt;/cell&gt;
        &lt;cell&gt;645&lt;/cell&gt;
        &lt;cell&gt;+11 (new tests)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Call to Action:&lt;/head&gt;
    &lt;p&gt;üåç Help us translate - Contribute translations at Weblate&lt;lb/&gt; üöÄ Sponsor us on GitHub to accelerate development: github.com/sponsors/uutils&lt;lb/&gt; üîó Download the latest release: https://uutils.github.io&lt;/p&gt;
    &lt;head rend="h2"&gt;What's Changed&lt;/head&gt;
    &lt;head rend="h2"&gt;basenc&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;basenc: Fix basenc.pl GNU-compat tests pass by @karanabe in #9203&lt;/item&gt;
      &lt;item&gt;fix(Basenc):fix GNU coreutils test bounded-memory.sh by @mattsu2020 in #9536&lt;/item&gt;
      &lt;item&gt;base32, base64, baseenc:Simplifying the base encoding uu_app and adding basic buffer tests by @ChrisDryden in #9409&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;chmod&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;chmod:fix safe traversal/access by @mattsu2020 in #9554&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;cksum&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;cksum/hashsum: merge digest computation &amp;amp; various improvements by @RenjiSann in #9135&lt;/item&gt;
      &lt;item&gt;cksum: Fix GNU test cksum-base64-untagged by @RenjiSann in #9344&lt;/item&gt;
      &lt;item&gt;Add --debug flag to cksum by @naoNao89 in #9088&lt;/item&gt;
      &lt;item&gt;cksum: Fix GNU test &lt;code&gt;cksum-c.sh&lt;/code&gt;after bump to 9.9 by @RenjiSann in #9511&lt;/item&gt;
      &lt;item&gt;cksum: small improvements, l10n by @RenjiSann in #9532&lt;/item&gt;
      &lt;item&gt;Fix hardware capabilities detection; cksum --debug by @RenjiSann in #9603&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;cp&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;cp: allow directory merging when destination was just created by @vikram-kangotra in #9325&lt;/item&gt;
      &lt;item&gt;cp: Adding test to cover no dereference when copying symlinks by @ChrisDryden in #9623&lt;/item&gt;
      &lt;item&gt;cp: Enabling cp force flag to run on windows by @ChrisDryden in #9624&lt;/item&gt;
      &lt;item&gt;Add comprehensive readonly file regression tests for cp by @naoNao89 in #9045&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;du&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;du: Alias -A --apparent-size by @oech3 in #9555&lt;/item&gt;
      &lt;item&gt;du: handle &lt;code&gt;--files0-from=-&lt;/code&gt;with piped in&lt;code&gt;-&lt;/code&gt;by @cakebaker in #8985&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;env&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adding integration tests for the braced variable parsing in env by @ChrisDryden in #9459&lt;/item&gt;
      &lt;item&gt;env: remove outdated comment by @cakebaker in #9496&lt;/item&gt;
      &lt;item&gt;env: use Command::exec() instead of libc::execvp() by @Ecordonnier in #9614&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;fold&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;fold:Improve fold bench data generation by @mattsu2020 in #9210&lt;/item&gt;
      &lt;item&gt;fix(fold): GNU fold-characters.sh test by @mattsu2020 in #9126&lt;/item&gt;
      &lt;item&gt;Fold: Adding combining character support by @ChrisDryden in #9328&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;hashsum&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hashsum: Fix length processing to fix last GNU test by @RenjiSann in #9569&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;install&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;install: ignore umask by @akretz in #9254&lt;/item&gt;
      &lt;item&gt;Enhance mode parsing to support comma-separated mode strings in install command by @Vesal-J in #9298&lt;/item&gt;
      &lt;item&gt;install: do not call chown when called as root by @Ecordonnier in #9477&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;ln&lt;/head&gt;
    &lt;head rend="h2"&gt;ls&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ls: prevent ReadDir from closing before entries are processed by @vikram-kangotra in #9410&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;mkfifo&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tests/mkfifo: added a test to check mkfifo permission denied error for code coverage by @asder8215 in #9586&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;nl&lt;/head&gt;
    &lt;head rend="h2"&gt;nohup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;nohup: use Command::exec() instead of libc::execvp() by @Ecordonnier in #9613&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;numfmt&lt;/head&gt;
    &lt;head rend="h2"&gt;od&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;od: fix GNU od.pl by @mattsu2020 in #9334&lt;/item&gt;
      &lt;item&gt;fix(od):fix GNU coreutils test od float.sh by @mattsu2020 in #9534&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;pr&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pr: fix header formatting for custom date formats starting with '+' by @sylvestre in #9252&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;ptx&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ptx: implement GNU mode with dumb terminal format by @sylvestre in #9573&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;readlink&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;readlink: test calling without args by @cakebaker in #9230&lt;/item&gt;
      &lt;item&gt;readlink:Correction to Symbolic Link Handling by @mattsu2020 in #9633&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;seq&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;fix(seq): handle BrokenPipe like GNU by @mattsu2020 in #9471&lt;/item&gt;
      &lt;item&gt;seq:fix test_broken_pipe_still_exits_success by @mattsu2020 in #9520&lt;/item&gt;
      &lt;item&gt;seq: adding large integers benchmarks by @ChrisDryden in #9561&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;shuf&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;shuf: add benchmarks by @sylvestre in #9320&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;sort&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;sort: make compression program failures non-fatal, warn and fallback to plain files by @sylvestre in #9266&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;stdbuf&lt;/head&gt;
    &lt;head rend="h2"&gt;stty&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adding TTY helper for unix to be able to create tests for stty and more by @ChrisDryden in #9348&lt;/item&gt;
      &lt;item&gt;Using the pty helper function for the more bin testing by @ChrisDryden in #9441&lt;/item&gt;
      &lt;item&gt;stty: baud parsing integration tests and validation by @ChrisDryden in #9454&lt;/item&gt;
      &lt;item&gt;stty: Implemented saved state parser for stty by @ChrisDryden in #9480&lt;/item&gt;
      &lt;item&gt;stty: Changing shell command to add recognizing a TTY for stty tests by @ChrisDryden in #9336&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;tail&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tail: batch inotify events to prevent redundant headers after SIGSTOP/SIGCONT by @sylvestre in #9574&lt;/item&gt;
      &lt;item&gt;tail: fix intermittent overlay-headers test by batching inotify events by @sylvestre in #9598&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;tee&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tee: fix poll timeout causing intermittent hangs with -p flag by @sylvestre in #9585&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;timeout&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reducing sleep times and timeout times in test_timeout by @ChrisDryden in #9448&lt;/item&gt;
      &lt;item&gt;timeout: cleanup return values by @Ecordonnier in #9576&lt;/item&gt;
      &lt;item&gt;timeout: remove FIXME in test by @Ecordonnier in #9580&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;dd&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;dd: Handle slow transfer rates in progress display by @martinkunkel2 in #9529&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;uucore&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;uucore: embed system locale on cargo install by @WaterWhisperer in #8604&lt;/item&gt;
      &lt;item&gt;feat(uucore): add shared hardware detection module by @naoNao89 in #9279&lt;/item&gt;
      &lt;item&gt;uucore: support cygwin by @ognevny in #9535&lt;/item&gt;
      &lt;item&gt;uucore: mode parsing: support comma-separated modes by @martinkunkel2 in #9578&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;uudoc&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;uudoc: fix manpage for individual utilities has wrong name (nit) by @shayelkin in #9152&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;CI &amp;amp; Build&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CICD.yml: split PROFILE= from CARGOFLAGS by @oech3 in #9219&lt;/item&gt;
      &lt;item&gt;GNUmakefile: use PROFILE_CMD at make test by @oech3 in #9214&lt;/item&gt;
      &lt;item&gt;GNUmakefile: generalize logic for SELINUX_PROGS for many platforms by @oech3 in #9221&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Let SELinux optional to use it locally without libselinux by @oech3 in #9220&lt;/item&gt;
      &lt;item&gt;freebsd.yml: remove not working PROFILE= by @oech3 in #9225&lt;/item&gt;
      &lt;item&gt;GNUmakefile: Remove check for LIBSELINUX_ENABLED by @oech3 in #9228&lt;/item&gt;
      &lt;item&gt;Update redoxer and reenable Redox OS in CI by @jackpot51 in #9233&lt;/item&gt;
      &lt;item&gt;GNUmakefile: drop not used use_default:=1 by @oech3 in #9231&lt;/item&gt;
      &lt;item&gt;ci: Mark runcon-no-reorder as SELinux required by @oech3 in #9234&lt;/item&gt;
      &lt;item&gt;github action: add openbsd in the ci by @sylvestre in #9196&lt;/item&gt;
      &lt;item&gt;openbsd.yml: Remove not working PROFILE= by @oech3 in #9238&lt;/item&gt;
      &lt;item&gt;OpenBSD CI: increase max open files for test job by @lcheylus in #9242&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Remove || true by @oech3 in #9256&lt;/item&gt;
      &lt;item&gt;Enable test test_hostname_ip on OpenBSD by @lcheylus in #9257&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Use system tools by @oech3 in #9251&lt;/item&gt;
      &lt;item&gt;ci: remove commented out line from &lt;code&gt;freebsd.yml&lt;/code&gt;by @cakebaker in #9239&lt;/item&gt;
      &lt;item&gt;GnuTests.yml: reduce deps by @oech3 in #9259&lt;/item&gt;
      &lt;item&gt;Update CICD.yml: Stop releasing duplicated binary by @oech3 in #9269&lt;/item&gt;
      &lt;item&gt;Avoid mixing wget and curl by @oech3 in #9258&lt;/item&gt;
      &lt;item&gt;CICD.yml: Remove if for .exe by @oech3 in #9271&lt;/item&gt;
      &lt;item&gt;GNUmakefile: Use any profile from make install by @oech3 in #8730&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Freeze SELinux build mode by @oech3 in #9270&lt;/item&gt;
      &lt;item&gt;CICD.yml: Avoid no space left by @oech3 in #9277&lt;/item&gt;
      &lt;item&gt;GNUmakefile: Add missing PROFILE_CMD by @oech3 in #9293&lt;/item&gt;
      &lt;item&gt;Cargo.toml: move panic=abort to release profile for binary size by @oech3 in #9240&lt;/item&gt;
      &lt;item&gt;Fix build failure without libselinux by @oech3 in #9290&lt;/item&gt;
      &lt;item&gt;Revert a patch for runcon-no-reorder (superseded) by @oech3 in #9291&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: fix the error on line 110 by @sylvestre in #9297&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: adjust the PATH for each run by @sylvestre in #9319&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Use any profile &amp;amp; cleanup vars by @oech3 in #9321&lt;/item&gt;
      &lt;item&gt;GnuTests.yml: Check that build-gnu.sh works without libselinux by @oech3 in #9299&lt;/item&gt;
      &lt;item&gt;android.yml: Reduce RAM (#9278) by @oech3 in #9436&lt;/item&gt;
      &lt;item&gt;l10n.yml:Don't apt-get build-essential by @oech3 in #9472&lt;/item&gt;
      &lt;item&gt;l10n.yml: Use PROFILE=release-small for faster CI by @oech3 in #9473&lt;/item&gt;
      &lt;item&gt;l10n.yml: Do not brew make (support Xcode make) by @oech3 in #9474&lt;/item&gt;
      &lt;item&gt;Update Dockerfile: Don't apt-get jq (preinstalled) by @oech3 in #9481&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Remove 2 not working sed hacks for tr by @oech3 in #9476&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Reduce time to build GNU coreutils by @oech3 in #9475&lt;/item&gt;
      &lt;item&gt;CICD.yml: Stop publishing conflicting artifacts by @oech3 in #9491&lt;/item&gt;
      &lt;item&gt;CICD.yml: Removed unused code for i586 by @oech3 in #9497&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Remove hfs dep from hardlink-case.sh by @oech3 in #9482&lt;/item&gt;
      &lt;item&gt;CICD.yml: Dedup a mkdir by @oech3 in #9504&lt;/item&gt;
      &lt;item&gt;CICD.yml: Drop a workaround for old package by @oech3 in #9505&lt;/item&gt;
      &lt;item&gt;Remove wget dep by @oech3 in #9522&lt;/item&gt;
      &lt;item&gt;Remove Makefile.toml by @oech3 in #9568&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Remove 2 non-GNU binary by @oech3 in #9583&lt;/item&gt;
      &lt;item&gt;validation.rs: Remove non GNU hashsum aliases by @oech3 in #9589&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: Enable misc/coreutils.sh by @oech3 in #9572&lt;/item&gt;
      &lt;item&gt;GHA-delete-GNU-workflow-logs.sh: Add fallback to jaq by @oech3 in #9581&lt;/item&gt;
      &lt;item&gt;benchmarks.yml: Stop unnecessary apt-get by @oech3 in #9608&lt;/item&gt;
      &lt;item&gt;Do not apt-get preinstalled tools to avoid delaying CI by @oech3 in #9466&lt;/item&gt;
      &lt;item&gt;build-gnu.sh: use GNU sed much more for macOS by @oech3 in #9467&lt;/item&gt;
      &lt;item&gt;ci: add locales for GNU tests by @cakebaker in #9052&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Documentation&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;README.md: profiles for binary size by @oech3 in #9268&lt;/item&gt;
      &lt;item&gt;installation.md: Fix wrong reference for AUR by @oech3 in #9447&lt;/item&gt;
      &lt;item&gt;README.md: note that separator is needed for PROG_PREFIX by @oech3 in #9444&lt;/item&gt;
      &lt;item&gt;installation.md: Ref MSYS2 package by @oech3 in #9456&lt;/item&gt;
      &lt;item&gt;installation.md: Add MSYS2 Cygwin package by @oech3 in #9566&lt;/item&gt;
      &lt;item&gt;why-skip.md: Remove an OOD doc by @oech3 in #9506&lt;/item&gt;
      &lt;item&gt;why-skip.md: Remove a passing test by @oech3 in #9507&lt;/item&gt;
      &lt;item&gt;why-skip.md: Remove 4 sparse-* by @oech3 in #9508&lt;/item&gt;
      &lt;item&gt;why-skip.md: Remove 3 tests by @oech3 in #9509&lt;/item&gt;
      &lt;item&gt;why-error.md: Cleanup by @oech3 in #9510&lt;/item&gt;
      &lt;item&gt;why-error.md: Cleanup and documenting by @oech3 in #9512&lt;/item&gt;
      &lt;item&gt;why-skip.md: Remove 1 passing root test by @oech3 in #9528&lt;/item&gt;
      &lt;item&gt;why-skip.md: Let spell-checker:ignore a comment by @oech3 in #9540&lt;/item&gt;
      &lt;item&gt;why-{skip,error}.md: Cleanup by @oech3 in #9602&lt;/item&gt;
      &lt;item&gt;why-{skip,error}.md: Remove stty tests and shared strings by @oech3 in #9626&lt;/item&gt;
      &lt;item&gt;lib.rs: Remove non GNU hashsum aliases by @oech3 in #9642&lt;/item&gt;
      &lt;item&gt;util.rs: Update obsolete comments by @oech3 in #9643&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Code Quality &amp;amp; Cleanup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;test: existing file is newer than non-existing file by @cakebaker in #9245&lt;/item&gt;
      &lt;item&gt;move the factor divan bench into the actual directory by @sylvestre in #9296&lt;/item&gt;
      &lt;item&gt;Remove high variance benchmark functions by @sylvestre in #9311&lt;/item&gt;
      &lt;item&gt;Bump &lt;code&gt;markdownlint_cli2_action&lt;/code&gt;&amp;amp; fix warnings by @cakebaker in #9309&lt;/item&gt;
      &lt;item&gt;replace number_prefix by unit-prefix by @sylvestre in #9322&lt;/item&gt;
      &lt;item&gt;coreutils: Print utility not found to stderr by @oech3 in #9588&lt;/item&gt;
      &lt;item&gt;use github URLs for fetching tldr.zip by @dgilman in #9584&lt;/item&gt;
      &lt;item&gt;unit test coverage: fix missing coverage by @martinkunkel2 in #9606&lt;/item&gt;
      &lt;item&gt;Removing the per process file flag to reduce the llvm filemerge time by @ChrisDryden in #9449&lt;/item&gt;
      &lt;item&gt;Making wild a windows only dependency and gating unit-prefix by @ChrisDryden in #9548&lt;/item&gt;
      &lt;item&gt;Splitting parser feature into multiple subfeatures to reduce dependency bloat by @ChrisDryden in #9546&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance &amp;amp; Benchmarking&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;benches: Migrate factor benchmarks from Criterion to Divan by @naoNao89 in #9247&lt;/item&gt;
      &lt;item&gt;Add functionality to show when tests were previously skipped and now failing accurately by @ChrisDryden in #9521&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Version Management&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;update gnu ref to 9.9 + improve the script by @sylvestre in #9216&lt;/item&gt;
      &lt;item&gt;prepare version 0.5.0 by @sylvestre in #9596&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Dependency Updates&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;chore(deps): update rust crate crc-fast to v1.7.1 by @renovate[bot] in #9224&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate indicatif to v0.18.3 by @renovate[bot] in #9226&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate crc-fast to v1.8.0 by @renovate[bot] in #9282&lt;/item&gt;
      &lt;item&gt;Update vmactions/freebsd-vm action to v1.2.7 by @renovate[bot] in #9304&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate clap to v4.5.52 by @renovate[bot] in #9316&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate clap_complete to v4.5.61 by @renovate[bot] in #9343&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate clap to v4.5.53 by @renovate[bot] in #9340&lt;/item&gt;
      &lt;item&gt;GNUmakefile: Use libstdbuf.* instead of libstdbuf* by @oech3 in #9345&lt;/item&gt;
      &lt;item&gt;chore(deps): update actions/checkout action to v6 by @renovate[bot] in #9416&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate parse_datetime to v0.13.3 by @renovate[bot] in #9434&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate hostname to v0.4.2 by @renovate[bot] in #9515&lt;/item&gt;
      &lt;item&gt;Bump &lt;code&gt;iana-time-zone&lt;/code&gt;and&lt;code&gt;windows-core&lt;/code&gt;by @cakebaker in #9519&lt;/item&gt;
      &lt;item&gt;chore(deps): update vmactions/freebsd-vm action to v1.2.8 by @renovate[bot] in #9524&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate ctor to v0.6.2 by @renovate[bot] in #9544&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate ctor to v0.6.3 by @renovate[bot] in #9562&lt;/item&gt;
      &lt;item&gt;chore(deps): update vmactions/freebsd-vm action to v1.2.9 by @renovate[bot] in #9593&lt;/item&gt;
      &lt;item&gt;chore(deps): update davidanson/markdownlint-cli2-action action to v22 by @renovate[bot] in #9610&lt;/item&gt;
      &lt;item&gt;chore(deps): update actions/cache action to v5 by @renovate[bot] in #9639&lt;/item&gt;
      &lt;item&gt;chore(deps): update rust crate crc-fast to v1.8.1 by @renovate[bot] in #9647&lt;/item&gt;
      &lt;item&gt;chore(deps): update github artifact actions (major) by @renovate[bot] in #9645&lt;/item&gt;
      &lt;item&gt;Bump &lt;code&gt;icu&lt;/code&gt;crates from&lt;code&gt;2.0.0&lt;/code&gt;to&lt;code&gt;2.1.1&lt;/code&gt;by @cakebaker in #9073&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;New Contributors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;@WaterWhisperer made their first contribution in #8604&lt;/item&gt;
      &lt;item&gt;@ChrisDryden made their first contribution in #9328&lt;/item&gt;
      &lt;item&gt;@FidelSch made their first contribution in #9342&lt;/item&gt;
      &lt;item&gt;@ognevny made their first contribution in #9535&lt;/item&gt;
      &lt;item&gt;@shayelkin made their first contribution in #9152&lt;/item&gt;
      &lt;item&gt;@dgilman made their first contribution in #9584&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Full Changelog: 0.4.0...0.5.0&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46264329</guid><pubDate>Sun, 14 Dec 2025 16:40:40 +0000</pubDate></item><item><title>Hashcards: A Plain-Text Spaced Repetition System</title><link>https://borretti.me/article/hashcards-plain-text-spaced-repetition</link><description>&lt;doc fingerprint="ac10b51d29260cb9"&gt;
  &lt;main&gt;
    &lt;p&gt;hashcards is a local-first spaced repetition app, along the lines of Anki or Mochi. Like Anki, it uses FSRS, the most advanced scheduling algorithm yet, to schedule reviews.&lt;/p&gt;
    &lt;p&gt;The thing that makes hashcards unique: it doesn‚Äôt use a database. Rather, your flashcard collection is just a directory of Markdown files, like so:&lt;/p&gt;
    &lt;code&gt;Cards/
  Math.md
  Chemistry.md
  Astronomy.md
  ...
&lt;/code&gt;
    &lt;p&gt;And each file, or ‚Äúdeck‚Äù, looks like this:&lt;/p&gt;
    &lt;code&gt;Q: What is the role of synaptic vesicles?
A: They store neurotransmitters for release at the synaptic terminal.

Q: What is a neurite?
A: A projection from a neuron: either an axon or a dendrite.

C: Speech is [produced] in [Broca's] area.

C: Speech is [understood] in [Wernicke's] area.
&lt;/code&gt;
    &lt;p&gt;You write flashcards more or less like you‚Äôd write ordinary notes, with lightweight markup to denote basic (question/answer) flashcards and cloze deletion flashcards. Then, to study, you run:&lt;/p&gt;
    &lt;code&gt;$ hashcards drill &amp;lt;path to the cards directory&amp;gt;
&lt;/code&gt;
    &lt;p&gt;This opens a web interface on &lt;code&gt;localhost:8000&lt;/code&gt;, where you can review the
flashcards. Your performance and review history is stored in an SQLite
database in the same directory as the cards. Cards are content-addressed, that
is, identified by the hash of their text.&lt;/p&gt;
    &lt;p&gt;This central design decision yields many benefits: you can edit your flashcards with your editor of choice, store your flashcard collection in a Git repo, track its changes, share it on GitHub with others (as I have). You can use scripts to generate flashcards from some source of structured data (e.g. a CSV of English/French vocabulary pairs). You can query and manipulate your collection using standard Unix tools, or programmatically, without having to dig into the internals of some app‚Äôs database.&lt;/p&gt;
    &lt;p&gt;Why build a new spaced repetition app? Mostly because I was dissatisfied with both Anki and Mochi. But also, additionally, because my flashcards collection is very important to me, and having it exist either in some remote database, or as an opaque unusable data blob on my computer, doesn‚Äôt feel good. ‚ÄúMarkdown files in a Git repo‚Äù gives me a level of ownership that other approaches lack.&lt;/p&gt;
    &lt;p&gt;The rest of this post explains my frustrations with Anki and Mochi, and how I landed on the design decisions for hashcards.&lt;/p&gt;
    &lt;head rend="h1"&gt;Anki&lt;/head&gt;
    &lt;p&gt;Anki was the first SR system I used. It‚Äôs open source, so it will be around forever; it has a million plugins; it was the first SR system to use FSRS for scheduling. It has really rich stats, which I think are mostly useless but are fun to look at. And the note types feature is really good: it lets you generate a large number of flashcards automatically from structured data.&lt;/p&gt;
    &lt;p&gt;The central problem with Anki is that the interface is really bad. This manifests in various ways.&lt;/p&gt;
    &lt;p&gt;First, it is ugly to look at, particularly the review screen. And this diminishes your enjoyment of what is already an often boring and frustrating process.&lt;/p&gt;
    &lt;p&gt;Second, doing simple things is hard. A nice feature of Mochi is that when you start the app you go right into review mode. You‚Äôre drilling flashcards before you even realize it. Anki doesn‚Äôt have a ‚Äústudy all cards due today‚Äù, rather, you have to manually go into a deck and click the ‚ÄúStudy Now‚Äù button. So what I would do is put all my decks under a ‚ÄúRoot‚Äù deck, and study that. But this is a hack.&lt;/p&gt;
    &lt;p&gt;And, third: card input uses WYSIWYG editing. So, you‚Äôre either jumping from the keyboard to the mouse (which increases latency, and makes flashcard creation more frustrating) or you have to remember all these keybindings to do basic things like ‚Äúmake this text a cloze deletion‚Äù or ‚Äúmake this TeX math‚Äù.&lt;/p&gt;
    &lt;p&gt;Finally, plugins are a double-edged sword. Because having the option to use them is nice, but the experience of actually using most plugins is bad. The whole setup feels janky, like a house of cards. Most of the time, if a feature is not built into the app itself, I would rather live without it than use a plugin.&lt;/p&gt;
    &lt;head rend="h1"&gt;Mochi&lt;/head&gt;
    &lt;p&gt;Mochi feels like it was built to address the main complaint about Anki: the interface. It is intuitive, good looking, shortcut-rich. No jank. Instead of WYSIWYG, card text is Markdown: this is delightful.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a few problems. While Markdown is a very low-friction way to write flashcards, cloze deletions in Mochi are very verbose. In hashcards, you can write this:&lt;/p&gt;
    &lt;code&gt;Speech is [produced] in [Broca's] area.
&lt;/code&gt;
    &lt;p&gt;The equivalent in Mochi is this:&lt;/p&gt;
    &lt;code&gt;Speech is {{1::produced}} in {{2::Broca's}} area.
&lt;/code&gt;
    &lt;p&gt;This is a lot of typing. And you might object that it‚Äôs only a few characters longer. But when you‚Äôre studying from a textbook, or when you‚Äôre copying words from a vocabulary table, these small frictions add up. If writing flashcards is frustrating, you‚Äôll write fewer of them: and that means less knowledge gained. Dually, a system that makes flashcard creation as frictionless as possible means more flashcards, and more knowledge.&lt;/p&gt;
    &lt;p&gt;Another problem is that Mochi doesn‚Äôt have an equivalent of Anki‚Äôs note types. For example: you can make a note type for chemical elements, with fields like atomic number, symbol, name, etc., and write templates to generate flashcards asking questions like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What is the atomic number of [name]?&lt;/item&gt;
      &lt;item&gt;What element has atomic number [number]?&lt;/item&gt;
      &lt;item&gt;What is the symbol for [name]?&lt;/item&gt;
      &lt;item&gt;What element has symbol [symbol]?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And so on for other properties. This is good. Automation is good. Less work, more flashcards. Mochi doesn‚Äôt have this feature. It has templates, but these are not as powerful.&lt;/p&gt;
    &lt;p&gt;But the biggest problem with Mochi, I think, is the algorithm. Until very recently, when they added beta support for FSRS, the algorithm used by Mochi was even simpler than SM-2. It was based on multipliers: remembering a card multiplies its interval by a number &amp;gt;1, forgetting a card multiplies its interval by a number between 0 and 1.&lt;/p&gt;
    &lt;p&gt;The supposed rationale for this is simplicity: the user can reason about the algorithm more easily. But I think this is pointless. The whole point of an SR app is the software manages the schedule for you, and the user is completely unaware of how the scheduler works. The optimality is to have the most advanced possible scheduling algorithm (meaning the one that yields the most recall for the least review time) under the most intuitive interface possible, and the user just reaps the benefits.&lt;/p&gt;
    &lt;p&gt;Obviously without an RCT we can‚Äôt compare Mochi/SM-2/FSRS, but my subjective experience of it is that the algorithm works well for the short-term, and falters on the long-term. It‚Äôs very bad when you forget a mature card: if a card has an interval of sixty days, and you click forget, you don‚Äôt reset the interval to one day (which is good, because it helps you reconsolidate the lost knowledge). Rather, the interval is multiplied by the forget multiplier (by default: 0.5) down to thirty days. What‚Äôs the use? If I forgot something after sixty days, I surely won‚Äôt have better recall in thirty.&lt;/p&gt;
    &lt;p&gt;You can fix this by setting the forget multiplier to zero. But you have to know this is how it works, and, crucially: I don‚Äôt want to configure things! I don‚Äôt want ‚Äúscheduler parameter finetuning‚Äù to be yet another skill I have to acquire: I want the scheduler to just work.&lt;/p&gt;
    &lt;p&gt;In general, I think spaced repetition algorithms are too optimistic. I‚Äôd rather see cards slightly more often, and spend more time reviewing things, than get stuck in ‚Äúforgetting hell‚Äù. But developers have to worry that making the system too burdensome will hurt retention.&lt;/p&gt;
    &lt;p&gt;In Anki, it‚Äôs the interface that‚Äôs frustrating, but the algorithm works marvelously. In Mochi, the interface is delightful, but it‚Äôs the algorithm that‚Äôs frustrating. Because you can spend months and months drilling flashcards, building up your collection, but when the cards cross some invisible age threshold, you start to forget them, and the algorithm does not help you relearn things you have forgotten. Eventually I burned out on it and stopped doing my reviews, because I expected to forget everything eventually anyhow. And now they added support for FSRS, but by now I have 1700 cards overdue.&lt;/p&gt;
    &lt;p&gt;Additionally: Mochi has only two buttons, ‚ÄúForgot‚Äù and ‚ÄúRemembered‚Äù. This is simpler for the user, yes, but most SR scheduling algorithms have more options for a reason: different degrees of recall adjust the card parameters by different magnitudes.&lt;/p&gt;
    &lt;head rend="h1"&gt;Hashcards&lt;/head&gt;
    &lt;p&gt;What do I want from a spaced repetition system?&lt;/p&gt;
    &lt;p&gt;The first thing is: card creation must be frictionless. I have learned that the biggest bottleneck in spaced repetition, for me, is not doing the reviews (I am very disciplined about this and have done SR reviews daily for months on end), it‚Äôs not even converting conceptual knowledge into flashcards, the biggest bottleneck is just entering cards into the system.&lt;/p&gt;
    &lt;p&gt;The surest way to shore up your knowledge of some concept or topic is to write more flashcards about it: asking the same question in different ways, in different directions, from different angles. More volume means you see the same information more often, asking in different ways prevents ‚Äúmemorizing the shape of the card‚Äù, and it acts as a kind of redundancy: there are multiple edges connecting that bit of knowledge to the rest of your mind.&lt;/p&gt;
    &lt;p&gt;And there have been many times where I have thought: I would make this more solid by writing another flashcard. But I opted not to because the marginal flashcard is too effortful.&lt;/p&gt;
    &lt;p&gt;If getting cards into the system involves a lot of friction, you write fewer cards. And there‚Äôs an opportunity cost: the card you don‚Äôt write is a concept you don‚Äôt learn. Integrated across time, it‚Äôs entire oceans of knowledge which are lost.&lt;/p&gt;
    &lt;p&gt;So: the system should make card entry effortless. This was the guiding principle behind the design of the hashcards text format. For example, cloze deletions use square brackets because in a US keyboard, square brackets can be typed without pressing shift (compare Mochi‚Äôs curly brace). And it‚Äôs one bracket, not two. Originally, the format was one line per card, with blank lines separating flashcards, and question-answer cards used slashes to separate the sides, like so:&lt;/p&gt;
    &lt;code&gt;What is the atomic number of carbon? / 6

The atomic number of [carbon] is [6].
&lt;/code&gt;
    &lt;p&gt;And this is strictly less friction. But it creates a problem for multi-line flashcards, which are common enough that they should not be a second-class citizen. Eventually, I settled on the current format:&lt;/p&gt;
    &lt;code&gt;Q: What is the atomic number of carbon?
A: 6

C: The atomic number of [carbon] is [6].
&lt;/code&gt;
    &lt;p&gt;Which is only slightly more typing, and has the benefit that you can easily visually identify where a card begins and ends, and what kind of card it is. I spent a lot of time arguing back and forth with Claude about what the optimal format should be.&lt;/p&gt;
    &lt;p&gt;Another source of friction is not creating the cards but editing them. The central problem is that your knowledge changes and improves over time. Often textbooks take this approach where Chapter 1 introduces one kind of ontology, and by Chapter 3 they tell you, ‚Äúactually that was a lie, here‚Äôs the real ontology of this subject‚Äù, and then you have to go back and edit the old flashcards to match. Because otherwise you have one card asking, e.g., for the undergraduate definition of some concept, while another asks you for the graduate-level definition, creating ambiguity.&lt;/p&gt;
    &lt;p&gt;For this reason, when studying from a textbook, I create a deck for the textbook, with sub-decks for each chapter. That makes it easy to match the flashcards to their source material (to ensure they are aligned) and each chapter deck only has a few tens of cards usually, keeping them navigable.&lt;/p&gt;
    &lt;p&gt;Sometimes you wrote multiple cards for the same concept, so you have to update them all at once. Finding the related ones can be hard if the deck is large. In hashcards, a deck is just a Markdown file. The cards immediately above and below a card are usually semantically related. You just scroll up and down and make the edits in place.&lt;/p&gt;
    &lt;p&gt;But why plain-text files in a Git repo? Why not use the above format, but in a ‚Äúnormal‚Äù app with a database?&lt;/p&gt;
    &lt;p&gt;The vague idea of a spaced repetition system where flashcards are stored as plain-text files in a Git repo had been kicking around my cranium for a long time. I remember asking an Ankihead on IRC circa 2011 if such a thing existed. At some point I read Andy Matuschak‚Äôs note on his implementation of an SR system. In his system, the flashcards are colocated with prose notes. The notation is similar to mine: &lt;code&gt;Q&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt; tags for
question-answer cards, and &lt;code&gt;{curly braces}&lt;/code&gt; for cloze deletions. And the cards
are content-addressed: identified by their hash. Which is an obviously good
idea. But his code is private and, besides, I feel that prose notes and
flashcards are very different beasts, and I don‚Äôt need or want them to mix.&lt;/p&gt;
    &lt;p&gt;But I think the idea of plain-text spaced repetition got bumped up the priority queue because I spontaneously started using a workflow that was similar to my current hashcards workflow.&lt;/p&gt;
    &lt;p&gt;When studying from a textbook or a website, I‚Äôd write flashcards in a Markdown file. Usually, I used a shorthand like &lt;code&gt;[foo]&lt;/code&gt; for cloze deletions. Then I‚Äôd use
a Python script to transform the shorthand into the &lt;code&gt;{{1::foo}}&lt;/code&gt; notation used by Mochi. And I‚Äôd edit the flashcards in the file, as
my knowledge built up and my sense of what was relevant and important to
remember improved. And then, when I was done with the chapter or document or
whatever, only then, I would manually import the flashcards into Mochi.&lt;/p&gt;
    &lt;p&gt;And it struck me that the last step was kind of unnecessary. I was already writing my flashcards as lightly-annotated Markdown in plain-text files. I had already implemented FSRS out of curiosity. I was looking for a personal project to build during funemployment. So hashcards was by then a very neatly-shaped hole that I just needed to paint inside.&lt;/p&gt;
    &lt;p&gt;It turns out that using plain-text storage has many synergies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can edit the cards using whatever editor you use, build up a library of card-creating macros, and navigate the collection using the editor‚Äôs file browser.&lt;/item&gt;
      &lt;item&gt;You can query and update the collection using standard Unix tools, or a programming language, e.g. using &lt;code&gt;wc&lt;/code&gt;to get the total number of words in the collection, or using&lt;code&gt;awk&lt;/code&gt;to make a bulk-update to a set of cards.&lt;/item&gt;
      &lt;item&gt;You can use Git for version control. Git is infinitely more featureful than the change-tracking of any SR app: you can edit multiple cards in one commit, branch, merge, use pull requests, etc.&lt;/item&gt;
      &lt;item&gt;You can make your flashcards public on GitHub. I often wish people put more of themselves out there: their blog posts, their dotfiles, their study notes. And why not their flashcards? Even if they are not useful to someone else, there is something enjoyable about reading what someone else finds interesting, or enjoyable, or worth learning.&lt;/item&gt;
      &lt;item&gt;You can generate flashcards using scripts (e.g., turn a CSV of foreign language vocabulary into a deck of flashcards), and write a Makefile to tie the script, data source, and target together. I do this in my personal deck. Anki‚Äôs note types don‚Äôt have to be built into hashcards, rather, you can DIY it using some Python and make.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The result is a system where creating and editing flashcards is nearly frictionless, that uses an advanced spaced repetition scheduler, and which provides an elegant UI for drilling flashcards. I hope others will find it useful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46264492</guid><pubDate>Sun, 14 Dec 2025 16:55:29 +0000</pubDate></item><item><title>Show HN: a Pager</title><link>https://www.udp7777.com/</link><description>&lt;doc fingerprint="de29e76d6f74c333"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;The noise is the enemy. The silence is the baseline.&lt;/p&gt;
      &lt;p&gt;No headers. No handshakes. No history.&lt;/p&gt;
      &lt;p&gt;Just the Signal.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;SHA256 CHECKSUMS VERIFIED.&lt;/p&gt;
    &lt;p&gt; "Pagers operate on very low signal levels... making them more reliable than terrestrial cellular networks during disaster." &lt;lb/&gt; ‚Äî London Ambulance Service / Wikipedia &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46264657</guid><pubDate>Sun, 14 Dec 2025 17:10:08 +0000</pubDate></item><item><title>GraphQL: The Enterprise Honeymoon Is Over</title><link>https://johnjames.blog/posts/graphql-the-enterprise-honeymoon-is-over</link><description>&lt;doc fingerprint="820e7a7bae8418de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GraphQL: the enterprise honeymoon is over&lt;/head&gt;
    &lt;p&gt;By John James&lt;/p&gt;
    &lt;p&gt;Published on December 14, 2025&lt;/p&gt;
    &lt;p&gt;Read time ~3 min&lt;/p&gt;
    &lt;p&gt;I‚Äôve used GraphQL, specifically Apollo Client and Server, for a couple of years in a real enterprise-grade application.&lt;/p&gt;
    &lt;p&gt;Not a toy app. Not a greenfield startup. A proper production setup with multiple teams, BFFs, downstream services, observability requirements, and real users.&lt;/p&gt;
    &lt;p&gt;And after all that time, I‚Äôve come to a pretty boring conclusion:&lt;/p&gt;
    &lt;p&gt;GraphQL solves a real problem, but that problem is far more niche than people admit. In most enterprise setups, it‚Äôs already solved elsewhere, and when you add up the tradeoffs, GraphQL often ends up being a net negative.&lt;/p&gt;
    &lt;p&gt;This isn‚Äôt a ‚ÄúGraphQL bad‚Äù post. It‚Äôs a ‚ÄúGraphQL after the honeymoon‚Äù post.&lt;/p&gt;
    &lt;head rend="h3"&gt;what GraphQL is supposed to solve&lt;/head&gt;
    &lt;p&gt;The main problem GraphQL tries to solve is overfetching.&lt;/p&gt;
    &lt;p&gt;The idea is simple and appealing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the client asks for exactly the fields it needs&lt;/item&gt;
      &lt;item&gt;no more, no less&lt;/item&gt;
      &lt;item&gt;no wasted bytes&lt;/item&gt;
      &lt;item&gt;no backend changes for every new UI requirement&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On paper, that‚Äôs great.&lt;/p&gt;
    &lt;p&gt;In practice, things are messier.&lt;/p&gt;
    &lt;head rend="h3"&gt;overfetching is already solved by BFFs&lt;/head&gt;
    &lt;p&gt;Most enterprise frontend architectures already have a BFF (Backend for Frontend).&lt;/p&gt;
    &lt;p&gt;That BFF exists specifically to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;shape data for the UI&lt;/item&gt;
      &lt;item&gt;aggregate multiple downstream calls&lt;/item&gt;
      &lt;item&gt;hide backend complexity&lt;/item&gt;
      &lt;item&gt;return exactly what the UI needs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you‚Äôre using REST behind a BFF, overfetching is already solvable. The BFF can scope down responses and return only what the UI cares about.&lt;/p&gt;
    &lt;p&gt;Yes, GraphQL can also do this.&lt;/p&gt;
    &lt;p&gt;But here‚Äôs the part people gloss over.&lt;/p&gt;
    &lt;p&gt;Most downstream services are still REST.&lt;/p&gt;
    &lt;p&gt;So now your GraphQL layer still has to overfetch from downstream REST APIs, then reshape the response. You didn‚Äôt eliminate overfetching. You just moved it down a layer.&lt;/p&gt;
    &lt;p&gt;That alone significantly diminishes GraphQL‚Äôs main selling point.&lt;/p&gt;
    &lt;p&gt;There is a case where GraphQL wins here. If multiple pages hit the same endpoint but need slightly different fields, GraphQL lets you scope those differences per query.&lt;/p&gt;
    &lt;p&gt;But let‚Äôs be honest about the trade.&lt;/p&gt;
    &lt;p&gt;You‚Äôre usually talking about saving a handful of fields per request, in exchange for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;more setup&lt;/item&gt;
      &lt;item&gt;more abstraction&lt;/item&gt;
      &lt;item&gt;more indirection&lt;/item&gt;
      &lt;item&gt;more code to maintain&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That‚Äôs a very expensive trade for a few extra kilobytes.&lt;/p&gt;
    &lt;head rend="h3"&gt;implementation time is much higher than REST&lt;/head&gt;
    &lt;p&gt;GraphQL takes significantly longer to implement than a REST BFF.&lt;/p&gt;
    &lt;p&gt;With REST, you typically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;call downstream services&lt;/item&gt;
      &lt;item&gt;adapt the response&lt;/item&gt;
      &lt;item&gt;return what the UI needs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With GraphQL, you now have to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;define a schema&lt;/item&gt;
      &lt;item&gt;define types&lt;/item&gt;
      &lt;item&gt;define resolvers&lt;/item&gt;
      &lt;item&gt;define data sources&lt;/item&gt;
      &lt;item&gt;write adapter functions anyway&lt;/item&gt;
      &lt;item&gt;keep schema, resolvers, and clients in sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GraphQL optimizes consumption at the cost of production speed.&lt;/p&gt;
    &lt;p&gt;In an enterprise environment, production speed matters more than theoretical elegance.&lt;/p&gt;
    &lt;head rend="h3"&gt;observability is worse by default&lt;/head&gt;
    &lt;p&gt;This one doesn‚Äôt get talked about enough.&lt;/p&gt;
    &lt;p&gt;GraphQL has this weird status code convention:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;400 if the query can‚Äôt be parsed&lt;/item&gt;
      &lt;item&gt;200 with an &lt;code&gt;errors&lt;/code&gt;array if something failed during execution&lt;/item&gt;
      &lt;item&gt;200 if it succeeded or partially succeeded&lt;/item&gt;
      &lt;item&gt;500 if the server is unreachable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From an observability standpoint, this is painful.&lt;/p&gt;
    &lt;p&gt;With REST:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2XX means success&lt;/item&gt;
      &lt;item&gt;4XX means client error&lt;/item&gt;
      &lt;item&gt;5XX means server error&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you filter dashboards by 2XX, you know those requests succeeded.&lt;/p&gt;
    &lt;p&gt;With GraphQL, a 200 can still mean partial or full failure.&lt;/p&gt;
    &lt;p&gt;Yes, Apollo lets you customize this behavior. But that‚Äôs kind of the point. You‚Äôre constantly paying a tax in extra configuration, extra conventions, and extra mental overhead just to get back to something REST gives you out of the box.&lt;/p&gt;
    &lt;p&gt;This matters when you‚Äôre on call, not when you‚Äôre reading blog posts.&lt;/p&gt;
    &lt;head rend="h3"&gt;caching sounds amazing until you live with it&lt;/head&gt;
    &lt;p&gt;Apollo‚Äôs normalized caching is genuinely impressive.&lt;/p&gt;
    &lt;p&gt;In theory.&lt;/p&gt;
    &lt;p&gt;In practice, it‚Äôs fragile.&lt;/p&gt;
    &lt;p&gt;If you have two queries where only one field differs, Apollo treats them as separate queries. You then have to manually wire things so:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;existing fields come from cache&lt;/item&gt;
      &lt;item&gt;only the differing field is fetched&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At that point:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you still have a roundtrip&lt;/item&gt;
      &lt;item&gt;you‚Äôve added more code&lt;/item&gt;
      &lt;item&gt;debugging cache issues becomes its own problem&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Meanwhile, REST happily overfetches a few extra fields, caches the whole response, and moves on.&lt;/p&gt;
    &lt;p&gt;Extra kilobytes are cheap. Complexity isn‚Äôt.&lt;/p&gt;
    &lt;head rend="h3"&gt;the ID requirement is a leaky abstraction&lt;/head&gt;
    &lt;p&gt;Apollo expects every object to have an &lt;code&gt;id&lt;/code&gt; or &lt;code&gt;_id&lt;/code&gt; field by default, or you need to configure a custom identifier.&lt;/p&gt;
    &lt;p&gt;That assumption does not hold in many enterprise APIs.&lt;/p&gt;
    &lt;p&gt;Plenty of APIs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;don‚Äôt return IDs&lt;/item&gt;
      &lt;item&gt;don‚Äôt have natural unique keys&lt;/item&gt;
      &lt;item&gt;aren‚Äôt modeled as globally identifiable entities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So now the BFF has to generate IDs locally just to satisfy the GraphQL client.&lt;/p&gt;
    &lt;p&gt;That means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;more logic&lt;/item&gt;
      &lt;item&gt;more fields&lt;/item&gt;
      &lt;item&gt;you‚Äôre always fetching one extra field anyway&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Which is ironic, considering the original goal was to reduce overfetching.&lt;/p&gt;
    &lt;p&gt;REST clients don‚Äôt impose this kind of constraint.&lt;/p&gt;
    &lt;head rend="h3"&gt;file uploads and downloads are awkward&lt;/head&gt;
    &lt;p&gt;GraphQL is simply not a good fit for binary data.&lt;/p&gt;
    &lt;p&gt;In practice, you end up:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;returning a download URL&lt;/item&gt;
      &lt;item&gt;then using REST to fetch the file anyway&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Embedding large payloads like PDFs directly in GraphQL responses leads to bloated responses and worse performance.&lt;/p&gt;
    &lt;p&gt;This alone breaks the ‚Äúsingle API‚Äù story.&lt;/p&gt;
    &lt;head rend="h3"&gt;onboarding is slower&lt;/head&gt;
    &lt;p&gt;Most frontend and full-stack developers are far more experienced with REST than GraphQL.&lt;/p&gt;
    &lt;p&gt;Introducing GraphQL means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;teaching schemas&lt;/item&gt;
      &lt;item&gt;teaching resolvers&lt;/item&gt;
      &lt;item&gt;teaching query composition&lt;/item&gt;
      &lt;item&gt;teaching caching rules&lt;/item&gt;
      &lt;item&gt;teaching error semantics&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That learning curve creates friction, especially when teams need to move fast.&lt;/p&gt;
    &lt;p&gt;REST is boring, but boring scales extremely well.&lt;/p&gt;
    &lt;head rend="h3"&gt;error handling is harder than it needs to be&lt;/head&gt;
    &lt;p&gt;GraphQL error responses are‚Ä¶ weird.&lt;/p&gt;
    &lt;p&gt;You have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;nullable vs non-nullable fields&lt;/item&gt;
      &lt;item&gt;partial data&lt;/item&gt;
      &lt;item&gt;errors arrays&lt;/item&gt;
      &lt;item&gt;extensions with custom status codes&lt;/item&gt;
      &lt;item&gt;the need to trace which resolver failed and why&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of this adds indirection.&lt;/p&gt;
    &lt;p&gt;Compare that to a simple REST setup where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;input validation fails, return a 400&lt;/item&gt;
      &lt;item&gt;backend fails, return a 500&lt;/item&gt;
      &lt;item&gt;zod error, done&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Simple errors are easier to reason about than elegant ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;the net result&lt;/head&gt;
    &lt;p&gt;GraphQL absolutely has valid use cases.&lt;/p&gt;
    &lt;p&gt;But in most enterprise environments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you already have BFFs&lt;/item&gt;
      &lt;item&gt;downstream services are REST&lt;/item&gt;
      &lt;item&gt;overfetching is not your biggest problem&lt;/item&gt;
      &lt;item&gt;observability, reliability, and speed matter more&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you add everything up, GraphQL often ends up solving a narrow problem while introducing a broader set of new ones.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why, after using it in production for years, I‚Äôd say this:&lt;/p&gt;
    &lt;p&gt;GraphQL isn‚Äôt bad.&lt;lb/&gt;It‚Äôs just niche.&lt;lb/&gt;And you probably don‚Äôt need it.&lt;/p&gt;
    &lt;p&gt;Especially if your architecture already solved the problem it was designed for.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46264704</guid><pubDate>Sun, 14 Dec 2025 17:13:30 +0000</pubDate></item><item><title>Sacrificing accessibility for not getting web scraped</title><link>https://tilschuenemann.de/projects/sacrificing-accessibility-for-not-getting-web-scraped</link><description>&lt;doc fingerprint="c090b716b91a65c5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;sacrificing accessibility for not getting web-scraped&lt;/head&gt;
    &lt;p&gt;ffGE ARrj XRejm XAj bZgui cB R EXZgl, Rmi mjji jrjg-DmygjREDmI XgRDmDmI iRXR XZ DlkgZrj. tZkBgDIAX uRbE IjX cgZejm, yZmXjmX IjXE RIIgjEEDrjuB EygRkji, Rmi jrjm XAZQIA BZQ lDIAX ARrj ijujXji BZQg ZgDIDmRu bZge, DX lDIAX MQEX EAZb Qk cjyRQEj DX IZX yRyAji Zg RgyADrji RX EZlj kZDmX.&lt;/p&gt;
    &lt;p&gt;OZb, DH BZQ EQcEygDcj XZ XAj DijR XARX BZQg yZmXjmX EAZQuim'X cj QEji HZg XgRDmDmI, BZQ iZm'X ARrj lQyA ERB. P bZmijgji AZb P kjgEZmRuuB bZQui lDXDIRXj XADE Zm R XjyAmDyRu ujrju.&lt;/p&gt;
    &lt;head rend="h2"&gt;et tu, caesar?&lt;/head&gt;
    &lt;p&gt;Pm lB uDmjRg RuIjcgR yuREE bj iDEyQEEji XAj yRjERg yDkAjg[1] RE R EDlkuj jmygBkXDZm RuIZgDXAl: KrjgB yARgRyXjg IjXE EADHXji cB m yARgRyXjgE. PH BZQ emZb (Zg IQjEE) XAj EADHX, BZQ yRm HDIQgj ZQX XAj ZgDIDmRu XjzX. ngQXj HZgyj Zg yARgRyXjg AjQgDEXDyE cgjRe XADE jREDuB.&lt;/p&gt;
    &lt;p&gt;nQX bj yRm RkkuB XADE EQcEXDXQXDZm lZgj IjmjgRuuB XZ R HZmX! w HZmX yZmXRDmE R ylRk (yARgRyXjg lRk), bADyA lRkE yZijkZDmXE Rmi IuBkAE. w yZijkZDmX ijHDmjE XAj yARgRyXjg, Zg yZlkujz EBlcZu, Rmi XAj IuBkA gjkgjEjmXE XAj rDEQRu EARkj. pj EygRlcuj XAj HZmX¬¥E yZijkZDmX-IuBkA-lRkkDmI, Rmi RiMQEX XAj XjzX bDXA XAj DmrjgEj ZH XAj EygRlcuj, EZ DX EXRBE DmXRyX HZg ZQg gjRijgE. PX iDEkuRBE yZggjyXuB, cQX XAj DmEkjyXji (Zg EygRkji) xSGf EXRBE EygRlcuji. SAjZgjXDyRuuB, BZQ yZQui RkkuB R iDHHjgjmX EygRlcuj XZ jRyA gjsQjEX.&lt;/p&gt;
    &lt;p&gt;SADE bZgeE RE uZmI RE EygRkjgE iZm'X QEj UtV HZg ARmiuDmI jiIj yREjE uDej XADE, cQX P iZm'X XADme DX bZQui cj HjREDcuj.&lt;/p&gt;
    &lt;p&gt;P RuEZ XjEXji DH tARXqoS yZQui ijyZij R yDkAjgXjzX DH P'i Xjuu DX XARX R EQcEXDXQXDZm yDkAjg bRE QEji, Rmi RHXjg EZlj cRye Rmi HZgXA, DX IRrj lj XAj gjEQuX: Umj iRB wuDyj bjmX iZbm R gRccDX AZuj, Rmi HZQmi AjgEjuH Dm pZmijguRmi, R EXgRmIj Rmi lRIDyRu kuRyj HDuuji bDXA...&lt;/p&gt;
    &lt;p&gt;...bADyA HQmmDuB iDim'X gjEjlcuj XAj ZgDIDmRu XjzX RX Ruu! SADE lDIAX ARrj ARkkjmji iQj XZ XAj XgRDmDmI yZgkQE yZmXRDmDmI wuDyj Rmi nZc[2] RE EXRmiRgi kRgXB uRcjuE HZg EAZbyREDmI jmygBkXDZm.&lt;/p&gt;
    &lt;p&gt;
      &lt;head&gt;SAj yZij P QEji HZg XjEXDmI: (yuDye XZ jzkRmi)&lt;/head&gt;
      &lt;code&gt;# /// script
# requires-python = "&amp;gt;=3.12"
# dependencies = [
#     "bs4",
#     "fonttools",
# ]
# ///
import random
import string
from typing import Dict

from bs4 import BeautifulSoup
from fontTools.ttLib import TTFont


def scramble_font(seed: int = 1234) -&amp;gt; Dict[str, str]:
	random.seed(seed)
	font = TTFont("src/fonts/Mulish-Regular.ttf")

	# Pick a Unicode cmap (Windows BMP preferred)
	cmap_table = None
	for table in font["cmap"].tables:
		if table.isUnicode() and table.platformID == 3:
			break
	cmap_table = table

	cmap = cmap_table.cmap

	# Filter codepoints for a-z and A-Z
	codepoints = [cp for cp in cmap.keys() if chr(cp) in string.ascii_letters]
	glyphs = [cmap[cp] for cp in codepoints]

	shuffled_glyphs = glyphs[:]
	random.shuffle(shuffled_glyphs)

	# Create new mapping
	scrambled_cmap = dict(zip(codepoints, shuffled_glyphs, strict=True))
	cmap_table.cmap = scrambled_cmap

	translation_mapping = {}
	for original_cp, original_glyph in zip(codepoints, glyphs, strict=True):
		for new_cp, new_glyph in scrambled_cmap.items():
			if new_glyph == original_glyph:
				translation_mapping[chr(original_cp)] = chr(new_cp)
				break

	font.save("src/fonts/Mulish-Regular-scrambled.ttf")

	return translation_mapping


def scramble_html(
	input: str,
	translation_mapping: Dict[str, str],
) -&amp;gt; str:
	def apply_cipher(text):
		repl = "".join(translation_mapping.get(c, c) for c in text)
		return repl

	# Read HTML file
	soup = BeautifulSoup(input, "html.parser")

	# Find all main elements
	main_elements = soup.find_all("main")
	skip_tags = {"code", "h1", "h2"}

	# Apply cipher only to text within main
	for main in main_elements:
		for elem in main.find_all(string=True):
			if elem.parent.name not in skip_tags:
				elem.replace_with(apply_cipher(elem))

	return str(soup)
            &lt;/code&gt;
    &lt;/p&gt;
    &lt;head rend="h2"&gt;drawbacks&lt;/head&gt;
    &lt;p&gt;SAjgj DE mZ Hgjj uQmyA, Rmi XADE ljXAZi yZljE bDXA lRMZg igRbcRyeE:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;yZkB-kREXj IjXE cgZejm&lt;/item&gt;
      &lt;item&gt;RyyjEEDcDuDXB HZg Eygjjm gjRijgE Zg mZm-IgRkADyRu cgZbEjgE uDej b3l DE IZmj&lt;/item&gt;
      &lt;item&gt;BZQg EjRgyA gRme bDuu igZk&lt;/item&gt;
      &lt;item&gt;HZmX-ejgmDmI yZQui IjX ljEEji Qk (DH BZQ Rgj mZX QEDmI R lZmZEkRyj HZmX)&lt;/item&gt;
      &lt;item&gt;kgZcRcuB lZgj&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46264955</guid><pubDate>Sun, 14 Dec 2025 17:38:12 +0000</pubDate></item><item><title>The Typeframe PX-88 Portable Computing System</title><link>https://www.typeframe.net/</link><description>&lt;doc fingerprint="7dd3de2a3599a586"&gt;
  &lt;main&gt;
    &lt;p&gt;The Typeframe PX-88 Portable Computing System&lt;/p&gt;
    &lt;p&gt;It's true. The odds are finally in your favor. &lt;lb/&gt;The Typeframe PX-88 is an integrated system that has been perfectly arranged to guarantee a superior outcome for the operator. Leave it to Typeframe to integrate these critical elements into one commanding machine.&lt;/p&gt;
    &lt;p&gt;The PX-88 delivers all the power and specialized features expected from a professional system - but built around a dedicated, uncompromising user experience. Is it a cyberdeck or a writerdeck? It's whatever you need it to be. The reliable Raspberry Pi 4 B core handles demanding web-based editors and complex tasks with robust performance. The compact size belies the strength within.&lt;/p&gt;
    &lt;p&gt;A mechanical keyboard provides a superior, tactile input experience - a professional tool unmatched by common consumer electronics. Furthermore, the system is designed for simple construction with minimal required soldering, and maintenance is streamlined - all internal components are easily reached via sliding access panels.&lt;/p&gt;
    &lt;p&gt;If you have been looking for a portable, professional computer where input quality meets core performance, look at the PX-88.&lt;/p&gt;
    &lt;p&gt;Typeframe. Built for your best work, built by you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46265015</guid><pubDate>Sun, 14 Dec 2025 17:43:39 +0000</pubDate></item></channel></rss>