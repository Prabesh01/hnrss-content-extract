<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 29 Oct 2025 07:11:17 +0000</lastBuildDate><item><title>Nvidia takes $1B stake in Nokia</title><link>https://www.cnbc.com/2025/10/28/nvidia-nokia-ai.html</link><description>&lt;doc fingerprint="ea9297f929ae83ae"&gt;
  &lt;main&gt;
    &lt;p&gt;Nokia announced on Tuesday that Nvidia is taking a $1 billion stake in the networking company, the latest partnership for the artificial intelligence chipmaker.&lt;/p&gt;
    &lt;p&gt;Shares of Nokia soared 22% higher following the news.&lt;/p&gt;
    &lt;p&gt;Nokia will issue more than 166 million new shares and will use the proceeds to fund its plans for AI and other general corporate purposes.&lt;/p&gt;
    &lt;p&gt;The two companies also struck a strategic partnership to work together to develop next-generation 6G cellular technology. Nokia said that it would adapt its 5G and 6G software to run on Nvidia's chips, and will collaborate on networking technology for AI.&lt;/p&gt;
    &lt;p&gt;Nokia said Nvidia would consider incorporating its technology into its future AI infrastructure plans.&lt;/p&gt;
    &lt;p&gt;Nokia, a Finnish company, is best known for its early cellphones, but in recent years, it has primarily been a supplier of 5G cellular equipment to telecom providers.&lt;/p&gt;
    &lt;p&gt;The announcement comes as Nvidia CEO Jensen Huang prepares to address an audience of policymakers and government leaders in Washington, D.C., to keynote the company's developer conference.&lt;/p&gt;
    &lt;p&gt;Nokia and Nvidia are expected to discuss some of their collaborations and plans at the conference.&lt;/p&gt;
    &lt;p&gt;Nvidia has taken several equity stakes in strategic partners in recent months as the company has found itself at the center of the AI world.&lt;/p&gt;
    &lt;p&gt;In September, it committed a $5 billion investment to one-time rival Intel, and said it would invest $100 billion in OpenAI. It also committed $500 million in self-driving car startup Wayve and a $667 million investment in U.K. cloud provider Nscale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45734486</guid><pubDate>Tue, 28 Oct 2025 15:53:52 +0000</pubDate></item><item><title>Using AI to negotiate a $195k hospital bill down to $33k</title><link>https://www.threads.com/@nthmonkey/post/DQVdAD1gHhw</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45734582</guid><pubDate>Tue, 28 Oct 2025 15:58:58 +0000</pubDate></item><item><title>The decline of deviance</title><link>https://www.experimental-history.com/p/the-decline-of-deviance</link><description>&lt;doc fingerprint="7e60911f0da285fd"&gt;
  &lt;main&gt;&lt;p&gt;People are less weird than they used to be. That might sound odd, but data from every sector of society is pointing strongly in the same direction: we’re in a recession of mischief, a crisis of conventionality, and an epidemic of the mundane. Deviance is on the decline.&lt;/p&gt;&lt;p&gt;I’m not the first to notice something strange going on—or, really, the lack of something strange going on. But so far, I think, each person has only pointed to a piece of the phenomenon. As a result, most of them have concluded that these trends are:&lt;/p&gt;&lt;p&gt;a) very recent, and therefore likely caused by the internet, when in fact most of them began long before&lt;/p&gt;&lt;p&gt;b) restricted to one segment of society (art, science, business), when in fact this is a culture-wide phenomenon, and&lt;/p&gt;&lt;p&gt;c) purely bad, when in fact they’re a mix of positive and negative.&lt;/p&gt;&lt;p&gt;When you put all the data together, you see a stark shift in society that is on the one hand miraculous, fantastic, worthy of a ticker-tape parade. And a shift that is, on the other hand, dismal, depressing, and in need of immediate intervention. Looking at these epoch-making events also suggests, I think, that they may all share a single cause.&lt;/p&gt;&lt;head rend="h1"&gt;1. THE DISAPPEARING MISCREANTS&lt;/head&gt;&lt;p&gt;Let’s start where the data is clear, comprehensive, and overlooked: compared to their parents and grandparents, teens today are a bunch of goody-two-shoes. For instance, high school students are less than half as likely to drink alcohol as they were in the 1990s:&lt;/p&gt;&lt;p&gt;They’re also less likely to smoke, have sex, or get in a fight, less likely to abuse painkillers, and less likely to do meth, ecstasy, hallucinogens, inhalants, and heroin. (Don’t kids vape now instead of smoking? No: vaping also declined from 2015 to 2023.) Weed peaked in the late 90s, when almost 50% of high schoolers reported that they had toked up at least once. Now that number is down to 30%. Kids these days are even more likely to use their seatbelts.&lt;/p&gt;&lt;p&gt;Surprisingly, they’re also less likely to bring a gun to school:&lt;/p&gt;&lt;p&gt;All of those findings rely on surveys, so maybe more and more kids are lying to us every year? Well, it’s pretty hard to lie about having a baby, and teenage pregnancy has also plummeted since the early 1990s:&lt;/p&gt;&lt;head rend="h1"&gt;2. THE GROWN-UPS HAVE FALLEN IN LINE&lt;/head&gt;&lt;p&gt;Adults are also acting out less than they used to. For instance, crime rates have fallen by half in the past thirty years:&lt;/p&gt;&lt;p&gt;Here’s some similar data from Northern Ireland on “anti-social behavior incidents”, because they happened to track those:&lt;/p&gt;&lt;p&gt;Serial killing, too, is on the decline:&lt;/p&gt;&lt;p&gt;Another disappearing form of deviance: people don’t seem to be joining cults anymore. Philip Jenkins, a historian of religion and author of a book on cults, reports that “compared to the 1970s, the cult issue has vanished almost entirely”.1 (Given that an increase in cults would be better for Jenkins’ book sales, I’m inclined to trust him on this one.) There is no comprehensive dataset on cult formation, but&lt;/p&gt;analyzed cults that have been covered on a popular and long-running podcast and found that most of them started in the 60s, 70s, and 80s, with a steep dropoff after 20002:&lt;p&gt;Crimes and cults are definitely deviant, and they appear to be on the decline. That’s good. But here’s where things get surprising: neutral and positive forms of deviance also seem to be getting rarer. For example—&lt;/p&gt;&lt;head rend="h1"&gt;3. PEOPLE ARE STAYING PUT&lt;/head&gt;&lt;p&gt;Moving away from home isn’t necessarily good or bad, but it is kinda weird. Ditching your hometown usually means leaving behind your family and friends, the institutions you understand, the culture you know, and perhaps even the language you speak. You have to be a bit of a misfit to do such a thing in the first place, and becoming a stranger makes you even stranger.&lt;/p&gt;&lt;p&gt;I always figured that every generation of Americans is more likely to move than the last. People used to be born and die in the same zip code; now they ping-pong across the country, even the whole world.&lt;/p&gt;&lt;p&gt;I was totally wrong about this. Americans have been getting less and less likely to move since the mid-1980s:&lt;/p&gt;&lt;p&gt;This effect is mainly driven by young people:&lt;/p&gt;&lt;p&gt;These days, “the typical adult lives only 18 miles from his or her mother“.&lt;/p&gt;&lt;head rend="h1"&gt;4. CULTURE IS STAGNATING&lt;/head&gt;&lt;p&gt;Creativity is just deviance put to good use. It, too, seems to be decreasing.&lt;/p&gt;&lt;p&gt;A few years ago, I analyzed a bunch of data and found that all popular forms of art had become “oligopolies”: fewer and fewer of the artists and franchises own more and more of the market. Before 2000, for instance, only about 25% of top-grossing movies were prequels, sequels, spinoffs, etc. Now it’s 75%.&lt;/p&gt;&lt;p&gt;The story is the same in TV, music, video games, and books—all of them have been oligpol-ized. As&lt;/p&gt;points out, we’re still reading comic books about superheroes that were invented in the 1960s, buying tickets to Broadway shows that premiered decades ago, and listening to the same music that our parents and grandparents listened to.&lt;p&gt;You see less variance even when you look only at the new stuff. According to analyses by The Pudding, popular music today is now more homogenous and has more repetitive lyrics than ever.&lt;/p&gt;&lt;p&gt;Also, the cover of every novel now looks like this:&lt;/p&gt;&lt;p&gt;But wait, shouldn’t we be drowning in new, groundbreaking art? Every day, people post ~100,000 songs to Spotify and upload 3.7 million videos to YouTube.3 Even accounting for Sturgeon’s Law (“90% of everything is crap”), that should still be more good stuff than anyone could appreciate in a lifetime. And yet professional art critics are complaining that culture has come to a standstill. According to The New York Times Magazine,&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We are now almost a quarter of the way through what looks likely to go down in history as the least innovative, least transformative, least pioneering century for culture since the invention of the printing press.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h1"&gt;5. THE INTERNET AIN’T INTERESTING ANYMORE&lt;/head&gt;&lt;p&gt;Remember when the internet looked like this?&lt;/p&gt;&lt;p&gt;That era is long gone. Take a stroll through the Web Design Museum and you’ll immediately notice two things:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Every site has converged on the same look: sleek, minimalist design elements with lots of pictures&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Website aesthetics changed a lot from the 90s to the 2000s and the 2010s, but haven’t changed much from the 2010s to now&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;A few examples:&lt;/p&gt;&lt;p&gt;This same kind of homogenization has happened on the parts of the internet that users create themselves. Every MySpace page was a disastrous hodgepodge; every Facebook profile is identical except for the pictures. On TikTok and Instagram, every influencer sounds the same4. On YouTube, every video thumbnail looks like it came out of one single content factory:&lt;/p&gt;&lt;p&gt;No doubt, the internet is still basically a creepy tube that extrudes a new weird thing every day: Trollface, the Momo Challenge, skibidi toilet. But notice that the raw materials for many of these memes is often decades old: superheroes (1930s-1970s), Star Wars (1977), Mario (1981), Pokémon (1996), Spongebob Squarepants (1999), Pepe the Frog (2005), Angry Birds (2009), Minions (2010), Minecraft (2011). Remember ten years ago, when people found a German movie that has a long sequence of Hitler shouting about something, and they started changing the subtitles to make Hitler complain about different things? Well, they’re still doing that.&lt;/p&gt;&lt;head rend="h1"&gt;6. THERE’S LESS TEXTURE IN ARCHITECTURE&lt;/head&gt;&lt;p&gt;The physical world, too, looks increasingly same-y. As Alex Murrell has documented5, every cafe in the world now has the same bourgeois boho style:&lt;/p&gt;&lt;p&gt;Every new apartment building looks like this:&lt;/p&gt;&lt;p&gt;The journalist Kyle Chayka has documented how every AirBnB now looks the same. And even super-wealthy mega-corporations work out of offices that look like this:&lt;/p&gt;&lt;p&gt;People usually assume that we don’t make interesting, ornate buildings anymore because it got too expensive to pay a bunch of artisans to carve designs into stone and wood.6 But the researcher Samuel Hughes argues that the supply-side story doesn’t hold up: many of the architectural flourishes that look like they have to be done by hand can, in fact, be done cheaply by machine, often with technology that we’ve had for a while. We’re still capable of making interesting buildings—we just choose not to.&lt;/p&gt;&lt;head rend="h1"&gt;7. BRANDS? MORE LIKE...BLANDS. HEH YEAH, GOT ‘EM&lt;/head&gt;&lt;p&gt;Brands seem to be converging on the same kind of logo: no images, only words written in a sans serif font that kinda looks like Futura.7&lt;/p&gt;&lt;p&gt;An analysis of branded twitter accounts found that they increasingly sound alike:&lt;/p&gt;&lt;p&gt;Most cars are now black, silver, gray, or white8:&lt;/p&gt;&lt;p&gt;When a British consortium of science museums analyzed the color of their artifacts over time, they found a similar, steady uptick in black, gray, and white:&lt;/p&gt;&lt;head rend="h1"&gt;8. SCIENCE IS STUCK&lt;/head&gt;&lt;p&gt;Science requires deviant thinking. So it’s no wonder that, as we see a decline in deviance everywhere else, we’re also seeing a decline in the rate of scientific progress. New ideas are less and less likely to displace old ideas, experts rate newer discoveries as less impressive than older discoveries, and we’re making fewer major innovations per person than we did 50 years ago.&lt;/p&gt;&lt;p&gt;You can spot this scientific bland-ification right away when you read older scientific writing. As&lt;/p&gt;(same guy who did the cult analysis) points out, scientific papers used to have style. Now they all sound the same, and they’re all boring. Essentially 100% of articles in medical journals, for instance, now use the same format (introduction, methods, results, and discussion):&lt;p&gt;This isn’t just an aesthetic shift. Standardizing your writing also standardizes your thinking—I know from firsthand experience that it’s hard to say anything interesting in a scientific paper.&lt;/p&gt;&lt;p&gt;Whenever I read biographies of famous scientists, I notice that a) they’re all pretty weird, and b) I don’t know anyone like them today, at least not in academia. I’ve met some odd people at universities, to be sure, but most of them end up leaving, a phenomenon the biologist&lt;/p&gt;calls “the flight of the Weird Nerd from academia”. The people who remain may be super smart, but they’re unlikely to rock the boat.&lt;head rend="h1"&gt;9. THESE COMPLAINTS ARE NEW&lt;/head&gt;&lt;p&gt;Whenever you notice some trend in society, especially a gloomy one, you should ask yourself: “Did previous generations complain about the exact same things?” If the answer is yes, you might have discovered an aspect of human psychology, rather than an aspect of human culture.&lt;/p&gt;&lt;p&gt;I’ve spent a long time studying people’s complaints from the past, and while I’ve seen plenty of gripes about how culture has become stupid, I haven’t seen many people complaining that it’s become stagnant.9 In fact, you can find lots of people in the past worrying that there’s too much new stuff. As&lt;/p&gt;relates, one hundred years ago, people were having nervous breakdowns about the pace of technological change. They were rioting at Stravinsky’s Rite of Spring and decrying the new approaches of artists like Kandinsky and Picasso. In 1965, Susan Sontag wrote that new forms of art “succeed one another so rapidly as to seem to give their audiences no breathing space to prepare”. Is there anyone who feels that way now?&lt;p&gt;Likewise, previous generations were very upset about all the moral boundaries that people were breaking, i.e.:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;In olden days, a glimpse of stocking&lt;/p&gt;&lt;p&gt;Was looked on as something shocking&lt;/p&gt;&lt;p&gt;But now, God knows&lt;/p&gt;&lt;p&gt;Anything goes&lt;/p&gt;&lt;p&gt;-Cole Porter, 1934&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Back then, as far as I can tell, nobody was encouraging young Americans to party more. Now they do. So as far as I can tell, the decline of deviance is not just a perennial complaint. People worrying about their culture being dominated by old stuff—that’s new.&lt;/p&gt;&lt;head rend="h1"&gt;COUNTERPOINT: AM I WRONG?&lt;/head&gt;&lt;p&gt;That’s the evidence for a decline in deviance. Let’s see the best evidence against.&lt;/p&gt;&lt;p&gt;As I’ve been collecting data for this post over the past 18 months or so, I’ve been trying to counteract my confirmation bias by keeping an eye out for opposing trends. I haven’t found many—so maybe that’s my bias at work—but here they are.&lt;/p&gt;&lt;p&gt;First, unlike other forms of violence, mass shootings have become more common since the 90s (although notice the Y-axis, we’re talking about an extremely small subset of all crime):&lt;/p&gt;&lt;p&gt;Baby names have gotten a lot more unique:&lt;/p&gt;&lt;p&gt;And when you look at timelines of fashion, you certainly see a lot more change from the 1960s to the 2010s than you do from the 1860s to 1910s:&lt;/p&gt;&lt;p&gt;That at least hints the decline of deviance isn’t a monotonic, centuries-long trend. And indeed, lots of the data we have suggest that things started getting more homogenous somewhere between the 1980s and 2000s.&lt;/p&gt;&lt;p&gt;There are a few people who disagree at least with parts of the cultural stagnation hypothesis. Literature Substacker&lt;/p&gt;reports that “literature is booming”, and music Substacker is skeptical about stagnation in his industry. The internet ethnographer Katherine Dee argues10 that the most interesting art is happening in domains we don’t yet consider “art”, like social media personalities, TikTok sketch comedy, and Pinterest mood boards. I’m sure there’s some truth to all of this, but I’m also pretty sure it’s not enough to cancel out the massive trends we see everywhere else.&lt;p&gt;Maybe I’m missing all the new and exciting things because I’m just not cool and plugged in enough? After all, I’ll be the first to tell you there’s a lot of writing on Substack (and the blogosphere more generally) that’s very good and very idiosyncratic—just look at the winners of my blog competitions this year and last year. But I only know about that stuff because I read tons of blogs. If I was as deep into YouTube or podcasts, maybe I’d see the same thing there too, and maybe I’d change my tune.&lt;/p&gt;&lt;p&gt;Anyway, I know that it’s easy to perceive a trend when there isn’t any (see: The Illusion of Moral Decline, You’re Probably Wrong About How Things Have Changed). There’s no way of randomly sampling all of society and objectively measuring its deviance over time. The data we don’t have might contradict the data we do have. But it would have to be a lot of data, and it would all have to point in the opposite direction.&lt;/p&gt;&lt;p&gt;It really does seem like we’re experiencing a decline of deviance, so what’s driving it? Any major social trend is going to have lots of causes, but I think one in particular deserves most of the credit and the blame:&lt;/p&gt;&lt;head rend="h1"&gt;WE CARE MORE ABOUT BEING ALIVE&lt;/head&gt;&lt;p&gt;Life is worth more now. Not morally, but literally. This fact alone can, I think, go a long way toward explaining why our weirdness is waning.&lt;/p&gt;&lt;p&gt;When federal agencies do cost-benefit analyses, they have to figure out how much a human life is worth. (Otherwise, how do you know if it’s worth building, say, a new interstate that will help millions get to work on time but might cause some excess deaths due to air pollution?) They do this by asking people how much they would be willing to pay to reduce their risk of dying, which they then use to calculate the “value of a statistical life”. According to an analysis by the Substacker&lt;/p&gt;, those statistical lives have gotten a lot more valuable over time:&lt;p&gt;There are, I suspect, two reasons we hold onto life more dearly now. First: we’re richer. Generations of economic development have put more cash in people’s pockets, and that makes them more willing to pay to de-risk their lives—both because they can afford it, and because the life they’re insuring is going to be more pleasant. But as Linch points out, the value of a statistical life has increased faster than GDP, so that can’t be the whole story.&lt;/p&gt;&lt;p&gt;Second: life is a lot less dangerous than it used to be. If you have a nontrivial risk of dying from polio, smallpox, snake bites, tainted water, raids from marauding bandits, literally slipping on a banana peel, and a million other things, would you really bother to wear your seatbelt? Once all those other dangers go away, though, doing 80mph in your Kia Sorento might suddenly become the riskiest part of your day, and you might consider buckling up for the occasion.&lt;/p&gt;&lt;p&gt;Our super-safe environments may fundamentally shift our psychology. When you’re born into a land of milk and honey, it makes sense to adopt what ecologists refer to as a “slow life history strategy”—instead of driving drunk and having unprotected sex, you go to Pilates and worry about your 401(k). People who are playing life on slow mode care a lot more about whether their lives end, and they care a lot more about whether their lives get ruined. Everything’s gotta last: your joints, your skin, and most importantly, your reputation. That makes it way less enticing to screw around, lest you screw up the rest of your time on Earth.&lt;/p&gt;&lt;p&gt;(“What is it you plan to do with your one wild and precious life?” Make sure I stand up from my desk chair every 20-30 minutes!)&lt;/p&gt;&lt;p&gt;I think about it this way: both of my grandfathers died in their 60s, which was basically on track with their life expectancy the year they were born. I’m sure they hoped to live much longer than that, but they knew they might not make it to their first Social Security check. Imagine how you differently you might live if you thought you were going to die at 65 rather than 95. And those 65 years weren’t easy, especially at the beginning: they were born during the Depression, and one of them grew up without electricity or indoor plumbing.&lt;/p&gt;&lt;p&gt;Plus, both of my grandpas were drafted to fight in the Korean War, which couldn’t have surprised them much—the same thing had happened to their parents’ generation in the 1940s and their grandparents’ generation in the 1910s. When you can reasonably expect your government to ship you off to the other side of the world to shoot people and be shot at in return, you just can’t be so precious about your life.11&lt;/p&gt;&lt;p&gt;My life is nothing like theirs was. Nobody has ever asked me to shoot anybody. I’ve got a big-screen TV. I could get sushi delivered to my house in 30 minutes. The Social Security Administration thinks I might make it to 80. Why would I risk all this? The things my grandparents did casually—smoking, hitching a ride in the back of a pickup truck, postponing medical treatment until absolutely necessary—all of those feel unthinkable to me now.12 I have a miniature heart attack just looking at the kinds of playgrounds they had back then:&lt;/p&gt;&lt;p&gt;I know life doesn’t feel particularly easy, safe, or comfortable. What about climate change, nuclear war, authoritarianism, income inequality, etc.? Dangers and disadvantages still abound, no doubt. But look, 100 years ago, you could die from a splinter. We just don’t live in that world anymore, and some part of us picks that up and behaves accordingly.&lt;/p&gt;&lt;p&gt;In fact, adopting a slow life strategy doesn’t have to be a conscious act, and probably isn’t. Like most mental operations, it works better if you can’t consciously muck it up. It operates in the background, nudging each decision toward the safer option. Those choices compound over time, constraining the trajectory of your life like bumpers on a bowling lane. Eventually this cycle becomes self-reinforcing, because divergent thinking comes from divergent living, and vice versa.13&lt;/p&gt;&lt;p&gt;This is, I think, how we end up in our very normie world. You start out following the rules, then you never stop, then you forget that it’s possible to break the rules in the first place. Most rule-breaking is bad, but some of it is necessary. We seem to have lost both kinds at the same time.14&lt;/p&gt;&lt;head rend="h1"&gt;THE STATUE OF LIMITATIONS&lt;/head&gt;&lt;p&gt;The sculptor Arturo di Modica ran away from his home in Sicily to go study art in Florence. He later immigrated to the US, working as a mechanic and a hospital technician to support himself while he did his art. Eventually he saved up enough to buy a dilapidated building in lower Manhattan, which he tore it down so he could illegally build his own studio—including two sub-basements—by hand, becoming an underground artist in the literal sense. He refused to work with an art dealer until 2012, when he was in his 70s. His most famous work, the Charging Bull statue that now lives on Wall Street, was deposited there without permission or payment; it was originally impounded before public outcry caused the city to put it back. Di Modica didn’t mean it as an avatar of capitalism—the stock market had tanked in 1987, and he intended the bull to symbolize resilience and self-reliance:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;My point was to show people that if you want to do something in a moment things are very bad, you can do it. You can do it by yourself. My point was that you must be strong.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Meanwhile, “Fearless Girl”, the statue of a girl standing defiantly with her hands on her hips that was installed in front of the bull in 2017, was commissioned by an investment company to promote a new index fund.&lt;/p&gt;&lt;p&gt;Who would live di Modica’s life now? Every step was inadvisable: don’t run away from home, don’t study art, definitely don’t study sculpture, don’t dig your own basement, don’t dump your art on the street! Even if someone was crazy enough to pull a di Modica today, who could? The art school would force you to return home to your parents, the real estate would be unaffordable, the city would shut you down.&lt;/p&gt;&lt;p&gt;The decline of deviance is mainly a good thing. Our lives have gotten longer, safer, healthier, and richer. But the rise of mass prosperity and disappearance of everyday dangers has also made trivial risks seem terrifying. So as we tame every frontier of human life, we have to find a way to keep the good kinds of weirdness alive. We need new institutions, new eddies and corners and tucked-away spaces where strange things can grow.&lt;/p&gt;&lt;p&gt;All of this is within our power, but we must decide to do it. For the first time in history, weirdness is a choice. And it’s a hard one, because we have more to lose than ever. If we want a more interesting future, if we want art that excites us and science that enlightens us, then we’ll have to tolerate a few illegal holes in the basement, and somebody will have to be brave enough to climb down into them.&lt;/p&gt;&lt;p&gt;I’d love to read a version of Robert Putnam’s Bowling Alone specifically about the death of cults. Drawing Pentagrams Alone?&lt;/p&gt;&lt;p&gt;Whenever I tell people about the cult deficit, they offer two counterarguments. First: “Isn’t SoulCycle a cult? Isn’t Taylor Swift fandom a cult? Aren’t, like, Lububus a cult?” I think this is an example of prevalence-induced concept change: now that there are fewer cults, we’re applying the “cult” label to more and more things that are less and less cult-y. If your spin class required you to sell all your possessions, leave your family behind, and get married to the instructor, that would be a cult.&lt;/p&gt;&lt;p&gt;Second: “Aren’t conspiracy theories way more popular now? Maybe people are satisfying all of their cult urges from the comforts of their own home, kinda like how people started going to church on Zoom during the pandemic.” It’s a reasonable hypothesis, but the evidence speaks against it. A team of researchers tracked 37 conspiracy beliefs over time, and found no change in the percentage of people who believe them. Nor did they find any increase or decrease in the number of people who endorse domain-general tinfoil-hat thinking, like “Much of our lives are being controlled by plots hatched in secret places”. It seems that hardcore cultists have become an endangered species, while more pedestrian conspiracy theorists are merely as prevalent as they ever were.&lt;/p&gt;&lt;p&gt;There is some skepticism about the Spotify numbers, and I’m sure the YouTube numbers are dubious as well—a big chunk of that content has to be spam, duplicates, etc. But reduce those amounts even by 90% and you still have an impossible amount of songs and videos.&lt;/p&gt;&lt;p&gt;According to&lt;/p&gt;, the Instagram accent is what you end up with when you optimize your speaking for attracting and holding people’s attention. For more insights like that one, check out his new book.&lt;p&gt;Or maybe the conspiracy theorists are right and it’s because some kind of apocalypse wiped out our architectural knowledge and the elites are keeping it hushed up.&lt;/p&gt;&lt;p&gt;Cracker Barrel tried to do the same thing recently and was hounded so hard on the internet that they brought their old logo back.&lt;/p&gt;&lt;p&gt;Note that this data comes from Poland, but if you look up images of American parking lots in previous decades vs. today, you’ll find the same thing.&lt;/p&gt;&lt;p&gt;For instance, T.S. Eliot, 1949:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We can assert with some confidence that our own period is one of decline; that the standards of culture are lower than they were fifty years ago; and that the evidences of this decline are visible in every department of human activity.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Dee’s original article is now paywalled, so I’m linking to a summary of her argument.&lt;/p&gt;&lt;p&gt;Almost all the data I’ve shown you is from the US, so I’m interested to hear what’s going on in other parts of the world. My prediction is that development at first causes a spike in idiosyncrasy as people gain more ways to express themselves—for example, all cars are going to look the same when the only one people can afford is a Model T, and then things get more interesting when more competitors emerge. But as the costs of being weird increase, you’ll eventually see a decline in deviance. That’s my guess, anyway.&lt;/p&gt;&lt;p&gt;Fast life strategies are still possible today, but they’re rarer. Once, in high school, I was over at a friend’s house and his mom lit a cigarette in the living room. I must have looked shocked, because she shrugged at me and said, “If the cigarettes don’t kill me, something else will.” I could at least understand where she was coming from: her husband had died in a car accident before he even turned 50. When you feel like your ticket could get punched at any time, why not enjoy yourself?&lt;/p&gt;&lt;p&gt;Perhaps that’s also why we’ve become so concerned about the safety of our children, when previous generations were much more laissez-faire. This map traces how the changes in a single family, but the pattern seems broadly true:&lt;/p&gt;&lt;p&gt;There’s a paradox here: shouldn’t safer, wealthier lives make us more courageous? Like, can’t you afford to take more risks when you have more money in the bank?&lt;/p&gt;&lt;p&gt;Yes, but you won’t want to. I saw this happen in real time when I was a resident advisor: getting an elite degree ought to increase a student’s options, but instead it makes them too afraid to choose all but a few of those options. Fifty percent of Harvard graduates go to work in finance, tech, and consulting. Most of them choose those careers not because they love making PowerPoints or squeezing an extra three cents of profit out of every Uber ride, but because those jobs are safe, lucrative, and prestigious—working at McKinsey means you won’t have to be embarrassed when you return for your five-year reunion. All of these kids dreamed of what they would gain by going to an Ivy League school; none of them realized it would give them something to lose.&lt;/p&gt;&lt;p&gt;In fact, the richest students are the most likely to pick the safest careers:&lt;/p&gt;&lt;p&gt;Also c’mon this chart is literally made by someone named Kingdollar.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45734620</guid><pubDate>Tue, 28 Oct 2025 16:01:00 +0000</pubDate></item><item><title>I've been loving Claude Code on the web</title><link>https://ben.page/claude-code-web</link><description>&lt;doc fingerprint="772919bc3fb5947f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I’ve been loving Claude Code on the Web&lt;/head&gt;
    &lt;p&gt;This week, I’ve been voraciously using Claude Code on the web.&lt;/p&gt;
    &lt;p&gt;It’s very much a “v1” product. You type a prompt to start a new thread, it launches a little container for your agent to work in, and you can keep talking to it. It produces a branch, which you can open a PR for (that’s the only way to see a diff of the changes Claude Code made, for now). Or if you want to keep working locally, you can copy a &lt;code&gt;claude --teleport &amp;lt;uuid&amp;gt;&lt;/code&gt; command that brings the branch down onto your computer and continues the same thread with Claude Code locally.&lt;/p&gt;
    &lt;p&gt;Something about this early product is really great. I’ve been using it as a “to-do list that does itself” — when I think of something small that I want to tweak, across a variety of projects (work, work-related side project, side project, open source project) I just throw it into a thread. Then I come back, sometimes later in the day and sometimes days later, to see what Claude did and to finish things up.&lt;/p&gt;
    &lt;p&gt;It’s also available in the Claude iOS app, which has been great. When I’m walking and have a thought for something I want to know more about (for example, “What screens could be impacted by this change that I should test more thoroughly?”), I can just ask and know that the answer will be there for me when I come back.&lt;/p&gt;
    &lt;p&gt;Cursor built this same thing 4 months ago. I’ve tried their version a few times since, and I’ve never liked this much. Why?&lt;/p&gt;
    &lt;p&gt;I had trouble nailing down an answer, but I think the answer is actually just product quality. Cursor’s implementation is a bit finicky, loading states a bit jumpy, and things feel fragile. The font’s too small too, in my opinion.&lt;/p&gt;
    &lt;p&gt;Claude Code on the web feels very solid and dependable. And for some reason, that’s made the difference for me this week.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45735264</guid><pubDate>Tue, 28 Oct 2025 16:46:57 +0000</pubDate></item><item><title>Fil-C: A memory-safe C implementation</title><link>https://lwn.net/SubscriberLink/1042938/658ade3768dd4758/</link><description>&lt;doc fingerprint="b67b189d94a636d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fil-C: A memory-safe C implementation&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt; Fil-C is a memory-safe implementation of C and C++ that aims to let C code — complete with pointer arithmetic, unions, and other features that are often cited as a problem for memory-safe languages — run safely, unmodified. Its dedication to being "&lt;quote&gt;fanatically compatible&lt;/quote&gt;" makes it an attractive choice for retrofitting memory-safety into existing applications. Despite the project's relative youth and single active contributor, Fil-C is capable of compiling an entire memory-safe Linux user space (based on Linux From Scratch), albeit with some modifications to the more complex programs. It also features memory-safe signal handling and a concurrent garbage collector. &lt;/p&gt;
    &lt;p&gt;Fil-C is a fork of Clang; it's available under an Apache v2.0 license with LLVM exceptions for the runtime. Changes from the upstream compiler are occasionally merged in, with Fil-C currently being based on version 20.1.8 from July 2025. The project is a personal passion of Filip Pizlo, who has previously worked on the runtimes of a number of managed languages, including Java and JavaScript. When he first began the project, he was not sure that it was even possible. The initial implementation was prohibitively slow to run, since it needed to insert a lot of different safety checks. This has given Fil-C reputation for slowness. Since the initial implementation proved viable, however, Pizlo has managed to optimize a number of common cases, making Fil-C-generated code only a few times slower than Clang-generated code, although the exact slowdown depends heavily on the structure of the benchmarked program.&lt;/p&gt;
    &lt;p&gt;Reliable benchmarking is notoriously finicky, but in order to get some rough feel for whether that level of performance impact would be problematic, I compiled Bash version 5.2.32 with Fil-C and tried using it as my shell. Bash is nearly a best case for Fil-C, because it spends more time running external programs than running its own code, but I still expected the performance difference to be noticeable. It wasn't. So, at least for some programs, the performance overhead of Fil-C does not seem to be a problem in practice.&lt;/p&gt;
    &lt;p&gt;In order to support its various run-time safety checks, Fil-C does use a different internal ABI than Clang does. As a result, objects compiled with Fil-C won't link correctly against objects generated by other compilers. Since Fil-C is a full implementation of C and C++ at the source-code level, however, in practice this just requires everything to be recompiled with Fil-C. Inter-language linking, such as with Rust, is not currently supported by the project.&lt;/p&gt;
    &lt;head rend="h4"&gt;Capabilities&lt;/head&gt;
    &lt;p&gt;The major challenge of rendering C memory-safe is, of course, pointer handling. This is especially complicated by the fact that, as the long road to CHERI-compatibility has shown, many programs expect a pointer to be 32 or 64 bits, depending on the architecture. Fil-C has tried several different ways to represent pointers since the project's beginning in 2023. Fil-C's first pointers were 256 bits, not thread-safe, and didn't protect against use-after-free bugs. The current implementation, called "InvisiCaps", allows for pointers that appear to match the natural pointer size of the architecture (although this requires storing some auxiliary information elsewhere), with full support for concurrency and catching use-after-free bugs, at the expense of some run-time overhead.&lt;/p&gt;
    &lt;p&gt;Fil-C's documentation compares InvisiCaps to a software implementation of CHERI: pointers are separated into a trusted "capability" piece and an untrusted "address" piece. Since Fil-C controls how the program is compiled, it can ensure that the program doesn't have direct access to the capabilities of any pointers, and therefore the runtime can rely on them being uncorrupted. The tricky part of the implementation comes from how these two pieces of information are stored in what looks to the program like 64 bits.&lt;/p&gt;
    &lt;p&gt;When Fil-C allocates an object on the heap, it adds two metadata words before the start of the allocated object: an upper bound, used to check accesses to the object based on its size, and an "aux word" that is used to store additional pointer metadata. When the program first writes a pointer value into an object, the runtime allocates a new auxiliary allocation of the same size as the object being written into, and puts an actual hardware-level pointer (i.e., one without an attached capability) to the new allocation into the aux word of the object. This auxiliary allocation, which is invisible to the program being compiled, is used to store the associated capability information for the pointer being stored (and is also reused for any additional pointers stored into the object later). The address value is stored into the object as normal, so any C bit-twiddling techniques that require looking at the stored value of the pointer work as expected.&lt;/p&gt;
    &lt;p&gt;This approach does mean that structures that contain pointers end up using twice as much memory, and every load of a pointer involves a pointer indirection through the aux word. In practice, the documentation claims that the performance overhead of this approach for most programs makes them run about four times more slowly, although that number depends on how heavily the program makes use of pointers. Still, he has ideas for several optimizations that he hopes can bring the performance overhead down over time.&lt;/p&gt;
    &lt;p&gt;One wrinkle with this approach is atomic access to pointers — i.e. using _Atomic or volatile. Luckily, there is no problem that cannot be solved with more pointer indirection: when the program loads or stores a pointer value atomically, instead of having the auxiliary allocation contain the capability information directly, it points to a third 128-bit allocation that stores the capability and pointer value together. That allocation can be updated with 128-bit atomic instructions, if the platform supports them, or by creating new allocations and atomically swapping the pointers to them.&lt;/p&gt;
    &lt;p&gt;Since the aux word is used to store a pointer value, Fil-C can use pointer tagging to store some additional information there as well; that is used to indicate special types of objects that need to be handled differently, such as functions, threads, and mmap()-backed allocations. It's also used to mark freed objects, so that any access results in an error message and a crash.&lt;/p&gt;
    &lt;head rend="h4"&gt;Memory management&lt;/head&gt;
    &lt;p&gt;When an object is freed, its aux word marks it as a free object, which lets the auxiliary allocation be reclaimed immediately. The original object can't be freed immediately, however. Otherwise, a program could free an object, allocate a new object in the same location, and thereby cover up use-after-free bugs. Instead, Fil-C uses a garbage collector to free an object's backing memory only once all of the pointers to it go away. Unlike other garbage collectors for C — such as the Boehm-Demers-Weiser garbage collector — Fil-C can use the auxiliary capability information to track live objects precisely.&lt;/p&gt;
    &lt;p&gt;Fil-C's garbage collector is both parallel (collection happens faster the more cores are available) and concurrent (collection happens without pausing the program). Technically, the garbage collector does require threads to occasionally pause just long enough to tell it where pointers are located on the stack, but that only occurs at special "safe points" — otherwise, the program can load and manipulate pointers without notifying the garbage collector. Safe points are used as a synchronization barrier: the collector can't know that an object is really garbage until every thread has passed at least one safe point since it finished marking. This synchronization is done with atomic instructions, however, so in practice threads never need to pause for longer than a few instructions.&lt;/p&gt;
    &lt;p&gt;The exception is the implementation of fork(), which uses the safe points needed by the garbage collector to temporarily pause all of the threads in the program in order to prevent race conditions while forking. Fil-C inserts a safe point at every backward control-flow edge, i.e., whenever code could execute in a loop. In the common case, the inserted code just needs to load a flag register and confirm that the garbage collector has not requested anything be done. If the garbage collector does have a request for the thread, the thread runs a callback to perform the needed synchronization.&lt;/p&gt;
    &lt;p&gt;Fil-C uses the same safe-point mechanism to implement signal handling. Signal handlers are only run when the interrupted thread reaches a safe point. That, in turn, allows signal handlers to allocate and free memory without interfering with the garbage collector's operation; Fil-C's malloc() is signal-safe.&lt;/p&gt;
    &lt;head rend="h4"&gt;Memory-safe Linux&lt;/head&gt;
    &lt;p&gt;Linux From Scratch (LFS) is a tutorial on compiling one's own complete Linux user space. It walks through the steps of compiling and installing all of the core software needed for a typical Linux user space in a chroot() environment. Pizlo has successfully run through LFS with Fil-C to produce a memory-safe version, although a non-Fil-C compiler is still needed to build some fundamental components, such as Fil-C's own runtime, the GNU C library, and the kernel. (While Fil-C's runtime relies on a normal copy of the GNU C library to make system calls, the programs that Fil-C compiles use a Fil-C-compiled version of the library.)&lt;/p&gt;
    &lt;p&gt;The process is mostly identical to LFS up through the end of chapter 7, because everything prior to that point consists of using cross-build tools to obtain a working compiler in the chroot() environment. The one difference is that the cross-build tools are built with a different configured prefix, so that they won't conflict with Fil-C. At that point, one can build a copy of Fil-C and use it to mostly replace the existing compiler. The remaining steps of LFS are unchanged.&lt;/p&gt;
    &lt;p&gt;Scripts to automate the process are included in the Fil-C Git repository, including some steps from Beyond Linux From Scratch that result in a working graphical user interface and a handful of more complicated applications such as Emacs.&lt;/p&gt;
    &lt;p&gt;Overall, Fil-C offers a remarkably complete solution for making existing C programs memory-safe. While it does nothing for undefined behavior that is not related to memory safety, the most pernicious and difficult-to-prevent security vulnerabilities in C programs tend to rely on exploiting memory-unsafe behavior. Readers who have already considered and rejected Fil-C for their use case due to its early performance problems may wish to take a second look — although anyone hoping for stability might want to wait for others to take the plunge, given the project's relative immaturity. That said, for existing applications where a sizeable performance hit is preferable to an exploitable vulnerability, Fil-C is an excellent choice.&lt;/p&gt;
    &lt;p&gt; Posted Oct 28, 2025 17:54 UTC (Tue) by tialaramex (subscriber, #21167) [Link] (4 responses) For users in a bunch of cases this is a no brainer, Daroc gave their shell as an example but I'm sure most of us run many programs every day where raw perf just isn't a big deal. I am interested in programmers rather than users because I think that influences whether Fil-C is just an interesting project for our moment or it becomes a "successor" to C in a way that Zig, Odin etc. never could. Posted Oct 28, 2025 18:15 UTC (Tue) by rahulsundaram (subscriber, #21946) [Link] (1 responses) My expectation is that there isn't going to be a single successor to C. For some group of people, that was C++ a long time back. For others it is going to Rust or Zig or something else. For the final group they are going to keep coding in C forever and it will more of a generational change eventually. Posted Oct 28, 2025 18:32 UTC (Tue) by daroc (editor, #160859) [Link] Posted Oct 28, 2025 18:34 UTC (Tue) by rweikusat2 (subscriber, #117920) [Link] (1 responses) Posted Oct 29, 2025 1:35 UTC (Wed) by rahulsundaram (subscriber, #21946) [Link] Posted Oct 28, 2025 19:55 UTC (Tue) by oldnpastit (subscriber, #95303) [Link] (5 responses) Posted Oct 28, 2025 20:28 UTC (Tue) by bertschingert (subscriber, #160729) [Link] But it would seem to be more robust than ASAN; from reading about how ASAN works, it seems that it puts "poisoned" bytes around an allocation, so that memory accesses shortly after the end of a buffer hit those poisoned bytes and are caught. However, ASAN wouldn't catch an invalid access to a non-poisoned address of memory via a particular a pointer, if that address was allocated in a separate allocation. [1] I assume Fil-C's pointer capability model is able to catch "provenance" violations like that. [1] https://blog.gistre.epita.fr/posts/benjamin.peter-2022-10... Posted Oct 28, 2025 20:33 UTC (Tue) by excors (subscriber, #95769) [Link] (3 responses) &amp;gt; Fil-C is engineered to prevent memory safety bugs from being used for exploitation rather than just simply flagging them often enough to find bugs. This makes Fil-C different from AddressSanitizer, HWAsan, or MTE, which can all be bypassed by attackers. The key difference that makes this possible is that Fil-C is capability based (so each pointer knows what range of memory it may access, and how it may access it) rather than tag based (where pointer accesses are allowed if they hit valid memory). Clang says "AddressSanitizer's runtime was not developed with security-sensitive constraints in mind and may compromise the security of the resulting executable", so it should not be used in production. Valgrind has much worse performance (the manual claims 10-50x slowdown, plus it's effectively single-threaded), which is probably bad enough to make it unusable in production, and similarly will miss many memory safety bugs. Posted Oct 28, 2025 21:26 UTC (Tue) by cyperpunks (subscriber, #39406) [Link] (2 responses) Posted Oct 29, 2025 4:57 UTC (Wed) by Cyberax (✭ supporter ✭, #52523) [Link] (1 responses) It's somewhat analogous to compiling C into WebAssembly and then JIT-compiling WebAssembly. The amazing thing is that it preserves most of C/C++ semantics. Posted Oct 29, 2025 5:53 UTC (Wed) by willmo (subscriber, #82093) [Link] But (at least WRT memory safety) only the semantics of the abstract machine described by the language standards, and not the additional semantics (aka undefined behavior) of the straightforward mappings to typical hardware that we’re all accustomed to. Very cool idea. :-) Posted Oct 29, 2025 6:18 UTC (Wed) by epa (subscriber, #39769) [Link] (1 responses) The unsafe-compiled code wouldn’t be exactly the same as you get from plain clang, as the memory layout is different, and it might be a bit slower because of that, but it could do without checks of pointer capability checking and, perhaps, other checks like overflow and array bounds. The rest of the program must assume that the unsafe code is correct. Posted Oct 29, 2025 7:01 UTC (Wed) by magfr (subscriber, #16052) [Link] &lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;Mixing safe and unsafe &lt;/head&gt;&lt;head&gt;Mixing safe and unsafe &lt;/head&gt;&lt;lb/&gt; The compiler defines a new ABI and you can't link system ABI libs with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45735877</guid><pubDate>Tue, 28 Oct 2025 17:25:04 +0000</pubDate></item><item><title>What we talk about when we talk about sideloading</title><link>https://f-droid.org/2025/10/28/sideloading.html</link><description>&lt;doc fingerprint="3f229112d91610f9"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;What We Talk About When We Talk About Sideloading&lt;/head&gt;Posted on Oct 28, 2025 by marcprux&lt;p&gt;We recently published a blog post with our reaction to the new Google Developer Program and how it impacts your freedom to use the devices that you own in the ways that you want. The post garnered quite a lot of feedback and interest from the community and press, as well as various civil society groups and regulatory agencies.&lt;/p&gt;&lt;p&gt;In this post, I hope to clarify and expand on some of the points and rebut some of the counter-messaging that we have witnessed.&lt;/p&gt;&lt;head rend="h3"&gt;Googleâs message that âSideloading is Not Going Awayâ is clear, concise, and false&lt;/head&gt;&lt;p&gt;Shortly after our post was published, Google aired an episode of their Android Developers Roundtable series, where they state unequivocally that âsideloading isnât going anywhereâ. They follow-up with a blog post:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Does this mean sideloading is going away on Android? Absolutely not. Sideloading is fundamental to Android and it is not going away.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This statement is untrue. The developer verification decree effectively ends the ability for individuals to choose what software they run on the devices they own.&lt;/p&gt;&lt;p&gt;It bears reminding that âsideloadâ is a made-up term. Putting software on your computer is simply called âinstallingâ, regardless of whether that computer is in your pocket or on your desk. This could perhaps be further precised as âdirect installingâ, in case you need to make a distinction between obtaining software the old-fashioned way versus going through a rent-seeking intermediary marketplace like the Google Play Store or the Apple App Store.&lt;/p&gt;&lt;p&gt;Regardless, the term âsideloadâ was coined to insinuate that there is something dark and sinister about the process, as if the user were making an end-run around safeguards that are designed to keep you protected and secure. But if we reluctantly accept that âsideloadingâ is a term that has wriggled its way into common parlance, then we should at least use a consistent definition for it. Wikipediaâs summary definition is:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;the transfer of apps from web sources that are not vendor-approved&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;By this definition, Googleâs statement that âsideloading is not going awayâ is simply false. The vendor â Google, in the case of Android certified devices â will, in point of fact, be approving the source. The supplicant app developer must register with Google, pay a fee, provide government identification, agree to non-negotiable (and ever-changing) terms and conditions, enumerate all their current and future application identifiers, upload evidence of their private signing key, and then hope and wait for Googleâs approval.&lt;/p&gt;&lt;head rend="h3"&gt;What this means for your rights&lt;/head&gt;&lt;p&gt;You, the consumer, purchased your Android device believing in Googleâs promise that it was an open computing platform and that you could run whatever software you choose on it. Instead, starting next year, they will be non-consensually pushing an update to your operating system that irrevocably blocks this right and leaves you at the mercy of their judgement over what software you are permitted to trust.&lt;/p&gt;&lt;p&gt;You, the creator, can no longer develop an app and share it directly with your friends, family, and community without first seeking Googleâs approval. The promise of Android â and a marketing advantage it has used to distinguish itself against the iPhone â has always been that it is âopenâ. But Google clearly feels that they have enough of a lock on the Android ecosystem, along with sufficient regulatory capture, that they can now jettison this principle with prejudice and impunity.&lt;/p&gt;&lt;p&gt;You, the state, are ceding the rights of your citizens and your own digital sovereignty to a company with a track record of complying with the extrajudicial demands of authoritarian regimes to remove perfectly legal apps that they happen to dislike. The software that is critical to the running of your businesses and governments will be at the mercy of the opaque whims of a distant and unaccountable corporation. Monocultures are perilous not just in agriculture, but in software distribution as well.&lt;/p&gt;&lt;p&gt;As a reminder, this applies not just to devices that exclusively use the Google Play Store: this is for every Android Certified device everywhere in the world, which encompasses over 95% of all Android devices outside of China. Regardless of whether the device owner prefers to use a competing app store like the Samsung Galaxy Store or the Epic Games Store, or a free and open-source app repository like F-Droid, they will be captive to the overarching policies unilaterally dictated by a competing corporate entity.&lt;/p&gt;&lt;head rend="h3"&gt;The place of greater safety&lt;/head&gt;&lt;p&gt;In promoting their developer registration program, Google purports:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Our recent analysis found over 50 times more malware from internet-sideloaded sources than on apps available through Google Play.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We havenât seen this recent analysis â or any other supporting evidence â but the â50 timesâ multiple does certainly sound like great cause for distress (even if it is a surprisingly round number). But given the recent news of â224 malicious apps removed from the Google Play Store after ad fraud campaign discoveredâ, we are left to wonder whether their energies might better be spent assessing and improving their own safeguards rather than casting vague disparagements against the software development communities that thrive outside their walled garden.&lt;/p&gt;&lt;p&gt;In addition, other recent news of over 19 million downloads of malware from the Play Store leads us to question whether the sole judgement of a single corporate entity can be trusted to identify and assess malware, especially when that judgement is clouded by commercial incentives that may not align with the well-being of their users.&lt;/p&gt;&lt;head rend="h3"&gt;What can be done?&lt;/head&gt;&lt;p&gt;Google has been facing public outcry against their heavy-handed policies for a long time, but this trend has accelerated recently. Last year they crippled ad-blockers in Chrome and Chromium-based browsers by forcing through their unpopular âmanifest v3â requirement for plugins, and earlier this year they closed off the development of the Android Open Source Project (AOSP), which is how they were able to clandestinely implement the verification infrastructure that enforces their developer registration decree.&lt;/p&gt;&lt;p&gt;Developer verification is an existential threat to free software distribution platforms like F-Droid as well as emergent commercial competitors to the Play Store. We are witnessing a groundswell of opposition to this attempt from both our user and developer communities, as well as the tech press and civil society groups, but public policymakers still need to be educated about the threat.&lt;/p&gt;&lt;p&gt;To learn more about what you can do as a consumer, visit keepandroidopen.org for information on how to contact your representative agencies and advocate for keeping the Android ecosystem open for consumers and competition.&lt;/p&gt;&lt;p&gt;If you are an app developer, we recommend against signing yourself up for Googleâs developer registration program at this time. We unequivocally reject their attempt to force this program upon the world.&lt;/p&gt;&lt;p&gt;Over half of all humankind uses an Android smartphone. Google does not own your phone. You own your phone. You have the right to decide who to trust, and where you can get your software from.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45736479</guid><pubDate>Tue, 28 Oct 2025 18:02:36 +0000</pubDate></item><item><title>HTTPS by default</title><link>https://security.googleblog.com/2025/10/https-by-default.html</link><description>&lt;doc fingerprint="a85815123b32f74a"&gt;
  &lt;main&gt;
    &lt;p&gt;One year from now, with the release of Chrome 154 in October 2026, we will change the default settings of Chrome to enable “Always Use Secure Connections”. This means Chrome will ask for the user's permission before the first access to any public site without HTTPS.&lt;/p&gt;
    &lt;p&gt;The “Always Use Secure Connections” setting warns users before accessing a site without HTTPS&lt;/p&gt;
    &lt;p&gt;Chrome Security's mission is to make it safe to click on links. Part of being safe means ensuring that when a user types a URL or clicks on a link, the browser ends up where the user intended. When links don't use HTTPS, an attacker can hijack the navigation and force Chrome users to load arbitrary, attacker-controlled resources, and expose the user to malware, targeted exploitation, or social engineering attacks. Attacks like this are not hypothetical—software to hijack navigations is readily available and attackers have previously used insecure HTTP to compromise user devices in a targeted attack.&lt;/p&gt;
    &lt;p&gt;Since attackers only need a single insecure navigation, they don't need to worry that many sites have adopted HTTPS—any single HTTP navigation may offer a foothold. What's worse, many plaintext HTTP connections today are entirely invisible to users, as HTTP sites may immediately redirect to HTTPS sites. That gives users no opportunity to see Chrome's "Not Secure" URL bar warnings after the risk has occurred, and no opportunity to keep themselves safe in the first place.&lt;/p&gt;
    &lt;p&gt;To address this risk, we launched the “Always Use Secure Connections” setting in 2022 as an opt-in option. In this mode, Chrome attempts every connection over HTTPS, and shows a bypassable warning to the user if HTTPS is unavailable. We also previously discussed our intent to move towards HTTPS by default. We now think the time has come to enable “Always Use Secure Connections” for all users by default.&lt;/p&gt;
    &lt;p&gt;For more than a decade, Google has published the HTTPS transparency report, which tracks the percentage of navigations in Chrome that use HTTPS. For the first several years of the report, numbers saw an impressive climb, starting at around 30-45% in 2015, and ending up around the 95-99% range around 2020. Since then, progress has largely plateaued.&lt;/p&gt;
    &lt;p&gt;HTTPS adoption expressed as a percentage of main frame page loads&lt;/p&gt;
    &lt;p&gt;This rise represents a tremendous improvement to the security of the web, and demonstrates that HTTPS is now mature and widespread. This level of adoption is what makes it possible to consider stronger mitigations against the remaining insecure HTTP.&lt;/p&gt;
    &lt;p&gt;While it may at first seem that 95% HTTPS means that the problem is mostly solved, the truth is that a few percentage points of HTTP navigations is still a lot of navigations. Since HTTP navigations remain a regular occurrence for most Chrome users, a naive approach to warning on all HTTP navigations would be quite disruptive. At the same time, as the plateau demonstrates, doing nothing would allow this risk to persist indefinitely. To balance these risks, we have taken steps to ensure that we can help the web move towards safer defaults, while limiting the potential annoyance warnings will cause to users.&lt;/p&gt;
    &lt;p&gt;One way we're balancing risks to users is by making sure Chrome does not warn about the same sites excessively. In all variants of the "Always Use Secure Connections" settings, so long as the user regularly visits an insecure site, Chrome will not warn the user about that site repeatedly. This means that rather than warn users about 1 out of 50 navigations, Chrome will only warn users when they visit a new (or not recently visited) site without using HTTPS.&lt;/p&gt;
    &lt;p&gt;To further address the issue, it's important to understand what sort of traffic is still using HTTP. The largest contributor to insecure HTTP by far, and the largest contributor to variation across platforms, is insecure navigations to private sites. The graph above includes both those to public sites, such as example.com, and navigations to private sites, such as local IP addresses like 192.168.0.1, single-label hostnames, and shortlinks like intranet/. While it is free and easy to get an HTTPS certificate that is trusted by Chrome for a public site, acquiring an HTTPS certificate for a private site unfortunately remains complicated. This is because private names are "non-unique"—private names can refer to different hosts on different networks. There is no single owner of 192.168.0.1 for a certification authority to validate and issue a certificate to.&lt;/p&gt;
    &lt;p&gt;example.com&lt;/p&gt;
    &lt;p&gt;192.168.0.1&lt;/p&gt;
    &lt;p&gt;intranet/&lt;/p&gt;
    &lt;p&gt;HTTP navigations to private sites can still be risky, but are typically less dangerous than their public site counterparts because there are fewer ways for an attacker to take advantage of these HTTP navigations. HTTP on private sites can only be abused by an attacker also on your local network, like on your home wifi or in a corporate network.&lt;/p&gt;
    &lt;p&gt;If you exclude navigations to private sites, then the distribution becomes much tighter across platforms. In particular, Linux jumps from 84% HTTPS to nearly 97% HTTPS when limiting the analysis to public sites only. Windows increases from 95% to 98% HTTPS, and both Android and Mac increase to over 99% HTTPS.&lt;/p&gt;
    &lt;p&gt;In recognition of the reduced risk HTTP to private sites represents, last year we introduced a variant of “Always Use Secure Connections” for public sites only. For users who frequently access private sites (such as those in enterprise settings, or web developers), excluding warnings on private sites significantly reduces the volume of warnings those users will see. Simultaneously, for users who do not access private sites frequently, this mode introduces only a small reduction in protection. This is the variant we intend to enable for all users next year.&lt;/p&gt;
    &lt;p&gt;“Always Use Secure Connections,” available at chrome://settings/security&lt;/p&gt;
    &lt;p&gt;In Chrome 141, we experimented with enabling “Always Use Secure Connections” for public sites by default for a small percentage of users. We wanted to validate our expectations that this setting keeps users safer without burdening them with excessive warnings.&lt;/p&gt;
    &lt;p&gt;Analyzing the data from the experiment, we confirmed that the number of warnings seen by any users is considerably lower than 3% of navigations—in fact, the median user sees fewer than one warning per week, and the ninety-fifth percentile user sees fewer than three warnings per week..&lt;/p&gt;
    &lt;p&gt;Once “Always Use Secure Connections” is the default and additional sites migrate away from HTTP, we expect the actual warning volume to be even lower than it is now. In parallel to our experiments, we have reached out to a number of companies responsible for the most HTTP navigations, and expect that they will be able to migrate away from HTTP before the change in Chrome 154. For many of these organizations, transitioning to HTTPS isn't disproportionately hard, but simply has not received attention. For example, many of these sites use HTTP only for navigations that immediately redirect to HTTPS sites—an insecure interaction which was previously completely invisible to users.&lt;/p&gt;
    &lt;p&gt;Another current use case for HTTP is to avoid mixed content blocking when accessing devices on the local network. Private addresses, as discussed above, often do not have trusted HTTPS certificates, due to the difficulties of validating ownership of a non-unique name. This means most local network traffic is over HTTP, and cannot be initiated from an HTTPS page—the HTTP traffic counts as insecure mixed content, and is blocked. One common use case for needing to access the local network is to configure a local network device, e.g. the manufacturer might host a configuration portal at config.example.com, which then sends requests to a local device to configure it.&lt;/p&gt;
    &lt;p&gt;config.example.com&lt;/p&gt;
    &lt;p&gt;Previously, these types of pages needed to be hosted without HTTPS to avoid mixed content blocking. However, we recently introduced a local network access permission, which both prevents sites from accessing the user’s local network without consent, but also allows an HTTPS site to bypass mixed content checks for the local network once the permission has been granted. This can unblock migrating these domains to HTTPS.&lt;/p&gt;
    &lt;p&gt;We will enable the "Always Use Secure Connections" setting in its public-sites variant by default in October 2026, with the release of Chrome 154. Prior to enabling it by default for all users, in Chrome 147, releasing in April 2026, we will enable Always Use Secure Connections in its public-sites variant for the over 1 billion users who have opted-in to Enhanced Safe Browsing protections in Chrome.&lt;/p&gt;
    &lt;p&gt;While it is our hope and expectation that this transition will be relatively painless for most users, users will still be able to disable the warnings by disabling the "Always Use Secure Connections" setting.&lt;/p&gt;
    &lt;p&gt;If you are a website developer or IT professional, and you have users who may be impacted by this feature, we very strongly recommend enabling the "Always Use Secure Connections" setting today to help identify sites that you may need to work to migrate. IT professionals may find it useful to read our additional resources to better understand the circumstances where warnings will be shown, how to mitigate them, and how organizations that manage Chrome clients (like enterprises or educational institutions) can ensure that Chrome shows the right warnings to meet those organizations' needs.&lt;/p&gt;
    &lt;p&gt;While we believe that warning on insecure public sites represents a significant step forward for the security of the web, there is still more work to be done. In the future, we hope to work to further reduce barriers to adoption of HTTPS, especially for local network sites. This work will hopefully enable even more robust HTTP protections down the road.&lt;/p&gt;
    &lt;p&gt;Post a Comment&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45736499</guid><pubDate>Tue, 28 Oct 2025 18:04:00 +0000</pubDate></item><item><title>Mapping the off-target effects of every FDA-approved drug in existence</title><link>https://www.owlposting.com/p/mapping-the-off-target-effects-of</link><description>&lt;doc fingerprint="261910b338a32758"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mapping the off-target effects of every FDA-approved drug in existence (EvE Bio)&lt;/head&gt;
    &lt;head rend="h3"&gt;6.2k words, 29 minutes reading time&lt;/head&gt;
    &lt;p&gt;Note: Thank you to Bill Busa, CEO and co-founder of EvE Bio, for an extremely helpful discussion while working on this essay.&lt;/p&gt;
    &lt;p&gt;This essay is long, and I recognize that many people don’t necessarily care about the details. The real headline point you need to be aware of is this dataset, which was produced by EvE Bio underneath a CC-NA license, and is a comprehensive mapping of the interactions between a significant fraction of clinically important human cellular receptors and 1,600~ FDA-approved drugs. I strongly believe that this data is really, really useful, and more people should be aware it exists.&lt;/p&gt;
    &lt;p&gt;If you’d like to understand why I think it is useful, and what the dataset exactly contains, read on!&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;If you were to be a fly on the wall during the 1-6 years of preclinical drug discovery research within a pharmaceutical company, one observation you may walk away with is that, while the work is certainly complicated, it is also frighteningly limited in scope. What you’ll learn is that drugs are made by corporations that are optimizing for one primary thing, and one thing only: work. ‘Working’ is obviously contextual, but it is a simple concept no matter the situation: reduce a worrying biomarker, improve mood, lengthen lifespan and so on and so on. What does this discovery process ignore? Simply put: everything else a drug could do beyond that.&lt;/p&gt;
    &lt;p&gt;Yes, that’s a roundabout way of describing ‘off-target effects’ — defined as the action of a drug at a gene product other than the gene product it was intended to affect — but I think it’s a helpful intuition pump. Viewing the drug discovery process as ‘not paying attention to anything that is unrelated to the drug working’ is useful in that it contextualizes the situation we’re in. Drugs are meant to make money, and money is derived from drugs working. To spend time on understanding what else a particular drug does beyond It Working for its intended task is time lost and money lost.&lt;/p&gt;
    &lt;p&gt;One unfamiliar with the drug discovery process may find this bizarre; why wouldn’t the well meaning scientists in charge of developing drugs try to deeply understand how it interacts with the body? On the other hand, those deeply in the medical field would find this thesis so obvious that stating it is unnecessary; of course a pharmaceutical company would limit their scope of understanding a drug to things that lie between it working and not working. There’s only so much time and resources to go around. Priorities!&lt;/p&gt;
    &lt;p&gt;Of course, if an off-target effect comes between the drug and It Working, then certainly resources will be allocated to deal with it. But beyond that, mapping everything else a clinical-stage drug does — every receptor it unintentionally binds, every pathway it nudges sideways, every gene it perturbs slightly — is deemed so high effort and so low ROI, that it is relegated to hoping an academic will study it. Only if post-marketing surveillance turns up something worrying shall further exploration occur. Because, again, a deep understanding of what exactly an exogenous chemical is doing inside a body is not the point of the drug discovery process. Working is the point!&lt;/p&gt;
    &lt;p&gt;With that background context, I am ready to present three claims I’m going to make in this essay and spend the remaining sections trying to prove:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Understanding off-target effects is really useful.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Learning about off-target effects at scale is possible.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No for-profit institution has a strong incentive to do this work.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the moment, let’s accept that these three are indeed true, and we can put our skeptic hat back on at the end of this section.&lt;/p&gt;
    &lt;p&gt;The subject of today’s essay is EvE Bio, and why I think they are doing something incredible.&lt;/p&gt;
    &lt;p&gt;EvE is a bit unlike the typical startups I write about, because they aren’t really a startup. They are a FRO, or Focused Research Organization. Many reading this blog are likely already familiar with this recent renaissance of strange scientific organizations (something I’ve written about in the past) and already understand this acronym, but to those who don’t, this Venn diagram is quite instructive:&lt;/p&gt;
    &lt;p&gt;Entire essays can (and have been!) written about the intricacies of FRO’s, but this essay will ignore much of their organizational structure, since it isn’t super relevant to what EvE is doing.&lt;/p&gt;
    &lt;p&gt;So what is EvE doing? EvE Bio is a scientific non-profit that has a clear, singular mission: map the off-target effects of every FDA-approved drug in existence and share the data. The data will be released underneath a non-commercial, creative commons license — free to use by academics, and available for licensing for commercial entities. Once they accomplish this task, they close up shop or spin off into their own thing. And if they don’t do it within 5-6 years, the same end result still happens. They do have some future plans that may come into the picture with time, which I’ll cover at the end, but the bolded bit is their primary thesis!&lt;/p&gt;
    &lt;p&gt;So why are they doing this? How will they do it? And why hasn’t anyone else done it yet?&lt;/p&gt;
    &lt;head rend="h1"&gt;Why is understanding off-target effects important?&lt;/head&gt;
    &lt;p&gt;There is a lazy answer that could be given here: "because we want to know if potential side effects of a drug exist". This is partially correct, but I think it pays to be more specific. On EvE’s website, they list six reasons why off-target effects are worth studying:&lt;/p&gt;
    &lt;p&gt;Now, fairly, some of these are at least a little fluffy. Is the institution doing off-target mapping really going to be the ones developing the autonomous lab assays of the future? Maybe! But it feels like a third-order, fourth-order, or even further consequence of their main mission. Bill did mention to me that there are already promising results in that direction, such as better reporting cell lines, but still. I think it’s generally good to limit ones assessment of an institution based on what their first-order impact will be, and, there, I think there will be three distinct areas that EvE will service: drug repurposing, validation for machine-learning models, and to a weaker degree, polypharmacology.&lt;/p&gt;
    &lt;p&gt;What about industrial chemical profiling and pharmacology profiling? I think EvE will certainly be important there, but it’s a bit fuzzier. Industrial chemical profiling may occur in the future but isn’t part of the current cohort of FDA-approved drugs that EvE is focusing on, and there’s a similar problem for pharmacology profiling as there is for ML-for-toxicity (which I have written about before as being a challenging proposition).&lt;/p&gt;
    &lt;p&gt;But even if we take my somewhat pessimistic stance that only three of these six things are genuinely tractable in the short term, those areas alone are extremely valuable. Let’s go over them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Drug repurposing&lt;/head&gt;
    &lt;p&gt;I think it is under-appreciated just how rich the cohort of FDA-approved drugs that are out there. Consider the fact that basically all drugs start off with singular indications, meant to cure, alleviate, or address one thing. Yet, 30%~ of FDA-approved drugs gain a new post-approval indication, based on a study of the 197 drugs approved by the FDA from 1997-2020. Funnily enough, the same paper that came up with that 30% number almost treats it as a matter of disappointment, given that 38% of all prescriptions written in the US are off-label! This implies that there are, potentially, hundreds of drugs that are already being used beyond their original scope, just without the formal validation or regulatory blessing. Which, in turn, implies that we’re sitting on a vast, under-explored landscape of therapeutic potential, one that clinicians are already intuitively poking into, but which the formal system has barely begun to chart.&lt;/p&gt;
    &lt;p&gt;Now, I think some caution is warranted. This 38% number does vary from paper to paper, one other study claims off-label prescriptions are as low as 25%. If we’re being even more fair, it’s questionable exactly how proven-out these off-label indications are. One 2006 study claims that of the 21% of off-label prescriptions they found, 73% of them had little-to-no scientific support. Hard to tell whether this is because there simply are no studies, or because the off-label usage was actively disproved!&lt;/p&gt;
    &lt;p&gt;Consider gabapentin, one of the most egregious cases of off-label drug prescriptions. Typically, most people view gabapentin as the nerve injury drug, right? But it, in fact, was not originally approved for that, only for seizures! Yet, 95% of its prescriptions usage are for pain; nerve pain, low-back pain, post-operative pain, and so on. But while the gabapentin is indeed effective for some specific types of nerve pain (diabetic neuropathy), it is ineffective for many other types (e.g sciatica), as confirmed by follow-up studies by Pfizer.&lt;/p&gt;
    &lt;p&gt;Yet, prescriptions for these ineffective off-label usages continue.&lt;/p&gt;
    &lt;p&gt;But even if the true rate of valid, effective off-label use is lower than we’d like to imagine, the value of actually stumbling across a chance to repurpose a drug is high enough as to almost certainly still be worth it! Why? New chemical entities must follow the typical clinical phase progression timeline, whereas any repurposed drugs can skip preclinical, phase 1, and (sometimes) phase 2 trials as a result of their already-collected toxicity data. Billions of dollars and years of time could be saved!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…repurposed drugs are generally approved sooner (3–12 years) and at reduced (50–60%) cost (5, 6). In addition, while ~10% of new drug applications gain market approval, approximately 30% of repurposed drugs are approved, giving companies a market-driven incentive to repurpose existing assets (5)….&lt;/p&gt;
      &lt;p&gt;For example, repurposing of the emergency contraceptive, mifepristone, for Cushing’s syndrome required a cohort of less than 30 patients to test its efficacy, whereas a clinical trial1 for the same indication evaluating the safety and efficacy of a new chemical entity, levoketoconazole, required ~90 individuals (2, 3)…..&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But, as it stands today, most drug repurposing efforts are done somewhat blindly; haphazardly glancing through the literature, relying on anecdotal case reports, or waiting for some academic lab to publish a five-mouse study from 2013 that hints at a secondary use. In many ways, it isn’t too dissimilar to the usual drug-discovery process! Given how promising (and relatively limited) the list of FDA-approved drugs are, the simple act of a pre-triaged list of drug-target maps (EvE’s mission!) may be extraordinarily impactful.&lt;/p&gt;
    &lt;p&gt;In such a world where this data is easily accessible, perhaps an order of magnitude more energy would be devoted to repurposing efforts, maybe vastly improving the currently horrific finances of modern day drug discovery.&lt;/p&gt;
    &lt;p&gt;But as with all seeming free-lunches, there’s a reason drug repurposing hasn’t been aggressively exploited beyond a few cases: economics. Unlike novel drugs, which come with fresh patents and a full runway of exclusivity, repurposed drugs necessarily rely on compounds whose original patents have expired or are near expiration1. This limits the sponsor’s ability to recoup development costs, because generic competition can quickly erode any profits once the drug hits the market, even if it’s approved for a new use. There are mechanisms to extend exclusivity for repurposed indications — such as the 7-year exclusivity period given by the FDA’s Orphan Drug Act for treatments of rare diseases or the 3-year-exclusivity granted in cases where new clinical data was needed to repurpose a drug — but it is a risky enough bet that most companies will shy away from it.&lt;/p&gt;
    &lt;p&gt;But as EvE is a non-profit, the economics don’t need to make sense. They plan to periodically announce opportunities for repurposing to the world, in hopes that other well-meaning non-profits take it on or, if the evidence is sufficiently convincing, that doctors simply take it as a useful datapoint for deciding whether an off-label prescription may be useful. And if they do most of the legwork in identifying good candidates for repurposing, it may even make the economics worth it for for-profit entities to pursue further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Validation data for models&lt;/head&gt;
    &lt;p&gt;One of the easiest ways to assure yourself that what you’re doing is valuable is if people come up to you and ask if they could use whatever you’re producing. This is true in typical SaaS products, and it is true for the fruits of R&amp;amp;D work. But beyond assessing value outright, it also helps you learn what your work is most valuable for.&lt;/p&gt;
    &lt;p&gt;And, curiously, the primary area in which EvE has found ‘product market fit’ is in companies asking to use their data for internal model validation efforts. As I mentioned before, while EvE’s dataset is free-to-use by academics, it requires a commercial license to be used by any for-profit entity. And they are currently in discussions with 4 such commercial entities, all of whom desire to use EvE’s dataset to validate their machine-learning models predictions.&lt;/p&gt;
    &lt;p&gt;Historically, model builders in drug discovery have had to make do with whatever internal datasets they could get their hands on, which were typically limited in scope, biased toward certain classes of molecules, or simply not reproducible. Public data from sources like ChEMBL, BindingDB, or PubChem BioAssay are much larger in size, but they tend to be noisy, heterogeneous in experimental methodology, and always lack negative results. Worse, they’re often cherry-picked around success stories or clustered around well-studied targets, introducing systemic biases that hamper generalization. We need not look further than Pat Walter’s famous essay on the topic: We Need Better Benchmarks for Machine Learning in Drug Discovery, which expands on these issues even more.&lt;/p&gt;
    &lt;p&gt;This is an area of EvE’s work that I cannot personally shed much light on, and obviously, Bill cannot tell me the exact details on what the commercial entities are working on. But it was a surprising learning from our conversation that this particular topic is where public interest is most rapidly coalescing! Very excited to hear about more public statements they make in this area soon.&lt;/p&gt;
    &lt;head rend="h2"&gt;(Maybe) Polypharmacology&lt;/head&gt;
    &lt;p&gt;I do think this is the weakest, day-one value-add for EvE’s dataset. So take this section with a grain of salt! It just felt too interesting to not cover.&lt;/p&gt;
    &lt;p&gt;Polypharmacology is a drug discovery approach where a drug is designed to target multiple molecular targets, instead of a more traditional single-target approach. It’s not a particularly new idea, most clinically useful drugs exhibit multi-target activity whether they were designed that way or not. But what’s changed in the past decade is the intentionality.&lt;/p&gt;
    &lt;p&gt;I think there are a lot of different arguments for the value of polypharmacology, the easiest one hinging on efficacy. There’s a very interesting story that could be told here about drugs that worked better because they modulated the activity of multiple receptors in parallel. A great, recent example is that of drugs that followed Ozempic. Ozempic simply targeted GLP-1, which reduces appetite and slows digestion. But the second-generation (e.g. Zepbound) also targeted GIP, which amplifies insulin response and regulates lipid metabolism differently in adipose tissue. The effects were incredible: 13.7% weight loss with Ozempic, 20.2% weight loss with Zepbound over 48 weeks. Synergistic effects! The third generation (e.g. retatrutide) tacks on interactions with glucagon receptors — potentially increasing metabolic rate — with early phase 2 results looking once again promising.&lt;/p&gt;
    &lt;p&gt;But a more interesting place to start is the very similarly named concept of polypharmacy.&lt;/p&gt;
    &lt;p&gt;Polypharmacy refers to the clinical practice of prescribing multiple drugs simultaneously (usually 5+), typically to manage complex or co-occurring conditions. It’s common in geriatrics, psychiatry, oncology, and increasingly just about everywhere else in medicine: ~17% of all adults in the US meet the definition for polypharmacy. The logic is straightforward: most diseases aren’t governed by a single pathway, and so tackling them with a single drug is often insufficient. Instead, clinicians stack therapies: an ACE inhibitor for the blood pressure, a statin for the cholesterol, metformin for the glucose, a GLP-1 for the weight, and so on.&lt;/p&gt;
    &lt;p&gt;As you may expect, polypharmacy is awful on the patient's physiology. One study estimates that nearly 10% of hospital admissions among older adults are directly attributable to adverse drug events from polypharmacy-related side effects. The more drugs we stack onto people, the more unpredictable the net interaction becomes, because even if each one has been individually safety-tested, nobody tests all the pairwise combinations in a clinically realistic setting.&lt;/p&gt;
    &lt;p&gt;The solution may very well be to bundle things up.&lt;/p&gt;
    &lt;p&gt;Rather than throwing five separately optimized molecules at a patient and hoping for cooperative behavior, we could, in principle, design a single molecule that alone engages the same therapeutic targets. This, in turn, allows clinical trials to suss out the net effect of such a drug in a controlled, interpretable way. Which naturally leads us to the utility of polypharmacology; not necessarily because it will give us magic drugs with efficacy far better than current ones (though it may!), but rather that it will simply avoid us having to deal with the current issues that polypharmacy presents.&lt;/p&gt;
    &lt;p&gt;But the obvious question: does EvE’s dataset help with polypharmacology efforts? There isn’t any current, empirical proof of this, but I think it will. If you squint, you could see it functioning as missing infrastructure, a dataset that is necessary for rational polypharmacology to occur at scale. But this is necessarily tied up with machine-learning for chemical design accelerating, so, again, this is not necessarily something I’d expect EvE’s work to contribute to by the end of the year. But perhaps soon!&lt;/p&gt;
    &lt;head rend="h1"&gt;How do you understand off-target effects in a tractable way?&lt;/head&gt;
    &lt;p&gt;This all said, even if you agreed that the value proposition that EvE is claiming is real, you may struggle to verbalize exactly how you would understand the off-target effects of the 13,000~ FDA approved drugs out there. What assays would you use? How do you dose any given drug? How do you understand the translation of your assay to real-world settings?&lt;/p&gt;
    &lt;p&gt;Let’s walk through the EvE workflow.&lt;/p&gt;
    &lt;p&gt;First, you need to decide what drugs you're actually going to test. While there are technically around 13,000 FDA-approved drugs out there, many of them aren't particularly relevant for this kind of screening. You can immediately exclude things like topical medications, inhalants, radioisotopes, and simple nutrients, stuff that is known to be largely innocuous or not have much systemic impact. After this initial filtering, you end up with about 1,600 small molecule drugs that are worth investigating. But this number gets further whittled down further based on practical constraints; availability, cost, licensing requirements, etc.&lt;/p&gt;
    &lt;p&gt;From this, EvE ended up with a library of 1,397 compounds to screen.&lt;/p&gt;
    &lt;p&gt;Then comes the harder question: what exactly are you screening against? The human body has somewhere around 20,000 protein-coding genes, and there is an argument that any drug could interact with any of them. But perhaps we’d be too zealous to immediately do an (everything x everything) screen. Shouldn’t we try to do something that’s closer to the Pareto optimal frontier? What if we suspect that the vast majority of clinically meaningful drug interactions occur with a tiny subset of those 20,000 genes?&lt;/p&gt;
    &lt;p&gt;And, indeed, that turns out to be the case.&lt;/p&gt;
    &lt;p&gt;The vast majority of genes have some nominal physiological function, yes, but when it comes to drug interactions, only a minority are commonly targeted. At least a minority of classes: nuclear receptors (NRs) and 7-transmembrane receptors (also known as GPCRs). In total, there are about 800~ GPCRs and 48 NRs, but only 110 GPCR’s and 12-13 NR’s are actually targeted by drugs. Per last count, EvE has currently created data for 56 GPCR’s and 29 NR’s. Over the course of their existence, they plan to cover, in total, a select set of the 200 GPCR’s and all 48 NR’s. Why not all 800 GPCR’s? I attached that information in the footnotes.2&lt;/p&gt;
    &lt;p&gt;They hope to do much more than this too, but we’ll cover that in the last section.&lt;/p&gt;
    &lt;p&gt;Both NRs and GPCRs have some nice properties, but most pertinent to EvE, they are known to be very ‘druggable’ classes of drugs, given that the cell often uses them to convey information from the outside world, and evolution has therefore made their binding pockets unusually receptive to small molecules. GPCRs, sitting on the cell surface, are natural sensors for hormones, neurotransmitters, and other circulating ligands, many of which resemble or inspire drug scaffolds. NRs, meanwhile, act as intracellular switches that come with protected internal pockets meant to bind to estrogen, cortisol, and so on, making them ideal for selective small-molecule engagement. As a result, both are involved in a lot of important physiological processes.&lt;/p&gt;
    &lt;p&gt;This ‘physiological importance’ is useful in two ways! One, a plurality of drugs target the two — 13% of FDA-approved drugs target NR’s, with that number jumping to 35% for GPCR’s — so mapping the interactions here may give a clinically meaningful view of off-target effects. And two, given the extreme importance of GPCR’s and NR’s in modern-day drug development, there has been a fair bit of work in improving how we study their interactions with ligands of interest. As in, new assays outright shouldn’t need to be developed to study them.&lt;/p&gt;
    &lt;p&gt;Speaking of that, let’s start talking about how they are building this drug x receptor interaction map. They rely on two well-established assays which I’ll discuss here, but feel free to skip, understanding the two isn’t particularly important.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;TR-FRET-based co-factor recruitment assays for NRs&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;When a drug successfully activates an NR, it usually causes a conformational shift that allows the receptor to recruit a specific co-factor protein, exposing what is often called the ‘AF2 domain’. These co-factors tend to have little peptide motifs (like an LXXLL motif) that latch onto that domain.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;TF-FRET exploits this. A chemical is tagged onto the NR domain and a chemical is tagged onto the co-factor protein, both of which are fluorophores. If the FDA-approved drug is an agonist, you’ll see a spike of light appear as the two fluorophores interact.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tango β-arrestin recruitment assays for 7TMs/GPCRs&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Instead of recruiting co-factors inside the nucleus like NR’s, GPCR’s sit on the surface of a cell and transmit signals inward. As the name of the protein class implies, this involves utilizing G-proteins. Unfortunately, G-proteins are quite specific to their GPCR, so using them in our assay as a way to understand activation would be difficult to scale. Luckily, there is a nearly universal binding protein: β-arrestin. When a GPCR is activated by [something], their signaling process almost always involves binding to that protein.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;In the assay, the GPCR (attached to a cell surface) is engineered to have a built-in “trap”, a little molecular tag connected to a transcription factor. When β-arrestin is recruited, it brings along a protease that snips the tag, releasing the transcription factor. That transcription factor then moves into the nucleus and turns on a reporter gene, which encodes for the enzyme β-lactamase. Meanwhile, the cell is loaded with CCF4-AM, a fluorescent substrate that shifts its emission profile when cleaved by β-lactamase. The stronger the GPCR activation by a drug, the more β-lactamase is produced, the more substrate is cleaved, and the bigger the fluorescence shift. That shift, measured as a ratio between ‘starting’ and ‘ending’ wavelengths, serves as a readout of how strongly the receptor was activated.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Reasonably simple! One note: the explanations I gave above is for assessing the difference between an inactive drug and an agonist. For assessing inactive versus antagonist, a separate experiment is run with a known ligand included.&lt;/p&gt;
    &lt;p&gt;Well, wait a minute. Aren’t we missing something? Off-target effects of a small molecule can be summarized purely by these GPCR/NR measurements, but we’d be failing to capture something else that is of vital importance: whether the drug outright kills the cell. One could imagine this also affecting our receptor experiments! Perhaps a drug is an antagonist and there is no color shift, or perhaps the cell is just dead, and nothing is being expressed at all. Conversely, a drug might look like an agonist due to signal drift as the cell’s internal environment falls apart.&lt;/p&gt;
    &lt;p&gt;EvE solves this in a pragmatic way: run a third assay which measures how healthy the cell is. How do you measure that? Well, one good proxy for how functional a cell is ATP production. Metabolically active cells generate ATP to power all their intracellular processes. Dead ones don’t. The assay EvE uses is called CellTiter-Glo. It works by adding a reagent that causes a fluorescent reaction in the presence of ATP. More ATP, more light. Less ATP, less light. No ATP? No light (and likely dead). Again, simple!&lt;/p&gt;
    &lt;p&gt;Is that all? One last thing: accounting for pan-assay interference compounds, or PAINS. These are molecules that often give false positives in high-throughput screening regimes. This can occur for many different reasons, but one relevant example is if a molecule itself is a fluorophore, leading to us falsely believing that it is an agonist during a run. EvE simply tracks how often a drug is leading to positive results, and flags it in their results if they believe it is a PAINS.&lt;/p&gt;
    &lt;p&gt;So they run these three assays across their pairwise (drug x receptor) combinations, producing readouts at multiple different concentrations with replicates for each one.&lt;/p&gt;
    &lt;p&gt;I’m going to skip over a lot at this point. EvE clearly put an immense amount of work into QA’ing this process and filtering through the data, and I think I would do both a disservice and detract from the point of this essay if I were to attempt to repeat it here. Summarizing it all down, using a complex logic table detailed here in Fig 7, EvE assigns 1 of 4 categories to each (drug x receptor) combination:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Inactive. Drug likely has no effect on the receptor, across all tested concentrations. Maybe it doesn’t bind or maybe it binds but does nothing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Likely Inactive. A little more ambiguous, perhaps there’s a single noisy point above baseline, but nothing more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Active – Unquantified. Something is happening, since there’s reproducible activity, but not enough clean data to fit a proper dose-response curve.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Active – Quantified. The drug produced a clear, dose-dependent response (as either an agonist or antagonist) with a well-behaved curve. From this, EvE fits a 4-parameter logistic model and extracts a pXC₅₀; the negative log concentration at which the drug produces half its maximal effect.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And…that’s it. A clean, rigorous, and tractable approach to understanding off-target effects, across hundreds of receptors, at multiple concentrations, using multiple modes of detection, with full transparency around the data.&lt;/p&gt;
    &lt;p&gt;How far along is EvE on their mission? Circa their last data release on 5/7/2025, 237,490 (drug x concentration x receptor) combinations have been screened, revealing 8 median agonists and 31 median antagonists per target. They run these experiments in 384 well plates, so that means they’ve run the process a little bit over 600~ times to generate their current dataset — though much of the current process is automated, very little human-done pipetting is going on. Data dumps of the data started in November 2024, with new ones dropping every few months.&lt;/p&gt;
    &lt;p&gt;I haven’t worked in a wet lab before, but I’ve been assured by at least one person I trust that the effort that went into assembling this all together is nothing short of extraordinary. But it is worth asking the question…&lt;/p&gt;
    &lt;head rend="h1"&gt;Why hasn’t anyone done this before?&lt;/head&gt;
    &lt;p&gt;When assessing the value of a seeming scientific achievement, it’s usually good to step back and ask one question: why wasn’t this done a decade back?&lt;/p&gt;
    &lt;p&gt;In some cases, the answer is boring: the technology wasn’t there yet to achieve it.&lt;/p&gt;
    &lt;p&gt;But here, the technology was almost certainly available! Eve’s assay for measuring NR activity has been around at least since 2008, and the one for GPCR since 2010, maybe even earlier for both. If it’s really that useful, why did it take so long for someone to start assembling this drug x receptor mapping together?&lt;/p&gt;
    &lt;p&gt;Haven’t I already given away this answer? In the introduction, I implied that pharma groups have no direct financial incentive to create such a dataset. And that is true to some degree, especially for smaller therapeutic companies that have bigger issues to focus on, but is that true for big pharma? A small slice of the billions in pharma spending couldn’t be sliced off to hand over to an internal research team? It’s not as if the data wouldn’t be useful for their own drug development pipelines. After all, off-target effects are among the most common reasons for late-stage trial failures and post-approval black box warnings, and even if the creation of an EvE-like dataset doesn’t fix the problem, I can’t imagine it’d hurt.&lt;/p&gt;
    &lt;p&gt;I should be fair: pharma companies do indeed do some of this. EvE’s own blog discusses this a little, referencing this paper:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The report’s authors, luminaries in the discipline of safety pharmacology, surveyed 18 major pharmaceutical companies regarding the numbers and identities of potential off-targets against which they test each and every one of their new drug candidates in the interest of safety. The numbers ranged from a low of 11 to a high of 104 potential off-targets routinely profiled per company, with a median of about 45. Interestingly, the industry’s opinions regarding which potential off-targets to screen vary widely. The total number of potential off-targets screened, across the universe of all 18 pharmas, was 763, yet only 12% of them were screened by more than a third of those companies.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So, yes, pharma companies do their own off-target screening. But, as we’ve discussed, this is a far cry from the universe of druggable receptors, and is only concentrated on their particular assets, not other ones. No attempt at creating a universal map!&lt;/p&gt;
    &lt;p&gt;But the same blogpost did reference another big pharma, Novartis, who also open-source a much larger map:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Novartis, who presented data collected “over a multi-year period” profiling drug/target interactions across a median of about 800 drugs per target and 105 gene product targets…&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is impressive! One may imagine that if a big pharma was willing to release this, why does an entity like EvE need to exist? For interest's sake, let’s ignore the obvious answer of ‘it is better for everyone if such a dataset is collected using a single, standardized protocol instead of compiled from unrelated experiments over years.’&lt;/p&gt;
    &lt;p&gt;I asked Bill exactly this question, and the answer was a two-parter.&lt;/p&gt;
    &lt;p&gt;For one, the dataset that was collected by Novartis, and indeed every large-scale dataset that will ever be collected by big pharma, will always be limited by the constraint we mentioned at the start: everybody only cares about the drug working. A logical conclusion of this is that nearly every receptor covered in these sorts of screens is a safety-oriented receptor. Cytochrome P450, hERG, serotonin subtypes, dopamine D₂, and the like. These are important receptors, not because of how mechanistically interesting they are, but because they are dangerous. Indeed, the vast majority of screened receptors lie within the so-called Bowes-44 set, which comes from a 2012 paper that identified 44 receptors known to be often implicated in safety-related drug failures. Though these do include NR’s and GPCR’s, it is a minimal set of them, as, again, the screening is not meant to assess how mechanistically interesting the receptors are.&lt;/p&gt;
    &lt;p&gt;And if a big pharma does decide to explore beyond the realm of safety-oriented receptors, they will almost certainly keep that dataset to themselves. Why release potential alpha to competitors? Hence, why nothing quite like EvE has come out in the past and it is unlikely it ever will in the future, at least from a for-profit entity.&lt;/p&gt;
    &lt;p&gt;And two, EvE eventually hopes to cover a lot more ground than any of the publicly available datasets. Currently, yes, the Novartis dataset is larger than EvE’s, but it won’t be for long. In fact, their plans for the upcoming few years ended up being so interesting that I decided to split it off into another section:&lt;/p&gt;
    &lt;head rend="h1"&gt;What does the future look like?&lt;/head&gt;
    &lt;p&gt;EvE is still quite young, just over 2 years old, and I think the future of it is going to look really, really crazy. At the end of my startup coverage articles, I typically focus on commercial/scientific risks. But given that EvE is assured funding on a multi-year horizon without needing to care about market demands, it may be much more instructive (and interesting!) to instead discuss their upcoming plans.&lt;/p&gt;
    &lt;p&gt;Earlier I noted that EvE has currently released data for 29 NRs and 56 GPCRs, out of a planned 40 NR’s and 200 GPCR’s. In my conversation with Bill, I asked him how much time is left till the remaining ones are released. I expected the answer to be, optimistically, ‘over the next few years’, given how EvE only started to release data back in November 2024 and that the Novartis dataset collection process also took several years. I was astonished to learn that he expected to have released the remainder of all GPCR + NR screens dataset by the end of this year. Setting up the assays, validation, and automation was the hard part, which is why their data releases have only started recently. But now that that’s all set up, they simply must turn the crank to get the rest out of the door.&lt;/p&gt;
    &lt;p&gt;What’s next? Bill told me that the next target of receptors are kinases, 500~ or so receptors that have been increasingly valuable drug targets over the last 20 years.&lt;/p&gt;
    &lt;p&gt;Then what? Bill said he’s open to exploring even more drug targets, but he also said, surprisingly, that EvE may add more chemicals on top of the 1,600~ planned FDA-approved drugs. The FDA-approved drugs, he said, are success stories. Potentially it’d be even more interesting to consider the failures as well. Especially the ones that everybody expected to work, arrived at phase 3, and set billions of dollars on fire after the trial results came out.&lt;/p&gt;
    &lt;p&gt;Even more exotic options are also on the table. For example, Bill discussed exploring how metabolites of approved drugs interact with targets. Some context: most secondary pharmacology work stops at the parent compound, but metabolic byproducts of a drug can have entirely different binding profiles, and, in some cases, they’re the ones responsible for efficacy (e.g codeine, which metabolizes into the much more effective morphine) or for toxicity (e.g. acetaminophen, which metabolizes into the very toxic NAPQI). He also mentioned potentially using EvE’s assaying work to develop our understanding of tool compounds, which are chemicals that don’t necessarily have therapeutic value themselves, but are used in research to probe specific biological pathways or validate target function. An ACS page has this to say about it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;While tool compounds have tremendous potential for advancing life science research, they are broadly defined, and it is often difficult for a researcher to determine the best tool compounds to employ during the research process. There remains a great need for more tool compound databases and authoritative sources of information from experts in the field.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And, as always, there is a (very short) Derek Lowe piece on how a commonly-relied upon tool compound moonlights as a ligand for a structurally unrelated receptor, likely muddying the literature the tiniest bit. More work here would almost certainly be deeply appreciated by those in the field.&lt;/p&gt;
    &lt;p&gt;Overall, EvE really exemplifies the thesis I put forward in a past essay about how smart people in biology should do more boring things. Very little that is directly sexy about doing an N x M screen, but the impact of doing something like it well can be immense. And I have little doubt that EvE Bio has been doing it well, and will continue to do so in their future projects. If you’re interested in checking out their dataset, check it out here.&lt;/p&gt;
    &lt;p&gt;Unless the owner of a still-existing patent is looking to expand indications!&lt;/p&gt;
    &lt;p&gt;In Bill’s words: (1) about half of all GPCRs are sensory receptors (taste/smell), generally regarded as not likely involved in many (or even any) diseases, and anyway smell receptors are hard to work with in HTS because their ligands are compounds with very high vapor pressures (basically, gasses); and (2) only about 170 of the remainder are validated drug targets, and only about 200 (including those 170) have compounds (either drugs or research chemicals) which are known to turn on the receptor (AKA, an agonist). It's pretty nearly impossible to design a meaningful assay for receptor activity if you don't have a positive control compound.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45736608</guid><pubDate>Tue, 28 Oct 2025 18:12:10 +0000</pubDate></item><item><title>Samsung makes ads on smart fridges official with upcoming software update</title><link>https://arstechnica.com/gadgets/2025/10/samsung-makes-ads-on-3499-smart-fridges-official-with-upcoming-software-update/</link><description>&lt;doc fingerprint="83d62ecba3d68389"&gt;
  &lt;main&gt;
    &lt;p&gt;After kicking off an unpopular pilot test last month, Samsung made the practice of having its expensive smart fridges display ads official this week.&lt;/p&gt;
    &lt;p&gt;The ads will be shown on Samsung’s 2024 Family Hub smart fridges. As of this writing, Samsung’s Family Hub fridges have MSRPs ranging from $1,899 to $3,499. The ads will arrive through a software update that Samsung will start issuing this month and display on the fridge’s integrated 21.5- or 32-inch (depending on the model) screen. The ads will show when the fridges are idle and display what Samsung calls Cover Screens.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As part of the Family Hub software update, we are piloting a new widget for select Cover Screens themes of Family Hub refrigerators. The widget will display useful day-to-day information such as news, calendar and weather forecasts, along with curated advertisements.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Samsung also said that its fridges will only show contextualized ads, instead of personalized ads, which rely on collecting data on users.&lt;/p&gt;
    &lt;p&gt;The Verge reported that the widget will appear as a rectangular box at the bottom of the screens. The box will change what it displays “every 10 seconds,” the publication said.&lt;/p&gt;
    &lt;p&gt;The software update will also introduce “a Daily Board theme that offers a new way to see useful information at a glance,” Samsung said. The Verge reported that this feature will also include ads, something that Samsung’s announcement neglected to state. The Daily Board theme will show five tiles with information such as appointments and the weather, and one with ads.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45737338</guid><pubDate>Tue, 28 Oct 2025 19:02:48 +0000</pubDate></item><item><title>Why do some radio towers blink?</title><link>https://www.jeffgeerling.com/blog/2025/why-do-some-radio-towers-blink</link><description>&lt;doc fingerprint="22301a734e64745d"&gt;
  &lt;main&gt;
    &lt;p&gt;One day on my drive home, I saw three towers. One of them had a bunch of blinking white lights, another one had red lights that kind of faded in and out, and the third one, well, it wasn't doing anything. I'm lucky to have a radio engineer for a dad, so Dad: why do some towers blink?&lt;/p&gt;
    &lt;p&gt;Joe: Well, blinking I would call like the way you described it, "flashing", "white light", or "strobe". All these lights are to aid pilots and air traffic. helicopters, fighter planes, regular jets. So that's the purpose of it.&lt;/p&gt;
    &lt;p&gt;Jeff: Well that one tower that I saw had red lights that faded in and out, but I even think there's a freestanding tower just north of here that has red and white on top.&lt;/p&gt;
    &lt;p&gt;Joe: Well red lighting is a thing. It's in the regulations, it specifies red lighting or white lighting, and one of the things like the red lights can be a bulb like this inside of a red housing, could be LED.&lt;/p&gt;
    &lt;head rend="h2"&gt;Giant Bulbs&lt;/head&gt;
    &lt;p&gt;Jeff: Where'd you find this bulb? This was not in my studio.&lt;/p&gt;
    &lt;p&gt;Joe: This bulb is a spare bulb for a tower site. And most of us use the same bulb, same socket, 620 watts. It's a very standard broadcast bulb for a broadcast tower. And a lot of the beacons, they're pretty tall and they'll have one right-side up, and one upside-down. So they'll have two bulbs in the socket. That way, when one bulb burns out, you realize you have a bulb out, but you're still legal with the second bulb.&lt;/p&gt;
    &lt;p&gt;Jeff: At my house, I don't have any of this kind of bulb anymore [incandescent]. I have, you know, this is a little bit different size, these are LED. Is that similar on towers?&lt;/p&gt;
    &lt;p&gt;Joe: Yeah. So it is the same. Like an LED assembly for a tower could be kind of flat, and it has, you know, it's set to just do what it needs to do. So in like white light, that's, typically it used to be always strobes. And at KMOX, you remember we had the whole assembly there. and you see the glass bulb, with xenon gas inside.&lt;/p&gt;
    &lt;p&gt;So those were the old way for that, but the newer ones are all white LED, and they can be little pancake-looking things compared to what the things are now. They can direct that LED light really to help airplanes and have less trouble for people who live in homes or apartment buildings nearby.&lt;/p&gt;
    &lt;p&gt;Jeff: So if they have a tower like the one that's just north of here that's in a residential area, you're not just flashing everybody constantly.&lt;/p&gt;
    &lt;p&gt;Joe: Yes, yes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Red light, white light?&lt;/head&gt;
    &lt;p&gt;Jeff: You know, thinking about that tower, though, why do different towers have different lighting and different colors and things? And I even remember, like, on the tower that's right by here, there's actually teeny tiny little lights that are, like, on the sides, on the legs of the tower.&lt;/p&gt;
    &lt;p&gt;Joe: Yeah, well, tower lighting is in the responsibility of FAA, and they have a detailed plan. The options depend on the heights of the tower, where they are at. The location is obviously important. And sometimes red is a good solution, sometimes white, and sometimes both, red and white solution. And an example is the tower that you're talking about.&lt;/p&gt;
    &lt;p&gt;Jeff: So why would you want both? It seems like that's twice the complexity.&lt;/p&gt;
    &lt;p&gt;Joe: You would have strobe in the day and red at night. And people in their homes at night, a pulsing red light is a lot easier than a big flashing white light.&lt;/p&gt;
    &lt;p&gt;Jeff: So sometimes if people are complaining about it, you might go to that solution.&lt;/p&gt;
    &lt;p&gt;Joe: You can propose the red at night and the dual lighting works. And then you literally see these two packs and one will be red and one will be white. That's what I've seen mostly, although I heard that they make one now. It's all in one.&lt;/p&gt;
    &lt;p&gt;Jeff: [sarcastically] You could have RGB lights—could you do Home Assistant for your tower lighting?&lt;/p&gt;
    &lt;p&gt;Joe: You'd have RGB, but that would not be allowed by the FAA.&lt;/p&gt;
    &lt;p&gt;Jeff: We'll actually talk about that a little bit too later, because there are some regulations for how you actually monitor these things.&lt;/p&gt;
    &lt;p&gt;Joe: Yes, there are.&lt;/p&gt;
    &lt;head rend="h2"&gt;No lights at all?&lt;/head&gt;
    &lt;p&gt;Jeff: But before we get to that, what about the one tower I saw didn't have any lights that were blinking at all. And I zoomed in and I saw that there was a light on top, but it just wasn't doing anything.&lt;/p&gt;
    &lt;p&gt;Joe: Yeah. Well, and this is about daytime mode. So a tower that's painted, and that's legal for obstruction marking, and you don't have to light during the day. So at night, their lights would come on. So you can be in a situation where, like here, you could see three towers. One might be day and night with white light, and two of them might be red light day only, but one kicks on earlier than the other because of their exact photo cell triggering the lights.&lt;/p&gt;
    &lt;p&gt;Jeff: Or if the photo cell is covered by a layer of soot from 40 years...&lt;/p&gt;
    &lt;p&gt;Joe: Yeah. So that tower is probably in daytime mode, or it could be an AM tower. An AM tower below 200 feet doesn't have to be lit, so that's another one. In fact, all towers, I guess, under 200 feet, unless you're in a particular area where the FAA would require it.&lt;/p&gt;
    &lt;p&gt;Jeff: You always have to refer back to the documentation.&lt;/p&gt;
    &lt;p&gt;Joe: You always refer to the documentation.&lt;/p&gt;
    &lt;p&gt;Jeff: But what is the tower site that we went to for the hot dogs?&lt;/p&gt;
    &lt;p&gt;Joe: KHOJ? KHOJ, yeah. Those don't have lights. Those do not have lights.&lt;/p&gt;
    &lt;p&gt;Jeff: Because they're under 200 feet.&lt;/p&gt;
    &lt;p&gt;Joe: Yes, so they don't need lighting. So, and then they don't need painting either. So that's one of those things that, again, always refer to that because just because you have a tower that's under 200 feet doesn't mean you don't have to make sure it complies or you may have to paint it or you may have to light and paint.&lt;/p&gt;
    &lt;p&gt;Tower painting has changed a lot over the years. The older towers have lead in them. So whenever there's a project on the tower, it's not unusual to see the guys in some kind of a, what do they call those?&lt;/p&gt;
    &lt;p&gt;Jeff: A full ghillie suit? Or I don't know what they're called.&lt;/p&gt;
    &lt;p&gt;Joe: Yeah. Yeah. So they have that too. So anyway, that's the bottom line from then till now.&lt;/p&gt;
    &lt;head rend="h2"&gt;But some short towers do have lights...&lt;/head&gt;
    &lt;p&gt;Jeff: So far, we've been talking about radio and TV towers, the ones that are really big. But what about little small towers? I know I see a lot of cell towers that don't have any lights at all, but sometimes they do. They just have one little light bulb on top. Or even if they're 50 to 100 feet tall, I've seen it sometimes.&lt;/p&gt;
    &lt;p&gt;Joe: Yeah, and there's always a reason for seeing a light somewhere, almost always on structures, including buildings and so forth. But if you look around the area where you're seeing that tower that you know is not 200 feet and it's got a light on the top, look you're probably near an airport, a heliport, or somewhere where there's an aviation hazard. You know hospitals, they have their helipads there.&lt;/p&gt;
    &lt;p&gt;So everywhere there is a possible hazard for flight for aircraft is going to have lights involved, and that's why we see a lot of buildings even have lights.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAA Rules&lt;/head&gt;
    &lt;p&gt;Jeff: So it sounds like we have towers and even buildings that have to have lights put up but who's in charge of all this? Like if I'm going to put up my own tower, who do I need to talk to to make sure that i'm doing it right and get it approved?&lt;/p&gt;
    &lt;p&gt;Joe: I think we go back to our friends at the FAA. They have that particular circular (AC 70/7460-1M - Obstruction Marking and Lighting). It describes all kinds of of places where lighting is need for air safety.&lt;/p&gt;
    &lt;p&gt;So that includes our broadcast towers, big chimney stacks, water towers, bridges, nuclear power plant cooling towers, wind turbines, tall electrical towers, you know, those ones that they cross a river, they'll have those extra tall ones with markings in the middle. Sometimes those are required to have lighting.&lt;/p&gt;
    &lt;p&gt;And there are also rules that even apply during construction projects. So you've got these big buildings going up And you'll notice the cranes will have lights on them. And there's a spec for that, how many lights have to be on the crane, what height it has to be at.&lt;/p&gt;
    &lt;p&gt;As you look around the next 12 months, look around at all those lights. You'll enjoy it like I do.&lt;/p&gt;
    &lt;p&gt;Jeff: You'll look like a radio engineer once you know about these things!&lt;/p&gt;
    &lt;head rend="h2"&gt;Estimating tower height by light&lt;/head&gt;
    &lt;p&gt;Jeff: So that's made me think, like, looking through these instructions, I did find this page. And I thought, like, could you use tower lights, the number of them, as a way to judge how tall a tower is? Kind of a rough estimate.&lt;/p&gt;
    &lt;p&gt;Joe: Yes, actually you can because the specifications, like for tall, these would apply to radio and TV towers. They require so many lights depending on the height.&lt;/p&gt;
    &lt;p&gt;Like around here, we would have the F4 version with four lights, four blinking light levels, a light at the top if the antenna here at the top is higher than 40 feet.&lt;/p&gt;
    &lt;p&gt;So you can kind of look and get an idea of whether you're looking at a 500-foot tower or a 1,000-foot tower. I don't think you could go like 700 versus 1,000 as easily.&lt;/p&gt;
    &lt;p&gt;But you can tell, and the guys know. I know a couple of pilots that are also engineers or selling in radio business, and they can do that. They'll tell you, you know, like I was passing a tower. What's a tower that's a 1,200-foot tower doing out here? So it's pretty fun, and you go to this document has everything you need if you want to take the time and study that and memorize it and then fly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Monitoring and reporting outages (and NOTAMs)&lt;/head&gt;
    &lt;p&gt;Jeff: One other thing that since we've been running this channel I keep getting emails about, if someone sees that there's lights out on a tower, like let's say it's nighttime and you're out there and you notice like one of these towers for the past few days hasn't had a light on, can you do anything about that? What should you do?&lt;/p&gt;
    &lt;p&gt;Joe: Well, first of all, if it's required to be lit, every tower that's required to be lit is also required to be monitored.&lt;/p&gt;
    &lt;p&gt;So in radio, we put a monitoring circuit on the electrical feed to the filaments and you can measure how much current is going through there, right? So you've got to monitor it. You've got to get alarmed by that. The alarm can call you or reach you. But you also have to call every day, check your circuit, your system, and make sure the lights are on. So if you only are lit at night, that means that call has to happen after it's dark.&lt;/p&gt;
    &lt;p&gt;You've got to verify that your lighting is working, and you have to report it within 30 minutes. And you find that if a light's out, you've got to report it. And we call that a NOTAM, where the FAA puts it on a system that all the pilots can have access to. And they know that there's a tower there, but it's lights out. And they can use it in their flight planning.&lt;/p&gt;
    &lt;p&gt;Jeff: You're an engineer and you have the tie-in with the FAA or whatever. But what would I be able to do if I did see a tower that was out?&lt;/p&gt;
    &lt;p&gt;Joe: Well, you can do a NOTAM search. Try to look through and see if you see it. The other thing, you could contact the tower owner or the engineer at the site, the tower's engineer, and they can check into it. They should already know because they have their monitoring equipment, right?&lt;/p&gt;
    &lt;p&gt;Jeff: They should.&lt;/p&gt;
    &lt;p&gt;Joe: They should already know. They do.&lt;/p&gt;
    &lt;p&gt;And then, of course, if you're out there, usually the tower, either on the tower or in the fencing around it or on the building, will have an ASR number, which identifies that tower to the FAA and the FCC, actually. But that number would be the exact tower that you'd be reporting.&lt;/p&gt;
    &lt;p&gt;Jeff: So hopefully you learned a little bit more about why there are so many flashing lights around at night and not just for radio towers, but for bridges, buildings, and more. What other things do you want to know about towers? Let us know in the comments.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45737941</guid><pubDate>Tue, 28 Oct 2025 19:38:53 +0000</pubDate></item><item><title>Boring is what we wanted</title><link>https://512pixels.net/2025/10/boring-is-what-we-wanted/</link><description>&lt;doc fingerprint="b748792552261154"&gt;
  &lt;main&gt;
    &lt;p&gt;We are coming up on five years since the first M1 Macs shipped. It was an incredible time to be a Mac user. Those first Apple silicon Macs looked like the Intel machines they replaced, but they were better in every single way.&lt;/p&gt;
    &lt;p&gt;In December 2020, John Gruber wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We knew this to be true: Computers could run fast and hot, or slow and cool. For laptops in particular, the best you could hope for is a middle ground: fast enough and cool enough. But if you wanted a machine that ran really fast, it wasn’t going to run cool (and wasn’t going to last long on battery), and if you wanted a computer that ran cool (and lasted long on battery), it wasn’t going to be fast.&lt;/p&gt;
      &lt;p&gt;We knew this to be true because that was the way things were. But now, with the M1 Macs, it’s not. M1 Macs run very fast and do so while remaining very cool and lasting mind-bogglingly long on battery. It was a fundamental trade-off inherent to PC computing, and now we don’t have to make it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite its Touch Bar, I immediately bought that first M1 MacBook Pro, and when the 14-inch MacBook Pro came out a year later, I moved to it.1 I’m typing these very words on my 14-inch MacBook Pro with an M4 Max inside. Each of these machines was faster than the one before it, outperforming my old iMac Pro and Mac Pro in new ways with every upgrade.&lt;/p&gt;
    &lt;p&gt;Apple silicon has been nothing but upside for the Mac, and yet some seem bored already. In the days since Apple announced the M5, I’ve seen and heard this sentiment more than I expected:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is just another boring incremental upgrade.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That 👏 is 👏 the 👏 point.&lt;/p&gt;
    &lt;p&gt;Back in the PowerPC and Intel days, Macs would sometimes go years between meaningful spec bumps, as Apple waited on its partners to deliver appropriate hardware for various machines. From failing NVIDIA cards in MacBook Pros to 27-inch Intel iMacs that ran so hot the fans were audible at all times, Mac hardware wasn’t always what Apple wanted.&lt;/p&gt;
    &lt;p&gt;Of course, some of the issues with previous generations of Mac were Apple’s fault — look no further than the butterfly keyboard or the years the Mac Pro spent in the wilderness. Apple will make questionable decisions in the future, just as it has in the past.&lt;/p&gt;
    &lt;p&gt;The difference is that with Apple silicon, Apple owns and controls the primary technologies behind the products it makes, as Tim Cook has always wanted. It means that it can ship updates to its SoCs on a regular cadence, making progress in terms of both power and efficiency each time.&lt;/p&gt;
    &lt;p&gt;A predictable update schedule means that incremental updates are inevitable. Revolution then evolution is not a bad thing; it’s okay that not every release is exciting or groundbreaking. It’s how technology has worked for decades.&lt;/p&gt;
    &lt;p&gt;…but some people have short memories. Before the Apple silicon introduction, we all wanted steady, predictable progress in Mac hardware development. We wanted each product in the lineup to be updated regularly and not wither on the vine for years. For the most part, Apple has delivered. Just look at this chart of the progress Apple has made since the M1:&lt;/p&gt;
    &lt;p&gt;I don’t see anything in those charts to complain about, especially given the frequency at which most people buy new computers. That’s one reason why Apple compared the M5 to the M1 in its press release announcing the new chip. Unless you buy a new computer every year, every update you experience will be meaningful.&lt;/p&gt;
    &lt;p&gt;That’s what we wanted when Apple announced the move away from Intel, and calling it boring five years in is missing the point and downplaying the success of Apple silicon thus far.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My review of the M1 Pro 14-inch MacBook Pro remains one of my favorite blog posts I’ve written. ↩&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45738247</guid><pubDate>Tue, 28 Oct 2025 19:57:16 +0000</pubDate></item><item><title>Generative AI Image Editing Showdown</title><link>https://genai-showdown.specr.net/image-editing</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45739080</guid><pubDate>Tue, 28 Oct 2025 20:58:22 +0000</pubDate></item><item><title>Tinkering is a way to acquire good taste</title><link>https://seated.ro/blog/tinkering-a-lost-art</link><description>&lt;doc fingerprint="7426e95752a204d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If you don't tinker, you don't have taste&lt;/head&gt;
    &lt;head rend="h2"&gt;tinÂ·ker&lt;/head&gt;
    &lt;head rend="h4"&gt;/ËtiNGkÉr/&lt;/head&gt;
    &lt;head rend="h4"&gt;to make small changes to something, especially in an attempt to repair or improve it.&lt;/head&gt;
    &lt;head rend="h1"&gt;In Hindsight&lt;/head&gt;
    &lt;p&gt;Growing up, I never stuck to a single thing, be it guitar lessons, art school, martial arts â I tried them all. when it came to programming, though, I never really tinkered. I was always amazed with video games and wondered how they were made but I never pursued that curiosity.&lt;/p&gt;
    &lt;p&gt;My tinkering habits picked up very late, and now I cannot go by without picking up new things in one form or another. Itâs how I learn. I wish I did it sooner. Itâs a major part of my learning process now, and I would never be the &lt;del&gt;programmer&lt;/del&gt; person I am today.&lt;/p&gt;
    &lt;head rend="h1"&gt;What the hell is tinkering?&lt;/head&gt;
    &lt;p&gt;Have you ever spent hours tweaking the mouse sensitivity in your favorite FPS game?&lt;/p&gt;
    &lt;p&gt;Have you ever installed a Linux distro, spent days configuring window managers, not because you had to, but purely because it gave you satisfaction and made your workflow exactly yours?&lt;/p&gt;
    &lt;p&gt;Ever pulled apart your mechanical keyboard, swapped keycaps, tested switches, and lubed stabilizers just for more thock?&lt;/p&gt;
    &lt;p&gt;That is what I mean.&lt;/p&gt;
    &lt;p&gt;I have come to understand that there are two kinds of people, those who do things only if it helps them achieve a goal, and those who do things just because. The ideal, of course, is to be a mix of both.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;when you tinker and throw away, thatâs practice, and practice should inherently be ephemeral, exploratory, and be frequent - @ludwigABAP&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;My approach to tinkering&lt;/head&gt;
    &lt;p&gt;There are plenty of people who still use the VSCode terminal as their default terminal, do not know what vim bindings are, GitHub desktop rather than the cli (at the very least). Iâm not saying these are bad things necessarily, just that this should be the minimum, not the median.&lt;/p&gt;
    &lt;p&gt;This does not mean I spend every waking hour fiddling with my neovim config. In fact, the last meaningful change to my config was 6 months ago. Finding that balance is where most people fail.&lt;/p&gt;
    &lt;p&gt;Over the years I have done so many things that in hindsight have made me appreciate programming more but were completely âunnecessaryâ in the strict sense.&lt;/p&gt;
    &lt;p&gt;In the past week I have, for the first time, written a glsl fragment shader, a rust procedural macro, template c++, a swift app, furthered my hatred for windows development (this is not new), and started using the helix editor more (mainly for good defaults + speed). I didnât have to do these things, but I did, for fun! And I know more about these things now.&lt;/p&gt;
    &lt;p&gt;No time spent learning, is time wasted.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why taste matters, especially now&lt;/head&gt;
    &lt;p&gt;Acquiring good taste comes through using various things, discarding the ones you donât like and keeping the ones you do. if you never try various things, you will not acquire good taste.&lt;/p&gt;
    &lt;p&gt;And what I mean by taste here is simply the honed ability to distinguish mediocrity from excellence. This will be highly subjective, and not everyoneâs taste will be the same, but that is the point, you should NOT have the same taste as someone else.&lt;/p&gt;
    &lt;p&gt;Question the status quo, experiment, break things, do this several times, do this everyday and keep doing it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45739499</guid><pubDate>Tue, 28 Oct 2025 21:31:50 +0000</pubDate></item><item><title>Keeping the Internet fast and secure: introducing Merkle Tree Certificates</title><link>https://blog.cloudflare.com/bootstrap-mtc/</link><description>&lt;doc fingerprint="6f39937a141a36d0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The world is in a race to build its first quantum computer capable of solving practical problems not feasible on even the largest conventional supercomputers. While the quantum computing paradigm promises many benefits, it also threatens the security of the Internet by breaking much of the cryptography we have come to rely on.&lt;/p&gt;
      &lt;p&gt;To mitigate this threat, Cloudflare is helping to migrate the Internet to Post-Quantum (PQ) cryptography. Today, about 50% of traffic to Cloudflare's edge network is protected against the most urgent threat: an attacker who can intercept and store encrypted traffic today and then decrypt it in the future with the help of a quantum computer. This is referred to as the harvest now, decrypt later threat.&lt;/p&gt;
      &lt;p&gt;However, this is just one of the threats we need to address. A quantum computer can also be used to crack a server's TLS certificate, allowing an attacker to impersonate the server to unsuspecting clients. The good news is that we already have PQ algorithms we can use for quantum-safe authentication. The bad news is that adoption of these algorithms in TLS will require significant changes to one of the most complex and security-critical systems on the Internet: the Web Public-Key Infrastructure (WebPKI).&lt;/p&gt;
      &lt;p&gt;The central problem is the sheer size of these new algorithms: signatures for ML-DSA-44, one of the most performant PQ algorithms standardized by NIST, are 2,420 bytes long, compared to just 64 bytes for ECDSA-P256, the most popular non-PQ signature in use today; and its public keys are 1,312 bytes long, compared to just 64 bytes for ECDSA. That's a roughly 20-fold increase in size. Worse yet, the average TLS handshake includes a number of public keys and signatures, adding up to 10s of kilobytes of overhead per handshake. This is enough to have a noticeable impact on the performance of TLS.&lt;/p&gt;
      &lt;p&gt;That makes drop-in PQ certificates a tough sell to enable today: they donât bring any security benefit before Q-day â the day a cryptographically relevant quantum computer arrives â but they do degrade performance. We could sit and wait until Q-day is a year away, but thatâs playing with fire. Migrations always take longer than expected, and by waiting we risk the security and privacy of the Internet, which is dear to us.&lt;/p&gt;
      &lt;p&gt;It's clear that we must find a way to make post-quantum certificates cheap enough to deploy today by default for everyone â not just those that can afford it. In this post, we'll introduce you to the plan weâve brought together with industry partners to the IETF to redesign the WebPKI in order to allow a smooth transition to PQ authentication with no performance impact (and perhaps a performance improvement!). We'll provide an overview of one concrete proposal, called Merkle Tree Certificates (MTCs), whose goal is to whittle down the number of public keys and signatures in the TLS handshake to the bare minimum required.&lt;/p&gt;
      &lt;p&gt;But talk is cheap. We know from experience that, as with any change to the Internet, it's crucial to test early and often. Today we're announcing our intent to deploy MTCs on an experimental basis in collaboration with Chrome Security. In this post, we'll describe the scope of this experiment, what we hope to learn from it, and how we'll make sure it's done safely.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The WebPKI today â an old system with many patches&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Why does the TLS handshake have so many public keys and signatures?&lt;/p&gt;
      &lt;p&gt;Let's start with Cryptography 101. When your browser connects to a website, it asks the server to authenticate itself to make sure it's talking to the real server and not an impersonator. This is usually achieved with a cryptographic primitive known as a digital signature scheme (e.g., ECDSA or ML-DSA). In TLS, the server signs the messages exchanged between the client and server using its secret key, and the client verifies the signature using the server's public key. In this way, the server confirms to the client that they've had the same conversation, since only the server could have produced a valid signature.&lt;/p&gt;
      &lt;p&gt;If the client already knows the server's public key, then only 1 signature is required to authenticate the server. In practice, however, this is not really an option. The web today is made up of around a billion TLS servers, so it would be unrealistic to provision every client with the public key of every server. What's more, the set of public keys will change over time as new servers come online and existing ones rotate their keys, so we would need some way of pushing these changes to clients.&lt;/p&gt;
      &lt;p&gt;This scaling problem is at the heart of the design of all PKIs.&lt;/p&gt;
      &lt;p&gt;Instead of expecting the client to know the server's public key in advance, the server might just send its public key during the TLS handshake. But how does the client know that the public key actually belongs to the server? This is the job of a certificate.&lt;/p&gt;
      &lt;p&gt;A certificate binds a public key to the identity of the server â usually its DNS name, e.g., &lt;code&gt;cloudflareresearch.com&lt;/code&gt;. The certificate is signed by a Certification Authority (CA) whose public key is known to the client. In addition to verifying the server's handshake signature, the client verifies the signature of this certificate. This establishes a chain of trust: by accepting the certificate, the client is trusting that the CA verified that the public key actually belongs to the server with that identity.&lt;/p&gt;
      &lt;p&gt;Clients are typically configured to trust many CAs and must be provisioned with a public key for each. Things are much easier however, since there are only 100s of CAs instead of billions. In addition, new certificates can be created without having to update clients.&lt;/p&gt;
      &lt;p&gt;These efficiencies come at a relatively low cost: for those counting at home, that's +1 signature and +1 public key, for a total of 2 signatures and 1 public key per TLS handshake.&lt;/p&gt;
      &lt;p&gt;That's not the end of the story, however. As the WebPKI has evolved, so have these chains of trust grown a bit longer. These days it's common for a chain to consist of two or more certificates rather than just one. This is because CAs sometimes need to rotate their keys, just as servers do. But before they can start using the new key, they must distribute the corresponding public key to clients. This takes time, since it requires billions of clients to update their trust stores. To bridge the gap, the CA will sometimes use the old key to issue a certificate for the new one and append this certificate to the end of the chain.&lt;/p&gt;
      &lt;p&gt;That's +1 signature and +1 public key, which brings us to 3 signatures and 2 public keys. And we still have a little ways to go.&lt;/p&gt;
      &lt;p&gt;The main job of a CA is to verify that a server has control over the domain for which itâs requesting a certificate. This process has evolved over the years from a high-touch, CA-specific process to a standardized, mostly automated process used for issuing most certificates on the web. (Not all CAs fully support automation, however.) This evolution is marked by a number of security incidents in which a certificate was mis-issued to a party other than the server, allowing that party to impersonate the server to any client that trusts the CA.&lt;/p&gt;
      &lt;p&gt;Automation helps, but attacks are still possible, and mistakes are almost inevitable. Earlier this year, several certificates for Cloudflare's encrypted 1.1.1.1 resolver were issued without our involvement or authorization. This apparently occurred by accident, but it nonetheless put users of 1.1.1.1 at risk. (The mis-issued certificates have since been revoked.)&lt;/p&gt;
      &lt;p&gt;Ensuring mis-issuance is detectable is the job of the Certificate Transparency (CT) ecosystem. The basic idea is that each certificate issued by a CA gets added to a public log. Servers can audit these logs for certificates issued in their name. If ever a certificate is issued that they didn't request itself, the server operator can prove the issuance happened, and the PKI ecosystem can take action to prevent the certificate from being trusted by clients.&lt;/p&gt;
      &lt;p&gt;Major browsers, including Firefox and Chrome and its derivatives, require certificates to be logged before they can be trusted. For example, Chrome, Safari, and Firefox will only accept the server's certificate if it appears in at least two logs the browser is configured to trust. This policy is easy to state, but tricky to implement in practice:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Operating a CT log has historically been fairly expensive. Logs ingest billions of certificates over their lifetimes: when an incident happens, or even just under high load, it can take some time for a log to make a new entry available for auditors.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Clients can't really audit logs themselves, since this would expose their browsing history (i.e., the servers they wanted to connect to) to the log operators.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;The solution to both problems is to include a signature from the CT log along with the certificate. The signature is produced immediately in response to a request to log a certificate, and attests to the log's intent to include the certificate in the log within 24 hours.&lt;/p&gt;
      &lt;p&gt;Per browser policy, certificate transparency adds +2 signatures to the TLS handshake, one for each log. This brings us to a total of 5 signatures and 2 public keys in a typical handshake on the public web.&lt;/p&gt;
      &lt;p&gt;The WebPKI is a living, breathing, and highly distributed system. We've had to patch it a number of times over the years to keep it going, but on balance it has served our needs quite well â until now.&lt;/p&gt;
      &lt;p&gt;Previously, whenever we needed to update something in the WebPKI, we would tack on another signature. This strategy has worked because conventional cryptography is so cheap. But 5 signatures and 2 public keys on average for each TLS handshake is simply too much to cope with for the larger PQ signatures that are coming.&lt;/p&gt;
      &lt;p&gt;The good news is that by moving what we already have around in clever ways, we can drastically reduce the number of signatures we need.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Crash course on Merkle Tree Certificates&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Merkle Tree Certificates (MTCs) is a proposal for the next generation of the WebPKI that we are implementing and plan to deploy on an experimental basis. Its key features are as follows:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;All the information a client needs to validate a Merkle Tree Certificate can be disseminated out-of-band. If the client is sufficiently up-to-date, then the TLS handshake needs just 1 signature, 1 public key, and 1 Merkle tree inclusion proof. This is quite small, even if we use post-quantum algorithms.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The MTC specification makes certificate transparency a first class feature of the PKI by having each CA run its own log of exactly the certificates they issue.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Let's poke our head under the hood a little. Below we have an MTC generated by one of our internal tests. This would be transmitted from the server to the client in the TLS handshake:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;-----BEGIN CERTIFICATE-----
MIICSzCCAUGgAwIBAgICAhMwDAYKKwYBBAGC2ksvADAcMRowGAYKKwYBBAGC2ksv
AQwKNDQzNjMuNDguMzAeFw0yNTEwMjExNTMzMjZaFw0yNTEwMjgxNTMzMjZaMCEx
HzAdBgNVBAMTFmNsb3VkZmxhcmVyZXNlYXJjaC5jb20wWTATBgcqhkjOPQIBBggq
hkjOPQMBBwNCAARw7eGWh7Qi7/vcqc2cXO8enqsbbdcRdHt2yDyhX5Q3RZnYgONc
JE8oRrW/hGDY/OuCWsROM5DHszZRDJJtv4gno2wwajAOBgNVHQ8BAf8EBAMCB4Aw
EwYDVR0lBAwwCgYIKwYBBQUHAwEwQwYDVR0RBDwwOoIWY2xvdWRmbGFyZXJlc2Vh
cmNoLmNvbYIgc3RhdGljLWN0LmNsb3VkZmxhcmVyZXNlYXJjaC5jb20wDAYKKwYB
BAGC2ksvAAOB9QAAAAAAAAACAAAAAAAAAAJYAOBEvgOlvWq38p45d0wWTPgG5eFV
wJMhxnmDPN1b5leJwHWzTOx1igtToMocBwwakt3HfKIjXYMO5CNDOK9DIKhmRDSV
h+or8A8WUrvqZ2ceiTZPkNQFVYlG8be2aITTVzGuK8N5MYaFnSTtzyWkXP2P9nYU
Vd1nLt/WjCUNUkjI4/75fOalMFKltcc6iaXB9ktble9wuJH8YQ9tFt456aBZSSs0
cXwqFtrHr973AZQQxGLR9QCHveii9N87NXknDvzMQ+dgWt/fBujTfuuzv3slQw80
mibA021dDCi8h1hYFQAA
-----END CERTIFICATE-----&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Looks like your average PEM encoded certificate. Let's decode it and look at the parameters:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ openssl x509 -in merkle-tree-cert.pem -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 531 (0x213)
        Signature Algorithm: 1.3.6.1.4.1.44363.47.0
        Issuer: 1.3.6.1.4.1.44363.47.1=44363.48.3
        Validity
            Not Before: Oct 21 15:33:26 2025 GMT
            Not After : Oct 28 15:33:26 2025 GMT
        Subject: CN=cloudflareresearch.com
        Subject Public Key Info:
            Public Key Algorithm: id-ecPublicKey
                Public-Key: (256 bit)
                pub:
                    04:70:ed:e1:96:87:b4:22:ef:fb:dc:a9:cd:9c:5c:
                    ef:1e:9e:ab:1b:6d:d7:11:74:7b:76:c8:3c:a1:5f:
                    94:37:45:99:d8:80:e3:5c:24:4f:28:46:b5:bf:84:
                    60:d8:fc:eb:82:5a:c4:4e:33:90:c7:b3:36:51:0c:
                    92:6d:bf:88:27
                ASN1 OID: prime256v1
                NIST CURVE: P-256
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature
            X509v3 Extended Key Usage:
                TLS Web Server Authentication
            X509v3 Subject Alternative Name:
                DNS:cloudflareresearch.com, DNS:static-ct.cloudflareresearch.com
    Signature Algorithm: 1.3.6.1.4.1.44363.47.0
    Signature Value:
        00:00:00:00:00:00:02:00:00:00:00:00:00:00:02:58:00:e0:
        44:be:03:a5:bd:6a:b7:f2:9e:39:77:4c:16:4c:f8:06:e5:e1:
        55:c0:93:21:c6:79:83:3c:dd:5b:e6:57:89:c0:75:b3:4c:ec:
        75:8a:0b:53:a0:ca:1c:07:0c:1a:92:dd:c7:7c:a2:23:5d:83:
        0e:e4:23:43:38:af:43:20:a8:66:44:34:95:87:ea:2b:f0:0f:
        16:52:bb:ea:67:67:1e:89:36:4f:90:d4:05:55:89:46:f1:b7:
        b6:68:84:d3:57:31:ae:2b:c3:79:31:86:85:9d:24:ed:cf:25:
        a4:5c:fd:8f:f6:76:14:55:dd:67:2e:df:d6:8c:25:0d:52:48:
        c8:e3:fe:f9:7c:e6:a5:30:52:a5:b5:c7:3a:89:a5:c1:f6:4b:
        5b:95:ef:70:b8:91:fc:61:0f:6d:16:de:39:e9:a0:59:49:2b:
        34:71:7c:2a:16:da:c7:af:de:f7:01:94:10:c4:62:d1:f5:00:
        87:bd:e8:a2:f4:df:3b:35:79:27:0e:fc:cc:43:e7:60:5a:df:
        df:06:e8:d3:7e:eb:b3:bf:7b:25:43:0f:34:9a:26:c0:d3:6d:
        5d:0c:28:bc:87:58:58:15:00:00&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;While some of the parameters probably look familiar, others will look unusual. On the familiar side, the subject and public key are exactly what we might expect: the DNS name is &lt;code&gt;cloudflareresearch.com&lt;/code&gt; and the public key is for a familiar signature algorithm, ECDSA-P256. This algorithm is not PQ, of course â in the future we would put ML-DSA-44 there instead.&lt;/p&gt;
      &lt;p&gt;On the unusual side, OpenSSL appears to not recognize the signature algorithm of the issuer and just prints the raw OID and bytes of the signature. There's a good reason for this: the MTC does not have a signature in it at all! So what exactly are we looking at?&lt;/p&gt;
      &lt;p&gt;The trick to leave out signatures is that a Merkle Tree Certification Authority (MTCA) produces its signatureless certificates in batches rather than individually. In place of a signature, the certificate has an inclusion proof of the certificate in a batch of certificates signed by the MTCA.&lt;/p&gt;
      &lt;p&gt;To understand how inclusion proofs work, let's think about a slightly simplified version of the MTC specification. To issue a batch, the MTCA arranges the unsigned certificates into a data structure called a Merkle tree that looks like this:&lt;/p&gt;
      &lt;p&gt;Each leaf of the tree corresponds to a certificate, and each inner node is equal to the hash of its children. To sign the batch, the MTCA uses its secret key to sign the head of the tree. The structure of the tree guarantees that each certificate in the batch was signed by the MTCA: if we tried to tweak the bits of any one of the certificates, the treehead would end up having a different value, which would cause the signature to fail.&lt;/p&gt;
      &lt;p&gt;An inclusion proof for a certificate consists of the hash of each sibling node along the path from the certificate to the treehead:&lt;/p&gt;
      &lt;p&gt;Given a validated treehead, this sequence of hashes is sufficient to prove inclusion of the certificate in the tree. This means that, in order to validate an MTC, the client also needs to obtain the signed treehead from the MTCA.&lt;/p&gt;
      &lt;p&gt;This is the key to MTC's efficiency:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Signed treeheads can be disseminated to clients out-of-band and validated offline. Each validated treehead can then be used to validate any certificate in the corresponding batch, eliminating the need to obtain a signature for each server certificate.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;During the TLS handshake, the client tells the server which treeheads it has. If the server has a signatureless certificate covered by one of those treeheads, then it can use that certificate to authenticate itself. That's 1 signature,1 public key and 1 inclusion proof per handshake, both for the server being authenticated.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Now, that's the simplified version. MTC proper has some more bells and whistles. To start, it doesnât create a separate Merkle tree for each batch, but it grows a single large tree, which is used for better transparency. As this tree grows, periodically (sub)tree heads are selected to be shipped to browsers, which we call landmarks. In the common case browsers will be able to fetch the most recent landmarks, and servers can wait for batch issuance, but we need a fallback: MTC also supports certificates that can be issued immediately and donât require landmarks to be validated, but these are not as small. A server would provision both types of Merkle tree certificates, so that the common case is fast, and the exceptional case is slow, but at least itâll work.&lt;/p&gt;
      &lt;p&gt;Ever since early designs for MTCs emerged, weâve been eager to experiment with the idea. In line with the IETF principle of ârunning codeâ, it often takes implementing a protocol to work out kinks in the design. At the same time, we cannot risk the security of users. In this section, we describe our approach to experimenting with aspects of the Merkle Tree Certificates design without changing any trust relationships.&lt;/p&gt;
      &lt;p&gt;Letâs start with what we hope to learn. We have lots of questions whose answers can help to either validate the approach, or uncover pitfalls that require reshaping the protocol â in fact, an implementation of an early MTC draft by Maximilian Pohl and Mia Celeste did exactly this. Weâd like to know:&lt;/p&gt;
      &lt;p&gt;What breaks? Protocol ossification (the tendency of implementation bugs to make it harder to change a protocol) is an ever-present issue with deploying protocol changes. For TLS in particular, despite having built-in flexibility, time after time weâve found that if that flexibility is not regularly used, there will be buggy implementations and middleboxes that break when they see things they donât recognize. TLS 1.3 deployment took years longer than we hoped for this very reason. And more recently, the rollout of PQ key exchange in TLS caused the Client Hello to be split over multiple TCP packets, something that many middleboxes weren't ready for.&lt;/p&gt;
      &lt;p&gt;What is the performance impact? In fact, we expect MTCs to reduce the size of the handshake, even compared to today's non-PQ certificates. They will also reduce CPU cost: ML-DSA signature verification is about as fast as ECDSA, and there will be far fewer signatures to verify. We therefore expect to see a reduction in latency. We would like to see if there is a measurable performance improvement.&lt;/p&gt;
      &lt;p&gt;What fraction of clients will stay up to date? Getting the performance benefit of MTCs requires the clients and servers to be roughly in sync with one another. We expect MTCs to have fairly short lifetimes, a week or so. This means that if the client's latest landmark is older than a week, the server would have to fallback to a larger certificate. Knowing how often this fallback happens will help us tune the parameters of the protocol to make fallbacks less likely.&lt;/p&gt;
      &lt;p&gt;In order to answer these questions, we are implementing MTC support in our TLS stack and in our certificate issuance infrastructure. For their part, Chrome is implementing MTC support in their own TLS stack and will stand up infrastructure to disseminate landmarks to their users.&lt;/p&gt;
      &lt;p&gt;As we've done in past experiments, we plan to enable MTCs for a subset of our free customers with enough traffic that we will be able to get useful measurements. Chrome will control the experimental rollout: they can ramp up slowly, measuring as they go and rolling back if and when bugs are found.&lt;/p&gt;
      &lt;p&gt;Which leaves us with one last question: who will run the Merkle Tree CA?&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Bootstrapping trust from the existing WebPKI&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Standing up a proper CA is no small task: it takes years to be trusted by major browsers. Thatâs why Cloudflare isnât going to become a ârealâ CA for this experiment, and Chrome isnât going to trust us directly.&lt;/p&gt;
      &lt;p&gt;Instead, to make progress on a reasonable timeframe, without sacrificing due diligence, we plan to "mock" the role of the MTCA. We will run an MTCA (on Workers based on our StaticCT logs), but for each MTC we issue, we also publish an existing certificate from a trusted CA that agrees with it. We call this the bootstrap certificate. When Chromeâs infrastructure pulls updates from our MTCA log, they will also pull these bootstrap certificates, and check whether they agree. Only if they do, theyâll proceed to push the corresponding landmarks to Chrome clients. In other words, Cloudflare is effectively just âre-encodingâ an existing certificate (with domain validation performed by a trusted CA) as an MTC, and Chrome is using certificate transparency to keep us honest.&lt;/p&gt;
      &lt;p&gt;With almost 50% of our traffic already protected by post-quantum encryption, weâre halfway to a fully post-quantum secure Internet. The second part of our journey, post-quantum certificates, is the hardest yet though. A simple drop-in upgrade has a noticeable performance impact and no security benefit before Q-day. This means itâs a hard sell to enable today by default. But here we are playing with fire: migrations always take longer than expected. If we want to keep an ubiquitously private and secure Internet, we need a post-quantum solution thatâs performant enough to be enabled by default today.&lt;/p&gt;
      &lt;p&gt;Merkle Tree Certificates (MTCs) solves this problem by reducing the number of signatures and public keys to the bare minimum while maintaining the WebPKI's essential properties. We plan to roll out MTCs to a fraction of free accounts by early next year. This does not affect any visitors that are not part of the Chrome experiment. For those that are, thanks to the bootstrap certificates, there is no impact on security.&lt;/p&gt;
      &lt;p&gt;Weâre excited to keep the Internet fast and secure, and will report back soon on the results of this experiment: watch this space! MTC is evolving as we speak, if you want to get involved, please join the IETF PLANTS mailing list.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45740214</guid><pubDate>Tue, 28 Oct 2025 22:39:24 +0000</pubDate></item><item><title>Project Shadowglass</title><link>https://shadowglassgame.com</link><description>&lt;doc fingerprint="a32b43117e80e528"&gt;
  &lt;main&gt;&lt;p&gt;Step into the shadows.&lt;/p&gt;&lt;p&gt;Step into the shadows.&lt;/p&gt;&lt;head rend="h1"&gt;Now in Development&lt;/head&gt;&lt;p&gt;Project Shadowglass (working title) is a love letter to classic immersive sims such as Thief, Deus Ex, and System Shock, built with unique 3D pixel art technology.&lt;/p&gt;&lt;p&gt;You play a struggling thief in a dark, oppressive world where the rich grow richer and the poor are pushed further into the shadows. To succeed, you will need to rely on your wits and tools of the trade.&lt;/p&gt;&lt;p&gt;Please Note: All content shown is early in development and subject to change, including the game title, features, and release timeline.&lt;/p&gt;&lt;head rend="h2"&gt;Game Screenshots and Media&lt;/head&gt;&lt;head rend="h2"&gt;Is this... AI?&lt;/head&gt;&lt;p&gt;No. Everything you see is 100% real and running realtime in-engine.&lt;/p&gt;&lt;p&gt;It may look similar to those pretty pixel art AI videos, but this is the playable real deal. No content or art featured in Project Shadowglass will be AI generated.&lt;/p&gt;&lt;p&gt;Even the placeholder assets you see in this early footage are open source creative commons creations made by real people (you can see the full credit list here).&lt;/p&gt;&lt;head rend="h2"&gt;The Tech&lt;/head&gt;&lt;p&gt;Project Shadowglass is built with unique 3D pixel art technology that delivers nostalgic pixelated graphics in smooth, full 360Â° freedom. It is based off of the culmination of 3D pixel art work I've spent over a decade perfecting.&lt;/p&gt;&lt;p&gt;Walk around objects, examine details from any angle, and climb structures while enjoying those perfect little square pixels.&lt;/p&gt;&lt;head rend="h2"&gt;Development Status&lt;/head&gt;&lt;head rend="h4"&gt;Early Development&lt;/head&gt;&lt;p&gt;Building core systems&lt;/p&gt;Current&lt;head rend="h4"&gt;Alpha Demo&lt;/head&gt;&lt;p&gt;First playable mission&lt;/p&gt;Early 2026&lt;head rend="h4"&gt;Beta Testing&lt;/head&gt;&lt;p&gt;Community feedback&lt;/p&gt;TBD&lt;head rend="h4"&gt;Early Access&lt;/head&gt;&lt;p&gt;Expanded content&lt;/p&gt;2027&lt;head rend="h4"&gt;Full Release&lt;/head&gt;&lt;p&gt;Complete experience&lt;/p&gt;TBD&lt;p&gt;Get early access to playable demos, dev updates, and help shape Project Shadowglass before release!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45741391</guid><pubDate>Wed, 29 Oct 2025 01:06:32 +0000</pubDate></item><item><title>Tips for stroke-surviving software engineers</title><link>https://blog.j11y.io/2025-10-29_stroke_tips_for_engineers/</link><description>&lt;doc fingerprint="f62a9b5785b1fadc"&gt;
  &lt;main&gt;
    &lt;p&gt;2025-10-29&lt;/p&gt;
    &lt;p&gt;This is a pretty niche topic; I don't imagine there are many of us out there.&lt;/p&gt;
    &lt;p&gt;Actually, to be strict, I'd say this advice is tailored to people who've had hemorrhagic stroke in the parietal lobe with residual epilepsy...&lt;/p&gt;
    &lt;p&gt;I was 29 and around 12 years into my career when it all happened, and in the six years since then I've had time to learn a bit more about my new self.&lt;/p&gt;
    &lt;p&gt;The first tip is to just stop. Fatigue, fuzziness, nausea, or affected-sided weird sensations are non-negotiable stop signals. So go lie down, hydrate, reset. Close your eyes and think about the cottage or lonely mountain you want to retire to. Escape the overwhelming mental or physical space.&lt;/p&gt;
    &lt;p&gt;HEADPHONES, blinders, and 'No'. Eliminate unwanted inputs at the earliest point of entry. Work from home or environments where you can control most variables. Routes of escape and rest are important.&lt;/p&gt;
    &lt;p&gt;Health above performance every single time. Metrics and productivity be damned. Self-advocate, and all that. Reject with directness any demands made of you that cross the threshold.&lt;/p&gt;
    &lt;p&gt;Laws. Use them. You don't have to rely on good behaviour and kindness. You are, depending on your location, usually protected by all types of anti-discrimination legislation, implicit and explicit. Use your employee assistance programs too.&lt;/p&gt;
    &lt;p&gt;Single-thread it all! Less context switching. Batch your work, finish one thing, then move to the next. Externalize working-memory. Use notebooks, whiteboards, and lists instead of juggling state in your head. I am not good at this, and over-stretch my brain, leading to auras, overwhelm, and general sickness. Terrible idea.&lt;/p&gt;
    &lt;p&gt;Related: Sssh to the AI naysayers. Use it as your help and scratchpad. Let it hold state so your brain can judge rather than store and needlessly cogitate on stuff. You don't have to do this alone out of some purity fetishism. You, too, have a limited context window. Sorry!&lt;/p&gt;
    &lt;p&gt;Do the heavy thinking in your peak window (for me, that's the morning); push everything else to later. Spend your time more carefully than your money.&lt;/p&gt;
    &lt;p&gt;Pick the route of least attention. Attention is expensive, and rarely needed as much as we think it is. It's a heavy toll to pay. Unless you're in an ops or monitoring role, you don't need to be synchronously active. DISABLE NOTIFICATIONS.&lt;/p&gt;
    &lt;p&gt;AVOID long meetings. Emails are good. Oh god am I bad at this? YES, I like people so I like some meetings, but communicating is so so expensive. Being polite is also expensive; It's not nice to have to tell people they're draining you.&lt;/p&gt;
    &lt;p&gt;I think that's mostly it. I'm still working on this stuff. And would probably grade myself pretty poorly. One day I'll be better at saying no, at advocating for myself, and knowing how to navigate the disappointment of others.&lt;/p&gt;
    &lt;p&gt;Footnote &amp;amp; some casual research: If you're into this, here's some stuff I found out related to my specific injury location and how it might apply to my work. This was gathered with help from gemini when I was struggling with left-arm and eye prodromes after long coding sessions:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Frontal and parietal cortices form a flexible control system that holds goals, routes attention, and updates task sets; this "multiple-demand" network scales with task complexity and underpins how we store, manipulate, and decide on information during work[1][2][3]. Superior parietal cortex is especially taxed when we transform or reorganize information in working memory rather than simply maintain it, which is why mental navigations, refactors, and other transformations feel costly[4][5]. Frequent context switches recruit lateral prefrontal and parietal regions and increase control load, so hopping between threads repeatedly spikes demand on this same circuitry[6][7]. After AVM resection (what I had!) or stroke generally, tissue near the lesion can remain hyperexcitable with impaired neurovascular coupling; heavy cognitive load lowers seizure threshold and can produce somatosensory auras and body-image distortions from parietal cortex[8][9][10].&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thanks for reading :) Tonnes of love to all the stroke survivors out there &amp;lt;3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742419</guid><pubDate>Wed, 29 Oct 2025 03:51:56 +0000</pubDate></item><item><title>uBlock Origin Lite Apple App Store</title><link>https://apps.apple.com/in/app/ublock-origin-lite/id6745342698</link><description>&lt;doc fingerprint="b62ad8dff7b5baa7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;uBlock Origin Lite 4+&lt;/head&gt;
    &lt;head rend="h2"&gt;An efficient content blocker&lt;/head&gt;
    &lt;head rend="h2"&gt;Raymond Hill&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Free&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Screenshots&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;uBO Lite (uBOL) is a reliable and efficient content blocker.&lt;lb/&gt;The default ruleset corresponds to uBlock Origin's default filterset:&lt;lb/&gt;- uBlock Origin's built-in filter lists&lt;lb/&gt;- EasyList&lt;lb/&gt;- EasyPrivacy&lt;lb/&gt;- Peter Loweâs Ad and tracking server list&lt;lb/&gt;You can enable more rulesets by visiting the options page -- click the _Cogs_ icon in the popup panel.&lt;lb/&gt;uBOL is entirely declarative, meaning there is no need for a permanent uBOL process for the filtering to occur, and CSS/JS injection-based content filtering is performed reliably by the browser itself rather than by the extension. This means that uBOL itself does not consume CPU/memory resources while content blocking is ongoing -- uBOL's service worker process is required _only_ when you interact with the popup panel or the option pages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whatâs New&lt;/head&gt;
    &lt;p&gt;Version 2025.1019.1656&lt;/p&gt;
    &lt;p&gt;â¢ Automatically select optimal for newly allowed hosts&lt;lb/&gt;â¢ Updated filter lists&lt;/p&gt;
    &lt;head rend="h2"&gt;Ratings and Reviews&lt;/head&gt;
    &lt;head rend="h3"&gt;The best content blocker is finally on iPadOS!!&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Itâs was a really long wait, but finally we are able to use it directly on the iPad. The first TestFlight version had a big battery drain, but itâs better now on the official release. The only limitation is that we canât add our own lists, but I am fine with the default lists ans it works perfect.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;I was looking for this long time finally itâs here.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;I love block. I do not want to use chrome. Itâs perfect. I can use this in safari..&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Finally ð¤©&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Waiting for many years. Added in all apple devices. Working properly. ðð&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;App Privacy&lt;/head&gt;
    &lt;p&gt;The developer, Raymond Hill, indicated that the appâs privacy practices may include handling of data as described below. For more information, see the developerâs privacy policy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Data Not Collected&lt;/head&gt;
    &lt;p&gt;The developer does not collect any data from this app.&lt;/p&gt;
    &lt;p&gt;Privacy practices may vary based on, for example, the features you use or your age. LearnÂ More&lt;/p&gt;
    &lt;head rend="h2"&gt;Information&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Provider&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Raymond Hill&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Size&lt;/item&gt;
      &lt;item rend="dd-2"&gt;6 MB&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Category&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Utilities&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Compatibility&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-5"&gt;iPhone&lt;/item&gt;
          &lt;item rend="dd-5"&gt;Requires iOS 18.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-6"&gt;iPad&lt;/item&gt;
          &lt;item rend="dd-6"&gt;Requires iPadOS 18.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-7"&gt;Mac&lt;/item&gt;
          &lt;item rend="dd-7"&gt;Requires macOS 13.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-8"&gt;Apple Vision&lt;/item&gt;
          &lt;item rend="dd-8"&gt;Requires visionOS 2.5 or later.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-9"&gt;Languages&lt;/item&gt;
      &lt;item rend="dd-9"&gt;
        &lt;p&gt;English&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-10"&gt;Age Rating&lt;/item&gt;
      &lt;item rend="dd-11"&gt;Learn More&lt;/item&gt;
      &lt;item rend="dt-12"&gt;Copyright&lt;/item&gt;
      &lt;item rend="dd-12"&gt;Â© Raymond Hill 2025&lt;/item&gt;
      &lt;item rend="dt-13"&gt;Price&lt;/item&gt;
      &lt;item rend="dd-13"&gt;Free&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742446</guid><pubDate>Wed, 29 Oct 2025 03:57:06 +0000</pubDate></item><item><title>Keep Android Open</title><link>http://keepandroidopen.org/</link><description>&lt;doc fingerprint="9da6d2e399ba52e9"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Keep Android Open&lt;/head&gt;
      &lt;p&gt;In August 2025, Google announced that starting next year, it will no longer be possible to develop apps for the Android platform without first registering centrally with Google.&lt;/p&gt;
      &lt;p&gt;This registration will involve:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Paying a fee to Google&lt;/item&gt;
        &lt;item&gt;Agreeing to Google’s Terms and Conditions&lt;/item&gt;
        &lt;item&gt;Providing government identification&lt;/item&gt;
        &lt;item&gt;Uploading evidence of an app’s private signing key&lt;/item&gt;
        &lt;item&gt;Listing all current and future application identifiers&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Some actions you can take to help oppose the enactment of this policy are:&lt;/p&gt;
      &lt;head rend="h2"&gt;Sign the Open Letter&lt;/head&gt;
      &lt;head rend="h2"&gt;European Union&lt;/head&gt;
      &lt;head rend="h2"&gt;United States&lt;/head&gt;
      &lt;head rend="h2"&gt;United Kingdom&lt;/head&gt;
      &lt;head rend="h2"&gt;Brazil&lt;/head&gt;
      &lt;head rend="h2"&gt;Other&lt;/head&gt;
      &lt;head rend="h2"&gt;References&lt;/head&gt;
      &lt;head rend="h3"&gt;Overview&lt;/head&gt;
      &lt;head rend="h3"&gt;Press Reactions&lt;/head&gt;
      &lt;head rend="h3"&gt;Video Responses&lt;/head&gt;
      &lt;head rend="h3"&gt;Editorials and Blogs&lt;/head&gt;
      &lt;head rend="h3"&gt;Discussions&lt;/head&gt;
      &lt;head rend="h3"&gt;Official Documentation&lt;/head&gt;
      &lt;head rend="h3"&gt;Miscellaneous&lt;/head&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742488</guid><pubDate>Wed, 29 Oct 2025 04:03:41 +0000</pubDate></item><item><title>Wacl – A Tcl Distribution for WebAssembly</title><link>https://github.com/ecky-l/wacl</link><description>&lt;doc fingerprint="9ecd0e53a7e724a0"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a Tcl distribution for WebAssembly (webassembly.org). It enables Web developers to embed a Tcl interpreter in the browser and integrate Tcl with JavaScript. It enables Tcl developers to use their tools and language of choice to create client side web applications. It enables all developers to reuse a great and (over decades) grown code base of useful packages and scripts, such as Tcllib, to be used in web browsers.&lt;/p&gt;
    &lt;p&gt;It is an extension of the Emtcl project from Aidan Hobsen, which can be found here. But Wacl takes things a few steps further: it integrates a fully featured Tcl interpreter into the webpage and adds the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A main tclsh interpreter and capability to get it via JavaScript&lt;/item&gt;
      &lt;item&gt;An event loop to process all Tcl events (timer events, fileevents, custom events)&lt;/item&gt;
      &lt;item&gt;Client sockets. The socket -async ... command connects to websocket servers with the binary protocol. The resulting handle can be used to transmit binary data as with normal TCP sockets.&lt;/item&gt;
      &lt;item&gt;The Tcl library: modules and packages in the Emscripten virtual filesystem. You can add your own packages!&lt;/item&gt;
      &lt;item&gt;Proper initialization via Tcl_Init()&lt;/item&gt;
      &lt;item&gt;An extension to call javascript functions from Tcl&lt;/item&gt;
      &lt;item&gt;various useful extensions (see below for a list and comments)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The original illustrative dom command has been moved to the "wacl" namespace in the package of same name, which is available right at startup. This package contains also a command ::wacl::jscall to call javascript functions from Tcl which have been registered before via the jswrap() module function.&lt;/p&gt;
    &lt;p&gt;The code compiles fine with Emscripten 1.37.9 to JavaScript and WebAssembly. The latter is the preferred format: WebAssembly is only half the size of the JavaScript "asm.js" output (~1.4MB vs. 2.9MB) and at least twice as fast! However, that could induce incompatibilities with older browsers, which don't (yet) support WebAssembly.&lt;/p&gt;
    &lt;p&gt;The following extensions are included in Wacl&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;wacl native extension with commands wacl::dom and wacl::jscall&lt;/item&gt;
      &lt;item&gt;tDOM for parsing and creating XML and HTML content&lt;/item&gt;
      &lt;item&gt;json and json::write from tcllib&lt;/item&gt;
      &lt;item&gt;html from tcllib&lt;/item&gt;
      &lt;item&gt;javascript from tcllib&lt;/item&gt;
      &lt;item&gt;ncgi as dependency for html&lt;/item&gt;
      &lt;item&gt;rl_json A Tcl_Obj type for efficient JSON parsing and generation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More extensions can easily be included and used. C extensions can be compiled with Emscripten (with USE_TCL_STUBS disabled and statically initialized via waclAppInit()) and Tcl extensions can be included in the library virtual filesystem.&lt;/p&gt;
    &lt;p&gt;But be aware that including extensions is a tradeoff: for the additional functionality you pay with a larger download size. The really useful tDOM extension for instance increases the Wacl distribution by not less than 400kB, which must be downloaded to the users client when (s)he wants to run a wacl based application, and this can be painful with lower bandwidth. Thus it is better to limit the number of packages to what is necessary rather than to build a batteries included distribution which contains everything.&lt;/p&gt;
    &lt;p&gt;You can try it out here. You can download the precompiled version with the index page to play on your own webpage by downloading the precompiled binary from here. Both of these pages require a recent browser with webassembly support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mozilla Firefox &amp;gt;= 52.0&lt;/item&gt;
      &lt;item&gt;Google Chrome &amp;gt;= 57.0&lt;/item&gt;
      &lt;item&gt;Microsoft Edge (Windows 10 "Creators" update)&lt;/item&gt;
      &lt;item&gt;Opera&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wacl will compile on a Unix/Linux environment with the following tools installed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the Emscripten SDK. Installation is documented on its web page&lt;/item&gt;
      &lt;item&gt;make, autoconf&lt;/item&gt;
      &lt;item&gt;diff, patch (some patches to the original sources must be applied, this is done mostly automatically)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Windows is not supported, but macOS with the appropriate tools from MacPorts will probably work (not tested by myself).&lt;/p&gt;
    &lt;p&gt;First step is to checkout this repository.This will checkout the files in the current directory. There is a Makefile with the build steps and a README with instructione. The make procedure does merely download the tcl core sources, apply a small patch and configure &amp;amp; build the interpreter to a webassembly plus accompanying .data + .js files. These files can be deployed to the corresponding web source directories. The Emscripten SDK must be on the PATH (i.e. via source $EMSCRIPTEN/emsdk_set_env.sh). Once wacl is built, it can be used in any browser which supports webassembly, also on Windows.&lt;lb/&gt; The build system can be changed to produce javascript instead of webassembly, by simply removing the -s WASM=1 flag from the BCFLAGS variable in the Makefile. This will generate a larger (~2.8MB), yet minified .js output, which is slower at runtime, but compatible with browsers that don't support webassembly.&lt;/p&gt;
    &lt;p&gt;To build it, you need the emscripten sdk on your path. Then:&lt;/p&gt;
    &lt;code&gt;$ make waclprep  # One off prep - tcl-core download, hacks.patch application and autoconf
$ make config    # create build directory and run emconfigure tcl/unix/configure
$ make [all]     # create the library and emtcl.js
$ make install   # copy emtcl.js to ../www/js/
&lt;/code&gt;
    &lt;p&gt;If you want to totally reset all build files in ./tcl/ and start again:&lt;/p&gt;
    &lt;code&gt;$ make reset
&lt;/code&gt;
    &lt;p&gt;This removes all changes and untracked files in there, so be careful!&lt;/p&gt;
    &lt;p&gt;There is a target to recreate the patch, if you changed anything important in tcl/&lt;/p&gt;
    &lt;code&gt;$ make patch
&lt;/code&gt;
    &lt;p&gt;It downloads tcl-core (it not already present), extracts it and runs diff between it and tcl/. The result is the patch that is applied above via "make tclprep"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742616</guid><pubDate>Wed, 29 Oct 2025 04:25:24 +0000</pubDate></item><item><title>Who needs Graphviz when you can build it yourself?</title><link>https://spidermonkey.dev/blog/2025/10/28/iongraph-web.html</link><description>&lt;doc fingerprint="af4c3a680161d9ab"&gt;
  &lt;main&gt;
    &lt;p&gt;We recently overhauled our internal tools for visualizing the compilation of JavaScript and WebAssembly. When SpiderMonkey’s optimizing compiler, Ion, is active, we can now produce interactive graphs showing exactly how functions are processed and optimized.&lt;/p&gt;
    &lt;p&gt;You can play with these graphs right here on this page. Simply write some JavaScript code in the &lt;code&gt;test&lt;/code&gt; function and see what graph is produced. You can click and drag to navigate, ctrl-scroll to zoom, and drag the slider at the bottom to scrub through the optimization process.&lt;/p&gt;
    &lt;p&gt;As you experiment, take note of how stable the graph layout is, even as the sizes of blocks change or new structures are added. Try clicking a block's title to select it, then drag the slider and watch the graph change while the block remains in place. Or, click an instruction's number to highlight it so you can keep an eye on it across passes.&lt;/p&gt;
    &lt;p&gt;We are not the first to visualize our compiler’s internal graphs, of course, nor the first to make them interactive. But I was not satisfied with the output of common tools like Graphviz or Mermaid, so I decided to create a layout algorithm specifically tailored to our needs. The resulting algorithm is simple, fast, produces surprisingly high-quality output, and can be implemented in less than a thousand lines of code. The purpose of this article is to walk you through this algorithm and the design concepts behind it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;As readers of this blog already know, SpiderMonkey has several tiers of execution for JavaScript and WebAssembly code. The highest tier is known as Ion, an optimizing SSA compiler that takes the most time to compile but produces the highest-quality output.&lt;/p&gt;
    &lt;p&gt;Working with Ion frequently requires us to visualize and debug the SSA graph. Since 2011 we have used a tool for this purpose called iongraph, built by Sean Stangl. It is a simple Python script that takes a JSON dump of our compiler graphs and uses Graphviz to produce a PDF. It is perfectly adequate, and very much the status quo for compiler authors, but unfortunately the Graphviz output has many problems that make our work tedious and frustrating.&lt;/p&gt;
    &lt;p&gt;The first problem is that the Graphviz output rarely bears any resemblance to the source code that produced it. Graphviz will place nodes wherever it feels will minimize error, resulting in a graph that snakes left and right seemingly at random. There is no visual intuition for how deeply nested a block of code is, nor is it easy to determine which blocks are inside or outside of loops. Consider the following function, and its Graphviz graph:&lt;/p&gt;
    &lt;code&gt;function foo(n) {
  let result = 0;
  for (let i = 0; i &amp;lt; n; i++) {
    if (!!(i % 2)) {
      result = 0x600DBEEF;
    } else {
      result = 0xBADBEEF;
    }
  }

  return result;
}
&lt;/code&gt;
    &lt;p&gt;Counterintuitively, the &lt;code&gt;return&lt;/code&gt; appears before the two assignments in the body of the loop. Since this graph mirrors JavaScript control flow, we’d expect to see the return at the bottom. This problem only gets worse as graphs grow larger and more complex.&lt;/p&gt;
    &lt;p&gt;The second, related problem is that Graphviz’s output is unstable. Small changes to the input can result in large changes to the output. As you page through the graphs of each pass within Ion, nodes will jump left and right, true and false branches will swap, loops will run up the right side instead of the left, and so on. This makes it very hard to understand the actual effect of any given pass. Consider the following before and after, and notice how the second graph is almost—but not quite—a mirror image of the first, despite very minimal changes to the graph’s structure:&lt;/p&gt;
    &lt;p&gt;None of this felt right to me. Control flow graphs should be able to follow the structure of the program that produced them. After all, a control flow graph has many restrictions that a general-purpose tool would not be aware of: they have very few cycles, all of which are well-defined because they come from loops; furthermore, both JavaScript and WebAssembly have reducible control flow, meaning all loops have only one entry, and it is not possible to jump directly into the middle of a loop. This information could be used to our advantage.&lt;/p&gt;
    &lt;p&gt;Beyond that, a static PDF is far from ideal when exploring complicated graphs. Finding the inputs or uses of a given instruction is a tedious and frustrating exercise, as is following arrows from block to block. Even just zooming in and out is difficult. I eventually concluded that we ought to just build an interactive tool to overcome these limitations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How hard could layout be?&lt;/head&gt;
    &lt;p&gt;I had one false start with graph layout, with an algorithm that attempted to sort blocks into vertical “tracks”. This broke down quickly on a variety of programs and I was forced to go back to the drawing board—in fact, back to the source of the very tool I was trying to replace.&lt;/p&gt;
    &lt;p&gt;The algorithm used by &lt;code&gt;dot&lt;/code&gt;, the typical hierarchical layout mode for Graphviz, is known as the Sugiyama layout algorithm, from a 1981 paper by Sugiyama et al. As introduction, I found a short series of lectures that broke down the Sugiyama algorithm into 5 steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cycle breaking, where the direction of some edges are flipped in order to produce a DAG.&lt;/item&gt;
      &lt;item&gt;Leveling, where vertices are assigned into horizontal layers according to their depth in the graph, and dummy vertices are added to any edge that crosses multiple layers.&lt;/item&gt;
      &lt;item&gt;Crossing minimization, where vertices on a layer are reordered in order to minimize the number of edge crossings.&lt;/item&gt;
      &lt;item&gt;Vertex positioning, where vertices are horizontally positioned in order to make the edges as straight as possible.&lt;/item&gt;
      &lt;item&gt;Drawing, where the final graph is rendered to the screen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These steps struck me as surprisingly straightforward, and provided useful opportunities to insert our own knowledge of the problem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cycle breaking would be trivial for us, since the only cycles in our data are loops, and loop backedges are explicitly labeled. We could simply ignore backedges when laying out the graph.&lt;/item&gt;
      &lt;item&gt;Leveling would be straightforward, and could easily be modified to better mimic the source code. Specifically, any blocks coming after a loop in the source code could be artificially pushed down in the layout, solving the confusing early-exit problem.&lt;/item&gt;
      &lt;item&gt;Permuting vertices to reduce edge crossings was actually just a bad idea, since our goal was stability from graph to graph. The true and false branches of a condition should always appear in the same order, for example, and a few edge crossings is a small price to pay for this stability.&lt;/item&gt;
      &lt;item&gt;Since reducible control flow ensures that a program’s loops form a tree, vertex positioning could ensure that loops are always well-nested in the final graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Taken all together, these simplifications resulted in a remarkably straightforward algorithm, with the initial implementation being just 1000 lines of JavaScript. (See this demo for what it looked like at the time.) It also proved to be very efficient, since it avoided the most computationally complex parts of the Sugiyama algorithm.&lt;/p&gt;
    &lt;head rend="h2"&gt;iongraph from start to finish&lt;/head&gt;
    &lt;p&gt;We will now go through the entire iongraph layout algorithm. Each section contains explanatory diagrams, in which rectangles are basic blocks and circles are dummy nodes. Loop header blocks (the single entry point to each loop) are additionally colored green.&lt;/p&gt;
    &lt;p&gt;Be aware that the block positions in these diagrams are not representative of the actual computed layout position at each point in the process. For example, vertical positions are not calculated until the very end, but it would be hard to communicate what the algorithm was doing if all blocks were drawn on a single line!&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 1: Layering&lt;/head&gt;
    &lt;p&gt;We first sort the basic blocks into horizontal tracks called “layers”. This is very simple; we just start at layer 0 and recursively walk the graph, incrementing the layer number as we go. As we go, we track the “height” of each loop, not in pixels, but in layers.&lt;/p&gt;
    &lt;p&gt;We also take this opportunity to vertically position nodes “inside” and “outside” of loops. Whenever we see an edge that exits a loop, we defer the layering of the destination block until we are done layering the loop contents, at which point we know the loop’s height.&lt;/p&gt;
    &lt;p&gt;A note on implementation: nodes are visited multiple times throughout the process, not just once. This can produce a quadratic explosion for large graphs, but I find that an early-out is sufficient to avoid this problem in practice.&lt;/p&gt;
    &lt;p&gt;The animation below shows the layering algorithm in action. Notice how the final block in the graph is visited twice, once after each loop that branches to it, and in each case, the block is deferred until the entire loop has been layered, rather than processed immediately after its predecessor block. The final position of the block is below the entirety of both loops, rather than directly below one of its predecessors as Graphviz would do. (Remember, horizontal and vertical positions have not yet been computed; the positions of the blocks in this diagram are hardcoded for demonstration purposes.)&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=layering*/function layerBlock(block, layer = 0) {
  // Omitted for clarity: special handling of our "backedge blocks"

  // Early out if the block would not be updated
  if (layer &amp;lt;= block.layer) {
    return;
  }

  // Update the layer of the current block
  block.layer = Math.max(block.layer, layer);

  // Update the heights of all loops containing the current block
  let header = block.loopHeader;
  while (header) {
    header.loopHeight = Math.max(header.loopHeight, block.layer - header.layer + 1);
    header = header.parentLoopHeader;
  }

  // Recursively layer successors
  for (const succ of block.successors) {
    if (succ.loopDepth &amp;lt; block.loopDepth) {
      // Outgoing edges from the current loop will be layered later
      block.loopHeader.outgoingEdges.push(succ);
    } else {
      layerBlock(succ, layer + 1);
    }
  }

  // Layer any outgoing edges only after the contents of the loop have
  // been processed
  if (block.isLoopHeader()) {
    for (const succ of block.outgoingEdges) {
      layerBlock(succ, layer + block.loopHeight);
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 2: Create dummy nodes&lt;/head&gt;
    &lt;p&gt;Any time an edge crosses a layer, we create a dummy node. This allows edges to be routed across layers without overlapping any blocks. Unlike in traditional Sugiyama, we always put downward dummies on the left and upward dummies on the right, producing a consistent “counter-clockwise” flow. This also makes it easy to read long vertical edges, whose direction would otherwise be ambiguous. (Recall how the loop backedge flipped from the right to the left in the “unstable layout” Graphviz example from before.)&lt;/p&gt;
    &lt;p&gt;In addition, we coalesce any edges that are going to the same destination by merging their dummy nodes. This heavily reduces visual noise.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 3: Straighten edges&lt;/head&gt;
    &lt;p&gt;This is the fuzziest and most ad-hoc part of the process. Basically, we run lots of small passes that walk up and down the graph, aligning layout nodes with each other. Our edge-straightening passes include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pushing nodes to the right of their loop header to “indent” them.&lt;/item&gt;
      &lt;item&gt;Walking a layer left to right, moving children to the right to line up with their parents. If any nodes overlap as a result, they are pushed further to the right.&lt;/item&gt;
      &lt;item&gt;Walking a layer right to left, moving parents to the right to line up with their children. This version is more conservative and will not move a node if it would overlap with another. This cleans up most issues from the first pass.&lt;/item&gt;
      &lt;item&gt;Straightening runs of dummy nodes so we have clean vertical lines.&lt;/item&gt;
      &lt;item&gt;“Sucking in” dummy runs on the left side of the graph if there is room for them to move to the right.&lt;/item&gt;
      &lt;item&gt;Straighten out any edges that are “nearly straight”, according to a chosen threshold. This makes the graph appear less wobbly. We do this by repeatedly “combing” the graph upward and downward, aligning parents with children, then children with parents, and so on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is important to note that dummy nodes participate fully in this system. If for example you have two side-by-side loops, straightening the left loop’s backedge will push the right loop to the side, avoiding overlaps and preserving the graph’s visual structure.&lt;/p&gt;
    &lt;p&gt;We do not reach a fixed point with this strategy, nor do we attempt to. I find that if you continue to repeatedly apply these particular layout passes, nodes will wander to the right forever. Instead, the layout passes are hand-tuned to produce decent-looking results for most of the graphs we look at on a regular basis. That said, this could certainly be improved, especially for larger graphs which do benefit from more iterations.&lt;/p&gt;
    &lt;p&gt;At the end of this step, all nodes have a fixed X-coordinate and will not be modified further.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 4: Track horizontal edges&lt;/head&gt;
    &lt;p&gt;Edges may overlap visually as they run horizontally between layers. To resolve this, we sort edges into parallel “tracks”, giving each a vertical offset. After tracking all the edges, we record the total height of the tracks and store it on the preceding layer as its “track height”. This allows us to leave room for the edges in the final layout step.&lt;/p&gt;
    &lt;p&gt;We first sort edges by their starting position, left to right. This produces a consistent arrangement of edges that has few vertical crossings in practice. Edges are then placed into tracks from the “outside in”, stacking rightward edges on top and leftward edges on the bottom, creating a new track if the edge would overlap with or cross any other edge.&lt;/p&gt;
    &lt;p&gt;The diagram below is interactive. Click and drag the blocks to see how the horizontal edges get assigned to tracks.&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=tracks*/function trackHorizontalEdges(layer) {
  const TRACK_SPACING = 20;

  // Gather all edges on the layer, and sort left to right by starting coordinate
  const layerEdges = [];
  for (const node of layer.nodes) {
    for (const edge of node.edges) {
      layerEdges.push(edge);
    }
  }
  layerEdges.sort((a, b) =&amp;gt; a.startX - b.startX);

  // Assign edges to "tracks" based on whether they overlap horizontally with
  // each other. We walk the tracks from the outside in and stop if we ever
  // overlap with any other edge.
  const rightwardTracks = []; // [][]Edge
  const leftwardTracks = [];  // [][]Edge
  nextEdge:
  for (const edge of layerEdges) {
    const trackSet = edge.endX - edge.startX &amp;gt;= 0 ? rightwardTracks : leftwardTracks;
    let lastValidTrack = null; // []Edge | null

    // Iterate through the tracks in reverse order (outside in)
    for (let i = trackSet.length - 1; i &amp;gt;= 0; i--) {
      const track = trackSet[i];
      let overlapsWithAnyInThisTrack = false;
      for (const otherEdge of track) {
        if (edge.dst === otherEdge.dst) {
          // Assign the edge to this track to merge arrows
          track.push(edge);
          continue nextEdge;
        }

        const al = Math.min(edge.startX, edge.endX);
        const ar = Math.max(edge.startX, edge.endX);
        const bl = Math.min(otherEdge.startX, otherEdge.endX);
        const br = Math.max(otherEdge.startX, otherEdge.endX);
        const overlaps = ar &amp;gt;= bl &amp;amp;&amp;amp; al &amp;lt;= br;
        if (overlaps) {
          overlapsWithAnyInThisTrack = true;
          break;
        }
      }

      if (overlapsWithAnyInThisTrack) {
        break;
      } else {
        lastValidTrack = track;
      }
    }

    if (lastValidTrack) {
      lastValidTrack.push(edge);
    } else {
      trackSet.push([edge]);
    }
  }

  // Use track info to apply offsets to each edge for rendering.
  const tracksHeight = TRACK_SPACING * Math.max(
    0,
    rightwardTracks.length + leftwardTracks.length - 1,
  );
  let trackOffset = -tracksHeight / 2;
  for (const track of [...rightwardTracks.toReversed(), ...leftwardTracks]) {
    for (const edge of track) {
      edge.offset = trackOffset;
    }
    trackOffset += TRACK_SPACING;
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 5: Verticalize&lt;/head&gt;
    &lt;p&gt;Finally, we assign each node a Y-coordinate. Starting at a Y-coordinate of zero, we iterate through the layers, repeatedly adding the layer’s height and its track height, where the layer height is the maximum height of any node in the layer. All nodes within a layer receive the same Y-coordinate; this is simple and easier to read than Graphviz’s default of vertically centering nodes within a layer.&lt;/p&gt;
    &lt;p&gt;Now that every node has both an X and Y coordinate, the layout process is complete.&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=verticalize*/function verticalize(layers) {
  let layerY = 0;
  for (const layer of layers) {
    let layerHeight = 0;
    for (const node of layer.nodes) {
      node.y = layerY;
      layerHeight = Math.max(layerHeight, node.height);
    }
    layerY += layerHeight;
    layerY += layer.trackHeight;
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 6: Render&lt;/head&gt;
    &lt;p&gt;The details of rendering are out of scope for this article, and depend on the specific application. However, I wish to highlight a stylistic decision that I feel makes our graphs more readable.&lt;/p&gt;
    &lt;p&gt;When rendering edges, we use a style inspired by railroad diagrams. These have many advantages over the Bézier curves employed by Graphviz. First, straight lines feel more organized and are easier to follow when scrolling up and down. Second, they are easy to route (vertical when crossing layers, horizontal between layers). Third, they are easy to coalesce when they share a destination, and the junctions provide a clear indication of the edge’s direction. Fourth, they always cross at right angles, improving clarity and reducing the need to avoid edge crossings in the first place.&lt;/p&gt;
    &lt;p&gt;Consider the following example. There are several edge crossings that may traditionally be considered undesirable—yet the edges and their directions remain clear. Of particular note is the vertical junction highlighted in red on the left: not only is it immediately clear that these edges share a destination, but the junction itself signals that the edges are flowing downward. I find this much more pleasant than the “rat’s nest” that Graphviz tends to produce.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this work?&lt;/head&gt;
    &lt;p&gt;It may seem surprising that such a simple (and stupid) layout algorithm could produce such readable graphs, when more sophisticated layout algorithms struggle. However, I feel that the algorithm succeeds because of its simplicity.&lt;/p&gt;
    &lt;p&gt;Most graph layout algorithms are optimization problems, where error is minimized on some chosen metrics. However, these metrics seem to correlate poorly to readability in practice. For example, it seems good in theory to rearrange nodes to minimize edge crossings. But a predictable order of nodes seems to produce more sensible results overall, and simple rules for edge routing are sufficient to keep things tidy. (As a bonus, this also gives us layout stability from pass to pass.) Similarly, layout rules like “align parents with their children” produce more readable results than “minimize the lengths of edges”.&lt;/p&gt;
    &lt;p&gt;Furthermore, by rejecting the optimization problem, a human author gains more control over the layout. We are able to position nodes “inside” of loops, and push post-loop content down in the graph, because we reject this global constraint-solver approach. Minimizing “error” is meaningless compared to a human maximizing meaning through thoughtful design.&lt;/p&gt;
    &lt;p&gt;And finally, the resulting algorithm is simply more efficient. All the layout passes in iongraph are easy to program and scale gracefully to large graphs because they run in roughly linear time. It is better, in my view, to run a fixed number of layout iterations according to your graph complexity and time budget, rather than to run a complex constraint solver until it is “done”.&lt;/p&gt;
    &lt;p&gt;By following this philosophy, even the worst graphs become tractable. Below is a screenshot of a zlib function, compiled to WebAssembly, and rendered using the old tool.&lt;/p&gt;
    &lt;p&gt;It took about ten minutes for Graphviz to produce this spaghetti nightmare. By comparison, iongraph can now lay out this function in 20 milliseconds. The result is still not particularly beautiful, but it renders thousands of times faster and is much easier to navigate.&lt;/p&gt;
    &lt;p&gt;Perhaps programmers ought to put less trust into magic optimizing systems, especially when a human-friendly result is the goal. Simple (and stupid) algorithms can be very effective when applied with discretion and taste.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;We have already integrated iongraph into the Firefox profiler, making it easy for us to view the graphs of the most expensive or impactful functions we find in our performance work. Unfortunately, this is only available in specific builds of the SpiderMonkey shell, and is not available in full browser builds. This is due to architectural differences in how profiling data is captured and the flags with which the browser and shell are built. I would love for Firefox users to someday be able to view these graphs themselves, but at the moment we have no plans to expose this to the browser. However, one bug tracking some related work can be found here.&lt;/p&gt;
    &lt;p&gt;We will continue to sporadically update iongraph with more features to aid us in our work. We have several ideas for new features, including richer navigation, search, and visualization of register allocation info. However, we have no explicit roadmap for when these features may be released.&lt;/p&gt;
    &lt;p&gt;To experiment with iongraph locally, you can run a debug build of the SpiderMonkey shell with &lt;code&gt;IONFLAGS=logs&lt;/code&gt;; this will dump information to &lt;code&gt;/tmp/ion.json&lt;/code&gt;. This file can then be loaded into the standalone deployment of iongraph. Please be aware that the user experience is rough and unpolished in its current state.&lt;/p&gt;
    &lt;p&gt;The source code for iongraph can be found on GitHub. If this subject interests you, we would welcome contributions to iongraph and its integration into the browser. The best place to reach us is our Matrix chat.&lt;/p&gt;
    &lt;p&gt;Thanks to Matthew Gaudet, Asaf Gartner, and Colin Davidson for their feedback on this article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742907</guid><pubDate>Wed, 29 Oct 2025 05:17:49 +0000</pubDate></item></channel></rss>