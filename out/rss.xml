<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 26 Jan 2026 14:52:07 +0000</lastBuildDate><item><title>First, make me care</title><link>https://gwern.net/blog/2026/make-me-care</link><description>&lt;doc fingerprint="77b044a15c934274"&gt;
  &lt;main&gt;
    &lt;p&gt;First, Make Me Care Writing advice: some nonfiction fails because it opens with background instead of a hook—readers leave before reaching the good material. Find the single anomaly or question that makes your topic interesting, lead with that, and let the background follow once you’ve earned attention. [Return to blog index]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757067</guid><pubDate>Sun, 25 Jan 2026 19:03:40 +0000</pubDate></item><item><title>I was right about ATProto key management</title><link>https://notes.nora.codes/atproto-again/</link><description>&lt;doc fingerprint="ed9ccb1f2d2256d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Note: this post has been revised to be split into two sections: a description of what happened, and my analysis. I hope to make it clear that, while I do not like ATProto in general, I am trying to make good-faith critcisms of specific design decisions and outcomes, and in fact, this post getting updoots on HackerNews appears to have gotten the attention of the team, so, mission accomplished. ref, ref My account has since been manually reinstated; this has not happened for any of the other users that have had this issue, as far as I know.&lt;/p&gt;
    &lt;p&gt;Today, I tried setting up an ATProto account for use with Bluesky, with did:web instead of did:plc. Let’s walk through the process:&lt;/p&gt;
    &lt;p&gt;Set up the PDS software on a server I control. Because I use NixOS, this was very easy.&lt;/p&gt;
    &lt;p&gt;Create a did:web. This means creating a public-private keypair; I initially tried following this tutorial from Mai Lapyst, but it’s very out of date, and doesn’t include a critical step.&lt;/p&gt;
    &lt;p&gt;With that did:web, upload the &lt;code&gt;did.json&lt;/code&gt; document to my webserver and set the appropriate DNS entries. Easy enough, except that I also had to set the CORS header for the &lt;code&gt;did.json&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Create an account on my new PDS. I was able to get an invite and create an account, but it was in the “deactivated” status, and I couldn’t activate it. This had to be done by making requests manually with &lt;code&gt;curl&lt;/code&gt;, reading the error outcomes in the PDS’s logs on my server.&lt;/p&gt;
    &lt;p&gt;Seek help in the ATProto Touchers Discord server, and at their advice delete the account.&lt;/p&gt;
    &lt;p&gt;Start over and re-create everything from scratch, correctly replacing the public key in my DID with the public key from &lt;code&gt;getRecommendedDidCredentials&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Log into Bluesky (bsky.app) and get a “Profile does not exist error.”&lt;/p&gt;
    &lt;p&gt;It was at this point that I found this GitHub issue, which seems to imply that, since I deleted my (completely empty and unused) account, my did:web is blacklisted from the remaining mostly-centralized bit of the system, the AppView. The term for this is being “burned”, and it was later confirmed by some more experienced users that this is a known but undocumented behavior of the Bluesky AppView.&lt;/p&gt;
    &lt;p&gt;I had one of my friends who uses Bluesky take a look, and the failure mode is interesting. On Bluesky, I didn’t exist at all. (She could not see my likes, or my follow of her.) On Blacksky production, my display name and bio were visible, but not my posts. On Blacksky’s own AppView, it’s the opposite; my posts appeared under an “invalid” profile. I have been un-“burned” by a manual process, but not because of my support request; this post made it to the front page of Hacker News, and Bryan Newbold saw it there.&lt;/p&gt;
    &lt;p&gt;This is bad.&lt;/p&gt;
    &lt;p&gt;So, a while ago, I wrote a post called “Key Management, ATProto, and Decentralization” in which I complained about ATProto’s approach to decentralization. Since then, Blacksky has spun up an AppView, which makes it theoretically possible to have an actually decentralized experience on Bluesky. This was my line in the sand, stated many times; I would make an account when and only when it was possible to do so without using anything running on Bluesky-the-company’s hardware. That’s now, so I figured I’d try it.&lt;/p&gt;
    &lt;p&gt;I use lots of systems I don’t love, like Signal, Matrix, and Mastodon. I use them because they give me access to social interaction with people I care about. ATProto, and specifically Bluesky, is the same; I have friends who don’t post anywhere else. Today, I follow them by RSS, but can’t interact with their posts. That’s where my motivation to use the network comes from, along with understanding how, and how well, the newly decentralized AppView layer works.&lt;/p&gt;
    &lt;p&gt;Very little of this process is documented. Sure, the individual endpoints are - kind of - but the only place the whole process is collected in one place is in the comments to this GitHub issue… which is closed as WONTFIX. The documentation for that &lt;code&gt;getRecommendedDidCredentials&lt;/code&gt; endpoint that I missed reads in full:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Describe the credentials that should be included in the DID doc of an account that is migrating to this service.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Note that I am not “migrating”; this account is new. Plus, the JSON keys it returns are almost, but not quite, the same as those in a DID document, and the key it returns actually has to be edited by hand in order to be usable.&lt;/p&gt;
    &lt;p&gt;This is not good! &lt;code&gt;did:web&lt;/code&gt; has been held up as the “less centralized” or “bring your own trust” option, as opposed to &lt;code&gt;did:plc&lt;/code&gt;, and it seems like there has been very little effort to make it usable, certainly not for “normal” users.&lt;/p&gt;
    &lt;p&gt;But there’s another issue, a bigger issue. Why is a centralized “burn” able to completely prevent me from interacting with people using Bluesky?&lt;/p&gt;
    &lt;p&gt;You may be aware that Mastodon has a similar system. If you set up a Mastodon server and then delete the database, anyone you’ve already federated with won’t federate with you again, because you can’t prove you’re the same instance. It’s a genuine issue - but it wouldn’t have resulted in this, because I hadn’t even made a post on my now-burned did:web identity, nor followed anyone.&lt;/p&gt;
    &lt;p&gt;Even if I had, though, that would have burned a connection, not all connections. My experience would be degraded, but not ruined, and I could work with the admins of the affected servers to remediate it. You could say the same here, of course; I had to get my account back by Bryan Newbold happening to see this post on Hacker News. There is only, really, one connection that matters; maybe two, if you count Blacksky, but their AppView is not generally available yet. That’s centralization. I don’t understand how you could call it anything else.&lt;/p&gt;
    &lt;p&gt;I don’t like Bluesky, or ATProto; I wish we lived in a world were community-driven projects got megabucks and we were all self-hosting little social media servers for our communities. We don’t live in that world, so we have to interoperate with VC-backed, corporate social media. When those platforms call themselves “decentralized”, I think they should deliver.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757357</guid><pubDate>Sun, 25 Jan 2026 19:31:23 +0000</pubDate></item><item><title>LED lighting undermines visual performance unless supplemented by wider spectra</title><link>https://www.nature.com/articles/s41598-026-35389-6</link><description>&lt;doc fingerprint="541a12230e7947d4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Life evolved under broad spectrum sunlight, from ultraviolet to infrared (300–2500 nm). This spectrally balanced light sculpted life’s physiology and metabolism. But modern lighting has recently become dominated by restricted spectrum light emitting diodes (350–650 nm LEDs). Absence of longer wavelengths in LEDs and their short wavelength dominance impacts physiology, undermining normal mitochondrial respiration that regulates metabolism, disease and ageing. Mitochondria are light sensitive. The 420–450 nm dominant in LEDs suppresses respiration while deep red/infrared (670–900 nm) increases respiration in aging and some diseases including in blood sugar regulation. Here we supplement LED light with broad spectrum lighting (400–1500 nm+) for 2 weeks and test colour contrast sensitivity. We show significant improvement in this metric that last for 2 months after the supplemental lighting is removed. Mitochondria communicate across the body with systemic impacts following regional light exposure. This likely involves shifting patterns of serum cytokine expression, raising the possibility of wider negative impacts of LEDs on human health particularly, in the elderly or in the clinical environment where individuals are debilitated. Changing the lighting in these environments could be a highly economic route to improved public health.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Ambient light impacts on human health. Sunlight, under which life evolved, extends over approximately 300–2500 nm. Older incandescent lighting common until recently has a similar spectral range. But because our visual sensitivity is limited to 400–700 nm we are unaware of infrared light (approximately 700–2500 nm). However, light in the built environment is now driven by light emitting diodes (LEDs), whose restricted spectrum (approximately 350–650 nm) is designed around our visual sensitivity and consequently is economic1.&lt;/p&gt;
    &lt;p&gt;Typical LED lighting produces strong elements in the shorter blue wavelengths (420–450 nm) with a second yellow peak which drops swiftly above 650 nm, with little light above 700 nm1. Short wavelength exposure in animals in the range of 420–450 nm reduces mitochondrial function, which provides the energy for physiological performance in the form of adenosine triphosphate (ATP). This short wavelength light reduces mitochondrial complex activity and ATP production, in a highly conserved manner. Hoh Kam et al. showed a significant decrease in mitochondrial enzymatic activity in fruit flies for complexes I-IV under 420 nm light2. Kaynezhad et al. used broadband near infrared spectroscopy (bNIRS) imaging the mouse retina and reported significant instability of deoxygenated haemoglobin and oxidised cytochrome-c-oxidase after exposure to 420 nm light. This instability remained significant through a 1 h recovery period when the light was withdrawn3. Short wavelength light (420 and 450 nm) also results in increased body weight. Hussaini et al. demonstrated that mice exposed to these wavelengths gaining weight rapidly compared to controls over the course of eight weeks4. Shorter wavelengths in similar ranges are also associated with reduced lifespan. Nash et al. revealed a 50% drop in the median lifespan of fruit flies exposed to unfiltered white LED light relative to those kept in darkness, but only 4% drop if this LED light was passed through a yellow filter, blocking the shorter wavelength light5. This negative influence is likely due to mitochondrial absorption by porphyrin that may increase proinflammatory oxygen singlet production reducing mitochondrial function as proposed by Kaynezhad et al.3.&lt;/p&gt;
    &lt;p&gt;Longer wavelengths (700 nm+) penetrate deeply and those in sunlight can be measured passing through the human body6. These are absent from standard LEDs but present in sunlight and incandescent lighting. Their presence increases mitochondrial performance and ATP production, particularly when challenged by age or disease. Gkotsi et al. demonstrated significantly increased ATP production in the retina, cortex, and thalamus of mice following exposure to 670 nm light7. Calaza et al. revealed a 50% increase in ATP in eight-month-old complement factor H knock out mice that have a mitochondrial deficit and are used as a murine model of macular degeneration8.&lt;/p&gt;
    &lt;p&gt;Increased mitochondrial performance is associated with increased lifespan and enhanced mobility. Begum et al. demonstrated using fruit flies that exposure to 670 nm resulted in a positive divergence of ageing survival rates of 10% at 4 weeks of age and up to nearly 180% by 8 weeks of age. The older animals also displayed an almost doubling of mobility against controls9. Neonicotinoid insecticides specifically target mitochondrial respiration inducing Parkinson like symptoms of immobility resulting in death. Here 670 exposure reversed damaged ATP levels to normal and corrects mobility and lifespan issues10.&lt;/p&gt;
    &lt;p&gt;Increased mitochondrial activity should result in reduced blood sugars and increased oxygen consumption as mitochondria use both in respiration. Powner and Jeffery found both in bumble bees exposed to 670 nm light11. The same authors translated this to humans showing again, reduced blood sugars and increased oxygen consumption in a standard glucose tolerance test following 15 min of 670 nm exposure. Here the spike in blood glucose was reduced significantly by around 27%12.&lt;/p&gt;
    &lt;p&gt;Changes in physiology produced by longer wavelengths translate to improved function. Shinhmar et al. revealed improved colour contrast sensitivity in humans after 3 min of morning exposure to 670 nm light13. Hence, exposure to different ends of the spectrum that impact differentially on mitochondria can translate into changes in key physiological metrics.&lt;/p&gt;
    &lt;p&gt;Similar changes are found at the population level. Those spending more time in sunlight generally have improved health including reduced incidents of cardiovascular disease and the incidence of cancer. They also have lower rates of type 2 diabetes14,15.&lt;/p&gt;
    &lt;p&gt;In this study we confront the impact of LED lighting on human visual performance by measuring colour contrast detection in an LED illuminated working environment that is then supplemented with incandescent lighting. The hypothesis is that LED lighting suppresses mitochondrial function in the retina and that this can be corrected by introduction of wide spectrum incandescent lights. The results highlight the potential damaging influence of LED lighting on human performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methods&lt;/head&gt;
    &lt;p&gt;The Subjects and their environment: The study was conducted in accordance with the Declaration of Helsinki and approved by University College London research ethics committee (16547/001). It was undertaken in University College London buildings in October to December. In October local daylight hours were approximately 10.37 with 75% cloud cover. In November local daylight hours were approximately 8.45 with 55% cloud cover. In December local daylight hours were approximately 7.5 with 90% cloud cover. Local sunset time in October is approximately 18.30. In November it is approximately 16.15 and in December it is approximately 16.00. Consequently, many subjects would be returning home after sunset in November and December. Subjects worked approximately 8 h a day 5 days a week and travel to and from work via public transport that was illuminated by LED devices. Most subjects did not leave the building in which they worked during the working day in these months. For those that did it was commonly for less than 15 min at lunch time. Within the work environment subjects were free to move around. Here the internal lighting they experienced was consistently LED based. Hence, natural daylight exposure during this latter part of the year was limited. We could not control for weekend exposure, however subjects homes were consistently illuminated with LEDs and because the weather in the UK at this time of year is inclement, their time outside buildings can be expected to be limited.&lt;/p&gt;
    &lt;p&gt;Each participant provided written informed consent prior to testing and data generated was anonymised. Subjects (N = 22) were of both sexes and between the ages of 23 and 65 years. Prior to the experiment all subjects were asked to confirm normal corrected visual function and general good systemic health. This was undertaken in a simple interview prior to their inclusion in the study. All were healthy without visual or other health problems. Experimental subjects (N = 11) worked exclusively under LED lighting in the back of the Here East building on the north side, &amp;gt; 50 m from what little light did manage to penetrate the entrance doors when open. The LED lighting delivered an illuminance of 1000 lx at working height, with a correlated colour temperature (CCT) of 4000 K, and a TM-30 average colour fidelity index, Rf, of 91. The infrared light that was introduced was provided by tungsten desk lamps placed around the working space was non-uniform. The visible component of the 60 W tungsten bulbs was small when compared with the 1000 lx of LED. The test subjects were not expected to use these as task lamps. The LED lighting delivered an irradiance of 3.7 W/m2 on the horizontal working plane.&lt;/p&gt;
    &lt;p&gt;Control subjects (N = 11) worked in similar environments under LED lighting without direct sunlight. The LED lighting delivered an illuminance of 900 lx at working height, with a CCT of 3000 K and a Rf of 85. The colour contrast tests were performed in a darkened room where the only light came from the test itself. There were no requirements restricting other light exposure patterns during the study.&lt;/p&gt;
    &lt;p&gt;The experimental location: Subjects worked at UCL Here East, a media and innovation complex located in East London (London E15 2GW), originally built as a press and broadcast centre for the London 2012 Olympics and subsequently repurposed as a campus. UCL Here East occupies part of the Broadcast Centre, taking up the ground and first floor of unit B. The footprint of the building is deep, with daylight only able to enter through the glazing at the front of the building. This glazing uses an infrared blocking film, which can be revealed using infrared photography.&lt;/p&gt;
    &lt;p&gt;A Canon 500D digital camera was modified to replace the infrared blocking layer with clear glass that passes infrared wavelengths. This was used in conjunction with filters that block visible and infrared wavelengths to explore the presence and absence of infrared light. Spectral measurements were made with two spectrophotometers (Ocean Optics SR-6XR250-50 and FLAME-NIR) with optic fibre and cosine correctors used to collect the incandescent spectra in the shorter and longer wavelengths.&lt;/p&gt;
    &lt;p&gt;Incandescent desk lighting was introduced into the work environment using desk lamps with 60 W clear Edison bulbs (Polaris UK) placed on work benches. All subjects had worked in this LED-lit environment for more than 2 years. Desk lamps with incandescent bulbs were introduced onto the benches where experimental subjects spent the majority of their time. They were given the incandescent lighting for 2 weeks and, while they spend the majority of their time working near these lights, they were free to move around and leave their desks as they wished. The introduced light showed a high degree of reflectance from the work surfaces.&lt;/p&gt;
    &lt;p&gt;Colour contrast testing: All subjects were tested for colour contrast ability using ChromaTest prior to the introduction of incandescent lighting and then again 2 weeks later. This test must be carried out in a darkened room, so a nearby windowless room was set aside for this purpose. The incandescent lighting was then removed and subjects retested at 4 and 6 weeks. Hence, this element of the experiment was a before and after design which avoids between subject variability. However, there was also a separate control group (N = 11) composed of subjects that worked under LED lighting similar to those in the experimental group.&lt;/p&gt;
    &lt;p&gt;ChromaTests is a sensitive measure of colour contrast detection of letters presented in a random order against a noisy visual background in either tritan (blue) or protan (red) visual axes13. If subjects correctly identify a letter its contrast is reduced in the next presentation of a letter. Likewise, if they fail to correctly identify the letter, the contrast is increased. This is repeated until thresholds are determined in 5 identical repeated trials. This normally involved around 70–100 separate presentations in total. Subjects were given an initial trial before testing to avoid a learning effect. Initial presentations were at high colour contrast. No learning effects were noted in the study.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Light assessment in the experimental environment&lt;/head&gt;
    &lt;p&gt;Figure 1 shows the front exterior of the Here East building using infrared imaging at ground level where experiments were undertaken. The windows are completely infrared reflective due to their blocking film and hence mirror-like. Figure 2 is an infrared image from inside the building looking out through the open doorway. Only infrared light coming through the open door and its reflectance can be seen, not light coming through adjacent windows. Hence, the building is relatively impervious to infrared light.&lt;/p&gt;
    &lt;p&gt;Figure 3 shows the working environment in Here East in visible light in which the experiment took place. Images in infrared were completely black. The distance between the work environment and the front door was &amp;gt; 50 m with multiple doors between.&lt;/p&gt;
    &lt;p&gt;The internal lighting throughout Here East was provided by arrays of ceiling mounted standard LED units. Spectral profiles of the lighting within the building are shown in Fig. 4. against incandescent lighting in black and red.&lt;/p&gt;
    &lt;p&gt;The blue curve shows the spectral profile of the light delivered to the horizontal working plane. As the work environment was deep in the building and lit only with LED lighting, it received no daylight and was devoid of any infrared illumination. The LED units delivered 1000 lx on the horizontal working plane with a correlated colour temperature (CCT) of 4000 K and a TM-30 colour fidelity index of 91. The irradiance of this LED light was 3.6 W/m2.&lt;/p&gt;
    &lt;p&gt;The specific spectra and energy levels were mapped over the workspace at fixed locations. This is shown in Fig. 5a and b. Here there is a plan of the space and measurements made at 9 locations of energy given in W/cm− 2 and lux, which is a metric corrected for the human eye. Also provided are the spectra at each location. Changes in brightness at different locations were largely not detectable by the human eye and were gradual. There were no differences in spectral profiles across the area only their relative intensity.&lt;/p&gt;
    &lt;p&gt;Light assessment was also undertaken at bench level where individual subjects worked. This is shown in Fig. 6. This confirmed the absence of any part of the infrared spectrum in the work environment and how this changed with the addition of the incandescent lamps.&lt;/p&gt;
    &lt;p&gt;In this study, responses to lighting were measured in test subjects both before and also after the lighting had been changed. However, there was also an independent control group that comprised individuals under similar LED lighting condition without daylight. A comparison of the lighting conditions in the two groups is shown in Fig. 7. Critically, the LEDs in both groups had very similar profiles with no infrared components. The overall brightness in the control group was slightly less than in the test group, although this was not apparent to the human eye. As in the test group, subjects were free to move around.&lt;/p&gt;
    &lt;head rend="h3"&gt;Visual responses to shifts in spectral lighting&lt;/head&gt;
    &lt;p&gt;Exposure to 60 W incandescent luminaires, which have a wider spectrum than LEDs extending into the infra-red1, resulted in significant improvements in visual performance in all experimental subjects across both the protan and tritan visual ranges. Improvements in both tritan and protan were of the order of 25%. Hence, significant improvements were uniform across visual ranges (Fig. 8). This is unlike experiments where specific red/infrared ranges have been used in LED devices, for example via 670 nm, where visual improvements have been biased towards tritan function13.&lt;/p&gt;
    &lt;p&gt;Figure 8 shows the results of both individual subjects on the left and also changes in the groups on the right. In spite of the universal improvement in visual function, in both tritan and protan range there was considerable variability between subjects. This variability validates the inclusion of a repeated measures design and the use of a sign test in the analysis. In all cases protan thresholds were lower than tritan consistent with previous studies13.&lt;/p&gt;
    &lt;p&gt;At the end of the 2 week period the incandescent luminaires were removed and the subjects returned to an exclusively LED dominated light working environment. They were then retested at 4 and 6 weeks. In previous experiments where 670 nm alone has been used, rather than the wide spectrum infrared produced by incandescent lighting, visual improvements decline in approximately a week13. However, following incandescent light exposure improvements remain unchanged across both visual domains at both 4 and 6 weeks. Hence, the impact of broad-spectrum incandescent light not only resulted in balanced improvements in colour contrast but also these improvements lasted much longer than previous interventions with restricted red/infrared ranges13.&lt;/p&gt;
    &lt;p&gt;An independent control group was used in addition to a before and after experimental design. Again, data between individuals was varied on both visual metrics. However, over a 2 week period there were no significant changes in proton or tritan visual thresholds (Fig. 9).&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;We demonstrate that the visual performance of those working under standard LED is significantly improved by exposure to incandescent lighting that has a spectrum similar to daylight with an extensive infrared component. These data are consistent with the hypothesis that LED lighting undermines human visual performance. This result is consistent with laboratory experiments where specific red/infrared wavelength ranges generated by LEDs have been used to improve visual function in animals and humans in a conserved manner13,16,17. But there are three critical differences from these earlier studies. First, we have simply changed environmental lighting in a free moving work environment. Second, we have obtained significant balanced improvements in both the protan and tritan range. Previously, exposure to restricted experimental 670 nm resulted in improvements biased strongly in favour of only tritan function13. Hence, exposure to full spectrum lighting results in a balanced pattern of improvement in visual performance. Third, we have shown that improvements in visual function following incandescent light exposure are sustained for up to 6 weeks, and possibly beyond, whereas benefits from single LED restricted range red light were confined to around 5 days13. These three features change the way in which long wavelength light may be applied to improve human physiology by delivery in normal environments with lasting balance effects. These results are novel and may have public health implications.&lt;/p&gt;
    &lt;p&gt;Our study used 22 subjects but was statistically significant using both a before and after metric and also against an independent control group. They are also similar to group sizes in aspects of Shinhmar et al.13 (Figs. 2, 3, 4 and 5). However, future studies would clearly benefit from inclusion of a larger number of subjects.&lt;/p&gt;
    &lt;p&gt;The evolution of life on earth extends over 4 billion years, and that of humans over approximately 4–5 million years from the last common primate ancestor. This has all taken place under sunlight that has a spectral range of approximately 300–2500 nm+, within which there has been an invariant balance between short and longer wavelengths. Human adoption of fire 1–2 million years ago supplemented sunlight as they moved out of Africa as its spectrum is similar having a large infrared component. Likewise, development of the Edison filament luminaire, common until approximately the year 2000 had a spectrum similar to sunlight. However, around 2010 LED lighting with its highly restricted spectrum (350–650 nm) and energy saving characteristics became common, resulting in a loss of infrared light in the built environment1.&lt;/p&gt;
    &lt;p&gt;The physiology of life forms are adapted to natural environmental light in a highly conserved pattern across species. Light impacts on mitochondrial function, which is a key regulator of metabolism and ageing in animals. When the balance of short and long wavelengths is shifted there are consequences for mitochondria. When shorter wavelength exposure is dominant, as in LED lighting, mitochondrial function declines. Mitochondrial complex proteins are reduced and there is reduced ATP production2,3. With reduced mitochondrial demand for glucose there is increased body weight and disruptions to serum cytokines4. Consequently, consistent with the mitochondrial theory of ageing there is an increased probability of cell/organism ageing and death18. It is suggested that this is partly due to 420–450 nm light, dominant in LEDs, being absorbed by porphyrin and the subsequent production of oxygen singlets driving inflammation3.&lt;/p&gt;
    &lt;p&gt;Conversely, exposure to longer wavelengths is associated with increased mitochondrial membrane potential and increased concentration of mitochondrial complex proteins that have declined with ageing and disease. This in turn is associated with elevated ATP, reduced inflammation and extended average lifespan7,9,10,19. The experimental use of longer wavelengths in such situations is commonly referred to as photobiomodulation.&lt;/p&gt;
    &lt;p&gt;The retina has the greatest metabolic rate in the body and a high mitochondrial concentration20. Retinal metabolism declines with age, but this can be partly corrected with long wavelength light across species16,21. In humans a single 3 min 670 nm exposure improves colour vision within 3 h, which is sustained for almost a week13. But what the authors of this study did not appreciate was that this was within a population who worked and lived mainly under LED lighting that may have undermined their baseline measurements. Here, we made no attempt to control light exposures or subject movements as would occur in laboratory-based experiments. Rather, our aim was to introduce wide spectrum long wavelengths into a work environment to improving human performance via mitochondrial manipulation in a translational step.&lt;/p&gt;
    &lt;p&gt;There is considerable evidence that introduction of longer wavelengths impact systemically. Durieux et al.22 stated in relation to experiments in C.elegans that “ We find that mitochondrial perturbation in one tissue is perceived and acted upon by the mitochondrial stress response pathway in distal tissue”. In mice there are significant distinct changes serum cytokine expression to exposures of both short and long wavelength light4,23. Similarly, long wavelength exposures to the surface of the human body excluding the eyes significantly reduces blood glucose levels and increases oxygen consumption in humans. This is likely because mitochondrial upregulation will increase carbohydrate demand to support increased ATP production12. Other systemic impacts can be found and are clear in experimentally induced Parkinson’s in primates. Light targeted by implants focusing on the substantia nigra are effective in reducing symptoms24, but so also are those that are directed at distal locations25.&lt;/p&gt;
    &lt;p&gt;Single 3 min 670 nm exposures remain effective for about 5 days13. But we show that with a wider spectrum they remain effective for 6 weeks, although we did not find the end of the effect. Here it is worth considering potential mechanisms of action which remain subject of debate. Historically, improvements with red light were thought to be due to light absorption by cytochrome C in the respiratory chain26. However, positive effects are found in vitro in the absence of this. Consequently, it has been suggested that longer wavelengths reduce water viscosity around rotary ATP pumps allowing the rotor to increase speed27. This cannot explain the sustained impacts of light exposure as this effect should be relatively transitory as viscosity would increase rapidly following light withdrawal. However, a key feature of long wavelength light absorption is increased respiratory chain protein synthesis. These proteins are in flux throughout the day28 and complex IV is upregulated following red light exposure19. Hence, while red light may initially increase rotor pump speed there rapidly follows an increase in protein synthesis which may establish greater respiratory chain capacity. The life of these proteins could then determine the length of effect.&lt;/p&gt;
    &lt;p&gt;Only thirteen polypeptides are made in mitochondrial protein synthesis. This probably slows with age and likely contributes to aged mitochondrial decline18. But critically, we do not know the speed of mitochondrial protein synthesis, the life of such proteins or the pace of their decline. We suggest that these may be key events in the length of the effects from light exposure.&lt;/p&gt;
    &lt;p&gt;LED lighting clearly has the ability to undermine visual performance probably via reduced mitochondrial function. As light induced changes in mitochondrial ability have been shown to have systemic impacts4,15,22,23,25, the effects of LEDs revealed here may be wider than initially anticipated. Given the prevalence of LEDs, this may represent an important issue in public health and clinical environments where changing lighting patterns in appreciation of this point can have significant positive outcomes29.&lt;/p&gt;
    &lt;p&gt;Given our results, it is important to ask what solutions may be found to improve health in terms of lighting in the built environment. Incandescent lights that we reveal here to have significant positive impact over standard LEDs are being phased out universally for reasons of energy efficiency, where focus is only on the visible light produced.&lt;/p&gt;
    &lt;p&gt;A solution may be found in creating lighting units with multiple longer wavelength LEDs to cover a wider span of the near infrared. However, our attempts here have had limited success. Multiple closely associated spectral peaks do not produce a smooth spectral output as found in incandescent lights and sunlight, which is problematic in improving function and has yet to deliver. This possibly may be overcome using a greater number of spectral peaks with tighter spacing. But this raises a different series of problems regarding cost and increased energy consumption making this solution no better than retention of incandescent sources in terms of environmental sustainability.&lt;/p&gt;
    &lt;p&gt;Key to this issue is the question of how much infrared is needed to sustain improved function? Infrared has relatively few absorbers in the built environment and in current studies relatively little has to be added to the environment for effect. However, a viable option is to run an incandescent light at a lower temperature which results in both energy savings and increased life of the unit and also shifts the peak spectral output towards longer wavelengths.&lt;/p&gt;
    &lt;p&gt;If this is done with a halogen bulb, which is a type of incandescent tungsten bulb, the filament lasts for a longer period as evaporated tungsten is redeposited on the filament rather than blackening the bulb glass. Hence, using a halogen bulb at lower voltage is a realistic alternative in terms of health and energy consumption.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data availability&lt;/head&gt;
    &lt;p&gt;The data sets used and/or analysed during the current study are available from the corresponding author on reasonable request.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Ratto, G. E., Videla, F. A. &amp;amp; Martinez Valiviezd, J. H. Artificial light: traditional and new sources, their potential impact on health, and coping strategies: preliminary spectral analysis. Proc. SPIE Conf. 11814. San Diego California. (2021).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hoh Kam, J., Hogg, C., Fosbury, R., Shinhmar, H. &amp;amp; Jeffery, G. Mitochondria are specifically vulnerable to 420nm light in drosophila which undermines their function and is associated with reduced fly mobility. Plos one. Sep 3;16(9):e0257149. (2021). https://pubmed.ncbi.nlm.nih.gov/34478469. PMID: 34478469.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kaynezhad, P. et al. Near infrared spectroscopy reveals instability in retinal mitochondrial metabolism and haemodynamics with blue light exposure at environmental levels. J.Biophotonics. ;15(4):e202100283. (2022). https://pubmed.ncbi.nlm.nih.gov/35020273/. PMID: 35020273.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Al-Hussaini, H. et al. Impact of short wavelength light exposure on body weight, mobility, anxiety like behaviour and cytokine expression. Sci. Rep. ;15(1):5927. (2025). https://pubmed.ncbi.nlm.nih.gov/39966413/. PMID: 39966413.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nash, T. R. et al. Daily blue-light exposure shortens lifespan and causes brain neurodegeneration in Drosophila. Aging. Mech. Dis 2019 Oct 17:5:8. https://pubmed.ncbi.nlm.nih.gov/31636947/. PMID: 31636947.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jeffery, G. et al. Longer wavelengths in sunlight pass through the human body and have a systemic impact which improves vision. Sci. Rep. 2025 July;15(1);24435. https://doi.org/10.1038/s41598-025-09785-3&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gkotsi, D. et al. Recharging mitochondrial batteries in old eyes. Near infra-red increases ATP. Exp.Eye Res. 2014 May:122:50 – 3. https://pubmed.ncbi.nlm.nih.gov/24631333/ PMID: 24631333.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Calaza, K. C., Hoh Kam, J., Hogg, C. &amp;amp; Jeffery, G. Mitochondrial decline precedes phenotype development in the complement factor H mouse model of retinal degeneration but can be corrected by near infrared light. Neurobiol. Aging. ;36(10):2869-76. (2015). https://pubmed.ncbi.nlm.nih.gov/26149919/ PMID: 26149919.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Begum, R. et al. Near-infrared light increases ATP, extends lifespan and improves mobility in aged Drosophila melanogaster. Biol.Lett. ;11(3):20150073. (2015). https://pubmed.ncbi.nlm.nih.gov/25788488/ PMID: 25788488.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Powner, P. MB, SaltTE, Hogg, C. &amp;amp; Jeffery, G. Improving mitochondrial function protects bumblebees from neonicotinoid pesticides. Plos One. 11 (11), e0166531 (2016). https://pubmed.ncbi.nlm.nih.gov/27846310/ PMID: 27846310.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Powner, P. MB &amp;amp; Jeffery, G. Systemic glucose levels are modulated by specific wavelengths in the solar light spectrum that shift mitochondrial metabolism. Plos One. 17 (11), e0276937 (2022). https://pubmed.ncbi.nlm.nih.gov/36327250/ PMID: 36327250.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Powner, M. B. &amp;amp; Jeffery, G. Light stimulation of mitochondria reduces blood glucose levels. J.Biophotonics. ;17(5):e202300521. (2024). https://pubmed.ncbi.nlm.nih.gov/38378043/. PMID: 38378043.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shinhmar, H., Hoog, C., Neveu, M. &amp;amp; Jeffery, G. Weeklong improved colour contrasts sensitivity after single 670 nm exposures associated with enhanced mitochondrial function. Sci. Rep. ;11(1):22872. (2021). https://pubmed.ncbi.nlm.nih.gov/34819619/. PMID: 34819619.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weller, R. B. &amp;amp; Sunlight Time for a Rethink? J.Invest. Dermatol. ;144(8):1724–1732. (2024). https://pubmed.ncbi.nlm.nih.gov/38661623/ PMID: 38661623.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shore-Lorenti, C. et al. Shining the light on Sunshine: a systematic review of the influence of sun exposure on type 2 diabetes mellitus-related outcomes. Clin. Endocrinol. ;81(6):799–811. (2014). https://pubmed.ncbi.nlm.nih.gov/25066830/ PMID: 25066830.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weinrich, T. W., Coyne, A., Salt, T. E., Hogg, C. &amp;amp; Jeffery, G. Improving mitochondrial function significantly reduces metabolic, visual, motor and cognitive decline in aged Drosophila melanogaster. Neurobiol. Aging. 2017 Dec:60:34–43. doi: 10.1016. https://pubmed.ncbi.nlm.nih.gov/28917665/ PMID: 28917665.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sivapathasuntharam, C., Sivaprasad, S., Hogg, C. &amp;amp; Jeffery, G. Aging retinal function is improved by near infrared light (670 nm) that is associated with corrected mitochondrial decline. Neurbiol. Aging 2017 Apr:52:66–70. https://pubmed.ncbi.nlm.nih.gov/28129566/ PMID: 28129566.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lopez-Otin, C., Blasco, M. A., Partridge, L., Serrano, M. &amp;amp; Kroemer, G. The hallmarks of aging. Cell 153 (6), 1194–1217 (2013). https://pubmed.ncbi.nlm.nih.gov/23746838/ PMID: 23746838.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Begum, R., Powner, P. MB, Hudson, N., Hogg, C. &amp;amp; Jeffery, G. Treatment with 670 Nm light up regulates cytochrome C oxidase expression and reduces inflammation in an Age-Related macular degeneration model. Plos One. 8 (2), e57828 (2013). https://pubmed.ncbi.nlm.nih.gov/23469078/ PMID: 23469078.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kocherlakota, S., Hurley, J. B., Shu, D. Y. &amp;amp; Editorial Retinal metabolism in health and disease. Front Ophthalmol (Lausanne). 2024 Jul 17:4:1459318. https://pubmed.ncbi.nlm.nih.gov/39086994/ PMID: 39086994.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hoh Kam, J. et al. Mitochondrial decline in the ageing old world primate retina: Little evidence for difference between the centre and periphery. Plos One. ;18(5):e0273882. (2023). https://pubmed.ncbi.nlm.nih.gov/37130143/ PMID: 37130143.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Durieux, J., Wolff, S. &amp;amp; Dillin, A. The cell-non-autonomous nature of electron transport chain-mediated longevity. Cell 144 (1), 79–91 (2011). https://pubmed.ncbi.nlm.nih.gov/21215371/ PMID: 21215371.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shinhmar, H., Hogg, C. &amp;amp; Jeffery, G. Exposure to long wavelength light that improves aged mitochondrial function shifts acute cytokine expression in serum and the retina. Plos One. 18 (7), e0284172 (2023). https://pubmed.ncbi.nlm.nih.gov/37478072/ PMID: 37478072.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Darlot, F. et al. Near-infrared light is neuroprotective in a monkey model of Parkinson disease. Ann. Neurol. ;79(1):59–75. (2016). https://pubmed.ncbi.nlm.nih.gov/26456231/ PMID: 26456231.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gordon, L. C. et al. Remote photobiomodulation targeted at the abdomen or legs provides effective neuroprotection against parkinsonian MPTP insult. Eur. J. Neurosci. ;57(9):1611–1624. (2023). https://pubmed.ncbi.nlm.nih.gov/36949610/. PMID: 36949610.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Salehpour, F., Mahmoudi, J., Kamari, F., Sadigh-Eteghad, Rasta, S. H. &amp;amp; Hamblin, M. R. Brain Photobiomodulation Therapy: a Narrative Review. Mol. Neurobiol. ;55(8):6601–6636. (2018). https://pubmed.ncbi.nlm.nih.gov/29327206/ PMID: 29327206.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sommer, A. P. Mitochondrial cytochrome c oxidase is not the primary acceptor for near infrared light-it is mitochondrial bound water: the principles of low-level light therapy. Ann. Transl. Med. ;7(Suppl 1):S13. (2019). https://pubmed.ncbi.nlm.nih.gov/31032294/. PMID: 31032294.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weinrich, T. et al. A day in the life of mitochondria reveals shifting workloads. Sci. Rep. ;9(1):13898. (2019). https://pubmed.ncbi.nlm.nih.gov/31554906/ PMID: 31554906.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Neto, R. P. M. et al. Photobiomodulation therapy (red/NIR LEDs) reduced the length of stay in intensive care unit and improved muscle function: A randomized, triple-blind, and sham-controlled trial. J.Biophotonics. ;17(5):e202300501. (2024). https://pubmed.ncbi.nlm.nih.gov/38262071/ PMID: 38262071.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We thank Chris Hogg for assistance with Chromatest, and Mandana Khanie for use of the Ocean Optics spectrophotometers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Funding&lt;/head&gt;
    &lt;p&gt;This research did not receive funding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Author information&lt;/head&gt;
    &lt;head rend="h3"&gt;Authors and Affiliations&lt;/head&gt;
    &lt;head rend="h3"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;GJ and EB designed the experiments undertook all the experimental work and wrote the manuscript.&lt;/p&gt;
    &lt;head rend="h3"&gt;Corresponding author&lt;/head&gt;
    &lt;head rend="h2"&gt;Ethics declarations&lt;/head&gt;
    &lt;head rend="h3"&gt;Competing interests&lt;/head&gt;
    &lt;p&gt;The authors declare no competing interests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional information&lt;/head&gt;
    &lt;head rend="h3"&gt;Publisher’s note&lt;/head&gt;
    &lt;p&gt;Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rights and permissions&lt;/head&gt;
    &lt;p&gt;Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.&lt;/p&gt;
    &lt;head rend="h2"&gt;About this article&lt;/head&gt;
    &lt;head rend="h3"&gt;Cite this article&lt;/head&gt;
    &lt;p&gt;Barrett, E.M., Jeffery, G. LED lighting (350-650nm) undermines human visual performance unless supplemented by wider spectra (400-1500nm+) like daylight. Sci Rep 16, 3061 (2026). https://doi.org/10.1038/s41598-026-35389-6&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Received:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Accepted:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Published:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Version of record:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DOI: https://doi.org/10.1038/s41598-026-35389-6&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46758644</guid><pubDate>Sun, 25 Jan 2026 21:44:10 +0000</pubDate></item><item><title>The future of software engineering is SRE</title><link>https://swizec.com/blog/the-future-of-software-engineering-is-sre/</link><description>&lt;doc fingerprint="bd4e911dfc4564fd"&gt;
  &lt;main&gt;
    &lt;p&gt;When code gets cheap operational excellence wins. Anyone can build a greenfield demo, but it takes engineering to run a service.&lt;/p&gt;
    &lt;p&gt;You may be wondering: With all the hype about agentic coding, will we even need software engineers anymore? Yes! We'll need more.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;SRE about to become the most hired job in engineering&lt;/p&gt;— Swizec Teller (@Swizec) January 13, 2026&lt;lb/&gt;Everybody wants to write a greenfield demo.&lt;lb/&gt;Nobody wants to run a service. https://t.co/THl9rBJ9rk&lt;/quote&gt;
    &lt;p&gt;Writing code was always the easy part of this job. The hard part was keeping your code running for the long time. Software engineering is programming over time. It's about how systems change.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons from the no-code and spreadsheets era&lt;/head&gt;
    &lt;p&gt;Let's take no-code and spreadsheets as an example of the kind of software people say is the future – custom-built, throwaway, built by non-experts to solve specific problems.&lt;/p&gt;
    &lt;p&gt;Joe Schmoe from accounting takes 10 hours to do a thing. He's does this every week and it feels repetitive, mechanical, and boring. Joe could do the work in his sleep.&lt;/p&gt;
    &lt;p&gt;But he can't get engineering resources to build a tool. The engineers are busy building the product. No worries, Joe is a smart dude. With a little Googling, a few no-code tools, and good old spreadsheet macros he builds a tool.&lt;/p&gt;
    &lt;p&gt;Amazing.&lt;/p&gt;
    &lt;p&gt;Joe's tool is a little janky but his 10 hour weekly task now takes 1 hour! 🎉 Sure, he finds a new edge case every every week and there's constant tinkering, but he's having a lot more fun.&lt;/p&gt;
    &lt;p&gt;Time passes, the business changes, accounting rules are in constant flux, and let's never talk about timezones or daylight savings ever again. Joe is sick of this bullshit.&lt;/p&gt;
    &lt;p&gt;All he wanted was to make his job easier and now he's shackled to this stupid system. He can't go on vacation, he can't train anyone else to run this thing successfully, and it never fucking works right.&lt;/p&gt;
    &lt;p&gt;Joe can't remember the last time running his code didn't fill him with dread. He spends hours carefully making sure it all worked.&lt;/p&gt;
    &lt;head rend="h2"&gt;The computer disease&lt;/head&gt;
    &lt;p&gt;Feynman called this the computer disease.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Feynman called this The Computer Disease pic.twitter.com/Zv4Bu4ftv1&lt;/p&gt;— Swizec Teller (@Swizec) December 26, 2025&lt;/quote&gt;
    &lt;p&gt;The problem with computers is that you tinker. Automating things is fun! You might forget you don't need to 😆&lt;/p&gt;
    &lt;p&gt;The part that's not fun is running things. Providing a service. Reliably, at scale, for years on end. A service that people will hire to do their jobs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why operational excellence is the future&lt;/head&gt;
    &lt;p&gt;People don't buy software, they hire a service.&lt;/p&gt;
    &lt;p&gt;You don't care how iCloud works, you just want your photos to magically show up across devices every time. You don't care about Word or Notion or gDocs, you just want to write what's on your mind, share it with others, and see their changes. And you definitely don't care how a payments network point of sale terminal and your bank talk to each other, you just want your $7 matcha latte to get you through the week.&lt;/p&gt;
    &lt;p&gt;Good software is invisible.&lt;/p&gt;
    &lt;p&gt;And that takes work. A lot of work. Because the first 90% to get a working demo is easy. It's the other 190% that matters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What's your uptime?&lt;/item&gt;
      &lt;item&gt;Defect rate?&lt;/item&gt;
      &lt;item&gt;How quickly do you recover from defects?&lt;/item&gt;
      &lt;item&gt;Do I have to reach out or will you know before me?&lt;/item&gt;
      &lt;item&gt;Can you own upstream dependencies?&lt;/item&gt;
      &lt;item&gt;When a vendor misbehaves, will you notice or wait until your users complain?&lt;/item&gt;
      &lt;item&gt;When users share ideas, how long does it take?&lt;/item&gt;
      &lt;item&gt;How do you keep engineers from breaking each other's systems?&lt;/item&gt;
      &lt;item&gt;Do you have systems to keep engineers moving without turning your app into a disjointed mess?&lt;/item&gt;
      &lt;item&gt;Can you build software bigger than fits in 1 person's brain?&lt;/item&gt;
      &lt;item&gt;When I'm in a 12 hour different timezone, your engineers are asleep, and there's a big issue ... will it be fixed before I give up?&lt;/item&gt;
      &lt;item&gt;Can you recover from failures, yours and upstream, or does important data get lost?&lt;/item&gt;
      &lt;item&gt;Are you keeping up with security updates?&lt;/item&gt;
      &lt;item&gt;Will you leak all my data?&lt;/item&gt;
      &lt;item&gt;Do I trust you?&lt;/item&gt;
      &lt;item&gt;Can I rely on you?&lt;/item&gt;
      &lt;item&gt;How can you be so sure?&lt;/item&gt;
      &lt;item&gt;Will you sign a legally binding guarantee that your software works when I need it? 😉&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Those are the ~~fun~~ hard engineering challenges. Writing code is easy.&lt;/p&gt;
    &lt;p&gt;Cheers,&lt;lb/&gt; ~Swizec&lt;/p&gt;
    &lt;head rend="h3"&gt;Scaling Fast book free preview&lt;/head&gt;
    &lt;p&gt;Enter your email to receive a sample chapter of Scaling Fast: Software Engineering Through the Hockeystick and learn how to navigate hypergrowth without burning out your team.&lt;/p&gt;
    &lt;p&gt;Have a burning question that you think I can answer? Hit me up on twitter and I'll do my best.&lt;/p&gt;
    &lt;p&gt;Who am I and who do I help? I'm Swizec Teller and I turn coders into engineers with "Raw and honest from the heart!" writing. No bullshit. Real insights into the career and skills of a modern software engineer.&lt;/p&gt;
    &lt;p&gt;Want to become a true senior engineer? Take ownership, have autonomy, and be a force multiplier on your team. The Senior Engineer Mindset ebook can help 👉 swizec.com/senior-mindset. These are the shifts in mindset that unlocked my career.&lt;/p&gt;
    &lt;p&gt;Curious about Serverless and the modern backend? Check out Serverless Handbook, for frontend engineers 👉 ServerlessHandbook.dev&lt;/p&gt;
    &lt;p&gt;Want to Stop copy pasting D3 examples and create data visualizations of your own? Learn how to build scalable dataviz React components your whole team can understand with React for Data Visualization&lt;/p&gt;
    &lt;p&gt;Want to get my best emails on JavaScript, React, Serverless, Fullstack Web, or Indie Hacking? Check out swizec.com/collections&lt;/p&gt;
    &lt;p&gt;Did someone amazing share this letter with you? Wonderful! You can sign up for my weekly letters for software engineers on their path to greatness, here: swizec.com/blog&lt;/p&gt;
    &lt;p&gt;Want to brush up on your modern JavaScript syntax? Check out my interactive cheatsheet: es6cheatsheet.com&lt;/p&gt;
    &lt;p&gt;By the way, just in case no one has told you it yet today: I love and appreciate you for who you are ❤️&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46759063</guid><pubDate>Sun, 25 Jan 2026 22:18:38 +0000</pubDate></item><item><title>Case study: Creative math – How AI fakes proofs</title><link>https://tomaszmachnik.pl/case-study-math-en.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46759352</guid><pubDate>Sun, 25 Jan 2026 22:44:50 +0000</pubDate></item><item><title>Scientists identify brain waves that define the limits of 'you'</title><link>https://www.sciencealert.com/scientists-identify-brain-waves-that-define-the-limits-of-you</link><description>&lt;doc fingerprint="b103d32cb0e38d8a"&gt;
  &lt;main&gt;
    &lt;p&gt;At what point do "you" end and the outside world begins?&lt;/p&gt;
    &lt;p&gt;It might feel like a weird question with an obvious answer, but your brain has to work surprisingly hard to judge that boundary. Now, scientists have linked a specific set of brain waves in a certain part of the brain to a sense of body ownership.&lt;/p&gt;
    &lt;p&gt;In a series of new experiments, researchers from Sweden and France put 106 participants through what's called the rubber hand illusion, monitoring and stimulating their brain activity to see what effect it had.&lt;/p&gt;
    &lt;p&gt;Related: Octopuses Fall For The Classic Fake Arm Trick – Just Like We Do&lt;/p&gt;
    &lt;p&gt;This classic illusion involves hiding one of a participant's hands from their view and replacing it with a rubber one instead. When both their real and fake hands are repeatedly touched at the same time, it can evoke the eerie sensation that the rubber hand is part of the person's body.&lt;/p&gt;
    &lt;p&gt;The tests, which in one experiment involved EEG (electroencephalography) readings of brain activity, revealed that a sense of body ownership seems to arise from the frequency of alpha waves in the parietal cortex, a brain region responsible for mapping the body, processing sensory input and building a sense of self.&lt;/p&gt;
    &lt;p&gt;"We have identified a fundamental brain process that shapes our continuous experience of being embodied," says lead author Mariano D'Angelo, a neuroscientist at Karolinska Institute in Sweden.&lt;/p&gt;
    &lt;p&gt;"The findings may provide new insights into psychiatric conditions such as schizophrenia, where the sense of self is disturbed."&lt;/p&gt;
    &lt;p&gt;In the first batch of experiments, participants had a robotic arm tap the index finger of their real and fake hands, either at the exact same time or with a delay of up to 500 milliseconds between each tap.&lt;/p&gt;
    &lt;p&gt;As expected, participants reported feeling that the fake hand was part of their body more strongly if the taps were synchronized, and the feeling steadily weakened as the gap widened between what they felt and what they saw.&lt;/p&gt;
    &lt;p&gt;The EEG readings from the second experiment added more detail to the story. The frequency of alpha waves in the parietal cortex seemed to correlate with how well participants could detect the time delay between taps.&lt;/p&gt;
    &lt;p&gt;Those with faster alpha waves appeared to rule out fake hands even with a tiny gap in taps, while those with slower waves were more likely to feel the fake hand as their own, even if the taps were farther apart.&lt;/p&gt;
    &lt;p&gt;Finally, the researchers investigated whether the frequency of these brain waves actually controls the sensation of body ownership, or if they were perhaps both effects of some other factor.&lt;/p&gt;
    &lt;p&gt;With a third group of participants, they used a non-invasive technique called transcranial alternating current stimulation to speed up or slow down the frequency of a person's alpha waves. And sure enough, this seemed to correlate with how real a fake hand felt.&lt;/p&gt;
    &lt;p&gt;Speeding up someone's alpha waves gave them a tighter sense of body ownership, making them more sensitive to small timing discrepancies. Slowing down the waves had the opposite effect, making it harder for people to tell the difference between their own body and the outside world.&lt;/p&gt;
    &lt;p&gt;"Our findings help explain how the brain solves the challenge of integrating signals from the body to create a coherent sense of self," says Henrik Ehrsson, neuroscientist at Karolinska.&lt;/p&gt;
    &lt;p&gt;The researchers say that the findings could lead to new understanding of or treatments for conditions where the brain's body maps have gone askew, such as schizophrenia or the sensation of 'phantom limbs' experienced by amputees.&lt;/p&gt;
    &lt;p&gt;It could also help make for more realistic prosthetic limbs or even virtual reality tools.&lt;/p&gt;
    &lt;p&gt;The research was published in the journal Nature Communications.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46760099</guid><pubDate>Mon, 26 Jan 2026 00:10:42 +0000</pubDate></item><item><title>Clawdbot - open source personal AI assistant</title><link>https://github.com/clawdbot/clawdbot</link><description>&lt;doc fingerprint="971cdd551f1f808f"&gt;
  &lt;main&gt;
    &lt;p&gt;EXFOLIATE! EXFOLIATE!&lt;/p&gt;
    &lt;p&gt;Clawdbot is a personal AI assistant you run on your own devices. It answers you on the channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, WebChat), plus extension channels like BlueBubbles, Matrix, Zalo, and Zalo Personal. It can speak and listen on macOS/iOS/Android, and can render a live Canvas you control. The Gateway is just the control plane — the product is the assistant.&lt;/p&gt;
    &lt;p&gt;If you want a personal, single-user assistant that feels local, fast, and always-on, this is it.&lt;/p&gt;
    &lt;p&gt;Website · Docs · Getting Started · Updating · Showcase · FAQ · Wizard · Nix · Docker · Discord&lt;/p&gt;
    &lt;p&gt;Preferred setup: run the onboarding wizard (&lt;code&gt;clawdbot onboard&lt;/code&gt;). It walks through gateway, workspace, channels, and skills. The CLI wizard is the recommended path and works on macOS, Linux, and Windows (via WSL2; strongly recommended).
Works with npm, pnpm, or bun.
New install? Start here: Getting started&lt;/p&gt;
    &lt;p&gt;Subscriptions (OAuth):&lt;/p&gt;
    &lt;p&gt;Model note: while any model is supported, I strongly recommend Anthropic Pro/Max (100/200) + Opus 4.5 for long‑context strength and better prompt‑injection resistance. See Onboarding.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Models config + CLI: Models&lt;/item&gt;
      &lt;item&gt;Auth profile rotation (OAuth vs API keys) + fallbacks: Model failover&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Runtime: Node ≥22.&lt;/p&gt;
    &lt;code&gt;npm install -g clawdbot@latest
# or: pnpm add -g clawdbot@latest

clawdbot onboard --install-daemon&lt;/code&gt;
    &lt;p&gt;The wizard installs the Gateway daemon (launchd/systemd user service) so it stays running.&lt;/p&gt;
    &lt;p&gt;Runtime: Node ≥22.&lt;/p&gt;
    &lt;p&gt;Full beginner guide (auth, pairing, channels): Getting started&lt;/p&gt;
    &lt;code&gt;clawdbot onboard --install-daemon

clawdbot gateway --port 18789 --verbose

# Send a message
clawdbot message send --to +1234567890 --message "Hello from Clawdbot"

# Talk to the assistant (optionally deliver back to any connected channel: WhatsApp/Telegram/Slack/Discord/Google Chat/Signal/iMessage/BlueBubbles/Microsoft Teams/Matrix/Zalo/Zalo Personal/WebChat)
clawdbot agent --message "Ship checklist" --thinking high&lt;/code&gt;
    &lt;p&gt;Upgrading? Updating guide (and run &lt;code&gt;clawdbot doctor&lt;/code&gt;).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;stable: tagged releases (&lt;code&gt;vYYYY.M.D&lt;/code&gt;or&lt;code&gt;vYYYY.M.D-&amp;lt;patch&amp;gt;&lt;/code&gt;), npm dist-tag&lt;code&gt;latest&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;beta: prerelease tags (&lt;code&gt;vYYYY.M.D-beta.N&lt;/code&gt;), npm dist-tag&lt;code&gt;beta&lt;/code&gt;(macOS app may be missing).&lt;/item&gt;
      &lt;item&gt;dev: moving head of &lt;code&gt;main&lt;/code&gt;, npm dist-tag&lt;code&gt;dev&lt;/code&gt;(when published).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Switch channels (git + npm): &lt;code&gt;clawdbot update --channel stable|beta|dev&lt;/code&gt;.
Details: Development channels.&lt;/p&gt;
    &lt;p&gt;Prefer &lt;code&gt;pnpm&lt;/code&gt; for builds from source. Bun is optional for running TypeScript directly.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/clawdbot/clawdbot.git
cd clawdbot

pnpm install
pnpm ui:build # auto-installs UI deps on first run
pnpm build

pnpm clawdbot onboard --install-daemon

# Dev loop (auto-reload on TS changes)
pnpm gateway:watch&lt;/code&gt;
    &lt;p&gt;Note: &lt;code&gt;pnpm clawdbot ...&lt;/code&gt; runs TypeScript directly (via &lt;code&gt;tsx&lt;/code&gt;). &lt;code&gt;pnpm build&lt;/code&gt; produces &lt;code&gt;dist/&lt;/code&gt; for running via Node / the packaged &lt;code&gt;clawdbot&lt;/code&gt; binary.&lt;/p&gt;
    &lt;p&gt;Clawdbot connects to real messaging surfaces. Treat inbound DMs as untrusted input.&lt;/p&gt;
    &lt;p&gt;Full security guide: Security&lt;/p&gt;
    &lt;p&gt;Default behavior on Telegram/WhatsApp/Signal/iMessage/Microsoft Teams/Discord/Google Chat/Slack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DM pairing (&lt;code&gt;dmPolicy="pairing"&lt;/code&gt;/&lt;code&gt;channels.discord.dm.policy="pairing"&lt;/code&gt;/&lt;code&gt;channels.slack.dm.policy="pairing"&lt;/code&gt;): unknown senders receive a short pairing code and the bot does not process their message.&lt;/item&gt;
      &lt;item&gt;Approve with: &lt;code&gt;clawdbot pairing approve &amp;lt;channel&amp;gt; &amp;lt;code&amp;gt;&lt;/code&gt;(then the sender is added to a local allowlist store).&lt;/item&gt;
      &lt;item&gt;Public inbound DMs require an explicit opt-in: set &lt;code&gt;dmPolicy="open"&lt;/code&gt;and include&lt;code&gt;"*"&lt;/code&gt;in the channel allowlist (&lt;code&gt;allowFrom&lt;/code&gt;/&lt;code&gt;channels.discord.dm.allowFrom&lt;/code&gt;/&lt;code&gt;channels.slack.dm.allowFrom&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run &lt;code&gt;clawdbot doctor&lt;/code&gt; to surface risky/misconfigured DM policies.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local-first Gateway — single control plane for sessions, channels, tools, and events.&lt;/item&gt;
      &lt;item&gt;Multi-channel inbox — WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, BlueBubbles, Microsoft Teams, Matrix, Zalo, Zalo Personal, WebChat, macOS, iOS/Android.&lt;/item&gt;
      &lt;item&gt;Multi-agent routing — route inbound channels/accounts/peers to isolated agents (workspaces + per-agent sessions).&lt;/item&gt;
      &lt;item&gt;Voice Wake + Talk Mode — always-on speech for macOS/iOS/Android with ElevenLabs.&lt;/item&gt;
      &lt;item&gt;Live Canvas — agent-driven visual workspace with A2UI.&lt;/item&gt;
      &lt;item&gt;First-class tools — browser, canvas, nodes, cron, sessions, and Discord/Slack actions.&lt;/item&gt;
      &lt;item&gt;Companion apps — macOS menu bar app + iOS/Android nodes.&lt;/item&gt;
      &lt;item&gt;Onboarding + skills — wizard-driven setup with bundled/managed/workspace skills.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gateway WS control plane with sessions, presence, config, cron, webhooks, Control UI, and Canvas host.&lt;/item&gt;
      &lt;item&gt;CLI surface: gateway, agent, send, wizard, and doctor.&lt;/item&gt;
      &lt;item&gt;Pi agent runtime in RPC mode with tool streaming and block streaming.&lt;/item&gt;
      &lt;item&gt;Session model: &lt;code&gt;main&lt;/code&gt;for direct chats, group isolation, activation modes, queue modes, reply-back. Group rules: Groups.&lt;/item&gt;
      &lt;item&gt;Media pipeline: images/audio/video, transcription hooks, size caps, temp file lifecycle. Audio details: Audio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Channels: WhatsApp (Baileys), Telegram (grammY), Slack (Bolt), Discord (discord.js), Google Chat (Chat API), Signal (signal-cli), iMessage (imsg), BlueBubbles (extension), Microsoft Teams (extension), Matrix (extension), Zalo (extension), Zalo Personal (extension), WebChat.&lt;/item&gt;
      &lt;item&gt;Group routing: mention gating, reply tags, per-channel chunking and routing. Channel rules: Channels.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS app: menu bar control plane, Voice Wake/PTT, Talk Mode overlay, WebChat, debug tools, remote gateway control.&lt;/item&gt;
      &lt;item&gt;iOS node: Canvas, Voice Wake, Talk Mode, camera, screen recording, Bonjour pairing.&lt;/item&gt;
      &lt;item&gt;Android node: Canvas, Talk Mode, camera, screen recording, optional SMS.&lt;/item&gt;
      &lt;item&gt;macOS node mode: system.run/notify + canvas/camera exposure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browser control: dedicated clawd Chrome/Chromium, snapshots, actions, uploads, profiles.&lt;/item&gt;
      &lt;item&gt;Canvas: A2UI push/reset, eval, snapshot.&lt;/item&gt;
      &lt;item&gt;Nodes: camera snap/clip, screen record, location.get, notifications.&lt;/item&gt;
      &lt;item&gt;Cron + wakeups; webhooks; Gmail Pub/Sub.&lt;/item&gt;
      &lt;item&gt;Skills platform: bundled, managed, and workspace skills with install gating + UI.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Channel routing, retry policy, and streaming/chunking.&lt;/item&gt;
      &lt;item&gt;Presence, typing indicators, and usage tracking.&lt;/item&gt;
      &lt;item&gt;Models, model failover, and session pruning.&lt;/item&gt;
      &lt;item&gt;Security and troubleshooting.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Control UI + WebChat served directly from the Gateway.&lt;/item&gt;
      &lt;item&gt;Tailscale Serve/Funnel or SSH tunnels with token/password auth.&lt;/item&gt;
      &lt;item&gt;Nix mode for declarative config; Docker-based installs.&lt;/item&gt;
      &lt;item&gt;Doctor migrations, logging.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;WhatsApp / Telegram / Slack / Discord / Google Chat / Signal / iMessage / BlueBubbles / Microsoft Teams / Matrix / Zalo / Zalo Personal / WebChat
               │
               ▼
┌───────────────────────────────┐
│            Gateway            │
│       (control plane)         │
│     ws://127.0.0.1:18789      │
└──────────────┬────────────────┘
               │
               ├─ Pi agent (RPC)
               ├─ CLI (clawdbot …)
               ├─ WebChat UI
               ├─ macOS app
               └─ iOS / Android nodes
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gateway WebSocket network — single WS control plane for clients, tools, and events (plus ops: Gateway runbook).&lt;/item&gt;
      &lt;item&gt;Tailscale exposure — Serve/Funnel for the Gateway dashboard + WS (remote access: Remote).&lt;/item&gt;
      &lt;item&gt;Browser control — clawd‑managed Chrome/Chromium with CDP control.&lt;/item&gt;
      &lt;item&gt;Canvas + A2UI — agent‑driven visual workspace (A2UI host: Canvas/A2UI).&lt;/item&gt;
      &lt;item&gt;Voice Wake + Talk Mode — always‑on speech and continuous conversation.&lt;/item&gt;
      &lt;item&gt;Nodes — Canvas, camera snap/clip, screen record, &lt;code&gt;location.get&lt;/code&gt;, notifications, plus macOS‑only&lt;code&gt;system.run&lt;/code&gt;/&lt;code&gt;system.notify&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Clawdbot can auto-configure Tailscale Serve (tailnet-only) or Funnel (public) while the Gateway stays bound to loopback. Configure &lt;code&gt;gateway.tailscale.mode&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;off&lt;/code&gt;: no Tailscale automation (default).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;serve&lt;/code&gt;: tailnet-only HTTPS via&lt;code&gt;tailscale serve&lt;/code&gt;(uses Tailscale identity headers by default).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;funnel&lt;/code&gt;: public HTTPS via&lt;code&gt;tailscale funnel&lt;/code&gt;(requires shared password auth).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;gateway.bind&lt;/code&gt;must stay&lt;code&gt;loopback&lt;/code&gt;when Serve/Funnel is enabled (Clawdbot enforces this).&lt;/item&gt;
      &lt;item&gt;Serve can be forced to require a password by setting &lt;code&gt;gateway.auth.mode: "password"&lt;/code&gt;or&lt;code&gt;gateway.auth.allowTailscale: false&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Funnel refuses to start unless &lt;code&gt;gateway.auth.mode: "password"&lt;/code&gt;is set.&lt;/item&gt;
      &lt;item&gt;Optional: &lt;code&gt;gateway.tailscale.resetOnExit&lt;/code&gt;to undo Serve/Funnel on shutdown.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Details: Tailscale guide · Web surfaces&lt;/p&gt;
    &lt;p&gt;It’s perfectly fine to run the Gateway on a small Linux instance. Clients (macOS app, CLI, WebChat) can connect over Tailscale Serve/Funnel or SSH tunnels, and you can still pair device nodes (macOS/iOS/Android) to execute device‑local actions when needed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gateway host runs the exec tool and channel connections by default.&lt;/item&gt;
      &lt;item&gt;Device nodes run device‑local actions (&lt;code&gt;system.run&lt;/code&gt;, camera, screen recording, notifications) via&lt;code&gt;node.invoke&lt;/code&gt;. In short: exec runs where the Gateway lives; device actions run where the device lives.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Details: Remote access · Nodes · Security&lt;/p&gt;
    &lt;p&gt;The macOS app can run in node mode and advertises its capabilities + permission map over the Gateway WebSocket (&lt;code&gt;node.list&lt;/code&gt; / &lt;code&gt;node.describe&lt;/code&gt;). Clients can then execute local actions via &lt;code&gt;node.invoke&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;system.run&lt;/code&gt;runs a local command and returns stdout/stderr/exit code; set&lt;code&gt;needsScreenRecording: true&lt;/code&gt;to require screen-recording permission (otherwise you’ll get&lt;code&gt;PERMISSION_MISSING&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;system.notify&lt;/code&gt;posts a user notification and fails if notifications are denied.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;canvas.*&lt;/code&gt;,&lt;code&gt;camera.*&lt;/code&gt;,&lt;code&gt;screen.record&lt;/code&gt;, and&lt;code&gt;location.get&lt;/code&gt;are also routed via&lt;code&gt;node.invoke&lt;/code&gt;and follow TCC permission status.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Elevated bash (host permissions) is separate from macOS TCC:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use &lt;code&gt;/elevated on|off&lt;/code&gt;to toggle per‑session elevated access when enabled + allowlisted.&lt;/item&gt;
      &lt;item&gt;Gateway persists the per‑session toggle via &lt;code&gt;sessions.patch&lt;/code&gt;(WS method) alongside&lt;code&gt;thinkingLevel&lt;/code&gt;,&lt;code&gt;verboseLevel&lt;/code&gt;,&lt;code&gt;model&lt;/code&gt;,&lt;code&gt;sendPolicy&lt;/code&gt;, and&lt;code&gt;groupActivation&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Details: Nodes · macOS app · Gateway protocol&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use these to coordinate work across sessions without jumping between chat surfaces.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sessions_list&lt;/code&gt;— discover active sessions (agents) and their metadata.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sessions_history&lt;/code&gt;— fetch transcript logs for a session.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sessions_send&lt;/code&gt;— message another session; optional reply‑back ping‑pong + announce step (&lt;code&gt;REPLY_SKIP&lt;/code&gt;,&lt;code&gt;ANNOUNCE_SKIP&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Details: Session tools&lt;/p&gt;
    &lt;p&gt;ClawdHub is a minimal skill registry. With ClawdHub enabled, the agent can search for skills automatically and pull in new ones as needed.&lt;/p&gt;
    &lt;p&gt;Send these in WhatsApp/Telegram/Slack/Google Chat/Microsoft Teams/WebChat (group commands are owner-only):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/status&lt;/code&gt;— compact session status (model + tokens, cost when available)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/new&lt;/code&gt;or&lt;code&gt;/reset&lt;/code&gt;— reset the session&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/compact&lt;/code&gt;— compact session context (summary)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/think &amp;lt;level&amp;gt;&lt;/code&gt;— off|minimal|low|medium|high|xhigh (GPT-5.2 + Codex models only)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;/verbose on|off&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/usage off|tokens|full&lt;/code&gt;— per-response usage footer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/restart&lt;/code&gt;— restart the gateway (owner-only in groups)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/activation mention|always&lt;/code&gt;— group activation toggle (groups only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Gateway alone delivers a great experience. All apps are optional and add extra features.&lt;/p&gt;
    &lt;p&gt;If you plan to build/run companion apps, follow the platform runbooks below.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Menu bar control for the Gateway and health.&lt;/item&gt;
      &lt;item&gt;Voice Wake + push-to-talk overlay.&lt;/item&gt;
      &lt;item&gt;WebChat + debug tools.&lt;/item&gt;
      &lt;item&gt;Remote gateway control over SSH.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: signed builds required for macOS permissions to stick across rebuilds (see &lt;code&gt;docs/mac/permissions.md&lt;/code&gt;).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pairs as a node via the Bridge.&lt;/item&gt;
      &lt;item&gt;Voice trigger forwarding + Canvas surface.&lt;/item&gt;
      &lt;item&gt;Controlled via &lt;code&gt;clawdbot nodes …&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Runbook: iOS connect.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pairs via the same Bridge + pairing flow as iOS.&lt;/item&gt;
      &lt;item&gt;Exposes Canvas, Camera, and Screen capture commands.&lt;/item&gt;
      &lt;item&gt;Runbook: Android connect.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workspace root: &lt;code&gt;~/clawd&lt;/code&gt;(configurable via&lt;code&gt;agents.defaults.workspace&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Injected prompt files: &lt;code&gt;AGENTS.md&lt;/code&gt;,&lt;code&gt;SOUL.md&lt;/code&gt;,&lt;code&gt;TOOLS.md&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Skills: &lt;code&gt;~/clawd/skills/&amp;lt;skill&amp;gt;/SKILL.md&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Minimal &lt;code&gt;~/.clawdbot/clawdbot.json&lt;/code&gt; (model + defaults):&lt;/p&gt;
    &lt;code&gt;{
  agent: {
    model: "anthropic/claude-opus-4-5"
  }
}&lt;/code&gt;
    &lt;p&gt;Full configuration reference (all keys + examples).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default: tools run on the host for the main session, so the agent has full access when it’s just you.&lt;/item&gt;
      &lt;item&gt;Group/channel safety: set &lt;code&gt;agents.defaults.sandbox.mode: "non-main"&lt;/code&gt;to run non‑main sessions (groups/channels) inside per‑session Docker sandboxes; bash then runs in Docker for those sessions.&lt;/item&gt;
      &lt;item&gt;Sandbox defaults: allowlist &lt;code&gt;bash&lt;/code&gt;,&lt;code&gt;process&lt;/code&gt;,&lt;code&gt;read&lt;/code&gt;,&lt;code&gt;write&lt;/code&gt;,&lt;code&gt;edit&lt;/code&gt;,&lt;code&gt;sessions_list&lt;/code&gt;,&lt;code&gt;sessions_history&lt;/code&gt;,&lt;code&gt;sessions_send&lt;/code&gt;,&lt;code&gt;sessions_spawn&lt;/code&gt;; denylist&lt;code&gt;browser&lt;/code&gt;,&lt;code&gt;canvas&lt;/code&gt;,&lt;code&gt;nodes&lt;/code&gt;,&lt;code&gt;cron&lt;/code&gt;,&lt;code&gt;discord&lt;/code&gt;,&lt;code&gt;gateway&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Details: Security guide · Docker + sandboxing · Sandbox config&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Link the device: &lt;code&gt;pnpm clawdbot channels login&lt;/code&gt;(stores creds in&lt;code&gt;~/.clawdbot/credentials&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Allowlist who can talk to the assistant via &lt;code&gt;channels.whatsapp.allowFrom&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If &lt;code&gt;channels.whatsapp.groups&lt;/code&gt;is set, it becomes a group allowlist; include&lt;code&gt;"*"&lt;/code&gt;to allow all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set &lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt;or&lt;code&gt;channels.telegram.botToken&lt;/code&gt;(env wins).&lt;/item&gt;
      &lt;item&gt;Optional: set &lt;code&gt;channels.telegram.groups&lt;/code&gt;(with&lt;code&gt;channels.telegram.groups."*".requireMention&lt;/code&gt;); when set, it is a group allowlist (include&lt;code&gt;"*"&lt;/code&gt;to allow all). Also&lt;code&gt;channels.telegram.allowFrom&lt;/code&gt;or&lt;code&gt;channels.telegram.webhookUrl&lt;/code&gt;as needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  channels: {
    telegram: {
      botToken: "123456:ABCDEF"
    }
  }
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set &lt;code&gt;SLACK_BOT_TOKEN&lt;/code&gt;+&lt;code&gt;SLACK_APP_TOKEN&lt;/code&gt;(or&lt;code&gt;channels.slack.botToken&lt;/code&gt;+&lt;code&gt;channels.slack.appToken&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set &lt;code&gt;DISCORD_BOT_TOKEN&lt;/code&gt;or&lt;code&gt;channels.discord.token&lt;/code&gt;(env wins).&lt;/item&gt;
      &lt;item&gt;Optional: set &lt;code&gt;commands.native&lt;/code&gt;,&lt;code&gt;commands.text&lt;/code&gt;, or&lt;code&gt;commands.useAccessGroups&lt;/code&gt;, plus&lt;code&gt;channels.discord.dm.allowFrom&lt;/code&gt;,&lt;code&gt;channels.discord.guilds&lt;/code&gt;, or&lt;code&gt;channels.discord.mediaMaxMb&lt;/code&gt;as needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  channels: {
    discord: {
      token: "1234abcd"
    }
  }
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires &lt;code&gt;signal-cli&lt;/code&gt;and a&lt;code&gt;channels.signal&lt;/code&gt;config section.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS only; Messages must be signed in.&lt;/item&gt;
      &lt;item&gt;If &lt;code&gt;channels.imessage.groups&lt;/code&gt;is set, it becomes a group allowlist; include&lt;code&gt;"*"&lt;/code&gt;to allow all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure a Teams app + Bot Framework, then add a &lt;code&gt;msteams&lt;/code&gt;config section.&lt;/item&gt;
      &lt;item&gt;Allowlist who can talk via &lt;code&gt;msteams.allowFrom&lt;/code&gt;; group access via&lt;code&gt;msteams.groupAllowFrom&lt;/code&gt;or&lt;code&gt;msteams.groupPolicy: "open"&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses the Gateway WebSocket; no separate WebChat port/config.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Browser control (optional):&lt;/p&gt;
    &lt;code&gt;{
  browser: {
    enabled: true,
    controlUrl: "http://127.0.0.1:18791",
    color: "#FF4500"
  }
}&lt;/code&gt;
    &lt;p&gt;Use these when you’re past the onboarding flow and want the deeper reference.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start with the docs index for navigation and “what’s where.”&lt;/item&gt;
      &lt;item&gt;Read the architecture overview for the gateway + protocol model.&lt;/item&gt;
      &lt;item&gt;Use the full configuration reference when you need every key and example.&lt;/item&gt;
      &lt;item&gt;Run the Gateway by the book with the operational runbook.&lt;/item&gt;
      &lt;item&gt;Learn how the Control UI/Web surfaces work and how to expose them safely.&lt;/item&gt;
      &lt;item&gt;Understand remote access over SSH tunnels or tailnets.&lt;/item&gt;
      &lt;item&gt;Follow the onboarding wizard flow for a guided setup.&lt;/item&gt;
      &lt;item&gt;Wire external triggers via the webhook surface.&lt;/item&gt;
      &lt;item&gt;Set up Gmail Pub/Sub triggers.&lt;/item&gt;
      &lt;item&gt;Learn the macOS menu bar companion details.&lt;/item&gt;
      &lt;item&gt;Platform guides: Windows (WSL2), Linux, macOS, iOS, Android&lt;/item&gt;
      &lt;item&gt;Debug common failures with the troubleshooting guide.&lt;/item&gt;
      &lt;item&gt;Review security guidance before exposing anything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Skills config&lt;/item&gt;
      &lt;item&gt;Default AGENTS&lt;/item&gt;
      &lt;item&gt;Templates: AGENTS&lt;/item&gt;
      &lt;item&gt;Templates: BOOTSTRAP&lt;/item&gt;
      &lt;item&gt;Templates: IDENTITY&lt;/item&gt;
      &lt;item&gt;Templates: SOUL&lt;/item&gt;
      &lt;item&gt;Templates: TOOLS&lt;/item&gt;
      &lt;item&gt;Templates: USER&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Clawdbot was built for Clawd, a space lobster AI assistant. 🦞 by Peter Steinberger and the community.&lt;/p&gt;
    &lt;p&gt;See CONTRIBUTING.md for guidelines, maintainers, and how to submit PRs. AI/vibe-coded PRs welcome! 🤖&lt;/p&gt;
    &lt;p&gt;Special thanks to Mario Zechner for his support and for pi-mono.&lt;/p&gt;
    &lt;p&gt;Thanks to all clawtributors:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46760237</guid><pubDate>Mon, 26 Jan 2026 00:27:41 +0000</pubDate></item><item><title>Video Games as Art</title><link>https://gwern.net/video-game-art</link><description>&lt;doc fingerprint="25e7211a03614af4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Video Games as Art&lt;/head&gt;
    &lt;p&gt;Video games are art, but a strange art: their essence is transformation of the player, not description to the player. This makes meaningful criticism nearly impossible—you can point at the moon, but it’s not the moon, and once someone sees it, they no longer need the pointing.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Video games are art. But they are a strange art. They are an art without good art criticism, and they occupy a peculiar position in popular culture: universal and dominant, and yet almost invisible outside their medium, unable to escape (compare movie adaptations of books vs games).&lt;/p&gt;
      &lt;p&gt;Why?&lt;/p&gt;
      &lt;p&gt;Because the essence of a video game, which makes it more than a low-quality animated movie, is that it is interactive and requires the player to enact the plot. It transforms the player’s mind.&lt;/p&gt;
      &lt;p&gt;Such transformations cannot be written down or filmed; if they could, they wouldn’t need to be a video game.&lt;/p&gt;
      &lt;p&gt;So video game criticism, and broader pop culture use of video games, is hamstrung. Criticism is often limited to serving as advertising, a finger pointing to the moon in hinting at the transformation, exegesis, parasocial gossip, or technical critique of the craft.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Roger Ebert once claimed that video games cannot be art. At this point, most people, including myself, disagree: it is simply obvious that they can be.&lt;/p&gt;
    &lt;p&gt;But what kind of art are they? At the risk of seeming to say something hopelessly obvious, the distinctive feature of video game art is that it is interactive rather than passive.&lt;/p&gt;
    &lt;p&gt;For all the 10-hour-long YouTube explainers or blog posts or endless Let’s-play or the rise of the streaming industry (based largely on video game as filler) or meritorious attempts at creating an academic literature around games (eg. Well Played) or celebrity critics like Yahtzee Croshaw, I find no form of criticism as unsatisfactory as video game criticism. To read a review or an attempted critique of a video game is scarcely more satisfying than someone telling you about a dream they had once; presenting a video of cutscene compilations or a few minutes of gameplay doesn’t add much. Even a psychedelic trip report or a music album review is more interesting and gives one more insight.&lt;/p&gt;
    &lt;p&gt;At this point, we can’t blame the immaturity of the form. Video games have been one of the largest media in the world for decades, and we are at least 3 generations into the art form, and practically every child in First World countries like the USA has played games. (M.U.L.E for example was 42 years ago, in 198343ya, and Tempest 44 years ago.) Billions of man-years have been spent creating, playing, and discussing games.&lt;/p&gt;
    &lt;p&gt;So, if they are art, and we are all extremely familiar with them and have great sophistication, and some of our most gifted young people have gone into games, why is it so hard to say what art they are or discuss things like what makes some great works of art but others just well-produced entertainment?&lt;/p&gt;
    &lt;p&gt;My answer, after all these years, is that a critique of a video game is indeed like someone telling you about a dream they had: “you had to be there—and doing it, like I was.”1&lt;/p&gt;
    &lt;p&gt;The critical difference between a movie, novel, album, painting, sculpture etc., and a video game (and perhaps other critically neglected art-forms, like perfume, where reviews are so mutually contradictory and our vocabulary impoverished) is that the former is something you feel or experience, while the latter is something you do or are.&lt;/p&gt;
    &lt;p&gt;An artful video game cannot be described, because it is not a description but a transformation. (Notably, the closest ‘passive’ art-forms I can think of in this respect are also some of the most demanding of their viewers, like The Ring opera cycles, in both intensity and time. Is it an accident that reviews of escape rooms or immersive theater never seem to successfully convey why you would want to bother?)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To be a student required a peculiar kind of capitulation, a willingness not simply to do as one is told, but to surrender the movements of one’s soul to the unknown complexities of another’s. A willingness, not simply to be moved, but to be remade.&lt;/p&gt;
      &lt;p&gt;—R. Scott Bakker, The Judging Eye (200917ya)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And so, good art game criticism can only be understood by those who have no need of it; a hand may point at the moon, but once you see the moon, you no longer need to look at the hand. Can anything convey the psychedelic trance horror of a Tempest player locked into an alternate state of consciousness after hours of pulsating vector-art warfare? Descriptions of such games, as below, can only point at the moon. (As such, video games are less extreme examples of “transformative experiences”.)&lt;/p&gt;
    &lt;p&gt;Oft-cited game art Journey embodies Antoine de Saint-Exupéry’s Wind, Sand and Stars/The Little Prince, in putting the player in a vast desert, seeking a transient connection with other humans while conducting an increasingly familiar ritual.&lt;/p&gt;
    &lt;p&gt;Rogue-likes or permadeath games like Apocalypse tailor their mechanics to teach their own lessons, about the nature of impermanence, the irrevocable passage of time, the virtues of caution and the value-of-information.&lt;/p&gt;
    &lt;p&gt;“Walking simulators” may seem passive, but are still far from a novel or a movie, where the creator controls your attention at every instant; the player must be trusted to choose to see things, and put the pieces together.&lt;/p&gt;
    &lt;p&gt;A Tetris player cannot describe the experience of dreaming about playing Tetris, or about starting to see everything as blocks which can be fitted together without gaps. (They can describe having had a Tetris dream, but not the experience of dreaming in this new way; the twist ending of Blow’s The Witness comes to mind as attempting to capture the effect of such puzzle games.) Once one has started dreaming in Tetris, and started hearing the bleeping muzak and seeing the endless onslaught of pieces dropping next to the pixel art onion domes, one might say that Tetris is done as an artwork.&lt;/p&gt;
    &lt;p&gt;Patrick McKenzie has given a good description of Factorio, which is about creating a vast factory and optimizing all its conveyor belts &amp;amp; pipes, furnaces, stockpiles etc. to “make number go up”. He praises it as some of the best training for an engineer or systems analyst. What makes Factorio so good at this? Is there some compelling actor or soundtrack? Does it have a brilliantly compelling SF plot written by a famous author about colonizing a new planet? Is it a best-selling book like The Goal, to be found in an airport bookstall near you?&lt;/p&gt;
    &lt;p&gt;It has none of that. But it works as video game art because in playing it, in order to play it well rather than continue losing like “a scrub”, one is continually forced to scrutinize one’s factory for bottlenecks, observe tradeoffs in systems like opportunity cost, spot unforeseen consequences of earlier shortcuts… And what has been seen cannot be unseen. Someone who plays Factorio long enough and well enough for Factorio to start playing them will start to see the world differently, as a system to optimize: as a series of pipes shunting material from place to place, with unexpected bottlenecks, and filled with serious design mistakes… and where the urge to optimize can become pathological.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;One recent Sunday, I had just installed a pump on a lake shore to feed water to my concrete plant, when it occurred to me that I hadn’t drunk any real-world water in several hours. My head was aching, but I didn’t want to get up from my computer. I wanted to solve the problem with a click of a mouse, the way I would in the game, running a few meters of pipe from the kitchen tap to my hunched form (and perhaps another few meters of pipe from my hunched form to the toilet).&lt;/p&gt;
      &lt;p&gt;I’ve been sucked into plenty of games before, but few have so completely disabled my conscious will, my sense of time, indeed any region of my brain that isn’t devoted to growing the factory.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When one learns to optimize, one must also learn to not optimize. And this may be the most valuable lesson that getting addicted to Factorio can teach: what does it feel like to be a paperclipper? To be optimizing for something that has long since ceased to matter? To be so caught up in playing the game as to forget to ask if this game is worth playing in the first place?&lt;/p&gt;
    &lt;p&gt;This is a lesson taught even more directly by Frank Lantz’s Universal Paperclips: many players get similarly caught up in the clicker game, despite “Clippy” being explicitly a cautionary thought experiment about mindless optimization run amok. And I remember my own experience playing Neopets—getting caught up in the grind and pursuing rare items and trying to make money on the Neopia stock market, until one day I was banned for manipulation; I asked myself, before trying to set up a new account, “why am I doing this?” and, unable to come up with a good answer, stopped playing forever.&lt;/p&gt;
    &lt;p&gt;Once one has learned ‘to see like a factory’, and the risks and benefits of this vision, Factorio is done as an artwork. The artwork has achieved its goal. You can keep playing, but now it is just entertainment, and a toolkit. (One is, however, now equipped to create one’s own artworks in the form of Factorio levels or challenges—like ‘avant-garde factories’ which deconstruct or subvert the ‘optimization esthetic’, which make no sense to outsiders who cannot even understand what makes a factory optimized.)&lt;/p&gt;
    &lt;p&gt;Similarly, Shadow of the Colossus is a work of art because—as one of the few good writings I have seen on it explains, “Losing Your Grip: Futility and Dramatic Necessity in Shadow of the Colossus”, Fortugno 200917ya—it expresses an esthetic of sorrow and loss, and the selfish, self-degrading nature of the protagonist’s quest to undo the death of his beloved. The necessity of accepting death and the need to let go is a familiar theme (eg. Orpheus), but most mediums can only show it; a video game like Shadow can embody it by making the player do the degrading to himself. (See also Papers, Please.)&lt;/p&gt;
    &lt;p&gt;In Shadow, the ‘monster slaying’ is only a small part of the world. Most of the world is unlocked: the player could just explore for hours and admire the scenery and the creatures. The player can watch the creatures he will slay, because they usually won’t attack him—he must choose to attack them. This is conveyed by the player’s own growing grief and sorrow over the beautiful landscapes he travels. (But just in case the point was lost on the player, the character art also grows paler and more demonic.) He seeks out and destroys each wonder of nature, becoming an ever paler shadow of himself…&lt;/p&gt;
    &lt;p&gt;Until in the end, he is sucked into an endless vortex. Elegantly, the player can avoid being sucked in by clinging as long as they can, using the skills they have mastered… but it changes nothing, beyond exhausting the player. Sooner or later, the player must—let go. And they do. The keenest sorrow is to recognize oneself as the cause of one’s misfortunes; but in this anagnorisis, there is hope of grace, if not redemption. It is too late for the character, but it is not too late for the player. Once the player has learned to let go, and sorrow over the character’s wasted life and devastation of Nature, they have learned to ‘see like Shadow’ and now understand the tragic but compassionate vision of Shadow of the Colossus.&lt;/p&gt;
    &lt;p&gt;Those who cannot or will not let themselves be changed by game art, cannot understand them as art. The concept of ‘scrub’ is an interesting example here: we might say that a scrub player is the philistine of games. They are too caught up in playing the game as they think it ‘should be’, to surrender themselves to the game as it is, no matter how often they lose. (Losing is the most powerful mechanism a game has for teaching the player, and if ignored, can silence the teacher’s voice—in some games, particularly Dark Souls-style games, the only voice the game has, because to speak too clearly would undo the player’s achievements.) They have many criticisms, and few observations: they can tell you what is bad and missing from a novel like Finnegans Wake, like proper spelling, but not what is good and present. And so they can never be transformed by a game.&lt;/p&gt;
    &lt;p&gt;They can appreciate isolated parts of the game, but this is a low level of ‘art as buffet’ appreciation. The virtue of those parts is that they will combine to more than the sum of their parts. If you admire the orchestral OST, or the art style, or memorable quotes, or a plot summary, this is all well and good, but it is like going to a cathedral and looking at it piece by piece, and admiring each one, but failing to recall that a cathedral is a place to do rituals, and is not a museum for passively admiring the art and craft.&lt;/p&gt;
    &lt;p&gt;This is the same failure mode as video game art criticism, in a way, in treating it as entertainment and just the sum of its parts. To tell me that the graphics are so many gigabytes of files, or the world has 30,000 rooms, or there is 300 hours of recorded NPC lines, or that it’s an arena shooter with microtransactions, is to tell me something as ultimately useless as “this movie cost $300 million to make”. Saying that this fantasy character killed this other character in a cutscene, in one alternate ending, is little better. Even talking about ‘fun’ is still missing the mark.&lt;/p&gt;
    &lt;p&gt;Game art criticism only works when it conveys the transformativeness on the player (ie. reviewer/critic). For example, this review of Grid Wars 2 manages to convey how it becomes a completely different space shooter game when one starts thinking of the random black holes as not merely environmental hazards, but as one’s real enemies, who feed on the ‘enemies’, and so it is not about shooting enemy spaceships but “black hole farming”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Sometimes it’s as simple as putting a hole between you and a pack of enemies (such as the dumb blue diamonds, which just head straight for you regardless of what might be in the way). Sometimes (eg. with the green squares which run away from your shots) it’s the slightly more sophisticated method of putting the enemies between the hole and your fire, driving them into the hole as they flee from the bullet stream. Sometimes it’s desperately hovering at the edge of the black hole and shooting the enemies before they fall into it, because otherwise it’ll be overwhelmed and explode before you have the chance to blow it up and get the points. And sometimes, most terrifyingly, it’s sitting in the midst of a huge wave of deadly enemies on the edge of a whirling star of death and not shooting at all.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Even accidental unintended games demonstrate this: the participants in what you might call the ‘accidental ARG’ of the Paul McCartney coverup conspiracy theory found their consciousnesses transformed into the paranoid schizophrenic mindset, in which they interrogate every scrap of the Beatles as not just evidence that Paul McCartney had died &amp;amp; been secretly replaced, but as deliberate communications from the Beatles about the coverup. After playing the ‘Paul is Dead’ ARG long enough, they literally saw different things on album covers and heard different things when playing records the wrong way. This is something that cannot be communicated except by experiencing yourself, and a good writeup like “Who Buried Paul?” succeeds by teaching us “Paul-is-Dead 101” thinking so we can feel what it is like to see messages about this tragedy &amp;amp; coverup hidden in plain sight. (See also Foucault’s Pendulum and Unsong.)&lt;/p&gt;
    &lt;p&gt;Perhaps it’s no surprise that such criticism is so unsatisfactory. Epiphanies do not come like clockwork to all. Personal transformation cannot be scheduled reliably for a tight magazine deadline, nor can it easily be conveyed in words. They are premised on a false model of how such art works, and how it should be critiqued. And even when they hit the mark, they are still derivative secondary works—at best pale echoes of an actual transformation some player once had.&lt;/p&gt;
    &lt;p&gt;Given the commercial realities, perhaps this cannot be fixed, and we must accept that timely reviews are ultimately the “Cliff Notes” of games.&lt;/p&gt;
    &lt;p&gt;But with this in mind, we can focus on reviews for older games, which have had enough time for transformation to happen, by players who have just recently been transformed—they are still capable of explaining it to the un-transformed.&lt;/p&gt;
    &lt;p&gt;that’s a possible justification. I think one can probably come up with a relatively short list of major functions video game art criticism could usefully serve: (1) professional analysis on the technical level of ‘how did they do X? why does Y work? how did they avoid Z?’ (2) serve as a meditation master to help players be enlightened, for ones who are just not quite Getting It and need a good ‘Kwatz!’ or koan; (3) do their best to take the reader through the transformation, knowing that they can’t really, but if they point at the moon, perhaps the right reader will be intrigued enough by the snack-sized sampler platter to go and do the real thing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46760998</guid><pubDate>Mon, 26 Jan 2026 02:07:34 +0000</pubDate></item><item><title>Running the Stupid Cricut Software on Linux</title><link>https://arthur.pizza/2025/12/running-stupid-cricut-software-under-linux/</link><description>&lt;doc fingerprint="171f99183bc7b5f9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Why The Hell Would You do This?&lt;/head&gt;
    &lt;p&gt;I’m more than happy building vector designs in Inkscape. It’s the most proficient vector designer app that is free, open source, and runs natively under Linux. However, simply having a quality SVG is only part of the process when it comes to using a plotter like the Cricut.&lt;/p&gt;
    &lt;p&gt;On my own, I would not recommend the Cricut brand, but this is a machine that was bought years ago, and I want to get the most usage out of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Install Cricut Design Space&lt;/head&gt;
    &lt;p&gt;As far as I can tell, almost everything works in the Cricut Design Space application under Linux. The only bug I’ve noticed is the application becomes invisible when going full screen. This could easily be a Wayland bug too. I have no idea. Ideally, Cricut could Easily make a Linux Application with WINE. I could see this as a Flatpak install. I understand why they don’t, as that comes with the expectation of support, and supporting another operating system costs money.&lt;/p&gt;
    &lt;p&gt;This is a multistep process that you only really have to do once. Because of the complexity, I decided to lay out the instructions, along with the ‘why’ of the situation. The more you understand how it works, the more armed you are if anything goes wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the Software&lt;/head&gt;
    &lt;p&gt;I HIGHTLY recommend grabbing a fresh build of WINE. I’m a Debian user, so depending on the build of Linux you’re using it might vary, but I start on the Wine HQ Git. I know a lot of people might want to run a platform like Wine Bottles, but in my experience a more granular project like this is just easier under regular WINE.&lt;/p&gt;
    &lt;head rend="h3"&gt;OS Detection on the Website&lt;/head&gt;
    &lt;p&gt;When you visit the Design Space Download Page, it seems that the developer chose to use OS detection on the website. If it sees your user agent listed as Linux, for some weird reason it defaults to Mac. Logically, it would make more sense to default to the Windows build, but I think there was some corner cutting on this.&lt;/p&gt;
    &lt;p&gt;I recommend the open source UserAgent Switcher for this, as it has a build for Firefox and Chromium based browsers. (And of course, is free and open source.) After setting to “Windows 10” mode and refreshing the page:&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing the Software&lt;/head&gt;
    &lt;p&gt;As of this tutorial, the latest build of Design Space is &lt;code&gt;CricutDesignSpace-Install-v9.47.92.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Make sure you’ve setup your install of WINE first. run &lt;code&gt;winecfg&lt;/code&gt; and make sure everything is working.&lt;/p&gt;
    &lt;code&gt;wine CricutDesignSpace-Install-v9.47.92.exe
&lt;/code&gt;
    &lt;p&gt;This will launch the installer. Just install like you normally would under Windows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Logging in, the Stupid Parts…&lt;/head&gt;
    &lt;p&gt;First we need to find the binary with the &lt;code&gt;where&lt;/code&gt; command. Here’s how it looked on my machine.&lt;/p&gt;
    &lt;code&gt;where cricut
cricut: aliased to wine '/home/art/.wine/drive_c/users/art/AppData/Local/Programs/Cricut Design Space/Cricut Design Space.exe'
&lt;/code&gt;
    &lt;p&gt;Then we need to open two terminals (or two sessions, I use tmux for that).&lt;/p&gt;
    &lt;p&gt;Terminal #1&lt;/p&gt;
    &lt;code&gt;wine Cricut\ Design\ Space.exe
&lt;/code&gt;
    &lt;p&gt;This will show the login panel and will want to launch your default browser. On my machine, that’s my native Firefox. Login there, and you’ll see a url asking for the Cricut software. Because your native browser and the wine wrapped cricut software can’t see each other, you gotta extract the part of the URL that features &lt;code&gt;code=&lt;/code&gt;. In the second terminal:&lt;/p&gt;
    &lt;p&gt;Terminal #2&lt;/p&gt;
    &lt;code&gt;wine Cricut\ Design\ Space.exe "cricut://?code=XXXXXXXXXXXXXXXXXXX
&lt;/code&gt;
    &lt;p&gt;You should be logged in!&lt;/p&gt;
    &lt;p&gt;Now you can start uploading your designs and colaborating with other through their locked down, proprietary platform. You can even waste money on stock images.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46761761</guid><pubDate>Mon, 26 Jan 2026 04:05:38 +0000</pubDate></item><item><title>The browser is the sandbox</title><link>https://simonwillison.net/2026/Jan/25/the-browser-is-the-sandbox/</link><description>&lt;doc fingerprint="5daa04924fbda384"&gt;
  &lt;main&gt;
    &lt;p&gt;the browser is the sandbox. Paul Kinlan is a web platform developer advocate at Google and recently turned his attention to coding agents. He quickly identified the importance of a robust sandbox for agents to operate in and put together these detailed notes on how the web browser can help:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This got me thinking about the browser. Over the last 30 years, we have built a sandbox specifically designed to run incredibly hostile, untrusted code from anywhere on the web, the instant a user taps a URL. [...]&lt;/p&gt;
      &lt;p&gt;Could you build something like Cowork in the browser? Maybe. To find out, I built a demo called Co-do that tests this hypothesis. In this post I want to discuss the research I've done to see how far we can get, and determine if the browser's ability to run untrusted code is useful (and good enough) for enabling software to do more for us directly on our computer.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Paul then describes how the three key aspects of a sandbox - filesystem, network access and safe code execution - can be handled by browser technologies: the File System Access API (still Chrome-only as far as I can tell), CSP headers with &lt;code&gt;&amp;lt;iframe sandbox&amp;gt;&lt;/code&gt; and WebAssembly in Web Workers.&lt;/p&gt;
    &lt;p&gt;Co-do is a very interesting demo that illustrates all of these ideas in a single application:&lt;/p&gt;
    &lt;p&gt;You select a folder full of files and configure an LLM provider and set an API key, Co-do then uses CSP-approved API calls to interact with that provider and provides a chat interface with tools for interacting with those files. It does indeed feel similar to Claude Cowork but without running a multi-GB local container to provide the sandbox.&lt;/p&gt;
    &lt;p&gt;My biggest complaint about &lt;code&gt;&amp;lt;iframe sandbox&amp;gt;&lt;/code&gt; remains how thinly documented it is, especially across different browsers. Paul's post has all sorts of useful details on that which I've not encountered elsewhere, including a complex double-iframe technique to help apply network rules to the inner of the two frames.&lt;/p&gt;
    &lt;p&gt;Thanks to this post I also learned about the &lt;code&gt;&amp;lt;input type="file" webkitdirectory&amp;gt;&lt;/code&gt; tag which turns out to work on Firefox, Safari and Chrome and allows a browser read-only access to a full directory of files at once. I had Claude knock up a webkitdirectory demo to try it out and I'll certainly be using it for projects in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wilson Lin on FastRender: a browser built by thousands of parallel agents - 23rd January 2026&lt;/item&gt;
      &lt;item&gt;First impressions of Claude Cowork, Anthropic's general agent - 12th January 2026&lt;/item&gt;
      &lt;item&gt;My answers to the questions I posed about porting open source code with LLMs - 11th January 2026&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46762150</guid><pubDate>Mon, 26 Jan 2026 05:23:01 +0000</pubDate></item><item><title>The Holy Grail of Linux Binary Compatibility: Musl and Dlopen</title><link>https://github.com/quaadgras/graphics.gd/discussions/242</link><description>&lt;doc fingerprint="adda3afb1a14b6a8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Holy Grail of Linux Binary Compatibility: musl + dlopen #242&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I guess using Go + Godot to build native &amp;amp; installable Android &amp;amp; iOS binaries (without any proprietary SDKs) was too easy. So it's time for a real challenge...&lt;/p&gt;
          &lt;head&gt;Linux Binary Compatibility&lt;/head&gt;
          &lt;p&gt;(some background reading: https://jangafx.com/insights/linux-binary-compatibility)&lt;/p&gt;
          &lt;p&gt;For a while now, it's been very easy to reliably ship command line software &amp;amp; servers for Linux, just run &lt;/p&gt;
          &lt;p&gt;The problems begin to creep in when you want access to hardware accelerated graphics. All the GPU drivers on Linux require accessing dynamic libraries via the C ABI. These C libraries are built against a particular libc, which is most commonly &lt;/p&gt;
          &lt;p&gt;In fact, I've directly experienced this, as I recently replaced the OS on my personal computer with the &lt;/p&gt;
          &lt;p&gt;That's a problem, firstly because this is my distro now, I need to be able to build graphics.gd projects! Secondly, in theory, &lt;/p&gt;
          &lt;head&gt;Supporting &lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 2 comments&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I use the dlopen/dlsym technique in Linux and loadlibrary/getprocaddress in my language's VM.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Doesn't work for me:&lt;/p&gt;
          &lt;code&gt;Thread 1 "dodge_the_creep" received signal SIGSEGV, Segmentation fault.
0x0000000002b13d16 in foreign_tramp ()
(gdb) bt
#0  0x0000000002b13d16 in foreign_tramp ()
#1  0x00007fffaa55a5a0 in ?? ()
#2  0x00007ffff75e4558 in ?? ()
#3  0x00007fff5ffec436 in ?? ()
#4  0x00007fffaa2ed710 in ?? ()
#5  0x00007fffffffc0d0 in ?? ()
#6  0x00007fff5ffe5ecc in ?? ()
#7  0x00007fffffffc0d0 in ?? ()
#8  0x00007fff5f9cc488 in ?? ()
#9  0x00007fffaa559a40 in ?? ()
#10 0x0000000000000000 in ?? ()
(gdb)&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46762882</guid><pubDate>Mon, 26 Jan 2026 07:41:52 +0000</pubDate></item><item><title>Being a Canadian in America (Eric Migicovsky)</title><link>https://ericmigi.com/blog/on-being-a-canadian-in-america-in-2026/</link><description>&lt;doc fingerprint="a68259701ff5afbe"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;An Evening Out Colette Berends&lt;/p&gt;
      &lt;p&gt;(I wrote a draft of post in early 2025. I picked it up and decided to publish it today, hence why it is more about ‘me’ than the current horrific news of the day)&lt;/p&gt;
      &lt;p&gt;Why do I, a fairly liberal Canadian, continue to live in California instead of moving home, given America’s rightward lurch?&lt;/p&gt;
      &lt;p&gt;The answer is that I’ve built a family and community here, which makes me reluctant to leave. Will that reluctance last forever? Obviously not, there’s always a red line.&lt;/p&gt;
      &lt;p&gt;The shooting of a protester yesterday in Minneapolis feels very close crossing the red line for me. Will there be due process? Could additional information potentially come to light that would convince me that this is not a fascist execution of a civilian in the street? Both seem unlikely. The split screen hypocrisy of Trump claiming to support Iranian protesters, while simultaneously having his troops at home kill protesters who he calls ‘domestic terrorists’ is mindblowing.&lt;/p&gt;
      &lt;p&gt;Could I help effect change? I can’t vote (not a US citizen) and at this point I don’t feel very comfortable joining demonstrations (like I did after George Floyd’s murder) not due to violence but it feels…incongruous to be an non-citizen protesting against policies that the majority of voting citizens in this country specifically voted for as recently as 1 year ago. I could theoretically donate money to candidates (as a green card holder) but that does not seem very effective given the immediate circumstance. The only tool I seem to have is voting with my feet and tax dollars.&lt;/p&gt;
      &lt;p&gt;I’ve lived here (in the same neighbourhood) for 15 years now. This feels like home. My family and I are not in danger…yet. Do we wait until it’s in our state? Or city? Moving away also feels incredibly privileged. Others are still fighting to move to the US to avoid even worse situations in their home country.&lt;/p&gt;
      &lt;p&gt;If you stand for nothing, what will you fall for?&lt;/p&gt;
      &lt;p&gt;I want to be clear about what I believe in. Here’s a short list:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;people should be free to move and pursue opportunity anywhere they’d like&lt;/item&gt;
        &lt;item&gt;people should be judged by the quality of their character, not their innate or unchangeable characteristics.&lt;/item&gt;
        &lt;item&gt;technology on average is a good thing, but it can amplify the worst of human tendencies&lt;/item&gt;
        &lt;item&gt;after receiving information that shows how our opinions or beliefs are incorrect, we should update our beliefs&lt;/item&gt;
        &lt;item&gt;free speech is a great thing, but I am not an absolutist&lt;/item&gt;
        &lt;item&gt;I don’t love the idea of people being able to make split second decisions/mistakes that could end someone else’s life (eg guns, texting while driving, nuclear weapons)&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;and probably many more things, but this is a good start.&lt;/p&gt;
      &lt;p&gt;In the past, I haven’t done a great job of supporting specific causes (either publicly or financially) but it is something I should do better. I am not claiming to be an expert, or that I’ve read all the literature. Strong opinions, weakly held! My ears are always open.&lt;/p&gt;
      &lt;p&gt;I would like to voice my support for the trans community. It’s an issue that’s dear to me, my friends and my family. My great aunt Colette (born 1934) transitioned and lived a spectacularly beautiful life (documented in a movie too!). I don’t have a strong opinion about the sports issue (I don’t play sports, nor am I a woman or trans) but I hate that it is being used as a gateway wedge issue to dehumanise trans people. These are people! Human people. The Overton window is shifting and I am scared that more people will start treating trans people with hatred or violence.&lt;/p&gt;
      &lt;p&gt;The other group that I want to stand up for are immigrants. I am an immigrant. I am incredibly lucky to have been Canadian during an era which (up until recently) allowed for easy economic migration to the US (at least for engineers under the TN visa program). Coming down to California and taking part in YC was an incredibly formative part of my life. I think I gave back pretty good to the US as well! Pebble and Beeper would probably not exist had I not migrated down here. I wouldn’t have met my wife. Same deal as trans community - I hate that immigrants are being dehumanized and blamed for all of societies ills. How do ICE officers know who to stop and interrogate, anyways? If you prick us, do we not bleed? Why does that even matter…we’re all just people. The parallels to the 1930s are scary.&lt;/p&gt;
      &lt;p&gt;There’s no magic solution or proposal at the end of this blog post. Sorry (🇨🇦). I have donated over the weekend to nipnlg.org and ilrc.org and I’ll continue to support others fighting for these causes.&lt;/p&gt;
      &lt;p&gt;Maybe I am too much of a hopeless optimist, but it doesn’t hurt to have kindness for other people.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46763803</guid><pubDate>Mon, 26 Jan 2026 10:12:10 +0000</pubDate></item><item><title>MapLibre Tile: a modern and efficient vector tile format</title><link>https://maplibre.org/news/2026-01-23-mlt-release/</link><description>&lt;doc fingerprint="4787f953dcbb25f4"&gt;
  &lt;main&gt;
    &lt;p&gt;Jan 23, 2026&lt;/p&gt;
    &lt;p&gt;Today we are happy to announce MapLibre Tile (MLT), a new modern and efficient vector tile format.&lt;/p&gt;
    &lt;p&gt;MapLibre Tile (MLT) is a succesor to Mapbox Vector Tile (MVT). It has been redesigned from the ground up to address the challenges of rapidly growing geospatial data volumes and complex next-generation geospatial source formats, as well as to leverage the capabilities of modern hardware and APIs.&lt;/p&gt;
    &lt;p&gt;MLT is specifically designed for modern and next-generation graphics APIs to enable high-performance processing and rendering of large (planet-scale) 2D and 2.5 basemaps. This current implementation offers feature parity with MVT1 while delivering on the following:&lt;/p&gt;
    &lt;p&gt;In addition, MLT was designed to support the following use cases in the future:&lt;/p&gt;
    &lt;p&gt;As with any MapLibre project, the future of MLT is decided by the needs of the community. There are a lot of exciting ideas for other future extensions and we welcome contributions to the project.&lt;/p&gt;
    &lt;p&gt;For a more in-depth exploration of MLT have a look at the following slides, watch this talk or read this publication by MLT inventor Markus Tremmel.&lt;/p&gt;
    &lt;p&gt;For the adventurous, the answer is: today. Both MapLibre GL JS and MapLibre Native now support MLT sources. You can use the new &lt;code&gt;encoding&lt;/code&gt; property on sources in your style JSON with a value of &lt;code&gt;mlt&lt;/code&gt; for MLT vector tile sources.&lt;/p&gt;
    &lt;p&gt;To try out MLT, you have the following options:&lt;/p&gt;
    &lt;p&gt;Refer to this page for a complete and up-to-date list of integrations and implementations. If you are an integrator working on supporting MLT, feel free to add your own project there.&lt;/p&gt;
    &lt;p&gt;We would love to hear your experience with using MLT! Join the &lt;code&gt;#maplibre-tile-format&lt;/code&gt; channel on our Slack or create an Issue or Discussion on the tile spec repo.&lt;/p&gt;
    &lt;p&gt;MapLibre Tile came to be thanks to a multi-year collaboration between academia, open source and enterprise. Thank you to everyone who was involved! We are very proud that our community can innovate like this.&lt;/p&gt;
    &lt;p&gt;Special thanks go to Markus Tremmel for inventing the format, Yuri Astrakhan for spearheading the project, Tim Sylvester for the C++ implementation, Harel Mazor, Benedikt Vogl and Niklas Greindl for working on the JavaScript implementation.&lt;/p&gt;
    &lt;p&gt;Also thanks to Microsoft and AWS for financing work on MLT.&lt;/p&gt;
    &lt;p&gt;One exception: unlike MVT, MLT does not support layers where a value in a column changes type from feature to feature. ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46763864</guid><pubDate>Mon, 26 Jan 2026 10:19:51 +0000</pubDate></item><item><title>Show HN: Only 1 LLM can fly a drone</title><link>https://github.com/kxzk/snapbench</link><description>&lt;doc fingerprint="4fd783f63903440d"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Inspired by Pokémon Snap (1999). VLM pilots a drone through 3D world to locate and identify creatures.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;%%{init: {'theme': 'base', 'themeVariables': { 'background': '#ffffff', 'primaryColor': '#ffffff'}}}%%
flowchart LR
    subgraph Controller["**Controller** (Rust)"]
        C[Orchestration]
    end

    subgraph VLM["**VLM** (OpenRouter)"]
        V[Vision-Language Model]
    end

    subgraph Simulation["**Simulation** (Zig/raylib)"]
        S[Game State]
    end

    C --&amp;gt;|"screenshot + prompt"| V
    C &amp;lt;--&amp;gt;|"cmds + state&amp;lt;br&amp;gt;**UDP:9999**"| S

    style Controller fill:#8B5A2B,stroke:#5C3A1A,color:#fff
    style VLM fill:#87CEEB,stroke:#5BA3C6,color:#1a1a1a
    style Simulation fill:#4A7C23,stroke:#2D5A10,color:#fff
    style C fill:#B8864A,stroke:#8B5A2B,color:#fff
    style V fill:#B5E0F7,stroke:#87CEEB,color:#1a1a1a
    style S fill:#6BA33A,stroke:#4A7C23,color:#fff
&lt;/code&gt;
    &lt;p&gt;The simulation generates procedural terrain and spawns creatures (cat, dog, pig, sheep) for the drone to discover. It handles drone physics and collision detection, accepting 8 movement commands plus &lt;code&gt;identify&lt;/code&gt; and &lt;code&gt;screenshot&lt;/code&gt;. The Rust controller captures frames from the simulation, constructs prompts enriched with position and state data, then parses VLM responses into executable command sequences. The objective: locate and successfully identify 3 creatures, where &lt;code&gt;identify&lt;/code&gt; succeeds when the drone is within 5 units of a target.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;demo_3x.mov&lt;/head&gt;
    &lt;p&gt;I gave 7 frontier LLMs a simple task: pilot a drone through a 3D voxel world and find 3 creatures.&lt;/p&gt;
    &lt;p&gt;Only one could do it.&lt;/p&gt;
    &lt;p&gt;Is this a rigorous benchmark? No. However, it's a reasonably fair comparison - same prompt, same seeds, same iteration limits. I'm sure with enough refinement you could coax better results out of each model. But that's kind of the point: out of the box, with zero hand-holding, only one model figured out how to actually fly.&lt;/p&gt;
    &lt;p&gt;The core differentiator wasn't intelligence - it was altitude control. Creatures sit on the ground. To identify them, you need to descend.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemini Flash: Actively adjusts altitude, descends to creature level, identifies&lt;/item&gt;
      &lt;item&gt;GPT-5.2-chat: Gets close horizontally but never lowers&lt;/item&gt;
      &lt;item&gt;Claude Opus: Attempts identification 160+ times, never succeeds - approaching at wrong angles&lt;/item&gt;
      &lt;item&gt;Others: Wander randomly or get stuck&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This left me puzzled. Claude Opus is arguably the most capable model in the lineup. It knows it needs to identify creatures. It tries - aggressively. But it never adjusts its approach angle.&lt;/p&gt;
    &lt;p&gt;Run 13 (seed 72) was the only run where any model found 2 creatures. Why? They happened to spawn near each other. Gemini Flash found one, turned around, and spotted the second.&lt;/p&gt;
    &lt;p&gt;In most other runs, Flash found one creature quickly but ran out of iterations searching for the others. The world is big. 50 iterations isn't a lot of time.&lt;/p&gt;
    &lt;p&gt;This was the most surprising finding. I expected:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Opus 4.5 (most expensive) to dominate&lt;/item&gt;
      &lt;item&gt;Gemini 3 Pro to outperform Gemini 3 Flash (same family, more capability)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead, the cheapest model beat models costing 10x more.&lt;/p&gt;
    &lt;p&gt;What's going on here? A few theories:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spatial reasoning doesn't scale with model size - at least not yet&lt;/item&gt;
      &lt;item&gt;Flash was trained differently - maybe more robotics data, more embodied scenarios?&lt;/item&gt;
      &lt;item&gt;Smaller models follow instructions more literally - "go down" means go down, not "consider the optimal trajectory"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I genuinely don't know. But if you're building an LLM-powered agent that needs to navigate physical or virtual space, the most expensive model might not be your best choice.&lt;/p&gt;
    &lt;p&gt;Anecdotally, creatures with higher contrast (gray sheep, pink pigs) seemed easier to spot than brown-ish creatures that blended into the terrain. A future version might normalize creature visibility. Or maybe that's the point - real-world object detection isn't normalized either.&lt;/p&gt;
    &lt;p&gt;Before this, I tried having LLMs pilot a real DJI Tello drone.&lt;/p&gt;
    &lt;p&gt;Results: it flew straight up, hit the ceiling, and did donuts until I caught it. (I was using Haiku 4.5, which in hindsight explains a lot.)&lt;/p&gt;
    &lt;p&gt;The Tello is now broken. I've ordered a BetaFPV and might get another Tello since they're so easy to program. Now that I know Gemini Flash can actually navigate, a real-world follow-up might be worth revisiting.&lt;/p&gt;
    &lt;p&gt;This is half-serious research, half "let's see what happens."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The simulation has rough edges (it's a side project, not a polished benchmark suite)&lt;/item&gt;
      &lt;item&gt;One blanket prompt is used for all models - model-specific tuning would likely improve results&lt;/item&gt;
      &lt;item&gt;The feedback loop is basic (position, screenshot, recent commands) - there's room to get creative with what information gets passed back&lt;/item&gt;
      &lt;item&gt;Iteration limits (50) may artificially cap models that are slower but would eventually succeed&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;Install&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Zig&lt;/cell&gt;
        &lt;cell&gt;≥0.15.2&lt;/cell&gt;
        &lt;cell&gt;ziglang.org/download&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;stable (2024 edition)&lt;/cell&gt;
        &lt;cell&gt;rust-lang.org/tools/install&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;≥3.11&lt;/cell&gt;
        &lt;cell&gt;python.org&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;uv&lt;/cell&gt;
        &lt;cell&gt;latest&lt;/cell&gt;
        &lt;cell&gt;docs.astral.sh/uv&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You'll also need an OpenRouter API key.&lt;/p&gt;
    &lt;code&gt;gh repo clone kxzk/snapbench
cd snapbench

# set your API key
export OPENROUTER_API_KEY="sk-or-..."&lt;/code&gt;
    &lt;code&gt;# terminal 1: start the simulation (with optional seed)
zig build run -Doptimize=ReleaseFast -- 42
# or
make sim

# terminal 2: start the drone controller
cargo run --release --manifest-path llm_drone/Cargo.toml -- --model google/gemini-3-flash-preview
# or
make drone&lt;/code&gt;
    &lt;code&gt;# runs all models defined in bench/models.toml
uv run bench/bench_runner.py
# or
make bench&lt;/code&gt;
    &lt;p&gt;Results get saved to &lt;code&gt;data/run_&amp;lt;id&amp;gt;.csv&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model-specific prompts: Tune instructions to each model's strengths&lt;/item&gt;
      &lt;item&gt;Richer feedback: Pass more spatial context (distance readings, compass, minimap?)&lt;/item&gt;
      &lt;item&gt;Multi-agent runs: What if you gave each model a drone and made them compete?&lt;/item&gt;
      &lt;item&gt;Extended iterations: Let slow models run longer to isolate reasoning from speed&lt;/item&gt;
      &lt;item&gt;Real drone benchmark: Gemini Flash vs. the BetaFPV&lt;/item&gt;
      &lt;item&gt;Pokémon assets: Found low-poly Pokémon models on Poly Pizza—leaning into the Pokémon Snap inspiration&lt;/item&gt;
      &lt;item&gt;World improvements: Larger terrain, better visuals, performance optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Drone by NateGazzard CC-BY via Poly Pizza&lt;/item&gt;
      &lt;item&gt;Cube World Kit by Quaternius via Poly Pizza&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Donated to Poly Pizza to support the platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46764170</guid><pubDate>Mon, 26 Jan 2026 11:00:44 +0000</pubDate></item><item><title>TSMC Risk</title><link>https://stratechery.com/2026/tsmc-risk/</link><description>&lt;doc fingerprint="6380d4514daed7ef"&gt;
  &lt;main&gt;
    &lt;p&gt;Listen to this post:&lt;/p&gt;
    &lt;p&gt;You probably think, given this title, you know what this Article is about. The most advanced semiconductors are made by TSMC in Taiwan,1 and Taiwan is claimed by China, which has not and will not take reunification-by-force off of the table.&lt;/p&gt;
    &lt;p&gt;Relatedly, AI obviously has significant national security implications; at Davos, Anthropic CEO Dario Amodei reiterated his objection to the U.S. allowing the sale of Nvidia chips to China. From Bloomberg:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Anthropic Chief Executive Officer Dario Amodei said selling advanced artificial intelligence chips to China is a blunder with “incredible national security implications” as the US moves to allow Nvidia Corp. to sell its H200 processors to Beijing. “It would be a big mistake to ship these chips,” Amodei said in an interview with Bloomberg Editor-in-Chief John Micklethwait at the World Economic Forum in Davos, Switzerland. “I think this is crazy. It’s a bit like selling nuclear weapons to North Korea.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The nuclear weapon analogy is an interesting one: a lot of game theory was developed to manage the risk of nuclear weapons, particularly once the U.S.S.R. gained/stole nuclear capability, ending the U.S.’s brief monopoly on the technology. Before that happened, however, the U.S. had a dominant military position, given we had nuclear weapons and no one else did. Perhaps Amodei believes the U.S. should have advanced AI and China should not, giving us a dominant military position?&lt;/p&gt;
    &lt;p&gt;The problem with that reality, however, is Taiwan, as I explained in AI Promise and Chip Precariousness. AI, in contrast to nuclear weapons, has a physical dependency in Taiwan that can be easily destroyed by Chinese missiles, even without an invasion; if we got to a situation where only the U.S. had the sort of AI that would give us an unassailable advantage militarily, then the optimal strategy for China would change to taking TSMC off of the board.&lt;/p&gt;
    &lt;p&gt;Given this dependency, my recommendations in the Article run counter to Amodei: I want China dependent on not just U.S. chips but also on TSMC directly, which is why I argued in favor of selling Nvidia chips to China, and further believe that Huawei and other Chinese companies ought to be able to source from TSMC (on the flip side, I would ban the sale of semiconductor manufacturing equipment to Chinese fabs). I think it’s a good thing the Trump administration moved on the first point, at least.&lt;/p&gt;
    &lt;p&gt;However, this risk is not what this Article is about: there is another TSMC risk facing the entire AI industry in particular; moreover, it’s a risk the downside of which is already being realized.&lt;/p&gt;
    &lt;head rend="h3"&gt;The TSMC Brake&lt;/head&gt;
    &lt;p&gt;There was one refrain that was common across Big Tech earnings last quarter: demand for AI exceeds supply. Here was Amazon CEO Andy Jassy on the company’s earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You’re going to see us continue to be very aggressive investing in capacity because we see the demand. As fast as we’re adding capacity right now, we’re monetizing it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here was Microsoft CFO Amy Hood on the company’s earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Azure AI services revenue was generally in line with expectations, and this quarter, demand again exceeded supply across workloads, even as we brought more capacity online.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here was Google CFO Anat Ashkenazi on the company’s earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In GCP, we see strong demand for enterprise AI infrastructure, including TPUs and GPUs, enterprise AI solutions driven by demand for Gemini 2.5 and our other AI models, and core GCP infrastructure and other services such as cybersecurity and data analytics. As I’ve mentioned on previous earnings calls, while we have been working hard to increase capacity and have improved the pace of server deployments and data center construction, we still expect to remain in a tight demand-supply environment in Q4 and 2026.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here was Meta CEO Mark Zuckerberg on the company’s earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To date, we keep on seeing this pattern where we build some amount of infrastructure to what we think is an aggressive assumption. And then we keep on having more demand to be able to use more compute, especially in the core business in ways that we think would be quite profitable than we end up having compute for.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Earlier this month, TSMC CEO C.C. Wei admitted that the shortage was a lack of chips, not power; from the company’s earnings call:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Talking about to build a lot of AI data center all over the world, I use one of my customers’ customers’ answer. I asked the same question. They told me that they planned this one, 5-6 years ago already. So, as I said, those cloud service providers are smart, very smart. So, they say that they work on the power supply 5-6 years ago. So, today, their message to me is: silicon from TSMC is a bottleneck, and asked me not to pay attention to all others, because they have to solve the silicon bottleneck first. But indeed, we do get the power supply, all over the world, especially in the US. Not only that, but we also look at, who support those kind of a power supply, like a turbine, like, what, nuclear power plant, the plan or those kinds of things. We also look at the supply of the rack. We also look at the supply of the cooling system. Everything, so far, so good. So we have to work hard to narrow the gap between the demand and supply from TSMC.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The cause of that gap is obvious if you look at TSMC’s financials, specifically the company’s annual capital expenditures:&lt;/p&gt;
    &lt;p&gt;After a big increase in CapEx in 2021, driven by the COVID shortages and a belief in 5G, TSMC’s annual CapEx in the following years was basically flat — it actually declined on a year-over-year basis in both 2023 and 2024. Note those dates! ChatGPT was released in November 2022; that kicked off a massive increase in CapEx amongst the hyperscalers in particular, but it sure seems like TSMC didn’t buy the hype.&lt;/p&gt;
    &lt;p&gt;That lack of increased investment earlier this decade is why there is a shortage today, and is why TSMC has been a de facto brake on the AI buildout/bubble; I wrote last quarter:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To put it another way, if Altman and OpenAI are the ones pushing to accelerate the AI infrastructure buildout, it’s Wei and TSMC that are the brakes. The extent to which all of Altman’s deals actually materialize is dependent on how much TSMC invests in capacity now, and while they haven’t shown their hand yet, the company is saying all of the right things about AI being a huge trend without having yet committed to a commensurate level of investment, at least relative to OpenAI’s goals.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That Update was about the future, but it’s important to note that the TSMC brake has — if all of those CEO and CFO comments above are to be believed — already cost the biggest tech companies a lot of money. That’s the implication of not having enough supply to satisfy demand: there was revenue to be made that wasn’t, because TSMC didn’t buy the AI hype at the same time everyone else did.&lt;/p&gt;
    &lt;head rend="h3"&gt;TSMC’s CapEx Plans&lt;/head&gt;
    &lt;p&gt;TSMC is, finally, starting to invest more. Last year’s CapEx increased 37% to $41 billion, and there’s another increase in store for this year to $52–$56 billion; if we take the midpoint, that represents an increase of 32%, a bit less than last year:&lt;/p&gt;
    &lt;p&gt;Make no mistake, $54 billion is a big number, one that Wei admitted made him nervous:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You essentially try to ask whether the AI demand is real or not. I’m also very nervous about it. Yeah, you bet, because we have to invest about USD52 billion to USD56 billion for the CapEx, right? If we did not do it carefully, that will be a big disaster to TSMC for sure. So, of course, I spent a lot of time in the last three-four months talking to my customers and then customers’ customers. I want to make sure that my customers’ demands are real.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Wei made clear that he was worried about the market several years down the line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you build a new fab, it takes two and three year, two to three years to build a new fab. So even we start to spend $52 billion to $56 billion, the contribution to this year is almost none, and 2027, a little bit. So we actually, we are looking for 2028-2029 supply, and we hope it’s a time that the gap will be narrow…So 2026-2027 for the short-term, we are looking to improve our productivity. 2028 to 2029, yes, we start to increase our capacity significantly. And it will continue this way if the AI demand megatrend as we expected.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;First off, this delayed impact explains why TSMC’s lack of CapEx increase a few years ago is resulting in supply-demand imbalance today. Secondly, notice how this year’s planned increase — which again, won’t really have an impact until 2028 — pales in comparison to the CapEx growth of the hyperscalers (2025 numbers are estimates; note that Amazon’s CapEx includes Amazon.com):&lt;/p&gt;
    &lt;p&gt;Remember, a significant portion of this CapEx growth is for chips that are supported by TSMC’s stagnant CapEx growth from a few years ago. It’s notable, then, that TSMC’s current and projected CapEx growth is still less than the hyperscalers: how much less is it going to be than the hyperscalers’ growth in 2028, when the fabs being built today start actually producing chips?&lt;/p&gt;
    &lt;p&gt;In short, the TSMC brake isn’t going anywhere — if anything, it’s being pressed harder than ever.&lt;/p&gt;
    &lt;head rend="h3"&gt;TSMC Risk&lt;/head&gt;
    &lt;p&gt;TSMC is, to be clear, being extremely rational. CapEx is inherently risky: you are spending money now in anticipation of demand that may or may not materialize. Moreover, the risk for a foundry is higher than basically any other business model: nearly all of a foundry’s costs are CapEx, which means that if demand fails to materialize, costs — in the form of depreciation — don’t go down as they might with a business model with a higher percentage of marginal costs. This is exacerbated by the huge dollar figures entailed in building fabs: $52–$56 billion may drive revenues with big margins, but those big margins can easily flip to being huge losses and years of diminished pricing power thanks to excess capacity. Therefore, it’s understandable that TSMC is trying to manage its risks. Sure, the company may be foregoing some upside in 2028, but what is top of Wei’s mind is avoiding “a big disaster.”&lt;/p&gt;
    &lt;p&gt;What is important to note, however, is that the risk TSMC is managing doesn’t simply go away: rather, it’s being offloaded to the hyperscalers in particular. Specifically, if we get to 2028, and TSMC still isn’t producing enough chips to satisfy demand, then that means the hyperscalers will be forgoing billions of dollars in revenue — even more than they are already forgoing today. Yes, that risk is harder to see than the risk TSMC is avoiding, because the hyperscalers aren’t going to be bankrupt for a lack of chips to satisfy demand. Still, the potential money not made — particularly when the number is potentially in the hundreds of billions of dollars — is very much a risk that the hyperscalers are incurring because of TSMC’s conservatism.&lt;/p&gt;
    &lt;p&gt;What the hyperscalers need to understand is that simply begging TSMC to make more isn’t going to fix this problem, because begging TSMC to make more is to basically ask TSMC to take back the risk TSMC is offloading to the hyperscalers — they already declined! Rather, the only thing that will truly motivate TSMC to take on more risk is competition. If TSMC were worried about not just forgoing its own extra revenue, but actually losing business to a competitor, then the company would invest more. Moreover, that extra investment would be stacked on top of the investment made by said competitor, which means the world would suddenly have dramatically more fab capacity.&lt;/p&gt;
    &lt;head rend="h3"&gt;If You Want a Bubble&lt;/head&gt;
    &lt;p&gt;In short, the only way to truly get an AI bubble, with all of the potential benefits that entails, or, in the optimistic case, to actually meet demand in 2028 and beyond, is to have competition in the foundry space. That, by extension, means Samsung or Intel — or both — actually being viable options.&lt;/p&gt;
    &lt;p&gt;Remember, however, the number one challenge facing those foundries: a lack of demand from the exact companies whom TSMC has deputized to take on their risk. I wrote in U.S. Intel:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our mythical startup, however, doesn’t exist in a vacuum: it exists in the same world as TSMC, the company who has defined the modern pure play foundry. TSMC has put in the years, and they’ve put in the money; TSMC has the unparalleled customer service approach that created the entire fabless chip industry; and, critically, TSMC, just as they did in the mobile era, is aggressively investing to meet the AI moment. If you’re an Nvidia, or an Apple in smartphones, or an AMD or a Qualcomm, why would you take the chance of fabricating your chips anywhere else? Sure, TSMC is raising prices in the face of massive demand, but the overall cost of a chip in a system is still quite small; is it worth risking your entire business to save a few dollars for worse performance with a worse customer experience that costs you time to market and potentially catastrophic product failures?&lt;/p&gt;
      &lt;p&gt;We know our mythical startup would face these challenges because they are the exact challenges Intel faces. Intel may need “a meaningful external customer to drive acceptable returns on [its] deployed capital”, but Intel’s needs do not drive the decision-making of those external customers, despite the fact that Intel, while not fully caught up to TSMC, is at least in the ballpark, something no startup could hope to achieve for decades.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Becoming a meaningful customer of Samsung or Intel is very risky: it takes years to get a chip working on a new process, which hardly seems worth it if that process might not be as good, and if the company offering the process definitely isn’t as customer service-centric as TSMC. I understand why everyone sticks with TSMC.&lt;/p&gt;
    &lt;p&gt;The reality that hyperscalers and fabless chip companies need to wake up to, however, is that avoiding the risk of working with someone other than TSMC incurs new risks that are both harder to see and also much more substantial. Except again, we can see the harms already: foregone revenue today as demand outstrips supply. Today’s shortages, however, may prove to be peanuts: if AI has the potential these companies claim it does, future foregone revenue at the end of the decade is going to cost exponentially more — surely a lot more than whatever expense is necessary to make Samsung and/or Intel into viable competitors for TSMC.&lt;/p&gt;
    &lt;p&gt;This, incidentally, is how the geographic risk issue will be fixed, if it ever is. It’s hard to get companies to pay for insurance for geopolitical risks that may never materialize. What is much more likely is that TSMC’s customers realize that their biggest risk isn’t that TSMC gets blown up by China, but that TSMC’s monopoly and reasonable reluctance to risk a rate of investment that matches the rest of the industry means that the rest of the industry fails to fully capture the value of AI.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, there are chips made in Arizona, but only a portion, and they need to be sent back to Taiwan for packaging and testing. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46764223</guid><pubDate>Mon, 26 Jan 2026 11:07:26 +0000</pubDate></item><item><title>Water 'Bankruptcy' Era Has Begun for Billions, Scientists Say</title><link>https://www.bloomberg.com/news/articles/2026-01-20/water-bankruptcy-era-has-begun-for-billions-scientists-say</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46765092</guid><pubDate>Mon, 26 Jan 2026 12:57:43 +0000</pubDate></item><item><title>Vibe Coding Kills Open Source</title><link>https://arxiv.org/abs/2601.15494</link><description>&lt;doc fingerprint="a49e240a44ff128c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Economics &amp;gt; General Economics&lt;/head&gt;&lt;p&gt; [Submitted on 21 Jan 2026]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Vibe Coding Kills Open Source&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Generative AI is changing how software is produced and used. In vibe coding, an AI agent builds software by selecting and assembling open-source software (OSS), often without users directly reading documentation, reporting bugs, or otherwise engaging with maintainers. We study the equilibrium effects of vibe coding on the OSS ecosystem. We develop a model with endogenous entry and heterogeneous project quality in which OSS is a scalable input into producing more software. Users choose whether to use OSS directly or through vibe coding. Vibe coding raises productivity by lowering the cost of using and building on existing code, but it also weakens the user engagement through which many maintainers earn returns. When OSS is monetized only through direct user engagement, greater adoption of vibe coding lowers entry and sharing, reduces the availability and quality of OSS, and reduces welfare despite higher productivity. Sustaining OSS at its current scale under widespread vibe coding requires major changes in how maintainers are paid.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;econ.GN&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46765120</guid><pubDate>Mon, 26 Jan 2026 13:01:00 +0000</pubDate></item><item><title>Transfering Files with gRPC</title><link>https://kreya.app/blog/transfering-files-with-grpc/</link><description>&lt;doc fingerprint="a5bbb94665a38ca0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Transfering files with gRPC&lt;/head&gt;
    &lt;p&gt;Is transfering files with gRPC a good idea? Or should that be handled by a separate REST API endpoint? In this post, we will implement a file transfer service in both, use Kreya to test those APIs, and finally compare the performance to see which one is better.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges when doing file transfers&lt;/head&gt;
    &lt;p&gt;When handling large files, it is important to stream the file from one place to another. This might sound obvious, but many developers (accidentally) buffer the whole file in memory, potentially leading to out-of-memory errors. For a web server that provides files for download, a correct implementation would stream the files directly from the file system into the HTTP response.&lt;/p&gt;
    &lt;p&gt;Another problem with very large files are network failures. Imagine you are downloading a 10 GB file on a slow connection, but your connection is interrupted for a second after downloading 90% of it. With REST, this could be solved by sending a HTTP &lt;code&gt;Range&lt;/code&gt; header, requesting the rest of the file content without download the first 9 GB again.
For the simplicity of the blogpost and since something similar is possible with gRPC, we are going to ignore this problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transfering files with REST&lt;/head&gt;
    &lt;p&gt;Handling file transfers with REST (more correctly plain HTTP) is pretty straight forward in most languages and frameworks. In C# or rather ASP.NET Core, an example endpoint offering a file for downloading could look like this:&lt;/p&gt;
    &lt;code&gt;[HttpGet("api/files/pdf")]&lt;/code&gt;
    &lt;p&gt;We are effectively telling the framework to stream the file &lt;code&gt;/files/test-file.pdf&lt;/code&gt; as the response.
Internally, the framework repeatedly reads a small chunk (usually a few KB) from the file and writes it to the response.&lt;/p&gt;
    &lt;p&gt;The whole response body will consist of the file content and Kreya, our API client, automatically renders it as a PDF. Other information about the file, such as content type or file name, will have to be sent via HTTP headers.&lt;/p&gt;
    &lt;p&gt;This is important. If you have a JSON REST API and try to send additional information in the response body like this:&lt;/p&gt;
    &lt;code&gt;{&lt;/code&gt;
    &lt;p&gt;This is bad! The whole file content will be Base64-encoded and takes up 30% more space than the file size itself. In most languages/frameworks (without additional workarounds), this would also buffer the whole file in memory since the Base64-encoding process is usually not streamed while creating the JSON response. If the file itself is also buffered in memory, you could see memory usage over twice the size of the file. This may be fine if your files are only a few KB in size. But even then, if multiple requests are concurrently hitting this endpoint, you may notice quite a lot of memory usage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transfering files with gRPC&lt;/head&gt;
    &lt;p&gt;While the REST implementation was straight forward, this is not the case with gRPC. The design of gRPC is based on protobuf messages. There is no concept of "streaming" the content of a message. Instead, gRPC is designed to buffer a message fully in memory. This is the reason why individual gRPC messages should be kept small. The default maximum size is set at 4 MB. So how do we send large files bigger than that?&lt;/p&gt;
    &lt;p&gt;While gRPC cannot stream the content of a message, it allows streaming multiple messages. The solution is to break up the file into small chunks (usually around 32 KB) and then send these chunks until the file is transferred completely. The protobuf definition for a file download service could look like this:&lt;/p&gt;
    &lt;code&gt;edition = "2023";&lt;/code&gt;
    &lt;p&gt;This defines the &lt;code&gt;FileService.DownloadFile&lt;/code&gt; server-streaming method, which means that the method accepts a single (empty) request and returns multiple responses.
While we could send the file metadata via gRPC metadata (=HTTP headers or trailers), I think it's nicer to define it explicitly via a message.
The server should send the metadata first, as it contains important information, such as the file size.&lt;/p&gt;
    &lt;p&gt;A naive server implementation in C# could look like this:&lt;/p&gt;
    &lt;code&gt;private const int ChunkSize = 32 * 1024;&lt;/code&gt;
    &lt;p&gt;This works, but has some issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A new byte array buffer is created for each request&lt;/item&gt;
      &lt;item&gt;The buffer is copied each time to create a &lt;code&gt;ByteString&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first point is easily solved by using a buffer from the shared pool, potentially re-using the same buffer for subsequent requests.&lt;/p&gt;
    &lt;p&gt;The second point happens due to the gRPC implementation in practically all languages. Since the implementation wants to guarantee that the bytes are not modified while sending them, it performs a copy first. This is a design decision which favors stability over performance. Luckily, there is a workaround by using a "unsafe" method, which is perfectly safe in our scenario and improves performance:&lt;/p&gt;
    &lt;code&gt;private const int ChunkSize = 32 * 1024;&lt;/code&gt;
    &lt;p&gt;Let's try this out. After importing the protobuf definition, we call the gRPC method with Kreya:&lt;/p&gt;
    &lt;p&gt;This works, but where is our PDF? Since we are sending individual chunks, we need to put them back together manually.&lt;/p&gt;
    &lt;p&gt;To achieve this, we simply need to append each chunk to a file. In Kreya, this is done via Scripting:&lt;/p&gt;
    &lt;code&gt;import { writeFile, appendFile } from 'fs/promises';&lt;/code&gt;
    &lt;p&gt;This allows us to view the PDF:&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparison&lt;/head&gt;
    &lt;p&gt;Great! So transfering files with gRPC is definitely possible. But how do these two technologies compare against each other? Which one is faster and has less overhead?&lt;/p&gt;
    &lt;head rend="h3"&gt;Total bytes transferred&lt;/head&gt;
    &lt;p&gt;The total amount of bytes transferred on the wire is actually a pretty difficult topic. It depends on a lot of factors, such as the HTTP protocol (HTTP/1.1, HTTP/2 or HTTP/3), the package size of TCP/IP, whether TLS is being used etc. We are going to take a look how this applies to REST and gRPC.&lt;/p&gt;
    &lt;p&gt;Streaming files over a gRPC connection generates overhead, although not much. Since gRPC uses HTTP/2 under the hood, each individual chunk message has a few bytes overhead due to the HTTP/2 DATA frame information needed. Additionally, each chunk needs a few bytes to describe the content of the gRPC message. You are looking at roughly 15 bytes per message chunk if it fits into one HTTP/2 DATA frame. Transfering 4 GB of data with a chunk size of 16 KB would need around 250,000 messages to transfer the file completely, incurring an overhead of ~3.7 MB. This may or may not be negilible depending on the use case.&lt;/p&gt;
    &lt;p&gt;Up- or downloading huge files with REST over HTTP/1.1 has less overhead. Since the bytes of the file are sent as the response/request body, there is not much else that takes up space. In case of uploads to a server, HTTP multipart requests incur a small overhead cost to define the multipart boundary. Additionally, HTTP headers and everything else that is needed to send the request over the wire take up space, but this is the case for all HTTP-based protocols. Downloading files, whether small or large, have roughly the same amount of bytes overhead with HTTP/1.1. Depending on the count and size of HTTP headers, this is around a few hundred bytes.&lt;/p&gt;
    &lt;p&gt;Funnily enough, transfering files with REST over HTTP/2 incurs a bigger overhead. HTTP/2 splits the payload into individual DATA frames, very similar to our custom gRPC solution. Each frame, often with a maximum size of 16 KB, has an overhead of 9 bytes. For a file 4 GB in size, this amounts to ~2.2 MB overhead. While HTTP/2 has many performance advantages, transfering a single, large file over HTTP/1.1 has less overhead.&lt;/p&gt;
    &lt;p&gt;In these examples, we omitted the overhead created by TCP/IP, TLS and the lower network layers, which both HTTP/1.1 and HTTP/2 share. Comparing it with HTTP/3, which uses UDP, would make everything even more complicated, so we leave this as exercise for the reader :)&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance and memory usage&lt;/head&gt;
    &lt;p&gt;I spun up the local server plus client and took a look at the CPU and memory usage. Please note that this is not an accurate benchmark, which would require a more complex setup. Nevertheless, it provides some insight into the differences between the approaches. The example file used was 4 GB in size.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;gRPC (naive)&lt;/cell&gt;
        &lt;cell role="head"&gt;gRPC (optimized)&lt;/cell&gt;
        &lt;cell role="head"&gt;REST (HTTP/1.1)&lt;/cell&gt;
        &lt;cell role="head"&gt;REST (HTTP/2)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Duration&lt;/cell&gt;
        &lt;cell&gt;24s&lt;/cell&gt;
        &lt;cell&gt;22s&lt;/cell&gt;
        &lt;cell&gt;20s&lt;/cell&gt;
        &lt;cell&gt;28s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Max memory usage&lt;/cell&gt;
        &lt;cell&gt;36 MB&lt;/cell&gt;
        &lt;cell&gt;35 MB&lt;/cell&gt;
        &lt;cell&gt;32 MB&lt;/cell&gt;
        &lt;cell&gt;35 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total memory allocation&lt;/cell&gt;
        &lt;cell&gt;4465 MB&lt;/cell&gt;
        &lt;cell&gt;165 MB&lt;/cell&gt;
        &lt;cell&gt;38 MB&lt;/cell&gt;
        &lt;cell&gt;137 MB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If done right, memory usage is no problem for streaming very large files. And as expected, the naive gRPC implementation which copies a lot of &lt;code&gt;ByteString&lt;/code&gt;s around allocates a lot of memory!
It needs constant garbage collections to clean up the mess.
The maximum memory usage however stays low for all approaches.&lt;/p&gt;
    &lt;p&gt;What really surprised me was the bad performance of HTTP/2 in comparison to HTTP/1.1. It was even slower than gRPC, which builds on top of it! I cannot really explain this huge difference, especially since the code is exactly the same, both on the client and the server. I was running these tests on .NET 10 on Windows 11.&lt;/p&gt;
    &lt;p&gt;The optimized gRPC version performs pretty well, but is still slower than REST via HTTP/1.1. Since it has to do more work, it takes longer and uses more memory (and CPU). Optimizing the gRPC code was very important, as the naive implementation allocates so much memory!&lt;/p&gt;
    &lt;p&gt;I also tested HTTP/1.1 with TLS disabled, but it did not really make a difference.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;A REST endpoint for up- and downloading files is still the best option. If you are forced to use gRPC or simply too lazy to add REST endpoints in addition to your gRPC services, transfering files via gRPC is not too bad!&lt;/p&gt;
    &lt;p&gt;If you use some kind of S3 compatible storage backend, the best options is to generate a presigned URL. Then, download your files directly from the S3 storage instead of piping it through your backend.&lt;/p&gt;
    &lt;p&gt;There are lots of points to consider when implementing file transfer APIs. For example, if your users may have slow networks, it may be useful to compress the data before sending it over the wire.&lt;/p&gt;
    &lt;p&gt;Should you need resumable up- or downloads, instead of rolling your own, you could use https://tus.io/. This open-source protocol has implementations in various languages.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46765273</guid><pubDate>Mon, 26 Jan 2026 13:17:19 +0000</pubDate></item><item><title>After two years of vibecoding, I'm back to writing by hand</title><link>https://atmoio.substack.com/p/after-two-years-of-vibecoding-im</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46765460</guid><pubDate>Mon, 26 Jan 2026 13:36:22 +0000</pubDate></item><item><title>Porting 100k lines from TypeScript to Rust using Claude Code in a month</title><link>https://blog.vjeux.com/2026/analysis/porting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html</link><description>&lt;doc fingerprint="23280a7e3da5c6e8"&gt;
  &lt;main&gt;
    &lt;p&gt;I read this post “Our strategy is to combine AI and Algorithms to rewrite Microsoft’s largest codebases [from C++ to Rust]. Our North Star is ‘1 engineer, 1 month, 1 million lines of code.” and it got me curious, how difficult is it really?&lt;/p&gt;
    &lt;p&gt;I've long wanted to build a competitive Pokemon battle AI after watching a lot of WolfeyVGC and following the PokéAgent challenge at NeurIPS. Thankfully there's an open source project called "Pokemon Showdown" that implements all the rules but it's written in JavaScript which is quite slow to run in a training loop. So my holiday project came to life: let's convert it to Rust using Claude!&lt;/p&gt;
    &lt;head rend="h2"&gt;Escaping the sandbox&lt;/head&gt;
    &lt;p&gt;Having the AI able to run arbitrary code on your machine is dangerous, so there's a lot of safeguards put in place. But... at the same time, this is what I want to do in this case. So let me walk through the ways I escaped the various sandboxes.&lt;/p&gt;
    &lt;head rend="h3"&gt;git push&lt;/head&gt;
    &lt;p&gt;Claude runs in a sandbox that limits some operations like ssh access. You need ssh access in order to publish to GitHub. This is very important as I want to be able to check how the AI is doing from my phone while I do some other activities 😉&lt;/p&gt;
    &lt;p&gt;What I realized is that I can run the code on my terminal but Claude cannot do it from its own terminal. So what I did was to ask Claude to write a nodejs script that opens an http server on a local port that executes the git commands from the url. Now I just need to keep a tab open on my terminal with this server active and ask Claude to write instructions in Claude.md for it to interact with it.&lt;/p&gt;
    &lt;head rend="h3"&gt;rustc&lt;/head&gt;
    &lt;p&gt;There's an antivirus on my computer that requires a human interaction when an unknown binary is being ran. Since every time we compile it's a new unknown binary, this wasn't going to work.&lt;/p&gt;
    &lt;p&gt;What I found is that I can setup a local docker instance and compile + run the code inside of docker which doesn't trigger the antivirus. Again, I asked Claude to generate the right instructions in Claude.md and problem solved.&lt;/p&gt;
    &lt;p&gt;The next hurdle was to figure out how to let Claude Code for hours without any human intervention.&lt;/p&gt;
    &lt;head rend="h3"&gt;--yes&lt;/head&gt;
    &lt;p&gt;Claude keeps asking for permission to do things. I tried adding a bunch of things to the allowed commands file and &lt;code&gt;--allow-dangerously-skip-permissions --dangerously-skip-permissions&lt;/code&gt;was disabled in my environment (it has now been resolved).&lt;/p&gt;
    &lt;p&gt;I realized that I could run an AppleScript that presses enter every few seconds in another tab. This way it's going to say Yes to everything Claude asks to do. So far it hasn't decided to hack my computer...&lt;/p&gt;
    &lt;code&gt;#!/bin/bash

osascript -e \
'tell application "System Events"
    repeat
        delay 5
        key code 36
    end repeat
end tell'&lt;/code&gt;
    &lt;head rend="h3"&gt;Never give up&lt;/head&gt;
    &lt;p&gt;Claude after working for some time seem to always stop to recap things. I tried prompting it to never do, even threatening it to no avail.&lt;/p&gt;
    &lt;p&gt;I tried using the Ralph Wiggum loop but it couldn't get it to work and apparently I'm not alone.&lt;/p&gt;
    &lt;p&gt;What ended up working is to copy in my clipboard the task I wanted it to do and to tweak the script above to hit the keys "cmd-v" after pressing enter. This way in case it asks a question the "enter" is being used and in case it's not it's queuing the prompt for when Claude is giving back control.&lt;/p&gt;
    &lt;head rend="h3"&gt;Auto-updates&lt;/head&gt;
    &lt;p&gt;There are programs on the computer like software updater that can steal the focus from the terminal window, for example showing a modal. Once that happens, then the cmd-v / enter are no longer sent to the terminal and the execution stops.&lt;/p&gt;
    &lt;p&gt;I used my trusty Auto Clicker by MurGaa from Minecraft days to simulate a left click every few seconds. I place my terminal on the edge of the screen and same for my mouse so that when a modal appears in the middle, it refocuses the terminal correctly.&lt;/p&gt;
    &lt;p&gt;It also prevents the computer from going to sleep so that it can run even when I'm not using the laptop or at night.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bugs 🐜&lt;/head&gt;
    &lt;p&gt;Reliability when running things for a long period of time is paramount. Overall it's been a pretty smooth ride but I ran into this specific error during a handful of nights which stopped the process. I hope they get to the bottom of it and solve it as I'm not the only one to report it!&lt;/p&gt;
    &lt;p&gt;This setup is far from optimal but has worked so far. Hopefully this gets streamlined in the future!&lt;/p&gt;
    &lt;head rend="h2"&gt;Porting Pokemon&lt;/head&gt;
    &lt;head rend="h3"&gt;One Shot&lt;/head&gt;
    &lt;p&gt;At the very beginning, I started with a simple prompt asking Claude to port the codebase and make sure that things are done line by line. At first it felt extremely impressive, it generated thousands of lines of Rust that was compiling.&lt;/p&gt;
    &lt;p&gt;Sadly it was only an appearance as it took a lot of shortcuts. For example, it created two different structures for what a move is in two different files so that they would both compile independently but didn't work when integrated together. It ported all the functions very loosely where anything that was remotely complicated would not be ported but instead "simplified".&lt;/p&gt;
    &lt;p&gt;I didn't realize it yet, I got the loop working to have it port more and more code. The issue is that it created wrong abstractions all over the place and kept adding hardcoded code to make whatever it was supposed to fix work. This wasn't going to go anywhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;Giving it structure&lt;/head&gt;
    &lt;p&gt;At this point I knew that I needed to be a lot more prescriptive for what I wanted out of it. Taking a step back, the end result should have every JavaScript file and every method inside to have a Rust equivalent.&lt;/p&gt;
    &lt;p&gt;So I asked Claude to write a script that takes all the files and methods in the JavaScript codebase and put comments in the rust codebase with the JavaScript source, next to the Rust methods.&lt;/p&gt;
    &lt;p&gt;It was really important for it to be a script as even when instructed to copy code over, it would mistranslate JavaScript code. Being deterministic here greatly increased the odds of getting the right results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Litte Islands&lt;/head&gt;
    &lt;p&gt;The next challenge is that the original files were thousands of lines long, double it with source comments we got to files more than 10k lines long. This causes a ton of issues with the context window where Claude straight up refuses to open the file. So it started reading the file in chunks but without a ton of precision. Also the context grew a lot quicker and compaction became way more frequent.&lt;/p&gt;
    &lt;p&gt;So I went ahead and split every method into its own file for the Rust version. This dramatically improved the results. For maximal efficiency I would need to do the same for the JavaScript codebase as well but I was too afraid to do it and accidentally change the behavior so decided not to.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cleanup&lt;/head&gt;
    &lt;p&gt;The process of porting went through two repeating phases. I would give a large task to Claude to do in a loop that would churn on it for a day, and then I would need to spend time cleaning up the places where it went into the wrong direction.&lt;/p&gt;
    &lt;p&gt;For the cleanup, I still used Claude but gave a lot more specific recommendations. For example, I noticed that it would hardcode moves/abilities/items/... behaviors everywhere in the code when left unchecked, even after explicitly telling it not to. So I would manually look for all these and tell it to move them into the right places.&lt;/p&gt;
    &lt;p&gt;This is where engineering skills come into play, all my experience building software let me figure out what went wrong and how to fix it. The good part is that I didn't have to do the cleanup myself, Claude was able to do it just fine when directed to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Integration&lt;/head&gt;
    &lt;head rend="h3"&gt;Build everything before testing&lt;/head&gt;
    &lt;p&gt;So far, I just made sure that the code compiled, but have never actually put all the pieces together to ensure it actually worked. What Claude really wanted was to do a traditional software building strategy where you make "simple" implementations of all of the pieces and then build them up as time goes.&lt;/p&gt;
    &lt;p&gt;But in our case, all this iteration has already happened for 10 years on the pokemon-showdown codebase. It's counter productive to try and re-learn all these lessons and will unlikely converge the same way. What works better is to port everything at once, and then do the integration at the end once.&lt;/p&gt;
    &lt;p&gt;I've learned this strategy from working on Skip, a compiler. For years all the building blocks were built independently and then it all came together with nothing to show for but within a month at the end it all worked. I was so shocked.&lt;/p&gt;
    &lt;head rend="h3"&gt;End-to-end test&lt;/head&gt;
    &lt;p&gt;Once most of the codebase was ported one to one, I started putting it all together. The good thing is that we can run and edit the code in JavaScript and in Rust, and the input/output is very simple and standardized: list of pokemons with their options (moves, items, nature, iv/ev spread...) and then the list of actions at each step (moves and switches). Given the same random sequence, it'll advance the state the same way.&lt;/p&gt;
    &lt;p&gt;Now I can let Claude generate this testing harness and go through all the issues one by one. Impressively, it was able to figure out all issues and fix them.&lt;/p&gt;
    &lt;p&gt;Over the course of 3 weeks it averaged fixing one issue every 20 minutes or so. It fixed hundreds of issues on its own. I never intervened, it was only a matter of time before it fixed every issue that it encountered.&lt;/p&gt;
    &lt;head rend="h3"&gt;Giving it structure&lt;/head&gt;
    &lt;p&gt;At the beginning, this process was extremely slow. Every time a compaction happened, Claude became "dumb" again and reinvented the wheel, writing down tons of markdown files and test scripts along the way. Or Claude decided to take the easy way out and just generate tons of tests but never actually making them match with JavaScript.&lt;/p&gt;
    &lt;p&gt;So, I started looking at what it did well and encoding it. For example, it added a lot of debugging around the PRNG steps and what actions happened at every turn with all the debugging metadata. So I asked it to create a single test script to print down this information for a single step and to print stack traces. Then add instruction to the Claude.md file. This way every investigation started right away.&lt;/p&gt;
    &lt;head rend="h3"&gt;The long slog&lt;/head&gt;
    &lt;p&gt;I built used the existing random number generator to generate battles and could put in a number as a seed. This let me generate consistent battles at an increasing size.&lt;/p&gt;
    &lt;p&gt;I started fixing the first 100 battles, then 1000, 10k, 100k and I'm almost done solving all the issues for the first 2.4 million battles! I'm not sure how many more issues there are but the good thing is that they are getting smaller and smaller as the batch size increases.&lt;/p&gt;
    &lt;head rend="h3"&gt;Types of issues&lt;/head&gt;
    &lt;p&gt;There are two broad classes of issues that were fixed. The first one that I expected is that Rust has different constraints than JavaScript which need to be taken into account and lead to bugs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust has the "borrow checker" where a mutable variable cannot be passed in two different contexts at once. The problem is that "Pokemon" and "Battle" have references to each others. So there's a lot of workarounds like doing copies, passing indices instead of the object, providing functions with mutable object as callback...&lt;/item&gt;
      &lt;item&gt;The JavaScript codebase uses dynamism heavily where some function return '', undefined, null, 0, 1, 5.2, Pokemon... which all are handled with different behaviors. At first the rust port started using Option&amp;lt;&amp;gt; to handle many of them but then moved to structs with all these variants.&lt;/item&gt;
      &lt;item&gt;Rust doesn't support optional arguments so every argument has to be spelled out literally.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the second one are due to itself... Claude Code is like a smart student that is trying to find every opportunity to avoid doing the hard work and take the easy way out if it thinks it can get away with it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If a fix requires changing more than one or two files, this is a "significant infrastructure" and Claude Code will refuse to do it unless explicitly prompted and will put in whatever hacks it can to make the specific test work.&lt;/item&gt;
      &lt;item&gt;Along the same lines, it is going to implement "simplified" versions of things. For some methods, it was better to delete everything and asking it to port it over from scratch than trying to fix all the existing code it created.&lt;/item&gt;
      &lt;item&gt;The JavaScript comments are supposed to be the source of truth. But Claude is not above changing the original code if it feels like this is the way to solve the problem...&lt;/item&gt;
      &lt;item&gt;If given a list of tasks, it's going to avoid doing the ones that seem difficult until it is absolutely forced to. This is inefficient if not careful as it's going to keep spending time investigating and then skipping all the "hard" ones. Compaction is basically wiping all its memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Prompts&lt;/head&gt;
    &lt;p&gt;I didn't write a single line of code myself in this project. I alternated between "co-op" where I work with Claude interactively during the day and creating a job for it to run overnight. I'll focus on the night ones for this section.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conversion&lt;/head&gt;
    &lt;p&gt;For the first phase of the project, I mostly used variations of this one. Asking it to go through all the big files one by one and implement them faithfully (it didn't really follow instructions as we've seen later...)&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Open BATTLE_TODO.md to get the list of all the methods in both battle*.rs.&lt;/p&gt;&lt;lb/&gt;Inspect every single one of them and make sure that they are a direct translation the JavaScript file. If there's a method with the same name, the JavaScript definition will be in the comment.&lt;lb/&gt;If there's no JavaScript definition, question whether this method should be there in the rust version. Our goal is to follow as closely as possible the JavaScript version to avoid any bugs in translation. If you notice that the implementation doesn't match, do all the refactoring needed to match 1 to 1.&lt;lb/&gt;This will be a complex project. You need to go through all the methods one by one, IN ORDER. YOU CANNOT skip a method because it is too hard or would requiring building new infrastructure. We will call this in a loop so spend as much time as you need building the proper infrastructure to make it 1 to 1 with the JavaScript equivalent. Do not give up.&lt;lb/&gt;Update BATTLE_TODO.md and do a git commit after each unit of work.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Todos&lt;/head&gt;
    &lt;p&gt;Claude Code while porting the methods one by one often decided to write a "simplified" version or add a "TODO" for later. I also found it to be useful when generating work to add the instructions in the codebase itself via a TODO comment, so I don't need to wish that it's going to be read from the context.&lt;/p&gt;
    &lt;p&gt;The master md file in practice didn't really work, it quickly became too big to be useful and Claude started creating a bunch more littering the repo with them. Instead I gave it a deterministic way to go through then by calling grep on the codebase, so it knew when to find them.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;We want to fix every TODO in the codebase.&lt;/p&gt;&lt;code&gt;TODO&lt;/code&gt;or&lt;code&gt;simplif&lt;/code&gt;in pokemon-showdown-rs/.&lt;lb/&gt;There are hundreds of them, so go diligently one by one. Do not skip them even if they are difficult. I will call this prompt again and again so you don't need to worry about taking too long on any single one.&lt;lb/&gt;The port must be exactly one to one. If the infrastructure doesn't exist, please implement it. Do not invent anything.&lt;lb/&gt;Make sure it still compiles after each addition and commit and push to git.&lt;/quote&gt;
    &lt;p&gt;At some point the context was poisoned where a TODO was inside of the original js codebase so it changed it to something else which made sense. But then it did the same for all the subsequent TODOs which didn't... Thankfully I could just revert all these commits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fixing&lt;/head&gt;
    &lt;p&gt;I put in all the instructions to debug in Claude.md and a script to run all the tests which outputs a txt file with progress report. This way Claude was able to just keep going fixing issues after issues.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We want to fix all the divergences in battles. Please look at 500-seeds-results.txt and fix them one by one. The only way you can fix is by making sure that the differences between javascript and rust are explained by language differences and not logic. Every line between the two must match one by one. If you fixed something specific, it's probably a larger issue, spend the time to figure out if other similar things are broken and do the work to do the larger infrastructure fixes. Make sure it still compiles after each addition and commit and push to git. Check if there are other parts of the codebase that make this mistake.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is really useful to have this txt file diff committed to GitHub to get a sense of progress on the go!&lt;/p&gt;
    &lt;head rend="h2"&gt;Epilogue&lt;/head&gt;
    &lt;head rend="h3"&gt;It works 🤯&lt;/head&gt;
    &lt;p&gt;I didn't quite know what to expect coming into this project. They usually tend to die due to the sheer amount of work needed to get anywhere close to something complete. But not this time!&lt;/p&gt;
    &lt;p&gt;We have a complete implementation of Pokemon battle system that produces the same results as the existing JavaScript codebase*. This was done through 5000 commits in 4 weeks and the Rust codebase is around 100k lines of code.&lt;/p&gt;
    &lt;p&gt;*I wish we had 0 divergences but right now there are 80 out of the first 2.4 million seeds or 0.003%. I need to run it for longer to solve these.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is it fast?&lt;/head&gt;
    &lt;p&gt;The whole point of the project was for it to be faster than the initial JavaScript implementation. Only towards the end of the project where we had a sizable amount of battles running perfectly I felt like it would be a fair time to do a performance comparison.&lt;/p&gt;
    &lt;p&gt;I asked Claude Code to parallelize both implementations and was relieved by the results, the Rust port is actually significantly faster, I didn't spend all this time for nothing!&lt;/p&gt;
    &lt;p&gt;I've tried asking Claude to optimize it further, it created a plan that looks reasonable (I've never interacted with Rust in my life) and it spent a day building many of these optimizations but at the end of the day, none of them actually improved the runtime and some even made it way worse.&lt;/p&gt;
    &lt;p&gt;This is a good example of how experience and expertise is still very required in order to get the best out of LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This is pretty wild that I was able to port a ~100k lines codebase from JavaScript to Rust in two weeks on my own with Claude Code running 24 hours a day for a month creating 5k commits! I have never written any line of Rust before in my life.&lt;/p&gt;
    &lt;p&gt;LLM-based coding agents are such a great new tool for engineers, there's no way I would have been able to do that without Claude Code. That said, it still feels like a tool that requires my engineering expertise and constant babysitting to produce these results.&lt;/p&gt;
    &lt;p&gt;Sadly I didn't get to build the Pokemon Battle AI and the winter break is over, so if anybody wants to do it, please have fun with the codebase!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46765694</guid><pubDate>Mon, 26 Jan 2026 13:58:27 +0000</pubDate></item></channel></rss>