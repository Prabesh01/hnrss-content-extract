<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 25 Oct 2025 20:36:04 +0000</lastBuildDate><item><title>The Swift SDK for Android</title><link>https://www.swift.org/blog/nightly-swift-sdk-for-android/</link><description>&lt;doc fingerprint="360e51139294ee0b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing the Swift SDK for Android&lt;/head&gt;
    &lt;p&gt;Swift has matured significantly over the past decade — extending from cloud services to Windows applications, browser apps, and microcontrollers. Swift powers apps and services of all kinds, and thanks to its great interoperability, you can share code across platforms.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is an open group, free for anyone to join, that aims to expand Swift to Android. Today, we are pleased to announce nightly preview releases of the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;This milestone reflects months of effort by the Android workgroup, building on many years of grassroots community effort. With the SDK, developers can begin developing Android applications in Swift, opening new avenues for cross-platform development and accelerating innovation across the mobile ecosystem.&lt;/p&gt;
    &lt;p&gt;The Swift SDK for Android is available today, bundled with the Windows installer or downloadable separately for use on Linux or macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;p&gt;We’ve published a Getting Started guide to help you set up your first native Swift code on an Android device. The Swift for Android Examples help demonstrate end‑to‑end application workflows on Android.&lt;/p&gt;
    &lt;p&gt;With the Swift SDK for Android, you can now start porting your Swift packages to Android. Over 25% of packages in the Swift Package Index already build for Android, and the Community Showcase now indicates Android compatibility.&lt;/p&gt;
    &lt;p&gt;The swift-java project enables you to interoperate between Java and Swift. It is both a library and a code generator, enabling you to integrate Swift and Java in both directions by automatically generating safe and performant bindings. To learn about generating bindings to bring your business logic to Android, check out the recent Swift Server Side meetup talk by Mads Odgaard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;This preview release opens many new opportunities to continue improving these tools. We encourage you to share your experiences, ideas, tools and apps on the Swift forums. This post has been published on an associated thread for discussion, and new posts can be shared in the Android category.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is drafting a vision document, currently under review, for directing future work regarding Swift on Android. This vision will outline priority areas and guide community efforts to maximize impact across the ecosystem. In addition, we maintain a project board that tracks the status of major efforts, as well as official CI for the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;If you’re as excited as we are, join us and help make this ecosystem even better!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45698570</guid><pubDate>Fri, 24 Oct 2025 20:06:52 +0000</pubDate></item><item><title>Ask HN: Not treated respectfully by colleague – advice?</title><link>https://news.ycombinator.com/item?id=45700879</link><description>&lt;doc fingerprint="a7509614588456fd"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;I work with a competent jerk: &lt;/p&gt;https://www.pickardlaws.com/myleadership/myfiles/rtdocs/hbr/...&lt;p&gt;Backstory -- I joined my current team a year ago. It was falling apart. The team members hated each other and were trying to get each other fired. The team lead who’d joined a quarter before had quit to join another team largely due to conflict with one difficult coworker.&lt;/p&gt;&lt;p&gt;Then I joined as the lead. I helped to stabilize the team over the last year. It’s grown from four to ten engineers. Three engineers joined specifically to work with me.&lt;/p&gt;&lt;p&gt;Yet the entire time I’ve been on that team, that one difficult coworker has been criticizing and fighting almost everything I’ve done. That coworker was relatively inexperienced, yet was told by a previous director that he was meant to be the lead of this platform. Hence the fighting with the other lead from a year ago. And with me over the past year. It’s burning me out bad.&lt;/p&gt;&lt;p&gt;It mostly comes across in passive-aggressive comments, and in trying to argue and prove he is right about trivial things, with every bit of disagreement. It used to come up in terms of aggression towards his peers. That stopped when me and my manager intervened. Yet continues with me. It's clear he doesn't respect me as lead, and makes that clear in team meetings.&lt;/p&gt;&lt;p&gt;It makes everything harder. Even in an incident that caused a global outage for three hours, and where we didn't have alerts and had to get told by our users we were down, I get pushback on calling for a post-mortem since his work was involved. Now I have to back-channel to my manager (who wasn't in the room), and still face the . Just the friction alone that he adds in getting anything done makes doing the right thing often not worth it.&lt;/p&gt;&lt;p&gt;I'm at a loss. My other teammates love working with me. I was promoted last year. I'm two levels above this guy which means my company trusts me. I'm frankly wishing I could leave the team but it's difficult to transfer since I'm in something of a specialty and there aren't other positions at my level in the company.&lt;/p&gt;&lt;p&gt;My manager's been resistant to doing much of anything. I think he's tired of me bringing it up. He says that the engineer "gets along with [junior engineer who never disagrees with him]". He says the difficult engineer is improving and sees him trying. His feedback to me is not to let it bother me so much. He asks me what he should do to change his behavior (he's the manager, not me...).&lt;/p&gt;&lt;p&gt;I really just want to be able to come in to work and do my job without dealing with an asshole trying to one-up me or "score points" against me all day and without expecting conflict every time we're in the same meeting. I'm tired of the status and perception games and his overall impact on the team vibe and culture.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45700879</guid><pubDate>Sat, 25 Oct 2025 02:14:39 +0000</pubDate></item><item><title>Key IOCs for Pegasus and Predator Spyware Removed with iOS 26 Update</title><link>https://iverify.io/blog/key-iocs-for-pegasus-and-predator-spyware-cleaned-with-ios-26-update</link><description>&lt;doc fingerprint="f60d4907cab1114d"&gt;
  &lt;main&gt;
    &lt;p&gt;Blog&lt;/p&gt;
    &lt;head rend="h1"&gt;Key IOCs for Pegasus and Predator Spyware Cleaned With iOS 26 Update&lt;/head&gt;
    &lt;p&gt;By Matthias Frielingsdorf, VP of Research&lt;/p&gt;
    &lt;p&gt;Oct 21, 2025&lt;/p&gt;
    &lt;p&gt;As iOS 26 is being rolled out, our team noticed a particular change in how the operating system handles the shutdown.log file: it effectively erases crucial evidence of Pegasus and Predator spyware infections. This development poses a serious challenge for forensic investigators and individuals seeking to determine if their devices have been compromised at a time when spyware attacks are becoming more common.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;The Power of the shutdown.log&lt;/head&gt;
    &lt;p&gt;For years, the shutdown.log file has been an invaluable, yet often overlooked, artifact in the detection of iOS malware. Located within the Sysdiagnoses in the Unified Logs section (specifically, Sysdiagnose Folder -&amp;gt; system_logs.logarchive -&amp;gt; Extra -&amp;gt; shutdown.log), it has served as a silent witness to the activities occurring on an iOS device, even during its shutdown sequence.&lt;/p&gt;
    &lt;p&gt;In 2021, the publicly known version of Pegasus spyware was found to leave discernible traces within this shutdown.log. These traces provided a critical indicator of compromise, allowing security researchers to identify infected devices. However, the developers behind Pegasus, NSO Group, are constantly refining their techniques, and by 2022 Pegasus had evolved.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Pegasus's Evolving Evasion Tactics&lt;/head&gt;
    &lt;p&gt;While still leaving evidence in the shutdown.log, their methods became more sophisticated. Instead of leaving obvious entries, they began to completely wipe the shutdown.log file. Yet, even with this attempted erasure, their own processes still left behind subtle traces. This meant that even a seemingly clean shutdown.log that began with evidence of a Pegasus sample was, in itself, an indicator of compromise. Multiple cases of this behavior were observed until the end of 2022, highlighting the continuous adaptation of these malicious actors.&lt;/p&gt;
    &lt;p&gt;Following this period, it is believed that Pegasus developers implemented even more robust wiping mechanisms, likely monitoring device shutdown to ensure a thorough eradication of their presence from the shutdown.log. Researchers have noted instances where devices known to be active had their shutdown.log cleared, alongside other IOCs for Pegasus infections. This led to the conclusion that a cleared shutdown.log could serve as a good heuristic for identifying suspicious devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Predator's Similar Footprint&lt;/head&gt;
    &lt;p&gt;The sophisticated Predator spyware, observed in 2023, also appears to have learned from the past. Given that Predator was actively monitoring the shutdown.log, and considering the similar behavior seen in earlier Pegasus samples, it is highly probable that Predator, too, left traces within this critical log file.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;iOS 26: An Unintended Cleanse&lt;/head&gt;
    &lt;p&gt;With iOS 26 Apple introduced a changeâeither an intentional design decision or an unforeseen bugâthat causes the shutdown.log to be overwritten on every device reboot instead of appended with a new entry every time, preserving each as its own snapshot. This means that any user who updates to iOS 26 and subsequently restarts their device will inadvertently erase all evidence of older Pegasus and Predator detections that might have been present in their shutdown.log.&lt;/p&gt;
    &lt;p&gt;This automatic overwriting, while potentially intended for system hygiene or performance, effectively sanitizes the very forensic artifact that has been instrumental in identifying these sophisticated threats. It could hardly come at a worse time - spyware attacks have been a constant in the news and recent headlines show that high-power executives and celebrities, not just civil society, are being targeted.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Identifying Pegasus 2022: A Specific IOC&lt;/head&gt;
    &lt;p&gt;For those still on iOS versions prior to 26, a specific IOC for Pegasus 2022 infections involved the presence of a /private/var/db/com.apple.xpc.roleaccountd.staging/com.apple.WebKit.Networking entry within the shutdown.log. This particular IOC also revealed a significant shift in NSO Group's tactics: they began using normal system process names instead of easily identifiable, similarly named processes, making detection more challenging.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Correlating Logs for Deeper Insight (&amp;lt; iOS 18)&lt;/head&gt;
    &lt;p&gt;For devices running iOS 18 or earlier, a more comprehensive approach to detection involved correlating containermanagerd log entries with shutdown.log events. Containermanagerd logs contain boot events and can retain data for several weeks. By comparing these boot events with shutdown.log entries, investigators could identify discrepancies. For example, if numerous boot events were observed before shutdown.log entries, it suggested that something was amiss and potentially being hidden.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Before You Update&lt;/head&gt;
    &lt;p&gt;Given the implications of iOS 26's shutdown.log handling, it is crucial for users to take proactive steps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Before updating to iOS 26, immediately take and save a sysdiagnose of your device. This will preserve your current shutdown.log and any potential evidence it may contain.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Consider holding off on updating to iOS 26 until Apple addresses this issue, ideally by releasing a bug fix that prevents the overwriting of the shutdown.log on boot.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;More Blogs&lt;/head&gt;
    &lt;head rend="h3"&gt;Get Our Latest Blog Posts Delivered Straight to Your Inbox&lt;/head&gt;
    &lt;p&gt;Subscribe to our blog to receive the latest research and industry trends delivered straight to your inbox. Our blog content covers sophisticated mobile threats, unpatched vulnerabilities, smishing, and the latest industry news to keep you informed and secure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45700946</guid><pubDate>Sat, 25 Oct 2025 02:31:55 +0000</pubDate></item><item><title>Mistakes I see engineers making in their code reviews</title><link>https://www.seangoedecke.com/good-code-reviews/</link><description>&lt;doc fingerprint="efcc91752d045dab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mistakes I see engineers making in their code reviews&lt;/head&gt;
    &lt;p&gt;In the last two years, code review has gotten much more important. Code is now easy to generate using LLMs, but it’s still just as hard to review1. Many software engineers now spend as much (or more) time reviewing the output of their own AI tools than their colleagues’ code.&lt;/p&gt;
    &lt;p&gt;I think a lot of engineers don’t do code review correctly. Of course, there are lots of different ways to do code review, so this is largely a statement of my engineering taste.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don’t just review the diff&lt;/head&gt;
    &lt;p&gt;The biggest mistake I see is doing a review that focuses solely on the diff2. Most of the highest-impact code review comments have very little to do with the diff at all, but instead come from your understanding of the rest of the system.&lt;/p&gt;
    &lt;p&gt;For instance, one of the most straightforwardly useful comments is “you don’t have to add this method here, since it already exists in this other place”. The diff itself won’t help you produce a comment like this. You have to already be familiar with other parts of the codebase that the diff author doesn’t know about.&lt;/p&gt;
    &lt;p&gt;Likewise, comments like “this code should probably live in this other file” are very helpful for maintaining the long-term quality of a codebase. The cardinal value when working in large codebases is consistency (I write about this more in Mistakes engineers make in large established codebases). Of course, you cannot judge consistency from the diff alone.&lt;/p&gt;
    &lt;p&gt;Reviewing the diff by itself is much easier than considering how it fits into the codebase as a whole. You can rapidly skim a diff and leave line comments (like “rename this variable” or “this function should flow differently”). Those comments might even be useful! But you’ll miss out on a lot of value by only leaving this kind of review.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don’t leave too many comments&lt;/head&gt;
    &lt;p&gt;Probably my most controversial belief about code review is that a good code review shouldn’t contain more than five or six comments. Most engineers leave too many comments. When you receive a review with a hundred comments, it’s very hard to engage with that review on anything other than a trivial level. Any really important comments get lost in the noise2.5.&lt;/p&gt;
    &lt;p&gt;What do you do when there are twenty places in the diff that you’d like to see updated - for instance, twenty instances of &lt;code&gt;camelCase&lt;/code&gt; variables instead of &lt;code&gt;snake_case&lt;/code&gt;? Instead of leaving twenty comments, I’d suggest leaving a single comment explaining the stylistic change you’d like to make, and asking the engineer you’re reviewing to make the correct line-level changes themselves.&lt;/p&gt;
    &lt;p&gt;There’s at least one exception to this rule. When you’re onboarding a new engineer to the team, it can be helpful to leave a flurry of stylistic comments to help them understand the specific dialect that your team uses in this codebase. But even in this case, you should bear in mind that any “real” comments you leave are likely to be buried by these other comments. You may still be better off leaving a general “we don’t do early returns in this codebase” comment than leaving a line comment on every single early return in the diff.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don’t review with a “how would I write it?” filter&lt;/head&gt;
    &lt;p&gt;One reason engineers leave too many comments is that they review code like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Look at a hunk of the diff&lt;/item&gt;
      &lt;item&gt;Ask themselves “how would I write this, if I were writing this code?”&lt;/item&gt;
      &lt;item&gt;Leave a comment with each difference between how they would write it and the actual diff&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a good way to end up with hundreds of comments on a pull request: an endless stream of “I would have done these two operations in a different order”, or “I would have factored this function slightly differently”, and so on.&lt;/p&gt;
    &lt;p&gt;I’m not saying that these minor comments are always bad. Sometimes the order of operations really does matter, or functions really are factored badly. But one of my strongest opinions about software engineering is that there are multiple acceptable approaches to any software problem, and that which one you choose often comes down to taste.&lt;/p&gt;
    &lt;p&gt;As a reviewer, when you come across cases where you would have done it differently, you must be able to approve those cases without comment, so long as either way is acceptable. Otherwise you’re putting your colleagues in an awkward position. They can either accept all your comments to avoid conflict, adding needless time and setting you up as the de facto gatekeeper for all changes to the codebase, or they can push back and argue on each trivial point, which will take even more time. Code review is not the time for you to impose your personal taste on a colleague.&lt;/p&gt;
    &lt;head rend="h3"&gt;If you do not want a change to be merged, leave a blocking review&lt;/head&gt;
    &lt;p&gt;So far I’ve only talked about review comments. But the “high-order bit” of a code review is not the content of the comments, but the status of the review: whether it’s an approval, just a set of comments, or a blocking review. The status of the review colors all the comments in the review. Comments in an approval read like “this is great, just some tweaks if you want”. Comments in a blocking review read like “here’s why I don’t want you to merge this in”.&lt;/p&gt;
    &lt;p&gt;If you want to block, leave a blocking review. Many engineers seem to think it’s rude to leave a blocking review even if they see big problems, so they instead just leave comments describing the problems. Don’t do this. It creates a culture where nobody is sure whether it’s okay to merge their change or not. An approval should mean “I’m happy for you to merge, even if you ignore my comments”. Just leaving comments should mean “I’m happy for you to merge if someone else approves, even if you ignore my comments.” If you would be upset if a change were merged, you should leave a blocking review on it. That way the person writing the change knows for sure whether they can merge or not, and they don’t have to go and chase up everyone who’s left a comment to get their informal approval.&lt;/p&gt;
    &lt;head rend="h3"&gt;Most reviews should be approvals&lt;/head&gt;
    &lt;p&gt;I should start with a caveat: this depends a lot on what kind of codebase we’re talking about. For instance, I think it’s fine if PRs against something like SQLite get mostly blocking reviews. But a standard SaaS codebase, where teams are actively developing new features, ought to have mostly approvals. I go into a lot more detail about the distinction between these two types of codebase in Pure and Impure Engineering.&lt;/p&gt;
    &lt;p&gt;If tons of PRs are being blocked, it’s usually a sign that there’s too much gatekeeping going on. One dynamic I’ve seen play out a lot is where one team owns a bottleneck for many other teams’ features - for instance, maybe they own the edge network configuration where new public-facing routes must be defined, or the database structure that new features will need to modify. That team is typically more reliability-focused than a typical feature team. Engineers on that team may have a different title, like SRE, or even belong to a different organization. Their incentives are thus misaligned with the feature teams they’re nominally supporting.&lt;/p&gt;
    &lt;p&gt;Suppose the feature team wants to update the public-facing ingress routes in order to ship some important project. But the edge networking team doesn’t care about that project - it doesn’t affect their or their boss’s review cycles. What does affect their reviews is any production problem the change might cause. That means they’re motivated to block any potentially-risky change for as long as possible. This can be very frustrating for the feature team, who is willing to accept some amount of risk for the sake of delivering new features3.&lt;/p&gt;
    &lt;p&gt;Of course, there are other reasons why many PRs might be getting blocking reviews. Maybe the company just hired a bunch of incompetent engineers, who ought to be prevented from merging their changes. Maybe the company has had a recent high-profile incident, and all risky changes should be blocked for a couple of weeks until their users forget about it. But in normal circumstances, a high rate of blocked reviews represents a structural problem.&lt;/p&gt;
    &lt;p&gt;For many engineers - including me - it feels good to leave a blocking review, for the same reasons that it feels good to gatekeep in general. It feels like you’re single-handedly protecting the quality of the codebase, or averting some production incident. It’s also a way to indulge a common vice among engineers: flexing your own technical knowledge on some less-competent engineer. Oh, looks like you didn’t know that your code would have caused an N+1 query! Well, I knew about it. Aren’t you lucky I took the time to read through your code?&lt;/p&gt;
    &lt;p&gt;This principle - that you should bias towards approving changes - is important enough that Google’s own guide to code review begins with it, calling it ”the senior principle among all of the code review guidelines”4.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;I’m quite confident that many competent engineers will disagree with most or all of the points in this post. That’s fine! I also believe many obviously true things about code review, but I didn’t include them here.&lt;/p&gt;
    &lt;p&gt;In my experience, it’s a good idea to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consider what code isn’t being written in the PR instead of just reviewing the diff&lt;/item&gt;
      &lt;item&gt;Leave a small number of well-thought-out comments, instead of dashing off line comments as you go and ending up with a hundred of them&lt;/item&gt;
      &lt;item&gt;Review with a “will this work” filter, not with a “is this exactly how I would have done it” filter&lt;/item&gt;
      &lt;item&gt;If you don’t want the change to be merged, leave a blocking review&lt;/item&gt;
      &lt;item&gt;Unless there are very serious problems, approve the change&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all more or less applies to reviewing code from agentic LLM systems. They are particularly prone to missing code that they ought to be writing, they also get a bit lost if you feed them a hundred comments at once, and they have their own style. The one point that does not apply to LLMs is the “bias towards approving” point. You can and should gatekeep AI-generated PRs as much as you want.&lt;/p&gt;
    &lt;p&gt;I do want to close by saying that there are many different ways to do code review. Here’s a non-exhaustive set of values that a code review practice might be trying to satisfy: making sure multiple people on the team are familiar with every part of the codebase, letting the team discuss the software design of each change, catching subtle bugs that a single person might not see, transmitting knowledge horizontally across the team, increasing perceived ownership of each change, enforcing code style and format rules across the codebase, and satisfying SOC2 “no one person can change the system alone” constraints. I’ve listed these in the order I care about them, but engineers who would order these differently will have a very different approach to code review.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Of course there are LLM-based reviewing tools. They’re even pretty useful! But at least right now they’re not as good as human reviewers, because they can’t bring to bear the amount of general context that a competent human engineer can.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For readers who aren’t software engineers, “diff” here means the difference between the existing code and the proposed new code, showing what lines are deleted, added, or edited.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is a special instance of a general truth about communication: if you tell someone one thing, they’ll likely remember it; if you tell them twenty things, they will probably forget it all.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In the end, these impasses are typically resolved by the feature team complaining to their director or VP, who complains to the edge networking team’s director or VP, who tells them to just unblock the damn change already. But this is a pretty crude way to resolve the incentive mismatch, and it only really works for features that are high-profile enough to receive air cover from a very senior manager.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Google’s principle is much more explicit, stating that you should approve a change if it’s even a minor improvement, not when it’s perfect. But I take the underlying message here to be “I know it feels good, but don’t be a nitpicky gatekeeper - approve the damn PR!”&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;October 25, 2025 │ Tags: good engineers, software design, explainers, ai&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45701404</guid><pubDate>Sat, 25 Oct 2025 04:42:09 +0000</pubDate></item><item><title>Why your social.org files can have millions of lines without performance issues</title><link>https://en.andros.dev/blog/4e12225f/why-your-socialorg-files-can-have-millions-of-lines-without-any-performance-issues/</link><description>&lt;doc fingerprint="8faca1727e6aa4c7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Your social.org Files Can Have Millions of Lines Without Any Performance Issues&lt;/head&gt;
    &lt;p&gt;As Org Social grows, users follow more feeds, and individual &lt;code&gt;social.org&lt;/code&gt; files accumulate hundreds of posts over time. Traditional approaches that download entire feeds sequentially create two major bottlenecks:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bandwidth waste: Downloading complete files when users only need recent posts&lt;/item&gt;
      &lt;item&gt;Time inefficiency: Sequential downloads that block the user interface&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This article explores how Org Social 2.3+ solves both problems with a sophisticated combination of concurrent queue processing and HTTP Range-based partial fetching while maintaining complete compatibility with all servers.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Challenge&lt;/head&gt;
    &lt;quote&gt;flowchart TB A[User Opens Timeline] --&amp;gt; B[20 Feeds to Download] B --&amp;gt; C[Traditional Approach: Sequential Downloads] C --&amp;gt; D[Feed 1: 27KB 150 posts] D --&amp;gt; E[Feed 2: 15KB 80 posts] E --&amp;gt; F[Feed 3: 12KB 60 posts] F --&amp;gt; G[... 17 more feeds] G --&amp;gt; H[Total: ~300KB and ~1500 posts] H --&amp;gt; I[Filter to last 14 days] I --&amp;gt; J[Actually needed: ~10 posts=first page] style C fill:#ffcccc,color:black style H fill:#ffcccc,color:black style J fill:#ccffcc,color:black&lt;/quote&gt;
    &lt;p&gt;Downloading 300KB and processing 1500 posts to get 10 posts... It is not good!&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimization&lt;/head&gt;
    &lt;p&gt;Org-social.el implements a sophisticated three-layer approach:&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 1: Concurrent Queue Processing&lt;/head&gt;
    &lt;p&gt;A process queue is a data structure that manages tasks to be executed. In Org Social, each feed to download is added to the queue as a pending task. The system then processes these tasks concurrently (multiple at the same time) using a worker pool—a limited number of threads that execute downloads in parallel.&lt;/p&gt;
    &lt;p&gt;This smart queue system manages parallel downloads without overwhelming system resources.&lt;/p&gt;
    &lt;quote&gt;flowchart LR A[Feed Queue] --&amp;gt; B[Worker Pool Max 20 concurrent] B --&amp;gt; C[Worker 1 Feed A] B --&amp;gt; D[Worker 2 Feed B] B --&amp;gt; E[Worker 3 Feed C] B --&amp;gt; F[...] B --&amp;gt; G[Worker 20 Feed T] C --&amp;gt; H{Done?} D --&amp;gt; H E --&amp;gt; H G --&amp;gt; H H --&amp;gt;|Yes| I[Process Next Pending Feed] H --&amp;gt;|Error| J[Mark Failed, Continue] I --&amp;gt; B J --&amp;gt; I style B fill:#e1f5ff,color:black style H fill:#fff4e1,color:black style I fill:#ccffcc,color:black&lt;/quote&gt;
    &lt;p&gt;Key Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configurable concurrency: &lt;code&gt;org-social-max-concurrent-downloads&lt;/code&gt;(default: 20)&lt;/item&gt;
      &lt;item&gt;Non-blocking threads: Each download runs in a separate thread&lt;/item&gt;
      &lt;item&gt;Automatic recovery: Failed downloads don't block the queue&lt;/item&gt;
      &lt;item&gt;Smart scheduling: New downloads start immediately when slots free up&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Layer 2: HTTP Range-Based Partial Fetching&lt;/head&gt;
    &lt;p&gt;HTTP has a built-in feature that allows downloading only specific parts of a file using the &lt;code&gt;Range&lt;/code&gt; header. When a client sends a request with &lt;code&gt;Range: bytes=0-999&lt;/code&gt;, the server responds with just the first 1000 bytes of the file instead of the entire content. This capability is commonly used for video streaming and resumable downloads, but it can also be used to paginate files—downloading them in chunks rather than all at once.&lt;/p&gt;
    &lt;p&gt;Instead of downloading entire &lt;code&gt;social.org&lt;/code&gt; files, Org Social uses HTTP Range requests to fetch only what's needed: the header section and recent posts.&lt;/p&gt;
    &lt;p&gt;This system is not compatible with all providers. While most traditional web servers (Apache, Nginx, Caddy) support HTTP Range requests natively, some hosting platforms have limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Cloudflare CDN: Does not provide&lt;/p&gt;&lt;code&gt;Content-Length&lt;/code&gt;or&lt;code&gt;Content-Range&lt;/code&gt;headers, making it impossible to determine file size or download specific byte ranges. The system automatically falls back to downloading the complete file and filtering client-side.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Codeberg.org: Implements aggressive rate limiting when multiple Range requests are made in quick succession. When HTTP 429 (Too Many Requests) is detected, the system falls back to a full download without filtering to avoid being blocked.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;GitHub Raw Content: Provides proper HTTP Range support and works optimally with partial downloads.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The system detects these limitations automatically and adapts its strategy to ensure 100% compatibility across all hosting platforms.&lt;/p&gt;
    &lt;quote&gt;sequenceDiagram participant C as Client participant S as Server participant F as social.org (27KB, 150 posts) Note over C,F: Step 1: Find Header C-&amp;gt;&amp;gt;S: Range: bytes=0-999 S-&amp;gt;&amp;gt;C: First 1KB (headers) Note over C: Found "* Posts" at byte 800 Note over C,F: Step 2: Get File Size C-&amp;gt;&amp;gt;S: Range: bytes=0-0 S-&amp;gt;&amp;gt;C: Content-Range: bytes 0-0/27656 Note over C: Total size: 27656 bytes Note over C,F: Step 3: Fetch Recent Posts C-&amp;gt;&amp;gt;S: Range: bytes=26656-27655 S-&amp;gt;&amp;gt;C: Last 1KB (recent posts) C-&amp;gt;&amp;gt;S: Range: bytes=25656-26655 S-&amp;gt;&amp;gt;C: Previous 1KB Note over C: Found post older than 14 days Note over C,F: Result: Downloaded 3KB instead of 27KB&lt;/quote&gt;
    &lt;p&gt;Algorithm:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Header Discovery (bytes 0 → forwards)&lt;/item&gt;
      &lt;item&gt;Download the first 1KB chunk (bytes 0-999)&lt;/item&gt;
      &lt;item&gt;If &lt;code&gt;* Posts&lt;/code&gt;is not found, download the next 1KB chunk (bytes 1000-1999)&lt;/item&gt;
      &lt;item&gt;Continue downloading subsequent chunks until &lt;code&gt;* Posts&lt;/code&gt;is found&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Typical header size: 500-1500 bytes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Backward Post Fetching (end → backwards)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;Start from the end of the file (most recent posts)&lt;/item&gt;
      &lt;item&gt;Download 1KB chunks moving backwards&lt;/item&gt;
      &lt;item&gt;Parse each post's &lt;code&gt;:ID:&lt;/code&gt;property (e.g.,&lt;code&gt;:ID: 2025-10-24T10:00:00+0200&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Stop when reaching posts older than&lt;/p&gt;&lt;code&gt;org-social-max-post-age-days&lt;/code&gt;(default: 14 days)&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Date Filtering&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;Parse post IDs (RFC 3339 timestamps)&lt;/item&gt;
      &lt;item&gt;Keep only posts ≥ start date&lt;/item&gt;
      &lt;item&gt;Discard older posts without downloading&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Trick for Range Support Detection&lt;/head&gt;
    &lt;p&gt;The system sends a test request with the &lt;code&gt;Range: bytes=0-0&lt;/code&gt; header to check if the server responds with &lt;code&gt;Content-Range&lt;/code&gt; or &lt;code&gt;Accept-Ranges: bytes&lt;/code&gt; headers, indicating Range support.&lt;/p&gt;
    &lt;head rend="h3"&gt;Edge Cases and Fallbacks: Compressed Content&lt;/head&gt;
    &lt;p&gt;Servers using gzip compression (e.g., Caddy) report compressed sizes:&lt;/p&gt;
    &lt;p&gt;Problem: HEAD request returns compressed size, but content arrives uncompressed&lt;/p&gt;
    &lt;p&gt;Solution: Use a Range request (&lt;code&gt;bytes=0-0&lt;/code&gt;) for size detection instead. The server responds with &lt;code&gt;Content-Range: bytes 0-0/TOTAL&lt;/code&gt; where TOTAL is the actual uncompressed file size.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 3: UI Pagination&lt;/head&gt;
    &lt;p&gt;Even after downloading only recent posts, rendering all of them at once would overwhelm Emacs. The Org Social UI uses widgets to create an interactive interface with buttons, images, and formatted text. Each widget consumes memory and processing power.&lt;/p&gt;
    &lt;p&gt;To keep the interface responsive, the system implements pagination that displays only 10 posts per page. This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When a user opens the timeline, only the first 10 most recent posts are rendered&lt;/item&gt;
      &lt;item&gt;Images, avatars, and interactive widgets are created only for these 10 visible posts&lt;/item&gt;
      &lt;item&gt;The remaining downloaded posts stay in memory but aren't rendered&lt;/item&gt;
      &lt;item&gt;Users can navigate to the next page, which then renders the next 10 posts&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;flowchart LR A[Downloaded Posts: 50] --&amp;gt; B[Page 1: Render 10 posts] A --&amp;gt; C[Page 2: 10 posts in memory] A --&amp;gt; D[Page 3: 10 posts in memory] A --&amp;gt; E[Page 4: 10 posts in memory] A --&amp;gt; F[Page 5: 10 posts in memory] B --&amp;gt; G[User sees: 10 posts with widgets &amp;amp; images] C -.-&amp;gt;|User clicks Next| H[Render next 10 posts] style A fill:#e1f5ff,color:black style B fill:#ccffcc,color:black style G fill:#d5ffe1,color:black style H fill:#ffe1cc,color:black&lt;/quote&gt;
    &lt;p&gt;Emacs widgets and image rendering are resource-intensive. Rendering 50 posts with avatars and buttons could slow down the editor. By rendering only 10 at a time, the UI stays fast and responsive regardless of how many posts were downloaded.&lt;/p&gt;
    &lt;p&gt;This is the final optimization layer: even if your &lt;code&gt;social.org&lt;/code&gt; has 10,000 posts, and you download 50 recent ones, you only render 10 on screen. The rest wait in memory until needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Benchmarks&lt;/head&gt;
    &lt;p&gt;The following table shows how the system scales with different feed sizes, assuming an average post size of 250 bytes and a 14-day filter (capturing approximately 20-30 recent posts):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Total Posts&lt;/cell&gt;
        &lt;cell role="head"&gt;Full File Size&lt;/cell&gt;
        &lt;cell role="head"&gt;With Partial Fetch (14 days)&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Empty feed&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;~1 KB&lt;/cell&gt;
        &lt;cell&gt;~1 KB&lt;/cell&gt;
        &lt;cell&gt;Only headers downloaded&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;New user&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;~1.5 KB&lt;/cell&gt;
        &lt;cell&gt;~1.5 KB&lt;/cell&gt;
        &lt;cell&gt;Single post, no optimization needed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Light user&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;~3.5 KB&lt;/cell&gt;
        &lt;cell&gt;~3.5 KB&lt;/cell&gt;
        &lt;cell&gt;All posts fit in 14-day window&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Regular user&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;~26 KB&lt;/cell&gt;
        &lt;cell&gt;~8 KB&lt;/cell&gt;
        &lt;cell&gt;Headers + ~30 recent posts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Active user&lt;/cell&gt;
        &lt;cell&gt;1,000&lt;/cell&gt;
        &lt;cell&gt;~250 KB&lt;/cell&gt;
        &lt;cell&gt;~8 KB&lt;/cell&gt;
        &lt;cell&gt;Headers + ~30 recent posts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Power user&lt;/cell&gt;
        &lt;cell&gt;10,000&lt;/cell&gt;
        &lt;cell&gt;~2.5 MB&lt;/cell&gt;
        &lt;cell&gt;~8 KB&lt;/cell&gt;
        &lt;cell&gt;Headers + ~30 recent posts&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Key insight: Once a feed exceeds ~100 posts, partial fetching maintains consistent download sizes (~8 KB) regardless of total feed size. A feed with 10,000 posts downloads the same amount of data as one with 1,000 posts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tuning Recommendations&lt;/head&gt;
    &lt;p&gt;Users can customize two main parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;org-social-max-concurrent-downloads&lt;/code&gt;: Maximum parallel downloads (default: 20)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;org-social-max-post-age-days&lt;/code&gt;: Maximum age of posts to fetch in days (default: 14)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fast connection + many feeds: Increase to 30 concurrent&lt;/item&gt;
      &lt;item&gt;Slow connection: Decrease to 10 concurrent&lt;/item&gt;
      &lt;item&gt;Large feeds + limited bandwidth: Decrease &lt;code&gt;max-post-age-days&lt;/code&gt;to 7&lt;/item&gt;
      &lt;item&gt;Small feeds: Set &lt;code&gt;max-post-age-days&lt;/code&gt;to nil (no optimization needed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The three-layer optimization approach (concurrent queue processing, HTTP Range-based partial fetching, and UI pagination) provides a significant bandwidth optimization on large feeds with date filtering and non-blocking UI. This architecture positions Org Social to scale efficiently as both individual feeds and follower counts grow, while maintaining the simplicity and decentralization that make Org Social unique.&lt;/p&gt;
    &lt;p&gt;Your &lt;code&gt;social.org&lt;/code&gt; can have millions of lines because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Only recent posts are downloaded (14 days by default)&lt;/item&gt;
      &lt;item&gt;Downloads happen in parallel without blocking&lt;/item&gt;
      &lt;item&gt;Only 10 posts are rendered on screen at once&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Enjoy it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HTTP Range Requests: RFC 7233&lt;/item&gt;
      &lt;item&gt;RFC 3339 Timestamps: RFC 3339&lt;/item&gt;
      &lt;item&gt;Org Social Specification: github.com/tanrax/org-social&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Challenge&lt;/item&gt;
      &lt;item&gt;Optimization&lt;/item&gt;
      &lt;item&gt;Layer 1: Concurrent Queue Processing&lt;/item&gt;
      &lt;item&gt;Layer 2: HTTP Range-Based Partial Fetching&lt;/item&gt;
      &lt;item&gt;Trick for Range Support Detection&lt;/item&gt;
      &lt;item&gt;Edge Cases and Fallbacks: Compressed Content&lt;/item&gt;
      &lt;item&gt;Layer 3: UI Pagination&lt;/item&gt;
      &lt;item&gt;Performance Benchmarks&lt;/item&gt;
      &lt;item&gt;Tuning Recommendations&lt;/item&gt;
      &lt;item&gt;Conclusion&lt;/item&gt;
      &lt;item&gt;Technical References&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This work is under a Attribution-NonCommercial-NoDerivatives 4.0 International license.&lt;/p&gt;
    &lt;head rend="h2"&gt;Will you buy me a coffee?&lt;/head&gt;
    &lt;p&gt;You can use the terminal.&lt;/p&gt;
    &lt;quote&gt;ssh customer@andros.dev -p 5555&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45701980</guid><pubDate>Sat, 25 Oct 2025 07:27:07 +0000</pubDate></item><item><title>React vs. Backbone in 2025</title><link>https://backbonenotbad.hyperclay.com/</link><description>&lt;doc fingerprint="a7f21d051da72e1d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;15 Years of Progress&lt;/head&gt;
    &lt;p&gt;Look at the two implementations above. The code is roughly the same length. They do exactly the same thing. One was written with a framework from 2010, the other with a framework that's had countless developer hours and a massive ecosystem behind it for over a decade.&lt;/p&gt;
    &lt;p&gt;The interesting part is not how much better React is—it's how little progress we've actually made.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Illusion of Simplicity&lt;/head&gt;
    &lt;p&gt;React looks cleaner. It reads better at first glance. But that readability comes at a cost: you're trading explicit simplicity for abstraction complexity.&lt;/p&gt;
    &lt;p&gt;The Backbone code is brutally honest about what it's doing. An event fires, a handler runs, you build some HTML, you put it in the DOM. It's verbose, sure, but there's no mystery. A junior developer can trace exactly what happens and when. The mental model is straightforward: "when this happens, do this."&lt;/p&gt;
    &lt;p&gt;The React code hides a lot. And once you move past simple examples, you hit problems that don't make sense until you understand React's internals.&lt;/p&gt;
    &lt;p&gt; Your input mysteriously clears itself. Turns out you switched a list item's key from a stable ID to an index, so React thinks it's a completely different component and remounts it, wiping state. Or maybe you forgot that &lt;code&gt;value&lt;/code&gt; can't be &lt;code&gt;undefined&lt;/code&gt;—React saw it flip from uncontrolled to controlled and reset the input.
        &lt;/p&gt;
    &lt;p&gt; You add a &lt;code&gt;useEffect&lt;/code&gt; to fetch data, and suddenly your app is stuck in an infinite loop. The dependency array includes an object that gets recreated every render, so React thinks it changed and runs the effect again. Now you need &lt;code&gt;useMemo&lt;/code&gt; and &lt;code&gt;useCallback&lt;/code&gt; sprinkled everywhere to "stabilize identities," which is a thing you never had to think about before.
        &lt;/p&gt;
    &lt;p&gt; Your click handler sees old state even though you just set it. That's a stale closure—the function captured the value from when it was created, and later renders don't magically update it. You either need to put the state in the dependency array (creating a new handler every time) or use functional updates like &lt;code&gt;setState(x =&amp;gt; x + 1)&lt;/code&gt;. Both solutions feel like workarounds.
        &lt;/p&gt;
    &lt;head rend="h3"&gt;Magic Has a High Price&lt;/head&gt;
    &lt;p&gt;These aren't edge cases. They're normal problems you hit building moderately complex apps. And debugging them requires understanding reconciliation algorithms, render phases, and how React's scheduler batches updates. Your code "just works" without you needing to understand why it works, which is nice until it breaks.&lt;/p&gt;
    &lt;p&gt;People say "you need to rebuild React from scratch to really understand it," and they're right. But that's kind of damning, isn't it? You shouldn't need to understand virtual DOM diffing, scheduling priorities, and concurrent rendering to build a password validator.&lt;/p&gt;
    &lt;p&gt;Backbone might be tedious, but it doesn't lie to you. jQuery is hackable. You can view source, understand it, and add to it easily. It's just DOM methods. React's abstraction layers make that much harder.&lt;/p&gt;
    &lt;head rend="h3"&gt;So, What's Next?&lt;/head&gt;
    &lt;p&gt;We understand the problem: event + state = UI. That's it. That's what both of these implementations are solving.&lt;/p&gt;
    &lt;p&gt;For massive apps with 1,000 components on the same page, maybe React's complexity is justified. But what the other 99% of apps? What about small apps that just want to do a job and don't need all the magic?&lt;/p&gt;
    &lt;p&gt;Is there a better model? Something feels as hard and steady as the DOM, but still feels intuitive to write? Something hackable like Backbone and jQuery were, where you can pop open devtools and understand what's happening?&lt;/p&gt;
    &lt;p&gt;— panphora&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45702558</guid><pubDate>Sat, 25 Oct 2025 09:43:54 +0000</pubDate></item><item><title>Making a micro Linux distro (2023)</title><link>https://popovicu.com/posts/making-a-micro-linux-distro/</link><description>&lt;doc fingerprint="579b191fd9a42fad"&gt;
  &lt;main&gt;
    &lt;p&gt;In this article, we’ll talk about building up a tiny (micro) Linux “distribution” from scratch. This distribution really won’t do much, but it will be built from scratch.&lt;/p&gt;
    &lt;p&gt;We will build the Linux kernel on our own, and write some software to package our micro-distro.&lt;/p&gt;
    &lt;p&gt;Lastly, we are doing this example on the RISC-V architecture, specifically QEMU’s &lt;code&gt;riscv64 virt&lt;/code&gt; machine. There’s very little in this article that is specific to this architecture, so you might as well do an almost identical exercise for other architectures like &lt;code&gt;x86&lt;/code&gt;. We recently went through the RISC-V boot process with SBI and bare metal programming for RISC-V, so this is just a continuation up the software stack.&lt;/p&gt;
    &lt;p&gt;Warning: This article is a very simplified view of a Linux distribution. There are things written below that are not 100% accurate, but more like 99.9%. This article is meant for beginners and helping them form a basic mental framework for understanding Linux systems. More advanced users may be triggered by over-simplification in some parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;What is an OS kernel?&lt;/head&gt;
    &lt;p&gt;Let’s assume we’re working on a single-core machine. They’re still around us, maybe not in our laptops and phones, but in some smaller devices, and historically they have been actually widely used even in our “big” personal devices like desktops. The latter ones have been capable of running multiple program simultaneously for many years, even as single cores. We’ll get into what simultaneous really means in a bit, but for now let’s just note that the one of the operating system kernel’s big tasks is to make that happen.&lt;/p&gt;
    &lt;p&gt;If you go back to the articles about bare metal programming and SBI on RISC-V, you can see how at the lowest layers of software we interact with our I/O devices. It usually (most often, but not necessarily always) boils down to the CPU writing some data at the appropriate address. Imagine if the application developers had to keep all these addresses in mind and they had to know which values exactly to send to those addresses! That would mean we’d have far fewer applications today, but we don’t, and that’s owing to the operating system kernels which abstract away these details and provide some simple high-level interfaces instead. In the RISC-V SBI article, we looked at an example of such interface for Linux on &lt;code&gt;x86&lt;/code&gt; — instead of knowing which addresses to write to and what values to send there, we focused on the logic and basically just told to the OS kernel that “we want message so and so written to the standard output”, and then the OS kernel dealt with the details of interacting with the hardware. So that’s another big task for the OS kernel: managing the hardware on the machine and making the interaction with it easier.&lt;/p&gt;
    &lt;p&gt;Going further, the OS kernel offers some really high-level programming interfaces like the filesystems. This may or may not be about managing some hardware and abstracting operations over it. For example, the most common case for the filesystems, of course, is to store some data on the disk and retrieve it later, and this has to do with the OS kernel managing the hardware related to disks on the machine (i.e. sending some data to certain addresses, which makes those hard disk devices respond in some way). However, this is not always the case, the files are not always some data stored on disk, and so filesystem is an interface exposed to us, meaning it’s a way of talking to the OS kernel, not necessarily a way to talk to the data. We’ll cover the filesystems in great detail in some other article, but let’s keep this in mind for now — the OS kernel needs to provide a straightforward way of doing high-level things through multiple interfaces.&lt;/p&gt;
    &lt;p&gt;Finally, the last thing I wanted to cover about the kernels is that they provide a programming model. Remember how we mentioned (as I’m sure you already know) that multiple programs can run even on a single-core device simultaneously? The OS enables the running applications to be programmed to not even know about each other, in other words, an application can live its lifecycle acting like it is the only application running on the computer and no one else is touching its memory. Imagine a world where your Python Django server needs to know about that texting app on your device in order to be working — we’d have far fewer Django apps and texting apps, for sure, as coding them would quickly get gnarly. However, the apps can also know about each other’s existence on the same machine. The operating system kernel facilitates both. It gives a programming model in which you can insulate applications from each other, or join a few apps in isolation from other apps, etc.&lt;/p&gt;
    &lt;p&gt;Basically, the OS kernel does a lot of heavy lifting to enable you to run your code easily on a very generic and complicated machinery such as your smartphone. What is written above probably doesn’t do full justice to the kernels, they do a whole lot of things, but the few paragraphs above should give a fairly good idea of kernel’s main tasks, and there are many.&lt;/p&gt;
    &lt;p&gt;Linux is an extremely popular operating system kernel. It can be built to run on many architectures (really, a lot), it is open source and free to use. And a lot of people are “Linux users”, but what does it exactly mean that someone “uses Linux”? Those Linux users typically install something like Debian, or Ubuntu on their machines, and they use Linux that way, and what does that mean?&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a Linux distribution?&lt;/head&gt;
    &lt;p&gt;We talked above about what kernels do, i.e. what are their tasks and we said Linux is an OS kernel, but can we really just take bare Linux and as end users who just want to watch YouTube, do something with it? The answer is likely no, we need a lot more layers on top of Linux to get to firing up a Chrome browser and watching YouTube.&lt;/p&gt;
    &lt;p&gt;How to go all the way towards the top of the software stack where we can just use those super simple and intuitive apps like graphical web browsers? We have previously discussed the boot process, and we went all the way from the very first operations on the machine after the power on, to the moment we land in the operating system kernel. We did not cover the bootloaders in any detail, we just briefly mentioned them because we were able to get QEMU to directly load our fake kernel into the memory in one go, which is typically not possible with full blown systems like desktop Linux (there is an intermediate booting stage where the bootloader fetches the OS image from something like a disk, or maybe even network and loads it into the memory). The kernel we wrote was a fake little stub that does effectively nothing, and so we ended our last article at the point where the OS kernel is in memory and ready to go, it’s just we had no kernel to run.&lt;/p&gt;
    &lt;p&gt;Based on what we see above, I think the right mental model for the kernel right now is that it is the infrastructure for running user applications on a complex machine, but it really doesn’t do anything for the user’s business logic. This is what I meant when I said the bare Linux on its own cannot fire up Chrome and let you watch YouTube — it is merely the infrastructure that the application developer uses to implement Chrome, and its streaming capabilities.&lt;/p&gt;
    &lt;p&gt;However, the kernel alone is not infrastructure for Chrome to run. We need to run sort of “infrastructure on top of infrastructure” to achieve the full infrastructure to run Chrome. Again, much like in the SBI article, we’re just layering abstractions on top of each other in some way, so essentially there is nothing new here, just the way we do it.&lt;/p&gt;
    &lt;p&gt;For example, in order for a machine to connect to the Internet, the OS kernel first needs to be able to drive the network device on the machine to send the signals out of the machine (to the switch, router, another machine or whatever it is connected to). However, in Linux, there is more or less where the kernel stops. Which networks you connect to, are you using VPN, how do you assign IPs to your machine (statically or dynamically) and that kind of business, it happens in the upper layers of the infrastructure.&lt;/p&gt;
    &lt;p&gt;You may now guess where this is going — a Linux distribution is really the Linux kernel plus the infrastructure on top of the kernel infrastructure. Let’s dig into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does “infrastructure on top of infrastructure” run?&lt;/head&gt;
    &lt;p&gt;Again, the kernel does a whole bunch of things, a million times more than what we can cover in a single article, but it definitely has its limits and it doesn’t do all the heavy lifting on your everyday personal device — and this is where something outside of the kernel gets into the picture.&lt;/p&gt;
    &lt;p&gt;Disclaimer: You can get really creative with Linux in a million different ways, and from this point on we’re going with a very basic, textbook-like, simple view of what happens in the mainstream distributions. There are many super complex things we can do, and there are lots of details we’re leaving out, but my hope here is that you get a general idea and enough knowledge to be able to understand more advanced material on this topic; there is plenty of it on the Internet.&lt;/p&gt;
    &lt;p&gt;The reason why I wrote the disclaimer above is mainly because we’re going to be assuming that your Linux has a filesystem going forward, as this is the most common path. How many times have you seen a Linux deployment without a filesystem? It certainly seems possible to do, but it may be borderline useless except for some super edge/advanced cases, and we’ll disregard them in this article. Check out this page to get more idea of what I’m talking about.&lt;/p&gt;
    &lt;p&gt;So what is the stuff outside the kernel? It’s what we call the user code! It’s just a normal code that runs within the Linux environment, just like you run basically anything on your Linux machine. Sure, some code is more privileged than the other, and there are a million more details that can get involved, but let’s just focus on the main distinction here: when you are running Linux on a machine, there is kernel code running, as well as the user code running, and everything that’s a part of the kernel itself is running in the kernel space, and everything that is running on the machine that is not a part of the kernel is running in the user space, and they are fairly isolated from each other.&lt;/p&gt;
    &lt;p&gt;So this “infrastructure on top of infrastructure” that we have talked about runs in the user space. Sure, it needs to bubble down to the kernel for many primitives, and we’ve seen already how that happens. Linux has a well defined ABI that exposes a set of services that the user space code can invoke in the kernel space. And where does this user space code come into the picture?&lt;/p&gt;
    &lt;head rend="h2"&gt;The &lt;code&gt;init&lt;/code&gt; process (and its “children”)&lt;/head&gt;
    &lt;p&gt;Once the kernel is done loading and making itself comfortable on the machine, it kicks off the first bit of the code in user space — the &lt;code&gt;init&lt;/code&gt; process. This is a piece of user space code that lives in a binary that sits somewhere on your filesystem, and the kernel will look for it in a few locations, beginning with &lt;code&gt;/init&lt;/code&gt; (if it doesn’t find it there, it will give a few more shots at different locations before throwing its hands up). Let’s say the kernel found a binary in the filesystem at &lt;code&gt;/init&lt;/code&gt; — it’s going to start it and assign the ID &lt;code&gt;1&lt;/code&gt;. This is basically the only user process that the kernel will start: the &lt;code&gt;init&lt;/code&gt; process then is the ancestor of all other user space processes. This means that &lt;code&gt;init&lt;/code&gt; will start some other processes, these other processes will in turn start some other processes, and so on. Very shortly you have a bunch of processes running on your machine, hopefully each one of them useful for the desired operations on the machine. The machine should at this point start actively interacting with the world around it: whether we’re talking about a smartphone giving the UI to its user, an embedded device that collects data off the sensors and sending it into the cloud, etc. Additionally, the machine will often have various tools available that are not actively running on the machine, but can be invoked in certain situations for some high level operations (e.g. a Python script can invoke a couple of tools like &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;cat&lt;/code&gt; or something to get a snapshot of what’s going on with the machine and then sending the data somewhere). Quick note is that even these periodically-started or ad-hoc tools are in some way descendants of &lt;code&gt;init&lt;/code&gt;; it’s not too important to know now, but it’s good to keep in mind.&lt;/p&gt;
    &lt;p&gt;The collection of kernel, the processes that get launched right after the kernel, and the tools that are available at your disposal represent the Linux distribution. It’s essentially a packaging for the kernel alongside all these useful tools that do more around the machine than what the kernel alone does (but it still provides the infrastructure for everything outside of the kernel to run, nothing bypasses the kernel).&lt;/p&gt;
    &lt;p&gt;Even a distrbution minimally useful for everyday use can get crufty pretty quickly. If you go onto the path of building your own custom little distro, as we actually will now, you will almost inevitably hit a lot of roadblocks where something that you expect to be working is just not working and the full solution is either to code some of your own software to talk to the kernel to get something done on the system, or just use an off-the-shelf software to do so. The latter is the path of least resistance, and you’ll likely keep adding stuff until you end up with a deployment that can do something remotely useful for you. At this point, you will have likely accumulated a significant number of software packages.&lt;/p&gt;
    &lt;p&gt;On the other hand, you have probably heard people criticizing certain distributions as being “bloated”, probably meaning they accumulated so much complexity in their packaging, they waste a lot of hardware resources doing things that are not useful, etc. Without discipline, I can easily see distrbution developers just randomly throwing different tools at the system just to get that one missing thing going, without retroactively cleaning up the excess later and just moving onto the next feature where they do the same — a (sadly) common pattern in software engineering.&lt;/p&gt;
    &lt;p&gt;Some distributions draw the line at different places where they just make a decision for the user and do something on the system, versus letting the user make the full decision and be more hands on. For example, you can install Arch Linux in a minimal way where it’s just a little more than the kernel booted up with a shell. All the subsequent decisions are on you, and you have to be very hands on in order to get it to a point where it’s very graphical and highly interactive. Or you can decide it’s just not worth your time setting it up so much, and just install a very user-friendly Ubuntu distrbution, which may be “bloated” for someone’s taste, but it gets you up and running very fast (I personally like it).&lt;/p&gt;
    &lt;head rend="h2"&gt;Building our almost useless Linux micro distrbibution&lt;/head&gt;
    &lt;p&gt;Let’s get our hands dirty and build something that’s basically useless but we’ll actually end up booting it for real. You may want to refresh your memory on the RISC-V boot process, I think it will be rewarding here.&lt;/p&gt;
    &lt;p&gt;First things first, let’s build the kernel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building a Linux operating system for RISC-V&lt;/head&gt;
    &lt;p&gt;I’m on an &lt;code&gt;x86&lt;/code&gt; platform here, so I will depend heavily on the cross-platform toolchain to build things for RISC-V. You will likely do something similar (I’m not sure I have yet seen someone build the RISC-V kernel on RISC-V itself).&lt;/p&gt;
    &lt;p&gt;Let’s get the source code for Linux. Linux development is done on top of the Git version control system, but we’ll take a shortcut here and just download a tarball with the sources for one branch, we won’t be syncing the whole Linux codebase with all the Git branches, experimental stuff and so on. We’ll be downloading the tarball from &lt;code&gt;kernel.org&lt;/code&gt; for version &lt;code&gt;6.5.2&lt;/code&gt; (here). You can also just download any tarball for whatever the latest stable version is from kernel.org homepage. Once it’s downloaded, go ahead and unpack that. Let’s also &lt;code&gt;cd&lt;/code&gt; into that directory.&lt;/p&gt;
    &lt;p&gt;Now is the time to configure the build. The first step is to make the &lt;code&gt;defconfig&lt;/code&gt; which basically initiates your configuration file.&lt;/p&gt;
    &lt;p&gt;Note: Here and below, you may want to use a different &lt;code&gt;CROSS_COMPILE&lt;/code&gt; prefix, depending on how the cross compilation tool is identified on your machine&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- defconfig&lt;/code&gt;
    &lt;p&gt;This was hopefully quick and the &lt;code&gt;.config&lt;/code&gt; file should be generated. The config file should contain a lot of IDs for individual configurations and the values for those, very often in yes/no format (e.g. &lt;code&gt;CONFIG_FOO=y&lt;/code&gt; or &lt;code&gt;CONFIG_FOO=n&lt;/code&gt;). You could edit the file manually, but I personally wouldn’t recommend it, especially as a beginner (I don’t consider myself an expert at this either). A better way to edit this is through the &lt;code&gt;curses&lt;/code&gt;-based pseudo-interface. You can get there by running&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- menuconfig&lt;/code&gt;
    &lt;p&gt;This interface has a few benefits.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You have a more readable, folder-like overview of the configs.&lt;/item&gt;
      &lt;item&gt;There are insights into dependencies between the configs, i.e. it may only make sense to be able to enable config &lt;code&gt;foo&lt;/code&gt;if&lt;code&gt;bar&lt;/code&gt;and&lt;code&gt;baz&lt;/code&gt;are also enabled.&lt;/item&gt;
      &lt;item&gt;This interface has a search feature, activated by pressing the &lt;code&gt;/&lt;/code&gt;button (I don’t think you’ll get far by searching there in natural language; my way of getting around here is by searching on Google and finding which exactly config key am I looking for, for example&lt;code&gt;CONFIG_TTY_PRINTK&lt;/code&gt;). When you find what you’re looking for, hit the button you see in the parentheses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We won’t be tweaking anything here for now, let’s just exit and move on.&lt;/p&gt;
    &lt;p&gt;It’s time to build the kernel! Quick note here, the make process famously has the &lt;code&gt;-j&lt;/code&gt; flag, which basically sets the concurrency in the build process, meaning it allows the build process to run a few things simultaneously. If you want to build faster, but not sure what to do, count the number of cores, and if it’s something like 8, just pass the flag &lt;code&gt;-j8&lt;/code&gt; below, as so. I will run the command like this (I’m on a 16-core machine):&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- -j16&lt;/code&gt;
    &lt;p&gt;This can take some time, though for the RISC-V build, it shouldn’t take awfully long, but I would expect at least a few minutes.&lt;/p&gt;
    &lt;p&gt;Once this is done, you will probably see something like this near the very bottom:&lt;/p&gt;
    &lt;code&gt;OBJCOPY arch/riscv/boot/Image&lt;/code&gt;
    &lt;p&gt;and this is the file we will be feeding to QEMU.&lt;/p&gt;
    &lt;p&gt;Great, let’s fire up QEMU!&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image&lt;/code&gt;
    &lt;p&gt;Switching to the UART view, we see that OpenSBI tidily started and the Linux took over! Great! We even see some references to the SBI layer that we have discussed before:&lt;/p&gt;
    &lt;code&gt;[    0.000000] Linux version 6.5.2 (uros@uros-debian-desktop) (riscv64-linux-gnu-gcc (Debian 10.2.1-6) 10.2.1 20210110, GNU ld (GNU Binutils for Debian) 2.35.2) #1 SMP Mon Sep 11 00:45:40 PDT 2023
[    0.000000] Machine model: riscv-virtio,qemu
[    0.000000] SBI specification v0.2 detected
[    0.000000] SBI implementation ID=0x1 Version=0x8
[    0.000000] SBI TIME extension detected
[    0.000000] SBI IPI extension detected
[    0.000000] SBI RFENCE extension detected&lt;/code&gt;
    &lt;p&gt;After reading about the boot process, we should now have a full understanding of what is going on here. This happened super early in the boot phase. There is a lot happening in these logs, and I’ll highlight a few things:&lt;/p&gt;
    &lt;code&gt;[    0.000000] riscv: base ISA extensions acdfim&lt;/code&gt;
    &lt;p&gt;Seems like Linux is capable of dynamically figuring out the capability of the underlying RISC-V hardware. I’m not sure what exactly is the mechanism behind it, could it be somehow passed through the device tree that we mentioned in the previous article, or something in the ISA itself tells this to the kernel, I’m not sure.&lt;/p&gt;
    &lt;code&gt;[    0.000000] Kernel command line:&lt;/code&gt;
    &lt;p&gt;This is interesting, a kernel has a command line? Turns out that the kernel, much like your everyday binaries, has startup flags. The kernel bootloader usually sets those up — after all, it knows how to fire up the kernel, and this could simply be a part of the starting process. With QEMU, remember, we’re sort of short circuiting the whole bootloader thing, and with passing the &lt;code&gt;-kernel&lt;/code&gt; flag, we let QEMU also wear the bootloader hat here by loading the kernel image into the memory and starting it up. QEMU actually has a flag called &lt;code&gt;-append&lt;/code&gt; with which you can append to this kernel command line. The command line itself is baked into the config file under &lt;code&gt;Boot options&lt;/code&gt; somewhere, I leave it to the reader to search for it, and the QEMU flag basically lets you adjust it with a VM launch, instead of having to rebuild the kernel to tweak the command line. In this case, the command line is just blank by default.&lt;/p&gt;
    &lt;code&gt;[    0.003376] printk: console [tty0] enabled&lt;/code&gt;
    &lt;p&gt;I guess this means that &lt;code&gt;printk&lt;/code&gt; will now write to &lt;code&gt;tty0&lt;/code&gt;? &lt;code&gt;printk&lt;/code&gt; is basically a way to write out messages from the kernel space. Remember, your typical &lt;code&gt;printf&lt;/code&gt; from C’s &lt;code&gt;stdio.h&lt;/code&gt; is meant for running in the user space, not kernel space, so kernel space must have its own solution, and it is &lt;code&gt;printk&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[    0.211634] Serial: 8250/16550 driver, 4 ports, IRQ sharing disabled
[    0.221544] 10000000.uart: ttyS0 at MMIO 0x10000000 (irq = 12, base_baud = 230400) is a 16550A
[    0.222659] printk: console [ttyS0] enabled&lt;/code&gt;
    &lt;p&gt;Great, Linux knows there is UART at &lt;code&gt;0x10000000&lt;/code&gt;, just like we established before. Linux can now choose whether to use the SBI interface to drive the UART, or talk to it directly (if the S-mode allows it on that machine, that is). On many platforms, the OS can disregard that a lower level software like BIOS may offer to interact with the hardware, and from what I hear, this actually indeed happens a lot.&lt;/p&gt;
    &lt;p&gt;There’s also a lot of other stuff in the kernel logs:&lt;/p&gt;
    &lt;code&gt;[    0.250030] SuperH (H)SCI(F) driver initialized&lt;/code&gt;
    &lt;p&gt;I don’t think we need this? I guess we can go back to the kernel config and not bake this driver into the kernel and thus slim the kernel down. What we’re building here is a generic build, really. We didn’t customize anything and presumably the authors of the default config thought this is a reasonable default that should just run on a lot of different setups, so they probably included a lot of things to be on the safe side. If you’re working on smaller hardware, with less generous memory, CPU, etc. you do have to carefully choose what gets baked into the kernel and what doesn’t.&lt;/p&gt;
    &lt;p&gt;Additionally, this generic build is smart enough to figure out that the console should go to the right UART device, which is really handy for us. Otherwise, we’d probably have to do a bunch of configs like making sure TTY (let’s not overfocus on what this is now) is enabled, we want to enable printing to UART as the kernel boots, etc. All this is basically configurable in the &lt;code&gt;menuconfig&lt;/code&gt; interface.&lt;/p&gt;
    &lt;p&gt;We’ll keep it simple in this article, and we won’t customize anything in the kernel unless we have to.&lt;/p&gt;
    &lt;head rend="h4"&gt;First obstacles&lt;/head&gt;
    &lt;p&gt;Scrolling down closer to the bottom of the output, we see this:&lt;/p&gt;
    &lt;code&gt;[    0.330411] /dev/root: Can't open blockdev
[    0.330743] VFS: Cannot open root device "" or unknown-block(0,0): error -6
[    0.330984] Please append a correct "root=" boot option; here are the available partitions:
[    0.331648] List of all bdev filesystems:
[    0.331785]  ext3
[    0.331803]  ext2
[    0.331882]  ext4
[    0.331950]  vfat
[    0.332028]  msdos
[    0.332098]  iso9660
[    0.332181]
[    0.332405] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0)
[    0.332756] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.5.2 #1
[    0.333018] Hardware name: riscv-virtio,qemu (DT)
[    0.333248] Call Trace:
[    0.333442] [&amp;lt;ffffffff8000537a&amp;gt;] dump_backtrace+0x1c/0x24
[    0.333940] [&amp;lt;ffffffff808890f8&amp;gt;] show_stack+0x2c/0x38
[    0.334138] [&amp;lt;ffffffff80894a48&amp;gt;] dump_stack_lvl+0x3c/0x54
[    0.334318] [&amp;lt;ffffffff80894a74&amp;gt;] dump_stack+0x14/0x1c
[    0.334493] [&amp;lt;ffffffff80889500&amp;gt;] panic+0x102/0x29e
[    0.334683] [&amp;lt;ffffffff80a015c6&amp;gt;] mount_root_generic+0x1e8/0x29c
[    0.334891] [&amp;lt;ffffffff80a0186c&amp;gt;] mount_root+0x1f2/0x224
[    0.335108] [&amp;lt;ffffffff80a01a68&amp;gt;] prepare_namespace+0x1ca/0x222
[    0.335320] [&amp;lt;ffffffff80a010c8&amp;gt;] kernel_init_freeable+0x23e/0x262
[    0.335539] [&amp;lt;ffffffff80896264&amp;gt;] kernel_init+0x1e/0x10a
[    0.335714] [&amp;lt;ffffffff800034c2&amp;gt;] ret_from_fork+0xa/0x1c
[    0.336208] ---[ end Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ]---&lt;/code&gt;
    &lt;p&gt;Whoops, we crashed! The kernel has fallen into a panic.&lt;/p&gt;
    &lt;p&gt;Remember how we talked that pretty much always Linux needs a filesystem to be useful and how all the “infrastructure on top of infrastructure” is in the user space? Well, we didn’t really pass anything related to the filesystem explicitly and we surely didn’t pass any user space code to serve as the &lt;code&gt;init&lt;/code&gt;, though we didn’t even get to the latter.&lt;/p&gt;
    &lt;p&gt;You might imagine that the filesystem needs to be on a disk, but that’s not necessarily the case. We’ll talk some other time about filesystems in great detail, but you can really have a filesystem be backed by RAM memory too. And this is actually very often used by Linux, most notably in the boot up phase. When the kernel gets to where it crashed for us just now, in a normal, typical situation, it will find the whole, fully functional filesystem actually loaded into the RAM. If this confuses you, just think about it this way — a disk is just a bunch of bytes, just like RAM is, though RAM is faster but much smaller; conceptually they’re basically the same. Who and how loads this memory?&lt;/p&gt;
    &lt;p&gt;One way is to bake the filesystem directly into the kernel image. In this case, as the kernel loads, so does the initial, memory-backed filesystem, and our system would be ready to go if we had done that. If you don’t want to bulk up your kernel image and you want your initial filesystem to be loaded by some other means, like through a bootloader or something, then you package it separately. In QEMU case, we can shortcircuit things a little bit again, and make it wear a few more hats — we’ll make it also load the initial filesystem into the memory as well. If you’re interested in building the filesystem into the kernel, read the discussion here and try it as an exercise after you’re done with this guide.&lt;/p&gt;
    &lt;p&gt;This initial filesystem has a name: &lt;code&gt;initramfs&lt;/code&gt;. You’ll often hear it called &lt;code&gt;initrd&lt;/code&gt; too (I imagine &lt;code&gt;rd&lt;/code&gt; is short for ramdisk?). The latter is how QEMU takes in the filesystem for loading (&lt;code&gt;-initrd&lt;/code&gt; flag).&lt;/p&gt;
    &lt;p&gt;The filesystem is packaged as a &lt;code&gt;cpio&lt;/code&gt; archive, which is conceptually similar to &lt;code&gt;tar&lt;/code&gt;, but it’s not the same binary format. Short discussion can be read here.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building the &lt;code&gt;initramfs&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The only real requirement for the &lt;code&gt;initramfs&lt;/code&gt; from the kernel is that it has a binary it can start up as the &lt;code&gt;init&lt;/code&gt; process, and the first place where the kernel will look for it is at the filesystem root, so the path is &lt;code&gt;/init&lt;/code&gt;. If you have absolutely nothing else on your filesystem, it’s questionably useful, but this is the bare requirement. Let’s start by writing the &lt;code&gt;init&lt;/code&gt; process in C. This process can be really anything, Linux won’t stop you from writing a useless &lt;code&gt;init&lt;/code&gt;, it will happily just execute it. We can go with a ‘hello world’ then?&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main(int argc, char *argv[]) {
  printf("Hello world\n");
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Great, now let’s package it up into a &lt;code&gt;cpio&lt;/code&gt; archive.&lt;/p&gt;
    &lt;code&gt;riscv64-linux-gnu-gcc -static -o init init.c
cpio -o -H newc &amp;lt; file_list.txt &amp;gt; initramfs.cpio&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;file_list.txt&lt;/code&gt; has a single line:&lt;/p&gt;
    &lt;code&gt;init&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We’re building a static binary because we do not want to dynamically depend on the standard C library. The filesystem won’t have it, we’re making a filesystem with &lt;code&gt;init&lt;/code&gt;alone.&lt;/item&gt;
      &lt;item&gt;Linux expects the &lt;code&gt;initramfs&lt;/code&gt;archive to be built with the&lt;code&gt;-H newc&lt;/code&gt;flag.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s run QEMU.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /PATH/TO/NEWLY_BUILT/initramfs.cpio&lt;/code&gt;
    &lt;p&gt;The kernel stil falls into a panic, but a different one!&lt;/p&gt;
    &lt;code&gt;[    0.351894] Run /init as init process
Hello world
[    0.379006] Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000000
[    0.379360] CPU: 0 PID: 1 Comm: init Not tainted 6.5.2 #1
[    0.379597] Hardware name: riscv-virtio,qemu (DT)
[    0.379812] Call Trace:
[    0.380005] [&amp;lt;ffffffff8000537a&amp;gt;] dump_backtrace+0x1c/0x24
[    0.380724] [&amp;lt;ffffffff808890f8&amp;gt;] show_stack+0x2c/0x38
[    0.380906] [&amp;lt;ffffffff80894a48&amp;gt;] dump_stack_lvl+0x3c/0x54
[    0.381095] [&amp;lt;ffffffff80894a74&amp;gt;] dump_stack+0x14/0x1c
[    0.381283] [&amp;lt;ffffffff80889500&amp;gt;] panic+0x102/0x29e
[    0.381447] [&amp;lt;ffffffff80013fd0&amp;gt;] do_exit+0x760/0x766
[    0.381623] [&amp;lt;ffffffff80014154&amp;gt;] do_group_exit+0x24/0x70
[    0.381806] [&amp;lt;ffffffff800141b8&amp;gt;] __wake_up_parent+0x0/0x20
[    0.382009] [&amp;lt;ffffffff80895482&amp;gt;] do_trap_ecall_u+0xe6/0xfa
[    0.382218] [&amp;lt;ffffffff8000337c&amp;gt;] ret_from_exception+0x0/0x64
[    0.382808] ---[ end Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000000 ]---&lt;/code&gt;
    &lt;p&gt;I guess this just means &lt;code&gt;init&lt;/code&gt; shouldn’t finish, so it should be easy to fix? Let’s just make it print something every 10 seconds and never stop. Important to note: our output worked, we see a “Hello world” string!&lt;/p&gt;
    &lt;p&gt;We’ll write a new &lt;code&gt;init&lt;/code&gt;, but let’s also make our &lt;code&gt;initramfs&lt;/code&gt; a little more complex too. Let’s remember how we said that &lt;code&gt;init&lt;/code&gt; starts up all the other processes on the machine. Wouldn’t it be nice if we actually had some sort of a shell? After all, that’s what we typically have with Linux — shells go well with Linux. We’ll build a useless shell, the one that just tells us what we asked it to do (echoes back the input).&lt;/p&gt;
    &lt;p&gt;Let’s first write the &lt;code&gt;init&lt;/code&gt; process. Before it begins looping and printing something every 10 seconds, it has an important job of spawning our “little shell”. The way a process can spawn another process in Linux is through 2 operations: &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;exec&lt;/code&gt;. &lt;code&gt;fork&lt;/code&gt; will start a new process by literally cloning the current process at the moment of &lt;code&gt;fork&lt;/code&gt;. The way the underlying code can differentiate the “parent” and “child” processes after that is by checking the return value of the &lt;code&gt;fork&lt;/code&gt; operation. If it is 0, this means the process is the child process, and it’s a parent otherwise (-1 is returned in an error case).&lt;/p&gt;
    &lt;p&gt;Next, it’s not useful for us here to just keep executing the &lt;code&gt;init&lt;/code&gt; program in 2 different processes. That’s where one of the many &lt;code&gt;exec&lt;/code&gt; operations come into the picture. When I say there are many &lt;code&gt;exec&lt;/code&gt; operations available on Linux, I mean there are &lt;code&gt;execl&lt;/code&gt;, &lt;code&gt;execlp&lt;/code&gt;, &lt;code&gt;execle&lt;/code&gt;, etc. Take a look at more documentation here, please. We’re going with &lt;code&gt;execl&lt;/code&gt; here, and the first parameter is which binary do we want to launch. We’ll package our fake shell as the &lt;code&gt;little_shell&lt;/code&gt; binary on the root. The rest of the parameters do not really matter (as evidenced by the value of the second parameter). More important, the mechanism of this operation is that we’re calling into the kernel to take whatever is running in the current process and replace it with the program that is loaded for execution from the binary listed as the first parameter. This is how programs get launched on Linux and when you’re working in your Bash shell, and you end up launching a program, this is what happens — a sequence of &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;exec&lt;/code&gt;-style calls.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main(int argc, char *argv[]) {
  pid_t pid = fork();

  if (pid == -1) {
    printf("Unable to fork!");
    return -1;
  }

  if (pid == 0) {
    // This is a child process.
    int status = execl("/little_shell", "irrelevant", NULL);

    if (status == -1) {
      printf("Forked process cannot start the little_shell");
      return -2;
    }
  }

  int count = 1;

  while (1) {
    printf("Hello from the original init! %d\n", count);
    count++;
    sleep(10);
  }

  return 0;
}&lt;/code&gt;
    &lt;p&gt;We build the &lt;code&gt;init&lt;/code&gt; the same way as we did before:&lt;/p&gt;
    &lt;code&gt;riscv64-linux-gnu-gcc -static -o init init.c&lt;/code&gt;
    &lt;p&gt;For the “shell” we’re building, I want to get a little more creative. Why don’t we write this one in Go instead of old school C?&lt;/p&gt;
    &lt;code&gt;package main

import (
	"bufio"
	"fmt"
	"os"
)

func main() {
	fmt.Println("Hello world from Go!")

	reader := bufio.NewReader(os.Stdin)

	for {
		fmt.Print("Enter your command: ")
		line, _ := reader.ReadString('\n')
		fmt.Printf("Your command is: %s", line)
	}
}&lt;/code&gt;
    &lt;p&gt;I am able to cross compile this to RISC-V out-of-the-box with my &lt;code&gt;go&lt;/code&gt; compiler.&lt;/p&gt;
    &lt;code&gt;GOOS=linux GOARCH=riscv64 go build little_shell.go&lt;/code&gt;
    &lt;p&gt;Nice thing that I really like about Go is that it’s very easy to reference other remote repositories on GitHub to include libraries, and things get neatly packaged up statically. I’m not going to lie, the &lt;code&gt;little_shell&lt;/code&gt; Go binary is pretty thick, weighing in at 1.9M on my machine, compared to only 454K for the statically-linked simple init, but in the days of desktops/laptops/phones with hundreds of GB of storage, if you’re building a distro for these kinds of devices, you may want to consider the tradeoff.&lt;/p&gt;
    &lt;p&gt;Note, there are situations where you may not be able to simply run your Go binary just like that on top of a bare kernel, it could start throwing Go panics all over the place. In order to run Go, you need to build your kernel with the right features in it, futex support feature being one of them (I think I’ve identified only 2 in my past experience). If you encounter any problems running the Go applications and you suspect you may not have the right kernel support, carefully read through the panics and you will be able to identify what is missing. Good news here is that the default config for the RISC-V kernel is good enough for running Go.&lt;/p&gt;
    &lt;p&gt;Let’s update our &lt;code&gt;file_list.txt&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;init
little_shell&lt;/code&gt;
    &lt;p&gt;Pack it all up again:&lt;/p&gt;
    &lt;code&gt;cpio -o -H newc &amp;lt; file_list.txt &amp;gt; initramfs.cpio&lt;/code&gt;
    &lt;p&gt;Let’s run it!&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /PATH/TO/NEWLY_BUILT/initramfs.cpio&lt;/code&gt;
    &lt;code&gt;[    0.356314] Run /init as init process
Hello from the original init! 1
Hello world from Go!
Enter your command: [[[mkdir hello]]]
Your command is: mkdir hello
Enter your command: [[[ls]]]
Your command is: ls
Enter your command: Hello from the original init! 2
[[[echo 123]]
Your command is: echo 123
Enter your command: [[[exit]]]
Your command is: exit
Enter your command: Hello from the original init! 3
[[[I give up!]]]
Your command is: I give up!&lt;/code&gt;
    &lt;p&gt;The bits in this console excerpt enclosed with triple square brackets are my user-provided input over UART. You can see 3 things interleaved on the UART&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Original &lt;code&gt;init&lt;/code&gt;’s period output every 10 seconds.&lt;/item&gt;
      &lt;item&gt;Output from the &lt;code&gt;little_shell&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Input from the user.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are using the sole UART device on the virtual machine for all this, but that is not the only reason why everything is mixed up here. &lt;code&gt;init&lt;/code&gt; process prints to the standard output, just like &lt;code&gt;little_shell&lt;/code&gt; does, and you may not be aware of it, but any sort of print on Linux is a print to an open file. Standard output, as far as Linux knows, is a file that is opened by a process and you are printing to the standard output by writing to that file. When we &lt;code&gt;fork&lt;/code&gt;-ed the &lt;code&gt;little_shell&lt;/code&gt; from &lt;code&gt;init&lt;/code&gt;, the &lt;code&gt;little_shell&lt;/code&gt; inherited the open files from &lt;code&gt;init&lt;/code&gt;. So they are literally sharing all the standard input and output streams. Even if we had multiple I/O devices that we used on this machine, they’d still be sending outputs over to the same output stream. When &lt;code&gt;init&lt;/code&gt; was started, its standard output was set to produce content over to UART, and this behavior was simply inherited by the &lt;code&gt;little_shell&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And there we have it, we have a pretty useless, but home-made Linux distribution! Go ahead and send it over to your friends! :)&lt;/p&gt;
    &lt;p&gt;Jokes aside, you can make an exercise out of this and implement some sort of a mini shell out of this &lt;code&gt;little_shell&lt;/code&gt;. Instead of just echoing back the commands given to it, you could make it actually understand what &lt;code&gt;mkdir&lt;/code&gt; is. You can even have it fork off a process to execute that elsewhere. Sky is the limit, you’re in the Linux userspace!&lt;/p&gt;
    &lt;p&gt;Let’s just step back a little and see if Linux kernel achieved the initial few promises for us:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;It’s abstracting away the hardware. Our&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;and our shell didn’t know anything about the UART. All they knew was they’re writing to some Linux file handle. It happens to be mapped to something abstract in the Linux kernel that invokes the UART driver in the Linux kernel, which may or may not use the SBI under the hood (I have honestly not verified if the kernel removes its dependence on SBI after it boots).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;It offers some high-level programming paradigms, like filesystems. Our&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;process located the other binary through the filesystem (the path was trivial, the binary was right in the root, but still, the paradigm is there).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There is a pretty clean isolation between the processes running. Once the shell was forked off from the&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;, the processes were basically running independently. The memory was not shared between them and they didn’t have to worry about each other’s memory layout. They did share something else, though, like the file handles, but this is a consequence of how they were launched into running. Linux enables you to actually change some of this behavior, e.g. you can set up some shared memory between the processes, if you explicitly want to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are many other things the kernel does for us, but let’s just stop here for now and appreciate this. It may not look like a lot, but the kernel gives us a pretty solid, portable infrastructure with which we can develop high level software while often disregarding the complexities of the underlying machine.&lt;/p&gt;
    &lt;head rend="h3"&gt;So what is an operating system?&lt;/head&gt;
    &lt;p&gt;This is now a game of words in my opinion. In my view, what matters is that the reader now has an understanding of what Linux as the kernel is, what “infrastructure” it offers, and what is running in the user space and what is running in the kernel space.&lt;/p&gt;
    &lt;p&gt;Some people may call the kernel itself an operating system, some people will refer to the whole distribution as the operating system, or they may come up with something completely different. I hope that at this point you have a good understanding of what is happening on a machine once Linux is started and where the responsibilities of each component end (or you can at least imagine the boundaries on a more complex system).&lt;/p&gt;
    &lt;p&gt;I hope this was useful!&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus section: making an actually useful micro distribution with &lt;code&gt;u-root&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;I thought about wrapping up here, but it wouldn’t make for a flashy demo. Why don’t we instead boot into something that’s actually useful, meaning that you can do things you would typically do on a Linux-based system, like run your &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;mkdir&lt;/code&gt;, &lt;code&gt;echo&lt;/code&gt; and whatnot. Let’s stick with the kernel we have previously built, and add some useful “infrastructure on top of infrastructure” in the user space domain to make the whole machine more useful.&lt;/p&gt;
    &lt;p&gt;I really like the u-root project for this.&lt;/p&gt;
    &lt;p&gt;Note: The title of their project mentions Go bootloaders, and this may stump you because as a careful reader, you know that Go programs are not really something you can run on bare metal. These bootloaders are somewhat exotic userspace bootloaders, meaning that they will actually run on top of a live Linux kernel, and then use this amazing Linux mechanism called &lt;code&gt;kexec&lt;/code&gt; to re-load a different kernel into the memory from user space. We won’t be using these bootloaders for now, we’ll just focus on the other user space goodies they have available, but I thought a quick paragraph here would help the confused readers.&lt;/p&gt;
    &lt;p&gt;The reason why I like the &lt;code&gt;u-root&lt;/code&gt; project is because it’s so insanely easy to use. Its usage is a bit creative though, so there are really 2 steps here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install &lt;code&gt;u-root&lt;/code&gt;per their instructions. You should end up with a&lt;code&gt;u-root&lt;/code&gt;binary in your&lt;code&gt;PATH&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Now to actually generate a functional &lt;code&gt;initramfs&lt;/code&gt;with&lt;code&gt;u-root&lt;/code&gt;, the easiest way is to clone their Git repo and&lt;code&gt;cd&lt;/code&gt;your way into the directory that you just cloned. From there, you can cross-compile a fully functional user space set of tools with a single command.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/u-root/u-root.git
cd u-root
GOOS=linux GOARCH=riscv64 u-root&lt;/code&gt;
    &lt;p&gt;I get a few lines of output, the last being:&lt;/p&gt;
    &lt;code&gt;18:31:31 Successfully built "/tmp/initramfs.linux_riscv64.cpio" (size 14827284).&lt;/code&gt;
    &lt;p&gt;And that’s really it, this &lt;code&gt;cpio&lt;/code&gt; file can now be just ran with QEMU and you’ll boot right into a shell! Go through the &lt;code&gt;u-root&lt;/code&gt; documentation to understand how you can customize this &lt;code&gt;initramfs&lt;/code&gt; image you get, including what sort of changes you can make to the &lt;code&gt;init&lt;/code&gt; process behavior, but I think the default setup is so amazing to explore with.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /tmp/initramfs.linux_riscv64.cpio&lt;/code&gt;
    &lt;p&gt;Wow, this booted really smoothly! Providing the bottom of the UART output.&lt;/p&gt;
    &lt;code&gt;[    0.400269] Run /init as init process
2023/09/12 01:34:33 Welcome to u-root!
                              _
   _   _      _ __ ___   ___ | |_
  | | | |____| '__/ _ \ / _ \| __|
  | |_| |____| | | (_) | (_) | |_
   \__,_|    |_|  \___/ \___/ \__|
&lt;/code&gt;
    &lt;p&gt;And as you can see by the little &lt;code&gt;/#&lt;/code&gt; prompt, you’re actually in a shell! &lt;code&gt;u-root&lt;/code&gt;’s &lt;code&gt;init&lt;/code&gt; forked off a shell process and gave it the control over the UART.&lt;/p&gt;
    &lt;code&gt;/# ls
bbin
bin
buildbin
dev
env
etc
go
init
lib
lib64
proc
root
sys
tcz
tmp
ubin
usr
var
/# pwd
/
/# echo "Hello world!"
Hello world!&lt;/code&gt;
    &lt;p&gt;This little shell that &lt;code&gt;u-root&lt;/code&gt; gives even supports Tab-completion! I will say I have encountered some hiccups occassionally with it, it’s definitely not your full blown Bash, but it’s more than just a toy.&lt;/p&gt;
    &lt;p&gt;The standard tools like &lt;code&gt;ls&lt;/code&gt; seem to be taking the standard flags:&lt;/p&gt;
    &lt;code&gt;/# ls -lah
dtrwxrwxrwx root 0 420 B  Sep 12 01:35 .
drwxr-xr-x  root 0 2.1 kB Jan  1 00:00 bbin
drwxr-xr-x  root 0 80 B   Jan  1 00:00 bin
drwxrwxrwx  root 0 40 B   Sep 12 01:34 buildbin
drwxr-xr-x  root 0 12 kB  Sep 12 01:34 dev
drwxr-xr-x  root 0 40 B   Sep 12 01:35 directory
drwxr-xr-x  root 0 40 B   Jan  1 00:00 env
drwxr-xr-x  root 0 80 B   Sep 12 01:34 etc
drwxrwxrwx  root 0 60 B   Sep 12 01:34 go
Lrwxrwxrwx  root 0 9 B    Jan  1 00:00 init -&amp;gt; bbin/init
drwxrwxrwx  root 0 40 B   Sep 12 01:34 lib
drwxr-xr-x  root 0 40 B   Jan  1 00:00 lib64
dr-xr-xr-x  root 0 0 B    Sep 12 01:34 proc
drwx------  root 0 40 B   Sep 11 07:43 root
dr-xr-xr-x  root 0 0 B    Sep 12 01:34 sys
drwxr-xr-x  root 0 40 B   Jan  1 00:00 tcz
dtrwxrwxrwx root 0 60 B   Sep 12 01:34 tmp
drwxr-xr-x  root 0 40 B   Jan  1 00:00 ubin
drwxr-xr-x  root 0 60 B   Jan  1 00:00 usr
drwxr-xr-x  root 0 60 B   Jan  1 00:00 var&lt;/code&gt;
    &lt;head rend="h3"&gt;Visit google.com from this!&lt;/head&gt;
    &lt;p&gt;One last flashy thing — let’s connect to google.com from this VM with our custom user-land!&lt;/p&gt;
    &lt;p&gt;First, we need to attach a network device. We add &lt;code&gt;-device virtio-net-device,netdev=usernet -netdev user,id=usernet,hostfwd=tcp::10000-:22&lt;/code&gt; to our QEMU CLI. I think the last 2 numbers do not really matter as we won’t be SSH’ing into this machine (maybe you can do that exercise yourself, but I’m afraid it won’t be easy). The default kernel build should indeed bake in the &lt;code&gt;virtio&lt;/code&gt; network device drivers, so this should more or less just work.&lt;/p&gt;
    &lt;p&gt;We’ll need a working IP address, and we’ll use something from &lt;code&gt;u-root&lt;/code&gt; to obtain it. That something requires 3 things present in the kernel config: &lt;code&gt;CONFIG_VIRTIO_PCI&lt;/code&gt;, &lt;code&gt;CONFIG_HW_RANDOM_VIRTIO&lt;/code&gt; and &lt;code&gt;CONFIG_CRYPTO_DEV_VIRTIO&lt;/code&gt;. My default settings for the kernel have all that flipped to &lt;code&gt;y&lt;/code&gt;, so I’m good to go and you should be too, but you can double check just in case. If you have changed any kernel settings, please rebuild the kernel image.&lt;/p&gt;
    &lt;p&gt;Finally, we need to attach an RNG (doesn’t matter what it is) device to our QEMU machine so we can obtain our IP address. We simply add &lt;code&gt;-device virtio-rng-pci&lt;/code&gt; to our QEMU CLI.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /tmp/initramfs.linux_riscv64.cpio -device virtio-net-device,netdev=usernet -netdev user,id=usernet,hostfwd=tcp::10000-:22 -device virtio-rng-pci&lt;/code&gt;
    &lt;p&gt;Once we’re in, we can run &lt;code&gt;ip addr&lt;/code&gt; to see what’s our IP address.&lt;/p&gt;
    &lt;code&gt;/# ip addr
1: lo: &amp;lt;UP,LOOPBACK&amp;gt; mtu 65536 state UNKNOWN
    link/loopback
    inet 127.0.0.1 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 state DOWN
    link/ether 52:54:00:12:34:56
3: sit0: &amp;lt;0&amp;gt; mtu 1480 state DOWN
    link/sit&lt;/code&gt;
    &lt;p&gt;Our Ethernet is not set up. Let’s enable IPv4 networking (we don’t need 6). In this little setup, QEMU is running a virtualized network and it embeds a little DHCP server which can dynamically assign IPs (documentation is here). So let’s run a DHCP helper from &lt;code&gt;u-root&lt;/code&gt; for this by running&lt;/p&gt;
    &lt;code&gt;dhclient -ipv6=false&lt;/code&gt;
    &lt;p&gt;The output I got was the following:&lt;/p&gt;
    &lt;code&gt;2023/09/12 03:46:59 Bringing up interface eth0...
2023/09/12 03:47:00 Attempting to get DHCPv4 lease on eth0
2023/09/12 03:47:00 Got DHCPv4 lease on eth0: DHCPv4 Message
  opcode: BootReply
  hwtype: Ethernet
  hopcount: 0
  transaction ID: 0x05f008e1
  num seconds: 0
  flags: Unicast (0x00)
  client IP: 0.0.0.0
  your IP: 10.0.2.15
  server IP: 10.0.2.2
  gateway IP: 0.0.0.0
  client MAC: 52:54:00:12:34:56
  server hostname:
  bootfile name:
  options:
    Subnet Mask: ffffff00
    Router: 10.0.2.2
    Domain Name Server: 10.0.2.3
    IP Addresses Lease Time: 24h0m0s
    DHCP Message Type: ACK
    Server Identifier: 10.0.2.2
2023/09/12 03:47:00 Configured eth0 with IPv4 DHCP Lease IP 10.0.2.15/24
2023/09/12 03:47:00 Finished trying to configure all interfaces.&lt;/code&gt;
    &lt;p&gt;The QEMU documentation will tell you why pinging won’t work, so let’s not bother with pinging. Let’s just “visit” google.com!&lt;/p&gt;
    &lt;code&gt;wget http://google.com&lt;/code&gt;
    &lt;p&gt;You can now read the downloaded &lt;code&gt;index.html&lt;/code&gt; file!&lt;/p&gt;
    &lt;code&gt;cat index.html&lt;/code&gt;
    &lt;p&gt;You’ll get a lot of obfuscated JavaScript, but this is great! It means we have successfully visited google.com through &lt;code&gt;wget&lt;/code&gt;! I hope this sparks your imagination to do some other cool things with &lt;code&gt;u-root&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Package managers&lt;/head&gt;
    &lt;p&gt;You might intuitively understand at this point that some of the most important software of a Linux distro is the package manager. It’s really the gateway to getting the functionality on your machine that you need. What we went through here is more of an embedded flow: we generated these somewhat monolithic software images and if we want to update something, we rebuild the whole image and re-image the device. This doesn’t work for desktops, phones, etc. Package managers are there to update, add or remove the software on our machines. We won’t be talking about them here, just giving them a brief shoutout and you can hopefully imagine from the high level how they work and what do they do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The monster of &lt;code&gt;init&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;init&lt;/code&gt; we created is definitely just a toy, and in the end it just started some sort of a shell. However, make no mistake about it, &lt;code&gt;init&lt;/code&gt; is an incredibly important thing on a Linux system and getting it right is a science. You’ll see a lot of strong opinions on different &lt;code&gt;init&lt;/code&gt; systems for Linux online. &lt;code&gt;init&lt;/code&gt; doesn’t usually just spawn one process off and call it a day, it can set up a whole bunch of things like different devices, for example. As an exercise, just run &lt;code&gt;ls /dev&lt;/code&gt; from your &lt;code&gt;u-root&lt;/code&gt;-based build and see all those devices set up. A lot of them come from the &lt;code&gt;init&lt;/code&gt;’s setup and many are extremely useful. You can then read some of the &lt;code&gt;u-root&lt;/code&gt; source code to see what’s going on there in &lt;code&gt;init&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The code for this guide is available here, where you can just sync and build the &lt;code&gt;initramfs&lt;/code&gt; images.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45703556</guid><pubDate>Sat, 25 Oct 2025 13:01:39 +0000</pubDate></item><item><title>TigerBeetle and Synadia pledge $512k to the Zig Software Foundation</title><link>https://tigerbeetle.com/blog/2025-10-25-synadia-and-tigerbeetle-pledge-512k-to-the-zig-software-foundation/#blog-post</link><description>&lt;doc fingerprint="bf70d8bf88a3ab96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Synadia and TigerBeetle Pledge $512,000 to the Zig Software Foundation&lt;/head&gt;
    &lt;p&gt;Synadia and TigerBeetle have together pledged $512,000 to the Zig Software Foundation over the next two years in support of the language, leadership, and communities building the future of simpler systems software.&lt;/p&gt;
    &lt;p&gt;I first saw Zig in 2018, seven years ago. Two years later, I chose Zig over C or Rust for TigerBeetle.&lt;/p&gt;
    &lt;p&gt;In 2020, I was following Rust closely. At the time, Rustâs default memory philosophy was to crash when out of memory (OOM). However, for TigerBeetle, I wanted explicit static allocation, following NASAâs Power of Ten Rules for Safety-Critical Code, which would become a part of TigerStyle, a methodology for creating safer software in less time.&lt;/p&gt;
    &lt;p&gt;What I learned is that if you could centralize resource allocation in time and space (the dimensions that prove tricky for humans writing software) then this could not only simplify memory management, to design away some of the need for a borrow checker in the first place, but, more importantly, also be a forcing function for propagating good design, to encourage teams to think through the explicit limits or physics of the software (you have no choice).&lt;/p&gt;
    &lt;p&gt;From a performance perspective, I didnât want TigerBeetle to be fearlessly multithreaded. Transaction processing workloads tend to have inherent contention, even to the point of power law, precluding partitioning and necessitating a single-threaded architecture. Therefore, Rustâs borrow checker, while a phenomenal tool for the class of problems it targets, made less sense for TigerBeetle. TigerBeetle never frees memory and never runs multithreaded, instead using explicit submission/completion queue interfaces by design.&lt;/p&gt;
    &lt;p&gt;Finally, while the borrow checker could achieve local memory safety, TigerBeetle needed more correctness properties. TigerBeetle needed to be always correct, and across literally thousands of invariants. As matklad would say, this is a harder problem! I had also spent enough time in memory safe languages to know that local memory safety is no guarantee of local correctness, let alone distributed system correctness. Per systems thinking, I believe that total correctness is a design problem, not a language problem. Language is valuable. But no human language can guarantee the next Hemingway or Nabokov. For this you need philosophy. Even then itâs not a guarantee but a probability.&lt;/p&gt;
    &lt;p&gt;With Rust off the table, the choice fell to C or Zig. A language of the past or future?&lt;/p&gt;
    &lt;p&gt;Zig was early, which gave me pause, but I felt that the quality of Andrew Kelleyâs design decisions in the language, the standard library (e.g. the unmanaged hashmap interface) and the cross-compilation toolchain, even five years ago, was already exceptional.&lt;/p&gt;
    &lt;p&gt;Andrewsâs philosophy resonated with what I wanted to explore in TigerStyle. No hidden memory allocations. No hidden control flow. No preprocessor. No macros. And then you get things like comptime, reducing the grammar and dimensionality of the language, while simultaneously multiplying its power. The primary benefit of Zig is the favorable ratio of expressivity to language complexity.&lt;/p&gt;
    &lt;p&gt;As a replacement for C, Zig fixed not only the small cuts, such as explicit alignment in the type system for Direct I/O, or safer casts, but the showstoppers of spatial memory safety through bounds checking, and, to a lesser degree (but not guarantee), temporal memory safety through the debug allocator.&lt;/p&gt;
    &lt;p&gt;Zig also enabled checked arithmetic by default in safe builds, which is something I believe only Ada and Swift do (remarkably, Rust disables checked arithmetic by default in safe buildsâa default I would love to see changed). TigerBeetle separates the data plane from the control plane by design, through batching, so the runtime cost of these safety checks was not material, being amortized in the data plane across bigger buffers. While a borrow checker or static allocation can simplify memory management, getting logic and arithmetic correct remains hard. Of course, you can enable checked arithmetic in other languages, but I appreciated Andrewâs concern for checked arithmetic and stricter operands by default.&lt;/p&gt;
    &lt;p&gt;In all these things, what impressed me most was Zigâs approach to safety when working with the metal. Not in terms of an on/off decision, but as a spectrum. Not aiming for 100% guarantees across 1 or 2 categories, but 90% and then across more categories. Not eliminating classes of bugs, but downgrading their probability. All while preserving the power-to-weight ratio of the language, to keep the language beautifully simple.&lt;/p&gt;
    &lt;p&gt;Many languages start simple and grow complex as features are added. Zigâs simplicity is unusual in that it comes from a subtractive discipline (e.g. no private fields) rather than a deferred complexity; minimizing surface area is part of the ethos of the language. The simplicity of Zig meant that we could hire great programmers from any language backgroundâthey could pick up Zig in a weekend. Indeed, Iâve never had to talk to a new hire about learning Zig.&lt;/p&gt;
    &lt;p&gt;Finally, there was the timing. Recognizing that TigerBeetle would take time to reach production (we shipped production in 2024, after 3.5 years of development), giving Zig time to mature, for our trajectories to intersect.&lt;/p&gt;
    &lt;p&gt;Investing in creating a database like TigerBeetle is a long term effort. Databases tend to have a long half life (e.g. Postgres is 30 years old). And so, while Zig being early in 2020 did give me pause, nevertheless Zigâs quality, philosophy and simplicity made sense for a multi-decade horizon.&lt;/p&gt;
    &lt;p&gt;How has the decision for Zig panned out?&lt;/p&gt;
    &lt;p&gt;TigerBeetle is tested end-to-end under some pretty extreme fuzzing. We did have three bugs that would have been prevented by the borrow checker, but these were caught by our fuzzers and online verification. We run a fuzzing fleet of 1,000 dedicated CPU cores 24/7. We invest in deterministic simulation testing (e.g. VOPR), as well as non-deterministic fault-injection harnesses (e.g. VÃ¶rtex). We engaged Kyle Kingsbury in one of the longest Jepsen audits to dateâfour times the typical duration. Through all this, Zigâs quality held up flawlessly.&lt;/p&gt;
    &lt;p&gt;Zig has also been a huge part of our success as a company. TigerBeetle is only 5 years old but is already migrating some of the largest brokerages, exchanges and wealth managements in their respective jurisdictions. Several of our key enterprise contracts were thanks to the CTOs and even CEOs of these companies also following Zig and seeing the quality we wanted to achieve with it. I donât think we could have written TigerBeetle as it is, in any other language, at least not to the same tight tolerances, let alone with the same velocity.&lt;/p&gt;
    &lt;p&gt;Zigâs language specification will only reach 1.0 when all experimental areas of the language (e.g. async I/O) are finally done. For TigerBeetle, we care only about the stable language features we use, testing our binaries end to end, as we would for any language. Nevertheless, upgrading to new versions, even with breaking changes, has only been a pleasure for us as a team. The upgrade work is usually fully paid for by compilation time reduction. For example, the upgrade from Zig 0.14.1 to Zig 0.15.2 (with the native x86_64 backend) makes debug builds 2x faster, and even LLVM release builds become 1.6x faster. With each release, you can literally feel the sheer amount of effort that the entire Zig core team put into making Zig the worldâs most powerful programming languageâand toolchain.&lt;/p&gt;
    &lt;p&gt;Back in 2020, from a production perspective, Zig was more or less a frontend to LLVM, the same compiler used by Rust, Swift and other languages. However, by not shying away from also investing in its own independent compiler backends and toolchain, by appreciating the value of replacing LLVM long term, Zig is becoming well positioned to gain a low-level precision and compilation speed that generic LLVM wonât always be able to match.&lt;/p&gt;
    &lt;p&gt;We want Andrew to take his time, to get these things right for the long term. Fred Brooks once said that conceptual integrity is âthe most important considerationâ in system design, that the design must proceed from one mind.&lt;/p&gt;
    &lt;p&gt;In this spirit, I am grateful for Andrewâs remarkably strong leadership (and taste) in the design of the language and toolchain. There can be thankless pressure on an open source project to give in to the tragedy of the commons. But if anything, in hindsight I think this is what Iâve most appreciated about choosing Zig for TigerBeetle, that Zig has a strong BDFL.&lt;/p&gt;
    &lt;p&gt;Of course, some may hear âBDFLâ and see governance risk. But I fear the opposite: conceptual risk, the harder problem. Brooks was rightâconceptual integrity is almost certainly doomed by committee. Whereas governance is easier solved: put it in the hands, not of the corporates, but of the people. The individuals who choose each day to continue to donate.&lt;/p&gt;
    &lt;p&gt;This is why our pledge today, along with all other ZSF donors, is a simple donation with no strings attached. The Zig Software Foundation is well managed, transparent and independent. We want it to remain this way. The last thing we want is some kind of foundation âseatâ. Andrew is Chef. We want to let him cook, and pay his core team sustainably (e.g. 92% percent of budget goes to directly paying contributors).&lt;/p&gt;
    &lt;p&gt;If cooking is one metaphor, then surfing is another. I believe that technology moves in waves. The art is not in paddling to the wave with a thousand surfers on it. But in spotting the swell before it breaks. And then enjoying the ride with the early adopters who did the same. River, Ghostty, Bun, Mach and many fellow surfers.&lt;/p&gt;
    &lt;p&gt;In fact, it was through Zig that I met Derek Collison, who like me had been sponsoring the language in his personal capacity since 2018. As a former CTO at VMware, Derek was responsible for backing antirez to work full time on Redis. Derek later went on to create NATS, founding Synadia.&lt;/p&gt;
    &lt;p&gt;As we were about to increase TigerBeetleâs yearly donation to Zig, I reached out to Derek, and we decided to do a joint announcement, following Mitchell Hashimotoâs lead. For each of our companies to donate $256,000 in monthly installments over the next two years, with Synadia matching TigerBeetle, for a total of $512,000âthe first installment already made.&lt;/p&gt;
    &lt;p&gt;Please consider donating or increasing your donation if you can. And if you are a CEO or CTO, please team up with another company to outmatch us! Thanks Andrew for creating something special, and to all who code for the joy of the craft:&lt;/p&gt;
    &lt;p&gt;Together we serve the users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45703926</guid><pubDate>Sat, 25 Oct 2025 13:54:33 +0000</pubDate></item><item><title>Against SQL (2021)</title><link>https://www.scattered-thoughts.net/writing/against-sql/</link><description>&lt;doc fingerprint="f7281873efb9704c"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;TLDR&lt;/head&gt;
    &lt;p&gt;The relational model is great:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A shared universal data model allows cooperation between programs written in many different languages, running on different machines and with different lifespans.&lt;/item&gt;
      &lt;item&gt;Normalization allows updating data without worrying about forgetting to update derived data.&lt;/item&gt;
      &lt;item&gt;Physical data independence allows changing data-structures and query plans without having to change all of your queries.&lt;/item&gt;
      &lt;item&gt;Declarative constraints clearly communicate application invariants and are automatically enforced.&lt;/item&gt;
      &lt;item&gt;Unlike imperative languages, relational query languages don't have false data dependencies created by loop counters and aliasable pointers. This makes relational languages: &lt;list rend="ul"&gt;&lt;item&gt;A good match for modern machines. Data can be rearranged for more compact layouts, even automatic compression. Operations can be reordered for high cache locality, pipeline-friendly hot loops, simd etc.&lt;/item&gt;&lt;item&gt;Amenable to automatic parallelization.&lt;/item&gt;&lt;item&gt;Amenable to incremental maintenance.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But SQL is the only widely-used implementation of the relational model, and it is:&lt;/p&gt;
    &lt;p&gt;This isn't just a matter of some constant programmer overhead, like SQL queries taking 20% longer to write. The fact that these issues exist in our dominant model for accessing data has dramatic downstream effects for the entire industry:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complexity is a massive drag on quality and innovation in runtime and tooling&lt;/item&gt;
      &lt;item&gt;The need for an application layer with hand-written coordination between database and client renders useless most of the best features of relational databases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The core message that I want people to take away is that there is potentially a huge amount of value to be unlocked by replacing SQL, and more generally in rethinking where and how we draw the lines between databases, query languages and programming languages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inexpressive&lt;/head&gt;
    &lt;p&gt;Talking about expressiveness is usually difficult, since it's a very subjective measure. But SQL is a particularly inexpressive language. Many simple types and computations can't be expressed at all. Others require far more typing than they need to. And often the structure is fragile - small changes to the computation can require large changes to the code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can't be expressed&lt;/head&gt;
    &lt;p&gt;Let's start with the easiest examples - things that can't be expressed in SQL at all.&lt;/p&gt;
    &lt;p&gt;For example, SQL:2016 added support for json values. In most languages json support is provided by a library. Why did SQL have to add it to the language spec?&lt;/p&gt;
    &lt;p&gt;First, while SQL allows user-defined types, it doesn't have any concept of a sum type. So there is no way for a user to define the type of an arbitrary json value:&lt;/p&gt;
    &lt;code&gt;enum Json {
    Null,
    Bool(bool),
    Number(Number),
    String(String),
    Array(Vec&amp;lt;Value&amp;gt;),
    Object(Map&amp;lt;String, Value&amp;gt;),
}
&lt;/code&gt;
    &lt;p&gt;The usual response to complaints about the lack of sum types in sql is that you should use an id column that joins against multiple tables, one for each possible type.&lt;/p&gt;
    &lt;code&gt;create table json_value(id integer);
create table json_bool(id integer, value bool)
create table json_number(id integer, value double);
create table json_string(id integer, value text);
create table json_array(id integer);
create table json_array_elements(id integer, position integer, value json_value, foreign key (value) references json_value(id));
create table json_object(id integer);
create table json_object_properties(id integer, key text, value json_value, foreign key (value) references json_value(id));
&lt;/code&gt;
    &lt;p&gt;This works for data modelling (although it's still clunky because you must try joins against each of the tables at every use site rather than just ask the value which table it refers to). But this solution is clearly inappropriate for modelling a value like json that can be created inside scalar expressions, where inserts into some global table are not allowed.&lt;/p&gt;
    &lt;p&gt;Second, parsing json requires iteration. SQLs &lt;code&gt;with recursive&lt;/code&gt; is limited to linear recursion and has a bizarre choice of semantics - each step can access only the results from the previous step, but the result of the whole thing is the union of all the steps. This makes parsing, and especially backtracking, difficult. Most SQL databases also have a procedural sublanguage that has explicit iteration, but there are few commonalities between the languages in different databases. So there is no pure-SQL json parser that works across different databases.&lt;/p&gt;
    &lt;p&gt;Third, most databases have some kind of extension system that allows adding new types and functions using a regular programming language (usually c). Indeed, this is how json support first appeared in many databases. But again these extension systems are not at all standardized so it's not feasible to write a library that works across many databases.&lt;/p&gt;
    &lt;p&gt;So instead the best we can do is add json to the SQL spec and hope that all the databases implement it in a compatible way (they don't).&lt;/p&gt;
    &lt;p&gt;The same goes for xml, regular expressions, windows, multi-dimensional arrays, periods etc.&lt;/p&gt;
    &lt;p&gt;Compare how flink exposes windowing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The interface is made out of objects and function calls, both of which are first-class values and can be stored in variables and passed as function arguments.&lt;/item&gt;
      &lt;item&gt;The style of windowing is defined by a WindowAssigner which simply takes a row and returns a set of window ids.&lt;/item&gt;
      &lt;item&gt;Several common styles of windows are provided as library code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vs SQL:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The interface adds a substantial amount of new syntax to the language.&lt;/item&gt;
      &lt;item&gt;The windowing style is purely syntactic - it is not a value that can be assigned to a variable or passed to a function. This means that we can't compress common windowing patterns.&lt;/item&gt;
      &lt;item&gt;Only a few styles of windowing are provided and they are hard-coded into the language.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why is the SQL interface defined this way?&lt;/p&gt;
    &lt;p&gt;Much of this is simply cultural - this is just how new SQL features are designed.&lt;/p&gt;
    &lt;p&gt;But even if we wanted to mimic the flink interface we couldn't do it in SQL.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Functions are not values that can be passed around, and they can't take tables or other functions as arguments. So complex operations such as windowing can't be added as stdlib functions.&lt;/item&gt;
      &lt;item&gt;Without sum types we can't even express the hardcoded windowing styles as a value. So we're forced to add new syntax whenever we want to parameterize some operation with several options.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Verbose to express&lt;/head&gt;
    &lt;p&gt;Joins are at the heart of the relational model. SQL's syntax is not unreasonable in the most general case, but there are many repeated join patterns that deserve more concise expression.&lt;/p&gt;
    &lt;p&gt;By far the most common case for joins is following foreign keys. SQL has no special syntax for this:&lt;/p&gt;
    &lt;code&gt;select foo.id, quux.value 
from foo, bar, quux 
where foo.bar_id = bar.id and bar.quux_id = quux.id
&lt;/code&gt;
    &lt;p&gt;Compare to eg alloy, which has a dedicated syntax for this case:&lt;/p&gt;
    &lt;code&gt;foo.bar.quux
&lt;/code&gt;
    &lt;p&gt;Or libraries like pandas or flink, where it's trivial to write a function that encapsulates this logic:&lt;/p&gt;
    &lt;code&gt;fk_join(foo, 'bar_id', bar, 'quux_id', quux)
&lt;/code&gt;
    &lt;p&gt;Can we write such a function in sql? Most databases don't allow functions to take tables as arguments, and also require the column names and types of the input and output tables to be fixed when the function is defined. SQL:2016 introduced polymorphic table functions, which might allow writing something like &lt;code&gt;fk_join&lt;/code&gt; but so far only oracle has implemented them (and they didn't follow the spec!).&lt;/p&gt;
    &lt;p&gt;Verbose syntax for such core operations has chilling effects downstream, such as developers avoiding 6NF even in situations where it's useful, because all their queries would balloon in size.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fragile structure&lt;/head&gt;
    &lt;p&gt;There are many cases where a small change to a computation requires totally changing the structure of the query, but subqueries are my favourite because they're the most obvious way to express many queries and yet also provide so many cliffs to fall off.&lt;/p&gt;
    &lt;code&gt;-- for each manager, find their employee with the highest salary
&amp;gt; select
&amp;gt;   manager.name,
&amp;gt;   (select employee.name
&amp;gt;    from employee
&amp;gt;    where employee.manager = manager.name
&amp;gt;    order by employee.salary desc
&amp;gt;    limit 1)
&amp;gt; from manager;
 name  | name
-------+------
 alice | bob
(1 row)

-- what if we want to return the salary too?
&amp;gt; select
&amp;gt;   manager.name,
&amp;gt;   (select employee.name, employee.salary
&amp;gt;    from employee
&amp;gt;    where employee.manager = manager.name
&amp;gt;    order by employee.salary desc
&amp;gt;    limit 1)
&amp;gt; from manager;
ERROR:  subquery must return only one column
LINE 3:   (select employee.name, employee.salary
          ^

-- the only solution is to change half of the lines in the query
&amp;gt; select manager.name, employee.name, employee.salary
&amp;gt; from manager
&amp;gt; join lateral (
&amp;gt;   select employee.name, employee.salary
&amp;gt;   from employee
&amp;gt;   where employee.manager = manager.name
&amp;gt;   order by employee.salary desc
&amp;gt;   limit 1
&amp;gt; ) as employee
&amp;gt; on true;
 name  | name | salary
-------+------+--------
 alice | bob  |    100
(1 row)
&lt;/code&gt;
    &lt;p&gt;This isn't terrible in such a simple example, but in analytics it's not uncommon to have to write queries that are hundreds of lines long and have many levels of nesting, at which point this kind of restructuring is laborious and error-prone.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incompressible&lt;/head&gt;
    &lt;p&gt;Code can be compressed by extracting similar structures from two or more sections. For example, if a calculation was used in several places we could assign it to a variable and then use the variable in those places. Or if the calculation depended on different inputs in each place, we could create a function and pass the different inputs as arguments.&lt;/p&gt;
    &lt;p&gt;This is programming 101 - variables, functions and expression substitution. How does SQL fare on this front?&lt;/p&gt;
    &lt;head rend="h3"&gt;Variables&lt;/head&gt;
    &lt;p&gt;Scalar values can be assigned to variables, but only as a column inside a relation. You can't name a thing without including it in the result! Which means that if you want a temporary scalar variable you must introduce a new &lt;code&gt;select&lt;/code&gt; to get rid off it. And also name all your other values.&lt;/p&gt;
    &lt;code&gt;-- repeated structure
select a+((z*2)-1), b+((z*2)-1) from foo;

-- compressed?
select a2, b2 from (select a+tmp as a2, b+tmp as b2, (z*2)-1 as tmp from foo);
&lt;/code&gt;
    &lt;p&gt;You can use &lt;code&gt;as&lt;/code&gt; to name scalar values anywhere they appear. Except in a &lt;code&gt;group by&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;-- can't name this value
&amp;gt; select x2 from foo group by x+1 as x2;
ERROR:  syntax error at or near "as"
LINE 1: select x2 from foo group by x+1 as x2;

-- sprinkle some more select on it
&amp;gt; select x2 from (select x+1 as x2 from foo) group by x2;
 ?column?
----------
(0 rows)
&lt;/code&gt;
    &lt;p&gt;Rather than fix this bizarre oversight, the SQL spec allows a novel form of variable naming - you can refer to a column by using an expression which produces the same parse tree as the one that produced the column.&lt;/p&gt;
    &lt;code&gt;-- this magically works, even though x is not in scope in the select
&amp;gt; select (x + 1)*2 from foo group by x+1;
 ?column?
----------
(0 rows)

-- but this doesn't, because it isn't the same parse tree
&amp;gt; select (x + +1)*2 from foo group by x+1;
ERROR:  column "foo.x" must appear in the GROUP BY clause or be used in an aggregate function
LINE 1: select (x + +1)*2 from foo group by x+1;
                ^
&lt;/code&gt;
    &lt;p&gt;Of course, you can't use this feature across any kind of syntactic boundary. If you wanted to, say, assign this table to a variable or pass it to a function, then you need to both repeat the expression and explicitly name it;&lt;/p&gt;
    &lt;code&gt;&amp;gt; with foo_plus as (select x+1 from foo group by x+1)
&amp;gt; select (x+1)*2 from foo_plus;
ERROR:  column "x" does not exist
LINE 2: select (x+1)*2 from foo_plus;
                ^
               
&amp;gt; with foo_plus as (select x+1 as x_plus from foo group by x+1)
&amp;gt; select x_plus*2 from foo_plus;
 ?column?
----------
(0 rows)
&lt;/code&gt;
    &lt;p&gt;SQL was first used in the early 70s, but if your repeated value was a table then you were out of luck until CTEs were added in SQL:99.&lt;/p&gt;
    &lt;code&gt;-- repeated structure
select * 
from 
  (select x, x+1 as x2 from foo) as foo1 
left join 
  (select x, x+1 as x2 from foo) as foo2 
on 
  foo1.x2 = foo2.x;
  
-- compressed?
with foo_plus as 
  (select x, x+1 as x2 from foo)
select * 
from 
  foo_plus as foo1 
left join 
  foo_plus as foo2 
on 
  foo1.x2 = foo2.x;
&lt;/code&gt;
    &lt;head rend="h3"&gt;Functions&lt;/head&gt;
    &lt;p&gt;Similarly, if your repeated calculations have different inputs then you were out of luck until scalar functions were added in SQL:99.&lt;/p&gt;
    &lt;code&gt;-- repeated structure
select a+((x*2)-1), b+((y*2)-1) from foo;

-- compressed?
create function bar(integer, integer) returns integer
    as 'select $1+(($2*2)-1);'
    language sql;
select bar(a,x), bar(b,y) from foo;
&lt;/code&gt;
    &lt;p&gt;Functions that return tables weren't added until SQL:2003.&lt;/p&gt;
    &lt;code&gt;-- repeated structure
(select x from foo)
union
(select x+1 from foo)
union
(select x+2 from foo)
union
(select x from bar)
union
(select x+1 from bar)
union
(select x+2 from bar);

-- compressed?
create function increments(integer) returns setof integer 
    as $$
        (select $1) 
        union 
        (select $1+1) 
        union 
        (select $1+2);
    $$
    language sql;
(select increments(x) from foo)
union
(select increments(x) from bar);
&lt;/code&gt;
    &lt;p&gt;What if you want to compress a repeated calculation that produces more than one table as a result? Tough!&lt;/p&gt;
    &lt;p&gt;What if you want to compress a repeated calculation where one of the inputs is a table? The spec doesn't explicitly disallow this, but it isn't widely supported. SQL server can do it with this lovely syntax:&lt;/p&gt;
    &lt;code&gt;-- compressed?

create type foo_like as table (x int);

create function increments(@foo foo_like readonly) returns table
    as return
        (select x from @foo) 
        union 
        (select x+1 from @foo) 
        union 
        (select x+2 from @foo);
        
declare @foo as foo_like;
insert into @foo select * from foo;

declare @bar as foo_like;
insert into @bar select * from bar;

increments(@foo) union increments(@bar);
&lt;/code&gt;
    &lt;p&gt;Aside from the weird insistence that we can't just pass a table directly to our function, this example points to a more general problem: column names are part of types. If in our example &lt;code&gt;bar&lt;/code&gt; happened to have a different column name then we would have had to write:&lt;/p&gt;
    &lt;code&gt;increments(@foo) union increments(select y as x from @bar)
&lt;/code&gt;
    &lt;p&gt;Since columns names aren't themselves first-class this makes it hard to compress repeated structure that happens to involve different names:&lt;/p&gt;
    &lt;code&gt;-- repeated structure
select a,b,c,x,y,z from foo order by a,b,c,x,y,z;

-- fantasy land
with ps as (columns 'a,b,c,x,y,z')
select $ps from foo order by $ps
&lt;/code&gt;
    &lt;p&gt;The same is true of windows, collations, string encodings, the part argument to &lt;code&gt;extract&lt;/code&gt; ... pretty much anything that involves one of the several hundred SQL keywords.&lt;/p&gt;
    &lt;p&gt;Functions and types are also not first-class, so repeated structures involving different functions or types can't be compressed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expression substitution&lt;/head&gt;
    &lt;p&gt;To be able to compress repeated structure we must be able to replace the verbose version with the compressed version. In many languages, there is a principle that it's always possible to replace any expression with another expression that has the same value. SQL breaks this principle in (at least) two ways.&lt;/p&gt;
    &lt;p&gt;Firstly, it's only possible to substitute one expression for another when they are both the same type of expression. SQL has statements (DDL), table expressions and scalar expressions.&lt;/p&gt;
    &lt;p&gt;Using a scalar expression inside a table expression requires first wrapping the entire thing with a new &lt;code&gt;select&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Using a table expression inside a scalar expression is generally not possible, unless the table expression returns only 1 column and either a) the table expression is guaranteed to return at most 1 row or b) your usage fits into one of the hard-coded patterns such as &lt;code&gt;exists&lt;/code&gt;. Otherwise, as we saw in the most-highly-paid-employee example earlier, it must be rewritten as a lateral join against the nearest enclosing table expression.&lt;/p&gt;
    &lt;p&gt;Secondly, table expressions aren't all made equal. Some table expressions depend not only on the value of an inner expression, but the syntax. For example:&lt;/p&gt;
    &lt;code&gt;-- this is fine - the spec allows `order by` to see inside the `(select ...)`
-- and make use of a column `y` that doesn't exist in the returned value
&amp;gt; (select x from foo) order by y;
 x
---
 3
(1 row)

-- same value in the inner expression
-- but the spec doesn't have a syntactic exception for this case
&amp;gt; (select x from (select x from foo) as foo2) order by y;
ERROR:  column "y" does not exist
LINE 1: (select x from (select x from foo) as foo2) order by y;
&lt;/code&gt;
    &lt;p&gt;In such cases it's not possible to compress repeated structure without first rewriting the query to explicitly select and then drop the magic column:&lt;/p&gt;
    &lt;code&gt;select x from ((select x,y from foo) order by y);
&lt;/code&gt;
    &lt;head rend="h2"&gt;Non-porous&lt;/head&gt;
    &lt;p&gt;I took the term 'porous' from Some Were Meant For C, where Stephen Kell argues that the endurance of c is down to it's extreme openness to interaction with other systems via foreign memory, FFI, dynamic linking etc. He contrasts this with managed languages which don't allow touching anything in the entire memory space without first notifying the GC, have their own internal notions of namespaces and linking which they don't expose to the outside world, have closed build systems which are hard to interface with other languages' build systems etc.&lt;/p&gt;
    &lt;p&gt;For non-porous languages to succeed they have to eat the whole world - gaining enough users that the entire programming ecosystem can be duplicated within their walled garden. But porous languages can easily interact with existing systems and make use of existing libraries and tools.&lt;/p&gt;
    &lt;p&gt;Whether or not you like this argument as applied to c, the notion of porousness itself is a useful lens for system design. When we apply it to SQL databases, we see that individual databases are often porous in many aspects of their design but the mechanisms are almost always not portable. So while individual databases can be extended in many ways, the extensions can't be shared between databases easily and the SQL spec is still left trying to eat the whole world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Language level&lt;/head&gt;
    &lt;p&gt;Most SQL databases have language-level escape hatches for defining new types and functions via a mature programming language (usually c). The syntax for declaring these in SQL is defined in the spec but the c interface and calling convention is not, so these are not portable across different databases.&lt;/p&gt;
    &lt;code&gt;-- sql side

CREATE FUNCTION add_one(integer) RETURNS integer
     AS 'DIRECTORY/funcs', 'add_one'
     LANGUAGE C STRICT;
&lt;/code&gt;
    &lt;code&gt;// c side

#include "postgres.h"
#include &amp;lt;string.h&amp;gt;
#include "fmgr.h"
#include "utils/geo_decls.h"

PG_MODULE_MAGIC;

PG_FUNCTION_INFO_V1(add_one);

Datum
add_one(PG_FUNCTION_ARGS)
{
    int32   arg = PG_GETARG_INT32(0);

    PG_RETURN_INT32(arg + 1);
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Runtime level&lt;/head&gt;
    &lt;p&gt;Many SQL databases also have runtime-level extension mechanisms for creating new index types and storage methods (eg postgis) and also for supplying hints to the optimizer. Again, these extensions are not portable across different implementations. At this level it's hard to see how they could be, as they can be deeply entangled with design decisions in the database runtime, but it's worth noting that if they were portable then much of the SQL spec would not need to exist.&lt;/p&gt;
    &lt;p&gt;The SQL spec also has an extension SQL/MED which defines how to query data that isn't owned by the database, but it isn't widely or portably implemented.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interface level&lt;/head&gt;
    &lt;p&gt;At the interface level, the status quo is much worse. Each database has a completely different interface protocol.&lt;/p&gt;
    &lt;p&gt;The protocols I'm familiar with are all ordered, synchronous and allow returning only one relation at a time. Many don't even support pipelining. For a long time SQL also lacked any way to return nested structures and even now (with json support) it's incredibly verbose.&lt;/p&gt;
    &lt;p&gt;This meant that if you wanted to return, say, a list of user profiles and their followers, you would have to make multiple round-trips to the database. Latency considerations make this unfeasible over longer distances. This practically mandates the existence of an application layer whose main purpose is to coalesce multiple database queries and reassemble their nested structure using hand-written joins over the output relations - duplicating work that the database is supposed to be good at.&lt;/p&gt;
    &lt;p&gt;Protocols also typically return metadata as text in an unspecified format with no parser supplied (even if there is a binary protocol for SQL values, metadata is still typically returned as a 1-row 1-column table containing a string). This makes it harder than necessary to build any kind of tooling outside of the database. Eg if we wanted to parse plans and verify that they don't contain any table scans or nested loops.&lt;/p&gt;
    &lt;p&gt;Similarly, SQL is submitted to the database as a text format identical to what the programmer would type. Since the syntax is so complicated, it's difficult for other languages to embed, validate and escape SQL queries and to figure out what types they return. (Query parameters are not a panacea for escaping - often you need to vary query structure depending on user input, not just values).&lt;/p&gt;
    &lt;p&gt;SQL databases are also typically monolithic. You can't, for example, just send a query plan directly to postgres. Or call the planner as a library to help make operational forecasts based on projected future workloads. Looking at the value unlocked by eg pg_query gives the sense that there could be a lot to gain by exposing more of the innards of SQL systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complexity drag&lt;/head&gt;
    &lt;p&gt;In modern programming languages, the language itself consists of a small number of carefully chosen primitives. Programmers combine these to build up the rest of the functionality, which can be shared in the form of libraries. This lowers the burden on the language designers to foresee every possible need and allows new implementations to reuse existing functionality. Eg if you implement a new javascript interpreter, you get the whole javascript ecosystem for free.&lt;/p&gt;
    &lt;p&gt;Because SQL is so inexpressive, incompressible and non-porous it was never able to develop a library ecosystem. Instead, any new functionality that is regularly needed is added to the spec, often with it's own custom syntax. So if you develop a new SQL implementation you must also implement the entire ecosystem from scratch too because users can't implement it themselves.&lt;/p&gt;
    &lt;p&gt;This results in an enormous language.&lt;/p&gt;
    &lt;p&gt;The core SQL language is defined in part 2 (of 9) of the SQL 2016 spec. Part 2 alone is 1732 pages. By way of comparison, the javascript 2021 spec is 879 pages and the c++ 2020 spec is 1853 pages.&lt;/p&gt;
    &lt;p&gt;But the SQL spec is not even complete!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A quick grep of the SQL standard indicates 411 occurrences of implementation-defined behavior. And not in some obscure corner cases, this includes basic language features. For a programming language that would be ridiculous. But for some reason people accept the fact that SQL is incredibly under-specified, and that it is impossible to write even relatively simple analytical queries in a way that is portable across database systems.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notably, the spec does not define type inference at all, which means that the results of basic arithmetic are implementation-defined. Here is an example from the sqlite test suite in various databases:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT DISTINCT - + 34 + + - 26 + - 34 + - 34 + + COALESCE ( 93, COUNT ( * ) + + 44 - 16, - AVG ( + 86 ) + 12 ) / 86 * + 55 * + 46;
2402

postgres&amp;gt; SELECT DISTINCT - + 34 + + - 26 + - 34 + - 34 + + COALESCE ( 93, COUNT ( * ) + + 44 - 16, - AVG ( + 86 ) + 12 ) / 86 * + 55 * + 46;
       ?column?
-----------------------
 2607.9302325581395290
(1 row)

mariadb&amp;gt; SELECT DISTINCT - + 34 + + - 26 + - 34 + - 34 + + COALESCE ( 93, COUNT ( * ) + + 44 - 16, - AVG ( + 86 ) + 12 ) / 86 * + 55 * + 46;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '* ) + + 44 - 16, - AVG ( + 86 ) + 12 ) / 86 * + 55 * + 46' at line 1
&lt;/code&gt;
    &lt;p&gt;The spec also declares that certain operations should produce errors when evaluated, but since it doesn't define an evaluation order the decision is left down to the optimizer. A query that runs fine in your database today might return an error tomorrow on the same data if the optimizer produces a different plan.&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; select count(foo.bar/0) from (select 1 as bar) as foo where foo.bar = 0;
0

postgres&amp;gt; select count(foo.bar/0) from (select 1 as bar) as foo where foo.bar = 0;
ERROR:  division by zero

mariadb&amp;gt; select count(foo.bar/0) from (select 1 as bar) as foo where foo.bar = 0;
+------------------+
| count(foo.bar/0) |
+------------------+
|                0 |
+------------------+
1 row in set (0.001 sec)
&lt;/code&gt;
    &lt;p&gt;And despite being enormous and not even definining the whole language, the spec still manages to define a language so anemic that every database ends up with a raft of non-standard extensions to compensate.&lt;/p&gt;
    &lt;p&gt;Even if all the flaws I listed in the previous sections were to be fixed in the future, SQL already ate a monstrous amount of complexity in workarounds for those flaws and that complexity will never be removed from the spec. This complexity has a huge impact on the effort required to implement a new SQL engine.&lt;/p&gt;
    &lt;p&gt;To take an example close to my heart: Differential dataflow is a dataflow engine that includes support for automatic parallel execution, horizontal scaling and incrementally maintained views. It totals ~16kloc and was mostly written by a single person. Materialize adds support for SQL and various data sources. To date, that has taken ~128kloc (not including dependencies) and I estimate ~15-20 engineer-years. Just converting SQL to the logical plan takes ~27kloc, more than than the entirety of differential dataflow.&lt;/p&gt;
    &lt;p&gt;Similarly, sqlite looks to have ~212kloc and duckdb ~141kloc. The count for duckdb doesn't even include the parser that they (sensibly) borrowed from postgres, which at ~47kloc is much larger than the entire ~30kloc codebase for lua.&lt;/p&gt;
    &lt;p&gt;Materialize passes more than 7 million tests, including the entire sqlite logic test suite and much of the cockroachdb logic test suite. And yet they are still discovering (my) bugs in such core components as name resolution, which in any sane language would be trivial.&lt;/p&gt;
    &lt;p&gt;The entire database industry is hauling a massive SQL-shaped parachute behind them. This complexity creates a drag on everything downstream.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quality of implementation suffers&lt;/head&gt;
    &lt;p&gt;There is so much ground to cover that it's not possible to do a good job of all of it. Subqueries, for example, add some much-needed expressiveness to SQL but their use is usually not recommended because most databases optimize them poorly or not at all.&lt;/p&gt;
    &lt;p&gt;This affects UX too. Every SQL database I've used has terrible syntax errors.&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; with q17_part as (
   ...&amp;gt;   select p_partkey from part where
   ...&amp;gt;   p_brand = 'Brand#23'
   ...&amp;gt;   and p_container = 'MED BOX'
   ...&amp;gt; ),
   ...&amp;gt; q17_avg as (
   ...&amp;gt;   select l_partkey as t_partkey, 0.2 * avg(l_quantity) as t_avg_quantity
   ...&amp;gt;   from lineitem
   ...&amp;gt;   where l_partkey IN (select p_partkey from q17_part)
   ...&amp;gt;   group by l_partkey
   ...&amp;gt; ),
   ...&amp;gt; q17_price as (
   ...&amp;gt;   select
   ...&amp;gt;   l_quantity,
   ...&amp;gt;   l_partkey,
   ...&amp;gt;   l_extendedprice
   ...&amp;gt;   from
   ...&amp;gt;   lineitem
   ...&amp;gt;   where
   ...&amp;gt;   l_partkey IN (select p_partkeyfrom q17_part)
   ...&amp;gt; ),
   ...&amp;gt; select cast(sum(l_extendedprice) / 7.0 as decimal(32,2)) as avg_yearly
   ...&amp;gt; from q17_avg, q17_price
   ...&amp;gt; where
   ...&amp;gt; t_partkey = l_partkey and l_quantity &amp;lt; t_avg_quantity;   
Error: near "select": syntax error
&lt;/code&gt;
    &lt;p&gt;But it's hard to produce good errors when your grammar contains 1732 non-terminals. And several hundred keywords. And allows using (some) keywords as identifiers. And contains many many ambiguities which mean that typos are often valid but nonsensical SQL.&lt;/p&gt;
    &lt;head rend="h3"&gt;Innovation at the implementation level is gated&lt;/head&gt;
    &lt;p&gt;Incremental maintenance, parallel execution, provenance, equivalence checking, query synthesis etc. These show up in academic papers, produce demos for simplified subsets of SQL, and then disappear.&lt;/p&gt;
    &lt;p&gt;In the programming language world we have a smooth pipeline that takes basic research and applies it to increasingly realistic languages, eventually producing widely-used industrial-quality tools. But in the database world there is a missing step between demos on toy relational algebras and handling the enormity of SQL, down which most compelling research quietly plummets. Bringing anything novel to a usable level requires a substantial investment of time and money that most researchers simply don't have.&lt;/p&gt;
    &lt;head rend="h3"&gt;Portability is a myth&lt;/head&gt;
    &lt;p&gt;The spec is too large and too incomplete, and the incentives to follow the spec too weak. For example, the latest postgres docs note that "at the time of writing, no current version of any database management system claims full conformance to Core SQL:2016". It also lists a few dozen departures from the spec.&lt;/p&gt;
    &lt;p&gt;This is exacerbated by the fact that every database also has to invent myriad non-standard extensions to cover the weaknesses of standard SQL.&lt;/p&gt;
    &lt;p&gt;Where the average javascript program can be expected to work in any interpreter, and the average c program might need to macro-fy some compiler builtins, the average body of SQL queries will need serious editing to run on a different database and even then can't be expected to produce the same answers.&lt;/p&gt;
    &lt;p&gt;One of the big selling points for supporting SQL in a new database is that existing tools that emit SQL will be able to run unmodified. But in practice, such tools almost always end up maintaining separate backends for every dialect, so unless you match an existing database bug-for-bug you'll still have to add a new backend to every tool.&lt;/p&gt;
    &lt;p&gt;Similarly, users will be able to carry across some SQL knowledge, but will be regularly surprised by inconsistencies in syntax, semantics and the set of available types and functions. And unlike the programming language world they won't be able to carry across any existing code or libraries.&lt;/p&gt;
    &lt;p&gt;This means that the network effects of SQL are much weaker than they are for programming languages, which makes it all the more surprising that we have a bounty of programming languages but only one relational database language.&lt;/p&gt;
    &lt;head rend="h2"&gt;The application layer&lt;/head&gt;
    &lt;p&gt;The original idea of relational databases was that they would be queried directly from the client. With the rise of the web this idea died - SQL is too complex to be easily secured against adversarial input, cache invalidation for SQL queries is too hard, and there is no way to easily spawn background tasks (eg resizing images) or to communicate with the rest of the world (eg sending email). And the SQL language itself was not an appealing target for adding these capabilities.&lt;/p&gt;
    &lt;p&gt;So instead we added the 'application layer' - a process written in a reasonable programming language that would live between the database and the client and manage their communication. And we invented ORM to patch over the weaknesses of SQL, especially the lack of compressibility.&lt;/p&gt;
    &lt;p&gt;This move was necessary, but costly.&lt;/p&gt;
    &lt;p&gt;ORMs are prone to n+1 query bugs and feral concurrency. To rephrase, they are bad at efficiently querying data and bad at making use of transactions - two of the core features of relational databases.&lt;/p&gt;
    &lt;p&gt;As for the application layer: Converting queries into rest endpoints by hand is a lot of error-prone boilerplate work. Managing cache invalidation by hand leads to a steady supply of bugs. If endpoints are too fine-grained then clients have to make multiple roundtrip calls, but if they're too coarse then clients waste bandwidth on data they didn't need. And there is no hope of automatically notifying clients when the result of their query has changed.&lt;/p&gt;
    &lt;p&gt;The success of GraphQL shows that these pains are real and that people really do want to issue rich queries directly from the client. Compared to SQL, GraphQL is substantially easier to implement, easier to cache, has a much smaller attack surface, has various mechanisms for compressing common patterns, makes it easy to follow foreign keys and return nested results, has first-class mechanisms for interacting with foreign code and with the outside world, has a rich type system (with union types!), and is easy to embed in other languages.&lt;/p&gt;
    &lt;p&gt;Similarly for firebase (before it was acqui-smothered by google). It dropped the entire application layer and offered streaming updates to client-side queries, built-in access control, client-side caching etc. Despite offering very little in the way of runtime innovation compared to existing databases, it was able to succesfully compete by recognizing that the current division of database + sql + orm + application-layer is a historical accident and can be dramatically simplified.&lt;/p&gt;
    &lt;p&gt;The overall vibe of the NoSQL years was "relations bad, objects good". I fear that what many researchers and developers are taking away from the success of GraphQL and co is but a minor update - "relations bad, &lt;del&gt;objects&lt;/del&gt; graphs good".&lt;/p&gt;
    &lt;p&gt;This is a mistake. GraphQL is still more or less a relational model, as evidenced by the fact that it's typically backed by wrappers like hasura that allow taking advantage of the mature runtimes of relational databases. The key to the success of GraphQL was not doing away with relations, but recognizing and fixing the real flaws in SQL that were hobbling relational databases, as well as unbundling the query language from a single monolithic storage and execution engine.&lt;/p&gt;
    &lt;head rend="h2"&gt;After SQL?&lt;/head&gt;
    &lt;p&gt;To summarize:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Design flaws in the SQL language resulted in a language with no library ecosystem and a burdensome spec which limits innovation.&lt;/item&gt;
      &lt;item&gt;Additional design flaws in SQL database interfaces resulted in moving as much logic as possible to the application layer and limiting the use of the most valuable features of the database.&lt;/item&gt;
      &lt;item&gt;It's probably too late to fix either of these.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the idea of modelling data with a declarative disorderly language is still valuable. Maybe more so than ever, given the trends in hardware. What should a new language learn from SQL's successes and mistakes?&lt;/p&gt;
    &lt;p&gt;We can get pretty far by just negating every mistake listed in this post, while ensuring we retain the ability to produce and optimize query plans:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start with the structure that all modern languages have converged towards. &lt;list rend="ul"&gt;&lt;item&gt;Everything is an expression.&lt;/item&gt;&lt;item&gt;Variables and functions have compact syntax.&lt;/item&gt;&lt;item&gt;Few keywords - most things are stdlib functions rather than builtin syntax.&lt;/item&gt;&lt;item&gt;Have an explicit type system rather than totally disjoint syntax for scalar expressions vs table expressions.&lt;/item&gt;&lt;item&gt;Ensure that it's always possible to replace a given expression with another expression that has the same type and value.&lt;/item&gt;&lt;item&gt;Define a (non-implementation specific) system for distributing and loading (and unloading!) libraries.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Keep the spec simple and complete. &lt;list rend="ul"&gt;&lt;item&gt;Simple denotational semantics for the core language.&lt;/item&gt;&lt;item&gt;Completely specify type inference, error semantics etc.&lt;/item&gt;&lt;item&gt;Should be possible for an experienced engineer to throw together a slow but correct interpreter in a week or two.&lt;/item&gt;&lt;item&gt;Encode semantics in a model checker or theorem prover to eg test optimizations. Ship this with the spec.&lt;/item&gt;&lt;item&gt;Lean on wasm as an extension language - avoids having to spec arithemetic, strings etc if they can be defined as a library over some bits type.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Make it compressible. &lt;list rend="ul"&gt;&lt;item&gt;Allow functions to take relations and other functions are arguments (can be erased by specialization before planning, ala rust or julia).&lt;/item&gt;&lt;item&gt;Allow functions to operate on relations polymorphically (ie without having to fix the columns and types when writing the function).&lt;/item&gt;&lt;item&gt;Make column names, orderings, collations, window specifications etc first-class values rather than just syntax (can use staging ala zig's comptime if these need to be constant at planning time).&lt;/item&gt;&lt;item&gt;Compact syntax for simple joins (eg snowflake schemas, graph traversal).&lt;/item&gt;&lt;item&gt;True recursion / fixpoints (allows expressing iterative algorithms like parsing).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Make it porous. &lt;list rend="ul"&gt;&lt;item&gt;Allow defining new types, functions, indexes, plan operators etc via wasm plugins (with the calling convention etc in the spec).&lt;/item&gt;&lt;item&gt;Expose plans, hints etc via api (not via strings).&lt;/item&gt;&lt;item&gt;Spec both a human-friendly encoding and a tooling-friendly encoding (probably text vs binary like wasm). Ship an embedabble library that does parsing and type inference.&lt;/item&gt;&lt;item&gt;Make returning nested structures (eg json) ergonomic, or at least allow returning multiple relations.&lt;/item&gt;&lt;item&gt;Create a subset of the language that can be easily verified to run in reasonable time (eg no table scans, no nested loops).&lt;/item&gt;&lt;item&gt;Allow exposing subset to clients via graphql-like authorization rules.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Better layering. &lt;list rend="ul"&gt;&lt;item&gt;Separate as much as possible out into embeddable libraries (ala pg_query).&lt;/item&gt;&lt;item&gt;Expose storage, transaction, execution as apis. The database server just receives and executes wasm against these apis.&lt;/item&gt;&lt;item&gt;Distribute query parser/planner/compiler as a library so clients can choose to use modified versions to produce wasm for the server.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Strategies for actually getting people to use the thing are much harder.&lt;/p&gt;
    &lt;p&gt;Tackling the entire stack at once seems challenging. Rethinkdb died. Datomic is alive but the company was acquihired. Neo4j, on the other hand, seems to be catnip for investors, so who knows.&lt;/p&gt;
    &lt;p&gt;A safer approach is to first piggy-back on existing databases runtime. EdgeDB uses the postgres runtime. Logica compiles to SQL. GraphQL has compilers for many different query languages.&lt;/p&gt;
    &lt;p&gt;Another option is to find an untapped niche and work outward from there. I haven't seen this done yet, but there are a lot of relational-ish query niches. Pandas targets data cleaning/analysis. Datascript is a front-end database. Bloom targets distributed systems algorithms. Semmle targets code analysis. Other potential niches include embedded databases in applications (ala fossils use of sqlite), incremental functions from state to UI, querying as an interface to the state of complex programs etc.&lt;/p&gt;
    &lt;p&gt;In a niche with less competition you could first grow the language and then try to expand the runtime outwards to cover more potential usecases, similar to how sqlite started as a tcl extension and ended up becoming the defacto standard for application-embedded databases, a common choice for data publishing format, and a backend for a variety of data-processing tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Why do you want json in your database? I'm personally not that interested in storing json in tables (although I hear that it's useful for modelling sparse data). But many usecases require returning disparate data that does not fit into a single relation (eg a monitoring dashboard or the logged-in landing page of many websites). If you can't return all this data from a single query, you need an entire extra process co-located with the database whose job it is to coalesce multiple synchronous queries and then join the results together into some structured form to send to the client. This results in duplication of work in the database, hides some of the query structure from the query planner, opens up the potential for n+1 bugs, adds the possibility of partial failure etc. It's a lot of extra complexity that can be avoided if your database can just return all the data in one query (eg see how Hasura translates GraphQL to SQL). This is why people are excited about databases providing GraphQL support.&lt;/p&gt;
    &lt;p&gt;That code belongs in the application layer. Use the right tool for the job! If your database query language is not the right tool for querying data, that seems like a problem.&lt;/p&gt;
    &lt;p&gt;SQL is the only relational language that has even been successful. It's just the natural way of expressing relational queries. LINQ, spark, flink, kafka streams, pandas, dataframes are all widely used examples of an expression-based language-embedded approach to relational queries. Logica, logiql, differential datalog, semmle, datomic are all examples of commercially-deployed datalog-based relational query languages.&lt;/p&gt;
    &lt;p&gt;But SQL enables transactions, logical data independence, plan optimization etc. The relational data model enables those things. None of them require the language to be SQL. Eg logicblox and datomic manage to have transactions, transparent indexes, query planners etc while still having much simpler and more orthogonal query languages than SQL.&lt;/p&gt;
    &lt;p&gt;Javascript is crazy too! Javascript has improved dramatically over the last decade or two, to the point that compatibility between different vendors is almost complete. But imagine a javascript without libraries, without polyfills, where functions couldn't take collections as arguments and where &lt;code&gt;for&lt;/code&gt; loops had a different syntax in each engine. I would, for the record, totally endorse a SQL STRICT MODE which discarded all the silly edge cases and produced a simpler, more orthogonal language. But the database vendors have no incentive to do this - SQL is their moat.&lt;/p&gt;
    &lt;p&gt;SQL has been around for more than 50 years. So has COBOL. Thousands of engineer-years have been invested in the COBOL ecosystem. But it sure seems that noticing COBOL's flaws and designing better successors paid off in the long run. It turns out that we learned a lot about language design in the last 50 years.&lt;/p&gt;
    &lt;p&gt;Complaining is easy. Where's your solution? Perhaps the first step in trying to replace something would be to carefully analyze and discuss the strengths and weaknesses of that thing. Those who don't study history...&lt;/p&gt;
    &lt;p&gt;You just need to learn how SQL works. Bruh.&lt;/p&gt;
    &lt;p&gt;I'd like to finish with this quote from Michael Stonebraker, one of the most prominent figures in the history of relational databases:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;My biggest complaint about System R is that the team never stopped to clean up SQL... All the annoying features of the language have endured to this day. SQL will be the COBOL of 2020...&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45704419</guid><pubDate>Sat, 25 Oct 2025 15:00:06 +0000</pubDate></item><item><title>Rock Tumbler Instructions</title><link>https://rocktumbler.com/tips/rock-tumbler-instructions/</link><description>&lt;doc fingerprint="e3c401461f672ad1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Rock Tumbler Instructions&lt;/head&gt;&lt;head rend="h3"&gt;Directions for Turning Rough Rocks into Beautiful Tumbled Stones&lt;/head&gt;&lt;p&gt;Working to transform rough rock into beautiful tumbled stones gives most people a great feeling of accomplishment. It doesn't matter how old you are or how many batches of rock you have tumbled in the past - when you finish the last tumbling step, rinse off the polish, and see a super-bright luster on colorful polished stones - you are amazed at what you have done.&lt;/p&gt;&lt;head rend="h2"&gt;Rock Tumbling Is Easy&lt;/head&gt;&lt;p&gt;Using a rock tumbler to convert rough rock into sparkling tumbled stones is easy if you follow a simple procedure and observe a few rules. We are writing this to share the procedure that we have used for many years with a number of rotary tumblers.&lt;/p&gt;&lt;p&gt;This procedure works well with materials that have the following properties:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;of adequate quality to accept a polish&lt;/item&gt;&lt;item&gt;a Mohs hardness between 6 and 7&lt;/item&gt;&lt;item&gt;a size between 3/8" and 1 1/2"&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Chalcedony&lt;/head&gt;: agate, bloodstone, carnelian, chrysoprase, jasper, chert, flint, and petrified (silicified) wood.&lt;head rend="h3"&gt;Quartz&lt;/head&gt;: amethyst, aventurine, citrine, milky quartz, rock crystal, rose quartz, smoky quartz, tiger's-eye.&lt;head rend="h3"&gt;Rock Types&lt;/head&gt;: andesite, basalt, diorite, gabbro, granite, mookaite, novaculite, quartzite, unakite.&lt;head rend="h2"&gt;The "Golden Rules" of Rock Tumbling&lt;/head&gt;&lt;p&gt;We follow three "Golden Rules" in all aspects of rock tumbling. They are:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;"Garbage in means garbage out"&lt;/item&gt;&lt;item&gt;"Avoid contamination"&lt;/item&gt;&lt;item&gt;"Great results take time."&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Tumbling will enable you to turn the rough rock on the left side of this photo into the sparkling tumbled stones on the right side of the photo. The results are amazing!&lt;/p&gt;&lt;head rend="h3"&gt;"Garbage in Means Garbage Out"&lt;/head&gt;&lt;p&gt;If you start with garbage (low-quality rough), you should expect low-quality tumbled stones. So, don't hesitate to discard a rock that is porous, fractured, misshapen, or that is not expected to produce an attractive tumbled stone.&lt;/p&gt;&lt;p&gt;You will spend a lot of time and valuable supplies tumbling a batch of rocks. Using quality rough saves time, gives you better value for your money, and produces tumbled stones that are of much higher quality.&lt;/p&gt;&lt;p&gt;We buy lots of tumbling rough from online vendors as part of our hobby and to educate ourselves. We have the best experience buying rough from vendors who: 1) provide clear written descriptions and large clear photos of the rough they are selling, 2) show photos of tumbled stones that they produced themselves from the rock they are selling, and, 3) provide a detailed description of the steps that they followed to tumble the stones. We have the best experience buying rough from people who are actively involved in rock tumbling.&lt;/p&gt;&lt;head rend="h3"&gt;"Avoid Contamination"&lt;/head&gt;&lt;p&gt;You will use a different size tumbler grit for each step of the tumbling process. If coarse grit gets into your medium grit step, it will scratch up the rocks and you might need to do the medium grit step over again.&lt;/p&gt;&lt;p&gt;Avoiding this type of contamination is easy: just thoroughly clean the rocks, the tumbler barrel, and your tools when you change from one grit size to another.&lt;/p&gt;&lt;p&gt;Another way that contamination occurs is when you include rocks that are brittle, or have a granular texture. These rocks might break or shed grains in the tumbler. These grains and broken pieces can scratch up every rock in the barrel.&lt;/p&gt;&lt;p&gt;Here is a test that we use to detect rocks that will shed grains in the tumbler. We pick up a piece of rough in each hand. We then rub them together while applying a bit of pressure. If we are easily dislodging grains from the rock, we believe that the rock will likely shed grains during tumbling.&lt;/p&gt;&lt;p&gt;This type of contamination is also easy to avoid. Simply examine your rocks before tumbling, and don't tumble suspect rocks in the same barrel with quality rough. Tumble new types of rough or suspicious materials separately.&lt;/p&gt;&lt;head rend="h3"&gt;"Great Results Take Time"&lt;/head&gt;&lt;p&gt;Don't be in a hurry. Spend time doing a great job. If you tumble a batch of rocks through the coarse grit step and they still have a few rough edges or are not nicely rounded, don't hesitate to run them through the coarse grit step again. Also, spend the time needed to thoroughly clean your work area, tumbler barrel, rocks, and tools between steps to avoid contamination.&lt;/p&gt;&lt;p&gt;"Garbage in means garbage out." The rocks in this photo do not have the potential to become nice tumbled stones. A rock with voids should be thrown away - the voids will trap grit and contaminate your pre-polish and polishing steps. Protrusions can be trimmed off with a rock saw - and that might yield two nicely rounded rocks.&lt;/p&gt;&lt;head rend="h2"&gt;Inspecting Your Rough&lt;/head&gt;&lt;p&gt;Remember the rule "garbage in means garbage out." Practice that by starting with quality rough, and you will have a chance to produce high-quality tumbled stones. We prepare to tumble by examining our rough rock. If we find porous pieces that might carry grit from one step to the next, we discard them.&lt;/p&gt;&lt;p&gt;Rocks that are fractured will break while tumbling and scratch other rocks in the batch. When we see a fractured rock in our rough, we discard it or break it along that fracture before it is placed in the barrel.&lt;/p&gt;&lt;p&gt;For best results, your tumbler barrel should be loaded with rocks of mixed sizes (from about 1/4 inch up to about 1 1/2 inches in diameter for a 2-pound or 3-pound-capacity barrel). If we need more rocks to fill the barrel to the proper level, we often add rocks that were previously polished but have a rough spot or a blemish that, if ground away, will improve the rock's appearance.&lt;/p&gt;&lt;p&gt;Two final tips before we load the barrel:&lt;/p&gt;&lt;p&gt;1.) Tumbling works best when all of the rocks in the barrel are about the same hardness. If soft rocks are tumbled with harder rocks, the softer rocks will wear away quickly - before the harder rocks are properly shaped and smoothed.&lt;/p&gt;&lt;p&gt;2.) Tumbling works best when all rocks in the barrel are of the same type. If you mix rock types, problems can result - and they will be difficult to diagnose.&lt;/p&gt;&lt;p&gt;When loading the tumbler barrel, you should have pieces of rough with a range of particle sizes. We would mix the above sizes together in the barrel. If you load the barrel with just a few large pieces, there will be very few points of contact between the rocks in the load. Those points of contact are where grit is trapped between the rocks and where grinding occurs. If you have lots of small pieces of rough between the big pieces, there will be many points of contact between the rocks of the load, and the tumbling process will be faster and more effective.&lt;/p&gt;&lt;p&gt;If you don't have small pieces of rock to tumble, you can add small ceramic media to the tumbler barrel. Ceramic media are used as small-size "filler" in tumbling. These tiny cylinders will also act like roller bearings in the barrel and make your load tumble with a smooth action - that smooth action will improve the grinding in the barrel and keep your stones from being bruised. See our video about selecting the right tumbling media.&lt;/p&gt;&lt;head rend="h2"&gt;The Four-Step Tumbling Process&lt;/head&gt;&lt;p&gt;Now you are ready to begin what most people call the "Four-Step Tumbling Process." This is described below for a rotary tumbler with a three-pound-capacity barrel such as the Thumler's Model A-R1, Thumler's Model A-R2, Lortone Model 3A, or the Lortone Model 33B.&lt;/p&gt;&lt;p&gt;If you are tumbling with the Thumler's Model MP-1 tumbler (which has a two-pound-capacity barrel), you can follow the instructions below, but use about two level tablespoons of grit or polish in each of the tumbling steps (Step 1 through Step 4).&lt;/p&gt;&lt;head rend="h2"&gt;Loading the Tumbler Barrel&lt;/head&gt;&lt;p&gt;Before you load the tumbler barrel, be sure that it is perfectly clean. There should be no grit or rock fragments left in the barrel from a previous tumble. To prevent leaks, the rim of the barrel and the lid should be totally free from grit or rock particles.&lt;/p&gt;&lt;p&gt;Once you have a clean barrel, add enough rock to fill the barrel about 1/2 to 2/3 full. With small tumblers it is best to tumble rocks that are between about 1/4" and 1 1/2 inches in size. If you don't have enough rough to fill the barrel at least 2/3 full, the rocks might be tossed around in the tumbler and bruised. (Varieties of quartz bruise very easily.)&lt;/p&gt;&lt;p&gt;It is best to add a variety of rock sizes to the barrel. If you use only large pieces there will be very few contact points between the rocks and very little grinding will occur. If you add a range of rock sizes the small rocks will fill the spaces between the large rocks, creating many more points of contact between the rocks. Grinding occurs when particles of grit get caught between the rocks - so the more points of contact you have, the more effective the grinding.&lt;/p&gt;&lt;p&gt;When tumbling you will place enough rocks in the barrel to make it about 1/2 to 2/3 full. Then, add about two level tablespoons of grit for each pound of rock. Finally, add enough water to almost cover the rock. Now seal the barrel and place it on the tumbler.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 1 - Coarse Grind&lt;/head&gt;&lt;p&gt;The first step of the four-step tumbling process is to run the rocks in the tumbler with coarse grit. We begin with a barrel that is about 1/2 to 2/3 full of tumbling rough, then add two level tablespoons of coarse grit (we use 60/90 grit silicon carbide) for each pound of rock. Then, add water until the water line is just below the top of the rocks. Seal the barrel and run for about seven days.&lt;/p&gt;&lt;p&gt;At the end of seven days, open the barrel. You will find a barrel of rocks in very muddy water! Dump the contents into a screen or a colander over a plastic bucket and rinse off every speck of grit and mud. Wear safety glasses to protect your eyes from a splash of mud.&lt;/p&gt;&lt;p&gt;Used grit and rock mud should never be washed down a drain. It can clog your plumbing system. We wash rocks in a plastic colander over a plastic bucket to keep the mud out of the drain.&lt;/p&gt;&lt;head rend="h3"&gt;Inspecting the Rocks:&lt;/head&gt;Now that you have washed the rocks, it is time to inspect them. Your goal is to determine if they are ready to move on to STEP 2, or if another week in STEP 1 would improve their appearance. We almost always tumble the rocks for a second week in coarse grit. We believe that improves their shape and removes more blemishes from their surface. Then, we usually move all of the rocks to the medium grit step.&lt;head rend="h3"&gt;Perfectionist Tumbling:&lt;/head&gt;Some people want to have more control over the tumbling process and only admit excellent rocks into STEP 2. These people sort their rocks into three categories:&lt;list rend="ul"&gt;&lt;item&gt;1) those that are ready for STEP 2&lt;/item&gt;&lt;item&gt;2) those that could be improved by another week in STEP 1&lt;/item&gt;&lt;item&gt;3) those that should be discarded or trimmed and returned to STEP 1&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here are some rocks right out of STEP 1. Note how they are covered with a gray "mud." This mud is spent grit and tiny rock particles that were worn off of the rocks during tumbling. Wash the rocks thoroughly so none of this grit goes into STEP 2. We wash our rocks in a colander over a plastic bucket so none of the mud goes down the drain. &lt;lb/&gt;IT IS VERY IMPORTANT TO WASH THE MUD FROM THE ROCKS IMMEDIATELY. If the rock mud is allowed to dry on the rocks, it is almost impossible to wash off.&lt;/p&gt;&lt;p&gt;IT IS VERY IMPORTANT TO WASH THE MUD FROM THE ROCKS IMMEDIATELY. If the rock mud is allowed to dry on the rocks, it is almost impossible to wash off.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 2 - Medium Grind&lt;/head&gt;&lt;p&gt;The second step of the four-step tumbling process is to run the rocks in the tumbler with medium grit. Before you begin it is extremely important to clean all of the coarse grit and rock mud from the rocks, from the tumbler barrel, and from the barrel lid. It is very important to avoid having even a few grains of coarse grit in the medium grind step.&lt;/p&gt;&lt;p&gt;During STEP 1, your rocks were reduced in size. When you return them to the barrel for STEP 2, they will probably not fill the barrel to the recommended 1/2 to 2/3 full level. If the barrel is only 1/2 full or less, the rocks can be tossed violently around in the tumbler. This can break or damage fragile materials such as quartz. So, when tumbling quartz or another fragile material, we always add enough ceramic media (or some rocks that need a little more tumbling) to bring the barrel up to the 1/2 to 2/3 full level.&lt;/p&gt;&lt;p&gt;(This is less important with varieties of chalcedony because it is a more durable material. However, if your tumbler barrel travels at more than about 60 revolutions per minute, we recommend adding enough ceramic media to bring it up to the 2/3 full level regardless of what type of rock is being tumbled.)&lt;/p&gt;&lt;p&gt;After your barrel is at the proper level, add two level tablespoons of medium grit (we use 110/220 grit or 150/220 grit silicon carbide) for each pound of rock (and ceramic media). Then add water until the water line is just below the top of the rocks. Now tumble for seven days.&lt;/p&gt;&lt;p&gt;At the end of seven days, open the barrel and clean all of the grit from the rocks, barrel, and lid (don't let any grit go down the drain). At this point in the tumbling process, a dry rock should have a smooth frosted surface. Inspect the rocks, looking for any that are cracked or broken. If you find any, these rocks should be discarded or saved for the next time you run Step 1.&lt;/p&gt;&lt;p&gt;Used grit and rock mud should never be washed down a drain. It can clog your plumbing system. We wash rocks in a plastic colander over a plastic bucket to keep the mud out of the drain.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 3 - Fine Grind / Pre-polish&lt;/head&gt;&lt;p&gt;The third step of the four-step tumbling process is a week in a fine grit such as 600 grit or 500 grit silicon carbide. Begin with a barrel that is perfectly clean. Place your rough and any ceramics that are with them into the barrel, and add two level tablespoons of fine grit per pound of material. Then add water until it fills the barrel up to just below the top of the rocks. Run this for about seven days, and then do a thorough cleaning of the rocks, the barrel, and the lid.&lt;/p&gt;&lt;p&gt;Remove any rocks that have broken or show signs of fracturing. At this point in the process, the rocks should be extremely smooth, and some of them might display a slight luster.&lt;/p&gt;&lt;p&gt;Be very clean! Before you replace the lid on your barrel, be sure that both the lid and the rim are perfectly clean. This will allow the lid to fit tightly and prevent leaks.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 4 - Polish&lt;/head&gt;&lt;p&gt;Now you are down to the final step - the one that puts a bright shine onto your tumbled stones. Be sure that the rocks and the equipment are perfectly clean. (Some people have an extra barrel that they use only for the polishing step.) A few specks of grit could ruin a great polish.&lt;/p&gt;&lt;p&gt;Place the rocks in the barrel and add two level tablespoons of rock polish (we use TXP aluminum oxide powder for almost all of our rotary tumbling) per pound of material in the barrel. Add water to just below the top of the rocks. Then, close the barrel and run for about seven days.&lt;/p&gt;&lt;p&gt;When you finish this step, your rocks should be bright and shiny. If they are, congratulations! Admire them for a while and share them with your friends.&lt;/p&gt;&lt;p&gt;If the stones have an extremely smooth surface but do not shine, they might need to be cleaned up using the burnishing step described below. If they have scratches on them, then you will need to go back to STEP 2 and repeat the medium grind, fine grind, and polishing steps.&lt;/p&gt;&lt;p&gt;For burnishing we grate up a bar of Ivory Soap with a vegetable grater. Then we add 1/2 tablespoon of grated soap for each pound of rock plus enough warm water to almost cover the rocks. See our video about burnishing polished stones.&lt;/p&gt;&lt;head rend="h2"&gt;Burnishing&lt;/head&gt;&lt;p&gt;Sometimes our stones are a little "hazy" when they come out of the polish, or small particles of polish are in micro-size crevices. We shine and clean them up by tumbling for an hour or so in soapy water. This is called "burnishing."&lt;/p&gt;&lt;p&gt;To burnish, we place the stones in our polish barrel with the normal amount of water, and then we add about 1/2 tablespoon of grated "Ivory" bar soap for each pound of rock (we use "ORIGINAL" Ivory soap - don't use a soap with aloe or abrasive or any other additive - honestly, just get a bar of Ivory soap). Burnishing usually makes the tumbled stones a little brighter, but sometimes it really kicks up the shine.&lt;/p&gt;&lt;p&gt;Print a copy of our free tumbling log and use it to keep your records.&lt;/p&gt;&lt;p&gt;Here are a few of our favorite tumbled stones!&lt;/p&gt;&lt;head rend="h2"&gt;Keeping Records&lt;/head&gt;&lt;p&gt;It is easy to forget what day you started the tumbler or what type of grit was used - especially if you are running multiple tumblers. Keeping records will keep you on track and provide a history that will help you learn. We record material tumbled, start date, abrasive used, media used, finishing date and duration, along with any comments or observations about the results.&lt;/p&gt;&lt;p&gt;To help you with your record keeping, we have prepared a printable tumbling log.&lt;/p&gt;&lt;p&gt;We usually have multiple tumblers running here, and we record every barrel of rock that we tumble on these logs. Even if your memory is better than ours, record-keeping is a good idea. When you learn something that works or something that doesn't, you will have it recorded. This information can help you repeat great results and avoid repeating bad ones. Also, we have trouble remembering which day a barrel of rocks was started. Using the log takes away the chance of forgetting.&lt;/p&gt;&lt;head rend="h2"&gt;Happy Tumbling!&lt;/head&gt;&lt;head rend="h3"&gt;RockTumbler.com Authors&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Hobart M. King has decades of rock tumbling experience and writes most of the articles on RockTumbler.com. He has a PhD in geology and is a GIA graduate gemologist. He also writes the articles about rocks, minerals and gems on Geology.com.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45705125</guid><pubDate>Sat, 25 Oct 2025 16:32:40 +0000</pubDate></item><item><title>We do not have sufficient links to the UK for Online Safety Act to be applicable</title><link>https://libera.chat/news/advised</link><description>&lt;doc fingerprint="1e189153792602e7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The good advice&lt;/head&gt;
    &lt;p&gt;First of all, a massive thank you to everyone who donated since our last post. Our income on Liberapay has roughly quadrupled from what it was before the post. We have also had people reach out to us for large one-time monetary and hardware donations. Your support is truly appreciated!&lt;/p&gt;
    &lt;p&gt;And now for a followup from our last post. TL;DR: the legal firm we’ve engaged has sent us a memo indicating that in their opinion we can reasonably argue we do not have sufficient links to the UK for the Online Safety Act to be applicable to us. They also believe we would be at low risk of attempted enforcement action even if Ofcom does consider us to be in-scope for the OSA. We will continue to ensure that this is the case by keeping internal estimates of our UK user base and by continuing with our current efforts to keep Libera.Chat reasonably safe. We have no plans to institute any ID requirements for the forseeable future.&lt;/p&gt;
    &lt;p&gt;If that’s all you wanted to know, then feel free to stop here. However, we feel it’s in the best interest of online communities like ours for us to summarise the advice we were given in hopes that it will be useful to others. This is not legal advice from us to you. This advice was provided to Libera Chat as an assessment of our specific case. We accept no responsibility if you decide to apply advice given to us to your own online service.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this even matter?&lt;/head&gt;
    &lt;p&gt;You might be asking why we’ve even bothered to get legal advice on this matter. Libera Chat (the non-profit that runs the Libera.Chat IRC network) is based in Sweden. Our bank is Swedish, and we do not rely on any UK-based payment providers. We have a few servers in the UK, but they can be migrated on short notice. In other words, the British government has relatively little authority over us. The most damaging action they can reasonably take is to instruct internet service providers in the UK to deny access to us.&lt;/p&gt;
    &lt;p&gt;Relatedly, some online communities have decided that they want to minimise the authority the British government has over them. In response to critical analyses of the OSA pointing out its potential for regulatory overreach, some online communities have taken the understandable precaution of entirely blocking access from known UK IP addresses, thus cutting off any reasonable argument that they somehow have links to the UK (more on that later).&lt;/p&gt;
    &lt;p&gt;The end result is the same: a denial of service to people in the UK solely because of the country they live in. It’s not an insurmountable barrier to access in either case, but it shouldn’t be necessary for individuals in the UK to look into censorship-defeating proxies just to engage with free software developers and peer-directed projects that choose to have a community on our IRC network. It doesn’t serve our users, it doesn’t serve our communities, and it doesn’t serve the UK open source movement. Therefore, it’s in everyone’s best interest for us look into what’s necessary to keep things from getting to the point where users in the UK cannot access Libera.Chat, and that means getting guidance on the OSA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who does the OSA apply to?&lt;/head&gt;
    &lt;p&gt;As the OSA is fairly vague in its definitions, Ofcom has significant latitude in deciding where the thresholds are for whether an organisation meets certain criteria or not. Ofcom also hasn’t been forthcoming with its opinions on where those thresholds are, so there are relatively few hard guarantees about the applicability of the OSA. Still, there is a strong argument that while we definitely meet one of the criteria for the OSA to apply to us, we do not meet the other.&lt;/p&gt;
    &lt;p&gt;The OSA applies to online service providers that provide a regulated service and have links to the UK. We unarguably provide a regulated service because Libera.Chat is a so-called U2U service, i.e. it “allows ‘user-generated content’ to be encountered by another user of the service”. This is an incredibly broad class of services. Some exceptions are made for user content that is posted in relation to service content (e.g. the comment section of a blog) and a few other service types, but none of them reasonably apply to us. Every chat service, forum, federated social media server, or code forge counts as a regulated service, and therefore meets one of the criteria for the OSA to apply to them.&lt;/p&gt;
    &lt;p&gt;So be it. What about our links to the UK? To quote the memo:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An online service provider has “links to the UK” for the purposes of the OSA if any one or more of the following apply:&lt;/p&gt;
      &lt;item&gt;the service has a “significant number of UK users”&lt;/item&gt;
      &lt;item&gt;UK users form a “target market” of the service; or&lt;/item&gt;
      &lt;item&gt;the service is capable of being used by individuals in the UK, and there are reasonable grounds to believe that there is a material risk of significant harm to individuals in the UK presented by the content generated by the service.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;One factor that does not automatically give us links to the UK is the fact that we have staff members in the UK. Curiously, employees of the service provider who do not engage with that service as users are actually excluded for the purposes of determining whether a service has a “significant number of UK users”. Our staffers are also users, but our UK staffers make up an insignificant portion of our user base.&lt;/p&gt;
    &lt;p&gt;Speaking of which, the memo implies that “significance” in this context is interpreted as being relative to the population of the UK, not relative to the user base of the service. We have seen risk assessments that take the other interpretation and consider their UK user base to be “significant” because it makes up a large portion of their overall user base, but the advice we received suggests we should not use this interpretation. The exact fraction of the UK’s online population that must use a given service to be considered “significant” is unknown, but based on our counsel’s observations of Ofcom’s previous regulatory actions, it appears to be much higher than our internal estimates of how large our UK user base is.&lt;/p&gt;
    &lt;p&gt;The “target market” criterion is meant to capture services with a low number of UK users that target the UK specifically. While our target market (people interested in using an IRC-based platform for discussing free software or other peer-directed projects) is inclusive of UK users, it isn’t specifically for them. Our network is predominantly English-speaking, but we do not promote, direct, or tailor our service to UK users in particular.&lt;/p&gt;
    &lt;p&gt;Finally, there is no atypical material risk of significant harm to individuals in the UK presented by the messages on Libera.Chat. We block spam and client exploits. We are proactive in ensuring that our network’s acceptable use policy is upheld. We do not tolerate incitement to violence, doxxing, or defamation. And finally, we do not provide file hosting that can be used to distribute pornographic or sexual abuse media, though when we sought legal advice from the firm, we acknowledged the existence of DCC as a commonly-supported mechanism for transferring files using an IRC network to establish a peer-to-peer connection.&lt;/p&gt;
    &lt;p&gt;In the coming weeks, we will be finalising a statement similar to this risk assessment that we can provide to Ofcom should we ever be contacted by them about the OSA.&lt;/p&gt;
    &lt;head rend="h2"&gt;What if the OSA does apply to us?&lt;/head&gt;
    &lt;p&gt;While it is our opinion that the OSA does not apply to us, Ofcom might disagree, and appealing that disagreement would likely involve further legal expenses. So, what is the risk that Ofcom would decide to try to impose fines or other regulatory penalties on us?&lt;/p&gt;
    &lt;p&gt;For the time being, services like ours do not appear to be Ofcom’s priority. Currently, according to our legal sources, the focus appears to be file and image hosts that are at high risk of being used to transmit sexually-explicit depictions of minors. IRC has been used as a way to facilitate piracy, but those days are generally in the past thanks to more attractive options. Even if they weren’t, using Libera.Chat for this purpose is risky. We prefer to exercise the minimum power necessary to keep the network clean, but that doesn’t mean we don’t have the tools necessary to proactively stop the network from being used for piracy or CSAM distribution.&lt;/p&gt;
    &lt;p&gt;We have also been reassured that Ofcom is very likely to contact us with concerns before attempting any sort of action against us. There are some classes of concerns that we would certainly be willing to hear out, and we do prefer a constructive approach to problem resolution where possible. We’re confident that there isn’t anything for them to be reasonably concerned about, but we are willing to engage with good-faith reports of potential abuse of our service.&lt;/p&gt;
    &lt;head rend="h2"&gt;Will Libera.Chat ever require my ID?&lt;/head&gt;
    &lt;p&gt;We have no plans to require users to provide us with proof of identity and will take every reasonable measure to avoid requiring it. The justification for us to compromise the privacy of our users given the content we forbid on Libera.Chat is not adequate, and the risk of material harm should an identity verification mechanism compromise our users’ privacy far outweighs the plausible harms caused by not having such a system. Such violations of privacy aren’t hypothetical; another chat platform recently was affected by a data breach that potentially exposed the legal identities of tens of thousands of its users.&lt;/p&gt;
    &lt;p&gt;That said, it’s conceivable that legislation will be created that could apply to us and could force us to identify or spy on our users. If that happens, we will evaluate our options once drafts of such legislation reach a point where they can conceivably pass. Until then, we hope that the general public will remain vocally opposed to such attempts at overreach. Popular opposition stalled Chat Control earlier this month. There will probably always be efforts to compromise the free internet, but their success is not inevitable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45705381</guid><pubDate>Sat, 25 Oct 2025 17:07:40 +0000</pubDate></item><item><title>Switzerland is spending millions revamping its vast network of bunkers</title><link>https://www.washingtonpost.com/world/2025/10/25/switzerland-nuclear-bunkers-overhaul/</link><description>&lt;doc fingerprint="1b83387246253a75"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;&amp;gt; The Journey Before main()_&lt;/head&gt;
    &lt;p&gt;October 25, 2025 · Amit Prasad&lt;/p&gt;
    &lt;p&gt;A while back, I worked on a RISC-V-based userspace simulator for fun. In doing so, taught myself a lot more than I wanted to know about what happens in-between when the Kernel is asked to run a program, and when the first line of our program’s &lt;code&gt;main&lt;/code&gt; function is actually executed. Here’s a summary of that rabbit hole.&lt;/p&gt;
    &lt;head rend="h2"&gt;In the beginning…&lt;/head&gt;
    &lt;p&gt;First question: When is the OS kernel actually asked to run any program? The answer, at least on Linux, is the &lt;code&gt;execve&lt;/code&gt; system call (“syscall”). Let’s take a quick look at that:&lt;/p&gt;
    &lt;code&gt;int execve(const char *filename, char *const argv[], char *const envp[]);&lt;/code&gt;
    &lt;p&gt;This is actually quite straightforward! We pass the name of the exectuable file, a list of arguments, and a list of environment variables. This signals to the kernel where, and how, to start loading the program.&lt;/p&gt;
    &lt;p&gt;Many programming languages provide an interface to execute commands that eventually call &lt;code&gt;execve&lt;/code&gt; under the hood. For example, in Rust, we have:&lt;/p&gt;
    &lt;code&gt;use std::process::Command;

Command::new("ls").arg("-l").spawn();&lt;/code&gt;
    &lt;p&gt;In these higher-level wrappers, the language’s standard library often handles translation of the command name to a full path, acting similarly to how a shell would resolve the command via the &lt;code&gt;PATH&lt;/code&gt; environment variable. The kernel itself, however, expects a proper path to an executable file.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A note on interpreters: If the executable file starts with a shebang (&lt;/p&gt;&lt;code&gt;#!&lt;/code&gt;), the kernel will use the shebang-specified interpreter to run the program. For example,&lt;code&gt;#!/usr/bin/python3&lt;/code&gt;will run the program using the Python interpreter,&lt;code&gt;#!/bin/bash&lt;/code&gt;will run the program using the Bash shell, etc.&lt;/quote&gt;
    &lt;head rend="h2"&gt;ELF&lt;/head&gt;
    &lt;p&gt;What does an executable file look like? On Linux, it’s ELF, which the kernel knows how to parse. Other operating systems have different formats (e.g. Mach-O on MacOS, PE on Windows), but ELF is the most common format on Linux. I won’t go into too much detail here, to keep things brief, but ELF files have grown out of the original &lt;code&gt;a.out&lt;/code&gt; format, and are expressive enough to support pretty much every program you’ll ever write. Here’s what the header of an ELF file looks like:&lt;/p&gt;
    &lt;code&gt;% readelf -h main # main is an ELF file
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 03 00 00 00 00 00 00 00 00
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - GNU
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           RISC-V
  Version:                           0x1
  Entry point address:               0x10358
  Start of program headers:          52 (bytes into file)
  Start of section headers:          675776 (bytes into file)
  Flags:                             0x1, RVC, soft-float ABI
  Size of this header:               52 (bytes)
  Size of program headers:           32 (bytes)
  Number of program headers:         7
  Size of section headers:           40 (bytes)
  Number of section headers:         32
  Section header string table index: 31&lt;/code&gt;
    &lt;p&gt;The important parts here are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The “ELF Magic” bytes, which tell the kernel that this is, indeed, an ELF file. &lt;code&gt;45 4c 46&lt;/code&gt;is ASCII for “ELF”!&lt;/item&gt;
      &lt;item&gt;“Class” tells us we’re dealing with a 32-bit executable.&lt;/item&gt;
      &lt;item&gt;“Start of …” tells us where things are in the file, and “Size of …” tells us how big they are; The kernel is effectively given a map of the file.&lt;/item&gt;
      &lt;item&gt;“Entry point address” — Relatively self-explanatory! But we’ll be coming back to this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other ELF files will have different entries and specific values, but the general structure is what we’re after here.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As you can see by the numerous mentions to “RISC-V”, this is an ELF file I compiled and linked targeting the RV32 architecture (which the aforementioned emulator is built for), hence the “32” in “ELF32”, the “RVC” flag, and the “RISC-V” machine type.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;More than just a header&lt;/head&gt;
    &lt;p&gt;ELF files contain everything our program needs to run, including the code, data, symbols, and more. We can see this again with the &lt;code&gt;readelf&lt;/code&gt; command with the &lt;code&gt;-a&lt;/code&gt; flag. Here’s what we care about:&lt;/p&gt;
    &lt;code&gt;Section Headers:
  [Nr] Name              Type            Addr     Off    Size
  [ 0]                   NULL            00000000 000000 000000
  [ 1] .note.ABI-tag     NOTE            00010114 000114 000020
  [ 2] .rela.plt         RELA            00010134 000134 00000c
  [ 3] .plt              PROGBITS        00010140 000140 000010
  [ 4] .text             PROGBITS        00010150 000150 03e652
  [ 5] .rodata           PROGBITS        0004e7b0 03e7b0 01b208
  ...
  [16] .data             PROGBITS        0007a008 069008 000dec
  [17] .sdata            PROGBITS        0007adf4 069df4 000004
  [18] .bss              NOBITS          0007adf8 069df8 002b6c
  ...
  [29] .symtab           SYMTAB          00000000 095124 009040
  [30] .strtab           STRTAB          00000000 09e164 006d10&lt;/code&gt;
    &lt;p&gt;These sections contain code (&lt;code&gt;.text&lt;/code&gt;), data (&lt;code&gt;.data&lt;/code&gt;), space for global variables (&lt;code&gt;.bss&lt;/code&gt;), shims for accessing shared library functions (&lt;code&gt;.plt&lt;/code&gt;), and quite a bit more (including symbol tables for debugging, relocation tables, etc.), most of which we won’t be discussing.&lt;/p&gt;
    &lt;p&gt;So evidently, there’s some code that we care about in the &lt;code&gt;.text&lt;/code&gt; section, so we copy that and call it a day? Not quite. There’s a massive amount of machinery inside the kernel to make all sorts of programs under all sorts of conditions run.&lt;/p&gt;
    &lt;p&gt;For example, the “PLT” (Procedure Linkage Table) is a section that allows us to call functions in “shared libraries”, for example, &lt;code&gt;libc&lt;/code&gt;, without having to package them alongside our program (“dynamically” vs “statically linking”). The ELF file contains a dynamic section which tells the kernel which shared libraries to load, and another section which tells the kernel to dynamically “relocate” pointers to those functions, so everything checks out.&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;libc&lt;/code&gt;is the C standard library, which contains all the “useful” functions:&lt;code&gt;printf&lt;/code&gt;,&lt;code&gt;malloc&lt;/code&gt;, etc. Various flavors implementing the&lt;code&gt;libc&lt;/code&gt;interfaces exist, most commonly&lt;code&gt;glibc&lt;/code&gt;and&lt;code&gt;musl&lt;/code&gt;. Most of the binaries that are discussed in this post are compiled and linked against&lt;code&gt;musl&lt;/code&gt;, since it’s much easier to statically link.&lt;/quote&gt;
    &lt;p&gt;The symbol table looks something like this:&lt;/p&gt;
    &lt;code&gt;Symbol table '.symtab' contains 2308 entries:
   Num:    Value  Size Type    Bind   Vis      Ndx Name
     0: 00000000     0 NOTYPE  LOCAL  DEFAULT  UND
     1: 00010114     0 SECTION LOCAL  DEFAULT    1 .note.ABI-tag
     2: 00010134     0 SECTION LOCAL  DEFAULT    2 .rela.plt
     3: 00010140     0 SECTION LOCAL  DEFAULT    3 .plt
     4: 00010150     0 SECTION LOCAL  DEFAULT    4 .text
     ...
     1782: 00010358    30 FUNC    GLOBAL HIDDEN     4 _start
     ...
     1917: 00010430    52 FUNC    GLOBAL DEFAULT    4 main
     2201: 00010506   450 FUNC    GLOBAL HIDDEN     4 __libc_start_main
     ...&lt;/code&gt;
    &lt;p&gt;You may ask: “Wow! &lt;code&gt;2308&lt;/code&gt; looks like a lot, right? What behemoth of a program could possibly need that many symbols?“.&lt;/p&gt;
    &lt;p&gt;Good question! Here’s the behemoth:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main() {
  printf("Hello, World!\n");
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Yeah, that’s it. Now, &lt;code&gt;2308&lt;/code&gt; may be slightly bloated because we link against &lt;code&gt;musl&lt;/code&gt; instead of &lt;code&gt;glibc&lt;/code&gt;, but the point still stands: There’s a lot of stuff going on behind the scenes here.&lt;/p&gt;
    &lt;p&gt;The kernel’s job here is to iterate over each section, loading those marked as “loadable”. Some security mitigations start to become relevant here, such as moving sections around in memory (ASLR — Address Space Layout Randomization), marking sections as non-executable (NX bit — hardware-level security), etc. But ultimately, the kernel loads the code and data into memory, sets up the stack, and prepares to jump to the entry point of the program.&lt;/p&gt;
    &lt;head rend="h2"&gt;The stack&lt;/head&gt;
    &lt;p&gt;Ah yes, the infamous stack! Fortunately for most of us, the stack is something we take for granted. Unfortunately for the kernel, the stack is not some omnipotent magical space that just exists — it needs to be set up properly before our program can run.&lt;/p&gt;
    &lt;p&gt;As a reminder: stack space is typically used for variables, function arguments, “frames” (to keep track of function-local variables, call trees, etc), and a variety of other things, depending on what, and how your program is running.&lt;/p&gt;
    &lt;p&gt;Hypothetically, if we simplify a bit and say that the ELF file is loaded into memory starting at the zero address, the stack is typically placed at the “opposite end” of the memory, from a high address, and grows “downwards” towards the lower addresses, with the space in-between used as heap space, and for other data (shared libraries, mmapped files, etc). This is a simplification, but in fairness, there is significant ambiguity as much of the semantics here depend on the program itself.&lt;/p&gt;
    &lt;p&gt;The stack is also something that is non-empty! Remember &lt;code&gt;argv&lt;/code&gt; and &lt;code&gt;envp&lt;/code&gt; from the &lt;code&gt;execve&lt;/code&gt; call above? Those are passed to the program via the stack. In most programming languages we frequently access these via the various &lt;code&gt;args&lt;/code&gt; and &lt;code&gt;env&lt;/code&gt; utilities, whether that be directly, like in C, or more indirectly, like in Rust (&lt;code&gt;std::env&lt;/code&gt;) or Python (&lt;code&gt;sys.argv&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The kernel also stores something called the “ELF auxiliary vector” in the nascent stack. This “auxv” contains information about the environment, such as the memory page size, metadata from the ELF file, and other system information. These are important! For example, &lt;code&gt;musl&lt;/code&gt; uses the “page size” entry of the auxv so that &lt;code&gt;malloc&lt;/code&gt; can request and manage memory more optimally. There are over 30 entries in the auxiliary vectors, but not all of them are used by every program (and some may not be defined by the kernel).&lt;/p&gt;
    &lt;p&gt;Let’s pretend we’re the kernel. Here’s a simplified version of how we might setup the stack of a new process (taken and simplified from my RISC-V emulator, which also emulates parts of the kernel):&lt;/p&gt;
    &lt;code&gt;// Choose an arbitrary high address for the stack
let mut sp = 0xCFFF_F000u32; // sp = "stack pointer"
let mut stack_init: Vec&amp;lt;u32&amp;gt; = vec![]; // The stack begins empty.

stack_init.push(args.len()); // argc: number of arguments
for &amp;amp;arg in args.iter().rev() {
    // Copy each argument to the stack
    sp -= arg.len() // move "downwards" in address space
    mem.copy_to(sp, arg);

    // Keep track of the arg pointer in the init vector
    stack_init.push(sp);
}
stack_init.push(0); // argv NULL terminator

// Environment variables are similar:
for &amp;amp;e in env.iter().rev() {
    sp -= e.len();
    mem.copy_to(sp, e);

    stack_init.push(sp);
}
stack_init.push(0); // envp NULL terminator

// Setup the auxiliary vector
stack_init.push(libc_riscv32::AT_PAGESZ); // Keys for auxv
stack_init.push(0x1000); // Values for auxv; this specifies a 4 KiB page size
stack_init.push(libc_riscv32::AT_ENTRY);
stack_init.push(self.pc); // N.B.: We'll be coming back to this
// ...

// Copy the stack init vector, with all the pointers, to the stack
sp -= (stack_init.len() * 4);

mem.copy_to(sp, &amp;amp;stack_init)&lt;/code&gt;
    &lt;p&gt;A diagram might help illustrate what the address space looks like at this point:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Argument Count (argc)&lt;/item&gt;
      &lt;item&gt;Arguments (argv)&lt;/item&gt;
      &lt;item&gt;Environment variables (envp)&lt;/item&gt;
      &lt;item&gt;Auxiliary Vector (auxv)&lt;/item&gt;
      &lt;item&gt;Local variables, stack frames, function calls&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;↓&lt;/p&gt;
    &lt;p&gt;Grows downward&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shared libraries (libc, etc.)&lt;/item&gt;
      &lt;item&gt;Memory-mapped files&lt;/item&gt;
      &lt;item&gt;Dynamic linker/loader&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;↑&lt;/p&gt;
    &lt;p&gt;Grows upward&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;malloc(), calloc(), realloc() allocations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.bss &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uninitialized global variables&lt;/item&gt;
      &lt;item&gt;Static variables initialized to zero&lt;/item&gt;
      &lt;item&gt;Zero-filled by kernel on program start&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.data &lt;lb/&gt; "read-write"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Initialized global variables&lt;/item&gt;
      &lt;item&gt;Static variables with initial values&lt;/item&gt;
      &lt;item&gt;Read-write data from the executable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.rodata &lt;lb/&gt; "read-only"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Literals ("Hello, World!")&lt;/item&gt;
      &lt;item&gt;Constant data&lt;/item&gt;
      &lt;item&gt;Read-only variables&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.text &lt;lb/&gt; "code"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Program instructions/machine code&lt;/item&gt;
      &lt;item&gt;_start function (entry point)&lt;/item&gt;
      &lt;item&gt;User code&lt;/item&gt;
      &lt;item&gt;Library function code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Entrypoint&lt;/head&gt;
    &lt;p&gt;Finally, we get to the “entry point” address, mentioned at several points. This is the address of the first instruction to run in the process. Typically, this is under a function called &lt;code&gt;_start&lt;/code&gt;. Both glibc and musl provide implementations of &lt;code&gt;_start&lt;/code&gt;, but it’s also possible to write your own. Again, here’s a Rust example:&lt;/p&gt;
    &lt;code&gt;// Disable the language runtime, we're DIYing it.
#![no_std]
#![no_main]

#[panic_handler]
fn panic(_info: &amp;amp;core::panic::PanicInfo) -&amp;gt; ! {
    loop {}
}

#[no_mangle]
pub extern "C" fn _start() -&amp;gt; ! {
    // Instead of "waiting" for main, we can immediately start execution.
    loop {}
}&lt;/code&gt;
    &lt;p&gt;Depending on your program, &lt;code&gt;_start&lt;/code&gt; may be the only thing between the entrypoint and your main function, but most languages have some sort of runtime that needs to be initialized first. For example, Rust has &lt;code&gt;std::rt::lang_start&lt;/code&gt;. It’s at this part that things like global constructors, thread-local storage, and other language-specific features are set up.&lt;/p&gt;
    &lt;p&gt;Here, our journey comes to an end — things become much more language-specific from this point on. Most languages will set up their own runtimes (yes, even C and C++ have a “runtime”!), and eventually call the standard &lt;code&gt;main&lt;/code&gt; function we’re normally familiar with.&lt;/p&gt;
    &lt;p&gt;In Rust, the generated code ends up looking like the following:&lt;/p&gt;
    &lt;code&gt;// the user defined main function
fn main() { println!("Hello, world!"); }

// the generated _start function
fn _start() -&amp;gt; {
    let argc = ...; // get argc from stack
    let argv = ...; // get argv from stack
    let envp = ...; // get envp from stack
    let main_fn = main; // pointer to user main function
    std::rt::lang_start(argc, argv, main_fn);
}&lt;/code&gt;
    &lt;p&gt;With the &lt;code&gt;lang_start&lt;/code&gt; function (defined here)[https://github.com/rust-lang/rust/blob/04ff05c9c0cfbca33115c5f1b8bb20a66a54b799/library/std/src/rt.rs#L199] and taking care of the rest.&lt;/p&gt;
    &lt;p&gt;C and C++ have similar, minimal setups. Languages that are traditionally thought to have “heavier” runtimes, such as Java or Python, work the same way, but with the &lt;code&gt;std::rt::lang_start&lt;/code&gt; equivalent doing far more than the Rust/C/C++ runtimes.&lt;/p&gt;
    &lt;p&gt;And there you have it! I’m missing lots of detail here, but hopefully this gives a rough idea of what happens before &lt;code&gt;main()&lt;/code&gt; gets called. I’ve left out complexity that is mostly internal to “real” linux kernels, such as how the kernel sets up address space, the process tables, various group semantics, and et cetera, but I hope this still serves as a decent primer.&lt;/p&gt;
    &lt;p&gt;Feel free to reach out to me with any questions or corrections!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706077</guid><pubDate>Sat, 25 Oct 2025 18:47:55 +0000</pubDate></item><item><title>The Journey Before main()</title><link>https://amit.prasad.me/blog/before-main</link><description>&lt;doc fingerprint="1b83387246253a75"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;&amp;gt; The Journey Before main()_&lt;/head&gt;
    &lt;p&gt;October 25, 2025 · Amit Prasad&lt;/p&gt;
    &lt;p&gt;A while back, I worked on a RISC-V-based userspace simulator for fun. In doing so, taught myself a lot more than I wanted to know about what happens in-between when the Kernel is asked to run a program, and when the first line of our program’s &lt;code&gt;main&lt;/code&gt; function is actually executed. Here’s a summary of that rabbit hole.&lt;/p&gt;
    &lt;head rend="h2"&gt;In the beginning…&lt;/head&gt;
    &lt;p&gt;First question: When is the OS kernel actually asked to run any program? The answer, at least on Linux, is the &lt;code&gt;execve&lt;/code&gt; system call (“syscall”). Let’s take a quick look at that:&lt;/p&gt;
    &lt;code&gt;int execve(const char *filename, char *const argv[], char *const envp[]);&lt;/code&gt;
    &lt;p&gt;This is actually quite straightforward! We pass the name of the exectuable file, a list of arguments, and a list of environment variables. This signals to the kernel where, and how, to start loading the program.&lt;/p&gt;
    &lt;p&gt;Many programming languages provide an interface to execute commands that eventually call &lt;code&gt;execve&lt;/code&gt; under the hood. For example, in Rust, we have:&lt;/p&gt;
    &lt;code&gt;use std::process::Command;

Command::new("ls").arg("-l").spawn();&lt;/code&gt;
    &lt;p&gt;In these higher-level wrappers, the language’s standard library often handles translation of the command name to a full path, acting similarly to how a shell would resolve the command via the &lt;code&gt;PATH&lt;/code&gt; environment variable. The kernel itself, however, expects a proper path to an executable file.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A note on interpreters: If the executable file starts with a shebang (&lt;/p&gt;&lt;code&gt;#!&lt;/code&gt;), the kernel will use the shebang-specified interpreter to run the program. For example,&lt;code&gt;#!/usr/bin/python3&lt;/code&gt;will run the program using the Python interpreter,&lt;code&gt;#!/bin/bash&lt;/code&gt;will run the program using the Bash shell, etc.&lt;/quote&gt;
    &lt;head rend="h2"&gt;ELF&lt;/head&gt;
    &lt;p&gt;What does an executable file look like? On Linux, it’s ELF, which the kernel knows how to parse. Other operating systems have different formats (e.g. Mach-O on MacOS, PE on Windows), but ELF is the most common format on Linux. I won’t go into too much detail here, to keep things brief, but ELF files have grown out of the original &lt;code&gt;a.out&lt;/code&gt; format, and are expressive enough to support pretty much every program you’ll ever write. Here’s what the header of an ELF file looks like:&lt;/p&gt;
    &lt;code&gt;% readelf -h main # main is an ELF file
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 03 00 00 00 00 00 00 00 00
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - GNU
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           RISC-V
  Version:                           0x1
  Entry point address:               0x10358
  Start of program headers:          52 (bytes into file)
  Start of section headers:          675776 (bytes into file)
  Flags:                             0x1, RVC, soft-float ABI
  Size of this header:               52 (bytes)
  Size of program headers:           32 (bytes)
  Number of program headers:         7
  Size of section headers:           40 (bytes)
  Number of section headers:         32
  Section header string table index: 31&lt;/code&gt;
    &lt;p&gt;The important parts here are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The “ELF Magic” bytes, which tell the kernel that this is, indeed, an ELF file. &lt;code&gt;45 4c 46&lt;/code&gt;is ASCII for “ELF”!&lt;/item&gt;
      &lt;item&gt;“Class” tells us we’re dealing with a 32-bit executable.&lt;/item&gt;
      &lt;item&gt;“Start of …” tells us where things are in the file, and “Size of …” tells us how big they are; The kernel is effectively given a map of the file.&lt;/item&gt;
      &lt;item&gt;“Entry point address” — Relatively self-explanatory! But we’ll be coming back to this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other ELF files will have different entries and specific values, but the general structure is what we’re after here.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As you can see by the numerous mentions to “RISC-V”, this is an ELF file I compiled and linked targeting the RV32 architecture (which the aforementioned emulator is built for), hence the “32” in “ELF32”, the “RVC” flag, and the “RISC-V” machine type.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;More than just a header&lt;/head&gt;
    &lt;p&gt;ELF files contain everything our program needs to run, including the code, data, symbols, and more. We can see this again with the &lt;code&gt;readelf&lt;/code&gt; command with the &lt;code&gt;-a&lt;/code&gt; flag. Here’s what we care about:&lt;/p&gt;
    &lt;code&gt;Section Headers:
  [Nr] Name              Type            Addr     Off    Size
  [ 0]                   NULL            00000000 000000 000000
  [ 1] .note.ABI-tag     NOTE            00010114 000114 000020
  [ 2] .rela.plt         RELA            00010134 000134 00000c
  [ 3] .plt              PROGBITS        00010140 000140 000010
  [ 4] .text             PROGBITS        00010150 000150 03e652
  [ 5] .rodata           PROGBITS        0004e7b0 03e7b0 01b208
  ...
  [16] .data             PROGBITS        0007a008 069008 000dec
  [17] .sdata            PROGBITS        0007adf4 069df4 000004
  [18] .bss              NOBITS          0007adf8 069df8 002b6c
  ...
  [29] .symtab           SYMTAB          00000000 095124 009040
  [30] .strtab           STRTAB          00000000 09e164 006d10&lt;/code&gt;
    &lt;p&gt;These sections contain code (&lt;code&gt;.text&lt;/code&gt;), data (&lt;code&gt;.data&lt;/code&gt;), space for global variables (&lt;code&gt;.bss&lt;/code&gt;), shims for accessing shared library functions (&lt;code&gt;.plt&lt;/code&gt;), and quite a bit more (including symbol tables for debugging, relocation tables, etc.), most of which we won’t be discussing.&lt;/p&gt;
    &lt;p&gt;So evidently, there’s some code that we care about in the &lt;code&gt;.text&lt;/code&gt; section, so we copy that and call it a day? Not quite. There’s a massive amount of machinery inside the kernel to make all sorts of programs under all sorts of conditions run.&lt;/p&gt;
    &lt;p&gt;For example, the “PLT” (Procedure Linkage Table) is a section that allows us to call functions in “shared libraries”, for example, &lt;code&gt;libc&lt;/code&gt;, without having to package them alongside our program (“dynamically” vs “statically linking”). The ELF file contains a dynamic section which tells the kernel which shared libraries to load, and another section which tells the kernel to dynamically “relocate” pointers to those functions, so everything checks out.&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;libc&lt;/code&gt;is the C standard library, which contains all the “useful” functions:&lt;code&gt;printf&lt;/code&gt;,&lt;code&gt;malloc&lt;/code&gt;, etc. Various flavors implementing the&lt;code&gt;libc&lt;/code&gt;interfaces exist, most commonly&lt;code&gt;glibc&lt;/code&gt;and&lt;code&gt;musl&lt;/code&gt;. Most of the binaries that are discussed in this post are compiled and linked against&lt;code&gt;musl&lt;/code&gt;, since it’s much easier to statically link.&lt;/quote&gt;
    &lt;p&gt;The symbol table looks something like this:&lt;/p&gt;
    &lt;code&gt;Symbol table '.symtab' contains 2308 entries:
   Num:    Value  Size Type    Bind   Vis      Ndx Name
     0: 00000000     0 NOTYPE  LOCAL  DEFAULT  UND
     1: 00010114     0 SECTION LOCAL  DEFAULT    1 .note.ABI-tag
     2: 00010134     0 SECTION LOCAL  DEFAULT    2 .rela.plt
     3: 00010140     0 SECTION LOCAL  DEFAULT    3 .plt
     4: 00010150     0 SECTION LOCAL  DEFAULT    4 .text
     ...
     1782: 00010358    30 FUNC    GLOBAL HIDDEN     4 _start
     ...
     1917: 00010430    52 FUNC    GLOBAL DEFAULT    4 main
     2201: 00010506   450 FUNC    GLOBAL HIDDEN     4 __libc_start_main
     ...&lt;/code&gt;
    &lt;p&gt;You may ask: “Wow! &lt;code&gt;2308&lt;/code&gt; looks like a lot, right? What behemoth of a program could possibly need that many symbols?“.&lt;/p&gt;
    &lt;p&gt;Good question! Here’s the behemoth:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main() {
  printf("Hello, World!\n");
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Yeah, that’s it. Now, &lt;code&gt;2308&lt;/code&gt; may be slightly bloated because we link against &lt;code&gt;musl&lt;/code&gt; instead of &lt;code&gt;glibc&lt;/code&gt;, but the point still stands: There’s a lot of stuff going on behind the scenes here.&lt;/p&gt;
    &lt;p&gt;The kernel’s job here is to iterate over each section, loading those marked as “loadable”. Some security mitigations start to become relevant here, such as moving sections around in memory (ASLR — Address Space Layout Randomization), marking sections as non-executable (NX bit — hardware-level security), etc. But ultimately, the kernel loads the code and data into memory, sets up the stack, and prepares to jump to the entry point of the program.&lt;/p&gt;
    &lt;head rend="h2"&gt;The stack&lt;/head&gt;
    &lt;p&gt;Ah yes, the infamous stack! Fortunately for most of us, the stack is something we take for granted. Unfortunately for the kernel, the stack is not some omnipotent magical space that just exists — it needs to be set up properly before our program can run.&lt;/p&gt;
    &lt;p&gt;As a reminder: stack space is typically used for variables, function arguments, “frames” (to keep track of function-local variables, call trees, etc), and a variety of other things, depending on what, and how your program is running.&lt;/p&gt;
    &lt;p&gt;Hypothetically, if we simplify a bit and say that the ELF file is loaded into memory starting at the zero address, the stack is typically placed at the “opposite end” of the memory, from a high address, and grows “downwards” towards the lower addresses, with the space in-between used as heap space, and for other data (shared libraries, mmapped files, etc). This is a simplification, but in fairness, there is significant ambiguity as much of the semantics here depend on the program itself.&lt;/p&gt;
    &lt;p&gt;The stack is also something that is non-empty! Remember &lt;code&gt;argv&lt;/code&gt; and &lt;code&gt;envp&lt;/code&gt; from the &lt;code&gt;execve&lt;/code&gt; call above? Those are passed to the program via the stack. In most programming languages we frequently access these via the various &lt;code&gt;args&lt;/code&gt; and &lt;code&gt;env&lt;/code&gt; utilities, whether that be directly, like in C, or more indirectly, like in Rust (&lt;code&gt;std::env&lt;/code&gt;) or Python (&lt;code&gt;sys.argv&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The kernel also stores something called the “ELF auxiliary vector” in the nascent stack. This “auxv” contains information about the environment, such as the memory page size, metadata from the ELF file, and other system information. These are important! For example, &lt;code&gt;musl&lt;/code&gt; uses the “page size” entry of the auxv so that &lt;code&gt;malloc&lt;/code&gt; can request and manage memory more optimally. There are over 30 entries in the auxiliary vectors, but not all of them are used by every program (and some may not be defined by the kernel).&lt;/p&gt;
    &lt;p&gt;Let’s pretend we’re the kernel. Here’s a simplified version of how we might setup the stack of a new process (taken and simplified from my RISC-V emulator, which also emulates parts of the kernel):&lt;/p&gt;
    &lt;code&gt;// Choose an arbitrary high address for the stack
let mut sp = 0xCFFF_F000u32; // sp = "stack pointer"
let mut stack_init: Vec&amp;lt;u32&amp;gt; = vec![]; // The stack begins empty.

stack_init.push(args.len()); // argc: number of arguments
for &amp;amp;arg in args.iter().rev() {
    // Copy each argument to the stack
    sp -= arg.len() // move "downwards" in address space
    mem.copy_to(sp, arg);

    // Keep track of the arg pointer in the init vector
    stack_init.push(sp);
}
stack_init.push(0); // argv NULL terminator

// Environment variables are similar:
for &amp;amp;e in env.iter().rev() {
    sp -= e.len();
    mem.copy_to(sp, e);

    stack_init.push(sp);
}
stack_init.push(0); // envp NULL terminator

// Setup the auxiliary vector
stack_init.push(libc_riscv32::AT_PAGESZ); // Keys for auxv
stack_init.push(0x1000); // Values for auxv; this specifies a 4 KiB page size
stack_init.push(libc_riscv32::AT_ENTRY);
stack_init.push(self.pc); // N.B.: We'll be coming back to this
// ...

// Copy the stack init vector, with all the pointers, to the stack
sp -= (stack_init.len() * 4);

mem.copy_to(sp, &amp;amp;stack_init)&lt;/code&gt;
    &lt;p&gt;A diagram might help illustrate what the address space looks like at this point:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Argument Count (argc)&lt;/item&gt;
      &lt;item&gt;Arguments (argv)&lt;/item&gt;
      &lt;item&gt;Environment variables (envp)&lt;/item&gt;
      &lt;item&gt;Auxiliary Vector (auxv)&lt;/item&gt;
      &lt;item&gt;Local variables, stack frames, function calls&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;↓&lt;/p&gt;
    &lt;p&gt;Grows downward&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shared libraries (libc, etc.)&lt;/item&gt;
      &lt;item&gt;Memory-mapped files&lt;/item&gt;
      &lt;item&gt;Dynamic linker/loader&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;↑&lt;/p&gt;
    &lt;p&gt;Grows upward&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;malloc(), calloc(), realloc() allocations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.bss &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uninitialized global variables&lt;/item&gt;
      &lt;item&gt;Static variables initialized to zero&lt;/item&gt;
      &lt;item&gt;Zero-filled by kernel on program start&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.data &lt;lb/&gt; "read-write"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Initialized global variables&lt;/item&gt;
      &lt;item&gt;Static variables with initial values&lt;/item&gt;
      &lt;item&gt;Read-write data from the executable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.rodata &lt;lb/&gt; "read-only"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Literals ("Hello, World!")&lt;/item&gt;
      &lt;item&gt;Constant data&lt;/item&gt;
      &lt;item&gt;Read-only variables&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.text &lt;lb/&gt; "code"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Program instructions/machine code&lt;/item&gt;
      &lt;item&gt;_start function (entry point)&lt;/item&gt;
      &lt;item&gt;User code&lt;/item&gt;
      &lt;item&gt;Library function code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Entrypoint&lt;/head&gt;
    &lt;p&gt;Finally, we get to the “entry point” address, mentioned at several points. This is the address of the first instruction to run in the process. Typically, this is under a function called &lt;code&gt;_start&lt;/code&gt;. Both glibc and musl provide implementations of &lt;code&gt;_start&lt;/code&gt;, but it’s also possible to write your own. Again, here’s a Rust example:&lt;/p&gt;
    &lt;code&gt;// Disable the language runtime, we're DIYing it.
#![no_std]
#![no_main]

#[panic_handler]
fn panic(_info: &amp;amp;core::panic::PanicInfo) -&amp;gt; ! {
    loop {}
}

#[no_mangle]
pub extern "C" fn _start() -&amp;gt; ! {
    // Instead of "waiting" for main, we can immediately start execution.
    loop {}
}&lt;/code&gt;
    &lt;p&gt;Depending on your program, &lt;code&gt;_start&lt;/code&gt; may be the only thing between the entrypoint and your main function, but most languages have some sort of runtime that needs to be initialized first. For example, Rust has &lt;code&gt;std::rt::lang_start&lt;/code&gt;. It’s at this part that things like global constructors, thread-local storage, and other language-specific features are set up.&lt;/p&gt;
    &lt;p&gt;Here, our journey comes to an end — things become much more language-specific from this point on. Most languages will set up their own runtimes (yes, even C and C++ have a “runtime”!), and eventually call the standard &lt;code&gt;main&lt;/code&gt; function we’re normally familiar with.&lt;/p&gt;
    &lt;p&gt;In Rust, the generated code ends up looking like the following:&lt;/p&gt;
    &lt;code&gt;// the user defined main function
fn main() { println!("Hello, world!"); }

// the generated _start function
fn _start() -&amp;gt; {
    let argc = ...; // get argc from stack
    let argv = ...; // get argv from stack
    let envp = ...; // get envp from stack
    let main_fn = main; // pointer to user main function
    std::rt::lang_start(argc, argv, main_fn);
}&lt;/code&gt;
    &lt;p&gt;With the &lt;code&gt;lang_start&lt;/code&gt; function (defined here)[https://github.com/rust-lang/rust/blob/04ff05c9c0cfbca33115c5f1b8bb20a66a54b799/library/std/src/rt.rs#L199] and taking care of the rest.&lt;/p&gt;
    &lt;p&gt;C and C++ have similar, minimal setups. Languages that are traditionally thought to have “heavier” runtimes, such as Java or Python, work the same way, but with the &lt;code&gt;std::rt::lang_start&lt;/code&gt; equivalent doing far more than the Rust/C/C++ runtimes.&lt;/p&gt;
    &lt;p&gt;And there you have it! I’m missing lots of detail here, but hopefully this gives a rough idea of what happens before &lt;code&gt;main()&lt;/code&gt; gets called. I’ve left out complexity that is mostly internal to “real” linux kernels, such as how the kernel sets up address space, the process tables, various group semantics, and et cetera, but I hope this still serves as a decent primer.&lt;/p&gt;
    &lt;p&gt;Feel free to reach out to me with any questions or corrections!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706380</guid><pubDate>Sat, 25 Oct 2025 19:33:22 +0000</pubDate></item><item><title>Show HN: Open-source shadcn/UI theme editor – design and share shadcn themes</title><link>https://shadcnthemer.com</link><description>&lt;doc fingerprint="5b5948a7543b02e7"&gt;
  &lt;main&gt;
    &lt;p&gt;All Themes GitHub ShadCN Themes Discover and create beautiful themes for shadcn/ui Import New Theme Filter by color: Red Orange Yellow Green Teal Blue Purple Pink Gray Black White Loading themes...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706487</guid><pubDate>Sat, 25 Oct 2025 19:51:24 +0000</pubDate></item><item><title>California invests in battery energy storage, leaving rolling blackouts behind</title><link>https://www.latimes.com/environment/story/2025-10-17/california-made-it-through-another-summer-without-a-flex-alert</link><description>&lt;doc fingerprint="235081ed63971539"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Share via&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;California hasn’t issued an emergency plea for the public to conserve energy, known as a Flex Alert, since 2022.&lt;/item&gt;
      &lt;item&gt;Experts credit much of the progress to a surge in battery energy storage systems in recent years.&lt;/item&gt;
      &lt;item&gt;Battery storage in California has grown more than 3,000% since 2020.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For decades, rolling blackouts and urgent calls for energy conservation were part of life in California — a reluctant summer ritual almost as reliable as the heat waves that drove them. But the state has undergone a quiet shift in recent years, and the California Independent System Operator hasn’t issued a single one of those emergency pleas, known as Flex Alerts, since 2022.&lt;/p&gt;
    &lt;p&gt;Experts and officials say the Golden State has reached a turning point, reflecting years of investment in making its electrical grid stronger, cleaner and more dependable. Much of that is new battery energy storage, which captures and stores electricity for later use.&lt;/p&gt;
    &lt;p&gt;In fact, batteries have been transformative for California, state officials say. In late afternoon, when the sun stops hitting solar panels and people are home using electricity, batteries now push stored solar energy onto the grid.&lt;/p&gt;
    &lt;p&gt;California has invested heavily in the technology, helping it mature and get cheaper in recent years. Battery storage in the state has grown more than 3,000% in six years — from 500 megawatts in 2020 to more than 15,700 megawatts today.&lt;/p&gt;
    &lt;p&gt;“There is no question that the battery fleet that has grown rapidly since 2020, along with the state’s expanding portfolio of other supply and demand-side resources, has been a real game changer for reliability during summer periods of peak demand,” said Elliot Mainzer, CAISO’s president and chief executive.&lt;/p&gt;
    &lt;p&gt;It was only five years ago that a record-shattering heat wave pushed the grid to its limit and plunged much of the state into darkness. In the wake of that event, California’s energy leaders vowed to take action to make the grid more resilient.&lt;/p&gt;
    &lt;p&gt;Since then, CAISO has overseen a massive build-out of new energy and storage resources, including more than 26,000 megawatts of new capacity overall, which has also helped make the grid more stable, Mainzer said. The state hasn’t seen rolling blackouts since 2020.&lt;/p&gt;
    &lt;p&gt;“Extreme weather events, wildfires and other emergencies can pose reliability challenges for any bulk electric system,” he said. “But the CAISO battery fleet, along with the additional capacity and close coordination with state and regional partners, have provided an indisputable benefit to reliability.”&lt;/p&gt;
    &lt;p&gt;An immense solar-plus-storage power plant in the desert is now pumping out inexpensive clean electricity at full bore.&lt;/p&gt;
    &lt;p&gt;Batteries are now key to California’s climate goals, including its mandate of 100% carbon neutrality by 2045.&lt;/p&gt;
    &lt;p&gt;Already, batteries have enabled the grid to operate with dramatic decreases in the use of planet-warming fossil fuels. Now they’re becoming a more cost-effective and reliable replacement for aging gas-fired power plants, according to Maia Leroy, founder of the California energy consulting firm Lumenergy LLC and co-author of a recent report on the rise of battery storage over gas generation in California.&lt;/p&gt;
    &lt;p&gt;“Historically, Flex Alerts have always come through in summertime when it’s super hot and everyone is cranking their AC,” Leroy said. “But also in the summertime, we’re seeing that gas plants underperform because combustion doesn’t work well with ambient heat. So when we’re able to shift that need from having to use gas plants to something more stable, dispatchable and flexible like battery storage, then we’re able to meet that demand in the summer without having to rely on those underperforming gas plants.”&lt;/p&gt;
    &lt;p&gt;Battery energy storage is not without challenges, however. Lithium-ion batteries — the most common type used for energy storage — typically have about four to six hours of capacity. It’s enough to support the grid during peak hours as the sun sets, but can still leave some gaps to be filled by natural gas.&lt;/p&gt;
    &lt;p&gt;Nikhil Kumar, program director with the energy policy nonprofit GridLab, said the technology already exists for longer-duration batteries, including through different chemistries such as iron-air batteries, which release energy through oxidation, and flow batteries, which store energy in liquid chemicals that flow through a reactor.&lt;/p&gt;
    &lt;p&gt;Those batteries are not yet as mature and can be more expensive and larger than their lithium-ion counterparts, Kumar said. But a recent GridLab report indicates that equation is changing, with the average cost of a new gas plant often on par with four-hour lithium-ion batteries and only slightly less expensive than longer-duration battery technologies.&lt;/p&gt;
    &lt;p&gt;“Batteries are going to get cheaper,” Kumar said. “Gas isn’t.”&lt;/p&gt;
    &lt;p&gt;The Trump administration said it will open 13 million acres of federal lands for coal mining and provide $625 million to recommission or modernize coal-fired power plants.&lt;/p&gt;
    &lt;p&gt;The battery storage shift is occurring as the Trump administration takes steps to stifle solar and other forms of renewable energy in favor of fossil fuels such as oil, gas and coal. At the end of September, the administration announced that it would open 13 million acres of federal lands for coal mining and provide $625 million to recommission or modernize coal-fired powered plants, which officials said would help strengthen the economy, protect jobs and advance American energy.&lt;/p&gt;
    &lt;p&gt;During an hourlong news conference on the initiative, Interior Secretary Doug Burgum described wind and solar energy as intermittent sources that are “literally dependent on the weather” — but neither he nor any other official mentioned the growth of battery storage that has made those sources more reliable and more promising.&lt;/p&gt;
    &lt;p&gt;It’s not a partisan issue. ERCOT, which operates Texas’ electrical grid, has more than 14,000 megawatts of batteries online, a nearly threefold increase from early 2023. California and Texas are constantly trading places as the top state for battery storage.&lt;/p&gt;
    &lt;p&gt;But Trump has made moves to support the production of batteries in the U.S. Currently, about three-quarters of the world’s batteries are made in China, and Trump’s tariffs — including a proposed 100% tariff on China — have been good for at least one Sacramento-based battery manufacturer, Sparkz.&lt;/p&gt;
    &lt;p&gt;“The administration wants critical material manufacturing to happen in the U.S.,” said Sanjiv Malhotra, founder and chief executive. “They basically are very much in favor of domestic manufacturing of batteries.”&lt;/p&gt;
    &lt;p&gt;Sparkz is making lithium-iron batteries that don’t use nickel and cobalt — a composition that has long been an industry darling but that depends on imported metals. Instead, their lithium-iron-phosphate batteries have a supply chain that is entirely based in the U.S., which means they can take advantage of federal tax credits that favor the production of clean energy components made mostly of domestic parts, Malhotra said. The company’s clients include data centers and utilities.&lt;/p&gt;
    &lt;p&gt;Malhotra added that California has done an excellent job “beefing up” the grid’s storage capacity in the last few years. He said batteries are a major reason why the state hasn’t seen a Flex Alert since 2022.&lt;/p&gt;
    &lt;p&gt;“The numbers basically tell the story that it was all because of, essentially, energy storage,” he said.&lt;/p&gt;
    &lt;p&gt;There is still work to do. While the state’s grid has seen improvements, it is more than a century old and was built primarily for gas plants. Experts and officials agree that it needs additional substantial upgrades and reforms to meet current energy demands and goals.&lt;/p&gt;
    &lt;p&gt;Permitting is also a hurdle, as California typically requires lengthy environmental review for new projects. The state, sometimes controversially, is now speeding review, and recently approved a massive solar and battery storage farm, the Darden Clean Energy Project in Fresno County, through a new fast-track permitting program. It will make enough electricity to power 850,000 homes for four hours, according to the California Energy Commission.&lt;/p&gt;
    &lt;p&gt;A plume of material released from the plant contained hydroflouride, a toxic gas, that is now being monitored by Monterey County.&lt;/p&gt;
    &lt;p&gt;Safety remains a considerable concern. In January, a fire tore through one of the world’s largest battery storage facilities in Moss Landing, Monterey County. The facility housed around 100,000 lithium-ion batteries, which are exceptionally dangerous when ignited because they burn extremely hot and cannot be extinguished with water, which can trigger a violent chemical reaction. The blaze emitted dangerous levels of nickel, cobalt and manganese that were measured within miles of the site.&lt;/p&gt;
    &lt;p&gt;“When you’re dealing with large technologies in general, there’s always going to be some kind of danger,” said Leroy, of Lumenergy. “This points to the big need for diversifying the technologies that we use.”&lt;/p&gt;
    &lt;p&gt;Other forms of energy, such as oil and coal, also pose considerable health and safety risks including the emission of air pollution — soot, mercury, nitrogen dioxide and carbon dioxide contributing to climate change.&lt;/p&gt;
    &lt;p&gt;California is in the process of eliminating coal power and expects to be completely coal-free by November. And while natural gas still makes up a large piece of the state’s portfolio, renewables represented nearly 60% of California’s in-state electricity generation in 2024, according to the U.S. Energy Information Administration.&lt;/p&gt;
    &lt;p&gt;The numbers continue to trend upward. In the first six months of this year, CAISO’s grid was powered by 100% clean energy for an average of almost seven hours each day.&lt;/p&gt;
    &lt;p&gt;“We have literally just demonstrated that California is able to run with super clean resources, with backups from natural gas,” said Kumar, of GridLab. “And it works. We don’t have Flex Alerts.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706527</guid><pubDate>Sat, 25 Oct 2025 19:58:55 +0000</pubDate></item><item><title>Show HN: Status of my favorite bike share stations</title><link>https://blog.alexboden.ca/toronto-bike-share-status/</link><description>&lt;doc fingerprint="1a4057c85b785679"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Toronto Bike Share Status&lt;/head&gt;
    &lt;p&gt;October 24, 2025&lt;/p&gt;
    &lt;p&gt;I love Toronto’s Bike Share network, but often times the bike station closest to my place has no bikes or the place I’m heading to has no docks. The Bike Share app is a bit clunky, requiring multiple taps and scrolling to check all of this info. In order to save those few extra seconds in the morning, I did the logical thing and spent a multiple hours to make a simple dashboard and iOS widget that surfaces the info I need at a glance.&lt;/p&gt;
    &lt;p&gt;Check out the project on GitHub&lt;/p&gt;
    &lt;p&gt;Here are some screenshots of the dashboard and widget in action:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706570</guid><pubDate>Sat, 25 Oct 2025 20:04:50 +0000</pubDate></item><item><title>In memory of the Christmas Island shrew</title><link>https://news.mongabay.com/2025/10/in-memory-of-the-christmas-island-shrew/</link><description>&lt;doc fingerprint="9773118fc882821f"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once abundant on Christmas Island, the tiny, five-gram shrew (Crocidura trichura) filled the night forest with its high, thin cry before vanishing into silence.&lt;/item&gt;
      &lt;item&gt;Introduced black rats and their parasites decimated the island’s native mammals, and by 1908 the shrew was thought extinct, its memory confined to museum drawers and field notes.&lt;/item&gt;
      &lt;item&gt;Brief rediscoveries in 1958 and 1984 brought fleeting hope, but the last known individuals died in captivity, and no others have been found despite decades of searching.&lt;/item&gt;
      &lt;item&gt;Its loss, now made official, adds to Australia’s grim record of extinctions—a quiet reminder of fragile lives erased by invasion, neglect, and the noise of human expansion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It never weighed more than a spoonful of sugar. Five or six grams of life, soft-furred and sharp-nosed, darting among the roots and leaf litter of a tiny island in the Indian Ocean. At night, its voice—a thin, high cry, part bat and part whisper—once filled the forest of Christmas Island. Now the forest is silent. Australia’s only shrew, Crocidura trichura, has been declared extinct.&lt;/p&gt;
    &lt;p&gt;Few knew it lived, fewer still that it was Australian. The shrew was a stranger in a land of pouched mammals, a migrant that arrived tens of thousands of years ago, likely clinging to a raft of vegetation from what is now Indonesia. On this isolated outpost, it built a quiet lineage of survivors. When British naturalists arrived in the 1890s, they found the forest alive with its shrill chatter. “Extremely common,” they wrote. And then, almost at once, it vanished.&lt;/p&gt;
    &lt;p&gt;The black rats came first, stowaways in bales of hay. With them came a parasite, Trypanosoma lewisi, that swept through the island’s naïve mammals like a plague. Within years, both native rats were gone. By 1908, the shrew was presumed lost too. Its name lingered only in museum drawers and in the footnotes of field reports.&lt;/p&gt;
    &lt;p&gt;Yet it was not quite gone. Half a century later, in 1958, two shrews appeared as bulldozers tore into the forest for phosphate mining. They were seen, released, and forgotten. Then, in 1984, came a miracle: a live female, found in a clump of fern by biologists clearing a path. For more than a year, she lived in a terrarium, fed on grasshoppers and care. A few months later, a male was caught. The world briefly held its breath for a reunion that might save a species. But the male, sickly and short-tempered, died within weeks. The female lingered alone until she, too, was gone.&lt;/p&gt;
    &lt;p&gt;No others were ever found. Searches in the following decades brought only silence—the kind of silence that deepens until it becomes its own proof. When scientists dissected hundreds of feral cats on the island, not a trace of shrew remained in their stomachs. The Red List, in its latest revision, made official what many already knew in their hearts: Crocidura trichura was no more.&lt;/p&gt;
    &lt;p&gt;To some, the loss of a creature so small may seem inconsequential. Yet its passing adds one more mark to Australia’s lamentable record—the thirty-ninth mammal species lost since colonization, more than any other country on Earth. The shrew’s absence is a story repeated across islands: an ancient ecosystem undone by the carelessness of arrival, by rats and cats, ants and snakes, by the unthinking traffic of an expanding world.&lt;/p&gt;
    &lt;p&gt;The Christmas Island shrew had survived what many thought impossible. For decades, it persisted unseen—a shadow among roots, defying extinction. It was officially rediscovered, officially lost, and then, improbably, rediscovered again. It endured eighty years of disappearance before the recorders caught up. That endurance was its last act of defiance.&lt;/p&gt;
    &lt;p&gt;In life, it asked for little: a patch of soil, a few beetles, a quiet forest. In death, it leaves questions that are larger than itself. How many other lives flicker out unseen before the world even learns their names? How many others wait somewhere in the darkness, unseen but breathing still?&lt;/p&gt;
    &lt;p&gt;There is always a chance—slim but not zero—that the shrew endures yet, hidden in the damp heart of Christmas Island, trembling but alive. Hope, after all, has a long history of outliving the species it mourns. But the forest is quieter now. And if this really is the end, the last of Australia’s shrews will have gone as it lived—small, secret, and almost entirely unnoticed, save for those who loved it enough to listen for its cry.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706624</guid><pubDate>Sat, 25 Oct 2025 20:13:55 +0000</pubDate></item><item><title>Load-time relocation of shared libraries (2011)</title><link>https://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries/</link><description>&lt;doc fingerprint="2f8599532e059648"&gt;
  &lt;main&gt;
    &lt;p&gt;This article's aim is to explain how a modern operating system makes it possible to use shared libraries with load-time relocation. It focuses on the Linux OS running on 32-bit x86, but the general principles apply to other OSes and CPUs as well.&lt;/p&gt;
    &lt;p&gt;Note that shared libraries have many names - shared libraries, shared objects, dynamic shared objects (DSOs), dynamically linked libraries (DLLs - if you're coming from a Windows background). For the sake of consistency, I will try to just use the name "shared library" throughout this article.&lt;/p&gt;
    &lt;head rend="h3"&gt;Loading executables&lt;/head&gt;
    &lt;p&gt;Linux, similarly to other OSes with virtual memory support, loads executables to a fixed memory address. If we examine the ELF header of some random executable, we'll see an Entry point address:&lt;/p&gt;
    &lt;code&gt;$ readelf -h /usr/bin/uptime
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF32
  [...] some header fields
  Entry point address:               0x8048470
  [...] some header fields
&lt;/code&gt;
    &lt;p&gt;This is placed by the linker to tell the OS where to start executing the executable's code [1]. And indeed if we then load the executable with GDB and examine the address 0x8048470, we'll see the first instructions of the executable's .text segment there.&lt;/p&gt;
    &lt;p&gt;What this means is that the linker, when linking the executable, can fully resolve all internal symbol references (to functions and data) to fixed and final locations. The linker does some relocations of its own [2], but eventually the output it produces contains no additional relocations.&lt;/p&gt;
    &lt;p&gt;Or does it? Note that I emphasized the word internal in the previous paragraph. As long as the executable needs no shared libraries [3], it needs no relocations. But if it does use shared libraries (as do the vast majority of Linux applications), symbols taken from these shared libraries need to be relocated, because of how shared libraries are loaded.&lt;/p&gt;
    &lt;head rend="h3"&gt;Load-time relocation in action&lt;/head&gt;
    &lt;p&gt;To see the load-time relocation in action, I will use our shared library from a simple driver executable. When running this executable, the OS will load the shared library and relocate it appropriately.&lt;/p&gt;
    &lt;p&gt;Curiously, due to the address space layout randomization feature which is enabled in Linux, relocation is relatively difficult to follow, because every time I run the executable, the libmlreloc.so shared library gets placed in a different virtual memory address [9].&lt;/p&gt;
    &lt;p&gt;This is a rather weak deterrent, however. There is a way to make sense in it all. But first, let's talk about the segments our shared library consists of:&lt;/p&gt;
    &lt;code&gt;$ readelf --segments libmlreloc.so

Elf file type is DYN (Shared object file)
Entry point 0x3b0
There are 6 program headers, starting at offset 52

Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  LOAD           0x000000 0x00000000 0x00000000 0x004e8 0x004e8 R E 0x1000
  LOAD           0x000f04 0x00001f04 0x00001f04 0x0010c 0x00114 RW  0x1000
  DYNAMIC        0x000f18 0x00001f18 0x00001f18 0x000d0 0x000d0 RW  0x4
  NOTE           0x0000f4 0x000000f4 0x000000f4 0x00024 0x00024 R   0x4
  GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0x4
  GNU_RELRO      0x000f04 0x00001f04 0x00001f04 0x000fc 0x000fc R   0x1

 Section to Segment mapping:
  Segment Sections...
   00     .note.gnu.build-id .hash .gnu.hash .dynsym .dynstr .gnu.version .gnu.version_r .rel.dyn .rel.plt .init .plt .text .fini .eh_frame
   01     .ctors .dtors .jcr .dynamic .got .got.plt .data .bss
   02     .dynamic
   03     .note.gnu.build-id
   04
   05     .ctors .dtors .jcr .dynamic .got
&lt;/code&gt;
    &lt;p&gt;To follow the myglob symbol, we're interested in the second segment listed here. Note a couple of things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the section to segment mapping in the bottom, segment 01 is said to contain the .data section, which is the home of myglob&lt;/item&gt;
      &lt;item&gt;The VirtAddr column specifies that the second segment starts at 0x1f04 and has size 0x10c, meaning that it extends until 0x2010 and thus contains myglob which is at 0x200C.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now let's use a nice tool Linux gives us to examine the load-time linking process - the dl_iterate_phdr function, which allows an application to inquire at runtime which shared libraries it has loaded, and more importantly - take a peek at their program headers.&lt;/p&gt;
    &lt;p&gt;So I'm going to write the following code into driver.c:&lt;/p&gt;
    &lt;code&gt;#define _GNU_SOURCE
#include &amp;lt;link.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;


static int header_handler(struct dl_phdr_info* info, size_t size, void* data)
{
    printf("name=%s (%d segments) address=%p\n",
            info-&amp;gt;dlpi_name, info-&amp;gt;dlpi_phnum, (void*)info-&amp;gt;dlpi_addr);
    for (int j = 0; j &amp;lt; info-&amp;gt;dlpi_phnum; j++) {
         printf("\t\t header %2d: address=%10p\n", j,
             (void*) (info-&amp;gt;dlpi_addr + info-&amp;gt;dlpi_phdr[j].p_vaddr));
         printf("\t\t\t type=%u, flags=0x%X\n",
                 info-&amp;gt;dlpi_phdr[j].p_type, info-&amp;gt;dlpi_phdr[j].p_flags);
    }
    printf("\n");
    return 0;
}


extern int ml_func(int, int);


int main(int argc, const char* argv[])
{
    dl_iterate_phdr(header_handler, NULL);

    int t = ml_func(argc, argc);
    return t;
}
&lt;/code&gt;
    &lt;p&gt;header_handler implements the callback for dl_iterate_phdr. It will get called for all libraries and report their names and load addresses, along with all their segments. It also invokes ml_func, which is taken from the libmlreloc.so shared library.&lt;/p&gt;
    &lt;p&gt;To compile and link this driver with our shared library, run:&lt;/p&gt;
    &lt;code&gt;gcc -g -c driver.c -o driver.o
gcc -o driver driver.o -L. -lmlreloc
&lt;/code&gt;
    &lt;p&gt;Running the driver stand-alone we get the information, but for each run the addresses are different. So what I'm going to do is run it under gdb [10], see what it says, and then use gdb to further query the process's memory space:&lt;/p&gt;
    &lt;code&gt; $ gdb -q driver
 Reading symbols from driver...done.
 (gdb) b driver.c:31
 Breakpoint 1 at 0x804869e: file driver.c, line 31.
 (gdb) r
 Starting program: driver
 [...] skipping output
 name=./libmlreloc.so (6 segments) address=0x12e000
                header  0: address=  0x12e000
                        type=1, flags=0x5
                header  1: address=  0x12ff04
                        type=1, flags=0x6
                header  2: address=  0x12ff18
                        type=2, flags=0x6
                header  3: address=  0x12e0f4
                        type=4, flags=0x4
                header  4: address=  0x12e000
                        type=1685382481, flags=0x6
                header  5: address=  0x12ff04
                        type=1685382482, flags=0x4

[...] skipping output
 Breakpoint 1, main (argc=1, argv=0xbffff3d4) at driver.c:31
 31    }
 (gdb)
&lt;/code&gt;
    &lt;p&gt;Since driver reports all the libraries it loads (even implicitly, like libc or the dynamic loader itself), the output is lengthy and I will just focus on the report about libmlreloc.so. Note that the 6 segments are the same segments reported by readelf, but this time relocated into their final memory locations.&lt;/p&gt;
    &lt;p&gt;Let's do some math. The output says libmlreloc.so was placed in virtual address 0x12e000. We're interested in the second segment, which as we've seen in readelf is at ofset 0x1f04. Indeed, we see in the output it was loaded to address 0x12ff04. And since myglob is at offset 0x200c in the file, we'd expect it to now be at address 0x13000c.&lt;/p&gt;
    &lt;p&gt;So, let's ask GDB:&lt;/p&gt;
    &lt;code&gt;(gdb) p &amp;amp;myglob
$1 = (int *) 0x13000c
&lt;/code&gt;
    &lt;p&gt;Excellent! But what about the code of ml_func which refers to myglob? Let's ask GDB again:&lt;/p&gt;
    &lt;code&gt;(gdb) set disassembly-flavor intel
(gdb) disas ml_func
Dump of assembler code for function ml_func:
   0x0012e46c &amp;lt;+0&amp;gt;:   push   ebp
   0x0012e46d &amp;lt;+1&amp;gt;:   mov    ebp,esp
   0x0012e46f &amp;lt;+3&amp;gt;:   mov    eax,ds:0x13000c
   0x0012e474 &amp;lt;+8&amp;gt;:   add    eax,DWORD PTR [ebp+0x8]
   0x0012e477 &amp;lt;+11&amp;gt;:  mov    ds:0x13000c,eax
   0x0012e47c &amp;lt;+16&amp;gt;:  mov    eax,ds:0x13000c
   0x0012e481 &amp;lt;+21&amp;gt;:  add    eax,DWORD PTR [ebp+0xc]
   0x0012e484 &amp;lt;+24&amp;gt;:  pop    ebp
   0x0012e485 &amp;lt;+25&amp;gt;:  ret
End of assembler dump.
&lt;/code&gt;
    &lt;p&gt;As expected, the real address of myglob was placed in all the mov instructions referring to it, just as the relocation entries specified.&lt;/p&gt;
    &lt;head rend="h3"&gt;Relocating function calls&lt;/head&gt;
    &lt;p&gt;So far this article demonstrated relocation of data references - using the global variable myglob as an example. Another thing that needs to be relocated is code references - in other words, function calls. This section is a brief guide on how this gets done. The pace is much faster than in the rest of this article, since I can now assume the reader understands what relocation is all about.&lt;/p&gt;
    &lt;p&gt;Without further ado, let's get to it. I've modified the code of the shared library to be the following:&lt;/p&gt;
    &lt;code&gt;int myglob = 42;

int ml_util_func(int a)
{
    return a + 1;
}

int ml_func(int a, int b)
{
    int c = b + ml_util_func(a);
    myglob += c;
    return b + myglob;
}
&lt;/code&gt;
    &lt;p&gt;ml_util_func was added and it's being used by ml_func. Here's the disassembly of ml_func in the linked shared library:&lt;/p&gt;
    &lt;code&gt;000004a7 &amp;lt;ml_func&amp;gt;:
 4a7:   55                      push   ebp
 4a8:   89 e5                   mov    ebp,esp
 4aa:   83 ec 14                sub    esp,0x14
 4ad:   8b 45 08                mov    eax,DWORD PTR [ebp+0x8]
 4b0:   89 04 24                mov    DWORD PTR [esp],eax
 4b3:   e8 fc ff ff ff          call   4b4 &amp;lt;ml_func+0xd&amp;gt;
 4b8:   03 45 0c                add    eax,DWORD PTR [ebp+0xc]
 4bb:   89 45 fc                mov    DWORD PTR [ebp-0x4],eax
 4be:   a1 00 00 00 00          mov    eax,ds:0x0
 4c3:   03 45 fc                add    eax,DWORD PTR [ebp-0x4]
 4c6:   a3 00 00 00 00          mov    ds:0x0,eax
 4cb:   a1 00 00 00 00          mov    eax,ds:0x0
 4d0:   03 45 0c                add    eax,DWORD PTR [ebp+0xc]
 4d3:   c9                      leave
 4d4:   c3                      ret
&lt;/code&gt;
    &lt;p&gt;What's interesting here is the instruction at address 0x4b3 - it's the call to ml_util_func. Let's dissect it:&lt;/p&gt;
    &lt;p&gt;e8 is the opcode for call. The argument of this call is the offset relative to the next instruction. In the disassembly above, this argument is 0xfffffffc, or simply -4. So the call currently points to itself. This clearly isn't right - but let's not forget about relocation. Here's what the relocation section of the shared library looks like now:&lt;/p&gt;
    &lt;code&gt;$ readelf -r libmlreloc.so

Relocation section '.rel.dyn' at offset 0x324 contains 8 entries:
 Offset     Info    Type            Sym.Value  Sym. Name
00002008  00000008 R_386_RELATIVE
000004b4  00000502 R_386_PC32        0000049c   ml_util_func
000004bf  00000401 R_386_32          0000200c   myglob
000004c7  00000401 R_386_32          0000200c   myglob
000004cc  00000401 R_386_32          0000200c   myglob
[...] skipping stuff
&lt;/code&gt;
    &lt;p&gt;If we compare it to the previous invocation of readelf -r, we'll notice a new entry added for ml_util_func. This entry points at address 0x4b4 which is the argument of the call instruction, and its type is R_386_PC32. This relocation type is more complicated than R_386_32, but not by much.&lt;/p&gt;
    &lt;p&gt;It means the following: take the value at the offset specified in the entry, add the address of the symbol to it, subtract the address of the offset itself, and place it back into the word at the offset. Recall that this relocation is done at load-time, when the final load addresses of the symbol and the relocated offset itself are already known. These final addresses participate in the computation.&lt;/p&gt;
    &lt;p&gt;What does this do? Basically, it's a relative relocation, taking its location into account and thus suitable for arguments of instructions with relative addressing (which the e8 call is). I promise it will become clearer once we get to the real numbers.&lt;/p&gt;
    &lt;p&gt;I'm now going to build the driver code and run it under GDB again, to see this relocation in action. Here's the GDB session, followed by explanations:&lt;/p&gt;
    &lt;code&gt; $ gdb -q driver
 Reading symbols from driver...done.
 (gdb) b driver.c:31
 Breakpoint 1 at 0x804869e: file driver.c, line 31.
 (gdb) r
 Starting program: driver
 [...] skipping output
 name=./libmlreloc.so (6 segments) address=0x12e000
               header  0: address=  0x12e000
                       type=1, flags=0x5
               header  1: address=  0x12ff04
                       type=1, flags=0x6
               header  2: address=  0x12ff18
                       type=2, flags=0x6
               header  3: address=  0x12e0f4
                       type=4, flags=0x4
               header  4: address=  0x12e000
                       type=1685382481, flags=0x6
               header  5: address=  0x12ff04
                       type=1685382482, flags=0x4

[...] skipping output
Breakpoint 1, main (argc=1, argv=0xbffff3d4) at driver.c:31
31    }
(gdb)  set disassembly-flavor intel
(gdb) disas ml_util_func
Dump of assembler code for function ml_util_func:
   0x0012e49c &amp;lt;+0&amp;gt;:   push   ebp
   0x0012e49d &amp;lt;+1&amp;gt;:   mov    ebp,esp
   0x0012e49f &amp;lt;+3&amp;gt;:   mov    eax,DWORD PTR [ebp+0x8]
   0x0012e4a2 &amp;lt;+6&amp;gt;:   add    eax,0x1
   0x0012e4a5 &amp;lt;+9&amp;gt;:   pop    ebp
   0x0012e4a6 &amp;lt;+10&amp;gt;:  ret
End of assembler dump.
(gdb) disas /r ml_func
Dump of assembler code for function ml_func:
   0x0012e4a7 &amp;lt;+0&amp;gt;:    55     push   ebp
   0x0012e4a8 &amp;lt;+1&amp;gt;:    89 e5  mov    ebp,esp
   0x0012e4aa &amp;lt;+3&amp;gt;:    83 ec 14       sub    esp,0x14
   0x0012e4ad &amp;lt;+6&amp;gt;:    8b 45 08       mov    eax,DWORD PTR [ebp+0x8]
   0x0012e4b0 &amp;lt;+9&amp;gt;:    89 04 24       mov    DWORD PTR [esp],eax
   0x0012e4b3 &amp;lt;+12&amp;gt;:   e8 e4 ff ff ff call   0x12e49c &amp;lt;ml_util_func&amp;gt;
   0x0012e4b8 &amp;lt;+17&amp;gt;:   03 45 0c       add    eax,DWORD PTR [ebp+0xc]
   0x0012e4bb &amp;lt;+20&amp;gt;:   89 45 fc       mov    DWORD PTR [ebp-0x4],eax
   0x0012e4be &amp;lt;+23&amp;gt;:   a1 0c 00 13 00 mov    eax,ds:0x13000c
   0x0012e4c3 &amp;lt;+28&amp;gt;:   03 45 fc       add    eax,DWORD PTR [ebp-0x4]
   0x0012e4c6 &amp;lt;+31&amp;gt;:   a3 0c 00 13 00 mov    ds:0x13000c,eax
   0x0012e4cb &amp;lt;+36&amp;gt;:   a1 0c 00 13 00 mov    eax,ds:0x13000c
   0x0012e4d0 &amp;lt;+41&amp;gt;:   03 45 0c       add    eax,DWORD PTR [ebp+0xc]
   0x0012e4d3 &amp;lt;+44&amp;gt;:   c9     leave
   0x0012e4d4 &amp;lt;+45&amp;gt;:   c3     ret
End of assembler dump.
(gdb)
&lt;/code&gt;
    &lt;p&gt;The important parts here are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In the printout from driver we see that the first segment (the code segment) of libmlreloc.so has been mapped to 0x12e000 [11]&lt;/item&gt;
      &lt;item&gt;ml_util_func was loaded to address 0x0012e49c&lt;/item&gt;
      &lt;item&gt;The address of the relocated offset is 0x0012e4b4&lt;/item&gt;
      &lt;item&gt;The call in ml_func to ml_util_func was patched to place 0xffffffe4 in the argument (I disassembled ml_func with the /r flag to show raw hex in addition to disassembly), which is interpreted as the correct offset to ml_util_func.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Obviously we're most interested in how (4) was done. Again, it's time for some math. Interpreting the R_386_PC32 relocation entry mentioned above, we have:&lt;/p&gt;
    &lt;p&gt;Take the value at the offset specified in the entry (0xfffffffc), add the address of the symbol to it (0x0012e49c), subtract the address of the offset itself (0x0012e4b4), and place it back into the word at the offset. Everything is done assuming 32-bit 2-s complement, of course. The result is 0xffffffe4, as expected.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extra credit: Why was the call relocation needed?&lt;/head&gt;
    &lt;p&gt;This is a "bonus" section that discusses some peculiarities of the implementation of shared library loading in Linux. If all you wanted was to understand how relocations are done, you can safely skip it.&lt;/p&gt;
    &lt;p&gt;When trying to understand the call relocation of ml_util_func, I must admit I scratched my head for some time. Recall that the argument of call is a relative offset. Surely the offset between the call and ml_util_func itself doesn't change when the library is loaded - they both are in the code segment which gets moved as one whole chunk. So why is the relocation needed at all?&lt;/p&gt;
    &lt;p&gt;Here's a small experiment to try: go back to the code of the shared library, add static to the declaration of ml_util_func. Re-compile and look at the output of readelf -r again.&lt;/p&gt;
    &lt;p&gt;Done? Anyway, I will reveal the outcome - the relocation is gone! Examine the disassembly of ml_func - there's now a correct offset placed as the argument of call - no relocation required. What's going on?&lt;/p&gt;
    &lt;p&gt;When tying global symbol references to their actual definitions, the dynamic loader has some rules about the order in which shared libraries are searched. The user can also influence this order by setting the LD_PRELOAD environment variable.&lt;/p&gt;
    &lt;p&gt;There are too many details to cover here, so if you're really interested you'll have to take a look at the ELF standard, the dynamic loader man page and do some Googling. In short, however, when ml_util_func is global, it may be overridden in the executable or another shared library, so when linking our shared library, the linker can't just assume the offset is known and hard-code it [12]. It makes all references to global symbols relocatable in order to allow the dynamic loader to decide how to resolve them. This is why declaring the function static makes a difference - since it's no longer global or exported, the linker can hard-code its offset in the code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Load-time relocation is one of the methods used in Linux (and other OSes) to resolve internal data and code references in shared libraries when loading them into memory. These days, position independent code (PIC) is a more popular approach, and some modern systems (such as x86-64) no longer support load-time relocation.&lt;/p&gt;
    &lt;p&gt;Still, I decided to write an article on load-time relocation for two reasons. First, load-time relocation has a couple of advantages over PIC on some systems, especially in terms of performance. Second, load-time relocation is IMHO simpler to understand without prior knowledge, which will make PIC easier to explain in the future. (Update 03.11.2011: the article about PIC was published)&lt;/p&gt;
    &lt;p&gt;Regardless of the motivation, I hope this article has helped to shed some light on the magic going behind the scenes of linking and loading shared libraries in a modern OS.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;For some more information about this entry point, see the section "Digression â process addresses and entry point" of this article.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[2]&lt;/cell&gt;
        &lt;cell&gt;Link-time relocation happens in the process of combining multiple object files into an executable (or shared library). It involves quite a lot of relocations to resolve symbol references between the object files. Link-time relocation is a more complex topic than load-time relocation, and I won't cover it in this article.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[3]&lt;/cell&gt;
        &lt;cell&gt;This can be made possible by compiling all your libraries into static libraries (with ar combining object files instead gcc -shared), and providing the -static flag to gcc when linking the executable - to avoid linkage with the shared version of libc.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[4]&lt;/cell&gt;
        &lt;cell&gt;ml simply stands for "my library". Also, the code itself is absolutely non-sensical and only used for purposes of demonstration.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[5]&lt;/cell&gt;
        &lt;cell&gt;Also called "dynamic linker". It's a shared object itself (though it can also run as an executable), residing at /lib/ld-linux.so.2 (the last number is the SO version and may be different).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[6]&lt;/cell&gt;
        &lt;cell&gt;If you're not familiar with how x86 structures its stack frames, this would be a good time to read this article.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[7]&lt;/cell&gt;
        &lt;cell&gt;You can provide the -l flag to objdump to add C source lines into the disassembly, making it clearer what gets compiled to what. I've omitted it here to make the output shorter.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[8]&lt;/cell&gt;
        &lt;cell&gt;I'm looking at the left-hand side of the output of objdump, where the raw memory bytes are. a1 00 00 00 00 means mov to eax with operand 0x0, which is interpreted by the disassembler as ds:0x0.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[9]&lt;/cell&gt;
        &lt;cell&gt;So ldd invoked on the executable will report a different load address for the shared library each time it's run.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[10]&lt;/cell&gt;
        &lt;cell&gt;Experienced readers will probably note that I could ask GDB about i shared to get the load-address of the shared library. However, i shared only mentions the load location of the whole library (or, even more accurately, its entry point), and I was interested in the segments.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[11]&lt;/cell&gt;
        &lt;cell&gt;What, 0x12e000 again? Didn't I just talk about load-address randomization? It turns out the dynamic loader can be manipulated to turn this off, for purposes of debugging. This is exactly what GDB is doing.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[12]&lt;/cell&gt;
        &lt;cell&gt;Unless it's passed the -Bsymbolic flag. Read all about it in the man page of ld.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706660</guid><pubDate>Sat, 25 Oct 2025 20:19:36 +0000</pubDate></item><item><title>Violent crime plummets 36% in downtown Seattle, lowest since 2017</title><link>https://mynorthwest.com/crime-blotter/seattle-downtown-crime/4146723</link><description>&lt;doc fingerprint="5751b8baed79a0cf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Violent crime plummets 36% in downtown Seattle, lowest since 2017&lt;/head&gt;
    &lt;p&gt;Oct 24, 2025, 1:30 PM&lt;/p&gt;
    &lt;p&gt;(Photo: Jason Rantz, KTTH)&lt;/p&gt;
    &lt;p&gt;Seattle is showing encouraging crime statistics as violent crime in the heart of the city has fallen to its lowest summer level since 2017, according to the data from the Downtown Seattle Association (DSA).&lt;/p&gt;
    &lt;p&gt;Between June and August 2025, officials reported that violent crime incidents in downtown Seattle dropped by 36% compared to the same period in 2024. This marks the lowest total for the summer months since 2017.&lt;/p&gt;
    &lt;head rend="h2"&gt;Downtown Seattle crime drops, economy rebounds&lt;/head&gt;
    &lt;p&gt;On the economic side, the DSA’s September 2025 dashboard revealed that downtown visitor numbers have bounced back to slightly above levels seen in September 2019.&lt;/p&gt;
    &lt;p&gt;Hotel room demand has returned to approximately 100% of 2019 levels. The number of occupied apartment units in downtown has grown to nearly 60,700, a roughly 20% increase compared with 2019.&lt;/p&gt;
    &lt;p&gt;City and business leaders said the drop in violent crime is an important sign of progress. Still, they said the recovery is not yet complete.&lt;/p&gt;
    &lt;p&gt;Worker foot traffic in downtown remains at about 62% of the weekday volume recorded in September 2019, showing that there is still room for more office workers to return.&lt;/p&gt;
    &lt;p&gt;For everyone in downtown, from businesses to residents and visitors, the news is looking brighter. With violent crime down and the economy showing strong signs of life, numbers indicate downtown Seattle is finally turning a corner.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706690</guid><pubDate>Sat, 25 Oct 2025 20:24:25 +0000</pubDate></item><item><title>Testing out BLE beacons with BeaconDB</title><link>https://blog.matthewbrunelle.com/testing-out-ble-beacons-with-beacondb/</link><description>&lt;doc fingerprint="ae4260d429ba004c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Testing Out BLE Beacons With beaconDB&lt;/head&gt;
    &lt;head rend="h2"&gt;What on earth is beaconDB?&lt;/head&gt;
    &lt;p&gt;I've been using GrapheneOS for about half a year now. Back in March they added support for network based location.[^0] This means you no longer need to rely on Google's location services. Looking into how the system works sent me down yet another rabbit hole of reading. [1]&lt;/p&gt;
    &lt;p&gt;Anyways, in 2013 Mozilla launched Mozilla Location Service (MLS) as a pilot project to provide location lookup using observations of public cell towers, BLE Beacons and WiFi access points. Sadly, in 2024 Mozilla retired MLS. Thankfully, beaconDB launched to continue the work! [2]&lt;/p&gt;
    &lt;p&gt;I have been hacking away on a project for contributing observations to beaconDB and I wanted some BLE beacons I could use for testing. This experiment sort of spun off from that work.&lt;/p&gt;
    &lt;p&gt;The plan is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Buy some BLE beacons.&lt;/item&gt;
      &lt;item&gt;Get their MAC addresses.&lt;/item&gt;
      &lt;item&gt;Query the beaconDB API to confirm no location is currently associated with the beacons.&lt;/item&gt;
      &lt;item&gt;Place the beacons in my yard. [3]&lt;/item&gt;
      &lt;item&gt;Take my dog on a walk around the block while running NeoStumbler.&lt;/item&gt;
      &lt;item&gt;Re-run the API query to see location estimates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What on earth are BLE Beacons?&lt;/head&gt;
    &lt;p&gt;I've been writing the phrase BLE beacons a lot without describing what they are. So to disambiguate [4]:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bluetooth - a wireless communication standard.&lt;/item&gt;
      &lt;item&gt;Bluetooth Low Energy - part of the Bluetooth 4.0 protocol, much lower power consumption, but also reduced transmission rates.&lt;/item&gt;
      &lt;item&gt;BLE beacons - BLE devices that are primarily transmit only.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Stationary BLE beacons are often used to mark locations in places where GPS signals are weak, like inside malls. Also, there is no single BLE beacon standard. Instead we have:&lt;/p&gt;
    &lt;p&gt;iBeacon which was released by Apple in 2013. Apple generally still supports them.&lt;/p&gt;
    &lt;p&gt;In 2014 Google launched the experimental URIBeacon. Then in 2015 Google replaced that with Eddystone. A one point Google was really into the concept of the Physical Web, but thankfully gave up on spamming users with notifications in 2018. This effectively reduced Google's involvement with the standard. Eddystone also powers the Waze beacons that saves me from missing my exit in the Ted Williams Tunnel.&lt;/p&gt;
    &lt;p&gt;AltBeacon was released in 2014 as an open standard. There are also other less used beacon standards out there.&lt;/p&gt;
    &lt;p&gt;Also, since BLE beacons are just BLE devices with extra details attached to the broadcast information, both iPhones and Android devices can scan for any standard. Best of all, most modern beacon devices should support broadcasting multiple beacon types at the same time.&lt;/p&gt;
    &lt;p&gt;In terms of collecting information about all these different types of beacons, Neostumbler uses the Android Beacon Library, which can detect the three main beacon types. Though they want to move away from the library so they can support custom scanning intervals.&lt;/p&gt;
    &lt;head rend="h2"&gt;Which BLE beacons did I choose?&lt;/head&gt;
    &lt;p&gt;When I was surveying the options I saw that many beacons included features like motion detection, lights, buttons, sound, etc. I wanted a stationary beacon with a long battery life, so I tried to avoid extra features if possible to keep the cost down. Additionally there are BLE beacons designed for broadcasting over extra long ranges. However the Bluetooth / WiFi accuracy section of the MLS documentation notes:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Bluetooth and WiFi networks have a fairly limited range. Bluetooth low-energy beacons typically reach just a couple meters and WiFi networks reach up to 100 meters. With obstacles like walls and people in the way, these distances get even lower.&lt;/p&gt;&lt;lb/&gt;...&lt;lb/&gt;This means position estimates based on WiFi networks are usually accurate to 100 meters. If a lot of networks are available in the area, accuracy tends to increase to about 10 or 20 meters. Bluetooth networks tend to be accurate to about 10 meters.&lt;/quote&gt;
    &lt;p&gt;So in my case, I specifically do not want a long range beacon in order to improve location accuracy.&lt;/p&gt;
    &lt;p&gt;In the end I settled on the Feasy FSC-BP104D and bought two of them:&lt;/p&gt;
    &lt;p&gt;There is the FeasyBeacon app which leaves a lot to be desired, but is not totally useless. [5] I powered up the two beacons and changed the following settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set a new PIN.&lt;/item&gt;
      &lt;item&gt;I reduced the broadcast interval time from the default 1300ms to 1000ms. This will increase the batter usage, but allows for more frequent updates.&lt;/item&gt;
      &lt;item&gt;Changed the name of the beacons to something fun.&lt;/item&gt;
      &lt;item&gt;Updated the broadcast URL so that I was not advertising a Feasy store page. Sadly, the character limit was not long enough to broadcast my blogs address. So I chose the beaconDB website instead.&lt;/item&gt;
      &lt;item&gt;Checked for firmware updates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then I uninstalled the app, hoping to never have to use it again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trying out the API and confirming the beacons are not associated with a location&lt;/head&gt;
    &lt;p&gt;beaconDB provides a geolocate endpoint. The old MLS version required an API key, but that is no longer needed:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Instead of using API keys to control access like Mozilla did, beaconDB expects clients to be pre-configured with a reasonable user agent. Ideally this identifies the software the client is using and includes info that can be used to narrow things down in the event a bad configuration or bug causes significant load on the server.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As a first step I wanted to confirm I could hit the API and get a valid location as a response. So I threw together a quick Python script:&lt;/p&gt;
    &lt;code&gt;import requests  
  
url = "https://api.beaconDB.net/v1/geolocate"  
  
headers = {'User-Agent': 'beaconDB test script, blog.matthewbrunelle.com',} 
  
body = {
    "wifiAccessPoints": [{
        "macAddress": "01:23:45:67:89:ab",
        "signalStrength": -51
    }, {
        "macAddress": "01:23:45:67:89:cd"
    }]
} 
  
response = requests.post(url, json=body, headers=headers)  
  
if response.status_code != 200:  
    print(f"Error: {response.status_code}")  
else:  
    print(response.json())

&lt;/code&gt;
    &lt;p&gt;Note: The example code for the blog post uses the example MAC addresses from the documentation, not real ones.&lt;/p&gt;
    &lt;p&gt;The API is pretty simple and most fields are optional. The main information you need to provide are MAC addresses and the signal strength for the observation. If I input the MAC addresses for my home access points [6], I get a response like:&lt;/p&gt;
    &lt;code&gt;{'location': {'lat': XX.XXXXXX, 'lng': -XX.XXXXXX}, 'accuracy': 132}
&lt;/code&gt;
    &lt;p&gt;Which gives the location of the house directly across the street from me. The accuracy is measured in meters, so the circle with a radius of 132 meters centered on that position does, in fact, contain my apartment. Not bad for locating off a single observation. beaconDB works best when you can query with multiple device observations at once.&lt;/p&gt;
    &lt;p&gt;Next, I wanted to hit the same endpoint, but using my BLE beacons. The app provided the beacons information, but they also had their MAC addresses printed on their side. I changed the body in the script above to:&lt;/p&gt;
    &lt;code&gt;body = {  
    "considerIp": False,  
    "bluetoothBeacons": [{
        "macAddress": "ff:23:45:67:89:ab",
        "age": 2000,
        "name": "beacon",
        "signalStrength": -110
    }],
    "fallbacks": {  sdf10adfasdfasf
        "lacf": False,  
        "ipf": False  
    }  
}
&lt;/code&gt;
    &lt;p&gt;Note that here I made sure to set &lt;code&gt;considerIp&lt;/code&gt; and &lt;code&gt;fallbacks&lt;/code&gt; to false, so that the API purely relies on the BLE beacon. From the docs:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;fallbacks&lt;/code&gt;section allows some control over the more coarse grained position sources. If no exact match can be found, these can be used to return a “404 Not Found” rather than a coarse grained estimate with a large accuracy value.&lt;/quote&gt;
    &lt;p&gt;As expected, I get a 404 back from the API.&lt;/p&gt;
    &lt;head rend="h2"&gt;Collecting observations with Neostumbler and testing the API&lt;/head&gt;
    &lt;p&gt;This part was pretty easy. NeoStumbler is a great app for contributing observations to beaconDB. I started recording and took my dog for a walk around the block. At the end of the walk I uploaded my observations. Then I just needed to wait, but not for too long:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;note that submissions will take at least 5 minutes to become available in the beaconDB&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Except... I was still getting 404s for my BLE beacons.&lt;/p&gt;
    &lt;p&gt;Neostumbler has a mechanism for filtering out moving devices, so the next thing I tried was disabling that. However I was still getting 404s...&lt;/p&gt;
    &lt;head rend="h2"&gt;Directly submitting observations to beaconDB&lt;/head&gt;
    &lt;p&gt;OK, so at this point I do not know if the issue is with Neostumbler submitting my observations, or beaconDB using them. Thankfully, Neostumbler lets you export your observations to csv.&lt;/p&gt;
    &lt;p&gt;So I was able to directly submit an observation using the geosubmit v2 endpoint. The export contains every piece of information you can submit, except for the GPS heading and the beacon name.&lt;/p&gt;
    &lt;code&gt;import requests  
  
url = "https://api.beacondb.net/v2/geosubmit"  
  
headers = {'User-Agent': 'BeaconDB test script, blog.matthewbrunelle.com',}  
  
{"items": [{
    "timestamp": 1405602028568,
    "position": {
        "latitude": -22.7539192,
        "longitude": -43.4371081,
        "accuracy": 10.0,
        "age": 1000,
        "altitude": 100.0,
        "altitudeAccuracy": 50.0,
        #"heading": 45.0,
        "pressure": 1013.25,
        "speed": 3.6,
        "source": "gps"
    },
    "bluetoothBeacons": [
        {
            "macAddress": "ff:23:45:67:89:ab",
            "age": 2000,
            #"name": "beacon",
            "signalStrength": -110
        }
    ],
}]}
  
response = requests.post(url, json=body, headers=headers)  
  
print(f"Status code: {response.status_code}")  
if response.status_code == 200:  
    print(response.json())
&lt;/code&gt;
    &lt;p&gt;Anyways, after running this script I wait again... and still a 404. At this point I realized that I had never run a test of a known good BLE beacon against the API to make sure I get a location back. So I dumped a full month of observations which contained a recent road trip I went on [7] and check some of those MAC addresses. Still nothing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Double checking the beaconDB source and final thoughts&lt;/head&gt;
    &lt;p&gt;So finally, I went to look at the beaconDB source to see what I could find. First I wanted to check if there was a minimum number of observations needed, sort of like the trilateration I learned about in HackerBox 0119 - Geopositioning. A comment from the repository revealed:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;At least two WiFi networks have to been known to accurately determine the position.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So I tried making the WiFi query I made before, but adding the BLE beacons to see if the accuracy improved... Nothing, the accuracy was the same. As I searched the codebase I realized the problem: beaconDB currently accepts and stores BLE beacons, but does not use them yet for geolocation. So I made an issue.&lt;/p&gt;
    &lt;p&gt;Not all projects end as expected. At the very least, I learned a lot along the way. [8] My initial question was "how does beaconDB use BLE beacons". I should have probably checked if the answer was "it currently doesn't" before I set everything up.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Especially since I am a fan of how the fused location provider in Play Services works. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There are some pretty graphs of observations over time. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I should note that there are several beacons around me already, so this is maybe not the most useful way to spend my time. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;By mostly looking at the first sentences of pages on Wikipedia. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Also amazingly exodus show no trackers. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I'm currently using two TP-Link EAP670. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scanning in New Hampshire is interesting compared to MA. I was basically only getting cell tower readings with the occasional WiFi. No BLE beacons. In my neighborhood I get 30-50 WiFi APs at a time. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most importantly, I was able to write an explanation of what BLE beacons are. Now I don't have to cram that into a future post I am working on. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706705</guid><pubDate>Sat, 25 Oct 2025 20:26:59 +0000</pubDate></item></channel></rss>