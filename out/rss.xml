<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 16 Oct 2025 17:09:22 +0000</lastBuildDate><item><title>A Gemma model helped discover a new potential cancer therapy pathway</title><link>https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/</link><description>&lt;doc fingerprint="95c2d199ccbb819e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How a Gemma model helped discover a new potential cancer therapy pathway&lt;/head&gt;
    &lt;p&gt;Today, as part of our research collaboration with Yale University, we’re releasing Cell2Sentence-Scale 27B (C2S-Scale), a new 27 billion parameter foundation model designed to understand the language of individual cells. Built on the Gemma family of open models, C2S-Scale represents a new frontier in single-cell analysis.&lt;/p&gt;
    &lt;p&gt;This announcement marks a milestone for AI in science. C2S-Scale generated a novel hypothesis about cancer cellular behavior and we have since confirmed its prediction with experimental validation in living cells. This discovery reveals a promising new pathway for developing therapies to fight cancer.&lt;/p&gt;
    &lt;p&gt;This launch builds upon our work from earlier this year, where we demonstrated that biological models follow clear scaling laws — just like with natural language, larger models perform better on biology. This work raised a critical question: Does a larger model just get better at existing tasks, or can it acquire entirely new capabilities? The true promise of scaling lies in the creation of new ideas, and the discovery of the unknown.&lt;/p&gt;
    &lt;head rend="h3"&gt;How C2S-Scale 27B works&lt;/head&gt;
    &lt;p&gt;A major challenge in cancer immunotherapy is that many tumors are “cold” — invisible to the body's immune system. A key strategy to make them “hot” is to force them to display immune-triggering signals through a process called antigen presentation.&lt;/p&gt;
    &lt;p&gt;We gave our new C2S-Scale 27B model a task: Find a drug that acts as a conditional amplifier, one that would boost the immune signal only in a specific “immune-context-positive” environment where low levels of interferon (a key immune-signaling protein) were already present, but inadequate to induce antigen presentation on their own. This required a level of conditional reasoning that appeared to be an emergent capability of scale; our smaller models could not resolve this context-dependent effect.&lt;/p&gt;
    &lt;p&gt;To accomplish that, we designed a dual-context virtual screen to find this specific synergistic effect. The virtual screen involved two stages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Immune-Context-Positive: We provided the model with real-world patient samples with intact tumor-immune interactions and low-level interferon signaling.&lt;/item&gt;
      &lt;item&gt;Immune-Context-Neutral: We provided the model with isolated cell line data with no immune context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We then simulated the effect of over 4,000 drugs across both contexts and asked the model to predict which drugs would only boost antigen presentation in the first context, to bias the screen towards the patient-relevant setting. Out of the many drug candidates highlighted by the model, a fraction (10-30%) of drug hits are already known in prior literature, while the remaining drugs are surprising hits with no prior known link to the screen.&lt;/p&gt;
    &lt;head rend="h2"&gt;From prediction to experimental validation&lt;/head&gt;
    &lt;p&gt;The model's predictions were clear. It identified a striking “context split” for the kinase CK2 inhibitor called silmitasertib (CX-4945). The model predicted a strong increase in antigen presentation when silmitasertib was applied in the “immune-context-positive” setting, but little to no effect in the “immune-context-neutral” one. What made this prediction so exciting was that it was a novel idea. Although CK2 has been implicated in many cellular functions, including as a modulator of the immune system, inhibiting CK2 via silmitasertib has not been reported in the literature to explicitly enhance MHC-I expression or antigen presentation. This highlights that the model was generating a new, testable hypothesis, and not just repeating known facts.&lt;/p&gt;
    &lt;p&gt;A prediction, however, is only valuable if it can be validated in clinical application. The real test is first in the lab, and eventually, in the clinic.&lt;/p&gt;
    &lt;p&gt;For the next phase of our project, we took this hypothesis to the lab bench and tested it in human neuroendocrine cell models — a cell type that was completely unseen by the model during training. The experiments demonstrated:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treating the cells with silmitasertib alone had no effect on antigen presentation (MHC-I).&lt;/item&gt;
      &lt;item&gt;Treating the cells with a low dose of interferon alone had a modest effect.&lt;/item&gt;
      &lt;item&gt;Treating the cells with both silmitasertib and low-dose interferon produced a marked, synergistic amplification of antigen presentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Remarkably, in our lab tests the combination of silmitasertib and low-dose interferon resulted in a roughly 50% increase in antigen presentation, which would make the tumor more visible to the immune system.&lt;/p&gt;
    &lt;p&gt;The model’s in silico prediction was confirmed multiple times in vitro. C2S-Scale had successfully identified a novel, interferon-conditional amplifier, revealing a new potential pathway to make “cold” tumors “hot,” and potentially more responsive to immunotherapy. While this is an early first step, it provides a powerful, experimentally-validated lead for developing new combination therapies, which use multiple drugs in concert to achieve a more robust effect.&lt;/p&gt;
    &lt;p&gt;This result also provides a blueprint for a new kind of biological discovery. It demonstrates that by following the scaling laws and building larger models like C2S-Scale 27B, we can create predictive models of cellular behavior that are powerful enough to run high-throughput virtual screens, discover context-conditioned biology, and generate biologically-grounded hypotheses.&lt;/p&gt;
    &lt;p&gt;Teams at Yale are now exploring the mechanism uncovered here and testing additional AI-generated predictions in other immune contexts. With further preclinical and clinical validation, such hypotheses may be able to ultimately accelerate the path to new therapies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started with C2S-Scale 27B&lt;/head&gt;
    &lt;p&gt;The new C2S-Scale 27B model and its resources are available today for the research community. We invite you to explore these tools, build on our work and help us continue to translate the language of life.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read the full scientific preprint on bioRxiv.&lt;/item&gt;
      &lt;item&gt;Explore the model and resources on Hugging Face.&lt;/item&gt;
      &lt;item&gt;Access the code on GitHub.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45597006</guid><pubDate>Wed, 15 Oct 2025 19:04:07 +0000</pubDate></item><item><title>Free applicatives, the handle pattern, and remote systems</title><link>https://exploring-better-ways.bellroy.com/free-applicatives-the-handle-pattern-and-remote-systems.html</link><description>&lt;doc fingerprint="721ee56b6f99d084"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Free applicatives, the handle pattern, and remote systems&lt;/head&gt;
    &lt;p&gt;We recently refactored some gnarly code that manipulated customer and order records in our enterprise resource planning (ERP) system. That system had a few idiosyncrasies which complicated this task:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating new records required referencing other entities by internal ID, so we had to do a number of lookups before issuing “create record” requests;&lt;/item&gt;
      &lt;item&gt;For some entity types, we found it easiest to issue “search” API calls and extract the required IDs from the returned search results. This necessitated an extra parsing step between “we have a successful response” and “we have the ID we’re looking for”; and&lt;/item&gt;
      &lt;item&gt;Requests are often slow, but the marginal cost of additional requests in a batch was quite low. This meant that we could expect some good results from batching related requests together.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The benefits of batching led us to seek a solution that permitted static analysis. Applicative functors have a completely static control flow, and cannot express computations where one step depends on the result of a previous step. A well-chosen applicative would let us analyse the requests we need to send without executing any of them, batch queries together without worrying about data dependencies, and then route relevant results to each individual query to parse (if necessary). Our library users could ignore batching details but still gain the efficiency benefits of a batch query API.&lt;/p&gt;
    &lt;p&gt;In this post, we’ll look how we’ve been using handles, what “free structures” are, how free applicatives elegantly solved some of our problems interfacing with a remote system, and how they interacted especially well with the “handle pattern”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Handles, as Bellroy uses them&lt;/head&gt;
    &lt;p&gt;The “handle pattern” is a Haskell idiom that is similar to dependency injection in mainstream languages. Instead of directly writing in the side effects we want our code to perform, we accept a record of functions that we call a “handle”. (In an object-oriented language, we’d probably accept an object that implements an abstract interface instead of a record.) These handles can group related functions into a single record but often only contain one:&lt;/p&gt;
    &lt;code&gt;newtype Handle e m = Handle {
    performRequest :: ERP.Request -&amp;gt; m (Either e Aeson.Value)

   }
-- Plus some other handle-making functions e.g. for testing.
newHandle ::
MonadIO m =&amp;gt; ERP.Credentials -&amp;gt; m (Handle ERP.Error m)   &lt;/code&gt;
    &lt;p&gt;Functions that consume handles generally look like this:&lt;/p&gt;
    &lt;code&gt;someFunction ::
-- When all side effects come from handles,
   -- we rarely need anything stronger than `Monad`.
   Monad m =&amp;gt;
   -- First: Any handles we need
   FooHandle m -&amp;gt; BarHandle m -&amp;gt;
   -- Second: Other "normal" arguments
   Argument1 -&amp;gt; .. -&amp;gt; ArgumentN -&amp;gt;
   Result   m &lt;/code&gt;
    &lt;p&gt;This idiom is a simpler, library-free alternative to effect system libraries like &lt;code&gt;effectful&lt;/code&gt;,
&lt;code&gt;bluefin&lt;/code&gt;,
&lt;code&gt;heftia&lt;/code&gt; and
&lt;code&gt;polysemy&lt;/code&gt;. We
previously wrote about an
experiment with &lt;code&gt;effectful&lt;/code&gt;,
but we have still not committed to an effect system. Instead, we are
refactoring towards handles as a way to encapsulate our side effects,
and because it should be easy to convert handle-using code to an
effect system if and when we choose one.&lt;/p&gt;
    &lt;p&gt;Because we have code written against other idioms (e.g. MTL-style classes), and because we often find it convenient to introduce an &lt;code&gt;ExceptT e&lt;/code&gt; or &lt;code&gt;MaybeT&lt;/code&gt; monad transformer in the body of our
functions, we sometimes need to change the monad of a handle that
we’ve been given. We do this by providing a &lt;code&gt;hoistHandle&lt;/code&gt; function:&lt;/p&gt;
    &lt;code&gt;hoistHandle :: (forall x . f x -&amp;gt; g x) -&amp;gt; Handle f -&amp;gt; Handle g
Handle {performRequest} =
 hoistHandle f Handle {performRequest = f . performRequest}   &lt;/code&gt;
    &lt;p&gt;That first argument, &lt;code&gt;forall x . f x -&amp;gt; g x&lt;/code&gt;, is worth commenting
on. A &lt;code&gt;forall&lt;/code&gt; in a type signature explicitly introduces a type
variable that is provided by the function’s caller. For a simpler
example of how &lt;code&gt;forall&lt;/code&gt; works here, let’s look at the &lt;code&gt;map&lt;/code&gt; function
on lists, but with explicit &lt;code&gt;forall&lt;/code&gt;s:&lt;/p&gt;
    &lt;code&gt;map :: forall a b . (a -&amp;gt; b) -&amp;gt; [a] -&amp;gt; [b]&lt;/code&gt;
    &lt;p&gt;The caller of &lt;code&gt;map&lt;/code&gt; gets to choose the types of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, and GHC
is often smart enough to figure this out automatically:&lt;/p&gt;
    &lt;code&gt;-- GHC concludes that it needs to call
-- `map` with `Int` for `a` and `String` for `b`.
&amp;gt; map show [1, 2, 3]
 ghci"1","2","3"] [&lt;/code&gt;
    &lt;p&gt;In our &lt;code&gt;hoistHandle&lt;/code&gt; function, we let the caller choose &lt;code&gt;f&lt;/code&gt; and
&lt;code&gt;g&lt;/code&gt;, but they must provide us a function where we are allowed to
choose &lt;code&gt;x&lt;/code&gt;. The types force this function to convert &lt;code&gt;f x&lt;/code&gt; into &lt;code&gt;g x&lt;/code&gt;
in a way that’s blind to what &lt;code&gt;x&lt;/code&gt; actually is — guaranteeing that
the conversion only changes structure, not wrapped values. It also
ensures that we can write &lt;code&gt;hoistHandle&lt;/code&gt; for a handle containing
multiple functions, because we can choose a different &lt;code&gt;x&lt;/code&gt; for each
one.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building our applicative&lt;/head&gt;
    &lt;p&gt;We want to build a structure that is essentially a syntax tree of the operations we want to perform. This means it needs to hold the requests we want to send, and because we want it to be an applicative, we’ll add constructors to represent &lt;code&gt;pure&lt;/code&gt; and &lt;code&gt;(&amp;lt;*&amp;gt;)&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;data Query a where
QueryAndParse ::
   FromJSON x =&amp;gt;
     ERP.Request -&amp;gt; (x -&amp;gt; Either Text a) -&amp;gt; Query a
     
-- Extra constructors to hold applicative structure
   Pure :: a -&amp;gt; Query a
   Ap :: Query (a -&amp;gt; b) -&amp;gt; Query a -&amp;gt; Query b
   
deriving stock instance Functor Query

instance Applicative Query where
pure = Pure
   
Pure f &amp;lt;*&amp;gt; Pure x = Pure $ f x
   QueryAndParse req f &amp;lt;*&amp;gt; Pure a =
   QueryAndParse req $ fmap ($ a) . f
     -- Plus another seven cases, being careful that
   -- each case obeys the applicative laws.   &lt;/code&gt;
    &lt;p&gt;&lt;code&gt;QueryAndParse&lt;/code&gt; is the only data constructor directly relevant to our
problem. It captures the request we want to make against the ERP, a
&lt;code&gt;FromJSON x&lt;/code&gt; constraint so we can parse the raw response into some
intermediate type representing an API response, and a function &lt;code&gt;x -&amp;gt; Either Text a&lt;/code&gt; to extract just the data we want from that API
response.&lt;/p&gt;
    &lt;p&gt;This design could work, but it’s a fair amount of boilerplate, and the next time we want an applicative like this we’d need to repeat most of it. In the next section, we’ll use a free applicative to separate the general “applicative” code from the specific “query and parse” code.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a “free structure”?&lt;/head&gt;
    &lt;p&gt;To understand how free applicatives help us with this problem, we need to have some idea what “freeness” means in this context. The Haskell community usually talks about taking “the free &lt;code&gt;$class&lt;/code&gt; over &lt;code&gt;$type&lt;/code&gt;”
as a way to make &lt;code&gt;$type&lt;/code&gt; an instance of &lt;code&gt;$class&lt;/code&gt;, by adding just
enough structure to construct a lawful instance of &lt;code&gt;$class&lt;/code&gt;. Packages
like &lt;code&gt;free&lt;/code&gt; provide
wrapping types that hold values of &lt;code&gt;$type&lt;/code&gt; and provide instances of
&lt;code&gt;$class&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let’s pare our &lt;code&gt;Query&lt;/code&gt; type back to something much smaller: a type
representing a single request against our ERP:&lt;/p&gt;
    &lt;code&gt;data OneQuery a where
QueryAndParse ::
   FromJSON x =&amp;gt;
     ERP.Request -&amp;gt; (x -&amp;gt; Either Text a) -&amp;gt; OneQuery a     &lt;/code&gt;
    &lt;p&gt;We will now re-write &lt;code&gt;Query&lt;/code&gt; as the free &lt;code&gt;Applicative&lt;/code&gt; over
&lt;code&gt;OneQuery&lt;/code&gt;. To make &lt;code&gt;OneQuery&lt;/code&gt; into an &lt;code&gt;Applicative&lt;/code&gt;, we’ll use the
&lt;code&gt;Ap&lt;/code&gt; wrapper from
&lt;code&gt;Control.Applicative.Free&lt;/code&gt;.
Here is its interface:&lt;/p&gt;
    &lt;code&gt;-- `Ap f` is the free applicative over `f`. We never use its
-- constructors directly; instead, we use `liftAp` and the
-- `Applicative` interface (`pure`, `(&amp;lt;*&amp;gt;)`, `liftA2`, etc.)
data Ap f a

-- For *any* `f`, `Ap f` is an applicative.
instance Applicative (Ap f)

-- We can turn any `f a` into an `Ap f a`.
liftAp :: f a -&amp;gt; Ap f a

-- If we can turn our `f` into some applicative `g`, then we can turn
-- `Ap f a` into `g a` in a way that respects the Applicative laws:
--
-- runAp _ (pure x) = pure x
-- runAp f (x &amp;lt;*&amp;gt; y) = (runAp f x) &amp;lt;*&amp;gt; (runAp f y)
--
-- Similar to the `forall x. f x -&amp;gt; g x` in `hoistHandle` above,
-- this lets us turn each `f x` stored in the `Ap f a` into a
-- corresponding `g x`, while remaining ignorant of the specific
-- type `x`.
runAp :: Applicative g =&amp;gt; (forall x. f x -&amp;gt; g x) -&amp;gt; Ap f a -&amp;gt; g a&lt;/code&gt;
    &lt;p&gt;We’ll skip the implementations because we won’t ever manually recurse through an &lt;code&gt;Ap f a&lt;/code&gt; value; from a modularity perspective, we are only
interested in the abstract interface. We declare &lt;code&gt;Query&lt;/code&gt; as the free
applicative over &lt;code&gt;OneQuery&lt;/code&gt;, make it a &lt;code&gt;newtype&lt;/code&gt; to establish an
abstraction boundary between the query library and its callers, and
use &lt;code&gt;deriving newtype&lt;/code&gt; to avoid writing any applicative structure
ourselves:&lt;/p&gt;
    &lt;code&gt;newtype Query a = Query (Free.Ap OneQuery a)
deriving stock Functor
   deriving newtype Applicative
   
-- Helper functions to avoid building `Query` values by hand.

query ::
FromJSON a =&amp;gt; ERP.Request -&amp;gt; Query a
   =
 query _ req Query . Free.liftAp $ QueryAndParse req Right
   
queryAndParse ::
FromJSON a =&amp;gt; ERP.Request -&amp;gt; (a -&amp;gt; Either Text b) -&amp;gt; Query b
   =
 queryAndParse req f Query . Free.liftAp $ QueryAndParse req f   &lt;/code&gt;
    &lt;head rend="h2"&gt;Building a &lt;code&gt;Query&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;From this infrastructure, we can write functions representing individual queries. These are direct applications of the &lt;code&gt;query&lt;/code&gt; and
&lt;code&gt;queryAndParse&lt;/code&gt; helpers:&lt;/p&gt;
    &lt;code&gt;queryLocationId ::
ERP.Location.Name -&amp;gt; Query ERP.Location.Id
   =
 queryLocationId locationName $ ERP.lookupLocation locationName
   query 
queryOrderId ::
ERP.Order.Name -&amp;gt; Query ERP.Order.Id
   =
 queryOrderId orderName 
   queryAndParse$ \case -&amp;gt;
     (ERP.searchOrders orderName) -&amp;gt; Right order
       [order] :_) -&amp;gt; Left "Multiple Orders in response"
       (_-&amp;gt; Left "No Orders in response"       [] &lt;/code&gt;
    &lt;p&gt;From these functions we can build up complex queries using applicative operations:&lt;/p&gt;
    &lt;code&gt;queryOrderAndLocation ::
ERP.Order.Name -&amp;gt; ERP.Location.Name -&amp;gt;
   Query (ERP.Order.Id, ERP.Location.Id)
   =
 queryOrderAndLocation orderName locationName    liftA2 (,) (queryOrderId orderName) (queryLocationId locationName)&lt;/code&gt;
    &lt;head rend="h2"&gt;Running a &lt;code&gt;Query&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;We can run a &lt;code&gt;Query&lt;/code&gt; by using
&lt;code&gt;runAp&lt;/code&gt;.
Because we’re in an applicative context and we’re making requests that
don’t alter the remote system, we can run every request and use a
&lt;code&gt;Validation&lt;/code&gt;
to collect all failures:&lt;/p&gt;
    &lt;code&gt;data RunQueryError e = RequestError e | JsonError Text | ParseResultError Text
type RunQueryErrors e = NonEmpty (RunQueryError e)

runQuery :: forall e m a.
Monad m =&amp;gt;
   ERP.Handle e m -&amp;gt;
   Query a -&amp;gt;
   Validation (RunQueryErrors e) a)
   m (ERP.Handle{performRequest} (Query q) =
 runQuery $ Free.runAp (Compose . go) q
   getCompose where
       go :: OneQuery x -&amp;gt; m (Validation (RunQueryErrors e) x)
QueryAndParse req parse) = performRequest req &amp;lt;&amp;amp;&amp;gt; \case
     go (Left reqErr -&amp;gt; Failure . NonEmpty.singleton $ RequestError reqErr
       Right value -&amp;gt; case Aeson.parseEither Aeson.parseJSON value of
       Left jsonErr -&amp;gt; Failure . NonEmpty.singleton . JsonError $ Text.pack jsonErr
         Right x -&amp;gt; case parse x of
         Left parseErr -&amp;gt; Failure . NonEmpty.singleton $ ParseResultError parseErr
           Right a -&amp;gt; Success a           &lt;/code&gt;
    &lt;p&gt;The implementation can be mostly derived by following the types, but we’ll highlight some specifics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Validation e a&lt;/code&gt;is a type that’s structurally isomorphic to&lt;code&gt;Either e a&lt;/code&gt;, but provides an&lt;code&gt;Applicative&lt;/code&gt;instance that accumulates errors:&lt;code&gt;-- From the validation-selective package. instance Semigroup e =&amp;gt; Applicative (Validation e) where pure = Success -- This asymmetric way of writing &amp;lt;*&amp;gt; maximises laziness. Failure e1 &amp;lt;*&amp;gt; b = Failure $ case b of Failure e2 -&amp;gt; e1 &amp;lt;&amp;gt; e2 Success _ -&amp;gt; e1 Success _ &amp;lt;*&amp;gt; Failure e = Failure e Success f &amp;lt;*&amp;gt; Success a = Success (f a)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Since the composition of any two applicatives is itself an applicative,&lt;/p&gt;&lt;code&gt;Data.Functor.Compose&lt;/code&gt;lets us combine the&lt;code&gt;m&lt;/code&gt;and&lt;code&gt;Validation e&lt;/code&gt;applicatives into&lt;code&gt;Compose m (Validation e)&lt;/code&gt;, which executes actions in&lt;code&gt;m&lt;/code&gt;and accumulates errors — exactly what we want.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Since we use the&lt;/p&gt;&lt;code&gt;Compose&lt;/code&gt;constructor to wrap the result of&lt;code&gt;go&lt;/code&gt;,&lt;code&gt;Free.runAp&lt;/code&gt;will return a&lt;code&gt;Compose m (Validation e) a&lt;/code&gt;which we must unwrap with&lt;code&gt;getCompose&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;go&lt;/code&gt;function processes a single request held in a&lt;code&gt;OneQuery x&lt;/code&gt;, and&lt;code&gt;Free.runAp&lt;/code&gt;uses it to build up the applicative combination of each result.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We accept a handle telling us how to contact the ERP. This is the key location where the handle pattern and the free applicative interact, giving the library user a lot of power: the handle parameter frees us from being coupled to any particular monad and makes it easier to write tests for this code. We’ll see another way to construct a&lt;/p&gt;&lt;code&gt;ERP.Handle&lt;/code&gt;very soon.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The caller of the&lt;/p&gt;&lt;code&gt;Query&lt;/code&gt;interface has no idea that we’re building and consuming free structures under the hood. It’s an implementation detail that doesn’t distort the abstraction boundary at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Extracting requests&lt;/head&gt;
    &lt;p&gt;Now that we can execute queries, let’s explore the main benefit of free applicatives: the ability to analyse the applicative program without running it. We can extract a monoidal summary of any free applicative’s structure via &lt;code&gt;runAp_&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;runAp_ :: Monoid m =&amp;gt; (forall x . f x -&amp;gt; m) -&amp;gt; Ap f a -&amp;gt; m&lt;/code&gt;
    &lt;p&gt;For an intuition why this is true, consider that the constant functor &lt;code&gt;Const r&lt;/code&gt;
has an &lt;code&gt;Applicative&lt;/code&gt; instance whenever &lt;code&gt;r&lt;/code&gt; is a monoid, because &lt;code&gt;pure&lt;/code&gt;
stores a &lt;code&gt;mempty&lt;/code&gt; value and &lt;code&gt;(&amp;lt;*&amp;gt;)&lt;/code&gt; combines the held values with
&lt;code&gt;(&amp;lt;&amp;gt;)&lt;/code&gt;. For a fun exercise, implement &lt;code&gt;runAp_&lt;/code&gt; in terms of &lt;code&gt;runAp&lt;/code&gt; and
&lt;code&gt;Const&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We can use &lt;code&gt;runAp_&lt;/code&gt; to extract a list of every request a &lt;code&gt;Query a&lt;/code&gt;
will send:&lt;/p&gt;
    &lt;code&gt;allRequests :: Query a -&amp;gt; [Request]
Query q) = ordNub $ Free.runAp_ go q
 allRequests (where
       go :: OneQuery x -&amp;gt; [Request]
QueryAndParse req _) = [req]     go (&lt;/code&gt;
    &lt;p&gt;Once we have the list of requests, we can look for ways to optimise them. De-duplicating the requests with &lt;code&gt;ordNub&lt;/code&gt;
is an easy optimisation, but if the remote API supports it, we could
do more advanced optimisations like using a batch request API.&lt;/p&gt;
    &lt;p&gt;As a simple demonstration, we can perform all the lookup requests in advance and construct a &lt;code&gt;Map Request Aeson.Value&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;type SavedRequests = Map Request Aeson.Value

saveRequests ::
forall e m.
   Monad m =&amp;gt;
   Handle e m -&amp;gt; [Request] -&amp;gt; m (Either e SavedRequests)
   Handle{performRequest} requests =
 saveRequests $ Map.fromList &amp;lt;$&amp;gt; traverse go requests
   runExceptT where
       go :: Request -&amp;gt; ExceptT e m (Request, Aeson.Value)
= (req,) &amp;lt;$&amp;gt; ExceptT $ performRequest req     go req &lt;/code&gt;
    &lt;p&gt;Using a collection of saved results, we can construct a handle that returns the saved responses instead of performing real requests:&lt;/p&gt;
    &lt;code&gt;newtype UnsavedRequestError = UnsavedRequestError Request

newHandleFromSavedRequests ::
Applicative m) =&amp;gt; SavedRequests -&amp;gt; Handle UnsavedRequestError m
   (=
 newHandleFromSavedRequests requests Handle
   = \req -&amp;gt;
     { performRequest pure . maybe (Left (UnsavedRequestError req)) Right $
         
           Map.lookup req requests     }&lt;/code&gt;
    &lt;p&gt;This gives us a great story for testing. Since our &lt;code&gt;runQuery&lt;/code&gt; works
with any handle, we can capture some real requests to a file, redact
any sensitive information, and create a pure handle built from saved
requests. We can then use this handle to write test cases that run
real code without performing side-effects.&lt;/p&gt;
    &lt;p&gt;If this example moved too quickly, or you want to see another application of free structures, Justin Le has a spectacular post on matching regular expressions using the free &lt;code&gt;Alternative&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Payoffs and limitations&lt;/head&gt;
    &lt;p&gt;What have we achieved? We decided that we wanted an applicative to describe queries against our remote system. Instead of inventing a complicated data structure to represent the syntax tree of &lt;code&gt;pure&lt;/code&gt; and
&lt;code&gt;(&amp;lt;*&amp;gt;)&lt;/code&gt; calls, we defined a type just to hold one request and took the
free applicative over it. We also used the handle pattern to ask for
only the side-effects that we needed. Both patterns are reasonably
easy to implement, and in exchange we got some pretty neat benefits
that would’ve been harder to realise with either technique alone:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;We can analyse a&lt;/p&gt;&lt;code&gt;Query&lt;/code&gt;without running it, and use the&lt;code&gt;Query&lt;/code&gt;to inform the handle we do eventually use;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As a special case of (1), library users can code against a convenient interface and request individual records, but we can inspect the set of queries before we begin execution and issue optimised, parallelised, de-duplicated and batched requests in their place;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We don’t have to abort at the first failed request — we can collect and report every problem with a&lt;/p&gt;&lt;code&gt;Query&lt;/code&gt;; and&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We can record and replay requests, giving us a great testing story in the style of Ruby’s&lt;/p&gt;&lt;code&gt;vcr&lt;/code&gt;library.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s not all roses, though. We lose a significant amount of expressive power by giving up the monadic interface. For APIs where we need to interleave pure queries and side-effecting requests, losing the &lt;code&gt;Monad&lt;/code&gt; instance might be a bridge too far. Chris Penner suggests
that &lt;code&gt;Selective&lt;/code&gt; functors could be closer to the sweet spot,
but then you lose the nice ergonomics of &lt;code&gt;-XApplicativeDo&lt;/code&gt;.
Chris Done identifies an “Applicative-wired Monad” pattern
which uses a monad only to plumb together applicative values.&lt;/p&gt;
    &lt;p&gt;So where does this leave us? The handle pattern has been working well for us and we plan to continue refactoring code to use handles for the foreseeable future. In narrow contexts where we want to take advantage of static analysis, a well-chosen free applicative has given us a surprising amount of modularity, testability and opportunities for automatic optimisation. In the function that “runs” the free applicative, these two idioms interacted in a very satisfying way: the handle parameter gave us a lot of flexibility without asking library users to write a lot of boilerplate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45601177</guid><pubDate>Thu, 16 Oct 2025 03:33:35 +0000</pubDate></item><item><title>TurboTax’s 20-year fight to stop Americans from filing taxes for free (2019)</title><link>https://www.propublica.org/article/inside-turbotax-20-year-fight-to-stop-americans-from-filing-their-taxes-for-free</link><description>&lt;doc fingerprint="2f18c95a29b9b569"&gt;
  &lt;main&gt;
    &lt;p&gt;ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up for ProPublica’s Big Story newsletter to receive stories like this one in your inbox as soon as they are published.&lt;/p&gt;
    &lt;p&gt;Last fall, Intuit’s longtime CEO Brad Smith embarked on a farewell tour of the company’s offices around the world. Smith had presided over 11 years of explosive growth, a period when Intuit had secured its place in the Silicon Valley pantheon, and the tour was like a long party.&lt;/p&gt;
    &lt;p&gt;In Ontario, employees wore T-shirts with Smith’s quasi-spiritual sayings: “Do whatever makes your heart beat fastest” and “Repetition doesn’t ruin the prayer.” In Bangalore, India, workers put on Smith face masks as they posed for selfies with the man himself. Fittingly, the tour culminated in San Diego, the home of TurboTax, the software that transformed the company’s fortunes. There, Smith arrived at his party in a DeLorean, and as he walked a red carpet, cheering employees waved “Brad is Rad” signs. To Smith’s delight, his favorite rock star, Gene Simmons of Kiss, emerged. The two posed for pictures, Simmons clad in black and the beaming CEO flashing the “rock on” hand sign.&lt;/p&gt;
    &lt;p&gt;Intuit began in the 1980s as an accounting software company focused on helping people with their bookkeeping. Over time, the company, like the other giants of Big Tech, cultivated an image of being not just good at what it did, but good, period. In a recent Super Bowl ad, Intuit portrayed itself as a gentle robot that liberates small-business owners from paperwork. The company stresses values above all, urging employees to “deliver awesome” and pursue “integrity without compromise.”&lt;/p&gt;
    &lt;p&gt;Intuit’s QuickBooks accounting product remains a steady moneymaker, but in the past two decades TurboTax, its tax preparation product, has driven the company’s steadily growing profits and made it a Wall Street phenom. When Smith took over in 2008, TurboTax was a market leader, but only a small portion of Americans filed their taxes online. By 2019, nearly 40% of U.S. taxpayers filed online and some 40 million of them did so with TurboTax, far more than with any other product.&lt;/p&gt;
    &lt;p&gt;But the success of TurboTax rests on a shaky foundation, one that could collapse overnight if the U.S. government did what most wealthy countries did long ago and made tax filing simple and free for most citizens.&lt;/p&gt;
    &lt;p&gt;For more than 20 years, Intuit has waged a sophisticated, sometimes covert war to prevent the government from doing just that, according to internal company and IRS documents and interviews with insiders. The company unleashed a battalion of lobbyists and hired top officials from the agency that regulates it. From the beginning, Intuit recognized that its success depended on two parallel missions: stoking innovation in Silicon Valley while stifling it in Washington. Indeed, employees ruefully joke that the company’s motto should actually be “compromise without integrity.”&lt;/p&gt;
    &lt;p&gt;Internal presentations lay out company tactics for fighting “encroachment,” Intuit’s catchall term for any government initiative to make filing taxes easier — such as creating a free government filing system or pre-filling people’s returns with payroll or other data the IRS already has. “For a decade proposals have sought to create IRS tax software or a ReturnFree Tax System; All were stopped,” reads a confidential 2007 PowerPoint presentation from an Intuit board of directors meeting. The company’s 2014-15 plan included manufacturing “3rd-party grass roots” support. “Buy ads for op-eds/editorials/stories in African American and Latino media,” one internal PowerPoint slide states.&lt;/p&gt;
    &lt;p&gt;The centerpiece of Intuit’s anti-encroachment strategy has been the Free File program, hatched 17 years ago in a moment of crisis for the company. Under the terms of an agreement with the federal government, Intuit and other commercial tax prep companies promised to provide free online filing to tens of millions of lower-income taxpayers. In exchange, the IRS pledged not to create a government-run system.&lt;/p&gt;
    &lt;p&gt;Since Free File’s launch, Intuit has done everything it could to limit the program’s reach while making sure the government stuck to its end of the deal. As ProPublica has reported, Intuit added code to the Free File landing page of TurboTax that hid it from search engines like Google, making it harder for would-be users to find.&lt;/p&gt;
    &lt;p&gt;Twelve years ago, Intuit launched its own “free” product: the similarly named “Free Edition” of TurboTax. But unlike the government program, this one comes with traps that can push customers lured with the promise of “free” into paying, some more than $200. Free Edition was a smash hit for Intuit and its pitch for “free” prep remains core to the company’s growth. Recently, it launched a “free, free free free” ad campaign for the Free Edition, including a crossword puzzle in The New York Times in which the answer to every clue was “f-r-e-e.”&lt;/p&gt;
    &lt;p&gt;Intuit knows it’s deceiving its customers, internal company documents obtained by ProPublica show. “The website lists Free, Free, Free and the customers are assuming their return will be free,” said a company PowerPoint presentation that reported the results of an analysis of customer calls this year. “Customers are getting upset.”&lt;/p&gt;
    &lt;p&gt;Intuit also continues to use “dark patterns” — design tricks to get users of its website to do things they don’t necessarily mean to do — to ensure that as many customers as possible pay, former employees say. A marketing concept frequently invoked at Intuit, which goes by the acronym “FUD,” seeks to tap into Americans’ fear, uncertainty and doubt about the tax filing process.&lt;/p&gt;
    &lt;p&gt;An Intuit spokesman declined to answer ProPublica’s detailed questions about its efforts to fend off a government filing system, but he provided a statement.&lt;/p&gt;
    &lt;p&gt;“We empower our customers to take control of their financial lives, which includes being in charge of their own tax preparation,” he said, adding that a “government-run pre-filled tax preparation system that makes the tax collector (who is also the investigator, auditor and enforcer) the tax preparer is fraught with conflicts of interest.”&lt;/p&gt;
    &lt;p&gt;The IRS is seemingly the biggest threat to Intuit and other commercial tax prep businesses, but it has more frequently acted as the industry’s ally, defending the Free File program even in the face of critical internal reviews. The IRS declined to comment for this article.&lt;/p&gt;
    &lt;p&gt;The consequences of Intuit’s efforts affect a huge proportion of the taxpaying public. Americans spend an estimated 1.7 billion hours and $31 billion doing their taxes each year. Just 2.8 million participated in the Free File program this year, down from 5.1 million at the program’s peak in 2005.&lt;/p&gt;
    &lt;p&gt;Intuit’s success has made the men who run the company rich. Smith, the CEO who stepped down last year and is now executive board chair, had a stake worth $20 million when he became chief executive. It ballooned to $220 million by last year. Co-founder Scott Cook is now among the country’s wealthiest people, his fortune soaring to $3.3 billion.&lt;/p&gt;
    &lt;p&gt;This year, Intuit was close to realizing a long-held goal: enshrining the Free File program in law, effectively closing the door on the IRS ever creating a free tax filing system. But an outcry followed ProPublica’s reporting on the matter and Intuit’s treatment of its customers, prompting the provision to be dropped and state and federal investigations into Intuit’s practices.&lt;/p&gt;
    &lt;p&gt;Yet even after this setback, the company remained steadfastly confident that its clout in Washington would win the day.&lt;/p&gt;
    &lt;p&gt;“What we’re not gonna do is fight this publicly because that is exactly what they want us to do,” said Sasan Goodarzi, the new CEO, in a video released to staff this May and obtained by ProPublica. “We are actually working with the IRS and members of the Congress to ensure that the facts are very clear.”&lt;/p&gt;
    &lt;p&gt;Intuit has dominated the tax software market since 1993, when for $225 million, it bought Chipsoft, the San Diego-based company that had created TurboTax. Even then, TurboTax was the most popular option, but Intuit pursued a plan of aggressive growth. The product necessarily came on a disk, and by the end of the 1990s TurboTax boxes were nearly ubiquitous, on shelves in office supply stores across America.&lt;/p&gt;
    &lt;p&gt;As internet speeds increased and dot-com mania took hold, it became apparent that Intuit’s future was not in a box on a shelf. It was online.&lt;/p&gt;
    &lt;p&gt;The prospect of TurboTax’s growth was vast for another reason. As late as 2001, around 45 million Americans still filled out their tax forms on paper. For Intuit, those were all potential customers.&lt;/p&gt;
    &lt;p&gt;But Intuit wasn’t alone in seeing possibilities in the spread of high-speed internet. In Washington, lawmakers began pushing the IRS to modernize and get more taxpayers to file electronically. It was a no-brainer: Filing taxes online would be easier, and the IRS would save staff costs on processing paper returns.&lt;/p&gt;
    &lt;p&gt;The danger to Intuit’s growing business was obvious. If the government succeeded in creating a system that allowed the vast majority of taxpayers to file online for free, TurboTax profits would plummet. Intuit recognized that the notion of “return-free filing” was not going away on its own.&lt;/p&gt;
    &lt;p&gt;And so in 1998, the company hired Bernie McKay, a onetime Carter administration aide and a senior lobbyist at AT&amp;amp;T, to be its vice president for corporate affairs. Intuit executives like to talk about having a “customer obsession” in developing their products. McKay’s obsession is stopping government encroachment. Known to physically bang the table to drive home a point, McKay’s style is “aggressive to the point of offense,” said one fellow tax prep lobbyist. An Intuit spokesman said, “This mischaracterization of Mr. McKay is pure fiction.”&lt;/p&gt;
    &lt;p&gt;McKay, for his part, when asked at a recent tax industry conference which Star Wars character he is, responded, “Darth Vader.”&lt;/p&gt;
    &lt;p&gt;The year McKay was hired, Congress passed a major overhaul of the IRS. The bill, reflecting Intuit’s lobbying, said that the IRS “should cooperate with and encourage the private sector” to increase electronic filing.&lt;/p&gt;
    &lt;p&gt;While McKay came through in his first big test, in 2002, the company found itself up against an unexpected foe, the George W. Bush administration. The threat came from a broad administration initiative to upgrade government technology. One of the proposals called for the IRS to develop “an easy, no-cost option for taxpayers to file their tax return online.”&lt;/p&gt;
    &lt;p&gt;Without such an option, taxpayers were stuck either filing on paper or, to file electronically, paying a tax professional or software company like TurboTax. Providing an alternative would be an obvious improvement, said Mark Forman, an official at the Office of Management and Budget who led the “e-government” program. The technology wasn’t all that complicated, and creating a free, automated filing system would help tens of millions of Americans. “This was seen as a low-cost, high-payoff initiative,” Forman recalled in a recent interview with ProPublica. Standing in the way, he said, was an industry “that lives off the complexity of the tax code.”&lt;/p&gt;
    &lt;p&gt;Intuit revved its new lobbying machine. Even before the OMB report was publicly released, a group of Republican lawmakers, led by TurboTax’s hometown congressman, wrote to the agency arguing that there was no reason for the government to “compete” with the “well-established” private tax prep companies. Intuit’s lobbyists also went above the OMB and pressed their case directly to the White House, Forman recalled.&lt;/p&gt;
    &lt;p&gt;At the IRS, “all hell broke loose,” remembered Terry Lutes, who was then the head of electronic filing at the agency. Intuit’s clout on the Hill meant that lawmakers were soon accusing the IRS of making “secret plans to undercut the industry,” Lutes said. The agency ran the risk of seeing its funding cut if it were to pursue the Bush plan.&lt;/p&gt;
    &lt;p&gt;The IRS commissioner at the time, Charles Rossotti, also opposed the idea. The IRS’ customer service staff, already too thin to respond adequately to Americans’ questions about the tax code, would have to grow substantially to handle millions of software queries. Congress “will never give you sufficient funding,” Rossotti told ProPublica.&lt;/p&gt;
    &lt;p&gt;So the IRS felt caught in the middle. The question became, Lutes said, “Is there some way to come out of this with something for taxpayers that addresses the administration’s objective and at the same time is acceptable to industry?”&lt;/p&gt;
    &lt;p&gt;Intuit, it turned out, did have a way. Since 1999, as part of the company’s strategy to head off encroachment, TurboTax had been offering free tax prep to the poorest filers. It was a program that served to bolster the company’s arguments that government intervention was unnecessary.&lt;/p&gt;
    &lt;p&gt;This became the basis for a deal. The industry would offer free tax prep to a larger portion of taxpayers. In exchange, the IRS would promise not to develop its own system.&lt;/p&gt;
    &lt;p&gt;Intuit organized a coalition of tax prep companies under the name the Free File Alliance, and after negotiations with the IRS, the group agreed to provide free federal filing to 60% of taxpayers, or about 78 million people at the time. Government officials touted the solution as a marvel of public and private cooperation. Americans would get free tax prep, and it would cost the government almost nothing.&lt;/p&gt;
    &lt;p&gt;For Intuit, it was the culmination of years of lobbying. The IRS had signed a contract that said it “will not compete with the [Free File Alliance] in providing free, online tax return preparation and filing services to taxpayers.”&lt;/p&gt;
    &lt;p&gt;What’s more, “free” wasn’t as unprofitable as it sounded. The alliance, guided by a lawyer who was also an Intuit lobbyist, won a series of concessions that made the program palatable to industry. Free File only required the companies to offer free federal returns. They could charge for other products. The state return was the most common, but they could also pitch loans, “audit defense” or even products that had nothing to do with taxes.&lt;/p&gt;
    &lt;p&gt;Free File had another bright side: The companies could tailor their Free File offers so that they didn’t cut into their base of paying customers. The agreement said the industry had to offer free federal services to at least 60% of taxpayers, but each company individually only had to cover 10% of taxpayers. Intuit and the others were free to limit their offers of free tax prep by age, income or state.&lt;/p&gt;
    &lt;p&gt;There was little incentive for the companies to publicize a free alternative to their paid products, and the IRS agreed that the Free File offers need only be listed on a special page of the agency’s website.&lt;/p&gt;
    &lt;p&gt;For Intuit, it was a major victory in the war against encroachment. The company could now focus on turning whatever new customers it acquired through the program into paying users, both that year and in the future.&lt;/p&gt;
    &lt;p&gt;The first year of Free File was 2003, and for Intuit, things went well. On paper, the Free File Alliance was a collection of 17 companies, all of them vying to serve the American taxpayer. But in reality, it was a group made up of two giants and a bunch of gnats. Intuit’s only significant competitor was H&amp;amp;R Block, and even it was a distant second. The rest of the alliance consisted mostly of tiny companies with names like Free1040TaxReturns.com. As a result, Intuit could tailor its Free File offer just the way it wanted.&lt;/p&gt;
    &lt;p&gt;But the next year, Intuit began to lose control of its creation. A scrappy competitor, TaxAct, decided to use Free File to stand out. The company decided it would try to pick up as many new customers as possible and then charge them for ancillary services. Instead of following Intuit’s lead and constraining its offer to a subset of low-income taxpayers, TaxAct went the opposite direction.&lt;/p&gt;
    &lt;p&gt;“Why not go for an offer that’s much simpler to understand?” is how Lance Dunn, the president of the maker of TaxAct, described the strategy in a later court hearing. It began advertising a pitch for “free federal online tax preparation and e-filing for all taxpayers. No restrictions. Everyone qualifies.”&lt;/p&gt;
    &lt;p&gt;TurboTax’s offer on the Free File page, meanwhile, was more difficult to parse: “if you are eligible for EIC, are age 22 or younger, age 62 or older, or active Military with a W2.” (EIC stood for the Earned Income Tax Credit.)&lt;/p&gt;
    &lt;p&gt;TaxAct’s ploy was a smashing success. The company’s volume exploded.&lt;/p&gt;
    &lt;p&gt;Alarmed, Intuit tried to get the other companies not to offer their products for free to too many potential customers, according to Dunn. Such a request could be collusion, a violation of antitrust law, Dunn said. “Intuit asked the Free File Alliance members that we should restrict offers, which I believe is probably not legal for that group to restrain trade,” he said.&lt;/p&gt;
    &lt;p&gt;ProPublica asked Intuit about Dunn’s accusation, but the company did not respond.&lt;/p&gt;
    &lt;p&gt;Dunn, who declined to speak with ProPublica, made these remarks during sworn testimony in 2011. The hearing was part of an antitrust case by the Justice Department against H&amp;amp;R Block after it tried to buy TaxAct. The U.S. argued that, by eliminating a competitor, the merger would create a duopoly of Intuit and H&amp;amp;R Block. Although the Justice Department ultimately blocked that takeover, the market has grown even more consolidated in recent years. In 2019, according to a ProPublica analysis of IRS data, the two giants accounted for 81% of all individual returns filed using tax prep software.&lt;/p&gt;
    &lt;p&gt;On the defensive, Intuit and H&amp;amp;R Block matched TaxAct’s “no restrictions” offer on Free File. Americans rushed to file for free, and in 2005, 5 million people filed their taxes through the program. Free File had become the most popular way to file taxes online.&lt;/p&gt;
    &lt;p&gt;Intuit viewed the popularity of Free File as a serious threat and took its case to Congress. That year, Brad Smith, then a senior vice president at the company and head of TurboTax, told a House committee that “the current Free File Alliance program has drifted very far from its original public service purpose and objective,” as he put it. The program wasn’t supposed to be for everyone, he said: It was for the “disadvantaged, underprivileged and underserved taxpayer populations.”&lt;/p&gt;
    &lt;p&gt;Intuit’s arguments quickly gained traction at the IRS. Already, in March 2005, the IRS had written to the Justice Department for legal advice on modifying the Free File program. The agency wanted to know: Would it run afoul of antitrust laws if the IRS barred companies in the Free File Alliance from offering a free product to everyone?&lt;/p&gt;
    &lt;p&gt;The Justice Department responded in a May 2005 letter. Clearly, wrote Renata Hesse, an antitrust section chief at the department, “any agreement among Alliance members to restrict such free service is likely a form of price fixing” and thus illegal. But there was still a way for Intuit to get what it wanted. She wrote that if the IRS itself were to impose such a restriction, it would be legal.&lt;/p&gt;
    &lt;p&gt;The IRS swooped in to beat back Intuit’s competition, doing for Intuit what the company could not on its own. Despite just 5 million Americans using a program that was purportedly available to 80 million, the IRS agreed that Free File needed to be reined in.&lt;/p&gt;
    &lt;p&gt;The agency made its reasoning clear in a previously unreported letter sent to the Free File Alliance the following year. Bert DuMars, then head of electronic filing at the IRS, wrote that there’d been a huge jump in people using Free File in 2005, but no corresponding boom in people paying for tax prep. “If this trend continued, the IRS was concerned that it could cause many vendors to go out of business,” he wrote. Stock market analysts, he pointed out, had said Free File “represented a threat to future revenues and profits of the publicly traded company participants.” The IRS decided to remove this threat.&lt;/p&gt;
    &lt;p&gt;The new agreement, struck between the IRS and the alliance in 2005, gave Intuit what it had sought. Companies were now expressly barred from offering free tax prep to everyone through the program. Instead, only taxpayers under an income cap, then $50,000 a year, would be eligible.&lt;/p&gt;
    &lt;p&gt;On paper, the program’s eligibility had actually increased to 70% of taxpayers, or about 93 million households, up from the previous 78 million. But in practice, because broad, easy-to-understand offers were now barred, it was clear the program’s use would decline.&lt;/p&gt;
    &lt;p&gt;Intuit had again bent the power of the federal government in its favor. After 2005, the Free File program was never again as popular, eventually falling to about half that year’s level.&lt;/p&gt;
    &lt;p&gt;With the threat of government encroachment on ice and high-speed internet access proliferating in the mid-2000s, Intuit looked forward to steady growth and big profits. The upside of the online software business was huge, with the cost of producing each additional unit approaching zero. And TurboTax was hardly a niche product: Intuit executives still excitedly talk about the TAM, total available market, of TurboTax as every single tax filer in the country, now over 150 million households.&lt;/p&gt;
    &lt;p&gt;But TaxAct’s Free File gambit had forever transformed the industry. Advertising “free” was a great lure, so TaxAct took the battle to a different venue. Barred from making a free offer to everyone through Free File on the IRS’ website, TaxAct decided to make the offer on its own website in 2006. Intuit recognized a credible challenge from the upstart and countered the next year, launching TurboTax Free Edition on its website.&lt;/p&gt;
    &lt;p&gt;Confusingly, there were now two distinct options: the government-sponsored Free File and the commercial free editions.&lt;/p&gt;
    &lt;p&gt;For customers who managed to qualify, the new commercial options offered by these companies were similar to what they could get on the IRS’ Free File website: The underlying software was the same, only the federal return was free, and the companies expected to make money on each customer through charging for a state tax return or other services.&lt;/p&gt;
    &lt;p&gt;But for the companies, there was a clear benefit to winning customers directly, rather than through the IRS program. The companies had complete control over how they handled customers from start to finish.&lt;/p&gt;
    &lt;p&gt;Intuit poured ad dollars into its Free Edition. Not only did the new product effectively meet TaxAct’s challenge, it quickly became the major driver of TurboTax’s customer growth.&lt;/p&gt;
    &lt;p&gt;That growth posed a challenge: how to, as internal company documents put it, “monetize free.” Over successive tax seasons, Intuit unleashed teams of designers, engineers, marketers and data scientists on that problem, working at its headquarters in Mountain View and TurboTax’s main offices in San Diego.&lt;/p&gt;
    &lt;p&gt;Part of the solution was to pitch users side products like loans or “Audit Defense.” But it also meant misleading customers. Frequently “free” didn’t mean free at all. Many who started in TurboTax Free Edition found that if their return required certain commonplace tax forms, they would have to upgrade to a paid edition in order to file.&lt;/p&gt;
    &lt;p&gt;The company came to a key insight: Americans’ anxiety around tax filing is so powerful that it usually trumps any frustration with the TurboTax product, according to three former Intuit staffers. So even if customers click on “free” and are ultimately asked to pay, they will usually do it rather than start the entire process anew. Intuit capitalized on this tendency by making sure the paywall popped up only when the taxpayer was deep into the filing process.&lt;/p&gt;
    &lt;p&gt;“There’s a lot of desperation — people will agree, will click, will do anything to file,” said a former longtime software developer.&lt;/p&gt;
    &lt;p&gt;Every fall before tax season, the company puts every aspect of the TurboTax homepage and filing process through rigorous user testing. Design decisions down to color, word choice and other features are picked to maximize how many customers pay, regardless if they are eligible for the free product. “Dark patterns are something that are spoken of with pride and encouraged in design all hands” meetings, said one former designer. In the design world, “dark patterns” are tactics to get users to do something they don’t necessarily mean to do. (ProPublica previously documented dark patterns encountered by people trying to file their taxes for free.)&lt;/p&gt;
    &lt;p&gt;On TurboTax’s homepage, for example, the company carefully chooses how it describes the different editions. Prominently featured next to Deluxe Edition, which costs around $100, is the phrase “maximize your deductions.”&lt;/p&gt;
    &lt;p&gt;If users initially click on the Deluxe software, they are never offered the choice to go to the Free Edition even if the no-cost option would produce the same return. “Maximize your deductions” was legendary at Intuit for its effectiveness in steering customers eligible for free filing to buy the paid product, according to a former marketing staffer.&lt;/p&gt;
    &lt;p&gt;Another celebrated feature, former staffers said, were the animations that appear as TurboTax users prepare their returns. One shows icons representing different tax deductions scrolling by, while another, at the end of the process, shows paper tax forms being scanned line-by-line and the phrase “Let’s comb through your returns.” What users are not told is that these cartoons reflect no actual processing or calculations; rather, Intuit’s designers deliberately added these delays to both reinforce and ease users’ “Fear, Uncertainty, and Doubt.” The animations emphasize that taxes are complicated but also reassure users that the technological wizardry of TurboTax will protect them from mistakes.&lt;/p&gt;
    &lt;p&gt;In a statement, the Intuit spokesman said, “The process of completing a tax return often has at least some level of stress and anxiety associated with it. … To offset these feelings, we use a variety of design elements — content, animation, movement, etc. — to ensure our customers’ peace of mind.”&lt;/p&gt;
    &lt;p&gt;The 2007 launch of Free Edition started a period of rapid growth for TurboTax. Within two years, use of its web products had almost doubled, and over the past decade, its website has grown each year by an average of 2 million more customers. The company reported this year that TurboTax online had handled 32 million returns. In a statement, it said around a third of that number used Free Edition.&lt;/p&gt;
    &lt;p&gt;The government’s Free File program, meanwhile, has mostly faded into the background, drowned out by Intuit’s and other companies’ “free” offers. The IRS did try advertising campaigns, spending around $2 million some years to spread the word. But compared with the reach of Intuit, this was a pittance: The company reported this year that it spent $800 million on advertising. With its budget slashed by Congress, the IRS has spent no money at all to advertise the program in recent years.&lt;/p&gt;
    &lt;p&gt;Amid its success, Intuit has sometimes had to put down insurgents bent on reforming the tax filing system. In 2007, the same year Intuit launched its Free Edition, Barack Obama, then a candidate for president, took aim at the tax prep industry. In a speech to an audience of tax wonks in Washington, he promised that the IRS would establish a simple return system. “This means no more worry, no more waste of time, no more extra expense for a tax preparer,” he declared.&lt;/p&gt;
    &lt;p&gt;But the Obama administration, as Bush’s had before, found that it was no match for Intuit.&lt;/p&gt;
    &lt;p&gt;Again, Bernie McKay, the lobbyist who had joined Intuit in the late 1990s and outlasted multiple CEOs, led the company’s campaign. In response to the Obama threat, McKay and Intuit’s small army of outside lobbyists turned to Congress, where lawmakers friendly to the company introduced a series of bills that would elevate Free File from a temporary deal with the IRS to the law of the land.&lt;/p&gt;
    &lt;p&gt;Republicans have historically been the company’s most reliable supporters, but some Democrats joined them. Rep. Zoe Lofgren, the California Democrat whose district includes part of Silicon Valley, has introduced or co-sponsored five bills over the years that would codify the Free File program, with names like the Free File Permanence Act. Lofgren’s spokesperson told ProPublica that the congresswoman believes the IRS, because of its role as tax collector, should not also be the tax preparer.&lt;/p&gt;
    &lt;p&gt;Hedging its bets, the company also sought to make sure the IRS could not spend a single dollar creating a public filing system. One internal document says Intuit would “advance legislative language in House Appropriations for ‘No Funds’ restriction on IRS spending” on such a system. It worked. Within a few years, Congress passed a 3,000-page appropriations bill that included a single sentence crucial to Intuit’s financial future: “No funds,” the law decreed, could be used “to provide to any person a proposed final return or statement.”&lt;/p&gt;
    &lt;p&gt;Another important aspect of Intuit’s influence strategy during the Obama years was covertly enlisting minority and women’s groups to press its case.&lt;/p&gt;
    &lt;p&gt;The internal 2014-15 “encroachment strategy” document discloses plans to “leverage trade groups to support House/Senate Free File bills.” It goes on to list the groups Women Impacting Public Policy, The Latino Coalition and the National Black Chamber of Commerce.&lt;/p&gt;
    &lt;p&gt;Intuit has given money to all of those groups over the years. All have signed letters urging Congress to make the Free File deal permanent. “The Free File program has been a clear success,” said one letter signed by The Latino Coalition and the Hispanic Leadership Fund.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Women Impacting Public Policy said it has received $70,000 from Intuit. The amounts given to the other groups are unknown, and they did not respond to requests for comment.&lt;/p&gt;
    &lt;p&gt;Company documents also outline plans to “mobilize” a “coalition” that included think tanks and academics, who published op-eds.&lt;/p&gt;
    &lt;p&gt;Will Marshall, president of the pro-business Progressive Policy Institute, opposed return-free filing in an op-ed in The Hill because doing one’s taxes is “a teachable moment [that] prompts us to review our financial circumstances.”&lt;/p&gt;
    &lt;p&gt;Anti-tax activist Grover Norquist, the most consistent champion of Intuit’s policy positions, warned that “big spenders in Washington, D.C. want to socialize all tax preparation in America.”&lt;/p&gt;
    &lt;p&gt;It is unclear whether they were paid by Intuit or the Free File Alliance. Norquist didn’t respond to a request for comment, and a Progressive Policy Institute spokesman declined to say whether Intuit gave the group money.&lt;/p&gt;
    &lt;p&gt;Whatever external challenges to the status quo Intuit has faced, the company has been able to rely on the IRS’ continuing enthusiastic support of the Free File program. Every few years, the IRS and the industry got together to renew the deal.&lt;/p&gt;
    &lt;p&gt;In part, that was due to the relationships Intuit had developed with high-ranking IRS officials. One, Dave Williams, served as the agency’s top negotiator on the Free File program for several years and “was very commercially sensitive,” said Mark Ernst, the CEO of H&amp;amp;R Block until 2007. Ernst, who later held a senior role at the IRS, told ProPublica that Williams “didn’t want to offend the industry,” noting that “he was particularly open to having sidebar conversations with key people where he could imagine himself landing some day.”&lt;/p&gt;
    &lt;p&gt;Today, Williams works at Intuit, where he’s held the title of chief tax officer since 2013. He is one of several former IRS employees who have gone on to work there. In a statement, Williams told ProPublica he did not have discussions about future employment with Intuit or other companies until after he left the IRS. He added that his career in government was focused on “what is best for the taxpayer” and that he “joined Intuit for the same reason: to help the American taxpayer.”&lt;/p&gt;
    &lt;p&gt;Despite Free File’s declining use, the IRS often claimed that the program was nevertheless meeting one of its original goals: driving more people to file electronically instead of on paper. Ernst, who served as a senior official at the IRS from 2009 to 2010, didn’t believe that a program used by so few people was having any such effect. “It was a talking point that got trotted out all the time to justify the Free File Alliance,” he said.&lt;/p&gt;
    &lt;p&gt;Internally, IRS managers have also argued that the program is, in a way, a success, because it created “a free marketplace,” as one internal management report in 2017 put it. Apparently, customers weren’t the only ones taken in by the word “free.”&lt;/p&gt;
    &lt;p&gt;In 2018, Intuit faced rare scrutiny from inside the IRS. The agency asked its Advisory Council, a group of outside experts, to take stock of Free File. To the company’s alarm, it soon became apparent that the council’s report might be sharply critical.&lt;/p&gt;
    &lt;p&gt;That July, council chair and University of California, Davis, law professor Dennis Ventry wrote two pieces criticizing an Intuit-backed bill in Congress that would make the program permanent. His op-ed in The Hill was called, “Free File providers scam taxpayers; Congress shouldn’t be fooled.”&lt;/p&gt;
    &lt;p&gt;In response, the IRS again rose to Intuit’s aid. It rushed to assure the company that Ventry’s power to affect the program was limited, according to emails to the Free File Alliance obtained through a public records request.&lt;/p&gt;
    &lt;p&gt;“The Commissioner has met directly with Mr. Ventry,” IRS official Ken Corbin wrote to Steve Ryan, a lobbyist for Intuit who also represented the alliance. “Mr. Ventry will recuse himself from participating or contributing to the topic of Free File.”&lt;/p&gt;
    &lt;p&gt;Corbin heads the IRS division that processes most Americans’ tax returns and negotiates the Free File deal with Intuit and the industry.&lt;/p&gt;
    &lt;p&gt;A few days later, Ryan arrived at the IRS’ Constitution Avenue headquarters in Washington to mount a defense of the program. A former Democratic Senate aide turned lawyer-lobbyist, Ryan is known on Capitol Hill for taking on politically fraught clients, including Trump attorney Michael Cohen and the government of Qatar. He helped create Free File in the early 2000s, and it was now his job to secure its future.&lt;/p&gt;
    &lt;p&gt;Ryan’s PowerPoint presentation at the IRS rehashed arguments that the company had been making for the past 15 years. It also highlighted a 2013 study by Brown University professor John Friedman, a former Obama National Economic Council official, to make the point that the program had been successful in generating “Free Tax Returns Outside of Free File.” The presentation did not mention that Friedman’s study was paid for by the Free File companies and was not published in an academic journal. Friedman declined to say what he was paid but told ProPublica he “wrote the piece based on my analysis of the issues, which I stand by.”&lt;/p&gt;
    &lt;p&gt;Ventry, who attended the meeting, got a call the next day alerting him that a California public records request had been filed for his emails — they were subject to such a request because he’s an employee of a state university. It came from the Free File Alliance, as The New York Times later reported. The request, Ventry believes, was designed to “freak me out.”&lt;/p&gt;
    &lt;p&gt;In early October, the council sent a version of its final report, which included a harsh appraisal of the Free File program, to the IRS to seek responses before releasing it publicly the following month.&lt;/p&gt;
    &lt;p&gt;But in mid-October, just weeks before the report saw the light of day, the Free File industry group fired off an “urgent” request to meet with IRS officials. The goal was to re-sign and “improve” the memorandum of understanding that governed the Free File program, according to the emails. The current agreement wasn’t expiring for another two years, but Ryan cited the “time urgency to make changes that will benefit taxpayers” in the coming tax season, adding, “I have not darkened your door in 2018 and need your … attention to this opportunity.”&lt;/p&gt;
    &lt;p&gt;The IRS’ Corbin signed the new deal on Oct. 31. Two weeks later, the Advisory Council report was released, with a damning indictment of the program: “The IRS’s deficient oversight and performance standards for the Free File program put vulnerable taxpayers at risk,” the report found.&lt;/p&gt;
    &lt;p&gt;The expert body recommended that the IRS negotiate a series of new provisions designed to increase the use and oversight of the program, including mandating advertising by the companies. But it was too late. A new deal had already been signed with modest changes. As it had in the past, Intuit and the alliance had effectively insulated the program from reform. Members of the council, Ventry said, were “pissed off.”&lt;/p&gt;
    &lt;p&gt;A spokesman for the Free File Alliance said the group had pushed to renegotiate the deal in 2018 because of the looming 2020 presidential campaign. “The reason for the timing of the extension of the agreement was the political season,” he said. The group had not seen the report before its release, he added.&lt;/p&gt;
    &lt;p&gt;(In August, ProPublica sued the IRS to get more correspondence between the agency and Intuit’s lobbyists. In response to our Freedom of Information Act requests, the agency has withheld over 100 pages. The case is ongoing.)&lt;/p&gt;
    &lt;p&gt;The new deal included rules that barred Free File companies from offering extra products to the relatively small number of users who access the program. This makes it much more difficult to convert those users into paying customers.&lt;/p&gt;
    &lt;p&gt;At around the same time, the industry took steps to make the program more difficult to find. Both Intuit and H&amp;amp;R Block added code to their Free File websites that shielded them from search engines such as Google. The Intuit spokesman said the company increased paid search advertising for Free File “by nearly 80 percent” over the last year and has data showing more people found the program through online search this year than last year, but he declined to provide specific figures.&lt;/p&gt;
    &lt;p&gt;What is clear is that Intuit’s business relies on keeping the use of Free File low. The company has repeatedly declined to say how many of its paying customers are eligible for the program, which is currently open to anyone who makes under $66,000. But based on publicly available data and statements by Intuit executives, ProPublica estimates that roughly 15 million paying TurboTax customers could have filed for free if they found Free File. That represents more than $1.5 billion in estimated revenue, or more than half the total that TurboTax generates. Those affected include retirees, students, people on disability and minimum-wage workers.&lt;/p&gt;
    &lt;p&gt;Customers, meanwhile, remain confused by Intuit’s myriad uses of “free,” and internal documents show the company knows it. Over just a two-week period this past filing season, Intuit received nearly 7,000 TurboTax customer calls in which the phrase “supposed to be free” was uttered, according to a company analysis. One customer complained that Intuit charged him even though “it says ‘free free free’ on the commercial.” The TurboTax representative responded: “That ad has been the bane of my existence.”&lt;/p&gt;
    &lt;p&gt;Even as TurboTax’s business thrived, 2019 has been a rocky year for Intuit’s long-running war against government encroachment. In April, the company was close to finally succeeding in its long-held goal to make Free File permanent. A bill called the Taxpayer First Act was sailing toward almost unanimous approval in Congress. But after ProPublica published a series of stories about the program, including a story showing that military families and students were particularly affected by Intuit’s business tactics, the bill stalled. Congress ultimately removed the provision that would have enshrined Free File in law.&lt;/p&gt;
    &lt;p&gt;After having enabled Intuit for so long, the IRS finally responded to the pressure. It hired a contractor to review the Free File program. But the contractor had previously argued against the IRS offering its own tax prep option, and the review did not recommend major changes. The agency has not yet announced its plans for the future of the program.&lt;/p&gt;
    &lt;p&gt;The agency’s inspector general also launched an audit, which is ongoing. Other investigations and litigation followed, ranging from class-action complaints, alleging that consumers had been deceived by Intuit’s tactics, to investigations and lawsuits by regulators and prosecutors in New York and California. Intuit has denied wrongdoing, saying it “has at all times been clear and fair with its customers.”&lt;/p&gt;
    &lt;p&gt;Despite the scrutiny, Wall Street has continued to embrace the company’s business model. The company recently announced it made $1.5 billion in profits for its fiscal year. It expects its TurboTax unit to grow by 10% next year. Last year the CEO was paid $20 million. The share price hit an all-time record.&lt;/p&gt;
    &lt;p&gt;The company has returned to its old strategy: stay the course and take its case directly to the IRS and Congress. Its allies in the Senate have again advanced an appropriations bill that would bar the IRS from developing its own tax filing system. In the spring, Sasan Goodarzi, a former head of the TurboTax unit who took over as CEO of the entire company in January, sought to reassure employees.&lt;/p&gt;
    &lt;p&gt;“Our view is this will be in the press until there is a resolution with the IRS,” he said, according to the video obtained by ProPublica. “And we’re working with them and we feel very good about where this will end.”&lt;/p&gt;
    &lt;p&gt;Doris Burke contributed research to this story.&lt;/p&gt;
    &lt;p&gt;Do you have information about Intuit, the IRS or tax prep? We want to hear from you. Fill out our questionnaire or contact Justin at [email protected] or via Signal at 774-826-6240.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45601750</guid><pubDate>Thu, 16 Oct 2025 05:31:31 +0000</pubDate></item><item><title>New coding models and integrations</title><link>https://ollama.com/blog/coding-models</link><description>&lt;doc fingerprint="3d49ccd3b51d4a3e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New coding models &amp;amp; integrations&lt;/head&gt;
    &lt;head rend="h2"&gt;October 16, 2025&lt;/head&gt;
    &lt;p&gt;GLM-4.6 and Qwen3-coder-480B are available on Ollama’s cloud service with easy integrations to the tools you are familiar with. Qwen3-Coder-30B has been updated for faster, more reliable tool calling in Ollama’s new engine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started&lt;/head&gt;
    &lt;p&gt;GLM-4.6&lt;/p&gt;
    &lt;code&gt;ollama run glm-4.6:cloud
&lt;/code&gt;
    &lt;p&gt;Qwen3-Coder-480B&lt;/p&gt;
    &lt;code&gt;ollama run qwen3-coder:480b-cloud
&lt;/code&gt;
    &lt;p&gt;For users with more than 300GB of VRAM, &lt;code&gt;qwen3-coder:480b&lt;/code&gt; is also available locally.&lt;/p&gt;
    &lt;p&gt;Qwen3-Coder-30B&lt;/p&gt;
    &lt;code&gt;ollama run qwen3-coder:30b
&lt;/code&gt;
    &lt;head rend="h3"&gt;Example prompts&lt;/head&gt;
    &lt;code&gt;Create a single-page app in a single HTML file with the following requirements:

Name: Ollama's Adventure 
Goal: Jump over obstacles to survive as long as possible.
Features: Increasing speed, high score tracking, retry button, and funny sounds for actions and events.

The UI should be colorful, with parallax scrolling backgrounds.
The characters should look cartoonish, related to alpacas and be fun to watch.
The game should be enjoyable for everyone.
&lt;/code&gt;
    &lt;p&gt;Example code by GLM-4.6 in a single prompt&lt;/p&gt;
    &lt;head rend="h2"&gt;Usage with VS Code&lt;/head&gt;
    &lt;p&gt;First, pull the coding models so they can be accessed via VS Code:&lt;/p&gt;
    &lt;code&gt;ollama pull glm-4.6:cloud
ollama pull qwen3-coder:480b-cloud
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open the copilot chat sidebar&lt;/item&gt;
      &lt;item&gt;Select the model dropdown → Manage models&lt;/item&gt;
      &lt;item&gt;Click on Ollama under Provider Dropdown, then select desired models&lt;/item&gt;
      &lt;item&gt;Select the model dropdown → and choose the model (e.g. &lt;code&gt;glm-4.6&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Usage with Zed&lt;/head&gt;
    &lt;p&gt;First pull the coding models so they can be accessed via Zed:&lt;/p&gt;
    &lt;code&gt;ollama pull glm-4.6:cloud
ollama pull qwen3-coder:480b-cloud
&lt;/code&gt;
    &lt;p&gt;Then, open Zed (now available for Windows!)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Click on the agent panel button (glittering stars)&lt;/item&gt;
      &lt;item&gt;Click on the model dropdown → Configure&lt;/item&gt;
      &lt;item&gt;Select LLM providers → Ollama&lt;/item&gt;
      &lt;item&gt;Confirm the Host URL is &lt;code&gt;http://localhost:11434&lt;/code&gt;, then click Connect&lt;/item&gt;
      &lt;item&gt;Select a model under Ollama&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Usage with Droid&lt;/head&gt;
    &lt;p&gt;First, install Droid:&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://app.factory.ai/cli | sh
&lt;/code&gt;
    &lt;p&gt;Add the following configuration to &lt;code&gt;~/.factory/config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "custom_models": [
    {
      "model_display_name": "GLM-4.6",
      "model": "glm-4.6:cloud",
      "base_url": "http://localhost:11434/v1",
      "api_key": "not-needed",
      "provider": "generic-chat-completion-api",
      "max_tokens": 16384
    },
    {
      "model_display_name": "Qwen3-Coder-480B",
      "model": "qwen3-coder:480b-cloud",
      "base_url": "http://localhost:11434/v1",
      "api_key": "not-needed",
      "provider": "generic-chat-completion-api",
      "max_tokens": 16384
    }
  ]
}
&lt;/code&gt;
    &lt;p&gt;Then run Droid and type &lt;code&gt;/model&lt;/code&gt; to change to the model:&lt;/p&gt;
    &lt;code&gt;╭──────────────────────────────────────────────────╮
│ &amp;gt; GLM-4.6 [current]                              │
│   Qwen3-Coder-480B                               │
│                                                  │
│ ↑/↓ to navigate, Enter to select, ESC to go back │
╰──────────────────────────────────────────────────╯
&lt;/code&gt;
    &lt;head rend="h2"&gt;Integrations&lt;/head&gt;
    &lt;p&gt;Ollama’s documentation now includes sections on using Ollama with popular coding tools:&lt;/p&gt;
    &lt;head rend="h2"&gt;Cloud API access&lt;/head&gt;
    &lt;p&gt;Cloud models such as &lt;code&gt;glm-4.6&lt;/code&gt; and &lt;code&gt;qwen3-coder:480b&lt;/code&gt; can also be accessed directly via ollama.com’s cloud API:&lt;/p&gt;
    &lt;p&gt;First, create an API key, and set it in your environment&lt;/p&gt;
    &lt;code&gt;export OLLAMA_API_KEY="your_api_key_here"
&lt;/code&gt;
    &lt;p&gt;Then, call ollama.com’s API&lt;/p&gt;
    &lt;code&gt;curl https://ollama.com/api/chat \
    -H "Authorization: Bearer $OLLAMA_API_KEY" \
    -d '{
    "model": "glm-4.6",
    "messages": [{
      "role": "user",
      "content": "Write a snake game in HTML."
    }]
}'
&lt;/code&gt;
    &lt;p&gt;For more information see the Ollama’s API documentation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45601834</guid><pubDate>Thu, 16 Oct 2025 05:46:50 +0000</pubDate></item><item><title>Upcoming Rust language features for kernel development</title><link>https://lwn.net/Articles/1039073/</link><description>&lt;doc fingerprint="9fa9d83718a13ee8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Upcoming Rust language features for kernel development&lt;/head&gt;
    &lt;p&gt;The Rust for Linux project has been good for Rust, Tyler Mandry, one of the co-leads of Rust's language-design team, said. He gave a talk at Kangrejos 2025 covering upcoming Rust language features and thanking the Rust for Linux developers for helping drive them forward. Afterward, Benno Lossin and Xiangfei Ding went into more detail about their work on the three most important language features for kernel development: field projections, in-place initialization, and arbitrary self types.&lt;/p&gt;
    &lt;p&gt; Many people have remarked that the development of new language features in Rust can be quite slow, Mandry said. Partly, that can be attributed to the care the Rust language team takes to avoid enshrining bad designs. But the biggest reason is "&lt;quote&gt;alignment in attention&lt;/quote&gt;". The Rust project is driven by volunteers, which means that if there are not people focusing on pushing a given feature or group of related features forward, they languish. The Rust for Linux project has actually been really helpful for addressing that, Mandry explained, because it is something that a lot of people are excited about, and that focuses effort onto the few specific things that the Linux kernel needs. &lt;/p&gt;
    &lt;p&gt; Mandry then went through a whirlwind list of upcoming language features, including types without known size information, reference-counting improvements, user-defined function modifiers of the same kind as const, and more. At the end, he asked which of those were most important to Rust for Linux, and how the assembled kernel developers would prioritize them. Beyond the three features to be discussed later, Lossin said that the project definitely wanted the ability to write functions that can be evaluated at compile time (called const functions in Rust) in trait definitions. Danilo Krummrich asked for specialization, which immediately prompted an "&lt;quote&gt;Oh no!&lt;/quote&gt;" from Lossin, due to the feature's nearly decade-long history of causing problems for Rust's type system. Specialization would allow two overlapping implementations for a single trait to exist, with the compiler picking the more specific one. Matthew Maurer asked for some ability to control what the compiler does on integer overflow. &lt;/p&gt;
    &lt;p&gt;Ultimately, Miguel Ojeda told Mandry that the priority should be on stabilizing the unstable language features that Rust for Linux currently uses, followed by language features that would change how the project structures its code, followed by everything else. The next two talks went into much more detail about the current status and future plans for some of those key language features.&lt;/p&gt;
    &lt;quote&gt;$ sudo subscribe today&lt;p&gt;Subscribe today and elevate your LWN privileges. You’ll have access to all of LWN’s high-quality articles as soon as they’re published, and help support LWN in the process. Act now and you can start with a free trial subscription.&lt;/p&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Field projections&lt;/head&gt;
    &lt;p&gt; Field projection refers to the idea of taking a pointer to a structure, and turning it into a pointer to a field of the structure. Rust does already have this for the built-in reference and pointer types, but it can't always be made to work for user-defined smart-pointer types. Since the Rust for Linux developers would like to have custom smart pointers to handle untrusted data, reference counting, external locking, and related kernel complications, they would benefit from a general language feature allowing field projections for all pointer types using the same syntax. Lossin spoke about his work on the problem, which has been ongoing since Kangrejos 2022. There has been "&lt;quote&gt;lots of progress&lt;/quote&gt;" so far, but the work is still in the design stage, with a few details left to work out. &lt;/p&gt;
    &lt;p&gt;The built-in field projections all have the same kind of type signature, Lossin explained. For example, the code for converting a reference to an object into a reference to one of its fields and the code for converting a raw pointer to an object into a raw pointer to one of its fields look different, but have similar signatures:&lt;/p&gt;
    &lt;quote&gt;fn project_reference(r: &amp;amp;MyStruct) -&amp;gt; &amp;amp;Field { &amp;amp;r.field } unsafe fn project_pointer(r: *mut MyStruct) -&amp;gt; *mut Field { unsafe { &amp;amp;raw mut (*r).field } } // The equivalent C code would look like this: struct field *project(struct my *r) { return &amp;amp;(r-&amp;gt;field); }&lt;/quote&gt;
    &lt;p&gt;This example uses the relatively recent raw borrow syntax.&lt;/p&gt;
    &lt;p&gt; The Pin type throws a bit of a wrench into things. The Rust compiler is, by default, free to move structures around for performance reasons. That doesn't work when the structure is being referenced from the C side, so the Pin type is used to mark structures that shouldn't be moved. Projecting a &lt;del&gt;Pin&amp;lt;MyStruct&amp;gt;&lt;/del&gt; Pin&amp;lt;&amp;amp;mut MyStruct&amp;gt; [Lossin sent LWN a correction: Pin is always used to wrap a pointer type, not a structure directly] might produce either a Pin&amp;lt;&amp;amp;mut Field&amp;gt; or a plain &amp;amp;mut Field depending on whether the field is also of a type that shouldn't be moved or not. So the most general possible signature for the field projection operation would be something like this, Lossin said: &lt;/p&gt;
    &lt;quote&gt;Container&amp;lt;'a, Struct&amp;gt; -&amp;gt; Output&amp;lt;'a, Field&amp;gt;&lt;/quote&gt;
    &lt;p&gt;That is, given some pointer type that wraps a structure and must be valid for lifetime a, projecting a field gives a (possibly different) output pointer type wrapping a field of that structure, valid for the same lifetime. Lossin then gave an example of how supporting this could make fully implementing read-copy-update (RCU) support in the kernel's Rust bindings a lot easier.&lt;/p&gt;
    &lt;p&gt;The RCU mechanism protects readers from concurrent writers, he explained, but it doesn't protect writers from each other. It's somewhat common in the kernel, therefore, to have a mutex protecting some data, with a frequently accessed field of that data being protected by RCU. That way, readers rely on the RCU lock (which is cheap), and writers synchronize with each other using the mutex. Translating that interface to Rust poses problems: Rust doesn't allow any access to the content inside a Mutex without locking it first, so the straightforward translation of this pattern wouldn't work. It would force Rust readers to lock the mutex in order to read the RCU field, which would be an unacceptable performance hit.&lt;/p&gt;
    &lt;p&gt;With generalized field projection in the language, though, the Rust for Linux developers could write bindings that permit projecting a &amp;amp;Mutex&amp;lt;MyStruct&amp;gt; into an &amp;amp;Rcu&amp;lt;Field&amp;gt; without holding the lock. In driver code, attempting to read from the RCU-protected field would look like a normal access, the same way it is in C — but the compiler would still check that the other, non-RCU-protected data isn't touched without holding the mutex.&lt;/p&gt;
    &lt;p&gt;Lossin ended by asking the assembled developers to keep an eye on the tracking issue for the feature, and provide feedback on it. Daniel Almeida asked whether testing the feature outside the mainline kernel was really helpful; Ojeda affirmed that it was, because that makes it easier to go to the Rust team and make a case to stabilize the feature. The Rust for Linux project is trying not to use any new unstable features (and to compile with a version of Rust equal to or older than the version packaged on Debian stable), so the feature needs to be completed and make it into Debian 14 (expected in 2027) before it will be widely usable in kernel code.&lt;/p&gt;
    &lt;p&gt; Andreas Hindborg asked: "&lt;quote&gt;Can we have this yesterday, please?&lt;/quote&gt;", to general amusement. The kernel's Rust bindings already feature a plethora of custom pointers encoding various invariants; this feature, whenever it becomes available to kernel code, may make them a good deal easier to use in driver code. &lt;/p&gt;
    &lt;head rend="h4"&gt;Arbitrary self types&lt;/head&gt;
    &lt;p&gt;Ding gave an update immediately afterward about another ergonomic language feature for custom pointers: arbitrary self types. In Rust, a method on a type can have a first argument that is an object of the type or that is a reference to one. Such a method can be called with the .method() syntax, instead of the more general Type::function() syntax. But the proliferation of smart pointers in kernel Rust code means that the programmer frequently does not have a plain reference; often, they instead have a Pin, an Arc, or some other smart-pointer type.&lt;/p&gt;
    &lt;p&gt;The arbitrary self types proposal that Ding has been working on would let programmers write methods that take smart pointers, instead of normal references:&lt;/p&gt;
    &lt;quote&gt;impl MyStruct { fn method(self: Pin&amp;lt;&amp;amp;mut MyStruct&amp;gt;) {} }&lt;/quote&gt;
    &lt;p&gt;Unfortunately, adding this to the compiler has not proved to be straightforward. The interaction with Rust's existing Deref trait, which makes custom smart pointers possible in the first place, complicates the implementation because not all of the type information is available while searching for matching methods. Currently, if the user has a Pin&amp;lt;&amp;amp;mut MyStruct&amp;gt; and they call a method on it, the compiler will first look for a matching method for Pin. If one isn't found, it will try to dereference the type, producing a &amp;amp;mut MyStruct. That type is checked for matching methods, and then is dereferenced one final time, producing a MyStruct. That type will finally have a matching method, or else the compiler will emit a type error.&lt;/p&gt;
    &lt;p&gt;By the time that procedure begins checking functions associated with MyStruct, it will have already discarded information about the wrapping types, which an implementation of arbitrary self types needs. Ding spent a few minutes explaining the approaches for rectifying the problem that he had tried and discarded, before focusing on the current approach. He has added another trait — tentatively called Receiver — that is used to mark types that can be used with arbitrary self types. Then the compiler can try following the chain of Receiver implementations before following the chain of Deref implementations. That does mean that a pointer type will have to opt into being used as an arbitrary self type, but Ding didn't see that as a downside. Letting the author of a pointer type decide when it should support the new feature eliminates a lot of concerns around accidentally introducing backward compatibility problems. For the kernel, it doesn't really impose a barrier, because the Rust developers can just add Receiver implementations as they run across cases that require them.&lt;/p&gt;
    &lt;p&gt;Ojeda asked how long Ding thought it would take to finalize the arbitrary self types feature; in particular, would it be ready within a year? Ding agreed that a year was possible, although he would need support from the Rust language team in order to make that happen. He wants to run Crater, the tool that the Rust community uses to check whether compiler changes break any published Rust libraries, against his change before submitting the code. Ojeda offered help with obtaining a large build machine to do that, since Ding has had trouble previously with the memory requirements to compile some packages during a Crater run.&lt;/p&gt;
    &lt;head rend="h4"&gt;In-place initialization&lt;/head&gt;
    &lt;p&gt;The other topic that Ding wanted to cover was his work on in-place initialization. Like the other new language features being discussed, this doesn't really enable new use cases, but it does make common kernel code cleaner. Currently, Rust code in the kernel uses the pin_init!() macro to create structures that are fixed in place after initialization (by being wrapped in Pin).&lt;/p&gt;
    &lt;p&gt; There's nothing wrong with pin_init!(): "&lt;quote&gt;We love pin_init!()! We want to make a language feature out of it.&lt;/quote&gt;" Adopting a language feature for in-place initialization would also help with a handful of sharp edges outside kernel code; it could make creating large Future values on the heap more ergonomic, and let some traits become dyn-compatible. The exact design of this language feature was more up in the air; Ding covered three different proposals for how it could work. &lt;/p&gt;
    &lt;p&gt;The simplest, proposed by Alice Ryhl and Lossin, would be to add a new keyword, init, before a structure-initializing expression in order to ask the compiler to automatically write an implementation of the kernel's PinInit trait. That has the nice benefit of being a fairly minimal change to the language, although it would lock in the use of the PinInit trait in its current form.&lt;/p&gt;
    &lt;p&gt;Another solution, proposed by Taylor Cramer, would introduce a new type of reference into the language. Rust's existing references can either be read from (&amp;amp;) or read from and written to (&amp;amp;mut). This proposal would add a third type, &amp;amp;out, that can only be written to, not read from. The only way to use an &amp;amp;out reference would be to either write to it, or use projection to break it apart into multiple &amp;amp;out references to various fields. Under this scheme, in-place initialization would look like allocating space on the heap, and then returning an &amp;amp;out reference. The calling code could then fill it in however it wants to, potentially passing off sub-parts to other functions. The compiler would track that the &amp;amp;out references are all used before allowing the code to obtain a normal &amp;amp;mut reference to the heap allocation.&lt;/p&gt;
    &lt;p&gt;That proposal was considerably less polished than Ryhl and Lossin's approach, however. Ding later told me that he, Mandry, and other compiler contributors at Kangrejos were actually working on figuring out how it would interact with some of the Rust compiler's internals in between talks that day. By the end of the conference, they had a rough idea of how it could be implemented, so a more detailed version of the out-pointer proposal may be forthcoming shortly.&lt;/p&gt;
    &lt;p&gt;The final design, taking inspiration from C++, would be a form of guaranteed optimization, where constructing a new value and then immediately moving it to the heap causes it to be constructed on the heap in the first place. Ding was less sure about the details of the final proposal; he suggested that the best way forward might be to implement both the PinInit proposal and the out-reference proposal, and see how well each approach works in practice.&lt;/p&gt;
    &lt;p&gt;Regardless of which approach ends up being chosen, it seems clear that Mandry's point about the Rust for Linux project driving language improvement is correct. While these features are in the early stages, adopting them could significantly simplify code involving user-defined smart pointers, both within and outside the kernel.&lt;/p&gt;
    &lt;p&gt;Update: Since the talks described in this article, the work on field projection has received an update. Lossin wrote in to inform LWN that all fields of all structures are now considered structurally pinned, so projecting a Pin will now always produce a Pin&amp;lt;&amp;amp;mut Field&amp;gt; or similar value.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Development tools/Rust&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Conference&lt;/cell&gt;
        &lt;cell&gt;Kangrejos/2025&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Oct 8, 2025 17:39 UTC (Wed) by NYKevin (subscriber, #129325) [Link] (2 responses) To the best of my understanding, this is true but incomplete. Rust generally assumes that it is safe to allow *the developer* to move structures around (for whatever reason they see fit). But there are some structures where doing this would invalidate some invariant or cause other problems. Normally, that would be the developer's problem, but in some cases, those invariants are required to avoid UB, so safe Rust must not allow a move in that case. FFI, as described in the article, is a good example of this. Another is self-referential structures (structures that hold pointers into themselves). The Pin type imposes an exception on this general rule - anything* behind a pinned pointer (a Pin&amp;lt;Ptr&amp;gt; where Ptr: Deref) is considered immovable. Aside for C++ developers: Because Rust always implements move by memcpy, this is nearly equivalent to saying that all Rust types are trivially movable unless protected by Pin. However, Rust does not run the destructor on the moved-from value (unlike C++, which does), so the vast majority of RAII types can be trivially movable in Rust but not in C++. Pin is only needed for the rare case where the move constructor would do something other than transferring ownership of a resource to the new instance. What makes Pin especially weird is the fact that it is not a language-level feature at all. It is a standard library feature, implemented entirely in terms of Rust's visibility and borrow-checking rules. In order to accomplish that, it wraps the pointer and makes it unsafe to obtain &amp;amp;mut T (where T is the pointee type), only allowing safe access to &amp;amp;T. Blocking &amp;amp;mut T is necessary because of functions like std::mem::swap() (roughly equivalent to C++'s std::swap()). But that would prevent safe Rust from mutating the pointee at all, so now you have to write a bunch of boilerplate to individually allow specific field mutations (even if the field is private, you probably still want a pub(crate) mutator for internal use). This boilerplate frequently contains a large amount of unsafe Rust to selectively unwrap the Pin, and that is generally considered problematic. If I'm reading between the lines correctly, this field projection proposal would let you put all of that unsafe boilerplate inside of your smart pointer implementation, so you don't have to keep rewriting it for each new pinnable (non-Unpin) type. That sounds like a pretty big win to me, but I imagine the RCU benefits are probably a bigger deal, since you can macro-generate the Pin boilerplate if you really want to. *** * Technically, there's an exception to the exception - (pointee) types that implement Unpin are immune to pinning and can be moved regardless. But Unpin is mostly just a convenience trait for generics (it allows them to use pinning pointers without having to worry about whether that's needlessly restrictive for any given pointee type), so for the purposes of this discussion, I'm going to ignore it. Posted Oct 8, 2025 17:51 UTC (Wed) by daroc (editor, #160859) [Link] In any case, I believe you're correct. If the pin-projection proposal is adopted, a lot of boiler-plate around accessing fields of pinned objects could go away. Instead you'd just use the same field-projection operator as with references, pointers, reference-counted pointers, etc. Posted Oct 9, 2025 7:19 UTC (Thu) by lossin (subscriber, #177724) [Link] Indeed, the field projection proposal is to have an operator that makes all the unsafe boilerplate obsolete. The wins around Pin are very big, as almost every type in the kernel will end up being pinned. That's because pinning is infectious -- as soon as one of the (transitive) fields of your struct requires it, the entire struct needs to be pinned. Field projections will also improve ergonomics for raw pointers, handling uninitialized data as well as untrusted data and many more user declared types. The RCU abstractions are a different beast, as without field projections, they probably aren't possible in a safe, ergonomic and performant way. Without it, we'd probably need two different APIs, one unsafe and performant, the other safe &amp;amp; ergonomic. But do note that we could also macro-generate the unsafe boilerplate similar to how it's possible with Pin. The issue is, that means yet another derive macro that one needs to think about (and rather bad ergonomics). &amp;gt; * Technically, there's an exception to the exception - (pointee) types that implement Unpin are immune to pinning and can be moved regardless. But Unpin is mostly just a convenience trait for generics (it allows them to use pinning pointers without having to worry about whether that's needlessly restrictive for any given pointee type), so for the purposes of this discussion, I'm going to ignore it. In the current version of the pin-ergonomics proposal, Unpin is going to play a more important role. It will allow coercions between Pin&amp;lt;&amp;amp;mut T&amp;gt; and &amp;amp;mut T if T: Unpin. Then we can just always allow pin projections, so going from Pin&amp;lt;&amp;amp;mut Struct&amp;gt; to Pin&amp;lt;&amp;amp;mut Field&amp;gt; and let the coercions &amp;amp; types handle the not structurally pinned fields. We still need field projections for pin projections in the kernel: our mutex guard is pinning the data and thus it also needs to provide this kind of projection. Posted Oct 8, 2025 18:48 UTC (Wed) by prittner (subscriber, #110534) [Link] (1 responses) This is a sorely needed feature for folks who work with large types. The cost of having to reserve lots of stack space, initialize the object, and then copy it to the heap can be significant; in some cases, it's impossible to construct the type on the stack at all due to its size (unless you also blow up your stack size). Inlining can help, but once you have to use dynamic dispatch that goes out the window. Posted Oct 9, 2025 7:30 UTC (Thu) by lossin (subscriber, #177724) [Link] Very much so. The Asahi GPU driver developed a similar macro-based solution to this problem around the same time we added the pin-init [1] crate that we use to this day. They regularly had structs with hundreds of fields that obviously overflowed the small kernel stack. IIRC, their use-case also wasn't inlined (at least not everywhere), which made it rather painful. This use-case is part of the core motivation for developing the language feature. &lt;head&gt;Pin is a bit more complicated than described&lt;/head&gt;&lt;head&gt;Pin is a bit more complicated than described&lt;/head&gt;&lt;head&gt;Pin is a bit more complicated than described&lt;/head&gt;&lt;lb/&gt; &amp;gt; &lt;lb/&gt; &amp;gt; If I'm reading between the lines correctly, this field projection proposal would let you put all of that unsafe boilerplate inside of your smart pointer implementation, so you don't have to keep rewriting it for each new pinnable (non-Unpin) type. That sounds like a pretty big win to me, but I imagine the RCU benefits are probably a bigger deal, since you can macro-generate the Pin boilerplate if you really want to.&lt;head&gt;Heap initialization is important&lt;/head&gt;&lt;head&gt;Heap initialization is important&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45601982</guid><pubDate>Thu, 16 Oct 2025 06:12:51 +0000</pubDate></item><item><title>The Hidden Math of Ocean Waves Crashes Into View</title><link>https://www.quantamagazine.org/the-hidden-math-of-ocean-waves-crashes-into-view-20251015/</link><description>&lt;doc fingerprint="f6481316a53293cc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Hidden Math of Ocean Waves Crashes Into View&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;The best perk of Alberto Maspero’s job, he says, is the view from his window. Situated on a hill above the ancient port city of Trieste, Italy, his office at the International School for Advanced Studies overlooks a broad bay at the northern tip of the Adriatic Sea. “It’s very inspiring,” the mathematician said. “For sure the most beautiful view I’ve ever had.”&lt;/p&gt;
    &lt;p&gt;Italians call Trieste la città della bora, after its famed “bora” wind, which blows erratically down off the Alps and over the city. When the bora is strong enough, it drives the waves into reverse. Instead of breaking against the docks, they stream away from the city, back toward the open sea.&lt;/p&gt;
    &lt;p&gt;But they never actually get there. Watching from his window on these gusty days, Maspero can see the retreating waves slowly disperse as they exit the port, eventually giving way to a calm, still surface.&lt;/p&gt;
    &lt;p&gt;The equations that mathematicians use to study the flow of water and other fluids — which Leonhard Euler first wrote down nearly 300 years ago — look simple enough. If you know the location and velocity of each droplet of water, and simplify the math by assuming there’s no internal friction, or viscosity, then solving Euler’s equations will allow you to predict how the water will evolve over any time period. The rich menagerie of phenomena we see in the world’s oceans — tsunamis, whirlpools, riptides — are all solutions to Euler’s equations.&lt;/p&gt;
    &lt;p&gt;But the equations are usually impossible to solve. Even one of the simplest and most common kinds of solutions — one that describes a steady train of gently rolling waves — is a mathematical nightmare to extract from Euler’s equations. Until about 30 years ago, the bulk of what we knew about these waves came only from a mix of real-world observations and guesswork. For the most part, proofs seemed like a fantasy.&lt;/p&gt;
    &lt;p&gt;“Before starting math, I thought water waves were something very understood — not a problem at all,” said Paolo Ventura, a postdoctoral fellow at the Swiss Federal Institute of Technology Lausanne and Maspero’s former graduate student. “But in reality, they are just strange.”&lt;/p&gt;
    &lt;p&gt;One strange phenomenon that has perplexed mathematicians for decades is that, even when friction is minimal, that steady train of gently rolling waves still eventually falls apart and becomes irregular. Mathematicians hadn’t expected to see such unstable behavior emerge from such a simple starting point. They wanted to prove it — to show that instabilities arise naturally from the Euler equations. But they couldn’t figure out how to do it.&lt;/p&gt;
    &lt;p&gt;Now Maspero and Ventura, along with their Trieste colleague Massimiliano Berti and Livia Corsi of Roma Tre University, have finally presented such a proof, showing exactly when these instabilities occur and when they don’t. The result is just the latest in a renaissance that’s starting to transform our mathematical understanding of Earth’s waves. Mathematicians have been using new computational tools to formulate conjectures about how waves behave. And they’re now developing sophisticated pen-and-paper techniques to prove those conjectures.&lt;/p&gt;
    &lt;p&gt;“It’s not one particular thing. It’s a whole wave of new types of analysis in multiple directions,” said Walter Strauss, a mathematician at Brown University. “I’m very impressed.”&lt;/p&gt;
    &lt;head rend="h2"&gt;A Slow Tide&lt;/head&gt;
    &lt;p&gt;The ancient Greeks often compared the unsteady beat of waves against the shore to laughter. Considering how those waves have eluded human understanding, perhaps they were right: The ocean has been laughing at us all along.&lt;/p&gt;
    &lt;p&gt;Even at the height of the Enlightenment in the late 17th and early 18th centuries, when waves took up much of the scientific discourse, the ocean always seemed to have the last word. A number of scientists had measured the speed of sound waves, and Newton and his detractors were locked in a conflict over the wavelike nature of light. But the oldest waves known to humans remained a mathematical enigma.&lt;/p&gt;
    &lt;p&gt;It would take more than a century for this to start to change. In the early 1800s, Sir George Stokes became fascinated with ocean waves when, as a boy, he was swimming near his home in Sligo, Ireland, and an enormous wave almost dragged him out to sea. In 1847, he published a monumental treatise on the topic. He started with Euler’s equations for a fluid with no viscosity and added the mathematical condition that its top surface be totally “free” — allowed to take any shape it pleased.&lt;/p&gt;
    &lt;p&gt;“They don’t look bad,” Strauss said of the resulting equations. “But just take a look at a lake with a little wind on it. You get all these complicated forms, like whitecaps and rolling waves, some parallel to each other, some not.”&lt;/p&gt;
    &lt;p&gt;Each of these varied forms, when understood as a solution to Euler’s equations, is mathematically distinct and terribly unwieldy. Make the tiniest change to the fluid’s initial state, and it might evolve in a vastly different way — bumps and eddies can become rogue waves and tsunamis.&lt;/p&gt;
    &lt;p&gt;These free, moving surfaces were what Stokes wanted to study. But the challenge was immense. Describing the motion of water confined within a box, or flowing through a pipe, is hard enough. But then, at least, you know where the system’s edges lie — no water can extend beyond those boundaries. If there’s no restriction other than the force of gravity on how high the water can reach and what shape it can take, the math becomes far more difficult.&lt;/p&gt;
    &lt;p&gt;“If I go to the beach at seven in the morning, it’s going to be very calm,” Corsi said. “But if you really look at the surface, how it moves, it’s a mess.”&lt;/p&gt;
    &lt;p&gt;Still, Stokes was able to conjecture one solution: that it’s possible for the surface of the water to form evenly spaced waves that travel in a single direction.&lt;/p&gt;
    &lt;p&gt;In the 1920s, mathematicians proved Stokes’ conjecture. Furthermore, they found that if there are no external disturbances, these solutions to the Euler equations persist forever: Once they form, so-called Stokes waves will continue cruising gaily along the water’s surface for all time, their form unchanged.&lt;/p&gt;
    &lt;p&gt;But what if the wake of a passing boat crosses the waves’ path? Will the waves absorb this disturbance and maintain their form, or will they be disrupted permanently, transforming into an entirely different pattern of waves?&lt;/p&gt;
    &lt;p&gt;For decades, mathematicians assumed that Stokes waves are stable, meaning that any small distortion will have a minimal effect. After all, the real world is full of such complications, yet the seas are rife with Stokes waves. If they fell apart at the tiniest poke, they’d never survive long enough to make it to shore.&lt;/p&gt;
    &lt;p&gt;Still, in 1967, the mathematician T. Brooke Benjamin decided to verify this basic assumption. He had his student Jim Feir perform a series of experiments in a wave tank — a narrow rectangular pool with an oscillating rudder at one end that could produce Stokes waves. But Feir couldn’t get the waves to reach the other end of the pool. At first, he thought there was a problem with the experimental setup. But soon it became apparent that the waves were, surprisingly, unstable.&lt;/p&gt;
    &lt;p&gt;In 1995, mathematicians finally proved that such “Benjamin-Feir instabilities” are an inevitable consequence of the Euler equations. But the work left researchers wondering about the nature of these instabilities. Which kinds of disturbances can kill waves, and which can’t? How rapidly do the instabilities balloon? Could a gust of wind at the center of the Pacific cause a train of waves to strike Malibu Beach weeks later, or would the formation break down before reaching the shore?&lt;/p&gt;
    &lt;head rend="h2"&gt;Strange Archipelagos&lt;/head&gt;
    &lt;p&gt;Maspero had never thought to wonder why the waves exiting Trieste’s bay were dying. His inspiration ultimately came from a computer, not the scene outside his window.&lt;/p&gt;
    &lt;p&gt;At a 2019 workshop on the mathematics of waves, he and his collaborators met Bernard Deconinck, an applied mathematician at the University of Washington who, along with Katie Oliveras of Seattle University, had been mapping all the different instabilities that could destroy Stokes waves. A few years earlier, the pair had noticed an astonishing pattern, and they hadn’t been able to stop thinking about it.&lt;/p&gt;
    &lt;p&gt;When a perfect train of Stokes waves encounters a disturbance that distorts the waves’ shape, sometimes the effects of the disturbance grow to destroy the entire train, and sometimes they barely interfere. The outcome depends on the frequency of the disturbance — how much it oscillates compared to the length of the original wave. A kayak, which produces a wake that consists of short, frequent oscillations, will deliver a higher-frequency impact than a massive ocean liner, which produces longer and slower oscillations.&lt;/p&gt;
    &lt;p&gt;In general, mathematicians expect waves to recover more easily from higher-frequency disruptions like the kayak’s, because their impacts are limited to a smaller region of a passing wave at any given moment. The wake of the ocean liner, on the other hand, can affect the entire wave at once, permanently disrupting it. Benjamin-Feir instabilities are caused by low-frequency disruptions.&lt;/p&gt;
    &lt;p&gt;In 2011, Deconinck and Oliveras simulated different disturbances with higher and higher frequencies and watched what happened to the Stokes waves. As they expected, for disturbances above a certain frequency, the waves persevered.&lt;/p&gt;
    &lt;p&gt;But as the pair continued to dial up the frequency, they suddenly began to see destruction again. At first, Oliveras worried that there was a bug in the computer program. “Part of me was like, this can’t be right,” she said. “But the more I dug, the more it persisted.”&lt;/p&gt;
    &lt;p&gt;In fact, as the frequency of the disturbance increased, an alternating pattern emerged. First there was an interval of frequencies where the waves became unstable. This was followed by an interval of stability, which was followed by yet another interval of instability, and so on.&lt;/p&gt;
    &lt;p&gt;Deconinck and Oliveras published their finding as a counterintuitive conjecture: that this archipelago of instabilities stretches off to infinity. They called all the unstable intervals “isole” — the Italian word for “islands.”&lt;/p&gt;
    &lt;p&gt;It was strange. The pair had no explanation for why instabilities would appear again, let alone infinitely many times. They at least wanted a proof that their startling observation was correct.&lt;/p&gt;
    &lt;p&gt;For years, no one could make any progress. Then, at the 2019 workshop, Deconinck approached Maspero and his team. He knew they had a lot of experience studying the math of wavelike phenomena in quantum physics. Perhaps they could figure out a way to prove that these striking patterns arise from the Euler equations.&lt;/p&gt;
    &lt;p&gt;The Italian group got to work immediately. They started with the lowest set of frequencies that seemed to cause waves to die. First, they applied techniques from physics to represent each of these low-frequency instabilities as arrays, or matrices, of 16 numbers. These numbers encoded how the instability would grow and distort the Stokes waves over time. The mathematicians realized that if one of the numbers in the matrix was always zero, the instability would not grow, and the waves would live on. If the number was positive, the instability would grow and eventually destroy the waves.&lt;/p&gt;
    &lt;p&gt;To show that this number was positive for the first batch of instabilities, the mathematicians had to compute a gigantic sum. It took 45 pages and nearly a year of work to solve it. Once they’d done so, they turned their attention to the infinitely many intervals of higher-frequency wave-killing disturbances — the isole.&lt;/p&gt;
    &lt;p&gt;First, they figured out a general formula — another complicated sum — that would give them the number they needed for each isola. Then they used a computer program to solve the formula for the first 21 isole. (After that, the calculations got too complicated for the computer to handle.) The numbers were all positive, as expected — and they also seemed to follow a simple pattern that implied they would be positive for all the other isole as well.&lt;/p&gt;
    &lt;p&gt;But a pattern isn’t a proof, and Maspero and his colleagues weren’t sure how to proceed. So they turned to a global community of computer experts for help.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Levee Breaks&lt;/head&gt;
    &lt;p&gt;Maspero had been scouring the mathematical literature for anything that could help him. The problem, he decided, was that he needed to somehow simplify the calculations he had to make. He found a book in which Doron Zeilberger, a mathematician at Rutgers University, outlined algorithmic approaches to performing difficult algebraic calculations on a computer. Unable to adapt them to his case, Maspero reached out to Zeilberger directly.&lt;/p&gt;
    &lt;p&gt;“We have recently encountered certain combinatorial problems that we cannot solve,” his email to Zeilberger began. “We wonder if you can help us.”&lt;/p&gt;
    &lt;p&gt;Zeilberger was intrigued. “The question was exactly my cup of tea,” he said. With some work, he was able to get his computer, which he calls Shalosh B. Ekhad (and which appears as a co-author on all his papers), to compute sums for the first 2,000 isole, verifying that the outputs were all positive and that they conformed to the pattern the Italian team had identified. Then he called on his network of computer-algebra enthusiasts to help, offering to make a $100 donation to the On-Line Encyclopedia of Integer Sequences in the name of whoever could establish that the pattern persisted forever.&lt;/p&gt;
    &lt;p&gt;In February 2024, Zeilberger paid up. After a lengthy email exchange with two of his frequent collaborators, he came back with a complete proof that the sums would never equal zero.&lt;/p&gt;
    &lt;p&gt;Deconinck and Oliveras had been right: Their isole were real. The result means that mathematicians now finally know precisely which types of disturbances will kill a Stokes wave and which will not — something they have hoped to understand for two centuries.&lt;/p&gt;
    &lt;p&gt;“It’s just like, holy crap, thank you,” Oliveras said.&lt;/p&gt;
    &lt;p&gt;It also leaves mathematicians with more work to do. Why do waves live and die in this alternating pattern? “OK, those isole were real,” she said. “Now we have to pay attention to them.”&lt;/p&gt;
    &lt;p&gt;The result is just the latest in a recent spate of papers that aim to illuminate the mathematics of water waves. Mathematicians are combining advances in computational and theoretical techniques to better understand solutions to the Euler equations, allowing them to prove more and more conjectures about how waves behave. Maspero and his colleagues hope that their methods can now be used to solve other problems in this area.&lt;/p&gt;
    &lt;p&gt;As for the bora-blown waves outside Maspero’s office window, and their eventual decline into flat water — at the moment, he can’t say for sure whether his team’s math explains this precise phenomenon. “I don’t know if there is a connection,” he said. “But I love to think it’s the same instabilities.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45602197</guid><pubDate>Thu, 16 Oct 2025 06:54:23 +0000</pubDate></item><item><title>Liquibase continues to advertise itself as "open source" despite license switch</title><link>https://github.com/liquibase/liquibase/issues/7374</link><description>&lt;doc fingerprint="ad5d765d4b0017b6"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 1.9k&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Open&lt;/p&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Search first&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I searched and no similar issues were found&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Description&lt;/head&gt;
    &lt;p&gt;Liquibase has migrated to the Functional Source License, which is not an open source license, as Liquibase itself acknowledges. But this repository continues to misleadingly characterize Liquibase as an open source project, particularly in various places in the file README.md.&lt;/p&gt;
    &lt;head rend="h3"&gt;Steps To Reproduce&lt;/head&gt;
    &lt;p&gt;View github.com/liquibase/liquibase, or the contents of the README.md file, to find out about the liquibase community project.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expected/Desired Behavior&lt;/head&gt;
    &lt;p&gt;The README.md file and any similar project documentation will no longer misleadingly suggest that liquibase is still an open source project.&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquibase Version&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Database Vendor &amp;amp; Version&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquibase Integration&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquibase Extensions&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;OS and/or Infrastructure Type/Provider&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional Context&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Are you willing to submit a PR?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I'm willing to submit a PR (Thank you!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;gedejong, ssddanbrown, nucatus, whyman10x, jalberto and 138 morejsimnzGregOriol, sd416, oguz-ismail, sklyar, wizardist and 3 morenikolaydubina, boxerab, vinylen, daltondiaz, Faroukhamadi and 1 more&lt;/p&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h3"&gt;Assignees&lt;/head&gt;
    &lt;head rend="h3"&gt;Labels&lt;/head&gt;
    &lt;p&gt;No labels&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45602676</guid><pubDate>Thu, 16 Oct 2025 08:02:52 +0000</pubDate></item><item><title>Jiga (YC W21) Is Hiring Full Stacks</title><link>https://www.workatastartup.com/jobs/44310</link><description>&lt;doc fingerprint="335ec57ff13aea3c"&gt;
  &lt;main&gt;
    &lt;p&gt;Jiga transforms the traditional way manufacturers do business.&lt;/p&gt;
    &lt;p&gt;We're building a digital platform that streamlines the complex, inefficient process of procuring manufactured parts directly from suppliers, making it automated, collaborative, and data-driven.&lt;/p&gt;
    &lt;p&gt;Now that I’ve got your attention with some AI cringe, read the job posting carefully before applying.&lt;/p&gt;
    &lt;p&gt;About Jiga:&lt;/p&gt;
    &lt;p&gt;We are on a mission to help engineers build physical products faster. Imagine the &lt;code&gt;npm install&lt;/code&gt; for mechanical engineers.&lt;/p&gt;
    &lt;p&gt;How we work:&lt;/p&gt;
    &lt;p&gt;Remote: We are a fully remote company with team members from over 5 countries.&lt;/p&gt;
    &lt;p&gt;Culture: We never count hours and measure team members by performance and communication only. We hate micro-management and we 100% trust team members to perform tasks and to be honest with each other.&lt;/p&gt;
    &lt;p&gt;We play online games weekly together, encourage people to ask the hard questions, and we fly everyone once per year for our annual offsite in a beautiful place in the nature.&lt;/p&gt;
    &lt;p&gt;Meetings: We have a no-BS-meeting policy. We will have one weekly with the whole company and another one with the dev team. That's it.&lt;/p&gt;
    &lt;p&gt;Engineering Values:&lt;/p&gt;
    &lt;p&gt;Funding: Fully funded with significant, growing revenue. We are transparent about our runway.&lt;/p&gt;
    &lt;p&gt;You should apply if&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;Benefits&lt;/p&gt;
    &lt;p&gt;No, we don’t have Friday happy hours or a fridge packed with 10 different flavors of ice cream in our office. In fact, we don’t have an office.&lt;/p&gt;
    &lt;p&gt;We do offer:&lt;/p&gt;
    &lt;p&gt;How to apply&lt;/p&gt;
    &lt;p&gt;Please send a short blurb about yourself and your favorite ice cream flavor (mine is cookies)&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;fulltimeUS / Remote (US)Full stack$13+ years&lt;/p&gt;
    &lt;p&gt;fulltimeRemoteFull stack3+ years&lt;/p&gt;
    &lt;p&gt;fulltimeIL / Remote (IL)3+ years&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604308</guid><pubDate>Thu, 16 Oct 2025 12:00:45 +0000</pubDate></item><item><title>A stateful browser agent using self-healing DOM maps</title><link>https://100x.bot/a/a-stateful-browser-agent-using-self-healing-dom-maps</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604451</guid><pubDate>Thu, 16 Oct 2025 12:21:36 +0000</pubDate></item><item><title>Hyperflask – Full stack Flask and Htmx framework</title><link>https://hyperflask.dev/</link><description>&lt;doc fingerprint="fa8a766ea56e588d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Backend-driven interactive apps&lt;/head&gt;
    &lt;p&gt;Hyperflask is built on top of Flask, a popular Python web framework. It is easy to use and master. Backend-driven apps ensure straighforward state management and limit a lot of footguns from frontend-heavy apps. Combined with HTMX and a component system, creating interactive apps is easier than ever.&lt;/p&gt;
    &lt;head rend="h2"&gt;A powerful component system&lt;/head&gt;
    &lt;p&gt;Hyperflask introduces component-driven architecture to Flask apps. Seamlessly create frontend (web components, react, etc...) and backend components and use them in your jinja templates. Use HTMX to create server-backed interactive components.&lt;/p&gt;
    &lt;head rend="h2"&gt;File-based routing&lt;/head&gt;
    &lt;p&gt;Hyperflask extends Flask with many powerful features, notably file-based routing using a new file format that combines python code and a jinja template (inspired by Astro pages).&lt;/p&gt;
    &lt;head rend="h2"&gt;Build beautiful UIs&lt;/head&gt;
    &lt;p&gt;Hyperflask includes beautiful components provided by daisyUI and icons by Bootstrap Icons. Use Tailwind to customize styling. Create beautiful UIs in minutes without any CSS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Batteries included&lt;/head&gt;
    &lt;p&gt; Send emails using MJML, run background jobs, send push events using SSE, translations, authentication, content streaming, optimized images, ... &lt;lb/&gt;Everything you need to build a product ! &lt;/p&gt;
    &lt;head rend="h2"&gt;Content driven when needed&lt;/head&gt;
    &lt;p&gt;Hyperflask can be used to generate static web sites. It can also run in an hybrid mode where the server is accessed only for dynamic requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;No messing around with your environment&lt;/head&gt;
    &lt;p&gt;Dev and prod environment are standardized on containers. With a tight integration with VS Code, everything is easy to setup and run. Easily deploy to VPS and various cloud services.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ensuring a thriving ecosystem&lt;/head&gt;
    &lt;p&gt;The Hyperflask framework itself is a small code base. It combines many Flask extensions in a seamless manner. All extensions and related projects are developed independently of the framework under the Hyperflask organization. Feel free to pick and choose the part you prefer from Hyperflask and use them in your own projects.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604673</guid><pubDate>Thu, 16 Oct 2025 12:46:28 +0000</pubDate></item><item><title>Launch HN: Inkeep (YC W23) – Collaborative agent builder for devs and non-devs</title><link>https://github.com/inkeep/agents</link><description>&lt;doc fingerprint="85708f8bb901b03a"&gt;
  &lt;main&gt;
    &lt;p&gt;With Inkeep, you can build and ship AI Agents with a No-Code Visual Builder or TypeScript SDK. Agents can be edited in code or no-code with full 2-way sync, so technical and non-technical teams can create and manage their Agents in a single platform.&lt;/p&gt;
    &lt;p&gt;Inkeep Agents can operate as real-time AI Chat Assistants, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a customer experience agent for customer support, technical docs, or in-app product copilot&lt;/item&gt;
      &lt;item&gt;an internal copilot to assist your support, sales, marketing, ops, and other teams&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Agents can also be used for AI workflow automation like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating and updating knowledge bases, documentation, and blogs&lt;/item&gt;
      &lt;item&gt;Updating CRMs, triaging helpdesk tickets, and streamlining repetitive tasks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To get started, see the docs.&lt;/p&gt;
    &lt;p&gt;A no-code drag-and-drop canvas designed to allow any team to create and manage teams of Agents visually.&lt;/p&gt;
    &lt;p&gt;A code-first approach for building and managing multi-agent systems. Engineering teams to build with the tools and developer experience they expect.&lt;/p&gt;
    &lt;code&gt;import { agent, subAgent } from "@inkeep/agents-sdk";

const helloAgent = subAgent({
  id: "hello-agent",
  name: "Hello Agent",
  description: "Says hello",
  prompt: 'Only reply with the word "hello", but you may do it in different variations like h3110, h3110w0rld, h3110w0rld! etc...',
});

export const basicAgent = agent({
  id: "basic-agent",
  name: "Basic Agent",
  description: "A basic agent",
  defaultSubAgent: helloAgent,
  subAgents: () =&amp;gt; [helloAgent],
});&lt;/code&gt;
    &lt;p&gt;The Visual Builder and TypeScript SDK are fully interoperable: your technical and non-technical teams can edit and manage Agents in either format and switch or collaborate with others at any time.&lt;/p&gt;
    &lt;p&gt;Inkeep Open Source includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Visual Builder &amp;amp; TypeScript SDK with 2-way sync&lt;/item&gt;
      &lt;item&gt;Multi-agent architecture to support teams of agents&lt;/item&gt;
      &lt;item&gt;MCP Tools with credentials management&lt;/item&gt;
      &lt;item&gt;A UI component library for dynamic chat experiences&lt;/item&gt;
      &lt;item&gt;Triggering Agents via MCP, A2A, &amp;amp; Vercel SDK APIs&lt;/item&gt;
      &lt;item&gt;Observability via a Traces UI &amp;amp; OpenTelemetry&lt;/item&gt;
      &lt;item&gt;Easy deployment to Vercel and using Docker&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a full overview, see the Concepts guide.&lt;/p&gt;
    &lt;p&gt;The Inkeep Agent Platform is composed of several key services and libraries that work together:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;agents-manage-api: An API that handles configuration of Agents, Sub Agents, MCP Servers, Credentials, and Projects with a REST API.&lt;/item&gt;
      &lt;item&gt;agents-manage-ui: Visual Builder web interface for creating and managing Agents. Writes to the &lt;code&gt;agents-manage-api&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;agents-sdk: TypeScript SDK (&lt;code&gt;@inkeep/agents-sdk&lt;/code&gt;) for declaratively defining Agents and custom tools in code. Writes to&lt;code&gt;agents-manage-api&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;agents-cli: Includes various handy utilities, including &lt;code&gt;inkeep push&lt;/code&gt;and&lt;code&gt;inkeep pull&lt;/code&gt;which sync your TypeScript SDK code with the Visual Builder.&lt;/item&gt;
      &lt;item&gt;agents-run-api: The Runtime API that exposes Agents as APIs and executes Agent conversations. Keeps conversation state and emits OTEL traces.&lt;/item&gt;
      &lt;item&gt;agents-ui: A UI component library of chat interfaces for embedding rich, dynamic Agent conversational experiences in web apps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Underneath the hood, the framework uses the Vercel AI SDK for interfacing with LLM providers. The &lt;code&gt;agents-sdk&lt;/code&gt;/ &lt;code&gt;agents-manage-api&lt;/code&gt; share many concepts with Vercel's &lt;code&gt;ai&lt;/code&gt; SDK, and &lt;code&gt;agents-run-api&lt;/code&gt; outputs a data stream compatible with Vercel's &lt;code&gt;useChat&lt;/code&gt; and AI Elements primitives for custom UIs.&lt;/p&gt;
    &lt;p&gt;The Inkeep Agent Framework is licensed under the Elastic License 2.0 (ELv2) subject to Inkeep's Supplemental Terms (SUPPLEMENTAL_TERMS.md). This is a fair-code, source-available license that allows broad usage while protecting against certain competitive uses.&lt;/p&gt;
    &lt;p&gt;Inkeep is designed to be extensible and open: you can use the LLM provider of your choice, use Agents via open protocols, and easily deploy and self-host Agents in your own infra.&lt;/p&gt;
    &lt;p&gt;If you'd like to contribute, follow our contribution guide.&lt;/p&gt;
    &lt;p&gt;Follow us to stay up to date, get help, and share feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604700</guid><pubDate>Thu, 16 Oct 2025 12:50:08 +0000</pubDate></item><item><title>Electricity can heal wounds three times as fast (2023)</title><link>https://www.chalmers.se/en/current/news/mc2-how-electricity-can-heal-wounds-three-times-as-fast/</link><description>&lt;doc fingerprint="fedd408029bd4e42"&gt;
  &lt;main&gt;
    &lt;p&gt;Chronic wounds are a major health problem for diabetic patients and the elderly – in extreme cases they can even lead to amputation. Using electric stimulation, researchers in a project at Chalmers University of Technology, Sweden, and the University of Freiburg, Germany, have developed a method that speeds up the healing process, making wounds heal three times faster.&lt;/p&gt;
    &lt;p&gt;There is an old Swedish saying that one should never neglect a small wound or a friend in need. For most people, a small wound does not lead to any serious complications, but many common diagnoses make wound healing far more difficult. People with diabetes, spinal injuries or poor blood circulation have impaired wound healing ability. This means a greater risk of infection and chronic wounds – which in the long run can lead to such serious consequences as amputation.&lt;/p&gt;
    &lt;p&gt;Now a group of researchers at Chalmers and the University of Freiburg have developed a method using electric stimulation to speed up the healing process.&lt;/p&gt;
    &lt;p&gt;“Chronic wounds are a huge societal problem that we don’t hear a lot about. Our discovery of a method that may heal wounds up to three times faster can be a game changer for diabetic and elderly people, among others, who often suffer greatly from wounds that won’t heal,” says Maria Asplund, Professor of Bioelectronics at Chalmers University of Technology and head of research on the project.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Electric guidance of cells for faster healing&lt;/head&gt;
    &lt;p&gt;The researchers worked from an old hypothesis that electric stimulation of damaged skin can be used to heal wounds. The idea is that skin cells are electrotactic, which means that they directionally ‘migrate’ in electric fields. This means that if an electric field is placed in a petri dish with skin cells, the cells stop moving randomly and start moving in the same direction. The researchers investigated how this principle can be used to electrically guide the cells in order to make wounds heal faster. Using a tiny engineered chip, the researchers were able to compare wound healing in artificial skin, stimulating one wound with electricity and letting one heal without electricity. The differences were striking.&lt;/p&gt;
    &lt;p&gt;“We were able to show that the old hypothesis about electric stimulation can be used to make wounds heal significantly faster. In order to study exactly how this works for wounds, we developed a kind of biochip on which we cultured skin cells, which we then made tiny wounds in. Then we stimulated one wound with an electric field, which clearly led to it healing three times as fast as the wound that healed without electric stimulation,” Asplund says.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hope for diabetes patients&lt;/head&gt;
    &lt;p&gt;In the study, the researchers also focused on wound healing in connection with diabetes, a growing health problem worldwide. One in 11 adults today has some form of diabetes according to the World Health Organization (WHO) and the International Diabetes Federation.&lt;/p&gt;
    &lt;p&gt;“We’ve looked at diabetes models of wounds and investigated whether our method could be effective even in those cases. We saw that when we mimic diabetes in the cells, the wounds on the chip heal very slowly. However, with electric stimulation we can increase the speed of healing so that the diabetes-affected cells almost correspond to healthy skin cells,” Asplund says.&lt;/p&gt;
    &lt;head rend="h3"&gt;Individualised treatment the next step&lt;/head&gt;
    &lt;p&gt;The Chalmers researchers recently received a large grant which will allow them to continue their research in the field, and in the long run enable the development of wound healing products for consumers on the market. Similar products have come out before, but more basic research is required to develop effective products that generate enough electric field strength and stimulate in the right way for each individual. This is where Asplund and her colleagues come into the picture:&lt;/p&gt;
    &lt;p&gt;“We are now looking at how different skin cells interact during stimulation, to take a step closer to a realistic wound. We want to develop a concept to be able to ‘scan’ wounds and adapt the stimulation based on the individual wound. We are convinced that this is the key to effectively helping individuals with slow-healing wounds in the future,” Asplund says.&lt;/p&gt;
    &lt;head rend="h3"&gt;More about the study:&lt;/head&gt;
    &lt;p&gt;• “Bioelectronic microfluidic wound healing: a platform for investigating direct current stimulation of injured cell collectives” was published in the journal Lab on a Chip. The article was written by Sebastian Shaner, Anna Savelyeva, Anja Kvartuh, Nicole Jedrusik, Lukas Matter, José Leal and Maria Asplund. The researchers work at the University of Freiburg in Germany and Chalmers University of Technology. &lt;lb/&gt;• In their study, the researchers showed that wound healing on artificial skin stimulated with electric current was three times faster than on the skin that healed naturally. The electric field was low, about 200 mV/mm, and did not have a negative impact on the cells. &lt;lb/&gt;• The method the researchers developed is based on a microfluidic biochip on which artificial skin can be grown, stimulated with an electric current and studied in an effective and controlled manner. The concept allows researchers to conduct multiple experiments in parallel on the same chip. &lt;lb/&gt;• The research project began in 2018 and is funded by the European Research Council (ERC). The project was recently granted new funding so the research can get to market and benefit patients.&lt;/p&gt;
    &lt;head rend="h3"&gt;For more information, please contact:&lt;/head&gt;
    &lt;p&gt;Maria Asplund, Professor of Bioelectronics, Department of Microtechnology and Nanoscience at Chalmers University of Technology, Sweden&lt;lb/&gt;maria.asplund@chalmers.se, +46 31 772 41 14&lt;/p&gt;
    &lt;p&gt;Sebastian Shaner, PhD Candidate, Department of Microsystems Engineering at the University of Freiburg, Germany&lt;lb/&gt;sebastian.shaner@blbt.uni-freiburg.de&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Professor, Electronics Material and Systems, Microtechnology and Nanoscience&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604779</guid><pubDate>Thu, 16 Oct 2025 12:59:08 +0000</pubDate></item><item><title>Lace: A New Kind of Cellular Automata Where Links Matter</title><link>https://www.novaspivack.com/science/introducing-lace-a-new-kind-of-cellular-automata</link><description>&lt;doc fingerprint="eac5901eba575779"&gt;
  &lt;main&gt;
    &lt;p&gt;This article is about a new kind of simple computational rule (“LACE rules” running on LACE, the Link Automata Computing Engine platform) which, when applied locally on a grid of cells, demonstrates fascinating emergent “artificial life” behavior. &lt;lb/&gt;For readers familiar with the Game of Life (GOL), this is a next-level class of cellular automata that utilizes neighborhood topology — the state of the grid is a function of both cell states and their connectivity (links).&lt;lb/&gt;To quickly get a sense of what LACE does – scroll down in this post to the video Gallery section and take a look at (a) the Game of Life, with links, and then as a comparison, check out (b) the “Amazing Dragons” LACE rule, and the many other “Realm of LACE” (ROL) rules that fully utilize topology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preamble&lt;/head&gt;
    &lt;p&gt;I have been thinking about cellular automata ever since I read a fascinating and important book, Three Scientists and Their Gods, during the summer of my sophomore year of college. This book changed my life. &lt;lb/&gt;After reading this book ferociously on the drive home from Oberlin college to Boston, for summer vacation, I became obsessed with the idea of digital physics – and particularly with the work of Ed Fredkin, (and later Stephen Wolfram as well). &lt;/p&gt;
    &lt;p&gt;And so I decided I had to find a way to get into Ed Fredkin’s lab at MIT. This became my mission in life&lt;/p&gt;
    &lt;p&gt;Finally, after many attempts, I did managed to finagle a summer internship in his lab at MIT, thanks to the kindness of Professors Norman Margolus and Tommaso Toffolli, two of the great minds in the field. They had authored an MIT Press book that I had found and read countless times, Cellular Automata Machines, which at the time was the bible in the field.&lt;/p&gt;
    &lt;p&gt;In their lab they had built a specialized supercomputer, CAM-6, for digital physics simulations. I spent every minute of that summer in their lab on CAM-6, exploring the computational universe. &lt;lb/&gt;For years after that internship, I filled dozens of journals with detailed ideas and theories about digital physics. I wrote code, I tested ideas, I learned and explored. But nothing really came close to the richness and complexity of the vision I had experienced, or our actual natural world. &lt;lb/&gt;These digital models were lacking something, but I wasn’t sure what. &lt;lb/&gt;Then, years later, it became more clear. &lt;lb/&gt;The topology of spacetime is not merely a fixed set of locations and states, it’s a graph. The links between things are important. (Note: As Stephen Wolfram would later point out to me, this idea harkens back to the dialectical debate between the views of Newton and Leibniz).&lt;/p&gt;
    &lt;p&gt;I needed a new kind of model, one where the shape of space could evolve, one where the shape of space could affect matter, and where matter could affect the shape of space, because after all, according to Einstein, they are really the same thing.&lt;/p&gt;
    &lt;p&gt;And from this insight I then undertook decades of exploration into the idea of a new kind of cellular automata — one in which the cells had links to each other, which could also figure into their states. Both the cell states and link states could interact, forming complex topologies and geometries. &lt;lb/&gt;Decades went by, along with many experiments that did not bear fruit. &lt;lb/&gt;But last year, I actually built something that demonstrates this new class of CA rather nicely. &lt;lb/&gt;Now I’m finally getting around to posting it…&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing LACE: The Link Automata Computing Engine&lt;/head&gt;
    &lt;p&gt;LACE is a new experimental platform written in python, for exploring a class of cellular automata in which both the states of cells and their connections (links) to each other are subject to the rules.&lt;lb/&gt;In LACE, the state of a cell is a function of both its neighbors AND the links it has to them, and link states in turn are a function of the cells they connect.&lt;lb/&gt;LACE rules can use topological properties of cells and neighborhoods, such as number of connections, neighbor degree, and other metrics.&lt;/p&gt;
    &lt;p&gt;This enables rules in which virtual neighborhood topology (the links) affects neighborhood states — rules in which topology can both evolve and shape the behavior of the system, and in which the behavior of the system can shape topology.&lt;lb/&gt;The added topological dimension enables rules that can have more interesting behavior than traditional “cells-only” CA rules, opening up a fascinating new computational vista (within Wolfram’s, Ruliad) of new kinds of rules that generate new species of stable patterns – oscillators – gliders, puffers, and more.&lt;/p&gt;
    &lt;p&gt;LACE rules range from link-aware variants of Conway’s famous Game of Life, but where edges have varying degrees of influence, to completely new kinds of “Realm of LACE” rules that use topological metrics in their computations. In theory, these rules could even be utilized to simulate neural networks.&lt;/p&gt;
    &lt;p&gt;What is fascinating about these new link-aware “Realm of LACE rules” is that they exhibit amazing new forms of stability and periodic structure. They produce new kinds of periodic gliders, oscillators, puffers, and other kinds of phenomena. Some of them even have behaviors that resemble forms of “artificial life.”&lt;lb/&gt;For more details on how these rules work, get the repo and open various rules in the rule editor, where all their parameters are explained. There are many new classes of rules to experiment with.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Code&lt;/head&gt;
    &lt;p&gt;You can read more about LACE here.&lt;/p&gt;
    &lt;p&gt;You can get the LACE python code repo here.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Gallery&lt;/head&gt;
    &lt;p&gt;(NOTE – click on the videos and use the settings icon to set quality to 1080p for best visuals)&lt;/p&gt;
    &lt;p&gt;First, let’s take a look at Conway’s Game of Life, but with links visible. This demonstration simply shows the links and their states, but in this example, the links do not affect the states of cells. This shows that LACE can run traditional 2D cellular automata rules, with the links visible, but without the links changing rule behavior.&lt;lb/&gt;In more sophisticated LACE rules (scroll down), we will actually modify the behavior of the Life rule based on the links&lt;/p&gt;
    &lt;p&gt;And here are some Life gliders, doing their thing, with links visible…&lt;/p&gt;
    &lt;p&gt;And here is a stable pattern in Life… with links….&lt;/p&gt;
    &lt;p&gt;Now, here is a version of Life, where edges influence rule behavior…&lt;/p&gt;
    &lt;p&gt;Here’s a variant of Life where edges play even more of a role. &lt;lb/&gt;In LACE we can adjust the influence that links have on rule behavior – from none (links are merely decorative), to strong influence (links condition cell states).&lt;/p&gt;
    &lt;p&gt;Above, we looked at some traditional Life rule cellular automata. These examples illustrated varying levels of link-awareness in rules. But they are still not showing the full capabilities of the LACE platform.&lt;lb/&gt;Next, we turn to more advanced LACE rules that fully utilize the capabilities of the system…&lt;/p&gt;
    &lt;head rend="h2"&gt;The Realm of LACE: LACE Rules&lt;/head&gt;
    &lt;p&gt;Now, take a look at the “Amazing Dragons” rule in the Realm of LACE (ROL), one of the more interesting rules in this universe.&lt;/p&gt;
    &lt;p&gt;This is not a life rule – it’s a LACE rule – a fully topological rule.&lt;/p&gt;
    &lt;p&gt;Here links and neighborhood topology play a major role in the behavior of cell states and cell states modify the states of links as well.&lt;lb/&gt;CORE PARAMETERS&lt;/p&gt;
    &lt;p&gt;——————————————————————————–&lt;/p&gt;
    &lt;p&gt;Neighborhood Type MOORE&lt;/p&gt;
    &lt;p&gt;RULE LOGIC&lt;/p&gt;
    &lt;p&gt;——————————————————————————–&lt;/p&gt;
    &lt;p&gt;Birth Eligibility Range Betweenness Sum [(1.4, 7.0)]&lt;/p&gt;
    &lt;p&gt;Birth Metric Aggregation: SUM&lt;/p&gt;
    &lt;p&gt;Birth Metric Type: BETWEENNESS&lt;/p&gt;
    &lt;p&gt;Final Check Metric: DEGREE&lt;/p&gt;
    &lt;p&gt;Final Death Metric Values Degree: [1, 7]&lt;/p&gt;
    &lt;p&gt;Final Life Metric Values Degree: [2, 4, 8]&lt;/p&gt;
    &lt;p&gt;Survival Eligibility Range Betweenness Sum [(0.9, 2.6)]&lt;/p&gt;
    &lt;p&gt;Survival Metric Aggregation: SUM&lt;/p&gt;
    &lt;p&gt; Survival Metric Type: BETWEENNESS&lt;/p&gt;
    &lt;p&gt;…and look what happens!&lt;/p&gt;
    &lt;p&gt;LACE also supports an optional high-performance, GPU-accelerated mode using Taichi, for large-scale simulations…&lt;/p&gt;
    &lt;p&gt;And here’s another interesting LACE rule…&lt;/p&gt;
    &lt;p&gt;and check this out…&lt;/p&gt;
    &lt;p&gt;and this one….&lt;/p&gt;
    &lt;p&gt;And now a gallery of many other interesting LACE rules…&lt;/p&gt;
    &lt;p&gt;This has been a preview of the Realm of LACE … an incredible new class of cellular automata rules, where links matter and topology evolves.&lt;lb/&gt;Learn more by playing with the repo, and please share your discoveries 🙂&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45605153</guid><pubDate>Thu, 16 Oct 2025 13:33:45 +0000</pubDate></item><item><title>Why I Chose Elixir Phoenix over Rails, Laravel, and Next.js</title><link>https://akarshc.com/post/phoenix-for-my-project.html</link><description>&lt;doc fingerprint="b7d4d06b17b5be94"&gt;
  &lt;main&gt;
    &lt;p&gt;October 16, 2025 · by Akarsh&lt;/p&gt;
    &lt;p&gt;First things first, why do we code? To solve problems in the most optimal way possible.&lt;/p&gt;
    &lt;p&gt;For me, the number one factor is speed: both application speed and development speed. That’s exactly what led me to Phoenix LiveView.&lt;/p&gt;
    &lt;p&gt;If I had chosen React or Next.js with Laravel, or even Inertia.js with Laravel, I would have had to maintain both sides of the stack, frontend and backend. As a solo developer, I didn’t have the time to manage state in two different places. I needed a solid monolithic solution that could handle everything together.&lt;/p&gt;
    &lt;p&gt;So I looked into Laravel Livewire and Rails Hotwire. Both are great tools that simplify frontend work without depending too much on JavaScript. I even thought about going full JavaScript with Next.js, but I’ve never been a big fan of using JS on the backend.&lt;/p&gt;
    &lt;p&gt;Rails Hotwire really caught my attention, especially because of how fast you can build an MVP with Rails. But I still needed background jobs, real-time updates, and two-way communication that just works. Those things are possible in Rails and Laravel, but they take a bit more effort to set up.&lt;/p&gt;
    &lt;p&gt;Then I came across Elixir and its framework Phoenix. It had all the elegance of Ruby on Rails, but with far better performance. It came with built-in background jobs through Oban, a familiar and clean syntax, and something truly special called LiveView.&lt;/p&gt;
    &lt;p&gt;LiveView feels like the perfect balance between traditional server-rendered apps and frontend-heavy frameworks. It’s way ahead of both Rails Hotwire and Laravel Livewire. LiveView communicates through WebSockets, which means real-time two-way updates without sending new requests every time something changes. You can still use Alpine.js or any JavaScript library you want through hooks when needed.&lt;/p&gt;
    &lt;p&gt;Phoenix also comes with Oban jobs built in. You can declare background jobs easily, and when something fails, it automatically restarts without breaking the app. That’s the beauty of Elixir. It’s a compiled language built on top of Erlang, which powers highly concurrent systems like WhatsApp and Discord.&lt;/p&gt;
    &lt;p&gt;I’m not saying Phoenix is better than Laravel, Rails, or Next.js. All of these are excellent frameworks, and I’ve personally used them to build applications. Phoenix just turned out to be the best fit for my specific use case. This is my project - Hyperzoned.com&lt;/p&gt;
    &lt;p&gt;I don’t know who needs to hear this, but try exploring beyond what you already know. You might find a better and more efficient way to solve your next problem. After all, never stop learning.&lt;/p&gt;
    &lt;p&gt;Thanks for reading! You can find me on X or Hyperzoned.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45605291</guid><pubDate>Thu, 16 Oct 2025 13:48:33 +0000</pubDate></item><item><title>Tor browser removing various Firefox AI features</title><link>https://blog.torproject.org/new-alpha-release-tor-browser-150a4/</link><description>&lt;doc fingerprint="1d0ed8139e94f6de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New Alpha Release: Tor Browser 15.0a4&lt;/head&gt;
    &lt;p&gt;Tor Browser 15.0a4 is now available from the Tor Browser download page and also from our distribution directory.&lt;/p&gt;
    &lt;p&gt;This version includes important security updates to Firefox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Release Candidate&lt;/head&gt;
    &lt;p&gt;If all goes as planned, this will be our last alpha release in the 15.0 series before it is promoted to stable in the last week of October. Next week we will be focusing primarily on QA and ensuring all the various features and scenarios supported in Tor Browser still work as expected. This QA work will be tracked in the following gitlab issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tor-browser#43984 - Tor Browser 15.0 Release QA - Desktop&lt;/item&gt;
      &lt;item&gt;tor-browser#43985 - Tor Browser 15.0 Release QA - Android&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we reach the home stretch, now would be a great time to download and try out Tor Browser Alpha! We would appreciate it if the community would evaluate and exercise these following changes:&lt;/p&gt;
    &lt;head rend="h3"&gt;ð¤ Removal of Various AI Features&lt;/head&gt;
    &lt;p&gt;Over the past year Mozilla has been working on integrating various AI features and integrations into Firefox (e.g. the AI chatbot sidebar). Such machine learning systems and platforms are inherently un-auditable from a security and privacy perspective. We also do not want to imply recommendation or promotion of such systems by including them in Tor Browser. Therefore, we have done what we can to remove such features from the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;âï¸ Rename &lt;code&gt;meek-azure&lt;/code&gt; pluggable-transport to just &lt;code&gt;meek&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;In the past, we have used various cloud platform to host &lt;code&gt;meek&lt;/code&gt; pluggable-transport backends including Google, Amazon, and Azure. However, as time passed these backends have moved and migrated and thus the cloud provider-specific name has become an historical artifact. Therefore, we have dropped the Azure part of the name and now just call it &lt;code&gt;meek&lt;/code&gt;. Let this be a lesson to you about naming things!&lt;/p&gt;
    &lt;head rend="h3"&gt;ðª Improved Dark Theme Support in Browser Chrome&lt;/head&gt;
    &lt;p&gt;We have improved the styling for our various Tor Browser-specific UI components for dark browser themes. All of our various purple elements should now look like they belong.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð¦ Removal/Replacement of New Firefox/Mozilla-specific Branding and Features&lt;/head&gt;
    &lt;p&gt;As part of ordinary incremental UI updates over the past year and the implementation of new features, Mozilla has added various new brand assets and service integrations. This includes things like those cute little fox graphics, Firefox Home, and the new History Sidebar. As of this release, there should not be any more Firefox or Mozilla specific branding, features, or service integrations accessible in Tor Browser. The new history sidebar in particular has been replaced with the legacy history panel from previous Tor Browser versions.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð§ Updated Emoji Font for Linux&lt;/head&gt;
    &lt;p&gt;We have included the Noto Color Emoji font with our Linux builds. Linux users should now have all the latest and greatest emoji provided by Noto Emoji.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð´ï¸ Improved CJK Glyph Rendering&lt;/head&gt;
    &lt;p&gt;At the suggestion of a cypherpunk, we have swapped out the Noto font family for Jigmo. This should allow more Chinese, Japanese, and Korean graphemes to render accurately in web content.&lt;/p&gt;
    &lt;head rend="h3"&gt;âï¸ Letterboxing Styling Improvements&lt;/head&gt;
    &lt;p&gt;We have tweaked our custom styling of the web-content letterboxing feature to confirm with and adapt to Firefox's own styling changes in Firefox 140. These tweaks should also play nicely with upstream's vertical tabs feature.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð« WebAssembly Restrictions Now Managed by NoScript&lt;/head&gt;
    &lt;p&gt;Historically, we have disabled WebAssembly globally when the browser is in the &lt;code&gt;Safer&lt;/code&gt; and &lt;code&gt;Safest&lt;/code&gt; security levels. However, with the latest Firefox version this has proven to be too aggressive, as doing so broke functionality in the built-in PDF reader. We therefore now rely on the NoScript extension built into the browser to handle disabling WebAssembly functionality in web content while the browser is in the &lt;code&gt;Safer&lt;/code&gt; and &lt;code&gt;Safest&lt;/code&gt; security levels, while also allowing WebAssembly to run unhindered in safe+privileged contexts like the PDF reader.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð Stopped Hiding Protocol in URL on Desktop&lt;/head&gt;
    &lt;p&gt;Mozilla has reversed course on when the protocol portion (e.g. &lt;code&gt;http&lt;/code&gt; or &lt;code&gt;https&lt;/code&gt;) of the URL in the URL bar is hidden since Firefox 128. We used to have logic in one of our patches around Onion Services (which are always end-to-end encrypted regardless of the application-level protocol used) to follow whatever Firefox does for &lt;code&gt;https&lt;/code&gt;. However, with the latest changes in Firefox, this patch became a bit gnarly to apply correctly so we took a step back and thought to ourselves, why are we even conditionally hiding this from the user?&lt;/p&gt;
    &lt;p&gt;So for now, we have decided not to hide the protocol from the user on Desktop platforms using a supported Firefox pref. We continue to follow upstream and always hide the protocol in the URL bar on Android (where horizontal space is at a premium). Users of Tor Browser Android can simply click the icon in the URL bar to get all the info about a websites HTTPS usage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Send us your feedback&lt;/head&gt;
    &lt;p&gt;If you find a bug or have a suggestion for how we could improve this release, please let us know.&lt;/p&gt;
    &lt;p&gt;â ï¸ Reminder: Tor Browser Alpha release channel is for testing only. If you are at risk or need strong anonymity, stick with the stable release channel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Full changelog&lt;/head&gt;
    &lt;p&gt;The full changelog since Tor Browser 15.0a3 is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All Platforms&lt;list rend="ul"&gt;&lt;item&gt;Updated NoScript to 13.2.1&lt;/item&gt;&lt;item&gt;Updated OpenSSL to 3.5.4&lt;/item&gt;&lt;item&gt;Bug tor-browser#19741: Opensearch (contextual search) does not obey FPI&lt;/item&gt;&lt;item&gt;Bug tor-browser#43850: Modify the Contrast Control settings for RFP&lt;/item&gt;&lt;item&gt;Bug tor-browser#43869: Hide pens with RFP&lt;/item&gt;&lt;item&gt;Bug tor-browser#44068: Handle migration from meek-azure to meek built-in bridge type&lt;/item&gt;&lt;item&gt;Bug tor-browser#44234: No images in PDF&lt;/item&gt;&lt;item&gt;Bug tor-browser#44240: Typo on dom.security.https_first_add_exception_on_failure&lt;/item&gt;&lt;item&gt;Bug tor-browser#44242: Hand over Security Level's WebAssembly controls to NoScript&lt;/item&gt;&lt;item&gt;Bug tor-browser#44250: Rebase Tor Browser Alpha onto 140.4esr&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41574: Update Snowflake builtin bridge lines&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Windows + macOS + Linux&lt;list rend="ul"&gt;&lt;item&gt;Updated Firefox to 140.4.0esr&lt;/item&gt;&lt;item&gt;Bug tor-browser#43900: Open newtab rather than firefoxview when unloading the last tab&lt;/item&gt;&lt;item&gt;Bug tor-browser#44101: Toolbar connection status is not visible when using vertical tabs&lt;/item&gt;&lt;item&gt;Bug tor-browser#44107: Switch tab search action is missing an icon&lt;/item&gt;&lt;item&gt;Bug tor-browser#44108: Fix the new history sidebar&lt;/item&gt;&lt;item&gt;Bug tor-browser#44123: Do not trim protocol off of URLs ever&lt;/item&gt;&lt;item&gt;Bug tor-browser#44153: Test search engines&lt;/item&gt;&lt;item&gt;Bug tor-browser#44159: Change or hide the sidebar settings description&lt;/item&gt;&lt;item&gt;Bug tor-browser#44177: Remove more urlbar actions&lt;/item&gt;&lt;item&gt;Bug tor-browser#44178: Search preservation does not work with duckduckgo in safest security level&lt;/item&gt;&lt;item&gt;Bug tor-browser#44184: Duckduckgo Onion Lite search does not work properly in safest when added as a search engine&lt;/item&gt;&lt;item&gt;Bug tor-browser#44187: TLS session tickets leak Private Browsing mode&lt;/item&gt;&lt;item&gt;Bug tor-browser#44192: Hovering unloaded tab causes console error&lt;/item&gt;&lt;item&gt;Bug tor-browser#44213: Reduce linkability concerns of the "Search with" contextual search action&lt;/item&gt;&lt;item&gt;Bug tor-browser#44214: Update letterboxing to reflect changes in ESR 140&lt;/item&gt;&lt;item&gt;Bug tor-browser#44215: Hide Firefox home settings in about:preferences&lt;/item&gt;&lt;item&gt;Bug tor-browser#44221: Backport MozBug 1984333 Bump Spoofed Processor Count&lt;/item&gt;&lt;item&gt;Bug tor-browser#44239: DDG HTML page and search results displayed incorrectly with Safest security setting&lt;/item&gt;&lt;item&gt;Bug tor-browser#44262: Disable adding search engines from HTML forms&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Linux&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser#44227: Some CJK characters cannot be rendered by Tor which uses the Noto font family&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41586: Replace Noto CJK with Jigmo on Linux&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Android&lt;list rend="ul"&gt;&lt;item&gt;Updated GeckoView to 140.4.0esr&lt;/item&gt;&lt;item&gt;Bug tor-browser#43401: Replace the constructor of Locale with a builder&lt;/item&gt;&lt;item&gt;Bug tor-browser#43643: Clean out unused tor connect strings&lt;/item&gt;&lt;item&gt;Bug tor-browser#43650: Survey banner behaves like a dialog on Android, rather than a card&lt;/item&gt;&lt;item&gt;Bug tor-browser#43676: Preemptively disable unified trust panel by default so we are tracking for next ESR&lt;/item&gt;&lt;item&gt;Bug tor-browser#44031: Implement YEC 2025 Takeover for Android Stable&lt;/item&gt;&lt;item&gt;Bug tor-browser#44218: Tor Browser Alpha for Android (15.0a2) doesn't work on Huawei devices P20 and P30&lt;/item&gt;&lt;item&gt;Bug tor-browser#44237: Revoke access to all advertising ids available in Android&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Build System&lt;list rend="ul"&gt;&lt;item&gt;All Platforms&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser-build#41568: Update instructions for manually building 7zip&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41576: Build expert bundles outside containers&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41579: Add zip to the list of Tor Browser Build dependencies&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41588: Restore legacy channel support in projects/release/update_responses_config.yml&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41589: Backport tor-browser-build-browser#41270: Add updater rewriterules to make 13.5.7 a watershed&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Windows + macOS + Linux&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser-build#41373: Remove &lt;code&gt;_ALL&lt;/code&gt;from mar filenames&lt;/item&gt;&lt;item&gt;Bug tor-browser#44131: Generate torrc-defaults and put it in objdir post-build&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41373: Remove &lt;/item&gt;&lt;item&gt;Windows + Linux + Android&lt;list rend="ul"&gt;&lt;item&gt;Updated Go to 1.24.9&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Windows&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser#44167: Move the nsis-uninstall.patch to tor-browser repository&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;macOS&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser-build#41571: Work-around to prevent older 7z versions to break rcodesign.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Linux&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser-build#41558: Share descriptions between Linux packages and archives&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41569: Use var/display_name in .desktop files&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Android&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser#44220: Disable the JS minifier as it produces invalid JS&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41577: Minify JS with UglifyJS on Android x86&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41582: Drop --pack-dyn-relocs=relr&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41583: Align tor and PTs to 16kB on Android&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;All Platforms&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45605842</guid><pubDate>Thu, 16 Oct 2025 14:33:10 +0000</pubDate></item><item><title>Improving the Trustworthiness of JavaScript on the Web</title><link>https://blog.cloudflare.com/improving-the-trustworthiness-of-javascript-on-the-web/</link><description>&lt;doc fingerprint="9e39b013996d6ecb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The web is the most powerful application platform in existence. As long as you have the right API, you can safely run anything you want in a browser.&lt;/p&gt;
      &lt;p&gt;Wellâ¦ anything but cryptography.&lt;/p&gt;
      &lt;p&gt;It is as true today as it was in 2011 that Javascript cryptography is Considered Harmful. The main problem is code distribution. Consider an end-to-end-encrypted messaging web application. The application generates cryptographic keys in the clientâs browser that lets users view and send end-to-end encrypted messages to each other. If the application is compromised, what would stop the malicious actor from simply modifying their Javascript to exfiltrate messages?&lt;/p&gt;
      &lt;p&gt;It is interesting to note that smartphone apps donât have this issue. This is because app stores do a lot of heavy lifting to provide security for the app ecosystem. Specifically, they provide integrity, ensuring that apps being delivered are not tampered with, consistency, ensuring all users get the same app, and transparency, ensuring that the record of versions of an app is truthful and publicly visible.&lt;/p&gt;
      &lt;p&gt;It would be nice if we could get these properties for our end-to-end encrypted web application, and the web as a whole, without requiring a single central authority like an app store. Further, such a system would benefit all in-browser uses of cryptography, not just end-to-end-encrypted apps. For example, many web-based confidential LLMs, cryptocurrency wallets, and voting systems use in-browser Javascript cryptography for the last step of their verification chains.&lt;/p&gt;
      &lt;p&gt;In this post, we will provide an early look at such a system, called Web Application Integrity, Consistency, and Transparency (WAICT) that we have helped author. WAICT is a W3C-backed effort among browser vendors, cloud providers, and encrypted communication developers to bring stronger security guarantees to the entire web. We will discuss the problem we need to solve, and build up to a solution resembling the current transparency specification draft. We hope to build even wider consensus on the solution design in the near future.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Defining the Web Application&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;In order to talk about security guarantees of a web application, it is first necessary to define precisely what the application is. A smartphone application is essentially just a zip file. But a website is made up of interlinked assets, including HTML, Javascript, WASM, and CSS, that can each be locally or externally hosted. Further, if any asset changes, it could drastically change the functioning of the application. A coherent definition of an application thus requires the application to commit to precisely the assets it loads. This is done using integrity features, which we describe now.&lt;/p&gt;
      &lt;p&gt;An important building block for defining a single coherent application is subresource integrity (SRI). SRI is a feature built into most browsers that permits a website to specify the cryptographic hash of external resources, e.g.,&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;&amp;lt;script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.13.7/underscore-min.js" integrity="sha512-dvWGkLATSdw5qWb2qozZBRKJ80Omy2YN/aF3wTUVC5+D1eqbA+TjWpPpoj8vorK5xGLMa2ZqIeWCpDZP/+pQGQ=="&amp;gt;&amp;lt;/script&amp;gt;&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This causes the browser to fetch &lt;code&gt;underscore.js&lt;/code&gt; from &lt;code&gt;cdnjs.cloudflare.com&lt;/code&gt; and verify that its SHA-512 hash matches the given hash in the tag. If they match, the script is loaded. If not, an error is thrown and nothing is executed.&lt;/p&gt;
      &lt;p&gt;If every external script, stylesheet, etc. on a page comes with an SRI integrity attribute, then the whole page is defined by just its HTML. This is close to what we want, but a web application can consist of many pages, and there is no way for a page to enforce the hash of the pages it links to.&lt;/p&gt;
      &lt;p&gt;We would like to have a way of enforcing integrity on an entire site, i.e., every asset under a domain. For this, WAICT defines an integrity manifest, a configuration file that websites can provide to clients. One important item in the manifest is the asset hashes dictionary, mapping a hash belonging to an asset that the browser might load from that domain, to the path of that asset. Assets that may occur at any path, e.g., an error page, map to the empty string:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;"hashes": {
"81db308d0df59b74d4a9bd25c546f25ec0fdb15a8d6d530c07a89344ae8eeb02": "/assets/js/main.js",
"fbd1d07879e672fd4557a2fa1bb2e435d88eac072f8903020a18672d5eddfb7c": "/index.html",
"5e737a67c38189a01f73040b06b4a0393b7ea71c86cf73744914bbb0cf0062eb": "/vendored/main.css",
"684ad58287ff2d085927cb1544c7d685ace897b6b25d33e46d2ec46a355b1f0e": "",
"f802517f1b2406e308599ca6f4c02d2ae28bb53ff2a5dbcddb538391cb6ad56a": ""
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;The other main component of the manifest is the integrity policy, which tells the browser which data types are being enforced and how strictly. For example, the policy in the manifest below will:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Reject any script before running it, if itâs missing an SRI tag and doesnât appear in the hashes&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Reject any WASM possibly after running it, if itâs missing an SRI tag and doesnât appear in hashes&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;quote&gt;
        &lt;code&gt;"integrity-policy": "blocked-destinations=(script), checked-destinations=(wasm)"&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Put together, these make up the integrity manifest:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;"manifest": {
  "version": 1,
  "integrity-policy": ...,
  "hashes": ...,
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Thus, when both SRI and integrity manifests are used, the entire site and its interpretation by the browser is uniquely determined by the hash of the integrity manifest. This is exactly what we wanted. We have distilled the problem of endowing authenticity, consistent distribution, etc. to a web application to one of endowing the same properties to a single hash.&lt;/p&gt;
      &lt;p&gt;Recall, a transparent web application is one whose code is stored in a publicly accessible, append-only log. This is helpful in two ways: 1) if a user is served malicious code and they learn about it, there is a public record of the code they ran, and so they can prove it to external parties, and 2) if a user is served malicious code and they donât learn about it, there is still a chance that an external auditor may comb through the historical web application code and find the malicious code anyway. Of course, transparency does not help detect malicious code or even prevent its distribution, but it at least makes it publicly auditable.&lt;/p&gt;
      &lt;p&gt;Now that we have a single hash that commits to an entire websiteâs contents, we can talk about ensuring that that hash ends up in a public log. We have several important requirements here:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Do not break existing sites. This one is a given. Whatever system gets deployed, it should not interfere with the correct functioning of existing websites. Participation in transparency should be strictly opt-in.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;No added round trips. Transparency should not cause extra network round trips between the client and the server. Otherwise there will be a network latency penalty for users who want transparency.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;User privacy. A user should not have to identify themselves to any party more than they already do. That means no connections to new third parties, and no sending identifying information to the website.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;User statelessness. A user should not have to store site-specific data. We do not want solutions that rely on storing or gossipping per-site cryptographic information.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Non-centralization. There should not be a single point of failure in the systemâif any single party experiences downtime, the system should still be able to make progress. Similarly, there should be no single point of trustâif a user distrusts any single party, the user should still receive all the security benefits of the system.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Ease of opt-in. The barrier of entry for transparency should be as low as possible. A site operator should be able to start logging their site cheaply and without being an expert.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Ease of opt-out. It should be easy for a website to stop participating in transparency. Further, to avoid accidental lock-in like the defunct HPKP spec, it should be possible for this to happen even if all cryptographic material is lost, e.g., in the seizure or selling of a domain.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Opt-out is transparent. As described before, because transparency is optional, it is possible for an attacker to disable the siteâs transparency, serve malicious content, then enable transparency again. We must make sure this kind of attack is detectable, i.e., the act of disabling transparency must itself be logged somewhere.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Monitorability. A website operator should be able to efficiently monitor the transparency information being published about their website. In particular, they should not have to run a high-network-load, always-on program just to notify them if their site has been hijacked.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;With these requirements in place, we can move on to construction. We introduce a data structure that will be essential to the design.&lt;/p&gt;
      &lt;p&gt;Almost everything in transparency is an append-only log, i.e., a data structure that acts like a list and has the ability to produce an inclusion proof, i.e., a proof that an element occurs at a particular index in the list; and a consistency proof, i.e., a proof that a list is an extension of a previous version of the list. A consistency proof between two lists demonstrates that no elements were modified or deleted, only added.&lt;/p&gt;
      &lt;p&gt;The simplest possible append-only log is a hash chain, a list-like data structure wherein each subsequent element is hashed into the running chain hash. The final chain hash is a succinct representation of the entire list.&lt;/p&gt;
      &lt;p&gt;A hash chain. The green nodes represent the chain hash, i.e., the hash of the element below it, concatenated with the previous chain hash. &lt;/p&gt;
      &lt;p&gt;The proof structures are quite simple. To prove inclusion of the element at index i, the prover provides the chain hash before &lt;code&gt;i&lt;/code&gt;, and all the elements after &lt;code&gt;i&lt;/code&gt;:&lt;/p&gt;
      &lt;p&gt;Proof of inclusion for the second element in the hash chain. The verifier knows only the final chain hash. It checks equality of the final computed chain hash with the known final chain hash. The light green nodes represent hashes that the verifier computes. &lt;/p&gt;
      &lt;p&gt;Similarly, to prove consistency between the chains of size i and j, the prover provides the elements between i and j:&lt;/p&gt;
      &lt;p&gt;Proof of consistency of the chain of size one and chain of size three. The verifier has the chain hashes from the starting and ending chains. It checks equality of the final computed chain hash with the known ending chain hash. The light green nodes represent hashes that the verifier computes. &lt;/p&gt;
      &lt;p&gt;We can use hash chains to build a transparency scheme for websites.&lt;/p&gt;
      &lt;p&gt;As a first step, letâs give every site its own log, instantiated as a hash chain (we will discuss how these all come together into one big log later). The items of the log are just the manifest of the site at a particular point in time:&lt;/p&gt;
      &lt;p&gt;A siteâs hash chain-based log, containing three historical manifests. &lt;/p&gt;
      &lt;p&gt;In reality, the log does not store the manifest itself, but the manifest hash. Sites designate an asset host that knows how to map hashes to the data they reference. This is a content-addressable storage backend, and can be implemented using strongly cached static hosting solutions.&lt;/p&gt;
      &lt;p&gt;A log on its own is not very trustworthy. Whoever runs the log can add and remove elements at will and then recompute the hash chain. To maintain the append-only-ness of the chain, we designate a trusted third party, called a witness. Given a hash chain consistency proof and a new chain hash, a witness:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Verifies the consistency proof with respect to its old stored chain hash, and the new provided chain hash.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;If successful, signs the new chain hash along with a signature timestamp.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Now, when a user navigates to a website with transparency enabled, the sequence of events is:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;The site serves its manifest, an inclusion proof showing that the manifest appears in the log, and all the signatures from all the witnesses who have validated the log chain hash.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The browser verifies the signatures from whichever witnesses it trusts.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The browser verifies the inclusion proof. The manifest must be the newest entry in the chain (we discuss how to serve old manifests later).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The browser proceeds with the usual manifest and SRI integrity checks.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;At this point, the user knows that the given manifest has been recorded in a log whose chain hash has been saved by a trustworthy witness, so they can be reasonably sure that the manifest wonât be removed from history. Further, assuming the asset host functions correctly, the user knows that a copy of all the received code is readily available.&lt;/p&gt;
      &lt;p&gt;The need to signal transparency. The above algorithm works, but we have a problem: if an attacker takes control of a site, they can simply stop serving transparency information and thus implicitly disable transparency without detection. So we need an explicit mechanism that keeps track of every website that has enrolled into transparency.&lt;/p&gt;
      &lt;p&gt;To store all the sites enrolled into transparency, we want a global data structure that maps a site domain to the site logâs chain hash. One efficient way of representing this is a prefix tree (a.k.a., a trie). Every leaf in the tree corresponds to a siteâs domain, and its value is the chain hash of that siteâs log, the current log size, and the siteâs asset host URL. For a site to prove validity of its transparency data, it will have to present an inclusion proof for its leaf. Fortunately, these proofs are efficient for prefix trees.&lt;/p&gt;
      &lt;p&gt;A prefix tree with four elements. Each leafâs path corresponds to a domain. Each leafâs value is the chain hash of its siteâs log. &lt;/p&gt;
      &lt;p&gt;To add itself to the tree, a site proves possession of its domain to the transparency service, i.e., the party that operates the prefix tree, and provides an asset host URL. To update the entry, the site sends the new entry to the transparency service, which will compute the new chain hash. And to unenroll from transparency, the site just requests to have its entry removed from the tree (an adversary can do this too; we discuss how to detect this below).&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h4"&gt;Proving to Witnesses and Browsers&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Now witnesses only need to look at the prefix tree instead of individual site logs, and thus they must verify whole-tree updates. The most important thing to ensure is that every siteâs log is append-only. So whenever the tree is updated, it must produce a âproofâ containing every new/deleted/modified entry, as well as a consistency proof for each entry showing that the site log corresponding to that entry has been properly appended to. Once the witness has verified this prefix tree update proof, it signs the root.&lt;/p&gt;
      &lt;p&gt;The sequence of updating a siteâs assets and serving the site with transparency enabled.&lt;/p&gt;
      &lt;p&gt;The client-side verification procedure is as in the previous section, with two modifications:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;The client now verifies two inclusion proofs: one for the integrity policyâs membership in the site log, and one for the site logâs membership in a prefix tree.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The client verifies the signature over the prefix tree root, since the witness no longer signs individual chain hashes. As before, the acceptable public keys are whichever witnesses the client trusts.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Signaling transparency. Now that there is a single source of truth, namely the prefix tree, a client can know a site is enrolled in transparency by simply fetching the siteâs entry in the tree. This alone would work, but it violates our requirement of âno added round trips,â so we instead require that client browsers will ship with the list of sites included in the prefix tree. We call this the transparency preload list.Â &lt;/p&gt;
      &lt;p&gt;If a site appears in the preload list, the browser will expect it to provide an inclusion proof in the prefix tree, or else a proof of non-inclusion in a newer version of the prefix tree, thereby showing theyâve unenrolled. The site must provide one of these proofs until the last preload list it appears in has expired. Finally, even though the preload list is derived from the prefix tree, there is nothing enforcing this relationship. Thus, the preload list should also be published transparently.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h4"&gt;Filling in Missing Properties&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Remember we still have the requirements of monitorability, opt-out being transparent, and no single point of failure/trust. We fill in those details now.&lt;/p&gt;
      &lt;p&gt;Adding monitorability. So far, in order for a site operator to ensure their site was not hijacked, they would have to constantly query every transparency service for its domain and verify that it hasnât been tampered with. This is certainly better than the 500k events per hour that CT monitors have to ingest, but it still requires the monitor to be constantly polling the prefix tree, and it imposes a constant load for the transparency service.&lt;/p&gt;
      &lt;p&gt;We add a field to the prefix tree leaf structure: the leaf now stores a âcreatedâ timestamp, containing the time the leaf was created. Witnesses ensure that the âcreatedâ field remains the same over all leaf updates (and it is deleted when the leaf is deleted). To monitor, a site operator need only keep the last observed âcreatedâ and âlog sizeâ fields of its leaf. If it fetches the latest leaf and sees both unchanged, it knows that no changes occurred since the last check.&lt;/p&gt;
      &lt;p&gt;Adding transparency of opt-out. We must also do the same thing as above for leaf deletions. When a leaf is deleted, a monitor should be able to learn when the deletion occurred within some reasonable time frame. Thus, rather than outright removing a leaf, the transparency service responds to unenrollment requests by replacing the leaf with a tombstone value, containing just a âcreatedâ timestamp. As before, witnesses ensure that this field remains unchanged until the leaf is permanently deleted (after some visibility period) or re-enrolled.&lt;/p&gt;
      &lt;p&gt;Permitting multiple transparency services. Since we require that there be no single point of failure or trust, we imagine an ecosystem where there are a handful of non-colluding, reasonably trustworthy transparency service providers, each with their own prefix tree. Like Certificate Transparency (CT), this set should not be too large. It must be small enough that reasonable levels of trust can be established, and so that independent auditors can reasonably handle the load of verifying all of them.&lt;/p&gt;
      &lt;p&gt;Ok thatâs the end of the most technical part of this post. Weâre now going to talk about how to tweak this system to provide all kinds of additional nice properties.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;(Not) Achieving Consistency&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Transparency would be useless if, every time a site updates, it serves 100,000 new versions of itself. Any auditor would have to go through every single version of the code in order to ensure no user was targeted with malware. This is bad even if the velocity of versions is lower. If a site publishes just one new version per week, but every version from the past ten years is still servable, then users can still be served extremely old, potentially vulnerable versions of the site, without anyone knowing. Thus, in order to make transparency valuable, we need consistency, the property that every browser sees the same version of the site at a given time.&lt;/p&gt;
      &lt;p&gt;We will not achieve the strongest version of consistency, but it turns out that weaker notions are sufficient for us. If, unlike the above scenario, a site had 8 valid versions of itself at a given time, then that would be pretty manageable for an auditor. So even though itâs true that users donât all see the same version of the site, they will all still benefit from transparency, as desired.&lt;/p&gt;
      &lt;p&gt;We describe two types of inconsistency and how we mitigate them.&lt;/p&gt;
      &lt;p&gt;Tree inconsistency occurs when transparency servicesâ prefix trees disagree on the chain hash of a site, thus disagreeing on the history of the site. One way to fully eliminate this is to establish a consensus mechanism for prefix trees. A simple one is majority voting: if there are five transparency services, a site must present three tree inclusion proofs to a user, showing the chain hash is present in three trees. This, of course, triples the tree inclusion proof size, and lowers the fault tolerance of the entire system (if three log operators go down, then no transparent site can publish any updates).&lt;/p&gt;
      &lt;p&gt;Instead of consensus, we opt to simply limit the amount of inconsistency by limiting the number of transparency services. In 2025, Chrome trusts eight Certificate Transparency logs. A similar number of transparency services would be fine for our system. Plus, it is still possible to detect and prove the existence of inconsistencies between trees, since roots are signed by witnesses. So if it becomes the norm to use the same version on all trees, then social pressure can be applied when sites violate this.&lt;/p&gt;
      &lt;p&gt;Temporal inconsistency occurs when a user gets a newer or older version of the site (both still unexpired), depending on some external factors such as geographic location or cookie values. In the extreme, as stated above, if a signed prefix root is valid for ten years, then a site can serve a user any version of the site from the last ten years.&lt;/p&gt;
      &lt;p&gt;As with tree inconsistency, this can be resolved using consensus mechanisms. If, for example, the latest manifest were published on a blockchain, then a user could fetch the latest blockchain head and ensure they got the latest version of the site. However, this incurs an extra network round trip for the client, and requires sites to wait for their hash to get published on-chain before they can update. More importantly, building this kind of consensus mechanism into our specification would drastically increase its complexity. Weâre aiming for v1.0 here.&lt;/p&gt;
      &lt;p&gt;We mitigate temporal inconsistency by requiring reasonably short validity periods for witness signatures. Making prefix root signatures valid for, e.g., one week would drastically limit the number of simultaneously servable versions. The cost is that site operators must now query the transparency service at least once a week for the new signed root and inclusion proof, even if nothing in the site changed. The sites cannot skip this, and the transparency service must be able to handle this load. This parameter must be tuned carefully.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Beyond Integrity, Consistency, and Transparency&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Providing integrity, consistency, and transparency is already a huge endeavor, but there are some additional app store-like security features that can be integrated into this system without too much work.&lt;/p&gt;
      &lt;p&gt;One problem that WAICT doesnât solve is that of provenance: where did the code the user is running come from, precisely? In settings where audits of code happen frequently, this is not so important, because some third party will be reading the code regardless. But for smaller self-hosted deployments of open-source software, this may not be viable. For example, if Alice hosts her own version of Cryptpad for her friend Bob, how can Bob be sure the code matches the real code in Cryptpadâs Github repo?&lt;/p&gt;
      &lt;p&gt;WEBCAT. The folks at the Freedom of Press Foundation (FPF) have built a solution to this, called WEBCAT. This protocol allows site owners to announce the identities of the developers that have signed the siteâs integrity manifest, i.e., have signed all the code and other assets that the site is serving to the user. Users with the WEBCAT plugin can then see the developerâs Sigstore signatures, and trust the code based on that.&lt;/p&gt;
      &lt;p&gt;Weâve made WAICT extensible enough to fit WEBCAT inside and benefit from the transparency components. Concretely, we permit manifests to hold additional metadata, which we call extensions. In this case, the extension holds a list of developersâ Sigstore identities. To be useful, browsers must expose an API for browser plugins to access these extension values. With this API, independent parties can build plugins for whatever feature they wish to layer on top of WAICT.&lt;/p&gt;
      &lt;p&gt;So far we have not built anything that can prevent attacks in the moment. An attacker who breaks into a website can still delete any code-signing extensions, or just unenroll the site from transparency entirely, and continue with their attack as normal. The unenrollment will be logged, but the malicious code will not be, and by the time anyone sees the unenrollment, it may be too late.&lt;/p&gt;
      &lt;p&gt;To prevent spontaneous unenrollment, we can enforce unenrollment cooldown client-side. Suppose the cooldown period is 24 hours. Then the rule is: if a site appears on the preload list, then the client will require that either 1) the site have transparency enabled, or 2) the site have a tombstone entry that is at least 24 hours old. Thus, an attacker will be forced to either serve a transparency-enabled version of the site, or serve a broken site for 24 hours.&lt;/p&gt;
      &lt;p&gt;Similarly, to prevent spontaneous extension modifications, we can enforce extension cooldown on the client. We will take code signing as an example, saying that any change in developer identities requires a 24 hour waiting period to be accepted. First, we require that extension &lt;code&gt;dev-ids&lt;/code&gt; has a preload list of its own, letting the client know which sites have opted into code signing (if a preload list doesnât exist then any site can delete the extension at any time). The client rule is as follows: if the site appears in the preload list, then both 1) &lt;code&gt;dev-ids&lt;/code&gt; must exist as an extension in the manifest, and 2) &lt;code&gt;dev-ids-inclusion&lt;/code&gt; must contain an inclusion proof showing that the current value of dev-ids was in a prefix tree that is at least 24 hours old. With this rule, a client will reject values of &lt;code&gt;dev-ids&lt;/code&gt; that are newer than a day. If a site wants to delete &lt;code&gt;dev-ids&lt;/code&gt;, they must 1) request that it be removed from the preload list, and 2) in the meantime, replace the dev-ids value with the empty string and update &lt;code&gt;dev-ids-inclusion&lt;/code&gt; to reflect the new value.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Deployment Considerations&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;There are a lot of distinct roles in this ecosystem. Letâs sketch out the trust and resource requirements for each role.&lt;/p&gt;
      &lt;p&gt;Transparency service. These parties store metadata for every transparency-enabled site on the web. If there are 100 million domains, and each entry is 256B each (a few hashes, plus a URL), this comes out to 26GB for a single tree, not including the intermediate hashes. To prevent size blowup, there would probably have to be a pruning rule that unenrolls sites after a long inactivity period. Transparency services should have largely uncorrelated downtime, since, if all services go down, no transparency-enabled site can make any updates. Thus, transparency services must have a moderate amount of storage, be relatively highly available, and have downtime periods uncorrelated with each other.&lt;/p&gt;
      &lt;p&gt;Transparency services require some trust, but their behavior is narrowly constrained by witnesses. Theoretically, a service can replace any leafâs chain hash with its own, and the witness will validate it (as long as the consistency proof is valid). But such changes are detectable by anyone that monitors that leaf.&lt;/p&gt;
      &lt;p&gt;Witness. These parties verify prefix tree updates and sign the resulting roots. Their storage costs are similar to that of a transparency service, since they must keep a full copy of a prefix tree for every transparency service they witness. Also like the transparency services, they must have high uptime. Witnesses must also be trusted to keep their signing key secret for a long period of time, at least long enough to permit browser trust stores to be updated when a new key is created.&lt;/p&gt;
      &lt;p&gt;Asset host. These parties carry little trust. They cannot serve bad data, since any query response is hashed and compared to a known hash. The only malicious behavior an asset host can do is refuse to respond to queries. Asset hosts can also do this by accident due to downtime.&lt;/p&gt;
      &lt;p&gt;Client. This is the most trust-sensitive part. The client is the software that performs all the transparency and integrity checks. This is, of course, the web browser itself. We must trust this.&lt;/p&gt;
      &lt;p&gt;We at Cloudflare would like to contribute what we can to this ecosystem. It should be possible to run both a transparency service and a witness. Of course, our witness should not monitor our own transparency service. Rather, we can witness other organizationsâ transparency services, and our transparency service can be witnessed by other organizations.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Supporting Alternate Ecosystems&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;WAICT should be compatible with non-standard ecosystems, ones where the large players do not really exist, or at least not in the way they usually do. We are working with the FPF on defining transparency for alternate ecosystems with different network and trust environments. The primary example we have is that of the Tor ecosystem.&lt;/p&gt;
      &lt;p&gt;A paranoid Tor user may not trust existing transparency services or witnesses, and there might not be any other trusted party with the resources to self-host these functionalities. For this use case, it may be reasonable to put the prefix tree on a blockchain somewhere. This makes the usual domain validation impossible (thereâs no validator server to speak of), but this is fine for onion services. Since an onion address is just a public key, a signature is sufficient to prove ownership of the domain.&lt;/p&gt;
      &lt;p&gt;One consequence of a consensus-backed prefix tree is that witnesses are now unnecessary, and there is only need for the single, canonical, transparency service. This mostly solves the problems of tree inconsistency at the expense of latency of updates.&lt;/p&gt;
      &lt;p&gt;We are still very early in the standardization process. One of the more immediate next steps is to get subresource integrity working for more data types, particularly WASM and images. After that, we can begin standardizing the integrity manifest format. And then after that we can start standardizing all the other features. We intend to work on this specification hand-in-hand with browsers and the IETF, and we hope to have some exciting betas soon.&lt;/p&gt;
      &lt;p&gt;In the meantime, you can follow along with our transparency specification draft, check out the open problems, and share your ideas. Pull requests and issues are always welcome!&lt;/p&gt;
      &lt;p&gt;Many thanks to Dennis Jackson from Mozilla for the lengthy back-and-forth meetings on design, to Giulio B and Cory Myers from FPF for their immensely helpful influence and feedback, and to Richard Hansen for great feedback.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606070</guid><pubDate>Thu, 16 Oct 2025 14:50:04 +0000</pubDate></item><item><title>Video game union workers rally against $55B private acquisition of EA</title><link>https://www.eurogamer.net/ea-union-workers-rally-against-55bn-saudi-backed-private-acquisition-with-formal-petition-to-regulators</link><description>&lt;doc fingerprint="af507d6bd06294a4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Video game union workers rally against $55bn Saudi-backed private acquisition of EA, with formal petition to regulators&lt;/head&gt;
    &lt;p&gt;Job losses "would be a choice, not a necessity, made to pad investors' pockets."&lt;/p&gt;
    &lt;p&gt;EA employees and the Communications Workers of America union have issued a statement against the proposed private acquisition of the company, claiming they were not represented in the negotiations and any jobs lost as a result would "be a choice, not a necessity, made to pad investors' pockets".&lt;/p&gt;
    &lt;p&gt;The acquisition of EA by a group of private investors was announced at the end of September. The deal, for $55bn, includes investment from Saudi Arabia's Public Investment Fund and Affinity Partners, the company of President Donald Trump's son-in-law Jared Kushner.&lt;/p&gt;
    &lt;p&gt;Following the announcement, there's been plenty of speculation around the future of EA and its multiple owned studios, split between EA Sports and EA Entertainment. Now, members of the United Videogame Workers union and the CWA have issued a formal response alongside a petition for regulators to scrutinise the deal.&lt;/p&gt;
    &lt;p&gt;"EA is not a struggling company," the statement reads. "With annual revenues reaching $7.5 billion and $1 billion in profit each year, EA is one of the largest video game developers and publishers in the world."&lt;/p&gt;
    &lt;p&gt;This success has been driven by company workers, the union stated. "Yet we, the very people who will be jeopardised as a result of this deal, were not represented at all when this buyout was negotiated or discussed."&lt;/p&gt;
    &lt;p&gt;Citing the number of layoffs across the industry since 2022, workers fear for "the future of our studios that are arbitrarily deemed 'less profitable' but whose contributions to the video game industry define EA's reputation."&lt;/p&gt;
    &lt;p&gt;"If jobs are lost or studios are closed due to this deal, that would be a choice, not a necessity, made to pad investors' pockets - not to strengthen the company," the statement reads.&lt;/p&gt;
    &lt;p&gt;"Every time private equity or billionaire investors take a studio private, workers lose visibility, transparency, and power," it continues. "Decisions that shape our jobs, our art, and our futures are made behind closed doors by executives who have never written a line of code, built worlds, or supported live services. We are calling on regulators and elected officials to scrutinise this deal and ensure that any path forward protects jobs, preserves creative freedom, and keeps decision-making accountable to the workers who make EA successful."&lt;/p&gt;
    &lt;p&gt;As such, workers have launched a petition in a "fight to make video games better for workers and players - not billionaires".&lt;/p&gt;
    &lt;p&gt;The statement concludes: "The value of video games is in their workers. As a unified voice, we, the members of the industry-wide video game workers' union UVW-CWA, are standing together and refusing to let corporate greed decide the future of our industry."&lt;/p&gt;
    &lt;p&gt;Eurogamer contacted the Federal Trade Commission following news of the proposed acquisition of EA, but it refused to comment "on pending mergers or acquisitions".&lt;/p&gt;
    &lt;p&gt;A report from The Financial Times suggested the deal won't face much opposition. As one source said: "What regulator is going to say no to the president's son-in-law?"&lt;/p&gt;
    &lt;p&gt;Eurogamer also spoke with the Human Rights Watch on the controversial acquisition. "We have found that the public investment fund has contributed to, and is responsible for, human rights abuses," said researcher Joey Shea on the involvement of Saudi Arabia's government. "This is a trillion dollars in Saudi state wealth that should be invested to realise the economic and social rights of Saudi citizens. We've found it's been invested in vanity mega projects inside and outside of the country.&lt;/p&gt;
    &lt;p&gt;"We see this as a deliberate attempt to distract from the country's human rights abuses [...] MBS himself wields enormous power over what is effectively public funds, and he wields this power in a highly arbitrary and personalised manner, rather than the benefit of the Saudi people more broadly. Effectively, Saudi Arabia's vast fossil fuel-derived state wealth is controlled by one person, which isn't good for human rights, or business either."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606394</guid><pubDate>Thu, 16 Oct 2025 15:12:19 +0000</pubDate></item><item><title>Why more SaaS companies are hiring chief trust officers</title><link>https://www.itbrew.com/stories/2025/10/14/why-more-saas-companies-are-hiring-chief-trust-officers</link><description>&lt;doc fingerprint="bed092891606b564"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why more SaaS companies are hiring chief trust officers&lt;/head&gt;
    &lt;head rend="h6"&gt;Software companies are hiring chief trust officers to build trust and transparency with consumers.&lt;/head&gt;
    &lt;p&gt;• 3 min read&lt;/p&gt;
    &lt;p&gt;Trust is the foundation of all relationships. That’s why several tech companies are expanding their C-suites with an executive whose sole job is ensuring their company maintains high levels of integrity with customers.&lt;/p&gt;
    &lt;p&gt;These chief trust officers, or CTrOs, are C-suite executives who build stakeholder trust in their company, including in how it handles and secures consumer data. According to an August Forrester report, the role has emerged over the past decade amongst tech and B2B software companies in response to various issues, including the potential to use technology and data for privacy abuse, bias, and harassment.&lt;/p&gt;
    &lt;p&gt;“Effectively, what the role does is offer assurance to the customers or potential customers of that organization that their data, their information, their technology, the infrastructure, the platform itself, can be trusted as those customers adopt it,” Forrester VP and Principal Analyst Jeff Pollard told IT Brew. “So, it goes a little bit beyond security.”&lt;/p&gt;
    &lt;p&gt;Trust guardians. In recent years, the CTrO role has found its way in the C-suites of various tech companies, including Salesforce and Autodesk. Pollard said it could be beneficial for a company to hire a CTrO because it escalates the responsibility of trust into an “executive priority.”&lt;/p&gt;
    &lt;p&gt;“You’re making them a member of the C-suite, which, in itself, shows an intentionality that I think is positive because it should influence the rest of what the company does,” Pollard said.&lt;/p&gt;
    &lt;p&gt;He added that designating a leader to be in charge of trust allows “more focus for the roles that roll up to the CTrO as opposed to everyone trying to do everything.”&lt;/p&gt;
    &lt;p&gt;However, adding a CTrO to your C-suite can come with baggage if not done properly, according to Pollard, who said the reporting structures associated with the role may unintentionally separate professionals from the technical side of their business.&lt;/p&gt;
    &lt;head rend="h5"&gt;Top insights for IT pros&lt;/head&gt;
    &lt;p&gt;From cybersecurity and big data to cloud computing, IT Brew covers the latest trends shaping business tech in our 4x weekly newsletter, virtual events with industry experts, and digital guides.&lt;/p&gt;
    &lt;p&gt;“One of the very realistic concerns here, from a chief trust officer perspective, is when you’re broken out of the technology work and you span beyond that…just by definition, you do lose some insight as to what the tech org is doing,” he said.&lt;/p&gt;
    &lt;p&gt;A peake at the role. Chris Peake became CTrO at revenue AI platform company Gong in August. He told IT Brew his role combines IT and security at the SaaS provider.&lt;/p&gt;
    &lt;p&gt;“Day-to-day operationally, it’s addressing historical things on both the IT side of making sure that we’ve got systems up and running that support the business, [and] controlling endpoints and all those kinds of things,” Peake said, adding he is also responsible for making sure security is built into Gong’s products.&lt;/p&gt;
    &lt;p&gt;Peake, a former CISO, said a lot of the skills from his previous role have translated into his current one. However, he said the CTrO role differs from the CISO role because it operates more on the “business level,” as the work done by a CTrO can directly impact revenue generation, contract negotiation, and onboarding new customers.&lt;/p&gt;
    &lt;p&gt;“I think that’s why we see a lot of the chief trust officers are former CISOs, because there is a natural fit there,” Peake said. “It’s just adding that next layer.”&lt;/p&gt;
    &lt;p&gt;Fad role? Forrester claims the CTrO role is more than just a “passing trend,” perhaps unlike other emerging C-suite positions.&lt;/p&gt;
    &lt;p&gt;“If it does disappear, I don’t think the need for someone to oversee trust disappears,” Pollard said. “Maybe the title does, but if it does then it’s subsumed by the CISO…hopefully they use that as part of their new remit to expand upon what they’ve done in the past and not just be the sort of legacy department of ‘no.’”&lt;/p&gt;
    &lt;head rend="h3"&gt;Top insights for IT pros&lt;/head&gt;
    &lt;p&gt;From cybersecurity and big data to cloud computing, IT Brew covers the latest trends shaping business tech in our 4x weekly newsletter, virtual events with industry experts, and digital guides.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606602</guid><pubDate>Thu, 16 Oct 2025 15:29:06 +0000</pubDate></item><item><title>Ld_preload, the Invisible Key Theft</title><link>https://bomfather.dev/blog/ld-preload-the-invisible-key-theft/</link><description>&lt;doc fingerprint="2e53cfb8e8ad5eea"&gt;
  &lt;main&gt;
    &lt;p&gt;Imagine you are running a Solana validator. You have your EDR agent running, and you have everything set up and think you are safe. But you realize your wallet is drained and you don’t know why. You start to investigate and see that the validator only accessed your private keys and nothing else. You check the directory’s permissions, logs from EDR, and everything seems to be in order.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Threat&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;LD_PRELOAD&lt;/code&gt; is an environment variable that allows you to load a shared library before the program starts. This is a powerful feature that can be used to hook system calls and intercept file operations. There are other similar variables like &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is not ENV variables alone. There are things like &lt;code&gt;/etc/ld.so.preload&lt;/code&gt; that can be used to load a shared library before the program starts.&lt;/p&gt;
    &lt;p&gt;Linux built this feature to allow developers to load shared libraries before the program starts so that they can debug and test their code.&lt;/p&gt;
    &lt;p&gt;Our implementation of this exploitation code is in our Github Repo.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Solana?&lt;/head&gt;
    &lt;p&gt;This is not just a Solana problem. This is a problem for any application that loads credentials from a file. The insider threat is real and a problem for any organization. This is another way to steal private keys. There is too much at stake not to be careful.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Vulnerability&lt;/head&gt;
    &lt;p&gt;Now, most of us write userspace applications, and use libraries like &lt;code&gt;glibc&lt;/code&gt; to handle the file operations, so we never really think about this issue. &lt;code&gt;glibc&lt;/code&gt; does the heavy lifting for us, but what if we hook into the file operations and intercept them before &lt;code&gt;glibc&lt;/code&gt; does its thing? This is what &lt;code&gt;LD_PRELOAD&lt;/code&gt; allows us to do. It is a way to hook into a library call.&lt;/p&gt;
    &lt;p&gt;In our case, we did this by hooking into the file operations and intercepting them before &lt;code&gt;glibc&lt;/code&gt; did its thing. When the validator read its keypair files from disk, we intercepted the &lt;code&gt;close()&lt;/code&gt; call and made a copy before the file descriptor closed.&lt;/p&gt;
    &lt;p&gt;We wrote a malicious shared library that hooks into the file operations. We also call the real &lt;code&gt;close()&lt;/code&gt; function, so the validator continues normally, the user has no idea that this is happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the Attack Works&lt;/head&gt;
    &lt;p&gt;Our malicious library does something clever, it hooks the &lt;code&gt;close()&lt;/code&gt; system call.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;close()&lt;/code&gt; and not &lt;code&gt;open()&lt;/code&gt; or &lt;code&gt;read()&lt;/code&gt;? Because when a file is closed, we know the application is done with it. We can check what file was just accessed by looking at &lt;code&gt;/proc/self/fd/{fd}&lt;/code&gt; before the file descriptor is closed.&lt;/p&gt;
    &lt;p&gt;The hook is surprisingly simple - about 30 lines of C. Here’s the core concept:&lt;/p&gt;
    &lt;code&gt;int close(int fd) {
    // 1. Get pointer to real close() using dlsym(RTLD_NEXT, "close")
    // 2. Read /proc/self/fd/{fd} to see what file this is
    // 3. If path contains "solana-run", copy the file to /tmp/stolen-validator-data
    // 4. Call real close() so the validator continues normally
}

&lt;/code&gt;
    &lt;p&gt;That’s it. No privilege escalation, kernel exploits, or complex techniques. Just intercepting a standard library call that every program uses.&lt;/p&gt;
    &lt;p&gt;The key trick is using &lt;code&gt;/proc/self/fd/{fd}&lt;/code&gt;, a Linux feature that lets you see what file a file descriptor points to. Before the file descriptor closes, we check if it’s one of the Solana keypair files. If it is, we make a copy.&lt;/p&gt;
    &lt;head rend="h2"&gt;A More In-Depth View&lt;/head&gt;
    &lt;p&gt;Both of the following are based on our attack code https://github.com/bomfather/tools/tree/main/ld-preload.&lt;/p&gt;
    &lt;head rend="h3"&gt;Method 1: Environment Variable (LD_PRELOAD)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compile &lt;code&gt;malicious.so&lt;/code&gt;and inject it into the validator container&lt;/item&gt;
      &lt;item&gt;Set &lt;code&gt;LD_PRELOAD=/tmp/malicious.so&lt;/code&gt;when starting the validator&lt;/item&gt;
      &lt;item&gt;When the validator opens keypair files, our library is already loaded&lt;/item&gt;
      &lt;item&gt;Every time &lt;code&gt;close()&lt;/code&gt;is called, we intercept it&lt;/item&gt;
      &lt;item&gt;Check if the file path contains “solana-run” (the ledger directory)&lt;/item&gt;
      &lt;item&gt;If yes, copy the file to our exfiltration directory&lt;/item&gt;
      &lt;item&gt;Call the real &lt;code&gt;close()&lt;/code&gt;so the validator continues normally&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Method 2: Persistent (/etc/ld.so.preload)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compile &lt;code&gt;malicious.so&lt;/code&gt;inside the container&lt;/item&gt;
      &lt;item&gt;Write &lt;code&gt;/tmp/malicious.so&lt;/code&gt;to&lt;code&gt;/etc/ld.so.preload&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Start the validator (library loads automatically)&lt;/item&gt;
      &lt;item&gt;Same interception and exfiltration process&lt;/item&gt;
      &lt;item&gt;Affects ALL processes in the container, not just the validator&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Deploying the Attack&lt;/head&gt;
    &lt;p&gt;Environment variable method:&lt;/p&gt;
    &lt;code&gt;LD_PRELOAD=/path/to/malicious.so ./program
&lt;/code&gt;
    &lt;p&gt;Persistent method:&lt;/p&gt;
    &lt;code&gt;echo "/path/to/malicious.so" &amp;gt; /etc/ld.so.preload
./program  # Library loads automatically
&lt;/code&gt;
    &lt;p&gt;It is as simple as that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters: Scope and Implications&lt;/head&gt;
    &lt;head rend="h3"&gt;Can containers be exploited?&lt;/head&gt;
    &lt;p&gt;Yes. Containers don’t protect against &lt;code&gt;LD_PRELOAD&lt;/code&gt; attacks because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The environment variable is set within the container’s namespace&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/etc/ld.so.preload&lt;/code&gt;is a file inside the container&lt;/item&gt;
      &lt;item&gt;The process inside the container runs the library inside the container&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Container isolation doesn’t help when the attack comes from inside the container.&lt;/p&gt;
    &lt;head rend="h3"&gt;Do I need to be root?&lt;/head&gt;
    &lt;p&gt;It depends on the method:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;LD_PRELOAD&lt;/code&gt; environment variable:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No root needed for your own processes&lt;/item&gt;
      &lt;item&gt;Can be set by any user for processes they start&lt;/item&gt;
      &lt;item&gt;That is the scary part&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;/etc/ld.so.preload&lt;/code&gt; file:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires root/privileged access to modify the file&lt;/item&gt;
      &lt;item&gt;But once set, it affects ALL processes system-wide&lt;/item&gt;
      &lt;item&gt;More dangerous, but requires privilege escalation first&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Remember the scenario at the beginning? The drained validator with everything “in order”? This is how it happens, with a single environment variable, no root access needed, and no alerts triggered. Just silent exfiltration of private keys while the validator runs normally.&lt;/p&gt;
    &lt;p&gt;The scary part isn’t the complexity of the attack but the simplicity. &lt;code&gt;LD_PRELOAD&lt;/code&gt; is a legitimate debugging feature. File access by the validator process is expected behavior.&lt;/p&gt;
    &lt;p&gt;Check if your EDR agent is handling things like this.&lt;/p&gt;
    &lt;p&gt;Complete source code and demo available at https://github.com/bomfather/tools/tree/main/ld-preload. The steps to run it are in the &lt;code&gt;README.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Disclaimer: This tool is for educational and authorized security testing only. Unauthorized access to computer systems is illegal. See LICENSE for complete terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606611</guid><pubDate>Thu, 16 Oct 2025 15:29:45 +0000</pubDate></item><item><title>Claude Skills</title><link>https://www.anthropic.com/news/skills</link><description>&lt;doc fingerprint="9e4c2e7f2f779869"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Claude Skills&lt;/head&gt;
    &lt;p&gt;Claude can now use Skills to improve how it performs specific tasks. Skills are folders that include instructions, scripts, and resources that Claude can load when needed.&lt;/p&gt;
    &lt;p&gt;Claude will only access a skill when it's relevant to the task at hand. When used, skills make Claude better at specialized tasks like working with Excel or following your organization's brand guidelines.&lt;/p&gt;
    &lt;p&gt;You've already seen Skills at work in Claude apps, where Claude uses them to create files like spreadsheets and presentations. Now, you can build your own skills and use them across Claude apps, Claude Code, and our API.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Skills work&lt;/head&gt;
    &lt;p&gt;While working on tasks, Claude scans available skills to find relevant matches. When one matches, it loads only the minimal information and files needed—keeping Claude fast while accessing specialized expertise.&lt;/p&gt;
    &lt;p&gt;Skills are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Composable: Skills stack together. Claude automatically identifies which skills are needed and coordinates their use.&lt;/item&gt;
      &lt;item&gt;Portable: Skills use the same format everywhere. Build once, use across Claude apps, Claude Code, and API.&lt;/item&gt;
      &lt;item&gt;Efficient: Only loads what's needed, when it's needed.&lt;/item&gt;
      &lt;item&gt;Powerful: Skills can include executable code for tasks where traditional programming is more reliable than token generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Think of Skills as custom onboarding materials that let you package expertise, making Claude a specialist on what matters most to you. For a technical deep-dive on the Agent Skills design pattern, architecture, and development best practices, read our engineering blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills work with every Claude product&lt;/head&gt;
    &lt;head rend="h3"&gt;Claude apps&lt;/head&gt;
    &lt;p&gt;Skills are available to Pro, Max, Team and Enterprise users. We provide skills for common tasks like document creation, examples you can customize, and the ability to create your own custom skills.&lt;/p&gt;
    &lt;p&gt;Claude automatically invokes relevant skills based on your task—no manual selection needed. You'll even see skills in Claude's chain of thought as it works.&lt;lb/&gt;Creating skills is simple. The "skill-creator" skill provides interactive guidance: Claude asks about your workflow, generates the folder structure, formats the SKILL.md file, and bundles the resources you need. No manual file editing required. &lt;/p&gt;
    &lt;p&gt;Enable Skills in Settings. For Team and Enterprise users, admins must first enable Skills organization-wide.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Developer Platform (API)&lt;/head&gt;
    &lt;p&gt;Agent Skills, which we often refer to simply as Skills, can now be added to Messages API requests and the new &lt;code&gt;/v1/skills&lt;/code&gt; endpoint gives developers programmatic control over custom skill versioning and management. Skills require the Code Execution Tool beta, which provides the secure environment they need to run.&lt;/p&gt;
    &lt;p&gt;Use Anthropic-created skills to have Claude read and generate professional Excel spreadsheets with formulas, PowerPoint presentations, Word documents, and fillable PDFs. Developers can create custom Skills to extend Claude's capabilities for their specific use cases.&lt;/p&gt;
    &lt;p&gt;Developers can also easily create, view, and upgrade skill versions through the Claude Console.&lt;/p&gt;
    &lt;p&gt;Explore the documentation or Anthropic Academy to learn more.&lt;/p&gt;
    &lt;quote&gt;Skills teaches Claude how to work with Box content. Users can transform stored files into PowerPoint presentations, Excel spreadsheets, and Word documents that follow their organization's standards—saving hours of effort.&lt;/quote&gt;
    &lt;quote&gt;Skills streamline our management accounting and finance workflows. Claude processes multiple spreadsheets, catches critical anomalies, and generates reports using our procedures. What once took a day, we can now accomplish in an hour.&lt;/quote&gt;
    &lt;quote&gt;Canva plans to leverage Skills to customize agents and expand what they can do. This unlocks new ways to bring Canva deeper into agentic workflows—helping teams capture their unique context and create stunning, high-quality designs effortlessly.&lt;/quote&gt;
    &lt;quote&gt;Skills streamline our management accounting and finance workflows. Claude processes multiple spreadsheets, catches critical anomalies, and generates reports using our procedures. What once took a day, we can now accomplish in an hour.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Claude Code&lt;/head&gt;
    &lt;p&gt;Skills extend Claude Code with your team's expertise and workflows. Install skills via plugins from the anthropics/skills marketplace. Claude loads them automatically when relevant. Share skills through version control with your team. You can also manually install skills by adding them to &lt;code&gt;~/.claude/skills&lt;/code&gt;. The Claude Agent SDK provides the same Agent Skills support for building custom agents. &lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude apps: User Guide &amp;amp; Help Center&lt;/item&gt;
      &lt;item&gt;API developers: Documentation&lt;/item&gt;
      &lt;item&gt;Claude Code: Documentation&lt;/item&gt;
      &lt;item&gt;Example Skills to customize: GitHub repository&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;We're working toward simplified skill creation workflows and enterprise-wide deployment capabilities, making it easier for organizations to distribute skills across teams.&lt;/p&gt;
    &lt;p&gt;Keep in mind, this feature gives Claude access to execute code. While powerful, it means being mindful about which skills you use—stick to trusted sources to keep your data safe. Learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45607117</guid><pubDate>Thu, 16 Oct 2025 16:05:47 +0000</pubDate></item></channel></rss>