<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 28 Oct 2025 07:10:55 +0000</lastBuildDate><item><title>Claude for Excel</title><link>https://www.claude.com/claude-for-excel</link><description>&lt;doc fingerprint="3d3f8e961dffc20a"&gt;
  &lt;main&gt;
    &lt;p&gt;Piloting Claude for Excel&lt;/p&gt;
    &lt;p&gt;Claude understands your entire workbookâfrom nested formulas to multiple tab dependencies. Get explanations with cell-level citations, and update assumptions while preserving formulas. Now in beta as a research preview.&lt;/p&gt;
    &lt;head rend="h2"&gt;How teams use Claude for Excel&lt;/head&gt;
    &lt;p&gt;Claude listens carefully, follows instructions precisely, â¨and thinks through complex problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get answers about any cell in seconds&lt;/head&gt;
    &lt;p&gt;Navigate complex models instantly. Ask Claude about specific formulas, entire worksheets, or calculation flows across tabs. Every explanation includes cell-level citations so you can verify the logic.&lt;/p&gt;
    &lt;head rend="h3"&gt;Test scenarios without breaking formulas&lt;/head&gt;
    &lt;p&gt;Update assumptions across your entire model while preserving all dependencies. Test different scenarios quicklyâClaude highlights every change with explanations for full transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Debug and fix errors&lt;/head&gt;
    &lt;p&gt;Trace #REF!, #VALUE!, and circular reference errors to their source in seconds. Claude explains what went wrong and how to fix it without disrupting the rest of your model.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build models or fill existing templates&lt;/head&gt;
    &lt;p&gt;Create draft financial models from scratch based on your requirements. Or populate existing templates with fresh data while maintaining all formulas and structure.&lt;/p&gt;
    &lt;p&gt;The Claude you trust, right in Excel&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency and visibility&lt;/head&gt;
    &lt;p&gt;See Claudeâs changes in real time with explanations&lt;/p&gt;
    &lt;head rend="h3"&gt;Formula integrity&lt;/head&gt;
    &lt;p&gt;Maintain Excel model structure and formatting&lt;/p&gt;
    &lt;head rend="h3"&gt;Enterprise security&lt;/head&gt;
    &lt;p&gt;Works within your existing compliance framework&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;Claude for Excel is available in beta as a research preview through a waitlist for 1,000 Max, Team and Enterprise plan customers. Weâll gradually expand access as we build confidence through this limited preview.&lt;/p&gt;
    &lt;p&gt;Claude for Excel works within your existing security framework. Claude can make mistakes, so you should always review changes before finalizing, especially for client-facing deliverables.&lt;/p&gt;
    &lt;p&gt;Claude for Excel is currently in beta as a research preview, so itâs best for model analysis, assumption updates, error debugging, template population, formula explanations, multi-tab navigation. Claude doesnât have advanced Excel capabilities including pivot tables, conditional formatting, data validation, data tables, macros, and VBA. Weâre actively working on these features.&lt;/p&gt;
    &lt;p&gt;Yes, Claude is trained to recognize common financial modeling patterns, formula structures, and industry-standard calculations. However, always verify outputs match your specific methodologies.&lt;/p&gt;
    &lt;p&gt;Currently .xlsx and .xlsm files are supported. File size limits apply based on your Claude plan.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45722639</guid><pubDate>Mon, 27 Oct 2025 16:09:22 +0000</pubDate></item><item><title>Show HN: JSON Query</title><link>https://jsonquerylang.org/</link><description>&lt;doc fingerprint="481baa954fb0882b"&gt;
  &lt;main&gt;
    &lt;code&gt;name(argument1, argument2, ...)&lt;/code&gt;
    &lt;p&gt;A function is defined as a function name followed by comma separated arguments wrapped in round brackets. it is important to understand functions like &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;sort&lt;/code&gt;, and &lt;code&gt;max&lt;/code&gt; are executed as a method in a chain: the operation is applied to the data input, and forwarded to the next method in the chain (if any).&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;sort(.address.city, "asc")&lt;/code&gt;
    &lt;code&gt;filter(.age &amp;gt;= 21) | sort(.age, "asc")&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;p&gt;Function reference:&lt;/p&gt;
    &lt;code&gt;left operator right&lt;/code&gt;
    &lt;p&gt;JSON Query supports all basic operators. Operators must have both a left and right hand side. To override the default precedence, an operator can be wrapped in parentheses &lt;code&gt;(...)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;.age &amp;gt;= 18&lt;/code&gt;
    &lt;code&gt;filter(.age &amp;gt;= 18 and .age &amp;lt;= 65)&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;p&gt;Operator reference:&lt;/p&gt;
    &lt;code&gt;query2 | query2 | ...&lt;/code&gt;
    &lt;p&gt;A pipe is an array containing a series of queries. The queries in the pipeline are executed one by one, and the output of the first is the input for the next.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;filter(.age &amp;gt;= 18) | sort(.name)&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;code&gt;{prop1: query1, prop2: query2, ...}&lt;/code&gt;
    &lt;p&gt;An object is defined as a regular JSON object with a property name as key, and a query as value. Objects can be used to transform data or to execute multiple query pipelines in parallel.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;{
  names: map(.name),
  numberOfNames: size()
}&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;code&gt;[query1, query2, ...]&lt;/code&gt;
    &lt;p&gt;An array is defined as a regular JSON array: enclosed in square brackets, with items separated by a comma.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;filter(.city in ["New York", "Atlanta"])&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;code&gt;.prop1.prop2&lt;/code&gt;
    &lt;p&gt;A property retrieves a property from an object. Multiple consecutive properties will retrieve a nested property.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;.age&lt;/code&gt;
    &lt;code&gt;.address.city&lt;/code&gt;
    &lt;code&gt;"first name"&lt;/code&gt;
    &lt;code&gt;get()&lt;/code&gt;
    &lt;code&gt;get("address", "city")&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
    &lt;code&gt;"string", number, boolean, null&lt;/code&gt;
    &lt;p&gt;JSON Query supports the following primitive values, the same as in JSON: &lt;code&gt;"string"&lt;/code&gt;, &lt;code&gt;number&lt;/code&gt;, &lt;code&gt;boolean&lt;/code&gt;, &lt;code&gt;null&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;"Hello world"&lt;/code&gt;
    &lt;code&gt;"Multi line text\nwith \"quoted\" contents"&lt;/code&gt;
    &lt;code&gt;42&lt;/code&gt;
    &lt;code&gt;2.74&lt;/code&gt;
    &lt;code&gt;-1.2e3&lt;/code&gt;
    &lt;code&gt;true&lt;/code&gt;
    &lt;code&gt;false&lt;/code&gt;
    &lt;code&gt;null&lt;/code&gt;
    &lt;p&gt;Documentation:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45722826</guid><pubDate>Mon, 27 Oct 2025 16:22:52 +0000</pubDate></item><item><title>Fnox, a secret manager that pairs well with mise</title><link>https://github.com/jdx/mise/discussions/6779</link><description>&lt;doc fingerprint="fb7f07ee0359b4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing fnox: A secret manager that pairs well with mise #6779&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I'm excited to announce fnox – a new secret management tool designed to work seamlessly alongside mise in your development workflow.&lt;/p&gt;
          &lt;p&gt;While it's brand new, I have labeled it 1.0 since it seems pretty feature complete and given my experience with several experiments with secrets over the years with mise, I think will be a lot more stable than its young age would indicate.&lt;/p&gt;
          &lt;head&gt;What is fnox?&lt;/head&gt;
          &lt;p&gt;fnox (think "Fort Knox") is a command-line secret manager that handles encrypted and remote secrets for development, CI/CD, and production environments. It provides a unified interface for managing sensitive data through either local encryption or remote storage backends.&lt;/p&gt;
          &lt;head&gt;Why fnox?&lt;/head&gt;
          &lt;p&gt;While mise has built-in secret support (age encryption and sops), these work best for simple, file-based scenarios. For more complex production needs, fnox provides:&lt;/p&gt;
          &lt;head&gt;🚀 Developer-Friendly&lt;/head&gt;
          &lt;head&gt;👥 Team-Ready&lt;/head&gt;
          &lt;head&gt;Getting Started&lt;/head&gt;
          &lt;p&gt;Install fnox with mise:&lt;/p&gt;
          &lt;code&gt;$ mise use -g fnox
$ fnox --version&lt;/code&gt;
          &lt;p&gt;Create your first secret:&lt;/p&gt;
          &lt;code&gt;$ fnox init
$ fnox provider add age --id main --recipients ~/.ssh/id_ed25519.pub
$ fnox secret set API_KEY --value "your-secret-value" --provider main&lt;/code&gt;
          &lt;p&gt;Use secrets in your workflow:&lt;/p&gt;
          &lt;code&gt;# Export secrets as environment variables
$ fnox exec -- your-command

# Get a single secret
$ fnox get API_KEY

# Shell integration (auto-load secrets on cd)
$ fnox shell hook&lt;/code&gt;
          &lt;head&gt;How It Works with mise&lt;/head&gt;
          &lt;p&gt;fnox and mise work independently but complement each other:&lt;/p&gt;
          &lt;p&gt;A typical setup:&lt;/p&gt;
          &lt;code&gt;[env]
NODE_ENV = "development"
DATABASE_HOST = "localhost"

[tools]
node = "20"
fnox = "latest"&lt;/code&gt;
          &lt;code&gt;[providers.age]
type = "age"
recipients = ["age1ql3z7..."]

[secrets]
DATABASE_PASSWORD = { provider = "age", value = "AGE-SECRET-KEY..." }
API_KEY = { provider = "1password", ref = "op://dev/api/credential" }&lt;/code&gt;
          &lt;p&gt;Then use both together:&lt;/p&gt;
          &lt;code&gt;$ mise x -- fnox x -- npm start&lt;/code&gt;
          &lt;p&gt;Or you can activate one or the other in your shell to avoid that.&lt;/p&gt;
          &lt;head&gt;Why Separate Tools?&lt;/head&gt;
          &lt;p&gt;You might wonder why fnox isn't built into mise. The answer comes down to fundamental architectural constraints:&lt;/p&gt;
          &lt;p&gt;The Performance Problem: mise reloads its environment frequently (on directory changes, after commands, etc.). If secrets relied on remote calls to services like KMS or 1Password, each reload would require network requests, making mise unacceptably slow.&lt;/p&gt;
          &lt;p&gt;The Security Tradeoff: Caching could solve the performance issue, but introduces security risks:&lt;/p&gt;
          &lt;p&gt;The Architecture Challenge: Making mise skip reloading certain env vars would require a major architectural overhaul—a change that would complicate the codebase significantly.&lt;/p&gt;
          &lt;p&gt;By creating fnox as a separate tool with its own shell integration, we avoid these problems entirely. Each tool can focus on what it does best:&lt;/p&gt;
          &lt;head&gt;What's going to happen to mise secrets?&lt;/head&gt;
          &lt;p&gt;They're still marked as experimental so the future is technically up in the air. That said, mise does work well for age/sops encryption so I think it could probably come out of experimental. For now, I don't have plans to introduce remote secret backends like fnox provides.&lt;/p&gt;
          &lt;head&gt;Learn More&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 1 comment 1 reply&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Since it's a verbatim copy of https://secretspec.dev, any chance of giving attribution?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45722931</guid><pubDate>Mon, 27 Oct 2025 16:29:38 +0000</pubDate></item><item><title>JetKVM – Control any computer remotely</title><link>https://jetkvm.com/</link><description>&lt;doc fingerprint="af5976106919f929"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Ultra-Low Latency&lt;/head&gt;
    &lt;p&gt;High-definition 1080p video at 60 FPS with 30-60 millisecond latency, using efficient H.264 encoding. Smooth mouse and keyboard action transfer for responsive remote interaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Free &amp;amp; Optional Cloud Access&lt;/head&gt;
    &lt;p&gt;Optional remote management via our open-source JetKVM Cloud using WebRTC. Privacy-first design with opt-in cloud access that provides secure and fast direct connections, even behind the most restrictive NAT environments, with our STUN and TURN servers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Source: Built for Collaboration&lt;/head&gt;
    &lt;p&gt;JetKVM is built on a robust Golang foundation and powered by Linux for adaptability and transparency. Whether you're a seasoned developer or an enthusiastic tinkerer, you can easily modify or fine-tune the software using familiar tooling and straightforward SSH uploads.&lt;/p&gt;
    &lt;head rend="h4"&gt;Available Source Code&lt;/head&gt;
    &lt;head rend="h5"&gt;KVM Runtime&lt;/head&gt;
    &lt;p&gt;Combining a Go-based backend with a React-powered WebRTC dashboard. Perfect for forking, submitting new features, fixing bugs, or customizing local streaming and control.&lt;/p&gt;
    &lt;head rend="h5"&gt;Cloud API &amp;amp; Dashboard&lt;/head&gt;
    &lt;p&gt;Our cloud-hosted management interface is fully open source. Delve into our secure remote connection orchestration or fork it to build specialized workflows and unique integrations.&lt;/p&gt;
    &lt;head rend="h5"&gt;Core System&lt;/head&gt;
    &lt;p&gt;Minimal Linux system built with BusyBox for core utilities. No bloat or unnecessary services - just the essential components needed for stable remote access.&lt;/p&gt;
    &lt;head rend="h2"&gt;Universally loved&lt;/head&gt;
    &lt;p&gt;Every single tech reviewer who's tested JetKVM has given it a glowing review. No exceptions. From professional data centers to home labs, the verdict is unanimous: this is the remote access solution the tech world has been waiting for.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unlimited Hackability&lt;/head&gt;
    &lt;p&gt;The JetKVM hardware is fully customizable. Through the RJ12 extension port, extra hardware capabilities can easily be added by anyone. The JetKVM extension port is the way to fully customize your device.&lt;/p&gt;
    &lt;head rend="h2"&gt;Seamless Remote Control&lt;/head&gt;
    &lt;p&gt;Experience fluid control and crystal-clear video quality that makes remote access feel local. Perfect for IT professionals, developers, and power users who demand responsive remote management.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stay updated on our latest projects&lt;/head&gt;
    &lt;p&gt;Join our newsletter to receive updates about new features, product launches, and early access opportunities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723159</guid><pubDate>Mon, 27 Oct 2025 16:44:17 +0000</pubDate></item><item><title>Why Busy Beaver hunters fear the Antihydra</title><link>https://benbrubaker.com/why-busy-beaver-hunters-fear-the-antihydra/</link><description>&lt;doc fingerprint="eeaad01055b00fc9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Busy Beaver Hunters Fear the Antihydra&lt;/head&gt;
    &lt;p&gt;In the summer of 2024, I reported on an online community that nailed down the precise value of a number called BB(5) — the first big breakthrough in 50 years on an old problem in theoretical computer science known as the busy beaver game. BB(5), now known to be 47,176,870, is the fifth of the so-called busy beaver numbers, which measure the complexity of the craziest computations that simple computer programs can complete.1The team recently released a paper describing their results in detail.&lt;/p&gt;
    &lt;p&gt;The next step in this idiosyncratic research effort is to identify the sixth busy beaver number BB(6), and there has been some notable progress on that front — I wrote a follow-up story about it a few months ago. But busy beaver researchers don’t expect to nail down the true value of BB(6) any time soon. That’s because doing so would require them to understand the behavior of a program with the awesome name “Antihydra,” which resembles a longstanding open problem in mathematics called the Collatz conjecture.2Antihydra should not be confused with the false hydra, a very cool and very terrifying monster conceived by D&amp;amp;D blogger Arnold Kemp. A twitter user sharing my first busy beaver story summed up this state of affairs more succinctly:&lt;/p&gt;
    &lt;p&gt;Both of my stories alluded to the Antihydra barrier only very briefly. In this blog post I will explore it in more detail: What exactly is Antihydra, what is the Collatz conjecture, how are they connected, and what makes them so daunting?&lt;/p&gt;
    &lt;head rend="h2"&gt;Busy Beaver Basics&lt;/head&gt;
    &lt;p&gt;If you haven’t already read my two Quanta stories about the busy beaver game, I recommend doing so before reading further, mainly just because they’re both really fun! Here I’ll recap how the busy beaver game works so that we’re all on the same page.&lt;/p&gt;
    &lt;p&gt;I wrote above that the busy beaver numbers “measure the complexity of the craziest computations that simple computer programs can complete.” To define them more precisely, we first need a mathematical framework for gauging the complexity of computer programs themselves, to decide which ones are “simple.” Then we need a way to quantify the complexity of computations — what computer programs do — so that we can identify the craziest ones.&lt;/p&gt;
    &lt;p&gt;In the busy beaver game, computer programs are represented by hypothetical devices called Turing machines, which compute in discrete steps by reading and writing 0s and 1s on an infinite tape divided into cells. A unique list of rules governs the behavior of each Turing machine. Anything you can do with an ordinary computer program, you can in principle do with the right set of Turing machine rules.3In the busy beaver literature, these rules are called “states.” “In principle” is doing a lot of work in this sentence — even if you managed to acquire the requisite infinite tape, computing with a Turing machine would be horrendously inefficient. But Turing machines are easier to analyze theoretically than more practical programming languages.&lt;/p&gt;
    &lt;p&gt;Let’s unpack how Turing machines work in a bit more detail. At each step, a Turing machine consults one of its rules and edits one cell on the tape. Each rule has two cases: what to do if the current cell contains a 0, and what to do if it contains a 1. “What to do” here means what to write in the current cell, which direction to move next, and which rule to consult for the next step. One case of one rule breaks this pattern: It tells the Turing machine to “halt,” or stop running. But by itself, the existence of this instruction doesn’t guarantee that a Turing machine will halt — the machine might never get there. Quanta’s visual designer Kristina Armitage encapsulated all of this in a beautiful infographic.4In my first Busy Beaver story, you will also find animations of Turing machines in action.&lt;/p&gt;
    &lt;p&gt;The number of rules that a Turing machine has will be our measure of program complexity. This choice lets us replace our vague question about the craziest things that simple computer programs can do with a series of specific questions about different degrees of craziness, corresponding to different busy beaver numbers. You learn the value of BB(1) by answering the question “what’s the most complex computation that a one-rule Turing machine can complete?” Likewise, BB(2) measures the most complex computation that a two-rule machine can complete, and so on.&lt;/p&gt;
    &lt;p&gt;To answer these questions, we need a precise definition of what makes one computation more complex than another. A natural measure is how many steps the Turing machine needs to complete the computation. “Complete” is important — every Turing machine that never halts will run for infinitely many steps, but that’s not really a fair comparison. The number of steps that a Turing machine takes before halting (and indeed, whether it halts at all) can depend on the initial pattern of 0s and 1s on the tape. For the busy beaver game, we always start from the so-called “blank tape,” which has 0s in every cell.&lt;/p&gt;
    &lt;p&gt;We now have all the necessary pieces to formally define the busy beaver numbers. Let’s take BB(6) to be specific: It is the longest finite runtime among all six-rule Turing machines, when those machines start with a blank tape. Finding this number is straightforward in principle. First, list out all possible six-rule Turing machines. Next, sort them into two categories: those that will eventually halt when they start running on the blank tape, and those that will run forever. Toss out all the non-halting machines. Finally, measure how many steps each of the halting machines takes before stopping. The largest number is BB(6).&lt;/p&gt;
    &lt;p&gt;The problem with this plan lies in the second step, where you divide the Turing machines into two groups based on whether or not they halt. It turns out that deciding whether a Turing machine will halt can be an extremely hard problem, to put it mildly. And if you can’t tell whether a given machine will halt, then you don’t know whether your list of halting Turing machines is complete, so you can’t know whether you’ve found the longest runtime! As of this writing, researchers have classified the vast majority of six-rule machines as either halting or non-halting. But there are 1,618 “holdouts” whose fate remains unknown.&lt;/p&gt;
    &lt;p&gt;Antihydra is one of these holdout machines. To nail down the value of BB(6), researchers must first determine whether Antihydra halts, and that seems to be beyond the reach of any known mathematical technique. To understand why, we need to take a step back and ask, “what exactly are these Turing machines doing?”&lt;/p&gt;
    &lt;head rend="h2"&gt;Leveling Up&lt;/head&gt;
    &lt;p&gt;You may object at this point that we already know exactly what these Turing machines are doing: Each one is just following a specific sequence of rules, writing 0s and 1s on the tape as it goes. But this “low-level” description is a bit like saying “when I push these buttons, my pocket calculator toggles transistors on and off in this specific pattern.” That may very well be true, but “high-level” descriptions like “when I push these buttons, my pocket calculator multiplies 3 and 4” are usually more useful.&lt;/p&gt;
    &lt;p&gt;There’s no guarantee that any given Turing machine’s behavior admits such a simple high-level description.5Also, in many cases low-level descriptions are perfectly adequate. For example, the easiest way to prove that a Turing machine halts is just to simulate it step by step until it stops running. When that happens, you don’t need a deeper understanding of why it halted: Just note its runtime and move on. But remember that Turing machines can carry out all possible computations — that means that at least some Turing machines must be executing programs with high-level descriptions that humans can understand.&lt;/p&gt;
    &lt;p&gt;Actually, the most notable five- and six-rule Turing machines that busy beaver researchers have studied so far all have relatively simple high-level descriptions — that includes the longest-running five- and six-rule machines that eventually halt, the most complex non-halting five-rule machines, and holdouts like Antihydra.6This is an empirical observation, not a self-evident truth. In fact, some researchers expected that the longest-running Turing machines would be “spaghetti code” machines that lack any high-level description!&lt;/p&gt;
    &lt;p&gt;Let’s look at a specific example. The fifth busy beaver, which runs for 47,176,870 steps before halting, obeys the following low-level rules:&lt;/p&gt;
    &lt;p&gt;In 1993, the mathematician Pascal Michel proved that these rules are equivalent to a simple high-level program:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set \(x = 0\).&lt;/item&gt;
      &lt;item&gt;Divide \(x\) by 3 and check the remainder. &lt;list rend="ul"&gt;&lt;item&gt;If the remainder is 0, calculate \((5x + 18)/3\). The result is your new value of \(x\).&lt;/item&gt;&lt;item&gt;If the remainder is 1, calculate \((5x + 22)/3\). The result is your new value of \(x\).&lt;/item&gt;&lt;item&gt;If the remainder is 2, halt.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;If you haven’t halted, go back to step 2 and plug in the new value of \(x\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you have a high-level description like this, you can use it to determine whether the machine will halt — and if so, exactly how many steps it will take.7Each step in a high-level program like this one corresponds to many individual Turing machine steps. Whenever you prove an equivalence between high-level and low-level descriptions, you get formulas that you can use to compute how long each high-level step will take. I won’t say anything about how to actually prove these equivalences. In this case, the high-level program just repeatedly plugs in new values of \(x\) until it finds one that leaves a remainder of 2 when divided by 3. One third of numbers have this property, so you might guess that the program will take three tries to find one, give or take a few. If you start from a random value of \(x\), you’ll find that three iterations is indeed typical. But it turns out that if you start from \(x = 0\), this program will repeat the second step 15 times before it lands on a number with remainder 2! Busy beaver researchers often like to anthropomorphize the Turing machines they study, imagining that the machines are actively trying to run for as long as possible. Adopting that perspective, we might say that this Turing machine got very lucky.&lt;/p&gt;
    &lt;p&gt;The fifth busy beaver is just one member of a family of “Collatz-like” Turing machines whose high-level behavior has the following general form:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set \(x\) equal to some starting value (which may or may not be 0).&lt;/item&gt;
      &lt;item&gt;Divide \(x\) by a fixed number \(N\). The remainder tells you what formula to use to get your new value of \(x\).&lt;/item&gt;
      &lt;item&gt;Check if you’ve met a specific halting condition. If not, go back to step 2 with the new value of \(x\).8As we saw in the above example, the halting condition can be as simple as “the remainder has a specific value.” Below we’ll see some examples with different halting conditions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The family of Collatz-like Turing machines includes both halting and non-halting machines. It gets its name from a procedure for generating number sequences devised in 1937 by the mathematician Lothar Collatz:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Choose a starting value for \(x\).&lt;/item&gt;
      &lt;item&gt;Check whether \(x\) is even or odd. &lt;list rend="ul"&gt;&lt;item&gt;If it’s even, calculate \(x/2\). The result is your new value of \(x\).&lt;/item&gt;&lt;item&gt;If it’s odd, calculate \(3x + 1\). The result is your new value of \(x\).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Check whether \(x = 1\). If not, go back to step 2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This looks very similar to our general description of high-level behavior for Collatz-like machines, with \(x = 1\) as the halting condition.9“Check whether \(x\) is even or odd” is just another way of saying “divide \(x\) by 2 and check the remainder.” Strictly speaking, we don’t have to specify that the sequence stops when \(x = 1\). But if we keep applying the rules after it hits 1, the sequence enters an infinite loop: 1 &amp;gt; 4 &amp;gt; 2 &amp;gt; 1 and so on. Try iterating these rules from any initial integer value of \(x\) — I’m willing to bet however much you like that you’ll eventually hit 1. The Collatz conjecture asserts that this happens for every positive integer, no matter how large. People have tested this empirically for all integers up to at least 2 billion trillion (!) without finding any counterexamples, which strongly suggests that the conjecture is true. But nobody knows how to rigorously prove it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cryptozoology&lt;/head&gt;
    &lt;p&gt;Let’s take a step back. At the beginning of this post I noted a link between the Collatz conjecture and Antihydra: Nobody knows how to prove the Collatz conjecture, and that’s why researchers don’t know how to conclusively determine whether Antihydra halts. But now I’ve instead linked the Collatz conjecture to the fifth busy beaver, a machine that has been proved to halt. What’s going on here?&lt;/p&gt;
    &lt;p&gt;The resolution to this apparent puzzle is that for the busy beaver game, we only care about whether a Turing machine halts when it starts running from a specific tape configuration, namely the blank tape. That means we only care about whether the corresponding Collatz-like sequence halts for a single input. The Collatz conjecture, meanwhile, asks whether you eventually hit \(x = 1\) for every input. It’s easy to show that the Collatz sequence ultimately hits \(x = 1\) for any one input, just as it’s easy to show that the fifth busy beaver halts (once you’ve established an equivalence between its low-level rules and the high-level Collatz-like program).10As it happens, the busy beaver hunters Heiner Marxen and Jürgen Buntrock first proved that the fifth busy beaver halted by direct simulation (albeit with some tricks to speed things up). Michel only identified its high-level behavior after the fact.&lt;/p&gt;
    &lt;p&gt;We can easily construct a variant of the Collatz problem that’s hard to solve even for a single input. All we need to do is change the \(3x + 1\) rule for odd numbers to \(5x + 1\). In that case, trajectories that start from certain inputs (such as \(x = 7\)) look like they will diverge, never hitting 1 or falling into a cycle. But researchers haven’t been able to prove that any of these trajectories diverges. There’s an inherent asymmetry here. If you want to prove that a sequence does eventually end up somewhere, you can always just use brute force, at least in principle. But if you want to prove that a sequence never terminates, even a single input can be hard.&lt;/p&gt;
    &lt;p&gt;We’re now finally ready to confront the terror that is Antihydra. It obeys the following high-level rules:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set \(x = 8\).11This may seem like a weird starting point, given that we’re supposed to start with the blank tape in the busy beaver game. That’s still true here — it’s just that Antihydra spends a while futzing around on the tape before it starts iterating this sequence, and the high-level effect of all that futzing is to set the starting value to 8.&lt;/item&gt;
      &lt;item&gt;Check whether \(x\) is even or odd. &lt;list rend="ul"&gt;&lt;item&gt;If it’s even, calculate \(3x/2\). The result is your new value of \(x\). Add one to a running tally of how many times you’ve applied this even rule.&lt;/item&gt;&lt;item&gt;If it’s odd, calculate \((3x-1)/2\). The result is your new value of \(x\). Add one to a running tally of how many times you’ve applied this odd rule.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Check whether your “odd” count is more than twice as large as your “even” count. If so, halt. If not, go back to step 2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a very curious set of rules. The formulas \(3x/2\) and \((3x-1)/2\) don’t appear to systematically favor odd or even numbers, so you might expect that iterating them again and again will look like repeatedly flipping a coin and keeping track of how often you get heads versus tails. Early on in a sequence of coin flips, it’s distinctly possible that you’ll end up with more than twice as many heads as tails. But if this doesn’t happen right away, it becomes less and less likely the longer you keep going. Researchers have now simulated the behavior of Antihydra out to more than 270 billion steps, and as expected, the “even” and “odd” tallies are pretty close to equal — nowhere near the extreme imbalance demanded by the halting condition. So it seems overwhelmingly likely that Antihydra never halts. But nobody knows how to prove it! The mathematician John Conway coined the delightful term “probviously” for situations like this — ones where the specific problem of interest is very hard to solve, but probabilistic reasoning about the “typical” behavior of similar problems makes the answer seem obvious.&lt;/p&gt;
    &lt;p&gt;Antihydra’s behavior is qualitatively similar to the \(5x + 1\) version of the Collatz conjecture, where we don’t know how to prove that any single trajectory diverges. I want to stress that as far as researchers know, there isn’t a more precise mathematical link between these two problems: If you resolved one of them, it wouldn’t automatically resolve the other. But the problems seem hard for very similar reasons. If someone does manage to prove the Collatz conjecture, the mathematical techniques used in the proof would likely be promising for the Antihydra problem (and vice versa).&lt;/p&gt;
    &lt;p&gt;Actually, Antihydra is just one of many probviously non-halting Turing machines with Collatz-like behavior. Busy beaver hunter Shawn Ligocki dubbed these machines “cryptids” when they were first identified in variants of the standard busy beaver game.12These variants use extra tape symbols in addition to 0 and 1. For example, the BB(3,3) version of the busy beaver game studies the behavior of Turing machines with three rules that can read and write three symbols: 0, 1, and 2.&lt;/p&gt;
    &lt;p&gt;The first two cryptids to be discovered were named Bigfoot and Hydra;13Antihydra was named for a mathematical connection to Hydra. researchers have now identified so many cryptids that it no longer makes sense to give each one its own name. The existence of all these cryptids implies that busy beaver numbers beyond BB(5) will remain out of reach until researchers develop new mathematical tools for tackling Collatz-like problems. And the legendary mathematician Paul Erdős reportedly said “Mathematics may not be ready for such problems.”&lt;/p&gt;
    &lt;p&gt;But that doesn’t mean busy beaver hunters should give up. There’s still plenty of questions to explore in what might be called “cryptid ecology.” How many subspecies of cryptids are there? How are they related to each other, and to other unsolved problems in mathematics beyond the Collatz conjecture? Since the beginning of the busy beaver game, avid hunters have repeatedly encountered surprising new Turing machine behavior, and that pattern shows no sign of letting up.&lt;/p&gt;
    &lt;p&gt;This past August I visited Tahquamenon Falls in Michigan’s upper peninsula, a part of the state that’s apparently an epicenter of bigfoot sightings. Fortunately I didn’t encounter any cryptids, but I did learn some new things about a few friendlier critters. Surprising discoveries can come from anywhere!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723359</guid><pubDate>Mon, 27 Oct 2025 16:56:04 +0000</pubDate></item><item><title>Sieve (YC X25) is hiring engineers to build video datasets for frontier AI</title><link>https://www.sievedata.com/</link><description>&lt;doc fingerprint="7d518f497b3a37b"&gt;
  &lt;main&gt;
    &lt;p&gt;High quality video data for AI applications.&lt;/p&gt;
    &lt;p&gt;500K hours of high quality, diverse video clips.&lt;/p&gt;
    &lt;p&gt;Contact us to request a sample or explore more options.&lt;/p&gt;
    &lt;p&gt;Purpose-built video understanding models paired with human QA help find just the highest quality, training-ready data.&lt;/p&gt;
    &lt;p&gt;Our growing library consists of thousands of petabytes of video data.&lt;/p&gt;
    &lt;p&gt;Video is collected from a variety of public, private, and synthetic sources.&lt;/p&gt;
    &lt;p&gt;New data shapes to unlock new model capabilities (paired, time-synced, conversational, and more).&lt;/p&gt;
    &lt;p&gt;Contact us to request a sample or explore more options.&lt;/p&gt;
    &lt;p&gt;Purpose-built video understanding models paired with human QA help find just the highest quality, training-ready data.&lt;/p&gt;
    &lt;p&gt;Our growing library consists of thousands of petabytes of video data.&lt;/p&gt;
    &lt;p&gt;Video is collected from a variety of public, private, and synthetic sources.&lt;/p&gt;
    &lt;p&gt;New data shapes to unlock new model capabilities (paired, time-synced, conversational, and more).&lt;/p&gt;
    &lt;p&gt;Explore pre-packaged datasets to determine which you are interested in.&lt;/p&gt;
    &lt;p&gt;Enter a purchase agreement based on dataset volume and characteristics.&lt;/p&gt;
    &lt;p&gt;Receive data within 1-2 days via storage bucket access.&lt;/p&gt;
    &lt;p&gt;Scalable API&lt;/p&gt;
    &lt;p&gt;Built to process millions of hours of video at any given moment.&lt;/p&gt;
    &lt;p&gt;Compliant&lt;/p&gt;
    &lt;p&gt;Request specific filtering and licensing needs to ensure full permission and compliance of your training data.&lt;/p&gt;
    &lt;p&gt;Dedicated partnership&lt;/p&gt;
    &lt;p&gt;We partner deeply with every research team to understand their needs and develop data with the same rigor they develop models.&lt;/p&gt;
    &lt;p&gt;Secure&lt;/p&gt;
    &lt;p&gt;End-to-end encryption, custom data retention, and SOC 2 Type 2 secured.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723426</guid><pubDate>Mon, 27 Oct 2025 17:01:05 +0000</pubDate></item><item><title>Avoid 2:00 and 3:00 am cron jobs (2013)</title><link>https://www.endpointdev.com/blog/2013/04/avoid-200-and-300-am-cron-jobs/</link><description>&lt;doc fingerprint="fe50537ca9a4b4f6"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Avoid 2:00 and 3:00 am cron jobs!&lt;/head&gt;
    &lt;p&gt;A word to the wise: Do not set any cron jobs for 2:00 am or 3:00 am on Sunday morning! Or to be safe, on other mornings besides Sunday as well, since jobs originally set to run on some particular day may eventually be changed to run on another day, or every day.&lt;/p&gt;
    &lt;p&gt;Most of the time such cron jobs will run fine, but if they run every Sunday morning, then twice per year they will run at the exact time daylight savings time (aka summer time) kicks in or ends, sometimes with very strange results.&lt;/p&gt;
    &lt;p&gt;On Linux with vixie-cron we saw two cron jobs run something like once per second between 3:00 and 3:01 when the most recent daylight savings time began. Thus they ran about 60 times, stepping all over each other and making a noisy mess in email. No serious harm was done, but thatâs only because they were not tasks capable of causing serious harm.&lt;/p&gt;
    &lt;p&gt;Feel free to wish for or agitate for or fund or write a better open source job scheduler that everyone will use, one that will ensure no overlapping runs, allow specifying time limits, etc. Better tools exist, but until one of them achieves cronâs level of ubiquity, we have to live with cron at least some places and sometimes.&lt;/p&gt;
    &lt;p&gt;Alternatively, where possible set the server timezone to UTC so that no daylight savings changes will happen at all.&lt;/p&gt;
    &lt;p&gt;Or most preferable: Governments of the world, stop the twice-yearly dance of daylight saving time altogether.&lt;/p&gt;
    &lt;p&gt;But in the meantime this particular problem can be entirely avoided by just not scheduling any cron jobs to run on Sunday morning at 2:00 or 3:00 server time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723554</guid><pubDate>Mon, 27 Oct 2025 17:08:33 +0000</pubDate></item><item><title>Show HN: Dlog – Journaling and AI coach that learns what drives wellbeing (Mac)</title><link>https://dlog.pro/</link><description>&lt;doc fingerprint="2e16e0446df4819b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Know yourself&lt;/head&gt;
    &lt;head rend="h6"&gt;Personal Science&lt;/head&gt;
    &lt;head rend="h2"&gt;Could this be the world’s most advanced AI journal and projects app?&lt;/head&gt;
    &lt;head rend="h6"&gt;You Decide&lt;/head&gt;
    &lt;head rend="h4"&gt;Pricing&lt;/head&gt;
    &lt;head rend="h6"&gt;Dlog is free for the first 14 days, and comes with 10,000 free tokens. The price is then 1.99 per month, plus tokens. If you purchase within 24 hours you get 1 million free tokens (normally 5.99 / million).&lt;/head&gt;
    &lt;head rend="h4"&gt;Quick Start Video&lt;/head&gt;
    &lt;head rend="h3"&gt;Harness personal strengths and understand the drivers behind your productivity and wellbeing.&lt;/head&gt;
    &lt;p&gt;Built on a decade of research. Dlog learns your personality, standards and constraints, then recommends precise steps to increase resources, strengthen character and raise well-being.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Dlog works.&lt;/head&gt;
    &lt;p&gt;You answer a baseline survey about your personality, character, resources, and well-being. Every journal and project you add is scored against that same survey. Then, Dlog’s AI applies regressions and structural equation modelling to map how one change drives another (e.g., habits, people or traits influencing mood, progress or resources&lt;/p&gt;
    &lt;p&gt;But it’s not just numbers. Dlog also runs a narrative analysis, tying those stats back to your own diary quotes.&lt;/p&gt;
    &lt;head rend="h4"&gt;So, when you ask why your mood dipped last week, Dlog can show: ‘Person X drove stress — and here’s what you wrote that day.’&lt;/head&gt;
    &lt;head rend="h4"&gt;Journaling in Dlog turns self-reflection into personal science. Achieve your goals with an AI coach informed by you.&lt;/head&gt;
    &lt;head rend="h6"&gt;Mixed Methodology&lt;/head&gt;
    &lt;head rend="h4"&gt;Dlog uses both narrative ↑ and ↓ quantitative analysis to optimise your well-being and resources.&lt;/head&gt;
    &lt;head rend="h3"&gt;“The backend of Dlog contains a model that links personality, well-being, character and resources; scores are generated by what you’ve written, as determined by ChatGPT 5 and your baseline survey. This gives a rich and vast dataset with which coach responses are informed.”&lt;/head&gt;
    &lt;head rend="h4"&gt;Dr J. de Borst, Dlog Developer and Scientist.&lt;/head&gt;
    &lt;p&gt;From this analysis Dlog’s Coach suggests an array of highly personalised and context aware recommendations, that you can take into consideration when maximising your productivity and well-being.&lt;/p&gt;
    &lt;head rend="h1"&gt;Customise&lt;/head&gt;
    &lt;head rend="h6"&gt;Customize the Experience&lt;/head&gt;
    &lt;head rend="h2"&gt;the Dlog Aesthetic.&lt;/head&gt;
    &lt;head rend="h4"&gt;Crafted with deep care and precision, it’s an app you’ll want to return to throughout the day.&lt;/head&gt;
    &lt;head rend="h3"&gt;Select from vibrant colors or gorgeous gradients.&lt;/head&gt;
    &lt;head rend="h1"&gt;4 Rings&lt;/head&gt;
    &lt;head rend="h6"&gt;Keeping you on track&lt;/head&gt;
    &lt;head rend="h2"&gt;The four rings guide Dlog as it analyses your journals to see if you’re on track.&lt;/head&gt;
    &lt;head rend="h3"&gt;The four rings are Dlog’s anchor for both narrative analysis and quants analysis.&lt;/head&gt;
    &lt;p&gt;The survey that you take when you start using the coach spans the four rings and acts as your baseline. Future journals and projects are scored against the baseline. Regressions and structural equation modelling are used to identify the factors in your life that drive changes in your productivity and wellbeing.&lt;/p&gt;
    &lt;p&gt;All guidance provided by Dlog’s Coach is based on both sentiment analysis of your Dlog Journals (to give scores) and narrative analysis; to give rich context.&lt;/p&gt;
    &lt;head rend="h1"&gt;Goals&lt;/head&gt;
    &lt;head rend="h6"&gt;Universal Truths&lt;/head&gt;
    &lt;head rend="h2"&gt;it all starts with a goal.&lt;/head&gt;
    &lt;head rend="h3"&gt;Dlog puts all your productivity tools into one place. Integrating goals, projects, journals and reminders into your calandar.&lt;/head&gt;
    &lt;p&gt;The secret to productivity and well-being is setting ourselves challenges that are neither too easy nor too simple for our abilities.&lt;/p&gt;
    &lt;p&gt;Journals and Projects are organised by goal. Ranging from short term to life goals. Dlog makes sure you don’t miss a beat.&lt;/p&gt;
    &lt;head rend="h4"&gt;Through regular, guided journaling and accountable projects, Dlog helps you achieve your goals with an AI coach informed by your personal data analysed through the Dlog Model.&lt;/head&gt;
    &lt;head rend="h1"&gt;Journals&lt;/head&gt;
    &lt;head rend="h6"&gt;Guided Journaling&lt;/head&gt;
    &lt;head rend="h2"&gt;Reflect and understand yourself on a deeper level.&lt;/head&gt;
    &lt;head rend="h3"&gt;Work through the 4 constructs, as you understand yourself and reflect.&lt;/head&gt;
    &lt;p&gt;The benefits of journaling are well documented. Dlog serves to enhance these benefits with its aesthetic interface and unique AI model. Delivering a journaling experience that is intuitive and in line with the modern needs of our technological societies.&lt;/p&gt;
    &lt;head rend="h4"&gt;Journals are scored, data is gathered and stored on device, and deep insights are gained through personal science.&lt;/head&gt;
    &lt;head rend="h4"&gt;Journals are stored in your calander and viewed in the journal browser. Like projects they can be attached to a goal and have their own reminders list.&lt;/head&gt;
    &lt;head rend="h1"&gt;Projects&lt;/head&gt;
    &lt;head rend="h6"&gt;Build Character, Complete Projects&lt;/head&gt;
    &lt;head rend="h2"&gt;Projects designed to achieve your goals.&lt;/head&gt;
    &lt;head rend="h3"&gt;Dlog makes organising project notes intuitive and simple with it’s goal centric system.&lt;/head&gt;
    &lt;p&gt;On the Dlog home screen you can select from a project or journal. Project entries are linked to your calander and each entry is assigned to a project file. Every project has its own reminders list to keep you on track.&lt;/p&gt;
    &lt;head rend="h4"&gt;In the project manager you can view, edit and assign projects to goals.&lt;/head&gt;
    &lt;head rend="h1"&gt;Dlog Coach&lt;/head&gt;
    &lt;head rend="h6"&gt;Guided Journaling&lt;/head&gt;
    &lt;head rend="h2"&gt;merging chat with personal statistics you can count on.&lt;/head&gt;
    &lt;head rend="h3"&gt;Dlog’s coach reveals how your personality, interacts with your character, resources and well-being. Uncovering the hidden drivers behind your productivity and well-being.&lt;/head&gt;
    &lt;p&gt;Coach has access to your journals and projects, and uses mixed methods analysis to gather insight into your life patterns.&lt;/p&gt;
    &lt;p&gt;Ask coach about your relationships, work habits, why your feeling stressed, your goals, or simply to review your week. Coach answers with your baseline in mind so that every response is personal to you and deeply contextualised.&lt;/p&gt;
    &lt;head rend="h4"&gt;Through regular journaling and project entries, Dlog helps you achieve your goals with an AI coach informed by your personal data and analysed through the Dlog Model.&lt;/head&gt;
    &lt;head rend="h6"&gt;Do something&lt;/head&gt;
    &lt;head rend="h3"&gt;Coach suggests an array of highly personalised and context aware recommendations, that you can take into consideration when maximising your productivity and well-being.&lt;/head&gt;
    &lt;p&gt;The coach is action oriented, and will suggest goals, nudges/reminders or projects using contextual data in relation to the 4 rings.&lt;/p&gt;
    &lt;head rend="h1"&gt;Words&lt;/head&gt;
    &lt;head rend="h6"&gt;Your Reality&lt;/head&gt;
    &lt;head rend="h2"&gt;Expore visualizers with data about you.&lt;/head&gt;
    &lt;head rend="h3"&gt;Your language is more than just data.&lt;/head&gt;
    &lt;p&gt;When you journal in Dlog your words aren’t just saved, they’re organised into themes derived from the Four Rings (Personality, Character, Resources, Well-Being). This process of thematic analysis turns raw text into a view of your life narrative.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dlog brings you back to the moment – through your own words.&lt;/head&gt;
    &lt;head rend="h4"&gt;Dlog provides a birds-eye view of your life patterns.&lt;/head&gt;
    &lt;p&gt;Your words appear where you need them. In weekly reviews and life reports, when a trend appears, Dlog links that trend to the original journal and project quotes that explain it, so that your insights are grounded in you.&lt;/p&gt;
    &lt;head rend="h4"&gt;Words have power and Dlog examines your journals using thematic analysis, including axial coding of entries that are then organised into themes and sub themes derived from the 4-Rings.&lt;/head&gt;
    &lt;head rend="h6"&gt;Word Cloud&lt;/head&gt;
    &lt;head rend="h4"&gt;Mentions of people, places and time markers are connected to variables such as mood, income, sense of purpose etc.&lt;/head&gt;
    &lt;head rend="h1"&gt;Numbers&lt;/head&gt;
    &lt;head rend="h6"&gt;Deep Fulfilment&lt;/head&gt;
    &lt;head rend="h2"&gt;Your numbers signal life patterns.&lt;/head&gt;
    &lt;head rend="h3"&gt;Discover the hidden factors driving your life across the Four Rings (Personality, Resources, Character, Well-Being).&lt;/head&gt;
    &lt;p&gt;Everyone’s Productivity and Well-Being is unique, that’s why generic insight isn’t insight at all. Dlog understands your context through the four rings and then scores your projects and journals against the same variables. Each entry receives sentiment scores that are compared to your baseline survey, showing where you are progressing or regressing, and why.&lt;/p&gt;
    &lt;p&gt;Trends and relationships between variables are displayed in time-series charts and scatter plots with regression lines – so you can see what drives the changes in your Productivity and Well-Being over time.&lt;/p&gt;
    &lt;head rend="h6"&gt;Understand What Drives Your Well-Being&lt;/head&gt;
    &lt;head rend="h4"&gt;How does extraversion (Personality) react with your Well-Being?&lt;/head&gt;
    &lt;head rend="h6"&gt;Your Happiness over Time&lt;/head&gt;
    &lt;head rend="h4"&gt;How did your happiness trend over the past week?&lt;/head&gt;
    &lt;head rend="h6"&gt;Distribution of your Happiness&lt;/head&gt;
    &lt;head rend="h4"&gt;Dlog shows how your life over the past week or month compares to your baseline, and explains the drivers behind any change.&lt;/head&gt;
    &lt;head rend="h4"&gt;Built on a decade of research. Dlog learns your personality, standards, and constraints. It recommends precise steps that increase resources, strengthen character, and raise the quality of your well-being—through personal science.&lt;/head&gt;
    &lt;head rend="h1"&gt;Personal Science&lt;/head&gt;
    &lt;head rend="h6"&gt;For your eyes only&lt;/head&gt;
    &lt;head rend="h2"&gt;A study of you, for you&lt;/head&gt;
    &lt;head rend="h3"&gt;Dlog produces a PhD grade article using mixed methods about you; just for your eyes.&lt;/head&gt;
    &lt;p&gt;Combining both thematic narrative analysis as well as regression and structured equation model analysis (see the Dlog Model below), the Dlog will produce a comprehensive academic article to help you understand your reality.&lt;/p&gt;
    &lt;head rend="h4"&gt;Narrative analysis is linked closely to the SEM model; i.e., themes are derived from the four constructs; and initial axial coding is grouped into constructs that are recognisable from the 4 Rings framework. The result is a rich and deeply insightful analysis that has significantly more depth than a simple LLM analysis; no AI on its own can compete with this human centric model and narrative analysis, no matter how much memory or parameters it may contain.&lt;/head&gt;
    &lt;head rend="h1"&gt;Dlog Model&lt;/head&gt;
    &lt;head rend="h6"&gt;Proprietary Model&lt;/head&gt;
    &lt;head rend="h2"&gt;How personality and character drive well-being and income&lt;/head&gt;
    &lt;head rend="h3"&gt;The Dlog model is the backbone of the quantitative and qualitative analysis.&lt;/head&gt;
    &lt;p&gt;Dlog’s private AI model is based on our lead scientist’s PhD and bolstered by decades of research into Productivity and Well-Being, mapping your life across the four constructs – Personality, Character, Resources and Well-Being.&lt;/p&gt;
    &lt;p&gt;When you first use the coach, you answer a baseline survey of the four constructs, so that Dlog knows where you have been over the past year. As you journal and work on projects, your entries are scored against the same baseline survey so that you can see if you are above or below your norm in any of the four rings.&lt;/p&gt;
    &lt;p&gt;From there, Dlog’s AI suggests next steps that you can choose from in order to maximise your personal Productivity and Well-Being.&lt;/p&gt;
    &lt;head rend="h4"&gt;Through regular, guided journaling and accountable projects, Dlog helps you achieve your goals with an AI coach informed by your personal data analysed through the Dlog Model.&lt;/head&gt;
    &lt;p&gt;Regressions and structural equation modelling (SEM) are then used to trace directed paths between constructs, showing how changes in one area drives changes in another (e.g., habits, people or traits influencing mood, progress or resources). Dlog focuses on the most informative positive and negative paths, and includes a Quality of Well-Being dimension to distinguish hollow wins from integrated fulfilment.&lt;/p&gt;
    &lt;p&gt;Numbers are never shown without context. Trends and relationships are paired with diary quotes, bringing you back to the moment and grounding the insights in you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723646</guid><pubDate>Mon, 27 Oct 2025 17:14:34 +0000</pubDate></item><item><title>The new calculus of AI-based coding</title><link>https://blog.joemag.dev/2025/10/the-new-calculus-of-ai-based-coding.html</link><description>&lt;doc fingerprint="375020776f1115df"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Driving at 200mph&lt;/head&gt;
    &lt;p&gt;Here's where it gets interesting. A typical software team, even an experienced one, doesn't get things right all the time. Even with good testing and engineering practices, bugs occasionally make it through. We've all heard the phrase "testing in production." That reality is the main reason I've always believed that focusing on testing alone is not enough, and investing in blast radius and time to recovery is just as important.&lt;/p&gt;
    &lt;p&gt;AI assisted code is no different, it may contain bugs even when thoroughly reviewed by a human, and I suspect the probabilities are not significantly different. However, when teams ship commits at 10x the rate, the overall math changes. What used to be a production impacting bug once or twice a year, can become a weekly occurrence. Even if most bugs get caught in integration or testing environments, they will still impact the shared code base, requiring investigation and slowing the rest of the team down. Once again, this is not just hyperbole—our team sees signs that these are the challenges that pop up with a step function increase in throughput.&lt;/p&gt;
    &lt;p&gt;I am increasingly convinced that in order for agentic development to increase engineering velocity by an order of magnitude, we need to decrease the probability of problematic commits by an order of magnitude too. And likely by even more than that, since at high velocities individual commits can begin interacting with each other in unexpected ways too.&lt;/p&gt;
    &lt;p&gt;In other words, driving at 200mph, you need a lot of downforce to keep the car on the track!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost-Benefit Rebalance&lt;/head&gt;
    &lt;p&gt;One of the best ways to reduce the chance of bugs is to improve testing. I'm an airplane geek, and have always admired the testing ideas used by the airplane manufacturers. From early simulations, to component testing, to wind tunnel testing, to testing to breaking point, and ultimately test flights of fully assembled aircraft. Even flight simulators play a role in improving the overall safety of the industry. Some of these ideas have been tried in the software industry, but they are far from ubiquitous.&lt;/p&gt;
    &lt;p&gt;As an example, I've always liked "wind tunnel" style tests, that test fully assembled system in a controlled environment. To achieve that, one pattern I've used is implementing high fidelity "fake" versions of external dependencies that can be run locally. If you do that, you can then write build-time tests that run locally and verify end-to-end behavior of the whole system. You can even inject unexpected behaviors and failures into fake dependencies, to test how the system handles them. Such tests are easy to write and execute because they run locally, and they are great at catching those sneaky bugs in the seams between components.&lt;/p&gt;
    &lt;p&gt;Unfortunately, faking all the external dependencies isn't always easy for a service with moderate level of complexity. And even if you do, you now have to own keeping up with the real dependencies as they evolve. For those reasons, in my experience most teams don't write such tests.&lt;/p&gt;
    &lt;p&gt;I think we are seeing early signs that agentic coding can change the calculus here. AI agents are great at spitting out large volumes of code, especially when the desired behavior is well known and there's little ambiguity. Ideas that were sound in principle, but too expensive to implement and maintain just had their costs decrease by an order of magnitude. I really love riding such shifts in the industry, because they open the doors to new approaches that weren't practical in the past.&lt;/p&gt;
    &lt;p&gt;Our project (with the help of an AI agent) maintains fake implementations of external dependencies like authentication, storage, chain replication, and inference engine to be used in tests. We then wrote a test harness that uses those fakes to spin up our entire distributed system, including all the micro-services, on developers' machines. Build-time tests then spin up our canaries against that fully assembled stack verifying the system as a whole works.&lt;/p&gt;
    &lt;p&gt;I'm really bullish on this approach catching a category of bugs that in the past could only be caught once the change was committed and made it to the test environment. A few years ago, ideas like these would receive resistance as nice, but too expensive. This time around, it took just a few days to implement for a relatively complex system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Driving Fast Requires Tighter Feedback Loop&lt;/head&gt;
    &lt;p&gt;Agentic coding changes that dynamic. In the amount of time it takes to build, package, and test one set of commits, another dozen might be waiting to go out. By the time a change set is ready to deploy to production, it may contain 100 or more commits. And if one of those commits contains a problem, the deployment needs to be rolled back grinding the pipeline to a halt. In the meantime, even more changes accumulate, adding to the chaos and the risk.&lt;/p&gt;
    &lt;p&gt;I'm a Formula 1 fan, and this reminds me of how an accident on the track can cause a Yellow Flag to be raised. Normally, the cars zoom around the track at immense speeds and accelerations. But if an accident occurs, the race marshals raise a yellow flag, which requires all the cars to slow down behind the pace car. An exciting race turns into a leisurely drive around the track until the debris is cleaned up and the track is safe again. To minimize such slow downs, race organizers go to great lengths to prepare for all types of accidents, and make sure they can clean up the track and restart the race in minutes.&lt;/p&gt;
    &lt;p&gt;Just like whole-system local tests help tighten the feedback loop for catching certain bugs, we may need to think similarly about how we implement our CICD pipelines. When teams are moving at the speed of dozen of commits per hour, problematic issues will need to be identified, isolated, and reverted in minutes instead of hours or days. That means that a typical build and test infrastructure will need to become an order of magnitude faster than it is today. Just like online video games become unplayable when there is high lag between player's inputs and the game's reaction, it's really hard to move 10x faster if every commit still requires a lengthy delay before you see the feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;The communication bottleneck&lt;/head&gt;
    &lt;p&gt;I enjoy observing well-run operations. If you've ever peeked behind the curtain of a busy restaurant, then at first sight you may think it's chaos. But if you take a second to notice the details, you'll see that all members are constantly coordinating with each other. Chefs, cooks, wait staff, bussers, and managers pass information back and forth in a continuous stream. By staying in constant sync, a well run restaurant manages to serve its patrons even during peak times, without sacrificing on quality or latency.&lt;/p&gt;
    &lt;p&gt;I believe that achieving similar increase in velocity for a software team requires constraints on how teams communicate. When your throughput increases by an order of magnitude, you're not just writing more code - you're making more decisions. Should we use this caching strategy or that one? How should we handle this edge case? What's the right abstraction here? At normal velocity, a team might make one or two of these decisions per week. At 10x velocity, they are making multiple each day.&lt;/p&gt;
    &lt;p&gt;The challenge is that many of these decisions impact what others are working on. Engineer A decides to refactor the authentication flow, which affects the API that Engineer B is about to extend. These aren't just implementation details - they're architectural choices that ripple through the codebase.&lt;/p&gt;
    &lt;p&gt;I find that traditional coordination mechanisms introduce too much latency here. Waiting for a Slack response or scheduling a quick sync for later in the day means either creating a bottleneck - the decision blocks progress - or risking going down the wrong path before realizing the conflict. At high throughput, the cost of coordination can dominate!&lt;/p&gt;
    &lt;p&gt;One approach is to eliminate coordination - if everybody works on independent components, they are unlikely to need to coordinate. But I find that ideal impractical in most real-world systems. So another alternative is to significantly decrease the cost of coordination. Our team sits on the same floor, and I think that's been critical to our velocity. When someone needs to make a decision that might impact others, they can walk over and hash it out in minutes in front of a whiteboard. We align on the approach, discuss trade-offs in real time, and both engineers get back to work. The decision gets made quickly, correctly, and without creating a pile-up of blocked work.&lt;/p&gt;
    &lt;p&gt;I recognize this doesn't solve the problem for distributed teams—that remains an open challenge.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Path Forward&lt;/head&gt;
    &lt;p&gt;I'm really excited about the potential of agentic development. I think it has the capability to not only improve the efficiency of software development, but also allow us to tackle problems that were previously too niche or expensive to solve. The gains are real - our team's 10x throughput increase isn't theoretical, it's measurable.&lt;/p&gt;
    &lt;p&gt;But here's the critical part: these gains won't materialize if we simply bolt AI agents onto our existing development practices. Like adding a turbocharger to a car with narrow tires and old brakes, the result won't be faster lap times - it will be crashes. At 10x code velocity, our current approaches to testing, deployment, and team coordination become the limiting factors. The bottleneck just moves.&lt;/p&gt;
    &lt;p&gt;This means we need to fundamentally rethink how we approach building software. CICD pipelines designed for 10 commits per day will buckle under 100. Testing strategies that were "good enough" at normal velocity will let too many bugs through at high velocity. Communication patterns that worked fine before will create constant pile-ups of blocked work.&lt;/p&gt;
    &lt;p&gt;The good news is that we already have great ideas for comprehensive testing, rapid deployment, and efficient coordination - ideas that have shown promise but haven't seen wide adoption because they were too expensive to implement and maintain. What's changed is that agentic development itself can dramatically lower those costs. The same AI agents that are increasing our code throughput can also help us build the infrastructure needed to sustain that throughput.&lt;/p&gt;
    &lt;p&gt;This is the real opportunity: not just writing more code faster, but using AI to make previously impractical engineering practices practical. The teams that succeed with agentic development will be the ones who recognize that the entire software development lifecycle needs to evolve in concert.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723686</guid><pubDate>Mon, 27 Oct 2025 17:17:38 +0000</pubDate></item><item><title>MCP-Scanner – Scan MCP Servers for vulnerabilities</title><link>https://github.com/cisco-ai-defense/mcp-scanner</link><description>&lt;doc fingerprint="1d5afd5c98027a90"&gt;
  &lt;main&gt;
    &lt;p&gt;A Python tool for scanning MCP (Model Context Protocol) servers and tools for potential security findings. The MCP Scanner combines Cisco AI Defense inspect API, YARA rules and LLM-as-a-judge to detect malicious MCP tools.&lt;/p&gt;
    &lt;p&gt;The MCP Scanner provides a comprehensive solution for scanning MCP servers and tools for security findings. It leverages three powerful scanning engines (Yara, LLM-as-judge, Cisco AI Defense) that can be used together or independently.&lt;/p&gt;
    &lt;p&gt;The SDK is designed to be easy to use while providing powerful scanning capabilities, flexible authentication options, and customization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple Modes: Run scanner as a stand-alone CLI tool or REST API server&lt;/item&gt;
      &lt;item&gt;Multi-Engine Security Analysis: Use all three scanning engines together or independently based on your needs.&lt;/item&gt;
      &lt;item&gt;Comprehensive Scanning: Scan MCP tools, prompts, and resources for security findings&lt;/item&gt;
      &lt;item&gt;Explicit Authentication Control: Fine-grained control over authentication with explicit Auth parameters.&lt;/item&gt;
      &lt;item&gt;OAuth Support: Full OAuth authentication support for both SSE and streamable HTTP connections.&lt;/item&gt;
      &lt;item&gt;Custom Endpoints: Configure the API endpoint to support any Cisco AI Defense environments.&lt;/item&gt;
      &lt;item&gt;MCP Server Integration: Connect directly to MCP servers to scan tools, prompts, and resources with flexible authentication.&lt;/item&gt;
      &lt;item&gt;Customizable YARA Rules: Add your own YARA rules to detect specific patterns.&lt;/item&gt;
      &lt;item&gt;Comprehensive Reporting: Detailed reports on detected security findings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.11+&lt;/item&gt;
      &lt;item&gt;uv (Python package manager)&lt;/item&gt;
      &lt;item&gt;A valid Cisco AI Defense API Key (optional)&lt;/item&gt;
      &lt;item&gt;LLM Provider API Key (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;uv venv -p &amp;lt;Python version less than or equal to 3.13&amp;gt; /path/to/your/choice/of/venv/directory
source /path/to/your/choice/of/venv/directory/bin/activate
uv pip install cisco-ai-mcp-scanner&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/cisco-ai-defense/mcp-scanner
cd mcp-scanner
# Install with uv (recommended)

uv venv -p &amp;lt;Python version less than or equal to 3.13&amp;gt; /path/to/your/choice/of/venv/directory

source /path/to/your/choice/of/venv/directory/bin/activate

uv pip install .
# Or install in development mode
uv pip install -e .&lt;/code&gt;
    &lt;code&gt;Cisco AI Defense API (only required for API analyzer)
export MCP_SCANNER_API_KEY="your_cisco_api_key"
export MCP_SCANNER_ENDPOINT="https://us.api.inspect.aidefense.security.cisco.com/api/v1"
# For other endpoints please visit https://developer.cisco.com/docs/ai-defense/getting-started/#base-url&lt;/code&gt;
    &lt;p&gt;Tested LLMs: OpenAI GPT-4o and GPT-4.1&lt;/p&gt;
    &lt;code&gt;# AWS Bedrock Claude with AWS credentials (profile)
export AWS_PROFILE="your-profile"
export AWS_REGION="us-east-1"
export MCP_SCANNER_LLM_MODEL="bedrock/anthropic.claude-sonnet-4-5-20250929-v2:0" # Any AWS Bedrock supported model

# AWS Bedrock Claude with API key (Bearer token)
export MCP_SCANNER_LLM_API_KEY="bedrock-api-key-..." # Generated via Amazon Bedrock -&amp;gt; API Keys
export AWS_REGION="us-east-1"
export MCP_SCANNER_LLM_MODEL="bedrock/us.anthropic.claude-sonnet-4-5-20250929-v2:0" # Any AWS Bedrock supported model

# LLM Provider API Key (required for LLM analyzer)
export MCP_SCANNER_LLM_API_KEY="your_llm_api_key"  # OpenAI

# LLM Model Configuration (optional - defaults provided)
export MCP_SCANNER_LLM_MODEL="gpt-4o"  # Any LiteLLM-supported model
export MCP_SCANNER_LLM_BASE_URL="https://api.openai.com/v1"  # Custom LLM endpoint
export MCP_SCANNER_LLM_API_VERSION="2024-02-01"  # API version (if required)

# For Azure OpenAI (example)
export MCP_SCANNER_LLM_BASE_URL="https://your-resource.openai.azure.com/"
export MCP_SCANNER_LLM_API_VERSION="2024-02-01"
export MCP_SCANNER_LLM_MODEL="azure/gpt-4"

# For Extended Thinking Models (longer timeout)
export MCP_SCANNER_LLM_TIMEOUT=300&lt;/code&gt;
    &lt;p&gt;If you are using a local LLM endpoint such as Ollama, vLLM, or LocalAI, the &lt;code&gt;MCP_SCANNER_LLM_API_KEY&lt;/code&gt; variable is still required but can be set to any value.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;export MCP_SCANNER_LLM_API_KEY=test
export MCP_SCANNER_LLM_ENDPOINT=http://localhost:11434&lt;/code&gt;
    &lt;p&gt;The fastest way to get started is using the &lt;code&gt;mcp-scanner&lt;/code&gt; CLI command. Global flags (like &lt;code&gt;--analyzers&lt;/code&gt;, &lt;code&gt;--format&lt;/code&gt;, etc.) must be placed before a subcommand.&lt;/p&gt;
    &lt;code&gt;# Scan well-known client configs on this machine
mcp-scanner --scan-known-configs --analyzers yara --format summary

# Stdio server (example using uvx mcp-server-fetch)
mcp-scanner --stdio-command uvx --stdio-arg=--from --stdio-arg=mcp-server-fetch --stdio-arg=mcp-server-fetch --analyzers yara --format summary

# Remote server (deepwiki example)
mcp-scanner --server-url https://mcp.deepwki.com/mcp --analyzers yara --format summary

# MCP Scanner as REST API
mcp-scanner-api --host 0.0.0.0 --port 8080
&lt;/code&gt;
    &lt;code&gt;import asyncio
from mcpscanner import Config, Scanner
from mcpscanner.core.models import AnalyzerEnum

async def main():
    # Create configuration with your API keys
    config = Config(
        api_key="your_cisco_api_key",
        llm_provider_api_key="your_llm_api_key"
    )

    # Create scanner
    scanner = Scanner(config)

    # Scan all tools on a remote server
    tool_results = await scanner.scan_remote_server_tools(
        "https://mcp.deepwki.com/mcp",
        analyzers=[AnalyzerEnum.API, AnalyzerEnum.YARA, AnalyzerEnum.LLM]
    )

    # Print tool results
    for result in tool_results:
        print(f"Tool: {result.tool_name}, Safe: {result.is_safe}")

    # Scan all prompts on a server
    prompt_results = await scanner.scan_remote_server_prompts(
        "http://127.0.0.1:8000/mcp",
        analyzers=[AnalyzerEnum.LLM]
    )

    # Print prompt results
    for result in prompt_results:
        print(f"Prompt: {result.prompt_name}, Safe: {result.is_safe}")

    # Scan all resources on a server
    resource_results = await scanner.scan_remote_server_resources(
        "http://127.0.0.1:8000/mcp",
        analyzers=[AnalyzerEnum.LLM],
        allowed_mime_types=["text/plain", "text/html"]
    )

    # Print resource results
    for result in resource_results:
        print(f"Resource: {result.resource_name}, Safe: {result.is_safe}, Status: {result.status}")

# Run the scanner
asyncio.run(main())&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;remote: scan a remote MCP server (SSE or streamable HTTP). Supports &lt;code&gt;--server-url&lt;/code&gt;, optional&lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;stdio: launch and scan a stdio MCP server. Requires &lt;code&gt;--stdio-command&lt;/code&gt;; accepts&lt;code&gt;--stdio-args&lt;/code&gt;,&lt;code&gt;--stdio-env&lt;/code&gt;, optional&lt;code&gt;--stdio-tool&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;config: scan servers from a specific MCP config file. Requires &lt;code&gt;--config-path&lt;/code&gt;; optional&lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;known-configs: scan servers from well-known client config locations on this machine; optional &lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;prompts: scan prompts on an MCP server. Requires &lt;code&gt;--server-url&lt;/code&gt;; optional&lt;code&gt;--prompt-name&lt;/code&gt;,&lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;resources: scan resources on an MCP server. Requires &lt;code&gt;--server-url&lt;/code&gt;; optional&lt;code&gt;--resource-uri&lt;/code&gt;,&lt;code&gt;--mime-types&lt;/code&gt;,&lt;code&gt;--bearer-token&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Top-level flags (e.g., &lt;code&gt;--server-url&lt;/code&gt;, &lt;code&gt;--stdio-*&lt;/code&gt;, &lt;code&gt;--config-path&lt;/code&gt;, &lt;code&gt;--scan-known-configs&lt;/code&gt;) remain supported when no subcommand is used, but subcommands are recommended.&lt;/p&gt;
    &lt;code&gt;# YARA-only scan of all servers defined in well-known config locations
mcp-scanner --scan-known-configs --analyzers yara --format summary

# Detailed output
mcp-scanner --scan-known-configs --analyzers yara --detailed&lt;/code&gt;
    &lt;code&gt;# Expand ~ yourself if needed by your shell
mcp-scanner --config-path "$HOME/.codeium/windsurf/mcp_config.json" \
 --analyzers yara --format by_tool&lt;/code&gt;
    &lt;code&gt;# Use repeated --stdio-arg for reliable argument passing
mcp-scanner --analyzers yara --format summary \
  stdio --stdio-command uvx \
  --stdio-arg=--from --stdio-arg=mcp-server-fetch --stdio-arg=mcp-server-fetch

# Or list-form (ensure it doesn't conflict with later flags)
mcp-scanner --analyzers yara --detailed \
  stdio --stdio-command uvx \
  --stdio-args --from mcp-server-fetch mcp-server-fetch

# Scan only a specific tool on the stdio server
mcp-scanner --analyzers yara --format summary \
  stdio --stdio-command uvx \
  --stdio-arg=--from --stdio-arg=mcp-server-fetch --stdio-arg=mcp-server-fetch \
  --stdio-tool fetch&lt;/code&gt;
    &lt;code&gt;# Direct remote server with Bearer token
mcp-scanner --analyzers yara --format summary \
  remote --server-url https://your-mcp-server/sse --bearer-token "$TOKEN"

# Apply Bearer token to all remote servers discovered from configs
mcp-scanner --analyzers yara --detailed known-configs --bearer-token "$TOKEN"
mcp-scanner --analyzers yara --format by_tool \
  config --config-path "$HOME/.codeium/windsurf/mcp_config.json" --bearer-token "$TOKEN"&lt;/code&gt;
    &lt;code&gt;# Scan all prompts on an MCP server
mcp-scanner --analyzers llm prompts --server-url http://127.0.0.1:8000/mcp

# Scan all prompts with detailed output
mcp-scanner --analyzers llm --detailed prompts --server-url http://127.0.0.1:8000/mcp

# Scan all prompts with table format
mcp-scanner --analyzers llm --format table prompts --server-url http://127.0.0.1:8000/mcp

# Scan a specific prompt by name
mcp-scanner --analyzers llm prompts --server-url http://127.0.0.1:8000/mcp --prompt-name "greet_user"

# Get raw JSON output
mcp-scanner --analyzers llm --raw prompts --server-url http://127.0.0.1:8000/mcp&lt;/code&gt;
    &lt;code&gt;# Scan all resources on an MCP server
mcp-scanner --analyzers llm resources --server-url http://127.0.0.1:8000/mcp

# Scan all resources with detailed output
mcp-scanner --analyzers llm --detailed resources --server-url http://127.0.0.1:8000/mcp

# Scan all resources with table format
mcp-scanner --analyzers llm --format table resources --server-url http://127.0.0.1:8000/mcp

# Scan a specific resource by URI
mcp-scanner --analyzers llm resources --server-url http://127.0.0.1:8000/mcp \
  --resource-uri "file://test/document.txt"

# Scan with custom MIME type filtering
mcp-scanner --analyzers llm resources --server-url http://127.0.0.1:8000/mcp \
  --mime-types "text/plain,text/html,application/json"&lt;/code&gt;
    &lt;p&gt;The API server provides a REST interface to the MCP scanner functionality, allowing you to integrate security scanning into web applications, CI/CD pipelines, or other services. It exposes the same scanning capabilities as the CLI tool but through HTTP endpoints.&lt;/p&gt;
    &lt;code&gt;# Start the API server (loads configuration from .env file)
mcp-scanner-api --port 8000

# Or with custom host and port
mcp-scanner-api --host 0.0.0.0 --port 8080

# Enable development mode with auto-reload
mcp-scanner-api --reload&lt;/code&gt;
    &lt;p&gt;Once running, the API server provides endpoints for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/scan-tool&lt;/code&gt;- Scan a specific tool on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-all-tools&lt;/code&gt;- Scan all tools on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-prompt&lt;/code&gt;- Scan a specific prompt on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-all-prompts&lt;/code&gt;- Scan all prompts on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-resource&lt;/code&gt;- Scan a specific resource on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/scan-all-resources&lt;/code&gt;- Scan all resources on an MCP server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/health&lt;/code&gt;- Health check endpoint&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Documentation is available in docs/api-reference.md or as interactive documentation at &lt;code&gt;http://localhost:8000/docs&lt;/code&gt; when the server is running.&lt;/p&gt;
    &lt;p&gt;The scanner supports multiple output formats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;summary&lt;/code&gt;: Concise overview with key findings&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;detailed&lt;/code&gt;: Comprehensive analysis with full findings breakdown&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;table&lt;/code&gt;: Clean tabular format&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;by_severity&lt;/code&gt;: Results grouped by severity level&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;raw&lt;/code&gt;: Raw JSON output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;mcp-scanner --server-url http://127.0.0.1:8001/sse --format detailed&lt;/code&gt;
    &lt;code&gt;=== MCP Scanner Detailed Results ===

Scan Target: http://127.0.0.1:8001/sse

Tool: execute_system_command
Status: completed
Safe: No
Analyzer Results:
  • api_analyzer:
    - Severity: HIGH
    - Threat Summary: Detected 1 threat: security violation
    - Threat Names: SECURITY VIOLATION
    - Total Findings: 1
  • yara_analyzer:
    - Severity: HIGH
    - Threat Summary: Detected 2 threats: system access, command injection
    - Threat Names: SECURITY VIOLATION, SUSPICIOUS CODE EXECUTION
    - Total Findings: 2
  • llm_analyzer:
    - Severity: HIGH
    - Threat Summary: Detected 2 threats: prompt injection, tool poisoning
    - Threat Names: PROMPT INJECTION, SUSPICIOUS CODE EXECUTION
    - Total Findings: 2
&lt;/code&gt;
    &lt;code&gt;mcp-scanner --server-url http://127.0.0.1:8002/sse --format table&lt;/code&gt;
    &lt;code&gt;=== MCP Scanner Results Table ===

Scan Target: http://127.0.0.1:8002/sse

Scan Target                   Tool Name     Status     API      YARA     LLM      Severity
-----------------------------------------------------------------------------------------
http://127.0.0.1:8002/sse     exec_secrets  UNSAFE     HIGH     HIGH     HIGH     HIGH
http://127.0.0.1:8002/sse     safe_command  SAFE       SAFE     SAFE     SAFE     SAFE
&lt;/code&gt;
    &lt;p&gt;For detailed documentation, see the docs/ directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Architecture - System architecture and components&lt;/item&gt;
      &lt;item&gt;Authentication - OAuth and security configuration&lt;/item&gt;
      &lt;item&gt;Programmatic Usage - Programmatic usage examples and advanced usage&lt;/item&gt;
      &lt;item&gt;API Reference - Complete REST API documentation&lt;/item&gt;
      &lt;item&gt;Output Formats - Detailed output format options&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;https://www.cisco.com/site/us/en/products/security/ai-defense/index.html&lt;/p&gt;
    &lt;p&gt;Distributed under the &lt;code&gt;Apache 2.0&lt;/code&gt; License. See LICENSE for more information.&lt;/p&gt;
    &lt;p&gt;Project Link: https://github.com/cisco-ai-defense/mcp-scanner&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45723699</guid><pubDate>Mon, 27 Oct 2025 17:18:39 +0000</pubDate></item><item><title>Creating an all-weather driver</title><link>https://waymo.com/blog/2025/10/creating-an-all-weather-driver</link><description>&lt;doc fingerprint="39afdc53bfb06cf0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Creating an all-weather Driver&lt;/head&gt;
    &lt;p&gt;Life doesn't freeze when winter comes—if anything, that's when riders need reliable transportation most, when being exposed to the elements becomes less appealing. Today, the Waymo Driver successfully navigates rain, fog, sandstorms, and freezing temperatures. As we expand to more cities across the U.S. and globally, we're applying the same systematic, scientific approach that enabled us to validate the Waymo Driver for these conditions to advance our capabilities for snowier, winter weather.&lt;/p&gt;
    &lt;p&gt;Our proven, safety-guided methodology involves four key steps:&lt;/p&gt;
    &lt;p&gt;Understanding the Challenge&lt;/p&gt;
    &lt;p&gt;Snow isn't a single phenomenon—it's a spectrum of conditions that can affect a human or autonomous driver in multiple ways. Atmospheric conditions can range from a light dusting to a complete whiteout, while road surfaces may be snow-covered or have icy patches, and environmental factors like snow buildup along roadsides add further complexity. For years, we've been advancing our system in some of the snowiest conditions across the country —regularly driving in Upstate New York, Michigan's Upper Peninsula, and the Sierra. We've amassed tens of thousands of miles in diverse, snowy conditions. This has allowed the Waymo Driver’s AI to learn from real driving experience and train to navigate a wide range of winter weather.&lt;/p&gt;
    &lt;p&gt;Designing Generalizable Solutions&lt;/p&gt;
    &lt;p&gt;At Waymo, we're building one autonomous system that works across diverse conditions—the same Waymo Driver navigating foggy San Francisco can navigate snowy Denver. Our 6th-generation Driver is informed by over 100 million fully autonomous miles of driving experience, combining state-of-the-art hardware and AI to adapt to and sustain fully autonomous operations in cities with harsher weather.&lt;/p&gt;
    &lt;p&gt;The Waymo Driver uses cameras, radar, and lidar to perceive the world around it, with each sensor providing a complementary field of view that's especially helpful in inclement weather. Its automated cleaning system –using clever engineering and heating elements – keeps the sensors clear so the vehicle can continue serving riders without needing to pull over.&lt;/p&gt;
    &lt;p&gt;Our system provides context not only about where it's operating, but also about the conditions it’s operating under. We're creating state-of-the-art AI, building on top of our existing models with richer inputs and advanced capabilities designed to navigate winter conditions. For example, our AI can distinguish between where there's snow, slush, ice, and normal road surface. The Waymo Driver then uses this information to adjust its driving behavior to match the road conditions in real-time, allowing the Waymo Driver to navigate based on what it sees (and feels), also inferring insights from other road users—adapting to blocked roads, detours, and changing surface conditions. When the system detects lower traction, it automatically adjusts its speed, acceleration, and braking. Each vehicle essentially acts as a mobile weather station, gathering data to inform its own driving decisions and share with the rest of the fleet in the city. These responses are consistent and thoroughly tested, providing predictable and safe navigation in challenging conditions.&lt;/p&gt;
    &lt;p&gt;Rigorously Validating Our Capabilities&lt;/p&gt;
    &lt;p&gt;We validate our generalizable system through real-world driving, closed-course testing, and large-scale simulation. With our growing operations in snowy cities like Detroit, Denver, and Washington D.C., in addition to visits to other areas, we're deepening our understanding of winter weather conditions and validating our capabilities. At closed-course testing facilities, we push the system to its limits in controlled environments, teaching it to recognize and respond to extreme scenarios like losing traction on ice. Then, we expand our learning year-round through simulation, long after the last snowflake has melted, so the Waymo Driver is prepared for rare and unusual events, like once-in-100-year snow New Orleans experienced this past winter.&lt;/p&gt;
    &lt;p&gt;Scaling Responsibly&lt;lb/&gt;Once we've validated our technology and operations by our Safety Framework and high caliber for rider excellence, we expand our service with clear guidelines about when our vehicles will operate based on local conditions. As we scale, we're also refining our operations to support winter service—from keeping our fleet clean and charged in freezing temperatures to optimizing the rider experience. Winter weather is complex, but we're committed to providing reliable service when riders need it most. As we continue expanding to more cities around the world, our progress is guided by safety, and riders can trust that the Waymo Driver is ready when we open our doors.&lt;lb/&gt;Looking for an all-weather Driver instead of all-weather tires? Follow along on our progress to bring Waymo to more cities at waymo.com/updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45724913</guid><pubDate>Mon, 27 Oct 2025 18:57:57 +0000</pubDate></item><item><title>Study finds growing social circles may fuel polarization</title><link>https://phys.org/news/2025-10-friends-division-social-circles-fuel.html</link><description>&lt;doc fingerprint="b71dc3835465878d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;More friends, more division: Study finds growing social circles may fuel polarization&lt;/head&gt;
    &lt;head rend="h5"&gt;Sadie Harley&lt;/head&gt;
    &lt;p&gt;scientific editor&lt;/p&gt;
    &lt;head rend="h5"&gt;Robert Egan&lt;/head&gt;
    &lt;p&gt;associate editor&lt;/p&gt;
    &lt;p&gt;Between 2008 and 2010, polarization in society increased dramatically alongside a significant shift in social behavior: the number of close social contacts rose from an average of two to four or five people. The connection between these two developments could provide a fundamental explanation for why societies around the world are increasingly fragmenting into ideological bubbles.&lt;/p&gt;
    &lt;p&gt;"The big question that not only we, but many countries are currently grappling with, is why polarization has increased so dramatically in recent years," says Stefan Thurner from the Complexity Science Hub (CSH), explaining the study's motivation. The research was published in Proceedings of the National Academy of Sciences.&lt;/p&gt;
    &lt;p&gt;The researchers' findings confirm that increasing polarization is not merely perceived—it is measurable and objectively occurring. "And this increase happened suddenly, between 2008 and 2010," says Thurner. The question remained: what caused it?&lt;/p&gt;
    &lt;head rend="h2"&gt;The friendship shift: From two to five close contacts&lt;/head&gt;
    &lt;p&gt;To investigate, Thurner and his team examined whether social networks had changed—specifically, whether people's close friendships had shifted. "For decades, sociological studies showed that people maintained an average of about two close friends—people who could influence their opinions on important issues," explains Thurner.&lt;/p&gt;
    &lt;p&gt;Here too, the researchers identified a striking change: "Around 2008, there was a sharp increase from an average of two close friends to four or five," explains CSH scientist Jan Korbel.&lt;/p&gt;
    &lt;head rend="h2"&gt;The paradox: More connection, more division&lt;/head&gt;
    &lt;p&gt;Are these two developments related? Do more close friends—and thus denser social networks—lead to network fragmentation and ultimately societal polarization?&lt;/p&gt;
    &lt;p&gt;Using a model based on real data, the researchers discovered this could indeed be the case: "When network density increases with more connections, polarization within the collective inevitably rises sharply," says Markus Hofer from CSH.&lt;/p&gt;
    &lt;p&gt;"This finding impressed us greatly because it could provide a fundamental explanation for the peculiar form of polarization we're currently observing simultaneously across many parts of the world—one that definitely threatens democracy," Thurner continues.&lt;/p&gt;
    &lt;p&gt;"When people are more connected with each other, they encounter different opinions more frequently. This inevitably leads to more conflict and thus greater societal polarization," adds Korbel.&lt;/p&gt;
    &lt;p&gt;Polarization has always existed, but what is happening now goes far beyond historical patterns. Greater connectivity has led to the formation of fewer but more tightly-knit groups with strongly differing opinions, between which there is hardly any exchange.&lt;/p&gt;
    &lt;p&gt;"There are few bridges between these 'bubbles,' and when they exist, they are often negative or even hostile," says Korbel. "This is called fragmentation, and it represents a new social phenomenon," adds Thurner.&lt;/p&gt;
    &lt;head rend="h2"&gt;Behind the numbers: Tracking polarization through decades of data&lt;/head&gt;
    &lt;p&gt;For their study, the researchers analyzed extensive existing survey data on both polarization and social networks.&lt;/p&gt;
    &lt;p&gt;"To measure political polarization, we used over 27,000 surveys from the Pew Research Center, which regularly records political attitudes of people in the US," explains Hofer.&lt;/p&gt;
    &lt;p&gt;"The key advantage of this data is that the questions have remained virtually unchanged over time, enabling reliable long-term comparisons."&lt;/p&gt;
    &lt;p&gt;The researchers found that political attitudes became significantly more one-sided between 1999 and 2017. For example, only 14% of respondents consistently expressed liberal views in 1999, but by 2017, this had risen to 31%. Conversely, only 6% of respondents consistently held conservative views in 1999, compared to 16% in 2017.&lt;/p&gt;
    &lt;p&gt;"More and more people are clearly aligning themselves with one political camp rather than holding a mixture of liberal and conservative views," explains Hofer.&lt;/p&gt;
    &lt;p&gt;To analyze friendship networks, the researchers combined 30 different surveys totaling over 57,000 respondents from Europe and the US, including the General Social Survey (US) and the European Social Survey.&lt;/p&gt;
    &lt;p&gt;"Despite minor differences between individual surveys, the data consistently show that the average number of close friendships rose from 2.2 in 2000 to 4.1 in 2024," says Hofer.&lt;/p&gt;
    &lt;p&gt;"The decisive contribution of this study is that it reconciled both phenomena using a mathematical social model," explains Thurner.&lt;/p&gt;
    &lt;p&gt;"This enabled us to show that increasing connectivity must lead to sudden polarization once a critical connectivity density is exceeded—just like a phase transition in physics, such as water turning to ice," adds Hofer.&lt;/p&gt;
    &lt;p&gt;"It is fascinating that these phase transitions also exist in societies. The exact location of these critical thresholds still needs clarification. According to our results, for close relationships, it lies somewhere between three and four people," the researchers note.&lt;/p&gt;
    &lt;head rend="h2"&gt;The smartphone era: When connection may have become fragmentation&lt;/head&gt;
    &lt;p&gt;The sharp rise in both polarization and the number of close friends occurred between 2008 and 2010—precisely when social media platforms and smartphones first achieved widespread adoption. This technological shift may have fundamentally changed how people connect with each other, indirectly promoting polarization.&lt;/p&gt;
    &lt;p&gt;"Democracy depends on all parts of society being involved in decision-making, which requires that everyone be able to communicate with each other. But when groups can no longer talk to each other, this democratic process breaks down," emphasizes Stefan Thurner.&lt;/p&gt;
    &lt;p&gt;Tolerance plays a central role. "If I have two friends, I do everything I can to keep them—I am very tolerant towards them. But if I have five and things become difficult with one of them, it's easier to end that friendship because I still have 'backups.' I no longer need to be as tolerant," explains Thurner.&lt;/p&gt;
    &lt;p&gt;What disappears as a result is a societal baseline of tolerance—a development that could contribute to the long-term erosion of democratic structures. To prevent societies from increasingly fragmenting, Thurner emphasizes the importance of learning early how to engage with different opinions and actively cultivating tolerance.&lt;/p&gt;
    &lt;p&gt;More information: Thurner, Stefan, Why more social interactions lead to more polarization in societies, Proceedings of the National Academy of Sciences (2025). DOI: 10.1073/pnas.2517530122. doi.org/10.1073/pnas.2517530122&lt;/p&gt;
    &lt;p&gt;Journal information: Proceedings of the National Academy of Sciences&lt;/p&gt;
    &lt;p&gt;Provided by Complexity Science Hub Vienna&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45725009</guid><pubDate>Mon, 27 Oct 2025 19:06:34 +0000</pubDate></item><item><title>Easy RISC-V</title><link>https://dramforever.github.io/easyriscv/</link><description>&lt;doc fingerprint="bae7c0d12017aa49"&gt;
  &lt;main&gt;&lt;p&gt;(Last updated: 2025-10-28 06:35)&lt;/p&gt;&lt;p&gt;This page is not designed to be used on a narrow screen or without CSS. If you’re having issues using the emulator, try the emulators disabled version.&lt;/p&gt;&lt;p&gt;An interactive introduction to RISC-V assembly programming, by dramforever.&lt;/p&gt;&lt;p&gt;Interested in the code? Want to report an issue? Check out the GitHub page: https://github.com/dramforever/easyriscv&lt;/p&gt;&lt;p&gt;Inspired by Easy 6502 by Nick Morgan, this is a quick-ish introductory tutorial to RISC-V assembly programming. This tutorial is intended for those with a basic familiarity with low level computer science concepts, but unfamiliar with RISC-V. If you’re curious about RISC-V, I hope this will be a good start to your journey to learning about it.&lt;/p&gt;&lt;p&gt;RISC-V (pronounced “risk-five”), as its name suggests, is RISC (Reduced instruction set computer) architecture. Having started its life at UC Berkerley, RISC-V has bred a lively community of students, researchers, engineers and hobbyists working on software and hardware. Some highlights of RISC-V include:&lt;/p&gt;&lt;p&gt;RISC-V is less mature than more established architectures like x86 or Arm, but it is quickly gaining steam and has found great success in many areas of application, such as embedded systems, custom processors, education, and research.&lt;/p&gt;&lt;p&gt;This article will cover the 32-bit bare bones RV32I_Zicsr instruction set with a tiny subset of the privileged architecture. You’ll probably never find a “real” chip with such bare bones instruction support. Most of them will have more extensions for other features like floating point or compressed instructions. However, I would still consider what we have here a “complete” instruction set. For example, Rust has Tier 2 support for the target &lt;code&gt;riscv32i-unknown-none-elf&lt;/code&gt;
which works completely fine with only the instructions we’ll cover
here.&lt;/p&gt;&lt;p&gt;Speaking of instructions we will cover, why don’t we meet the 45 of them right here and now:&lt;/p&gt;&lt;code&gt;lui auipc
jal jalr
beq bne blt bge bltu bgeu
lb lh lw lbu lhu sb sh sw
addi slti sltiu xori ori andi slli srli srai
add sub slt sltu xor or and sll srl sra
ecall ebreak
csrrw csrrs csrrc csrrwi csrrsi csrrci&lt;/code&gt;&lt;p&gt;Some of these instruction names should ring a bell (&lt;code&gt;add&lt;/code&gt;,
&lt;code&gt;or&lt;/code&gt;, &lt;code&gt;xor&lt;/code&gt;). Others will look like they have some
pattern to it. A few weird ones like &lt;code&gt;auipc&lt;/code&gt; stand out. These
instructions form the foundation of RISC-V, performing the basic tasks a
processor would do.&lt;/p&gt;&lt;p&gt;You will also catch a glimpse of what creating an operating system on RISC-V is like, namely handling exceptions and privilege levels.&lt;/p&gt;&lt;p&gt;Let’s get started.&lt;/p&gt;&lt;p&gt;Throughout this article you will see emulator panes like these:&lt;/p&gt;&lt;p&gt;(If you just see a code block, there’s a JavaScript problem. Make sure you’ve enabled JavaScript, probably…)&lt;/p&gt;&lt;p&gt;You can use the buttons to control each emulator. Go ahead and click on ‘Start’. A register view should pop up showing the state of the emulator. Now click on ‘Run’. You’ll notice that:&lt;/p&gt;&lt;code&gt;a0 (x10) 0x00000000&lt;/code&gt;&lt;p&gt;Changed into:&lt;/p&gt;&lt;code&gt;a0 (x10) 0x00000123&lt;/code&gt;&lt;p&gt;And the emulator stopped. Congratulations, you’ve run your first RISC-V assembly program. First here, at least.&lt;/p&gt;&lt;p&gt;‘Start’ assembles your code and, well, starts the emulator. If there’s a problem with your code, it will tell you about it and the emulator will not start.&lt;/p&gt;&lt;p&gt;When the emulator is started, you can see the current state of the registers in the side pane. More controls also becomes available. ‘Run’ runs until the end or until you hit ‘Pause’. ‘Step’ runs a single step.&lt;/p&gt;&lt;p&gt;If you hit ‘Step’, you’ll notice that the above program takes two steps to run. You may have guessed correctly that the first step corresponds to &lt;code&gt;addi&lt;/code&gt;, and the second corresponds to
&lt;code&gt;ebreak&lt;/code&gt;. The top of the register panel shows
&lt;code&gt;pc&lt;/code&gt;, the current instruction address, and in parentheses the
current instruction.&lt;/p&gt;&lt;p&gt;‘Dump’ opens a new window containing some text. There are two sections: the first is the symbol table, which tells you about the labels in your code:&lt;/p&gt;&lt;code&gt;# Symbols
# 0x40000000 start&lt;/code&gt;&lt;p&gt;The second section is an annotated version of your code:&lt;/p&gt;&lt;code&gt;start:
{ 0x40000000: 12300513 } addi x10, x0, 0x123
{ 0x40000004: 00100073 } ebreak&lt;/code&gt;&lt;p&gt;This tells you that the &lt;code&gt;addi&lt;/code&gt; instruction encodes to hex
&lt;code&gt;12300513&lt;/code&gt;, and starts at address hex &lt;code&gt;40000000&lt;/code&gt;.
Similarly, &lt;code&gt;ebreak&lt;/code&gt; encodes as &lt;code&gt;00100073&lt;/code&gt; at
address hex &lt;code&gt;40000004&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;(Note: RISC-V instructions are little-endian, meaning that the four bytes of &lt;code&gt;addi&lt;/code&gt; are actually
&lt;code&gt;13 05 30 12&lt;/code&gt;.)&lt;/p&gt;&lt;p&gt;We’ll talk in detail about all of &lt;code&gt;pc&lt;/code&gt;, registers,
instructions, labels, and the two checkboxes later.&lt;/p&gt;&lt;p&gt;Now you may have also guessed that &lt;code&gt;addi x10, x0, 0x123&lt;/code&gt;
means &lt;code&gt;x10 = x0 + 0x123&lt;/code&gt;. As for &lt;code&gt;ebreak&lt;/code&gt;, for
now, just remember that &lt;code&gt;ebreak&lt;/code&gt; stops the emulator.&lt;/p&gt;&lt;p&gt;The program counter, or &lt;code&gt;pc&lt;/code&gt; is the address of
the current instruction. It points to the instruction to be
executed.&lt;/p&gt;&lt;p&gt;RV32I has 31 general purpose registers numbered &lt;code&gt;x1&lt;/code&gt; through
&lt;code&gt;x31&lt;/code&gt;. These can contain any 32-bit data.&lt;/p&gt;&lt;p&gt;(If you’re wondering, there are no flags for RV32I.)&lt;/p&gt;&lt;p&gt;The register &lt;code&gt;x0&lt;/code&gt; is a
special “zero register”. For computational instructions, you can use
&lt;code&gt;x0&lt;/code&gt; anywhere a register is expected. Reading it always gives
zero, and writing to it just gets ignored. The use of a special register
simplifies the design of the architecture, and this design is shared by
MIPS and Arm AArch64. We will make good use of &lt;code&gt;x0&lt;/code&gt; soon.&lt;/p&gt;&lt;p&gt;(Note: In the emulator, the instruction listed in parenthesis next to &lt;code&gt;pc&lt;/code&gt; in the register view is provided as a convenience and is
not part of the processor state.)&lt;/p&gt;&lt;p&gt;But before we can start talking about instructions themselves, we need a way to talk about the instruction syntax so I can, you know, write it down for you.&lt;/p&gt;&lt;p&gt;The syntax of an instruction is the instruction name and then several comma-separated operands. For example, for this instruction we’ve seen above:&lt;/p&gt;&lt;code&gt;addi x10, x0, 0x123&lt;/code&gt;&lt;p&gt;&lt;code&gt;x10&lt;/code&gt; is the destination register or
&lt;code&gt;rd&lt;/code&gt;. The next operand is
the first (and only) source
register or &lt;code&gt;rs1&lt;/code&gt;. The last operand is an
immediate value or &lt;code&gt;imm&lt;/code&gt;. Using these
abbreviations, we can summarize that the syntax for &lt;code&gt;addi&lt;/code&gt;
is:&lt;/p&gt;&lt;code&gt;addi rd, rs1, imm&lt;/code&gt;&lt;p&gt;Some other instructions have a second source register or &lt;code&gt;rs2&lt;/code&gt;. For example, the
non-immediate &lt;code&gt;add&lt;/code&gt; instruction has this syntax:&lt;/p&gt;&lt;code&gt;add rd, rs1, rs2&lt;/code&gt;&lt;p&gt;Some other instructions have no operands, like &lt;code&gt;ebreak&lt;/code&gt;.
Others have slightly more complex operands.&lt;/p&gt;&lt;p&gt;Using the registers as a playground of numbers, we can use computational instructions to work with them.&lt;/p&gt;&lt;p&gt;As we’ve seen above, you can get a RISC-V machine to add numbers together.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;addi&lt;/code&gt;
instruction adds the value in &lt;code&gt;rs1&lt;/code&gt; to the immediate value
&lt;code&gt;imm&lt;/code&gt;, and puts the result in &lt;code&gt;rd&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;addi rd, rs1, imm&lt;/code&gt;&lt;p&gt;The &lt;code&gt;add&lt;/code&gt; instruction
adds the value in &lt;code&gt;rs1&lt;/code&gt; to the value in &lt;code&gt;rs2&lt;/code&gt;, and
puts the result in &lt;code&gt;rd&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;add rd, rs1, rs2&lt;/code&gt;&lt;p&gt;The opposite of addition is subtraction. The &lt;code&gt;sub&lt;/code&gt; instruction subtracts the
value in &lt;code&gt;rs2&lt;/code&gt; from the value in &lt;code&gt;rs1&lt;/code&gt;
(i.e. &lt;code&gt;rs1 - rs2&lt;/code&gt;), and puts the result in &lt;code&gt;rd&lt;/code&gt;.
There’s no corresponding &lt;code&gt;subi&lt;/code&gt; instruction — Just use
&lt;code&gt;addi&lt;/code&gt; with a negative number.&lt;/p&gt;&lt;code&gt;sub rd, rs1, rs2&lt;/code&gt;&lt;p&gt;Step through this demo program and try writing your own additions and subtractions:&lt;/p&gt;&lt;p&gt;One thing you should note is that the immediate value has a limited range, namely &lt;code&gt;[-2048, 2047]&lt;/code&gt;, the range of a 12-bit two’s
complement signed integer. This limitation is because RV32I uses fixed
32-bit i.e. 4-byte instructions, and only the top 12 bits are available
to encode an immediate value. You can see the hexadecimal value encoded
in the instruction from the ‘Dump’. This article will not go into much
further detail about instruction encodings.&lt;/p&gt;&lt;code&gt;{ 0x40000000: 12300513 } addi x10, x0, 0x123
{ 0x40000004: 55500593 } addi x11, x0, 0x555&lt;/code&gt;&lt;p&gt;Even instructions as simple as addition and subtraction have other interesting uses. We have already used &lt;code&gt;addi x10, x0, 0x123&lt;/code&gt;
to put &lt;code&gt;0x123&lt;/code&gt; in the register &lt;code&gt;x10&lt;/code&gt;. When writing
in assembly, we can use a little shortcut called pseudoinstructions. The
&lt;code&gt;li&lt;/code&gt; (“load immediate”)
pseudoinstruction is a convenient way to put a small value in a
register. It expands to &lt;code&gt;addi rd, x0, imm&lt;/code&gt; when
&lt;code&gt;imm&lt;/code&gt; is in the range &lt;code&gt;[-2048, 2047]&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;li rd, imm&lt;/code&gt;&lt;p&gt;When &lt;code&gt;imm&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;addi&lt;/code&gt; copies the
value without changing it because adding zero is the same as doing
nothing. The &lt;code&gt;mv&lt;/code&gt; (“move”)
pseudoinstruction copies the value from &lt;code&gt;rs1&lt;/code&gt; to
&lt;code&gt;rd&lt;/code&gt;. It expands to &lt;code&gt;addi rd, rs1, 0&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;mv rd, rs1&lt;/code&gt;&lt;p&gt;Using the pseudoinstruction is exactly equivalent to using the “real” instruction. You can see in the dump that the two are assembled exactly the same way.&lt;/p&gt;&lt;p&gt;Subtracting from zero is negation. What’s the negative of &lt;code&gt;0x123&lt;/code&gt;?&lt;/p&gt;&lt;p&gt;Hmm, we get &lt;code&gt;0xfffffedd&lt;/code&gt;. That’s the 32-bit two’s complement
representation of &lt;code&gt;-291&lt;/code&gt;, or &lt;code&gt;-0x123&lt;/code&gt;. There’s
plenty of tutorials on this out there, so we’ll just note that whenever
something is “signed”, RISC-V uses two’s complement representation. The
benefit of this is that there are fewer instructions for separate signed
and unsigned instructions — both signed and unsigned numbers have the
same overflow wrap-around behavior.&lt;/p&gt;&lt;p&gt;Speaking of overflow wrap-around, what happens if we add something too much and it overflows? We’ll use &lt;code&gt;add&lt;/code&gt; to repeatedly
double &lt;code&gt;0x123&lt;/code&gt; and see what happens:&lt;/p&gt;&lt;p&gt;As &lt;code&gt;0x123&lt;/code&gt; crawls up to the upper bits and eventually we
get to &lt;code&gt;0x9180_0000&lt;/code&gt;, in the next iteration it turns into
&lt;code&gt;0x2300_0000&lt;/code&gt;. There was an overflow! Doubling of
&lt;code&gt;0x9180_0000&lt;/code&gt; gives &lt;code&gt;0x1_2300_0000&lt;/code&gt;, but that
needs 33 bits in binary, so the highest bit can’t be put in the result.
Since RISC-V doesn’t have flag bits for carry or overflow, it’s simply
gone. The programmer is expected to deal with this.&lt;/p&gt;&lt;p&gt;While we’re talking about bits, another thing we can do with bits is performing bitwise logical operations on them.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;and&lt;/code&gt; instruction
performs a bitwise-“and” between the bits of &lt;code&gt;rs1&lt;/code&gt; and
&lt;code&gt;rs2&lt;/code&gt; and puts the result in &lt;code&gt;rd&lt;/code&gt;. The &lt;code&gt;or&lt;/code&gt; and &lt;code&gt;xor&lt;/code&gt; instructions similarly
performs bitwise-“or” and bitwise-“xor”, respectively.&lt;/p&gt;&lt;code&gt;and rd, rs1, rs2
or rd, rs1, rs2
xor rd, rs1, rs2&lt;/code&gt;&lt;p&gt;Immediate operand versions of the three, namely &lt;code&gt;andi&lt;/code&gt;, &lt;code&gt;ori&lt;/code&gt;, &lt;code&gt;xori&lt;/code&gt; also exist.&lt;/p&gt;&lt;code&gt;andi rd, rs1, imm
ori rd, rs1, imm
xori rd, rs1, imm&lt;/code&gt;&lt;p&gt;Here are some random bit operation examples you can play with:&lt;/p&gt;&lt;p&gt;Remember that the immediate value is in the range &lt;code&gt;[-2048, 2047]&lt;/code&gt;. For negative values, the two’s complement
representation used means that the high bits are all ones. For example,
using &lt;code&gt;-1&lt;/code&gt; as &lt;code&gt;imm&lt;/code&gt; means the second operand is
binary all ones, or &lt;code&gt;0xffff_ffff&lt;/code&gt;. This allows us to use
&lt;code&gt;xori rd, rs1, -1&lt;/code&gt; as bitwise-“not”.&lt;/p&gt;&lt;p&gt;Another interesting operation you can do is to round/align something up or down to a multiple of a power of two. For example, if you want to find the closest multiple of 16 below &lt;code&gt;a&lt;/code&gt;, in binary that would be clearing the lowest
4 bits, or &lt;code&gt;a &amp;amp; ~0b1111&lt;/code&gt;. Conveniently, that’s
&lt;code&gt;a &amp;amp; -16&lt;/code&gt; in two’s complement.&lt;/p&gt;&lt;p&gt;Aligning up is less intuitive, but one idea would be adding 16 first. However that gives an incorrect result for multiples of 16. It’s easy enough to fix though: adding one less works exactly right: &lt;code&gt;(a + 15) &amp;amp; -16&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Usually when you write a comparison of some sort like &lt;code&gt;a == b&lt;/code&gt; or &lt;code&gt;a &amp;gt;= b&lt;/code&gt;, it’s used as a condition
for some &lt;code&gt;if&lt;/code&gt; or loop, but… those things are complicated!
We’ll get to it later.&lt;/p&gt;&lt;p&gt;Sometimes you just want a boolean value out of a comparison. The C convention uses 1 for true and 0 for false, and since the world runs on C now, that’s what RISC-V provides.&lt;/p&gt;&lt;p&gt;In C there are six comparison operators:&lt;/p&gt;&lt;code&gt;== != &amp;lt; &amp;gt; &amp;lt;= &amp;gt;=&lt;/code&gt;&lt;p&gt;The values being compared can also be both signed or both unsigned.&lt;/p&gt;&lt;p&gt;How many comparison instructions do we have at our disposal? Let’s see…&lt;/p&gt;&lt;p&gt;The &lt;code&gt;slt&lt;/code&gt; (“set less
than”) instruction compares &lt;code&gt;rs1&lt;/code&gt; and &lt;code&gt;rs2&lt;/code&gt; as
signed 32-bit integers, and sets &lt;code&gt;rd&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; if
&lt;code&gt;rs1 &amp;lt; rs2&lt;/code&gt;, and &lt;code&gt;0&lt;/code&gt; otherwise
(&lt;code&gt;rs1 &amp;gt;= rs2&lt;/code&gt;). The &lt;code&gt;sltu&lt;/code&gt; instruction is similar
but it treats the operands as unsigned values. &lt;code&gt;slti&lt;/code&gt; and &lt;code&gt;sltiu&lt;/code&gt; are similar but the
second operand is an immediate value.&lt;/p&gt;&lt;code&gt;slt rd, rs1, rs2
sltu rd, rs1, rs2
slti rd, rs1, imm
sltiu rd, rs1, imm&lt;/code&gt;&lt;p&gt;(Of particular note is &lt;code&gt;sltiu&lt;/code&gt;, where the immediate
operand still has the range &lt;code&gt;[-2048, 2047]&lt;/code&gt; but is sign
extended to 32 bits and then treated as an unsigned value, like what
would happen in C with &lt;code&gt;a &amp;lt; (unsigned)-1&lt;/code&gt;.)&lt;/p&gt;&lt;p&gt;That’s… one of the six comparisons settled. What about the others? As it turns out, we can synthesize any of the other five, using up to two instructions.&lt;/p&gt;&lt;p&gt;Making &lt;code&gt;&amp;gt;&lt;/code&gt; from &lt;code&gt;&amp;lt;&lt;/code&gt; is easy, as you can
just swap the operands. Using &lt;code&gt;xori&lt;/code&gt; with &lt;code&gt;1&lt;/code&gt; we
can invert the result of a comparison, giving as &lt;code&gt;&amp;lt;=&lt;/code&gt; and
&lt;code&gt;&amp;gt;=&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;That was signed comparison but unsigned comparison works the same using &lt;code&gt;sltu&lt;/code&gt; instead of &lt;code&gt;slt&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;As for &lt;code&gt;==&lt;/code&gt; and &lt;code&gt;!=&lt;/code&gt;, let’s tackle the easier
case of &lt;code&gt;a == 0&lt;/code&gt; and &lt;code&gt;a != 0&lt;/code&gt; first. We will use
the fact that for unsigned values, &lt;code&gt;a != 0&lt;/code&gt; is equivalent to
&lt;code&gt;a &amp;gt; 0&lt;/code&gt;. The negation of that is &lt;code&gt;a &amp;lt;= 0&lt;/code&gt;,
which is the same as &lt;code&gt;a &amp;lt; 1&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;As a bonus, this is also how we get logical not and converting integer to boolean.&lt;/p&gt;&lt;p&gt;Now that we have these, &lt;code&gt;a == b&lt;/code&gt; is just
&lt;code&gt;(a - b) == 0&lt;/code&gt;, and &lt;code&gt;a != b&lt;/code&gt; is just
&lt;code&gt;(a - b) != 0&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In summary: (&lt;code&gt;[u]&lt;/code&gt; means use &lt;code&gt;u&lt;/code&gt; for unsigned
comparison and nothing for signed comparison)&lt;/p&gt;&lt;code&gt;a &amp;lt; b&lt;/code&gt;: &lt;code&gt;slt[u]&lt;/code&gt;&lt;code&gt;a &amp;gt; b&lt;/code&gt;: &lt;code&gt;slt[u] reversed&lt;/code&gt;&lt;code&gt;a &amp;lt;= b&lt;/code&gt;: &lt;code&gt;slt[u] reversed ; xori 1&lt;/code&gt;&lt;code&gt;a &amp;gt;= b&lt;/code&gt;: &lt;code&gt;slt[u] ; xori 1&lt;/code&gt;&lt;code&gt;a == 0&lt;/code&gt;: &lt;code&gt;sltu x0&lt;/code&gt;&lt;code&gt;a != 0&lt;/code&gt;: &lt;code&gt;sltiu 1&lt;/code&gt;&lt;code&gt;a == b&lt;/code&gt;: &lt;code&gt;sub ; sltu x0&lt;/code&gt;&lt;code&gt;a != b&lt;/code&gt;: &lt;code&gt;sub ; sltiu 1&lt;/code&gt;&lt;p&gt;There is no way I can do justice to the usage of bit shifts in the middle of a tutorial on RISC-V assembly. If you’re here, you’ve probably heard of them. There’s nothing really special to the way they appear in usage for RISC-V.&lt;/p&gt;&lt;p&gt;There are two variants for right shifting: &lt;code&gt;srl&lt;/code&gt; and &lt;code&gt;srli&lt;/code&gt; (“shift right logical
(immediate)”) performs “logical” or unsigned right shift where the
leftmost or most significant bits are filled with zeros.&lt;/p&gt;&lt;p&gt;&lt;code&gt;sra&lt;/code&gt; and &lt;code&gt;srai&lt;/code&gt; (“shift right
arithmetic (immediate)”) performs “arithmetic” or signed right shift
where the leftmost bits are filled with the same of what highest/sign
bit was. So if you shift a negative value, you get a negative result; if
you shift a non-negative value, you get a non-negative result.&lt;/p&gt;&lt;code&gt;srl rd, rs1, rs2
sra rd, rs1, rs2
srli rd, rs1, imm
srai rd, rs1, imm&lt;/code&gt;&lt;p&gt;As before, the ones with the &lt;code&gt;i&lt;/code&gt; suffix take an immediate
value as the second operand, and the ones without &lt;code&gt;i&lt;/code&gt; take a
register.&lt;/p&gt;&lt;p&gt;So &lt;code&gt;a&lt;/code&gt; means “arithmetic”, &lt;code&gt;l&lt;/code&gt; means “logical”.
Got it.&lt;/p&gt;&lt;p&gt;Left shifts have no such distinction. For consistency they are still “logical”: &lt;code&gt;sll&lt;/code&gt; is left
shift, and &lt;code&gt;slli&lt;/code&gt; is
left shift with immediate.&lt;/p&gt;&lt;code&gt;sll rd, rs1, rs2
slli rd, rs1, imm&lt;/code&gt;&lt;p&gt;Aha, now we can blow up &lt;code&gt;0x123&lt;/code&gt; without repeating myself
so much:&lt;/p&gt;&lt;p&gt;The immediate value for shift instructions are special: they can only be in the range of 0 to 31, inclusive, because it doesn’t make sense to shift by a negative amount, or by more than 31. When the shift amount is taken from a register, the value is considered modulo 32, or in other words only the last 5 bits are taken into account:&lt;/p&gt;&lt;p&gt;For some fun, let’s try multiplying a value by 10, something you would do when parsing decimal numbers: &lt;code&gt;a * 10&lt;/code&gt; can be
rewritten as &lt;code&gt;(a &amp;lt;&amp;lt; 1) + (a &amp;lt;&amp;lt; 3)&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;That’s it?&lt;/p&gt;&lt;p&gt;You may have noticed some glaring omissions. What we’ve learned doesn’t even cover grade school math: multiplication and division are missing.&lt;/p&gt;&lt;p&gt;RISC-V is designed with extensions in mind. Remember that as said in the introduction, RV32I is the barest bones of the barest bones we’ve got. Forcing everyone to make their processors with multiplication and division even for tasks that don’t need them would waste silicon area and money on every chip. Instead those making RISC-V processors have great freedom to choose, and indeed some would say they have too much freedom.&lt;/p&gt;&lt;p&gt;For us… Honestly, I’m just glad we’ve been dealt a hand that we can tackle completely in full. There’s no way I’m finishing writing this tutorial if RV32I wasn’t so bare boned.&lt;/p&gt;&lt;p&gt;(Operand &lt;code&gt;a&lt;/code&gt; is &lt;code&gt;rs1&lt;/code&gt;, and &lt;code&gt;b&lt;/code&gt; is
&lt;code&gt;rs2&lt;/code&gt; or immediate. In the instruction name &lt;code&gt;[i]&lt;/code&gt;
means an immediate variant is available. Subscript &lt;code&gt;u&lt;/code&gt; means
unsigned and &lt;code&gt;s&lt;/code&gt; means two’s complement signed.)&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Instruction&lt;/cell&gt;&lt;cell role="head"&gt;Operation&lt;/cell&gt;&lt;cell role="head"&gt;Immediate range&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;add[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a + b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;sub&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a - b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;(n/a)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;slt[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;(a &amp;lt;s b) ? 1 : 0&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;slt[i]u&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;(a &amp;lt;u b) ? 1 : 0&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;xor[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a ^ b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;or[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a | b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;and[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a &amp;amp; b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[-2048, 2047]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;sll[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a &amp;lt;&amp;lt; b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[0, 31]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;&lt;code&gt;srl[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a &amp;gt;&amp;gt;u b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[0, 31]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;code&gt;sra[i]&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;a &amp;gt;&amp;gt;s b&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;[0, 31]&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The &lt;code&gt;addi&lt;/code&gt; instruction has limit on the immediate value.
How do we make bigger values?&lt;/p&gt;&lt;p&gt;The &lt;code&gt;lui&lt;/code&gt; (“load upper
immediate”) instruction takes an immediate in the range
&lt;code&gt;[0, 1048575]&lt;/code&gt; (i.e. up to &lt;code&gt;220 - 1&lt;/code&gt;)
and sets &lt;code&gt;rd&lt;/code&gt; to that value left shifted 12 bits:&lt;/p&gt;&lt;code&gt;lui rd, imm20&lt;/code&gt;&lt;p&gt;That was… slightly confusing. Why don’t we give it a try:&lt;/p&gt;&lt;p&gt;Instead of &lt;code&gt;li&lt;/code&gt; loading a “low” immediate, we control the
upper 20 bits of what we put in the register. After that, we
can use another &lt;code&gt;addi&lt;/code&gt; instruction to fill in the lower bits.
For example, if we want &lt;code&gt;0x12345&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;For convenience, in assembly you can use &lt;code&gt;%hi()&lt;/code&gt; and &lt;code&gt;%lo()&lt;/code&gt; to extract the, well,
high 20 and low 12 bits of a value. The previous example could also be
written:&lt;/p&gt;&lt;p&gt;Letting &lt;code&gt;lui&lt;/code&gt; handle the high 20 bits, and
&lt;code&gt;addi&lt;/code&gt; for the low 12 bits, you can make any 32-bit
value.&lt;/p&gt;&lt;p&gt;(A small complication arises if you want to use values with bit 11 set. In that case, the immediate operand to &lt;code&gt;addi&lt;/code&gt; will have
to be negative. However &lt;code&gt;%hi&lt;/code&gt; understands this and adds one
to compensate, so this &lt;code&gt;%hi&lt;/code&gt;/&lt;code&gt;%lo&lt;/code&gt; combination
does work for everything.)&lt;/p&gt;&lt;p&gt;So far, everything that we’ve had so far can be done on even the most basic programmer’s calculator. To truly make a computer… do computer stuff, we’d want loops and conditionals.&lt;/p&gt;&lt;p&gt;In RISC-V parlance, a branch is a conditional transfer of control flow, and a jump is an unconditional transfer of control flow.&lt;/p&gt;&lt;p&gt;I think the branch instructions are slightly simpler, so let’s start with those.&lt;/p&gt;&lt;p&gt;All the branch instruction follow the form “If some comparison, go to somewhere.” The conditions are:&lt;/p&gt;&lt;code&gt;beq&lt;/code&gt;:
&lt;code&gt;rs1 == rs2&lt;/code&gt; (“equal”)&lt;code&gt;bne&lt;/code&gt;:
&lt;code&gt;rs1 != rs2&lt;/code&gt; (“not equal”)&lt;code&gt;blt&lt;/code&gt;:
&lt;code&gt;rs1 &amp;lt; rs2&lt;/code&gt; signed (“less than”)&lt;code&gt;bge&lt;/code&gt;:
&lt;code&gt;rs1 &amp;gt;= rs2&lt;/code&gt; signed (“greater or equal”)&lt;code&gt;bltu&lt;/code&gt;:
&lt;code&gt;rs1 &amp;lt; rs2&lt;/code&gt; signed (“less than unsigned”)&lt;code&gt;bgeu&lt;/code&gt;:
&lt;code&gt;rs1 &amp;gt;= rs2&lt;/code&gt; signed (“greater or equal unsigned”)&lt;p&gt;(In case you’re wondering about the confusing choice of ordering operators here, it’s just that the negation of &lt;code&gt;&amp;lt;&lt;/code&gt; is
&lt;code&gt;&amp;gt;=&lt;/code&gt;.)&lt;/p&gt;&lt;code&gt;beq rs1, rs2, label
bne rs1, rs2, label
blt rs1, rs2, label
bge rs1, rs2, label
bltu rs1, rs2, label
bgeu rs1, rs2, label&lt;/code&gt;
&lt;p&gt;Oh, right, almost forgot to explain what labels are. Labels are convenience identifiers for addresses at some line of your code. They are some identifier followed by a colon (like &lt;code&gt;this:&lt;/code&gt;). They
can appear on a line of its own, or before any instruction on the line.
You can see which address they point to using the “Dump” button. The
third operand of a branch instruction is a label to jump to if the
condition holds.&lt;/p&gt;&lt;p&gt;Let’s add up all the numbers from 1 to 100:&lt;/p&gt;&lt;p&gt;You can try your hands on making your favorite loops, like fibonacci numbers or something. Speaking of trying your hands, just so we’re ready, here’s what an infinite loop looks like. Try pausing or stopping the loop, and single stepping through the instructions.&lt;/p&gt;&lt;p&gt;(If you know a thing or two about JavaScript in the browser, you’ll know that a real infinite loop in JavaScript makes the whole page becomes unresponsive, unless it’s in a worker or something. The “Run” button here just runs the emulator for a certain number of steps, pausing by giving back control to the event loop in between.)&lt;/p&gt;&lt;p&gt;(This isn’t the preferred way to write an unconditional jump. We’ll see what is later.)&lt;/p&gt;&lt;p&gt;By the way, there’s no &lt;code&gt;bgt[u]&lt;/code&gt; or &lt;code&gt;ble[u]&lt;/code&gt;
because you can just swap &lt;code&gt;rs1&lt;/code&gt; and &lt;code&gt;rs2&lt;/code&gt; to get
those.&lt;/p&gt;&lt;p&gt;There are two jump instructions in RISC-V. One of them is &lt;code&gt;jal&lt;/code&gt; “jump and link”, which
sets &lt;code&gt;rd&lt;/code&gt; to the address of the following instruction, and
then jumps to a label:&lt;/p&gt;&lt;code&gt;jal rd, label&lt;/code&gt;
&lt;p&gt;Another is &lt;code&gt;jalr&lt;/code&gt;
“jump and link register”, which sets &lt;code&gt;rd&lt;/code&gt; to the address of
the following instruction, and then jumps to the address at
&lt;code&gt;imm + rs1&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;jalr rd, imm(rs1)&lt;/code&gt;
&lt;p&gt;(Actually, the address jumped to is &lt;code&gt;(imm + rs1) &amp;amp; ~1&lt;/code&gt;, i.e. the least significant bit is
cleared. This distinction won’t come up in normal code, like, pretty
much ever.)&lt;/p&gt;&lt;p&gt;Eesh, that’s some funky looking syntax. When you see parentheses like this, it has something to do with an address. Parens means address.&lt;/p&gt;&lt;p&gt;That’s… still a lot going on. Let’s take on some simpler cases first: If &lt;code&gt;rd&lt;/code&gt; is &lt;code&gt;x0&lt;/code&gt; then the only thing these
instructions do is jumping. We can use it instead of the branch
instructions for an unconditional jump.&lt;/p&gt;&lt;p&gt;For convenience, a pseudoinstruction is available for you: &lt;code&gt;j&lt;/code&gt; (“jump”) is for
&lt;code&gt;jal&lt;/code&gt; with &lt;code&gt;rd&lt;/code&gt; being &lt;code&gt;x0&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;j label&lt;/code&gt;
&lt;p&gt;As for why you would want to do this… Well, we only have 32 bits per instruction, and since the &lt;code&gt;jal&lt;/code&gt; instruction only needs one
register number instead of the branch instructions’ two, and it doesn’t
need a condition, the instruction encoding permits jumping over a longer
range. So this is always preferred over something like
&lt;code&gt;beq x0, x0, label&lt;/code&gt; for a jump.&lt;/p&gt;&lt;p&gt;As for &lt;code&gt;jalr&lt;/code&gt;, you can jump to an address that’s stored in
a register. In C, that would be dealing with function pointers. You’d
need this any time dynamic dispatch is needed. For example, we load the
address of &lt;code&gt;foo&lt;/code&gt; into a register first before jumping to
it.&lt;/p&gt;&lt;p&gt;In case you forgot by now, the &lt;code&gt;lui&lt;/code&gt;/&lt;code&gt;addi&lt;/code&gt;
combo at the start puts the address of the label &lt;code&gt;foo&lt;/code&gt; in
register &lt;code&gt;x10&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Similar to &lt;code&gt;j&lt;/code&gt;, &lt;code&gt;jr&lt;/code&gt; (“jump register”) is a
psuedoinstruction for &lt;code&gt;jalr&lt;/code&gt; with &lt;code&gt;rd&lt;/code&gt; being
&lt;code&gt;x0&lt;/code&gt; and &lt;code&gt;imm&lt;/code&gt; being &lt;code&gt;0&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;jr rs1&lt;/code&gt;
&lt;p&gt;Hmmm… If I didn’t really need the address in &lt;code&gt;x10&lt;/code&gt;, that
&lt;code&gt;addi&lt;/code&gt; would be unnecessary, since &lt;code&gt;jalr&lt;/code&gt; has the
ability to add a low immediate on its own:&lt;/p&gt;&lt;p&gt;What’s the advantage of this over &lt;code&gt;jal x0&lt;/code&gt;? Since
&lt;code&gt;%hi&lt;/code&gt; and &lt;code&gt;%lo&lt;/code&gt; can represent any 32-bit value,
this two-instruction combo can jump to any address, free from range
restrictions. You do need a free scratch register for the high part of
the address though, but since RISC-V gives you 31 of them, this
shouldn’t be too much of a problem.&lt;/p&gt;&lt;p&gt;What’s the deal with the destination register then? What do you need the address of the next instruction for? For jumping back of course. We can use this functionality to call functions and return back.&lt;/p&gt;&lt;p&gt;Note that I used the register &lt;code&gt;x1&lt;/code&gt; for this, which is the
register for providing the return address by convention. For
convenience, if the destination register is omitted in &lt;code&gt;jal&lt;/code&gt;,
it defaults to &lt;code&gt;x1&lt;/code&gt;. Meanwhile, &lt;code&gt;ret&lt;/code&gt; (“return”) is a
pseudoinstruction that stands for &lt;code&gt;jr x1&lt;/code&gt;,
i.e. &lt;code&gt;jalr x0, 0(x1)&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;jal label
ret&lt;/code&gt;
&lt;p&gt;So the example above can be rewritten more conveniently as:&lt;/p&gt;&lt;p&gt;That’s a nice computer we have here. Now we have… all of 31 × 4 = 124 bytes of storage in the form of registers to work with. I want more…&lt;/p&gt;&lt;p&gt;The emulator has 1 MiB of memory starting at address &lt;code&gt;0x4000_0000&lt;/code&gt;. That’s &lt;code&gt;0x4000_0000&lt;/code&gt; to
&lt;code&gt;0x400f_ffff&lt;/code&gt;, inclusive. The assembler starts assembling at
the beginning of memory, as you can see in the dump, starting at address
&lt;code&gt;0x4000_0000&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;.word&lt;/code&gt; directive straight up puts a
4-byte/32-bit word into the current position. You can specify multiple
values separated by commas.&lt;/p&gt;&lt;code&gt;.word value [ , value [ , ...  ] ]&lt;/code&gt;
&lt;p&gt;The &lt;code&gt;lw&lt;/code&gt; (“load word”)
instruction loads a word from the address &lt;code&gt;rs1 + imm&lt;/code&gt; and
puts it in &lt;code&gt;rd&lt;/code&gt;, in other words it reads the word from
memory:&lt;/p&gt;&lt;code&gt;lw rd, imm(rs1)&lt;/code&gt;
&lt;p&gt;As with &lt;code&gt;jalr&lt;/code&gt;, you can combine it with &lt;code&gt;lui&lt;/code&gt;
to access any address.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;sw&lt;/code&gt; (“store word”)
instruction stores &lt;code&gt;rs2&lt;/code&gt; to a word in memory at address
&lt;code&gt;rs2 + imm&lt;/code&gt;, in other words it writes the word to memory:&lt;/p&gt;&lt;code&gt;sw rs2, imm(rs1)&lt;/code&gt;
&lt;p&gt;Just to make absolutely sure we’re clear on this, load means reading from memory, store means writing to memory. Both words can be nouns and verbs. Also, a word is 32-bit for RISC-V.&lt;/p&gt;&lt;p&gt;Let’s have some fun. Can we have the program read itself?&lt;/p&gt;&lt;p&gt;Ohh that’s fun. Does this mean I can also write programs with just &lt;code&gt;.word&lt;/code&gt;?&lt;/p&gt;&lt;p&gt;Oh that’s nice. Just a peek into the world of machine code and instruction encodings… which we will not be getting into.&lt;/p&gt;&lt;p&gt;With memory accesses under our belt, we can address a lot more data easily. Here’s an example where we find the sum of all the values in an array. Note how we can access different addresses of memory, whereas there is no way to address a register by a number in another register.&lt;/p&gt;&lt;p&gt;The equivalent in C would be something like&lt;/p&gt;&lt;code&gt;uint32_t array[], length;

uint32_t *current = array;
uint32_t *end = array + length;
uint32_t sum = 0;

for (; current != end; current ++) {
    sum += *current;
}&lt;/code&gt;
&lt;p&gt;Note how adding one to a pointer to word bumps the address by 4, because the addresses are all byte addresses, and one word is four bytes. In C, the compiler handles the multiplier for you, but in assembly you have to remember to do it manually.&lt;/p&gt;&lt;p&gt;Not everything in memory is word sized. You’ve already seen an array, which is multiple-word-sized. There are also stuff smaller than word-sized.&lt;/p&gt;&lt;p&gt;An obvious one is the byte, which is, well, 1-byte/8-bit and written &lt;code&gt;[u]int8_t&lt;/code&gt; in C. In
the middle is the halfword,
which is 2-byte/16-bit and written &lt;code&gt;[u]int16_t&lt;/code&gt; in C. You can
use the directives &lt;code&gt;.byte&lt;/code&gt; and &lt;code&gt;.half&lt;/code&gt; respectively for those
data types.&lt;/p&gt;&lt;code&gt;.byte value [ , value [ , ...  ] ]
.half value [ , value [ , ...  ] ]&lt;/code&gt;
&lt;p&gt;And just in case you don’t remember those, &lt;code&gt;.2byte&lt;/code&gt; means the same as
&lt;code&gt;.half&lt;/code&gt;, and &lt;code&gt;.4byte&lt;/code&gt; means the same as
&lt;code&gt;.word&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;.2byte value [ , value [ , ...  ] ] # Same as .half
.4byte value [ , value [ , ...  ] ] # Same as .word&lt;/code&gt;
&lt;p&gt;There’s a small problem with loading smaller-than-word sized values into word-sized registers: What do you do with the rest of the bits? Obviously the lowest of the bits gets the actual value loaded. There are two most useful ways to fill the upper bits:&lt;/p&gt;&lt;p&gt;Zero extension is easy enough. As the name suggests, sign extension has something to do with signed values. It’s what happens when you convert a narrower signed value into a wider one.&lt;/p&gt;&lt;p&gt;(Keeping the rest of the bits unchanged isn’t a good option. It complicates the implementation for processor, especially of modern high performance design, to just write parts of a register. It would be easiest if the new value didn’t depend on the old value.)&lt;/p&gt;&lt;p&gt;For example, the signed byte value &lt;code&gt;-100&lt;/code&gt; is
&lt;code&gt;0x9c&lt;/code&gt;. Since the highest bit i.e. the sign bit of it is
&lt;code&gt;1&lt;/code&gt;, when we expand it into 32 bits we fill the high 24 bits
with one so the new value, &lt;code&gt;0xffff_ff9c&lt;/code&gt; still represents
&lt;code&gt;-100&lt;/code&gt;. This is sign extension.&lt;/p&gt;&lt;p&gt;If we want to convert the unsigned byte value &lt;code&gt;156&lt;/code&gt;, still
&lt;code&gt;0x9c&lt;/code&gt;, into an unsigned word, it would have to be
&lt;code&gt;0x0000_009c&lt;/code&gt; to preserve its value.&lt;/p&gt;&lt;p&gt;For bytes, the &lt;code&gt;lb&lt;/code&gt;
(“load byte”) instruction loads a byte and sign extends the result, and
the &lt;code&gt;lbu&lt;/code&gt; (“load byte
unsigned”) instruction does the same but zero extends the result. As
with &lt;code&gt;lw&lt;/code&gt;, the address is &lt;code&gt;rs1 + imm&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;lb rd, imm(rs1)
lbu rd, imm(rs1)&lt;/code&gt;
&lt;p&gt;Similarly for &lt;code&gt;lh&lt;/code&gt;
(“load half”) and &lt;code&gt;lhu&lt;/code&gt;
(“load half unsigned”), just for unsigned halfwords (two bytes each,
remember):&lt;/p&gt;&lt;code&gt;lh rd, imm(rs1)
lhu rd, imm(rs1)&lt;/code&gt;
&lt;p&gt;We can try out the sign extension and zero extension example from earlier.&lt;/p&gt;&lt;p&gt;Correspondingly, the &lt;code&gt;sb&lt;/code&gt; (“store byte”) and &lt;code&gt;sh&lt;/code&gt; (“store half”) do the
opposite of &lt;code&gt;lb&lt;/code&gt; and &lt;code&gt;lh&lt;/code&gt;, storing bytes and
halfwords to memory. Instead of widening small values to register size,
these take the lowest order bits from &lt;code&gt;rs1&lt;/code&gt; and stores it to
memory. (There’s no &lt;code&gt;sbu&lt;/code&gt; and &lt;code&gt;shu&lt;/code&gt; because stores
are narrowing instead of widening operations.)&lt;/p&gt;&lt;code&gt;sb rs2, imm(rs1)
sh rs2, imm(rs1)&lt;/code&gt;
&lt;p&gt;While we’re at it, here’s two more minor details. Firstly, endianness. While theoretically big endian RISC-V machines can exist, I’ve never seen one… and this emulator is little endian, meaning that the four bytes in a word are laid out in memory lowest first. So, &lt;code&gt;.byte 0x1, 0x2, 0x3, 0x4&lt;/code&gt; would be
the same as &lt;code&gt;.word 0x04030201&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Secondly, memory accesses should be aligned for maximum efficiency. This means that the address for a halfword/2byte should be a multiple of two, and the address for a word/4byte should be a multiple of four. Misaligned accesses (meaning, well, when the address is not aligned) may not work as expected.&lt;/p&gt;&lt;p&gt;For user programs running on a rich operating systems, misaligned accesses are supported but may be slow. In embedded application running on microcontrollers and such, it might not work at all.&lt;/p&gt;&lt;p&gt;This emulator supports misaligned memory accesses.&lt;/p&gt;&lt;p&gt;Now you can try translating some basic C code into RISC-V assembly. Functions are… still out of the question for now. Variables have to be either global or put in registers. What else are we missing…&lt;/p&gt;&lt;p&gt;Is it Hello World time? I think it’s Hello World time…&lt;/p&gt;&lt;p&gt;For a computer to not just be a space heater, we need some way for it to at least generate output and take input. While other architectures may have dedicated I/O instructions, RISC-V uses memory mapped I/O. Essentially, this means that loads and stores to special addresses communicate with other devices. They do not work like normal memory, and you should only use the supported widths to access them.&lt;/p&gt;&lt;p&gt;One output device we have here is at address &lt;code&gt;0x1000_0000&lt;/code&gt;. Any 32-bit writes to it appends the lowest 8
bits as a byte to the text in the output pane. In other words, a
&lt;code&gt;sw&lt;/code&gt; to that address writes a byte of output.&lt;/p&gt;&lt;p&gt;(The output pane uses UTF-8 encoding.)&lt;/p&gt;&lt;p&gt;Eh, close enough to greeting the entire world. We could refactor it a bit to use a loop, or whatever… Now that we think about it, how about going one step further and organize our code into some functions?&lt;/p&gt;&lt;p&gt;We already know how to call a function and return back. Namely, &lt;code&gt;jal&lt;/code&gt; calls a function, and &lt;code&gt;ret&lt;/code&gt; returns. Usually
functions take arguments, uses local variables, and returns results.
Since there’s no real difference between the 31 general purpose
registers, on account of them being, well, general purpose, we could
just use any of them as we wish. Usually though, there are some standard
conventions to follow&lt;/p&gt;&lt;p&gt;This whole time you probably have noticed that registers are listed with two names each, and indeed both work identically in assembly.&lt;/p&gt;&lt;p&gt;These register aliases are named after their uses:&lt;/p&gt;&lt;code&gt;s0&lt;/code&gt; through
&lt;code&gt;s11&lt;/code&gt; are saved registers&lt;code&gt;t0&lt;/code&gt; through
&lt;code&gt;t6&lt;/code&gt; are temporary registers&lt;code&gt;a0&lt;/code&gt; through
&lt;code&gt;a7&lt;/code&gt; are argument registers&lt;code&gt;zero&lt;/code&gt; is the,
well, zero register&lt;code&gt;ra&lt;/code&gt; is for the
return address, by convention, as we’ve seen&lt;code&gt;sp&lt;/code&gt; … we’ll talk
about &lt;code&gt;sp&lt;/code&gt; later&lt;code&gt;tp&lt;/code&gt;
and &lt;code&gt;gp&lt;/code&gt; is out of the
scope of this document.)&lt;p&gt;(Yeah it’s… all placed in a weird order. The reason is out of the scope of this tutorial.)&lt;/p&gt;&lt;p&gt;When you call a function, you put up to eight arguments in the… well, argument registers, in the order &lt;code&gt;a0&lt;/code&gt;, &lt;code&gt;a1&lt;/code&gt;, …,
&lt;code&gt;a7&lt;/code&gt;. After that you use &lt;code&gt;jal&lt;/code&gt; or something, which
puts the return address in &lt;code&gt;ra&lt;/code&gt;, and jumps to the
function.&lt;/p&gt;&lt;p&gt;Inside, the function, if it wishes to use the call-saved registers &lt;code&gt;s0&lt;/code&gt; through &lt;code&gt;s11&lt;/code&gt;, it must save their values at
the start of the function, and restore them before returning. The non
call-saved registers &lt;code&gt;a0&lt;/code&gt; through &lt;code&gt;a7&lt;/code&gt;,
&lt;code&gt;t0&lt;/code&gt; through &lt;code&gt;t6&lt;/code&gt; and &lt;code&gt;ra&lt;/code&gt; may be
modified without restoring their values.&lt;/p&gt;&lt;p&gt;When the called function is done, it would, as mentioned, restore any used call-saved registers, and jump back to the return address, resuming the calling code.&lt;/p&gt;&lt;p&gt;Here’s a basic-ish example:&lt;/p&gt;&lt;code&gt;int memcmp(const void *a, const void *b, size_t n)&lt;/code&gt;
&lt;p&gt;The parameter &lt;code&gt;a&lt;/code&gt; is passed in &lt;code&gt;a0&lt;/code&gt;,
&lt;code&gt;b&lt;/code&gt; is passed in &lt;code&gt;a1&lt;/code&gt;, and &lt;code&gt;n&lt;/code&gt; is
passed in &lt;code&gt;a2&lt;/code&gt;. The return value will be in &lt;code&gt;a0&lt;/code&gt;.
Here’s an implementation and test run:&lt;/p&gt;&lt;p&gt;Here’s a slightly better-organized “Hello World”, using a &lt;code&gt;puts&lt;/code&gt; function:&lt;/p&gt;&lt;p&gt;Although we can write some very basic functions now, there are still a few problems:&lt;/p&gt;&lt;code&gt;ra&lt;/code&gt; would be overwritten, and then you can’t return back
from the outer function anymore.&lt;p&gt;Clearly, both would require using memory somehow. We can feed two birds with one scone by using memory in a structured way: The stack.&lt;/p&gt;&lt;p&gt;Unlike some other architectures, the &lt;code&gt;sp&lt;/code&gt; register is not
really special in any way. But just like how we can designate how
&lt;code&gt;a0&lt;/code&gt; is used, we can have some conventions about how
&lt;code&gt;sp&lt;/code&gt; is supposed to be used:&lt;/p&gt;&lt;code&gt;sp&lt;/code&gt; needs to have the same value as when the
function was entered&lt;code&gt;sp&lt;/code&gt; always points to somewhere in an area of
memory called the “stack”, and it is always 16-byte
aligned.&lt;p&gt;And, for the stack itself:&lt;/p&gt;&lt;code&gt;address &amp;gt;= sp&lt;/code&gt; are “in the stack”, and
&lt;code&gt;address &amp;lt; sp&lt;/code&gt; are free space that the stack can grow
into.&lt;code&gt;sp&lt;/code&gt;, and deallocate space by incrementing &lt;code&gt;sp&lt;/code&gt;.
Of course, allocations and deallocations must be balanced properly.&lt;p&gt;An example is in order. Let’s say you have a function &lt;code&gt;foo&lt;/code&gt; which just calls &lt;code&gt;bar&lt;/code&gt; twice.&lt;/p&gt;&lt;code&gt;void foo() {
    bar();
    bar();
}&lt;/code&gt;
&lt;p&gt;Inside &lt;code&gt;foo&lt;/code&gt;, it would need to save the initial
&lt;code&gt;ra&lt;/code&gt;, so it can return back later. Even though
&lt;code&gt;ra&lt;/code&gt; takes only 4 bytes, &lt;code&gt;sp&lt;/code&gt; needs to be 16-byte
aligned at all times, so we round that up to 16 bytes. Decrementing
&lt;code&gt;sp&lt;/code&gt; by 16 we allocate the space:&lt;/p&gt;&lt;code&gt;foo:
    addi sp, sp, -16&lt;/code&gt;
&lt;p&gt;Now, in addition to all of the non call-saved registers, we have 16 bytes of scratch space at &lt;code&gt;sp&lt;/code&gt; through &lt;code&gt;sp + 15&lt;/code&gt;.
We can backup the value of &lt;code&gt;ra&lt;/code&gt; here&lt;/p&gt;&lt;code&gt;    ...
    sw ra, 0(sp)&lt;/code&gt;
&lt;p&gt;Then we just call &lt;code&gt;bar&lt;/code&gt; twice, which overwrites
&lt;code&gt;ra&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;    ...
    jal bar
    jal bar&lt;/code&gt;
&lt;p&gt;At the end of the function, we just need to get back the return address, deallocate the stack space, and return. Although using any register would suffice for the return address, since it is the backed up value of &lt;code&gt;ra&lt;/code&gt; after all, we load it back to
&lt;code&gt;ra&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;    ...
    lw ra, 0(sp)
    addi sp, sp, 16
    ret&lt;/code&gt;
&lt;p&gt;In a similar way you can save and restore the &lt;code&gt;s&lt;/code&gt;
(remember, call-saved) registers. Usually, the most convenient way to
manage this is to put values that need to be preserved across inner
function calls in the &lt;code&gt;s&lt;/code&gt; registers, and then add code at the
beginning to save them, and add code at the end to restore them.&lt;/p&gt;&lt;p&gt;Obligatory recursive Fibonacci time!&lt;/p&gt;&lt;p&gt;The algorithm should be fairly straightforward:&lt;/p&gt;&lt;code&gt;fibonacci(n) {
    if (n &amp;lt; 2) { return n; }
    else { return fib(n - 1) + fib(n - 2); }
}&lt;/code&gt;
&lt;p&gt;What’s worth noting here is the fairly symmetric pattern of saving registers at the start:&lt;/p&gt;&lt;code&gt;    addi sp, sp, -16
    sw ra, 0(sp)
    sw s0, 4(sp)
    sw s1, 8(sp)&lt;/code&gt;
&lt;p&gt;And restoring them at the end:&lt;/p&gt;&lt;code&gt;    lw ra, 0(sp)
    lw s0, 4(sp)
    lw s1, 8(sp)
    addi sp, sp, 16
    ret&lt;/code&gt;
&lt;p&gt;A little thing to also note that the &lt;code&gt;s&lt;/code&gt; registers are
only saved in the more complex branch, where as the simpler branch just
returns directly. This is also acceptable from a calling convention
perspective.&lt;/p&gt;&lt;p&gt;(Note: In the emulator, the &lt;code&gt;sp&lt;/code&gt; register is initialized
to an address that would be convenient for you for use as a stack, as a,
well, convenience.)&lt;/p&gt;&lt;p&gt;Let’s go back to this example:&lt;/p&gt;&lt;code&gt;    # void puts(const char *);
puts:
    lui t1, %hi(0x10000000)
puts_loop:
    lb t0, 0(a0)
    beq t0, zero, puts_done
    sw t0, 0(t1)
    addi a0, a0, 1
    j puts_loop

puts_done:
    ret&lt;/code&gt;
&lt;p&gt;Having to name things like &lt;code&gt;puts_loop&lt;/code&gt;,
&lt;code&gt;puts_done&lt;/code&gt; is a bit annoying. There’s a shorter way: numeric labels.&lt;/p&gt;&lt;p&gt;A numeric label is one with a name of a decimal number. To refer to a numeric label, use the number and a &lt;code&gt;f&lt;/code&gt; suffix for “forward”,
and &lt;code&gt;b&lt;/code&gt; for “backward”, and it will correspond to the nearest
numeric label with that number, searching forwards or backwards,
respectively.&lt;/p&gt;&lt;p&gt;So, the &lt;code&gt;puts&lt;/code&gt; example from earlier can be rewritten:&lt;/p&gt;&lt;code&gt;    # void puts(const char *);
puts:
    lui t1, %hi(0x10000000)
1:
    lb t0, 0(a0)
    beq t0, zero, 2f
    sw t0, 0(t1)
    addi a0, a0, 1
    j 1b

2:
    ret&lt;/code&gt;
&lt;p&gt;Yeah I don’t really like this syntax either, but it is what we’ve got.&lt;/p&gt;&lt;p&gt;Remember that oddball instruction I mentioned way back, &lt;code&gt;auipc&lt;/code&gt;?&lt;/p&gt;&lt;p&gt;I don’t know about your experience, but the first time I saw RISC-V disassembly, this is the one instruction that caught my eye. And this memory has stuck with me ever since. It’s a rather common occurrence in real RISC-V programs, and somehow I’ve been hiding it from you this whole time. If you take a sneak peek at the next section’s title, you’ll see how far we’ve come without &lt;code&gt;auipc&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;So what does it do?&lt;/p&gt;&lt;p&gt;The &lt;code&gt;auipc&lt;/code&gt; (“add
upper immediate to pc”) instruction is very similar to &lt;code&gt;lui&lt;/code&gt;.
Instead of setting &lt;code&gt;rd&lt;/code&gt; to &lt;code&gt;imm20 &amp;lt;&amp;lt; 12&lt;/code&gt;, it
sets it to &lt;code&gt;pc + (imm20 &amp;lt;&amp;lt; 12)&lt;/code&gt;, where &lt;code&gt;pc&lt;/code&gt;
is the address of the &lt;code&gt;auipc&lt;/code&gt; instruction itself.&lt;/p&gt;&lt;code&gt;auipc rd, imm20&lt;/code&gt;
&lt;p&gt;It works very similarly to &lt;code&gt;lui&lt;/code&gt;. You can think of them as
a pair: the “base” of &lt;code&gt;lui&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;, whereas the
“base” of &lt;code&gt;auipc&lt;/code&gt; is the address of the &lt;code&gt;auipc&lt;/code&gt;
instruction. So this code:&lt;/p&gt;&lt;code&gt;start:
    lui a0, 3
    addi a0, a0, 4&lt;/code&gt;
&lt;p&gt;Gives you &lt;code&gt;0x3004&lt;/code&gt;, whereas this:&lt;/p&gt;&lt;code&gt;start:
    auipc a0, 3
    addi a0, a0, 4&lt;/code&gt;
&lt;p&gt;Gives you &lt;code&gt;start + 0x3004&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Why would you need this? On modern systems, it’s often desirable to have machine code that can be moved around in address space. For example, a shared library i.e. dynamically linked library can be loaded into any program, at any address. It would be helpful if the machine code does not need to be patched every time. This is called position independent code (PIC).&lt;/p&gt;&lt;p&gt;Some instructions already exhibit position independence. For example, as mentioned earlier when we talked about using &lt;code&gt;lui&lt;/code&gt; and
&lt;code&gt;jalr&lt;/code&gt; as a pair, the branch instructions and
&lt;code&gt;jal&lt;/code&gt; are encoded, as with all RV32I instructions, into
32-bit instruction words, so they can’t possibly be able to encode every
possible address. Instead, the jump destination is &lt;code&gt;pc&lt;/code&gt; plus
some offset (&lt;code&gt;pc&lt;/code&gt; being, as before, the jump/branch
instruction itself), and the offset itself is encoded.&lt;/p&gt;&lt;p&gt;You can see these are three different instructions that jump to itself. Since the offset is &lt;code&gt;0&lt;/code&gt; in each case, the encoding is
the same. Use the “Dump” button to see for yourself.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;auipc&lt;/code&gt; instruction allows for very flexible position
independence. You can make arbitrary calculations based on the address
at which code is located. The immediate-bit operand mirroring
&lt;code&gt;lui&lt;/code&gt; means that it is well suited for two-instruction pairs,
just like &lt;code&gt;lui&lt;/code&gt;. These kind of “&lt;code&gt;pc&lt;/code&gt; plus
something” calculations are known as pc-relative
addressing.&lt;/p&gt;&lt;p&gt;The syntax for getting the assembler to generate the immediate values for pc-relative addressing a bit arcane but hear me out:&lt;/p&gt;&lt;p&gt;Like &lt;code&gt;%hi()&lt;/code&gt; and &lt;code&gt;%lo()&lt;/code&gt;, &lt;code&gt;%pcrel_hi()&lt;/code&gt; and &lt;code&gt;%pcrel_lo()&lt;/code&gt; gives you
the immediate values needed for pc-relative addressing. You pass the
label you want to address to &lt;code&gt;%pcrel_hi()&lt;/code&gt;, but pass a label
to the &lt;code&gt;auipc&lt;/code&gt; instruction to
&lt;code&gt;%pcrel_lo()&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Unlike &lt;code&gt;%lo()&lt;/code&gt;, We need the address of the
&lt;code&gt;auipc&lt;/code&gt; instruction itself to calculate the immediate value,
and this is why you need to pass a label to it. You don’t need to write
&lt;code&gt;foo&lt;/code&gt; again, since the assembler will look at the
&lt;code&gt;auipc&lt;/code&gt; instruction and see it’s supposed to be for
&lt;code&gt;foo&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;If you hate writing that, you can also use the convenience pseudoinstruction &lt;code&gt;la&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;la rd, label&lt;/code&gt;
&lt;p&gt;Just like a &lt;code&gt;lui&lt;/code&gt; + &lt;code&gt;jalr&lt;/code&gt; pair, an
&lt;code&gt;auipc&lt;/code&gt; + &lt;code&gt;jalr&lt;/code&gt; can be used to jump to somewhere
farther away than one &lt;code&gt;jal&lt;/code&gt; can reach in position-independent
code.&lt;/p&gt;&lt;p&gt;One very common case is to call a function that might not be within reach of &lt;code&gt;jal&lt;/code&gt;. You can use the pseudoinstruction &lt;code&gt;call&lt;/code&gt; for that.&lt;/p&gt;&lt;code&gt;call label&lt;/code&gt;
&lt;p&gt;This expands to:&lt;/p&gt;&lt;code&gt;1:
    auipc ra, %pcrel_hi(label)
    jalr ra, %pcrel_lo(1b)(ra)&lt;/code&gt;
&lt;p&gt;Notice how &lt;code&gt;ra&lt;/code&gt; is used as a temporary register to store
the intermediate result, which is immediately overwritten by
&lt;code&gt;jalr&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In fact, there really isn’t any reason to prefer &lt;code&gt;lui&lt;/code&gt;
over &lt;code&gt;auipc&lt;/code&gt; when using a label. This is why you if you
disassemble a real RISC-V program, you see it everywhere, even in
non-position-independent code.&lt;/p&gt;&lt;p&gt;Now would be a good time to take a break, since we’re ready to head into…&lt;/p&gt;&lt;p&gt;We’re going to write an extremely bare bones operating system.&lt;/p&gt;&lt;p&gt;One of the tasks an operating system performs is to control what programs can and cannot do. On RISC-V, the most basic of this control is implemented using privilege levels. RISC-V defines… let’s just say, several privilege levels, but we’re only going to use two here:&lt;/p&gt;&lt;p&gt;The lower the privilege level number goes, the less privileged that level is. Higher privilege levels treat lower privilege levels as generally completely unreliable and untrusted, and must isolate themselves from adversarial software and failures of lower privilege levels.&lt;/p&gt;&lt;p&gt;(However, we won’t be talking about all of the features that make this full isolation possible, and the emulator you’ve been seeing does not have enough features for that anyway. Therefore, the operating system we’ll be building will leave itself unprotected in various ways.)&lt;/p&gt;&lt;p&gt;The privilege levels are sometimes called “modes” for short. And, if that’s not short enough, we can shorten the level names themselves, ending up with M-mode and U-mode. All of the ways to refer to these privilege levels are interchangable.&lt;/p&gt;&lt;p&gt;When a RISC-V machine starts (This is known as “reset”), it begins execution in Machine mode. On a typical “embedded” system where only Machine mode and User mode are implemented, execution begins in the initialization code read from flash memory. This code can either perform what needs to be done itself, or it can be an operating system that manages some tasks, each executing in User mode.&lt;/p&gt;&lt;p&gt;The former design is used for simpler programs, and is analogous to the programs we’ve seen and run so far. The latter is more complicated. We’ll see the basics of how to achieve that soon.&lt;/p&gt;&lt;p&gt;The control and status registers (CSRs) deal with various features that are in some sense “special”. No I don’t have a better explanation of what “special” means.&lt;/p&gt;&lt;p&gt;Six instructions are available for manipulating CSRs.&lt;/p&gt;&lt;code&gt;csrrw rd, csr, rs1
csrrs rd, csr, rs1
csrrc rd, csr, rs1
csrrwi rd, csr, uimm5
csrrsi rd, csr, uimm5
csrrci rd, csr, uimm5&lt;/code&gt;
&lt;p&gt;To refer to a CSR in these instructions, use its name in assembly code. We’ll get to those in a bit.&lt;/p&gt;&lt;p&gt;The pattern works like this. Each of the instructions atomically reads the old value of the CSR, and writes the new value based on some operation performed on the old value and the last operand. The possible operations are:&lt;/p&gt;&lt;code&gt;csrrw&lt;/code&gt; (“CSR read
write”): &lt;code&gt;{ csr = rs1; rd = csr_old; }&lt;/code&gt;&lt;code&gt;csrrs&lt;/code&gt; (“CSR read
set”): &lt;code&gt;{ csr = csr | rs1; rd = csr_old; }&lt;/code&gt;&lt;code&gt;csrrc&lt;/code&gt; (“CSR read
clear”): &lt;code&gt;{ csr = csr &amp;amp; ~rs1; rd = csr_old; }&lt;/code&gt;&lt;p&gt;Where &lt;code&gt;&amp;amp;&lt;/code&gt;, &lt;code&gt;|&lt;/code&gt;, &lt;code&gt;~&lt;/code&gt; are bitwise
“and”, “or”, “not” respectively.&lt;/p&gt;&lt;p&gt;Specifically, note that &lt;code&gt;rd&lt;/code&gt; and &lt;code&gt;rs1&lt;/code&gt; can be
the same. For example, this instruction swaps the value in
&lt;code&gt;a0&lt;/code&gt; and &lt;code&gt;mscratch&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;csrrw a0, mscratch, a0&lt;/code&gt;
&lt;p&gt;For the “immediate” variants, instead of a register, they take an “unsigned”/zero-extended 5-bit immediate value, i.e. an immediate value 0 through 31, inclusive. This is represented using &lt;code&gt;uimm5&lt;/code&gt; in
the assembly syntax description. The operation is the same
otherwise.&lt;/p&gt;&lt;code&gt;csrrwi&lt;/code&gt; (“CSR
read write immediate”): &lt;code&gt;{ csr = uimm5; rd = csr_old; }&lt;/code&gt;&lt;code&gt;csrrsi&lt;/code&gt; (“CSR
read set immediate”):
&lt;code&gt;{ csr = csr | uimm5; rd = csr_old; }&lt;/code&gt;&lt;code&gt;csrrci&lt;/code&gt; (“CSR
read clear immediate”):
&lt;code&gt;{ csr = csr &amp;amp; ~uimm5; rd = csr_old; }&lt;/code&gt;&lt;p&gt;The full feature set of these instructions are designed for manipulating bit fields in CSRs, which we will not be doing that much of in this tutorial. Still, this orthogonal design should be fairly intuitive to remember.&lt;/p&gt;&lt;p&gt;CSRs and fields in CSRs do not behave like general purpose registers: Some of them are read/write, some are read-only. Also, invalid values have special behaviors. We will touch on more details as we introduce the individual CSRs themselves, but one thing you may have noticed is that we don’t seem to have read-only CSR instructions. Read-only access is achieved using special cases in the instruction encodings:&lt;/p&gt;&lt;code&gt;csrrs&lt;/code&gt; and &lt;code&gt;csrrc&lt;/code&gt; do not write to the CSR if
&lt;code&gt;rs1&lt;/code&gt; is &lt;code&gt;x0&lt;/code&gt; (a.k.a. &lt;code&gt;zero&lt;/code&gt;) (Note
that just the value of &lt;code&gt;rs1&lt;/code&gt; being 0 is not enough.)&lt;code&gt;csrrsi&lt;/code&gt; and &lt;code&gt;csrrci&lt;/code&gt; do not write to the CSR
if &lt;code&gt;uimm5&lt;/code&gt; is 0.&lt;p&gt;While we’re at it:&lt;/p&gt;&lt;code&gt;csrrw&lt;/code&gt; and &lt;code&gt;csrrwi&lt;/code&gt; do not read the CSR if
&lt;code&gt;rd&lt;/code&gt; is &lt;code&gt;x0&lt;/code&gt; (a.k.a. &lt;code&gt;zero&lt;/code&gt;). (Note
that writing to &lt;code&gt;x0&lt;/code&gt; has no effect anyway, since it’s
constant 0.)&lt;p&gt;(No standard RISC-V CSR is write-only, or has side effects on read.)&lt;/p&gt;&lt;p&gt;As a convenience, the pseudoinstructions &lt;code&gt;csrr&lt;/code&gt; (“CSR read”) and &lt;code&gt;csrw&lt;/code&gt; (“CSR write”) are
available. &lt;code&gt;csrw csr, rs1&lt;/code&gt; expands to
&lt;code&gt;csrrw x0, csr, rs1&lt;/code&gt;. Meanwhile, &lt;code&gt;csrr rd, csr&lt;/code&gt;
expands specifically to &lt;code&gt;csrrs rd, csr, x0&lt;/code&gt;, just so we can
agree on an encoding.&lt;/p&gt;&lt;code&gt;csrw csr, rs1
csrr rd, csr&lt;/code&gt;
&lt;p&gt;You may have seen these CSR things if you’ve scrolled down on the register view. Yes, we’re finally getting into those.&lt;/p&gt;&lt;p&gt;An example of CSRs is counters. Two basic read-only counters are &lt;code&gt;cycle&lt;/code&gt; and
&lt;code&gt;instret&lt;/code&gt;. These
counters, well, count the number of “cycles” and “instructions
retired”. “Retired” is a technical term basically meaning “successfully
completed”.&lt;/p&gt;&lt;p&gt;Since a 32-bit counter will overflow quite fast, on RV32, the counters have “high” counterparts: &lt;code&gt;cycleh&lt;/code&gt; and &lt;code&gt;instreth&lt;/code&gt;. So, for
example, the full cycle counter has 64 bits, with the lower 32 bits in
the CSR &lt;code&gt;cycle&lt;/code&gt; and higher 32 bits in the CSR
&lt;code&gt;cycleh&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;While the emulator is running, scroll down on the register view panel, and on the bottom you’ll see the values of these counters. For convenience, they’re shown combined, so, &lt;code&gt;cycle = 0x11223344_55667788&lt;/code&gt; means &lt;code&gt;cycleh&lt;/code&gt; is
&lt;code&gt;0x11223344&lt;/code&gt;, and &lt;code&gt;cycle&lt;/code&gt; is
&lt;code&gt;0x55667788&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;On real hardware &lt;code&gt;cycle&lt;/code&gt; is coupled to the clock cycle. In
this emulator, every time you press “Step”, it counts as a cycle. When
you press “Run” and it starts, well, running, a certain number of cycles
happen periodically.&lt;/p&gt;&lt;p&gt;Let’s look at a really simple example:&lt;/p&gt;&lt;p&gt;It takes 4 cycles for this program to stop, but &lt;code&gt;instret&lt;/code&gt;
ends up at only 3 because the final &lt;code&gt;ebreak&lt;/code&gt; instruction
never actually completes.&lt;/p&gt;&lt;p&gt;(Do not confuse “retired” with “retried”.)&lt;/p&gt;&lt;p&gt;A program can read its own counters. For example, this fun little program loops until the cycle count is over 1000, assuming the low 32 bits doesn’t overflow before it has time to react:&lt;/p&gt;&lt;p&gt;Technically &lt;code&gt;cycle&lt;/code&gt; and &lt;code&gt;instret&lt;/code&gt; are not part
of the privileged architecture. The real fun begins now.&lt;/p&gt;&lt;p&gt;The emulator shows the current privilege level as &lt;code&gt;(priv)&lt;/code&gt;. It is in parentheses to remind you of a very
important fact:&lt;/p&gt;&lt;p&gt;There is no CSR for the current privilege level.&lt;/p&gt;&lt;p&gt;In general, it is not possible for a RISC-V program to learn what privilege level it’s in. This is required for the Popek and Goldberg conditions of virtualization to work, specifically because being able to read the current privilege level at a lower-than-maximum privilege level would be a “sensitive” but “unprivileged” instruction.&lt;/p&gt;&lt;p&gt;If you’re writing a program for a certain privilege level, you should simply assume that it is correctly being run at that privilege level.&lt;/p&gt;&lt;p&gt;A fundamental way an operating system does its job is through handling exceptions. In general, exceptions occur when there’s a problem with a specific instruction, and execution cannot continue. For example, since &lt;code&gt;cycle&lt;/code&gt; is a read-only CSR, writing to it is
an illegal instruction:&lt;/p&gt;&lt;p&gt;Since we have no exception handling in the program, we’ll have to inspect what happened manually in the emulator. Indeed, a lot has happened:&lt;/p&gt;&lt;p&gt;Firstly, this message tells you that an exception happened:&lt;/p&gt;&lt;code&gt;[ Exception: Illegal instruction (2) | tval = 0xc0001073, epc = 0x4000000c ]&lt;/code&gt;
&lt;p&gt;The same information is now also available in the CSRs, as follows:&lt;/p&gt;&lt;code&gt;mcause&lt;/code&gt; (“M-mode
trap cause”): The kind of exception.&lt;code&gt;mepc&lt;/code&gt; (“M-mode
exception pc”): The address of the instruction that caused the
exception.&lt;code&gt;mtval&lt;/code&gt; (“M-mode
trap value”): Extra information about the exception.&lt;code&gt;mstatus&lt;/code&gt; (“M-mode
status”): It is set to &lt;code&gt;0x00001800&lt;/code&gt;. The two bits in the
middle, &lt;code&gt;mstatus[12:11]&lt;/code&gt; (In C syntax,
&lt;code&gt;(mstatus &amp;gt;&amp;gt; 11) &amp;amp; 0x3&lt;/code&gt;) is the
&lt;code&gt;mstatus.MPP&lt;/code&gt; (“M-mode previous privilege level”) field,
which contains 3, meaning that the exception occurred while running in
Machine mode.&lt;p&gt;When an exception happens, in addition to recording the exception information in these CSR fields, &lt;code&gt;pc&lt;/code&gt; is set to
&lt;code&gt;mtvec&lt;/code&gt;, which is supposed to be the handler address. Let’s
write ourselves an exception handler that simply prints a message and
stops the emulator, and see the handling in action:&lt;/p&gt;&lt;p&gt;Yeah it just prints &lt;code&gt;Oh no!&lt;/code&gt; on error. Baby steps…&lt;/p&gt;&lt;p&gt;The checkboxes “Pause on exc.” and “Print on exc.” control whether the emulator should pause or print a message, respectively, when an exception occurs. You can uncheck those if you want the exception handler set in the program to run without interference.&lt;/p&gt;&lt;p&gt;(Another case that will cause a jump to &lt;code&gt;mtvec&lt;/code&gt; is interrupts. However, this feature
does not exist in the emulator. The two cases are collectively called
traps.)&lt;/p&gt;&lt;p&gt;These are the exceptions possible in this emulator, and their respective numeric codes:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;Instruction address misaligned&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;Instruction access fault&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;Illegal instruction&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;cell&gt;Breakpoint&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;Load access fault&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;Store/AMO access fault&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;Environment call from User mode&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;11&lt;/cell&gt;&lt;cell&gt;Environment call from Machine mode&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;“Instruction address misaligned” happens when attempting to jump to an instruction that is not 4-byte aligned. The exception happens on the jump or branch instruction, not the target.&lt;/p&gt;&lt;p&gt;“Load access fault” and “Store/AMO access fault” happens when accessing an invalid memory address, or accessing a memory address in an invalid way.&lt;/p&gt;&lt;p&gt;(“AMO” stands for “atomic memory operation”, which we will not talk about and is not featured in the emulator.)&lt;/p&gt;&lt;p&gt;“Illegal instruction” happens not only in the self explanatory way when an invalid instruction is executed, but also when accessing a CSR in an invalid way, or from too low a privilege level.&lt;/p&gt;&lt;p&gt;“Breakpoint”, “Environment call from User mode” and “Environment call from Machine mode” will be explained in a future section.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;mret&lt;/code&gt; (“M-mode
return”) instruction performs the reverse of part of what happens when
an exception occurs. To be precise, what happens is:&lt;/p&gt;&lt;code&gt;mstatus.MPP&lt;/code&gt;&lt;code&gt;mstatus.MPP&lt;/code&gt; is set to 0&lt;code&gt;pc&lt;/code&gt; is set to &lt;code&gt;mepc&lt;/code&gt;&lt;p&gt;(You can think of the privilege mode bits as shifting in a chain &lt;code&gt;0 → MPP → priv&lt;/code&gt;. And, to be even more precise,
&lt;code&gt;mstatus.MPP&lt;/code&gt; is set to the lowest supported privilege mode
since it’s not supposed to contain unsupported modes.)&lt;/p&gt;&lt;p&gt;&lt;code&gt;mret&lt;/code&gt; takes no operands, so the assembly syntax is
simply:&lt;/p&gt;&lt;code&gt;mret&lt;/code&gt;
&lt;p&gt;If we do &lt;code&gt;mret&lt;/code&gt; after getting an exception, then we simply
go back to retrying the same instruction again. This is useful for more
featureful implementations, where for example, after handling a page
fault the correct course of action is to retry the faulting
instruction.&lt;/p&gt;&lt;p&gt;However, &lt;code&gt;mstatus&lt;/code&gt; and &lt;code&gt;mepc&lt;/code&gt; are also
writable. This gives us more flexibility in the use of
&lt;code&gt;mret&lt;/code&gt;. As an analogy, the same &lt;code&gt;jr&lt;/code&gt; instruction
(really &lt;code&gt;jalr&lt;/code&gt; instruction) can be used to return from a
call, and also can be used to jump to any address. Similarly,
&lt;code&gt;mret&lt;/code&gt; not only lets us return from an exception, but also
lets us jump to any address and switch to any privilege
level.&lt;/p&gt;&lt;p&gt;Even though &lt;code&gt;mret&lt;/code&gt; is named “return”, it is in fact the
only way to lower the privilege level to enter User mode.
Here’s an example of entering User mode, with a User mode program that
does something bad:&lt;/p&gt;&lt;p&gt;As you can see, after we enter User mode, all of the CSRs used for exception handling become completely inaccessible, not even readable. As with writing a read-only CSR, accessing an CSR without permission also causes an illegal instruction exception.&lt;/p&gt;&lt;p&gt;Moreover, when an exception happens, we go back to Machine mode, so the exception handler runs in Machine mode. Here the handler does nothing except stopping the emulator.&lt;/p&gt;&lt;p&gt;Sometimes, a program may wish to intentionally cause an exception. There are several well-defined way to do that:&lt;/p&gt;&lt;code&gt;unimp&lt;/code&gt; has the same encoding
as &lt;code&gt;csrrw zero, cycle, zero&lt;/code&gt;, and it is the canonical RV32I
illegal instruction. It causes causes an “Illegal instruction”
exception.&lt;code&gt;ebreak&lt;/code&gt; causes a
“Breakpoint” exception&lt;code&gt;ecall&lt;/code&gt; causes an
“Environment call from User mode” exception when executed in User mode,
and “Environment call from Machine mode” exception when executed in
Machine mode.&lt;p&gt;Give those exceptions a try here:&lt;/p&gt;&lt;p&gt;As the names suggest, &lt;code&gt;ebreak&lt;/code&gt; is used for debugging
breakpoints. As a special case, in this emulator &lt;code&gt;ebreak&lt;/code&gt; in
Machine mode stops the emulator. You can think of it as the emulator
being a debugger, and the debugger catching the breakpoint.&lt;/p&gt;&lt;p&gt;&lt;code&gt;unimp&lt;/code&gt; can be used to intentionally crash a program upon
detection of some unrecoverable error.&lt;/p&gt;&lt;p&gt;Meanwhile, &lt;code&gt;ecall&lt;/code&gt; is used for things like system calls.
“Environment call from User mode” is a distinct exception cause code to
make it easy to check specifically for this case.&lt;/p&gt;&lt;p&gt;One thing that you would want in your trap handler is to not trust or disturb any general purpose registers in the code that the trap occurred in, unless you intentionally want to do so, for example to return a value from a system call. So you’d want to save all the registers to memory, before doing anything else. However, accessing memory requires a general purpose register.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;mscratch&lt;/code&gt;
(“M-mode scratch”) CSR can help with this. This register, unlike all the
others, have no special functionality. It can hold any 32-bit value.
However, like all the other M-mode CSRs, it can only be accessed in
Machine mode. User mode code cannot change the value of it.&lt;/p&gt;&lt;p&gt;So for example, you can stash the operating system stack pointer in &lt;code&gt;mscratch&lt;/code&gt; before switching to User mode, and it will stay in
&lt;code&gt;mscratch&lt;/code&gt; untouched in User mode. At the top of the handler,
&lt;code&gt;csrrw sp, mscratch, sp&lt;/code&gt; to swap from the user stack pointer
to the operating system stack pointer.&lt;/p&gt;&lt;code&gt;handler:
    csrrw sp, mscratch, sp
    # Save registers except sp
    csrr t0, mscratch
    # t0 = user sp, save it
    # Save user pc
    ...&lt;/code&gt;
&lt;p&gt;And, to restore:&lt;/p&gt;&lt;code&gt;    lw t0, ... # Load user pc
    csrw mepc, t0
    lw t0, ... # Load user sp
    csrw mscratch, t0
    # Restore registers except sp
    csrrw sp, mscratch, sp
    mret&lt;/code&gt;
&lt;p&gt;We’ll see the full code for this in the following section.&lt;/p&gt;&lt;p&gt;We have enough of to write a very very bare bones operating system. It will support these features:&lt;/p&gt;&lt;code&gt;a7 = 1&lt;/code&gt;: putchar, &lt;code&gt;a0&lt;/code&gt; is the byte to
write&lt;code&gt;a7 = 2&lt;/code&gt;: exit&lt;p&gt;We design the exception handling as follows:&lt;/p&gt;&lt;code&gt;mscratch&lt;/code&gt; is 0.&lt;code&gt;mscratch&lt;/code&gt; points to the operating
system stack pointer&lt;code&gt;mscratch&lt;/code&gt; is 0, the exception came
from M-mode, which we cannot handle, so we report a fatal
exception.&lt;code&gt;trap_main&lt;/code&gt;, which manipulates
U-mode registers in memory&lt;code&gt;trap_main&lt;/code&gt;, we restore registers from memory,
deallocate the space from the stack, and go back to U-mode, as outlined
in the previous section.&lt;p&gt;The structure to save registers in is fairly simple:&lt;/p&gt;&lt;code&gt;struct regs {
  unsigned long pc;
  unsigned long ra; // x1
  unsigned long sp; // x2
  ...
  unsigned long t6; // x31
};&lt;/code&gt;
&lt;p&gt;Basically you can think of it as an array where element 0 is &lt;code&gt;pc&lt;/code&gt;, and elements 1 through 31 are registers x1 through
x31.&lt;/p&gt;&lt;p&gt;Inside &lt;code&gt;trap_main&lt;/code&gt;, we check &lt;code&gt;mcause&lt;/code&gt; to see if
it’s a system call. If it is, we dispatch based on &lt;code&gt;a7&lt;/code&gt;. If
it’s not, we report an exception from U-mode.&lt;/p&gt;&lt;p&gt;At the beginning, we simply initialize the &lt;code&gt;struct regs&lt;/code&gt;
structure on stack, initialize user &lt;code&gt;sp&lt;/code&gt; and &lt;code&gt;pc&lt;/code&gt;
in it, and jump to the same code that handles returning to U-mode.&lt;/p&gt;&lt;p&gt;Here’s the assembly code with User mode code at the bottom. You may want to uncheck “Pause on exc.” and “Print on exc.” for convenience.&lt;/p&gt;&lt;p&gt;Do not be too hard on yourself if you have trouble understanding the code fully. This is, after all, a fairly complete OS kernel entry and exit implementation. Really, the most important part I’m showing you here is that it is possible.&lt;/p&gt;&lt;p&gt;For reference, here’s some of the OS code in pseudo-C.&lt;/p&gt;&lt;code&gt;void trap_main(struct regs *regs) {
    unsigned long cause = csr_read(mcause);
    if (cause != 8)
        do_bad_exception(regs, cause);

    # Call do_syscall with args from ecall
    unsigned long ret = do_syscall(regs-&amp;gt;a0, ..., regs-&amp;gt;a7);
    regs-&amp;gt;a0 = ret;

    // Bump user pc by 4, skip over ecall instruction
    regs-&amp;gt;pc += 4;
}

unsigned long do_syscall(
    unsigned long a0,
    ...,
    unsigned long a7
) {
    if (a7 == 1)
        sys_putchar(a0);
    else if (a7 == 8)
        sys_exit();
    else
        return -1;
}

unsigned long sys_putchar(char a) {
    kputchar(a);
    return 0;
}

[[noreturn]]
unsigned long sys_exit(char a) {
    ebreak();
}

[[noreturn]]
void do_bad_exception(struct regs *regs, unsigned long cause) {
    kputs("Exception 0x");
    kputchar(hex_chars[cause]);
    kputchar('\n');
    ebreak();
}

[[noreturn]]
void fatal() {
    kputs("Fatal exception\n");
    ebreak();
}

void kputs(const char *str) {
    while (*str) {
        u32 val = (u32)*str;
        writel(0x10000000, val); // MMIO write
        str ++;
    }
}

void kputchar(char c) {
    u32 val = (u32)c;
    writel(0x10000000, val); // MMIO write
}&lt;/code&gt;
&lt;p&gt;And here’s the user code, again in pseudo C:&lt;/p&gt;&lt;code&gt;[[noreturn]]
void user_entry() {
    puts(...);
    exit();
}

void puts(const char *str) {
    while (*str) {
        putchar(*str);
        str ++;
    }
}

void putchar(char c) {
    ecall(a0 = c, a7 = 1);
}

void exit() {
    ecall(a7 = 2);
}&lt;/code&gt;
&lt;p&gt;As long as this tutorial is, some simplifications have been made. Here are some of the most egregious lies and omissions, compared to the “real” RISC-V architecture and “real” RISC-V assembly code found in the world:&lt;/p&gt;&lt;code&gt;li&lt;/code&gt; pseudoinstruction should support a wider range
of constants.&lt;code&gt;mstatus&lt;/code&gt; is a lot more complicated than what I have
described.&lt;code&gt;%hi&lt;/code&gt;, &lt;code&gt;%lo&lt;/code&gt;, &lt;code&gt;%pcrel_hi&lt;/code&gt;,
&lt;code&gt;%pcrel_lo&lt;/code&gt; are more complicated than what I have
described.&lt;p&gt;There are also very important topics that are common or even ubiquitous in the RISC-V world, but I chose not to cover:&lt;/p&gt;&lt;p&gt;However, what I’ve taught you should be more than enough to get you started into learning more on your own, or with further materials.&lt;/p&gt;&lt;p&gt;Here are some references and tutorials I would personally recommend, if you’re looking to get further into RISC-V low-level development&lt;/p&gt;&lt;p&gt;Other useful resources that I have used while writing this tutorial:&lt;/p&gt;&lt;code&gt;arch/riscv/kernel/entry.S&lt;/code&gt; from Linux https://elixir.bootlin.com/linux/latest/source/arch/riscv/kernel/entry.S&lt;p&gt;Thanks to these folks for UI design help and content suggestions:&lt;/p&gt;&lt;p&gt;And thanks to you for coming along with me on this journey. Come on over to https://github.com/dramforever/easyriscv if you have suggestions, grievances, or just want to share some thoughts.&lt;/p&gt;&lt;p&gt;This tutorial is under the CC0 license. To the maximum extent permitted by law, this tutorial is dedicated to the public domain.&lt;/p&gt;&lt;code&gt;add&lt;/code&gt;&lt;code&gt;addi&lt;/code&gt;&lt;code&gt;and&lt;/code&gt;&lt;code&gt;andi&lt;/code&gt;&lt;code&gt;auipc&lt;/code&gt;&lt;code&gt;beq&lt;/code&gt;&lt;code&gt;bge&lt;/code&gt;&lt;code&gt;bgeu&lt;/code&gt;&lt;code&gt;blt&lt;/code&gt;&lt;code&gt;bltu&lt;/code&gt;&lt;code&gt;bne&lt;/code&gt;&lt;code&gt;call&lt;/code&gt;&lt;code&gt;csrr&lt;/code&gt;&lt;code&gt;csrrc&lt;/code&gt;&lt;code&gt;csrrci&lt;/code&gt;&lt;code&gt;csrrs&lt;/code&gt;&lt;code&gt;csrrsi&lt;/code&gt;&lt;code&gt;csrrw&lt;/code&gt;&lt;code&gt;csrrwi&lt;/code&gt;&lt;code&gt;csrw&lt;/code&gt;&lt;code&gt;ebreak&lt;/code&gt;&lt;code&gt;ecall&lt;/code&gt;&lt;code&gt;j&lt;/code&gt;&lt;code&gt;jal&lt;/code&gt;&lt;code&gt;jalr&lt;/code&gt;&lt;code&gt;jr&lt;/code&gt;&lt;code&gt;la&lt;/code&gt;&lt;code&gt;lb&lt;/code&gt;&lt;code&gt;lbu&lt;/code&gt;&lt;code&gt;lh&lt;/code&gt;&lt;code&gt;lhu&lt;/code&gt;&lt;code&gt;li&lt;/code&gt;&lt;code&gt;lui&lt;/code&gt;&lt;code&gt;lw&lt;/code&gt;&lt;code&gt;mret&lt;/code&gt;&lt;code&gt;mv&lt;/code&gt;&lt;code&gt;or&lt;/code&gt;&lt;code&gt;ori&lt;/code&gt;&lt;code&gt;ret&lt;/code&gt;&lt;code&gt;sb&lt;/code&gt;&lt;code&gt;sh&lt;/code&gt;&lt;code&gt;sll&lt;/code&gt;&lt;code&gt;slli&lt;/code&gt;&lt;code&gt;slt&lt;/code&gt;&lt;code&gt;slti&lt;/code&gt;&lt;code&gt;sltiu&lt;/code&gt;&lt;code&gt;sltu&lt;/code&gt;&lt;code&gt;sra&lt;/code&gt;&lt;code&gt;srai&lt;/code&gt;&lt;code&gt;srl&lt;/code&gt;&lt;code&gt;srli&lt;/code&gt;&lt;code&gt;sub&lt;/code&gt;&lt;code&gt;sw&lt;/code&gt;&lt;code&gt;unimp&lt;/code&gt;&lt;code&gt;xor&lt;/code&gt;&lt;code&gt;xori&lt;/code&gt;&lt;code&gt;imm&lt;/code&gt;&lt;code&gt;pc&lt;/code&gt;&lt;code&gt;rd&lt;/code&gt;&lt;code&gt;rs1&lt;/code&gt;&lt;code&gt;rs2&lt;/code&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45726192</guid><pubDate>Mon, 27 Oct 2025 20:57:12 +0000</pubDate></item><item><title>OpenAI says over a million people talk to ChatGPT about suicide weekly</title><link>https://techcrunch.com/2025/10/27/openai-says-over-a-million-people-talk-to-chatgpt-about-suicide-weekly/</link><description>&lt;doc fingerprint="340b5d718824effe"&gt;
  &lt;main&gt;
    &lt;p&gt;OpenAI released new data on Monday illustrating how many of ChatGPT’s users are struggling with mental health issues and talking to the AI chatbot about it. The company says that 0.15% of ChatGPT’s active users in a given week have “conversations that include explicit indicators of potential suicidal planning or intent.” Given that ChatGPT has more than 800 million weekly active users, that translates to more than a million people a week.&lt;/p&gt;
    &lt;p&gt;The company says a similar percentage of users show “heightened levels of emotional attachment to ChatGPT,” and that hundreds of thousands of people show signs of psychosis or mania in their weekly conversations with the AI chatbot.&lt;/p&gt;
    &lt;p&gt;OpenAI says these types of conversations in ChatGPT are “extremely rare,” and thus difficult to measure. That said, the company estimates these issues affect hundreds of thousands of people every week.&lt;/p&gt;
    &lt;p&gt;OpenAI shared the information as part of a broader announcement about its recent efforts to improve how models respond to users with mental health issues. The company claims its latest work on ChatGPT involved consulting with more than 170 mental health experts. OpenAI says these clinicians observed that the latest version of ChatGPT “responds more appropriately and consistently than earlier versions.”&lt;/p&gt;
    &lt;p&gt;In recent months, several stories have shed light on how AI chatbots can adversely affect users struggling with mental health challenges. Researchers have previously found that AI chatbots can lead some users down delusional rabbit holes, largely by reinforcing dangerous beliefs through sycophantic behavior.&lt;/p&gt;
    &lt;p&gt;Addressing mental health concerns in ChatGPT is quickly becoming an existential issue for OpenAI. The company is currently being sued by the parents of a 16-year-old boy who confided his suicidal thoughts to ChatGPT in the weeks leading up to his suicide. State attorneys general from California and Delaware — which could block the company’s planned restructuring — have also warned OpenAI that it needs to protect young people who use their products.&lt;/p&gt;
    &lt;p&gt;Earlier this month, OpenAI CEO Sam Altman claimed in a post on X that the company has “been able to mitigate the serious mental health issues” in ChatGPT, though he did not provide specifics. The data shared on Monday appears to be evidence for that claim, though it raises broader issues about how widespread the problem is. Nevertheless, Altman said OpenAI would be relaxing some restrictions, even allowing adult users to start having erotic conversations with the AI chatbot.&lt;/p&gt;
    &lt;head rend="h3"&gt;2-FOR-1 DISCOUNT: Bring a +1 and save 60%&lt;/head&gt;
    &lt;head rend="h4"&gt;Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla — some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don’t miss 300+ showcasing startups in all sectors.&lt;lb/&gt;Bring a +1 and save 60% on their pass, or get your pass by Oct 27 to save up to $444.&lt;/head&gt;
    &lt;head rend="h3"&gt;2-FOR-1 DISCOUNT: Bring a +1 and save 60%&lt;/head&gt;
    &lt;head rend="h4"&gt;Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla — some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don’t miss 300+ showcasing startups in all sectors. Bring a +1 and save 60% on their pass, or get your pass by Oct 27 to save up to $444.&lt;/head&gt;
    &lt;p&gt;In the Monday announcement, OpenAI claims the recently updated version of GPT-5 responds with “desirable responses” to mental health issues roughly 65% more than the previous version. On an evaluation testing AI responses around suicidal conversations, OpenAI says its new GPT-5 model is 91% compliant with the company’s desired behaviors, compared to 77% for the previous GPT‑5 model.&lt;/p&gt;
    &lt;p&gt;The company also says its latest version of GPT-5 also holds up to OpenAI’s safeguards better in long conversations. OpenAI has previously flagged that its safeguards were less effective in long conversations.&lt;/p&gt;
    &lt;p&gt;On top of these efforts, OpenAI says it’s adding new evaluations to measure some of the most serious mental health challenges facing ChatGPT users. The company says its baseline safety testing for AI models will now include benchmarks for emotional reliance and non-suicidal mental health emergencies.&lt;/p&gt;
    &lt;p&gt;OpenAI has also recently rolled out more controls for parents of children who use ChatGPT. The company says it’s building an age prediction system to automatically detect children using ChatGPT, and impose a stricter set of safeguards.&lt;/p&gt;
    &lt;p&gt;Still, it’s unclear how persistent the mental health challenges around ChatGPT will be. While GPT-5 seems to be an improvement over previous AI models in terms of safety, there still seems to be a slice of ChatGPT’s responses that OpenAI deems “undesirable.” OpenAI also still makes its older and less-safe AI models, including GPT-4o, available for millions of its paying subscribers.&lt;/p&gt;
    &lt;p&gt;If you or someone you know needs help, call 1-800-273-8255 for the National Suicide Prevention Lifeline. You can also text HOME to 741-741 for free; text 988; or get 24-hour support from the Crisis Text Line. Outside of the U.S., please visit the International Association for Suicide Prevention for a database of resources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45727060</guid><pubDate>Mon, 27 Oct 2025 22:26:30 +0000</pubDate></item><item><title>Are these real CVEs? VulDB entries for dnsmasq rely on replacing config files</title><link>https://seclists.org/oss-sec/2025/q4/79</link><description>&lt;doc fingerprint="33f230fc92fc080b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;oss-sec mailing list archives&lt;/head&gt;
    &lt;head rend="h1"&gt;Re: Questionable CVE's reported against dnsmasq&lt;/head&gt;
    &lt;p&gt;From: Moritz Mühlenhoff &amp;lt;jmm () inutil org&amp;gt;&lt;/p&gt;
    &lt;p&gt;Date: Mon, 27 Oct 2025 19:21:54 +0000&lt;/p&gt;
    &lt;quote&gt;On Mon, Oct 27, 2025 at 09:34:03AM -0700, Alan Coopersmith wrote:&lt;/quote&gt;
    &lt;quote&gt;Among the new CVE's published this weekend were these from the VulDB CNA: For all three bugs, the documented "exploit" requires "Replace the default configuration file (/etc/dnsmasq.conf) with the provided malicious file." and if you can replace the server's configuration file you don't need to play games with putting invalid contents in to break the parser, but can simply change the configuration directly.&lt;/quote&gt;
    &lt;quote&gt;The same nonsense also happened for the Kamailio SIP server (CVE-2025-12204, CVE-2025-12205, CVE-2025-12206 and CVE-2025-12207). Cheers, Moritz&lt;/quote&gt;
    &lt;head rend="h3"&gt;Current thread:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Questionable CVE's reported against dnsmasq Alan Coopersmith (Oct 27) &lt;list rend="ul"&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Jeremy Stanley (Oct 27) &lt;list rend="ul"&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Andrew Latham (Oct 27) &lt;list rend="ul"&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Sebastian Pipping (Oct 27)&lt;/item&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Stuart Henderson (Oct 27)&lt;/item&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Sebastian Pipping (Oct 27)&lt;/item&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Matthew Fernandez (Oct 27)&lt;/item&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Eli Schwartz (Oct 27)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Andrew Latham (Oct 27) &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Re: Questionable CVE's reported against dnsmasq Jeremy Stanley (Oct 27) &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Re: Questionable CVE's reported against dnsmasq Collin Funk (Oct 27)&lt;/item&gt;
      &lt;item&gt;Re: Questionable CVE's reported against dnsmasq Michael Orlitzky (Oct 27) &lt;list rend="ul"&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Hank Leininger (Oct 27)&lt;/item&gt;&lt;item&gt;Re: Questionable CVE's reported against dnsmasq Solar Designer (Oct 27)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Re: Questionable CVE's reported against dnsmasq Demi Marie Obenour (Oct 27)&lt;/item&gt;
      &lt;item&gt;Re: Questionable CVE's reported against dnsmasq nightmare . yeah27 (Oct 27)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45727137</guid><pubDate>Mon, 27 Oct 2025 22:35:00 +0000</pubDate></item><item><title>Iroh-blobs</title><link>https://www.iroh.computer/blog/iroh-blobs-0-95-new-features</link><description>&lt;doc fingerprint="1e5ea82579a148e8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;iroh-blobs 0.95 - New features&lt;/head&gt;by rklaehn&lt;p&gt;Iroh-blobs 0.95 contains a number of significant new features that are worth explaining in detail. There are several new features that are useful for blobs users and also for iroh users in general.&lt;/p&gt;&lt;p&gt;Let's start with a feature that is essential for blobs itself, but can also be useful for many other protocols.&lt;/p&gt;&lt;head rend="h1"&gt;Connection pool&lt;/head&gt;&lt;p&gt;There is a new connection pool in &lt;code&gt;util::connection_pool&lt;/code&gt;. This is useful whenever you have a protocol that has to talk to a large number of endpoints while keeping an upper bound of concurrent open connections. In blobs, this is used whenever you use the downloader to orchestrate blobs downloads from multiple providers.&lt;/p&gt;&lt;p&gt;Iroh connections are relatively lightweight, but even so you don't want to keep thousands of them open at the same time. But opening a new connection every time you do a small exchange with a peer is very wasteful. The &lt;code&gt;ConnectionPool&lt;/code&gt; gives you an API to deal with these tradeoffs.&lt;/p&gt;&lt;head rend="h2"&gt;Basic usage&lt;/head&gt;&lt;p&gt;Let's first look at basic usage:&lt;/p&gt;&lt;code&gt;let pool = ConnectionPool::new(ep, iroh_blobs::ALPN, Options::default());
let conn = pool.get_or_connect(remote_id)?;
/// use the connection as usual.
&lt;/code&gt;&lt;p&gt;&lt;code&gt;get_or_connect&lt;/code&gt; will try to get an existing connection from the pool. If there is none, it will create one and store it. The connection will be kept in the pool for a configurable time. Idle connections will be closed as needed. So you can just use this as a drop-in replacement for endpoint.connect and be sure that you won't ever create an unbounded number of connections.&lt;/p&gt;&lt;head rend="h2"&gt;Advanced features&lt;/head&gt;&lt;p&gt;There are some advanced features that can be configued using non-default options.&lt;/p&gt;&lt;code&gt;pub struct Options {
    pub idle_timeout: Duration,
    pub connect_timeout: Duration,
    pub max_connections: usize,
    pub on_connected: Option&amp;lt;OnConnected&amp;gt;,
}
&lt;/code&gt;&lt;p&gt;You can configure the max number of connections to be retained, the maximum tolerable duration for connection establishment, and the max duration connections are kept when idle.&lt;/p&gt;&lt;p&gt;So far, pretty straightforward. There is an additional option to perform some setup before the connection is handed out to the user. For example, you can reject connections based on the data available at this time from the endpoint and the connection, or wait for the connection to reach a certain state before handing it out.&lt;/p&gt;&lt;p&gt;As an example, you might want to do iroh-blobs transfers only on direct connections in order to get good performance or reduce bandwidth use on the relay. If establishing direct connections is not possible, the connection establishment would time out, and you would never even attempt a transfer from such a node.&lt;/p&gt;&lt;code&gt;async fn on_connected(ep: Endpoint, conn: Connection) -&amp;gt; io::Result&amp;lt;()&amp;gt; {
    let Ok(id) = conn.remote_node_id() else {
        return Err(io::Error::other("unable to get node id"));
    };
    let Some(watcher) = ep.conn_type(id) else {
        return Err(io::Error::other("unable to get conn_type watcher"));
    };
    let mut stream = watcher.stream();
    while let Some(status) = stream.next().await {
        if let ConnectionType::Direct { .. } = status {
            return Ok(());
        }
    }
    Err(io::Error::other("connection closed before becoming direct"))
};
let options = Options::default().with_on_connected(on_connected);
let pool = ConnectionPool::new(ep, iroh_blobs::ALPN, options);

let conn = pool.get_or_connect(remote_id)?;
/// use the connection as usual.
&lt;/code&gt;&lt;p&gt;The code to await a direct connection will change quite a bit once we have QUIC multipath. But the capability will remain, and we will update the test code to reflect the new API.&lt;/p&gt;&lt;p&gt;The connection pool is generic enough that it will move to its own crate together with some other iroh utilities. It lives in blobs only until iroh 1.0 is released.&lt;/p&gt;&lt;p&gt;Until then, just depend on iroh-blobs. Iroh-blobs without persistent storage is a very lightweight dependency.&lt;/p&gt;&lt;p&gt;One thing to keep in mind when using the connection pool: the connection pool needs the ability to track which connections are currently being used. To do this, the connection pool does not return &lt;code&gt;Connection&lt;/code&gt; but &lt;code&gt;ConnectionRef&lt;/code&gt;, a struct that derefs to &lt;code&gt;Connection&lt;/code&gt; but contains some additional lifetime tracking.&lt;/p&gt;&lt;p&gt;But &lt;code&gt;Connection&lt;/code&gt; is &lt;code&gt;Clone&lt;/code&gt;, so in principle there is nothing stopping you from cloning the wrapped connection and losing the lifetime tracking. Don't do this. If you work with connections from the pool, you should pass around either a &lt;code&gt;ConnectionRef&lt;/code&gt; or a &lt;code&gt;&amp;amp;Connection&lt;/code&gt; to make sure the underlying &lt;code&gt;ConnectionRef&lt;/code&gt; stays alive.&lt;/p&gt;&lt;p&gt;Incorrect usage of &lt;code&gt;ConnectionRef&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;fn handle_connection(connection: Connection) { tokio::spawn(...) }

let conn = pool.get_or_connect(remote_id)?;
handle_connection(conn.clone()); // clones the Connection out of the ConnectionRef.
/// The ConnectionRef will be dropped here, and the pool will consider the connection idle!
&lt;/code&gt;&lt;p&gt;Correct usage of &lt;code&gt;ConnectionRef&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;fn handle_connection(connection: ConnectionRef) { tokio::spawn(...) }

let conn = pool.get_or_connect(remote_id)?;
handle_connection(conn.clone());
/// The ConnectionRef will be moved into the task, and its lifetime will be properly tracked!
&lt;/code&gt;&lt;p&gt;We experimented with a safer callback-based API, but it turned out to be just too inconvenient to use.&lt;/p&gt;&lt;head rend="h1"&gt;Abstract request and response streams&lt;/head&gt;&lt;p&gt;Iroh-blobs is a protocol that tries to avoid overabstraction. For example as of now you can only use the BLAKE3 hash function, and we hardcode the chunk group size to a value that should work well for all users.&lt;/p&gt;&lt;p&gt;But sometimes there are cases where a bit of abstraction is needed. There was a user request to be able to use compression with iroh-blobs in sendme. One way to do this is to compress files before adding them to the blob store. But this has various downsides. It requires you to create a copy of all data before adding it to the blob store, and will also not lead to very good compression rates when dealing with a large number of small files, since each file will have to be compressed in isolation.&lt;/p&gt;&lt;p&gt;It would be better to compress requests and response streams of the entire protocol and expose the resulting protocol under a different ALPN. With this approach the compression algorithm would be able to find redundancies between multiple files when handling a request for multiple blobs.&lt;/p&gt;&lt;p&gt;This was previously impossible since iroh-blobs worked directly with &lt;code&gt;iroh::endpoint::SendStream&lt;/code&gt; and &lt;code&gt;iroh::endpoint::RecvStream&lt;/code&gt;. So we added traits to allow wrapping send and receive stream in a transform such as compression/decompression.&lt;/p&gt;&lt;p&gt;By default, iroh-blobs still works directly with &lt;code&gt;iroh::endpoint::SendStream&lt;/code&gt; and &lt;code&gt;iroh::endpoint::RecvStream&lt;/code&gt;, so for normal use nothing changes.&lt;/p&gt;&lt;p&gt;The traits are a bit similar to Stream and Sink, but with two important additions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;We allow sending and receiving Bytes, since iroh streams work with bytes internally. That way we avoid a copy in the default case.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;We have methods stop and reset to close the stream, and on the send stream a method stopped that returns a future that resolves when the remote side has closed the stream.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Wrapping the entire iroh-blobs protocol into compression is pretty straightforward except for some boilerplate. We have an example compression.rs that shows how to do this.&lt;/p&gt;&lt;p&gt;We will have this as an optional feature of sendme in one of the next releases.&lt;/p&gt;&lt;p&gt;Just like the connection pool, these traits are generally useful whenever you want to derive iroh protocols by wrapping existing protocols, so they will move to a separate crate once iroh 1.0 is released.&lt;/p&gt;&lt;head rend="h1"&gt;Enhanced provider events&lt;/head&gt;&lt;p&gt;This change is from iroh-blobs 0.93&lt;/p&gt;&lt;p&gt;On the provider side, it is now possible to have very detailed events about what the provider is doing. The provider events are now implemented as an irpc protocol. For each request type you can use an event mask to configure if you want to be notified at all, and if you need the ability to intercept the request, e.g. if you only want to serve certain hashes.&lt;/p&gt;&lt;p&gt;There is an example how to use the new provider events to limit by provider node id or hash.&lt;/p&gt;&lt;p&gt;Here is a provider event handler that serves only blobs requests for hashes in a fixed set of allowed hashes:&lt;/p&gt;&lt;code&gt;fn limit_by_hash(allowed_hashes: HashSet&amp;lt;Hash&amp;gt;) -&amp;gt; EventSender {
    let mask = EventMask {
        // We want to get a request for each get request that we can answer
        // with OK or not OK depending on the hash. We do not want detailed
        // events once it has been decided to handle a request.
        get: RequestMode::Intercept,
        ..EventMask::DEFAULT
    };
    let (tx, mut rx) = EventSender::channel(32, mask);
    n0_future::task::spawn(async move {
        while let Some(msg) = rx.recv().await {
            if let ProviderMessage::GetRequestReceived(msg) = msg {
                let res = if !msg.request.ranges.is_blob() {
                    Err(AbortReason::Permission)
                } else if !allowed_hashes.contains(&amp;amp;msg.request.hash) {
                    Err(AbortReason::Permission)
                } else {
                    Ok(())
                };
                msg.tx.send(res).await.ok();
            }
        }
    });
    tx
}
&lt;/code&gt;&lt;head rend="h1"&gt;What's next&lt;/head&gt;&lt;p&gt;The next major feature in iroh-blobs will be a minimal version of multiprovider downloads for individual blobs.&lt;/p&gt;&lt;p&gt;As soon as iroh 1.0 is released, several generic parts of iroh-blobs will move to a separate iroh utilities crate.&lt;/p&gt;&lt;p&gt;To get started, take a look at our docs, dive directly into the code, or chat with us in our discord channel.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45727557</guid><pubDate>Mon, 27 Oct 2025 23:28:13 +0000</pubDate></item><item><title>AI can code, but it can't build software</title><link>https://bytesauna.com/post/coding-vs-software-engineering</link><description>&lt;doc fingerprint="15f1b593ac321d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you noticed that quite a few people are looking for technical cofounders or CTOs right now? I, for one, get a surprising amount of these queries; most of them along the lines of “hey, I have this vibe-coded app, would you like to make it production-ready”. I have sort of a profile for these people. Think someone who knows their business but has always lacked the technical skills to make their ideas happen — a legal counsel, perhaps, or an account manager.&lt;/p&gt;
    &lt;p&gt;Why would these people need me? That's what I've thought about a little bit, and I think there is an important signal here: What is it exactly that they can’t get done with GenAI alone? This is something everyone is trying to understand, right? Everyone wants to know what these models can do. Or, to be a little blunt, everyone wants to know which jobs are soon to become obsolete. The fact that I get these requests says something about software engineering. I mean, if software engineering was automated, no one would be looking for technical cofounders.&lt;/p&gt;
    &lt;p&gt;Well, I think I know why we get these proposals. The thing is that AI can code, but it can't build software. This is the conclusion I've come to after spending a significant amount of time writing AI-assisted code and watching demos by other people.&lt;/p&gt;
    &lt;p&gt;There is old wisdom that says: Coding is easy, software engineering is hard. It seems fair enough to say that LLMs are already able to automate a lot of coding. GPT-5 and the like solve isolated well-defined problems with a pretty nice success rate. Coding, however, is not what most people are getting paid for. Building a production-ready app is not coding, it’s software engineering.&lt;/p&gt;
    &lt;p&gt;The way I see it is that coding becomes software engineering around the point where you try to turn your demo into a real product — which happens to be exactly the point where these people reach out to you with their pitch.&lt;/p&gt;
    &lt;p&gt;I don’t really know why AI can't build software (for now). Maybe it has to do with the nature of the job. When you write software for a living, your main task is to deal with complexity. The average production software only does a bunch of easy things. The challenge is doing hundreds of these easy things at once, and keeping the whole thing maintainable. Or, to rephrase this in the present context: It's one thing to demonstrate a feature. It's a much more difficult thing to build that feature in a manner that supports integration, expansion, and long-term maintainability.&lt;/p&gt;
    &lt;p&gt;When you look at the code these people send you, you realize that “making the app production-ready” really means torching the whole thing and starting from scratch.&lt;/p&gt;
    &lt;p&gt;I think this says a lot about where we are at right now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45727664</guid><pubDate>Mon, 27 Oct 2025 23:41:32 +0000</pubDate></item><item><title>Complete Digitization of Leonardo da Vinci's Codex Atlanticus</title><link>https://www.openculture.com/2025/10/digitization-of-leonardo-da-vincis-codex-atlanticus.html</link><description>&lt;doc fingerprint="3a053512a6ea9c9c"&gt;
  &lt;main&gt;
    &lt;p&gt;No historical figure better fits the definition of “Renaissance man” than Leonardo da Vinci, but that term has become so overused as to become misleading. We use it to express mild surprise that one person could use both their left and right hemispheres equally well. But in Leonardo’s day, people did not think of themselves as having two brains, and the worlds of art and science were not so far apart as they are now.&lt;/p&gt;
    &lt;p&gt;That Leonardo was able to combine fine arts and fine engineering may not have been overly surprising to his contemporaries, though he was an extraordinarily brilliant example of the phenomenon. The more we learn about him, the more we see how closely related the two pursuits were in his mind.&lt;/p&gt;
    &lt;p&gt;He approached everything he did as a technician. The uncanny effects he achieved in painting were the result, as in so much Renaissance art, of mathematical precision, careful study, and firsthand observation.&lt;/p&gt;
    &lt;p&gt;His artistic projects were also experiments. Some of them failed, as most experiments do, and some he abandoned, as he did so many scientific projects. No matter what, he never undertook anything, whether mechanical, anatomical, or artistic, without careful planning and design, as his copious notebooks testify. As more and more of those notebooks have become available online, both Renaissance scholars and laypeople alike have learned considerably more about how Leonardo’s mind worked.&lt;/p&gt;
    &lt;p&gt;First, there was the Codex Arundel. It is, writes Jonathan Jones at The Guardian, “the living record of a universal mind”—but also, specifically, the mind of a “technophile.” Then, the Victoria and Albert National Art Library announced the digitization of Codex Forster, which contains some of Leonardo’s earliest notebooks. Now The Visual Agency has released a complete digitization of Leonardo’s Codex Atlanticus, a huge collection of the artist, engineer, and inventor’s finely-illustrated notes.&lt;/p&gt;
    &lt;p&gt;“No other collection counts more original papers written by Leonardo,” notes Google. The Codex Atlanticus “consists of 1119 papers, most of them drawn or written on both sides.” Its name has “nothing to do with the Atlantic Ocean, or with some esoteric, mysterious content hidden in its pages.” The 12-volume collection acquired its title because the drawings and writings were bound with the same size paper that was used for making atlases. Gathered in the 16th century by sculptor Pompeo Leoni, the papers descended from Leonardo’s close student Giovan Francesco Melzi, who was entrusted with them after his teacher’s death.&lt;/p&gt;
    &lt;p&gt;The history of the Codex itself makes for a fascinating narrative, much of which you can learn at Google’s Ten Key Facts slideshow. The notebooks span Leonardo’s career, from 1478, when he was “still working in his native Tuscany, to 1519, when he died in France.” The collection was taken from Milan by Napoleon and brought to France, where it remained in the Louvre until 1815, when the Congress of Vienna ruled that all artworks stolen by the former Emperor be returned. (The emissary tasked with returning the Codex could not decipher Leonardo’s mirror writing and took it for Chinese.)&lt;/p&gt;
    &lt;p&gt;The Codex contains not only engineering diagrams, anatomy studies, and artistic sketches, but also fables written by Leonardo, inspired by Florentine literature. And it features Leonardo’s famed “CV,” a letter he wrote to the Duke of Milan describing in nine points his qualifications for the post of military engineer. In point four, he writes, “I still have very convenient bombing methods that are easy to transport; they launch stones and similar such in a tempest full of smoke to frighten the enemy, causing great damage and confusion.”&lt;/p&gt;
    &lt;p&gt;As if in illustration, elsewhere in the Codex, the drawing above appears, “one of the most celebrated” of the collection.” It was “shown to traveling foreigners visiting the Ambrosiana [the Biblioteca Ambrosiana in Milan, where the Codex resides] since the 18th century, usually arousing much amazement.” It is still amazing, especially if we consider the possibility that its artistry might have been something of a byproduct for its creator, whose primary motivation seems to have been solving technical problems—in the most elegant ways imaginable.&lt;/p&gt;
    &lt;p&gt;See the complete digitization of Leonardo’s Codex Atlanticus here.&lt;/p&gt;
    &lt;p&gt;Note: An earlier version of this post appeared on our site in 2019.&lt;/p&gt;
    &lt;p&gt;Related Content:&lt;/p&gt;
    &lt;p&gt;How Leonardo da Vinci Drew an Accurate Satellite Map of an Italian City (1502)&lt;/p&gt;
    &lt;p&gt;Leonardo da Vinci’s Handwritten Resume (Circa 1482)&lt;/p&gt;
    &lt;p&gt;Leonardo Da Vinci’s To-Do List from 1490: The Plan of a Renaissance Man&lt;/p&gt;
    &lt;p&gt;Josh Jones is a writer and musician based in Durham, NC.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45728975</guid><pubDate>Tue, 28 Oct 2025 03:32:09 +0000</pubDate></item><item><title>I built the same app 10 times: Evaluating frameworks for mobile performance</title><link>https://www.lorenstew.art/blog/10-kanban-boards/</link><description>&lt;doc fingerprint="b72ff3394d23c5d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Built the Same App 10 Times: Evaluating Frameworks for Mobile Performance&lt;/head&gt;
    &lt;p&gt;Started evaluating 3 frameworks for work, ended up building 10. Next-gen frameworks (Marko, SolidStart, SvelteKit, Qwik) all deliver instant 35-39ms performance. The real differentiator? Bundle sizes range from 28.8 kB to 176.3 kB compressed. Choose based on your priorities, not microscopic FCP differences.&lt;/p&gt;
    &lt;head rend="h1"&gt;I Built the Same App 10 Times: Evaluating Frameworks for Mobile Performance&lt;/head&gt;
    &lt;head rend="h2"&gt;Why I Built This&lt;/head&gt;
    &lt;p&gt;My team needed to choose a framework for an upcoming app. The requirements were clear: it had to work well on mobile. Not “acceptable on mobile,” but actually good. We’re building tools for real estate agents working in the field: open houses, parking lots, spotty cellular signals. When someone’s standing in front of a potential buyer trying to look professional, a slow-loading app isn’t just an annoyance. It’s a liability.&lt;/p&gt;
    &lt;p&gt;I started with what seemed like a reasonable comparison: Next.js (our current default when a framework is required) versus SolidStart and SvelteKit (alternatives I’d heard good things about). Three frameworks, should be straightforward. But when I built the first implementations and started measuring, something became clear: the issues I was seeing with Next.js weren’t specific to Next.js. They were fundamental to React’s architecture. I wondered whether the other dominant frameworks (Angular, Vue) might have similar mobile web performance limitations.&lt;/p&gt;
    &lt;p&gt;That question changed the scope. If I was going to make a real recommendation for the team, I needed to test all the major meta-frameworks and understand the full landscape of alternatives. Three frameworks became ten. What started as a practical evaluation for work turned into something bigger: a semi-comprehensive look at what’s actually possible for mobile web performance in 2025.&lt;/p&gt;
    &lt;p&gt;This post shares what I discovered. The measurements are real, the kanban apps are identical (same features, same database, same styling), and the differences are dramatic. Marko delivers 12.6 kB raw (6.8 kB compressed). Next.js ships 497.8 kB raw (154.5 kB compressed). That’s a 39x difference in raw size that translates to real seconds on cellular networks.&lt;/p&gt;
    &lt;p&gt;If you’re interested in the theoretical implications of why framework diversity matters, I wrote about that in React Won by Default. This post focuses on the data: what I built, what I measured, and what it means for teams making similar decisions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Takeaways (TL;DR)&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Next-Gen Frameworks Deliver Instant Performance: Marko (39ms), SolidStart (35ms), SvelteKit (38ms), and Nuxt (38ms) all achieve essentially instant First Contentful Paint in the 35-39ms range. This is 12 to 13 times faster than Next.js at 467ms. The 4ms spread between fastest and slowest is statistically measurable but perceptually meaningless to users. All next-gen frameworks feel instant. The real performance story isn’t splitting hairs over 3ms differences, it’s the massive gap between next-gen and React/Angular.&lt;/p&gt;
      &lt;p&gt;Bundle Size Champion: Marko delivers 88.8 kB raw (28.8 kB compressed) for the board page, 6.36 times smaller than Next.js’s 564.9 kB raw (176.3 kB compressed). This is 44% smaller than the next closest competitor (SolidStart at 41.5 kB compressed), making Marko the clear choice when bundle size is the absolute top priority.&lt;/p&gt;
      &lt;p&gt;Resumability Pattern: Qwik City at 114.8 kB raw (58.4 kB compressed) eliminates traditional hydration via resumability, yielding instant interactivity for larger client-side apps. Different architectural approach that solves different problems.&lt;/p&gt;
      &lt;p&gt;Nuxt Proves Established Frameworks Can Compete: At 224.9 kB raw (72.3 kB compressed) with 38ms FCP, Nuxt demonstrates that established “big three” frameworks can achieve next-gen performance when properly configured. Vue’s architecture allows competitive mobile web performance while maintaining a mature ecosystem. React and Angular show no path to similar results.&lt;/p&gt;
      &lt;p&gt;Critical scaling difference: MPA frameworks (Marko, HTMX) ship minimal JavaScript per page, staying lean as you add features. SPA frameworks ship routing and framework runtime upfront, with higher baselines even using code splitting. Marko delivers around 12.6 to 88.8 kB raw regardless of total routes. SPAs maintain 85.9 to 666.5 kB raw baselines plus route chunks.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The key finding? The dominant frameworks show dramatically different results. React has an unavoidable performance ceiling. TanStack Start achieves 373.6 kB raw (118.2 kB compressed) bundles using React 19, only 1.51 times better than Next.js’s 564.9 kB raw (176.3 kB compressed). Angular ships similarly heavy bundles via Analog at 666.5 kB raw (203.4 kB compressed). But Vue (via Nuxt) proved different, achieving competitive 224.9 kB raw (72.3 kB compressed) bundles with instant 38ms FCP that matches next-gen frameworks. Meanwhile, next-gen frameworks like SolidStart deliver 128.6 kB raw (41.5 kB compressed) bundles with equally instant 35ms FCP, 4.39 times smaller than Next.js and 2.91 times smaller than TanStack Start with React. The perfect controlled comparison: TanStack Start with React (373.6 kB raw) versus TanStack Start with Solid (182.6 kB raw). Same meta-framework, same patterns, but React bundles are 2x the size of Solid, isolating React’s runtime cost.&lt;/p&gt;
    &lt;p&gt;Mobile is the web. These measurements matter because mobile web is the primary internet for billions of people. If your app is accessible via URL, people will use it on phones with cellular connections. Optimizing for desktop and hoping mobile is good enough is backwards. The web is mobile. Build for that reality.&lt;/p&gt;
    &lt;p&gt;Each build uses the same database, features, and UI so the comparison stays fair. The differences in raw bundle size for the board page range from 4.39 to 6.36 times smaller compared to Next.js for modern alternatives. Important: These measurements represent disciplined baseline implementations with minimal dependencies. Real production apps typically ship 5 to 10 times more JavaScript from analytics, authentication, feature flags, and third party libraries, meaning the framework differences compound significantly in practice. On mobile devices with cellular connections, this matters enormously.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Before diving in, a reminder from my Progressive Complexity Manifesto: The frameworks compared here represent Level 5 complexity. They are powerful tools for when you need unified client-side state, a lot of reactivity, and/or client-side navigation. But most applications thrive at lower levels. For instance, Level 3 (server-rendered HTML enhanced with HTMX and vanilla JavaScript, as demonstrated in the kanban-htmx app in this repo) can handle complex interactive applications with minimal JavaScript. Level 4 adds occasional Web Components using Lit for reusable elements. These simpler approaches often deliver even smaller bundles and much simpler codebases. This post focuses on Level 5 options for cases that demand them, while remembering simpler paths often suffice.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Why Mobile Web Performance Matters&lt;/head&gt;
    &lt;p&gt;For this evaluation, mobile performance wasn’t just a nice to have. It was the primary constraint. Our users are real estate agents working in the field: open houses with 30 people hammering the same cell tower, parking lots between showings, anywhere but a desk with WiFi. They need tools that work instantly, not “eventually load.”&lt;/p&gt;
    &lt;p&gt;We’re not building a native app. We’re building for the web, which means if it has a URL, people will access it on their phones. And for our users, the app could be used on a phone just as frequently as a desktop.&lt;/p&gt;
    &lt;p&gt;This reality shaped the evaluation. I couldn’t just pick a framework that “works on mobile.” I needed something that genuinely performs well on cellular connections with mid-tier devices. The difference between a framework shipping 30 kB versus 170 kB isn’t academic. It’s the difference between an app that feels professional and one that makes our users look bad in front of clients.&lt;/p&gt;
    &lt;p&gt;The business cost of slow performance: Research from Tammy Everts at SpeedCurve reveals something surprising. While site downtime causes 9% permanent user abandonment, slow performance causes 28% permanent abandonment. That’s over 3x worse. Even more revealing: slowdowns occur 10x more frequently than outages, resulting in roughly 2x total revenue impact despite lower per-hour costs. Beyond the abandonment numbers, slow performance creates a psychological effect where users start perceiving your entire brand negatively. Content seems “boring,” design looks “tacky,” even when those elements haven’t changed. Slowness poisons everything. These aren’t abstract metrics. They’re measurable business costs that compound with every framework kilobyte you ship to mobile users.&lt;/p&gt;
    &lt;p&gt;The real-world cost: A 113 kB difference at 3G speeds (750 kbps) means 1.2 seconds for download plus 500ms to 1s for parse/execution on mobile CPUs. Total: 1.5 to 2 seconds slower between frameworks. On 4G the gap shrinks but remains noticeable. On spotty connections (like an open house with 30 people hammering the same cell tower) it becomes painful.&lt;/p&gt;
    &lt;p&gt;“But it’s cached!” This objection misses reality. Cache busting is standard. Every deployment means users download again. First impressions matter. So do second, third, and tenth impressions. Your users remember waiting.&lt;/p&gt;
    &lt;p&gt;This is why I expanded the evaluation beyond the initial three frameworks. I needed to understand what’s actually possible. When someone pulls up the app in a parking lot between showings, every second counts. Building for mobile performance first means desktop on WiFi is excellent by default. The reverse isn’t true. Optimize for desktop and mobile users suffer.&lt;/p&gt;
    &lt;p&gt;I discovered the difference between frameworks reflects fundamentally different engineering priorities. Some frameworks prioritize runtime flexibility, shipping extensive abstractions to support wide use cases. Others prioritize runtime size and mobile performance from the ground up. The bundle sizes I measured for the board page varied by up to 7x (from 28.8 kB compressed to 203.4 kB compressed), differences that matter enormously on cellular networks.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Experiment Setup&lt;/head&gt;
    &lt;p&gt;I built a Kanban board application ten times, once in each of these frameworks: Next.js 16 (React 19 with built-in compiler) representing React’s Virtual DOM approach with automatic optimization, TanStack Start (also React 19) for a leaner React meta-framework without App Router overhead, TanStack Start + Solid (SolidJS 1.9) using the same meta-framework with fine-grained reactivity, Nuxt 4 (Vue 3) for Vue’s reactive refs with SSR-first developer experience, Analog (Angular 20) using Angular’s modern signals API with meta-framework tooling, Marko (@marko/run) for streaming SSR with fine-grained reactivity, SolidStart (SolidJS 1.9) for native Solid integration with fine-grained reactivity through signals, SvelteKit (Svelte 5) for fine-grained reactivity with runes, Qwik City for resumability instead of hydration, and Astro + HTMX for a traditional MPA approach.&lt;/p&gt;
    &lt;p&gt;Each implementation includes the exact same features: board creation and listing pages, four fixed lists per board (Todo, In Progress, QA, Done), full CRUD operations for cards, drag-and-drop card reordering within lists and movement between lists, assignee assignment from a static user list, tag management, comments on cards with authorship tracking, completion status toggles, optimistic UI updates for drag-and-drop and chart changes (HTMX lacks this though), and server-side form validation using Valibot.&lt;/p&gt;
    &lt;p&gt;All ten apps share the same foundation. The database is SQLite with Drizzle ORM using an identical schema across all implementations. Styling comes from Tailwind CSS plus DaisyUI to keep the UI consistent. Each framework implementation contains roughly 17 components. Most importantly, every app performs real database queries against relational data (boards → lists → cards → tags/comments/users) rather than working with hardcoded arrays.&lt;/p&gt;
    &lt;p&gt;You can check out the code here.&lt;/p&gt;
    &lt;p&gt;A critical choice about dependencies: These apps intentionally minimize dependencies compared to what many developers typically reach for. For mobile web applications, every dependency represents a choice to ship additional kilobytes to users. I used necessary UI libraries like drag-and-drop packages (which vary by ecosystem), but deliberately avoided data fetching libraries, state management helpers, and other utilities that frameworks already handle natively. Each ecosystem has popular packages that add convenience but increase bundle size (React developers often reach for tanstack-query for data fetching, state management libraries, or form helpers). To illustrate the trade-off: tanstack-query alone weighs approximately 13 kB gzipped. That single dependency is already larger than Marko’s entire homepage bundle at 6.8 kB. By avoiding these “nice to have” dependencies and using each framework’s built-in capabilities instead, the bundle differences you’ll see reflect framework architectural choices, not different amounts of functionality or third-party helpers.&lt;/p&gt;
    &lt;p&gt;Measurement Methodology: All bundle sizes in this comparison represent median values from 10 measurement runs with browser cache cleared between each run to ensure cold-load performance measurements. Server warmup requests and IQR outlier removal ensure robust statistics. I report both raw (uncompressed) JavaScript sizes and compressed transfer sizes. The raw size reflects actual code volume generated by each framework and is more consistent for comparison since it doesn’t vary by server compression settings. The compressed size shows what users actually download over the network. See the complete measurement methodology for details on statistical approach, test conditions, and limitations.&lt;/p&gt;
    &lt;p&gt;Here’s how the tech stacks compare:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="11"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Next.js&lt;/cell&gt;
        &lt;cell role="head"&gt;TanStack Start&lt;/cell&gt;
        &lt;cell role="head"&gt;TanStack Start + Solid&lt;/cell&gt;
        &lt;cell role="head"&gt;Nuxt&lt;/cell&gt;
        &lt;cell role="head"&gt;Analog&lt;/cell&gt;
        &lt;cell role="head"&gt;Marko&lt;/cell&gt;
        &lt;cell role="head"&gt;SolidStart&lt;/cell&gt;
        &lt;cell role="head"&gt;SvelteKit&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwik&lt;/cell&gt;
        &lt;cell role="head"&gt;Astro + HTMX&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="11"&gt;
        &lt;cell&gt;Framework&lt;/cell&gt;
        &lt;cell&gt;Next.js 16 (App Router)&lt;/cell&gt;
        &lt;cell&gt;TanStack Start 1.133.8 (w/React)&lt;/cell&gt;
        &lt;cell&gt;TanStack Start 1.133.8&lt;/cell&gt;
        &lt;cell&gt;Nuxt 4&lt;/cell&gt;
        &lt;cell&gt;Analog (Angular)&lt;/cell&gt;
        &lt;cell&gt;@marko/run 0.8&lt;/cell&gt;
        &lt;cell&gt;SolidStart 1.1.0&lt;/cell&gt;
        &lt;cell&gt;SvelteKit + Svelte 5&lt;/cell&gt;
        &lt;cell&gt;Qwik City&lt;/cell&gt;
        &lt;cell&gt;Astro 5 + HTMX&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="11"&gt;
        &lt;cell&gt;UI Library&lt;/cell&gt;
        &lt;cell&gt;React 19 + Compiler&lt;/cell&gt;
        &lt;cell&gt;React 19 + Compiler&lt;/cell&gt;
        &lt;cell&gt;SolidJS 1.9&lt;/cell&gt;
        &lt;cell&gt;Vue 3&lt;/cell&gt;
        &lt;cell&gt;Angular 20&lt;/cell&gt;
        &lt;cell&gt;Marko 6&lt;/cell&gt;
        &lt;cell&gt;SolidJS 1.9&lt;/cell&gt;
        &lt;cell&gt;Svelte 5&lt;/cell&gt;
        &lt;cell&gt;Qwik&lt;/cell&gt;
        &lt;cell&gt;HTMX (server-driven)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="11"&gt;
        &lt;cell&gt;Reactivity Model&lt;/cell&gt;
        &lt;cell&gt;Virtual DOM + Compiler&lt;/cell&gt;
        &lt;cell&gt;Virtual DOM + Compiler&lt;/cell&gt;
        &lt;cell&gt;Signals (fine-grained)&lt;/cell&gt;
        &lt;cell&gt;Reactive refs&lt;/cell&gt;
        &lt;cell&gt;Signals (zoneless)&lt;/cell&gt;
        &lt;cell&gt;Signals (fine-grained)&lt;/cell&gt;
        &lt;cell&gt;Signals (fine-grained)&lt;/cell&gt;
        &lt;cell&gt;Runes (fine-grained)&lt;/cell&gt;
        &lt;cell&gt;Signals + Resumability&lt;/cell&gt;
        &lt;cell&gt;Server-driven (HTMX)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="11"&gt;
        &lt;cell&gt;Data Fetching&lt;/cell&gt;
        &lt;cell&gt;Server Components&lt;/cell&gt;
        &lt;cell&gt;TanStack Router loaders&lt;/cell&gt;
        &lt;cell&gt;TanStack Router loaders&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;useAsyncData&lt;/code&gt; / &lt;code&gt;useFetch&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;injectLoad&lt;/code&gt; + DI&lt;/cell&gt;
        &lt;cell&gt;Route data handlers&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;createAsync&lt;/code&gt; with cache&lt;/cell&gt;
        &lt;cell&gt;Remote functions (&lt;code&gt;query&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;routeLoader$&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Route handlers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="11"&gt;
        &lt;cell&gt;Mutations&lt;/cell&gt;
        &lt;cell&gt;Server Actions&lt;/cell&gt;
        &lt;cell&gt;Server functions (RPC)&lt;/cell&gt;
        &lt;cell&gt;Server functions (RPC)&lt;/cell&gt;
        &lt;cell&gt;API routes (&lt;code&gt;server/api/*&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;ApiService&lt;/code&gt; + RxJS&lt;/cell&gt;
        &lt;cell&gt;POST handlers&lt;/cell&gt;
        &lt;cell&gt;Server functions&lt;/cell&gt;
        &lt;cell&gt;Remote functions (&lt;code&gt;form&lt;/code&gt;/&lt;code&gt;command&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Server actions&lt;/cell&gt;
        &lt;cell&gt;API routes + HTMX&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="11"&gt;
        &lt;cell&gt;Database&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
        &lt;cell&gt;Drizzle ORM + better-sqlite3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="11"&gt;
        &lt;cell&gt;Styling&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
        &lt;cell&gt;DaisyUI&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="11"&gt;
        &lt;cell&gt;Drag &amp;amp; Drop&lt;/cell&gt;
        &lt;cell&gt;@dnd-kit/core, @dnd-kit/sortable&lt;/cell&gt;
        &lt;cell&gt;@dnd-kit/core, @dnd-kit/sortable&lt;/cell&gt;
        &lt;cell&gt;@thisbeyond/solid-dnd&lt;/cell&gt;
        &lt;cell&gt;@formkit/drag-and-drop&lt;/cell&gt;
        &lt;cell&gt;@angular/cdk&lt;/cell&gt;
        &lt;cell&gt;@formkit/drag-and-drop&lt;/cell&gt;
        &lt;cell&gt;@thisbeyond/solid-dnd&lt;/cell&gt;
        &lt;cell&gt;Native HTML5&lt;/cell&gt;
        &lt;cell&gt;Native HTML5&lt;/cell&gt;
        &lt;cell&gt;@formkit/drag-and-drop&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Build Tool&lt;/cell&gt;
        &lt;cell&gt;Turbopack&lt;/cell&gt;
        &lt;cell&gt;Vite&lt;/cell&gt;
        &lt;cell&gt;Vite&lt;/cell&gt;
        &lt;cell&gt;Vite&lt;/cell&gt;
        &lt;cell&gt;Vite + Angular&lt;/cell&gt;
        &lt;cell&gt;Vite&lt;/cell&gt;
        &lt;cell&gt;Vinxi&lt;/cell&gt;
        &lt;cell&gt;Vite&lt;/cell&gt;
        &lt;cell&gt;Vite + Qwik optimizer&lt;/cell&gt;
        &lt;cell&gt;Vite&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This isn’t a todo list with hardcoded arrays. It’s a real app with database persistence, complex state management, and the kind of interactions you’d actually build for a real product.&lt;/p&gt;
    &lt;head rend="h3"&gt;Framework Architectures at a Glance&lt;/head&gt;
    &lt;p&gt;To avoid repetition throughout this post, here are the key architectural approaches for each framework tested:&lt;/p&gt;
    &lt;p&gt;React-based (Next.js, TanStack Start + React): Use Virtual DOM reconciliation where components re-render and React diffs changes before updating the DOM. React’s compiler automatically optimizes components through memoization using a Control Flow Graph-based High-Level Intermediate Representation, reducing manual optimization needs but not bundle size. Next.js employs React Server Components (RSC) which serialize component trees into a special RSC Payload format, adding meta-framework overhead. TanStack Start uses traditional SSR without RSC complexity. Both ship React’s runtime including Virtual DOM reconciler, synthetic event system, and platform abstractions, creating unavoidable baseline costs for mobile users.&lt;/p&gt;
    &lt;p&gt;Solid-based (SolidStart, TanStack Start + Solid): Fine-grained reactivity via signals with read/write segregation where getters are separate from setters. JSX syntax similar to React, but signals automatically track dependencies, eliminating manual dependency arrays and rules of hooks. Components run once during initial render; subsequent updates happen directly at the reactive primitive level without re-executing component functions, minimizing CPU overhead on mobile devices. TanStack Start provides more feature-rich routing which causes slightly larger bundles compared to SolidStart’s leaner integration.&lt;/p&gt;
    &lt;p&gt;SvelteKit: Compile-time optimization that transforms components into imperative DOM updates, with minimal runtime overhead since the compiler does most work at build time. Runes ($state, $derived, $effect) powered by signals enable fine-grained reactivity, with universal reactive primitives that work in .js/.ts files beyond just .svelte components. The compiler converts developer-written code into lean, optimized production code. This approach generates JavaScript with smaller bundles through aggressive tree-shaking, helping mobile performance on both network transfer and parse time.&lt;/p&gt;
    &lt;p&gt;Nuxt (Vue): Reactive refs with &lt;code&gt;.value&lt;/code&gt; access for reactivity tracking. Uses aggressive optimization including compile cache for faster cold starts and reactive keys for intelligent data fetching. In Vue 3 the reactivity system has been refactored for improved performance and memory efficiency, critical for mobile devices. Vapor Mode (experimental, not used here) offers a compile-first approach that bypasses Virtual DOM entirely, compiling templates directly to native DOM operations with significantly smaller runtime overhead. Despite being a “big three” framework, Nuxt achieves competitive bundle sizes and exceptional runtime performance, with support for mixed component trees combining different rendering strategies.&lt;/p&gt;
    &lt;p&gt;Analog (Angular): Modern signals API provides fine-grained reactivity through primitives like signal, effect, linkedSignal, queries, and inputs. Zoneless mode enables removing zone.js from bundles entirely, eliminating its synchronization overhead which improves mobile CPU efficiency. Uses dependency injection patterns and ships with RxJS for enterprise reactive patterns, creating heavier bundles despite signals-based reactivity. Angular remains a “batteries-included” framework where common functionality is built-in rather than requiring third-party libraries. Incremental hydration reduces time-to-interactive by hydrating components progressively rather than all at once.&lt;/p&gt;
    &lt;p&gt;Marko: Streaming SSR with fine-grained reactivity powered by compiler analysis. The compiler analyzes the reactivity graph at build time and compiles away components themselves, shipping only the minimal code needed for events and effects, achieving zero component overhead at runtime. Statically analyzes which components are stateful versus server-only, breaking pages into top-level stateful components and selectively serializing only needed data. HTML-first syntax with automatic dependency tracking eliminates boilerplate. Supports streaming asynchronous SSR with selective hydration where only interactive parts ship JavaScript to the client, critical for minimizing mobile bundle size.&lt;/p&gt;
    &lt;p&gt;Qwik City: Resumability architecture that serializes application state and component boundaries directly into HTML during server rendering, allowing the client to “resume” execution without traditional hydration that requires re-executing components to attach event listeners. Employs fine-grained lazy loading down to the component level, deferring JavaScript downloads until actual user interaction occurs. Event handlers, component logic, and complex interactions are delivered lazily on-demand, eliminating bulk initial JavaScript execution that burdens mobile CPUs. Optimized for edge platforms with distributed deployment, delivering sub-second load times on mobile networks. Best suited for complex client-heavy applications requiring instant interactivity.&lt;/p&gt;
    &lt;p&gt;Astro + HTMX: Multi-page architecture (MPA) where Astro serves as a simple HTML renderer with no client-side JavaScript framework. HTMX handles all interactivity through declarative HTML attributes that trigger server requests and swap HTML fragments into the page. Instead of client-side state management, interactions send requests to the server which returns HTML snippets that HTMX injects into the DOM. This approach ships minimal JavaScript (just the HTMX library) and keeps pages lean as routes increase. Best suited for applications where server round trips are acceptable and client-side reactivity isn’t critical. Trades rich client-side state management for extreme simplicity and tiny bundles, optimal for form-driven or content-heavy applications.&lt;/p&gt;
    &lt;p&gt;TanStack Start: Meta-framework with a client-first architecture philosophy (versus server-first approaches like Next.js RSC), maintaining powerful server capabilities while prioritizing client-side routing and state management. Router-centric design where the majority of framework functionality comes from TanStack Router, which is framework-agnostic and supports React and Solid. Provides isomorphic loaders that work on both server and client, streaming SSR for progressive rendering, and server functions for type-safe RPC. React version ships traditional hydration with React’s baseline costs, while Solid version achieves roughly half the bundle size using identical routing infrastructure, demonstrating how UI library choice impacts mobile performance.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Fairness Check: Pinned versions, identical data volume on Board page, normalized CSS/icon handling (treeshake/purge). All measurements use Chrome Lighthouse with mobile emulation (Pixel 5, 4G throttling, 1x CPU). The measurement script uses 1x CPU to isolate bundle size impact from CPU performance variance. Cache is cleared between each measurement run to simulate first-visit experience.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Why I Expanded from Three to Ten Frameworks&lt;/head&gt;
    &lt;p&gt;When I started this evaluation, I expected to compare Next.js, SolidStart, and SvelteKit, then make a recommendation. But building those first three implementations revealed something I hadn’t anticipated: the performance issues I saw in Next.js weren’t just a React problem. They were likely systemic across the “big three” dominant framework ecosystems (React, Angular, Vue).&lt;/p&gt;
    &lt;p&gt;React (via Next.js) ships 154.5 to 176.3 kB compressed (486.1 to 564.9 kB raw) with poor runtime performance at 467ms FCP. Angular (via Analog) ships 125.4 to 203.4 kB compressed (430.3 to 666.5 kB raw). Both suffer from heavy baseline bundles that create performance costs for mobile users. Vue (via Nuxt) tells a dramatically different story. Nuxt ships competitive bundle sizes at 72.3 kB compressed (224.9 kB raw) AND achieves exceptional 38ms FCP, making it faster than all React and Angular options and competitive with next-gen frameworks like SvelteKit (38ms, tied) and Marko (39ms). This puts Nuxt in a unique position: it’s the only “big three” meta-framework that competes on mobile web performance. React requires architectural changes to achieve similar results. Angular has no clear path forward. Nuxt proved that with proper optimization, even established frameworks can deliver next-gen performance.&lt;/p&gt;
    &lt;p&gt;React’s explicit strategy: React Native for mobile. In practice, React’s web runtime trades bundle size for other goals. Many teams pursuing top-tier mobile performance choose React Native. The architectural choices that make React heavy on the web are deliberate. They solve real problems for desktop and app development. But for mobile web, React’s position is essentially: use React Native instead.&lt;/p&gt;
    &lt;p&gt;This is a strategic business decision, not a technical oversight. Facebook (Meta) doesn’t build heavy React web apps on mobile. They invest heavily in React Native and native apps. When you use their mobile app, you’re not running a web browser rendering a React SPA. You’re running native code. React Native is their solution for mobile performance. The React web framework can be expensive because the assumption is that if you care about mobile, you should use a different tool.&lt;/p&gt;
    &lt;p&gt;The problem with this strategy is that it abandons the open web. React Native requires building two separate applications. Your company needs React engineers for the web, different engineers or different skill sets for native mobile development, and App Store difficulties on top.&lt;/p&gt;
    &lt;p&gt;This isn’t just an inconvenience. It’s technofeudalism. React Native solves the mobile performance problem, but it does so by pushing developers out of the open web and into app store platforms where Apple and Google extract up to 30% of transactions, control distribution, and can revoke access at will. React’s mobile strategy inherently drives teams toward platform capture. The web offers an alternative: no gatekeepers, no platform fees, direct distribution. (I explore this dynamic in depth in “The Web is the Last Commons” section below, building on economist Yanis Varoufakis’s analysis of how app stores operate as digital fiefdoms rather than competitive markets.)&lt;/p&gt;
    &lt;p&gt;Other frameworks make a different bet: the web should work well on mobile without requiring a parallel native technology. The teams behind Marko, Solid, Svelte, Qwik, and Vue have done phenomenal engineering work rethinking these fundamentals from first principles. They’ve built innovative solutions that optimize for the web as a first-class platform for mobile. They’re all saying: you shouldn’t need a completely different technology stack just to reach people with phones. The web should be competitive on its own.&lt;/p&gt;
    &lt;p&gt;React’s choice is coherent within their ecosystem strategy. It makes sense given their investment in React Native. But it’s not neutral. It’s a choice that deprioritizes mobile web performance in favor of extensive runtime abstractions. For teams building mobile first web applications, it’s a choice that works against you.&lt;/p&gt;
    &lt;p&gt;That’s why I expanded the evaluation to ten frameworks. If I was going to make an honest recommendation for the team, I needed to understand what’s actually possible. React’s heavy bundle sizes aren’t bugs or poor engineering. They’re the predictable cost of React’s runtime architectural overhead. Angular has similar bundle size issues. Vue showed that the “big three” can compete on mobile web performance when properly configured. For teams building mobile first web applications without the resources for React Native, React and Angular create unavoidable performance limitations, but Nuxt offers a viable path forward.&lt;/p&gt;
    &lt;p&gt;The measurements that follow show exactly what that tradeoff looks like in practice. They also show what happens when frameworks prioritize mobile web performance from the start. Marko at 6.8 kB compressed. Solid at 30.6 kB compressed. Svelte at 47.8 kB compressed. These aren’t just smaller numbers. They’re fundamentally different architectural approaches that treat the web as a first class platform for mobile.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bundle Size Reality Check&lt;/head&gt;
    &lt;head rend="h3"&gt;The Numbers (Versions used)&lt;/head&gt;
    &lt;p&gt;Production builds measured showing raw JavaScript size (with compressed/gzipped transfer size in parentheses). Raw size reflects actual code volume and is more consistent for comparison. Compressed size shows what users download over the network.&lt;/p&gt;
    &lt;p&gt;Framework versions tested: Next.js 16.0.0-beta.0 (React 19.2.0), TanStack Start 1.133.8 (React 19.2.0), Nuxt 4.1.2 (Vue 3.5.22), Analog (Angular core 20.3.3), Marko 6.0.85 with @marko/run 0.8.1, SolidStart (@solidjs/start 1.2.0, solid-js 1.9.9), SvelteKit 2.43.6 (Svelte 5), Qwik City 1.16.1 (Qwik 1.16.1), Astro 5.14.5 + HTMX.&lt;/p&gt;
    &lt;p&gt;These are minimal baseline implementations. Typical production apps include authentication, analytics, feature flags, form libraries, and other dependencies that multiply these numbers significantly. The framework overhead shown here compounds with every additional dependency.&lt;/p&gt;
    &lt;p&gt;Table ordered by board page size (smallest first):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Framework&lt;/cell&gt;
        &lt;cell role="head"&gt;Board Page Raw (Compressed)&lt;/cell&gt;
        &lt;cell role="head"&gt;Homepage Raw (Compressed)&lt;/cell&gt;
        &lt;cell role="head"&gt;Difference from Next.js (Board Page)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Marko&lt;/cell&gt;
        &lt;cell&gt;88.8 kB (28.8 kB)&lt;/cell&gt;
        &lt;cell&gt;12.6 kB (6.8 kB)&lt;/cell&gt;
        &lt;cell&gt;6.36x smaller&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Qwik City&lt;/cell&gt;
        &lt;cell&gt;114.8 kB (58.4 kB)&lt;/cell&gt;
        &lt;cell&gt;88.5 kB (43.6 kB)&lt;/cell&gt;
        &lt;cell&gt;4.92x smaller&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SvelteKit&lt;/cell&gt;
        &lt;cell&gt;125.2 kB (54.1 kB)&lt;/cell&gt;
        &lt;cell&gt;103.4 kB (47.8 kB)&lt;/cell&gt;
        &lt;cell&gt;4.51x smaller&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Astro + HTMX&lt;/cell&gt;
        &lt;cell&gt;127.3 kB (34.3 kB)&lt;/cell&gt;
        &lt;cell&gt;88.9 kB (22.0 kB)&lt;/cell&gt;
        &lt;cell&gt;4.44x smaller&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SolidStart&lt;/cell&gt;
        &lt;cell&gt;128.6 kB (41.5 kB)&lt;/cell&gt;
        &lt;cell&gt;85.9 kB (30.6 kB)&lt;/cell&gt;
        &lt;cell&gt;4.39x smaller&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;TanStack Start + Solid&lt;/cell&gt;
        &lt;cell&gt;182.6 kB (60.4 kB)&lt;/cell&gt;
        &lt;cell&gt;153.0 kB (52.0 kB)&lt;/cell&gt;
        &lt;cell&gt;3.09x smaller&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nuxt&lt;/cell&gt;
        &lt;cell&gt;224.9 kB (72.3 kB)&lt;/cell&gt;
        &lt;cell&gt;224.9 kB (72.3 kB)&lt;/cell&gt;
        &lt;cell&gt;2.51x smaller&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;TanStack Start&lt;/cell&gt;
        &lt;cell&gt;373.6 kB (118.2 kB)&lt;/cell&gt;
        &lt;cell&gt;316.8 kB (100.7 kB)&lt;/cell&gt;
        &lt;cell&gt;1.51x smaller&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Next.js 16&lt;/cell&gt;
        &lt;cell&gt;564.9 kB (176.3 kB)&lt;/cell&gt;
        &lt;cell&gt;497.8 kB (154.5 kB)&lt;/cell&gt;
        &lt;cell&gt;Baseline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Analog&lt;/cell&gt;
        &lt;cell&gt;666.5 kB (203.4 kB)&lt;/cell&gt;
        &lt;cell&gt;430.3 kB (125.4 kB)&lt;/cell&gt;
        &lt;cell&gt;1.18x larger&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Field data validation: The Chrome User Experience Report (CrUX) provides real-world Core Web Vitals data from millions of actual websites using these frameworks on mobile devices. This field data complements the controlled measurements in this post. Important caveat: CrUX data reflects how these frameworks are used in production by average developers, not optimal implementations. If a framework shows poorly in CrUX but well in these tests, it demonstrates what’s possible with proper configuration, performance tuning, and dependency discipline. The gap between field data and optimized implementations reveals opportunity for improvement in real-world usage patterns.&lt;/p&gt;
    &lt;p&gt;The difference between Marko’s 88.8 kB raw (28.8 kB compressed) and Next.js’s 564.9 kB raw (176.3 kB compressed) translates to roughly 1.5 seconds on cellular. These seconds are the baseline. Time waiting to load increases with every feature and every dependency added. Those aren’t just abstract kilobytes. That’s their time, their patience, and ultimately their impression of your product.&lt;/p&gt;
    &lt;p&gt;Critical scaling consideration: These bundle sizes represent a mid-complexity app with multiple routes. MPA frameworks like Marko ship minimal JavaScript per page (6.8 to 28.8 kB compressed per route), staying lean as you add features. SPA frameworks ship routing and framework runtime upfront. Even with code splitting, SPAs maintain higher baselines: Solid/Svelte start at 30.6 to 54.1 kB compressed then add route chunks, while React/Vue/Angular start at 72.3 to 203.4 kB compressed. The architectural model creates different scaling characteristics.&lt;/p&gt;
    &lt;p&gt;Important context on HTMX: The Astro + HTMX implementation achieves excellent bundle sizes with the simplest codebase, but sacrifices client-side reactivity for server-driven interactions. HTMX excels for simpler, form-driven applications where most interactions trigger server requests. However, as your app’s need for rich client-side state management grows, HTMX becomes less practical. For reactive applications, Marko (6.8 to 28.8 kB compressed), Solid (30.6 to 41.5 kB compressed), and Svelte (47.8 to 54.1 kB compressed) maintain small bundles while delivering rich reactivity.&lt;/p&gt;
    &lt;head rend="h3"&gt;React’s Ceiling in Practice (TanStack vs Next)&lt;/head&gt;
    &lt;p&gt;TanStack Start achieves 100.7 to 118.2 kB compressed bundles (316.8 to 373.6 kB raw) while Next.js ships 154.5 to 176.3 kB compressed (497.8 to 564.9 kB raw) in this measurement. Both use React 19. That’s only a 33 to 35% improvement, primarily reflecting App Router + RSC and related runtime.&lt;/p&gt;
    &lt;p&gt;The answer reveals that React’s runtime architecture is the primary cost, not just Next.js’s meta-framework choices.&lt;/p&gt;
    &lt;p&gt;What’s the difference? Next.js ships the full React Server Components runtime plus serialization layers, component boundary management, caching infrastructure, App Router with all its routing features, progressive enhancement for Server Actions, image optimization, and middleware. TanStack Start strips most of that out: traditional SSR without RSC, leaner routing, and simple RPC-style server functions.&lt;/p&gt;
    &lt;p&gt;Both use server-side rendering, but Next.js’s RSC model adds substantial overhead. Server Components render on the server only, Client Components get marked with &lt;code&gt;"use client"&lt;/code&gt;, the server serializes everything to a special format, and the client needs runtime code to deserialize and coordinate those boundaries. TanStack Start uses the simpler traditional SSR approach: render on server, ship HTML, hydrate everything on the client. No serialization, no boundary coordination.&lt;/p&gt;
    &lt;p&gt;In this measurement, Next.js’s App Router + RSC adds roughly 53 to 58 kB compressed. The remaining 100.7 to 118.2 kB compressed (316.8 to 373.6 kB raw) is React’s core runtime cost: reconciliation, event system, and hydration baseline.&lt;/p&gt;
    &lt;p&gt;Compare that to alternatives. SolidStart delivers 30.6 to 41.5 kB compressed (85.9 to 128.6 kB raw) using JSX, 2.91x smaller than TanStack Start with React. SvelteKit achieves 47.8 to 54.1 kB compressed (103.4 to 125.2 kB raw), which is 1.97x to 2.47x smaller than TanStack Start. Qwik delivers 43.6 to 58.4 kB compressed (88.5 to 114.8 kB raw), which is 1.72x to 2.31x smaller.&lt;/p&gt;
    &lt;p&gt;For React teams, the path forward isn’t straightforward. TanStack Start proves you can remove Next.js’s overhead, but you’re still carrying React’s 100.7 to 118.2 kB compressed (316.8 to 373.6 kB raw) baseline. SolidStart offers similar JSX syntax with 2.91x smaller bundles. And if you like TanStack Start’s approach, you can use it with Solid for the same routing patterns with dramatically smaller bundles.&lt;/p&gt;
    &lt;p&gt;Here’s the bottom line: React’s architecture (not just the Virtual DOM, but also synthetic events, platform patching, and sheer feature complexity) creates unavoidable performance costs that no meta-framework optimization can eliminate. To be fair, Virtual DOM implementations can be small (see Preact at 4 kB). React’s size reflects deliberate choices to circumvent platform constraints and provide extensive features. TanStack Start proves this: removing App Router overhead yields only a 33 to 35% improvement. To escape this ceiling and achieve 3 to 4 times smaller bundles, you need a fundamentally different architectural approach. Frameworks that lean into the platform instead of circumventing it can deliver dramatic size reductions. The React team chose to accept these costs to solve other problems (Server Components, unified patterns). That’s a legitimate choice. But it’s not negotiable within React.&lt;/p&gt;
    &lt;head rend="h3"&gt;TanStack Start: React vs Solid&lt;/head&gt;
    &lt;p&gt;Here’s where it gets interesting. TanStack Start is a new meta-framework that currently supports both React and Solid. Using the same meta-framework with two different UI libraries gives us the perfect controlled comparison.&lt;/p&gt;
    &lt;p&gt;TanStack Start with React: Ships 373.6 kB raw (118.2 kB compressed) compared to Next.js’s 564.9 kB raw (176.3 kB compressed). That’s 34% smaller by raw size. If you’re stuck maintaining an existing Next.js codebase, TanStack Start offers a legitimate escape path from App Router complexity while staying in React. But that’s still 373.6 kB raw (118.2 kB compressed) of React’s core runtime.&lt;/p&gt;
    &lt;p&gt;TanStack Start with Solid: Delivers 182.6 kB raw (60.4 kB compressed). That’s 30% larger than SolidStart’s 128.6 kB raw (41.5 kB compressed), but still dramatically better than any React option. The size difference is largely due to TanStack Router having more features than SolidStart’s Router. This buys you additional routing capabilities and framework flexibility.&lt;/p&gt;
    &lt;p&gt;The controlled comparison that matters: React at 373.6 kB raw (118.2 kB compressed) versus Solid at 182.6 kB raw (60.4 kB compressed) using identical TanStack Start infrastructure. Same routing, same SSR approach, same patterns. React bundles are 2x the size of Solid. This isolates React’s runtime cost versus Solid’s architecture. No meta-framework differences, no excuses.&lt;/p&gt;
    &lt;p&gt;All four implementations achieve perfect 100 Lighthouse scores. Bundle size differences are real, but modern devices handle them without impacting perceived performance in this test.&lt;/p&gt;
    &lt;p&gt;For greenfield projects? Don’t choose React. TanStack Start with Solid gives you 182.6 kB raw (60.4 kB compressed) bundles, but native SolidStart delivers 128.6 kB raw (41.5 kB compressed) with tighter integration. If you want the absolute smallest with this architecture, go SolidStart. If you like TanStack Start’s patterns and might want framework flexibility later, TanStack Start with Solid is reasonable. But starting a new project with React (whether Next.js or TanStack Start) means voluntarily accepting 2x to 3x larger bundles for no performance gain.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Verdict: What I’m Recommending&lt;/head&gt;
    &lt;p&gt;After building ten implementations (with help, of course; see the acknowledgements below) and measuring everything, the data gives clear direction. For our mobile first requirements, here’s what I found:&lt;/p&gt;
    &lt;p&gt;The next-gen frameworks all achieve essentially instant performance. The 35-39ms FCP range feels perceptually identical to users, and it’s 12 to 13 times faster than Next.js at 467ms. Since all next-gen frameworks feel equally fast, choose based on bundle size priorities and developer experience rather than microscopic FCP differences.&lt;/p&gt;
    &lt;p&gt;That said, context matters. Not every project can or should switch frameworks.&lt;/p&gt;
    &lt;p&gt;When Next.js still makes sense: For large existing React codebases, migration costs may outweigh performance benefits. If you’re stuck with React and can’t migrate, consider TanStack Start over Next.js for a 21-31% bundle reduction without App Router complexity. That’s a practical business decision. But for greenfield projects? There’s no legacy to maintain, no migration costs to weigh. You’re choosing to build on a foundation that costs your users 2x to 3x more JavaScript on every visit. You’re voluntarily accepting worse performance when better options cost nothing extra. That’s not a neutral technical choice. “We only know React” isn’t a technical constraint, it’s a learning investment decision. And “organizational politics” is real, but it’s not a technical justification. It’s an admission that better options exist but can’t be chosen.&lt;/p&gt;
    &lt;p&gt;Reality check on common objections:&lt;/p&gt;
    &lt;p&gt;“But hiring!” Competent developers learn frameworks. That’s the job. These alternatives are actually easier to learn than React: no rules of hooks, no dependency arrays, no manual memoization dance. The real difficulty isn’t learning curve, it’s creating a engineering culture that acknowledges constraints and makes intentional decisions with these constraints in mind.&lt;/p&gt;
    &lt;p&gt;“But ecosystem!” React’s ecosystem is both advantage and liability. Large libraries ship code for scenarios you’ll never encounter. That date picker with every locale? You need 3 features, you’re shipping 300. For mobile-first projects where every kilobyte matters, this becomes a problem. Modern AI tools make building exactly what you need feasible: generate the function instead of importing 50kB for 3 features. Smaller bundles, code you understand.&lt;/p&gt;
    &lt;p&gt;“But it’s risky!” Shipping 3x larger bundles to mobile users on cellular is the actual risk. Slow loads damage your brand and cost conversions. The “safe choice” has measurable costs.&lt;/p&gt;
    &lt;p&gt;“But my users are desktop-only!” Let’s be honest: “desktop-only” is usually an excuse to skip performance discipline entirely. And it’s rarely true for long. Six months later someone asks “can I check this on my phone?” and suddenly you’re stuck. Better to build it right from the start. Desktop users still benefit from faster parsing and execution. Even on WiFi, 30.6 kB compressed loads noticeably faster than 176.3 kB compressed. More importantly, why would you voluntarily accept 3x worse performance when the better option costs nothing extra? Performance is a feature regardless of screen size. Building with constraints makes you a better engineer. “Desktop-only” shouldn’t mean “no discipline.”&lt;/p&gt;
    &lt;p&gt;Why you should seriously consider the alternatives: The mental models are often simpler (see Framework Architectures section). Alternatives like Solid, Svelte, and Marko streamline patterns with automatic reactivity. Performance comes by default with 2x to 6x smaller bundles requiring no optimization work. Mobile web matters with real users on phones, cellular connections, and mid-tier devices. You’ll write less code, ship less JavaScript, and debug fewer framework quirks. Most importantly, greenfield projects deserve choices made on merit rather than defaults.&lt;/p&gt;
    &lt;p&gt;These alternatives are especially compelling for mobile-first applications where bundle size directly impacts user experience. They matter for the growing demographic of people who prefer phones over computers. Mobile professionals like real estate agents, field service workers, healthcare staff, delivery drivers, and sales reps benefit most. Teams building internal tools or MVPs without enterprise politics constraining decisions can move faster. Developers who value technical excellence over popularity contests will appreciate the engineering quality. Importantly, teams save significant money by maintaining a single high-performance web codebase instead of splitting resources between separate web and native applications. This often means smaller teams, lower overhead, and faster iteration cycles compared to organizations maintaining web apps and native mobile apps.&lt;/p&gt;
    &lt;p&gt;Choosing among the alternatives (organized by primary use case):&lt;/p&gt;
    &lt;p&gt;Smallest Bundles: Choose Marko for the absolute best bundle sizes (6.8 to 28.8 kB compressed). Marko delivers 44% smaller bundles than the next closest competitor, making it the clear winner when bundle size is your top priority. The MPA architecture ships minimal JavaScript per page, staying lean as you add routes. The developer experience is excellent once you embrace its streaming model. Note: Marko 6 is currently in beta (tagged as &lt;code&gt;next&lt;/code&gt; on npm) and expected to leave beta by end of year, with no expected API changes but ongoing bug fixes and optimizations.&lt;/p&gt;
    &lt;p&gt;JSX Familiarity: Choose SolidStart if you want the easiest migration path from React. At 128.6 kB raw (41.5 kB compressed), SolidStart uses JSX syntax with automatic dependency tracking that eliminates manual memoization. This delivers 4.39x smaller bundles than Next.js while feeling immediately familiar to React developers. The mental model is actually simpler than React because signals are more straightforward than hooks.&lt;/p&gt;
    &lt;p&gt;Best All-Around Developer Experience: Choose SvelteKit for approachable syntax and excellent defaults. At 125.2 kB raw (54.1 kB compressed), SvelteKit delivers 4.51x smaller bundles than Next.js with progressive enhancement by default and minimal framework overhead. The compiler-based approach means less runtime code and cleaner component logic. Best for developers from any background seeking readable code with few framework quirks.&lt;/p&gt;
    &lt;p&gt;Resumability Pattern: Choose Qwik City if you have a larger application that demands immediate interactivity on load with significant client-side functionality. At 88.5 to 114.8 kB raw (43.6 to 58.4 kB compressed), Qwik uses resumability instead of hydration, yielding instant time-to-interactive. Different architectural approach that solves different scaling problems.&lt;/p&gt;
    &lt;p&gt;Established Ecosystem: Choose Nuxt if you want Vue’s mature plugin ecosystem with competitive mobile web performance. At 224.9 kB raw (72.3 kB compressed), Nuxt proves that established “big three” frameworks can achieve next-gen performance when properly configured. Best for teams already familiar with Vue, projects that benefit from extensive community plugins, or teams that value the safety of a well-established framework. Nuxt bridges the gap between the familiar and the performant.&lt;/p&gt;
    &lt;p&gt;Important scaling consideration: Marko’s MPA architecture ships minimal JavaScript per page (stays lean as you add routes), while SPAs like SvelteKit and SolidStart ship routing and framework runtime upfront then add route chunks. Both use code splitting, but the architectural models create different performance characteristics at scale.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As discussed in my Progressive Complexity Manifesto, these Level 5 frameworks are only necessary when you need unified client-side state and complex reactivity. Most apps can thrive at lower levels with simpler tools like HTMX and vanilla JS/TS.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When developers have real alternatives, everyone wins. React wouldn’t be adding a compiler if SolidStart and Svelte weren’t proving that automatic optimization matters. The entire ecosystem improves when we stop accepting “good enough” as the ceiling.&lt;/p&gt;
    &lt;p&gt;My recommendation for the team: After building all these implementations, Marko, SolidStart, and SvelteKit are all excellent choices that would serve our mobile first requirements well. All three feel perceptually instant to users. The real question is priorities: absolute smallest bundles (Marko), easiest React migration (SolidStart), or best all-around developer experience (SvelteKit). If the team has Vue experience, Nuxt is also compelling with its mature ecosystem and competitive performance.&lt;/p&gt;
    &lt;p&gt;For personal projects outside of work, I’ll be reaching for SvelteKit and increasingly Marko. Their developer experience just feels right, the code flows naturally, and they make building things fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Reality: What Lighthouse Hides&lt;/head&gt;
    &lt;p&gt;Mobile performance scores on the Board page (median from 10 runs), ordered by FCP (fastest first):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Framework&lt;/cell&gt;
        &lt;cell role="head"&gt;Score&lt;/cell&gt;
        &lt;cell role="head"&gt;FCP (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;LCP (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;TBT (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;CLS&lt;/cell&gt;
        &lt;cell role="head"&gt;Bundle Size Raw (Compressed)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;SolidStart&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;35&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.000&lt;/cell&gt;
        &lt;cell&gt;128.6 kB (41.5 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Nuxt&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.000&lt;/cell&gt;
        &lt;cell&gt;224.9 kB (72.3 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;SvelteKit&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;38&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.000&lt;/cell&gt;
        &lt;cell&gt;125.2 kB (54.1 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Marko&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;39&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.000&lt;/cell&gt;
        &lt;cell&gt;88.8 kB (28.8 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;TanStack Start + Solid&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.013&lt;/cell&gt;
        &lt;cell&gt;182.6 kB (60.4 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;TanStack Start&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.000&lt;/cell&gt;
        &lt;cell&gt;373.6 kB (118.2 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Analog&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;53&lt;/cell&gt;
        &lt;cell&gt;53&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.000&lt;/cell&gt;
        &lt;cell&gt;666.5 kB (203.4 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Astro + HTMX&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;54&lt;/cell&gt;
        &lt;cell&gt;54&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.001&lt;/cell&gt;
        &lt;cell&gt;127.3 kB (34.3 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Qwik&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;58&lt;/cell&gt;
        &lt;cell&gt;58&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.000&lt;/cell&gt;
        &lt;cell&gt;114.8 kB (58.4 kB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Next.js 16&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
        &lt;cell&gt;467&lt;/cell&gt;
        &lt;cell&gt;467&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0.000&lt;/cell&gt;
        &lt;cell&gt;564.9 kB (176.3 kB)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Key Metrics:&lt;/p&gt;
    &lt;p&gt;FCP (First Contentful Paint) indicates when the first content appears on screen. LCP (Largest Contentful Paint) shows when the main content becomes visible. TBT (Total Blocking Time) measures how long the main thread remains blocked and unavailable for user interactions. CLS (Cumulative Layout Shift) evaluates visual stability, where 0 represents perfect stability with no unexpected layout shifts.&lt;/p&gt;
    &lt;p&gt;Measurement Conditions: These scores represent mobile device emulation using Pixel 5 profile with 4G network throttling (10 Mbps download, 40ms round-trip time). I use 1x CPU (no throttling) to isolate bundle size and network impact from CPU performance, which allows fair comparison across frameworks with different runtime characteristics. Each measurement was run 10 times with cache cleared between runs to capture cold-load (first-visit) performance. Server warmup requests and IQR outlier removal ensure robust statistics. Standard deviations vary based on framework characteristics.&lt;/p&gt;
    &lt;p&gt;But here’s what these metrics hide. All frameworks achieve perfect or near perfect scores (100), but the paint times tell the real story about architectural differences. SolidStart achieves the fastest board page load at 35ms FCP. Nuxt and SvelteKit tie for second at 38ms FCP, with Marko close behind at 39ms. TanStack Start with Solid and TanStack Start with React follow at 40ms and 43ms respectively, showing excellent optimization. The top eight frameworks (SolidStart, Nuxt, SvelteKit, Marko, TanStack variants, Analog, Astro, Qwik) all render in under 60ms, achieving near-instant perceived performance. Only Next.js lags dramatically behind at 467ms FCP, representing a 13.3x performance gap compared to SolidStart for identical functionality.&lt;/p&gt;
    &lt;p&gt;Those paint time differences matter in the real world. SolidStart’s 35ms FCP feels instant, while Next.js’s 467ms FCP is noticeably slower. The framework choice directly impacts whether your app feels like a premium tool or a liability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where the Difference Shows&lt;/head&gt;
    &lt;p&gt;All frameworks feel instant in optimal conditions. The twist is this. Smaller bundles (Marko, Solid, Svelte, Qwik) win dramatically on slow networks and mid-tier devices.&lt;/p&gt;
    &lt;p&gt;On desktop with WiFi, all these frameworks are fast. On cellular with a mid-tier phone, 3.54 to 39.2x smaller bundles create measurably better UX. The 130 kB you saved doesn’t just download faster. It also parses and executes faster on mobile CPUs.&lt;/p&gt;
    &lt;p&gt;Hydration vs Resumability:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Traditional (React, Solid, Svelte, Vue, Angular): Download bundle → Parse JS → Execute all components → Attach event listeners (hydration)&lt;/item&gt;
      &lt;item&gt;Marko: Download minimal JS → Run effects and attach event listeners directly (fine-grained tree shaking, no re-execution of server code)&lt;/item&gt;
      &lt;item&gt;Qwik: Download minimal JS → Resume from serialized HTML → Lazily load interaction handlers on demand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both Marko and Qwik are resumable frameworks that avoid traditional hydration. The key difference is that Qwik uses lazy resumability, progressively loading JavaScript based on actual interactions, while Marko analyzes the reactivity graph at build time to bundle exactly the code needed for events and effects; no lazy loading, but maximum precision in what gets shipped. Traditional frameworks must re-execute all components just to wire up event listeners.&lt;/p&gt;
    &lt;head rend="h3"&gt;Addressing Common Critiques&lt;/head&gt;
    &lt;p&gt;I know what some of you are thinking. “Aren’t you comparing MPAs to SPAs unfairly?” And “Is this app complex enough?”&lt;/p&gt;
    &lt;p&gt;App Complexity Defense: This isn’t some toy todo list. It’s a solid mid-complexity app with real database persistence using SQLite plus Drizzle ORM, relational queries across boards to lists to cards to tags and comments, drag-and-drop reordering, (some) optimistic updates, modals, and server validation. It matches the kind of internal tools or MVPs teams crank out every day. The bundle differences come straight from framework overhead, not the features themselves, and those gaps only get bigger at scale with more routes and dependencies. If your production app throws in auth or real-time stuff, framework baselines would just bloat even more, not shrink.&lt;/p&gt;
    &lt;p&gt;MPA vs SPA Nuances: The routing pattern debate misses the point, reactivity models matter way more. With features like View Transitions API and the Speculation Rules API, MPAs like Marko or HTMX feel just as snappy as SPAs for navigation. The real split is in scaling. MPAs ship minimal JS per page, for example Marko sticks to 6.8 to 28.8 kB, while SPAs lug around upfront runtime from 83.9 to 666.5 kB baselines plus chunks.&lt;/p&gt;
    &lt;p&gt;A Note on Ecosystems: the “small ecosystem” concern is often overstated. For mobile-first applications, we should be extremely selective about every dependency we add. Each package increases bundle size and maintenance burden. Modern AI tools like Claude, ChatGPT, and Cursor excel at generating focused code for your specific use case. Instead of importing a 50 kB library for 3 features, AI can help you write exactly what you need in 2 kB. This approach yields smaller bundles, code you actually understand, and fewer supply chain risks. Large ecosystems are sometimes advantageous, but they’re also a liability when every import costs your mobile users.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does Complexity Buy You Anything?&lt;/head&gt;
    &lt;p&gt;If Next.js’s bundles are only 21 to 31% larger than TanStack Start (both using React), what are you actually getting for that extra 44 to 54 kB? And more importantly, do the dominant frameworks’ baselines (React at 100.7 to 118.2 kB compressed, Vue at 72.3 kB compressed, Angular at 125.4 to 203.4 kB compressed) buy you anything compared to alternatives that deliver 30.6 to 54.1 kB compressed?&lt;/p&gt;
    &lt;head rend="h3"&gt;The Complexity Tax&lt;/head&gt;
    &lt;p&gt;Each of the big three has conceptual complexity that alternatives avoid. React has rules around hooks and dependency arrays. Angular carries dependency injection patterns and RxJS complexity. Vue requires understanding refs and reactivity tracking. After seeing that TanStack Start (with React) only achieves marginal improvements over Next.js (21 to 31%), it’s clear that framework complexity and bundle weight often go hand-in-hand. Here’s how state management, effects, and data fetching compare:&lt;/p&gt;
    &lt;p&gt;1. State Management&lt;/p&gt;
    &lt;code&gt;// React - useState with functional updates to avoid stale closures
const [count, setCount] = useState(0);
setCount((prev) =&amp;gt; prev + 1);

// Solid - getter/setter pattern, explicit read/write
const [count, setCount] = createSignal(0);
setCount(count() + 1);

// Svelte - looks like normal variables
let count = $state(0);
count = count + 1;

// Vue/Nuxt - .value access for reactivity
const count = ref(0);
count.value = count.value + 1;

// Qwik - .value property, serializable
const count = useSignal(0);
count.value = count.value + 1;

// Angular/Analog - getter/setter like Solid
const count = signal(0);
count.set(count() + 1);

// Marko - direct assignment with automatic reactivity
&amp;lt;let/count=0&amp;gt;
&amp;lt;script&amp;gt;
  count = count + 1; // Automatically reactive
&amp;lt;/script&amp;gt;&lt;/code&gt;
    &lt;p&gt;2. Effects with Dependencies&lt;/p&gt;
    &lt;code&gt;// React - manual dependency array (explicit but error-prone)
useEffect(() =&amp;gt; {
  console.log(count);
}, [count]); // You must maintain this manually. Mistakes cause hard-to-debug stale closures and infinite loops. This is not a documentation problem, it's a design problem.

// Solid - automatic tracking
createEffect(() =&amp;gt; {
  console.log(count()); // Automatically subscribes to count
});

// Svelte - automatic tracking
$effect(() =&amp;gt; {
  console.log(count); // Automatically subscribes to count
});

// Vue/Nuxt - explicit or automatic
watch(
  () =&amp;gt; count.value,
  (val) =&amp;gt; console.log(val)
); // explicit
watchEffect(() =&amp;gt; console.log(count.value)); // automatic

// Qwik - explicit tracking
useTask$(({ track }) =&amp;gt; {
  track(() =&amp;gt; count.value);
  console.log(count.value);
});

// Angular/Analog - automatic tracking
effect(() =&amp;gt; {
  console.log(count()); // Automatically subscribes
});

// Marko - automatic tracking with &amp;lt;script&amp;gt;
&amp;lt;let/count=0&amp;gt;
&amp;lt;script&amp;gt;
  console.log(count); // Automatically subscribes to count
&amp;lt;/script&amp;gt;&lt;/code&gt;
    &lt;p&gt;3. Server Data Fetching&lt;/p&gt;
    &lt;code&gt;// Next.js - async Server Component (implicit server boundary)
// Looks like regular component code but runs on server. No explicit data contract.
// This works beautifully until it doesn't, and when bugs arise from the server/client
// boundary being invisible, they're extremely hard to debug.
export default async function Page() {
  const board = await db.query.boards.findFirst();
  return &amp;lt;div&amp;gt;{board.name}&amp;lt;/div&amp;gt;;
}

// SvelteKit - remote functions with query (experimental)
// .remote.ts file defines server-side query function
export const getBoardData = query(v.string(), async (board_id) =&amp;gt; {
  return await db.query.boards.findFirst();
});

// In component: use $derived rune with await
const boardData = $derived(await getBoardData(params.id));

// SolidStart - streaming with Suspense
// Explicit async resource with streaming support
const board = createAsync(() =&amp;gt; getBoard());
&amp;lt;Suspense&amp;gt;{board()?.name}&amp;lt;/Suspense&amp;gt;;

// Qwik - automatic serialization
// $ suffix marks server-only code, automatically serialized
export const useBoard = routeLoader$(async () =&amp;gt; {
  return await db.query.boards.findFirst();
});

// Nuxt - built-in caching
// Composable with explicit key and built-in deduplication
const { data: board } = await useAsyncData("board", () =&amp;gt;
  db.query.boards.findFirst()
);

// Analog - DI with server data
// Dependency injection brings Angular patterns to server data
export const load = injectLoad(() =&amp;gt; {
  const service = inject(BoardService);
  return service.getBoard();
});

// Marko - route handlers with $global context
// Server handler runs first, sets data on context for page component
// +handler.ts
export async function GET(context) {
  const board = await db.query.boards.findFirst();
  context.board = board; // Available as $global.board in component
}

// +page.marko
&amp;lt;div&amp;gt;${$global.board.name}&amp;lt;/div&amp;gt;;&lt;/code&gt;
    &lt;p&gt;The “big three” frameworks each carry conceptual complexity, though with different outcomes. React has rules around hooks and dependency arrays. Angular brings enterprise patterns like dependency injection alongside RxJS streams. Vue requires understanding refs and .value access, but unlike React and Angular, Vue (via Nuxt) has proven it can deliver competitive mobile web performance despite this complexity. These patterns become familiar with practice but never fully disappear and add cognitive overhead. Meta-frameworks compound these complexities: Next.js adds Server/Client boundaries and RSC serialization, Analog brings full Angular DI to the server, and Nuxt adds caching layers and composable patterns.&lt;/p&gt;
    &lt;p&gt;Most alternatives are conceptually simpler once learned. Svelte is most approachable (looks like enhanced HTML/JS). Solid and Marko use automatic tracking that eliminates manual dependency management. No rules of hooks, no dependency arrays, no .value boilerplate. The simpler mental models correlate with smaller bundles: less runtime complexity means less code to ship.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does Next.js 16’s Built-in Compiler Change Anything?&lt;/head&gt;
    &lt;p&gt;The React team recognizes the complexity problem. Their solution, now fully integrated in Next.js 16: the React Compiler automatically handles memoization to reduce re-renders.&lt;/p&gt;
    &lt;head rend="h3"&gt;What the Compiler Does&lt;/head&gt;
    &lt;p&gt;The React Compiler analyzes your code and inserts &lt;code&gt;useMemo&lt;/code&gt; and &lt;code&gt;useCallback&lt;/code&gt; automatically. In Next.js 16, this is no longer experimental; it’s built-in and enabled by default. It’s a genuine improvement that reduces boilerplate.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Results&lt;/head&gt;
    &lt;p&gt;The compiler helps with re-render optimization and removes manual memoization boilerplate. But it doesn’t address Next.js’s bundle size: Next.js ships 176.3 kB compressed (564.9 kB raw) for the board page while TanStack Start ships 118.2 kB compressed (373.6 kB raw) with the same React 19. The compiler can’t eliminate dependency arrays for &lt;code&gt;useEffect&lt;/code&gt;. You still need to understand hooks, closures, and React’s rendering model.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Irony&lt;/head&gt;
    &lt;p&gt;The compiler improves React’s developer experience, which is valuable. But it highlights an interesting contrast. React optimizes Virtual DOM re-renders, while alternatives like SolidStart eliminate re-renders entirely through fine-grained reactivity. SvelteKit’s compiler eliminates much of the runtime overhead at build time. Qwik eliminates hydration cost through resumability.&lt;/p&gt;
    &lt;p&gt;The difference in philosophy is clear. React’s compiler optimizes the existing model. The alternatives questioned the model itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Web is the Last Commons: Why This Matters Beyond Frameworks&lt;/head&gt;
    &lt;p&gt;Here’s where this gets bigger than framework choice. When you ship a native app to the App Store or Google Play instead of building a web app, you’re not just making a technical decision. You’re accepting a deal that would’ve been unthinkable twenty years ago. Apple and Google each take up to 30% of every transaction (with exceptions depending on program and category). They set rules. They decide what you can ship. They can revoke your access tomorrow with no recourse. You have no alternative market. You can’t even compete on price because the fee is baked into many transactions.&lt;/p&gt;
    &lt;p&gt;Economist Yanis Varoufakis calls this “technofeudalism” in his book of the same name. The App Store isn’t a marketplace, it’s a fiefdom. Developers are digital serfs, bound to the cloud lords’ land (their platforms) with no exit. Users get locked into this too. The App Store is a curated garden where algorithms owned by two companies decide what you see. Your data gets harvested. Your choices get filtered. You’re not a customer with alternatives, you’re a subject in a walled garden.&lt;/p&gt;
    &lt;p&gt;The web? The web is different. No single company takes a cut. No algorithm curates your choices. Distribution is direct. Users can actually vote with their feet. It’s not perfect, but it’s the closest thing we have left to an open market where developers retain agency and users retain choice.&lt;/p&gt;
    &lt;p&gt;When companies abandon the web to go app-only, they’re not making a neutral technical decision. They’re voluntarily moving their users from a competitive marketplace into a feudal system. And yeah, I know that sounds dramatic, but Varoufakis has spent years documenting how the economics of digital platforms have created exactly this dynamic.&lt;/p&gt;
    &lt;p&gt;Why you should care, no matter what you believe:&lt;/p&gt;
    &lt;p&gt;If you lean capitalist, app stores create an environment that is the opposite of what capitalism is supposed to be. Monopolistic rent extraction replacing competition and innovation. No market mechanism to challenge them. That’s not capitalism, that’s just extraction.&lt;/p&gt;
    &lt;p&gt;If you lean anti-capitalist, technofeudalism is arguably worse than regular capitalism because at least capitalism has friction and regulatory handles. This has neither. It’s total private control with zero market competition.&lt;/p&gt;
    &lt;p&gt;Either way, the web is the last place where economic activity can happen outside the thumb of tech oligarchs. Building web apps matters. Shipping small, fast, performant web apps matters even more, and most web traffic comes from the mobile web. Every kilobyte you save is another reason for teams to choose the web over building a native app subject to app store control and fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion: What This Evaluation Revealed&lt;/head&gt;
    &lt;p&gt;What started as a simple framework comparison for an upcoming work project turned into something more revealing. The data shows clearly what’s possible when frameworks prioritize mobile web performance from the start.&lt;/p&gt;
    &lt;p&gt;The evaluation revealed what happens when you rethink fundamentals. SolidStart, SvelteKit, Qwik, and Marko represent different architectural priorities that push boundaries in ways the dominant frameworks cannot. Competition drives innovation. These alternatives show what’s achievable when mobile web performance is the primary design constraint, not an afterthought.&lt;/p&gt;
    &lt;p&gt;For teams serving mobile professionals on cellular networks (like ours), these costs are paid on every visit. React and Angular often face architectural performance ceilings. Vue proved that established frameworks can compete when properly configured. And remember, these measurements represent initial page loads. MPA frameworks maintain their lean profile across routes, while SPAs add route chunks to their baseline.&lt;/p&gt;
    &lt;p&gt;For anyone starting a new project, the evaluation raises an important question: Is there any reason to make your app 25.9x heavier than necessary? (And that’s before adding the authentication, analytics, and third party libraries that typically multiply production bundle sizes 5 to 10 times, making the real world gap even larger.) Building an app that works well on mobile isn’t difficult if you make good architectural decisions at the beginning. The right choice early on means your app performs well everywhere, not just on desktop WiFi. Choose MPA frameworks for the leanest per page bundles, or lightweight SPAs for excellent performance with familiar patterns.&lt;/p&gt;
    &lt;p&gt;Here’s what the evaluation made clear: if you want to build a better product than your competitors, why would you build exactly like them? When everyone uses Next.js, winning on performance requires fighting React’s architecture. When you use Marko, SolidStart, SvelteKit, or Nuxt instead, the advantage comes easily. Your app is faster by default. Your bundle is smaller without optimization work. Your users get a better experience without extra effort. That’s not just good engineering. That’s differentiation.&lt;/p&gt;
    &lt;p&gt;When you pick Marko and ship 28.8 kB instead of Next.js at 176.3 kB, you’re not just making your users’ experience better on cellular networks. You’re making the web more competitive. You’re making it a more attractive place for companies to exist. You’re pushing back against the gravity that pulls everything toward native only distribution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reproducing These Results&lt;/head&gt;
    &lt;p&gt;All measurements in this comparison follow a rigorous statistical methodology designed for reproducibility and defensibility. Each framework was measured 10 times per page using Chrome Lighthouse with mobile emulation (Pixel 5, 4G throttling, 1x CPU). Server warmup requests stabilize performance before measurements. IQR (Interquartile Range) outlier removal ensures robust statistics. Browser cache was cleared between runs to measure cold-load performance that simulates first-visit experience. I report median values to reduce the impact of outliers, and standard deviations quantify measurement reliability. The complete methodology including statistical approach, test environment details, compression detection, known limitations, and reproducibility instructions is documented at METHODOLOGY.md.&lt;/p&gt;
    &lt;head rend="h2"&gt;Call to Action&lt;/head&gt;
    &lt;p&gt;Try it yourself: Clone the repository, build all ten implementations, and test them on a throttled 3G connection in Chrome DevTools. When mobile web is your only option, the numbers tell a clear story.&lt;/p&gt;
    &lt;p&gt;On a less serious note: You can take a look at all ten apps to examine how the code looks and get a general feel for each framework. I advocate coding for fun, and the code in the repo might be a great place to help you try something new.&lt;/p&gt;
    &lt;p&gt;Share your experience: Have you tried Marko, SolidStart, SvelteKit, Qwik, or Nuxt? What framework would you choose for a mobile-first project and why? I’d love to hear your thoughts on Twitter or Bluesky.&lt;/p&gt;
    &lt;p&gt;Keep exploring: The full metrics data and measurement methodology are available in the repository for you to verify, reproduce, or extend. Build your own comparison and share your findings.&lt;/p&gt;
    &lt;p&gt;The real winner? You. Your team. Your users. When you start your next project with Marko, SolidStart, or SvelteKit, you’ll ship faster, smaller, and with less framework overhead. That’s a real competitive advantage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;A huge thanks to everyone who helped with this evaluation.&lt;/p&gt;
    &lt;p&gt;Early draft reviewers: Alex Russell and Dylan Piercey provided invaluable feedback on the post structure and arguments.&lt;/p&gt;
    &lt;p&gt;Framework-specific reviews:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Marko: Dylan Piercey for reviewing the implementation&lt;/item&gt;
      &lt;item&gt;Qwik: Wout Mertens for reviewing the Qwik implementation&lt;/item&gt;
      &lt;item&gt;Analog: Brandon Roberts for his review&lt;/item&gt;
      &lt;item&gt;Solid/SolidStart: Martin Rapp, Ryan Carniato, and Atila Fassina for their assistance&lt;/item&gt;
      &lt;item&gt;Next.js: Sunny_man for reviewing the Next code&lt;/item&gt;
      &lt;item&gt;Svelte/SvelteKit: The helpful folks in the Svelte Discord, especially Kevin Åberg Kultalahti, Simon H, and Ben McCann&lt;/item&gt;
      &lt;item&gt;TanStack: Manuel Schiller and Brenley Dueck for help with the TanStack apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your contributions made this post more accurate and comprehensive. Thank you!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45729437</guid><pubDate>Tue, 28 Oct 2025 05:22:57 +0000</pubDate></item><item><title>Picture gallery: Amiga prototype "Lorraine" at the Amiga 40 event</title><link>https://www.amiga-news.de/en/news/AN-2025-10-00110-EN.html</link><description>&lt;doc fingerprint="76b8ee2d4cb73196"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;DEUTSCHE VERSION&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Links&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Forums&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Comments&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Report news&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Chat&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Polls&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Newsticker&lt;/cell&gt;
        &lt;cell&gt;|&lt;/cell&gt;
        &lt;cell&gt;Archive&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[Login] [Register] [Forgot your password?]&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="20"/&gt;
      &lt;row span="20"&gt;
        &lt;cell&gt;23.Oct.2025&lt;p&gt;a1k.org (Webseite)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; Picture gallery: Amiga prototype "Lorraine" at the Amiga 40&lt;p&gt;Dale Luck from the original Amiga development team has preserved the very first Amiga prototype, which used three huge stacks of well-equipped breadboards instead of custom chips. The computer was on display in Germany for the first time last weekend at the Amiga 40 event. Amiga user ‘Pittrock’ took pictures of the exhibit and kindly gave us permission to publish them here:&lt;/p&gt;&lt;p&gt;(cg)&lt;/p&gt;&lt;p&gt;[News message: 23. Oct. 2025, 22:53] [Comments: 0]&lt;/p&gt;&lt;p&gt;[Send via e-mail] [Print version] [ASCII version]&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; Masthead | Privacy policy | Netiquette | Advertising | Contact &lt;p&gt;Copyright © 1998-2025 by amiga-news.de - all rights reserved.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45729467</guid><pubDate>Tue, 28 Oct 2025 05:28:55 +0000</pubDate></item></channel></rss>