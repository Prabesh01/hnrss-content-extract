<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 14 Dec 2025 16:43:30 +0000</lastBuildDate><item><title>An off-grid, flat-packable washing machine</title><link>https://www.positive.news/society/flat-pack-washing-machine-spins-a-fairer-future/</link><description>&lt;doc fingerprint="ec0349190ebcc67a"&gt;
  &lt;main&gt;
    &lt;p&gt;A former Dyson engineer is rolling out a revolution for household chores in deprived communities after inventing an off-grid, flat-packable washing machine&lt;/p&gt;
    &lt;p&gt;Some five billion people in remote and developing regions still wash their clothes by hand. It’s a task that unfairly burdens women and young girls, who can spend up to 20 hours a week on the chore.&lt;/p&gt;
    &lt;p&gt;Enter Navjot Sawhney, who founded the UK-based social enterprise The Washing Machine Project (TWMP) to tackle this, and has now shipped almost 500 of his hand-crank Divya machines to 13 countries, including Mexico, Ghana, Iraq and the US.&lt;/p&gt;
    &lt;p&gt;The Divya washing machine, made up of an outer drum and an inner one which rotates, operates a 30-minute wash cycle where it completes a 5kg load needing only a few minutes of manual turning.&lt;/p&gt;
    &lt;p&gt;It works like this: after loading the clothes, detergent and water, and letting it sit for 10-15 minutes, users can close the lid and turn the handle for two minutes, repeating this twice more after ten minutes of letting the clothes sit in between spins. And voila — the machine can then be drained using the tap at the front.&lt;/p&gt;
    &lt;p&gt;This saves up to 75% of time for its user, and halves water consumption. “The machine takes a task that is exhausting and time-consuming and transforms it into something simple, easier to manage, and time saving,” said Sawhney.&lt;/p&gt;
    &lt;p&gt;The Divya project’s development didn’t end with its invention. “We went back to the drawing board and really listened to the people we were designing for, for the context in which they lived. That research changed everything,” said Laura Tuck, the organisation’s R&amp;amp;D Lead.&lt;/p&gt;
    &lt;p&gt;One crucial consideration was making sure Divyas were fit for the locations where they would be used. For example, in Uganda, machines were delivered to a small island on Lake Victoria using a fishing boat. Repairs or replacements could not get there easily, so the TWMP team needed to rethink how the originally complex gear-system machine could work in these conditions. The solution? Designing a product that was simpler, more intuitive, and repairable locally using the skills and infrastructure available.&lt;/p&gt;
    &lt;p&gt;Guided by feedback from real users during workshops and focus groups, TWMP improved the machine’s durability, physical strain, and usability, with the team introducing robust metal frames, simplified workflows, and improved seals and taps.&lt;/p&gt;
    &lt;p&gt;The innovation has already impacted the lives of almost 50,000 people – and Sawhney is just getting started.&lt;/p&gt;
    &lt;p&gt;TWMP hopes to reach 1,000,000 people by 2030, but says it cannot do this alone; it is building a network of partners including NGOs, UN agencies, and local communities, including the Whirlpool Foundation, the charity wing of the US-based home appliance firm.&lt;/p&gt;
    &lt;p&gt;Localised production will begin in early 2026, manufacturing a new generation of machines in India, closer to those who use them. The project is also piloting ‘Hubs’, where machines can be assembled and distributed, but also offering training, workshops, and educational activities, extending the impact of the time saved by Divya machines.&lt;/p&gt;
    &lt;p&gt;It is also seeking policy engagement to embed laundry access in wider strategies around water, sanitation, hygiene, and gender equality.&lt;/p&gt;
    &lt;p&gt;Images: The Washing Machine Project&lt;/p&gt;
    &lt;head rend="h3"&gt;Be part of the solution&lt;/head&gt;
    &lt;p&gt;At Positive News, we’re not chasing clicks or profits for media moguls – we’re here to serve you and have a positive social impact. We can’t do this unless enough people like you choose to support our journalism.&lt;/p&gt;
    &lt;p&gt;Give once from just £1, or join 1,700+ others who contribute an average of £3 or more per month. Together, we can build a healthier form of media – one that focuses on solutions, progress and possibilities, and empowers people to create positive change.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46258906</guid><pubDate>Sat, 13 Dec 2025 22:38:08 +0000</pubDate></item><item><title>Linux Sandboxes and Fil-C</title><link>https://fil-c.org/seccomp</link><description>&lt;doc fingerprint="ea639991fc2930e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linux Sandboxes And Fil-C&lt;/head&gt;
    &lt;p&gt;Memory safety and sandboxing are two different things. It's reasonable to think of them as orthogonal: you could have memory safety but not be sandboxed, or you could be sandboxed but not memory safe.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Example of memory safe but not sandboxed: a pure Java program that opens files on the filesystem for reading and writing and accepts filenames from the user. The OS will allow this program to overwrite any file that the user has access to. This program can be quite dangerous even if it is memory safe. Worse, imagine that the program didn't have any code to open files for reading and writing, but also had no sandbox to prevent those syscalls from working. If there was a bug in the memory safety enforcement of this program (say, because of a bug in the Java implementation), then an attacker could cause this program to overwrite any file if they succeeded at achieving code execution via weird state.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Example of sandboxed but not memory safe: a program written in assembly that starts by requesting that the OS revoke all of its capabilities beyond just pure compute. If the program did want to open a file or write to it, then the kernel will kill the process, based on the earlier request to have this capability revoked. This program could have lots of memory safety bugs (because it's written in assembly), but even if it did, then the attacker cannot make this program overwrite any file unless they find some way to bypass the sandbox.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In practice, sandboxes have holes by design. A typical sandbox allows the program to send and receive messages to broker processes that have higher privileges. So, an attacker may first use a memory safety bug to make the sandboxed process send malicious messages, and then use those malicious messages to break into the brokers.&lt;/p&gt;
    &lt;p&gt;The best kind of defense is to have both a sandbox and memory safety. This document describes how to combine sandboxing and Fil-C's memory safety by explaining what it takes to port OpenSSH's seccomp-based Linux sandbox code to Fil-C.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Fil-C is a memory safe implementation of C and C++ and this site has a lot of documentation about it. Unlike most memory safe languages, Fil-C enforces safety down to where your code meets Linux syscalls and the Fil-C runtime is robust enough that it's possible to use it in low-level system components like &lt;code&gt;init&lt;/code&gt; and &lt;code&gt;udevd&lt;/code&gt;. Lots of programs work in Fil-C, including OpenSSH, which makes use of seccomp-BPF sandboxing.&lt;/p&gt;
    &lt;p&gt;This document focuses on how OpenSSH uses seccomp and other technologies on Linux to build a sandbox around its unprivileged &lt;code&gt;sshd-session&lt;/code&gt; process. Let's review what tools Linux gives us that OpenSSH uses:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;chroot&lt;/code&gt;to restrict the process's view of the filesystem.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running the process with the&lt;/p&gt;&lt;code&gt;sshd&lt;/code&gt;user and group, and giving that user/group no privileges.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setrlimit&lt;/code&gt;to prevent opening files, starting processes, or writing to files.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;seccomp-BPF syscall filter to reduce the attack surface by allowlisting only the set of syscalls that are legitimate for the unprivileged process. Syscalls not in the allowlist will crash the process with&lt;/p&gt;&lt;code&gt;SIGSYS&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Chromium developers and the Mozilla developers both have excellent notes about how to do sandboxing on Linux using seccomp. Seccomp-BPF is a well-documented kernel feature that can be used as part of a larger sandboxing story.&lt;/p&gt;
    &lt;p&gt;Fil-C makes it easy to use &lt;code&gt;chroot&lt;/code&gt; and different users and groups. The syscalls that are used for that part of the sandbox are trivially allowed by Fil-C and no special care is required to use them.&lt;/p&gt;
    &lt;p&gt;Both &lt;code&gt;setrlimit&lt;/code&gt; and seccomp-BPF require special care because the Fil-C runtime starts threads, allocates memory, and performs synchronization. This document describes what you need to know to make effective use of those sandboxing technologies in Fil-C. First, I describe how to build a sandbox that prevents thread creation without breaking Fil-C's use of threads. Then, I describe what tweaks I had to make to OpenSSH's seccomp filter. Finally, I describe how the Fil-C runtime implements the syscalls used to install seccomp filters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preventing Thread Creation Without Breaking The Fil-C Runtime&lt;/head&gt;
    &lt;p&gt;The Fil-C runtime uses multiple background threads for garbage collection and has the ability to automatically shut those threads down when they are not in use. If the program wakes up and starts allocating memory again, then those threads are automatically restarted.&lt;/p&gt;
    &lt;p&gt;Starting threads violates the "no new processes" rule that OpenSSH's &lt;code&gt;setrlimit&lt;/code&gt; sandbox tries to achieve (since threads are just lightweight processes on Linux). It also relies on syscalls like &lt;code&gt;clone3&lt;/code&gt; that are not part of OpenSSH's seccomp filter allowlist.&lt;/p&gt;
    &lt;p&gt;It would be a regression to the sandbox to allow process creation just because the Fil-C runtime relies on it. Instead, I added a new API to &lt;code&gt;&amp;lt;stdfil.h&amp;gt;&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;void zlock_runtime_threads(void);
&lt;/code&gt;
    &lt;p&gt;This forces the runtime to immediately create whatever threads it needs, and to disable shutting them down on demand. Then, I added a call to &lt;code&gt;zlock_runtime_threads()&lt;/code&gt; in OpenSSH's &lt;code&gt;ssh_sandbox_child&lt;/code&gt; function before either the &lt;code&gt;setrlimit&lt;/code&gt; or seccomp-BPF sandbox calls happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tweaks To The OpenSSH Sandbox&lt;/head&gt;
    &lt;p&gt;Because the use of &lt;code&gt;zlock_runtime_threads()&lt;/code&gt; prevents subsequent thread creation from happening, most of the OpenSSH sandbox just works. I did not have to change how OpenSSH uses &lt;code&gt;setrlimit&lt;/code&gt;. I did change the following about the seccomp filter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Failure results in&lt;/p&gt;&lt;code&gt;SECCOMP_RET_KILL_PROCESS&lt;/code&gt;rather than&lt;code&gt;SECCOMP_RET_KILL&lt;/code&gt;. This ensures that Fil-C's background threads are also killed if a sandbox violation occurs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MAP_NORESERVE&lt;/code&gt;is added to the&lt;code&gt;mmap&lt;/code&gt;allowlist, since the Fil-C allocator uses it. This is not a meaningful regression to the filter, since&lt;code&gt;MAP_NORESERVE&lt;/code&gt;is not a meaningful capability for an attacker to have.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sched_yield&lt;/code&gt;is allowed. This is not a dangerous syscall (it's semantically a no-op). The Fil-C runtime uses it as part of its lock implementation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nothing else had to change, since the filter already allowed all of the &lt;code&gt;futex&lt;/code&gt; syscalls that Fil-C uses for synchronization.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Fil-C Implements &lt;code&gt;prctl&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The OpenSSH seccomp filter is installed using two &lt;code&gt;prctl&lt;/code&gt; calls. First, we &lt;code&gt;PR_SET_NO_NEW_PRIVS&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) == -1) {
        debug("%s: prctl(PR_SET_NO_NEW_PRIVS): %s",
            __func__, strerror(errno));
        nnp_failed = 1;
}
&lt;/code&gt;
    &lt;p&gt;This prevents additional privileges from being acquired via &lt;code&gt;execve&lt;/code&gt;. It's required that unprivileged processes that install seccomp filters first set the &lt;code&gt;no_new_privs&lt;/code&gt; bit.&lt;/p&gt;
    &lt;p&gt;Next, we &lt;code&gt;PR_SET_SECCOMP, SECCOMP_MODE_FILTER&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;if (prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, &amp;amp;preauth_program) == -1)
        debug("%s: prctl(PR_SET_SECCOMP): %s",
            __func__, strerror(errno));
else if (nnp_failed)
        fatal("%s: SECCOMP_MODE_FILTER activated but "
            "PR_SET_NO_NEW_PRIVS failed", __func__);
&lt;/code&gt;
    &lt;p&gt;This installs the seccomp filter in &lt;code&gt;preauth_program&lt;/code&gt;. Note that this will fail in the kernel if the &lt;code&gt;no_new_privs&lt;/code&gt; bit is not set, so the fact that OpenSSH reports a fatal error if the filter is installed without &lt;code&gt;no_new_privs&lt;/code&gt; is just healthy paranoia on the part of the OpenSSH authors.&lt;/p&gt;
    &lt;p&gt;The trouble with both syscalls is that they affect the calling thread, not all threads in the process. Without special care, Fil-C runtime's background threads would not have the &lt;code&gt;no_new_privs&lt;/code&gt; bit set and would not have the filter installed. This would mean that if an attacker busted through Fil-C's memory safety protections (in the unlikely event that they found a bug in Fil-C itself!), then they could use those other threads to execute syscalls that bypass the filter!&lt;/p&gt;
    &lt;p&gt;To prevent even this unlikely escape, the Fil-C runtime's wrapper for &lt;code&gt;prctl&lt;/code&gt; implements &lt;code&gt;PR_SET_NO_NEW_PRIVS&lt;/code&gt; and &lt;code&gt;PR_SET_SECCOMP&lt;/code&gt; by handshaking all runtime threads using this internal API:&lt;/p&gt;
    &lt;code&gt;/* Calls the callback from every runtime thread. */
PAS_API void filc_runtime_threads_handshake(void (*callback)(void* arg), void* arg);
&lt;/code&gt;
    &lt;p&gt;The callback performs the requested &lt;code&gt;prctl&lt;/code&gt; from each runtime thread. This ensures that the &lt;code&gt;no_new_privs&lt;/code&gt; bit and the filter are installed on all threads in the Fil-C process.&lt;/p&gt;
    &lt;p&gt;Additionally, because of ambiguity about what to do if the process has multiple user threads, these two &lt;code&gt;prctl&lt;/code&gt; commands will trigger a Fil-C safety error if the program has multiple user threads.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The best kind of protection if you're serious about security is to combine memory safety with sandboxing. This document shows how to achieve this using Fil-C and the sandbox technologies available on Linux, all without regressing the level of protection that those sandboxes enforce or the memory safety guarantees of Fil-C.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46259064</guid><pubDate>Sat, 13 Dec 2025 22:58:29 +0000</pubDate></item><item><title>Closures as Win32 Window Procedures</title><link>https://nullprogram.com/blog/2025/12/12/</link><description>&lt;doc fingerprint="a6c91f3926ae8748"&gt;
  &lt;main&gt;
    &lt;p&gt; nullprogram.com/blog/2025/12/12/ &lt;/p&gt;
    &lt;p&gt;Back in 2017 I wrote about a technique for creating closures in C using JIT-compiled wrapper. It’s neat, though rarely necessary in real programs, so I don’t think about it often. I applied it to &lt;code&gt;qsort&lt;/code&gt;,
which sadly accepts no context pointer. More practical would be
working around insufficient custom allocator interfaces, to
create allocation functions at run-time bound to a particular allocation
region. I’ve learned a lot since I last wrote about this subject, and a
recent article had me thinking about it again, and how I could do
better than before. In this article I will enhance Win32 window procedure
callbacks with a fifth argument, allowing us to more directly pass extra
context. I’m using w64devkit on x64, but the everything here should
work out-of-the-box with any x64 toolchain that speaks GNU assembly.&lt;/p&gt;
    &lt;p&gt;A window procedure has this prototype:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;LRESULT Wndproc(
  HWND hWnd,
  UINT Msg,
  WPARAM wParam,
  LPARAM lParam,
);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To create a window we must first register a class with &lt;code&gt;RegisterClass&lt;/code&gt;,
which accepts a set of properties describing a window class, including a
pointer to one of these functions.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    MyState *state = ...;

    RegisterClassA(&amp;amp;(WNDCLASSA){
        // ...
        .lpfnWndProc   = my_wndproc,
        .lpszClassName = "my_class",
        // ...
    });

    HWND hwnd = CreateWindowExA("my_class", ..., state);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The thread drives a message pump with events from the operating system, dispatching them to this procedure, which then manipulates the program state in response:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    for (MSG msg; GetMessageW(&amp;amp;msg, 0, 0, 0);) {
        TranslateMessage(&amp;amp;msg);
        DispatchMessageW(&amp;amp;msg);  // calls the window procedure
    }
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;All four &lt;code&gt;WNDPROC&lt;/code&gt; parameters are determined by Win32. There is no context
pointer argument. So how does this procedure access the program state? We
generally have two options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Global variables. Yucky but easy. Frequently seen in tutorials.&lt;/item&gt;
      &lt;item&gt;A &lt;code&gt;GWLP_USERDATA&lt;/code&gt; pointer attached to the window.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The second option takes some setup. Win32 passes the last &lt;code&gt;CreateWindowEx&lt;/code&gt;
argument to the window procedure when the window created, via &lt;code&gt;WM_CREATE&lt;/code&gt;.
The procedure attaches the pointer to its window as &lt;code&gt;GWLP_USERDATA&lt;/code&gt;. This
pointer is passed indirectly, through a &lt;code&gt;CREATESTRUCT&lt;/code&gt;. So ultimately it
looks like this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    case WM_CREATE:
        CREATESTRUCT *cs = (CREATESTRUCT *)lParam;
        void *arg = (struct state *)cs-&amp;gt;lpCreateParams;
        SetWindowLongPtr(hwnd, GWLP_USERDATA, (LONG_PTR)arg);
        // ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;In future messages we can retrieve it with &lt;code&gt;GetWindowLongPtr&lt;/code&gt;. Every time
I go through this I wish there was a better way. What if there was a fifth
window procedure parameter though which we could pass a context?&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef LRESULT Wndproc5(HWND, UINT, WPARAM, LPARAM, void *);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;We’ll build just this as a trampoline. The x64 calling convention passes the first four arguments in registers, and the rest are pushed on the stack, including this new parameter. Our trampoline cannot just stuff the extra parameter in the register, but will actually have to build a stack frame. Slightly more complicated, but barely so.&lt;/p&gt;
    &lt;head rend="h3"&gt;Allocating executable memory&lt;/head&gt;
    &lt;p&gt;In previous articles, and in the programs where I’ve applied techniques like this, I’ve allocated executable memory with &lt;code&gt;VirtualAlloc&lt;/code&gt; (or &lt;code&gt;mmap&lt;/code&gt;
elsewhere). This introduces a small challenge for solving the problem
generally: Allocations may be arbitrarily far from our code and data, out
of reach of relative addressing. If they’re further than 2G apart, we need
to encode absolute addresses, and in the simple case would just assume
they’re always too far apart.&lt;/p&gt;
    &lt;p&gt;These days I’ve more experience with executable formats, and allocation, and I immediately see a better solution: Request a block of writable, executable memory from the loader, then allocate our trampolines from it. Other than being executable, this memory isn’t special, and allocation works the usual way, using functions unaware it’s executable. By allocating through the loader, this memory will be part of our loaded image, guaranteed to be close to our other code and data, allowing our JIT compiler to assume a small code model.&lt;/p&gt;
    &lt;p&gt;There are a number of ways to do this, and here’s one way to do it with GNU-styled toolchains targeting COFF:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;        .section .exebuf,"bwx"
        .globl exebuf
exebuf:	.space 1&amp;lt;&amp;lt;21
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;This assembly program defines a new section named &lt;code&gt;.exebuf&lt;/code&gt; containing 2M
of writable (&lt;code&gt;"w"&lt;/code&gt;), executable (&lt;code&gt;"x"&lt;/code&gt;) memory, allocated at run time just
like &lt;code&gt;.bss&lt;/code&gt; (&lt;code&gt;"b"&lt;/code&gt;). We’ll treat this like an arena out of which we can
allocate all trampolines we’ll probably ever need. With careful use of
&lt;code&gt;.pushsection&lt;/code&gt; this could be basic inline assembly, but I’ve left it as a
separate source. On the C side I retrieve this like so:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef struct {
    char *beg;
    char *end;
} Arena;

Arena get_exebuf()
{
    extern char exebuf[1&amp;lt;&amp;lt;21];
    Arena r = {exebuf, exebuf+sizeof(exebuf)};
    return r;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Unfortunately I have to repeat myself on the size. There are different ways to deal with this, but this is simple enough for now. I would have loved to define the array in C with the GCC &lt;code&gt;section&lt;/code&gt; attribute,
but as is usually the case with this attribute, it’s not up to the task,
lacking the ability to set section flags. Besides, by not relying on the
attribute, any C compiler could compile this source, and we only need a
GNU-style toolchain to create the tiny COFF object containing &lt;code&gt;exebuf&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;While we’re at it, a reminder of some other basic definitions we’ll need:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;#define S(s)            (Str){s, sizeof(s)-1}
#define new(a, n, t)    (t *)alloc(a, n, sizeof(t), _Alignof(t))

typedef struct {
    char     *data;
    ptrdiff_t len;
} Str;

Str clone(Arena *a, Str s)
{
    Str r = s;
    r.data = new(a, r.len, char);
    memcpy(r.data, s.data, (size_t)r.len);
    return r;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Which have been discussed at length in previous articles.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trampoline compiler&lt;/head&gt;
    &lt;p&gt;From here the plan is to create a function that accepts a &lt;code&gt;Wndproc5&lt;/code&gt; and a
context pointer to bind, and returns a classic &lt;code&gt;WNDPROC&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;WNDPROC make_wndproc(Arena *, Wndproc5, void *arg);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Our window procedure now gets a fifth argument with the program state:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;LRESULT my_wndproc(HWND, UINT, WPARAM, LPARAM, void *arg)
{
    MyState *state = arg;
    // ...
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;When registering the class we wrap it in a trampoline compatible with &lt;code&gt;RegisterClass&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    RegisterClassA(&amp;amp;(WNDCLASSA){
        // ...
        .lpfnWndProc   = make_wndproc(a, my_wndproc, state),
        .lpszClassName = "my_class",
        // ...
    });
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;All windows using this class will readily have access to this state object through their fifth parameter. It turns out setting up &lt;code&gt;exebuf&lt;/code&gt; was the
more complicated part, and &lt;code&gt;make_wndproc&lt;/code&gt; is quite simple!&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;WNDPROC make_wndproc(Arena *a, Wndproc5 proc, void *arg)
{
    Str thunk = S(
        "\x48\x83\xec\x28"      // sub   $40, %rsp
        "\x48\xb8........"      // movq  $arg, %rax
        "\x48\x89\x44\x24\x20"  // mov   %rax, 32(%rsp)
        "\xe8...."              // call  proc
        "\x48\x83\xc4\x28"      // add   $40, %rsp
        "\xc3"                  // ret
    );
    Str r   = clone(a, thunk);
    int rel = (int)((uintptr_t)proc - (uintptr_t)(r.data + 24));
    memcpy(r.data+ 6, &amp;amp;arg, sizeof(arg));
    memcpy(r.data+20, &amp;amp;rel, sizeof(rel));
    return (WNDPROC)r.data;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The assembly allocates a new stack frame, with callee shadow space, and with room for the new argument, which also happens to re-align the stack. It stores the new argument for the &lt;code&gt;Wndproc5&lt;/code&gt; just above the shadow space.
Then calls into the &lt;code&gt;Wndproc5&lt;/code&gt; without touching other parameters. There
are two “patches” to fill out, which I’ve initially filled with dots: the
context pointer itself, and a 32-bit signed relative address for the call.
It’s going to be very near the callee. The only thing I don’t like about
this function is that I’ve manually worked out the patch offsets.&lt;/p&gt;
    &lt;p&gt;It’s probably not useful, but it’s easy to update the context pointer at any time if hold onto the trampoline pointer:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;void set_wndproc_arg(WNDPROC p, void *arg)
{
    memcpy((char *)p+6, &amp;amp;arg, sizeof(arg));
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;So, for instance:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    MyState *state[2] = ...;  // multiple states
    WNDPROC proc = make_wndproc(a, my_wndproc, state[0]);
    // ...
    set_wndproc_arg(proc, state[1]);  // switch states
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Though I expect the most common case is just creating multiple procedures:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    WNDPROC procs[] = {
        make_wndproc(a, my_wndproc, state[0]),
        make_wndproc(a, my_wndproc, state[1]),
    };
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To my slight surprise these trampolines still work with an active Control Flow Guard system policy. Trampolines do not have stack unwind entries, and I thought Windows might refuse to pass control to them.&lt;/p&gt;
    &lt;p&gt;Here’s a complete, runnable example if you’d like to try it yourself: &lt;code&gt;main.c&lt;/code&gt; and &lt;code&gt;exebuf.s&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Better cases&lt;/head&gt;
    &lt;p&gt;This is more work than going through &lt;code&gt;GWLP_USERDATA&lt;/code&gt;, and real programs
have a small, fixed number of window procedures — typically one — so this
isn’t the best example, but I wanted to illustrate with a real interface.
Again, perhaps the best real use is a library with a weak custom allocator
interface:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;typedef struct {
    void *(*malloc)(size_t);   // no context pointer!
    void  (*free)(void *);     // "
} Allocator;

void *arena_malloc(size_t, Arena *);

// ...

    Allocator perm_allocator = {
        .malloc = make_trampoline(exearena, arena_malloc, perm);
        .free   = noop_free,
    };
    Allocator scratch_allocator = {
        .malloc = make_trampoline(exearena, arena_malloc, scratch);
        .free   = noop_free,
    };
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Something to keep in my back pocket for the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46259334</guid><pubDate>Sat, 13 Dec 2025 23:39:48 +0000</pubDate></item><item><title>An Implementation of J (1992)</title><link>https://www.jsoftware.com/ioj/ioj.htm</link><description>&lt;doc fingerprint="8f2792390b4c60ef"&gt;
  &lt;main&gt;&lt;p&gt; An Implementation of J&lt;lb/&gt; Roger K.W. Hui &lt;/p&gt;&lt;p&gt;Preface&lt;/p&gt;J is a dialect of APL freely available on a wide variety of machines. It is the latest in the line of development known as "dictionary APL". The spelling scheme uses the ASCII alphabet. The underlying concepts, such as arrays, verbs, adverbs, and rank, are extensions and generalizations of ideas in APL\360. Anomalies have been removed. The result is at once simpler and more powerful than previous dialects.&lt;p&gt;Ex ungue leonem.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;0. Introduction&lt;/cell&gt;&lt;cell&gt;6. Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;6.1 Numeric Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1. Interpreting a Sentence&lt;/cell&gt;&lt;cell&gt;6.2 Boxed Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.1 Word Formation&lt;/cell&gt;&lt;cell&gt;6.3 Formatted Display&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.2 Parsing&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.3 Trains&lt;/cell&gt;&lt;cell&gt;7. Comparatives&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;1.4 Name Resolution&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Appendices&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2. Nouns&lt;/cell&gt;&lt;cell&gt;A. Incunabulum&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.1 Arrays&lt;/cell&gt;&lt;cell&gt;B. Special Code&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.2 Types&lt;/cell&gt;&lt;cell&gt;C. Test Scripts&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.3 Memory Management&lt;/cell&gt;&lt;cell&gt;D. Program Files&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2.4 Global Variables&lt;/cell&gt;&lt;cell&gt;E. Foreign Conjunction&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;F. System Summary&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3. Verbs&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.1 Anatomy of a Verb&lt;/cell&gt;&lt;cell&gt;Bibliography&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.2 Rank&lt;/cell&gt;&lt;cell&gt;Glossary and Index&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.3 Atomic (Scalar) Verbs&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.4 Obverses, Identities, and Variants&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;3.5 Error Handling&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;4. Adverbs and Conjunctions&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5. Representation&lt;/cell&gt;&lt;cell&gt;5.1 Atomic Representation&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5.2 Boxed Representation&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;5.3 Tree Representation&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;5.4 Linear Representation&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46259702</guid><pubDate>Sun, 14 Dec 2025 00:34:56 +0000</pubDate></item><item><title>Lean theorem prover mathlib</title><link>https://github.com/leanprover-community/mathlib4</link><description>&lt;doc fingerprint="adf4ba731ce775e1"&gt;
  &lt;main&gt;
    &lt;p&gt;Mathlib is a user maintained library for the Lean theorem prover. It contains both programming infrastructure and mathematics, as well as tactics that use the former and allow to develop the latter.&lt;/p&gt;
    &lt;p&gt;You can find detailed instructions to install Lean, mathlib, and supporting tools on our website. Alternatively, click on one of the buttons below to open a GitHub Codespace or a Gitpod workspace containing the project.&lt;/p&gt;
    &lt;p&gt;Please refer to https://github.com/leanprover-community/mathlib4/wiki/Using-mathlib4-as-a-dependency&lt;/p&gt;
    &lt;p&gt;Got everything installed? Why not start with the tutorial project?&lt;/p&gt;
    &lt;p&gt;For more pointers, see Learning Lean.&lt;/p&gt;
    &lt;p&gt;Besides the installation guides above and Lean's general documentation, the documentation of mathlib consists of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The mathlib4 docs: documentation generated automatically from the source &lt;code&gt;.lean&lt;/code&gt;files.&lt;/item&gt;
      &lt;item&gt;A description of currently covered theories, as well as an overview for mathematicians.&lt;/item&gt;
      &lt;item&gt;Some extra Lean documentation not specific to mathlib (see "Miscellaneous topics")&lt;/item&gt;
      &lt;item&gt;Documentation for people who would like to contribute to mathlib&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Much of the discussion surrounding mathlib occurs in a Zulip chat room, and you are welcome to join, or read along without signing up. Questions from users at all levels of expertise are welcome! We also provide an archive of the public discussions, which is useful for quick reference.&lt;/p&gt;
    &lt;p&gt;The complete documentation for contributing to &lt;code&gt;mathlib&lt;/code&gt; is located
on the community guide contribute to mathlib&lt;/p&gt;
    &lt;p&gt;You may want to subscribe to the &lt;code&gt;mathlib4&lt;/code&gt; channel on Zulip to introduce yourself and your plan to the community.
Often you can find community members willing to help you get started and advise you on the fit and
feasibility of your project.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;To obtain precompiled&lt;/p&gt;&lt;code&gt;olean&lt;/code&gt;files, run&lt;code&gt;lake exe cache get&lt;/code&gt;. (Skipping this step means the next step will be very slow.)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;To build&lt;/p&gt;&lt;code&gt;mathlib4&lt;/code&gt;run&lt;code&gt;lake build&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;To build and run all tests, run&lt;/p&gt;&lt;code&gt;lake test&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can use&lt;/p&gt;&lt;code&gt;lake build Mathlib.Import.Path&lt;/code&gt;to build a particular file, e.g.&lt;code&gt;lake build Mathlib.Algebra.Group.Defs&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you added a new file, run the following command to update&lt;/p&gt;
        &lt;code&gt;Mathlib.lean&lt;/code&gt;
        &lt;quote&gt;lake exe mk_all&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mathlib has the following guidelines and conventions that must be followed&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The style guide&lt;/item&gt;
      &lt;item&gt;A guide on the naming convention&lt;/item&gt;
      &lt;item&gt;The documentation style&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can run &lt;code&gt;lake exe cache get&lt;/code&gt; to download cached build files that are computed by &lt;code&gt;mathlib4&lt;/code&gt;'s automated workflow.&lt;/p&gt;
    &lt;p&gt;If something goes mysteriously wrong, you can try one of &lt;code&gt;lake clean&lt;/code&gt; or &lt;code&gt;rm -rf .lake&lt;/code&gt; before trying &lt;code&gt;lake exe cache get&lt;/code&gt; again.
In some circumstances you might try &lt;code&gt;lake exe cache get!&lt;/code&gt;
which re-downloads cached build files even if they are available locally.&lt;/p&gt;
    &lt;p&gt;Call &lt;code&gt;lake exe cache&lt;/code&gt; to see its help menu.&lt;/p&gt;
    &lt;p&gt;The mathlib4_docs repository is responsible for generating and publishing the mathlib4 docs.&lt;/p&gt;
    &lt;p&gt;That repo can be used to build the docs locally:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/leanprover-community/mathlib4_docs.git
cd mathlib4_docs
cp ../mathlib4/lean-toolchain .
lake exe cache get
lake build Mathlib:docs&lt;/code&gt;
    &lt;p&gt;The last step may take a while (&amp;gt;20 minutes). The HTML files can then be found in &lt;code&gt;.lake/build/doc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For users familiar with Lean 3 who want to get up to speed in Lean 4 and migrate their existing Lean 3 code we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A survival guide for Lean 3 users&lt;/item&gt;
      &lt;item&gt;Instructions to run &lt;code&gt;mathport&lt;/code&gt;on a project other than mathlib.&lt;code&gt;mathport&lt;/code&gt;is the tool the community used to port the entirety of&lt;code&gt;mathlib&lt;/code&gt;from Lean 3 to Lean 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are a mathlib contributor and want to update dependencies, use &lt;code&gt;lake update&lt;/code&gt;,
or &lt;code&gt;lake update batteries aesop&lt;/code&gt; (or similar) to update a subset of the dependencies.
This will update the &lt;code&gt;lake-manifest.json&lt;/code&gt; file correctly.
You will need to make a PR after committing the changes to this file.&lt;/p&gt;
    &lt;p&gt;Please do not run &lt;code&gt;lake update -Kdoc=on&lt;/code&gt; as previously advised, as the documentation related
dependencies should only be included when CI is building documentation.&lt;/p&gt;
    &lt;p&gt;For a list containing more detailed information, see https://leanprover-community.github.io/teams/maintainers.html&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Anne Baanen (@Vierkantor): algebra, number theory, tactics&lt;/item&gt;
      &lt;item&gt;Matthew Robert Ballard (@mattrobball): algebra, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Riccardo Brasca (@riccardobrasca): algebra, number theory, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Kevin Buzzard (@kbuzzard): algebra, number theory, algebraic geometry, category theory&lt;/item&gt;
      &lt;item&gt;Mario Carneiro (@digama0): lean formalization, tactics, type theory, proof engineering&lt;/item&gt;
      &lt;item&gt;Bryan Gin-ge Chen (@bryangingechen): documentation, infrastructure&lt;/item&gt;
      &lt;item&gt;Johan Commelin (@jcommelin): algebra, number theory, category theory, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Anatole Dedecker (@ADedecker): topology, functional analysis, calculus&lt;/item&gt;
      &lt;item&gt;Rémy Degenne (@RemyDegenne): probability, measure theory, analysis&lt;/item&gt;
      &lt;item&gt;Floris van Doorn (@fpvandoorn): measure theory, model theory, tactics&lt;/item&gt;
      &lt;item&gt;Frédéric Dupuis (@dupuisf): linear algebra, functional analysis&lt;/item&gt;
      &lt;item&gt;Sébastien Gouëzel (@sgouezel): topology, calculus, geometry, analysis, measure theory&lt;/item&gt;
      &lt;item&gt;Markus Himmel (@TwoFX): category theory&lt;/item&gt;
      &lt;item&gt;Yury G. Kudryashov (@urkud): analysis, topology, measure theory&lt;/item&gt;
      &lt;item&gt;Robert Y. Lewis (@robertylewis): tactics, documentation&lt;/item&gt;
      &lt;item&gt;Jireh Loreaux (@j-loreaux): analysis, topology, operator algebras&lt;/item&gt;
      &lt;item&gt;Heather Macbeth (@hrmacbeth): geometry, analysis&lt;/item&gt;
      &lt;item&gt;Patrick Massot (@patrickmassot): documentation, topology, geometry&lt;/item&gt;
      &lt;item&gt;Bhavik Mehta (@b-mehta): category theory, combinatorics&lt;/item&gt;
      &lt;item&gt;Kyle Miller (@kmill): combinatorics, tactics, metaprogramming&lt;/item&gt;
      &lt;item&gt;Kim Morrison (@kim-em): category theory, tactics&lt;/item&gt;
      &lt;item&gt;Oliver Nash (@ocfnash): algebra, geometry, topology&lt;/item&gt;
      &lt;item&gt;Joël Riou (@joelriou): category theory, homology, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Michael Rothgang (@grunweg): differential geometry, analysis, topology, linters&lt;/item&gt;
      &lt;item&gt;Damiano Testa (@adomani): algebra, algebraic geometry, number theory, tactics, linter&lt;/item&gt;
      &lt;item&gt;Adam Topaz (@adamtopaz): algebra, category theory, algebraic geometry&lt;/item&gt;
      &lt;item&gt;Eric Wieser (@eric-wieser): algebra, infrastructure&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jeremy Avigad (@avigad): analysis&lt;/item&gt;
      &lt;item&gt;Reid Barton (@rwbarton): category theory, topology&lt;/item&gt;
      &lt;item&gt;Gabriel Ebner (@gebner): tactics, infrastructure, core, formal languages&lt;/item&gt;
      &lt;item&gt;Johannes Hölzl (@johoelzl): measure theory, topology&lt;/item&gt;
      &lt;item&gt;Simon Hudon (@cipher1024): tactics&lt;/item&gt;
      &lt;item&gt;Chris Hughes (@ChrisHughes24): algebra&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46260128</guid><pubDate>Sun, 14 Dec 2025 01:49:11 +0000</pubDate></item><item><title>Compiler Engineering in Practice</title><link>https://chisophugis.github.io/2025/12/08/compiler-engineering-in-practice-part-1-what-is-a-compiler.html</link><description>&lt;doc fingerprint="4c3f098e613990c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compiler Engineering in Practice - Part 1: What is a Compiler?&lt;/head&gt;
    &lt;p&gt;“Compiler Engineering in Practice” is a blog series intended to pass on wisdom that seemingly every seasoned compiler developer knows, but is not systematically written down in any textbook or online resource. Some (but not much) prior experience with compilers is needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a compiler?&lt;/head&gt;
    &lt;p&gt;The first and most important question is “what is a compiler?”. In short, a compiler is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a translator that translates between two different languages, where those languages represent a description of a computation, and&lt;/item&gt;
      &lt;item&gt;the behavior of the computation in the output language must “match” the behavior of the computation in the input language (more on this below).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, an input language can be C, and the output can be x86 assembly. By this definition, an assembler is also a compiler (albeit a simple one), in that it reads x86 textual assembly and outputs x86 binary machine code, which are two different languages. The &lt;code&gt;python&lt;/code&gt; program that executes Python code contains a compiler – one that reads Python source code and outputs Python interpreter bytecode.&lt;/p&gt;
    &lt;p&gt;This brings me to my first important point about practical compiler engineering – it’s not some mystical art. Compilers, operating systems, and databases are usually considered some kind of special corner of computer science / software engineering for being complex, and indeed, there are some corners of compilers that are a black art. But taking a step back, a compiler is simply a program that reads a file and writes a file. From a development perspective, it’s not that different from &lt;code&gt;cat&lt;/code&gt; or &lt;code&gt;grep&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Why does this matter? Because it means that compilers are easy to debug if you build them right. There are no time-dependent interrupts like an operating system, async external events like a web browser, or large enough scale that hardware has to be considered unreliable like a database. It’s just a command line program (or can be reduced to one if engineered right), such that nearly all bugs are reproducible and debuggable in isolation from the comfort of your workstation. No connecting to a flaky dev board, no extensive mocking of various interfaces.&lt;/p&gt;
    &lt;p&gt;You might say – wait a minute – if I’m running on my company’s AI hardware, I may need to connect to a dev board. Yes, but if you do things right, you will rarely need to do that when debugging the compiler proper. Which brings me to…&lt;/p&gt;
    &lt;head rend="h2"&gt;Reliability&lt;/head&gt;
    &lt;p&gt;Compilers are like operating systems and databases in that the bar for reliability is extremely high. One cannot build a practical compiler haphazardly. Why? Because of miscompiles.&lt;/p&gt;
    &lt;p&gt;Miscompiles are when the compiler produces an output file in the output language that does not “match” the specification of its computation in the input language. To avoid a miscompile, the output program must behave identically to the input program, as far as can be observed by the outside world, such as network requests, values printed to the console, values written to files, etc.&lt;/p&gt;
    &lt;p&gt;For integer programs, bit-exact results are required, though there are some nuances regarding undefined behavior, as described in John Regehr’s “laws of physics of compilers”. For floating point programs, the expectation of bit-exact results is usually too strict. Transformations on large floating point computations (like AI programs) need some flexibility to produce slightly different outputs in order to allow efficient execution. There is no widely-agreed-upon formal definition of this, though there are reasonable ways to check for it in practice (“atol/rtol” go a long way).&lt;/p&gt;
    &lt;head rend="h3"&gt;How bad is a miscompile?&lt;/head&gt;
    &lt;p&gt;Miscompiles can have massive consequences for customers. A miscompile of a database can cause data loss. A miscompile of an operating system can cause a security vulnerability. A miscompile of an AI program can cause bad medical advice. The stakes are extremely high, and debugging a miscompile when it happens “in the wild” can easily take 3+ months (and it can take months for a customer to even realize that their issue is caused by a miscompile).&lt;/p&gt;
    &lt;p&gt;If that weren’t enough, there’s a self-serving reason to avoid miscompiles – if you have too many of them, your development velocity on your compiler will grind to a halt. Miscompiles can easily take 100x or 1000x of the time to debug vs a bug that makes itself known during the actual execution of the compiler (rather than the execution of the program that was output by the compiler). That’s why most aspects of practical compiler development revolve around ensuring that if something goes wrong, that it halts the compiler before a faulty output program is produced.&lt;/p&gt;
    &lt;p&gt;A miscompile is a fundamental failure of the compiler’s contract with its user. Every miscompile should be accompanied by a deep look in the mirror and self-reflection about what went wrong to allow it to sneak through, and what preventative measures can (and should immediately) be taken to ensure that this particular failure mode never happens again.&lt;/p&gt;
    &lt;p&gt;Especially in the AI space, there are lots of compilers that play fast and loose with this, and as a result get burned. The best compiler engineers tend to be highly pedantic and somewhat paranoid about what can go wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why compilers are hard – the IR data structure&lt;/head&gt;
    &lt;p&gt;Compilers do have an essential complexity that makes them “hard”, and this again comes from the whole business of making sure that the input program and the output of the compiler have the same behavior. To understand this, we have to discuss how a compiler represents the meaning of the input program and how it preserves that meaning when producing the output program. This notion of “meaning” is sometimes called the program semantics.&lt;/p&gt;
    &lt;p&gt;The primary data structure in a compiler is usually some form of graph data structure that represents the compiler’s understanding of “what computation this program is supposed to do”. Hence, it represents the computation that the compiler needs to preserve all the way to the output program. This data structure is usually called an IR (intermediate representation). The primary way that compilers work is by taking an IR that represents the input program, and applying a series of small transformations all of which have been individually verified to not change the meaning of the program (i.e. not miscompile). In doing so, we decompose one large translation problem into many smaller ones, making it manageable.&lt;/p&gt;
    &lt;p&gt;I think it’s fair to say that compiler IR’s are the single most complex monolithic data structure in all of software engineering, in the sense that interpreting what can and cannot be validly done with the data structure is complex. To be clear, compiler IR’s are not usually very complex in the implementation sense like a “lock-free list” that uses subtle atomic operations to present a simple insert/delete/etc. interface.&lt;/p&gt;
    &lt;p&gt;Unlike a lock-free list, compiler IR’s usually have a very complex interface, even if they have a very simple internal implementation. Even specifying declaratively or in natural language what are the allowed transformations on the data structure is usually extremely difficult (you’ll see things like “memory models” or “abstract machines” that people spend years or decades trying to define properly).&lt;/p&gt;
    &lt;head rend="h3"&gt;A very complex schema&lt;/head&gt;
    &lt;p&gt;Firstly, the nodes in the graph usually have a complex schema. For example, a simple “integer multiply operation” (a node in the graph) is only allowed to have certain integer types as operands (incoming edges). And there may easily be thousands of kinds of operations at varying abstraction levels in any practical compiler, each with their own unique requirements. For example, a simple C &lt;code&gt;*&lt;/code&gt; (multiplication) operator will go through the following evolution in Clang:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It first becomes Clang’s &lt;code&gt;BinaryOperator&lt;/code&gt;node, which takes two “expressions” as operands (which may be mutable uint32_t values, for example).&lt;/item&gt;
      &lt;item&gt;It will then be converted to an LLVM IR &lt;code&gt;mul&lt;/code&gt;operation, which takes as operands an&lt;code&gt;llvm::Value&lt;/code&gt;, which represents an immutable value of the&lt;code&gt;i32&lt;/code&gt;type, say.&lt;/item&gt;
      &lt;item&gt;It will then be converted to a GlobalISel &lt;code&gt;G_MUL&lt;/code&gt;operation, whose operands represent not only an 32-bit integer, but also begin to capture notions like which “register bank” the value should eventually live in.&lt;/item&gt;
      &lt;item&gt;It will then be turned into a target-specific MIR node like &lt;code&gt;IMUL32rri&lt;/code&gt;or&lt;code&gt;IMUL32rr&lt;/code&gt;selecting among a variety of physical x86 instructions which can implement a multiplication. At this level, operands may represent physical, mutable hardware registers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From a compiler developer’s perspective, all these “multiply operations” are deeply different from each other because of the different information captured at each abstraction level (again, compiler developers are usually very pedantic). Failing to adequately differentiate between abstraction levels is a common disease among poorly written compilers.&lt;/p&gt;
    &lt;p&gt;At every level, precise attention to detail is needed – for example, if the multiplication is expected to overflow mod 2^32 in the source program, and we accidentally convert it to overflow mod 2^64 (such as by using a 64-bit register), then we have introduced a miscompile. Each operation has its own unique set of constraints and properties like these which apply when transforming the program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Complex interactions between operations&lt;/head&gt;
    &lt;p&gt;Additionally, how these operations in the IR graph relate to each other can be very complex, especially when mutable variables and control flow are involved. For example, you may realize that an operation always executes, but we may be able to move it around to hide it under an &lt;code&gt;if&lt;/code&gt; condition to optimize the program. Consider the program:&lt;/p&gt;
    &lt;code&gt;x = y + z;
...
if (condition) {
    print(x); // The only time that `x` is referenced.
}
&lt;/code&gt;
    &lt;p&gt;Is it safe to convert this to&lt;/p&gt;
    &lt;code&gt;...
if (condition) {
    print(y + z);
}
&lt;/code&gt;
    &lt;p&gt;? Well, it depends on what’s hidden in that &lt;code&gt;...&lt;/code&gt;. For example, if the program is:&lt;/p&gt;
    &lt;code&gt;x = y + z;
...
y += 5;
...
if (condition) {
    print(x);
}
&lt;/code&gt;
    &lt;p&gt;Then it’s not legal, since by the time we get to the &lt;code&gt;if&lt;/code&gt;, the value of &lt;code&gt;y&lt;/code&gt; will have changed and we’ll print the wrong value. One of the primary considerations when designing compiler IR’s is how to make the transformations as simple and obviously correct as possible (more on that in another blog post).&lt;/p&gt;
    &lt;p&gt;Usually production compilers will deal with IR graphs from thousands to millions of nodes. Understandably then, the compounding effect of the IR complexity is front and center in all compiler design discussions. A single invalid transformation can result in a miscompile.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compilers are just software&lt;/head&gt;
    &lt;p&gt;Practical compilers are often live for years or decades and span millions of lines of code, so the entire suite of software engineering wisdom applies to them – good API design, testing, reusability, etc. though usually with additional compiler-specific twists.&lt;/p&gt;
    &lt;p&gt;For example, while API design is very important for most programs’ code (as it is for compilers’), compilers also have an additional dimension of “IR design”. As described above, the IR can be very complex to understand and transform, and designing it right can greatly mitigate this. (more on this in a future blog post)&lt;/p&gt;
    &lt;p&gt;Similarly, since compilers are usually decomposed into the successive application of multiple “passes” (self-contained IR transformations), there are a variety of testing and debugging strategies specific to compilers. (more on this in a future blog post).&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion and acknowledgements&lt;/head&gt;
    &lt;p&gt;I hope you have found this post helpful. I have a few more sketched out that should be coming soon. Please let me know on my LinkedIn if you have any feedback or topics you’d like to suggest. Big thanks to Bjarke Roune for his recent blog post that inspired me to finally get this series off the ground. Also to Dan Gohman for his blog post on canonicalization from years back. There’s too few such blog posts giving the big picture of practical compiler development. Please send me any other ones you know about on LinkedIn.&lt;/p&gt;
    &lt;p&gt;Stay tuned for future parts of this series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modern Compilers in the Age of AI&lt;/item&gt;
      &lt;item&gt;Organizing a Compiler&lt;/item&gt;
      &lt;item&gt;Testing, Code Review, and Robustness&lt;/item&gt;
      &lt;item&gt;The Compiler Lifecycle&lt;/item&gt;
      &lt;item&gt;…&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46261452</guid><pubDate>Sun, 14 Dec 2025 07:45:15 +0000</pubDate></item><item><title>Willison on Merchant's "Copywriters reveal how AI has decimated their industry"</title><link>https://simonwillison.net/2025/Dec/14/copywriters-reveal-how-ai-has-decimated-their-industry/</link><description>&lt;doc fingerprint="f335dd74be55a9c1"&gt;
  &lt;main&gt;
    &lt;p&gt;Copywriters reveal how AI has decimated their industry. Brian Merchant has been collecting personal stories for his series AI Killed My Job - previously covering tech workers, translators, and artists - and this latest piece includes anecdotes from 12 professional copywriters all of whom have had their careers devastated by the rise of AI-generated copywriting tools.&lt;/p&gt;
    &lt;p&gt;It's a tough read. Freelance copywriting does not look like a great place to be right now.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;AI is really dehumanizing, and I am still working through issues of self-worth as a result of this experience. When you go from knowing you are valuable and valued, with all the hope in the world of a full career and the ability to provide other people with jobs... To being relegated to someone who edits AI drafts of copy at a steep discount because “most of the work is already done” ...&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The big question for me is if a new AI-infested economy creates new jobs that are a great fit for people affected by this. I would hope that clear written communication skills are made even more valuable, but the people interviewed here don't appear to be finding that to be the case.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI are quietly adopting skills, now available in ChatGPT and Codex CLI - 12th December 2025&lt;/item&gt;
      &lt;item&gt;GPT-5.2 - 11th December 2025&lt;/item&gt;
      &lt;item&gt;Useful patterns for building HTML tools - 10th December 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46261998</guid><pubDate>Sun, 14 Dec 2025 10:01:25 +0000</pubDate></item><item><title>Shai-Hulud compromised a dev machine and raided GitHub org access: a post-mortem</title><link>https://trigger.dev/blog/shai-hulud-postmortem</link><description>&lt;doc fingerprint="c417348bcef3474a"&gt;
  &lt;main&gt;
    &lt;p&gt;On November 25th, 2025, we were on a routine Slack huddle debugging a production issue when we noticed something strange: a PR in one of our internal repos was suddenly closed, showed zero changes, and had a single commit from... Linus Torvalds?&lt;/p&gt;
    &lt;p&gt;The commit message was just "init."&lt;/p&gt;
    &lt;p&gt;Within seconds, our #git Slack channel exploded with notifications. Dozens of force-pushes. PRs closing across multiple repositories. All attributed to one of our engineers.&lt;/p&gt;
    &lt;p&gt;We had been compromised by Shai-Hulud 2.0, a sophisticated npm supply chain worm that compromised over 500 packages, affected 25,000+ repositories, and spread across the JavaScript ecosystem. We weren't alone: PostHog, Zapier, AsyncAPI, Postman, and ENS were among those hit.&lt;/p&gt;
    &lt;p&gt;This is the complete story of what happened, how we responded, and what we've changed to prevent this from happening again.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;No Trigger.dev packages were ever compromised. The&lt;/p&gt;&lt;code&gt;@trigger.dev/*&lt;/code&gt;packages and&lt;code&gt;trigger.dev&lt;/code&gt;CLI were never infected with Shai-Hulud malware. This incident involved one of our engineers installing a compromised package on their development machine, which led to credential theft and unauthorized access to our GitHub organization. Our published packages remained safe throughout.&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Attack Timeline&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, 04:11&lt;/cell&gt;
        &lt;cell&gt;Malicious packages go live&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, ~20:27&lt;/cell&gt;
        &lt;cell&gt;Engineer compromised&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 24, 22:36&lt;/cell&gt;
        &lt;cell&gt;First attacker activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 02:56-05:32&lt;/cell&gt;
        &lt;cell&gt;Overnight reconnaissance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 09:08-15:08&lt;/cell&gt;
        &lt;cell&gt;Legitimate engineer work (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 09:10-09:17&lt;/cell&gt;
        &lt;cell&gt;Attacker monitors engineer activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 15:17-15:27&lt;/cell&gt;
        &lt;cell&gt;Final recon&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 15:27-15:37&lt;/cell&gt;
        &lt;cell&gt;Destructive attack&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, ~15:32&lt;/cell&gt;
        &lt;cell&gt;Detection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, ~15:36&lt;/cell&gt;
        &lt;cell&gt;Access revoked&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 16:35&lt;/cell&gt;
        &lt;cell&gt;AWS session blocked&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 25, 22:35&lt;/cell&gt;
        &lt;cell&gt;All branches restored&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nov 26, 20:16&lt;/cell&gt;
        &lt;cell&gt;GitHub App key rotated&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;The compromise&lt;/head&gt;
    &lt;p&gt;On the evening of November 24th, around 20:27 UTC (9:27 PM local time in Germany), one of our engineers was experimenting with a new project. They ran a command that triggered &lt;code&gt;pnpm install&lt;/code&gt;. At that moment, somewhere in the dependency tree, a malicious package executed.&lt;/p&gt;
    &lt;p&gt;We don't know exactly which package delivered the payload. The engineer was experimenting at the time and may have deleted the project directory as part of cleanup. By the time we investigated, we couldn't trace back to the specific package. The engineer checked their shell history and they'd only run install commands in our main trigger repo, cloud repo, and one experimental project.&lt;/p&gt;
    &lt;p&gt;This is one of the frustrating realities of these attacks: once the malware runs, identifying the source becomes extremely difficult. The package doesn't announce itself. The &lt;code&gt;pnpm install&lt;/code&gt; completes successfully. Everything looks normal.&lt;/p&gt;
    &lt;p&gt;What we do know is that the Shai-Hulud malware ran a &lt;code&gt;preinstall&lt;/code&gt; script that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Downloaded and executed TruffleHog, a legitimate security tool repurposed for credential theft&lt;/item&gt;
      &lt;item&gt;Scanned the engineer's machine for secrets: GitHub tokens, AWS credentials, npm tokens, environment variables&lt;/item&gt;
      &lt;item&gt;Exfiltrated everything it found&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the engineer later recovered files from their compromised laptop (booted in recovery mode), they found the telltale signs:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;.trufflehog-cache&lt;/code&gt; directory and &lt;code&gt;trufflehog_3.91.1_darwin_amd64.tar.gz&lt;/code&gt; file found on the compromised machine. The &lt;code&gt;extract&lt;/code&gt; directory was empty, likely cleaned up by the malware to cover its tracks.&lt;/p&gt;
    &lt;head rend="h2"&gt;17 hours of reconnaissance&lt;/head&gt;
    &lt;p&gt;The attacker had access to our engineer's GitHub account for 17 hours before doing anything visible. According to our GitHub audit logs, they operated methodically.&lt;/p&gt;
    &lt;p&gt;Just over two hours after the initial compromise, the attacker validated their stolen credentials and began mass cloning:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
        &lt;cell role="head"&gt;Activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:36:50&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;First attacker access, mass cloning begins&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:36-22:39&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;73 repositories cloned&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:48-22:50&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~70 more repositories cloned (second wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:55-22:56&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~90 repositories cloned (third wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;22:59-23:04&lt;/cell&gt;
        &lt;cell&gt;US&lt;/cell&gt;
        &lt;cell&gt;~70 repositories cloned (fourth wave)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;23:32:59&lt;/cell&gt;
        &lt;cell&gt;India&lt;/cell&gt;
        &lt;cell&gt;Attacker switches to India-based infrastructure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;23:32-23:37&lt;/cell&gt;
        &lt;cell&gt;India&lt;/cell&gt;
        &lt;cell&gt;73 repositories cloned&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;23:34-23:35&lt;/cell&gt;
        &lt;cell&gt;US + India&lt;/cell&gt;
        &lt;cell&gt;Simultaneous cloning from both locations&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The simultaneous activity from US and India confirmed we were dealing with a single attacker using multiple VPNs or servers, not separate actors.&lt;/p&gt;
    &lt;p&gt;While our engineer slept in Germany, the attacker continued their reconnaissance. More cloning at 02:56-02:59 UTC (middle of the night in Germany), sporadic activity until 05:32 UTC. Total repos cloned: 669 (527 from US infrastructure, 142 from India).&lt;/p&gt;
    &lt;p&gt;Here's where it gets unsettling. Our engineer woke up and started their normal workday:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Actor&lt;/cell&gt;
        &lt;cell role="head"&gt;Activity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;09:08:27&lt;/cell&gt;
        &lt;cell&gt;Engineer&lt;/cell&gt;
        &lt;cell&gt;Triggers workflow on cloud repo (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;09:10-09:17&lt;/cell&gt;
        &lt;cell&gt;Attacker&lt;/cell&gt;
        &lt;cell&gt;Git fetches from US, watching the engineer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;09:08-15:08&lt;/cell&gt;
        &lt;cell&gt;Engineer&lt;/cell&gt;
        &lt;cell&gt;Normal PR reviews, CI workflows (from Germany)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The attacker was monitoring our engineer's activity while they worked, unaware they were compromised.&lt;/p&gt;
    &lt;p&gt;During this period, the attacker created repositories with random string names to store stolen credentials, a known Shai-Hulud pattern:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/xfjqb74uysxcni5ztn&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/ls4uzkvwnt0qckjq27&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;github.com/[username]/uxa7vo9og0rzts362c&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They also created three repos marked with "Sha1-Hulud: The Second Coming" as a calling card. These repositories were empty by the time we examined them, but based on the documented Shai-Hulud behavior, they likely contained triple base64-encoded credentials.&lt;/p&gt;
    &lt;head rend="h2"&gt;10 minutes of destruction&lt;/head&gt;
    &lt;p&gt;At 15:27 UTC on November 25th, the attacker switched from reconnaissance to destruction.&lt;/p&gt;
    &lt;p&gt;The attack began on our &lt;code&gt;cloud&lt;/code&gt; repo from India-based infrastructure:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Repo&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:35&lt;/cell&gt;
        &lt;cell&gt;First force-push&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;Attack begins&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:37&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;PR #300 closed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;15:27:44&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/cloud&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:27:50&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev&lt;/cell&gt;
        &lt;cell&gt;PR #2707 closed&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The attack continued on our main repository:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:28:13&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2706 (release PR)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:30:51&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2451&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:31:10&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2382&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:31:16&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push to trigger.dev&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:31:31&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/trigger.dev PR #2482&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;At 15:32:43-46 UTC, 12 PRs on jsonhero-web were closed in 3 seconds. Clearly automated. PRs #47, #169, #176, #181, #189, #190, #194, #197, #204, #206, #208 all closed within a 3-second window.&lt;/p&gt;
    &lt;p&gt;Our critical infrastructure repository was targeted next:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Event&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:41&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/infra PR #233&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:45&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push (India)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15:35:48&lt;/cell&gt;
        &lt;cell&gt;PR closed&lt;/cell&gt;
        &lt;cell&gt;triggerdotdev/infra PR #309&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15:35:49&lt;/cell&gt;
        &lt;cell&gt;BLOCKED&lt;/cell&gt;
        &lt;cell&gt;Branch protection rejected force-push (India)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The final PR was closed on json-infer-types at 15:37:13 UTC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Detection and response&lt;/head&gt;
    &lt;p&gt;We got a lucky break. One of our team members was monitoring Slack when the flood of notifications started:&lt;/p&gt;
    &lt;p&gt;Our #git Slack channel during the attack. A wall of force-pushes, all with commit message "init."&lt;/p&gt;
    &lt;p&gt;Every malicious commit was authored as:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;Author: Linus Torvalds &amp;lt;[email protected]&amp;gt;Message: init&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;An attacked branch: a single "init" commit attributed to Linus Torvalds, thousands of commits behind main.&lt;/p&gt;
    &lt;p&gt;We haven't found reports of other Shai-Hulud victims seeing this same "Linus Torvalds" vandalism pattern. The worm's documented behavior focuses on credential exfiltration and npm package propagation, not repository destruction. This destructive phase may have been unique to our attacker, or perhaps a manual follow-up action after the automated worm had done its credential harvesting.&lt;/p&gt;
    &lt;p&gt;Within 4 minutes of detection we identified the compromised account, removed them from the GitHub organization, and the attack stopped immediately.&lt;/p&gt;
    &lt;p&gt;Our internal Slack during those first minutes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Urmmm guys? what's going on?"&lt;/p&gt;
      &lt;p&gt;"add me to the call @here"&lt;/p&gt;
      &lt;p&gt;"Nick could you double check Infisical for any machine identities"&lt;/p&gt;
      &lt;p&gt;"can someone also check whether there are any reports of compromised packages in our CLI deps?"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Within the hour:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:36&lt;/cell&gt;
        &lt;cell&gt;Removed from GitHub organization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:40&lt;/cell&gt;
        &lt;cell&gt;Removed from Infisical (secrets manager)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~15:45&lt;/cell&gt;
        &lt;cell&gt;Removed from AWS IAM Identity Center&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;~16:00&lt;/cell&gt;
        &lt;cell&gt;Removed from Vercel and Cloudflare&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16:35&lt;/cell&gt;
        &lt;cell&gt;AWS SSO sessions blocked via deny policy (sessions can't be revoked)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;16:45&lt;/cell&gt;
        &lt;cell&gt;IAM user console login deleted&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;The damage&lt;/head&gt;
    &lt;p&gt;Repository clone actions: 669 (public and private), including infrastructure code, internal documentation, and engineering plans.&lt;/p&gt;
    &lt;p&gt;Branches force-pushed: 199 across 16 repositories&lt;/p&gt;
    &lt;p&gt;Pull requests closed: 42&lt;/p&gt;
    &lt;p&gt;Protected branch rejections: 4. Some of our repositories have main branch protection enabled, but we had not enabled it for all repositories at the time of the incident.&lt;/p&gt;
    &lt;p&gt;npm packages were not compromised. This is the difference between "our repos got vandalized" and "our packages got compromised."&lt;/p&gt;
    &lt;p&gt;Our engineer didn't have an npm publishing token on their machine, and even if they did we had already required 2FA for publishing to npm. Without that, Shai-Hulud would have published malicious versions of &lt;code&gt;@trigger.dev/sdk&lt;/code&gt;, &lt;code&gt;@trigger.dev/core&lt;/code&gt;, and others, potentially affecting thousands of downstream users.&lt;/p&gt;
    &lt;p&gt;Production databases or any AWS resources were not accessed. Our AWS CloudTrail audit showed only read operations from the compromised account:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Event Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Count&lt;/cell&gt;
        &lt;cell role="head"&gt;Service&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ListManagedNotificationEvents&lt;/cell&gt;
        &lt;cell&gt;~40&lt;/cell&gt;
        &lt;cell&gt;notifications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DescribeClusters&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;ECS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DescribeTasks&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;ECS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DescribeMetricFilters&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;CloudWatch&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These were confirmed to be legitimate operations by our engineer.&lt;/p&gt;
    &lt;p&gt;One nice surprise: AWS actually sent us a proactive alert about Shai-Hulud. They detected the malware's characteristic behavior (ListSecrets, GetSecretValue, BatchGetSecretValue API calls) on an old test account that hadn't been used in months, so we just deleted it. But kudos to AWS for the proactive detection and notification.&lt;/p&gt;
    &lt;head rend="h2"&gt;The recovery&lt;/head&gt;
    &lt;p&gt;GitHub doesn't have server-side reflog. When someone force-pushes, that history is gone from GitHub's servers.&lt;/p&gt;
    &lt;p&gt;But we found ways to recover.&lt;/p&gt;
    &lt;p&gt;Push events are retained for 90 days via the GitHub Events API. We wrote a script that fetched pre-attack commit SHAs:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;# Find pre-attack commit SHA from eventsgh api repos/$REPO/events --paginate | \  jq -r '.[] | select(.type=="PushEvent") |  select(.payload.ref=="refs/heads/'$BRANCH'") |  .payload.before' | head -1&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Public repository forks still contained original commits. We used these to verify and restore branches.&lt;/p&gt;
    &lt;p&gt;Developers who hadn't run &lt;code&gt;git fetch --prune&lt;/code&gt; (all of us?) still had old SHAs in their local reflog.&lt;/p&gt;
    &lt;p&gt;Within 7 hours, all 199 branches were restored.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub app private key exposure&lt;/head&gt;
    &lt;p&gt;During the investigation, our engineer was going through files recovered from the compromised laptop and discovered something concerning: the private key for our GitHub App was in the trash folder.&lt;/p&gt;
    &lt;p&gt;When you create a private key in the GitHub App settings, GitHub automatically downloads it. The engineer had created a key at some point, and while the active file had been deleted, it was still in the trash, potentially accessible to TruffleHog.&lt;/p&gt;
    &lt;p&gt;Our GitHub App has the following permissions on customer repositories:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Permission&lt;/cell&gt;
        &lt;cell role="head"&gt;Access Level&lt;/cell&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;contents&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could read/write repository contents&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;pull_requests&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could read/create/modify PRs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;deployments&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could create/trigger deployments&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;checks&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could create/modify check runs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;commit_statuses&lt;/cell&gt;
        &lt;cell&gt;read/write&lt;/cell&gt;
        &lt;cell&gt;Could mark commits as passing/failing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;metadata&lt;/cell&gt;
        &lt;cell&gt;read&lt;/cell&gt;
        &lt;cell&gt;Could read repository metadata&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To generate valid access tokens, an attacker would need both the private key (potentially compromised) and the installation ID for a specific customer (stored in our database which was not compromised, not on the compromised machine).&lt;/p&gt;
    &lt;p&gt;We immediately rotated the key:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Time (UTC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 26, 18:51&lt;/cell&gt;
        &lt;cell&gt;Private key discovered in trash folder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nov 26, 19:54&lt;/cell&gt;
        &lt;cell&gt;New key deployed to test environment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nov 26, 20:16&lt;/cell&gt;
        &lt;cell&gt;New key deployed to production&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We found no evidence of unauthorized access to any customer repositories. The attacker would have needed installation IDs from our database to generate tokens, and our database was not compromised as previously mentioned.&lt;/p&gt;
    &lt;p&gt;However, we cannot completely rule out the possibility. An attacker with the private key could theoretically have called the GitHub API to enumerate all installations. We've contacted GitHub Support to request additional access logs. We've also analyzed the webhook payloads to our GitHub app, looking for suspicious push or PR activity from connected installations &amp;amp; repositories. We haven't found any evidence of unauthorized activity in these webhook payloads.&lt;/p&gt;
    &lt;p&gt;We've sent out an email to potentially effected customers to notify them of the incident with detailed instructions on how to check if they were affected. Please check your email for more details if you've used our GitHub app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical deep-dive: how Shai-Hulud works&lt;/head&gt;
    &lt;p&gt;For those interested in the technical details, here's what we learned about the malware from Socket's analysis and our own investigation.&lt;/p&gt;
    &lt;p&gt;When npm runs the &lt;code&gt;preinstall&lt;/code&gt; script, it executes &lt;code&gt;setup_bun.js&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detects OS/architecture&lt;/item&gt;
      &lt;item&gt;Downloads or locates the Bun runtime&lt;/item&gt;
      &lt;item&gt;Caches Bun in &lt;code&gt;~/.cache&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Spawns a detached Bun process running &lt;code&gt;bun_environment.js&lt;/code&gt;with output suppressed&lt;/item&gt;
      &lt;item&gt;Returns immediately so &lt;code&gt;npm install&lt;/code&gt;completes successfully with no warnings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The malware runs in the background while you think everything is fine.&lt;/p&gt;
    &lt;p&gt;The payload uses TruffleHog to scan &lt;code&gt;$HOME&lt;/code&gt; for GitHub tokens (from env vars, gh CLI config, git credential helpers), AWS/GCP/Azure credentials, npm tokens from &lt;code&gt;.npmrc&lt;/code&gt;, environment variables containing anything that looks like a secret, and GitHub Actions secrets (if running in CI).&lt;/p&gt;
    &lt;p&gt;Stolen credentials are uploaded to a newly-created GitHub repo with a random name. The data is triple base64-encoded to evade GitHub's secret scanning.&lt;/p&gt;
    &lt;p&gt;Files created:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;contents.json&lt;/code&gt;(system info and GitHub credentials)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;environment.json&lt;/code&gt;(all environment variables)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cloud.json&lt;/code&gt;(cloud provider credentials)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;truffleSecrets.json&lt;/code&gt;(filesystem secrets from TruffleHog)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;actionsSecrets.json&lt;/code&gt;(GitHub Actions secrets if any)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If an npm publishing token is found, the malware validates the token against the npm registry, fetches packages maintained by that account, downloads each package, patches it with the malware, bumps the version, and re-publishes, infecting more packages.&lt;/p&gt;
    &lt;p&gt;This is how the worm spread through the npm ecosystem, starting from PostHog's compromised CI on November 24th at 4:11 AM UTC. Our engineer was infected roughly 16 hours after the malicious packages went live.&lt;/p&gt;
    &lt;p&gt;If no credentials are found to exfiltrate or propagate, the malware attempts to delete the victim's entire home directory. Scorched earth.&lt;/p&gt;
    &lt;p&gt;File artifacts to look for: &lt;code&gt;setup_bun.js&lt;/code&gt;, &lt;code&gt;bun_environment.js&lt;/code&gt;, &lt;code&gt;cloud.json&lt;/code&gt;, &lt;code&gt;contents.json&lt;/code&gt;, &lt;code&gt;environment.json&lt;/code&gt;, &lt;code&gt;truffleSecrets.json&lt;/code&gt;, &lt;code&gt;actionsSecrets.json&lt;/code&gt;, &lt;code&gt;.trufflehog-cache/&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;Malware file hashes (SHA1):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;:&lt;code&gt;d60ec97eea19fffb4809bc35b91033b52490ca11&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bun_environment.js&lt;/code&gt;:&lt;code&gt;3d7570d14d34b0ba137d502f042b27b0f37a59fa&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setup_bun.js&lt;/code&gt;:&lt;code&gt;d1829b4708126dcc7bea7437c04d1f10eacd4a16&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We've published a detection script that checks for Shai-Hulud indicators.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we've changed&lt;/head&gt;
    &lt;p&gt;We disabled npm scripts globally:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;npm config set ignore-scripts true --location=global&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;This prevents &lt;code&gt;preinstall&lt;/code&gt;, &lt;code&gt;postinstall&lt;/code&gt;, and other lifecycle scripts from running. It's aggressive and some packages will break, but it's the only reliable protection against this class of attack.&lt;/p&gt;
    &lt;p&gt;We upgraded to pnpm 10. This was significant effort (had to migrate through pnpm 9 first), but pnpm 10 brings critical security improvements. Scripts are ignored by default. You can explicitly whitelist packages that need to run scripts via &lt;code&gt;pnpm.onlyBuiltDependencies&lt;/code&gt;. And the &lt;code&gt;minimumReleaseAge&lt;/code&gt; setting prevents installing packages published recently.&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;# pnpm-workspace.yamlminimumReleaseAge: 4320 # 3 days in minutespreferOffline: true&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;To whitelist packages that legitimately need build scripts:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm approve-builds&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;This prompts you to select which packages to allow (like &lt;code&gt;esbuild&lt;/code&gt;, &lt;code&gt;prisma&lt;/code&gt;, &lt;code&gt;sharp&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;For your global pnpm config:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm config set minimumReleaseAge 4320pnpm config set --json minimumReleaseAgeExclude '["@trigger.dev/*", "trigger.dev"]'&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;We switched npm publishing to OIDC. No more long-lived npm tokens anywhere. Publishing now uses npm's trusted publishers with GitHub Actions OIDC. Even if an attacker compromises a developer machine, they can't publish packages because there are no credentials to steal. Publishing only happens through CI with short-lived, scoped tokens.&lt;/p&gt;
    &lt;p&gt;We enabled branch protection on all repositories. Not just critical repos or just OSS repos. Every repository with meaningful code now has branch protection enabled.&lt;/p&gt;
    &lt;p&gt;We've adopted Granted for AWS SSO. Granted encrypts SSO session tokens on the client side, unlike the AWS CLI which stores them in plaintext.&lt;/p&gt;
    &lt;p&gt;Based on PostHog's analysis of how they were initially compromised (via &lt;code&gt;pull_request_target&lt;/code&gt;), we've reviewed our GitHub Actions workflows. We now require approval for external contributor workflow runs on all our repositories (previous policy was only for public repositories).&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons for other teams&lt;/head&gt;
    &lt;p&gt;The ability for packages to run arbitrary code during installation is the attack surface. Until npm fundamentally changes, add this to your &lt;code&gt;~/.npmrc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;ignore-scripts=true&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Yes, some things will break. Whitelist them explicitly. The inconvenience is worth it.&lt;/p&gt;
    &lt;p&gt;pnpm 10 ignores scripts by default and lets you set a minimum age for packages:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt;pnpm config set minimumReleaseAge 4320  # 3 days&lt;lb/&gt;&lt;/code&gt;
    &lt;p&gt;Newly published packages can't be installed for 3 days, giving time for malicious packages to be detected.&lt;/p&gt;
    &lt;p&gt;Branch protection takes 30 seconds to enable. It prevents attackers from pushing to a main branch, potentially executing malicious GitHub action workflows.&lt;/p&gt;
    &lt;p&gt;Long-lived npm tokens on developer machines are a liability. Use trusted publishers with OIDC instead.&lt;/p&gt;
    &lt;p&gt;If you don't need a credential on your local machine, don't have it there. Publishing should happen through CI only.&lt;/p&gt;
    &lt;p&gt;Our #git Slack channel is noisy. That noise saved us.&lt;/p&gt;
    &lt;head rend="h2"&gt;A note on the human side&lt;/head&gt;
    &lt;p&gt;One of the hardest parts of this incident was that it happened to a person.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Sorry for all the trouble guys, terrible experience"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Our compromised engineer felt terrible, even though they did absolutely nothing wrong. It could have happened to any team member.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;npm install&lt;/code&gt; is not negligence. Installing dependencies is not a security failure. The security failure is in an ecosystem that allows packages to run arbitrary code silently.&lt;/p&gt;
    &lt;p&gt;They also discovered that the attacker had made their GitHub account star hundreds of random repositories during the compromise. Someone even emailed us: "hey you starred my repo but I think it was because you were hacked, maybe remove the star?"&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from compromise to first attacker activity&lt;/cell&gt;
        &lt;cell&gt;~2 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time attacker had access before destructive action&lt;/cell&gt;
        &lt;cell&gt;~17 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Duration of destructive attack&lt;/cell&gt;
        &lt;cell&gt;~10 minutes (15:27-15:37 UTC)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from first malicious push to detection&lt;/cell&gt;
        &lt;cell&gt;~5 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time from detection to access revocation&lt;/cell&gt;
        &lt;cell&gt;~4 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time to full branch recovery&lt;/cell&gt;
        &lt;cell&gt;~7 hours&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Repository clone actions by attacker&lt;/cell&gt;
        &lt;cell&gt;669&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Repositories force-pushed&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Branches affected&lt;/cell&gt;
        &lt;cell&gt;199&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pull requests closed&lt;/cell&gt;
        &lt;cell&gt;42&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Protected branch rejections&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Resources&lt;/head&gt;
    &lt;p&gt;About the Attack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Socket.dev: Shai-Hulud Strikes Again V2 - Technical deep-dive into the malware&lt;/item&gt;
      &lt;item&gt;PostHog Post-Mortem - Another company's experience with Shai-Hulud&lt;/item&gt;
      &lt;item&gt;Wiz Blog: Shai-Hulud 2.0 Supply Chain Attack&lt;/item&gt;
      &lt;item&gt;The Hacker News Coverage&lt;/item&gt;
      &lt;item&gt;Endor Labs Analysis&lt;/item&gt;
      &lt;item&gt;HelixGuard Advisory (referenced in AWS alert)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mitigation Resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;npm Trusted Publishers - OIDC-based publishing&lt;/item&gt;
      &lt;item&gt;pnpm onlyBuiltDependencies - Whitelist packages allowed to run scripts&lt;/item&gt;
      &lt;item&gt;pnpm minimumReleaseAge - Delay installation of new packages&lt;/item&gt;
      &lt;item&gt;Granted - AWS SSO credential management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have questions about this incident? Reach out on Twitter/X or Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262021</guid><pubDate>Sun, 14 Dec 2025 10:07:00 +0000</pubDate></item><item><title>Efficient Basic Coding for the ZX Spectrum</title><link>https://blog.jafma.net/2020/02/24/efficient-basic-coding-for-the-zx-spectrum/</link><description>&lt;doc fingerprint="bb19c69cc4bc9fbe"&gt;
  &lt;main&gt;
    &lt;p&gt;[Click here to read this in English ]&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Éste es el primero de una serie de artículos que explican los fundamentos de la (in)eficiencia de los programas en BASIC puro para el ZX Spectrum:&lt;/p&gt;
      &lt;p&gt;I. Sobre los números de línea&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;El intérprete de lenguaje Sinclair BASIC incluido en la ROM del ZX Spectrum es, en muchos aspectos, una maravilla del software, concretamente de la programación en ensamblador, y daría para hablar durante mucho tiempo. En esta serie queremos destacar los puntos más importantes a tener en cuenta para que los programas escritos en ese lenguaje sean lo más eficientes posibles, en primer lugar en tiempo de ejecución, pero también en espacio ocupado en memoria.&lt;/p&gt;
    &lt;p&gt;En esta primera entrega de la serie trataremos de las líneas de dichos programas; más allá de la necesidad de numerarlas, algo que no se hace desde hace décadas en ningún lenguaje de programación, está el propio hecho de la eficiencia del intérprete a la hora de manejarlas.&lt;/p&gt;
    &lt;p&gt;Antes de meternos en el meollo, conviene resumir los límites que existen en esta máquina relativos a las líneas de programa:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Las líneas de programa, una vez éste queda almacenado en la memoria listo para su ejecución, ocupan 2 bytes (por cierto, almacenados en formato big-endian, el único caso de este formato en el ZX). Esto podría llevar a pensar que tenemos disponibles desde la línea 0 a la 65535 (el máximo número que puede almacenarse en 2 bytes), pero no es exactamente así. A la hora de editar manualmente un programa sólo se nos permite numerar las líneas desde 1 a 9999. Si el programa es manipulado fuera del editor (se puede hacer con &lt;code&gt;POKE&lt;/code&gt;), es posible tener la línea 0, y ésta aparecer al listarlo, pero no será editable. De la misma manera (manipulando el programa con&lt;code&gt;POKE&lt;/code&gt;) se pueden numerar líneas por encima de la 9999; sin embargo, esto causará problemas en ejecución: muchas sentencias del lenguaje que admiten un número de línea como parámetro, como&lt;code&gt;GO TO&lt;/code&gt;o&lt;code&gt;RESTORE&lt;/code&gt;, dan error si la línea es mayor de 32767; la pila de llamadas dejará de funcionar correctamente si se hace un&lt;code&gt;GO SUB&lt;/code&gt;a una línea mayor de 15871 (3DFF en hexadecimal); el intérprete reserva el número de línea 65534 para indicar que está ejecutando código escrito en el buffer de edición (y no en el listado del programa); por último, listar programas por pantalla tampoco funciona bien con líneas mayores de 9999, y en cuanto las editemos manualmente volverán a quedar con sólo 4 dígitos decimales.&lt;/item&gt;
      &lt;item&gt;La longitud en bytes de cada línea de programa se almacena justo después del número de línea, ocupando 2 bytes (esta vez en little-endian). Esta longitud no incluye ni el número de línea ni la longitud en sí misma. Por tanto, podríamos esperar poder tener líneas de un máximo de 65535 bytes en su contenido principal (menos 1, porque siempre tiene que haber un 0x0D al final para indicar el fin de línea); asimismo, las líneas más cortas ocuparán en memoria 2+2+1+1 = 6 bytes: serían aquéllas que contienen una sola sentencia que no tiene parámetros, p.ej., &lt;code&gt;10 CLEAR&lt;/code&gt;. Una rutina muy importante en la ROM del Spectrum, la encargada de buscar la siguiente línea o la siguiente variable saltándose la actual (llamada&lt;code&gt;NEXT-ONE&lt;/code&gt;y situada en la dirección 0x19B8) funciona perfectamente con rangos de tamaño de línea entre 0 y 65535, pero en ejecución el intérprete dejará de interpretar una línea en cuanto se encuentre un 0x0D al comienzo de una sentencia (si la línea es más larga, por ejemplo porque se haya extendido mediante manipulaciones externas, ignorará el resto, por lo que puede ser usado ese espacio para almacenar datos dentro del programa). Más importante aún: dará error al tratar de ejecutar más de 127 sentencias en una misma línea, es decir, una línea en ejecución sólo puede tener, en la práctica, desde 1 hasta 127 sentencias.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Una vez resumidos los datos básicos sobre las líneas y los números de línea, nos centraremos en una característica muy concreta del intérprete de BASIC que resulta fundamental para conseguir incrementar su eficiencia en la ejecución de programas:&lt;/p&gt;
    &lt;p&gt;El intérprete no usa una tabla indexada de líneas de programa&lt;/p&gt;
    &lt;p&gt;Los programas BASIC del ZX se pre-procesan nada más teclearlos (tras teclear líneas completas en el caso del ZX Spectrum +2 y superiores), lo que ahorra espacio en ROM al evitar el analizador léxico que haría falta posteriormente. En ese pre-proceso no sólo se resumen palabras clave de varias letras en un sólo byte, es decir, se tokeniza (qué palabro más feo), sino que se aprovecha para insertar en los lugares más convenientes para la ejecución algunos elementos pre-calculados: un ejemplo es el propio tamaño en memoria de cada línea, como se ha explicado antes, pero también se almacenan silenciosamente los valores numéricos de los literales escritos en el texto (justo tras dichos literales), y se reservan huecos para recoger los argumentos de las funciones de usuario (justo tras los nombres de los correspondientes parámetros en la sentencia &lt;code&gt;DEF FN&lt;/code&gt;), por ejemplo. &lt;/p&gt;
    &lt;p&gt;Lo que nunca, nunca se hace es reservar memoria para almacenar una tabla con las direcciones en memoria de cada línea de programa. Es decir, una tabla que permita saber, a partir de un número de línea y con complejidad computacional constante (tardando siempre lo mismo independientemente del número de línea, lo que formalmente se escribe O(1)), el lugar de memoria donde comienza el contenido tokenizado de dicha línea, para poder acceder rápidamente a las sentencias correspondientes y ejecutarlas.&lt;/p&gt;
    &lt;p&gt;Esto tiene una consecuencia importante para el intérprete: cualquier sentencia del lenguaje que admita como parámetro una línea (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, etc.) implica, durante su ejecución, buscar activamente el comienzo de dicha línea a lo largo de toda la memoria donde reside el programa. Desde el punto de vista de la complejidad computacional, esto no es constante, sino lineal (o sea, peor): O(n), siendo n el número de líneas de programa; en otras palabras: tarda más cuanto más lejos esté la línea que se busca del comienzo del programa. El intérprete implementa esa búsqueda con un puntero (o sea, una dirección de memoria) que empieza apuntando a donde reside la primera línea en memoria; mientras no sea ésta la línea que se busca, o la inmediatamente posterior a la que se busca si se busca una que no existe, suma al puntero el tamaño que ocupa el contenido de la línea en memoria, obteniendo un nuevo puntero que apunta al lugar de memoria donde reside la siguiente línea, y repite el proceso.&lt;/p&gt;
    &lt;p&gt;Un importante resultado de esta implementación del intérprete es que toda sentencia que implique un salto a una línea de programa (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt;, &lt;code&gt;FN&lt;/code&gt;) incrementará su tiempo de cómputo linealmente con el número de líneas que haya antes de la de destino. Esto se puede comprobar con un programa que mide el tiempo para distintas líneas de destino, como el que puede descargarse aquí. Tras ejecutarlo (¡cuidado!: tarda más de 17 horas en terminar debido al nivel de precisión con el que queremos estimar los tiempos) obtenemos los siguientes resultados:&lt;/p&gt;
    &lt;p&gt;Como se observa, los saltos incrementan su tiempo en 71 microsegundos por cada línea más que haya antes de la de destino; eso supone unos 7 milisegundos cuando hay 100 líneas antes, lo que puede ser mucho si el salto se repite a menudo (por ejemplo, si lo hace un bucle &lt;code&gt;FOR&lt;/code&gt;–&lt;code&gt;NEXT&lt;/code&gt;). El programa anterior toma 10000 medidas de tiempo para calcular la media mostrada finalmente en la gráfica, por lo que el Teorema del Límite Central  indica que los resultados expuestos arriba tienen una incertidumbre  pequeña, del orden de 115.5 microsegundos si consideramos como fuente de  incertidumbre original más importante los 20 milisegundos producidos como máximo por la discretización del tiempo de la variable del sistema &lt;code&gt;FRAMES&lt;/code&gt; (el hecho de tomar tantos datos hace, por el mismo teorema, que la distribución de la estimación sea simétrica y no tenga bias, por lo que la media mostrada en la figura será prácticamente la verdadera, a pesar de dicha incertidumbre). También se observan en la gráfica los 5.6 milisegundos de media que se tarda en ejecutar todo lo que no es el salto en el programa de prueba.&lt;/p&gt;
    &lt;p&gt;Por tanto, aquí va la primera regla de eficiencia para mejorar el tiempo de cómputo:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Si quieres que cierta parte de tu programa BASIC se ejecute más rápido, y esa parte contiene el destino de bucles (&lt;/p&gt;&lt;code&gt;GO TO&lt;/code&gt;,&lt;code&gt;NEXT&lt;/code&gt;) o es llamada muy frecuentemente por otras (&lt;code&gt;GO SUB&lt;/code&gt;o&lt;code&gt;DEF FN&lt;/code&gt;), deberías moverla al principio del programa, o lo más cerca del principio que puedas; de esa manera, el intérprete tardará sensiblemente menos en encontrar las líneas a las que hay que saltar.&lt;/quote&gt;
    &lt;p&gt;Para ayudar en la tarea de identificar estos problemas, el intérprete de BASIC incluido en la herramienta ZX-Basicus puede producir un perfil de la frecuencia de ejecución de cada sentencia de un programa (opción &lt;code&gt;--profile&lt;/code&gt;); si la lista ordenada de frecuencias que recopila no va en orden creciente de número de línea, significa que algunas líneas de las más frecuentemente llamadas podrían estar mal situadas.&lt;/p&gt;
    &lt;p&gt;Existe un truco en BASIC para hacer que el intérprete no tenga que buscar desde el principio del programa para encontrar una línea, sino que empiece la búsqueda en otro lugar (más cercano a lo que busque). Consiste en cambiar el contenido de la variable del sistema &lt;code&gt;PROG&lt;/code&gt;, que está en la dirección 23635 y ocupa 2 bytes, por la dirección de memoria donde resida la primera línea que queramos que el intérprete use para sus búsquedas (eso hará que el intérprete ignore la existencia de todas las anteriores, así que ¡éstas dejarán de ser accesibles!). En general no hay modo fácil de saber en qué dirección de memoria reside una línea, pero la variable del sistema &lt;code&gt;NXTLIN&lt;/code&gt; (dirección 23637, 2 bytes) guarda en todo momento la dirección de la línea siguiente a la que estamos (la herramienta de análisis de ZX-Basicus también puede ser útil, pues produce un listado con la localización de cada elemento del programa BASIC en memoria si éste se ha guardado en un fichero &lt;code&gt;.tap&lt;/code&gt;). Por tanto, para, por ejemplo, hacer que un bucle vaya más rápido, se puede hacer &lt;code&gt;POKE&lt;/code&gt; a los dos bytes de &lt;code&gt;PROG&lt;/code&gt; con el valor que tengan los de &lt;code&gt;NXTLIN&lt;/code&gt; cuando estemos en la línea anterior a la del bucle; desde ese momento, la primera línea del bucle irá tan rápida como si fuera la primera de todo el programa. Eso sí, ¡es importante recuperar el valor original de &lt;code&gt;PROG&lt;/code&gt; si queremos volver a ejecutar alguna vez el resto!&lt;/p&gt;
    &lt;p&gt;El problema de la búsqueda secuencial de líneas que hace la ROM del ZX tiene un efecto particular en el caso de las funciones de usuario (&lt;code&gt;DEF FN&lt;/code&gt;): dado que están pensadas para ser llamadas desde diversos puntos del programa, deberían ir al principio del mismo si esas llamadas van a ser frecuentes, pues cada vez que sean llamadas el intérprete tiene que buscarlas. (Una alternativa, preferida por muchos programadores, es no utilizar &lt;code&gt;DEF FN&lt;/code&gt;, dado el mayor coste de su ejecución respecto a insertar la expresión directamente donde se necesite.) El perfil de frecuencias de uso producido por el intérprete de ZX-Basicus también informa sobre el número de veces que se ha llamado a cada función de usuario con &lt;code&gt;FN&lt;/code&gt;, y la utilidad de transformación tiene una opción (&lt;code&gt;--delunusedfn&lt;/code&gt;) que borra automáticamente todas las sentencias &lt;code&gt;DEF FN&lt;/code&gt; no utilizadas en el código.&lt;/p&gt;
    &lt;p&gt;Es importante hacer notar aquí que el intérprete de BASIC no sólo tiene un comportamiento lineal (O(n)) a la hora de buscar líneas de programa, sino también al buscar sentencias. Es decir: si el programa pretende saltar a una sentencia distinta de la primera de una línea, el intérprete tendrá que buscar dicha sentencia recorriendo todas las anteriores. En Sinclair BASIC existen instrucciones de salto a sentencias distintas de la primera de una línea: &lt;code&gt;NEXT&lt;/code&gt; y &lt;code&gt;RETURN&lt;/code&gt;, que por tanto sufren del problema de las búsquedas lineales. Es conveniente situar el retorno de la llamada o el principio del bucle al principio de la línea, para que el intérprete no tenga que buscar la sentencia concreta dentro de la misma, yendo sentencia a sentencia hasta encontrarla.&lt;/p&gt;
    &lt;p&gt;No existen instrucciones para saltar a sentencias (distintas de la primera) explícitamente dadas por el usuario, pero esto se puede lograr engañando al intérprete con un truco, que podríamos llamar el “GOTO con POKE”, cuya existencia me ha señalado Rafael Velasco al verlo usado en algún programa escrito en una sola línea de BASIC. Este truco se basa en dos variables del sistema: &lt;code&gt;NEWPPC&lt;/code&gt; (dirección 23618 de memoria, 2 bytes) y &lt;code&gt;NSPPC&lt;/code&gt; (dirección 23620, 1 byte). En caso de que una sentencia del programa haga un salto (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;RETURN&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt; …), se rellenan con la línea (en &lt;code&gt;NEWPPC&lt;/code&gt;) y la sentencia (en &lt;code&gt;NSPPC&lt;/code&gt;) a donde hay que saltar, mientras que si no hace un salto, sólo se rellena &lt;code&gt;NSPPC&lt;/code&gt; con 255. Antes de ejecutar la siguiente sentencia, el intérprete consulta &lt;code&gt;NSPPC&lt;/code&gt;, y, si su bit nº 7 no es 1, salta a donde indiquen estas dos variables, mientras que si es 1, sigue ejecutando la siguiente sentencia del programa. El truco del “GOTO con POKE” consiste en manipular estas variables con &lt;code&gt;POKE&lt;/code&gt;, primero en &lt;code&gt;NEWPPC&lt;/code&gt; y luego en &lt;code&gt;NSPPC&lt;/code&gt;, de forma que, justo tras ejecutar el &lt;code&gt;POKE&lt;/code&gt; de &lt;code&gt;NSPPC&lt;/code&gt;, el intérprete se cree que tiene que hacer un salto a donde indican. De esta manera podemos ir a cualquier punto del programa, línea y sentencia incluidas.&lt;/p&gt;
    &lt;p&gt;Recuperando el hilo principal de esta entrada, las sentencias del lenguaje Sinclair BASIC afectadas por el problema de los números de línea / número de sentencia son:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;GO TO&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;GO SUB&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FN&lt;/code&gt;(requiere buscar la línea del correspondiente&lt;code&gt;DEF FN&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RETURN&lt;/code&gt;(debe retornar a un número de línea almacenado en la pila de direcciones de retorno)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NEXT&lt;/code&gt;(debe ir a la línea correspondiente al&lt;code&gt;FOR&lt;/code&gt;de su variable)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RESTORE&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RUN&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LIST&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LLIST&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Como las cuatro últimas no suelen usarse más que esporádicamente (las tres últimas prácticamente nunca dentro de un programa), la identificación de las zonas de código que deben moverse al principio debería enfocarse en bucles, rutinas y funciones de usuario (&lt;code&gt;FN&lt;/code&gt;). &lt;/p&gt;
    &lt;p&gt;Así, los &lt;code&gt;RETURN&lt;/code&gt; deberían hacerse hacia lugares próximos al comienzo del programa, es decir, los &lt;code&gt;GO SUB&lt;/code&gt;  correspondientes deberían estar allí (al principio del programa), y, si puede ser, en la primera sentencia de sus respectivas líneas para que no haya que buscar dentro de la línea la sentencia en cuestión, búsqueda que también se hace linealmente. &lt;/p&gt;
    &lt;p&gt;Los bucles &lt;code&gt;FOR&lt;/code&gt; pueden sustituirse por réplicas consecutivas del cuerpo en caso de que éstas no sean muy numerosas (esto se llama “desenrrollado de bucles”),  lo cual queda muy feo y ocupa más memoria de programa pero evita el coste  adicional de ejecución del salto &lt;code&gt;NEXT&lt;/code&gt; (y el de creación de variable en el &lt;code&gt;FOR&lt;/code&gt;).  &lt;/p&gt;
    &lt;p&gt;En pocas palabras: el código que llama mucho a otro código, es llamado mucho por otro código, o tiene muchos bucles internos debería ir al principio de un programa BASIC y en las primeras sentencias de dichas líneas.&lt;/p&gt;
    &lt;p&gt;Quiero aprovechar para mencionar en este punto que, aunque es de lo más común, en muchos casos sería recomendable no usar expresiones para las referencias a líneas, al menos en las primeras etapas de la escritura de un programa (es decir, no escribir “saltos paramétricos” como &lt;code&gt;GO TO 2*n+100&lt;/code&gt;, &lt;code&gt;GO SUB x*1000&lt;/code&gt;, etc., sino solamente con literales numéricos, como &lt;code&gt;GOTO 100&lt;/code&gt;, &lt;code&gt;GO SUB 2000&lt;/code&gt;). El uso de los saltos paramétricos hace el mantenimiento del programa un verdadero infierno, e impide su análisis automático. De todas formas, hay que admitir que usar expresiones como argumento de &lt;code&gt;GO TO&lt;/code&gt; / &lt;code&gt;GO SUB&lt;/code&gt; puede ser más rápido que escribir sentencias &lt;code&gt;IF&lt;/code&gt; para lograr el mismo objetivo. &lt;/p&gt;
    &lt;p&gt;Todo el asunto de los números de línea tiene una segunda consecuencia:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Para acelerar lo más posible todo el programa deberías escribir líneas lo más largas posible. Así, la búsqueda de una línea particular será más rápida, ya que habrá que recorrer menos líneas hasta llegar a ella (ir de una línea a la siguiente durante la búsqueda que hace el intérprete de la ROM cuesta el mismo tiempo independientemente de su longitud).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ZX-Basicus tiene una transformación disponible con la opción &lt;code&gt;--mergelines&lt;/code&gt; que hace esto automáticamente: aumenta el tamaño de las líneas siempre que esto respete el flujo del programa original. &lt;/p&gt;
    &lt;p&gt;Nótese que el usar menos líneas pero más largas ahorra también espacio en memoria, ya que no hay que almacenar números, longitudes ni marcas de fin de esas líneas. Por contra, con líneas largas es más costoso encontrar una sentencia a la que haya que retornar con un &lt;code&gt;RETURN&lt;/code&gt; o volver con un &lt;code&gt;NEXT&lt;/code&gt;, así como buscar una función de usuario (&lt;code&gt;DEF FN&lt;/code&gt;) que no esté al principio de su línea, por lo que hay que tener también eso en cuenta y llegar a una solución de compromiso.&lt;/p&gt;
    &lt;p&gt;Aún hay una tercera consecuencia de esta limitación del intérprete de BASIC de la ROM:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Las sentencias no ejecutables (&lt;/p&gt;&lt;code&gt;REM&lt;/code&gt;y sentencias vacías) que ocupan una sola línea deberían eliminarse siempre que se pueda, pues incrementan el tiempo de búsqueda, o bien ponerlas al final del todo. Asimismo, las sentencias&lt;code&gt;DATA&lt;/code&gt;, que normalmente no se usan más de una vez durante la ejecución del programa, deberían estar al final del programa.&lt;/quote&gt;
    &lt;p&gt;ZX-Basicus también ayuda en esto: permite eliminar automáticamente comentarios &lt;code&gt;REM&lt;/code&gt; (opción &lt;code&gt;--delrem&lt;/code&gt;) y sentencias vacías (opción &lt;code&gt;--delempty&lt;/code&gt;). La primera opción permite preservar algunos comentarios sin ser eliminados: los que comiencen por algún carácter que nosotros decidamos, pues siempre es interesante no dejar el código totalmente indocumentado. &lt;/p&gt;
    &lt;p&gt;En cualquier caso, quizás la opción más importante del optimizador de código de que dispone ZX-Basicus es &lt;code&gt;--move&lt;/code&gt;, que da la posibilidad de mover trozos de código de un lugar a otro con menos esfuerzo que a mano. Con ella se puede cambiar de sitio una sección completa del programa; la utilidad se encarga de renumerar el resultado automáticamente. Hay que tener en cuenta, sin embargo, que esta utilidad (como cualquier otra existente) no puede renumerar ni trabajar con números de línea calculados mediante expresiones, por lo que todas las referencias a líneas de programa deberían estar escritas como literales, tal y como se ha recomendado antes.&lt;/p&gt;
    &lt;p&gt;.oOo.&lt;/p&gt;
    &lt;p&gt;[Click here to read this in Spanish ]&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is the first in a series of posts that explain the foundations of the (in)efficiency of pure BASIC programs written for the ZX Spectrum:&lt;/p&gt;
      &lt;p&gt;I. On line numbers&lt;/p&gt;
      &lt;p&gt;II. On variables&lt;/p&gt;
      &lt;p&gt;III. On expressions&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Sinclair BASIC interpreter that the ZX Spectrum included in ROM was, in so many aspects, a wonder of software, particularly in assembly programming.&lt;/p&gt;
    &lt;p&gt;In this series of posts we will visit the main issues that allow our BASIC programs to execute efficiently, mainly considering time, but also memory consumption.&lt;/p&gt;
    &lt;p&gt;In this first post we are concerned in particular with the lines in a program; beyond the need for numbering them explicitly, something that does not exist in any programming language since decades, we are interested in the efficciency of the BASIC interpreter when managing lines and their numbers.&lt;/p&gt;
    &lt;p&gt;Before going to the point, we summarize here some limits that the ZX Spectrum has related to program lines:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Program line numbers, once the program is stored in memory and ready to be executed, take 2 bytes (by the way, they are stored in big-endian format, the only case of that in the ZX). This could lead to line numbers in the range 0 to 65535 (maximum value that can be stored into 2 bytes), but unfortunately that cannot be done easily. When editing a program manually, only lines from 1 to 9999 are allowed. If the program is manipulated outside the editor (which can be done with &lt;code&gt;POKE&lt;/code&gt;), it is possible to have a line numbered as 0, and that line will appear in the listing of the program, but it will no longer be editable. In the same way (using&lt;code&gt;POKE&lt;/code&gt;) you can have lines above 9999, but this causes trouble: many statements that admit a line number as a parameter, such as&lt;code&gt;GOTO&lt;/code&gt;or&lt;code&gt;RESTORE&lt;/code&gt;, produce an error if that line is greater than 32767; the call stack stop working correctly if we do a&lt;code&gt;GO SUB&lt;/code&gt;to a line greater than 15871 (3DFF in hexadecimal); the interpreter reserves the line number 65534 to indicate that it is executing code from the edition buffer (and not from the program listing); also, listing the program on the screen does not work well with lines greater than 9999, and right at the moment we edit these lines manually, they will be set to line numbers with just 4 digits.&lt;/item&gt;
      &lt;item&gt;The length of each program line (in bytes) is stored after the line number, and occupies 2 bytes (this time in little-endian). This length does not take into account the 2 bytes of the line number or the 2 bytes of itself. We could think that each line can have up to 65535 bytes (a 0x0D byte has to always be at the end to mark the end of the line), and that the shortest line takes 2+2+1+1 = 6 bytes of memory if it contains just one statement without parameters, e.g., &lt;code&gt;10 CLEAR&lt;/code&gt;. A very important ROM routine, the one in charge of finding the line or variable that is after the current one, skipping the latter (called&lt;code&gt;NEXT-ONE&lt;/code&gt;and located at 0x19B8) works perfectly well with line lengths in the range 0 to 65535. However, during execution, the interpreter stops its work on a line as soon as it finds 0x0D in the beginning of a statement (if the line is longer because it has been externally manipulated, it will ignore the rest, thus the remaining space can be used for storing -hidden- data within the program), and more importantly: the interpreter yields an error if trying to execute more than 127 statements in a given line. Consequently, a line in execution can only have from 1 to 127 statements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once we have summarized these data, we will focus on a very specific feature of the BASIC interpreter of the ZX Spectrum, one that is crucial for the efficiency of running BASIC programs:&lt;/p&gt;
    &lt;p&gt;There is no table of program addresses indexed with line numbers&lt;/p&gt;
    &lt;p&gt;BASIC programs were pre-processed right after typing them (after typing whole lines in the case of ZX Spectrum +2 and up), which saved space in ROM by not implementing a lexical analyzer. In that pre-processing, multi-character keywords were summarized into one-byte tokens, but many other things happened too: number literals were coded in binary form and hidden near the source numbers, line lengths were stored at the beginning of each line, placeholders were prepared for the parameters of user functions (&lt;code&gt;DEF FN&lt;/code&gt;) in order to store arguments when they are called, etc.&lt;/p&gt;
    &lt;p&gt;Unfortunately, there is one thing that was not done before executing the program: to build a table that, for each line number, provides in constant time (computational complexity O(1)) the memory address where that line is stored.&lt;/p&gt;
    &lt;p&gt;This has an important effect in the interpreter execution: every time it finds a statement in the program that has a line number as a parameter, (e.g., &lt;code&gt;GOTO&lt;/code&gt;, &lt;code&gt;GOSUB&lt;/code&gt;, etc.), the interpreter must search the entire program memory, line by line, until finding the place in memory where the referred line resides. This has a computational complexity of O(n), being n the number of lines in the program, i.e., it is linearly more costly to find the last lines in the program than the earlier ones. The interpreter works like this: it starts with a memory address that points to the beginning of the program, reads the line number that is there, if it is the one searched for, or the one immediatly after it, ends, otherwise reads the line length, add that length to the pointer, and repeats the process.&lt;/p&gt;
    &lt;p&gt;The result of this interpreter inner workings is that any statement that involves a jump to a line in the program (&lt;code&gt;GOTO&lt;/code&gt;, &lt;code&gt;GOSUB&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt;, &lt;code&gt;FN&lt;/code&gt;) will increase its execution time linearly with the number of lines that exist before the one of destination. That can be checked out with a BASIC program that measures that time for different destinations, such as the one you can download here. After executing it (care!: it takes more than 17 hours to achieve the precision we require in the estimations) we got this:&lt;/p&gt;
    &lt;p&gt;As you can see, the execution time in a jump increases in 71 microseconds per line of the program that we add before the destination line; that amounts to about 7 milliseconds if you have 100 lines before the destination, which can be a lot if the jump is part of a loop that repeats a lot of times. Our testing program takes 10000 measurements to get the final average time, thus the Central Limit Theorem suggests that the results in the figure above have a small amount of uncertainty, of around 115.5 microseconds if we consider as the main source of original uncertainty the [0,20] milliseconds produced by the time discretization of the &lt;code&gt;FRAMES&lt;/code&gt; system variable (this uncertainty does not affect the fact that, due to the same theorem and the large number of measurements, the average estimates will be distributed symmetrically and unbiasedly, i.e., they are practically equal to the real ones). You can also observe in the graph above that the parts of the loops in the testing program that are not the jump itself consume 5.6 milliseconds on average.&lt;/p&gt;
    &lt;p&gt;The first consequence of this is the first rule for writing efficient programs in pure Sinclair BASIC for the ZX Spectrum:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Those parts of the program that require a faster execution should be placed at the beginning (smaller line numbers). The same should be done for parts that contain loops or routines that are frequently called.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;ZX-Basicus has an optimizing tool that can help in this aspect. For instance, it can execute a BASIC program in the PC and collect a profile with the frequency of execution of each statement (using the &lt;code&gt;--profile&lt;/code&gt; option). In this way, you can identify those parts of the code that would require to be re-located earlier in the listing.&lt;/p&gt;
    &lt;p&gt;There is a BASIC trick to cheat the interpreter and make it to search for a line starting in a place different from the start of the program. It consists in changing the value of the system variable &lt;code&gt;PROG&lt;/code&gt;, which is located at the memory address 23635 and occupies 2 bytes, to the memory address of the first line we wish the interpreter to use for its line search (therefore ignoring all the previous ones). In general, it is not easy to get the memory address of a line, but you can consult the system variable &lt;code&gt;NXTLIN&lt;/code&gt; (at 23637, 2 bytes), which stores the address of the next line to be executed (the analysis tool of ZX-Basicus also provides this kind of information with the location in memory of every element in the BASIC program if it is stored in a &lt;code&gt;.tap&lt;/code&gt; file). You can make, for example, a loop faster: do &lt;code&gt;POKE&lt;/code&gt; in the two bytes of &lt;code&gt;PROG&lt;/code&gt; with the value stored in &lt;code&gt;NXTLIN&lt;/code&gt;, and do that right at the line previous to the one of the loop; the result is that the loop will be as fast as though it was in first line of the program. However, do not forget to restore the original value of &lt;code&gt;PROG&lt;/code&gt; in order to access previous parts of that program!&lt;/p&gt;
    &lt;p&gt;User functions definitions (&lt;code&gt;DEF FN&lt;/code&gt;) are specially sensitive to the problem of searching line numbers. They are devised for being called repeteadly, therefore, they should also be at the beginning of the program. However, many programmers choose not to use them because of their high execution cost (which includes finding the line where they are defined, evaluating arguments, placing their values in the placeholders, and evaluating the expression of their bodies). The profile produced by ZX-Basicus also reports the number of calls to user functions (&lt;code&gt;FN&lt;/code&gt;), and it provides an option (&lt;code&gt;--delunusedfn&lt;/code&gt;) that automatically delete all &lt;code&gt;DEF FN&lt;/code&gt; that are not called in the program.&lt;/p&gt;
    &lt;p&gt;It is important to note that the BASIC interpreter has a linear (O(n)) behaviour not only when searching for lines, but also when searching for statements within a line. If the program tries to jump to a statement different from the first one in a line, the interpreter will search for that statement by skipping all the previous ones. In Sinclair BASIC we have instructions that may jump to statements different from the first ones in their lines: &lt;code&gt;NEXT&lt;/code&gt; and &lt;code&gt;RETURN&lt;/code&gt;, that, consequently, suffer from the problem of the linear searches. It is better to place the return of the call or the start of the loop at the beginning of a line to prevent the interpreter to conduct a linear search (statement by statement) to find them.&lt;/p&gt;
    &lt;p&gt;There are no instructions in the language to jump to statements that are explicitly given by the user, but that can be achieved by cheating the interpreter with a trick, that we could call “GOTO-with-POKE”, whose has been brought to my attention by Rafael Velasco, that saw it in a BASIC program entirely written in a single line. It is based on two system variables: &lt;code&gt;NEWPPC&lt;/code&gt; (address 23618, 2 bytes) and &lt;code&gt;NSPPC&lt;/code&gt; (address 23620, 1 byte). When a program statement makes a jump (&lt;code&gt;GO TO&lt;/code&gt;, &lt;code&gt;GO SUB&lt;/code&gt;, &lt;code&gt;RETURN&lt;/code&gt;, &lt;code&gt;NEXT&lt;/code&gt; …), the target line is stored into &lt;code&gt;NEWPPC&lt;/code&gt; and the target statement into &lt;code&gt;NSPPC&lt;/code&gt;; if the statement does not make a jump, &lt;code&gt;NSPPC&lt;/code&gt; is filled with 255; before executing the next statement, the interpret reads &lt;code&gt;NSPPC&lt;/code&gt; and, if the bit 7 of this variables is not 1, jumps to the place defined by &lt;code&gt;NEWPPC&lt;/code&gt;:&lt;code&gt;NSPPC&lt;/code&gt;, but if that bit is 1 it just goes on with the next statement. The “GOTO-with-POKE” trick consists in &lt;code&gt;POKE&lt;/code&gt;ing those variables, firstly &lt;code&gt;NEWPPC&lt;/code&gt;, then &lt;code&gt;NSPPC&lt;/code&gt;; right after the last &lt;code&gt;POKE&lt;/code&gt;, the interpreter believes there is a jump to do. In this way, we can go to any line and statement in our program.&lt;/p&gt;
    &lt;p&gt;Recovering the main thread of this post, the statements of the Sinclair BASIC language that involve to search lines in the program are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;GO TO&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;GO SUB&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FN&lt;/code&gt;(since&lt;code&gt;DEF FN&lt;/code&gt;must be searched for)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RETURN&lt;/code&gt;(it returns to a certain number of line and statement)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NEXT&lt;/code&gt;(it jumps to the corresponding&lt;code&gt;FOR&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RESTORE&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;RUN&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LIST&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;LLIST&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since the last four are used sporadically (the last three are very rare inside a program), the identification of parts of the program to be placed at the beginning for gaining in efficiency should focus on loops, routines and user functions. &lt;code&gt;RETURN&lt;/code&gt; statements should be used to return to places close to the beginning too, if they are frequently used, i.e., the corresponding &lt;code&gt;GO SUB&lt;/code&gt;  should be placed at the beginning, and, if possible, at the beginning  of their lines in order to reduce the cost of searching them within those lines. Also, in cases where they can not be re-placed, &lt;code&gt;FOR&lt;/code&gt; loops can be unrolled  (repeating their bodies as many times as iterations they have) to avoid  the jumps and the maintainance of the iteration variable. In summary: the code that calls a lot of routines, or is called frequently, or has many internal loops, should be placed at the beginning of the program. &lt;/p&gt;
    &lt;p&gt;I also recommend to only use literal numbers in the parameters of the statements that need a line (e.g., &lt;code&gt;GOTO 100&lt;/code&gt;, &lt;code&gt;GO SUB 2000&lt;/code&gt;), at least in the first stages of the writing of a program; do not use expressions at that time (“parametrical jumps”, e.g., &lt;code&gt;GO TO 2*n+100&lt;/code&gt;, &lt;code&gt;GO SUB x*1000&lt;/code&gt;, etc.), since that makes the maintainance and analysis of the program really difficult. I have to admit, though, that  using expressions as arguments in &lt;code&gt;GO TO&lt;/code&gt; / &lt;code&gt;GO SUB&lt;/code&gt; usually runs faster than writing &lt;code&gt;IF&lt;/code&gt; statements to achieve the same functionality. &lt;/p&gt;
    &lt;p&gt;The second consequence of the interpreter lacking an efficient line number table is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lines should be long (the maximum length is 127 statements in a line for the ROM interpreter not to issue an error). In that way, the search for a particular one will be more efficient, since traversing the lines has the same cost independently on their lengths (it only depends on the number of lines).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In this aspect, ZX-Basicus has an option (&lt;code&gt;--mergelines&lt;/code&gt;) that automatically merges contiguous lines, as long as that does not changes the program execution flow, in order to obtain the least number of lines.&lt;/p&gt;
    &lt;p&gt;Notice that having less but longer lines also saves memory space, since there are less line numbers and lengths (and end-line markers) to store. However, having longer lines makes less efficient the search for some statement within them (as in the case of &lt;code&gt;FOR&lt;/code&gt;…&lt;code&gt;NEXT&lt;/code&gt;, or &lt;code&gt;GO SUB&lt;/code&gt;, or &lt;code&gt;DEF FN&lt;/code&gt;). A suitable trade-off must be reached.&lt;/p&gt;
    &lt;p&gt;Finally, the third consequence of not having a line number table is:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Non-executable statements (&lt;/p&gt;&lt;code&gt;REM&lt;/code&gt;and empty statements) that fill entire lines should be eliminated or placed at the end, since they increase the search time for no reason. Also,&lt;code&gt;DATA&lt;/code&gt;statements, that are commonly used only once during the program execution, are excellent candidates to be placed at the end of the program.&lt;/quote&gt;
    &lt;p&gt;In this, ZX-Basicus has also some help for the programmer: it can delete automatically empty statements (&lt;code&gt;--delempty&lt;/code&gt;) and &lt;code&gt;REM&lt;/code&gt; (&lt;code&gt;--delrem&lt;/code&gt;); it can preserve some of the latter for keeping minimum documentation, though.&lt;/p&gt;
    &lt;p&gt;All in all, there is a fundamental tool in ZX-Basicus that is related to this post: option &lt;code&gt;--move&lt;/code&gt; re-locates portions of code, renumbering automatically all the line references (it can also serve to renumber the whole program, but that has no relation to speed-ups). Only take into account that it cannot work with line references that are not literal numbers (expressions, variables, etc.). &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262480</guid><pubDate>Sun, 14 Dec 2025 12:04:44 +0000</pubDate></item><item><title>Europeans' health data sold to US firm run by ex-Israeli spies</title><link>https://www.ftm.eu/articles/europe-health-data-us-firm-israel-spies</link><description>&lt;doc fingerprint="bfd091199664b185"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Europeans’ health data sold to U.S. firm run by ex-Israeli spies&lt;/head&gt;&lt;p&gt;The European messaging service Zivver – which is used for confidential communication by governments and hospitals in the EU and the U.K. – has been sold to Kiteworks, an American company with strong links to Israeli intelligence. Experts have expressed deep concerns over the deal.&lt;/p&gt;&lt;head rend="h3"&gt;This story in 1 minute&lt;/head&gt;&lt;p&gt;What’s the news?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;With the sale of Amsterdam-based data security company Zivver, sensitive information about European citizens is now in the hands of Kiteworks.&lt;/item&gt;&lt;item&gt;The CEO of the American tech company is a former cyber specialist from an elite unit of the Israeli army, as are several other members of its top management.&lt;/item&gt;&lt;item&gt;Various institutions in Europe and the U.K. – from hospitals to courts and immigration services – use Zivver to send confidential documents. While Zivver says these documents are encrypted, an investigation by Follow the Money shows that the company is able to read their contents.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Why does this matter?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Cybersecurity and intelligence experts told Follow the Money that the takeover should either have been prevented or properly assessed in advance.&lt;/item&gt;&lt;item&gt;Zivver processes information that could be extremely valuable to third parties, such as criminals or foreign intelligence services.&lt;/item&gt;&lt;item&gt;That information is now subject to invasive U.S. law, and overseen by a company with well-documented links to Israeli intelligence.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How was this investigated?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Follow the Money investigated the acquisition of Zivver and the management of Kiteworks, and spoke to experts in intelligence services and cyber security.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This article is part of an ongoing series.&lt;/p&gt;The EU Files&lt;p&gt;When the American data security company Kiteworks bought out its Dutch industry peer Zivver in June, CEO Jonathan Yaron described it as “a proud moment for all of us”.&lt;/p&gt;&lt;p&gt;The purchase was “a significant milestone in Kiteworks’ continued mission to safeguard sensitive data across all communication channels”, he added in a LinkedIn post.&lt;/p&gt;&lt;p&gt;But what Yaron did not mention was that this acquisition – coming at a politically charged moment between the U.S. and the EU – put highly sensitive, personal data belonging to European and British citizens directly into American hands.&lt;/p&gt;&lt;p&gt;Zivver is used by institutions including hospitals, health insurers, government services and immigration authorities in countries including the Netherlands, Germany, Belgium and the U.K.&lt;/p&gt;&lt;p&gt;Neither did Yaron mention that much of Kiteworks’ top management – himself included – are former members of an elite Israeli Defence Force unit that specialised in eavesdropping and breaking encrypted communications.&lt;/p&gt;&lt;p&gt;Our journalism is only possible thanks to the trust of our paying members. Not a member yet? Sign up now&lt;/p&gt;&lt;p&gt;In addition to this, an investigation by Follow the Money shows that data processed by Zivver is less secure than the service leads its customers to believe. Research found that emails and documents sent by Zivver can be read by the company itself. This was later confirmed by Zivver to Follow the Money.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;“All of the red flags should have been raised during this acquisition”&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Zivver maintained, however, that it does not have access to the encryption keys used by customers, and therefore cannot hand over data to U.S. authorities.&lt;/p&gt;&lt;p&gt;This is despite independent researchers confirming that the data was – for a brief period – accessible to the company. If U.S. officials wanted access to such communication, Zivver would be legally obligated to provide it.&lt;/p&gt;&lt;p&gt;Cybersecurity experts now point to serious security concerns, and ask why this sale seems to have gone through without scrutiny from European authorities.&lt;/p&gt;&lt;p&gt;“All of the red flags should have been raised during this acquisition,” said intelligence expert Hugo Vijver, a former long-term officer in AIVD, the Dutch security service.&lt;/p&gt;&lt;head rend="h2"&gt;Classified documents&lt;/head&gt;&lt;p&gt;Amsterdam-based Zivver – which was founded in 2015 by Wouter Klinkhamer and Rick Goud – provides systems for the encrypted exchange of information via email, chat and video, among other means.&lt;/p&gt;&lt;p&gt;Dutch courts, for example, work with Zivver to send classified documents, and solicitors use the service to send confidential information to the courts.&lt;/p&gt;&lt;p&gt;Other government agencies in the Netherlands – including the immigration service – also use Zivver. So do vital infrastructure operators such as the Port of Rotterdam and The Hague Airport.&lt;/p&gt;&lt;p&gt;In the U.K., a number of NHS hospitals and local councils use the company. In Belgium and Germany it is used in major hospitals.&lt;/p&gt;&lt;p&gt;The information that Zivver secures for its customers is therefore confidential and sensitive by nature.&lt;/p&gt;&lt;p&gt;When approached by Follow the Money, a number of governmental agencies said the company’s Dutch origins were a big factor in their decision to use Zivver.&lt;/p&gt;&lt;p&gt;Additionally, the fact that the data transferred via Zivver was stored on servers in Europe also played a role in their decisions.&lt;/p&gt;&lt;p&gt;Now that Zivver has been acquired by a company in the United States, that data is subject to far-reaching American laws. This means that the U.S. government can request access to this information if it wishes, regardless of where the data is stored.&lt;/p&gt;&lt;head rend="h2"&gt;The Trump effect&lt;/head&gt;&lt;p&gt;These laws are not new, but they have become even more draconian since U.S. President Donald Trump's return to office, according to experts.&lt;/p&gt;&lt;p&gt;Bert Hubert, a former regulator of the Dutch intelligence services, warned: “America is deteriorating so rapidly, both legally and democratically, that it would be very naive to hand over your courts and hospitals to their services.”&lt;/p&gt;&lt;p&gt;“Trump recently called on Big Tech to ignore European legislation. And that is what they are going to do. We have no control over it,” he added.&lt;/p&gt;&lt;p&gt;In Europe, Hubert said: “We communicate almost exclusively via American platforms. And that means that the U.S. can read our communications and disrupt our entire society if they decide that they no longer like us.”&lt;/p&gt;&lt;p&gt;Zivver had offered an alternative – a European platform governed by EU law.&lt;/p&gt;&lt;p&gt;“We are now throwing that away. If you want to share something confidential with a court or government, consider using a typewriter. That's about all we have left,” Hubert said.&lt;/p&gt;&lt;head rend="h2"&gt;Israeli ties&lt;/head&gt;&lt;p&gt;Beyond American jurisdiction, Kiteworks’ management raises another layer of concern: its links to Israeli intelligence.&lt;/p&gt;&lt;p&gt;Several of the company’s top executives, including CEO Yaron, are veterans of Unit 8200, the elite cyber unit of the Israel Defence Force (IDF). The unit is renowned for its code-breaking abilities and feared for its surveillance operations.&lt;/p&gt;&lt;quote&gt;In Israel, there is a revolving door between the army, lobby, business and politics&lt;/quote&gt;&lt;p&gt;Unit 8200 has been linked to major cyber operations, including the Stuxnet attack on Iranian nuclear facilities in 2007. More recently, it was accused of orchestrating the detonation of thousands of pagers in Lebanon, an incident the United Nations said violated international law and killed at least two children.&lt;/p&gt;&lt;p&gt;The unit employs thousands of young recruits identified for their digital skills. It is able to intercept global telephone and internet traffic.&lt;/p&gt;&lt;p&gt;International media have reported that Unit 8200 intercepts and stores an average of one million Palestinian phone calls every hour, data that ends up on Microsoft servers in Europe.&lt;/p&gt;&lt;p&gt;Some veterans themselves have also objected to the work of the unit. In 2014, dozens of reservists signed a letter to Israeli leaders saying they no longer wanted to participate in surveillance of the occupied territories.&lt;/p&gt;&lt;p&gt;“The lines of communication between the Israeli defence apparatus and the business community have traditionally been very short,” said Dutch intelligence expert Vijver. “In Israel, there is a revolving door between the army, lobby, business and politics.”&lt;/p&gt;&lt;head rend="h2"&gt;Veterans at the helm&lt;/head&gt;&lt;p&gt;That revolving door is clearly visible in big U.S. tech companies – and Kiteworks is no exception.&lt;/p&gt;&lt;p&gt;Aside from Yaron, both Chief Business Officer Yaron Galant and Chief Product Officer Amit Toren served in Unit 8200, according to publicly available information.&lt;/p&gt;&lt;p&gt;They played a direct role in negotiating the acquisition of Zivver. Their background was known to Zivver’s directors Goud and Klinkhamer at the time.&lt;/p&gt;&lt;p&gt;Other senior figures also have military intelligence backgrounds. Product director Ron Margalit worked in Unit 8200 before serving in the office of Israeli Prime Minister Benjamin Netanyahu. Mergers and acquisitions director Uri Kedem is a former Israeli naval captain.&lt;/p&gt;&lt;p&gt;Kiteworks is not unique in this respect.&lt;/p&gt;&lt;p&gt;Increasing numbers of U.S. cybersecurity firms now employ former Israeli intelligence officers. This trend, experts say, creates vulnerabilities that are rarely discussed.&lt;/p&gt;&lt;p&gt;An independent researcher quoted by U.S. Drop Site News said: “Not all of these veterans will send classified data to Tel Aviv. But the fact that so many former spies work for these companies does create a serious vulnerability: no other country has such access to the American tech sector.”&lt;/p&gt;&lt;p&gt;Or, as the ex-intelligence regulator Hubert put it: “Gaining access to communication flows is part of Israel’s long-term strategy. A company like Zivver fits perfectly into that strategy.”&lt;/p&gt;&lt;p&gt;The information handled by Zivver – confidential communications between governments, hospitals and citizens – is a potential goldmine for intelligence services.&lt;/p&gt;&lt;p&gt;According to intelligence expert Vijver, access to this kind of material makes it easier to pressure individuals into cooperating with intelligence agencies. Once an intelligence service has access to medical, financial and personal data, it can more easily pressure people into spying for it, he said.&lt;/p&gt;&lt;p&gt;But the gain for intelligence services lies not just in sensitive information, said Hubert: “Any data that allows an agency to tie telephone numbers, addresses or payment data to an individual is of great interest to them.”&lt;/p&gt;&lt;p&gt;He added: “It is exactly this type of data that is abundantly present in communications between civilians, governments and care institutions. In other words, the information that flows through a company like Zivver is extremely valuable for intelligence services.”&lt;/p&gt;&lt;head rend="h2"&gt;A flawed security model?&lt;/head&gt;&lt;p&gt;These geopolitical concerns become more pronounced when combined with technical worries about Zivver’s encryption.&lt;/p&gt;&lt;p&gt;For years, Zivver presented itself as a European alternative that guaranteed privacy. Its marketing materials claimed that messages were encrypted on the sender’s device and that the company had “zero access” to content.&lt;/p&gt;&lt;p&gt;But an investigation by two cybersecurity experts at a Dutch government agency, at the request of Follow the Money, undermines this claim.&lt;/p&gt;&lt;p&gt;The experts, who participated in the investigation on condition of anonymity, explored what happened when that government agency logged into Zivver’s web application to send information.&lt;/p&gt;&lt;p&gt;Tests showed that when government users sent messages through Zivver’s web application, the content – including attachments – was uploaded to Zivver’s servers as readable text before being encrypted. The same process applied to email addresses of senders and recipients.&lt;/p&gt;&lt;p&gt;“In these specific cases, Zivver processed the messages in readable form,” said independent cybersecurity researcher Matthijs Koot, who verified the findings.&lt;/p&gt;&lt;p&gt;“Even if only briefly, technically speaking it is possible that Zivver was able to view these messages,” he said.&lt;/p&gt;&lt;p&gt;He added: “Whether a message is encrypted at a later stage makes little difference. It may help against hackers, but it no longer matters in terms of protection against Zivver.”&lt;/p&gt;&lt;p&gt;Despite these findings, Zivver continues to insist on its website and in promotional material elsewhere – including on the U.K. government’s Digital Marketplace – that “contents of secure messages are inaccessible to Zivver and third parties”.&lt;/p&gt;&lt;p&gt;So far, no evidence has surfaced that Zivver misused its technical access. But now that the company is owned by Kiteworks, experts see a heightened risk.&lt;/p&gt;&lt;p&gt;Former intelligence officer Vijver puts it bluntly: “Given the links between Zivver, Kiteworks and Unit 8200, I believe there is zero chance that no data is going to Israel. To think otherwise is completely naive.”&lt;/p&gt;&lt;head rend="h2"&gt;A missed safeguard&lt;/head&gt;&lt;p&gt;The sale of Zivver could technically have been blocked or investigated under Dutch law. According to the Security Assessment of Investments, Mergers and Acquisitions Act, such sensitive takeovers are supposed to be reviewed by a specialised agency.&lt;/p&gt;&lt;p&gt;But the Dutch interior ministry declared that Zivver was not part of the country’s “critical infrastructure,” meaning that no review was carried out.&lt;/p&gt;&lt;p&gt;That, in Hubert’s view, was “a huge blunder”.&lt;/p&gt;&lt;p&gt;“It’s bad enough that a company that plays such an important role in government communications is falling into American hands, but the fact that there are all kinds of Israeli spies there is very serious,” he said.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;“The takeover is taking place in an unsafe world full of geopolitical tensions”&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Experts say the Zivver case highlights Europe’s lack of strategic control over its digital infrastructure.&lt;/p&gt;&lt;p&gt;Mariëtte van Huijstee of the Netherlands-based Rathenau Institute said: “I doubt whether the security of sensitive emails and files … should be left to the private sector. And if you think that is acceptable, should we leave it to non-European parties over whom we have no control?”&lt;/p&gt;&lt;p&gt;“We need to think much more strategically about our digital infrastructure and regulate these kinds of issues much better, for example by designating encryption services as vital infrastructure,” she added.&lt;/p&gt;&lt;p&gt;Zivver, for its part, claimed that security will improve under Kiteworks. Zivver’s full responses to Follow the Money’s questions can be read here and here.&lt;/p&gt;&lt;p&gt;But Van Huijstee was not convinced.&lt;/p&gt;&lt;p&gt;“Kiteworks employs people who come from a service that specialises in decrypting files,” she said.&lt;/p&gt;&lt;p&gt;“The takeover is taking place in an unsafe world full of geopolitical tensions, and we are dealing with data that is very valuable. In such a case, trust is not enough and more control is needed.”&lt;/p&gt;&lt;head rend="h3"&gt;Related articles&lt;/head&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;p&gt;Account required&lt;/p&gt;&lt;head rend="h3"&gt;Collection&lt;/head&gt;&lt;head rend="h3"&gt;Authors&lt;/head&gt;&lt;head rend="h3"&gt;Sebastiaan Brommersma&lt;/head&gt;&lt;p&gt;Former IP lawyer. Currently writing about digital technology, from big tech and big data to AI and cybercrime.&lt;/p&gt;&lt;head rend="h3"&gt;Siem Eikelenboom&lt;/head&gt;&lt;p&gt;Experienced investigative journalist. Was part of the international team that investigated the Panama Papers.&lt;/p&gt;Send a news tip&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262524</guid><pubDate>Sun, 14 Dec 2025 12:15:09 +0000</pubDate></item><item><title>The Gorman Paradox: Where Are All the AI-Generated Apps?</title><link>https://codemanship.wordpress.com/2025/12/14/the-gorman-paradox-where-are-all-the-ai-generated-apps/</link><description>&lt;doc fingerprint="7b1708caec5e440f"&gt;
  &lt;main&gt;
    &lt;p&gt;In 1950, while discussing the recent wave of flying saucer reports over lunch with colleagues at Los Alamos National Laboratory in New Mexico, physicist Enrico Fermi asked a simple question.&lt;/p&gt;
    &lt;p&gt;There are hundreds of billions of stars in our Milky Way galaxy, and – presumed at the time – a significant percentage have Earth-like habitable planets orbiting them. The galaxy is billions of years old, and the odds are high that there should be other technological civilisations out there. But we see no convincing sign of them.&lt;/p&gt;
    &lt;p&gt;So, where is everybody?&lt;/p&gt;
    &lt;p&gt;This question is now know as the Fermi Paradox.&lt;/p&gt;
    &lt;p&gt;In the last couple of years, I’ve been seeing another paradox. Many people claim that working software can now be produced for pennies on the pound, in a fraction of the time that it takes humans. Some go so far as to claim that we’re in the age of commoditised software, throwaway software, and hail the end of the software industry as we know it.&lt;/p&gt;
    &lt;p&gt;Why buy a CRM solution or a ERM system when “AI” can generate one for you in hours or even minutes? Why sign up for a SaaS platform when Cursor can spit one out just as good in the blink of an eye?&lt;/p&gt;
    &lt;p&gt;But when we look beyond the noise – beyond these sensational flying saucer reports – we see nothing of the sort. No AI-generated Spotify or Salesforce or SAP. No LLM-generated games bothering the charts. No noticeable uptick in new products being added to the app stores.&lt;/p&gt;
    &lt;p&gt;So, where is everybody?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262545</guid><pubDate>Sun, 14 Dec 2025 12:18:59 +0000</pubDate></item><item><title>Baumol's Cost Disease</title><link>https://en.wikipedia.org/wiki/Baumol_effect</link><description>&lt;doc fingerprint="ef48805ab3ed0642"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Baumol effect&lt;/head&gt;&lt;p&gt;In economics, the Baumol effect, also known as Baumol's cost disease, first described by William J. Baumol and William G. Bowen in the 1960s, is the tendency for wages in jobs that have experienced little or no increase in labor productivity to rise in response to rising wages in other jobs that did experience high productivity growth.[1][2] In turn, these sectors of the economy become more expensive over time, because the input costs increase while productivity does not. Typically, this affects services more than manufactured goods, and in particular health, education, arts and culture.[3]&lt;/p&gt;&lt;p&gt;This effect is an example of cross elasticity of demand. The rise of wages in jobs without productivity gains results from the need to compete for workers with jobs that have experienced productivity gains and so can naturally pay higher wages. For instance, if the retail sector pays its managers low wages, those managers may decide to quit and get jobs in the automobile sector, where wages are higher because of higher labor productivity. Thus, retail managers' salaries increase not due to labor productivity increases in the retail sector, but due to productivity and corresponding wage increases in other industries.&lt;/p&gt;&lt;p&gt;The Baumol effect explains a number of important economic developments:[3]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The share of total employment in sectors with high productivity growth decreases, while that of low productivity sectors increases.[4]&lt;/item&gt;&lt;item&gt;Economic growth slows down, due to the smaller proportion of high growth sectors in the whole economy.[4]&lt;/item&gt;&lt;item&gt;Government spending is disproportionately affected by the Baumol effect, because of its focus on services like health, education and law enforcement.[3][5]&lt;/item&gt;&lt;item&gt;Increasing costs in labor-intensive service industries, or below average cost decreases, are not necessarily a result of inefficiency.[3]&lt;/item&gt;&lt;item&gt;Due to income inequality, services whose prices rise faster than incomes can become unaffordable to many workers. This happens despite overall economic growth, and has been exacerbated by the rise in inequality in recent decades.[4]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Baumol referred to the difference in productivity growth between economic sectors as unbalanced growth. Sectors can be differentiated by productivity growth as progressive or non-progressive. The resulting transition to a post-industrial society, i.e. an economy where most workers are employed in the tertiary sector, is called tertiarization.&lt;/p&gt;&lt;head rend="h2"&gt;Description&lt;/head&gt;[edit]&lt;p&gt;Increases in labor productivity tend to result in higher wages.[6][7] Productivity growth is not uniform across the economy, however. Some sectors experience high productivity growth, while others experience little or negative productivity growth.[8] Yet wages have tended to rise not only in sectors with high productivity growth, but also in those with little to no productivity growth.&lt;/p&gt;&lt;p&gt;The American economists William J. Baumol and William G. Bowen proposed that wages in sectors with stagnant productivity rise out of the need to compete for workers with sectors that experience higher productivity growth, which can afford to raise wages without raising prices. With higher labor costs, but little increase in productivity, sectors with low productivity growth see their costs of production rise. As summarized by Baumol in a 1967 paper:[9]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;If productivity per man hour rises cumulatively in one sector relative to its rate of growth elsewhere in the economy, while wages rise commensurately in all areas, then relative costs in the nonprogressive sectors must inevitably rise, and these costs will rise cumulatively and without limit...Thus, the very progress of the technologically progressive sectors inevitably adds to the costs of the technologically unchanging sectors of the economy, unless somehow the labor markets in these areas can be sealed off and wages held absolutely constant, a most unlikely possibility.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h2"&gt;Origins&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Jean Fourastié: unbalanced growth in economic sectors&lt;/head&gt;[edit]&lt;p&gt;Studying various price series over time, Jean Fourastié noticed the unequal technological progress in different industries.[10]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;But what is essential is that very large sectors of economic activity have remained practically unaffected by technological progress. For example, the men's barber does not cut more clients' hair in 1948 than in 1900; entire professions have not changed their working methods from 1900 to 1930. ... (1949: 27).&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;He predicted that this would lead to a gradual increase in the share of services in the economy, and the resulting post-industrial society:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;... the absolute volume of secondary production continues to grow; but from a certain state of economic development, the value of these growing productions diminishes in relation to the total volume of national production. Thus, tertiary values invade economic life; that is why it can be said that the civilization of technical progress will be a tertiary civilization. (1949: 59)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In a 2003 article, Baumol noted: "For the origins of the analysis, see Fourastié (1963)."[11][12]&lt;/p&gt;&lt;head rend="h3"&gt;Baumol and Bowen: rising wages despite productivity stagnation&lt;/head&gt;[edit]&lt;p&gt;The original study on the Baumol effect was conducted for the performing arts sector.[1] American economists Baumol and Bowen in 1965 said "the output per man-hour of the violinist playing a Schubert quartet in a standard concert hall is relatively fixed." In other words, they said the productivity of classical music performance had not increased. However, the real wages of musicians had increased substantially since the 19th century. Gambling and Andrews pointed out in 1984 that productivity does go up with the size of the performance halls.[13] Furthermore Greenfield pointed out in 1995 that far more people hear the performance due to advances in amplification, recording and broadcasting, so productivity has increased many-fold.[14][15]&lt;/p&gt;&lt;head rend="h2"&gt;Effects&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Price and output&lt;/head&gt;[edit]&lt;p&gt;Firms may respond to increases in labor costs induced by the Baumol effect in a variety of ways, including:[16]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Cost and price disease: Prices in stagnant industries tend to grow faster than average&lt;/item&gt;&lt;item&gt;Stagnant output: Real output in low-productivity-growth industries tends to grow more slowly relative to the overall economy&lt;/item&gt;&lt;item&gt;Employment effects: Firms in stagnant industries may reduce employment, decrease hours, or increase non-monetary compensation&lt;/item&gt;&lt;/list&gt;&lt;p&gt;An important implication of Baumol effect is that it should be expected that, in a world with technological progress, the costs of manufactured goods will tend to fall (as productivity in manufacturing continually increases) while the costs of labor-intensive services like education, legal services, and health care (where productivity growth is persistently slow) will tend to rise (see chart).[a][19]&lt;/p&gt;&lt;p&gt;A 2008 study by American economist William Nordhaus showed as much, concluding that "Baumol-type diseases" in technologically stagnant sectors have led to "rising relative prices and declining relative real outputs."[16] In the realm of prices, Nordhaus showed that in the United States from 1948–2001 "productivity trends are associated almost percentage-point for percentage-point with price decline." Industries with low productivity growth thus saw their relative prices increase, leading Nordhaus to conclude: "The hypothesis of a cost-price disease due to slow productivity growth is strongly supported by the historical data. Industries with relatively lower productivity growth show a percentage-point for percentage-point higher growth in relative prices." A similar conclusion held for real output: "The real output/stagnation hypothesis is strongly confirmed. Technologically stagnant industries have shown slower growth in real output than have the technologically dynamic ones. A one percentage-point higher productivity growth was associated with a three-quarters percentage-point higher real output growth."&lt;/p&gt;&lt;head rend="h3"&gt;Affordability and inequality&lt;/head&gt;[edit]&lt;p&gt;While the Baumol effect suggests that costs in low-productivity-growth industries will continually rise, Baumol argues the "stagnant-sector services will never become unaffordable to society. This is because the economy's constantly growing productivity simultaneously increases the population's overall purchasing power."[20] To see this, consider an economy with a real national income of $100 billion with healthcare spending amounting to $20 billion (20% of national income), leaving $80 billion for other purchases. Say that, over 50 years, due to productivity growth real national income doubles to $200 billion (an annual growth rate of about 1.4%). In this case, even if healthcare spending were to rise by 500% to $120 billion, there would still be $80 billion left over for other purchases—exactly the same amount as 50 years prior. In this scenario, healthcare now accounts for 60% of national income, compared to 20% fifty years prior, and yet the amount of income left to purchase other goods remains unchanged. Further, if healthcare costs were to account for anything less than 60% of national income, there would be more income left over for other purchases (for instance, if healthcare costs were to rise from 20% of national income to 40% of national income, there would be $120 billion left over for other purchases—40% more than 50 years prior). So it can be seen that even if productivity growth were to lead to substantial healthcare cost increases as a result of Baumol's cost disease, the wealth increase brought on by that productivity growth would still leave society able to purchase more goods than before.&lt;/p&gt;&lt;p&gt;While this is true for society in the aggregate, it is not the case for all workers as individuals. Baumol noted that the increase in costs "disproportionally affects the poor."[4] Although a person's income may increase over time, and the affordability of manufactured goods may increase too, the price increases in industries subject to the Baumol effect can be larger than the increase in many workers' wages (see chart above, note average wages). These services become less affordable, especially to low income earners, despite the overall economic growth. This effect is exacerbated by the increase in income inequality observed in recent decades.[4]&lt;/p&gt;&lt;head rend="h3"&gt;Government spending&lt;/head&gt;[edit]&lt;p&gt;The Baumol effect has major implications for government spending. Since most government spending goes towards services that are subject to the cost disease—law enforcement, education, healthcare etc.—the cost to the government of providing these services will rise as time goes on.[5][21]&lt;/p&gt;&lt;head rend="h3"&gt;Labor force distribution&lt;/head&gt;[edit]&lt;p&gt;One implication of the Baumol effect is a shift in the distribution of the labor force from high-productivity industries to low-productivity industries.[9] In other words, the effect predicts that the share of the workforce employed in low-productivity industries will rise over time.&lt;/p&gt;&lt;p&gt;The reasoning behind this can be seen through a thought experiment offered by Baumol in his book The Cost Disease:[22]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Let us assume for simplicity that the share of the economy's total output that comes from the progressive sector [industries with high productivity growth], as measured in physical units rather than money, does not change. Because the economy has only two sectors, progressive and stagnant [industries with low productivity growth], whose production together accounts for all of its output, it follows that the stagnant sector also must maintain a constant share of the total.&lt;/p&gt;&lt;p&gt;This has significant implications for the distribution of the economy's labor force. By definition, labor productivity grows significantly faster in the progressive sector than in the stagnant sector, so to keep a constant proportion between the two sectors' output, more and more labor has to move from the progressive sector into the stagnant sector.[b]&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;As predicted by the Baumol effect, the proportion of the United States labor force employed in stagnant industries has grown substantially since the 1960s. In particular, the United States has morphed from a manufacturing economy into a service economy (see chart).[23] However, how much of this is due to the Baumol effect rather than other causes is disputed.[24][25] In a 2010 study, the economist Talan B. İşcan devised a model from which he concluded that both Baumol and Engel effects played significant roles in the rising share of employment in services in the United States (though he noted that "considerable gaps between the calibrated model and the actual data remain").[26] An older 1968 study by economist Victor Fuchs likewise concluded that the Baumol effect played a major role in the shift to services, although he determined that demand shifts like those proposed in Engel's law played only a minor role.[27] The economists Robert Rowthorn and Ramana Ramaswamy also concluded that relatively faster growth of productivity in manufacturing played a role in the shift to services.[28] The economist Tom Elfring, however, argued in a 1989 paper that the Baumol effect has played a secondary role to growth in demand for services since the 1970s.[29] Alternative theories for the shift to services include demand-side theories (the Baumol effect is broadly a supply-side explanation) like the three-sector model devised by Allan Fisher[30] and Colin Clark[31] in the 1930s, which posit that services satisfy higher needs than goods and so as income grows a higher share of income will be used for the purchase of services;[25] changes in the inter-industry division of labor, favoring specialized service activities;[25] outsourcing to countries with lower labor costs;[32] increasing participation of women in the labor force;[33] and trade specialization.[34]&lt;/p&gt;&lt;p&gt;The Baumol effect has also been used to describe the reallocation of labor out of agriculture (in the United States, in 1930 21.5% of the workforce was employed in agriculture and agriculture made up 7.7% of GDP; by 2000, only 1.9% of the workforce was employed in agriculture and agriculture made up only 0.7% of GDP[35]).[36] In a 2009 study, the economists Benjamin N. Dennis and Talan B. İşcan concluded that after the 1950s relatively faster productivity growth in agriculture was the key driver behind the continuing shift in employment from agriculture to non-farm goods (prior to the 1950s, they determined that Engel's law explained almost all labor reallocation out of agriculture).[37]&lt;/p&gt;&lt;head rend="h3"&gt;Economic growth and aggregate productivity&lt;/head&gt;[edit]&lt;p&gt;In his original paper on the cost disease, Baumol argued that in the long run the cost disease implies a reduction in aggregate productivity growth and correspondingly a reduction in economic growth.[9] This follows straightforwardly from the labor distribution effects of the cost disease. As a larger and larger share of the workforce moves from high-productivity-growth industries to low-productivity-growth industries, it is natural to expect that the overall rate of productivity growth will slow. Since economic growth is driven in large part by productivity growth, economic growth would also slow.&lt;/p&gt;&lt;p&gt;The economist Nicholas Oulton, however, argued in a 2001 paper that Baumol effect may counterintuitively result in an increase in aggregate productivity growth.[38] This could occur if many services produce intermediate inputs for the manufacturing sector, i.e. if a significant number of services are business services.[c] In this case, even though the slow-growth service sector is increasing in size, because these services further boost the productivity growth of the shrinking manufacturing sector overall productivity growth may actually increase. Relatedly, the economist Maurizio Pugno described how many stagnant services, like education and healthcare, contribute to human capital formation, which enhances growth and thus "oppos[es] the negative Baumol effect on growth."[39]&lt;/p&gt;&lt;p&gt;The economist Hiroaki Sasaki, however, disputed Oulton's argument in a 2007 paper.[40] Sasaki constructed an economic model that takes into account the use of services as intermediate inputs in high-productivity-growth industries and still concluded that a shift in labor force distribution from higher-productivity-growth manufacturing to lower-productivity-growth services decreases the rate of economic growth in the long run. Likewise, the economists Jochen Hartwig and Hagen Krämer concluded in a 2019 paper that, while Outlon's theory is "logically consistent", it is "not in line with the data", which shows a lowering of aggregate productivity growth.[41]&lt;/p&gt;&lt;head rend="h2"&gt;Sectors&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Education&lt;/head&gt;[edit]&lt;p&gt;The Baumol effect has been applied to the education sector,[42][43][44] including by Baumol himself.[45][46] By most measures, productivity growth in the education sector over the last several decades has been low or even negative;[47][48] the average student-teacher ratio in American universities, for instance, was sixteen to one in 2011, just as it was in 1981.[44] Yet, over this period, tuition costs have risen substantially.[49] It has been proposed that this is at least partially explained by the Baumol effect: even though there has been little or even negative productivity growth in the education sector, because of productivity increases across other sectors of the economy universities today would not be able to attract professors with 1980s-level salaries, so they are forced to raise wages to maintain their workforce. To afford the increased labor costs, universities raise tuition fees (i.e. they increase prices).[50]&lt;/p&gt;&lt;p&gt;Evidence on the role of the Baumol effect in rising education costs has been mixed. Economists Robert B. Archibald and David H. Feldman, both of the College of William &amp;amp; Mary, argued in a 2006 study, for instance, that the Baumol effect is the dominant driver behind increasing higher education costs.[51] Other studies, however, have found a lesser role for the Baumol effect. In a 2014 study, the economists Robert E. Martin and Carter Hill devised a model that determined that the Baumol effect explained only 23%–32% of the rise in higher education costs.[52] The economists Gary Rhoades and Joanna Frye went further in a 2015 study and argued that the Baumol effect could not explain rising tuition costs at all, as "relative academic labor costs have gone down as tuition has gone up."[53] The cost disease may also have only limited effects on primary and secondary education: a 2016 study on per-pupil public education spending by Manabu Nose, an economist at the International Monetary Fund, found that "the contribution of Baumol's effect was much smaller than implied by theory"; Nose argued that it was instead rising wage premiums paid for teachers in excess of market wages that were the dominant reason for increasing costs, particularly in developing countries.[54]&lt;/p&gt;&lt;p&gt;Alternative explanations for rising higher education costs include Bowen's revenue theory of cost,[52][55] reduced public subsidies for education,[56][57][58] administrative bloat,[56][59] the commercialization of higher education,[60] increased demand for higher education,[61] the easy availability of federal student loans,[62][63] difficulty comparing prices of different universities,[64] technological change,[43] and lack of a central mechanism to control price increases.[57]&lt;/p&gt;&lt;head rend="h3"&gt;Healthcare&lt;/head&gt;[edit]&lt;p&gt;The Baumol effect has been applied to the rising cost of healthcare,[46] as the healthcare industry has long had low productivity growth.[65][66] Empirical studies have largely confirmed the large role of the Baumol effect in the rising cost of healthcare in the United States,[67][68][69][70][71] although there is some disagreement.[72] Likewise, a 2021 study determined that "Baumol's cost disease ha[s] a significant positive impact on health expenditure growth" in China.[73] However, a paper by economists Bradley Rossen and Akhter Faroque on healthcare costs in Canada found that "the cost disease ... is a relatively minor contributor [in the growth of health-care spending in Canada], while technical progress in health care and growth in per capita incomes are by far the biggest contributors."[74]&lt;/p&gt;&lt;p&gt;Despite substantial technological innovation and capital investment, the healthcare industry has struggled to significantly increase productivity. As summarized by the economists Alberto Marino, David Morgan, Luca Lorenzoni, and Chris James:[75]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Technological advancements, capital investments and economies of scale do not make for a cumulative rise in output that is on par with progressive sectors of the economy ... [A]utomation and better technology generally do not allow for large productivity increases. A health professional is difficult to substitute, in particular by using new technologies, which may actually also bring an increase in volume (e.g. faster diagnostic tests). Increases in volume likely brought about by new technology will also drive up expenditure, since new health professionals will have to be hired to treat everyone. Moreover, new technologies require more specialised training for say [sic] doctors, driving wages up further since more years of experience are required.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Service industry&lt;/head&gt;[edit]&lt;p&gt;Baumol's cost disease is often used to describe consequences of the lack of growth in productivity in the quaternary sector of the economy and public services, such as public hospitals and state colleges.[42] Labor-intensive sectors that rely heavily on non-routine human interaction or activities, such as health care, education, or the performing arts, have had less growth in productivity over time. As with the string quartet example, it takes nurses the same amount of time to change a bandage or college professors the same amount of time to mark an essay today as it did in 1966.[76] In contrast, goods-producing industries, such as the car manufacturing sector and other activities that involve routine tasks, workers are continually becoming more productive by technological innovations to their tools and equipment.&lt;/p&gt;&lt;p&gt;The reported productivity gains of the service industry in the late 1990s are largely attributable to total factor productivity.[77] Providers decreased the cost of ancillary labor through outsourcing or technology. Examples include offshoring data entry and bookkeeping for health care providers and replacing manually-marked essays in educational assessment with multiple choice tests that can be automatically marked.&lt;/p&gt;&lt;head rend="h2"&gt;Technical description&lt;/head&gt;[edit]&lt;p&gt;In the 1967 paper Macroeconomics of Unbalanced Growth: The Anatomy of Urban Crisis, Baumol introduced a simple two-sector model to demonstrate the cost disease.[9] To do so, he imagined an economy consisting of only two sectors: sector one, which has constant productivity (that is, the number of goods workers can produce per man hour does not change as time goes on), and sector two, which sees productivity grow at a constant compounded rate (that is, the number of goods workers can produce per man hour grows at a rate , where is time). To simplify, he assumed that the quantity of goods produced by these two sectors (the "output" of each of the two sectors) is directly proportional to the quantity of labor employed (that is, doubling the number of workers doubles the output, tripling the number of workers triples the output, and so on) and that output depends only upon labor productivity and the quantity of labor. Since there is no increase in labor productivity in sector one, the output of sector one at time (denoted ) is:&lt;/p&gt;&lt;p&gt;where is the quantity of labor employed in sector one and is a constant that can be thought of as the amount of output each worker can produce at time . This equation simply says that the amount of output sector one produces equals the number of workers in sector one multiplied by the number of goods each worker can produce. Since productivity does not increase, the number of goods each worker produces remains and output remains constant through time for a given number of workers.&lt;/p&gt;&lt;p&gt;Since the labor productivity of sector two increases at a constant compounded rate , the output of sector two at time (denoted ) is:&lt;/p&gt;&lt;p&gt;where is the quantity of labor employed in sector two and is a constant that can be thought of as the amount of output each worker can produce at time . Since productivity grows at a constant compounded rate , the number of goods each worker produces at time equals , and the output of sector two grows at a rate proportional to productivity growth.&lt;/p&gt;&lt;p&gt;To more clearly demonstrate how wages and costs change through time, wages in both sectors are originally set at the same value . It is then assumed that wages rise in direct proportion to productivity (i.e., a doubling of productivity results in a doubling of wages, a tripling of productivity results in a tripling of wages, and so on). This means that the wages of the two sectors at time determined solely by productivity are:&lt;/p&gt;&lt;p&gt;(since productivity remains unchanged), and&lt;/p&gt;&lt;p&gt;(since productivity increases at a rate ).&lt;/p&gt;&lt;p&gt;These values, however, assume that workers do not move between the two sectors. If workers are equally capable of working in either sector, and they choose which sector to work in based upon which offers a higher wage, then they will always choose to work in the sector that offers the higher wage. This means that if sector one were to keep wages fixed at , then as wages in sector two grow with productivity workers in sector one would quit and seek jobs in sector two. Firms in sector one are thus forced to raise wages to attract workers. More precisely, in this model the only way firms in either sector can attract workers is to offer the same wage as firms in the other sector—if one sector were to offer lower wages, then all workers would work in the other sector.&lt;/p&gt;&lt;p&gt;So to maintain their workforces, wages in the two sectors must equal each other: . And since it is sector two that sees its wage naturally rise with productivity, while sector one's does not naturally rise, it must be the case that:&lt;/p&gt;&lt;p&gt;.&lt;/p&gt;&lt;p&gt;This typifies the labor aspect of the Baumol effect: as productivity growth in one sector of the economy drives up that sector's wages, firms in sectors without productivity growth must also raise wages to compete for workers.[d]&lt;/p&gt;&lt;p&gt;From this simple model, the consequences on the costs per unit output in the two sectors can be derived. Since the only factor of production within this model is labor, each sector's total cost is the wage paid to workers multiplied by the total number of workers. The cost per unit output is the total cost divided by the amount of output, so with representing the unit cost of goods in sector one at time and representing the unit cost of goods in sector two at time :&lt;/p&gt;&lt;p&gt;Plugging in the values for and from above:&lt;/p&gt;&lt;p&gt;It can be seen that in the sector with growing labor productivity (sector two), the cost per unit output is constant since both wages and output rise at the same rate. However, in the sector with stagnant labor productivity (sector one), the cost per unit output rises exponentially since wages rise exponentially faster than output.&lt;/p&gt;&lt;p&gt;This demonstrates the cost aspect of the Baumol effect (the "cost disease"). While costs in sectors with productivity growth do not increase, in sectors with little to no productivity growth costs necessarily rise due to the rising prevailing wage. Furthermore, if the productivity growth differential persists (that is, the low-productivity-growth sectors continue to see low productivity growth into the future while high-productivity-growth sectors continue to see high productivity growth), then costs in low-productivity-growth sectors will rise cumulatively and without limit.&lt;/p&gt;&lt;p&gt;Baumol's model can also be used to demonstrate the effect on the distribution of labor. Assume that, despite the change in the relative costs and prices of the two industries, the magnitude of the relative outputs of the two sectors are maintained. A situation similar to this could occur, for instance, "with the aid of government subsidy, or if demand for the product in question were sufficiently price inelastic or income elastic." The output ratio and its relation to the labor ratio, ignoring constants and , is then given by:&lt;/p&gt;&lt;p&gt;Letting (i.e. is the total labor supply), it follows that:&lt;/p&gt;&lt;p&gt;It can be seen that as approaches infinity, the quantity of labor in the non-progressive sector approaches the total labor supply while the quantity of labor in the progressive sector approaches zero. Hence, "if the ratio of the outputs of the two sectors is held constant, more and more of the total labor force must be transferred to the non-progressive sector and the amount of labor in the other sector will tend to approach zero."&lt;/p&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;head rend="h2"&gt;Notes&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Note that low productivity growth does not afflict all services. Telecommunications, for example, has seen substantial productivity growth.[17] Service industries are simply more likely than manufactured goods industries to be immune to productivity growth, for the reasons that they are often less able to be standardized and that the quality of services is more likely to be closely linked to the amount of labor provided.[18]&lt;/item&gt;&lt;item&gt;^ A technical description of this effect can be found in Technical description&lt;/item&gt;&lt;item&gt;^ In the two-sector model Baumol devised in his original paper (see Technical description), services are produced only for final consumption.&lt;/item&gt;&lt;item&gt;^ Note that this is an assumption of the model. As Baumol states in the original paper, "We suppose wages are equal in the two sectors and are fixed at dollars per unit of labor, where itself grows in accord with the productivity of sector 2, our 'progressive' sector." The preceding two paragraphs simply demonstrate the logic of that assumption.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b Baumol, W. J.; Bowen, W. G. (1965). "On the Performing Arts: The Anatomy of Their Economic Problems". The American Economic Review. 55 (1/2): 495–502. JSTOR 1816292.&lt;/item&gt;&lt;item&gt;^ Baumol, William J.; Bowen, William G. (1966). Performing Arts, The Economic Dilemma: A Study of Problems Common to Theater, Opera, Music, and Dance. Cambridge, Mass.: M.I.T. Press. ISBN 0262520117.&lt;/item&gt;&lt;item&gt;^ a b c d Lee, Timothy B. (May 4, 2017). "William Baumol, whose famous economic theory explains the modern world, has died". Vox. Archived from the original on January 31, 2022. Retrieved March 1, 2022.&lt;/item&gt;&lt;item&gt;^ a b c d e Hartwig, Jochen; Krämer, Hagen M. (December 2023). "Revisiting Baumol's Disease: Structural Change, Productivity Slowdown and Income Inequality". Intereconomics. 58 (6): 320–325. doi:10.2478/ie-2023-0066. hdl:10419/281398.&lt;/item&gt;&lt;item&gt;^ a b Baumol 2012, p. 27.&lt;/item&gt;&lt;item&gt;^ Anderson, Richard G. (2007). "How Well Do Wages Follow Productivity Growth?" (PDF). Federal Reserve Bank of St. Louis. Archived (PDF) from the original on December 3, 2021. Retrieved March 1, 2022.&lt;/item&gt;&lt;item&gt;^ Feldstein, Martin (July 2008). "Did wages reflect growth in productivity?". Journal of Policy Modeling. 30 (4): 591–594. doi:10.1016/j.jpolmod.2008.04.003.&lt;/item&gt;&lt;item&gt;^ Baily, Martin Neil; Bosworth, Barry; Doshi, Siddhi (January 2020). "Productivity comparisons: Lessons from Japan, the United States, and Germany" (PDF). Brookings Institution. p. 14. Archived (PDF) from the original on December 18, 2021. Retrieved March 1, 2022.&lt;/item&gt;&lt;item&gt;^ a b c d Baumol, William J. (June 1967). "Macroeconomics of Unbalanced Growth: The Anatomy of Urban Crisis" (PDF). The American Economic Review. 57 (3): 415–426. JSTOR 1812111. Archived (PDF) from the original on February 26, 2022.&lt;/item&gt;&lt;item&gt;^ Brown, Edmund A. (1951). "Review of Le Grand espoir du XXe Siècle: Progrès technique, Progrès Èconomique, Progrès Social.; La Civilisation de 1960., Jean Fourastié". Political Science Quarterly. 66 (4): 603–606. doi:10.2307/2145452. JSTOR 2145452.&lt;/item&gt;&lt;item&gt;^ Baumol, William J. (2003). "The Cost Disease of the Personal Services". The Encyclopedia of Public Choice. pp. 456–460. doi:10.1007/978-0-306-47828-4_70. ISBN 978-0-7923-8607-0. This is a reprint of Fourastié 1949.&lt;/item&gt;&lt;item&gt;^ Alcouffe, A.; Le Bris, D. (2020). "Technical Progress and Structural Change in Jean Fourastié's Theory of Development". History of Political Economy. 52 (1): 101–133.&lt;/item&gt;&lt;item&gt;^ Gambling, Trevor; Andrews, Gordon (1984). "Does Baumol's Disease Exist? Some Findings from the Royal Shakespeare Company". Journal of Cultural Economics. 8 (2): 73–92. ISSN 0885-2545 – via JSTOR.&lt;/item&gt;&lt;item&gt;^ Greenfield, Harry (March 1, 2005). "Letter: curing 'Baumol's Disease'". The Service Industries Journal. 25 (2): 289–290. doi:10.1080/0264206042000305466. ISSN 0264-2069 – via Semantic Scholar.&lt;/item&gt;&lt;item&gt;^ Akehurst, Gary. "What Do We Really Know About Services?". Service Business. 2 (1): 1–15. Archived from the original on September 7, 2024 – via Pennsylvania State University.&lt;/item&gt;&lt;item&gt;^ a b Nordhaus, William D (January 27, 2008). "Baumol's Diseases: A Macroeconomic Perspective" (PDF). The B.E. Journal of Macroeconomics. 8 (1). doi:10.2202/1935-1690.1382. S2CID 153319511.&lt;/item&gt;&lt;item&gt;^ Modica, Nathan F.; Chansky, Brian (May 2019). "Productivity trends in the wired and wireless telecommunications industries" (PDF). Bureau of Labor Statistics. Archived (PDF) from the original on January 14, 2022. Retrieved March 24, 2022.&lt;/item&gt;&lt;item&gt;^ Baumol 2012, pp. 22–24.&lt;/item&gt;&lt;item&gt;^ Lee, Timothy B. (May 4, 2017). "William Baumol, whose famous economic theory explains the modern world, has died". Vox. Archived from the original on January 31, 2022. Retrieved March 1, 2022.&lt;/item&gt;&lt;item&gt;^ Baumol 2012, p. xx.&lt;/item&gt;&lt;item&gt;^ Baumol, William J. (1993). "Health care, education and the cost disease: A looming crisis for public choice". The Next Twenty-five Years of Public Choice. pp. 17–28. doi:10.1007/978-94-017-3402-8_3. ISBN 978-94-017-3404-2.&lt;/item&gt;&lt;item&gt;^ Baumol 2012, p. 80.&lt;/item&gt;&lt;item&gt;^ Short, Doug (September 5, 2011). "Charting The Incredible Shift From Manufacturing To Services In America". Business Insider. Archived from the original on April 21, 2021. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ Urquhart, Michael (April 1984). "The employment shift to services: where did it come from?" (PDF). Monthly Labor Review. 107 (4): 15–22. Archived from the original (PDF) on January 30, 2022 – via Bureau of Labor Statistics. &lt;quote&gt;Suggested explanations for the faster growth of services employment include changes in the demand for goods and services as a result of rising incomes and relative price movements, slower productivity growth in services, the increasing participation of women in the labor force since World War II, and the growing importance of the public and nonprofit sector in general. But no consensus exists on the relative importance of the above factors in developing an adequate explanation of the sectoral shifts in employment.&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;^ a b c Schettkat, Ronald; Yocarini, Lara (2003). The Shift to Services: A Review of the Literature (Report). hdl:10419/20200. S2CID 154141056. SSRN 487282.&lt;/item&gt;&lt;item&gt;^ Iscan, Talan (January 30, 2010). "How Much Can Engel's Law and Baumol's Disease Explain the Rise of Service Employment in the United States?". The B.E. Journal of Macroeconomics. 10 (1). doi:10.2202/1935-1690.2001. S2CID 154824000.&lt;/item&gt;&lt;item&gt;^ Fuchs, Victor (1968). The Service Economy. National Bureau of Economic Research. ISBN 978-0870144769.&lt;/item&gt;&lt;item&gt;^ Rowthorn, Robert; Ramaswamy, Ramana (1999). "Growth, Trade, and Deindustrialization". IMF Staff Papers. 46 (1): 18–41. doi:10.2307/3867633. JSTOR 3867633.&lt;/item&gt;&lt;item&gt;^ Elfring, Tom (July 1989). "The Main Features and Underlying Causes of the Shift to Services". The Service Industries Journal. 9 (3): 337–356. doi:10.1080/02642068900000040.&lt;/item&gt;&lt;item&gt;^ Fisher, Allan G. B. (1935). The Clash of Progress and Security. Macmillan. ISBN 9780678001585. &lt;code&gt;{{cite book}}&lt;/code&gt;: ISBN / Date incompatibility (help)[page needed]&lt;/item&gt;&lt;item&gt;^ Clark, Colin (1940). The Conditions of Economic Progress. Macmillan. ISBN 9780598475732. &lt;code&gt;{{cite book}}&lt;/code&gt;: ISBN / Date incompatibility (help)[page needed]&lt;/item&gt;&lt;item&gt;^ Scharpf, F. W. (1990). "Structures of Postindustrial Society or Does Mass Unemployment Disappear in the Service and Information Economy". In Appelbaum, E. (ed.). Labor Market Adjustments to Structural Change and Technological Progress. New York: Praeger. pp. 17–36. ISBN 978-0-275-93376-0.&lt;/item&gt;&lt;item&gt;^ Urquhart, Michael (April 1984). "The employment shift to services: where did it come from?" (PDF). Monthly Labor Review. 107 (4): 15–22. Archived from the original (PDF) on January 30, 2022.&lt;/item&gt;&lt;item&gt;^ Rowthorn, R. E.; Wells, J.R. (1987). De-Industrialization and Foreign Trade. Cambridge University Press. ISBN 978-0521269476.&lt;/item&gt;&lt;item&gt;^ Dimitri, Carolyn; Effland, Anne; Conklin, Neilson (June 2005). "The 20th Century Transformation of U.S. Agriculture and Farm Policy" (PDF). United States Department of Agriculture. Archived from the original (PDF) on March 24, 2022. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ Baumol 2012, pp. 117–119.&lt;/item&gt;&lt;item&gt;^ Dennis, Benjamin N.; İşcan, Talan B. (April 2009). "Engel versus Baumol: Accounting for structural change using two centuries of U.S. data". Explorations in Economic History. 46 (2): 186–202. doi:10.1016/j.eeh.2008.11.003.&lt;/item&gt;&lt;item&gt;^ Oulton, N. (October 2001). "Must the growth rate decline? Baumol's unbalanced growth revisited". Oxford Economic Papers. 53 (4): 605–627. doi:10.1093/oep/53.4.605.&lt;/item&gt;&lt;item&gt;^ Pugno, Maurizio (January 2006). "The service paradox and endogenous economic growth" (PDF). Structural Change and Economic Dynamics. 17 (1): 99–115. doi:10.1016/j.strueco.2005.02.003. hdl:11572/43500.&lt;/item&gt;&lt;item&gt;^ Sasaki, Hiroaki (December 2007). "The rise of service employment and its impact on aggregate productivity growth". Structural Change and Economic Dynamics. 18 (4): 438–459. doi:10.1016/j.strueco.2007.06.003.&lt;/item&gt;&lt;item&gt;^ Hartwig, Jochen; Krämer, Hagen (December 2019). "The 'Growth Disease' at 50 – Baumol after Oulton". Structural Change and Economic Dynamics. 51: 463–471. doi:10.1016/j.strueco.2019.02.006. hdl:10419/170670. S2CID 115407478.&lt;/item&gt;&lt;item&gt;^ a b Helland, Eric; Alexander Tabarrok (2019). "Why Are the Prices So Damn High? Health, Education, and the Baumol Effect" (PDF). Mercatus Center.&lt;/item&gt;&lt;item&gt;^ a b Archibald, Robert B.; Feldman, David H. (2014). Why Does College Cost So Much?. Oxford University Press. ISBN 978-0190214104.&lt;/item&gt;&lt;item&gt;^ a b Surowiecki, James (November 13, 2011). "Debt by Degrees". The New Yorker.&lt;/item&gt;&lt;item&gt;^ Baumol, William J. (June 1967). "Macroeconomics of Unbalanced Growth: The Anatomy of Urban Crisis" (PDF). The American Economic Review. 57 (3): 415–426. JSTOR 1812111. Archived (PDF) from the original on February 26, 2022. &lt;quote&gt;The relatively constant productivity of college teaching ... suggests that, as productivity in the remainder of the economy continues to increase, costs of running the educational organizations will mount correspondingly, so that whatever the magnitude of the funds they need today, we can be reasonably certain that they will require more tomorrow, and even more on the day after that.&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;^ a b Baumol 2012, pp. 3–32.&lt;/item&gt;&lt;item&gt;^ "Labor Productivity and Costs: Elementary and Secondary Schools". Bureau of Labor Statistics. February 23, 2018. Archived from the original on January 4, 2022. Retrieved February 27, 2022.&lt;/item&gt;&lt;item&gt;^ Garrett, Thomas A; Poole, William (January 1, 2006). "Stop Paying More for Less: Ways to Boost Productivity in Higher Education". Federal Reserve Bank of St. Louis. Archived from the original on January 11, 2022. Retrieved February 27, 2022.&lt;/item&gt;&lt;item&gt;^ Carnevale, Anthony P.; Gulish, Artem; Campbell, Kathryn Peltier (2021). "If Not Now, When? The Urgent Need for an All-One-System Approach to Youth Policy" (PDF). McCourt School of Public Policy: Center on Education and the Workforce. Georgetown University. p. 13. Archived (PDF) from the original on October 20, 2021. Retrieved February 27, 2022.&lt;/item&gt;&lt;item&gt;^ Bundick, Brent; Pollard, Emily (2019). "The Rise and Fall of College Tuition Inflation" (PDF). Federal Reserve Bank of Kansas City. Archived (PDF) from the original on December 17, 2021. Retrieved February 27, 2022.&lt;/item&gt;&lt;item&gt;^ Archibald, Robert B.; Feldman, David H. (May 2008). "Explaining Increases in Higher Education Costs" (PDF). The Journal of Higher Education. 79 (3): 268–295. doi:10.1080/00221546.2008.11772099. S2CID 158250944.&lt;/item&gt;&lt;item&gt;^ a b Martin, Robert E.; Hill, R. Carter (2012). Measuring Baumol and Bowen Effects in Public Research Universities (Report). S2CID 153016802. SSRN 2153122.&lt;/item&gt;&lt;item&gt;^ Rhoades, Gary; Frye, Joanna (April 2015). "College tuition increases and faculty labor costs: A counterintuitive disconnect" (PDF). University of Arizona. Archived (PDF) from the original on October 2, 2021. Retrieved February 27, 2021.&lt;/item&gt;&lt;item&gt;^ Nose, Manabu (June 2017). "Estimation of drivers of public education expenditure: Baumol's effect revisited". International Tax and Public Finance. 24 (3): 512–535. doi:10.1007/s10797-016-9410-7. S2CID 155747172.&lt;/item&gt;&lt;item&gt;^ Matthews, Dylan (September 2, 2013). "The Tuition is Too Damn High, Part VI – Why there's no reason for big universities to rein in spending". The Washington Post. Archived from the original on November 13, 2020. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ a b "Why Tuition Costs Are Rising So Quickly". Challenge. 45 (4): 88–108. July 2002. doi:10.1080/05775132.2002.11034164.&lt;/item&gt;&lt;item&gt;^ a b Ripley, Amanda (September 11, 2018). "Why Is College in America So Expensive?". The Atlantic. Archived from the original on March 24, 2022. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Baum, Sandy; McPherson, Michael; Braga, Breno; Minton, Sarah (February 28, 2018). "Tuition and State Appropriations". Urban Institute. Retrieved August 12, 2024.&lt;/item&gt;&lt;item&gt;^ Johnson, J. David (2020). "Administrative Bloat in Higher Education" (PDF). Cambridge Scholars. Archived (PDF) from the original on October 13, 2021. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Renehan, Stewart (August 2015). "Rising Tuition in Higher Education: Should we be Concerned?". Visions for the Liberal Arts. 1 (1) – via CORE.&lt;/item&gt;&lt;item&gt;^ Hoffower, Hillary (June 26, 2019). "College is more expensive than it's ever been, and the 5 reasons why suggest it's only going to get worse". Business Insider. Archived from the original on March 23, 2022. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Bennet, William J. (February 18, 1987). "Our Greedy Colleges". The New York Times. Archived from the original on January 27, 2022. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Robinson, Jenna A. (December 2017). "The Bennett Hypothesis Turns 30" (PDF). James G. Martin Center for Academic Renewal. Archived (PDF) from the original on January 28, 2021. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Akers, Beth (August 27, 2020). "A New Approach for Curbing College Tuition Inflation". Manhattan Institute for Policy Research. Archived from the original on November 29, 2021. Retrieved March 25, 2022.&lt;/item&gt;&lt;item&gt;^ Sheiner, Louise; Malinovskaya, Anna (May 2016). "Measuring Productivity in Healthcare: An Analysis of the Literature" (PDF). Brookings Institution. Archived (PDF) from the original on March 24, 2022. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ "Labor Productivity and Costs: Private Community Hospitals". Bureau of Labor Statistics. June 11, 2021. Archived from the original on January 4, 2022. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ Colombier, Carsten (November 2017). "Drivers of Health-Care Expenditure: What Role Does Baumol's Cost Disease Play?". Social Science Quarterly. 98 (5): 1603–1621. doi:10.1111/ssqu.12384.&lt;/item&gt;&lt;item&gt;^ Hartwig, Jochen (May 2008). "What drives health care expenditure?—Baumol's model of 'unbalanced growth' revisited". Journal of Health Economics. 27 (3): 603–623. doi:10.1016/j.jhealeco.2007.05.006. hdl:10419/50843. PMID 18164773. S2CID 219356527. SSRN 910879.&lt;/item&gt;&lt;item&gt;^ Hartwig, Jochen (January 2011). "Can Baumol's model of unbalanced growth contribute to explaining the secular rise in health care expenditure? An alternative test". Applied Economics. 43 (2): 173–184. doi:10.1080/00036840802400470. hdl:10419/50451. S2CID 154307473.&lt;/item&gt;&lt;item&gt;^ Bates, Laurie J.; Santerre, Rexford E. (March 2013). "Does the U.S. health care sector suffer from Baumol's cost disease? Evidence from the 50 states". Journal of Health Economics. 32 (2): 386–391. doi:10.1016/j.jhealeco.2012.12.003. PMID 23348051.&lt;/item&gt;&lt;item&gt;^ Pomp, Marc; Vujić, Sunčica (December 2008). "Rising health spending, new medical technology and the Baumol effect" (PDF). Bureau for Economic Policy Analysis. Archived (PDF) from the original on January 22, 2022. Retrieved March 26, 2022.&lt;/item&gt;&lt;item&gt;^ Atanda, Akinwande; Menclova, Andrea Kutinova; Reed, W. Robert (May 2018). "Is health care infected by Baumol's cost disease? Test of a new model". Health Economics. 27 (5): 832–849. doi:10.1002/hec.3641. PMID 29423941. S2CID 46855963.&lt;/item&gt;&lt;item&gt;^ Wang, Linan; Chen, Yuqian (2021). "Determinants of China's health expenditure growth: based on Baumol's cost disease theory". International Journal for Equity in Health. 20 (1): 213. doi:10.1186/s12939-021-01550-y. PMC 8474747. PMID 34565389. S2CID 233774285.&lt;/item&gt;&lt;item&gt;^ Rossen, Bradley; Faroque, Akhter (May 2016). "Diagnosing the Causes of Rising Health-Care Expenditure in Canada: Does Baumol's Cost Disease Loom Large?". American Journal of Health Economics. 2 (2): 184–212. doi:10.1162/AJHE_a_00041. S2CID 57569390.&lt;/item&gt;&lt;item&gt;^ Marino, Alberto; Morgan, David; Lorenzoni, Luca; James, Chris (June 2017). Future trends in health care expenditure: A modelling framework for cross-country forecasts (Report). OECD Health Working Papers. doi:10.1787/247995bb-en. ProQuest 1915769062.&lt;/item&gt;&lt;item&gt;^ Surowiecki, James (July 7, 2003). "What Ails Us". The New Yorker. Archived from the original on May 9, 2021.&lt;/item&gt;&lt;item&gt;^ Bosworth, Barry P.; Triplett, Jack E. (September 1, 2003). "Productivity Measurement Issues in Services Industries: 'Baumol's Disease' Has been Cured". Brookings Institution.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Sources&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Baumol, William J. (2012). The Cost Disease: Why Computers Get Cheaper and Health Care Doesn't. Yale University Press. ISBN 978-0-300-19815-7.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;External links&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Charles Hugh Smith (December 11, 2010). "America's Economic Malady: A Bad Case of 'Baumol's Disease'". DailyFinance.com. Archived from the original on December 17, 2010. Retrieved December 14, 2010.&lt;/item&gt;&lt;item&gt;Sparviero, Sergio; Preston, Paschal (September 2010). "Creativity and the positive reading of Baumol cost disease". The Service Industries Journal. 30 (11): 1903–1917. doi:10.1080/02642060802627541. S2CID 154148314. SSRN 1531602.&lt;/item&gt;&lt;item&gt;Preston, Paschal; Sparviero, Sergio (November 30, 2009). "Creative Inputs as the Cause of Baumol's Cost Disease: The Example of Media Services". Journal of Media Economics. 22 (4): 239–252. doi:10.1080/08997760903375910. S2CID 154146664. SSRN 1531555.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262592</guid><pubDate>Sun, 14 Dec 2025 12:34:35 +0000</pubDate></item><item><title>Kimi K2 1T model runs on 2 512GB M3 Ultras</title><link>https://twitter.com/awnihannun/status/1943723599971443134</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262734</guid><pubDate>Sun, 14 Dec 2025 13:04:34 +0000</pubDate></item><item><title>Vacuum Is a Lie: About Your Indexes</title><link>https://boringsql.com/posts/vacuum-is-lie/</link><description>&lt;doc fingerprint="4914033578e42db5"&gt;
  &lt;main&gt;
    &lt;p&gt;There is common misconception that troubles most developers using PostgreSQL: tune VACUUM or run VACUUM, and your database will stay healthy. Dead tuples will get cleaned up. Transaction IDs recycled. Space reclaimed. Your database will live happily ever after.&lt;/p&gt;
    &lt;p&gt;But there are couple of dirty "secrets" people are not aware of. First of them being VACUUM is lying to you about your indexes.&lt;/p&gt;
    &lt;head rend="h2"&gt;The anatomy of storage&lt;/head&gt;
    &lt;p&gt;When you delete a row in PostgreSQL, it is just marked as a 'dead tuple'. Invisible for new transactions but still physically present. Only when all transactions referencing the row are finished, VACUUM can come along and actually remove them - reclamining the space in the heap (table) space.&lt;/p&gt;
    &lt;p&gt;To understand why this matters differently for tables versus indexes, you need to picture how PostgreSQL actually stores your data.&lt;/p&gt;
    &lt;p&gt;Your table data lives in the heap - a collection of 8 KB pages where rows are stored wherever they fit. There's no inherent order. When you INSERT a row, PostgreSQL finds a page with enough free space and slots the row in. Delete a row, and there's a gap. Insert another, and it might fill that gap - or not - they might fit somewhere else entirely.&lt;/p&gt;
    &lt;p&gt;This is why &lt;code&gt;SELECT * FROM users&lt;/code&gt; without an ORDER BY can return rows in order
initially, and after some updates in seemingly random order, and that order can
change over time. The heap is like Tetris. Rows drop into whatever space is
available, leaving gaps when deleted.&lt;/p&gt;
    &lt;p&gt;When VACUUM runs, it removes those dead tuples and compacts the remaining rows within each page. If an entire page becomes empty, PostgreSQL can reclaim it entirely.&lt;/p&gt;
    &lt;p&gt;And while indexes are on surface the same collection of 8KB pages, they are different. A B-tree index must maintain sorted order - that's the whole point of their existence and the reason why &lt;code&gt;WHERE id = 12345&lt;/code&gt; is so
fast. PostgreSQL can binary-search down the tree instead of scanning every
possible row. You can learn more about the fundamentals of B-Tree Indexes and
what makes them fast.&lt;/p&gt;
    &lt;p&gt;But if the design of the indexes is what makes them fast, it's also their biggest responsibility. While PostgreSQL can fit rows into whatever space is available, it can't move the entries in index pages to fit as much as possible.&lt;/p&gt;
    &lt;p&gt;VACUUM can remove dead index entries. But it doesn't restructure the B-tree. When VACUUM processes the heap, it can compact rows within a page and reclaim empty pages. The heap has no ordering constraint - rows can be anywhere. But B-tree pages? They're locked into a structure. VACUUM can remove dead index entries, yes.&lt;/p&gt;
    &lt;p&gt;Many developers assume VACUUM treats all pages same. No matter whether they are heap or index pages. VACUUM is supposed to remove the dead entries, right?&lt;/p&gt;
    &lt;p&gt;Yes. But here's what it doesn't do - it doesn't restructure the B-tree.&lt;/p&gt;
    &lt;p&gt;What VACUUM actually does&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Removes dead tuple pointers from index pages&lt;/item&gt;
      &lt;item&gt;Marks completely empty pages as reusable&lt;/item&gt;
      &lt;item&gt;Updates the free space map&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What VACUUM cannot do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Merge sparse pages together (can do it for empty pages)&lt;/item&gt;
      &lt;item&gt;Reduce tree depth&lt;/item&gt;
      &lt;item&gt;Deallocate empty-but-still-linked pages&lt;/item&gt;
      &lt;item&gt;Change the physical structure of the B-tree&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your heap is Tetris, gaps can get filled. Your B-tree is a sorted bookshelf. VACUUM can pull books out, but can't slide the remaining ones together. You're left walking past empty slots every time you scan.&lt;/p&gt;
    &lt;head rend="h2"&gt;The experiment&lt;/head&gt;
    &lt;p&gt;Let's get hands-on and create a table, fill it, delete most of it and watch what happens.&lt;/p&gt;
    &lt;code&gt;CREATE EXTENSION IF NOT EXISTS pgstattuple;
CREATE TABLE demo (id integer PRIMARY KEY, data text);

-- insert 100,000 rows
INSERT INTO demo (id, data)
SELECT g, 'Row number ' || g || ' with some extra data'
FROM generate_series(1, 100000) g;

ANALYZE demo;
&lt;/code&gt;
    &lt;p&gt;At this point, our index is healthy. Let's capture the baseline:&lt;/p&gt;
    &lt;code&gt;SELECT
    relname,
    pg_size_pretty(pg_relation_size(oid)) as file_size,
    pg_size_pretty((pgstattuple(oid)).tuple_len) as actual_data
FROM pg_class
WHERE relname IN ('demo', 'demo_pkey');
&lt;/code&gt;
    &lt;code&gt;relname  | file_size | actual_data
-----------+-----------+-------------
demo      | 7472 kB   | 6434 kB
demo_pkey | 2208 kB   | 1563 kB
&lt;/code&gt;
    &lt;p&gt;Now remove some data, 80% to be precise - somewhere in the middle:&lt;/p&gt;
    &lt;code&gt;DELETE FROM demo WHERE id BETWEEN 10001 AND 90000;
&lt;/code&gt;
    &lt;p&gt;The goal is to simulate a common real-world pattern: data retention policies, bulk cleanup operations, or the aftermath of a data migration gone wrong.&lt;/p&gt;
    &lt;code&gt;VACUUM demo;

SELECT
    relname,
    pg_size_pretty(pg_relation_size(oid)) as file_size,
    pg_size_pretty((pgstattuple(oid)).tuple_len) as actual_data
FROM pg_class
WHERE relname IN ('demo', 'demo_pkey');
&lt;/code&gt;
    &lt;code&gt;relname  | file_size | actual_data
-----------+-----------+-------------
demo      | 7472 kB   | 1278 kB
demo_pkey | 2208 kB   | 1563 kB
&lt;/code&gt;
    &lt;p&gt;The table shrunk significantly, while index remained unchanged. You now have 20,000 rows indexed by a structure build to handle 100,000. Please, also notice &lt;code&gt;file_size&lt;/code&gt; remain unchanged. VACUUM doesn't return space to the OS, it only
marks pages as reusable within PostgreSQL.&lt;/p&gt;
    &lt;p&gt;This experiment is really an extreme case, but demonstrates the problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding page states&lt;/head&gt;
    &lt;p&gt;Leaf pages have several states:&lt;/p&gt;
    &lt;p&gt;Full page (&amp;gt;80% density), when the page contains many index entries, efficiently utilizing space. Each 8KB page read returns substantial useful data. This is optimal state.&lt;/p&gt;
    &lt;p&gt;Partial page (40-80% density) with some wasted space, but still reasonably efficient. Common at tree edges or after light churn. Nothing to be worried about.&lt;/p&gt;
    &lt;p&gt;Sparse page (&amp;lt;40% density) is mostly empty. You're reading an 8KB page to find a handful of entries. The I/O cost is the same as a full page, but you get far less value.&lt;/p&gt;
    &lt;p&gt;Empty page (0% density) with zero live entries, but the page still exists in the tree structure. Pure overhead. You might read this page during a range scan and find absolutely nothing useful.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on fillfactor&lt;/head&gt;
    &lt;p&gt;You might be wondering how can fillfactor help with this? It's the setting you can apply both for heap and leaf pages, and controls how full PostgreSQL packs the pages during the data storage. The default value for B-tree indexes is 90%. This leaves 10% of free space on each leaf page for future insertions.&lt;/p&gt;
    &lt;code&gt;CREATE INDEX demo_index ON demo(id) WITH (fillfactor = 70);
&lt;/code&gt;
    &lt;p&gt;A lower fillfactor (like 70%) leaves more room, which can reduce page splits when you're inserting into the middle of an index - useful for tables random index column inserts or those with heavily updated index columns.&lt;/p&gt;
    &lt;p&gt;But if you followed carefully the anatomy of storage section, it doesn't help with the bloat problem. Quite the oppossite. If you set lower fillfactor and then delete majority of your rows, you actually start with more pages, and bigger chance to end up with more sparse pages than partial pages.&lt;/p&gt;
    &lt;p&gt;Leaf page fillfactor is about optimizing for updates and inserts. It's not a solution for deletion or index-column update bloat.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why the planner gets fooled&lt;/head&gt;
    &lt;p&gt;PostgreSQL's query planner estimates costs based on physical statistics, including the number of pages in an index.&lt;/p&gt;
    &lt;code&gt;EXPLAIN ANALYZE SELECT * FROM demo WHERE id BETWEEN 10001 AND 90000;
&lt;/code&gt;
    &lt;code&gt;QUERY PLAN
--------------------------------------------------------------------------------------------------------------------
  Index Scan using demo_pkey on demo  (cost=0.29..29.29 rows=200 width=41) (actual time=0.111..0.112 rows=0 loops=1)
    Index Cond: ((id &amp;gt;= 10001) AND (id &amp;lt;= 90000))
  Planning Time: 1.701 ms
  Execution Time: 0.240 ms
(4 rows)
&lt;/code&gt;
    &lt;p&gt;While the execution is almost instant, you need to look behind the scenes. The planner estimated 200 rows and got zero. It traversed the B-tree structure expecting data that doesn't exist. On a single query with warm cache, this is trivial. Under production load with thousands of queries and cold pages, you're paying I/O cost for nothing. Again and again.&lt;/p&gt;
    &lt;p&gt;If you dig further you discover much bigger problem.&lt;/p&gt;
    &lt;code&gt;SELECT relname, reltuples::bigint as row_estimate, relpages as page_estimate
FROM pg_class 
WHERE relname IN ('demo', 'demo_pkey');
&lt;/code&gt;
    &lt;code&gt;relname  | row_estimate | page_estimate
-----------+--------------+---------------
demo      |        20000 |           934
demo_pkey |        20000 |           276
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;relpages&lt;/code&gt; value comes from the physical file size divided by the 8 KB page
size. PostgreSQL updates it during VACUUM and ANALYZE, but it reflects the
actual file on disk - not how much useful data is inside. Our index file is still
2.2 MB (276 pages × 8 KB), even though most pages are empty.&lt;/p&gt;
    &lt;p&gt;The planner sees 276 pages for 20,000 rows and calculates a very low rows-per-page ratio. This is when planner can come to conclusion - this index is very sparse - let's do a sequential scan instead. Oops.&lt;/p&gt;
    &lt;p&gt;"But wait," you say, "doesn't ANALYZE fix statistics?"&lt;/p&gt;
    &lt;p&gt;Yes and no. &lt;code&gt;ANALYZE&lt;/code&gt; updates the row count estimate. It will no longer think you
have 100,000 rows but 20,000. But it does not shrink relpages, because that
reflects the physical file size on disk. &lt;code&gt;ANALYZE&lt;/code&gt; can't change that.&lt;/p&gt;
    &lt;p&gt;The planner now has accurate row estimates but wildly inaccurate page estimates. The useful data is packed into just ~57 pages worth of entries, but the planner doesn't know that.&lt;/p&gt;
    &lt;code&gt;cost = random_page_cost × pages + cpu_index_tuple_cost × tuples
&lt;/code&gt;
    &lt;p&gt;With a bloated index:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pages is oversize (276 instead of ~57)&lt;/item&gt;
      &lt;item&gt;The per-page cost gets multiplied by empty pages&lt;/item&gt;
      &lt;item&gt;Total estimated cost is artificially high&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The hollow index&lt;/head&gt;
    &lt;p&gt;We can dig even more into the index problem when we look at internal stats:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM pgstatindex('demo_pkey');
&lt;/code&gt;
    &lt;code&gt;-[ RECORD 1 ]------+--------
version            | 4
tree_level         | 1
index_size         | 2260992
root_block_no      | 3
internal_pages     | 1
leaf_pages         | 57
empty_pages        | 0
deleted_pages      | 217
avg_leaf_density   | 86.37
leaf_fragmentation | 0
&lt;/code&gt;
    &lt;p&gt;Wait, what? The avg_leaf_density is 86% and it looks perfectly healthy. That's a trap. Due to the hollow index (we removed 80% right in the middle) we have 57 well-packed leaf pages, but the index still contains 217 deleted pages.&lt;/p&gt;
    &lt;p&gt;This is why &lt;code&gt;avg_leaf_density&lt;/code&gt; alone is misleading. The density of used pages
looks great, but 79% of your index file is dead weight.&lt;/p&gt;
    &lt;p&gt;The simplest way to spot index bloat is comparing actual size to expected size.&lt;/p&gt;
    &lt;code&gt;SELECT
    c.relname as index_name,
    pg_size_pretty(pg_relation_size(c.oid)) as actual_size,
    pg_size_pretty((c.reltuples * 40)::bigint) as expected_size,
    round((pg_relation_size(c.oid) / nullif(c.reltuples * 40, 0))::numeric, 1) as bloat_ratio
FROM pg_class c
JOIN pg_index i ON c.oid = i.indexrelid
WHERE c.relkind = 'i' 
  AND c.reltuples &amp;gt; 0
  AND c.relname NOT LIKE 'pg_%'
  AND pg_relation_size(c.oid) &amp;gt; 1024 * 1024  -- only indexes &amp;gt; 1 MB
ORDER BY bloat_ratio DESC NULLS LAST;
&lt;/code&gt;
    &lt;code&gt;index_name | actual_size | expected_size | bloat_ratio
------------+-------------+---------------+-------------
demo_pkey  | 2208 kB     | 781 kB        |         2.8
&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;bloat_ratio&lt;/code&gt; of 2.8 means the index is nearly 3x larger than expected. Anything
above 1.8 - 2.0 deserves investigation.&lt;/p&gt;
    &lt;p&gt;We filter to indexes over 1 MB - bloat on tiny indexes doesn't matter that much. Please, adjust the threshold based on your environment; for large databases, you might only care about indexes over 100 MB.&lt;/p&gt;
    &lt;p&gt;But here comes BIG WARNING: pgstatindex() we used earlier physically reads the entire index. On a 10 GB index, that's 10 GB of I/O. Don't run it against all indexes on a production server - unless you know what you are doing!&lt;/p&gt;
    &lt;head rend="h2"&gt;REINDEX&lt;/head&gt;
    &lt;p&gt;How to actually fix index bloat problem? &lt;code&gt;REINDEX&lt;/code&gt; is s straightforward solution as
it rebuilds the index from scratch.&lt;/p&gt;
    &lt;code&gt;REINDEX INDEX CONCURRENTLY demo_pkey ;
&lt;/code&gt;
    &lt;p&gt;After which we can check the index health:&lt;/p&gt;
    &lt;code&gt;SELECT * FROM pgstatindex('demo_pkey');
&lt;/code&gt;
    &lt;code&gt;-[ RECORD 1 ]------+-------
version            | 4
tree_level         | 1
index_size         | 466944
root_block_no      | 3
internal_pages     | 1
leaf_pages         | 55
empty_pages        | 0
deleted_pages      | 0
avg_leaf_density   | 89.5
leaf_fragmentation | 0
&lt;/code&gt;
    &lt;p&gt;And&lt;/p&gt;
    &lt;code&gt;SELECT
    relname,
    pg_size_pretty(pg_relation_size(oid)) as file_size,
    pg_size_pretty((pgstattuple(oid)).tuple_len) as actual_data
FROM pg_class
WHERE relname IN ('demo', 'demo_pkey');
&lt;/code&gt;
    &lt;code&gt;relname  | file_size | actual_data
-----------+-----------+-------------
demo      | 7472 kB   | 1278 kB
demo_pkey | 456 kB    | 313 kB
&lt;/code&gt;
    &lt;p&gt;Our index shrunk from 2.2 MB to 456 KB - 79% reduction (not a big surprise though).&lt;/p&gt;
    &lt;p&gt;As you might have noticed we have used &lt;code&gt;CONCURRENTLY&lt;/code&gt; to avoid using ACCESS
EXCLUSIVE lock. This is available since PostgreSQL 12+, and while there's an
option to omit it - the pretty much only reason to do so is during planned
maintenance to speed up the index rebuild time.&lt;/p&gt;
    &lt;head rend="h2"&gt;pg_squeeze&lt;/head&gt;
    &lt;p&gt;If you look above at the file_size of our relations, we have managed to reclaim the disk space for the affected index (it was &lt;code&gt;REINDEX&lt;/code&gt; after all), but the table
space was not returned back to the operating system.&lt;/p&gt;
    &lt;p&gt;That's where pg_squeeze shines. Unlike trigger-based alternatives, pg_squeeze uses logical decoding, resulting in lower impact on your running system. It rebuilds both the table and all its indexes online, with minimal locking:&lt;/p&gt;
    &lt;code&gt;CREATE EXTENSION pg_squeeze;

SELECT squeeze.squeeze_table('public', 'demo');
&lt;/code&gt;
    &lt;p&gt;The exclusive lock is only needed during the final swap phase, and its duration can be configured. Even better, pg_squeeze is designed for regular automated processing - you can register tables and let it handle maintenance whenever bloat thresholds are met.&lt;/p&gt;
    &lt;p&gt;pg_squeeze makes sense when both table and indexes are bloated, or when you want automated management. REINDEX CONCURRENTLY is simpler when only indexes need work.&lt;/p&gt;
    &lt;p&gt;There's also older tool pg_repack - for a deeper comparison of bloat-busting tools, see article The Bloat Busters: pg_repack vs pg_squeeze.&lt;/p&gt;
    &lt;head rend="h2"&gt;VACUUM FULL (The nuclear option)&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;VACUUM FULL&lt;/code&gt; rewrites the entire table and all indexes. While it fixes
everything it comes with a big but - it requires an ACCESS EXCLUSIVE
lock - completely blocking all reads and writes for the entire duration. For a
large table, this could mean hours of downtime.&lt;/p&gt;
    &lt;p&gt;Generally avoid this in production. Use pg_squeeze instead for the same result without the downtime.&lt;/p&gt;
    &lt;head rend="h2"&gt;When to act, and when to chill&lt;/head&gt;
    &lt;p&gt;Before you now go and &lt;code&gt;REINDEX&lt;/code&gt; everything in sight, let's talk about when index
bloat actually matters.&lt;/p&gt;
    &lt;p&gt;B-trees expand and contract with your data. With random insertions affecting index columns - UUIDs, hash keys, etc. the page splits happen constantly. Index efficiency might get hit at occassion and also settle around 70 - 80% over different natural cycles of your system usage. That's not bloat. That's the tree finding its natural shape for your data.&lt;/p&gt;
    &lt;p&gt;The bloat we demonstrated - 57 useful pages drowning in 217 deleted ones - is extreme. It came from deleting 80% of contiguous data. You won't see this from normal day to day operations.&lt;/p&gt;
    &lt;p&gt;When do you need to act immediately:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;after a massive DELETE (retention policy, GDPR purge, failed migration cleanup)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bloat_ratio&lt;/code&gt;exceeds 2.0 and keeps climbing&lt;/item&gt;
      &lt;item&gt;query plans suddenly prefer sequential scans on indexed columns&lt;/item&gt;
      &lt;item&gt;index size is wildly disproportionate to row count&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But in most cases you don't have to panic. Monitor weekly and when indexes bloat ratio continously grow above warning levels, schedule a &lt;code&gt;REINDEX CONCURRENTLY&lt;/code&gt;
during low traffic period.&lt;/p&gt;
    &lt;p&gt;Index bloat isn't an emergency until it is. Know the signs, have the tools ready, and don't let VACUUM's silence fool you into thinking everything's fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;VACUUM is essential for PostgreSQL. Run it. Let autovacuum do its job. But understand its limitations: it cleans up dead tuples, not index structure.&lt;/p&gt;
    &lt;p&gt;The truth about PostgreSQL maintenance is that VACUUM handles heap bloat reasonably well, but index bloat requires explicit intervention. Know when your indexes are actually sick versus just breathing normally - and when to reach for REINDEX.&lt;/p&gt;
    &lt;p&gt;VACUUM handles heap bloat. Index bloat is your problem. Know the difference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262777</guid><pubDate>Sun, 14 Dec 2025 13:13:41 +0000</pubDate></item><item><title>AI and the ironies of automation – Part 2</title><link>https://www.ufried.com/blog/ironies_of_ai_2/</link><description>&lt;doc fingerprint="b6d01b1d01739db2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI and the ironies of automation - Part 2&lt;/head&gt;
    &lt;p&gt;Some (well-known) consequences of AI automating work&lt;/p&gt;
    &lt;head rend="h1"&gt;AI and the ironies of automation - Part 2&lt;/head&gt;
    &lt;p&gt;In the previous post, we discussed several observations, Lisanne Bainbridge made in her much-noticed paper “The ironies of automation”, she published in 1983 and what they mean for the current “white-collar” work automation attempts leveraging LLMs and AI agents based on LLMs, still requiring humans in the loop. We stopped at the end of the first chapter, “Introduction”, of the paper.&lt;/p&gt;
    &lt;p&gt;In this post, we will continue with the second chapter, “Approaches to solutions”, and see what we can learn there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing apples and oranges?&lt;/head&gt;
    &lt;p&gt;However, before we start: Some of the observations and recommendations made in the paper must be taken with a grain of salt when applying them to the AI-based automation attempts of today. When monitoring an industrial production plant, it is often a matter of seconds until a human operator must act if something goes wrong to avoid severe or even catastrophic accidents.&lt;/p&gt;
    &lt;p&gt;Therefore, it is of the highest importance to design industrial control stations in a way that a human operator can recognize deviations and malfunctions as easily as possible and immediately trigger countermeasures. A lot of work is put into the design of all the displays and controls, like, e.g., the well-known emergency stop switch in a screaming red color that is big enough to be punched with a flat hand, fist or alike within a fraction of a second if needed.&lt;/p&gt;
    &lt;p&gt;When it comes to AI-based solutions automating white-collar work, we usually do not face such critical conditions. However, this is not a reason to dismiss the observations and recommendations in the paper easily because, e.g.:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Most companies are efficiency-obsessed. Hence, they also expect AI solutions to increase “productivity”, i.e., efficiency, to a superhuman level. If a human is meant to monitor the output of the AI and intervene if needed, this requires that the human needs to comprehend what the AI solution produced at superhuman speed – otherwise we are down to human speed. This presents a quandary that can only be solved if we enable the human to comprehend the AI output at superhuman speed (compared to producing the same output by traditional means).&lt;/item&gt;
      &lt;item&gt;Most companies have a tradition of nurturing a culture of urgency and scarcity, resulting in a lot of pressure towards and stress for the employees. Stress is known to trigger the fight-or-flight mode (an ancient survival mechanism built into us to cope with dangerous situations) which massively reduces the normal cognitive capacity of a human. While this mechanism supports humans in making very quick decisions and taking quick actions (essential in dangerous situations), it deprives them of the ability to conduct any deeper analysis (not being essential in dangerous situations). If deeper analysis is required to make a decision, this may take a lot longer than without stress – if possible at all. This means we need to enable humans to conduct deeper analysis under stress as well or to provide the information in a way that eliminates the need for deeper analysis (which is not always possible).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we let this sink in (plus a few other aspects, I did not write down here but you most likely will add in your mind), we quickly come to the conclusion that also in our AI-related automation context humans are often expected to make quick decisions and act based on them, often under conditions that make it hard (if not impossible) to conduct any in-depth analysis.&lt;/p&gt;
    &lt;p&gt;If we then also take into account, that depending on the situation a wrong result produced by an AI solution which eluded the human operator may have severe consequences in the worst case (e.g., assume a major security incident due to a missed wrongdoing of the AI solution), the situation is not that far away anymore from the situation in an industrial plant’s control station.&lt;/p&gt;
    &lt;p&gt;Summarizing, we surely need to add the necessary grain of salt, i.e., ask ourselves how strict the timing constraints in our specific setting are to avoid comparing apples and oranges in the worst case. However, in general we need to consider the whole range of possible settings which will – probably more often than we think – include that humans need to make decisions in a very short time under stressful conditions (which makes things more precarious).&lt;/p&gt;
    &lt;head rend="h2"&gt;The worst UI possible&lt;/head&gt;
    &lt;p&gt;This brings us immediately to Lisanne Bainbridge’s first recommendation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In any situation where a low probability event must be noticed quickly then the operator must be given artificial assistance, if necessary even alarms on alarms.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, the system must support the human operator as well as possible in detecting a problem, especially if it tends to occur rarely. It is a consequence of the “monitoring fatigue” problem we discussed in the previous post.&lt;/p&gt;
    &lt;p&gt;Due to the learnings people have made, a lot of effort has been put into the design of the displays, the controls and also the alerting mechanisms of industrial production control stations, making sure the human operators can make their jobs as good, as stress-free and as reliable as possible.&lt;/p&gt;
    &lt;p&gt;Enter AI agents.&lt;/p&gt;
    &lt;p&gt;The usual idea is that a single human controls a fleet of AI agents that are designed to do some kind of job, e.g., writing code. Sometimes, most agents are generic “workers”, orchestrated by some kind of supervisor that delegates parts of the work to the worker agents. Sometimes, the different agents are “specialists”, each for a certain aspect of the job to be done, that collaborate using some kind of choreography (or are also orchestrated by a supervisor). While the generic workers are easier to set up, the specialized workers usually produce more accurate results.&lt;/p&gt;
    &lt;p&gt;Because these AI-based agents sometimes produce errors, a human – in our example a software developer – needs to supervise the AI agent fleet and ideally intervenes before the AI agents do something they should not do. Therefore, the AI agents typically create a plan of what they intend to do first (which as a side effect also increases the likelihood that they do not drift off). Then, the human verifies the plan and approves it if it is correct, and the AI agents execute the plan. If the plan is not correct, the human rejects it and sends the agents back to replanning, providing information about what needs to be altered.&lt;/p&gt;
    &lt;p&gt;Let us take Lisanne Bainbridge’s recommendation and compare it to this approach that is currently “best practice” to control an AI agent fleet.&lt;/p&gt;
    &lt;p&gt;Unless we tell them to act differently, LLMs and also AI agents based on them are quite chatty. Additionally, they tend to communicate with an air of utter conviction. Thus, they present to you this highly detailed, multi-step plan of what they intend to do, including lots of explanations, in this perfectly convinced tone. Often, these plans are more than 50 or 100 lines of text, sometimes even several hundred lines.&lt;/p&gt;
    &lt;p&gt;Most of the time, the plans are fine. However, sometimes the AI agents mess things up. They make wrong conclusions, or they forget what they are told to do and drift off – not very often, but it happens. Sometimes the problem is obvious at first sight. But more often, it is neatly hidden somewhere behind line 123: “… and because 2 is bigger than 3, it is clear, we need to &amp;lt; do something critical &amp;gt;”. But because it is so much text the agents flood you with all the time and because the error is hidden so well behind this wall of conviction, we miss it – and the AI agent does something critical wrong.&lt;/p&gt;
    &lt;p&gt;We cannot blame the person for missing the error in the plan. The problem is that this is probably the worst UI and UX possible for anyone who is responsible for avoiding errors in a system that rarely produces errors.&lt;/p&gt;
    &lt;p&gt;But LLM-based agents make errors all the time, you may say. Well, not all the time. Sometimes they do. And the better the instructions and the setup of the interacting agents, the fewer errors they produce. Additionally, we can expect more specialized and refined agents in the future that become increasingly better in their respective areas of expertise. Still, most likely they will never become completely error-free because of the underlying technology that cannot guarantee consistent correctness.&lt;/p&gt;
    &lt;p&gt;This is the setting we need to ponder if we talk about the user interface for a human observer: a setting where the agent fleet only rarely makes errors but we still need a human monitoring and intervening if things should go wrong. It is not yet clear how such an interface should look like, but most definitely not as it looks now. Probably we could harvest some good insights from our UX/UI design colleagues for industrial production plant control stations. We would need only to ask them …&lt;/p&gt;
    &lt;head rend="h2"&gt;The training paradox&lt;/head&gt;
    &lt;p&gt;Lisanne Bainbridge then makes several recommendations regarding the required training of the human operator. This again is a rich section, and I can only recommend reading it on your own because it contains several subtle yet important hints that are hard to bring across without citing the whole chapter. Here, I will highlight only a few aspects. She starts with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[Some points made in the previous section] make it clear that it can be important to maintain manual skills.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Then she talks about letting the human operator take over control regularly, i.e., do the job instead of the machine as a very effective training option. Actually, without doing hands-on work regularly, the skills of a human expert deteriorate surprisingly fast.&lt;/p&gt;
    &lt;p&gt;But if taking over the work regularly is not an option, e.g., because we want continuous superhuman productivity leveraging AI agents (no matter if it makes sense or not), we still need to make sure that the human operator can take over if needed. In such a setting, training must take place in some other way, usually using some kind of simulator.&lt;/p&gt;
    &lt;p&gt;However, there is a problem with simulators, especially if human intervention is only needed (and wanted) if things do not work as expected:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are problems with the use of any simulator to train for extreme situations. Unknown faults cannot be simulated, and system behaviour may not be known for faults which can be predicted but have not been experienced.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The consequence of this issue is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This means that training must be concerned with general strategies rather than specific responses […]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is inadequate to expect the operator to react to unfamiliar events solely by consulting operating procedures. These cannot cover all the possibilities, so the operator is expected to monitor them and fill in the gaps.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Which leaves us with the irony:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;However, it is ironic to train operators in following instructions and then put them in the system to provide intelligence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is a problem we will need to face with AI agents and their supervising humans in the future, too. The supervising experts are meant to intervene whenever things become messy, whenever the AI agents get stuck, often in unforeseen ways. These are not regular tasks. Often, these are also not the issues we expect an AI agent to run into and thus can provide training for. These are extraordinary situations, the ones we do not expect – and the more refined and specialized the AI agents will become in the future, the more often the issues that require human intervention will be of this kind.&lt;/p&gt;
    &lt;p&gt;The question is twofold:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;How can we train human operators at all to be able to intervene skillfully in exceptional, usually hard to solve situations?&lt;/item&gt;
      &lt;item&gt;How can we train a human operator so that their skills remain sharp over time and they remain able to address an exceptional situation quickly and resourcefully?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The questions seem to hint at a sort of paradox, and an answer to both questions is all but obvious. At the moment, we still have enough experienced subject matter experts that the questions may feel of lower importance. But if we only start to address the questions when they become pressing, they will be even harder – if not impossible – to solve.&lt;/p&gt;
    &lt;p&gt;To end this consideration with the words of Lisanne Bainbridge:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Perhaps the final irony is that it is the most successful automated systems, with rare need for manual intervention, which may need the greatest investment in human operator training.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words, we cannot simply take a few available human experts and make them supervise agents that took over their work without any further investments in the humans. Instead, we need to train them continuously, and the better the agents become, the more expensive the training of the supervisors will become. I highly doubt that decision makers who primarily think about saving money when it comes to AI agents are aware of this irony.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interlude&lt;/head&gt;
    &lt;p&gt;As I wrote in the beginning of first part of this blog series, “The ironies of automation” is a very rich and dense paper. We are still only at the end of the second chapter “Approaches to solutions” which is two and a half pages into the paper and there is still a whole third chapter called “Human-computer collaboration” which takes up another page until we get to the conclusion.&lt;/p&gt;
    &lt;p&gt;While this third chapter also contains a lot of valuable advice that goes well beyond our focus here, I will leave it to you to read it on your own. As I indicated at the beginning, this paper is more than worth the time spent on it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The leadership dilemma&lt;/head&gt;
    &lt;p&gt;However, before finishing this little blog series, I would like to mention a new kind of dilemma that Lisanne Bainbridge did not discuss in her paper because the situation was a bit different with industrial production plant automation than with AI-agent-based automation. But as this topic fits nicely into the just-finished training paradox section, I decided to add it here.&lt;/p&gt;
    &lt;p&gt;The issue is that just monitoring an AI agent fleet doing its work and intervening if things go wrong usually is not sufficient, at least not yet. All the things discussed before apply, but there is more to interacting with AI agents because we cannot simply be reactive with AI agents. We cannot simply watch them doing their work and only intervene if things go wrong. Instead, we additionally need to be proactive with them: We need to direct them.&lt;/p&gt;
    &lt;p&gt;We need to tell the AI agents what to do, what not to do, which chunks to pick and so on. This is basically a leadership role. While you do not lead humans, the kind of work is quite similar: You are responsible for the result; you are allowed to set directions and constraints, but you do not immediately control the work. You only control it through communicating with the agents and trying to direct them in the right direction with orders, with feedback, with changed orders, with setting different constraints, etcetera.&lt;/p&gt;
    &lt;p&gt;This is a skill set most people do not have naturally. Usually, they need to develop it over time. Typically, before people are put in a leadership role directing humans, they will get a lot of leadership training teaching them the skills and tools needed to lead successfully. For most people, this is essential because if they come from the receiving end of orders (in the most general sense of “orders”), typically they are not used to setting direction and constraints. This tends to be a completely new skill they need to learn.&lt;/p&gt;
    &lt;p&gt;This does not apply only to leading humans but also to leading AI agents. While AI agents are not humans, and thus leadership will be different in detail, the basic skills and tools needed are the same. This is, BTW, one of the reasons why the people who praise agentic AI on LinkedIn and the like are very often managers who lead (human) teams. For them, leading an AI agent fleet feels very natural because it is very close to the work they do every day. However, for the people currently doing the work, leading an AI agent fleet usually does not feel natural at all.&lt;/p&gt;
    &lt;p&gt;However, I have not yet seen anyone receiving any kind of leadership training before being left alone with a fleet of AI agents, and I still see little discussion about the issue. “If it does not work properly, you need better prompts” is the usual response if someone struggles with directing agents successfully.&lt;/p&gt;
    &lt;p&gt;Sorry, but it is not that easy. The issue is much bigger than just optimizing a few prompts. The issue is that people have to change their approach completely to get any piece of work done. Instead of doing it directly, they need to learn how to get it done indirectly. They need to learn how to direct a group of AI agents effectively, how to lead them.&lt;/p&gt;
    &lt;p&gt;This also adds to the training irony of the previous topic. Maybe the AI agent fleets will become good enough in the future that we can omit the proactive part of the work and only need to focus on the reactive part of the work, the monitor-and-intervene part. But until then, we need to teach human supervisors of AI agent fleets how to lead them effectively.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving on&lt;/head&gt;
    &lt;p&gt;We discussed several ironies and paradoxes from Lisanne Bainbridge’s “The ironies of automation” and how they also apply to agentic AI. We looked at the unlearning and recall dilemma and what it means for the next generation of human supervisors. We discussed monitoring fatigue and the status issue. We looked at the UX and UI deficiencies of current AI agents and the training paradox. And we finally looked at the leadership dilemma, which Lisanne Bainbridge did not discuss in her paper but which complements the training paradox.&lt;/p&gt;
    &lt;p&gt;I would like to conclude with the conclusion of Lisanne Bainbridge:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[…] humans working without time-pressure can be impressive problem solvers. The difficulty remains that they are less effective when under time pressure. I hope this paper has made clear both the irony that one is not by automating necessarily removing the difficulties, and also the possibility that resolving them will require even greater technological ingenuity than does classic automation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I could not agree more.&lt;/p&gt;
    &lt;p&gt;I think over time we will become clear on how much “The ironies of automation” also applies to automation done with AI agents and that we cannot ignore the insights known for more than 40 years meanwhile. I am also really curious how the solutions to the ironies and paradoxes will look like.&lt;/p&gt;
    &lt;p&gt;Until then, I hope I gave you a bit of food for thought to ponder. If you should have some good ideas regarding the ironies and how to address them, please do not hesitate to share them with the community. We learn best by sharing and discussing, and maybe your contribution will be a step towards solving the issues discussed …&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262816</guid><pubDate>Sun, 14 Dec 2025 13:19:15 +0000</pubDate></item><item><title>Apple Maps claims it's 29,905 miles away</title><link>https://mathstodon.xyz/@dpiponi/115651419771418748</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46262950</guid><pubDate>Sun, 14 Dec 2025 13:45:41 +0000</pubDate></item><item><title>Illuminating the processor core with LLVM-mca</title><link>https://abseil.io/fast/99</link><description>&lt;doc fingerprint="e30b0622655c5f48"&gt;
  &lt;main&gt;&lt;p&gt;Originally posted as Fast TotW #99 on September 29, 2025&lt;/p&gt;&lt;p&gt;Updated 2025-10-07&lt;/p&gt;&lt;p&gt;Quicklink: abseil.io/fast/99&lt;/p&gt;&lt;p&gt;The RISC versus CISC debate ended in a draw: Modern processors decompose instructions into micro-ops handled by backend execution units. Understanding how instructions are executed by these units can give us insights on optimizing key functions that are backend bound. In this episode, we walk through using &lt;code&gt;llvm-mca&lt;/code&gt; to analyze
functions and identify performance insights from its simulation.&lt;/p&gt;&lt;p&gt;&lt;code&gt;llvm-mca&lt;/code&gt;, short for Machine Code Analyzer, is a tool within LLVM. It uses the
same datasets that the compiler uses for making instruction scheduling
decisions. This ensures that improvements made to compiler optimizations
automatically flow towards keeping &lt;code&gt;llvm-mca&lt;/code&gt; representative. The flip side is
that the tool is only as good as LLVM’s internal modeling of processor designs,
so certain quirks of individual microarchitecture generations might be omitted.
It also models the processor behavior statically, so cache
misses, branch mispredictions, and other dynamic properties aren’t considered.&lt;/p&gt;&lt;p&gt;Consider Protobuf’s &lt;code&gt;VarintSize64&lt;/code&gt; method:&lt;/p&gt;&lt;quote&gt;size_t CodedOutputStream::VarintSize64(uint64_t value) { #if PROTOBUF_CODED_STREAM_H_PREFER_BSR // Explicit OR 0x1 to avoid calling absl::countl_zero(0), which // requires a branch to check for on platforms without a clz instruction. uint32_t log2value = (std::numeric_limits&amp;lt;uint64_t&amp;gt;::digits - 1) - absl::countl_zero(value | 0x1); return static_cast&amp;lt;size_t&amp;gt;((log2value * 9 + (64 + 9)) / 64); #else uint32_t clz = absl::countl_zero(value); return static_cast&amp;lt;size_t&amp;gt;( ((std::numeric_limits&amp;lt;uint64_t&amp;gt;::digits * 9 + 64) - (clz * 9)) / 64); #endif }&lt;/quote&gt;&lt;p&gt;This function calculates how many bytes an encoded integer will consume in Protobuf’s wire format. It first computes the number of bits needed to represent the value by finding the log2 size of the input, then approximates division by 7. The size of the input can be calculated using the &lt;code&gt;absl::countl_zero&lt;/code&gt; function. However this has two possible
implementations depending on whether the processor has a &lt;code&gt;lzcnt&lt;/code&gt; (Leading Zero
Count) instruction available or if this operation needs to instead leverage the
&lt;code&gt;bsr&lt;/code&gt; (Bit Scan Reverse) instruction.&lt;/p&gt;&lt;p&gt;Under the hood of &lt;code&gt;absl::countl_zero&lt;/code&gt;, we need to check whether the argument is
zero, since &lt;code&gt;__builtin_clz&lt;/code&gt; (Count Leading Zeros) models the behavior of x86’s
&lt;code&gt;bsr&lt;/code&gt; (Bit Scan Reverse) instruction and has unspecified behavior if the input
is 0. The &lt;code&gt;| 0x1&lt;/code&gt; avoids needing a branch by ensuring the argument is non-zero
in a way the compiler can follow.&lt;/p&gt;&lt;p&gt;When we have &lt;code&gt;lzcnt&lt;/code&gt; available, the compiler optimizes &lt;code&gt;x == 0 ? 32 :
__builtin_clz(x)&lt;/code&gt; in &lt;code&gt;absl::countl_zero&lt;/code&gt; to &lt;code&gt;lzcnt&lt;/code&gt; without branches. This makes
the &lt;code&gt;| 0x1&lt;/code&gt; unnecessary.&lt;/p&gt;&lt;p&gt;Compiling this gives us two different assembly sequences depending on whether the &lt;code&gt;lzcnt&lt;/code&gt; instruction is available or not:&lt;/p&gt;&lt;p&gt;&lt;code&gt;bsr&lt;/code&gt; (&lt;code&gt;-march=ivybridge&lt;/code&gt;):&lt;/p&gt;&lt;quote&gt;orq $1, %rdi bsrq %rdi, %rax leal (%rax,%rax,8), %eax addl $73, %eax shrl $6, %eax&lt;/quote&gt;&lt;p&gt;&lt;code&gt;lzcnt&lt;/code&gt; (&lt;code&gt;-march=haswell&lt;/code&gt;):&lt;/p&gt;&lt;quote&gt;lzcntq %rdi, %rax leal (%rax,%rax,8), %ecx movl $640, %eax subl %ecx, %eax shrl $6, %eax&lt;/quote&gt;&lt;p&gt;We can now use Compiler Explorer to run these sequences through &lt;code&gt;llvm-mca&lt;/code&gt; and get an analysis of how they would execute on a
simulated Skylake processor (&lt;code&gt;-mcpu=skylake&lt;/code&gt;) for a single invocation
(&lt;code&gt;-iterations=1&lt;/code&gt;) and include &lt;code&gt;-timeline&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;&lt;code&gt;bsr&lt;/code&gt; (&lt;code&gt;-march=ivybridge&lt;/code&gt;):&lt;/p&gt;&lt;quote&gt;Iterations: 1 Instructions: 5 Total Cycles: 10 Total uOps: 5 Dispatch Width: 6 uOps Per Cycle: 0.50 IPC: 0.50 Block RThroughput: 1.0 Timeline view: Index 0123456789 [0,0] DeER . . orq $1, %rdi [0,1] D=eeeER . bsrq %rdi, %rax [0,2] D====eER . leal (%rax,%rax,8), %eax [0,3] D=====eER. addl $73, %eax [0,4] D======eER shrl $6, %eax&lt;/quote&gt;&lt;p&gt;&lt;code&gt;lzcnt&lt;/code&gt; (&lt;code&gt;-march=haswell&lt;/code&gt;):&lt;/p&gt;&lt;quote&gt;Iterations: 1 Instructions: 5 Total Cycles: 9 Total uOps: 5 Dispatch Width: 6 uOps Per Cycle: 0.56 IPC: 0.56 Block RThroughput: 1.0 Timeline view: Index 012345678 [0,0] DeeeER . lzcntq %rdi, %rax [0,1] D===eER . leal (%rax,%rax,8), %ecx [0,2] DeE---R . movl $640, %eax [0,3] D====eER. subl %ecx, %eax [0,4] D=====eER shrl $6, %eax&lt;/quote&gt;&lt;p&gt;This can also be obtained via the command line&lt;/p&gt;&lt;code&gt;$ clang file.cpp -O3 --target=x86_64 -S -o - | llvm-mca -mcpu=skylake -iterations=1 -timeline
&lt;/code&gt;&lt;p&gt;There’s two sections to this output, the first section provides some summary statistics for the code, the second section covers the execution “timeline.” The timeline provides interesting detail about how instructions flow through the execution pipelines in the processor. There are three columns, and each instruction is shown on a separate row. The three columns are as follows:&lt;/p&gt;&lt;p&gt;The timeline is counted in cycles. Each instruction goes through several steps:&lt;/p&gt;&lt;code&gt;D&lt;/code&gt; the instruction is dispatched by the processor; modern desktop or server
processors can dispatch many instructions per cycle. Little Arm cores like
the Cortex-A55 used in smartphones are more limited.&lt;code&gt;=&lt;/code&gt; the instruction is waiting to execute. In this case, the instructions
are waiting for the results of prior instructions to be available. In other
cases, there might be a bottleneck in the processor’s backend.&lt;code&gt;e&lt;/code&gt; the instruction is executing.&lt;code&gt;E&lt;/code&gt; the instruction’s output is available.&lt;code&gt;-&lt;/code&gt; the instruction has completed execution and is waiting to be retired.
Instructions generally retire in program order, the order instructions
appear in the program. An instruction will wait to retire until prior ones
have also retired. On some architectures like the Cortex-A55, there is no
&lt;code&gt;R&lt;/code&gt; phase in the timeline as some instructions retire
out-of-order.&lt;code&gt;R&lt;/code&gt; the instruction has been retired, and is no longer occupying execution
resources.&lt;p&gt;The output is lengthy, but we can extract a few high-level insights from it:&lt;/p&gt;&lt;code&gt;lzcnt&lt;/code&gt; implementation is quicker to execute (9 cycles) than the “bsr”
implementation (10 cycles). This is seen under the &lt;code&gt;Total Cycles&lt;/code&gt; summary as
well as the timeline.&lt;code&gt;movl&lt;/code&gt;, the instructions depend
on each other sequentially (&lt;code&gt;E&lt;/code&gt;-finishing to &lt;code&gt;e&lt;/code&gt;-starting vertically
aligning, pairwise, in the timeline view).&lt;code&gt;or&lt;/code&gt; of &lt;code&gt;0x1&lt;/code&gt; delays &lt;code&gt;bsrq&lt;/code&gt;’s input being available by 1 cycle,
contributing to the longer execution cost.&lt;code&gt;movl&lt;/code&gt; starts immediately in the &lt;code&gt;lzcnt&lt;/code&gt; implementation,
it can’t retire until prior instructions are retired, since we retire in
program order.&lt;code&gt;lzcnt&lt;/code&gt; implementation has
higher instruction-level parallelism
(ILP) because
the &lt;code&gt;mov&lt;/code&gt; has no dependencies. This demonstrates that counting instructions
need not tell us the cycle cost.&lt;p&gt;&lt;code&gt;llvm-mca&lt;/code&gt; is flexible and can model other processors:&lt;/p&gt;&lt;code&gt;D&lt;/code&gt; phase of
instructions starting later.&lt;p&gt;When designing microbenchmarks, we sometimes want to distinguish between throughput and latency microbenchmarks. If the input of one benchmark iteration does not depend on the prior iteration, the processor can execute multiple iterations in parallel. Generally for code that is expected to execute in a loop we care more about throughput, and for code that is inlined in many places interspersed with other logic we care more about latency.&lt;/p&gt;&lt;p&gt;We can use &lt;code&gt;llvm-mca&lt;/code&gt; to model execution of the block of code in a tight loop.
By specifying &lt;code&gt;-iterations=100&lt;/code&gt; on the &lt;code&gt;lzcnt&lt;/code&gt; version, we get a very different
set of results because one iteration’s execution can overlap with the next:&lt;/p&gt;&lt;quote&gt;Iterations: 100 Instructions: 500 Total Cycles: 134 Total uOps: 500 Dispatch Width: 6 uOps Per Cycle: 3.73 IPC: 3.73 Block RThroughput: 1.0&lt;/quote&gt;&lt;p&gt;We were able to execute 100 iterations in only 134 cycles (1.34 cycles/element) by achieving high ILP.&lt;/p&gt;&lt;p&gt;Achieving the best performance may sometimes entail trading off the latency of a basic block in favor of higher throughput. Inside of the protobuf implementation of &lt;code&gt;VarintSize&lt;/code&gt;
(protobuf/wire_format_lite.cc),
we use a vectorized version for realizing higher throughput albeit with worse
latency. A single iteration of the loop takes 29 cycles to process 32 elements
(Compiler Explorer) for 0.91 cycles/element,
but 100 iterations (3200 elements) only requires 1217 cycles (0.38
cycles/element - about 3x faster) showcasing the high throughput once setup
costs are amortized.&lt;/p&gt;&lt;p&gt;When we are looking at CPU profiles, we are often tracking when instructions retire. Costs are attributed to instructions that took longer to retire. Suppose we profile a small function that accesses memory pseudo-randomly:&lt;/p&gt;&lt;quote&gt;unsigned Chains(unsigned* x) { unsigned a0 = x[0]; unsigned b0 = x[1]; unsigned a1 = x[a0]; unsigned b1 = x[b0]; unsigned b2 = x[b1]; return a1 | b2; }&lt;/quote&gt;&lt;p&gt;&lt;code&gt;llvm-mca&lt;/code&gt; models memory loads being an L1 hit (Compiler
Explorer): It takes 5 cycles for the value of
a load to be available after the load starts execution. The output has been
annotated with the source code to make it easier to read.&lt;/p&gt;&lt;quote&gt;Iterations: 1 Instructions: 6 Total Cycles: 19 Total uOps: 9 Dispatch Width: 6 uOps Per Cycle: 0.47 IPC: 0.32 Block RThroughput: 3.0 Timeline view: 012345678 Index 0123456789 [0,0] DeeeeeER . . . movl (%rdi), %ecx // ecx = a0 = x[0] [0,1] DeeeeeER . . . movl 4(%rdi), %eax // eax = b0 = x[1] [0,2] D=====eeeeeER . . movl (%rdi,%rax,4), %eax // eax = b1 = x[b0] [0,3] D==========eeeeeER. movl (%rdi,%rax,4), %eax // eax = b2 = x[b1] [0,4] D==========eeeeeeER orl (%rdi,%rcx,4), %eax // eax |= a1 = x[a0] [0,5] .DeeeeeeeE--------R retq&lt;/quote&gt;&lt;p&gt;In this timeline the first two instructions load &lt;code&gt;a0&lt;/code&gt; and &lt;code&gt;b0&lt;/code&gt;. Both of these
operations can happen immediately. However, the load of &lt;code&gt;x[b0]&lt;/code&gt; can only happen
once the value for &lt;code&gt;b0&lt;/code&gt; is available in a register - after a 5 cycle delay. The
load of &lt;code&gt;x[b1]&lt;/code&gt; can only happen once the value for &lt;code&gt;b1&lt;/code&gt; is available after
another 5 cycle delay.&lt;/p&gt;&lt;p&gt;This program has two places where we can execute loads in parallel: the pair &lt;code&gt;a0&lt;/code&gt; and &lt;code&gt;b0&lt;/code&gt; and the pair &lt;code&gt;a1 and b1&lt;/code&gt; (note: &lt;code&gt;llvm-mca&lt;/code&gt; does not correctly
model the memory load uop from &lt;code&gt;orl&lt;/code&gt; for &lt;code&gt;a1&lt;/code&gt; starting). Since the processor
retires instructions in program order we expect the profile weight to appear on
the loads for &lt;code&gt;a0&lt;/code&gt;, &lt;code&gt;b1&lt;/code&gt;, and &lt;code&gt;b2&lt;/code&gt;, even though we had parallel loads in-flight
simultaneously.&lt;/p&gt;&lt;p&gt;If we examine this profile, we might try to optimize one of the memory indirections because it appears in our profile. We might do this by miraculously replacing &lt;code&gt;a0&lt;/code&gt; with a constant (Compiler
Explorer).&lt;/p&gt;&lt;quote&gt;unsigned Chains(unsigned* x) { unsigned a0 = 0; unsigned b0 = x[1]; unsigned a1 = x[a0]; unsigned b1 = x[b0]; unsigned b2 = x[b1]; return a1 | b2; }&lt;/quote&gt;&lt;quote&gt;Iterations: 1 Instructions: 5 Total Cycles: 19 Total uOps: 8 Dispatch Width: 6 uOps Per Cycle: 0.42 IPC: 0.26 Block RThroughput: 2.5 Timeline view: 012345678 Index 0123456789 [0,0] DeeeeeER . . . movl 4(%rdi), %eax [0,1] D=====eeeeeER . . movl (%rdi,%rax,4), %eax [0,2] D==========eeeeeER. movl (%rdi,%rax,4), %eax [0,3] D==========eeeeeeER orl (%rdi), %eax [0,4] .DeeeeeeeE--------R retq&lt;/quote&gt;&lt;p&gt;Even though we got rid of the “expensive” load we saw in the CPU profile, we didn’t actually change the overall length of the critical path that was dominated by the 3 load long “b” chain. The timeline view shows the critical path for the function, and performance can only be improved if the duration of the critical path is reduced.&lt;/p&gt;&lt;p&gt;CRC32C is a common hashing function and modern architectures include dedicated instructions for calculating it. On short sizes, we’re largely dealing with handling odd numbers of bytes. For large sizes, we are constrained by repeatedly invoking &lt;code&gt;crc32q&lt;/code&gt; (x86) or similar every few bytes of the input. By examining
the repeated invocation, we can look at how the processor will execute it
(Compiler Explorer):&lt;/p&gt;&lt;quote&gt;uint32_t BlockHash() { asm volatile("# LLVM-MCA-BEGIN"); uint32_t crc = 0; for (int i = 0; i &amp;lt; 16; ++i) { crc = _mm_crc32_u64(crc, i); } asm volatile("# LLVM-MCA-END" : "+r"(crc)); return crc; }&lt;/quote&gt;&lt;p&gt;This function doesn’t hash anything useful, but it allows us to see the back-to-back usage of one &lt;code&gt;crc32q&lt;/code&gt;’s output with the next &lt;code&gt;crc32q&lt;/code&gt;’s inputs.&lt;/p&gt;&lt;quote&gt;Iterations: 1 Instructions: 32 Total Cycles: 51 Total uOps: 32 Dispatch Width: 6 uOps Per Cycle: 0.63 IPC: 0.63 Block RThroughput: 16.0 Instruction Info: [1]: #uOps [2]: Latency [3]: RThroughput [4]: MayLoad [5]: MayStore [6]: HasSideEffects (U) [1] [2] [3] [4] [5] [6] Instructions: 1 0 0.17 xorl %eax, %eax 1 3 1.00 crc32q %rax, %rax 1 1 0.25 movl $1, %ecx 1 3 1.00 crc32q %rcx, %rax ... Resources: [0] - SKLDivider [1] - SKLFPDivider [2] - SKLPort0 [3] - SKLPort1 [4] - SKLPort2 [5] - SKLPort3 [6] - SKLPort4 [7] - SKLPort5 [8] - SKLPort6 [9] - SKLPort7 Resource pressure per iteration: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] - - 4.00 18.00 - 1.00 - 5.00 6.00 - Resource pressure by instruction: [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] Instructions: - - - - - - - - - - xorl %eax, %eax - - - 1.00 - - - - - - crc32q %rax, %rax - - - - - - - - 1.00 - movl $1, %ecx - - - 1.00 - - - - - - crc32q %rcx, %rax - - - - - - - 1.00 - - movl $2, %ecx - - - 1.00 - - - - - - crc32q %rcx, %rax ... - - - - - - - - 1.00 - movl $15, %ecx - - - 1.00 - - - - - - crc32q %rcx, %rax - - - - - 1.00 - 1.00 1.00 - retq Timeline view: 0123456789 0123456789 0 Index 0123456789 0123456789 0123456789 [0,0] DR . . . . . . . . . . xorl %eax, %eax [0,1] DeeeER . . . . . . . . . crc32q %rax, %rax [0,2] DeE--R . . . . . . . . . movl $1, %ecx [0,3] D===eeeER . . . . . . . . . crc32q %rcx, %rax [0,4] DeE-----R . . . . . . . . . movl $2, %ecx [0,5] D======eeeER . . . . . . . . crc32q %rcx, %rax ... [0,30] . DeE---------------------------------------R . movl $15, %ecx [0,31] . D========================================eeeER crc32q %rcx, %rax&lt;/quote&gt;&lt;p&gt;Based on the “&lt;code&gt;Instruction Info&lt;/code&gt;” table, &lt;code&gt;crc32q&lt;/code&gt; has latency 3 and throughput
1: Every clock cycle, we can start processing a new invocation on port 1 (&lt;code&gt;[3]&lt;/code&gt;
in the table), but it takes 3 cycles for the result to be available.&lt;/p&gt;&lt;p&gt;Instructions decompose into individual micro operations (or “uops”). The resources section lists the processor execution pipelines (often referred to as ports). Every cycle uops can be issued to these ports. There are constraints - no port can take every kind of uop and there is a maximum number of uops that can be dispatched to the processor pipelines every cycle.&lt;/p&gt;&lt;p&gt;For the instructions in our function, there is a one-to-one correspondence so the number of instructions and the number of uops executed are equivalent (32). The processor has several backends for processing uops. From the resource pressure tables, we see that while &lt;code&gt;crc32&lt;/code&gt; must execute on port 1, the &lt;code&gt;movl&lt;/code&gt;
executes on any of ports 0, 1, 5, and 6.&lt;/p&gt;&lt;p&gt;In the timeline view, we see that for our back-to-back sequence, we can’t actually begin processing the 2nd &lt;code&gt;crc32q&lt;/code&gt; for several clock cycles until the
1st &lt;code&gt;crc32q&lt;/code&gt; hasn’t completed. This tells us that we’re underutilizing port 1’s
capabilities, since its throughput indicates that an instruction can be
dispatched to it once per cycle.&lt;/p&gt;&lt;p&gt;If we restructure &lt;code&gt;BlockHash&lt;/code&gt; to compute 3 parallel streams with a simulated
combine function (the code uses a bitwise or as a placeholder for the correct
logic that this approach requires), we can accomplish the same amount of work in
fewer clock cycles (Compiler Explorer):&lt;/p&gt;&lt;quote&gt;uint32_t ParallelBlockHash(const char* p) { uint32_t crc0 = 0, crc1 = 0, crc2 = 0; for (int i = 0; i &amp;lt; 5; ++i) { crc0 = _mm_crc32_u64(crc0, 3 * i + 0); crc1 = _mm_crc32_u64(crc1, 3 * i + 1); crc2 = _mm_crc32_u64(crc2, 3 * i + 2); } crc0 = _mm_crc32_u64(crc0, 15); return crc0 | crc1 | crc2; }&lt;/quote&gt;&lt;quote&gt;Iterations: 1 Instructions: 36 Total Cycles: 22 Total uOps: 36 Dispatch Width: 6 uOps Per Cycle: 1.64 IPC: 1.64 Block RThroughput: 16.0 Timeline view: 0123456789 Index 0123456789 01 [0,0] DR . . . .. xorl %eax, %eax [0,1] DR . . . .. xorl %ecx, %ecx [0,2] DeeeER . . .. crc32q %rcx, %rcx [0,3] DeE--R . . .. movl $1, %esi [0,4] D----R . . .. xorl %edx, %edx [0,5] D=eeeER . . .. crc32q %rsi, %rdx [0,6] .DeE--R . . .. movl $2, %esi [0,7] .D=eeeER . . .. crc32q %rsi, %rax [0,8] .DeE---R . . .. movl $3, %esi [0,9] .D==eeeER . . .. crc32q %rsi, %rcx [0,10] .DeE----R . . .. movl $4, %esi [0,11] .D===eeeER. . .. crc32q %rsi, %rdx ... [0,32] . DeE-----------R.. movl $15, %esi [0,33] . D==========eeeER. crc32q %rsi, %rcx [0,34] . D============eER. orl %edx, %eax [0,35] . D=============eER orl %ecx, %eax&lt;/quote&gt;&lt;p&gt;The implementation invokes &lt;code&gt;crc32q&lt;/code&gt; the same number of times, but the end-to-end
latency of the block is 22 cycles instead of 51 cycles The timeline view shows
that the processor can issue a &lt;code&gt;crc32&lt;/code&gt; instruction every cycle.&lt;/p&gt;&lt;p&gt;This modeling can be evidenced by microbenchmark results for &lt;code&gt;absl::ComputeCrc32c&lt;/code&gt;
(absl/crc/crc32c_benchmark.cc).
The real implementation uses multiple streams (and correctly combines them).
Ablating these shows a regression, validating the value of the technique.&lt;/p&gt;&lt;quote&gt;name CYCLES/op CYCLES/op vs base BM_Calculate/0 5.007 ± 0% 5.008 ± 0% ~ (p=0.149 n=6) BM_Calculate/1 6.669 ± 1% 8.012 ± 0% +20.14% (p=0.002 n=6) BM_Calculate/100 30.82 ± 0% 30.05 ± 0% -2.49% (p=0.002 n=6) BM_Calculate/2048 285.6 ± 0% 644.8 ± 0% +125.78% (p=0.002 n=6) BM_Calculate/10000 906.7 ± 0% 3633.8 ± 0% +300.78% (p=0.002 n=6) BM_Calculate/500000 37.77k ± 0% 187.69k ± 0% +396.97% (p=0.002 n=6)&lt;/quote&gt;&lt;p&gt;If we create a 4th stream for &lt;code&gt;ParallelBlockHash&lt;/code&gt; (Compiler
Explorer), &lt;code&gt;llvm-mca&lt;/code&gt; shows that the overall
latency is unchanged since we are bottlenecked on port 1’s throughput. Unrolling
further adds additional overhead to combine the streams and makes prefetching
harder without actually improving performance.&lt;/p&gt;&lt;p&gt;To improve performance, many fast CRC32C implementations use other processor features. Instructions like the carryless multiply instruction (&lt;code&gt;pclmulqdq&lt;/code&gt; on
x86) can be used to implement another parallel stream. This allows additional
ILP to be extracted by using the other ports of the processor without worsening
the bottleneck on the port used by &lt;code&gt;crc32&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;While &lt;code&gt;llvm-mca&lt;/code&gt; can be a useful tool in many situations, its modeling has
limits:&lt;/p&gt;&lt;p&gt;Memory accesses are modeled as L1 hits. In the real world, we can have much longer stalls when we need to access the L2, L3, or even main memory.&lt;/p&gt;&lt;p&gt;It cannot model branch predictor behavior.&lt;/p&gt;&lt;p&gt;It does not model instruction fetch and decode steps.&lt;/p&gt;&lt;p&gt;Its analysis is only as good as LLVM’s processor models. If these do not accurately model the processor, the simulation might differ from the real processor.&lt;/p&gt;&lt;p&gt;For example, many ARM processor models are incomplete, and &lt;code&gt;llvm-mca&lt;/code&gt; picks
a processor model that it estimates to be a good substitute; this is
generally fine for compiler heuristics, where differences only matter if it
would result in different generated code, but it can derail manual
optimization efforts.&lt;/p&gt;&lt;p&gt;Understanding how the processor executes and retires instructions can give us powerful insights for optimizing functions. &lt;code&gt;llvm-mca&lt;/code&gt; lets us peer into the
processor to let us understand bottlenecks and underutilized resources.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46263530</guid><pubDate>Sun, 14 Dec 2025 15:05:06 +0000</pubDate></item><item><title>Claude Code's DX is too good. And that's a problem</title><link>https://www.bharath.sh/writing/claude-code-dx</link><description>&lt;doc fingerprint="be9475252cdacfa0"&gt;
  &lt;main&gt;
    &lt;p&gt;thinking out loud writing work photos claude code's DX is too good. and that's a problem. Dec 14, 2025 Loved reading this? Subscribe to get more articles like this in your inbox. Subscribe © 2025 thinking out loud · llms.txt&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46263838</guid><pubDate>Sun, 14 Dec 2025 15:43:51 +0000</pubDate></item><item><title>Private Equity Finds a New Source of Profit: Volunteer Fire Departments</title><link>https://www.nytimes.com/2025/12/14/us/fire-department-software-private-equity.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46263903</guid><pubDate>Sun, 14 Dec 2025 15:49:59 +0000</pubDate></item><item><title>Update Now: iOS 26.2 Fixes 20 Security Vulnerabilities, 2 Actively Exploited</title><link>https://www.macrumors.com/2025/12/12/ios-26-2-security-vulnerabilities/</link><description>&lt;doc fingerprint="a6e4cd140936312c"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Update Now: iOS 26.2 Fixes 20+ Security Vulnerabilities&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;Apple today released iOS 26.2, iPadOS 26.2, and macOS 26.2, all of which introduce new features, bug fixes, and security improvements. Apple says that the updates address over 20 vulnerabilities, including two bugs that are known to have been actively exploited.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;There are a pair of WebKit vulnerabilities that could allow maliciously crafted web content to execute code or cause memory corruption. Apple says that the bugs might have been exploited in an attack against targeted individuals on versions of iOS before iOS 26.&lt;/p&gt;
          &lt;quote&gt;
            &lt;p&gt;Processing maliciously crafted web content may lead to arbitrary code execution. Apple is aware of a report that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals on versions of iOS before iOS 26.&lt;/p&gt;
            &lt;p&gt;Processing maliciously crafted web content may lead to memory corruption. Apple is aware of a report that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals on versions of iOS before iOS 26.&lt;/p&gt;
          &lt;/quote&gt;
          &lt;p&gt;One of the WebKit bugs was fixed with improved memory management, while the other was addressed with improved validation.&lt;/p&gt;
          &lt;p&gt;There are several other vulnerabilities that were fixed too, across apps and services. An App Store bug could allow users to access sensitive payment tokens, processing a malicious image file could lead to memory corruption, photos in the Hidden Album could be viewed without authentication, and passwords could be unintentionally removed when remotely controlling a device with FaceTime.&lt;/p&gt;
          &lt;p&gt;Now that these vulnerabilities have been publicized by Apple, even those that were not exploited before might be taken advantage of now. Apple recommends all users update their devices to iOS 26.2, iPadOS 26.2, and macOS Tahoe 26.2 as soon as possible.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple seeded the second iOS 26.2 Release Candidate to developers earlier this week, meaning the update will be released to the general public very soon. Apple confirmed iOS 26.2 would be released in December, but it did not provide a specific date. We expect the update to be released by early next week. iOS 26.2 includes a handful of new features and changes on the iPhone, such as a new...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today released new firmware designed for the AirPods Pro 3 and the prior-generation AirPods Pro 2. The AirPods Pro 3 firmware is 8B30, up from 8B25, while the AirPods Pro 2 firmware is 8B28, up from 8B21. There's no word on what's include in the updated firmware, but the AirPods Pro 2 and AirPods Pro 3 are getting expanded support for Live Translation in the European Union in iOS...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Macworld's Filipe Espósito today revealed a handful of features that Apple is allegedly planning for iOS 26.4, iOS 27, and even iOS 28. The report said the features are referenced within the code for a leaked internal build of iOS 26 that is not meant to be seen by the public. However, it appears that Espósito and/or his sources managed to gain access to it, providing us with a sneak peek...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Google Maps on iOS quietly gained a new feature recently that automatically recognizes where you've parked your vehicle and saves the location for you. Announced on LinkedIn by Rio Akasaka, Google Maps' senior product manager, the new feature auto-detects your parked location even if you don't use the parking pin function, saves it for up to 48 hours, and then automatically removes it once...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today released iOS 26.2, the second major update to the iOS 26 operating system that came out in September, iOS 26.2 comes a little over a month after iOS 26.1 launched. iOS 26.2 is compatible with the iPhone 11 series and later, as well as the second-generation iPhone SE. The new software can be downloaded on eligible iPhones over-the-air by going to Settings &amp;gt;...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple has ordered 22 million OLED panels from Samsung Display for the first foldable iPhone, signaling a significantly larger production target than the display industry had previously anticipated, ET News reports. In the now-seemingly deleted report, ET News claimed that Samsung plans to mass-produce 11 million inward-folding OLED displays for Apple next year, as well as 11 million...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;The AirTag 2 will include a handful of new features that will improve tracking capabilities, according to a new report from Macworld. The site says that it was able to access an internal build of iOS 26, which includes references to multiple unreleased products. Here's what's supposedly coming: An improved pairing process, though no details were provided. AirTag pairing is already...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple is about to release iOS 26.2, the second major point update for iPhones since iOS 26 was rolled out in September, and there are at least 15 notable changes and improvements worth checking out. We've rounded them up below. Apple is expected to roll out iOS 26.2 to compatible devices sometime between December 8 and December 16. When the update drops, you can check Apple's servers for the ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today released macOS Tahoe 26.2, the second major update to the macOS Tahoe operating system that came out in September. macOS Tahoe 26.2 comes five weeks after Apple released macOS Tahoe 26.1. Mac users can download the macOS Tahoe update by using the Software Update section of System Settings. macOS Tahoe 26.2 includes Edge Light, a feature that illuminates your face with soft...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46264101</guid><pubDate>Sun, 14 Dec 2025 16:13:55 +0000</pubDate></item></channel></rss>