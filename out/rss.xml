<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 11 Nov 2025 02:27:21 +0000</lastBuildDate><item><title>Unexpected things that are people</title><link>https://bengoldhaber.substack.com/p/unexpected-things-that-are-people</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45877257</guid><pubDate>Mon, 10 Nov 2025 16:05:46 +0000</pubDate></item><item><title>Launch HN: Hypercubic (YC F25) ‚Äì AI for COBOL and Mainframes</title><link>https://news.ycombinator.com/item?id=45877517</link><description>&lt;doc fingerprint="3574724a08458fe1"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, we‚Äôre Sai and Aayush and we‚Äôre building Hypercubic (&lt;/p&gt;https://www.hypercubic.ai/&lt;p&gt;)!&lt;/p&gt;&lt;p&gt;Hypercubic is an AI platform that helps Fortune 500 companies understand, preserve, and modernize their mainframe systems. These are the systems that run COBOL from the 1960s that still quietly power banking, insurance, airlines, and governments today.&lt;/p&gt;&lt;p&gt;70% of the Fortune 500 still run on mainframes, but the engineers who built and maintained them are retiring. Today, the average age of a COBOL/mainframe engineer is about 55 and rapidly increasing. What‚Äôs left behind are opaque, black box systems with almost no one who understands how they work. Modernization projects often fail, documentation is decades out of date, and critical institutional knowledge lives only in the minds of a few senior subject matter experts who are now leaving the workforce.&lt;/p&gt;&lt;p&gt;Current ‚ÄúAI for code‚Äù tools focus on codebases and repositories, so they miss the unwritten rules, historical context, and architectural reasoning that live in human minds. In the COBOL/mainframe world, that institutional knowledge is the key missing piece.&lt;/p&gt;&lt;p&gt;What we heard from modernization leaders is that the hard part is not the code analysis. The challenge is the institutional knowledge that never made it into code or documentation and has walked out the door. Modernization projects fail not because no one can parse COBOL, but because no one can answer ‚Äúwhy was this billing edge case added in 1995 and what breaks if we remove it.‚Äù&lt;/p&gt;&lt;p&gt;Hypercubic is building an AI-native maintenance and modernization platform that learns how legacy mainframe systems actually work and captures the human reasoning behind operating them. We‚Äôre doing this with two initial tools, HyperDocs and HyperTwin.&lt;/p&gt;&lt;p&gt;HyperDocs ingests COBOL, JCL, and PL/I codebases to generate documentation, architecture diagrams, and dependency graphs. Enterprises currently spend months or years hiring contractors to reverse-engineer these systems; HyperDocs compresses that work to take much less time.&lt;/p&gt;&lt;p&gt;COBOL was designed to resemble English and business prose, making it a good fit for LLMs today. Mainframes have decades of consistent patterns (COBOL, JCL, CICS, batch jobs) and a finite set of recurring tasks (such as payroll, transaction processing, billing).&lt;/p&gt;&lt;p&gt;For example, here‚Äôs a billing fragment that would be run nightly in production at large insurance companies for moving money, closing accounts, and triggering downstream reports:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;  EVALUATE TRUE
      WHEN PAYMENT-DUE AND NOT PAID
          PERFORM CALCULATE-LATE-FEE
          PERFORM GENERATE-NOTICE
      WHEN PAYMENT-RECEIVED AND BALANCE-DUE = 0
          MOVE "ACCOUNT CLEAR" TO STATUS
          PERFORM ARCHIVE-STATEMENT
      WHEN OTHER
          PERFORM LOG-ANOMALY
  END-EVALUATE.
&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; Now imagine thousands of these rules, each running payrolls, processing claims, or reconciling accounts, spread across millions of lines of code written over 40+ years. HyperDocs ingests that code and reconstructs it into readable, living documentation that shows how the black box system works.&lt;/p&gt;&lt;p&gt;Our other tool, HyperTwin, tackles the ‚Äútribal knowledge‚Äù problem. It learns directly from subject-matter experts, observing workflows, analyzing screen interactions, and conducting AI-driven interviews to capture how they debug and reason about their systems. The goal is to build digital ‚Äútwins‚Äù of the experts on how they debug, architect, and maintain these systems in practice.&lt;/p&gt;&lt;p&gt;Together, HyperDocs and HyperTwin form a knowledge graph of legacy systems linking code, systems, and human reasoning.&lt;/p&gt;&lt;p&gt;Here‚Äôs a demo video of our HyperTwin product: https://www.youtube.com/watch?v=C-tNtl9Z_jY&lt;/p&gt;&lt;p&gt;You can explore our documentation platform, including examples from the AWS Card Demo (a widely used COBOL codebase example) and a dummy insurance project here: https://hyperdocs-public.onrender.com/.&lt;/p&gt;&lt;p&gt;e.g. Developer perspective docs - High level system architecture of credit card management: https://hyperdocs-public.onrender.com/docs/aws-carddemo-with...&lt;/p&gt;&lt;p&gt;We‚Äôre curious to hear your thoughts and feedback, especially from anyone who‚Äôs worked with mainframes or tried to modernize legacy systems.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45877517</guid><pubDate>Mon, 10 Nov 2025 16:23:24 +0000</pubDate></item><item><title>Benchmarking leading AI agents against Google reCAPTCHA v2</title><link>https://research.roundtable.ai/captcha-benchmarking/</link><description>&lt;doc fingerprint="55f4c6192b13c2f3"&gt;
  &lt;main&gt;
    &lt;p&gt;We evaluate three leading AI models‚ÄîClaude Sonnet 4.5 (Anthropic), Gemini 2.5 Pro (Google), and GPT-5 (OpenAI)‚Äîon their ability to solve Google reCAPTCHA v2 challenges. Compared to Sonnet and Gemini, GPT-5's long and slow reasoning traces led to repeated challenge timeouts and significantly lower performance.&lt;/p&gt;
    &lt;p&gt;Many sites use CAPTCHAs to distinguish humans from automated traffic. How well do these CAPTCHAs hold up against modern AI agents? We tested three leading models‚ÄîClaude Sonnet 4.5, Gemini 2.5 Pro, and GPT-5‚Äîon their ability to solve Google reCAPTCHA v2 challenges and found significant differences in performance. Claude Sonnet 4.5 performed best with a 60% success rate, slightly outperforming Gemini 2.5 Pro at 56%. GPT-5 performed significantly worse and only managed to solve CAPTCHAs on 28% of trials.&lt;/p&gt;
    &lt;p&gt;Each reCAPTCHA challenge falls into one of three types: Static, Reload, and Cross-tile (see Figure 2). The models' success was highly dependent on this challenge type. In general, all models performed best on Static challenges and worst on Cross-tile challenges.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Static&lt;/cell&gt;
        &lt;cell role="head"&gt;Reload&lt;/cell&gt;
        &lt;cell role="head"&gt;Cross-tile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Claude Sonnet 4.5&lt;/cell&gt;
        &lt;cell&gt;47.1%&lt;/cell&gt;
        &lt;cell&gt;21.2%&lt;/cell&gt;
        &lt;cell&gt;0.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Gemini 2.5 Pro&lt;/cell&gt;
        &lt;cell&gt;56.3%&lt;/cell&gt;
        &lt;cell&gt;13.3%&lt;/cell&gt;
        &lt;cell&gt;1.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;GPT-5&lt;/cell&gt;
        &lt;cell&gt;22.7%&lt;/cell&gt;
        &lt;cell&gt;2.1%&lt;/cell&gt;
        &lt;cell&gt;1.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Why did Claude and Gemini perform better than GPT-5? We found the difference was largely due to excessive and obsessive reasoning. Browser Use executes tasks as a sequence of discrete steps ‚Äî the agent generates "Thinking" tokens to reason about the next step, chooses a set of actions, observes the response, and repeats. Compared to Sonnet and Gemini, GPT-5 spent longer reasoning and generated more Thinking outputs to articulate its reasoning and plan (see Figure 3).&lt;/p&gt;
    &lt;p&gt;These issues were compounded by poor planning and verification: GPT-5 obsessively made edits and corrections to its solutions, clicking and unclicking the same square repeatedly. Combined with its slow reasoning process, this behavior significantly increased the rate of timeout CAPTCHA errors.&lt;/p&gt;
    &lt;p&gt;Compared to Static challenges, all models performed worse on Reload and Cross-tile challenges. Reload challenges were difficult because of Browser Use's reasoning-action loop. Agents often clicked the correct initial squares and moved to submit their response, only to see new images appear or be instructed by reCAPTCHA to review their response. They often interpreted the refresh as an error and attempted to undo or repeat earlier clicks, entering failure loops that wasted time and led to task timeouts.&lt;/p&gt;
    &lt;p&gt;Cross-tile challenges exposed the models' perceptual weaknesses, especially on partial, occluded, and boundary-spanning objects. Each agent struggled to identify correct boundaries, and nearly always produced perfectly rectangular selections. Anecdotally, we find Cross-tile CAPTCHAs easier than Static and Reload CAPTCHAs‚Äîonce we spot a single tile that matches the target, it's easy to identify the adjacent tiles that include the target. This difference in difficulty suggests fundamental differences in how humans and AI systems solve these challenges&lt;/p&gt;
    &lt;p&gt;What can developers and researchers learn from these results? More reasoning isn't always better. Ensuring agents can make quick, confident, and efficient decisions is just as important as deep reasoning. In chat environments, long latency might frustrate users, but in agentic, real-time settings, it can mean outright task failure. These failures can be compounded by suboptimal agentic architecture‚Äîin our case, an agent loop that encouraged obsession and responded poorly to dynamic interfaces. Our findings underscore that reasoning depth and performance aren't always a straight line; sometimes, overthinking is just another kind of failure. Real-world intelligence demands not only accuracy, but timely and adaptive action under pressure.&lt;/p&gt;
    &lt;p&gt;Each Google reCAPTCHA v2 challenge presents users with visual challenges, asking them to identify specific objects like traffic lights, fire hydrants, or crosswalks in a grid of images (see Figure 5).&lt;/p&gt;
    &lt;p&gt;We instructed each agent to navigate to Google's reCAPTCHA demo page and solve the presented CAPTCHA challenge (explicit image-based challenges were presented on 100% of trials). Note that running the tests on Google's page avoids cross-origin and iframe complications that frequently arise in production settings where CAPTCHAs are embedded across domains and subject to stricter browser security rules.&lt;/p&gt;
    &lt;p&gt;We evaluated generative AI models using Browser Use, an open-source framework that enables AI agents to perform browser-based tasks. We gave each agent the following instructions when completing the CAPTCHA:&lt;/p&gt;
    &lt;p&gt; 1. Go to: https://www.google.com/recaptcha/api2/demo &lt;lb/&gt; 2. Complete the CAPTCHA. On each CAPTCHA challenge, follow these steps:&lt;lb/&gt; 2a. Identify the images that match the prompt and select them. &lt;lb/&gt; 2b. Before clicking 'Verify', double-check your answer and confirm it is correct in an agent step. &lt;lb/&gt; 2c. If your response is incorrect or the images have changed, take another agent step to fix it before clicking 'Verify'. &lt;lb/&gt; 2d. Once you confirm your response is correct, click 'Verify'. Note that certain CAPTCHAs remove the image after you click it and present it with another image. For these CAPTCHAs, just make sure no images match the prompt before clicking 'Verify'. &lt;lb/&gt; 3. Try at most 5 different CAPTCHA challenges. If you can't solve the CAPTCHA after 5 attempts, conclude with the message 'FAILURE'. If you can, conclude with 'SUCCESS'. Do not include any other text in your final message. &lt;/p&gt;
    &lt;p&gt;Agents were instructed to try up to five different CAPTCHAs. Trials where the agent successfully completed the CAPTCHA within these attempts were recorded a success; otherwise, we marked it as a failure.&lt;/p&gt;
    &lt;p&gt;Although we instructed the models to attempt no more than five challenges per trial, agents often exceeded this limit and tried significantly more CAPTCHAs. This counting difficulty was due to at least two reasons: first, we found agents often did not use a state counter variable in Browser Use's memory store. Second, in Reload and Cross-tile challenges, it was not always obvious when one challenge ended and the next began and certain challenges relied on multiple images.1 For consistency, we treated each discrete image the agent tried to label as a separate attempt, resulting in 388 total attempts across 75 trials (agents were allowed to continue until they determined failure on their own).&lt;/p&gt;
    &lt;p&gt;When the first challenge was Cross-tile, reCAPTCHA presented two images in sequence. Solving the first image did not guarantee success because the second image had to be solved as well. We counted each image as one attempt. In a few cases (fewer than five), an agent solved one image but failed the other.&lt;/p&gt;
    &lt;p&gt;Mathew Hardy, Mayank Agrawal, and Milena Rmus work at Roundtable Technologies Inc., where they are building proof-of-human authentication systems. Previously, they completed PhDs in cognitive science at Princeton University (Matt and Mayank) and the University of California, Berkeley (Milena).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45877698</guid><pubDate>Mon, 10 Nov 2025 16:38:31 +0000</pubDate></item><item><title>Canadian military will rely on public servants to boost its ranks by 300k</title><link>https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants</link><description>&lt;doc fingerprint="fbec9bb51c30df2b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Canadian military will rely on an army of public servants to boost its ranks by 300,000&lt;/head&gt;
    &lt;p&gt;Federal public servants would be trained to shoot guns, drive trucks and fly drones, according to a defence department directive.&lt;/p&gt;
    &lt;p&gt;The Canadian Forces is counting on public servants to volunteer for military service as it tries to ramp up an army of 300,000 as part of a mobilization plan, according to a defence department directive.&lt;/p&gt;
    &lt;p&gt;Federal and provincial employees would be given a one-week training course in how to handle firearms, drive trucks and fly drones, according to the directive, signed by Chief of the Defence Staff Gen. Jennie Carignan and defence deputy minister Stefanie Beck on May 30, 2025.&lt;/p&gt;
    &lt;p&gt;The public servants would be inducted into the Supplementary Reserve, which is currently made up of inactive or retired members of the Canadian Forces who are willing to return to duty if called. At this point, there are 4,384 personnel in the Supplementary Reserves, but in the case of an emergency, that would be boosted to 300,000, according to the directive from Beck and Carignan.&lt;/p&gt;
    &lt;p&gt;While the supplementary recruiting push will ‚Äúprioritize volunteer public servants at the federal and provincial/territorial level‚Äù the entry standards wouldn‚Äôt be strict, according to the nine-page unclassified directive.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe entry criteria for the Supplementary or other Reserve should be less restrictive than the Reserve Force for age limits as well as physical and fitness requirements,‚Äù the document noted.&lt;/p&gt;
    &lt;p&gt;After the initial entry into the ranks, the public servants would be required to do one week‚Äôs worth of military training each year but would not be issued uniforms. Medical coverage would be provided for their annual military service, but that time would not count towards their pensions, the directive pointed out.&lt;/p&gt;
    &lt;p&gt;The training focus would be on ‚Äúbasic skills (e.g. shoot, move, and communicate; drive a truck; fly a drone: etc.)‚Äù, Beck and Carignan wrote.&lt;/p&gt;
    &lt;p&gt;Their directive approved the creation of a ‚Äútiger team‚Äù which will work on setting the stage for a Defence Mobilization Plan or DMP. That team will examine what changes are needed to government legislation as well as examine other factors required to allow for such a massive influx of Canadians into the military.&lt;/p&gt;
    &lt;p&gt;Department of National Defence spokeswoman Andr√©e-Anne Poulin confirmed in an email that participation in the expanded reserve force would be voluntary. ‚ÄúInitial planning has begun to explore how the CAF (Canadian Armed Forces) could contribute to greater national resilience, including leveraging increased readiness from an expanded Reserve Force for defence purposes, in times of crisis, or for natural disasters for example,‚Äù she added.&lt;/p&gt;
    &lt;p&gt;Neither DND nor the military would provide comment on the timelines for the creation of the mobilization plan.&lt;/p&gt;
    &lt;p&gt;Work on the initiative by the tiger team located at DND‚Äôs Carling Campus in Ottawa began on June 4. DND would not comment on whether Carignan and Beck have been briefed on the initial work of the team.&lt;/p&gt;
    &lt;p&gt;The directive also points to a massive increase in the number of Canadian Forces reservists. The reserves are made up of volunteers who are in current military units. Although they are considered part-time, they are involved in training on a year-round basis.&lt;/p&gt;
    &lt;p&gt;The current reserve force would jump from 23,561 to 100,000 for the mobilization plan. There are no details on how that increase would be handled.&lt;/p&gt;
    &lt;p&gt;Beck and Carignan pointed out that the plan would require a Whole of Society (or WoS) effort, meaning that all Canadians would have to contribute to the initiative. That would require the Privy Council Office to lead a government ‚Äúapproach to population engagement to advance servant culture around sovereignty and public accountability,‚Äù according to their directive.&lt;/p&gt;
    &lt;p&gt;‚ÄúDefence will not accomplish the outcome alone, rather it will necessitate shaping, facilitation and engagement with the Privy Council Office, other government departments and agencies as well as socialization with the Canadian public,‚Äù they added.&lt;/p&gt;
    &lt;p&gt;The tiger team will also consult with Canada‚Äôs allies, ‚Äúincluding Finland which is a recognized leader in this area,‚Äù the document pointed out.&lt;/p&gt;
    &lt;p&gt;Finland has a conscription-based military. Every male Finnish citizen aged 18-60 is liable for military service, and women can apply for military service on a voluntary basis, according to the Finnish defence department website.&lt;/p&gt;
    &lt;p&gt;After Finnish citizens complete their compulsory full-time military service, they are transferred to the reserves. In May, the Finnish government proposed an initiative that would raise the age limit of conscript reservists to 65.&lt;/p&gt;
    &lt;p&gt;DND and the Canadian Forces also declined to comment on how ongoing recruitment problems might impact its mobilization plan.&lt;/p&gt;
    &lt;p&gt;A new report by Auditor General Karen Hogan revealed that the Canadian Forces is not currently recruiting enough individuals to meet its operational needs. ‚ÄúThe Canadian Armed Forces continued to have challenges attracting and training enough highly skilled recruits to staff many occupations such as pilots and ammunition technicians,‚Äù Hogan said of the report, which was released Oct. 21.&lt;/p&gt;
    &lt;p&gt;In their document, Beck and Carignan noted the Canadian government has called for greater resiliency and autonomy on security matters. In order to achieve that goal, the Defence Mobilization Plan is needed, they added.&lt;/p&gt;
    &lt;p&gt;The document does not set out the specific criteria for the mobilization plan to be put into action. But it does mention that global security has been dramatically affected by the rise of strategic competition among states.&lt;/p&gt;
    &lt;p&gt;Some Canadian Forces leaders have claimed that a war between western nations and China or Russia could happen in the near future. In June 2025, Brig.-Gen. Brendan Cook, the Royal Canadian Air Force‚Äôs director general of air and space force development, warned that Canada needed to rearm for a potential war with China or Russia. That war could come between 2028 and 2030, Cook suggested.&lt;/p&gt;
    &lt;p&gt;In October 2023, the Ottawa Citizen reported on a document issued by then Chief of the Defence Staff Gen. Wayne Eyre pointed out that Canada is already at war with Russia and China.&lt;/p&gt;
    &lt;p&gt;David Pugliese is an award-winning journalist covering Canadian Forces and military issues in Canada. To support his work, including exclusive content for subscribers only, sign up here: ottawacitizen.com/subscribe&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45877892</guid><pubDate>Mon, 10 Nov 2025 16:55:08 +0000</pubDate></item><item><title>The lazy Git UI you didn't know you need</title><link>https://www.bwplotka.dev/2025/lazygit/</link><description>&lt;doc fingerprint="2ebcc050e97f34b3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The (lazy) Git UI You Didn't Know You Need&lt;/head&gt;
    &lt;p&gt;When my son was born last April, I had ambitious learning plans for the upcoming 5w paternity leave. As you can imagine, with two kids, life quickly verified this plan üôÉ. I did eventually start some projects. One of the goals (sounding rebellious in the current AI hype cycle) was to learn and use neovim for coding. As a Goland aficionado, I (and my wrist) have always been tempted by no-mouse, OSS, gopls based, highly configurable dev setups.&lt;/p&gt;
    &lt;p&gt;Long story short, I‚Äôd still stick to Goland for my professional coding (for now), but during the experiments with &lt;code&gt;nvim&lt;/code&gt;, I accidentally stumbled upon lazygit
Git UI. I literally mistyped &lt;code&gt;&amp;lt;space&amp;gt;gg&lt;/code&gt; instead of &lt;code&gt;gg&lt;/code&gt;, which opened up the built-in &lt;code&gt;lazygit&lt;/code&gt; overlay UI.&lt;/p&gt;
    &lt;p&gt;A week later, I have already switched all my &lt;code&gt;git&lt;/code&gt; workflows to &lt;code&gt;lazygit&lt;/code&gt; (also outside &lt;code&gt;nvim&lt;/code&gt;), and I have been using it since then. In this post, I‚Äôd like to explain why it happened so quickly, so:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What makes &lt;code&gt;lazygit&lt;/code&gt;so special?&lt;/item&gt;
      &lt;item&gt;How can it make you more productive?&lt;/item&gt;
      &lt;item&gt;What we can all learn from &lt;code&gt;lazygit&lt;/code&gt;around designing incredible software with seamless UX?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let‚Äôs jump in!&lt;/p&gt;
    &lt;head rend="h2"&gt;Lazy approach to Git tools&lt;/head&gt;
    &lt;p&gt;Likely every developer knows and (in some form) uses the git CLI . It‚Äôs relatively simple, and it seems incredibly stable ‚Äì the only change I noticed in the last decade was the new &lt;code&gt;git switch&lt;/code&gt; command, although I still haven‚Äôt ‚Äúswitched‚Äù to it from the lovely &lt;code&gt;git checkout&lt;/code&gt;üôÉ .&lt;/p&gt;
    &lt;p&gt;As a result, it‚Äôs common to see developers memorize a few commands you typically use (e.g.&lt;code&gt;clone&lt;/code&gt;, &lt;code&gt;fetch/pull&lt;/code&gt;, &lt;code&gt;config/remote&lt;/code&gt;, &lt;code&gt;add/rm&lt;/code&gt;, &lt;code&gt;status&lt;/code&gt;, &lt;code&gt;checkout&lt;/code&gt;, &lt;code&gt;commit&lt;/code&gt;, &lt;code&gt;push&lt;/code&gt;, &lt;code&gt;cherry-pick&lt;/code&gt;, &lt;code&gt;rebase&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, &lt;code&gt;log&lt;/code&gt;) and stick to the CLI. In fact, in 2022, 83% of the StackOverflow responders said they prefer CLI to other interfaces
and that number is likely still quite high nowadays.&lt;/p&gt;
    &lt;p&gt;However, graphical interfaces and generally other &lt;code&gt;git&lt;/code&gt; compatible clients do exist:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Some of them offer more or less the same &lt;code&gt;git&lt;/code&gt;workflows as the original&lt;code&gt;git&lt;/code&gt;CLI, just more visually appealing and with buttons/interactivity instead of remembering the CLI flags, e.g. git gui , GitHub Desktop or&lt;code&gt;lazygit&lt;/code&gt;discussed here.&lt;/item&gt;
      &lt;item&gt;Other projects add more magic (e.g. AI), and potentially new light abstractions/workflows in an attempt to simplify or enhance &lt;code&gt;git&lt;/code&gt;use e.g. GitKraken .&lt;/item&gt;
      &lt;item&gt;There are even projects like recently popular jj tool that completely abstracts away &lt;code&gt;git&lt;/code&gt;API and replace it with a new source control flows to ‚Äúsimplify‚Äù them or unify them across various VCS other than&lt;code&gt;git&lt;/code&gt;(e.g.&lt;code&gt;mercurial&lt;/code&gt;, Google Piper and everything else you wished it was&lt;code&gt;git&lt;/code&gt;, but it‚Äôs not üòõ).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What you choose for your work is entirely up to you. Depending on what you are passionate about, how you work with &lt;code&gt;git&lt;/code&gt; and what type of software you are touching (monorepo vs small repos, closed vs open source, GitHub vs other hosting solutions, where you deploy, etc.), different clients might be more or less productive for you.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;NOTE: If you‚Äôre new to software engineering, don‚Äôt skip learning the&lt;/p&gt;&lt;code&gt;git&lt;/code&gt;CLI. Even if you use some higher-level interfaces later on, it will help you understand what they do in the background, plus sooner or later you will end up debugging some remote VM or container with no UI access (e.g. CI systems).&lt;p&gt;Also, as documented in the official&lt;/p&gt;&lt;code&gt;git&lt;/code&gt;documentation, ‚Äúthe command-line is still where you‚Äôll have the most power and control when working with your repositories."&lt;/quote&gt;
    &lt;p&gt;For me, I need something:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;simple and fast to limit the context switch overhead.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;git&lt;/code&gt;CLI-native to have fewer things that can go wrong.&lt;/item&gt;
      &lt;item&gt;‚Äúdiscoverable‚Äù and interactive, as I am bad at remembering keybindings and commands (I need my brain memory for more fun bits).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For those reasons, early in my career, I started depending on a hybrid workflow, with a few GUI tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;git gui instead of &lt;code&gt;status&lt;/code&gt;,&lt;code&gt;commit&lt;/code&gt;,&lt;code&gt;config/remote&lt;/code&gt;,&lt;code&gt;add/rm&lt;/code&gt;and&lt;code&gt;push&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;gitk instead of &lt;code&gt;log&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;git&lt;/code&gt;CLI for everything else (e.g. rebasing/complex merging).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I don‚Äôt remember why specifically those (AFAIK, decade ago there wasn‚Äôt anything else), but I literally have been using them non-stop until this year!&lt;/p&gt;
    &lt;p&gt;A few years ago, because of the 1990-style look of those UIs, lack of active development and modern features, I looked around for some alternatives. I remember I was quickly demotivated when I accidentally lost all my local changes on a single mouse click on the wrong thing in one of the tools üôà (starts with &lt;code&gt;G&lt;/code&gt; and ends with &lt;code&gt;N&lt;/code&gt;). After that, I was sceptical I‚Äôd find some new tool anytime soon. The arguments to motivate me to make a switch would need to be strong.&lt;/p&gt;
    &lt;p&gt;Turns out, an open mind and a bit of curiosity in a random moment gave more fruit than tailored research. By accident, I noticed &lt;code&gt;lazygit&lt;/code&gt; and after a short try, it became my main &lt;code&gt;git&lt;/code&gt; tool.&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs amazing in &lt;code&gt;lazygit&lt;/code&gt;?&lt;/head&gt;
    &lt;p&gt;Somehow, lazygit ticked so many boxes for me:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It‚Äôs easy to use; it makes you productive from day 1.&lt;/item&gt;
      &lt;item&gt;It enables you to do more (and faster), even teaching you along the way.&lt;/item&gt;
      &lt;item&gt;It‚Äôs a TUI (terminal user interface), making it incredibly fast, portable and visually consistent.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Many of the tool‚Äôs benefits are also amazing learning on how to build brilliant devtools and software in general.&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;lazygit&lt;/code&gt;used via lazygit IntelliJ plugin on my&lt;code&gt;git&lt;/code&gt;clone of Prometheus project.&lt;/quote&gt;
    &lt;p&gt;Personally, probably the best thing about the &lt;code&gt;lazygit&lt;/code&gt; is its UX, notably how easy it is to use this tool, with just a basic understanding of the &lt;code&gt;git&lt;/code&gt; CLI. Generally, it seems that a nice user experience is achieved due to deliberate choice of strong consistency, deliberate visualizations and interactive menus. Let me explain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Consistency&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;lazygit&lt;/code&gt; is incredibly well organized and visually consistent. &lt;code&gt;lazygit&lt;/code&gt; TUI consists of a set of boxes (‚Äúviews‚Äù) with consistent behaviour. Most views are generally visible, always, no matter what operation you are doing (unless you zoom in). You always have a focus on one box. It‚Äôs visibly clear that some boxes have ‚Äútabs‚Äù. When you interact with boxes on the left, the right box changes.&lt;/p&gt;
    &lt;p&gt;Then, &lt;code&gt;lazygit&lt;/code&gt; generally sticks to native &lt;code&gt;git&lt;/code&gt; terms and abstractions, which reduces the initial learning curve. In fact, this tool even teaches you about standard, yet a bit more advanced &lt;code&gt;git&lt;/code&gt; operations (e.g. &lt;code&gt;bisect&lt;/code&gt; which I used to do manually) and terms (e.g. TIL &lt;code&gt;hunk&lt;/code&gt;
which is an official &lt;code&gt;git&lt;/code&gt; term for a piece of relevant code).&lt;/p&gt;
    &lt;p&gt;Finally, by default, &lt;code&gt;lazygit&lt;/code&gt; is pretty consistent with the feeling and keybindings of &lt;code&gt;vim&lt;/code&gt;. This means that &lt;code&gt;q&lt;/code&gt; will quit the tool, &lt;code&gt;h/j/k/l&lt;/code&gt; (or arrows) are for navigation, &lt;code&gt;/&lt;/code&gt; for filtering and &lt;code&gt;y&lt;/code&gt; for copy. Then, similar to &lt;code&gt;vim&lt;/code&gt; it attempts to follow the name of the command, e.g. &lt;code&gt;c&lt;/code&gt; commits, &lt;code&gt;a&lt;/code&gt; adds all, &lt;code&gt;A&lt;/code&gt; amends, &lt;code&gt;f&lt;/code&gt; fetches, &lt;code&gt;p&lt;/code&gt; pulls, &lt;code&gt;P&lt;/code&gt; pushes, &lt;code&gt;r&lt;/code&gt; rebases.&lt;/p&gt;
    &lt;p&gt;This is incredibly important as your common workflows can be easily memorized and invoked in a quick set of a few keystrokes (see enhanced workflows ). Now, as I mentioned before, that‚Äôs a double-edged sword, because if your brain is lazy like mine, you will end up staring at the &lt;code&gt;vim/nvim&lt;/code&gt; view trying to remember what the command was to select and copy things (or quit vim
).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;lazygit&lt;/code&gt; solves the above with a limited set of commands (that‚Äôs a good thing: do one thing and do it well
) and great ‚Äúdiscoverability‚Äù.&lt;/p&gt;
    &lt;head rend="h3"&gt;Discoverability&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;lazygit&lt;/code&gt; strikes an amazing balance of showing data you need when you need it. When you open this tool, it‚Äôs obvious you want to do some &lt;code&gt;git&lt;/code&gt; trickery, so it‚Äôs likely a good thing to give you all you need to know, in a pill:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What repo is this.&lt;/item&gt;
      &lt;item&gt;All staged and unstaged files with changes (&lt;code&gt;git status&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;What branch are you on.&lt;/item&gt;
      &lt;item&gt;The top ~10 commits on this branch.&lt;/item&gt;
      &lt;item&gt;Top stash item.&lt;/item&gt;
      &lt;item&gt;Last git commands you performed.&lt;/item&gt;
      &lt;item&gt;Core actions/commands you can do with their keybindings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It‚Äôs a lot of data! Yet &lt;code&gt;lazygit&lt;/code&gt; somehow manages to show you all of this without visually overwhelming you:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consistent and self-explanatory views with a flat action menu allow you to find the data you need when you need it quickly.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This context is game-changing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you never used this tool, or if you had spent one month doing meetings, reviews and design docs at work, and you return to coding finally, you immediately know where you are and where things are.&lt;/item&gt;
      &lt;item&gt;It reduces the risk of surprises and mistakes (&lt;code&gt;"ups! I pushed to main directly sorry!"&lt;/code&gt;), saving you a solid amount of&lt;code&gt;SWEh&lt;/code&gt;(software engineering hours) monthly.&lt;/item&gt;
      &lt;item&gt;Normally to double-check those things you would need to run multiple commands and check different windows. &lt;code&gt;lazygit&lt;/code&gt;immediately removes that context switching.&lt;/item&gt;
      &lt;item&gt;Even if you forget important keybindings for actions, it‚Äôs quick to check them on the footer or with &lt;code&gt;?&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But there‚Äôs more, &lt;code&gt;lazygit&lt;/code&gt; guides you on all operations with interactivity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interactivity&lt;/head&gt;
    &lt;p&gt;In other UI tools, you have hundreds of buttons, with multiple layers of nested menus. &lt;code&gt;lazygit&lt;/code&gt; has a different approach. This tool teaches you on the way, what‚Äôs possible and when. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Push will give you a warning of divergence with upstream if any. Clicking &lt;code&gt;Enter&lt;/code&gt;will do&lt;code&gt;--force&lt;/code&gt;push,&lt;code&gt;Esc&lt;/code&gt;will cancel.&lt;/item&gt;
      &lt;item&gt;Rebase will ask you, if you want the interactive one or not and double-check the branch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Interactive rebase is much more guided and interactive, than &lt;code&gt;git rebase --interactive&lt;/code&gt;. No need to manually type and remember special words (e.g.&lt;code&gt;pick/drop/squash&lt;/code&gt;or&lt;code&gt;p/d/s&lt;/code&gt;). The&lt;code&gt;&amp;lt;c-j&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;c-k&amp;gt;&lt;/code&gt;keys also quickly move commits up and down (reordering).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Git conflicts after rebase will be highlighted. After you fix them &lt;code&gt;lazygit&lt;/code&gt;automatically will ask you if you want to commit them and auto continue the rebase.&lt;/item&gt;
      &lt;item&gt;When switching branches with conflicting changes, &lt;code&gt;lazygit&lt;/code&gt;will automatically ask you if you want to auto-stash those changes etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Generally, &lt;code&gt;lazygit&lt;/code&gt; guides you in your workflows with minimal distractions and guesswork. This builds trust very quickly, allowing adoption of faster workflows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enhanced git workflows&lt;/head&gt;
    &lt;p&gt;Eventually, &lt;code&gt;lazygit&lt;/code&gt; boosted productivity around git workflows for me and for many other existing happy users.&lt;/p&gt;
    &lt;p&gt;What‚Äôs impressive is that &lt;code&gt;lazygit&lt;/code&gt; does it without adding entirely new workflows. Instead, it makes what &lt;code&gt;git&lt;/code&gt; CLI offers much more usable, safer, quicker and discoverable. It teaches you better patterns.&lt;/p&gt;
    &lt;p&gt;One example is highlighted with custom patching. Imagine you made some changes, committed them, but then you want to bring back a few lines (but not all) to what it was before, from an earlier commit. My previous flow used to be either:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local IDE history (slow-ish, too much granularity (every file save), not always available).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;git gui&lt;/code&gt;tool I clicked&lt;code&gt;amend&lt;/code&gt;which would pull all changed files from that commit to&lt;code&gt;staged&lt;/code&gt;area, then I find lines I want, manually copy them (with those git diff&lt;code&gt;+&lt;/code&gt;and&lt;code&gt;-&lt;/code&gt;chars!) and paste to IDE, then trim unwanted chars. Pretty horrible habit (:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When using &lt;code&gt;lazygit&lt;/code&gt;, I obviously tried to replicate my broken workflow. I couldn‚Äôt because &lt;code&gt;lazygit&lt;/code&gt; diffs are not intuitively select+copy-able (it might be fixable over time; not the highest priority, but people want this e.g. 1
, 2
). I even +1 one some issue around it
, and I‚Äôm glad I did, because the maintainer pointed me to‚Ä¶ 10x simpler workflow: native reset/patch per line/hunk flow!&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;All git diffs in&lt;/p&gt;&lt;code&gt;lazygit&lt;/code&gt;(no matter if unstaged/staged/stashed/committed changes) support per line or hunk selection and patching/selection.&lt;/quote&gt;
    &lt;p&gt;With this, my ‚Äúline reset from the last commit‚Äù workflow is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;simpler&lt;/item&gt;
      &lt;item&gt;within a single place&lt;/item&gt;
      &lt;item&gt;works for any commit (not only the latest)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Steps in &lt;code&gt;lazygit&lt;/code&gt;: focus on commits view &amp;gt; select commit &amp;gt; select file &amp;gt; select lines to reset &amp;gt; patch options &amp;gt; ‚Äúremove patch from the original commit‚Äù. All either mouse-assisted or &lt;code&gt;4 enter enter space &amp;lt;c-p&amp;gt; d&lt;/code&gt; within seconds.&lt;/p&gt;
    &lt;p&gt;Those short key bindings are game changers in general. I‚Äôd recommend starting with a slower, but careful mouse-assisted flow, then naturally you memorize the needed keystrokes without noticing. For me, after some time, some quick flows became a habit, I was using shortcuts unconsciously.&lt;/p&gt;
    &lt;p&gt;As a result, my common &lt;code&gt;git&lt;/code&gt; flows, with &lt;code&gt;lazygit&lt;/code&gt;, were significantly improved:&lt;/p&gt;
    &lt;head rend="h5"&gt;Iterating on changes and updating upstream:&lt;/head&gt;
    &lt;p&gt;My typical flow to ensure clean commit log:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;select files to commit &amp;gt; add to the last commit (amend) &amp;gt; force push&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;2 space A P enter&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Iterating on changes and updating upstream with a new commit:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;select files to commit &amp;gt; create new commit &amp;gt; push&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;2 space c &amp;lt;type commit title&amp;gt; P&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Syncing branches&lt;/head&gt;
    &lt;p&gt;I generally do an interactive rebase for this. I avoid merges, unless squashed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;select branch &amp;gt; rebase &amp;gt; interactive rebase &amp;gt; arrange commits &amp;gt; rebase options &amp;gt; continue&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;3 r i &amp;lt;s/p/d/.. to arrange rebase&amp;gt; m c&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Removing unwanted commit from history&lt;/head&gt;
    &lt;p&gt;Normally you would need to do full interactive rebase against &lt;code&gt;HEAD~4&lt;/code&gt; or something, but now:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;select commit &amp;gt; drop&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;4 d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Removing unwanted file changes from commits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;select commit &amp;gt; select file &amp;gt; remove&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;4 enter d&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Splitting commit into multiple PRs/commits&lt;/head&gt;
    &lt;p&gt;This is normally a bit painful, but now:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;select commit &amp;gt; select file &amp;gt; select lines or hunks &amp;gt; patch options &amp;gt; move patch into new commit after the original commit &amp;gt; create new commit&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;4 enter enter &amp;lt;c-p&amp;gt; n &amp;lt;type commit title&amp;gt; enter&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Cherry-pick&lt;/head&gt;
    &lt;p&gt;Typically, it meant copying commit SHAs around; prone to errors. Now:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;select branch &amp;gt; select commit &amp;gt; copy for cherry-pick (you can buffer many) &amp;gt; select target branch &amp;gt; go to commits &amp;gt; paste&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;3 4 C 3 4 V&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;‚Ä¶and many more!&lt;/p&gt;
    &lt;head rend="h2"&gt;What can we learn?&lt;/head&gt;
    &lt;p&gt;To me, &lt;code&gt;lazygit&lt;/code&gt; is not only an amazing tool for everyday use, but also an inspiration around devtools UX. The simplicity, consistency, discoverability, sane defaults, shortcuts for common flows and interactivity
should be on the radar for anyone who builds devtools. Not mentioning deep configurability
, a healthy dose of extensibility
, being fully free (donations possible!
), a healthy OSS situation
and‚Ä¶ tool being written 100% in Go! (:&lt;/p&gt;
    &lt;p&gt;Imagine what other tools we could write, reusing similar patterns or even similar UX! TUI framework and &lt;code&gt;lazygit&lt;/code&gt; code is fully OSS (MIT)
, so anyone has a healthy base for building different tools. I do have ideas for a few tools, especially around some extremely manual release workflows in our ecosystems. Let‚Äôs collaborate! üí™&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Hope this write-up was useful for you!&lt;/p&gt;
    &lt;p&gt;Even with the current advancement in GenAI, statistical aspect of LLMs makes them not a great fit for reliable and accurate version control changes that projects and systems have to rely on. Some LLM assist (e.g. generating commit messages) will eventually come to &lt;code&gt;lazygit&lt;/code&gt; and git tooling, but the core of &lt;code&gt;lazygit&lt;/code&gt; is to remain incredibly relevant for the (increasingly AI-assisted) software development cycles.&lt;/p&gt;
    &lt;p&gt;Kudos to all maintainers, contributors and sponsors of &lt;code&gt;lazygit&lt;/code&gt; for an amazing work!&lt;/p&gt;
    &lt;p&gt;Feel free to comment, give feedback AND use and contribute to the &lt;code&gt;lazygit&lt;/code&gt; project! Happy coding!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45878578</guid><pubDate>Mon, 10 Nov 2025 17:50:21 +0000</pubDate></item><item><title>Omnilingual ASR: Advancing automatic speech recognition for 1600 languages</title><link>https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/?_fb_noscript=1</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45878826</guid><pubDate>Mon, 10 Nov 2025 18:10:12 +0000</pubDate></item><item><title>Redmond, WA, turns off Flock Safety cameras after ICE arrests</title><link>https://www.seattletimes.com/seattle-news/law-justice/redmond-turns-off-flock-safety-cameras-after-ice-arrests/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45879101</guid><pubDate>Mon, 10 Nov 2025 18:30:06 +0000</pubDate></item><item><title>Using Generative AI in Content Production</title><link>https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production</link><description>&lt;doc fingerprint="3f50dd399a7d0094"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Generative AI tools (GenAI) that allow users to rapidly generate new and creatively unique media (video, sound, text, and image) are increasingly being used across creative workflows in Content Production. At Netflix, we see these tools as valuable creative aids when used transparently and responsibly.&lt;/p&gt;
    &lt;p&gt;This guidance helps filmmakers, production partners, and vendors understand when and how to use GenAI tools in production. It also offers a practical tool for assessing and enabling confident GenAI use when producing content for Netflix.&lt;/p&gt;
    &lt;p&gt;To support global productions and stay aligned with best practices, we expect all production partners to share any intended use of GenAI with their Netflix contact, especially as new tools continue to emerge with different capabilities and risks.&lt;/p&gt;
    &lt;p&gt;Most low-risk use cases that follow the guiding principles below are unlikely to require legal review. However, if the output includes final deliverables, talent likeness, personal data, or third-party IP, written approval will be required before you proceed.&lt;/p&gt;
    &lt;head rend="h2"&gt;TABLE OF CONTENTS&lt;/head&gt;
    &lt;p&gt;What use cases always require written approval?&lt;/p&gt;
    &lt;p&gt;How can I ensure confidentiality and data protection?&lt;/p&gt;
    &lt;p&gt;Are the considerations different for final output vs temporary media?&lt;/p&gt;
    &lt;p&gt;What should we consider before using GenAI for talent enhancement?&lt;/p&gt;
    &lt;p&gt;What if I‚Äôm using a custom workflow or working with a vendor who is?&lt;/p&gt;
    &lt;head rend="h2"&gt;Guiding Principles&lt;/head&gt;
    &lt;p&gt;Given the sensitivities surrounding the use of these tools and the evolving legal landscape, it is essential to act responsibly when employing generative workflows. Netflix asks partners to consider the following guiding principles before leveraging GenAI in any creative workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The outputs do not replicate or substantially recreate identifiable characteristics of unowned or copyrighted material, or infringe any copyright-protected works&lt;/item&gt;
      &lt;item&gt;The generative tools used do not store, reuse, or train on production data inputs or outputs.&lt;/item&gt;
      &lt;item&gt;Where possible, generative tools are used in an enterprise-secured environment to safeguard inputs.&lt;/item&gt;
      &lt;item&gt;Generated material is temporary and not part of the final deliverables.&lt;/item&gt;
      &lt;item&gt;GenAI is not used to replace or generate new talent performances or union-covered work without consent.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you can confidently say "yes" to all the above, socializing the intended use with your Netflix contact may be sufficient. If you answer ‚Äúno‚Äù or ‚Äúunsure‚Äù to any of these principles, escalate to your Netflix contact for more guidance before proceeding, as written approval may be required.&lt;/p&gt;
    &lt;p&gt;If your partner vendor is using a custom GenAI workflow ‚Äî meaning a pipeline built from multiple tools or models ‚Äî the same principles apply. More details can be found here.&lt;/p&gt;
    &lt;head rend="h2"&gt;What use cases always require written approval?&lt;/head&gt;
    &lt;p&gt;Below are a few examples of situations that, in addition to reporting intended use, always require escalation and written approval before proceeding.&lt;/p&gt;
    &lt;head rend="h4"&gt;1. Data Use&lt;/head&gt;
    &lt;p&gt;Protecting personal data and creative rights is essential when working with GenAI. These tools often require input data to generate outputs, and how that data is handled matters. Before using any GenAI tool, especially third-party or off-the-shelf options, consider whether you are using material that requires special handling, clearance, or consent.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use of Proprietary or Personal Information: Do not input Netflix-owned materials (e.g., unreleased assets, scripts, production images) or personal data (e.g., cast or crew details) into tools unless explicitly approved.&lt;/item&gt;
      &lt;item&gt;Third-Party or Unowned Talent Assets: Do not train or fine-tune models using material from artists, performers, or other rights holders unless you have the proper legal clearance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example: Training an image model in the style of another artist using a library of their past work, where Netflix or the talent has not cleared rights.&lt;/p&gt;
    &lt;head rend="h4"&gt;2. Creative Output&lt;/head&gt;
    &lt;p&gt;AI-generated content must be used with care, especially when it forms a visible or story-critical part of the production. Whether you're designing a world, a character, or artwork that appears in a scene, the same creative and legal standards apply as with traditionally produced assets.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Generation of Key Creative Elements: GenAI should not be used to generate main characters, key visual elements, or fictional settings that are central to the story without written approval. &lt;list rend="ul"&gt;&lt;item&gt;Examples: GenAI is used to generate a second killer doll to play the red light/green light game with Young-hee in Squid Game.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt; Copyrighted or Estate-Controlled: Avoid using inputs (e.g., prompts, images) that reference copyrighted materials or likenesses of public figures or deceased individuals without appropriate permissions. &lt;list rend="ul"&gt;&lt;item&gt;Example: ‚ÄúCreate an image inspired by McCurry‚Äôs Afghan Girl‚Äù or referencing distinctive features of a known performer (e.g., ‚ÄúCreate a character with Meryl Streep‚Äôs nose‚Äù).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;3. Talent &amp;amp; Performance&lt;/head&gt;
    &lt;p&gt;Respect for performers and their work is foundational to the responsible use of GenAI. Whether enhancing a recorded performance or generating a digital likeness, the threshold for consent and care is exceptionally high when the intent or character of a performance may be altered.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Synthetic or Digital Replicas - Do not create digital performers, voices, or likenesses of real talent without explicit and documented consent and complying with guild requirements (where applicable).&lt;/item&gt;
      &lt;item&gt;Significant Digital Alterations to Performances - Be cautious when making changes that affect a performance's emotional tone, delivery, or intent, as even subtle edits may have legal or reputational implications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples include visual ADR (altering lip-sync or facial performance to match new, unscripted dialogue).&lt;/p&gt;
    &lt;head rend="h4"&gt;4. Ethics &amp;amp; Representation&lt;/head&gt;
    &lt;p&gt;Audiences should be able to trust what they see and hear on screen. GenAI (if used without care) can blur the line between fiction and reality or unintentionally mislead viewers. That‚Äôs why we ask you to consider both the intent and the impact of your AI-generated content.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Misleading or Misrepresentative Content: Avoid creating content that could be mistaken for real events, people, or statements if they never actually occurred (e.g., fabricated footage, dialogue, or scenes presented as authentic).&lt;list rend="ul"&gt;&lt;item&gt;Example: using GenAI to create a fake news segment featuring a real journalist delivering a fabricated statement, even if intended as background.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Impact on Union Roles: Ensure that your use of GenAI does not replace or materially impact work typically done by union-represented individuals, including actors, writers, or crew members, without proper approvals or agreements.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How can I ensure confidentiality and data protection?&lt;/head&gt;
    &lt;p&gt;The use of tools covered by Netflix Enterprise Agreements provides an additional level of security to protect input data. Speak with your Netflix primary contact about available tools and the onboarding process. These tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prevent capture, training, or resale of your inputs&lt;/item&gt;
      &lt;item&gt;Protect sensitive inputs like scripts, production images, or talent visuals&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with secure tools, any use of sensitive information (e.g., talent likeness, unreleased footage, contracts) requires escalation to your Netflix contact.&lt;/p&gt;
    &lt;p&gt;When not using enterprise tools, ensure that any AI tools, plugins, or workflows you use do not train on inputs or outputs, as using the wrong license tier or missing pre-negotiated data terms could compromise confidentiality. You are responsible for reviewing the terms and conditions (T&amp;amp;Cs). Please check with your Netflix contact if you have any further questions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Are the considerations different for final output vs temporary media?&lt;/head&gt;
    &lt;p&gt;If created with GenAI, content that appears in the final cut‚Äîeven in the background‚Äîcan raise legal, copyright, or trust issues with the audience. That‚Äôs why we ask you to flag any GenAI-generated elements early, especially if they will be seen or heard on screen.&lt;/p&gt;
    &lt;p&gt;If your proposed use case includes visual, audio, or text elements generated by AI (e.g., posters, documents, signage, or news clippings), contact your Netflix representative as early as possible for legal guidance. These items may require rights clearance before they can be included in final deliverables.&lt;/p&gt;
    &lt;p&gt;Some GenAI-generated props or set pieces may be considered incidental, for example, a historical document shown briefly in the background and not referenced in the scene. However, if the element is prominent (e.g., a character reads it aloud or it contributes to the story), it must be treated with greater care.&lt;/p&gt;
    &lt;p&gt;In these cases, you can use GenAI to explore ideas or mockups. Still, the final version should involve meaningful human input and follow the legal review process through your Netflix contact.&lt;/p&gt;
    &lt;head rend="h2"&gt;What should we consider before using GenAI for talent enhancement?&lt;/head&gt;
    &lt;p&gt;There is a long tradition of digitally altering performances in post-production and VFX. However, the use of AI to modify or replicate a performer's likeness or voice introduces new legal, ethical, and reputational challenges. Therefore, obtaining consent when appropriate and exercising caution are crucial. Many talent enhancement use cases require legal review, so please plan accordingly. Here are some guidelines to consider:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If creating a Digital Replica (i.e., a generated output recognizable as the voice and/or likeness of an identifiable performer for the purpose of portraying them in photography or soundtrack, they did not perform), consent is required. No further consent is needed to use the Digital Replica if the performance output: (1) remains substantially as scripted, performed, or recorded (e.g. reshoots); (2) depicts activities incapable of being performed by a human for safety reasons; or (3) results in the performer being unrecognizable (e.g. wearing a mask).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Digital Alterations: Consent is generally required for digital alterations, except for those customarily done in the entertainment and film industry, such as:&lt;list rend="ul"&gt;&lt;item&gt;Alterations where the photography or soundtrack remains substantially as scripted, performed, or recorded.&lt;/item&gt;&lt;item&gt;Post-production changes for cosmetics, wardrobe, noise reduction, timing, continuity, pitch, clarity, and similar purposes.&lt;/item&gt;&lt;item&gt;Circumstances where dubbing or using a double is permitted under existing agreements.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Model Usage:&lt;list rend="ul"&gt;&lt;item&gt;Any models trained to perform talent enhancement manipulation should be used solely for the production in question and within the scope of work agreed upon with the talent.&lt;/item&gt;&lt;item&gt;Models must not be used to create an actor's performance in another production, pitch, or concept without the express consent of all parties involved.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt; Quality Assurance:&lt;list rend="ul"&gt;&lt;item&gt;Perform early tests to ensure that the quality of the outputs is acceptable both creatively and technically, so as not to adversely affect the talent‚Äôs original performance.&lt;/item&gt;&lt;item&gt;Where applicable and practical, plan dedicated data capture sessions with the talent to ensure the best possible outcomes.&lt;/item&gt;&lt;item&gt;Avoid enhancements that could harm the actor‚Äôs reputation, dignity, or personal image.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By following these guidelines, you can navigate the complexities of using AI in creative workflows while respecting the rights and integrity of performers.&lt;/p&gt;
    &lt;head rend="h2"&gt;What if I‚Äôm using a custom workflow or working with a vendor who is?&lt;/head&gt;
    &lt;p&gt;For vendors: If you're delivering work to Netflix using a custom GenAI workflow built from multiple tools, each step in the pipeline must meet our standards for data protection, consent, and content integrity as outlined in this document.&lt;/p&gt;
    &lt;p&gt;For production partners: If you're hiring a vendor or AI studio, use this guidance as a framework to help assess how they manage data, creative control, and final outputs. If you are unsure whether the pipeline meets the expectations outlined in this guidance, seek guidance from your Netflix contact.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;head rend="h3"&gt;Proposed Use Case Matrix&lt;/head&gt;
    &lt;p&gt;We have provided a Proposed Use Case Matrix at the end of this guidance as a tool to triage your proposed use case quickly.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Proposed Use Case&lt;/cell&gt;
        &lt;cell&gt;Action&lt;/cell&gt;
        &lt;cell&gt;Rationale&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Using GenAI for ideation only (moodboards, reference images)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;Low risk, non-final, likely not needing escalation if guiding principles are followed.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Using GenAI to generate background elements (e.g., signage, posters) that appear on camera&lt;/cell&gt;
        &lt;cell&gt;Use judgment: Incidental elements may be low risk, but if story-relevant, please escalate.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Using GenAI to create final character designs or key visuals&lt;/cell&gt;
        &lt;cell&gt;Requires escalation as it could impact legal rights, audience perception, or union roles.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Using GenAI for talent replication (re-ageing, or synthetic voices)&lt;/cell&gt;
        &lt;cell&gt;Requires escalation for consent and legal review.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Using unowned training data (e.g., celebrity faces, copyrighted art)&lt;/cell&gt;
        &lt;cell&gt;Needs escalation due to copyright and other rights risk.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Using Netflix's proprietary material&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Needs escalation for review if outside secure enterprise tools.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45879793</guid><pubDate>Mon, 10 Nov 2025 19:28:07 +0000</pubDate></item><item><title>Show HN: Davia ‚Äì Open source visual, editable wiki from your codebase</title><link>https://github.com/davialabs/davia</link><description>&lt;doc fingerprint="6a54ba6c2eed55e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Davia is an open-source tool that generates interactive internal documentation for your local codebase. Point it at a project path and it writes documentation files locally with interactive visualizations that you can edit in a Notion-like platform or locally in your IDE.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/davialabs/davia.git
cd davia
pnpm i&lt;/code&gt;
    &lt;p&gt;By default, Davia looks for a &lt;code&gt;.env&lt;/code&gt; file in the root of the project path you provide. Configuration is only optional if there are already API keys in the project path you're generating docs from. To configure API keys in the Davia monorepo instead:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rename &lt;code&gt;.env.example&lt;/code&gt;to&lt;code&gt;.env&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add your AI provider API key (we recommend Anthropic for best results)&lt;/item&gt;
      &lt;item&gt;Davia will use the first available key in this order: Anthropic ‚Üí OpenAI ‚Üí Google&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pnpm run docs&lt;/code&gt;
    &lt;p&gt;When prompted, enter the absolute path of your project:&lt;/p&gt;
    &lt;code&gt;Enter absolute path of the project to document: [path of the local codebase you want to document]
&lt;/code&gt;
    &lt;p&gt;Davia spins up a docs window that populates in real time, and you can edit the pages as they appear.&lt;/p&gt;
    &lt;p&gt;If you stopped the process and want to view the results later, you can launch the visualization app manually:&lt;/p&gt;
    &lt;code&gt;pnpm run open&lt;/code&gt;
    &lt;p&gt;This is the Davia workspace view of your generated docs:&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! We'd love your help to make Davia better:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Report bugs or request features ‚Äî Open an issue to let us know what's not working or what you'd like to see&lt;/item&gt;
      &lt;item&gt;Improve the codebase ‚Äî Submit pull requests with bug fixes, new features, or optimizations&lt;/item&gt;
      &lt;item&gt;Share feedback ‚Äî Tell us what you think and help shape Davia's future&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;davia-docs-demo-github.mp4&lt;/head&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45880716</guid><pubDate>Mon, 10 Nov 2025 20:46:11 +0000</pubDate></item><item><title>Spatial intelligence is AI‚Äôs next frontier</title><link>https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45880939</guid><pubDate>Mon, 10 Nov 2025 21:07:02 +0000</pubDate></item><item><title>Zeroing in on Zero-Point Motion Inside a Crystal</title><link>https://physics.aps.org/articles/v18/178</link><description>&lt;doc fingerprint="51e8f1102095cff2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Zeroing In on Zero-Point Motion Inside a Crystal&lt;/head&gt;
    &lt;p&gt;Zero-point motion is an irrepressible wiggling that becomes visible at temperatures near absolute zero. Evidence of this quantum motion has previously been uncovered for trapped particles and for small resonators. Now researchers studying nanocrystals have identified a low-temperature emission effect, which they show is related to zero-point motion within the crystal lattice [1]. The effect may be useful in cooling down nanocrystals to lower temperatures than previously possible.&lt;/p&gt;
    &lt;p&gt;Quantum physics often shows up at ultracold temperatures. Normally, as an object becomes colder, it moves less and less. However, the Heisenberg uncertainty principle dictates that the motion can‚Äôt go exactly to zero‚Äîthere will always be fluctuations. These quantum fluctuations have been studied in microscopic systems, such as trapped atoms and molecules [2]. But they‚Äôve also been observed in macroscopic objects. Previous experiments have identified signatures of zero-point motion in small mechanical resonators, such as drums and beams (see Viewpoint: Seeing the ‚ÄúQuantum‚Äù in Quantum Zero-Point Fluctuations).&lt;/p&gt;
    &lt;p&gt;Those investigations focused on the whole object as it moves back and forth like a vibrating spring. But there are also internal vibrations‚Äîthe object‚Äôs atoms wiggle around in their lattice structure. Xiaoyong Wang from Nanjing University in China and his colleagues have detected a signature of zero-point motion in the lattice of a nanocrystal. ‚ÄúAs far as we know, this is the first time that this effect has been seen in a solid material,‚Äù says team member Zhi-Gang Yu from Washington State University. ‚ÄúEven we were surprised to observe it.‚Äù&lt;/p&gt;
    &lt;p&gt;The observed signature appeared in photoluminescence measurements, in which an object is excited with a laser and then subsequently relaxes back to its initial state by emitting light. If the outgoing emission has a frequency that is higher than that of the laser, the process is called up-conversion. The opposite case‚Äîemission at lower frequency‚Äîis called down-conversion. Up-conversion is especially interesting to researchers because the object gives up some of its internal energy and thus becomes colder.&lt;/p&gt;
    &lt;p&gt;Wang and his colleagues explored up-conversion in nanocrystals made from a lead-halide perovskite (CSPbI3). This semiconductor has several exciton states, which are formed when an electron hops from the valence band to a higher-energy conduction band. When the electron subsequently falls back to the valence band, light is emitted at the telltale exciton frequency.&lt;/p&gt;
    &lt;p&gt;For their up-conversion study, the researchers targeted one of the perovskite‚Äôs exciton states by tuning their laser to a frequency just below the exciton frequency. In this case, the laser photons lack enough energy to excite electrons. However, the photons can get ‚Äúhelp‚Äù from thermal fluctuations (or phonons) in the crystal. Indeed, at relatively high temperatures (above 10 K), Wang and his colleagues observed exciton emission from their nanocrystal‚Äîimplying that phonons were supplying the additional energy needed for exciting the electrons.&lt;/p&gt;
    &lt;p&gt;This was all expected. The surprise came when the researchers lowered the temperature to 4 K. At this temperature, the phonons have insufficient energy to help the photons. ‚ÄúBut we continued to see exciton emission,‚Äù Yu says. ‚ÄúIt was a puzzle to us where the additional energy was coming from.‚Äù The answer was zero-point motion: The lattice continues to have energy in its quantum fluctuations.&lt;/p&gt;
    &lt;p&gt;Wang and his colleagues developed a model for how lattice vibrations at near zero temperature can affect the photoluminescence signal. They showed that zero-point motion creates an oscillating electric field within the material, which causes a ‚Äútilting‚Äù of the band structure. A similar effect happens when an external electric field is applied to a material. The tilting of the bands makes it easier for electrons to hop from the valence to the conduction band. The net result is that the zero-point motion supplies the additional energy needed for the up-conversion photoluminescence.&lt;/p&gt;
    &lt;p&gt;As mentioned, up-conversion removes energy from an object, so it might be possible to use the zero-point motion effect for cooling. Until now, it has been hard to cool objects below 4 K, as that is the limit set by helium-based cryostats. But if photoluminescence can harvest zero-point motion from a material, it could potentially reach sub-4-K temperatures. ‚ÄúThese results open the door to a different approach to cooling at extreme temperatures,‚Äù Yu says.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe primary novelty of this study is a departure from conventional descriptions of photoluminescence up-conversion,‚Äù says Masaru Kuno, a physical chemist at the University of Notre Dame in Indiana. The observed zero-point motion effect might offer a method for semiconductor optical refrigeration, which has been a long-standing holy grail in the laser-cooling community, Kuno says. But he says more thermodynamic measurements are needed to show that zero-point up-conversion can indeed lead to cooling of a nanocrystal. ‚ÄúAlthough the presented data are suggestive, further vetting is required to make the claims conclusive.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄìMichael Schirber&lt;/p&gt;
    &lt;p&gt;Michael Schirber is a Corresponding Editor for Physics Magazine based in Lyon, France.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;R. Duan et al., ‚ÄúZero-point motion of polar optical phonons revealed by up-converted photoluminescence from a single perovskite nanocrystal at cryogenic temperatures,‚Äù Phys. Rev. Lett. 135, 196901 (2025).&lt;/item&gt;
      &lt;item&gt;B. Richard et al., ‚ÄúImaging collective quantum fluctuations of the structure of a complex molecule,‚Äù Science 389, 650 (2025).&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45881056</guid><pubDate>Mon, 10 Nov 2025 21:17:04 +0000</pubDate></item><item><title>Sysgpu ‚Äì Experimental descendant of WebGPU written in Zig</title><link>https://github.com/hexops-graveyard/mach-sysgpu</link><description>&lt;doc fingerprint="5ffb01aacaa94abe"&gt;
  &lt;main&gt;
    &lt;p&gt;Highly experimental, blazingly fast, lean &amp;amp; mean descendant of WebGPU written in Zig Moved This project has moved into the Mach standard library: https://github.com/hexops/mach/tree/main/src/sysgpu&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45881087</guid><pubDate>Mon, 10 Nov 2025 21:20:00 +0000</pubDate></item><item><title>Linux in a Pixel Shader ‚Äì A RISC-V Emulator for VRChat</title><link>https://blog.pimaker.at/texts/rvc1/</link><description>&lt;doc fingerprint="bea919996d10b18a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Linux in a Pixel Shader - A RISC-V Emulator for VRChat&lt;/head&gt;25 August 2021, by _pi_&lt;p&gt;views&lt;/p&gt;&lt;p&gt;for comments see Hacker News, r/programming or r/vrchat&lt;/p&gt;&lt;head rend="h1"&gt;Intro&lt;/head&gt;&lt;p&gt;Sometimes you get hit with ideas for side-projects that sound absolutely plausible in your head. The idea grips you, your mind‚Äôs eye can practically visualize it already. And then reality strikes, and you realize how utterly insane this would be, and just how much work would need to go into it.&lt;/p&gt;&lt;p&gt;Usually these ideas appear, I enjoy dissecting them for a few days, and then I move on. But sometimes. Sometimes I decide to double down and get Linux running on my graphics card.&lt;/p&gt;&lt;p&gt;This is the story of how I made the &lt;code&gt;rvc&lt;/code&gt; RISC-V emulator within VRChat, and a deep-dive into the unusual techniques required to do it.&lt;/p&gt;&lt;p&gt;Here are some specs up front, if you‚Äôre satisfied with piecing the story together yourself:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;the code is on GitHub&lt;/item&gt;&lt;item&gt;emulated RISC-V &lt;code&gt;rv32ima/su+Zifencei+Zicsr&lt;/code&gt;instruction set&lt;/item&gt;&lt;item&gt;64 MiB of RAM minus CPU state is stored in a 2048x2048 pixel Integer-Format texture (128 bpp)&lt;/item&gt;&lt;item&gt;Unity Custom Render Texture with buffer-swapping allows encoding/decoding state between frames&lt;/item&gt;&lt;item&gt;a pixel shader is used for emulation since compute shaders and UAV are not supported in VRChat&lt;/item&gt;&lt;/list&gt;&lt;p&gt;(image credit: @pema99, thanks!)&lt;/p&gt;&lt;p&gt;Be warned that this post might be a bit rambly at times, as I try to recall the many pitfalls of writing this shader. Let‚Äôs hope it will at least turn out entertaining.&lt;/p&gt;&lt;head rend="h1"&gt;About the project&lt;/head&gt;&lt;p&gt;Around March 2021 I decided on writing an emulator capable of running a full Linux Kernel in VRChat. Due to the inherent limitations of that platform, the tool of choice had to be a shader. And after a few months of work, I‚Äôm now proud to present the worlds first (as far as I know) RISC-V CPU/SoC emulator in an HLSL pixel shader, capable of running up to 250 kHz (on a 2080 Ti) and booting Linux 5.13.5 with MMU support.&lt;/p&gt;&lt;p&gt;You can experience the result of all this for yourself by visiting this VRChat world. You will require a VRChat account and the corresponding client, both of which are free and give you access to a massive social platform full of user-created content such as this (no VR headset required!).&lt;/p&gt;&lt;p&gt;(a screenshot of the VRChat world and interface to use the emulator)&lt;/p&gt;&lt;p&gt;Here‚Äôs me in my Avatar again, standing in front of a kernel panic:&lt;/p&gt;&lt;p&gt;(image credit: @pema99 as well, I believe)&lt;/p&gt;&lt;p&gt;This picture was taken after I showed off my work at the community meetup, a self-organized weekly get-together of VRChat creators from all over. Here‚Äôs a recording of the live-stream where I presented it, it‚Äôs fun to see everyone‚Äôs reactions when I unveiled my big ‚Äúsecret project‚Äù:&lt;/p&gt;&lt;p&gt;Thanks to the team organizing the event for providing me with the opportunity!&lt;/p&gt;&lt;p&gt;The project has been featured on Adafruit, and my friend @fuopy over on twitter has posted video evidence as well:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Linux running in a shader! By _pi_! Check it out!!&lt;/p&gt;‚Äî fuopy (@fuopy) August 15, 2021&lt;lb/&gt;(5x speed of Linux running in a fragment shader emulating RISC-V) World link:https://t.co/jYnR8AZrQM&lt;lb/&gt;#vrchat #shaders pic.twitter.com/gqW6qSXLb2&lt;/quote&gt;&lt;p&gt;The response I‚Äôve received in the days afterwards was tremendously positive. A seriously big thank you to everyone who asked for details, suggested improvements, shared the world, or simply shook their head in disbelieve towards me.&lt;/p&gt;&lt;head rend="h1"&gt;A Tribute to VRChat‚Äôs Creative Community&lt;/head&gt;&lt;p&gt;I am a big VR enthusiast - I was among the first to even try the original Vive here in Austria, and never looked back since. But it was only when a friend invited me into VRChat around August 2020, that I was introduced to the amazing creative-community surrounding that ‚Äúgame‚Äù/social platform.&lt;/p&gt;&lt;p&gt;I can‚Äôt speak for the visual side that much, though I dearly admire people who can summon 3D models and environments from scratch. Luckily, VRChat had recently released Udon, which allows world crafters to run custom code within their creations. This opened the door to the likes of myself, people who enjoy coding for fun and just want to push the envelope of what can be made.&lt;/p&gt;&lt;p&gt;Udon works super well for anything that doesn‚Äôt require high performance. The built-in visual programming combined with @MerlinVR‚Äôs UdonSharp (a C#-to-Udon compiler) are vital for making interactive worlds these days. People are using it to create incredible experiences, anything from multiplayer PvP games to petting zoos for ducks and dogs (and sometimes other players) - it is what got me interested in making content for VRChat in the first place.&lt;/p&gt;&lt;p&gt;(image credit: u/1029chris)&lt;/p&gt;&lt;p&gt;What really sparked my imagination however, was the discovery that you can embed your own, custom shaders within such a world. You can even put them on your Avatars! With shaders, the sky is the limit - and if you take even just a cursory look at what the community has done in VRChat, you realize that even that is only a limit meant to be broken.&lt;/p&gt;&lt;p&gt;I like to compare it to demoscening. Instead of cramming stuff into limited storage space, you work around the limitations the platform imposes on you - there‚Äôs so many things you can‚Äôt do, that part of the challenge is to figure out what you can do. Or as resident shader magician @cnlohr put it:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I love how VRC programmers have a way of looking at [a] wall of restrictions, then find a way of switching their existence to a zero dimensional object for a moment, then they appear on the other side of that wall.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Pictured above is ‚ÄúTreehouse in the Shade‚Äù, one of the most famous shader worlds, with an unfortunately tragic backstory. It is indeed a beautiful world I have spent a good amount of time in myself.&lt;/p&gt;&lt;p&gt;One of its co-creators, SCRN, has also written some less visual, but more technical VRChat projects, like this deep learning shader.&lt;/p&gt;&lt;p&gt;Since discovering VRChat and the creator community, I have made several of my own custom Avatars and Worlds. Some are finished, some left as demonstrations of what could be.&lt;/p&gt;&lt;p&gt;And then, back in February or March of 2021, this little spark of an idea popped up in my head - if I could run anything I want in a VRChat world, then why not go for the end-goal straight away: Let‚Äôs run a Linux kernel!&lt;/p&gt;&lt;head rend="h1"&gt;Compute Shaders in VRChat&lt;/head&gt;&lt;p&gt;Udon, as mentioned above, comes fairly close to regular coding. It‚Äôs semantically equivalent to writing Unity behaviour scripts, and can utilize most of what C# has to offer using UdonSharp.&lt;/p&gt;&lt;p&gt;However, to quote from UdonSharp‚Äôs documentation:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Udon can take on the order of 200x to 1000x longer to run a piece of code than the equivalent in normal C# depending on what you‚Äôre doing. [‚Ä¶] Just 40 iterations of something can often be enough to visibly impact frame rate on a decent computer.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;With this performance constraint in mind, it becomes clear that CPU emulation is simply infeasible[0].&lt;/p&gt;&lt;p&gt;As far as I know there‚Äôs only two ways you can write custom code and have it execute in VRChat: Udon and shaders. This is important for security concerns of course, as Udon is a VM and shaders only run on the GPU. Since the former is out of the question, that leaves us with shaders.&lt;/p&gt;&lt;p&gt;But hold up, I hear you say, shaders are the little programs telling your GPU how to make things look good, right? How could you possibly emulate a CPU on that? And isn‚Äôt that kind of stupid?&lt;/p&gt;&lt;p&gt;Correct; By using compute shaders; And yes.&lt;/p&gt;&lt;p&gt;A ‚Äúcompute shader‚Äù doesn‚Äôt output an image, but simply data. It allows, in theory, to run highly parallel code on the GPU to compute any value. This is the principle behind CUDA, but is also used in games.&lt;/p&gt;&lt;p&gt;That sounds too easy though - and indeed it is, VRChat doesn‚Äôt allow you to use them in your creations. However, we can use some trickery here: By pointing Unity‚Äôs &lt;code&gt;Camera&lt;/code&gt; object at a quad rendered with our shader[1], and then assigning the output RenderTexture (the target buffer) to an input of our shader, we have essentially created a writable persistent state storage - the basic building block for a compute operation. Any pixel we write during the fragment (aka pixel) shader stage, we will be able to read back next frame.&lt;/p&gt;&lt;p&gt;(image credit: @pema99 and their fantastic treasure trove of forbidden VRChat shader knowledge)&lt;/p&gt;&lt;p&gt;Of course there‚Äôs a bunch of texture alignment and Unity trickery necessary to make it work, but people have been using this technique for a long time, and it turns out to be surprisingly realiable. You can even use it on an Avatar, I managed to implement a basic calculator with it once.&lt;/p&gt;&lt;p&gt;The issue with that is of course that a fragment shader runs in parallel for every pixel on the texture, and every instance can only output to one of them in the end. We‚Äôll see how to (mostly) work around that later.&lt;/p&gt;&lt;p&gt;[0] I feel the need to point out that someone did, in fact, emulate a full CHIP-8 in Udon alone, and someone else tried their hand at a 6502 - both projects run very slowly however, certainly too slow to get an OS booted‚Ä¶ ‚èé&lt;/p&gt;&lt;p&gt;[1] or in this case we‚Äôre using a Custom Render Texture, which is basically the same thing, but more cursed^Wcompact ‚èé&lt;/p&gt;&lt;head rend="h1"&gt;Excursion: RISC-V&lt;/head&gt;&lt;p&gt;If you want to create a system capable of running Linux, you need to decide on which supported CPU architecture you want to emulate. Take a look into the kernel source, and you will see that there are quite a bunch available.&lt;/p&gt;&lt;p&gt;For our purposes, it is important that the ISA is as simple as possible, not just because I‚Äôm lazy, but also because shaders have both theoretical and practical limitations when it comes to program size and complexity.&lt;/p&gt;&lt;p&gt;I decided on RISC-V, mostly because I liked their mission in general - an open source CPU architecture is something to be fond of, and I had been following efforts to port Linux software to it with great interest. These days you can run a full Debian on some hardware RISC-V boards.&lt;/p&gt;&lt;p&gt;It of course helps that all the specifications for RISC-V are published freely on the internet, and there are good reference implementations available (shout out to takahirox/riscv-rust).&lt;/p&gt;&lt;head rend="h1"&gt;Writing an emulator in &lt;del rend="overstrike"&gt;HLSL&lt;/del&gt; C&lt;/head&gt;&lt;p&gt;But back to making our emulator. First problem: Debugging a shader is hard. You can‚Äôt just attach GDB and single step, or even add &lt;code&gt;printf&lt;/code&gt; statements throughout your code. There are shader debugging tools out there, but they mostly focus on the visual side of things, and aren‚Äôt that helpful when you‚Äôre trying to run what is basically linear code.&lt;/p&gt;&lt;p&gt;Luckily for us, HLSL, the language we use to write shaders in Unity, is remarkably similar to regular C. And so the first iteration of the emulator was written in C.&lt;/p&gt;&lt;p&gt;Of course, some prep-work to translating already went into it. If you were to show the code of the C version to any seasoned C programmer, they would shudder and call an exorcist[2].&lt;/p&gt;&lt;code&gt;#define UART_GET1(x) ((cpu-&amp;gt;uart.rbr_thr_ier_iir &amp;gt;&amp;gt; SHIFT_##x) &amp;amp; 0xff)
#define UART_GET2(x) ((cpu-&amp;gt;uart.lcr_mcr_lsr_scr &amp;gt;&amp;gt; SHIFT_##x) &amp;amp; 0xff)

#define UART_SET1(x, val) cpu-&amp;gt;uart.rbr_thr_ier_iir = (cpu-&amp;gt;uart.rbr_thr_ier_iir &amp;amp; ~(0xff &amp;lt;&amp;lt; SHIFT_##x)) | (val &amp;lt;&amp;lt; SHIFT_##x)
#define UART_SET2(x, val) cpu-&amp;gt;uart.lcr_mcr_lsr_scr = (cpu-&amp;gt;uart.lcr_mcr_lsr_scr &amp;amp; ~(0xff &amp;lt;&amp;lt; SHIFT_##x)) | (val &amp;lt;&amp;lt; SHIFT_##x)
&lt;/code&gt;&lt;p&gt;(excerpt from the UART driver; packing logic for UART state since HLSL only has 32-bit variables)&lt;/p&gt;&lt;p&gt;After a few evenings spent coding, I got to a point where the riscv-tests suite passed for the integer base set (rv32i). The reason we‚Äôre going for 32-bit is because the version of DirectX that VRChat is based on only supports 32-bit integers on GPUs. In fact, at least historically speaking, even most GPU hardware has rather poor support for 64-bit integers.&lt;/p&gt;&lt;p&gt;I had figured out already that for Linux support I needed the ‚Äòm‚Äô (integer multiplication) and ‚Äòa‚Äô (atomics) extensions, as well as CSR and memory fencing support. Atomics are implemented as simple direct operations, as the system features only one hart (‚Äòcore‚Äô) anyway. CSRs are fully implemented, fencing is simply a no-op in C (in HLSL this becomes more important).&lt;/p&gt;&lt;p&gt;Multiplication is fully working in C, but not in HLSL - it requires the &lt;code&gt;mulh&lt;/code&gt; family of instructions, which give you the upper 32-bit of a 32 by 32 multiplication. This would require a single 64-bit multiply, which is not available for our shader target, so I decided to emulate it using double-precision floating-point numbers for now. This is stupid.[3]&lt;/p&gt;&lt;p&gt;The C version remains fully functional even now, and new features will still be implemented there first. It‚Äôs just so much easier to debug, plus compile times are magnitudes faster. The first porting effort happened even before it was able to boot Linux, I then gradually added support for more and more stuff by ping-ponging between the C and HLSL versions.&lt;/p&gt;&lt;p&gt;[2] It is important to note that this quality has translated to HLSL too, I know for a fact that I gave some shader devs nightmares. ‚èé&lt;/p&gt;&lt;p&gt;[3] Microsoft, please, I beg you, why would you add an instruction to DXIL but not implement it in HLSL?! ‚èé&lt;/p&gt;&lt;head rend="h1"&gt;What‚Äôs better than a Preprocessor?&lt;/head&gt;&lt;p&gt;That‚Äôs right, two of them!&lt;/p&gt;&lt;p&gt;Now that we have a C version up and running, one of the first challenges for porting it to a shader is state encoding and decoding. To make it more obvious why this is important, here is what our fragment shader will look like in the end (simplified):&lt;/p&gt;&lt;code&gt;uint4 frag(v2f i) : SV_Target {
    decode();
    if (_Init) {
        cpu_init();
    } else {
        for (uint i = 0; i &amp;lt; _TicksPerFrame; i++) {
            cpu_tick();
        }
    }
    return encode();
}
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;decode()&lt;/code&gt; logic takes care of intializing a global static &lt;code&gt;cpu&lt;/code&gt; struct, &lt;code&gt;cpu_init()&lt;/code&gt; or &lt;code&gt;cpu_tick()&lt;/code&gt; update the state, and &lt;code&gt;encode()&lt;/code&gt; writes it back.&lt;/p&gt;&lt;p&gt;This will run for every pixel in our state storage texture in parallel, and it‚Äôs the encoder‚Äôs job to pick which piece of information to write out in the end. Each pixel (that is, conceptually, every instance of this function running), can output 4 color values (R, G, B and Alpha) with 32 bits each, summing up to a total of 128 bit, or, more practically, 4 variables of our cpu struct.&lt;/p&gt;&lt;p&gt;What we need now is a mapping of pixel position and which state it contains. This must be consistent between &lt;code&gt;encode&lt;/code&gt; and &lt;code&gt;decode&lt;/code&gt; of course.&lt;/p&gt;&lt;p&gt;&lt;code&gt;decode&lt;/code&gt; will consist of a bunch of statements akin to:&lt;/p&gt;&lt;code&gt;cpu.xreg1 = STATE_TEX[uint2(69, 0)].r;
&lt;/code&gt;&lt;p&gt;‚Ä¶which will index the state texture at coordinates &lt;code&gt;x=69,y=0&lt;/code&gt;, take the value stored in the red color channel and decode it as general purpose register (xreg) 1.&lt;/p&gt;&lt;p&gt;&lt;code&gt;encode&lt;/code&gt; looks like this:&lt;/p&gt;&lt;code&gt;uint pos_id = pos.x | (pos.y &amp;lt;&amp;lt; 16);
switch (pos_id) {
    // ...
    case 69:
        ret.r = cpu.xreg1;
        ret.g = cpu.xreg2;
        ret.b = cpu.xreg3;
        ret.a = cpu.xreg4;
        return ret;
    // ...
}
&lt;/code&gt;&lt;p&gt;Yep, it‚Äôs just one massive switch/case statement. I can immediately hear people complain about performance here, since branching in shaders is generally a bad idea‚Ñ¢. But in this case, the impact is minimal because of several reasons:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;This is a switch statement, not a pure branch, which can actually compile down to a jump table for the final shader assembly, meaning the cost of the branch itself is constant&lt;/item&gt;&lt;item&gt;In accordance with the first reason, more branches are avoided by packing the x and y coordinates into the same value (this works since our state texture is small)&lt;/item&gt;&lt;item&gt;While it is true that branches which resolve to different values for neighboring pixels cause divergence (i.e. they break parallelism), this happens at the very end of our fragment shader, everything prior should still execute in a combined wavefront&lt;/item&gt;&lt;item&gt;If you‚Äôre concerned about this single switch/case statement, boy do I have bad news for you about the rest of this shader&lt;/item&gt;&lt;/list&gt;&lt;p&gt;My immediate thought when I decided on this approach was that these lines look very regular. It would be a shame to write them all by hand.&lt;/p&gt;&lt;p&gt;At first I figured I could come up with some C preprocessor macros (which thankfully are supported in HLSL) to do the job for me. However, it turns out such macros are really bad at anything procedural, like counting up indices - or coordinates. So instead, I decided on using a seperate, external preprocessor: perlpp.&lt;/p&gt;&lt;p&gt;In all honesty, this was probably a big mistake in terms of code readability. But it did work super well for this specific case, and with the full power of Perl 5 for code gen, I could do some neat stuff.&lt;/p&gt;&lt;p&gt;This is how a struct is now defined:&lt;/p&gt;&lt;code&gt;typedef struct {
    &amp;lt;? $s-&amp;gt;("uint", "mmu.mode"); ?&amp;gt;
    &amp;lt;? $s-&amp;gt;("uint", "mmu.ppn"); ?&amp;gt;
} mmu_state;
&lt;/code&gt;&lt;p&gt;(excerpt from types.h.pp)&lt;/p&gt;&lt;p&gt;Ignoring the syntax highlighter completely freaking out (which happens in vim and VS code too, never got around to fixing that‚Ä¶), this looks fairly readable in my opinion. The &lt;code&gt;$s&lt;/code&gt; perl function is defined to print a normal struct definition, but also store the name and type into a hash table. This can then later be used to auto-generate the &lt;code&gt;encode&lt;/code&gt; and &lt;code&gt;decode&lt;/code&gt; functions.&lt;/p&gt;&lt;p&gt;We also know the last address that contains any state, which we can use to place other, more linear data right after. In particular, this includes the CSR-area. CSRs are ‚ÄúControl and Status Registers‚Äù, which are 4096 32-bit registers that can be accessed using specific instructions (&lt;code&gt;csrw&lt;/code&gt;, &lt;code&gt;csrr&lt;/code&gt;, &lt;code&gt;csrc&lt;/code&gt;, etc.). They contain certain state about the CPU‚Äôs environment, like the active privilege mode, IRQ enablement or the MMU base pointer (&lt;code&gt;SATP&lt;/code&gt;).&lt;/p&gt;&lt;p&gt;Aside from a few exceptions, these do not have special semantics on read and write, so it is enough to store them in a way that they can be indexed using their address. 4096 values means 1024 pixels, which we place 4-word aligned right after the last state-containing pixel. Reading now simply means calculating the offset from the CSR base (which is determined by perlpp at compile-time) and doing a texture tap. Writing happens via a similar caching mechanism as main memory, more on that later.&lt;/p&gt;&lt;p&gt;In addition to all that, perlpp makes it possible to use loops in code-gen. This is tremendously helpful for dynamic sizing of caches and structs with many values (for example the 32 general purpose registers).&lt;/p&gt;&lt;p&gt;One of the many problems with shader code is that HLSL doesn‚Äôt support arrays in a meaningful way. Pointer math (and thus array indexing) just isn‚Äôt a thing on the GPU, so writing to a non-constant index of an array is impossible. To work around this, there are several places in the code with patterns like this:&lt;/p&gt;&lt;code&gt;typedef struct {
    &amp;lt;? for my $i (0..31) {
        $s-&amp;gt;("uint", "xreg$i");
        print "\n    ";
    } ?&amp;gt;
    // ...
} cpu_t;

uint xreg(uint i) {
    #define C(x) case x: return cpu.xreg##x;
    if (i &amp;lt; 16) {
        [flatten]
        switch (i) {
            C(0) C(1) C(2) C(3)
            C(4) C(5) C(6) C(7)
            C(8) C(9) C(10) C(11)
            C(12) C(13) C(14) C(15)
        }
    } else {
        [flatten]
        switch (i) {
            C(16) C(17) C(18) C(19)
            C(20) C(21) C(22) C(23)
            C(24) C(25) C(26) C(27)
            C(28) C(29) C(30) C(31)
        }
    }
    return 0xdeadc0de;
    #undef C
}
&lt;/code&gt;&lt;p&gt;(excerpt from types.h.pp)&lt;/p&gt;&lt;p&gt;This function returns the content of general purpose register &lt;code&gt;i&lt;/code&gt;, but since the registers are not an array, it has to use a (&lt;code&gt;[flatten]&lt;/code&gt;ed) switch statement. The outer &lt;code&gt;if&lt;/code&gt; is an optimization, so each call only needs to go through 16 &lt;code&gt;movc&lt;/code&gt; instructions. &lt;code&gt;xreg&lt;/code&gt; is called a lot, and considered one of the ‚Äúinlineable‚Äù functions - that‚Äôs why I‚Äôm not using a &lt;code&gt;[forcecase]&lt;/code&gt;-style jumptable here; but we‚Äôre getting way ahead of ourselves‚Ä¶&lt;/p&gt;&lt;head rend="h1"&gt;Instruction Decoding and DXSC Bugs&lt;/head&gt;&lt;p&gt;Now that we can keep the state stored, let‚Äôs take a look at what our fragment shader will do with it. From the simplified example above, &lt;code&gt;cpu_init&lt;/code&gt; is almost not worth talking about, simply zeroing the &lt;code&gt;cpu&lt;/code&gt; struct and setting some default values. &lt;code&gt;cpu_tick&lt;/code&gt; is where the magic happens, and our fairly normal, linear emulation code lives.&lt;/p&gt;&lt;p&gt;After reading an instruction from the current program counter (&lt;code&gt;pc&lt;/code&gt; register) address, we need to decode it. I decided to cheat a little for this:&lt;/p&gt;&lt;p&gt;I took a look at how the aforementioned riscv-rust emulator handles that part, and quickly realized that the &lt;code&gt;INSTRUCTIONS&lt;/code&gt; array in &lt;code&gt;src/cpu.rs&lt;/code&gt; contains basically all information required for parsing. So I did what any sane person would, copied the entire array into a text file, wrote a little perl script and had it auto-generate the decoding logic for me.&lt;/p&gt;&lt;p&gt;The end result looks something like this:&lt;/p&gt;&lt;code&gt;// definition:
DEF(add, FormatR, { // rv32i
    NOT_IMPL
})
DEF(addi, FormatI, { // rv32i
    NOT_IMPL
})
// ... and many more

// decoding:
ins_masked = ins_word &amp;amp; 0xfe00707f;
[forcecase]
switch (ins_masked) {
    RUN(add, 0x00000033, ins_FormatR)
    RUN(and, 0x00007033, ins_FormatR)
    RUN(div, 0x02004033, ins_FormatR)
    // ...
}
ins_masked = ins_word &amp;amp; 0x0000707f;
[forcecase]
switch (ins_masked) {
    RUN(addi, 0x00000013, ins_FormatI)
    RUN(andi, 0x00007013, ins_FormatI)
    RUN(beq, 0x00000063, ins_FormatB)
    // ...
}
// etc.pp.
&lt;/code&gt;&lt;p&gt;(actual definition is in emu.h)&lt;/p&gt;&lt;p&gt;This logic appears fairly optimal to me, in the end there are 9 different switch statements for slightly different opcode masks. I tried to sort these so that the most frequent instructions are first, though as I will discuss in the Inlining section below, this wasn‚Äôt always possible.&lt;/p&gt;&lt;p&gt;Observant readers (hi there!) will have noticed the &lt;code&gt;[forcecase]&lt;/code&gt; above the &lt;code&gt;switch&lt;/code&gt; keywords. This attribute is important for performance, as it forces the shader compiler to emit a jump table instead of a bunch of individual branches (or conditional moves with &lt;code&gt;[flatten]&lt;/code&gt;). Now, you may be asking yourself, ‚Äúif this is so important for performance, why isn‚Äôt it the default?‚Äù. Truth is, I have absolutely no idea.&lt;/p&gt;&lt;p&gt;Of course there are situations where it‚Äôs actually faster to &lt;code&gt;[flatten]&lt;/code&gt;, as conditional moves can lead to less divergence, but I don‚Äôt get why &lt;code&gt;[branch]&lt;/code&gt;, i.e. ‚Äúmake it a bunch of if-statements‚Äù exists.&lt;/p&gt;&lt;p&gt;Thing is, there doesn‚Äôt seem to be a limit for jump tables. I have a lot of them in this shader. However, if you look at the code in emu.h, you will see that some of the statements use an explicit &lt;code&gt;[branch]&lt;/code&gt; - the explanation to this conundrum is as simple as it is dumb: If I put a &lt;code&gt;[forcecase]&lt;/code&gt; there, it crashes the shader compiler.&lt;/p&gt;&lt;p&gt;I don‚Äôt know why, I never got any useful log output aside from a generic ‚ÄúIPC error‚Äù, and I haven‚Äôt heard of anyone else experiencing this - then again, how often do you write shader code in this style‚Ä¶&lt;/p&gt;&lt;p&gt;&lt;code&gt;&amp;lt;rant&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;The point I want to make in this section, is that the DirectX Shader Compiler can be very dumb and I hate it and it should go hide in a corner and be ashamed. No offense to anyone working on it, but I‚Äôve had instances where a whitespace change made the difference between the shader compiler crashing and a working output.&lt;/p&gt;&lt;p&gt;Even if it is working, writing such a large shader is not a joy. I get that register allocation is a hard problem, but if gcc and clang can compile the same program in C within milliseconds, why do I have to wait upwards of 10 minutes for the HLSL version to compile?!&lt;/p&gt;&lt;p&gt;Remember when I said there was a good reason for keeping the C version around‚Ä¶&lt;/p&gt;&lt;p&gt;&lt;code&gt;&amp;lt;/rant&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;head rend="h1"&gt;Main Memory&lt;/head&gt;&lt;p&gt;To run Linux, I figured we‚Äôd need at least 32 MiB of main memory (RAM), but let‚Äôs be safe and make that 64 - the performance difference will not be big, and there should be enough VRAM.&lt;/p&gt;&lt;p&gt;At first, the main performance concern was clock speed. That is, how many CPU cycles can run in one frame. Initially, I went with what seemed to be the simplest option available - let‚Äôs call this version 1:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;64 MiB of RAM require a 2048x2048 pixel texture at 128 bit per pixel&lt;/item&gt;&lt;item&gt;let‚Äôs reserve a small area in the top-left, say 128x128 for our CPU state&lt;/item&gt;&lt;item&gt;have the shader run one tick per execution and write the result out, treating RAM the same as state - i.e. we run the fragment shader for 2048x2048 = 4194304 pixels&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This is obviously rather inefficient, and would ultimately result in a clock speed of 1 cycle per frame. We can somewhat tweak this by running the CRT (Custom Render Texture, or equivalent camera loop with Udon) multiple times per frame, but this incurs the hefty cost of double-buffering (and thus swapping) the entire 64 MiB texture every time. Instead, let‚Äôs leave this concept behind a bit and focus on version 2:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;same 2048x2048 texture with 128x128 state area as before&lt;/item&gt;&lt;item&gt;shader split into two passes: &lt;code&gt;CPUTick&lt;/code&gt;does a CPU cycle but writes to a memory cache area within the 128x128 pixels, and&lt;code&gt;Commit&lt;/code&gt;writes that cache back to RAM&lt;/item&gt;&lt;item&gt;the Custom Render Texture is set up so it renders multiple times, first a bunch of &lt;code&gt;CPUTick&lt;/code&gt;passes on just the 128x128 area, then it finishes up with a single full-texture&lt;code&gt;Commit&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This implementation already gets us a lot further. On the bright side, Unity is smart enough to realize that when you only update the 128 by 128 area, it also only needs to buffer swap this part of the texture. And since this area is fairly small, it fits entirely within the L2 cache of almost any modern GPU, making the swapping process very cheap. On the downside, this now means we need a seperate memory cache - no problem though, we have enough space left over in the state area to hold all the data we want.&lt;/p&gt;&lt;p&gt;Version 2 got up to around 35-40 kHz in-game, pretty decent, but still not fast enough for my liking. Enter the current version 3:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;same area splitting as before, keep the two-pass design&lt;/item&gt;&lt;item&gt;instead of multiple passes in the CRT, simply loop directly in the shader and run multiple ticks at once&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This option has the least non-compute overhead of all the above. There‚Äôs only two buffer swaps, and one of them is for the small area. This caching strategy (I call it the ‚ÄúL1 write cache‚Äù) is what makes this shader fast enough to run Linux. 300 kHz is not out of the question on a high-end GPU, my 2080 Ti regularly pushes over 200.&lt;/p&gt;&lt;p&gt;(image credit: @d4rkpl4y3r_vr, who had the patience to show that the emulator can calculate pi to 1234 places)&lt;/p&gt;&lt;p&gt;However, there is now a glaring issue: If we run multiple ticks per iteration, we cannot use the 128x128 px state area as a cache anymore. In a fragment shader, we can only write output at the end of execution, but memory writes can happen anytime during emulation, and must be architecturally visible immediately - that is, in RISC-V, a write followed by a read to the same address must always return the previously written value.[4]&lt;/p&gt;&lt;p&gt;With this in mind, the L1 cache only has one place to live: The GPU‚Äôs register file. I‚Äôve been told modern architectures should support up to 64 kB of instance state (I suppose it can evict to VRAM?), but in practice the limit you‚Äôre going to hit is once again the shader compiler. Use too many variables, and we‚Äôre back at waiting 15 minutes for an ‚ÄúIPC error‚Äù.&lt;/p&gt;&lt;p&gt;At the time of writing, L1 is a two-way set associative cache with 16 sets and 5 words per line[5]. This comes out to 320 bytes per frame - with the current setup, a good GPU can push up to 4000 instructions per frame, and with &lt;code&gt;sw&lt;/code&gt; (‚Äústore word‚Äù) being one of them, this cache will fill up in as little as 80. If the cache is full, the CPU stalls until the next &lt;code&gt;Commit&lt;/code&gt;. A little trick is to double up the &lt;code&gt;Commit&lt;/code&gt; passes and do two &lt;code&gt;CPUTick&lt;/code&gt;s as well - that way we can at least get twice the throughput, while only incuring a moderate performance hit for buffer swapping the full 64 MiB twice.&lt;/p&gt;&lt;p&gt;This caching strategy is the tradeoff I made for clock speed - memory write performance absolutely sucks. But it‚Äôs decidedly faster than limiting the clockspeed itself, the ‚Äúreal-world‚Äù performance is certainly better this way.&lt;/p&gt;&lt;p&gt;A neat little side-effect of storing main memory in a texture, is that you can display it visually! Below is a (jpeg-compressed and downsized) picture of the main memory with a fully booted linux kernel.&lt;/p&gt;&lt;p&gt;Notice the two large zeroed (black) areas at the top (128px state area + OpenSBI and reserved bootloader memory), and the fascinating blue memory pattern at the bottom (that‚Äôs the high addresses, I believe the regular stripes are due to early memory poisoning of the SLAB/SLUB allocator in the kernel, feel free to correct me on this):&lt;/p&gt;&lt;p&gt;The texture is also on display in the VRChat world, where you can take a closer look during execution yourself. It‚Äôs quite fun to see memory framention visibly become worse, the more userspace programs are started.&lt;/p&gt;&lt;p&gt;As an aside, you might be wondering why I called the cache ‚ÄúL1‚Äù, as in ‚ÄúLayer 1‚Äù. The reason is that in the future I‚Äôm planning on extending this concept to become a sort of hybrid between version 2 and 3. The idea is that there will be multiple ticks before a commit, each one being followed by a &lt;code&gt;Writeback&lt;/code&gt; pass, that still only operates on the 128x128 texture (for cheap double-buffering) and flushes the L1 cache to a page-based L2 variant.&lt;/p&gt;&lt;p&gt;The tricky part here is that this has to go away from being set- or fully-associated, as both of these variants would not only incur massive performance penalties for lots of branching, but also for the repeated texture taps (as I can‚Äôt allocate any more registers for the L2 without crashing the compiler again). Instead, I‚Äôm planning on having only a few registers allocated that contain page base addresses that then point to a linear cache area in the state texture. This is somewhat hard to keep coherent though, and requires a new concept of stalling only until a &lt;code&gt;Writeback&lt;/code&gt;, so I couldn‚Äôt get it done in time for the initial presentation.&lt;/p&gt;&lt;p&gt;[4] an exception to this rule is instruction loading, which only needs to be consistent after a &lt;code&gt;fencei&lt;/code&gt; instruction - we can make use of this by omitting the somewhat expensive cache-load logic for memory reads and just tap the texture directly, simply stalling the CPU on &lt;code&gt;fencei&lt;/code&gt; until the next &lt;code&gt;Commit&lt;/code&gt; since it is called very infrequently ‚èé&lt;/p&gt;&lt;p&gt;[5] another benefit of perlpp: all of these values are configurable in a single ‚Äúheader‚Äù file ‚èé&lt;/p&gt;&lt;head rend="h1"&gt;A Note on Inlining&lt;/head&gt;&lt;p&gt;HLSL has the peculiarity that there are no function calls. All functions are inlined at the call site[6], if you call a function four times, it will be included four times in the output assembly. This is of course recursive, so a function that calls other functions will also inline those at every callsite.&lt;/p&gt;&lt;p&gt;This doesn‚Äôt sound like a big issue, but it turns out it actually is - one of the biggest performance tricks I learned during development of the emulator, is that avoiding multiple callsites can improve performance quite a bit. I‚Äôm not 100% sure why that is, but I would assume it has to with locality/recency in the L1i cache of the GPU. So less code = less assembly = less thrashing in the instruction cache, and thus better performance.&lt;/p&gt;&lt;p&gt;Additionally, how could it be any different, it also helps with actually getting the thing to compile. More inlines means more code to translate, and the shader compiler really hates code.&lt;/p&gt;&lt;p&gt;This gives rise to some awkward optimizations, that would produce the opposite result almost anywhere else. The main example of this is coalescing memory reads and writes:&lt;/p&gt;&lt;p&gt;The C code simply calls &lt;code&gt;mem_get_word&lt;/code&gt; in the execution path of each instruction. This works, because the function call is cheap. But if it were to be inlined in every instruction that reads from memory, it would slow down the shader a lot. Instead, we do it preventatively - before even executing the instruction-specific code, check by way of the opcode if the instruction might need to read a value from memory. If that is the case, figure out where from (which is different for regular &lt;code&gt;lX&lt;/code&gt; and atomic ops), and load the memory once. This way, &lt;code&gt;mem_get_word&lt;/code&gt; only needs to be inlined once for the entire instruction emulation path.&lt;/p&gt;&lt;p&gt;We also handle unaligned memory reads creatively. Off the top of my head, this would be the obvious solution:&lt;/p&gt;&lt;code&gt;off = (read_addr &amp;amp; 3) * 8;
val = mem_get_word(read_addr &amp;amp; (~3)) &amp;gt;&amp;gt; off;
val |= mem_get_word(read_addr &amp;amp; (~3) + 4) &amp;lt;&amp;lt; (32 - off);
&lt;/code&gt;&lt;p&gt;‚Ä¶but instead, we use the one tool HLSL gives us to avoid multiple inlining, loops with the &lt;code&gt;[loop]&lt;/code&gt; attribute that prevents them from being unrolled:&lt;/p&gt;&lt;code&gt;uint w1 = 0, w2 = 0;
[loop]
for (uint ui = 0; ui &amp;lt; ((read_addr &amp;amp; 0x3) ? 2 : 1); ui++) {
    uint tmp = mem_get_word((read_addr &amp;amp; (~0x3)) + 0x4 * ui);
    [flatten]
    if (ui) { w2 = tmp; }
    else { w1 = tmp; }
}
val = w1 &amp;gt;&amp;gt; ((do_mem_read &amp;amp; 0x3) * 8);
val |= w2 &amp;lt;&amp;lt; ((4 - (do_mem_read &amp;amp; 0x3)) * 8);
&lt;/code&gt;&lt;p&gt;There are several places in the code that seemingly make no sense, but are actually intentionally written with the goal of avoiding inlining. Try to keep that in mind, if you dare read through the source yourself.&lt;/p&gt;&lt;p&gt;[6] there is a &lt;code&gt;[call]&lt;/code&gt; attribute for switch/case, but once again I don‚Äôt know why you wouldn‚Äôt just use &lt;code&gt;[forcecase]&lt;/code&gt;, in my testing it unconditionally made performance worse - it does however actually compile to a jump and return, meaning the capability must exist in shader assembly, but even functions with &lt;code&gt;[noinline]&lt;/code&gt; (which is a valid attribute) are always inlined‚Ä¶ ‚èé&lt;/p&gt;&lt;head rend="h1"&gt;Excursion: Debug View&lt;/head&gt;&lt;p&gt;For debugging purposes, and later on also actual data extraction, we need a way to communicate values from our shader to the user. And ideally not just the enduser, but also Udon, where we can further process the data on the CPU. Udon does not expose &lt;code&gt;Graphics.Blit&lt;/code&gt;, which is the usual Unity way of reading shader output to the CPU, so we need some trickery again.&lt;/p&gt;&lt;p&gt;The only way currently to get pixel data from a shader into Udon is via the &lt;code&gt;OnPostRender&lt;/code&gt; callback. If the behaviour is on a &lt;code&gt;Camera&lt;/code&gt; object, this will be called once per frame. Within it, &lt;code&gt;Buffer.ReadPixels&lt;/code&gt; can be used to retrieve the actual pixel data into a Read/Write enabled static &lt;code&gt;Texture2D&lt;/code&gt; object. The individual pixels can then be accessed as &lt;code&gt;Color&lt;/code&gt; structs. But not so fast, a Unity &lt;code&gt;Color&lt;/code&gt; contains four float values at 8-bit precision, and alpha is premultiplied - so simply reading our state/RAM texture which uses Integer-Format with 128 bpp is out of the question.&lt;/p&gt;&lt;p&gt;Instead, we write a secondary shader, a ‚Äúhelper‚Äù shader if you so will, that stretches the state texture (and only the state part, not the entire RAM) onto a seperate, floating-point enabled texture 6-times the width (and only using the 3 base color channels). Doing some ‚Äúclever‚Äù floating-point math and bit-twiddling allows us to finally recover the original value.&lt;/p&gt;&lt;code&gt;#define PACK_MASK 0xFF
#define PACK_SHIFT 8
void pack_uint4(in uint4 data, out float3 result[6]) {
    result[0] = (data.rgb &amp;amp; PACK_MASK) / 255.0f;
    result[1] = ((data.rgb &amp;gt;&amp;gt; PACK_SHIFT) &amp;amp; PACK_MASK) / 255.0f;
    result[2] = ((data.rgb &amp;gt;&amp;gt; (PACK_SHIFT*2)) &amp;amp; PACK_MASK) / 255.0f;
    result[3] = ((data.rgb &amp;gt;&amp;gt; (PACK_SHIFT*3)) &amp;amp; PACK_MASK) / 255.0f;
    result[4].r = (data.a &amp;amp; PACK_MASK) / 255.0f;
    result[4].g = ((data.a &amp;gt;&amp;gt; PACK_SHIFT) &amp;amp; PACK_MASK) / 255.0f;
    result[4].b = ((data.a &amp;gt;&amp;gt; (PACK_SHIFT*2)) &amp;amp; PACK_MASK) / 255.0f;
    result[5].r = ((data.a &amp;gt;&amp;gt; (PACK_SHIFT*3)) &amp;amp; PACK_MASK) / 255.0f;
    result[5].gb = 0;
}
#undef PACK_SHIFT
#undef PACK_MASK
&lt;/code&gt;&lt;p&gt;(excerpt from helpers.cginc, for encoding)&lt;/p&gt;&lt;code&gt;private const float MULT = 255.0f;
private const float ADD = 0.5f;
private uint decodePackedData(int x, int y, int c)
{
    Color[] col = new Color[6] {
        Buffer.GetPixel(x, y),
        Buffer.GetPixel(x + 128, y),
        Buffer.GetPixel(x + 128*2, y),
        Buffer.GetPixel(x + 128*3, y),
        Buffer.GetPixel(x + 128*4, y),
        Buffer.GetPixel(x + 128*5, y)
    };

    switch (c) {
        case 0:
            return (
                (uint)(col[0].r * MULT + ADD) |
                ((uint)(col[1].r * MULT + ADD) &amp;lt;&amp;lt; 8) |
                ((uint)(col[2].r * MULT + ADD) &amp;lt;&amp;lt; 16) |
                ((uint)(col[3].r * MULT + ADD) &amp;lt;&amp;lt; 24)
            );
        // ...
    }
}
&lt;/code&gt;&lt;p&gt;(excerpt from NixDebug.cs, for decoding - this file is in desperate need of a cleanup :/)&lt;/p&gt;&lt;p&gt;(the debug display as seen in-game, the spherical buttons allow for single-stepping)&lt;/p&gt;&lt;p&gt;This is fairly expensive, especially since it‚Äôs running in Udon, so we limit the rendering of this &lt;code&gt;Camera&lt;/code&gt; to once every 15 frames or so. Certainly not pretty, but works well enough for debugging (and unfortunately also some device state).&lt;/p&gt;&lt;head rend="h1"&gt;MMU and Devices&lt;/head&gt;&lt;p&gt;The emulator includes a full SV32 memory management unit. I didn‚Äôt even plan on adding this at first, but it turns out Linux only supports NOMMU mode on 64-bit RISC-V. I suppose this project is a fairly niche use-case‚Ä¶&lt;/p&gt;&lt;p&gt;Fortunately, this ended up being easier than expected. I‚Äôm not sure what it was about the MMU, but it sounded quite difficult at first, only to turn out to be a straightforward implementation of the paging algorithm described in the RISC-V privileged spec.&lt;/p&gt;&lt;p&gt;Once again, the two-layer pagewalk is performed with avoiding inlining in mind. The &lt;code&gt;load_page&lt;/code&gt; function only has one callsite, with the recursive walk taken care of by a loop. I felt that the MMU logic was optimized enough that I could get away with using &lt;code&gt;mem_get_cached_or_tex&lt;/code&gt;, which includes the cache logic - this means that page tables are fully coherent, and &lt;code&gt;sfence.vma&lt;/code&gt; (what would be a TLB flush on x86) can be a no-op.&lt;/p&gt;&lt;p&gt;There are two devices on the emulated SoC that can issue interrupts - the CLINT timer and the UART. Additionally, software interrupts into both machine and supervisor mode are supported as well. All of this is covered under the umbrella term ‚Äútrap handling‚Äù, which deals with IRQs and exceptions. Most of this logic is borrowed fairly directly from riscv-rust again, with the exception being that PLIC and CLINT are handled all at once.[7]&lt;/p&gt;&lt;p&gt;The timer‚Äôs frequency is in sync with the CPU clock, i.e. one clock cycle equals one timer tick. That being said, this is not what our device tree is communicating to Linux. The frequency given as &lt;code&gt;timebase-frequency = &amp;lt;0x1000000&amp;gt;;&lt;/code&gt; ranges in the MHz, obviously way faster than what it actually runs at. I‚Äôm not entirely certain why that is necessary, but if I set this to a more natural 200 kHz-ish, Linux schedules it‚Äôs own timer interrupt so frequently as to completely stall the boot process.&lt;/p&gt;&lt;p&gt;The UART is a bit more tricky: While the emulator-facing side is a fairly simple 8250/16550a serial port, it also needs to communicate data out to the user and read inputs.&lt;/p&gt;&lt;p&gt;Output is currently handled via a ring-buffer that is shared with Udon using the same mechanism as the Debug View mentioned above. Udon then simply puts the characters to a Unity UI &lt;code&gt;Canvas&lt;/code&gt;. I plan on replacing this with a fully shader-based terminal renderer in the future, this would also allow me to properly implement ANSI/VT100 escape codes - &lt;code&gt;vim&lt;/code&gt; vs &lt;code&gt;emacs&lt;/code&gt; live debate in VRChat anyone?&lt;/p&gt;&lt;p&gt;(a bug in the UART output causing me to boot the knockoff ‚Äúninux‚Äù built with ‚ÄúGNU Bnnutils‚Äù)&lt;/p&gt;&lt;p&gt;Input is rather simple too, using a regular shader parameter to pass the input ASCII character from Udon (specifically a modified version of @FairlySadPanda‚Äôs Keyboard script) to the shader. It also needs the Debug View mechanism however, since the guest running in the emulator might not acknowledge the received character, in which case it will remain in the buffer and &lt;code&gt;RBR&lt;/code&gt; will stay set. This of course also limits input speed to how often we decide to render the performance-hungry debug &lt;code&gt;Camera&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;There is currently no disk emulated, since VRChat doesn‚Äôt support world persistancy at the moment anyway. Linux boots entirely from RAM, the initramfs stays mounted as the rootfs.&lt;/p&gt;&lt;p&gt;[7] Side-note that I mention for no particular reason and definitely didn‚Äôt spend a full day tracking down a related bug on: Did you know that bitwise negate in Rust is &lt;code&gt;!&lt;/code&gt;, which, if copied to C, will compile successfully and without warning, but actually perform a boolean negate? Now you do! &lt;code&gt;~&lt;/code&gt; is what you need, obviously. ‚èé&lt;/p&gt;&lt;head rend="h1"&gt;Payloads&lt;/head&gt;&lt;p&gt;Speaking of the initramfs, compiling software to run on the emulator is suprisingly straightforward. I used Buildroot to generate a riscv32 GNU toolchain for cross-compiling on my host, and also to generate a cpio image containing BusyBox, QuickJS and my little &lt;code&gt;init&lt;/code&gt; script to print a neat ASCII art logo.&lt;/p&gt;&lt;p&gt;The kernel itself is version 5.13.5, which was the latest stable before the presentation. It runs completely stock with an &lt;code&gt;allnoconfig&lt;/code&gt; and only configuring what‚Äôs absolutely necessary, but I did patch in a few tweaks. At the moment, these consist of:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Not poisoning boot memory, as that takes too long and is mostly for security (which, as you might have guessed, does not have the highest priority in this project)&lt;/item&gt;&lt;item&gt;Printing more information during initramfs loading (as otherwise it just looks like it got stuck for a while; did I mention memory write/copy is really slow?)&lt;/item&gt;&lt;item&gt;Currently disabled, but for future use a paravirtualized &lt;code&gt;memcpy&lt;/code&gt;implementation, that uses custom CSR registers to copy memory in the&lt;code&gt;Commit&lt;/code&gt;stage instead of going through L1 cache&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I have a prototype of the last point working now, but before the presentation some last-minute bugs prevented me from showing it off.&lt;/p&gt;&lt;p&gt;Of course, Linux is not the only thing that runs on the emulator. The GitHub has some instructions on how to build other payloads. Aside from a very basic bare-metal C program to test functionality, the two more interesting ones are Micropython and Rust-Test.&lt;/p&gt;&lt;p&gt;The first one, Micropython, provides a Python 3 REPL where you can experiment with writing your own code live in VRChat. The benefit of it being that it boots way quicker than Linux. I had to add a riscv32 port myself, based on the existing riscv64 version, it certainly isn‚Äôt the cleanest but it boots and does what it‚Äôs supposed to showcase.&lt;/p&gt;&lt;p&gt;(image credit: @pema99, a sierpi≈Ñski triangle rendered with Micropython)&lt;/p&gt;&lt;p&gt;The Rust-Test or rust_payload program is my experiment in building native, &lt;code&gt;no-std&lt;/code&gt; Rust for use on the emulator. I needed to patch the &lt;code&gt;rustc&lt;/code&gt; compiler to not emit compressed instructions (which are not implemented, as decoding them would only take more assembly-space and RAM is actually the one resource we have more than enough of). This was among the first things to run successfully on the emulator!&lt;/p&gt;&lt;p&gt;This one gave me some interesting ideas for potential future use-cases as well. Imagine having an interface to call Unity functions from the emulator (e.g. via the previously mentioned debug interface), and then writing your world scripts in Rust. Probably too slow to be useful, but an intriguing concept.&lt;/p&gt;&lt;p&gt;And just to have it noted, all payloads (aside from the bare-metal test) run on top of OpenSBI, which, if you‚Äôre coming from x86 you can think of as sort of a ‚ÄúBIOS‚Äù or ‚Äúfirmware‚Äù. It runs in machine mode, handles basic initialization tasks and prepares the environment for the stage-2 payload. It also provides some functionality to the next stage running in supervisor mode, most importantly timer scheduling (which requires machine privileges) and a basic UART driver (really useful in the Rust-Demo, as we can print to the console easily using it).&lt;/p&gt;&lt;p&gt;(me standing in front of OpenSBI in VR for the first time)&lt;/p&gt;&lt;head rend="h1"&gt;The End?&lt;/head&gt;&lt;p&gt;This was a big project for me, spanning over several months and bringing together many areas of knowledge I had aquired so far.&lt;/p&gt;&lt;p&gt;During development, I kept this project a secret for the longest time - I just love the thrill and payoff that comes with presenting something that absolutely nobody expected to see. Making this in VRChat has not only provided an additional challenge to overcome, but also brought with it the potential of demonstrating this live, in front of an audience, and then continue to chat with talented creators from all over. I thank everyone that answered my sometimes cryptic requests on Discord and in VRChat itself, and also everyone that didn‚Äôt ask what I was even working on when I frustratedly vented about something (probably the shader compiler) again.&lt;/p&gt;&lt;p&gt;This project has given me the opportunity to learn about the inner workings of RISC-V, it taught me more about the Linux Kernel‚Äôs boot process and hardware interface than most people would want to know, and it gave me an excuse to dive deeper and deeper into the magical world of shaders.&lt;/p&gt;&lt;p&gt;Once again, feel free to read the full code on GitHub, or check out the world for yourself in VRChat - I‚Äôm always happy to chat in person, if you manage to catch me there :)&lt;/p&gt;&lt;p&gt;I‚Äôll probably continue working on this project for a while, I still have a bunch of ideas at the ready. Maybe, just maybe, I‚Äôll even write more blog posts about it.&lt;/p&gt;&lt;p&gt;So until next time,&lt;/p&gt;&lt;p&gt;~ _pi_&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45881404</guid><pubDate>Mon, 10 Nov 2025 21:50:03 +0000</pubDate></item><item><title>3D Heterogeneous Integration Powers New DARPA Fab</title><link>https://spectrum.ieee.org/3d-heterogeneous-integration</link><description>&lt;doc fingerprint="5130c244c2378f7f"&gt;
  &lt;main&gt;
    &lt;p&gt;A 1980s-era semiconductor fab in Austin, Texas, is getting a makeover. The Texas Institute for Electronics (TIE), as it‚Äôs called now, is tooling up to become the only advanced packaging plant in the world that is dedicated to 3D heterogenous integration (3DHI)‚Äîthe stacking of chips made of multiple materials, both silicon and non-silicon.&lt;/p&gt;
    &lt;p&gt;The fab is the infrastructure behind DARPA‚Äôs Next-Generation Microelectronics Manufacturing (NGMM) program. ‚ÄúNGMM is focused on a revolution in microelectronics through 3D heterogeneous integration,‚Äù said Michael Holmes, managing director of the program.&lt;/p&gt;
    &lt;p&gt;Stacking two or more silicon chips inside the same package makes them act as if they are all one integrated circuit. It already powers some of the most advanced processors in the world. But DARPA predicts silicon-on-silicon stacking will result in no more than a 30-fold boost in performance over what‚Äôs possible with 2D integration. By contrast, doing it with a mix of materials‚Äîgallium nitride, silicon carbide, and other semiconductors‚Äîcould deliver a 100-fold boost, Holmes told engineers and other interested parties at the program‚Äôs unofficial coming out party, the NGMM Summit, late last month.&lt;/p&gt;
    &lt;p&gt;The new fab will make sure these unusual stacked chips are prototyped and manufactured in the United States. Startups, and there were many at the launch event, are looking for a place to prototype and begin manufacturing ideas that are too weird for anywhere else‚Äîand hopefully bypassing the lab-to-fab valley of death that claims many hardware startups.&lt;/p&gt;
    &lt;p&gt;The state of Texas is contributing $552 million to stand up the fab and its programs, with DARPA contributing the remaining $840 million. After NGMM‚Äôs five-year mission is complete, the fab is expected to be a self-sustaining business. ‚ÄúWe are, frankly, a startup,‚Äù said TIE CEO Dwayne LaBrake. ‚ÄúWe have more runway than a typical startup, but we have to stand on our own.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Starting up a 3DHI Fab&lt;/head&gt;
    &lt;p&gt;Getting to that point will take a lot of work, but the TIE foundry is off to a quick start. On a tour of the facility, IEEE Spectrum saw multiple chip manufacturing and testing tools in various states of installation and met several engineers and technicians who had started within the last three months. TIE expects all the fab‚Äôs tools to be in place in the first quarter of 2026.&lt;/p&gt;
    &lt;p&gt;Just as important as the tools themselves is the ability of foundry customers to use them in a predictable manufacturing process. That‚Äôs something that is particularly difficult to develop, TIE officials explained. At the most basic level, non-silicon wafers are often not the same size as each other. And they have different mechanical properties, meaning they expand and contract with temperature at different rates. Yet much of the fab‚Äôs work will be linking these chips together with micrometer precision.&lt;/p&gt;
    &lt;p&gt;The first phase of getting that done is the development of what are called a process design kit and an assembly design kit. The former provides the rules that constrain semiconductor design at the fab. The latter, the assembly design kit, is the real heart of things, because it gives the rules for the 3D assembly and other advanced packaging.&lt;/p&gt;
    &lt;p&gt;Next, TIE will refine those by way of three 3DHI projects, which NGMM is calling exemplars. These are a phased-array radar, an infrared imager called a focal plane array, and a compact power converter. Piloting those through production ‚Äúgives us an initial roadmap‚Ä¶ an on-ramp into tremendous innovation across a broader application space,‚Äù said Holmes.&lt;/p&gt;
    &lt;p&gt;These three very different products are emblematic of how the fab will have to operate once it‚Äôs up and running. Executives described it as a ‚Äúhigh-mix, low-volume‚Äù foundry, meaning it‚Äôs going to have to be good at doing many different things, but it‚Äôs not going to make a lot of any one thing.&lt;/p&gt;
    &lt;p&gt;This is the opposite of most silicon foundries. A high-volume silicon foundry gets to run lots of similar test wafers through its process to work out the bugs. But TIE can‚Äôt do that, so instead it‚Äôs relying on AI‚Äîdeveloped by Austin startup Sandbox Semiconductor‚Äîto help predict the outcome of tweaks to its processes.&lt;/p&gt;
    &lt;p&gt;Along the way, NGMM will provide a number of research opportunities. ‚ÄúWhat we have with NGMM is a very rare opportunity,‚Äù said Ted Moise, a professor at UT Dallas and an IEEE Fellow. With NGMM, universities are planning to work on new thermal conductivity films, microfluidic cooling technology, understanding failure mechanisms in complex packages, and more.&lt;/p&gt;
    &lt;p&gt;‚ÄúNGMM is a weird program for DARPA,‚Äù admitted Whitney Mason, director of the agency‚Äôs Microsystems Technology Office. ‚ÄúIt‚Äôs not our habit to stand up facilities that do manufacturing.‚Äù&lt;/p&gt;
    &lt;p&gt;But ‚ÄúKeep Austin Weird‚Äù is the city‚Äôs unofficial motto, so maybe NGMM and TIE will prove a perfect fit.&lt;/p&gt;
    &lt;p&gt;Samuel K. Moore is the senior editor at IEEE Spectrum in charge of semiconductors coverage. An IEEE member, he has a bachelor's degree in biomedical engineering from Brown University and a master's degree in journalism from New York University.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45881555</guid><pubDate>Mon, 10 Nov 2025 22:04:28 +0000</pubDate></item><item><title>High-performance 2D graphics rendering on the CPU using sparse strips [pdf]</title><link>https://github.com/LaurenzV/master-thesis/blob/main/main.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45881568</guid><pubDate>Mon, 10 Nov 2025 22:05:16 +0000</pubDate></item><item><title>What Caused Performance Issues in My Tiny RPG</title><link>https://jslegenddev.substack.com/p/what-caused-performance-issues-in</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45882305</guid><pubDate>Mon, 10 Nov 2025 23:27:53 +0000</pubDate></item><item><title>Rademacher Complexity and Models of Group Competition</title><link>https://www.symmetrybroken.com/group-selection/</link><description>&lt;doc fingerprint="8f1999911725de6d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Rademacher Complexity and Models of Group Competition&lt;/head&gt;
    &lt;p&gt;What do Radamacher Complexity and mean field approximations have to do with kin and group selection models in evolutionary biology? And what might consideration of the Radamacher Complexity of kin and group selection models suggest to us about cooperation and competition in human groups? Here are tentative answers to what seem like important questions that nobody is asking.[1]&lt;/p&gt;
    &lt;head rend="h2"&gt;Group Selection in Evolutionary Biology&lt;/head&gt;
    &lt;head rend="h3"&gt;The Puzzle of Altruism&lt;/head&gt;
    &lt;p&gt;Curious students learning the theory of natural selection for the first time have been known to ask: "What about altruism?" What explains how animals, and especially humans, sometimes make a willing sacrifice of their own self-interest for others? Charles Darwin himself acknowledged in the Origin of Species that this is a "special difficulty, which at first appeared ... insuperable, and actually fatal to my whole theory." Darwin's own answer, using neuter or sterile female insects (like worker ants or bees) as an example, was as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We can see how useful their production may have been to a social community of insects, on the same principle that the division of labour is useful to civilised man. As ants work by inherited instincts and by inherited tools or weapons, and not by acquired knowledge and manufactured instruments, a perfect division of labour could be effected with them only by the workers being sterile; for had they been fertile, they would have intercrossed, and their instincts and structure would have become blended. And nature has, as I believe, effected this admirable division of labour in the communities of ants, by the means of natural selection. But I am bound to confess, that, with all my faith in this principle, I should never have anticipated that natural selection could have been efficient in so high a degree, had not the case of these neuter insects convinced me of the fact.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Origin of the Species, Chapter VII (last paragraph before Summary) (1859).&lt;/p&gt;
    &lt;head rend="h3"&gt;The Kin Selection Theory Solution&lt;/head&gt;
    &lt;p&gt;Darwin's emphasis on "inherited instincts" vs. "acquired knowledge", which enforced "a perfect division of labour" (my italics) that "could be effected ... only by the workers being sterile" was in 1964 formalized by William D. Hamilton into a mathematical rule:[2]&lt;/p&gt;
    &lt;p&gt;$$rB &amp;gt; C$$&lt;/p&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;p&gt;r represents the genetic relatedness of the recipient of an altruistic act to the actor&lt;lb/&gt; B represents the benefit gained by the recipient of the altruistic act&lt;lb/&gt; C represents the reproductive cost to the actor of performing the act&lt;/p&gt;
    &lt;p&gt;The Wikipedia article on kin selection provides a representative sample of the success that Hamilton's Rule has had in its simplicity and predictive power. Before Hamilton's Rule, Darwin's theory of natural selection lacked precision in making predictions about when pro-social behavior by individuals. Hamilton's Rule and later elaborations of kin selection theory ‚Äî especially those ramifying on the implications of a default to reciprocity in repeated social interactions ‚Äî have been successful at explaining many observations of altruism that might otherwise be puzzling.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Group Selection Theory Alternative&lt;/head&gt;
    &lt;p&gt;Notwithstanding the success of Hamilton's Rule, Nowak, Tarnita, and Wilson (hereinafter, NTW) published in 2010 The evolution of eusociality, a manifesto arguing that kin selection and inclusive fitness theory in evolutionary biology are "not a general theory of evolution," and that Hamilton's Rule specifically "almost never holds." According to NTW, "inclusive fitness considerations rest on fragile assumptions, which do not hold in general."&lt;/p&gt;
    &lt;p&gt;What assumptions? In their own words: "Inclusive fitness theory claims to be a gene-centered approach, but instead it is ‚Äòworker-centered‚Äô. It puts the worker into the center of the analysis...", an approach that requires tracking the effects of an action on relatives, a detailed accounting that is so complex that it must adopt simplifying assumptions (such as pairwise additive interactions or weak selection) to be tractable.&lt;/p&gt;
    &lt;p&gt;NTW proposed an alternative a model (detailed in Box 1 and Supplementary Information Part C) that shifted the focus from the individual worker's decisions to the overall reprodutive success of the queen and colony. An important claim in this essay is that group selection theory as presented in NTW 2010 represents a mean-field theory as that term is used in theoretical physics to describe condensed matter and quantum fields.&lt;/p&gt;
    &lt;p&gt;In the next section, I'll explain what a mean field theory is and how the mathematical model of NTW 2010 can be understood as a mean field theory. At the end of that section, I'll explain how this recognition helps resolve much of the controversy among evolutionary biologists over whether kin selection theory or group selection theory are correct.&lt;/p&gt;
    &lt;head rend="h2"&gt;Group Selection Theory as a Mean Field Approximation&lt;/head&gt;
    &lt;head rend="h3"&gt;Mean Field Theories in Physics&lt;/head&gt;
    &lt;p&gt;Physicists have been faced with the same mathematical difficulty in tracking the larger scale and longer term effects of a single particle's action on its neighbors (the "worker-centered" approach necessitated by kin selection theory) for over 100 years. Especially before the invention of computers, the ability to do mathematical analysis of the cumulative effects of interactions among many particles was basically impossible as a practical matter. This is why as early as 1907, Pierre Curie and Pierre Weiss proposed that instead of tracking all interactions between the many particles in a material, that an approximation of those interactions be made by assuming an averaged over or effective interaction ‚Äî a "mean field theory."&lt;/p&gt;
    &lt;p&gt;Even after the invention of computers, mean field theories have been useful to physicists. Only in the case of weak interactions is it possible to do numerical approximations of the behavior of systems with even a tiny fraction of the number of molecules in a real system ‚Äî by Avogadro's number (‚âà\(10^{23}\)) a glass contains a trillion trillion molecules. Even the number of water molecules that have dissociated into ions at pH 7 and standard temperature and pressure is 10s of thousands of trillions! Even with computers, modeling the interactions of each molecule with all other molecules in the glass is impossible unless you assume that the glass is almost empty ‚Äî that is, that the water molecules are in the gas phase and only occasionally bumping into each other.&lt;/p&gt;
    &lt;p&gt;But much of behavior that's interesting to somebody studying how molecules (or electrons, or whatever) interact is the behavior that obtains when they're strongly interacting. For studying strong interactions of trillions of anything, you need to make approximations, and mean field theories represent one set of approximations.&lt;/p&gt;
    &lt;head rend="h3"&gt;NTW 2010 Present a Mean Field Theory&lt;/head&gt;
    &lt;p&gt;NTW 2010 approximate by averaging over the worker contributions across the colony. Referring to Part C of the Supplementary Information, the complex social interactions within the colony ‚Äî foraging, defense, nursing, and potential conflicts ‚Äî are not tracked individually. Instead, they are abstracted into averaged demographic parameters. The model defines the queen's reproductive rate (\(b_i\)) and death rate (\(d_i\)) as functions of the colony size \(i\). The specific contribution of any single worker is ignored; what matters is the net effect of having \(i-1\) workers on the queen‚Äôs fitness. This replacement of detailed, localized interactions with an averaged effect based on colony size is a mean field approximation.&lt;/p&gt;
    &lt;p&gt;NTW 2010 ignore what in physics would be called localized correlations and in Hamilton's Rule is called "relatedness" (\(r\)). Under their mean field theory, the success of an individual allele is based on the demographic advantages conferred on the queen, regardless of the details of relatedness. NTW explicitly acknowledge this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In our model relatedness does not drive the evolution of eusociality. ... The competition between the eusocial and the solitary allele is described by a standard selection equation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The omission of the detailed local correlations (here, the genetic relatedness of workers to the queen) that are central to kin selection theory here is a characteristic of all mean field theories. The group selection theory proposed in NTW 2010 achieves its simplifications by viewing the workers not as independent agents making strategic decisions or engaging in a cooperative dilemma, but instead as extensions of the queen:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[T]he workers are not independent agents. Their properties depend on the genotype of the queen and the sperm she has stored. Their properties are determined by the alleles that are present in the queen (both in her own genome and in that of the sperm she has stored). The workers can be seen as ‚Äòrobots‚Äô that are built by the queen. They are part of the queen‚Äôs strategy for reproduction.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Supplementary Information at 38.&lt;/p&gt;
    &lt;p&gt;By treating workers as "robots" rather than independent evolutionary actors, the model explicitly glosses over the microscopic strategic interactions and behavioral fluctuations within the colony, focusing only on the macroscopic output of the colony as a whole.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reconciling Kin and Group Selection Theory as Valid Alternative Models&lt;/head&gt;
    &lt;p&gt;If you've been following along, then at this point you probably have a sense that there's a mathematical way to resolve the arguments among evolutionary biologists over whether kin or group selection theory is the one true theory of evolution by natural selection. The mathematical answer is that they're both valid models of natural selection. While kin selection theory may ultimately be the most rigorous method for expressing the process of genetic evolution, it is often intractable as a model of how an actual colony of ants or bees ‚Äî much less humans ‚Äî behave in nature. In many cases, a mean field approximation like the one proposed in NTW 2010 provides a useful model of how the colony behaves by averaging over the details of individual workers.&lt;/p&gt;
    &lt;p&gt;Bearing that in mind, it's an interesting exercise to review the critique of group selection theory offered by Steven Pinker and responses here, and I was not able to overcome the tempatation to review the arguments presented here in view of this mathematical perspective.&lt;/p&gt;
    &lt;p&gt;I love Steven Pinker, who I consider to be a clear thinker that also seems to operate with a high level of tolerance and integrity. But Pinker, Dawkins, Coyne and others (whom I also admire) are as bad as Nowak, Tarnitas, and Wilson in their refusal to acknowledge any merit in an alternative model. Ironically, it is in his unselfconscious teasing of physicists who "assume a spherical cow" (in order to help a farmer increase milk yields), that Pinker reveals the ultimate weakness of his position ‚Äî a failure to acknowledge the pragmatic value that an approximation might have for understanding a phenomena that is otherwise too complex to be modeled at all. I will grant the group of Pinker, Dawkins, and Coyne at least an honorable mention for acknowledging that the two theories can produce the same predictions under the right assumptions, but I'm disappointed at their refusal to acknowledge the potential instrumental value of mean field approximations in describing collective behavior ‚Äî why should we not explore and use any mathematical model that offers us a mathematical model of aspects of natural selection that would otherise be intractable? Indeed, one has the sense that the debate is less about the merits of one mathematical model vs. another than it is about fascism vs. individualism or communism vs. democracy or some other (in some cases, thinly) suppressed premise.&lt;/p&gt;
    &lt;p&gt;Among those who participated, Joseph Henrich and David Queller seem to come closest to getting it right. Henrich emphasizes that group selection theory is simply a model that has proved useful in certain regimes. ("Which accounting system is best entirely depends on the problem and the assumptions one is willing to make in obtaining an answer.") Queller uses a "two languages" metaphor, acknowledging that the two models are "mathematically equivalent" frameworks that "divide up fitness in slightly different ways," recognizing that both can be valid and useful. But nobody explicitly identifies group selection theory as a mean field approximation of kin selection theory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Radamacher Complexity as Measure of Model Richness&lt;/head&gt;
    &lt;p&gt;Having sketched out a reconcilation of what otherwise might seem like conflicting models of evolution, I turn now to showing how a metric developed in machine learning theory, Radamacher Complexity, can be used to provide a mathematical measure of the instrumental value of one model vs. the other.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Example of Non-Additive Games&lt;/head&gt;
    &lt;p&gt;For definiteness and ease of explanation, let's take a regime explicitly addressed in NTW 2010, Section 7.2. This is the regime in which the fitness effect of actions combines non-linearly.[3] In an additive game, the total fitness payoff for a worker is simply the sum of individual contributions by each worker and her partners. If cooperation provides a benefit B at cost C to a particular worker, these benefits and costs are constant regardless of what other workers do. In a non-additive game, by contrast, there is an additional effect that arises when certain workers act together, so that the benefit B may be achieved only upon some threshold of cooperation among the workers.&lt;/p&gt;
    &lt;head rend="h4"&gt;From 3-Person Stag Hunt to Ant and Bee Colonies&lt;/head&gt;
    &lt;p&gt;The 3-person stag hunt game described by NTW 2010 in Section 7.2 demonstrates non-additive interactions where a group (of 3 stag hunters) can only accomplish a task (the successful hunt) if all three members cooperate. If all three cooperate, each pays cost c but receives benefit b (payoff: b-c for each). However, if only one or two members cooperate, the hunt fails and cooperators pay the cost without any benefit (payoffs: -c for cooperators, 0 for defectors), making the benefits synergistic rather than simply additive from pairwise interactions.&lt;/p&gt;
    &lt;p&gt;A kin selection model of three 3-worker groups built up from n members is still tractable small n. But in most more realistic models of workers in an ant or bee colony, kin selection models become intractable. Consider extending the 3-person stag hunt example from Section 7.2 as follows to polyandrous colonies evolving with environmental shocks:&lt;/p&gt;
    &lt;code&gt;class SynergisticColonyModel:
    def __init__(self, max_caste_types=5, max_colony_size=1000):
        # Fitness depends on synchronized caste combinations
        # e.g., foragers + nurses + guards in specific ratios
        self.caste_interactions = self.generate_synergistic_matrix()
    
    def fitness(self, colony_composition):
        # Non-additive: certain combinations produce emergent benefits
        # Intractable because we need to evaluate all possible 
        # combinations of related individuals across castes
        return self.evaluate_higher_order_interactions(colony_composition)
&lt;/code&gt;
    &lt;p&gt;If we let the population evolve with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(N_{spatial}\) = 100 (patches in environment)&lt;/item&gt;
      &lt;item&gt;\(N_{colony}\) ‚àà [1, 1000] (individuals per colony)&lt;/item&gt;
      &lt;item&gt;\(G\) = 10 (relevant genetic loci)&lt;/item&gt;
      &lt;item&gt;\(P\) = 5 (pathogen strains)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To calculate inclusive fitness directly, we'd need to track:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Relatedness between all pairs: O(N_colony¬≤) per colony&lt;/item&gt;
      &lt;item&gt;How relatedness changes with colony composition: O(2^G) possible genetic states&lt;/item&gt;
      &lt;item&gt;Spatial correlations in relatedness: O(N_spatial¬≤) comparisons&lt;/item&gt;
      &lt;item&gt;Pathogen-mediated selection on genetic diversity: O(P √ó 2^G) interactions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus the total calculations required per generation would be of order:&lt;/p&gt;
    &lt;p&gt;$$O(N_{spatial} √ó N_{colony}^2 √ó 2^G √ó P) ‚âà 100 √ó 10^6 √ó 1024 √ó 5 ‚âà 5 √ó 10^{11}$$&lt;/p&gt;
    &lt;head rend="h4"&gt;A Kin Theory Based Neural Network Approximation&lt;/head&gt;
    &lt;p&gt;Let's say that we're ideologically pre-committed to kin selection theory, and are therefore unwilling on principle to consider making a mean field approximation. It's 2025 and Nvidia now has attained a market cap of $5 trillion selling GPUs optimized to do machine learning! Instead of tracking all these states explicitly, we can train a neural network with a compressed representation:&lt;/p&gt;
    &lt;code&gt;def neural_approximation(colony_state, spatial_context):
    # Reduce N_colony √ó G dimensional genetic info to fixed embedding
    genetic_embedding = embed_relatedness_pattern(colony_state)  # O(1) output size
    
    # Reduce N_spatial dimensional context to fixed size
    spatial_embedding = embed_spatial_structure(spatial_context)  # O(1) output size
    
    # Predict fitness with fixed complexity
    return fitness_network(genetic_embedding, spatial_embedding)  # O(1) computation
&lt;/code&gt;
    &lt;p&gt;This is still an approximation, of course, but as basically everybody understands today, given enough data, neural networks can be surprisingly successful at capturing relatively nuanced features of large, complex data sets. This is not the only approach that might be taken to building a realistic kin selection model of natural selection of an actual ant or bee colony, of course. But I believe it's at least representative of what might be considered state of the art.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Group Selection (Mean Field) Approximation Alternative&lt;/head&gt;
    &lt;p&gt;Now let's say tht we're ideologically pre-committed to group selection theory, and must therefore insist on using a mean field approximation to model natural selection in the same ant or bee colony. Following NTW 2010's approach:&lt;/p&gt;
    &lt;code&gt;def group_selection_mean_field(params):
    # Aggregate at colony level
    avg_colony_resistance = f(genetic_diversity)
    colony_productivity = g(queen_mating_number, worker_number)
    
    # Simple ODEs as in NTW Part C
    dx_mono/dt = (b_mono * resistance_mono - d_mono) * x_mono
    dx_poly/dt = (b_poly * resistance_poly - d_poly) * x_poly
    
    return equilibrium_frequencies
&lt;/code&gt;
    &lt;p&gt;And now we have an alternative (tractable) mathematical model of natural selection. Both models are now tractable to computation on large clusters. So which of these models is more useful? At this point, we could fall back to debates about collectivism vs. individualism, the Scottish Enlightenment vs. Confucianism, etc. But the second important claim in this essay is that the machine learning concept of Radamacher Complexity gives us a mathematical way to measure and choose between these models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Radamacher Complexity Comparison&lt;/head&gt;
    &lt;head rend="h4"&gt;What is Radamacher Complexity?&lt;/head&gt;
    &lt;p&gt;Radamacher Complexity boils down to a measure of how easy (or difficult) a theory is to falsify with (out of training sample) experimental evidence. To calculate Radamacher Complexity, you start with the predictions your model would make for a given set of measurements, then randomly flip the predictions and check whether the model would still fit the predictions. Models with high Radamacher Complexity have the potential to be more accurate, but are also more likely to overfit whatever data was used to build the model. Models with lower Radamacher Complexity that give good predictions are more robust in the sense that they have higher predictive power given a fixed amount of data used for training. Lower Radamacher Complexity is, therefore, stricly better so long as model predictions are accurate.&lt;/p&gt;
    &lt;p&gt;A simple example of machine learning models trained to detect whether an email is spam or not might help illustrate what Radamacher Complexity is and how it is calculated:&lt;/p&gt;
    &lt;head rend="h5"&gt;The Setup: Spam Detection&lt;/head&gt;
    &lt;p&gt;Imagine we are building a model to classify emails as Spam (+1) or Not Spam (-1).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The Data (\(m\)): We have a tiny dataset of \(m=4\) emails.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Email 1 (\(x_1\)): "Cheap meds now" (Actual: Spam +1)&lt;/item&gt;
          &lt;item&gt;Email 2 (\(x_2\)): "Meeting at 3 PM" (Actual: Not Spam -1)&lt;/item&gt;
          &lt;item&gt;Email 3 (\(x_3\)): "Win a free cruise" (Actual: Spam +1)&lt;/item&gt;
          &lt;item&gt;Email 4 (\(x_4\)): "Project status update" (Actual: Not Spam -1)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;\(m\) (Dimension of the vectors): This is purely the size of your dataset.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Here, \(m = 4\). Every vector \(a\) in our set \(A\) will have 4 entries, corresponding to the 4 emails above.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;\(n\) (Number of vectors in set \(A\)): This is the number of different ways your model can make specific mistakes.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Even though a model has infinite possible weights, it can only classify these 4 emails in a finite number of ways (specifically, \(2^4 = 16\) possible combinations of labels).&lt;/item&gt;
          &lt;item&gt;\(n\) is how many of those 16 combinations your model is actually capable of producing.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Model Predictions: (\(a\))&lt;/head&gt;
    &lt;p&gt;To make the intuition easiest, let's slightly adjust the definition of the vector \(a\) to be the model's raw predictions rather than losses. (The math is equivalent, but "fitting noise" is easier to see this way).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Let vector \(a = [h(x_1), h(x_2), h(x_3), h(x_4)]\), where \(h(x_i)\) is the model's prediction (-1 or +1) for email \(i\).&lt;/item&gt;
      &lt;item&gt;A perfect model for our data would produce the vector: &lt;code&gt;[+1, -1, +1, -1]&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Computing Radamacher Complexity: Fitting Noise&lt;/head&gt;
    &lt;p&gt;To find the Rademacher Complexity, we don't use the real labels. We use Rademacher noise (\(\sigma\)) as the labels.&lt;/p&gt;
    &lt;p&gt;We generate a random \(\sigma\) vector, say: &lt;code&gt;[+1, +1, -1, +1]&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This is "noise" because we are essentially lying to the model. We are telling it: &lt;list rend="ul"&gt;&lt;item&gt;"Cheap meds" is Spam (+1)&lt;/item&gt;&lt;item&gt;"Meeting at 3 PM" is Spam (+1) (Lie)&lt;/item&gt;&lt;item&gt;"Win a free cruise" is Not Spam (-1) (Lie)&lt;/item&gt;&lt;item&gt;"Project status update" is Spam (+1) (Lie)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We then train our two models to see if they can learn these "crazy" labels.&lt;/p&gt;
    &lt;head rend="h6"&gt;Model A: The Linear Model (Low Complexity)&lt;/head&gt;
    &lt;p&gt;A linear model tries to separate these emails with a straight line based on features (e.g., count of the word "free").&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Challenge: Can a straight line classify "Meeting" and "Project update" as opposites, while also classifying "Meds" and "Cruise" as opposites?&lt;/item&gt;
      &lt;item&gt;Reality: Probably not. If it learns "free" indicates non-spam (to fit the lie about Email 3), it will likely misclassify Email 1.&lt;/item&gt;
      &lt;item&gt;Result: The linear model cannot produce a prediction vector \(a\) that matches this noise \(\sigma\). The best it might do is get 2 out of 4 right.&lt;/item&gt;
      &lt;item&gt;Correlation: The sum \(\sum \sigma_i a_i\) will be low (e.g., \(1+1-1-1 = 0\)).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Model B: Deep Neural Network (High Complexity)&lt;/head&gt;
    &lt;p&gt;A deep neural net has millions of parameters. It doesn't just look at "free"; it looks at every character combination.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Challenge: Can it learn the crazy labels?&lt;/item&gt;
      &lt;item&gt;Reality: Yes. It can likely find a very specific, convoluted set of weights that perfectly memorizes this specific noise pattern.&lt;/item&gt;
      &lt;item&gt;Result: The neural net can produce a prediction vector \(a\) that exactly matches \(\sigma\): &lt;code&gt;[+1, +1, -1, +1]&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Correlation: The sum \(\sum \sigma_i a_i\) will be maximum (\(1+1+1+1 = 4\)).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;The Spam Detection Models Compared&lt;/head&gt;
    &lt;p&gt;If you repeat this 10,000 times with different random \(\sigma\) noise vectors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Linear Model will rarely be able to match the noise. Its average maximum correlation (and thus its Rademacher Complexity) will be low.&lt;/item&gt;
      &lt;item&gt;The Neural Net will almost always be able to perfectly match the noise. Its average maximum correlation will be very high (close to 1.0 on a normalized scale).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why not try a Radamacher Complexity comparison of group and kin selection models?&lt;/p&gt;
    &lt;head rend="h3"&gt;The Comparison of Group and Kin Selection Models&lt;/head&gt;
    &lt;p&gt;If you've followed along this far, then hopefully you already have a sense of how the calculation of Radamacher Complexity might provide a useful metric for comparison of group and kin selection models of natural selection ‚Äî Radamacher Complexity tells us whether whatever massive dimensionality reduction we've made preserves the essential evolutionary dynamics or loses critical information.&lt;/p&gt;
    &lt;code&gt;def compare_model_complexity(evolutionary_data, n_epochs=100):
    # Split data into training and test
    train_data, test_data = split_colony_evolution_data(evolutionary_data)
    
    # Compute empirical Rademacher complexities
    R_kin = compute_rademacher_neural(
        train_data,
        model=KinSelectionNN(),
        n_rademacher_samples=1000
    )
    
    R_group = compute_rademacher_meanfield(
        train_data,
        param_bounds=get_biological_bounds(),
        n_rademacher_samples=1000
    )
    
    # Key insight: R_kin &amp;gt;&amp;gt; R_group, but does it help?
    generalization_gap_kin = train_error_kin - test_error_kin
    generalization_gap_group = train_error_group - test_error_group
    
    # The instructive finding would be if:
    # R_kin/R_group ‚âà 10-100 (much higher complexity)
    # But if generalization_gap_kin ‚âà generalization_gap_group
    # then the added complexity doesn't improve prediction
&lt;/code&gt;
    &lt;head rend="h4"&gt;Why the Comparison is Useful&lt;/head&gt;
    &lt;p&gt;Finally, we get to the conclusion: estimating the Radamacher Complexity of kin and group selection models provides a (non-ideological!) metric for the relative accuracy of the models. In principle, we can even map out where each approach works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small colonies (N &amp;lt; 50): Both tractable, direct calculation possible&lt;/item&gt;
      &lt;item&gt;Medium colonies (50 &amp;lt; N &amp;lt; 500): Neural approximation valuable&lt;/item&gt;
      &lt;item&gt;Large colonies (N &amp;gt; 500): Mean field becomes necessary&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As an added bonus, an estimation of Radamacher Complexity also gives us a method for testing one of NTW 2010's core claims that "relatedness is a consequence of eusociality, but not a cause" through interventional studies and temporal analysis. To be clear, Rademacher Complexity analysis alone cannot validate this claim because multiple causal structures can generate identical observable distributions, but their claim about causality does imply a specific temporal ordering:&lt;/p&gt;
    &lt;code&gt;class TemporalCausalityTest:
    def test_granger_causality(self, time_series_data):
        # Extract time series
        group_benefits_t = data['ecological_benefits_of_grouping']
        relatedness_t = data['average_within_colony_relatedness']
        eusociality_t = data['degree_of_reproductive_skew']
        
        # NTW predicts this temporal structure:
        # group_benefits[t] ‚Üí eusociality[t+1] ‚Üí relatedness[t+2]
        
        # Kin selection predicts:
        # relatedness[t] ‚Üí eusociality[t+1] ‚Üí group_benefits[t+2]
        
        # Use Rademacher complexity to compare models with different lag structures
        R_ntw = self.compute_rademacher_temporal(
            model=lambda t: f(group_benefits[t-2], eusociality[t-1]),
            target=relatedness_t
        )
        
        R_kin = self.compute_rademacher_temporal(
            model=lambda t: f(relatedness[t-2], eusociality[t-1]),
            target=group_benefits_t
        )
        
        return R_ntw &amp;lt; R_kin  # Lower complexity suggests correct causal orderW
&lt;/code&gt;
    &lt;p&gt;If the group selection model were to show lower complexity, then that would at least suggest that NTW's causality claim is correct.&lt;/p&gt;
    &lt;head rend="h4"&gt;A Rough Test of Temporal Causal Models&lt;/head&gt;
    &lt;p&gt;I went ahead and implemented a rough test of this. You can find the code under an MIT license on GitHub here.&lt;/p&gt;
    &lt;p&gt;Here are the results:&lt;/p&gt;
    &lt;code&gt;============================================================
Testing Rademacher Complexity of Temporal Causal Models
Comparing Kin Selection vs Group Selection
============================================================

1. Running complexity comparison experiments...

=== Summary Statistics ===
                           rademacher_complexity         test_error         n_parameters
                                            mean     std       mean     std         mean
true_model test_model                                                                   
group      group_selection                0.0092  0.0007     0.0103  0.0001         90.0
           kin_selection                  0.0144  0.0007     0.0103  0.0001       7350.0
kin        group_selection                0.0161  0.0003     0.0094  0.0001         90.0
           kin_selection                  0.0142  0.0015     0.0094  0.0001       7350.0

True model: group
  Kin selection - Error: 0.0103, Complexity: 0.0144
  Group selection - Error: 0.0103, Complexity: 0.0092
  ‚úì Simpler group model generalizes as well with lower complexity

True model: kin
  Kin selection - Error: 0.0094, Complexity: 0.0142
  Group selection - Error: 0.0094, Complexity: 0.0161
  ‚úó Complex kin model shows advantage

2. Testing temporal causality (NTW's claim) with enhanced parameters...
This may take a few minutes due to increased sample sizes...

=== Enhanced Temporal Causality Test ===
Testing whether wrong causal direction increases model complexity

Enhanced features:
  - 8 causal factors with different time scales
  - 5 hidden states with decay rates 0.85 to 0.99
  - 15 confounders (autocorrelated, periodic, trending)
  - 3000 time points for better statistical power
  - 5 regularization values tested
  - 1000 Rademacher samples

Complexity measurements (best regularization):
  Correct (causes‚Üíeffect):      0.0040 (Œ±=10.0)
  Wrong (consequence‚Üíeffect):   0.0024 (Œ±=10.0)
  Ratio (wrong/correct):        0.61

Predictive error:
  Correct direction:            0.0107
  Wrong direction:              0.0110
  Error ratio (wrong/correct):  1.04

Combined difficulty ratio:      0.82

‚úó Result does not clearly support NTW's claim
  The consequence may still contain sufficient information

3. Generating visualizations...

============================================================
CONCLUSIONS
============================================================
1. GROUP SELECTION SUFFICIENT: Simple group model generalizes
   as well as complex kin model for group selection dynamics
3. EFFICIENCY: Group selection is 0.0x more
   parameter-efficient than kin selection
&lt;/code&gt;
    &lt;p&gt;No real surprises. Model results are context dependent. When the "true" dynamics are group selection, both achieve identical test error (0.0103). When the true dynamics are kin selection, both models achieve identical test error (0.0094).&lt;/p&gt;
    &lt;p&gt;But the Rademacher Complexity reveals some important differences. When modeling group selection dynamics, the group selection model has lower Rademacher Complexity (0.0092) than the kin selection model (0.0144). Conversely, when modeling kin selection dynamics, the group selection model has higher complexity (0.0161) than the kin selection model (0.0142). The group selection model has to work harder to capture kin selection dynamics. It's not always the simpler model.&lt;/p&gt;
    &lt;p&gt;Perhaps most importantly, the causality test fails to support NTW's claim about causality. The causal chain tested is ecological benefits =&amp;gt; eusociality =&amp;gt; increased relatedness. Model complexity for the correct causal direction (causes =&amp;gt; effect) has Rademacher Complexity 0.0040, while the counter-factual direction (consequence =&amp;gt; effect) has lower complexity at 0.0024. Because the complexity is lower for the counter-factual (ratio of 0.61), this test does not validate NTW 2010's claim that "relatedness is a consequence of eusociality, but not a cause."&lt;/p&gt;
    &lt;p&gt;It is important to emphasize, however, that this test's failure to validate their claim also does not disprove their claim. This is a computational test of temporal causality using Rademacher complexity on synthetic data with specific assumptions. But it is provocative to the extent that it shows the opposite of what NTW might have predicted - the "wrong" causal direction appears less complex to model.&lt;/p&gt;
    &lt;p&gt;In the final section of this essay, I offer speculative obseverations about what the relationship between group and kin selection models as framed by Radamacher Complexity might imply about the nature of competition among human groups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Competition as Cooperation&lt;/head&gt;
    &lt;p&gt;What the mathematical models together seem to be showing is something rather profound about the relationship between competition and cooperation: they are not opposing forces but complementary aspects of natural selection. When groups compete for scarce resources, the selective pressure at the group level can overwhelm individual-level incentives for defection, creating conditions where cooperation becomes the dominant strategy within groups.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inter-Group Competition as a Catalyst for Intra-Group Cooperation&lt;/head&gt;
    &lt;p&gt;When groups compete for limited resources‚Äîwhether territory, food sources, or market share‚Äîthe groups with superior internal coordination gain decisive advantages. This creates a powerful selection pressure for mechanisms that enhance within-group cooperation, even when such cooperation seems costly at the individual level. The framework developed here offers several insights for understanding human social organization and conflict:&lt;/p&gt;
    &lt;head rend="h4"&gt;1. The Dual Nature of Social Institutions&lt;/head&gt;
    &lt;p&gt;Social institutions can be understood as solutions to a two-level optimization problem: they must simultaneously (a) manage internal conflicts to maintain group cohesion and (b) organize collective action for inter-group competition. The most successful institutions are those that align individual incentives with group success, effectively converting the competitive instincts of individuals into cooperative advantages for the group.&lt;/p&gt;
    &lt;head rend="h4"&gt;2. The Role of Cultural Evolution&lt;/head&gt;
    &lt;p&gt;While genetic evolution operates slowly, cultural evolution can rapidly adjust the parameters that determine cooperation levels. Cultural innovations ‚Äî from religious beliefs that increase group cohesion to legal systems that punish defectors ‚Äî can be understood as mechanisms for moving groups into regions of parameter space where cooperation is evolutionarily stable.&lt;/p&gt;
    &lt;head rend="h4"&gt;3. The Violence Trap and Institutional Development&lt;/head&gt;
    &lt;p&gt;This framework aligns remarkably well with North, Wallis, and Weingast's (NWW) theory of violence and social orders. Their "double balance" between political and economic power can be understood as an equilibrium in our multi-level selection framework:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Limited Access Orders emerge when small coalitions (the dominant coalition) cooperate internally to extract rents from the larger population. This represents a local optimum where within-coalition cooperation is maintained by the threat of violence and the distribution of rents.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open Access Orders represent a different equilibrium where cooperation extends beyond narrow coalitions to encompass broader segments of society. The transition requires solving a coordination problem: moving from a Nash equilibrium based on violence potential to one based on impersonal rules and competitive markets.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The mathematical framework suggests why this transition is so difficult: it requires simultaneous changes in multiple parameters (institutional capacity, belief systems, economic structures) to move from one basin of attraction to another. The "doorstep conditions" identified by NWW ‚Äî rule of law for elites, perpetually lived organizations, and consolidated political control of violence ‚Äî can be understood as the minimum changes needed to make the transition feasible.&lt;/p&gt;
    &lt;head rend="h4"&gt;4. Scale-Dependent Governance&lt;/head&gt;
    &lt;p&gt;The Rademacher Complexity analysis suggests that different scales of human organization require fundamentally different governance approaches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Small communities can rely on reputation, reciprocity, and social sanctions‚Äîmechanisms that map naturally onto kin selection dynamics.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Large societies require impersonal institutions, formal laws, and abstract principles of justice‚Äîcorresponding to mean field approximations where individual relationships matter less than aggregate behaviors.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This might explain why attempts to scale up traditional community-based governance often fail, and why large-scale societies that lose institutional capacity often fragment into smaller, kinship-based units.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Paradox of Competition-Induced Cooperation&lt;/head&gt;
    &lt;p&gt;Perhaps the deepest insight from this analysis is that competition between groups can be the strongest force for creating cooperation within groups. This apparent paradox ‚Äî that conflict begets cooperation ‚Äî helps explain many puzzling features of human society:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why do external threats often lead to increased internal cohesion?&lt;/item&gt;
      &lt;item&gt;Why do diverse societies sometimes achieve higher levels of cooperation than homogeneous ones when facing common challenges?&lt;/item&gt;
      &lt;item&gt;Why do periods of intense inter-group competition often coincide with peaks of cultural and technological innovation?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The answer lies in the multi-level nature of selection. When groups compete, traits that enhance group success are favored even if they impose costs on individuals. This creates evolutionary pressure for mechanisms‚Äîbiological, cultural, or institutional‚Äîthat align individual interests with group success.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tenative Conclusions&lt;/head&gt;
    &lt;p&gt;The mathematical framework developed here, connecting evolutionary biology with theoretical physics through the lens of model complexity, offers a path toward understanding one of the fundamental tensions in human society: the simultaneous pull toward individual freedom and collective coordination.&lt;/p&gt;
    &lt;p&gt;One important insight is that kin selection and group selection are not mutually exclusive theories but complementary descriptions, each of which may be more or less useful at different scales and in different parameter regimes. Like quantum mechanics and classical mechanics in physics, both are correct within their domains of applicability. We do not need to choose one over the other, and instead can benefit from understanding when each applies and how to structure institutions that leverage both individual initiative and collective action.&lt;/p&gt;
    &lt;p&gt;This understanding has practical implications for addressing contemporary challenges from climate change to global inequality. These challenges require cooperation at unprecedented scales, where neither pure market mechanisms (individual selection) nor traditional state structures (group selection) seem adequate. The framework developed here suggests that solutions will require new institutional forms that can operate effectively across multiple scales, aligning individual incentives with collective goods while maintaining the flexibility to adapt to changing conditions.&lt;/p&gt;
    &lt;p&gt;The oppostion of competition and cooperation dissolves when we recognize them as different aspects of a single evolutionary process operating across multiple levels. In this light, human history can be read not as a struggle between selfishness and altruism, but as an ongoing experiment in finding stable configurations that balance individual autonomy with collective effectiveness‚Äîan experiment whose outcome remains very much in progress.&lt;/p&gt;
    &lt;p&gt;If you made it this far, thanks for reading! If you have feedback, please @ me on X.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This long essay lays out what must be necessarily a short introduction to several mathematical concepts from evolutionary biology (group selection theory), theoretical physics (mean field theory), and machine learning (Rademacher Complexity) along with an explanation of their relevance to one another and their implications to debates about group competition. If you're not interested in mathematical models, then this essay is probably not for you. If you're only interested in physics, evolutionary biology, or machine learning, then this essay is probably not for you. But if you have some familiarity already with one or more of these fields and are curious to explore their connections, then I encourage you to read on. And please correct my (non-expert) mistakes! I'm at best an amateur at any of this. I wrote this basically to see what would come out after getting a small amount of encouragement from Carson Mulligan on X. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The identity of this equation and the Hand Formula for setting the standard of care in neglience liability was part of the inspiration for the perspective offered in this essay. In another essay, I may write more about how the Hand Formula might be ramified or substituted with a mean-field approximation in more general considerations of the standard of care for negligence. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Worth noting that this regime was first introduced to evolutionary biology by David Queller in 1985. Perhaps this is why the benefits to a mean field approximation were more obvious to him? ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45882592</guid><pubDate>Tue, 11 Nov 2025 00:09:15 +0000</pubDate></item><item><title>Toucan Wireless Split Keyboard with Touchpad</title><link>https://shop.beekeeb.com/products/toucan-wireless-piantor-wireless-split-keyboard-with-touchpad</link><description>&lt;doc fingerprint="fd44942767afd744"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;[Pre-order] Toucan Wireless Split Keyboard with Touchpad&lt;/head&gt;
    &lt;p&gt; This keyboard PCB comes without key switches, keycaps, and cases.&lt;lb/&gt;To build a complete keyboard, please select the necessary add-ons from the options below.&lt;/p&gt;
    &lt;p&gt;To build a complete keyboard, please select the necessary add-ons from the options below.&lt;/p&gt;
    &lt;p&gt; Have questions? Email support@beekeeb.com &lt;/p&gt;
    &lt;p&gt; Description &lt;/p&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;The Toucan is a keyboard with an integrated touchpad, built around the familiar Cantor (Piantor) layout, designs with the goal of creating a truly portable split keyboard, something easy to bring along for work, travel, or even to use comfortably on a flight.&lt;/p&gt;
    &lt;head rend="h3"&gt;Features of Toucan&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wireless - Powered by the latest Seeed Studio XIAO nRF52840 Plus controller, the Toucan connects to iPads, notebooks, and computers via Bluetooth. It can also pair with a Bluetooth dongle, such as the Prospector.&lt;/item&gt;
      &lt;item&gt;DIY Fun - Like the original Piantor, the Toucan uses microcontroller boards instead of embedded controllers, allowing it to remain a true DIY kit. You can enjoy the process of soldering and assembling it yourself, or choose a pre-soldered and fully assembled version if you prefer. You can even easily solder wires to the unused pins for DIY modding.&lt;/item&gt;
      &lt;item&gt;Portable &amp;amp; All-in-One Design - The keyboard and trackpad are integrated into a single compact device, making it easy to carry wherever you go. The trackpad is Cirque‚Äôs 40mm GlidePoint circular trackpad, you do not have to worry about forgetting your mouse on a trip again.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compact enclosed case design - The case is designed to be as lightweight and compact as possible, making it easy to carry anywhere. The enclosed case helps protect the PCB and internal components from dust and dirts, making the keyboard more durable and reliable for everyday use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compatible with both Choc v1 and Choc v2 Switches - The Toucan supports Kailh Choc key switches, known for their reliability and low profile. You can use your favorite switches such as Twilight (Choc v1) or Deep Sea Silent Mini Islet (Choc v2). Note that the Toucan uses the same Choc spacing as the Piantor. If you use Choc v2 switches, you will need to pair them with Choc-spacing MX-stem keycaps (for example, Tai-Hao‚Äôs MT165).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The DIY Kit / Pre-soldered keyboard include&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The inner - PCB and components &lt;list rend="ul"&gt;&lt;item&gt;PCB (standalone)&lt;/item&gt;&lt;item&gt;Components (standalone / pre-soldered on the board, like hotswap sockets, cable connector, ) &lt;/item&gt;&lt;item&gt;2 x Seeed Studio XIAO nRF52840 Plus controllers&lt;/item&gt;&lt;item&gt;1 x memory-in-pixel display&lt;/item&gt;&lt;item&gt;1 x 40mm Cirque GlidePoint Circle Trackpad&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The outer - Keyboard case &lt;list rend="ul"&gt;&lt;item&gt;Anodized aluminum top plate in silver color&lt;/item&gt;&lt;item&gt;3D printed bottom case in black&lt;/item&gt;&lt;item&gt;Acrylic decoration, protectors, screws, rubber feet&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The key switches and keycaps are not included. Please select them from the optional Add-Ons. They are tested and compatibility-guaranteed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Important&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you select the pre-soldered option, no soldering tool is needed. Additionally, if you place the order together with the compatible key switches, we will assemble the keyboard.&lt;/item&gt;
      &lt;item&gt;If you select the DIY kit option, you will need to solder the components on the board. &lt;/item&gt;
      &lt;item&gt;Because I cannot send batteries internationally. Batteries are not included in the kit.&lt;/item&gt;
      &lt;item&gt;This is a pre-order item. Shipments planned to begin in mid-December 2025.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45882736</guid><pubDate>Tue, 11 Nov 2025 00:31:58 +0000</pubDate></item><item><title>Warren Buffett's final shareholder letter [pdf]</title><link>https://berkshirehathaway.com/news/nov1025.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45882837</guid><pubDate>Tue, 11 Nov 2025 00:51:44 +0000</pubDate></item><item><title>I hate screenshots of text</title><link>https://parkscomputing.com/page/i-hate-screenshots-of-text</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45883124</guid><pubDate>Tue, 11 Nov 2025 01:36:47 +0000</pubDate></item></channel></rss>