<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 13 Oct 2025 17:09:16 +0000</lastBuildDate><item><title>LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives</title><link>https://arxiv.org/abs/2510.03761</link><description>&lt;doc fingerprint="ad9eb10045f3128f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Cryptography and Security&lt;/head&gt;&lt;p&gt; [Submitted on 4 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The widespread use of preprint repositories such as arXiv has accelerated the communication of scientific results but also introduced overlooked security risks. Beyond PDFs, these platforms provide unrestricted access to original source materials, including LaTeX sources, auxiliary code, figures, and embedded comments. In the absence of sanitization, submissions may disclose sensitive information that adversaries can harvest using open-source intelligence. In this work, we present the first large-scale security audit of preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates pattern matching, logical filtering, traditional harvesting techniques, and large language models (LLMs) to uncover hidden disclosures within non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25 state-of-the-art models. Our analysis uncovered thousands of PII leaks, GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders, editable private SharePoint links, exposed GitHub and Google credentials, and cloud API keys. We also uncovered confidential author communications, internal disagreements, and conference submission credentials, exposing information that poses serious reputational risks to both researchers and institutions. We urge the research community and repository operators to take immediate action to close these hidden security gaps. To support open science, we release all scripts and methods from this study but withhold sensitive findings that could be misused, in line with ethical principles. The source code and related material are available at the project website this https URL&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45566123</guid><pubDate>Mon, 13 Oct 2025 08:33:34 +0000</pubDate></item><item><title>Spotlight on pdfly, the Swiss Army knife for PDF files</title><link>https://chezsoi.org/lucas/blog/spotlight-on-pdfly.html</link><description>&lt;doc fingerprint="d2c16a62e09737b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Project documentation: pdfly.readthedocs.io&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pdfly&lt;/code&gt; is the youngest project of the &lt;code&gt;py-pdf&lt;/code&gt; organization.
It has been created by Martin Thoma in 2022.&lt;/p&gt;
    &lt;p&gt;It's simply a CLI tool to manipulate PDF files, written in Python and based on the fpdf2 &amp;amp; pypdf libraries.&lt;/p&gt;
    &lt;p&gt;I'm a maintainer of the project ğŸ™‚&lt;/p&gt;
    &lt;head rend="h2"&gt;What can it do?&lt;/head&gt;
    &lt;p&gt;It has meany features, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;display PDF metadata using &lt;code&gt;pdfly meta&lt;/code&gt;and&lt;code&gt;pdfly pagemeta&lt;/code&gt;commands. Example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ pdfly meta minimal-document.pdf
                      Operating System Data
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ         Attribute â”ƒ Value                     â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         File Name â”‚ /tmp/minimal-document.pdf â”‚
â”‚  File Permissions â”‚ -rw-r--r--                â”‚
â”‚         File Size â”‚ 16,978 bytes              â”‚
â”‚     Creation Time â”‚ 2025-10-13 09:44:32       â”‚
â”‚ Modification Time â”‚ 2025-10-13 09:44:32       â”‚
â”‚       Access Time â”‚ 2025-10-13 09:44:46       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       PDF Data
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ          Attribute â”ƒ Value                                                    â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚       CreationDate â”‚ 2022-04-03 18:05:42+02:00                                â”‚
â”‚            Creator â”‚ TeX                                                      â”‚
â”‚           Producer â”‚ pdfTeX-1.40.23                                           â”‚
â”‚              Pages â”‚ 1                                                        â”‚
â”‚          Encrypted â”‚ None                                                     â”‚
â”‚   PDF File Version â”‚ %PDF-1.5                                                 â”‚
â”‚        Page Layout â”‚                                                          â”‚
â”‚          Page Mode â”‚                                                          â”‚
â”‚             PDF ID â”‚ ID1=b'q\x96\xc3\xe3U\xc1|\x9fS\xba\x9a\r\xcap\xcd\xd0'   â”‚
â”‚                    â”‚ ID2=b'q\x96\xc3\xe3U\xc1|\x9fS\xba\x9a\r\xcap\xcd\xd0'   â”‚
â”‚ Fonts (embedded) â”‚                                                          â”‚
â”‚   Fonts (embedded) â”‚ /KNEUFH+CMR10                                            â”‚
â”‚        Attachments â”‚ []                                                       â”‚
â”‚             Images â”‚ 0 images (0 bytes)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;pdfly&lt;/code&gt;can also combine files into new PDF documents: it can extract specific pages &amp;amp; merge documents (&lt;code&gt;pdfly cat&lt;/code&gt;); selectively remove pages (&lt;code&gt;pdfly rm&lt;/code&gt;); convert images to PDF documents (&lt;code&gt;pdfly x2pdf&lt;/code&gt;); and even compress documents (&lt;code&gt;pdfly compress&lt;/code&gt;) or build booklets (&lt;code&gt;pdfly 2-up&lt;/code&gt;&amp;amp;&lt;code&gt;pdfly booklet&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pdfly&lt;/code&gt;includes some commands to pull out specific content from PDF files:&lt;code&gt;pdfly extract-images&lt;/code&gt;&amp;amp;&lt;code&gt;pdfly extract-annotated-text&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;sometimes you want to edit a PDF file manually, in a text editor. But when you do so, you break its&lt;/p&gt;&lt;code&gt;xref&lt;/code&gt;table, that is an index of byte offsets in the document.&lt;code&gt;pdfly update-offsets&lt;/code&gt;is there to save the day, fixing manually-edited PDF documents, so that they can be opened in a PDF viewer again!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Release 0.5.0 &amp;amp; new features&lt;/head&gt;
    &lt;p&gt;Today we released a new version: &lt;code&gt;pdfly release 0.5.0&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Thanks to several contributors, including developers taking part in Hacktoberfest, new exciting features have been added:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;pdfly sign&lt;/code&gt;allows you to easily sign PDF documents, while&lt;code&gt;pdfly check-sign&lt;/code&gt;makes it easy to check a PDF document signature. Thanks to @moormaster for implementing this in PRs #165 &amp;amp; #166 ğŸ‘ğŸ™.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pdfly extract-annotated-pages&lt;/code&gt;extract only annotated pages from a PDF, hence helping to review or rework pages from a large document iteratively. Thanks to Hal Wine (@hwine) for implementing this in PR #128 ğŸ‘ğŸ™.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pdfly rotate&lt;/code&gt;rotate specific pages of a document. Thanks to Subhajit Sahu (@wolfram77) for implementing this in PR #98 ğŸ‘ğŸ™.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;We have a bunch of feature ideas: &lt;code&gt;up-for-grabs&lt;/code&gt; issues, including some &lt;code&gt;good first issues&lt;/code&gt; aimed specially at new contributors, that are willing to help but new to open-source.&lt;/p&gt;
    &lt;p&gt;Personally, I think the &lt;code&gt;pdfly sign&lt;/code&gt; &amp;amp; &lt;code&gt;check-sign&lt;/code&gt; could become handy to many end-users, and I think we should continue to extend those commands usage options, as described in issue #71.&lt;/p&gt;
    &lt;p&gt;We would also be happy to get your feedbacks, bug reports &amp;amp; feature suggestions! ğŸ™‚&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45566139</guid><pubDate>Mon, 13 Oct 2025 08:36:33 +0000</pubDate></item><item><title>Some graphene firms have reaped its potential but others are struggling</title><link>https://www.theguardian.com/business/2025/oct/13/lab-to-fab-are-promises-of-a-graphene-revolution-finally-coming-true</link><description>&lt;doc fingerprint="6088806b0f7d57fb"&gt;
  &lt;main&gt;
    &lt;p&gt;After graphene was first produced at the University of Manchester in 2004, it was hailed as a wonder material, stronger than steel but lighter than paper. But two decades on, not every UK graphene company has made the most of that potential. Some show promise but others are struggling.&lt;/p&gt;
    &lt;p&gt;Extracted from graphite, commonly used in pencils, graphene is a latticed sheet of carbon one atom thick, and is highly effective at conducting heat and electricity. China is the worldâ€™s biggest producer, using it to try to get ahead in the global race to produce microchips and in sectors such as construction.&lt;/p&gt;
    &lt;p&gt;In the UK, a graphene-enhanced, low-carbon concrete was laid at a Northumbrian Water site in July, developed by the Graphene Engineering Innovation Centre (GEIC) at the University of Manchester and Cemex UK.&lt;/p&gt;
    &lt;p&gt;â€œThe material when it came out of academia was hyped to death â€¦ but the challenge is going from lab to fab,â€ says Ben Jensen, the chief executive of 2D Photonics, a startup spun out from the University of Cambridge that makes graphene-based photonic technology for datacentres.&lt;/p&gt;
    &lt;p&gt;Jensen also invented Vantablack coatings, made of carbon nanotubes â€“ rolled-up sheets of graphene â€“ and known as the worldâ€™s â€œblackest blackâ€ because it absorbs 99.96% of light, at the UK company Surrey NanoSystems that he founded in 2007. The materialâ€™s artistic rights were sold exclusively to the sculptor Anish Kapoor, and BMW used it on its X6 coupe to create the â€œblackest black carâ€ six years ago.&lt;/p&gt;
    &lt;p&gt;â€œThis is the challenge when you have new materials trying to displace an incumbent technology,â€ Jensen says. â€œThe value proposition must be extremely good, but there also must be a way to manufacture the material and manufacture it at scale for the application â€¦ then you have to meet price expectations because thereâ€™s no point in delivering something thatâ€™s costing 10 times more than the incumbent.â€&lt;/p&gt;
    &lt;p&gt;Germanyâ€™s Bayer tried to produce carbon nanotube products in bulk but shut down its pilot factory more than a decade ago after the expected surge in demand failed to materialise. The material is now mainly used as a filler to strengthen plastic products. The company described the potential applications of nanotubes as â€œfragmentedâ€.&lt;/p&gt;
    &lt;p&gt;More promising are the graphene-based optical microchips developed by 2D Photonicsâ€™ subsidiary CamGraPhIC, which are based on research done at the University of Cambridge and Italyâ€™s CNIT research institute.&lt;/p&gt;
    &lt;p&gt;At the moment, silicon-photonics microchips convert electrical data into optical data to transmit it through fibre-optic cables, but the firm says its graphene chips deliver more data in the same period of time and at far lower cost.&lt;/p&gt;
    &lt;p&gt;They consume 80% less energy and can operate in a much wider range of temperatures, reducing the need for costly water- and energy-hungry cooling systems for AI datacentres.&lt;/p&gt;
    &lt;p&gt;Sending data via silicon also causes delays. Jensen compares it to a 16-lane motorway that suddenly narrows to a single lane because of roadworks, forcing everything to slow down. Graphene photonics, he says, are like a motorway with hundreds of lanes.&lt;/p&gt;
    &lt;p&gt;â€œWhat weâ€™ve solved is the ability to grow consistent ultra high-performance graphene and to build it into a device,â€ he says. â€œAnd donâ€™t forget, this is a material thatâ€™s one atom thick. Itâ€™s insanely difficult to do this.â€&lt;/p&gt;
    &lt;p&gt;CamGraPhIC was founded in 2018 by the Cambridge nanotechnology professor Andrea Ferrari, who also runs the Cambridge graphene centre, and Marco Romagnoli, who leads advanced photonics at CNIT in Pisa and is the startupâ€™s chief scientific officer.&lt;/p&gt;
    &lt;p&gt;Its parent, 2D Photonics, has just secured funding of Â£25m from backers including Italyâ€™s sovereign wealth fund, Nato and Sony innovation funds, Bosch Ventures and the UKâ€™s Frontier IP Group, and uses a former Pirelli photonics research site in Pisa. It plans to create a pilot manufacturing site in the Milan area to produce 200mm-wide wafers at scale, and is confident of obtaining the necessary funding of â‚¬317m (Â£276m) by the end of the year.&lt;/p&gt;
    &lt;p&gt;Aside from datacentres, the companyâ€™s chips can be used in high-performance computing, 5G and 6G mobile data systems, aircraft systems, autonomous cars, advanced digital radar and satellite-free space communications.&lt;/p&gt;
    &lt;p&gt;Paragraf, a University of Cambridge spinout based in the nearby village of Somersham, has also fared well over the past decade with backing from the UK Treasury. It produces graphene-based electronic devices, including sensors for electric cars, and biosensors for the early detection of disease and other uses in healthcare and agriculture. It recently secured $55m (Â£41m) from investors including the United Arab Emirates sovereign wealth fund, which took a 12.8% stake in Paragraf.&lt;/p&gt;
    &lt;p&gt;A more recent startup, Graphene Innovations Manchester, launched in 2021 by Vivek Koncherry, struck a deal with Saudi Arabia last December for the worldâ€™s first commercial production of graphene-enriched carbon fibre, used in construction for roofs and facades, as well as street light poles. It has started making it in Tabuk with a local partner, and says it is on track to produce 3,000 tonnes by 2026.&lt;/p&gt;
    &lt;p&gt;Other businesses, however, have found the going tougher. One of the sectorâ€™s first companies was Applied Graphene Materials, founded by Prof Karl Coleman in 2010 and spun out from Durham University. It launched a number of products including an anti-corrosion primer and a protective bike-detailing spray, which made it on to the shelves in Halfords. But the loss-making enterprise was wound down in 2023 and Canadaâ€™s Universal Matter acquired its main business.&lt;/p&gt;
    &lt;p&gt;Ron Mertens, the owner of the website Graphene-Info, says: â€œAs is true in the wider materials industry, things take a very long time to reach the market. Many graphene producers and developers never managed to generate meaningful revenues or become profitable.â€&lt;/p&gt;
    &lt;p&gt;The Gloucestershire-based Versarien grew from a garage startup in 2010. Supported by Innovate UK, a government agency, it has developed graphene powders and other products for use in sensors, low-carbon concrete, paints, inks for electronics and textiles such as Umbro running gear and prototype stealth materials for the US military.&lt;/p&gt;
    &lt;p&gt;The Aim-listed company expanded into Spain and South Korea but ran into financial difficulties and was forced to place several subsidiaries in administration or voluntary liquidation in July. Versarien has been seeking to sell its assets, including its patent portfolio, and only has enough cash to keep going until the end of October.&lt;/p&gt;
    &lt;p&gt;Depending on the deal, it may undertake a solvent liquidation of the company or become a cash shell. An investment deal with a Chinese partner collapsed after the UK government intervened to block any collaboration on technology. It would be a sad end for a once promising graphene business.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45566253</guid><pubDate>Mon, 13 Oct 2025 08:53:17 +0000</pubDate></item><item><title>MPTCP for Linux</title><link>https://www.mptcp.dev/</link><description>&lt;doc fingerprint="a7b5e91a5cbdcba9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Multipath TCP or MPTCP is an extension to the standard TCP and is described in RFC 8684. It allows a device to make use of multiple interfaces at once to send and receive TCP packets over a single MPTCP connection. MPTCP can aggregate the bandwidth of multiple interfaces or prefer the one with the lowest latency. It also allows a fail-over if one path is down, and the traffic is seamlessly reinjected on other paths.&lt;/p&gt;
    &lt;code&gt;graph TD;
    subgraph MPTCP
        direction LR
        C_1(&amp;lt;div style="display: inline-block; min-width: 32px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-mobile&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;)
        S_1((&amp;lt;div style="display: inline-block; min-width: 55px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-cloud&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;))
    end

    subgraph TCP
        direction LR
        C_2(&amp;lt;div style="display: inline-block; min-width: 32px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-mobile&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;)
        S_2((&amp;lt;div style="display: inline-block; min-width: 55px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-cloud&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;))
    end

    C_1 &amp;lt;== "5G" ==&amp;gt; S_1
    C_1 &amp;lt;== "Wi-Fi&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;Multiple paths (&amp;lt;i&amp;gt;subflows&amp;lt;/i&amp;gt;)&amp;lt;br /&amp;gt;at the same time" ==&amp;gt; S_1

    C_2 x-. "5G" .-x S_2
    C_2 &amp;lt;== "Wi-Fi&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;One path at a time" ==&amp;gt; S_2

    linkStyle 0 stroke:green;
    linkStyle 1 stroke:green;
    linkStyle 2 stroke:red;
    linkStyle 3 stroke:green;
&lt;/code&gt;
    &lt;head rend="h3"&gt;Use cases&lt;/head&gt;
    &lt;p&gt;Thanks to MPTCP, being able to use multiple paths in parallel or simultaneously brings new use-cases, compared to TCP:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seamless handovers: switching from one path to another while preserving established connections, e.g. Apple is using Multipath TCP on smartphones mainly for this reason since 2013.&lt;/item&gt;
      &lt;item&gt;Best network selection: using the â€œbestâ€ available path depending on some conditions, e.g. latency, losses, cost, bandwidth, etc.&lt;/item&gt;
      &lt;item&gt;Network aggregation: using multiple paths at the same time to have a higher throughput, e.g. to combine fixed and mobile networks to send files faster.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Concepts&lt;/head&gt;
    &lt;p&gt;Technically, when a new socket is created with the &lt;code&gt;IPPROTO_MPTCP&lt;/code&gt; protocol (Linux-specific), a subflow (or path) is created. This subflow consists of a regular TCP connection that is used to transmit data through one interface. Additional subflows can be negotiated later between the hosts. For the remote host to be able to detect the use of MPTCP, a new field is added to the TCP option field of the underlying TCP subflow. This field contains, amongst other things, a &lt;code&gt;MP_CAPABLE&lt;/code&gt; option that tells the other host to use MPTCP if it is supported. If the remote host or any middlebox in between does not support it, the returned &lt;code&gt;SYN+ACK&lt;/code&gt; packet will not contain MPTCP options in the TCP option field. In that case, the connection will be â€œdowngradedâ€ to plain TCP, and it will continue with a single path.&lt;/p&gt;
    &lt;p&gt;This behavior is made possible by two internal components: the path manager, and the packet scheduler.&lt;/p&gt;
    &lt;head rend="h3"&gt;Path Manager&lt;/head&gt;
    &lt;p&gt;The Path Manager is in charge of subflows, from creation to deletion, and also address announcements. Typically, it is the client side that initiates subflows, and the server side that announces additional addresses via the &lt;code&gt;ADD_ADDR&lt;/code&gt; and &lt;code&gt;REMOVE_ADDR&lt;/code&gt; options.&lt;/p&gt;
    &lt;code&gt;graph LR;
    C_1(&amp;lt;div style="display: inline-block; min-width: 35px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-mobile&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;)
    S_1((&amp;lt;div style="display: inline-block; min-width: 60px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-cloud&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;))

    C_1 -. "Potential subflow" -.- S_1
    C_1 &amp;lt;== "Initial subflow" ==&amp;gt; S_1
    C_1 ~~~|"Subflows creation"| C_1
    S_1 ~~~|"Addresses announcement"| S_1

    linkStyle 0 stroke:orange;
    linkStyle 1 stroke:green;
&lt;/code&gt;
    &lt;p&gt;As of Linux v5.19, there are two path managers, controlled by the &lt;code&gt;net.mptcp.pm_type&lt;/code&gt; sysctl knob: the in-kernel one (type &lt;code&gt;0&lt;/code&gt;) where the same rules are applied for all the connections (see: &lt;code&gt;ip mptcp&lt;/code&gt;) ; and the userspace one (type &lt;code&gt;1&lt;/code&gt;), controlled by a userspace daemon (i.e. &lt;code&gt;mptcpd&lt;/code&gt;) where different rules can be applied for each connection.&lt;/p&gt;
    &lt;head rend="h3"&gt;Packet Scheduler&lt;/head&gt;
    &lt;p&gt;The Packet Scheduler is in charge of selecting which available subflow(s) to use to send the next data packet. It can decide to maximize the use of the available bandwidth, only to pick the path with the lower latency, or any other policy depending on the configuration.&lt;/p&gt;
    &lt;code&gt;graph LR;
    A_2(&amp;lt;div style="display: inline-block; min-width: 40px"&amp;gt;&amp;lt;font size="7"&amp;gt;fa:fa-user&amp;lt;/font&amp;gt;&amp;lt;/div&amp;gt;)

    PS{Packet&amp;lt;br /&amp;gt;Scheduler}

    I_21(subflow 1)
    I_22(subflow 2)

    A_2 == "&amp;lt;div style='display: inline-block; min-width: 50px'&amp;gt;fa:fa-box fa:fa-box fa:fa-box&amp;lt;/div&amp;gt;" ==&amp;gt; PS
    PS -- "&amp;lt;div style='display: inline-block; min-width: 32px'&amp;gt;fa:fa-box fa:fa-box&amp;lt;/div&amp;gt;" --&amp;gt; I_21
    PS -- "&amp;lt;div style='display: inline-block; min-width: 14px'&amp;gt;fa:fa-box&amp;lt;/div&amp;gt;" --&amp;gt; I_22
    PS ~~~|"Packets distribution between subflows"| PS
&lt;/code&gt;
    &lt;p&gt;As of Linux v6.8, there is only one packet scheduler, controlled by sysctl knobs in &lt;code&gt;net.mptcp&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;p&gt;As of Linux v6.10, major features of MPTCP include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support of the &lt;code&gt;IPPROTO_MPTCP&lt;/code&gt;protocol in&lt;code&gt;socket()&lt;/code&gt;system calls.&lt;/item&gt;
      &lt;item&gt;Fallback from MPTCP to TCP if the peer or a middlebox do not support MPTCP.&lt;/item&gt;
      &lt;item&gt;Path management using either an in-kernel or userspace path manager.&lt;/item&gt;
      &lt;item&gt;Socket options that are commonly used with TCP sockets.&lt;/item&gt;
      &lt;item&gt;Debug features including MIB counters, diag support (used by the &lt;code&gt;ss&lt;/code&gt;command), and tracepoints.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the ChangeLog for more details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Communication&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mailing List: mptcp@lists.linux.dev (plain text only): &lt;list rend="ul"&gt;&lt;item&gt;Archives&lt;/item&gt;&lt;item&gt;Info&lt;/item&gt;&lt;item&gt;Subscribe by sending an empty email in plain text to mptcp+subscribe@lists.linux.dev, and by replying to the challenge email.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;IRC: #mptcp on libera.chat&lt;/item&gt;
      &lt;item&gt;Online Meetings&lt;/item&gt;
      &lt;item&gt;Blog&lt;/item&gt;
      &lt;item&gt;Fediverse&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Projects&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Maintained by MPTCP community members&lt;/item&gt;
      &lt;item&gt;Projects with MPTCP-related enhancements &lt;list rend="ul"&gt;&lt;item&gt;iproute2 (for the &lt;code&gt;ip mptcp&lt;/code&gt;command)&lt;/item&gt;&lt;item&gt;Network Manager: MPTCP features are included starting with v1.40.&lt;/item&gt;&lt;item&gt;Multipath TCP applications: A project to coordinate MPTCP updates for popular TCP applications.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;iproute2 (for the &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45566441</guid><pubDate>Mon, 13 Oct 2025 09:25:45 +0000</pubDate></item><item><title>American solar farms</title><link>https://tech.marksblogg.com/american-solar-farms.html</link><description>&lt;doc fingerprint="ab95e1f33b2412f0"&gt;
  &lt;main&gt;
    &lt;p&gt;Last week, Jake Stid, a postdoctoral research associate at Michigan State University, announced Ground-Mounted Solar Energy in the United States (GM-SEUS). This is a 15K-array, 2.9M-panel dataset of utility and commercial-grade solar farms across the lower 48 states plus the District of Columbia. This dataset was constructed by a team of researchers including alumni from NOAA, NASA and the USGS.&lt;/p&gt;
    &lt;p&gt;Below is a heatmap of the assets catalogued in this dataset.&lt;/p&gt;
    &lt;p&gt;GM-SEUS is broken up into two datasets, one for arrays and another panels. Below you can see a solar farm with the array outlined in red and the panels covered purple.&lt;/p&gt;
    &lt;p&gt;In this post, I'll explore GM-SEUS's Solar Farm dataset.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Workstation&lt;/head&gt;
    &lt;p&gt;I'm using a 5.7 GHz AMD Ryzen 9 9950X CPU. It has 16 cores and 32 threads and 1.2 MB of L1, 16 MB of L2 and 64 MB of L3 cache. It has a liquid cooler attached and is housed in a spacious, full-sized Cooler Master HAF 700 computer case.&lt;/p&gt;
    &lt;p&gt;The system has 96 GB of DDR5 RAM clocked at 4,800 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.&lt;/p&gt;
    &lt;p&gt;The system is powered by a 1,200-watt, fully modular Corsair Power Supply and is sat on an ASRock X870E Nova 90 Motherboard.&lt;/p&gt;
    &lt;p&gt;I'm running Ubuntu 24 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and ArcGIS Pro only supports Windows natively.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing Prerequisites&lt;/head&gt;
    &lt;p&gt;I'll use GDAL 3.9.3 and a few other tools to help analyse the data in this post.&lt;/p&gt;
    &lt;code&gt;$ sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable
$ sudo apt update
$ sudo apt install \
    gdal-bin \
    jq
&lt;/code&gt;
    &lt;p&gt;I'll use DuckDB v1.4.1, along with its H3, JSON, Lindel, Parquet and Spatial extensions, in this post.&lt;/p&gt;
    &lt;code&gt;$ cd ~
$ wget -c https://github.com/duckdb/duckdb/releases/download/v1.4.1/duckdb_cli-linux-amd64.zip
$ unzip -j duckdb_cli-linux-amd64.zip
$ chmod +x duckdb
$ ~/duckdb
&lt;/code&gt;
    &lt;code&gt;INSTALL h3 FROM community;
INSTALL lindel FROM community;
INSTALL json;
INSTALL parquet;
INSTALL spatial;
&lt;/code&gt;
    &lt;p&gt;I'll set up DuckDB to load every installed extension each time it launches.&lt;/p&gt;
    &lt;code&gt;$ vi ~/.duckdbrc
&lt;/code&gt;
    &lt;code&gt;.timer on
.width 180
LOAD h3;
LOAD lindel;
LOAD json;
LOAD parquet;
LOAD spatial;
&lt;/code&gt;
    &lt;p&gt;The maps in this post were mostly rendered with QGIS version 3.44. QGIS is a desktop application that runs on Windows, macOS and Linux. The application has grown in popularity in recent years and has ~15M application launches from users all around the world each month.&lt;/p&gt;
    &lt;p&gt;I used QGIS' Tile+ plugin to add basemaps from Esri to the maps in this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Analysis-Ready Data&lt;/head&gt;
    &lt;p&gt;I'll download a dataset containing the US CENSUS State codes. This will let me map the state ID in the arrays dataset to their state name.&lt;/p&gt;
    &lt;code&gt;$ wget https://gist.github.com/a8dx/2340f9527af64f8ef8439366de981168/raw/81d876daea10eab5c2675811c39bcd18a79a9212/US_State_Bounding_Boxes.csv
&lt;/code&gt;
    &lt;p&gt;I'll download the ZIP file of deliverables for GM-SEUS.&lt;/p&gt;
    &lt;code&gt;$ wget -O GMSEUS_v1_0.zip \
    'https://zenodo.org/records/14827819/files/GMSEUS_v1_0.zip?download=1'
$ unzip GMSEUS_v1_0.zip
&lt;/code&gt;
    &lt;p&gt;I'll extract the projection used. This proj4 string will be used to below to re-project the data into EPSG:4326.&lt;/p&gt;
    &lt;code&gt;$ gdalsrsinfo \
    -o proj4 \
    GMSEUS_v1_0/GPKG/GMSEUS_Arrays_Final.gpkg
&lt;/code&gt;
    &lt;code&gt;+proj=aea +lat_0=37.5 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs
&lt;/code&gt;
    &lt;p&gt;I'll use DuckDB to clean up the values and produce both a geometry field and a bounding box for each feature in this dataset. This will make working with this dataset remotely, such as from AWS S3, much easier.&lt;/p&gt;
    &lt;code&gt;$ ~/duckdb
&lt;/code&gt;
    &lt;p&gt;This following produced a ZStandard-compressed, spatially-sorted Parquet file of the arrays dataset. I dropped the Z dimension as it was unused. The unknown values have been turned into NULLs. The original GPKG file was 108 MB and the resulting Parquet file is 37 MB.&lt;/p&gt;
    &lt;code&gt;COPY (
   WITH a AS (
       SELECT * EXCLUDE(geom),
              ST_FORCE2D(
                ST_FLIPCOORDINATES(
                  ST_TRANSFORM(
                      geom,
                      '+proj=aea +lat_0=37.5 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs',
                      'EPSG:4326'))) geometry
       FROM   ST_READ('GMSEUS_v1_0/GPKG/GMSEUS_Arrays_Final.gpkg')
   )
   SELECT   a.* EXCLUDE (geometry,
                         tilt,
                         tiltEst,
                         instYr,
                         instYrLT,
                         effInit,
                         avgAzimuth,
                         avgLength,
                         avgSpace,
                         avgWidth),
            {'xmin': ST_XMIN(ST_EXTENT(geometry)),
             'ymin': ST_YMIN(ST_EXTENT(geometry)),
             'xmax': ST_XMAX(ST_EXTENT(geometry)),
             'ymax': ST_YMAX(ST_EXTENT(geometry))} AS bbox,
             ST_ASWKB(geometry) geometry,
             CASE WHEN instYr::INT     = -9999 THEN NULL ELSE instYr::INT   END AS instYr,
             CASE WHEN instYrLT::INT   = -9999 THEN NULL ELSE instYrLT::INT END AS instYrLT,
             CASE WHEN numRow::INT     = -9999 THEN NULL ELSE numRow::INT   END AS numRow,
             CASE WHEN tilt::INT       = -9999 THEN NULL ELSE tilt::INT     END AS tilt,
             CASE WHEN tiltEst::INT    = -9999 THEN NULL ELSE tiltEst::INT  END AS tiltEst,
             CASE WHEN effInit::INT    = -9999 THEN NULL ELSE effInit       END AS effInit,
             CASE WHEN avgAzimuth::INT = -9999 THEN NULL ELSE avgAzimuth    END AS avgAzimuth,
             CASE WHEN avgLength::INT  = -9999 THEN NULL ELSE avgLength     END AS avgLength,
             CASE WHEN avgSpace::INT   = -9999 THEN NULL ELSE avgSpace      END AS avgSpace,
             CASE WHEN avgWidth::INT   = -9999 THEN NULL ELSE avgWidth      END AS avgWidth,
             b.NAME state_name
   FROM     a
   JOIN     'US_State_Bounding_Boxes.csv' b ON a.STATEFP = b.STATEFP
   ORDER BY HILBERT_ENCODE([ST_Y(ST_CENTROID(geometry)),
                            ST_X(ST_CENTROID(geometry))]::double[2])
) TO 'arrays.parquet' (
   FORMAT            'PARQUET',
   CODEC             'ZSTD',
   COMPRESSION_LEVEL 22,
   ROW_GROUP_SIZE    15000);
&lt;/code&gt;
    &lt;p&gt;The original GPKG file for the panels dataset was 1.1 GB and the resulting Parquet file is 334 MB.&lt;/p&gt;
    &lt;code&gt;COPY (
   WITH a AS (
       SELECT * EXCLUDE(geom),
              ST_FORCE2D(
                ST_FLIPCOORDINATES(
                  ST_TRANSFORM(
                      geom,
                      '+proj=aea +lat_0=37.5 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs',
                      'EPSG:4326'))) geometry
       FROM   ST_READ('GMSEUS_v1_0/GPKG/GMSEUS_Panels_Final.gpkg')
   )
   SELECT   * EXCLUDE (geometry,
                       rowSpace),
            {'xmin': ST_XMIN(ST_EXTENT(geometry)),
             'ymin': ST_YMIN(ST_EXTENT(geometry)),
             'xmax': ST_XMAX(ST_EXTENT(geometry)),
             'ymax': ST_YMAX(ST_EXTENT(geometry))} AS bbox,
             ST_ASWKB(geometry) geometry,
             CASE WHEN rowSpace::INT = -9999 THEN NULL ELSE rowSpace END AS rowSpace
   FROM     a
   ORDER BY HILBERT_ENCODE([ST_Y(ST_CENTROID(geometry)),
                            ST_X(ST_CENTROID(geometry))]::double[2])
) TO 'panels.parquet' (
   FORMAT            'PARQUET',
   CODEC             'ZSTD',
   COMPRESSION_LEVEL 22,
   ROW_GROUP_SIZE    15000);
&lt;/code&gt;
    &lt;head rend="h2"&gt;Solar Arrays&lt;/head&gt;
    &lt;p&gt;The arrays Parquet file has 15,017 rows. Below is an example record.&lt;/p&gt;
    &lt;code&gt;$ echo "SELECT * EXCLUDE(bbox,
                         geometry),
               bbox::JSON bbox
        FROM   'arrays.parquet'
        LIMIT  1" \
    | ~/duckdb -json \
    | jq -S .
&lt;/code&gt;
    &lt;code&gt;[
  {
    "COUNTYFP": "019",
    "GCR1": 0.6996,
    "GCR2": 0.614,
    "STATEFP": "45",
    "Source": "OSM",
    "arrayID": 2807,
    "avgAzimuth": 170.63,
    "avgLength": 47.76166666666666,
    "avgSpace": 3.003333333333333,
    "avgWidth": 4.776666666666666,
    "bbox": {
      "xmax": -79.97229830431786,
      "xmin": -79.97325770533094,
      "ymax": 32.87833627192598,
      "ymin": 32.87808294640646
    },
    "capMW": 0.246,
    "capMWest": 0.246,
    "effInit": 0.197963503102977,
    "instYr": 2021,
    "instYrLT": 2021,
    "latitude": 32.87818725544087,
    "longitude": -79.97276617375104,
    "modType": "c-si",
    "mount": "fixed_axis",
    "nativeID": "9324",
    "newBound": 1,
    "numRow": 6.0,
    "numRow_1": 6,
    "state_name": "South Carolina",
    "tilt": 30,
    "tiltEst": 30,
    "totArea": 1779.0,
    "totRowArea": 1244.93,
    "version": "v1.0"
  }
]
&lt;/code&gt;
    &lt;p&gt;Below are the field names, data types, percentages of NULLs per column, number of unique values and minimum and maximum values for each column.&lt;/p&gt;
    &lt;code&gt;$ ~/duckdb
&lt;/code&gt;
    &lt;code&gt;SELECT   column_name,
         column_type,
         null_percentage,
         approx_unique,
         min,
         max
FROM     (SUMMARIZE
          FROM READ_PARQUET('arrays.parquet'))
WHERE    column_name != 'geometry'
AND      column_name != 'bbox'
ORDER BY 1;
&lt;/code&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ column_name â”‚ column_type â”‚ null_percentage â”‚ approx_unique â”‚         min         â”‚        max         â”‚
â”‚   varchar   â”‚   varchar   â”‚  decimal(9,2)   â”‚     int64     â”‚       varchar       â”‚      varchar       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ COUNTYFP    â”‚ VARCHAR     â”‚            0.00 â”‚           235 â”‚ 001                 â”‚ 810                â”‚
â”‚ GCR1        â”‚ DOUBLE      â”‚            0.00 â”‚          5057 â”‚ 0.1047              â”‚ 1.0                â”‚
â”‚ GCR2        â”‚ DOUBLE      â”‚            0.00 â”‚          5013 â”‚ 0.1245              â”‚ 0.988              â”‚
â”‚ STATEFP     â”‚ VARCHAR     â”‚            0.00 â”‚            49 â”‚ 01                  â”‚ 56                 â”‚
â”‚ Source      â”‚ VARCHAR     â”‚            0.00 â”‚             6 â”‚ CCVPV               â”‚ USPVDB             â”‚
â”‚ arrayID     â”‚ BIGINT      â”‚            0.00 â”‚         13155 â”‚ 1                   â”‚ 15017              â”‚
â”‚ avgAzimuth  â”‚ DOUBLE      â”‚           32.84 â”‚          4295 â”‚ 25.0                â”‚ 269.27             â”‚
â”‚ avgLength   â”‚ DOUBLE      â”‚           39.79 â”‚          8358 â”‚ 4.02                â”‚ 449.5004           â”‚
â”‚ avgSpace    â”‚ DOUBLE      â”‚           39.79 â”‚          8623 â”‚ 0.024               â”‚ 20.0               â”‚
â”‚ avgWidth    â”‚ DOUBLE      â”‚           39.79 â”‚          9185 â”‚ 0.67625             â”‚ 29.80222222222222  â”‚
â”‚ capMW       â”‚ DOUBLE      â”‚            0.00 â”‚          5280 â”‚ 0.001250225184651   â”‚ 1051.703           â”‚
â”‚ capMWest    â”‚ DOUBLE      â”‚            0.00 â”‚          7863 â”‚ 0.004               â”‚ 3170.1             â”‚
â”‚ effInit     â”‚ DOUBLE      â”‚            0.49 â”‚            39 â”‚ 0.132210289727273   â”‚ 0.205484167047619  â”‚
â”‚ instYr      â”‚ INTEGER     â”‚            0.00 â”‚            24 â”‚ 1985                â”‚ 2024               â”‚
â”‚ instYrLT    â”‚ INTEGER     â”‚            0.24 â”‚            17 â”‚ 2009                â”‚ 2023               â”‚
â”‚ latitude    â”‚ DOUBLE      â”‚            0.00 â”‚         16986 â”‚ 25.53796582594631   â”‚ 48.99547137225406  â”‚
â”‚ longitude   â”‚ DOUBLE      â”‚            0.00 â”‚         15656 â”‚ -124.10440474967092 â”‚ -67.15066374183608 â”‚
â”‚ modType     â”‚ VARCHAR     â”‚            0.00 â”‚             3 â”‚ c-si                â”‚ thin-film          â”‚
â”‚ mount       â”‚ VARCHAR     â”‚            0.00 â”‚            10 â”‚ dual_axis           â”‚ unknown            â”‚
â”‚ nativeID    â”‚ VARCHAR     â”‚            0.00 â”‚         15141 â”‚ 1                   â”‚ York Solar         â”‚
â”‚ newBound    â”‚ BIGINT      â”‚            0.00 â”‚             2 â”‚ 0                   â”‚ 1                  â”‚
â”‚ numRow      â”‚ DOUBLE      â”‚            0.00 â”‚          1461 â”‚ 0.0                 â”‚ 56782.0            â”‚
â”‚ numRow_1    â”‚ INTEGER     â”‚            0.00 â”‚          1117 â”‚ 0                   â”‚ 56782              â”‚
â”‚ state_name  â”‚ VARCHAR     â”‚            0.00 â”‚            57 â”‚ Alabama             â”‚ Wyoming            â”‚
â”‚ tilt        â”‚ INTEGER     â”‚           55.46 â”‚            47 â”‚ 0                   â”‚ 83                 â”‚
â”‚ tiltEst     â”‚ INTEGER     â”‚           55.46 â”‚            30 â”‚ 10                  â”‚ 43                 â”‚
â”‚ totArea     â”‚ DOUBLE      â”‚            0.00 â”‚         13182 â”‚ 54.0                â”‚ 13735113.0         â”‚
â”‚ totRowArea  â”‚ DOUBLE      â”‚            0.00 â”‚         15396 â”‚ 44.97               â”‚ 7223924.662        â”‚
â”‚ version     â”‚ VARCHAR     â”‚            0.00 â”‚             1 â”‚ v1.0                â”‚ v1.0               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 29 rows                                                                                      6 columns â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;I'll generate a heatmap of the asset locations in this dataset.&lt;/p&gt;
    &lt;code&gt;CREATE OR REPLACE TABLE h3_4_stats AS
    SELECT   H3_LATLNG_TO_CELL(
                bbox.ymin,
                bbox.xmin, 4) AS h3_4,
             COUNT(*) num_buildings
    FROM     READ_PARQUET('arrays.parquet')
    WHERE    bbox.xmin BETWEEN -178.5 AND 178.5
    GROUP BY 1;

COPY (
    SELECT ST_ASWKB(H3_CELL_TO_BOUNDARY_WKT(h3_4)::geometry) geometry,
           num_buildings
    FROM   h3_4_stats
) TO 'h3_4_stats.gpkg'
  WITH (FORMAT GDAL,
        DRIVER 'GPKG',
        LAYER_CREATION_OPTIONS 'WRITE_BBOX=YES');
&lt;/code&gt;
    &lt;p&gt;Normally I would produce a Parquet file as even with 10s of thousands of records it'll generate in seconds versus a minute or so with GPKG. But ArcGIS Pro 3.5 didn't want to open the Parquet file I generated. QGIS 3.44 was fine with it but I wanted to use Esri's Nova basemap for the rendering below.&lt;/p&gt;
    &lt;p&gt;ArcGIS Pro 3.6 should be released sometime in the next few weeks so I'll re-examine this issue when it's out.&lt;/p&gt;
    &lt;p&gt;Below is the relationship between the sources of data and the installation year.&lt;/p&gt;
    &lt;code&gt;PIVOT    'arrays.parquet'
ON       Source
USING    COUNT(*)
GROUP BY instYr
ORDER BY instYr;
&lt;/code&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ instYr â”‚ CCVPV â”‚ CWSD  â”‚ GMSEUSgeorect â”‚  OSM  â”‚  SAM  â”‚ USPVDB â”‚
â”‚ int32  â”‚ int64 â”‚ int64 â”‚     int64     â”‚ int64 â”‚ int64 â”‚ int64  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1985 â”‚     0 â”‚     0 â”‚             0 â”‚     0 â”‚     0 â”‚      1 â”‚
â”‚   1986 â”‚     0 â”‚     0 â”‚             0 â”‚     1 â”‚     0 â”‚      0 â”‚
â”‚   2002 â”‚     0 â”‚     0 â”‚             0 â”‚     0 â”‚     0 â”‚      1 â”‚
â”‚   2005 â”‚     0 â”‚     0 â”‚             0 â”‚    26 â”‚     0 â”‚      0 â”‚
â”‚   2006 â”‚     0 â”‚     0 â”‚             0 â”‚     2 â”‚     0 â”‚      1 â”‚
â”‚   2007 â”‚     0 â”‚     0 â”‚             0 â”‚    44 â”‚     0 â”‚      5 â”‚
â”‚   2008 â”‚     0 â”‚     0 â”‚             0 â”‚    58 â”‚     1 â”‚     11 â”‚
â”‚   2009 â”‚     5 â”‚     0 â”‚             0 â”‚    10 â”‚     5 â”‚     19 â”‚
â”‚   2010 â”‚    20 â”‚     0 â”‚             0 â”‚    71 â”‚    20 â”‚     37 â”‚
â”‚   2011 â”‚    24 â”‚     0 â”‚             2 â”‚   193 â”‚    30 â”‚    102 â”‚
â”‚   2012 â”‚    59 â”‚     0 â”‚             2 â”‚   267 â”‚    88 â”‚    157 â”‚
â”‚   2013 â”‚    83 â”‚     0 â”‚             3 â”‚   259 â”‚    82 â”‚    209 â”‚
â”‚   2014 â”‚   102 â”‚     0 â”‚             1 â”‚   335 â”‚   119 â”‚    291 â”‚
â”‚   2015 â”‚   107 â”‚     3 â”‚             0 â”‚   532 â”‚   125 â”‚    320 â”‚
â”‚   2016 â”‚   145 â”‚     1 â”‚             2 â”‚   564 â”‚   170 â”‚    412 â”‚
â”‚   2017 â”‚   135 â”‚     0 â”‚             1 â”‚   661 â”‚   167 â”‚    476 â”‚
â”‚   2018 â”‚    66 â”‚    34 â”‚             4 â”‚   644 â”‚   210 â”‚    414 â”‚
â”‚   2019 â”‚    28 â”‚    39 â”‚             6 â”‚   467 â”‚   178 â”‚    453 â”‚
â”‚   2020 â”‚    10 â”‚    75 â”‚             1 â”‚   437 â”‚   186 â”‚    496 â”‚
â”‚   2021 â”‚     5 â”‚    33 â”‚             6 â”‚   406 â”‚   241 â”‚    446 â”‚
â”‚   2022 â”‚     1 â”‚   173 â”‚             3 â”‚   231 â”‚   354 â”‚    166 â”‚
â”‚   2023 â”‚     0 â”‚     0 â”‚             3 â”‚   176 â”‚   722 â”‚    134 â”‚
â”‚   2024 â”‚     0 â”‚     0 â”‚             0 â”‚    31 â”‚  1571 â”‚      0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 23 rows                                               7 columns â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Below is the relationship between the mount and mod type.&lt;/p&gt;
    &lt;code&gt;PIVOT    'arrays.parquet'
ON       modType
USING    COUNT(*)
GROUP BY mount
ORDER BY mount;
&lt;/code&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    mount    â”‚ c-si  â”‚  csp  â”‚ thin-film â”‚
â”‚   varchar   â”‚ int64 â”‚ int64 â”‚   int64   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dual_axis   â”‚   301 â”‚    18 â”‚         1 â”‚
â”‚ fixed_axis  â”‚  6057 â”‚    32 â”‚       208 â”‚
â”‚ mixed       â”‚     2 â”‚     0 â”‚         0 â”‚
â”‚ mixed_df    â”‚   189 â”‚     7 â”‚         0 â”‚
â”‚ mixed_dfs   â”‚    94 â”‚     0 â”‚         0 â”‚
â”‚ mixed_ds    â”‚    38 â”‚     1 â”‚         0 â”‚
â”‚ mixed_fs    â”‚    60 â”‚     0 â”‚         1 â”‚
â”‚ single_axis â”‚  2876 â”‚    11 â”‚       231 â”‚
â”‚ unknown     â”‚  4885 â”‚     5 â”‚         0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Below are the array capacity counts rounded to the neared 100 MW and broken down by source.&lt;/p&gt;
    &lt;code&gt;WITH a AS (
    SELECT   Source,
             ROUND(capMW / 100) * 100 AS capacity,
             COUNT(*) num_recs
    FROM     'arrays.parquet'
    GROUP BY 1, 2
)
PIVOT    a
ON       Source
USING    SUM(num_recs)
GROUP BY capacity
ORDER BY capacity;
&lt;/code&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ capacity â”‚ CCVPV  â”‚  CWSD  â”‚ GMSEUSgeorect â”‚  OSM   â”‚  SAM   â”‚ USPVDB â”‚
â”‚  double  â”‚ int128 â”‚ int128 â”‚    int128     â”‚ int128 â”‚ int128 â”‚ int128 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      0.0 â”‚    790 â”‚    356 â”‚            33 â”‚   5295 â”‚   4022 â”‚   3669 â”‚
â”‚    100.0 â”‚   NULL â”‚      2 â”‚          NULL â”‚     67 â”‚    143 â”‚    350 â”‚
â”‚    200.0 â”‚   NULL â”‚   NULL â”‚             1 â”‚     22 â”‚     49 â”‚     73 â”‚
â”‚    300.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚     17 â”‚     21 â”‚     49 â”‚
â”‚    400.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚      6 â”‚     13 â”‚      7 â”‚
â”‚    500.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚      4 â”‚     11 â”‚      2 â”‚
â”‚    600.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚      2 â”‚      3 â”‚   NULL â”‚
â”‚    700.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚      1 â”‚      3 â”‚   NULL â”‚
â”‚    800.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚   NULL â”‚      2 â”‚      1 â”‚
â”‚    900.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚   NULL â”‚      1 â”‚   NULL â”‚
â”‚   1000.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚   NULL â”‚      1 â”‚   NULL â”‚
â”‚   1100.0 â”‚   NULL â”‚   NULL â”‚          NULL â”‚      1 â”‚   NULL â”‚   NULL â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 12 rows                                                     7 columns â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;head rend="h2"&gt;Solar Panels&lt;/head&gt;
    &lt;p&gt;The panels Parquet file has 2,917,782 rows. Below is an example record.&lt;/p&gt;
    &lt;code&gt;$ echo "SELECT * EXCLUDE(bbox,
                         geometry),
               bbox::JSON bbox
        FROM   'panels.parquet'
        LIMIT  1" \
    | ~/duckdb -json \
    | jq -S .
&lt;/code&gt;
    &lt;code&gt;[
  {
    "Source": "gmseus",
    "arrayID": 2807.0,
    "bbox": {
      "xmax": -79.97312295800064,
      "xmin": -79.97325770533483,
      "ymax": 32.87833627193374,
      "ymin": 32.87830393275682
    },
    "panelID": 2620732,
    "rowArea": 29.1,
    "rowAzimuth": 174.62,
    "rowLength": 12.77,
    "rowMount": "fixed_axis",
    "rowSpace": 8.42,
    "rowWidth": 3.0,
    "version": "v1.0"
  }
]
&lt;/code&gt;
    &lt;p&gt;Below are the field names, data types, percentages of NULLs per column, number of unique values and minimum and maximum values for each column.&lt;/p&gt;
    &lt;code&gt;$ ~/duckdb
&lt;/code&gt;
    &lt;code&gt;SELECT   column_name,
         column_type,
         null_percentage,
         approx_unique,
         min,
         max
FROM     (SUMMARIZE
          FROM READ_PARQUET('panels.parquet'))
WHERE    column_name != 'geometry'
AND      column_name != 'bbox'
ORDER BY 1;
&lt;/code&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ column_name â”‚ column_type â”‚ null_percentage â”‚ approx_unique â”‚      min      â”‚     max     â”‚
â”‚   varchar   â”‚   varchar   â”‚  decimal(9,2)   â”‚     int64     â”‚    varchar    â”‚   varchar   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Source      â”‚ VARCHAR     â”‚            0.00 â”‚             3 â”‚ CCVPV         â”‚ gmseus      â”‚
â”‚ arrayID     â”‚ DOUBLE      â”‚            0.08 â”‚          9451 â”‚ 1.0           â”‚ 15017.0     â”‚
â”‚ panelID     â”‚ BIGINT      â”‚            0.00 â”‚       2703164 â”‚ 1             â”‚ 2917782     â”‚
â”‚ rowArea     â”‚ DOUBLE      â”‚            0.00 â”‚         88974 â”‚ 15.01         â”‚ 1999.76     â”‚
â”‚ rowAzimuth  â”‚ DOUBLE      â”‚            0.00 â”‚         14901 â”‚ 90.0          â”‚ 270.0       â”‚
â”‚ rowLength   â”‚ DOUBLE      â”‚            0.00 â”‚         26759 â”‚ 4.02          â”‚ 530.05      â”‚
â”‚ rowMount    â”‚ VARCHAR     â”‚            0.00 â”‚             3 â”‚ dual_axis     â”‚ single_axis â”‚
â”‚ rowSpace    â”‚ DOUBLE      â”‚            0.17 â”‚         13376 â”‚ 7.4765186e-08 â”‚ 20.0        â”‚
â”‚ rowWidth    â”‚ DOUBLE      â”‚            0.00 â”‚          1863 â”‚ 0.45          â”‚ 102.14      â”‚
â”‚ version     â”‚ VARCHAR     â”‚            0.00 â”‚             1 â”‚ v1.0          â”‚ v1.0        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 10 rows                                                                         6 columns â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Below is the relationship between the sources of data and the row mount.&lt;/p&gt;
    &lt;code&gt;PIVOT    'panels.parquet'
ON       Source
USING    COUNT(*)
GROUP BY rowMount
ORDER BY rowMount;
&lt;/code&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  rowMount   â”‚ CCVPV  â”‚  OSM   â”‚ gmseus  â”‚
â”‚   varchar   â”‚ int64  â”‚ int64  â”‚  int64  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dual_axis   â”‚     44 â”‚   5975 â”‚   80225 â”‚
â”‚ fixed_axis  â”‚  13344 â”‚ 118639 â”‚  163512 â”‚
â”‚ single_axis â”‚ 189699 â”‚ 743371 â”‚ 1602973 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Of the 15,017 arrays in this dataset, only 5,358 have any panels in them.&lt;/p&gt;
    &lt;code&gt;.maxrows 20

SELECT   a.arrayID,
         COUNT(DISTINCT b.panelID)
FROM     READ_PARQUET('arrays.parquet') a
JOIN     READ_PARQUET('panels.parquet') b
ON       ST_COVERS(a.geometry, b.geometry)
GROUP BY a.arrayID
ORDER BY 2 DESC;
&lt;/code&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ arrayID â”‚ count(DISTINCT b.panelID) â”‚
â”‚  int64  â”‚           int64           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   11958 â”‚                     56762 â”‚
â”‚   14225 â”‚                     51140 â”‚
â”‚   12162 â”‚                     43741 â”‚
â”‚   12433 â”‚                     37304 â”‚
â”‚   14461 â”‚                     31898 â”‚
â”‚   13229 â”‚                     30093 â”‚
â”‚    6589 â”‚                     27080 â”‚
â”‚   13329 â”‚                     25120 â”‚
â”‚   12597 â”‚                     24054 â”‚
â”‚   12224 â”‚                     23449 â”‚
â”‚      Â·  â”‚                         Â· â”‚
â”‚      Â·  â”‚                         Â· â”‚
â”‚      Â·  â”‚                         Â· â”‚
â”‚    1792 â”‚                         1 â”‚
â”‚    2286 â”‚                         1 â”‚
â”‚     863 â”‚                         1 â”‚
â”‚    1816 â”‚                         1 â”‚
â”‚    8997 â”‚                         1 â”‚
â”‚   12358 â”‚                         1 â”‚
â”‚    6564 â”‚                         1 â”‚
â”‚    3845 â”‚                         1 â”‚
â”‚    6574 â”‚                         1 â”‚
â”‚     991 â”‚                         1 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5358 rows (20 shown)      2 columns â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Below is a solar farm in Nevada where some arrays have panels and others do not.&lt;/p&gt;
    &lt;p&gt;I was interested in seeing the solar farm with 56K panels. Below are its coordinates.&lt;/p&gt;
    &lt;code&gt;SELECT ST_CENTROID(geometry)
FROM   'arrays.parquet'
WHERE  arrayID = 11958;
&lt;/code&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             st_centroid(geometry)              â”‚
â”‚                    geometry                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ POINT (-115.34248808114013 35.611919498003175) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Even this has arrays without marked panels.&lt;/p&gt;
    &lt;p&gt;I'm looking forward to v2 of this dataset with better panel detection. It'll be great to get a good approximation of how many are deployed in the US.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45566638</guid><pubDate>Mon, 13 Oct 2025 10:02:40 +0000</pubDate></item><item><title>Two Paths to Memory Safety: CHERI and OMA</title><link>https://ednutting.com/2025/10/05/cheri-vs-oma.html</link><description>&lt;doc fingerprint="a4eb1c9974b7e359"&gt;
  &lt;main&gt;
    &lt;p&gt;The last year has been brutal for businesses globally. Taking examples from my home country, the UK, the cost is over Â£1B and still rising, as well as the loss of at least one life due to cybercrime.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Marks &amp;amp; Spencer lost Â£300M when ransomware crippled its systems for weeks.&lt;/item&gt;
      &lt;item&gt;The Co-op suffered a related attack, losing over Â£200M in sales and the customer data of more than 20 million people.&lt;/item&gt;
      &lt;item&gt;Jaguar Land Roverâ€™s assembly lines have been shut down for weeks, haemorrhaging Â£70M per week and requiring a Â£1.5B loan secured by the government.&lt;/item&gt;
      &lt;item&gt;Transport for Londonâ€™s systems were compromised, with the ensuing disruption lasting months, costing Â£39mn and exposing 5,000 customersâ€™ banking details. Two teenagers are being prosecuted for the attack.&lt;/item&gt;
      &lt;item&gt;Most tragically, a patient at Kingâ€™s College Hospital died after ransomware delayed critical blood test results. Speaking to friends that were sat in meetings to decide who got blood tests each day, the human toll was evident. Cyberattacks arenâ€™t just about money!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These arenâ€™t isolated incidents - theyâ€™re symptoms of a systemic vulnerability in how we build computer systems.&lt;/p&gt;
    &lt;p&gt;According to the Verizon 2025 Data Breach Investigations Report, credential abuse and exploitation of vulnerabilities continue to dominate as attack vectors, accounting for 22% and 20% of breaches respectively. The exploitation of vulnerabilities saw a 34% surge year-over-year, creating what Verizon describes as a â€œconcerning threat landscapeâ€.&lt;/p&gt;
    &lt;p&gt;Weâ€™re yet to learn the root causes and attack chains involved in each of the examples above, but many involved ransomware, which frequently uses software exploits as a post-initial-access vector to gain control of target systems and spread across a network.&lt;/p&gt;
    &lt;p&gt;Hereâ€™s the kicker: approximately 70% of all software vulnerabilities stem from a single root cause - memory safety issues. This isnâ€™t a new problem. Google, Microsoft, Apple, Mozilla and the Linux Foundation have all reported similar figures for their software over the last two decades. The uncomfortable truth is that current CPUs are fundamentally incapable of preventing these vulnerabilities, and traditional software patches have proven woefully inadequate.&lt;/p&gt;
    &lt;p&gt;Rewriting all the worldâ€™s software into memory safe languages, such as C#, Java and Rust, is unviable. While new projects may be adopting Rust over C/C++, and some critical components are being rewritten into safe languages, the scale and depth of the C and C++ ecosystems makes it practically impossible to rewrite all the worldâ€™s unsafe software. The risk of introducing other (non-memory-safety) issues during a software rewrite also poses a substantial barrier. Given sufficient software compatibility, it is actually easier to swap the hardware!&lt;/p&gt;
    &lt;p&gt;Two architectural approaches have emerged to tackle this trillion-dollar problem at the hardware level:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CHERI: Capability Hardware Enhanced RISC Instructions - pioneered at University of Cambridge (UK), and&lt;/item&gt;
      &lt;item&gt;OMA: Object Memory Architecture - pioneered at University of Bristol (UK) and now being commercialised by Doubtless Computing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both aim to make memory-unsafe systems safe-by-design but they take different paths to get there. Understanding these differences matters because the choice between them will shape the security and performance characteristics of computing for decades to come.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Memory Safety Crisis&lt;/head&gt;
    &lt;p&gt;Before diving into solutions, itâ€™s worth understanding what weâ€™re solving. When software runs, it constantly allocates and deallocates memory - think of it like booking rooms in a hotel. Memory safety vulnerabilities arise when this process goes wrong. If you stay in the same hotel twice, you shouldnâ€™t be able to access your old room even if you remember the number (use-after-free/ use-after-reallocate). Similarly, you shouldnâ€™t be able to enter a neighbouring room (buffer overflow), or use a room without booking one in the first place (invalid pointer dereference). Software has these same problems with memory allocations (room bookings).&lt;/p&gt;
    &lt;p&gt;These bugs become catastrophic vulnerabilities when attackers exploit them to read sensitive data they shouldnâ€™t access, manipulate critical system variables, or inject malicious code. The underlying architecture of todayâ€™s processors - paging-based virtual memory - lacks the granularity needed to enforce security within a single application or process.&lt;/p&gt;
    &lt;p&gt;Memory safety breaks down into three categories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Referential safety ensures pointers genuinely reference allocated memory and canâ€™t be forged. Think of it as ensuring software has a valid booking for a room, ensuring accesses to memory are authorized, and that bookings canâ€™t be faked.&lt;/item&gt;
      &lt;item&gt;Spatial safety prevents accessing memory outside allocated bounds - no going into neighbouring rooms.&lt;/item&gt;
      &lt;item&gt;Temporal safety addresses what happens over time, ensuring memory canâ€™t be accessed after itâ€™s been freed and reallocated. In our hotel analogy, a second stay at the hotel shouldnâ€™t allow you to access your previous room, even if you remember the room number.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Traditional architectures like x86, Arm, and RISC-V rely on coarse-grained page-level protection (typically 4KB or larger pages), which is far too blunt an instrument for modern security needs.&lt;/p&gt;
    &lt;head rend="h2"&gt;CHERI: Capabilities Meet Legacy Systems&lt;/head&gt;
    &lt;p&gt;CHERI, developed over more than a decade by the University of Cambridge and SRI International, extends conventional instruction set architectures with hardware-enforced capabilities. A CHERI capability is a form of fat pointer - it contains not just a memory address but also bounds information, permissions, and validity metadata. Every memory access gets checked against these constraints in hardware, catching violations before they can be exploited.&lt;/p&gt;
    &lt;p&gt;The architecture provides strong referential and spatial safety guarantees. When you have a CHERI capability, you provably have legitimate access to a specific bounded region of memory, and the hardware wonâ€™t let you stray outside those bounds. CHERI achieves this while maintaining compatibility with existing paged memory architectures, which is both its greatest strength and a source of limitations.&lt;/p&gt;
    &lt;p&gt;Hereâ€™s where it gets interesting: CHERIâ€™s capabilities are large. On a 64-bit system, a CHERI pointer requires 129 bits (including the hidden tag bit) - essentially double the data width of the base architecture. This decision to encode all protection metadata within the pointer itself has profound implications. Every data structure that stores pointers effectively doubles in memory consumption for those fields. Capabilities in memory (stack/heap) must be aligned to natural 128-bit boundaries. Cache lines, which are precious and limited, now hold fewer actual pointers. Memory bandwidth requirements increase because for each pointer youâ€™re moving twice as much data around.&lt;/p&gt;
    &lt;p&gt;CHERI provides hardware-enforced referential and spatial safety but leaves temporal safety to software. You can achieve temporal memory safety with CHERI, but it requires modifying your memory allocator and implementing pointer revocation mechanisms - essentially software to scan memory to find and invalidate stale pointers. This software-based approach to temporal safety remains part of the trusted computing base and requires careful verification. Itâ€™s also closely related to software garbage collection.&lt;/p&gt;
    &lt;p&gt;Research has explored various temporal safety mechanisms for CHERI, but they all involve non-trivial software complexity and performance overhead. In theory, hardware acceleration may be possible but is likely to always require software involvement. This is because a CHERI capability covers a range of memory, which may include more than one object. Software allocation and object type information is required to differentiate objects and thus revoke capabilities appropriately.&lt;/p&gt;
    &lt;p&gt;The software ecosystem for CHERI has made impressive progress. Most code recompiles with minimal changes, though the capability width difference can require significant rewrites for certain applications. Additionally, it causes a division in the ISA where load/stores of capabilities must be handled separately from ordinary data. This leads to some complexity in the compiler to detect edge cases where the compiler does not know for certain whether a register or memory slot contains a capability or not. C/C++ code which abuses pointers by treating them as integers, which is uncommon but frequent enough to cause a headache, requires some effort to address.&lt;/p&gt;
    &lt;p&gt;Armâ€™s Morello project, which implemented CHERI on a modified Neoverse N1 core, revealed performance challenges that have pushed commercial CHERI efforts toward smaller embedded processors for the time being. Notably, Arm declined to join the CHERI Alliance, instead indicating they will take a step back from new work on Morello and wait to see if CHERI gains the long-sought commercial traction.&lt;/p&gt;
    &lt;head rend="h2"&gt;OMA: Rethinking Memory From the Ground Up&lt;/head&gt;
    &lt;p&gt;Doubtless Computingâ€™s Object Memory Architecture takes a fundamentally different approach. Rather than extending paged memory, OMA implements object-based memory management directly in hardware. Every allocation becomes a first-class hardware object with its own identity, bounds, and metadata maintained by the processor itself.&lt;/p&gt;
    &lt;p&gt;This architectural choice enables several key advantages. OMA pointers are leaner - 65 bits on a 64-bit architecture, including the hidden tag bit. Rather than carrying all metadata with every pointer, OMA stores object information centrally in hardware-managed directories. This reduces memory bandwidth requirements and means that multiple pointers to the same object donâ€™t duplicate metadata. The hardware maintains a complete understanding of object relationships and lifecycles, enabling optimizations that software-only approaches canâ€™t match.&lt;/p&gt;
    &lt;p&gt;A critical differentiator is temporal safety. OMA implements garbage collection in hardware, scanning for and reclaiming unreachable objects in real-time as part of the processorâ€™s normal operation. This isnâ€™t the same as software garbage collection - itâ€™s parallel, highly optimized, and doesnâ€™t block program execution. By managing object lifecycles in hardware, OMA provides hardware-guaranteed temporal safety alongside referential and spatial protections, completing the trinity of memory safety properties.&lt;/p&gt;
    &lt;p&gt;It would be tempting to say that memory safety is solved by using a managed language like Java, JavaScript, Swift or Python. Unfortunately, this doesnâ€™t hold up in practice. Managed language runtimes, as well as many supporting libraries, are written in C/C++ and suffer memory safety issues just as much as any other C/C++ code. The operating systems and hypervisors are also exposed to these languages, offering yet another attack surface. This leaves managed language apps vulnerable. Memory safe languages, including both Rust and managed languages, are a distinct improvement over traditional C and C++, but only hardware can provide the safety guarantees we need in todayâ€™s systems.&lt;/p&gt;
    &lt;p&gt;For managed languages like Java, JavaScript, Python, C# and Go, the OMA architecture delivers dramatic performance improvements. Doubtless Computingâ€™s analysis of CPython 3.12 reveals that 32-44% of instructions are spent on memory management operations - allocation, deallocation, reference counting, and garbage collection. Moving these operations into parallel hardware execution, along with microarchitectural optimisations derived from hardwareâ€™s new understanding of the structure of data in memory, yields 2-5x speedups for managed language applications. Even C/C++ applications see 1.2-2x improvements as the hardware optimizes memory management functions and eliminates per-object metadata from cache.&lt;/p&gt;
    &lt;p&gt;The architecture maintains full source code compatibility for managed languages - all changes are confined to the runtime. For C/C++, the story is much the same as with CHERI: recompilation with modified standard libraries and a modified compiler, such as LLVM or GCC. Maintaining the pointer width the same as the data width, and the same alignment requirements, avoids the ISA-level split for handling pointers, which simplifies the compiler and improves compatibility with legacy C/C++ code. This compatibility approach differs from CHERIâ€™s and aligns with OMAâ€™s target market: server-class and application processors, where managed languages dominate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fundamental Trade-offs: Where the Architectures Diverge&lt;/head&gt;
    &lt;p&gt;The philosophical differences between CHERI and OMA create distinct trade-off profiles. CHERI carries all metadata with pointers, enabling incremental adoption where different parts of a program can use capabilities independently. OMAâ€™s centralized metadata requires the hardware to maintain a consistent view of all objects but enables more aggressive optimization. CHERI works within the existing paged memory model, simplifying system software migration. OMA introduces a new memory model that requires deeper changes but delivers performance gains that paged architectures canâ€™t match.&lt;/p&gt;
    &lt;p&gt;These differences manifest in pointer width - CHERIâ€™s 129-bit capabilities versus OMAâ€™s 65-bit pointers. While both exceed the base address width, the doubling effect in CHERI has more severe implications for data structure layouts, cache efficiency, and memory bandwidth. Research on CHERI implementations has shown there is a long road ahead to achieve performance parity for managed languages. In the meantime, OMA offers a shorter path with substantial speedups rather than equal performance.&lt;/p&gt;
    &lt;p&gt;Temporal safety represents perhaps the most significant divergence in security. CHERIâ€™s software-based pointer revocation requires explicit memory scanning and manipulation, adding complexity to the trusted computing base and verification burden. OMAâ€™s hardware garbage collection happens transparently and continuously, providing stronger guarantees with less software complexity. This matters enormously for total cost of ownership - every line of security-critical software that doesnâ€™t need to be written, verified, and maintained is a win.&lt;/p&gt;
    &lt;p&gt;The instruction set philosophies differ too. CHERI historically opts for ISA changes beyond pure memory safety to achieve its security goals, which can complicate adoption. OMA has historically prioritized backward compatibility, though this is adaptable based on market requirements. The consensus in the industry is that software compatibility presents the primary barrier to new processor designs, which favours architectures that minimize disruption.&lt;/p&gt;
    &lt;head rend="h2"&gt;Industrial Relevance and Market Fit&lt;/head&gt;
    &lt;p&gt;CHERI and OMA target fundamentally different computing environments, which is why calling them competitors misses the point. Theyâ€™re complementary solutions to a shared problem, each optimized for distinct use cases.&lt;/p&gt;
    &lt;p&gt;CHERI finds its natural home in embedded systems and microcontrollers. These environments predominantly use C, C++, or Rust with restricted or no dynamic memory allocation. The code bases are smaller and more amenable to the verification required to ensure CHERI capabilities are used correctly. The memory overhead from wider pointers, while still present, matters less in resource-constrained designs that carefully manage every allocation. Four companies - SCI Semiconductor, Codasip, lowRISC, and Secqai - are actively commercializing CHERI for embedded applications. SCIâ€™s ICENI family of CHERIoT microcontrollers, built on Microsoftâ€™s open-source CHERIoT-Ibex core, targets the IoT and operational technology markets. Codasip offers CHERI-enabled RISC-V IP cores for custom processor designs. lowRISCâ€™s Sonata platform provides an open-source FPGA-based development environment for CHERIoT research and prototyping.&lt;/p&gt;
    &lt;p&gt;Armâ€™s experience with CHERI tells an important story about scaling limitations. The Morello project, which implemented CHERI on a modified Neoverse N1 server-class core, yielded results that Arm appears to have found unsatisfactory. There has been no apparent follow-up on the substantial initial investment made into the Arm Morello designs. This assessment seems to reflect the performance challenges that CHERI faces in larger systems.&lt;/p&gt;
    &lt;p&gt;OMAâ€™s sweet spot sits at the opposite end of the spectrum. Application-class and server-class processors running managed languages benefit enormously from hardware-accelerated memory management. Python, Java, JavaScript, C#, and Go all share similar memory models that align naturally with OMAâ€™s object-based approach. These environments already use garbage collection extensively, so moving that functionality into hardware removes overhead, rather than adding it as it would in embedded systems. The performance gains - up to 5x for managed languages - become transformational for data centre workloads where every percentage point of efficiency translates to millions in operating costs.&lt;/p&gt;
    &lt;p&gt;The market dynamics favour different adoption paths. CHERI benefits from strong government backing, particularly from the now-ended UKâ€™s Digital Security by Design programme and recognition from the US White House and NSA. This institutional support hopes to accelerate adoption in defence and critical infrastructure applications. CHERIâ€™s open-source foundation through the CHERI Alliance creates a broad ecosystem but limits opportunities for proprietary differentiation.&lt;/p&gt;
    &lt;p&gt;OMAâ€™s proprietary nature and performance advantages position it for commercial data centre deployment. The technology directly addresses the performance problems that hindered CHERI at scale. While OMA lacks CHERIâ€™s first-mover advantage and government momentum, it offers compelling value for cloud providers and enterprises running managed language workloads. The economic argument is straightforward: if you can eliminate 70% of vulnerabilities while quintupling performance for your Python services, the return on investment is measured in weeks from deployment, rather than years.&lt;/p&gt;
    &lt;p&gt;OMAâ€™s proprietary technology makes it attractive for investment as it can be patented. However, CHERIâ€™s openness makes it possible for independent security teams to verify the safety of the architecture. Open implementations of CHERI processors also enables those designs to be independently verified. Doubtless Computing will need to make its ISA public, which is inevitable anyway for a new CPU as customers will require it. Doubtless will also need to offer a public platform for independent researchers to build confidence in the security claims.&lt;/p&gt;
    &lt;head rend="h2"&gt;The CHERI Ecosystem: Whoâ€™s Building What&lt;/head&gt;
    &lt;p&gt;The CHERI Alliance, formally launched in 2024, coordinates standardization and adoption efforts across industry and academia. Founding members include the FreeBSD Foundation, Capabilities Limited, SCI Semiconductor, Codasip, lowRISC, and the University of Cambridge. Googleâ€™s participation as a founding member signals serious industry interest, though notably Arm is not a member.&lt;/p&gt;
    &lt;p&gt;SCI Semiconductor, based in Cambridge, leads commercialization of Microsoftâ€™s open-source CHERIoT Ibex implementation for embedded systems. Their ICENI family of processors targets microcontroller applications in automotive, industrial control, defence, and aerospace. The company has secured strategic distribution through EPS Global, which specializes in automotive tier-one suppliers and contract manufacturers. SCIâ€™s early access program, in collaboration with lowRISC, allows select partners to begin development on lowRISCâ€™s FPGA-based Sonata platform with guaranteed migration paths to production silicon.&lt;/p&gt;
    &lt;p&gt;Codasip, a RISC-V processor IP vendor, offers the X730 - a CHERI-enabled 64-bit application-class core based on their A730 design. Their â€œCustom Computeâ€ methodology allows customers to license CHERI-enhanced cores or customize them further using Codasip Studio. The company has donated a CHERI SDK built on open-source tools to the CHERI Alliance, making it freely available for anyone implementing CHERI on RISC-V. Codasip is also developing Linux kernel support for RISC-V CHERI, which will be crucial for broader adoption.&lt;/p&gt;
    &lt;p&gt;lowRISC, a not-for-profit organization spun out of Cambridge University, maintains the Sonata evaluation platform and leads the UK-government-funded Sunburst Project. Sonata provides a complete FPGA-based development environment for CHERIoT, enabling software development and hardware experimentation before silicon is available. The Sunburst Projectâ€™s recent expansion to include SCI Semiconductor aims to validate CHERIoT designs through commercial tapeout on GlobalFoundriesâ€™ 22nm process, with all project deliverables remaining open-source.&lt;/p&gt;
    &lt;p&gt;zeroRISC is a startup and partner in the OpenTitan project administered by lowRISC. Their goal is to commercialise the OpenTitan silicon IP through their Integrity Management Platform.&lt;/p&gt;
    &lt;p&gt;Microsoftâ€™s role deserves special mention. Microsoft Research developed CHERIoT-Ibex, an open-source RISC-V core optimized for embedded systems. Theyâ€™ve made this core freely available and co-maintain the CHERIoT Platform repository with SCI Semiconductor. David Weston, Microsoftâ€™s VP of Enterprise and OS Security, has publicly endorsed SCIâ€™s commercialization efforts, stating that CHERI represents a â€œpromising technology that can be used to enhance computer security.â€ This corporate backing from a major software vendor adds credibility to the embedded CHERI ecosystem.&lt;/p&gt;
    &lt;p&gt;The UK governmentâ€™s support through the now-ended Digital Security by Design programme and UKRI funding has been instrumental in advancing CHERI. The programme provided ~Â£190 million in research funding over five years and continues to support development through initiatives like Sunburst. This institutional backing, combined with endorsements from the US White House and NSA, positions CHERI advantageously for government and defence procurements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;CHERI and OMA represent two responses to the memory safety crisis, each with distinct strengths that make them suited to different computing environments. The notion that one must â€œwinâ€ while the other â€œlosesâ€ misunderstands the landscape - the computing world is large enough, and varied enough, that multiple approaches can and should coexist. Cybersecurity principles also demand diversity of solutions.&lt;/p&gt;
    &lt;p&gt;CHERIâ€™s compatibility with existing paged memory architectures and incremental deployment model make it an excellent fit for embedded systems where code bases are manageable, languages are predominantly C/C++/Rust, and the verification burden is acceptable. The active CHERI ecosystem, backed by government support and open-source collaboration, has created momentum that shouldnâ€™t be underestimated. For IoT devices, industrial control systems, and safety-critical embedded applications, CHERI offers a practical path to hardware-enforced memory safety that companies can adopt today.&lt;/p&gt;
    &lt;p&gt;OMAâ€™s object-based architecture and integrated hardware garbage collection (IHGC) deliver transformational performance for managed language workloads. By tackling temporal safety in hardware alongside referential and spatial protections, OMA provides more complete memory safety with less software complexity. The performance gains - up to 5x for Python, Java, JavaScript, C#, and Go - directly address the scalability problems that have limited CHERI in larger systems. For data centres, cloud infrastructure, and application servers where managed languages dominate, OMA presents compelling advantages.&lt;/p&gt;
    &lt;p&gt;Both architectures eliminate memory safety vulnerabilities. The formal guarantees that CHERI can provide are a subset of what OMA delivers, since OMA includes hardware-enforced temporal safety. However, CHERIâ€™s earlier start and ecosystem momentum matter significantly in technology adoption. The question isnâ€™t which architecture is â€œbetterâ€ in absolute terms but rather which is more appropriate for specific use cases and deployment contexts.&lt;/p&gt;
    &lt;p&gt;Looking ahead, memory safety will increasingly become a non-negotiable requirement. The UK National Cyber Security Centre, US White House, and NSA have all called for fundamental changes in how we build secure systems. The attacks on Marks &amp;amp; Spencer, Co-op, Jaguar Land Rover, the NHS, Transport for London, and many others, demonstrate that our current approach isnâ€™t working. Software-only solutions like Rust, while valuable, face adoption barriers that make them insufficient on their own. Hardware-based memory safety, whether through CHERI, OMA, or future approaches we havenâ€™t yet invented, represents the most practical path to eliminating this class of vulnerability at scale.&lt;/p&gt;
    &lt;p&gt;The semiconductor industry moves slowly, with design cycles measured in years and deployment timelines measured in decades. Todayâ€™s architectural decisions will shape computing security through 2040 and beyond. The good news is that we now have proven approaches to memory safety that work in real hardware. CHERI has demonstrated its viability in embedded systems. OMA has shown it can deliver both security and performance for managed languages with a hardware prototype on AWS Cloud FPGAs supporting CPython 3.12 and Jupyter Notebooks. The challenge now isnâ€™t technical feasibility - itâ€™s economic deployment and ecosystem coordination.&lt;/p&gt;
    &lt;p&gt;For embedded designers, CHERI offers immediate benefits with manageable overhead. For cloud and data centre operators, OMA promises to eliminate vulnerabilities while dramatically improving performance. The fundamental insight is that both approaches work by making the right choices for their target markets. We donâ€™t need to pick one winner. We need both, deployed where each makes the most sense, steadily displacing the insecure architectures that enabled the attacks weâ€™ve seen this year. The trillion-dollar memory safety problem is solvable - and the will to deploy the solutions weâ€™ve built is growing as organisations can no longer afford the risk of being vulnerable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45566660</guid><pubDate>Mon, 13 Oct 2025 10:05:50 +0000</pubDate></item><item><title>Matrices can be your Friends</title><link>https://www.sjbaker.org/steve/omniv/matrices_can_be_your_friends.html</link><description>&lt;doc fingerprint="2eac01b79ba60999"&gt;
  &lt;main&gt;
    &lt;p&gt;Take an OpenGL matrix:&lt;/p&gt;
    &lt;quote&gt;float m [ 16 ] ;Consider this as a 4x4 array with it's elements laid out into four columns like this:&lt;/quote&gt;
    &lt;quote&gt;m[0] m[4] m[ 8] m[12] m[1] m[5] m[ 9] m[13] m[2] m[6] m[10] m[14] m[3] m[7] m[11] m[15]WARNING: Mathematicians like to see their matrices laid out on paper this way (with the array indices increasing down the columns instead of across the rows as a programmer would usually write them). Look CAREFULLY at the order of the matrix elements in the layout above!&lt;/quote&gt;
    &lt;p&gt;...but we are OpenGL programmers - not mathematicians - right?! The reason OpenGL arrays are laid out in what some people would consider to be the opposite direction to mathematical convention is somewhat lost in the mists of time. However, it turns out to be a happy accident as we will see later.&lt;/p&gt;
    &lt;p&gt;If you are dealing with a matrix which only deals with rigid bodies (ie no scale, shear, squash, etc) then the last row (array elements 3,7,11 and 15) are always 0,0,0 and 1 respectively and so long as they always maintain those values, we can safely forget about them for now.&lt;/p&gt;
    &lt;p&gt;The first three elements of the rightmost column of the matrix is just the overall translation. If you imagine some kind of neat little compact object (like a teapot), then array elements 12,13 and 14 tell you where it is in the world. It doesn't matter what combinations of rotations and translations it took to produce the matrix, the rightmost column tells you where the object basically is. It is often fortunate that the OpenGL matrix array is laid out the way it is because it results in those three elements being consecutive in memory.&lt;/p&gt;
    &lt;p&gt;OK, so now we are down to only nine random-looking numbers. These are the top three elements of each of the first three columns - and collectively they represent the rotation of the object.&lt;/p&gt;
    &lt;p&gt;The easy way to decode those numbers is to imagine what happens to four points near to the origin after they are transformed by the matrix:&lt;/p&gt;
    &lt;quote&gt;(0,1,0) | /(0,0,1) | / |/___(1,0,0) (0,0,0)These are four vertices on a 1x1x1 cube that has one corner at the origin.&lt;/quote&gt;
    &lt;p&gt;After the matrix has transformed this cube, where does it end up?&lt;/p&gt;
    &lt;p&gt;Well, if we neglect the translation part (the bottom row), then the pure rotation part simply describes the new location of the points on the cube:&lt;/p&gt;
    &lt;quote&gt;(1,0,0) ---&amp;gt; ( m[0], m[1], m[2] ) (0,1,0) ---&amp;gt; ( m[4], m[5], m[6] ) (0,0,1) ---&amp;gt; ( m[8], m[9], m[10]) (0,0,0) ---&amp;gt; ( 0, 0, 0 )After that, you just add the translation onto each point so that:&lt;/quote&gt;
    &lt;quote&gt;(1,0,0) ---&amp;gt; ( m[0], m[1], m[2] ) + ( m[12], m[13], m[14] ) (0,1,0) ---&amp;gt; ( m[4], m[5], m[6] ) + ( m[12], m[13], m[14] ) (0,0,1) ---&amp;gt; ( m[8], m[9], m[10]) + ( m[12], m[13], m[14] ) (0,0,0) ---&amp;gt; ( 0, 0, 0 ) + ( m[12], m[13], m[14] )Once you know this, it becomes quite easy to use matrices to position objects exactly where you need them without messing around with multiple calls to glRotate.&lt;/quote&gt;
    &lt;p&gt;Just imagine a little cube at the origin - pretend it's firmly attached to your model. Think about where the cube ends up as the model moves - write down where it's vertices would end up and there is your matrix.&lt;/p&gt;
    &lt;p&gt;So, if I gave you this matrix:&lt;/p&gt;
    &lt;quote&gt;0.707, -0.707, 0, 10 0.707, 0.707, 0, 10 0 , 0 , 1, 0 0 , 0 , 0, 1...you could easily see that the X axis of that little cube is now pointing somewhere between the X and Y axes, the Y axis is pointing somewhere between Y and negative X and the Z axis is unchanged. The entire cube has been moved 10 units off in X and Y. This is a 45 degree rotation about Z and a 10,10,0 translation! You didn't need any hard math - just a mental picture of what the little cube did - and no concerns about the order of operations or anything hard like that. What would have happened to something out at 100,100,0? Well, just imagine it was glued to the cube (on the end of a long stick)...as the cube rotated, the thing at 100,100 would have moved quite a bit too - in fact, you can see that the rotation would put it onto the Y axis and the translation would have moved it 10 units up and to the right.&lt;/quote&gt;
    &lt;p&gt;With practice, you can figure out what that last row of numbers does to the little cube too.&lt;/p&gt;
    &lt;p&gt;So, would you like to know how to use a matrix to squash, stretch, shear, etc? Just think about where the axes of that little cube end up - write them down and you are done. What does a cube of jello look like when there is a strong wind blowing from X=-infinity?&lt;/p&gt;
    &lt;quote&gt;1, 0.3, 0, 0 0, 0.9, 0, 0 0, 0 , 1, 0 0, 0 , 0, 1Look - the Y axis is leaning a third of a unit to the right and the cube got a bit shorter.&lt;/quote&gt;
    &lt;p&gt;Suppose your cartoon character is going to jump vertically, and you want to do a bit of pre-squash before the jump... and post-stretch during the jump. Just gradually vary the matrix from:&lt;/p&gt;
    &lt;quote&gt;1 , 0 , 0, 0 1 , 0 , 0, 0 0 , 0.8, 0, 0 0 , 1.2, 0, 0 0 , 0 , 1, 0 ===&amp;gt; 0 , 0 , 1, 0 0 , 0 , 0, 1 0 , 0 , 0, 1Not bad - he got shorter then longer - how about getting a bit fatter too (conservation of cartoon volume) ?&lt;/quote&gt;
    &lt;quote&gt;1.2, 0 , 0 , 0 0.9,0 , 0 , 0 0 , 0.8, 0 , 0 0 ,1.2, 0 , 0 0 , 0 , 1.2, 0 ===&amp;gt; 0 ,0 ,0.9, 0 0 , 0 , 0 , 1 0 ,0 , 0 , 1Now the cube got smaller in Y and bigger in X and Z then got bigger in Y and smaller in X/Z...easy!&lt;/quote&gt;
    &lt;p&gt;Not only is it easier to think transforms out this way, but it's invariably more efficient too. By seeing the entire transformation as one whole operation on a unit cube, you save a long sequence of glRotate/glTranslate/glScale commands - which each imply a complicated set of multiply/add steps to concatenate the new transform with whatever is on the top of the stack.&lt;/p&gt;
    &lt;p&gt;Finally, there is one matrix that we all need to know - the "Identity" matrix:&lt;/p&gt;
    &lt;quote&gt;1, 0, 0, 0 0, 1, 0, 0 0, 0, 1, 0 0, 0, 0, 1As you can see, this matrix leaves all the axes completely alone and performs no translation. This is a "do nothing" matrix.&lt;/quote&gt;
    &lt;p&gt;Matrices are really easy - it's just a matter of looking at them pictorially.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45566766</guid><pubDate>Mon, 13 Oct 2025 10:23:08 +0000</pubDate></item><item><title>The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel 2025</title><link>https://www.nobelprize.org/prizes/economic-sciences/2025/summary/</link><description>&lt;doc fingerprint="d53d327d895121e6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel 2025 was awarded "for having explained innovation-driven economic growth" with one half to Joel Mokyr "for having identified the prerequisites for sustained growth through technological progress" and the other half jointly to Philippe Aghion and Peter Howitt "for the theory of sustained growth through creative destruction"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Nobel Prizes and laureates&lt;/head&gt;
    &lt;p&gt;Six prizes were awarded for achievements that have conferred the greatest benefit to humankind. The 14 laureates' work and discoveries range from quantum tunnelling to promoting democratic rights.&lt;/p&gt;
    &lt;p&gt;See them all presented here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Explore prizes and laureates&lt;/head&gt;
    &lt;p&gt; Look for popular awards and laureates in different fields, and discover the history of the Nobel Prize. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45567153</guid><pubDate>Mon, 13 Oct 2025 11:23:20 +0000</pubDate></item><item><title>Show HN: SQLite Online â€“ 11 years of solo development, 11K daily users</title><link>https://sqliteonline.com/</link><description>&lt;doc fingerprint="5120987566fd4fbd"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;Chart for Data Science&lt;/head&gt;
    &lt;code&gt;-- Change first word "SELECT" to "QLINE-SELECT"&lt;/code&gt;
    &lt;quote&gt;SELECT QLINE-SELECT&lt;/quote&gt;
    &lt;code&gt;Ã¢&lt;/code&gt;
    &lt;code&gt;-- Axis X:&lt;/code&gt;
    &lt;code&gt;-- X - column name, axis: x1, x2, ..xn Value: Number&lt;/code&gt;
    &lt;code&gt;-- L - column name, axis: l Value: Text&lt;/code&gt;
    &lt;code&gt;-- T - column name, axis: t Value: UnixTime Number&lt;/code&gt;
    &lt;code&gt;-- Axis Y:&lt;/code&gt;
    &lt;code&gt;-- Y - column name, axis: y1, y2, ..yn Value: Number&lt;/code&gt;
    &lt;code&gt;-- Y - color line: y_cFF00FF (HEX6)&lt;/code&gt;
    &lt;code&gt;-- Option:&lt;/code&gt;
    &lt;code&gt;-- C - color point: c  Value: FF00FF (HEX6)&lt;/code&gt;
    &lt;code&gt;-- V - radius point: v  Value: Number&lt;/code&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QLINE-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QAREA-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QBAR-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QPIE-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QBUBBLE-SELECT example&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45567770</guid><pubDate>Mon, 13 Oct 2025 12:46:52 +0000</pubDate></item><item><title>Smartphones and being present</title><link>https://herman.bearblog.dev/being-present/</link><description>&lt;doc fingerprint="3411991d2ac390d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Smartphones and being present&lt;/head&gt;
    &lt;p&gt;I read an article yesterday, stating that on average, people spend 4 hours and 37 minutes on their phones per day1, with South Africans coming in fourth highest in the world at a whopping 5 hours and 11 minutes2.&lt;/p&gt;
    &lt;p&gt;This figure seems really high to me. If we assume people sleep roughly 8 hours per day, that means that one third of their day is spent on their phones. If we also assume people work 8 hours per day (ignoring the fact that they may be using their phones during work hours), that suggests that people spend over half of their free time (and up to 65% of it) glued to their screens.&lt;/p&gt;
    &lt;p&gt;I never wanted to carry the internet around in my pocket. It's too distracting and pulls me out of the present moment, fracturing my attention. I've tried switching to old-school black and white phones before, but always begrudgingly returned to using a smartphone due to the utility of it. The problem, however, is that it comes with too many attention sinks tucked in alongside the useful tools.&lt;/p&gt;
    &lt;p&gt;I care about living an intentional and meaningful life, nurturing relationships, having nuanced conversations, and enjoying the world around me. I don't want to spend this limited time I have on earth watching short form video and getting into arguments on Twitter.&lt;/p&gt;
    &lt;p&gt;This is what I enjoy. Picture taken yesterday in Scarborough, South Africa.&lt;/p&gt;
    &lt;p&gt;I've written at length about how I manage my digital consumption, from turning off notifications to forgoing social media entirely. The underlying premise here is that if you're trying to lose weight, you shouldn't carry cookies around in your pockets. And my phone is the bag of cookies in this metaphor.&lt;/p&gt;
    &lt;p&gt;We're wired to seek out distraction, novel information, and entertainment, and avoid boredom at all costs. But boredom is where creativity and self-reflection do their best work. It's why "all the best ideas come when you're in the shower"â€”we don't usually take our phones with us into the shower (yet).&lt;/p&gt;
    &lt;p&gt;According to Screen Time on my iPhone, on average I spend 30 minutes per day on it, which I think is reasonable, especially considering the most-used apps are by-and-large utility apps like banking and messages. This isn't because I have more self-control than other people. I don't think I do. It's because I know myself, and have set up my digital life to be a positive force, and not an uninspired time-sink.&lt;/p&gt;
    &lt;p&gt;There are many apps and systems to incentivise better relationships with our phones, mostly based around time limits. But these are flawed in three ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I'm an adult, I know how to circumvent these limits, and I will if motivation is low.&lt;/item&gt;
      &lt;item&gt;Time limits don't affect the underlying addiction. You don't quit smoking by only smoking certain hours of the day.&lt;/item&gt;
      &lt;item&gt;The companies that build these apps have tens of thousands of really smart people (and billions of dollars) trying to get me hooked and keep me engaged. The only way to win this game isn't by trying to beat them (I certainly can't), but by not playing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The only way I've found to have a good relationship with my phone is to make it as uninteresting as possible. The first way is to not have recommendation media (think Instagram, TikTok, and all the rest). I'm pro deleting these accounts completely, because it's really easy to re-download the apps on a whim, or visit them in-browser. However some people have found that having them on a dedicated device works by isolating those activities. Something like a tablet at home that is "the only place you're allowed to use Instagram". I can't comment too much on this route, but it seems reasonable.&lt;/p&gt;
    &lt;p&gt;My biggest time sink over the past few years has been YouTube. The algorithm knew me too well and would recommend video after engaging, but ultimately useless video. I could easily burn an entire evening watching absolute junkâ€”leaving me feeling like I'd just wasted what could have otherwise been a beautiful sunset or a tasty home-cooked lasagne. However, at the beginning of this year I learnt that you can turn off your YouTube watch history entirely, which means no recommendations. Here's what my YouTube home screen now looks like:&lt;/p&gt;
    &lt;p&gt;Without the recommendations I very quickly run out of things to watch from the channels I'm subscribed to. It's completely changed my relationship with YouTube since I only watch the videos I actually want to watch, and none of the attention traps. You can turn off your YouTube watch history here, and auto delete your other Google history (like historic searches and navigation) here, which I think is just good practice.&lt;/p&gt;
    &lt;p&gt;I also used my adblocker, AdGuard on Safari which has a useful "block element" feature, to block the recommended videos on the right of YouTube videos. I use this feature to hide shorts as well, since I have no interest in watching them either, and YouTube intentionally makes them impossible to remove. If you're interested in a similar setup, here are the selectors I use to block those elements:&lt;/p&gt;
    &lt;code&gt;youtube.com###items &amp;gt; ytd-item-section-renderer.style-scope.ytd-watch-next-secondary-results-renderer:last-child
youtube.com###sections
youtube.com##[is-shorts]
youtube.com###secondary
&lt;/code&gt;
    &lt;p&gt;The only media that I do sometimes consume on my phone are my RSS feeds, but it's something I'm completely comfortable with since it's explicitly opt-in by design and low volume.&lt;/p&gt;
    &lt;p&gt;While I still have the twitch to check my phone when I'm waiting for a coffee, or in-between activitiesâ€”because my brain's reward system has been trained to do thisâ€”I'm now rewarded with nothing. Over time, I find myself checking my phone less and less. Sometimes I notice the urge, and just let it go, instead focusing on the here and now.&lt;/p&gt;
    &lt;p&gt;I think that while the attention-span-degrading effects of recommendation media are getting most of the headlines, what isn't spoken about as much is the sheer number of hours lost globally to our phones (3.8 million years per day, according to my back-of-the-napkin-math). And while people may argue that this could involve productive work or enjoyable leisure, I suspect that the vast (vast!) majority of that time is short-form entertainment.&lt;/p&gt;
    &lt;p&gt;My solution may sound overkill to many people, but I can say with absolute certainty that it has turned me into a more present, less distracted, and more optimistic person. I have much more time to spend in nature, with friends, or on my hobbies and projects. I can't imagine trading it in for a tiny screen, ever.&lt;/p&gt;
    &lt;p&gt;Give it a try.&lt;/p&gt;
    &lt;p&gt;Happily on the beach for sunset.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568613</guid><pubDate>Mon, 13 Oct 2025 14:20:33 +0000</pubDate></item><item><title>Software update bricks some Jeep 4xe hybrids over the weekend</title><link>https://arstechnica.com/cars/2025/10/software-update-bricks-some-jeep-4xe-hybrids-over-the-weekend/</link><description>&lt;doc fingerprint="155561cbaa040794"&gt;
  &lt;main&gt;
    &lt;p&gt;Owners of some Jeep Wrangler 4xe hybrids have been left stranded after installing an over-the-air software update this weekend. The automaker pushed out a telematics update for the Uconnect infotainment system that evidently wasn't ready, resulting in cars losing power while driving and then becoming stranded.&lt;/p&gt;
    &lt;p&gt;Stranded Jeep owners have been detailing their experiences in forum and Reddit posts, as well as on YouTube. The buggy update doesn't appear to brick the car immediately. Instead, the failure appears to occur while drivingâ€”a far more serious problem. For some, this happened close to home and at low speed, but others claim to have experienced a powertrain failure at highway speeds.&lt;/p&gt;
    &lt;p&gt;Jeep pulled the update after reports of problems, but the software had already downloaded to many owners' cars by then. A member of Stellantis' social engagement team told 4xe owners at a Jeep forum to ignore the update pop-up if they haven't installed it yet.&lt;/p&gt;
    &lt;p&gt;Owners were also advised to avoid using either hybrid or electric modes if they had updated their 4xe and not already suffered a powertrain failure. Yesterday, Jeep pushed out a fix.&lt;/p&gt;
    &lt;p&gt;As Crowdstrike showed last year, Friday afternoons are a bad time to push out a software update. Now Stellantis has learned that lesson, too. Ars has reached out to Stellantis, and we'll update this post if we get a reply.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568700</guid><pubDate>Mon, 13 Oct 2025 14:28:25 +0000</pubDate></item><item><title>Ofcom fines 4chan Â£20K and counting for violating UK's Online Safety Act</title><link>https://www.theregister.com/2025/10/13/4chan_ofcom_fine/</link><description>&lt;doc fingerprint="8ecb095014b22f02"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ofcom fines 4chan Â£20K and counting for pretending UK's Online Safety Act doesn't exist&lt;/head&gt;
    &lt;head rend="h2"&gt;Regulator warns penalties will pile up until internet toilet does its paperwork&lt;/head&gt;
    &lt;p&gt;Ofcom, the UK's Online Safety Act regulator, has fined online message board 4chan Â£20,000 ($26,680) for failing to protect children from harmful content.&lt;/p&gt;
    &lt;p&gt;The fine could rise by a further Â£6,000 â€“ Â£100 per day for a maximum 60 days â€“ if it continues to ignore its duties to comply with the regulator's request for information regarding two separate matters.&lt;/p&gt;
    &lt;p&gt;4chan can stop the additional fines by providing copies of its illegal content risk assessments and information about its qualifying worldwide revenue to Ofcom.&lt;/p&gt;
    &lt;p&gt;The enforcement action announced today is months in the making after Ofcom first opened an investigation into the notorious image board on June 10.&lt;/p&gt;
    &lt;p&gt;It requested the aforementioned risk assessments on April 14, and to this day 4chan still has not complied, the regulator said.&lt;/p&gt;
    &lt;p&gt;When opening the investigation, Ofcom said it was looking to understand whether 4chan has failed, or is failing, to abide by its duties under the Online Safety Act.&lt;/p&gt;
    &lt;p&gt;The watchdog also highlighted that the maximum penalties for these failures, as specified in the legislation, are Â£18 million ($24 million) or 10 percent of an organization's qualifying worldwide revenue, whichever is greater.&lt;/p&gt;
    &lt;p&gt;The Register contacted 4chan for its side of the story.&lt;/p&gt;
    &lt;p&gt;Ofcom's fine is the first made under the Online Safety Act since in-scope organizations' illegal content duties came into force on March 17. It also announced two provisional decisions to take action against file-sharing service Im.ge and pornography service provider AVS Group Ltd for similar failures to respond to information requests.&lt;/p&gt;
    &lt;p&gt;In Im.ge's case, this relates to its duty to implement measures to prevent the circulation of child sexual abuse material (CSAM), and AVS Group was rapped over its duty to implement age-check mechanisms.&lt;/p&gt;
    &lt;p&gt;Both organizations have the chance to appeal Ofcom's findings before it makes a final decision on how to reprimand them.&lt;/p&gt;
    &lt;p&gt;Another porn provider, Youngtek Solutions Ltd, is also under an expanded investigation over its failure to respond to information requests regarding age-checking requirements.&lt;/p&gt;
    &lt;p&gt;Tech secretary Liz Kendall said: "The Online Safety Act is not just law, it's a lifeline. Today we've seen it in action, holding platforms to account so we can protect people across the UK.&lt;/p&gt;
    &lt;p&gt;"Services can no longer ignore illegal content, like encouraging self-harm or suicide, circulating online which can devastate young lives and leave families shattered.&lt;/p&gt;
    &lt;p&gt;"This fine is a clear warning to those who fail to remove illegal content or protect children from harmful material. We fully back the regulator in taking action against all platforms that do not protect users from the darkest corners of the internet."&lt;/p&gt;
    &lt;p&gt;In total, since March 2025, Ofcom has opened 21 investigations into the providers of in-scope apps and websites, and launched five enforcement programs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Playing by the rules&lt;/head&gt;
    &lt;p&gt;In brighter news, others under Ofcom investigation have shown improvements, and several of these cases are now closed.&lt;/p&gt;
    &lt;p&gt;Four file-sharing services under investigation for their child safety measures avoided further action by geo-blocking UK users, much to Ofcom's delight. Krakenfiles, Nippydrive, Nippyshare, and Nippyspace have all blocked British IP addresses instead of following other measures set out in the regulator's codes of practice.&lt;/p&gt;
    &lt;p&gt;Ofcom said it has closed the cases into these sites, and that the measures have "significantly reduced the likelihood that people in the UK will be exposed to any illegal or harmful content."&lt;/p&gt;
    &lt;p&gt;"We will continue to monitor their availability in the UK and reserve the right to reopen our investigations if we have reason to do so. We are pursuing further lines of inquiry against file-sharing services Nippybox and Yolobit, and these investigations remain ongoing."&lt;/p&gt;
    &lt;p&gt;A suicide forum is also now geo-blocking UK IPs after Ofcom began enforcement proceedings.&lt;/p&gt;
    &lt;p&gt;Satisfied for now, the regulator said it will keep tabs on the unnamed provider to see whether that block remains in place over the long term, and ensure it does not provide information on how to bypass it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord says 70,000 photo IDs compromised in customer service breach&lt;/item&gt;
      &lt;item&gt;Germany slams brakes on EU's Chat Control device-scanning snoopfest&lt;/item&gt;
      &lt;item&gt;Imgur yanks Brit access to memes as parent company faces fine&lt;/item&gt;
      &lt;item&gt;Charities warn Ofcom too soft on Online Safety Act violators&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bypassing these measures has been a hot point of discussion since the Online Safety Act's most noticeable rules came into force in July, triggering a surge in VPN subscriptions within days of Brits having to submit their ID cards for age verification purposes.&lt;/p&gt;
    &lt;p&gt;While platforms are forbidden from guiding users toward these types of workarounds, this alone is unlikely to prevent VPNs being used to bypass geo-blocks and similar measures.&lt;/p&gt;
    &lt;p&gt;They do not appear to be going anywhere either. The UK government has previously stated that it does not wish to ban them, since they have many legitimate purposes. But if platforms promote VPNs and other workarounds to children as a means to access their services, then Ofcom will pursue action against them.&lt;/p&gt;
    &lt;head rend="h3"&gt;First look at beefed-up requirements&lt;/head&gt;
    &lt;p&gt;Among Ofcom's proposed amendments to its obligations to platforms was the requirement for in-scope apps and websites to make use of hash-matching technology, which is seen as a more accurate, automated way of preventing the dissemination of illegal content such as CSAM.&lt;/p&gt;
    &lt;p&gt;Hash matching involves a system fingerprinting an image and comparing the hash it generates to a database of known harmful images, which are also hashed. If an image's hash matches or shows signs of similarity with one in the database, then it can be removed entirely autonomously and reported to local authorities for follow-up investigations.&lt;/p&gt;
    &lt;p&gt;Ofcom previously identified "serious compliance concerns" with its CSAM enforcement program at 1Fichier.com and Gofile.io, leading to investigations being opened into them both.&lt;/p&gt;
    &lt;p&gt;After constructive engagement with the regulator, both now deploy hash-matching tech and escaped further action.&lt;/p&gt;
    &lt;p&gt;Suzanne Cater, director of enforcement at Ofcom, said: "Today sends a clear message that any service which flagrantly fails to engage with Ofcom and their duties under the Online Safety Act can expect to face robust enforcement action.&lt;/p&gt;
    &lt;p&gt;"We're also seeing some services take steps to introduce improved safety measures as a direct result of our enforcement action. Services who choose to restrict access rather than protect UK users remain on our watchlist as we continue to monitor their availability to UK users." Â®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568767</guid><pubDate>Mon, 13 Oct 2025 14:34:40 +0000</pubDate></item><item><title>A16Z-backed data firms Fivetran, dbt Labs to merge in all-stock deal</title><link>https://www.reuters.com/business/a16z-backed-data-firms-fivetran-dbt-labs-merge-all-stock-deal-2025-10-13/</link><description>&lt;doc fingerprint="a990c55b08b8a7d1"&gt;
  &lt;main&gt;
    &lt;p&gt;Oct 13 (Reuters) - Data startups Fivetran and dbt Labs will merge in an all-stock deal, creating a combined data infrastructure company with nearly $600 million in annual revenue, the two companies told Reuters.&lt;/p&gt;
    &lt;p&gt;The deal is structured as an all-stock exchange based on an agreed ratio tied to revenues and growth rates, Fivetran Chief Executive George Fraser said in an interview. The combined entity is worth more than its last private valuation, Fraser said, but added that the valuation will ultimately be determined by the market.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;Fivetran was last publicly valued at $5.6 billion in September 2021, while dbt Labs was valued at $4.2 billion in a Series D round in February 2022. The companies share some investors, including Andreessen Horowitz.&lt;/p&gt;
    &lt;p&gt;Upon closing, Fraser will serve as CEO of the combined company, while dbt Labs CEO Tristan Handy will become co-founder and president.&lt;/p&gt;
    &lt;p&gt;The transaction, which unites two highly valued venture-backed startups, marks significant consolidation in the data tooling market as enterprises race to adapt infrastructure for artificial intelligence applications, which require organized access to internal data.&lt;/p&gt;
    &lt;p&gt;"The thing is really unique about this combination is our emphasis on open infrastructure and interoperability... as everyone is trying to figure out how to use their business data in the context of AI," Fraser said.&lt;/p&gt;
    &lt;p&gt;Oakland, California-based Fivetran specializes in automated data movement, helping companies pull information from various sources into a central data warehouse. Philadelphia-based dbt Labs created dbt, an open-source tool for transforming and preparing that data for analysis.&lt;/p&gt;
    &lt;p&gt;Describing the companies' products as complementary, Fraser estimated that 80% to 90% of Fivetran's customers use dbt's tools. The new company will also keep dbt Core, the popular open-source version of dbt Labs' software, available under its current license.&lt;/p&gt;
    &lt;p&gt;The goal is to build a more comprehensive platform for enterprises' data needs, Fraser said, with the increased scale and broader platform strengthening its position for a public listing, even though an IPO is not imminent.&lt;/p&gt;
    &lt;p&gt;Fraser added that the deal is a merger of equals rather than an acquisition as the new company's board will have representation from both Fivetran and dbt Labs, and that the firm will be near cash-flow break-even.&lt;/p&gt;
    &lt;p&gt;Fivetran and dbt Labs expect the deal to close within a year. The Information had earlier reported on talks between the two firms.&lt;/p&gt;
    &lt;p&gt;Reporting by Krystal Hu in San Francisco; Editing by Janane Venkatraman&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568842</guid><pubDate>Mon, 13 Oct 2025 14:42:19 +0000</pubDate></item><item><title>America is getting an AI gold rush instead of a factory boom</title><link>https://www.washingtonpost.com/business/2025/10/13/manufacturing-artificial-intelligence/</link><description>&lt;doc fingerprint="177340801723f9e0"&gt;
  &lt;main&gt;&lt;p&gt;A gulf is opening up in the heart of American business as two industries championed as central to the countryâ€™s future â€” manufacturing and artificial intelligence â€” appear to be heading in different directions.&lt;/p&gt;&lt;p&gt;By Aaron Gregg&lt;/p&gt; and &lt;list rend="ul"&gt;&lt;item&gt;1Meryl KornfieldandHannah NatansonHistoric wave of retirements is putting huge strains on the government&lt;/item&gt;&lt;item&gt;2Shannon OsakaMicroplastics are everywhere. You can do one simple thing to avoid them.&lt;/item&gt;&lt;item&gt;3OpinionYan WuStruggling with Gen Z slang? Embrace the delulu.&lt;/item&gt;&lt;item&gt;4Si LibermanIâ€™m 101 years old. Here are 7 things I think are â€˜longevity secrets.â€™&lt;/item&gt;&lt;item&gt;5Niha Masih,Kelly Kasulis Cho,Abbie Cheeseman,Leo Sands,Cate BrownandMaham JavaidLive updates: Trump says war in Gaza â€˜is over,â€™ arrives in Egypt as Israel celebrates release of hostages&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568915</guid><pubDate>Mon, 13 Oct 2025 14:48:47 +0000</pubDate></item><item><title>AI and the Future of American Politics</title><link>https://www.schneier.com/blog/archives/2025/10/ai-and-the-future-of-american-politics.html</link><description>&lt;doc fingerprint="bd10923da2f9c589"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;AI and the Future of American Politics&lt;/head&gt;
    &lt;p&gt;Two years ago, Americans anxious about the forthcoming 2024 presidential election were considering the malevolent force of an election influencer: artificial intelligence. Over the past several years, we have seen plenty of warning signs from elections worldwide demonstrating how AI can be used to propagate misinformation and alter the political landscape, whether by trolls on social media, foreign influencers, or even a street magician. AI is poised to play a more volatile role than ever before in Americaâ€™s next federal election in 2026. We can already see how different groups of political actors are approaching AI. Professional campaigners are using AI to accelerate the traditional tactics of electioneering; organizers are using it to reinvent how movements are built; and citizens are using it both to express themselves and amplify their sideâ€™s messaging. Because there are so few rules, and so little prospect of regulatory action, around AIâ€™s role in politics, there is no oversight of these activities, and no safeguards against the dramatic potential impacts for our democracy.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Campaigners&lt;/head&gt;
    &lt;p&gt;Campaignersâ€”messengers, ad buyers, fundraisers, and strategistsâ€”are focused on efficiency and optimization. To them, AI is a way to augment or even replace expensive humans who traditionally perform tasks like personalizing emails, texting donation solicitations, and deciding what platforms and audiences to target.&lt;/p&gt;
    &lt;p&gt;This is an incremental evolution of the computerization of campaigning that has been underway for decades. For example, the progressive campaign infrastructure group Tech for Campaigns claims it used AI in the 2024 cycle to reduce the time spent drafting fundraising solicitations by one-third. If AI is working well here, you wonâ€™t notice the difference between an annoying campaign solicitation written by a human staffer and an annoying one written by AI.&lt;/p&gt;
    &lt;p&gt;But AI is scaling these capabilities, which is likely to make them even more ubiquitous. This will make the biggest difference for challengers to incumbents in safe seats, who see AI as both a tacitly useful tool and an attention-grabbing way to get their race into the headlines. Jason Palmer, the little-known Democratic primary challenger to Joe Biden, successfully won the American Samoa primary while extensively leveraging AI avatars for campaigning.&lt;/p&gt;
    &lt;p&gt;Such tactics were sometimes deployed as publicity stunts in the 2024 cycle; they were firsts that got attention. Pennsylvania Democratic Congressional candidate Shamaine Daniels became the first to use a conversational AI robocaller in 2023. Two long-shot challengers to Rep. Don Beyer used an AI avatar to represent the incumbent in a live debate last October after he declined to participate. In 2026, voters who have seen years of the official White House X account posting deepfaked memes of Donald Trump will be desensitized to the use of AI in political communications.&lt;/p&gt;
    &lt;p&gt;Strategists are also turning to AI to interpret public opinion data and provide more fine-grained insight into the perspective of different voters. This might sound like AIs replacing people in opinion polls, but it is really a continuation of the evolution of political polling into a data-driven science over the last several decades.&lt;/p&gt;
    &lt;p&gt;A recent survey by the American Association of Political Consultants found that a majority of their membersâ€™ firms already use AI regularly in their work, and more than 40 percent believe it will â€œfundamentally transformâ€ the future of their profession. If these emerging AI tools become popular in the midterms, it wonâ€™t just be a few candidates from the tightest national races texting you three times a day. It may also be the member of Congress in the safe district next to you, and your state representative, and your school board members.&lt;/p&gt;
    &lt;p&gt;The development and use of AI in campaigning is different depending on what side of the aisle you look at. On the Republican side, Push Digital Group is going â€œall inâ€ on a new AI initiative, using the technology to create hundreds of ad variants for their clients automatically, as well as assisting with strategy, targeting, and data analysis. On the other side, the National Democratic Training Committee recently released a playbook for using AI. Quiller is building an AI-powered fundraising platform aimed at drastically reducing the time campaigns spend producing emails and texts. Progressive-aligned startups Chorus AI and BattlegroundAI are offering AI tools for automatically generating ads for use on social media and other digital platforms. DonorAtlas automates data collection on potential donors, and RivalMind AI focuses on political research and strategy, automating the production of candidate dossiers.&lt;/p&gt;
    &lt;p&gt;For now, there seems to be an investment gap between Democratic- and Republican-aligned technology innovators. Progressive venture fund Higher Ground Labs boasts $50 million in deployed investments since 2017 and a significant focus on AI. Republican-aligned counterparts operate on a much smaller scale. Startup Caucus has announced one investmentâ€”of $50,000â€”since 2022. The Center for Campaign Innovation funds research projects and events, not companies. This echoes a longstanding gap in campaign technology between Democratic- and Republican-aligned fundraising platforms ActBlue and WinRed, which has landed the former in Republicansâ€™ political crosshairs.&lt;/p&gt;
    &lt;p&gt;Of course, not all campaign technology innovations will be visible. In 2016, the Trump campaign vocally eschewed using data to drive campaign strategy and appeared to be falling way behind on ad spending, but wasâ€”we learned in retrospectâ€”actually leaning heavily into digital advertising and making use of new controversial mechanisms for accessing and exploiting votersâ€™ social media data with vendor Cambridge Analytica. The most impactful uses of AI in the 2026 midterms may not be known until 2027 or beyond.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Organizers&lt;/head&gt;
    &lt;p&gt;Beyond the realm of political consultants driving ad buys and fundraising appeals, organizers are using AI in ways that feel more radically new.&lt;/p&gt;
    &lt;p&gt;The hypothetical potential of AI to drive political movements was illustrated in 2022 when a Danish artist collective used an AI model to found a political party, the Synthetic Party, and generate its policy goals. This was more of an art project than a popular movement, but it demonstrated that AIsâ€”synthesizing the expressions and policy interests of humansâ€”can formulate a political platform. In 2025, Denmark hosted a â€œsummitâ€ of eight such AI political agents where attendees could witness â€œcontinuously orchestrate[d] algorithmic micro-assemblies, spontaneous deliberations, and impromptu policy-makingâ€ by the participating AIs.&lt;/p&gt;
    &lt;p&gt;The more viable version of this concept lies in the use of AIs to facilitate deliberation. AIs are being used to help legislators collect input from constituents and to hold large-scale citizen assemblies. This kind of AI-driven â€œsensemakingâ€ may play a powerful role in the future of public policy. Some research has suggested that AI can be as or more effective than humans in helping people find common ground on controversial policy issues.&lt;/p&gt;
    &lt;p&gt;Another movement for â€œPublic AIâ€ is focused on wresting AI from the hands of corporations to put people, through their governments, in control. Civic technologists in national governments from Singapore, Japan, Sweden, and Switzerland are building their own alternatives to Big Tech AI models, for use in public administration and distribution as a public good.&lt;/p&gt;
    &lt;p&gt;Labor organizers have a particularly interesting relationship to AI. At the same time that they are galvanizing mass resistance against the replacement or endangerment of human workers by AI, many are racing to leverage the technology in their own work to build power.&lt;/p&gt;
    &lt;p&gt;Some entrepreneurial organizers have used AI in the past few years as tools for activating, connecting, answering questions for, and providing guidance to their members. In the UK, the Centre for Responsible Union AI studies and promotes the use of AI by unions; theyâ€™ve published several case studies. The UK Public and Commercial Services Union has used AI to help their reps simulate recruitment conversations before going into the field. The Belgian union ACV-CVS has used AI to sort hundreds of emails per day from members to help them respond more efficiently. Software companies such as Quorum are increasingly offering AI-driven products to cater to the needs of organizers and grassroots campaigns.&lt;/p&gt;
    &lt;p&gt;But unions have also leveraged AI for its symbolic power. In the U.S., the Screen Actors Guild held up the specter of AI displacement of creative labor to attract public attention and sympathy, and the ETUC (the European confederation of trade unions) developed a policy platform for responding to AI.&lt;/p&gt;
    &lt;p&gt;Finally, some union organizers have leveraged AI in more provocative ways. Some have applied it to hacking the â€œbosswareâ€ AI to subvert the exploitative intent or disrupt the anti-union practices of their managers.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Citizens&lt;/head&gt;
    &lt;p&gt;Many of the tasks weâ€™ve talked about so far are familiar use cases to anyone working in office and management settings: writing emails, providing user (or voter, or member) support, doing research.&lt;/p&gt;
    &lt;p&gt;But even mundane tasks, when automated at scale and targeted at specific ends, can be pernicious. AI is not neutral. It can be applied by many actors for many purposes. In the hands of the most numerous and diverse actors in a democracyâ€”the citizensâ€”that has profound implications.&lt;/p&gt;
    &lt;p&gt;Conservative activists in Georgia and Florida have used a tool named EagleAI to automate challenging voter registration en masse (although the toolâ€™s creator later denied that it uses AI). In a nonpartisan electoral management context with access to accurate data sources, such automated review of electoral registrations might be useful and effective. In this hyperpartisan context, AI merely serves to amplify the proclivities of activists at the extreme of their movements. This trend will continue unabated in 2026.&lt;/p&gt;
    &lt;p&gt;Of course, citizens can use AI to safeguard the integrity of elections. In Ghanaâ€™s 2024 presidential election, civic organizations used an AI tool to automatically detect and mitigate electoral disinformation spread on social media. The same year, Kenyan protesters developed specialized chatbots to distribute information about a controversial finance bill in Parliament and instances of government corruption.&lt;/p&gt;
    &lt;p&gt;So far, the biggest way Americans have leveraged AI in politics is in self-expression. About ten million Americans have used the chatbot Resistbot to help draft and send messages to their elected leaders. Itâ€™s hard to find statistics on how widely adopted tools like this are, but researchers have estimated that, as of 2024, about one in five consumer complaints to the U.S. Consumer Financial Protection Bureau was written with the assistance of AI.&lt;/p&gt;
    &lt;p&gt;OpenAI operates security programs to disrupt foreign influence operations and maintains restrictions on political use in its terms of service, but this is hardly sufficient to deter use of AI technologies for whatever purpose. And widely available free models give anyone the ability to attempt this on their own.&lt;/p&gt;
    &lt;p&gt;But this could change. The most ominous sign of AIâ€™s potential to disrupt elections is not the deepfakes and misinformation. Rather, it may be the use of AI by the Trump administration to surveil and punish political speech on social media and other online platforms. The scalability and sophistication of AI tools give governments with authoritarian intent unprecedented power to police and selectively limit political speech.&lt;/p&gt;
    &lt;head rend="h3"&gt;What About the Midterms?&lt;/head&gt;
    &lt;p&gt;These examples illustrate AIâ€™s pluripotent role as a force multiplier. The same technology used by different actorsâ€”campaigners, organizers, citizens, and governmentsâ€”leads to wildly different impacts. We canâ€™t know for sure what the net result will be. In the end, it will be the interactions and intersections of these uses that matters, and their unstable dynamics will make future elections even more unpredictable than in the past.&lt;/p&gt;
    &lt;p&gt;For now, the decisions of how and when to use AI lie largely with individuals and the political entities they lead. Whether or not you personally trust AI to write an email for you or make a decision about you hardly matters. If a campaign, an interest group, or a fellow citizen trusts it for that purpose, they are free to use it.&lt;/p&gt;
    &lt;p&gt;It seems unlikely that Congress or the Trump administration will put guardrails around the use of AI in politics. AI companies have rapidly emerged as among the biggest lobbyists in Washington, reportedly dumping $100 million toward preventing regulation, with a focus on influencing candidate behavior before the midterm elections. The Trump administration seems open and responsive to their appeals.&lt;/p&gt;
    &lt;p&gt;The ultimate effect of AI on the midterms will largely depend on the experimentation happening now. Candidates and organizations across the political spectrum have ample opportunityâ€”but a ticking clockâ€”to find effective ways to use the technology. Those that do will have little to stop them from exploiting it.&lt;/p&gt;
    &lt;p&gt;This essay was written with Nathan E. Sanders, and originally appeared in The American Prospect.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568955</guid><pubDate>Mon, 13 Oct 2025 14:51:51 +0000</pubDate></item><item><title>Vodafone admits 'major outage' as more than 130,000 report problems</title><link>https://www.bbc.co.uk/news/articles/c5yldldx659o</link><description>&lt;doc fingerprint="f4b179711c30d6c1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Vodafone admits 'major outage' as more than 130,000 report problems&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Published&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thousands of Vodafone customers across the UK have reported its services are down.&lt;/p&gt;
    &lt;p&gt;Downdetector, which monitors web outages, showed more than 130,000 people had flagged problems affecting their Vodafone broadband or mobile network on Monday afternoon.&lt;/p&gt;
    &lt;p&gt;According to its website, the firm has more than 18 million customers in the UK, including nearly 700,000 home broadband customers.&lt;/p&gt;
    &lt;p&gt;In an updated statement on Monday evening, Vodafone apologised to customers and said its network was "recovering".&lt;/p&gt;
    &lt;p&gt;"This afternoon the Vodafone network had an issue affecting broadband, 4G and 5G services," a company spokesperson said.&lt;/p&gt;
    &lt;p&gt;"2G voice calls and SMS messaging were unaffected and the network is now recovering.&lt;/p&gt;
    &lt;p&gt;"We apologise for any inconvenience this caused our customers."&lt;/p&gt;
    &lt;p&gt;It comes after people on social media said they were struggling to access Vodafone customer service operators, amid ongoing issues affecting mobile data and broadband.&lt;/p&gt;
    &lt;p&gt;Many also said they have had difficulty accessing the company's website and app, which typically allow people to view the status of its network services.&lt;/p&gt;
    &lt;p&gt;Customers have also taken to social media to complain of "complete outages" in their area.&lt;/p&gt;
    &lt;p&gt;The issues appear to have begun for customers shortly after 15:00 BST.&lt;/p&gt;
    &lt;p&gt;Internet monitor Netblocks said in a post on X, external that live network data showed Vodafone was experiencing "a national outage" impacting both broadband and mobile data.&lt;/p&gt;
    &lt;p&gt;Some customers expressed being doubly frustrated by not being able to access their Wi-Fi or mobile data.&lt;/p&gt;
    &lt;p&gt;"Sort it out soon please," wrote one frustrated X user - who said they were having to use a coffee shop's Wi-Fi to access online services, without the means to do so using their mobile data or broadband.&lt;/p&gt;
    &lt;p&gt;Another said they were self-employed and could not work because of the outage, adding: "Never regretted more having my mobile and broadband on the same network."&lt;/p&gt;
    &lt;p&gt;The issues are also understood to have impacted some Vodafone shops.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Dropped off the internet'&lt;/head&gt;
    &lt;p&gt;The issues affecting Vodafone services have also impacted customers of other telecoms firms that use its network.&lt;/p&gt;
    &lt;p&gt;Downdetector saw a similar spike in reports on Monday afternoon from users of the mobile network Voxi, which is owned by Vodafone.&lt;/p&gt;
    &lt;p&gt;Lebara, which piggy-backs off Vodafone's network, has also been affected by the company's outage.&lt;/p&gt;
    &lt;p&gt;"Outages have been reported across multiple networks across broadband and mobile services," said Sabrina Hoque, telecoms expert at Uswitch.&lt;/p&gt;
    &lt;p&gt;These, she added, can be "a really frustrating experience for customers, especially when it's not clear how long it could last".&lt;/p&gt;
    &lt;p&gt;Vodafone has not yet said how long it expects its outage to last - though its website since appears to have come back online.&lt;/p&gt;
    &lt;p&gt;Cloudflare Radar, which tracks and displays patterns in global internet traffic, said in a post on Bluesky, external earlier it had "effectively dropped off the internet, with traffic dropping to zero".&lt;/p&gt;
    &lt;p&gt;The company has also not said what caused the issue affecting its networks.&lt;/p&gt;
    &lt;p&gt;"Incidents like this are often caused by a technical fault or configuration error rather than a major cyber-attack, so until more details are confirmed it's best not to speculate," said Daniel Card, a cyber expert with BCS, The Chartered Institute for IT.&lt;/p&gt;
    &lt;p&gt;"Having teams capable of diagnosing and responding rapidly to network failures is key to maintaining public trust and keeping the UK's digital infrastructure running smoothly."&lt;/p&gt;
    &lt;p&gt;Additional reporting by Ewan Somerville.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published30 September 2019&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published5 December 2024&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sign up for our Tech Decoded newsletter to follow the world's top tech stories and trends. Outside the UK? Sign up here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45569108</guid><pubDate>Mon, 13 Oct 2025 15:03:53 +0000</pubDate></item><item><title>NanoChat â€“ The best ChatGPT that $100 can buy</title><link>https://github.com/karpathy/nanochat</link><description>&lt;doc fingerprint="8c19a322f1657e6"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;The best ChatGPT that $100 can buy.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like speedrun.sh, that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.&lt;/p&gt;
    &lt;p&gt;The fastest way to feel the magic is to run the speedrun script speedrun.sh, which trains and inferences the $100 tier of nanochat. On an 8XH100 node at $24/hr, this gives a total run time of about 4 hours. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like Lambda), and kick off the training script:&lt;/p&gt;
    &lt;code&gt;bash speedrun.sh&lt;/code&gt;
    &lt;p&gt;Alternatively, since the script runs for 4 hours, I like to launch it like this inside a new screen session &lt;code&gt;speedrun&lt;/code&gt; (and also log output to &lt;code&gt;speedrun.log&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;screen -L -Logfile speedrun.log -S speedrun bash speedrun.sh&lt;/code&gt;
    &lt;p&gt;See the screen cheatsheet if you are less familiar. You can watch it go inside the screen session, or detach with &lt;code&gt;Ctrl-a d&lt;/code&gt; and &lt;code&gt;tail speedrun.log&lt;/code&gt; to view progress. Now wait 4 hours. Once it's done, you can talk to your LLM via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run &lt;code&gt;source .venv/bin/activative&lt;/code&gt;), and serve it:&lt;/p&gt;
    &lt;code&gt;python -m scripts.chat_web&lt;/code&gt;
    &lt;p&gt;And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example http://209.20.xxx.xxx:8000/, etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).&lt;/p&gt;
    &lt;p&gt;You can also &lt;code&gt;cat report.md&lt;/code&gt; file which appeared in the project directory and contains the "report card" of the run, i.e. a bunch of evaluations and metrics. At the vert end, you'll see a summary table, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Characters: 333,989&lt;/item&gt;
      &lt;item&gt;Lines: 8,304&lt;/item&gt;
      &lt;item&gt;Files: 44&lt;/item&gt;
      &lt;item&gt;Tokens (approx): 83,497&lt;/item&gt;
      &lt;item&gt;Dependencies (uv.lock lines): 2,004&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;BASE&lt;/cell&gt;
        &lt;cell role="head"&gt;MID&lt;/cell&gt;
        &lt;cell role="head"&gt;SFT&lt;/cell&gt;
        &lt;cell role="head"&gt;RL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CORE&lt;/cell&gt;
        &lt;cell&gt;0.2219&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ARC-Challenge&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.2875&lt;/cell&gt;
        &lt;cell&gt;0.2807&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ARC-Easy&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.3561&lt;/cell&gt;
        &lt;cell&gt;0.3876&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;GSM8K&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0250&lt;/cell&gt;
        &lt;cell&gt;0.0455&lt;/cell&gt;
        &lt;cell&gt;0.0758&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HumanEval&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0671&lt;/cell&gt;
        &lt;cell&gt;0.0854&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMLU&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.3111&lt;/cell&gt;
        &lt;cell&gt;0.3151&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChatCORE&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0730&lt;/cell&gt;
        &lt;cell&gt;0.0884&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Total wall clock time: 3h51m&lt;/p&gt;
    &lt;p&gt;(Your table might be missing the RL number by default). For a lot more information around the speedrun script and what to look for and expect, please refer to the walkthrough that I posted in Discussions of the repo: "Introducing nanochat: The best ChatGPT that $100 can buy".&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~12 hours, which slightly outperforms GPT-2 CORE score. Second is the $1000 tier (~41.6 hours), just because it's a nice round number. But both of these are not yet fully supported and therefore not attached here in the master branch yet.&lt;/p&gt;
    &lt;p&gt;That said, to give a sense, the example changes needed for the speedrun.sh file to train a GPT-2 grade model d26 only involve three changes:&lt;/p&gt;
    &lt;code&gt;...
# you'll need to download more data shards for pretraining
# get the number of parameters, multiply 20 to get tokens, multiply by 4.8 to get chars,
# divide by 250 million to get number of shards. todo need to improve this...
python -m nanochat.dataset -n 450 &amp;amp;
...
# use --depth to increase model size. to not oom, halve device bath size 32 -&amp;gt; 16:
torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- --depth=26 --device_batch_size=16
...
# make sure to use the same later during midtraining:
torchrun --standalone --nproc_per_node=8 -m scripts.mid_train -- --device_batch_size=16&lt;/code&gt;
    &lt;p&gt;That's it! The biggest thing to pay attention to is making sure you have enough data shards to train on (the code will loop and do more epochs over the same training set otherwise, decreasing learning speed a bit), and managing your memory/VRAM, primarily by decreasing the &lt;code&gt;device_batch_size&lt;/code&gt; until things fit (the scripts automatically compensates by increasing the number of gradient accumulation loops, simply turning parallel compute to sequential compute).&lt;/p&gt;
    &lt;p&gt;And a bit more about computing environments that will run nanochat:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.&lt;/item&gt;
      &lt;item&gt;All code will run just fine on even a single GPU by omitting &lt;code&gt;torchrun&lt;/code&gt;, and will produce ~identical results (code will automatically switch to gradient accumulation), but you'll have to wait 8 times longer.&lt;/item&gt;
      &lt;item&gt;If your GPU(s) have less than 80GB, you'll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for &lt;code&gt;--device_batch_size&lt;/code&gt;in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you'll have to know a bit more what you're doing and get more creative.&lt;/item&gt;
      &lt;item&gt;Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven't implemented this out of the box so it might take a bit of tinkering.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;nanochat is designed to be short and sweet. One big advantage of this is that we can package up all of the files together and copy paste them to your favorite LLM to ask arbitrary questions. As an example, I like to package up the repo using the files-to-prompt utility like so:&lt;/p&gt;
    &lt;code&gt;files-to-prompt . -e py -e md -e rs -e html -e toml -e sh --ignore "*target*" --cxml &amp;gt; packaged.txt&lt;/code&gt;
    &lt;p&gt;This includes all py, rs, html, toml, sh files, excludes the &lt;code&gt;rustbpe/target&lt;/code&gt; folder, and chooses the cxml output format. Everything is written to the &lt;code&gt;packaged.txt&lt;/code&gt; file, which atm measures ~330KB (i.e. well below ~100K tokens for a state of the art LLM), and ~8K lines of code in 45 files.&lt;/p&gt;
    &lt;p&gt;Alternatively, I recommend using DeepWiki from Devin/Cognition to ask questions of this repo. In the URL of this repo, simply change github.com to deepwiki.com, and you're off.&lt;/p&gt;
    &lt;p&gt;I haven't invested too much here but some tests exist, especially for the tokenizer. Run e.g. as:&lt;/p&gt;
    &lt;code&gt;python -m pytest tests/test_rustbpe.py -v -s&lt;/code&gt;
    &lt;p&gt;nanochat is nowhere finished. The goal is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &amp;lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM "framework"; there will be no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable "strong baseline" codebase designed to run start to end and produce a concrete ChatGPT clone and its report card.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The name (nanochat) derives from my earlier project nanoGPT, which only covered pretraining.&lt;/item&gt;
      &lt;item&gt;nanochat is also inspired by modded-nanoGPT, which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.&lt;/item&gt;
      &lt;item&gt;Thank you to HuggingFace for fineweb and smoltalk.&lt;/item&gt;
      &lt;item&gt;Thank you Lambda for the compute used in developing this project.&lt;/item&gt;
      &lt;item&gt;Thank you to chief LLM whisperer ğŸ§™â™‚ï¸ Alec Radford for advice/guidance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you find nanochat helpful in your research cite simply as:&lt;/p&gt;
    &lt;code&gt;@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that $100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45569350</guid><pubDate>Mon, 13 Oct 2025 15:22:47 +0000</pubDate></item><item><title>Android's sideloading limits are its most anti-consumer move yet</title><link>https://www.makeuseof.com/androids-sideloading-limits-are-anti-consumer-move-yet/</link><description>&lt;doc fingerprint="8f42830190be3051"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâ€™m a huge fan of open source, and thatâ€™s one of the reasons Iâ€™m drawn to Android. However, new requirements surrounding sideloaded apps, which will start rolling out in October 2025, may be the most anti-consumer move yet by Google. Mandatory enforcement of the requirement will begin in September 2026 (starting with specific countries), marking a turning point where the freedom to install any app comes with conditions set by Google.&lt;/p&gt;
    &lt;p&gt;Iâ€™ve used apps like NewPipe (a media/YouTube client) and Blokada (an ad blocker) for years now. However, these apps arenâ€™t available on the Google Play Store, so I have to obtain them from third-party sources, such as F-Droid. With Google tightening the rules around sideloaded apps, I fear I may lose access to some of the apps I love most on Android because they arenâ€™t verified. Sideloading isnâ€™t going away, but people may seek alternatives because it may feel like the gates are narrowing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Google actually changed&lt;/head&gt;
    &lt;head rend="h3"&gt;The rules, the timeline, and what â€œcertifiedâ€ really means&lt;/head&gt;
    &lt;p&gt;Google's talk around "verified developers" sounds harmless and, in some ways, helpful. As reported on the Android Developer Blog, it is like "an ID check at the airport which confirms a traveler's identity but is separate from the security screening of their bags." Google's analogy, however, may be oversimplified. When this is enforced, the only way a developerâ€™s app will be installable on devices that include Google Mobile Services (GMS) â€” which typically provide access to the Play Store â€” is by completing ID verification using government-issued documents or contact information. This will be rolled out globally in 2027.&lt;/p&gt;
    &lt;p&gt;Apps will be blocked from installing on most mainstream phones if their developer can't complete this verification. However, there are certain devices that will remain unaffected, even though they are just a tiny fraction of the total devices. These categories include all devices that do not pass Google's certification test, primarily custom ROMs or de-Googled phones.&lt;/p&gt;
    &lt;p&gt;Strictly speaking, Google is not removing sideloading, but it is redefining and limiting participation in the Android ecosystem by creating a mandatory Google-controlled choke point. While this may be a subtle shift, it clearly takes an open source project from anyone being able to participate (including anonymous or pseudonymous distribution) to only those whom Google allows to participate (via centralized developer identity verification).&lt;/p&gt;
    &lt;head rend="h2"&gt;Security theater or real gain?&lt;/head&gt;
    &lt;head rend="h3"&gt;Testing Googleâ€™s justification&lt;/head&gt;
    &lt;p&gt;There is a rational justification for tightening rules around sideloaded apps. It could be framed as user protection against malicious apps or against bad actors who cloak themselves with fake identities. While this is reasonable, the real question is whether it adds significant security for everyday users.&lt;/p&gt;
    &lt;p&gt;This is a valid question because security checks already exist. Google Play Protect makes Android secure by scanning sideloaded apps. Android flags unsafe installs, and itâ€™s always given us the choice of blocking apps from unknown sources. Even if these are imperfect, theyâ€™re defenses that already exist.&lt;/p&gt;
    &lt;p&gt;Googleâ€™s new move almost feels like itâ€™s based on the assumption that identity equals integrity. Does a verified government-issued identification equate to user safety? This logic is flawed: historically, we've seen malware slip through the Play Storeâ€”signed and â€œverifiedâ€â€”several times. However, what the new rule does is shift the basis of trust away from existing on-device security warnings and your best judgment.&lt;/p&gt;
    &lt;p&gt;Critics may even contend that this new rule erodes your right to make informed decisions about your own devices, and that feels more like selective control. Ultimately, many people may view this as Googleâ€™s way of shielding itself from criticism over sideloaded malware and protecting the integrity of its ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;There will be collateral damage&lt;/head&gt;
    &lt;head rend="h3"&gt;The ecosystems that depend on openness&lt;/head&gt;
    &lt;p&gt;This may be the most significant anti-consumer move, simply due to its profound impact. It could hit big developers or commercial apps, as well as entire ecosystems built around freely distributed APKs without verification. F-Droid hosts an incredible number of apps not available on the Play Store. Many of these tools exist because they see a need to operate outside the long, controlling arm of Google. This sideloading rule may make them unavailable on mainstream devices even though theyâ€™re safe.&lt;/p&gt;
    &lt;p&gt;This is a risk that also affects indie developers and hobbyists. Certain apps can no longer justify the time, effort, or privacy trade-offs required for identity verification. Many one-off projects and apps for niche communities may fall under this category. Ultimately, what we may end up with is a shrunken ecosystem, and if this happens, it will hurt all of us.&lt;/p&gt;
    &lt;p&gt;However, innovation may be the biggest casualty in all of this. Android is great because of its flexibility. It is an ecosystem for everyone. The imposition of a single, centralized gatekeeper will stifle grassroots innovation, as not everyone will be willing or able to contribute, and this will invariably impact the pace and extent of innovation we see on Android.&lt;/p&gt;
    &lt;head rend="h2"&gt;The new reality for Android users&lt;/head&gt;
    &lt;p&gt;Although Google would argue that the intentions behind the new rules for sideloading apps are to protect and secure users, it will likely feel limiting to many Android users, let alone removing the sense of autonomy on our devices. Of course, sideloading will still be possible, but it creates friction for people who use or make apps that arenâ€™t officially available on the Play Store. The fear is that it may be the beginning of the end for independent developers, hobbyists, and niche app communities.&lt;/p&gt;
    &lt;p&gt;Of course, there are workarounds: using non-certified devices, backing up APKs, or exploring alternative app stores. Sadly, the trade-offs for each workaround may range from technical complexity to potential security risks. You should be careful when sideloading apps on Android. However, one thing is clear: Android's openness is closing. What we donâ€™t know is if it will become a completely closed ecosystem someday.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45569371</guid><pubDate>Mon, 13 Oct 2025 15:24:08 +0000</pubDate></item><item><title>$19B Wiped Out in Crypto's Biggest Liquidation</title><link>https://decrypt.co/344038/morning-minute-19b-wiped-out-in-cryptos-biggest-liquidation-ever</link><description>&lt;doc fingerprint="f3a4425dd52193a4"&gt;
  &lt;main&gt;
    &lt;p&gt;Morning Minute is a daily newsletter written by Tyler Warner. The analysis and opinions expressed are his own and do not necessarily reflect those of Decrypt. Subscribe to the Morning Minute on Substack.&lt;/p&gt;
    &lt;p&gt;GM!&lt;/p&gt;
    &lt;p&gt;Todayâ€™s top news:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Crypto majors green and recovering from Fridayâ€™s crash; Bitcoin at $114,100&lt;/item&gt;
      &lt;item&gt;Over $19B liquidated from the crypto market on Friday in bloodiest day ever&lt;/item&gt;
      &lt;item&gt;Aster team does 100M token buy back, drops updated airdrop checker&lt;/item&gt;
      &lt;item&gt;Hyperliquidâ€™s HIP-3 upgrade coming today will unlock permissionless perp market creation&lt;/item&gt;
      &lt;item&gt;Polymarket says a token is coming but unlikely in 2025; Kalshi raises at $5B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ğŸ§¨ A Historic Flush: ~$19B in Crypto Liquidations&lt;/p&gt;
    &lt;p&gt;Friday wasnâ€™t just a bad day for crypto.&lt;/p&gt;
    &lt;p&gt;It was the worst day in cryptoâ€™s existence.&lt;/p&gt;
    &lt;p&gt;And all due to a miscommunication at the highest levelâ€¦&lt;/p&gt;
    &lt;head rend="h2"&gt;ğŸ“Œ What Happened&lt;/head&gt;
    &lt;p&gt;The crypto market saw the largest liquidation wave on record on Friday with ~$19B liquidated in 24 hours, as forced unwinds spanned exchanges and Perps protocols.&lt;/p&gt;
    &lt;p&gt;After President Trump threatened 100% tariffs on Chinese tech and tighter export controls, BTC slid from $118,000 to $101,000 in the span of a few hours.&lt;/p&gt;
    &lt;p&gt;Alts got absolutely obliterated, with some like SUI falling as much as 80%. ~8â€“13% intraday and dragged majors with it.&lt;/p&gt;
    &lt;p&gt;As prices knifed lower, ~$19.1B of positions were liquidated, ~$16.7B of them longs, per CoinDesk.&lt;/p&gt;
    &lt;p&gt;Hyperliquid was &amp;gt;$1.2B in trader equity vaporized and left 6,300 wallets in the red.&lt;/p&gt;
    &lt;p&gt;Open interest was wiped by $65B.&lt;/p&gt;
    &lt;p&gt;Effectively, a full market reset, sending positioning back to midsummer levels.&lt;/p&gt;
    &lt;head rend="h2"&gt;ğŸ—£ï¸ What Theyâ€™re Saying&lt;/head&gt;
    &lt;p&gt;â€œYesterday was the worst liquidation event in crypto history with over $20B+ in liquidations in CeFi and several hundred million in DeFi reported. Market participants are pointing to escalating trade tensions between China and US but itâ€™s almost irrelevant what news caused this.&lt;/p&gt;
    &lt;p&gt;In one hour, BTC fell -13% from peak to trough. Altcoins losses were even more severe â€“ ATOM fell -100% in the span of an hour to virtually zero. Prices have rebounded from these extreme lows but no doubt there will be bodies left in the wake of this wreckage.â€ - Jonathan Man, PM at Bitwise&lt;/p&gt;
    &lt;p&gt;â€œRumor mill currently saying two large trading firms were liquidated to zero. Hearing different takes but the idea is they owned a book of top 100 mcap tokens which were collateralized against each other in size ($1B+) &amp;amp; became forced market sellers of their entire bookâ€ - Ayyyyeandy on X&lt;/p&gt;
    &lt;head rend="h2"&gt;ğŸ§  Why It Matters&lt;/head&gt;
    &lt;p&gt;As the dust settles, we still donâ€™t know exactly what happened.&lt;/p&gt;
    &lt;p&gt;It seems it was an algorithmically driven selloff which liquidated some major players who were cross-collateralized with assets which all went down 40-80% or more.&lt;/p&gt;
    &lt;p&gt;Sites like Binance have refunded users $280M+ for issues on their end which led to liquidations. Others havenâ€™t been so lucky.&lt;/p&gt;
    &lt;p&gt;So what happens from here?&lt;/p&gt;
    &lt;p&gt;If the â€œTrade War 2â€ narrative is totally overblown, we could very well V-shape recover to new ATHs within weeks or even days.&lt;/p&gt;
    &lt;p&gt;Wiping leverage out of the system is typically a bullish setup for price action going forward, and no real aspect of the macro bull case (the debasement trade) has changed in the past few days.&lt;/p&gt;
    &lt;p&gt;New ATHs in October are still on the table.&lt;/p&gt;
    &lt;p&gt;For those who got liquidated, it was a sobering wake up call to the dangers of trading on leverage. The broader perps trade (Hype, Aster, Lighter, etc.) likely took a hit here.&lt;/p&gt;
    &lt;p&gt;Time will tell.&lt;/p&gt;
    &lt;p&gt;As for today, letâ€™s hope there are no more miscommunications between Trump and Chinaâ€¦&lt;/p&gt;
    &lt;head rend="h2"&gt;ğŸŒ Macro Crypto and Memes&lt;/head&gt;
    &lt;p&gt;A few Crypto and Web3 headlines that caught my eye:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Crypto majors are green after Fridayâ€™s selloff; BTC +2% at $114,100, ETH +7% at $4,080, BNB +5% at $1,290, SOL +6% at $192&lt;/item&gt;
      &lt;item&gt;SNX (+130%), TAO (+29%) and ENA (+16%) led top movers&lt;/item&gt;
      &lt;item&gt;$19B in liquidations hit the crypto market during Fridayâ€™s flash crash after Trump threatened 100% tariffs on China, which saw Bitcoin fall to $101,000 and several alts down 60-80%&lt;/item&gt;
      &lt;item&gt;Binance refunded its impacted users $283M as some of their tokens depegged on Friday&lt;/item&gt;
      &lt;item&gt;A consortium of major banks, including Santander, Bank of America, Barclays, Citi, and Goldman Sachs, is exploring a jointly issued stablecoin&lt;/item&gt;
      &lt;item&gt;The Ethereum Foundation launched a new initiative for funding privacy development&lt;/item&gt;
      &lt;item&gt;Coinbase is partnering with American Express to launch a crypto credit card with Bitcoin rewards&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Corporate Treasuries / ETFs&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bitcoin miner MARA acquired another 400 BTC during the market crash on Friday&lt;/item&gt;
      &lt;item&gt;Chinaâ€™s Renaissance Banks is in talks to raise $600M for a BNB Treasury&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Memes&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memecoin leaders are green and rebounding after Fridayâ€™s selloff; DOGE +8%, Shiba +7%, PEPE +11%, PENGU +13%, BONK +14%, TRUMP +5%, SPX +16%, and FARTCOIN +17%&lt;/item&gt;
      &lt;item&gt;USELESS rebounded nearly 100% off Fridayâ€™s lows, back to $385M; ZEREBRO (+48%), Tokabu (+50%) and VINE (+35%) led other Solana movers&lt;/item&gt;
      &lt;item&gt;4 rebounded 30% to $170M leading movers on BSC&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;ğŸ’° Token, Airdrop &amp;amp; Protocol Tracker&lt;/head&gt;
    &lt;p&gt;Hereâ€™s a rundown of major token, protocol and airdrop news from the day:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kalshi raised at a $5 billion valuation from Coinbase, CapitalG, a16z, and Paradigm, with Sequoia also participating&lt;/item&gt;
      &lt;item&gt;Polymarket wonâ€™t release its token until it has fully entered the US market&lt;/item&gt;
      &lt;item&gt;Aster announced a 100M ASTER buyback on Friday, and also opened up its refreshed airdrop checker ahead of new Oct 20 airdrop date&lt;/item&gt;
      &lt;item&gt;Hyperliquid announced a network upgrade today for HIP 3&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;ğŸšš What is happening in NFTs?&lt;/head&gt;
    &lt;p&gt;Here is the list of other notable headlines from the day in NFTs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NFT leaders were mostly green after a weekend crash; Punks +1% at 47 ETH, Pudgy +6% at 7.45, BAYC +8% at 7.75 ETH; Hypurrâ€™s +13% at 1,248 HYPE&lt;/item&gt;
      &lt;item&gt;Quine (+45%) and Chimpers (+20%) were notable top movers&lt;/item&gt;
      &lt;item&gt;Punk Strategyâ€™s PNKSTR token rebounded to $140M after selling off to $100M in Fridayâ€™s crash&lt;/item&gt;
      &lt;item&gt;RTFKT co-founder Benoit Pagotto passed away yesterday at the age of 41&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45570299</guid><pubDate>Mon, 13 Oct 2025 16:31:41 +0000</pubDate></item><item><title>Environment variables are a legacy mess: Let's dive deep into them</title><link>https://allvpv.org/haotic-journey-through-envvars/</link><description>&lt;doc fingerprint="bc5368305ea99e14"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Environment variables are a legacy mess: Let's dive deep into them&lt;/head&gt;
    &lt;p&gt;Programming languages have rapidly evolved in recent years. But in software development, the new often meets the old, and the scaffolding that OS gives for running new processes hasnâ€™t changed much since Unix.&lt;/p&gt;
    &lt;p&gt;If you need to parametrize your application at runtime by passing a few ad-hoc variables (without special files or a custom solution involving IPC or networking), youâ€™re doomed to a pretty awkward, outdated interface:&lt;/p&gt;
    &lt;head rend="h2"&gt;Environment variables.&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;export SECRET_API_KEY=2u845102348u234&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;There are no namespaces for them, no types. Just a flat, embarrassingly global dictionary of strings.&lt;/p&gt;
    &lt;p&gt;But what exactly are these envvars? Is it some kind of special dictionary inside the OS? If not, who owns them and how do they propagate?&lt;/p&gt;
    &lt;head rend="h2"&gt;Where do they come from?&lt;/head&gt;
    &lt;p&gt;In a nutshell: theyâ€™re passed from parent to child.&lt;/p&gt;
    &lt;code&gt;    841 ?        00:00:00 sshd
   1520 ?        00:00:00  \_ sshd-session
   1616 ?        00:00:00      \_ sshd-session
   5521 pts/0    00:00:00          \_ bash
   5545 pts/0    00:00:00              \_ nu
   5549 pts/0    00:00:00                  \_ bash
   5560 pts/0    00:00:00                      \_ ps
&lt;/code&gt;
    &lt;p&gt;On Linux, a program must use the &lt;code&gt;execve&lt;/code&gt; syscall to execute another program.
Whether you type &lt;code&gt;ls&lt;/code&gt; in Bash, call &lt;code&gt;subprocess.run&lt;/code&gt; in Python, or launch a
code editor, it ultimately comes down to &lt;code&gt;execve&lt;/code&gt;, preceded by a
&lt;code&gt;clone&lt;/code&gt;/&lt;code&gt;fork&lt;/code&gt;. The &lt;code&gt;exec*&lt;/code&gt; family of C functions also relies on &lt;code&gt;execve&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;SYSCALL_DEFINE3(execve,
		const char __user *, filename,
		const char __user *const __user *, argv,
		const char __user *const __user *, envp)
&lt;/code&gt;
    &lt;p&gt;This system call takes three arguments: &lt;code&gt;filename&lt;/code&gt;, &lt;code&gt;argv&lt;/code&gt;, &lt;code&gt;envp&lt;/code&gt;.
For example, for an &lt;code&gt;ls -lah&lt;/code&gt; invocation:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;/usr/bin/ls&lt;/code&gt;is the&lt;code&gt;filename&lt;/code&gt;(the executable path),&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;['ls', '-lah']&lt;/code&gt;is the&lt;code&gt;argv&lt;/code&gt;array of command line arguments â€“ the implicit first (â€œzeroâ€) argument is usually the executable name,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;['PATH=/bin:/usr/bin', 'USER=allvpv']&lt;/code&gt;is the&lt;code&gt;envp&lt;/code&gt;array of envvars (typically much longer).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By default, all envvars are passed from the parent to the child. However, nothing prevents a parent process from passing a completely different or even empty environment when calling &lt;code&gt;execve&lt;/code&gt;! In practice, most tooling passes the
environment down: Bash, Pythonâ€™s &lt;code&gt;subprocess.run&lt;/code&gt;, the C library &lt;code&gt;execl&lt;/code&gt;, and
so on.&lt;/p&gt;
    &lt;p&gt;And this is what you expect â€“ variables are inherited by child processes. Thatâ€™s the point â€“ to track the environment.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Which tools do not pass the parentâ€™s environment? For example, the&lt;/p&gt;&lt;code&gt;login&lt;/code&gt;executable, used when signing into a system, sets up a fresh environment for its children.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Where do they go?&lt;/head&gt;
    &lt;p&gt;After launching the new program, the kernel dumps the variables on the stack as a sequence of null-terminated strings which contain the envvar definitions. Here is a hex view:&lt;/p&gt;
    &lt;code&gt;    484f 4d45 3d2f 0069 6e69 743d 2f73 6269  HOME=/ init=/sbi
    6e2f 696e 6974 004e 4554 574f 524b 5f53  n/init NETWORK_S
    4b49 505f 454e 534c 4156 4544 3d00 5445  KIP_ENSLAVED= TE
    524d 3d6c 696e 7578 0042 4f4f 545f 494d  RM=linux BOOT_IM
    4147 453d 2f76 6d6c 696e 757a 2d36 2e31  AGE=/vmlinuz-6.1
    342e 302d 3333 2d67 656e 6572 6963 0064  4.0-33-generic.d
    726f 705f 6361 7073 3d00 5041 5448 3d2f  rop_caps= PATH=/
    7573 722f 6c6f 6361 6c2f 7362 696e 3a2f  usr/local/sbin:/
    7573 722f 6c6f 6361 6c2f 6269 6e3a 2f75  usr/local/bin:/u
    7372 2f73 6269 6e3a 2f75 7372 2f62 696e  sr/sbin:/usr/bin
    3a2f 7362 696e 3a2f 6269 6e00 5057 443d  :/sbin:/bin PWD=
    2f00 726f 6f74 6d6e 743d 2f72 6f6f 7400  / rootmnt=/root
&lt;/code&gt;
    &lt;p&gt;This static layout canâ€™t easily be modified or extended; the program must copy those variables into its own data structure. Letâ€™s look at how Bash, C, and Python store envvars internally. I analyzed their source code and here is a summary.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bash&lt;/head&gt;
    &lt;p&gt;It stores the variables in a hashmap. Or, more precisely, in a stack of hashmaps.&lt;/p&gt;
    &lt;p&gt;When you spawn a new process using Bash, it traverses the stack of hashmaps to find variables marked as exported and copies them into the environment array passed to the child.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Side note: Why is traversing the stack needed?&lt;/p&gt;&lt;p&gt;Each function invocation in Bash creates a new local scope â€“ a new entry on the stack. If you declare your variable with&lt;/p&gt;&lt;code&gt;local&lt;/code&gt;, it ends up in this locally-scoped hashmap.&lt;p&gt;Whatâ€™s interesting is that you can export a&lt;/p&gt;&lt;code&gt;local&lt;/code&gt;variable too!&lt;code&gt;function locallyScoped() { local PATH="$PATH:/opt/secret/bin" export PATH env # &amp;lt;- sees the PATH with /opt/scecret/bin } locallyScoped env # &amp;lt;- sees the PATH without modification&lt;/code&gt;&lt;p&gt;I wouldnâ€™t have learned this without diving into Bash source. My intuitive (wrong) assumption was that&lt;/p&gt;&lt;code&gt;export&lt;/code&gt;automatically makes the variable global â€“ like&lt;code&gt;declare -g&lt;/code&gt;! Super interesting stuff.&lt;/quote&gt;
    &lt;head rend="h3"&gt;The default C library on Linux: &lt;code&gt;glibc&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;glibc&lt;/code&gt; exposes a dynamic &lt;code&gt;environ&lt;/code&gt; array, managed via &lt;code&gt;putenv&lt;/code&gt; and &lt;code&gt;getenv&lt;/code&gt;
library functions. It uses an array, so the time complexity of &lt;code&gt;getenv&lt;/code&gt; and
&lt;code&gt;putenv&lt;/code&gt; is linear in the number of envvars. Remember â€“ envvars are not a
high-performance dictionary and you should not abuse them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Python&lt;/head&gt;
    &lt;p&gt;Python couples its environment to the C library, which can cause surprising inconsistencies.&lt;/p&gt;
    &lt;p&gt;If youâ€™ve programmed some Python, youâ€™ve probably used the &lt;code&gt;os.environ&lt;/code&gt;
dictionary. On startup, &lt;code&gt;os.environ&lt;/code&gt; is built from the C libraryâ€™s &lt;code&gt;environ&lt;/code&gt;
array.&lt;/p&gt;
    &lt;p&gt;But those dictionary values are NOT the â€œground truthâ€ for child processes. Rather, each change to &lt;code&gt;os.environ&lt;/code&gt; invokes the native &lt;code&gt;os.putenv&lt;/code&gt; function,
which in turn calls the C libraryâ€™s &lt;code&gt;putenv&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note that the propagation is one-directional: modifying&lt;/p&gt;&lt;code&gt;os.environ&lt;/code&gt;will call&lt;code&gt;os.putenv&lt;/code&gt;, but not the other way around. Call&lt;code&gt;os.putenv&lt;/code&gt;, and&lt;code&gt;os.environ&lt;/code&gt;wonâ€™t be updated.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Liberal format&lt;/head&gt;
    &lt;p&gt;The Linux kernel is very liberal about the format of environment variables, and so is &lt;code&gt;glibc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For example, your C program can manipulate the environment â€“ the global &lt;code&gt;environ&lt;/code&gt; array â€“ such that several variables share the same name but have
different values. And when you execute a child process, it will inherit this
â€œbrokenâ€ setup.&lt;/p&gt;
    &lt;p&gt;You donâ€™t even need an equals sign separating name from value! The usual entry is &lt;code&gt;NAME=VALUE&lt;/code&gt;, but nothing prevents you from adding &lt;code&gt;NONSENSE_WITH_EMOJI ğŸ˜€&lt;/code&gt;
to the array.&lt;/p&gt;
    &lt;p&gt;The kernel happily accepts any null-terminated string as an â€œenvironment variableâ€ definition. It just imposes a size limitation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Single variable: 128 KiB on a typical x64 Intel CPU. This is for the whole definition â€“ name + equal sign + value. Itâ€™s computed as&lt;/p&gt;&lt;code&gt;PAGE_SIZE * 32&lt;/code&gt;. No modern hardware uses pages smaller than 4 KiB, so you can treat it as a lower bound, unless you need to deal with some legacy embedded systems.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Total: 2 MiB on a typical machine. This limit is shared by envvars and the command line arguments. The calculation is a bit more complicated (see the&lt;/p&gt;&lt;code&gt;execve(2)&lt;/code&gt;man page):&lt;code&gt;max(32 * PAGE_SIZE, min(MAX_STACK_SIZE / 4, 6 MB))&lt;/code&gt;&lt;p&gt;On a typical system, the limiting factor is the&lt;/p&gt;&lt;code&gt;MAX_STACK_SIZE&lt;/code&gt;. Remember, initially the envvars are dumped on the stack! To prevent unpredictable crashes, the system allows only 1/4 of the stack for the envvars.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Quirks&lt;/head&gt;
    &lt;p&gt;But the fact that you can do something does not mean that you should. For example, if you start Bash with the â€œbrokenâ€ environment â€“ duplicated names and entries without &lt;code&gt;=&lt;/code&gt; â€“ it deduplicates the variables and drops the nonsense.&lt;/p&gt;
    &lt;p&gt;One interesting edge case is a space inside the variable name. My beloved shell â€“ Nushell â€“ has no problem with the following assignment:&lt;/p&gt;
    &lt;code&gt;$env."Deployment Environment" = "prod"
&lt;/code&gt;
    &lt;p&gt;Python is fine with it, too. Bash, on the other hand, canâ€™t reference it because whitespace isnâ€™t allowed in variable names. Fortunately, the variable isnâ€™t lost â€“ Bash keeps such entries in a special hashmap called &lt;code&gt;invalid_env&lt;/code&gt; and still passes them to child processes.&lt;/p&gt;
    &lt;head rend="h2"&gt;The standard format&lt;/head&gt;
    &lt;p&gt;So what name and value can you safely use for your envvar? A popular misconception, repeated on StackOverflow and by ChatGPT, is that POSIX permits only uppercase envvars, and everything else is undefined behavior.&lt;/p&gt;
    &lt;p&gt;But this is seriously NOT what the standard says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;These strings have the form name=value; names shall not contain the character â€˜=â€™. For values to be portable across systems conforming to POSIX.1-2017, the value shall be composed of characters from the portable character set (except NUL and as indicated below). There is no meaning associated with the order of strings in the environment. If more than one string in an environment of a process has the same name, the consequences are undefined.&lt;/p&gt;
      &lt;p&gt;Environment variable names used by the utilities in the Shell and Utilities volume of POSIX.1-2017 consist solely of uppercase letters, digits, and the &amp;lt;underscore&amp;gt; ( â€˜_â€™ ) from the characters defined in Portable Character Set and do not begin with a digit. Other characters may be permitted by an implementation; applications shall tolerate the presence of such names. Uppercase and lowercase letters shall retain their unique identities and shall not be folded together. The name space of environment variable names containing lowercase letters is reserved for applications. Applications can define any environment variables with names from this name space without modifying the behavior of the standard utilities.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, POSIX-specified utilities use uppercase envvars, but thatâ€™s not prescriptive for your programs. Quite the contrary: youâ€™re encouraged to use lowercase for your envvars so they donâ€™t collide with the standard tools.&lt;/p&gt;
    &lt;p&gt;The only strict rule is that a variable name cannot contain an equals sign. POSIX requires compliant applications to preserve all variables that conform to this rule.&lt;/p&gt;
    &lt;p&gt;But in reality, not many applications use lowercase. The proper etiquette in software development is to use &lt;code&gt;ALL_UPPERCASE&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;My pragmatic recommendation isâ€¦&lt;/head&gt;
    &lt;p&gt;â€¦to use &lt;code&gt;^[A-Z_][A-Z0-9_]*$&lt;/code&gt; for names, and UTF-8 for values. You shouldnâ€™t
hit problems on Linux. If you want to be super safe: instead of UTF-8, use the
POSIX-mandated Portable Character Set
(PCS) â€“ essentially
ASCII without control characters.&lt;/p&gt;
    &lt;p&gt;Please subscribe to my RSS feed! ğŸ˜‡&lt;/p&gt;
    &lt;p&gt;Independent blogging is not possible without RSS. Start using RSS today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wow, I really enjoyed writing thisâ€¦&lt;/head&gt;
    &lt;p&gt;â€¦and I hope it wasnâ€™t a boring read.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45570537</guid><pubDate>Mon, 13 Oct 2025 16:49:15 +0000</pubDate></item></channel></rss>