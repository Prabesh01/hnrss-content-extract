<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 12 Nov 2025 07:37:09 +0000</lastBuildDate><item><title>Pikaday: A friendly guide to front-end date pickers</title><link>https://pikaday.dbushell.com</link><description>&lt;doc fingerprint="2611da0b238d3ebc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Who needs a JavaScript date picker?&lt;/head&gt;
    &lt;p&gt;The answer, in most cases, is nobody! Complex UI leads to more errors and abandoned forms. There can be easier ways to pick a date than a calendar widget. This guide provides alternate ideas and aims to send developers on a path towards user-friendly interfaces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Native date and time inputs&lt;/head&gt;
    &lt;p&gt;If you absolutely must use a calendar widget then itâs wise to use the native input. All modern browsers support native date and time inputs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Date input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;date&lt;/code&gt; input type provides a native date picker.
        &lt;/p&gt;
    &lt;head rend="h3"&gt;Time input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;time&lt;/code&gt; input type allows users to specify hours and minutes.
          &lt;/p&gt;
    &lt;head rend="h3"&gt;Datetime input&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;datetime-local&lt;/code&gt; input type combines both date and time.
          &lt;/p&gt;
    &lt;head rend="h2"&gt;Why use native inputs&lt;/head&gt;
    &lt;p&gt;Native inputs are super easy to implement with one line of code. The web browser handles many important details for developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Accessibility (mostly*)&lt;/item&gt;
      &lt;item&gt;Performance&lt;/item&gt;
      &lt;item&gt;Internationalisation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let browsers do the hard work! Browsers allow keyboard users to type numbers in sequence. Most browsers provide alternate UI for time and date selection like the classic calendar widget. They're not perfect but do you trust a JavaScript library to do better?&lt;/p&gt;
    &lt;p&gt;*Oh dear! Even native date pickers have some accessibility issues.&lt;/p&gt;
    &lt;head rend="h2"&gt;Separate inputs&lt;/head&gt;
    &lt;p&gt;A single date picker can be tricky to operate. For memorable dates using separate inputs can improve usability. The example below is based on GOV.UK date input component.&lt;/p&gt;
    &lt;head rend="h3"&gt;Select elements&lt;/head&gt;
    &lt;p&gt;If only a limited set of data is valid then using select elements may be suitable. They can require fewer interactions to use and they eliminate typing errors.&lt;/p&gt;
    &lt;p&gt;Numeric month labels can be helpful but take care in how theyâre written. Screen readers may mistakenly announce â1 Januaryâ as âthe 1st of Januaryâ, for example.&lt;/p&gt;
    &lt;p&gt;Travel booking often has a fixed schedule with limited time options, such as every 15 minutes. Relative dates like âTodayâ and âTomorrowâ can be easier to understand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Masked inputs&lt;/head&gt;
    &lt;p&gt;Another common alternative to date pickers is a single input with a placeholder mask. This can be used for full or partial dates. JavaScript can enhancement the experience.&lt;/p&gt;
    &lt;p&gt; The examples above provide client-side validation with errors such as âPlease enter a valid day for February (1 to 28)â. Valid dates are confirmed in full and formatted with the &lt;code&gt;Intl&lt;/code&gt; API.
        &lt;/p&gt;
    &lt;p&gt;Caution! Updating input values with JavaScript can break native undo/redo.&lt;/p&gt;
    &lt;p&gt;Itâs even possible to visually combined mutliple inputs using CSS to appear as one.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ranges and limited options&lt;/head&gt;
    &lt;p&gt; JavaScript date pickers that support range selection across two calendars are difficult to use, especially without a pointer. Consider providing two inputs instead to reduce complexity. If users are required to select an available date then a group of &lt;code&gt;radio&lt;/code&gt; inputs can do the job.
        &lt;/p&gt;
    &lt;p&gt;The example below illustrates the idea but is not fully interactive.&lt;/p&gt;
    &lt;p&gt;There are many design variations of this pattern. This idea is to replace complicated UI with a series of simple tasks. Such a pattern can be implemented as a multi-page form with JavaScript used to enhance it into a single page interactive experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;
    &lt;head&gt;What if I use a JavaScript framework like React?&lt;/head&gt;
    &lt;p&gt; All good JavaScript frameworks allow you to use native HTML elements. Not everything needs to be a custom component. Native input events can integrate with framework callbacks. Use attributes like &lt;code&gt;value&lt;/code&gt; for two-way state binding.
            &lt;/p&gt;
    &lt;head&gt;How do I style the native date picker?&lt;/head&gt;
    &lt;p&gt; The on-page &lt;code&gt;input&lt;/code&gt; element
              can be partially styled but other parts are not stylable.
              That is a good thing! Native system UI is familiar to the user.
              The design will differ based on operating system and input method.
              Date pickers even look different across browsers and that's fine too, you don't need to add yet another design to the mix!
            &lt;/p&gt;
    &lt;head&gt;A stakeholder is demanding a JavaScript date picker, how do I dissuade them?&lt;/head&gt;
    &lt;p&gt;Remember: the end goal is a successful form submission. Complex and fragile UI leads to more errors. All date pickers have accessibility issues. Combining basic inputs can be more user-friendly. Untested JavaScript UI may fall foul of regulation like the European Accessibility Act. Keep it simple for success!&lt;/p&gt;
    &lt;head&gt;How do I test and guarantee accessibility?&lt;/head&gt;
    &lt;p&gt;Itâs critical to understand the relevant accessibility guidelines. You donât need to memorise WCAG but there are no shortcuts to learning the important parts. Leverage existing web standards to avoid mistakes trying to code custom UI.&lt;/p&gt;
    &lt;p&gt;Browser dev tools have built-in accessibility features to help identify mistakes. However, no tool is perfect. The only way to know for sure is to conduct user testing.&lt;/p&gt;
    &lt;p&gt;Accessibility overlays are strongly discouraged and can make matters worse.&lt;/p&gt;
    &lt;head&gt;Where can I learn more about date picker accessibility?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collecting dates in an accessible way by Graham Armfield&lt;/item&gt;
      &lt;item&gt;What makes an accessible date picker? Is it even possible? by Russ Weakley&lt;/item&gt;
      &lt;item&gt;Maybe You Donât Need a Date Picker by Adrian Roselli&lt;/item&gt;
      &lt;item&gt;Date Picker Dialog Example by ARIA Authoring Practices Guide&lt;/item&gt;
      &lt;item&gt;Designing The Perfect Date And Time Picker by Vitaly Friedman&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;This is all great but can you please recommend a JavaScript date picker?&lt;/head&gt;
    &lt;p&gt;Sorry, no! There is no universal solution and all date pickers have issues. I hope this guide has given you the knowledge to evaluate your own requirements. Try to achieve your goal in the simplest way. A date picker is probably not the answer.&lt;/p&gt;
    &lt;p&gt;Before you go! Remember to test and gather feedback from real users :)&lt;/p&gt;
    &lt;p&gt;This guide is a work in progress, feedback is welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45887957</guid><pubDate>Tue, 11 Nov 2025 14:58:47 +0000</pubDate></item><item><title>We ran over 600 image generations to compare AI image models</title><link>https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/</link><description>&lt;doc fingerprint="371511f4993ed21d"&gt;
  &lt;main&gt;
    &lt;p&gt;tl:dr; We’ve been making photo apps for iOS for long enough that we have gray hairs now, and using our experience we ran over 600 image generations to compare which AI models work best for which image edits. If you want, you can jump right to the image comparisons, or the conclusion, but promise us you won’t presumptuous comments on Hacker News until you’ve also read the background!&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Hi! We’re LateNiteSoft, and we’ve been working on photography-related iOS apps for 15 years now. Working on market-leading apps such as Camera+, Photon and REC, we’ve always had our finger on the pulse on what users want out of their mobile photography.&lt;/p&gt;
    &lt;p&gt;With the ground-breaking release of OpenAI’s gpt-image-1 image generation model earlier this year, we started investigating all the interesting use cases we could think of for AI image editing.&lt;/p&gt;
    &lt;p&gt;But as a company that has never taken any venture capital investment, all our products have to pay for themselves. We’re in it to delight our users, not just capture market share and sell them out. When considering AI projects, one thing has been clear – we can’t take the AI startup road where you have a generous free tier, charge an unreasonably small monthly fee for “unlimited”, and hope you’re going to make it up on scale (code for “someone please acquire us”).&lt;/p&gt;
    &lt;p&gt;All the AI-focused billing systems we could find out there were based on this. Assuming you want to claim unlimited access, and then sandbag users with “fair use” clauses and prevent them from any actual unlimited usage (which is, obviously, untenable, since you’ll end up with one $20/mo user reselling to everyone else).&lt;/p&gt;
    &lt;p&gt;Since we want to fairly charge our customers for what they actually use, we’ve built a credit-based “pay per generation”-style billing system (that internally we’ve been calling CreditProxy). We’ve also been planning on providing this as a service, since nobody else seems to be doing it, so if you’re interested in being a trial user, get in touch!&lt;/p&gt;
    &lt;p&gt;We released our app MorphAI as a public proof of concept to give CreditProxy a proper real world-test, and have marketed it to the users of Camera+, which includes traditional photo-editing functionality, including a whole host of popular photo filters, giving us a built-in audience of customers ready for the next step in image editing.&lt;/p&gt;
    &lt;p&gt;With the release of newer models like nanoBanana and Seedream, we’ve had to consider which models make sense to support. We need to explore the trade-offs between quality, prompt complexity, and pricing.&lt;/p&gt;
    &lt;p&gt;A couple of hastily-hacked together scripts, and many, many AI generation credits later, we have some results! So that everyone else also doesn’t have to waste their money, we figured we’d share what we found:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Tests&lt;/head&gt;
    &lt;p&gt;Based on our experience with Camera+ and the kind of edits our users have been making with MorphAI, we picked a host of somewhat naive prompts. Veteran Midjourney users may scoff at these, but in our experience these are the kinds of prompts that our average user is likely to use.&lt;/p&gt;
    &lt;p&gt;As for test photos, we chose some some representative things people like to take photos of: their pets, their kids, landscapes, their cars, and product photography.&lt;/p&gt;
    &lt;p&gt;Image generation times are also relevant. During our test period, the generation time for all models was fairly consistent, and didn’t vary by image or prompt complexity.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OpenAI (High)&lt;/cell&gt;
        &lt;cell&gt;Gemini&lt;/cell&gt;
        &lt;cell&gt;Seedream&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;80 seconds&lt;/cell&gt;
        &lt;cell&gt;11 seconds&lt;/cell&gt;
        &lt;cell&gt;9 seconds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;OpenAI also has a quality setting, the images included here were all generated on High quality, but we also tested Medium, and those generations averaged 36 seconds. We can include the Medium quality images as well if there is any interest!&lt;/p&gt;
    &lt;p&gt;There are a ton of photos to compare here, so to make things easier to flip through, here are some keyboard shortcuts to help you out: Click on a photo to see it larger. Now you can use the arrow keys to switch between models. Press the tab key to switch between test images. Hit ESC to leave the view.&lt;/p&gt;
    &lt;head rend="h2"&gt;Classic filters&lt;/head&gt;
    &lt;p&gt;These are the types of filters that we used to implement manually, by painstakingly hand-crafting textures and Photoshop layers and then converting those to Objective-C code. Now all you need is a few words into a language model (and to burn down half of a rain forest or so; just the cost of progress).&lt;/p&gt;
    &lt;p&gt;Our conclusion for this category is that for photo realistic filters like this, Gemini really shines by preserving details from the original and minimizing hallucinations, but often at the expense of the strength and creativity of the effect. Especially with photos of people, Gemini seems to refuse to apply any edits at all, with a strong bias towards photo realism.&lt;/p&gt;
    &lt;p&gt;OpenAI really likes to mess with the details of the photo, giving a characteristic “AI slop” feel, which can be a deal breaker on things like human faces.&lt;/p&gt;
    &lt;head rend="h3"&gt;Grungy vintage photo&lt;/head&gt;
    &lt;head rend="h3"&gt;Use soft, diffused lighting&lt;/head&gt;
    &lt;head rend="h3"&gt;Transform into a kaleidoscopic pattern&lt;/head&gt;
    &lt;p&gt;Gemini took some really odd shortcuts in generating some of these!&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a heat map effect&lt;/head&gt;
    &lt;p&gt;It’s clear that none of the models actually have a concept of what generates heat here, aside from Seedream knowing that humans generate heat, clearly revealing that without any ground truth the models struggle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like a long exposure photograph&lt;/head&gt;
    &lt;p&gt;This is an interesting test since in some of the sample photos a long exposure doesn’t make sense. In the ones where it makes the most sense – the landscape and the car, OpenAI did the best, but on the other hand it completely messed up the cats and the product, and the portrait photo turned into a trippy art piece.&lt;/p&gt;
    &lt;p&gt;Gemini, maybe logically, did nothing. Seedream liked adding light streaks as if a car drove past, with only the portrait photo seemingly making any sense.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pinhole camera&lt;/head&gt;
    &lt;p&gt;In this case, it was funny to watch Gemini take a literal approach and generate actual pictures of cameras! For this reason we re-worked this prompt by just adding the word “effect”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pinhole camera effect&lt;/head&gt;
    &lt;p&gt;Gemini liked to generate a literal pinhole camera here so we tried modifying the prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Add a layer of fog or mist&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like it’s golden hour&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like it’s etched in glass&lt;/head&gt;
    &lt;p&gt;With this prompt, there is ambiguity in what “it” is, so we tried a reworded prompt as well. Only OpenAI consistently knew what a traditional etched glass effect looks like. Seedream’s glass item effect looks really cool!&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like the photo is etched in glass&lt;/head&gt;
    &lt;p&gt;Gemini has a really interesting interpretation here! And Seedream had some pretty fantastic results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Remove background&lt;/head&gt;
    &lt;p&gt;This is a classic job people have spent their lives doing manually in Photoshop since the early 90’s. But what is a “background”, really? Is the ground in front of a car the “background”? We also retried this with a tweaked prompt.&lt;/p&gt;
    &lt;p&gt;OpenAI’s “sloppification” of the details of objects makes it useless for this purpose.&lt;/p&gt;
    &lt;head rend="h3"&gt;Isolate the object&lt;/head&gt;
    &lt;p&gt;With the tweaked prompt, Gemini’s API actually returned a followup question: “Which object would you like to isolate? There are two cats in the image.”, which our generation script was not prepared to handle! So it is missing from this comparison.&lt;/p&gt;
    &lt;head rend="h3"&gt;Give it a metallic sheen&lt;/head&gt;
    &lt;p&gt;Another case where “it” is vague and we can retry with a more specific prompt. The product imagery is another case where Seedream created a really stunning result, even adding a reflection of someone taking the photo with their phone!&lt;/p&gt;
    &lt;head rend="h3"&gt;Give the object a metallic sheen&lt;/head&gt;
    &lt;p&gt;Modifying the prompt here really only changed OpenAI’s interpretation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lens effects&lt;/head&gt;
    &lt;p&gt;One of the filter packs we had worked on for Camera+ using traditional methods was a lens effect filter pack. But unlike traditional edits, with generative AI you can also create wide-angle lens effects that can just make up the portions of the image that the camera couldn’t capture.&lt;/p&gt;
    &lt;p&gt;This is another category where it’s very visible how OpenAI regenerates and hallucinates all the details in a picture, where Gemini and Seedream’s results are very faithful to the original and look more like actual lens permutations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a fish-eye lens effect&lt;/head&gt;
    &lt;head rend="h3"&gt;Strong bokeh blur&lt;/head&gt;
    &lt;p&gt;It was pretty surprising how poorly the models did here considering how common this must be among the training data. OpenAI give a strong blur but no bokeh effects. Gemini gives us a bunch of random circles in front of the image, demonstrating an understanding of what people want out of a bokeh filter but not how it works. Seedream does really well here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a Dutch angle (canted frame)&lt;/head&gt;
    &lt;p&gt;OpenAI really lost it’s mind here on the car photo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Change to a bird’s-eye view&lt;/head&gt;
    &lt;head rend="h2"&gt;Style transfer&lt;/head&gt;
    &lt;p&gt;Style Transfer is the process of applying an artistic style to a photo. This technique predates the current AI model by quite a few years with popular apps generating Van Gogh paintings out of your photos. We were also early out in attempting style transfer for our apps, shout out to Noel’s Intel iMac which had to run at full blast all night just to generate a 256x256px image, since it was our only machine with a compatible GPU.&lt;/p&gt;
    &lt;p&gt;While Gemini was good at preserving reality in the more photorealistic effects in the previous section, when it comes to the more artistic styles, OpenAI has them beat, while Gemini keeps things far too conservative, especially with photos of a human in them, where it sometimes seems to just do nothing at all, is this some kind of safety guardrail?&lt;/p&gt;
    &lt;head rend="h3"&gt;Draw this in the style of a Studio Ghibli movie&lt;/head&gt;
    &lt;p&gt;ChatGPT went viral with this prompt, with Sam Altman even making it his profile on X. And OpenAI keeps the crown – is Google too conservative in order to avoid a lawsuit? Seedream makes an attempt but they just end up looking like “generic Anime”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform into watercolor painting&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like a pastel artwork&lt;/head&gt;
    &lt;head rend="h3"&gt;Transform into Art Nouveau style&lt;/head&gt;
    &lt;head rend="h3"&gt;Apply a ukiyo-e Japanese woodblock print style&lt;/head&gt;
    &lt;p&gt;A very stark example of Gemini failing to apply a style on photos with humans. This is a prompt where Seedream knocked it out of the park, perhaps showing a larger portion of their training data being sourced from asian cultures than the western models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform into low poly art&lt;/head&gt;
    &lt;p&gt;Seedream blows everyone else away here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Portrait effects&lt;/head&gt;
    &lt;p&gt;For prompts about human appearance, we have only applied them to the portrait photo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like a caricature&lt;/head&gt;
    &lt;p&gt;Seedream seems to be biased towards asian culture, giving an anime look instead of a western-style cartoon caricature.&lt;/p&gt;
    &lt;head rend="h3"&gt;Turn them into an action figure in the blister pack&lt;/head&gt;
    &lt;p&gt;OpenAI’s style here went viral a while back, but Gemini is stunningly realistic. Seedream is a weird mix of realistic and hallucinations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generative edits&lt;/head&gt;
    &lt;p&gt;The place where generative AI really shines is when it can show off some creativity, and these were some prompts we added as suggestions in MorphAI to showcase that and inspire our users. OpenAI still seems to win here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a 70’s vinyl record cover&lt;/head&gt;
    &lt;p&gt;This is an example of a prompt that has a small viral moment with OpenAI, but the other models can’t even get the aspect ratio right.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduce mythical creatures native to this environment&lt;/head&gt;
    &lt;p&gt;This one showcases OpenAI’s creativity. Gemini seems kind of creepy?&lt;/p&gt;
    &lt;head rend="h3"&gt;Add a mystical portal or gateway&lt;/head&gt;
    &lt;p&gt;Gemini replacing the face with a portal is certainly a choice!&lt;/p&gt;
    &lt;head rend="h3"&gt;Incorporate futuristic technology elements&lt;/head&gt;
    &lt;p&gt;Another example of OpenAI being far more creative and willing to re-do the whole image.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look whimsical and enchanting&lt;/head&gt;
    &lt;p&gt;This one also shows OpenAI being more artistic, and Gemini being more realistic while still trying to incorporate the prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform the scene to a stormy night&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;If you made it all the way down here you probably don’t need a summary, but for our purposes, we’ve at least concluded that there is no one-size-fits all model at this point.&lt;/p&gt;
    &lt;p&gt;OpenAI is great for fully transformative filters like style transfer or more creative generative applications, whereas Gemini works better for more realistic edits. Seedream lies somewhere in the middle and is a bit of a jack of all trades, and for the price and performance may be a good replacement for OpenAI.&lt;/p&gt;
    &lt;p&gt;We’ve been experimenting on working on a “prompt classifier” to automatically choose a model – sending artistic prompts to OpenAI and more realistic prompts to Gemini, if there’s any interest we can follow up with how that worked out!&lt;/p&gt;
    &lt;head rend="h4"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;Tests were performed on October 8 with &lt;code&gt;gpt-image-1&lt;/code&gt;, &lt;code&gt;gemini-2.5-flash-image&lt;/code&gt; and &lt;code&gt;seedream-4-0-250828&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Timings were measured on a consumer internet connection in Japan (Fiber connection, 10 Gbps nominal bandwidth) during a limited test run in a short time period.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45890186</guid><pubDate>Tue, 11 Nov 2025 17:26:54 +0000</pubDate></item><item><title>Terminal Latency on Windows (2024)</title><link>https://chadaustin.me/2024/02/windows-terminal-latency/</link><description>&lt;doc fingerprint="140ed15eb7063ff7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Terminal Latency on Windows&lt;/head&gt;
    &lt;p&gt;UPDATE 2024-04-15: Windows Terminal 1.19 contains a fix that reduces latency by half! Itâs now competitive with WSLtty on my machine. Details in the GitHub Issue.&lt;/p&gt;
    &lt;p&gt;In 2009, I wrote about why MinTTY is the best terminal on Windows. Even today, that post is one of my most popular.&lt;/p&gt;
    &lt;p&gt;Since then, the terminal situation on Windows has improved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cygwin defaults to MinTTY; you no longer need to manually install it.&lt;/item&gt;
      &lt;item&gt;Windows added PTY support, obviating the need for offscreen console window hacks that add latency.&lt;/item&gt;
      &lt;item&gt;Windows added basically full support for ANSI terminal sequences in both the legacy conhost.exe consoles and its new Windows Terminal.&lt;/item&gt;
      &lt;item&gt;We now have a variety of terminals to choose from, even on Windows: Cmder, ConEmu, Alacritty, WezTerm, xterm.js (component of Visual Studio Code)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beginning of a year is a great time to look at your tools and improve your environment.&lt;/p&gt;
    &lt;p&gt;Iâd already enabled 24-bit color in all of my environments and streamlined my tmux config. Itâs about time that I take a look at the newer terminals.&lt;/p&gt;
    &lt;p&gt;Roughly in order, I care about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimum feature set: 24-bit color, reasonable default fonts with emoji support, italics are nice.&lt;/item&gt;
      &lt;item&gt;Input latency.&lt;/item&gt;
      &lt;item&gt;Throughput at line rate, for example, when I &lt;code&gt;cat&lt;/code&gt;a large file.&lt;/item&gt;
      &lt;item&gt;Support for multiple tabs in one window would be nice, but tmux suffices for me.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Which terminals should I test?&lt;/head&gt;
    &lt;p&gt;I considered the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Legacy conhost.exe (also known as Windows Console), Windows 10 19045&lt;/item&gt;
      &lt;item&gt;MinTTY (3.7.0)&lt;/item&gt;
      &lt;item&gt;Alacritty (0.13.1)&lt;/item&gt;
      &lt;item&gt;WezTerm (20240203-110809-5046fc22)&lt;/item&gt;
      &lt;item&gt;Windows Terminal (1.18.10301.0)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Testing Features&lt;/head&gt;
    &lt;p&gt;Testing color and italics support is easy with my colortest.rs script. To test basic emoji, you can cat the Unicode emoji 1.0 emoji-data.txt. To test more advanced support, try the zero-width joiner list in the latest/ directory.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Emoji&lt;/cell&gt;
        &lt;cell role="head"&gt;Font Attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No italics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY&lt;/cell&gt;
        &lt;cell&gt;Black and white&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;Black and white&lt;/cell&gt;
        &lt;cell&gt;Everything but double underline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;Color&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;Color&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Everything but conhost.exe meets my bar.&lt;/p&gt;
    &lt;p&gt;Itâs also worth noting that conhost.exe has a terrible default palette. The default yellow is a pukey green and dark blue is barely visible. You can change palettes, but defaults matter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Latency&lt;/head&gt;
    &lt;p&gt;I set up two latency tests. One with an 80x50 blank window in the upper left corner of the screen. The other fullscreen, editing an Emacs command at the bottom of the screen.&lt;/p&gt;
    &lt;p&gt;Since latencies are additive, system configuration doesnât matter as much as the absolute milliseconds of latency each terminal adds, but Iâll describe my entire setup and include total keypress-to-pixels latency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 10&lt;/item&gt;
      &lt;item&gt;Intel i7-4771 @ 3.5 GHz&lt;/item&gt;
      &lt;item&gt;NVIDIA GTX 1060&lt;/item&gt;
      &lt;item&gt;Keyboard: Sweet 16 Macro Pad&lt;/item&gt;
      &lt;item&gt;Display: LG 27GP950-B at 4K, 120 Hz, adaptive sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Measurement Methodology&lt;/head&gt;
    &lt;p&gt;With Is It Snappy?, I measured the number of frames between pressing a key and pixels changing on the screen.&lt;/p&gt;
    &lt;p&gt;To minimize ambiguity about when the key was pressed, I slammed a pencilâs eraser into the key, and always measured the key press as the second frame after contact. (The first frame was usually when the eraser barely touched the key. It would usually clear the activation depth by the second frame.)&lt;/p&gt;
    &lt;p&gt;I considered the latency to end when pixels just started to change on the screen. In practice, pixels take several 240 Hz frames to transition from black to white, but I consistently marked the beginning of that transition.&lt;/p&gt;
    &lt;p&gt;I took five measurements for each configuration and picked the median. Each measurement was relatively consistent, so average would have been a fine metric too. It doesnât change the results below.&lt;/p&gt;
    &lt;head rend="h3"&gt;80x50&lt;/head&gt;
    &lt;p&gt;80x50 window, upper left of screen, cleared terminal, single keypress.&lt;/p&gt;
    &lt;p&gt;Confirmed window size with:&lt;/p&gt;
    &lt;code&gt;$ echo $(tput cols)x$(tput lines)
80x50
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Latency (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;240 Hz Camera Frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe WSL1&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe Cygwin&lt;/cell&gt;
        &lt;cell&gt;41.3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm cmd.exe&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty WSL1&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm WSL1&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows Terminal WSL1&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Fullscreen&lt;/head&gt;
    &lt;p&gt;Maximized emacs, editing a command in the bottom row of the terminal. I only tested WSL1 this time.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Latency (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;240 Hz Camera Frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY&lt;/cell&gt;
        &lt;cell&gt;52.42&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;75&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;75&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Throughput&lt;/head&gt;
    &lt;p&gt;I generated a 100,000-line file with:&lt;/p&gt;
    &lt;code&gt;$ yes "This sentence has forty-five (45) characters." | head -n 100000 &amp;gt; /tmp/lines.txt
&lt;/code&gt;
    &lt;p&gt;Then I measured the wall-clock duration of:&lt;/p&gt;
    &lt;code&gt;$ time cat /tmp/lines.txt
&lt;/code&gt;
    &lt;p&gt;This benchmark captures the case that I accidentally dump a ton of output and Iâm sitting there just waiting for the terminal to become responsive again. I have a gigabit internet connection, and itâs embarrassing to be CPU-bound instead of IO-bound.&lt;/p&gt;
    &lt;p&gt;I did include Cygwin in this test, just to have two different MinTTY datapoints.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Elapsed Time (s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;0.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;2.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;5.25&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;6.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;21.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I assume this means MinTTY throttles display updates in some way. Of course this is totally fine, because you couldnât read the output either way.&lt;/p&gt;
    &lt;p&gt;To test the hypothesis that MinTTY was caching cell rendering by their contents, I also tried generating a file that rotated through different lines, with no effect.&lt;/p&gt;
    &lt;code&gt;with open("/tmp/lines2.txt", "w") as f:
  for i in range(100000):
    sentence="This sentence has forty-five (45) characters."
    print(sentence[i%len(sentence):]+sentence[:i%len(sentence)], file=f)
&lt;/code&gt;
    &lt;head rend="h3"&gt;CPU Usage During Repeated Keypresses&lt;/head&gt;
    &lt;p&gt;While making these measurements, I noticed some strange behaviors. My monitor runs at 120 Hz and animation and window dragging are generally smooth. But right after you start Alacritty, dragging the window animates at something like 30-60 frames per second. Itâs noticeably chunkier. WezTerm does the same, but slightly worse. Maybe 20 frames per second.&lt;/p&gt;
    &lt;p&gt;I donât know if I can blame the terminals themselves, because I sometimes experience this even with Notepad.exe too. But the choppiness stands out much more. Maybe something is CPU-bound in responding to window events?&lt;/p&gt;
    &lt;p&gt;This made me think of a new test: if I open a terminal and hold down the âaâ button on autorepeat, how much CPU does the terminal consume?&lt;/p&gt;
    &lt;p&gt;To measure this, I set the terminal processâs affinity to my third physical core, and watched the CPU usage graph in Task Manager. Not a great methodology, but it gave a rough sense. Again, 80x50.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Percent of Core&lt;/cell&gt;
        &lt;cell role="head"&gt;Private Bytes After Startup (KiB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;6,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
        &lt;cell&gt;74,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;10,200&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;10,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;73,700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;85%&lt;/cell&gt;
        &lt;cell&gt;134,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The WezTerm CPU usage has to be a bug. Iâll report it.&lt;/p&gt;
    &lt;head rend="h3"&gt;CPU Usage (Idle)&lt;/head&gt;
    &lt;p&gt;I often have a pile of idle terminals sitting around. I donât want them to chew battery life. So letâs take a look at CPU Cycles Delta (courtesy of Process Explorer) with a fresh, idle WSL session.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Idle Cycles/s (Focused)&lt;/cell&gt;
        &lt;cell role="head"&gt;Idle Cycles/s (Background)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost&lt;/cell&gt;
        &lt;cell&gt;~900,000&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;~2,400,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;~2,600,000&lt;/cell&gt;
        &lt;cell&gt;~1,600,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;~55,000,000&lt;/cell&gt;
        &lt;cell&gt;~6,100,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;~120,000,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;~120,000,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These numbers arenât great at all! For perspective, I have a pile of Firefox tabs open, some of them actively running JavaScript, and theyâre âonlyâ using a few hundred million cycles per second.&lt;/p&gt;
    &lt;p&gt;Raymond Chen once wrote a blog post about the importance of properly idling in the Windows Terminal Server days. You might have a dozen users logged into a host, and if a program is actively polling, itâs eating performance that others could use.&lt;/p&gt;
    &lt;p&gt;Today, we often run on batteries, so idling correctly still matters, but it seems to be something of a lost art. The only terminal that idles completely is the old conhost.exe.&lt;/p&gt;
    &lt;p&gt;The other lesson we can draw is that Microsoftâs own replacement for conhost.exe, Windows Terminal, uses over 10x the RAM, 60x the CPU when focused, and infinitely more CPU when idle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;conhost.exe consistently has the best latency, with MinTTY not much behind. MinTTY handily dominates the throughput test, supports all major ANSI character attributes, and has a better default palette.&lt;/p&gt;
    &lt;p&gt;As in 2009, Iâd say MinTTY is still pretty great. (I should try to track down that idle CPU consumption. It feels more like a bug than a requirement.)&lt;/p&gt;
    &lt;p&gt;If you want to use MinTTY as the default terminal for WSL, install WSLtty.&lt;/p&gt;
    &lt;p&gt;The others all have slightly worse latencies, but theyâre in a similar class. Iâm particularly sensitive to latency, so Iâd had a suspicion even before measuring. Maybe itâs some consequence of being GPU-accelerated? Out of curiousity, I put Windows Terminal in software-rendered mode, and it shaved perhaps 4 ms off (median of 62.5 ms, 15 frames). Perhaps just measurement noise.&lt;/p&gt;
    &lt;p&gt;While Iâm going to stick with MinTTY, one thing is clear: there is room to improve all of the above.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45890726</guid><pubDate>Tue, 11 Nov 2025 18:07:51 +0000</pubDate></item><item><title>FFmpeg to Google: Fund us or stop sending bugs</title><link>https://thenewstack.io/ffmpeg-to-google-fund-us-or-stop-sending-bugs/</link><description>&lt;doc fingerprint="3772b5d090a791bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FFmpeg to Google: Fund Us or Stop Sending Bugs&lt;/head&gt;
    &lt;p&gt;You may never have heard of FFmpeg, but you’ve used it. This open source program’s robust multimedia framework is used to process video and audio media files and streams across numerous platforms and devices. It provides tools and libraries for format conversion, aka transcoding, playback, editing, streaming, and post-production effects for both audio and video media.&lt;/p&gt;
    &lt;p&gt;FFmpeg’s libraries, such as libavcodec and libavformat, are essential for media players and software, including VLC, Kodi, Plex, Google Chrome, Firefox, and even YouTube’s video processing backend. It is also, like many other vital open source programs, terribly underfunded.&lt;/p&gt;
    &lt;head rend="h2"&gt;Corporate Responsibility vs. Volunteer Labor&lt;/head&gt;
    &lt;p&gt;A lively debate on Twitter began between Dan Lorenc, CEO and co-founder of Chainguard, the software supply chain security company, the FFmpeg project, Google, and security researchers over security disclosures and the responsibilities of large tech companies in open-source software.&lt;/p&gt;
    &lt;p&gt;The core of the discussion revolves around how vulnerabilities should be reported, who is responsible for fixing them, and the challenges that arise when AI is used to uncover a flood of potentially meaningless security issues. But at heart, it’s about money.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Obscure Bug Ignites the Controversy&lt;/head&gt;
    &lt;p&gt;This discussion has been heating up for some time. In mid-October, FFmpeg tweeted that “security issues are taken extremely seriously in FFmpeg, but fixes are written by volunteers.” This point cannot be emphasised enough. As FFmpeg tweeted later, “FFmpeg is written almost exclusively by volunteers.”&lt;/p&gt;
    &lt;p&gt;Thus, as Mark Atwood, an open source policy expert, pointed out on Twitter, he had to keep telling Amazon to not do things that would mess up FFmpeg because, he had to keep explaining to his bosses that “They are not a vendor, there is no NDA, we have no leverage, your VP has refused to help fund them, and they could kill three major product lines tomorrow with an email. So, stop, and listen to me … ”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Growing Burden on Open Source Maintainers&lt;/head&gt;
    &lt;p&gt;The latest episode was sparked after a Google AI agent found an especially obscure bug in FFmpeg. How obscure? This “medium impact issue in ffmpeg,” which the FFmpeg developers did patch, is “an issue with decoding LucasArts Smush codec, specifically the first 10-20 frames of Rebel Assault 2, a game from 1995.”&lt;/p&gt;
    &lt;p&gt;Wow.&lt;/p&gt;
    &lt;p&gt;FFmpeg added, “FFmpeg aims to play every video file ever made.” That’s all well and good, but is that a valuable use of an assembly programmer’s time? Oh, right, you may not know. FFmpeg’s heart is assembly language. As a former assembly language programmer, it is not, in any way, shape, or form, easy to work with.&lt;/p&gt;
    &lt;p&gt;As FFmpeg put it, this is “CVE slop.”&lt;/p&gt;
    &lt;p&gt;Many in the FFmpeg community argue, with reason, that it is unreasonable for a trillion-dollar corporation like Google, which heavily relies on FFmpeg in its products, to shift the workload of fixing vulnerabilities to unpaid volunteers. They believe Google should either provide patches with vulnerability reports or directly support the project’s maintenance.&lt;/p&gt;
    &lt;p&gt;Earlier, FFmpeg pointed out that it’s far from the only open source project to face such issues.&lt;/p&gt;
    &lt;p&gt;Specifically, the project team mentions Nick Wellnhofer, the former maintainer of libxml2, a widely used open source software library for parsing Extensible Markup Language (XML). Wellnhofer recently resigned from maintaining libxml2 because he had to “spend several hours each week dealing with security issues reported by third parties. Most of these issues aren’t critical, but it’s still a lot of work.&lt;/p&gt;
    &lt;p&gt;“In the long term, this is unsustainable for an unpaid volunteer like me. … In the long run, putting such demands on OSS maintainers without compensating them is detrimental. … It’s even more unlikely with Google Project Zero, the best white-hat security researchers money can buy, breathing down the necks of volunteers.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Google’s Controversial Security Disclosure Policy&lt;/head&gt;
    &lt;p&gt;What made this a hot issue was that back in July, Google Project Zero (GPZ) announced a trial of its new Reporting Transparency policy. With this policy change, GPZ announces that it has reported an issue on a specific project within a week of discovery, and the security standard 90-day disclosure clock then starts, regardless of whether a patch is available or not.&lt;/p&gt;
    &lt;p&gt;Many volunteer open source program maintainers and developers feel this is massively unfair to put them under such pressure when Google has billions to address the problem.&lt;/p&gt;
    &lt;p&gt;FFmpeg tweeted, “We take security very seriously, but at the same time, is it really fair that trillion-dollar corporations run AI to find security issues in people’s hobby code? Then expect volunteers to fix.”&lt;/p&gt;
    &lt;p&gt;True, Google does offer a Patch Rewards Program, but as a Twitter user using the handle Ignix The Salamander observed, “FFmpeg already mentioned the program is too limited for them, and they point out the three patches per month limit. Please don’t assume people complain just for the sake of complaining, there is a genuine conflict between corporate security &amp;amp; usage vs open source support IMHO.”&lt;/p&gt;
    &lt;p&gt;Lorenc argues back, in an e-mail to me, that “Creating and publishing software under an open source license is an act of contribution to the digital commons. Finding and publishing information about security issues in that software is also an act of contribution to the same commons.&lt;/p&gt;
    &lt;p&gt;“The position of the FFmpeg X account is that somehow disclosing vulnerabilities is a bad thing. Google provides more assistance to open source software projects than almost any other organization, and these debates are more likely to drive away potential sponsors than to attract them.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Differing Perspectives on Vulnerability Disclosures&lt;/head&gt;
    &lt;p&gt;The fundamental problem remains that the FFmpeg team lacks the financial and developer resources to address a flood of AI-created CVEs.&lt;/p&gt;
    &lt;p&gt;On the other hand, security experts are certainly right in thinking that FFmpeg is a critical part of the Internet’s technology framework and that security issues do need to be made public responsibly and addressed. After all, hackers can use AI to find vulnerabilities in the same way Google does with its AI bug finder, Big Sleep, and Google wants to identify potential security holes ahead of them.&lt;/p&gt;
    &lt;p&gt;The reality is, however, that without more support from the trillion-dollar companies that profit from open source, many woefully underfunded, volunteer-driven critical open-source projects will no longer be maintained at all.&lt;/p&gt;
    &lt;p&gt;For example, Wellnhofer has said he will no longer maintain libxml2 in December. Libxml2 is a critical library in all web browsers, web servers, LibreOffice and numerous Linux packages. We don’t need any more arguments; we need real support for critical open source programs before we have another major security breach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891016</guid><pubDate>Tue, 11 Nov 2025 18:32:11 +0000</pubDate></item><item><title>Agentic pelican on a bicycle</title><link>https://www.robert-glaser.de/agentic-pelican-on-a-bicycle/</link><description>&lt;doc fingerprint="3f239a28bf3adc7d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Agentic Pelican on a Bicycle&lt;/head&gt;
    &lt;p&gt;The agentic loop—generate, assess, improve—seems like a natural fit for iterating on pelicans on bicycles.&lt;/p&gt;
    &lt;p&gt;Simon Willison has been running his own informal model benchmark for years: “Generate an SVG of a pelican riding a bicycle.” It’s delightfully absurd—and surprisingly revealing. Even the model labs channel this benchmark in their marketing campaigns announcing new models.&lt;/p&gt;
    &lt;p&gt;Simon’s traditional approach is zero-shot: throw the prompt at the model, get SVG back. Maybe—if you’re lucky—you get something resembling a pelican on a bicycle.&lt;/p&gt;
    &lt;p&gt;Nowadays everyone is talking about agents. Models running in a loop using tools. Sometimes they have vision capabilities, too. They can look at what they just created, cringe a little, and try again. The agentic loop—generate, assess, improve—seems like a natural fit for such a task.&lt;/p&gt;
    &lt;p&gt;So I ran a different experiment: what if we let models iterate on their pelicans? What if they could see their own output and self-correct?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Prompt&lt;/head&gt;
    &lt;code&gt;Generate an SVG of a pelican riding a bicycle

- Convert the .svg to .jpg using chrome devtools, then look at the .jpg using your vision capabilities.
- Improve the .svg based on what you see in the .jpg and what's still to improve.
- Keep iterating in this loop until you're satisfied with the generated svg.
- Keep the .jpg for every iteration along the way.&lt;/code&gt;
    &lt;p&gt;Besides the file system and access to a command line, the models had access to Chrome DevTools MCP server (for SVG-to-JPG conversion) and their own multimodal vision capabilities. They could see what they’d drawn, identify problems, and iterate. The loop continued until they declared satisfaction.&lt;/p&gt;
    &lt;p&gt;I used the Chrome DevTools MCP server to give every model the same rasterizer. Without this, models would fall back to whatever SVG-to-image conversion they prefer or have available locally—ImageMagick, Inkscape, browser screenshots, whatever. Standardizing the rendering removes one variable from the equation.&lt;/p&gt;
    &lt;p&gt;The prompt itself is deliberately minimal. I could have steered the iterative loop with more specific guidance—“focus on anatomical accuracy,” “prioritize mechanical realism,” “ensure visual balance.” But that would defeat the point. Simon’s original benchmark is beautifully unconstrained, and I wanted to preserve that spirit. The question isn’t “can models follow detailed improvement instructions?” It’s “when left to their own judgment, what do they choose to fix?”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Models&lt;/head&gt;
    &lt;p&gt;I tested six models across the frontier, all multimodal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Opus 4.1, Claude Sonnet 4.5, Claude Haiku 4.5, all with thinking&lt;/item&gt;
      &lt;item&gt;GPT-5 (on medium reasoning effort)&lt;/item&gt;
      &lt;item&gt;GPT-5-Codex (on medium reasoning effort)&lt;/item&gt;
      &lt;item&gt;Gemini 2.5 Pro&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each model decided independently when to stop iterating. Some made four passes. Others kept going for six. None knew when to quit.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results&lt;/head&gt;
    &lt;p&gt;Let’s see what happened. For each model, I’m showing the first attempt (left) and the final result (right) after self-correction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Opus 4.1 (4 iterations)&lt;/head&gt;
    &lt;p&gt;Opus started with a serviceable pelican-bicycle combo and then did something interesting: it added realism. The final version has an actual bicycle chain connecting the pedals to the rear wheel. The wheels gained more spokes. The pelican’s proportions improved, and it got arms holding the handlebars. This wasn’t just “add more details”—it was “make this mechanically coherent.” Interestingly, we got the catch of the day on a special plate on the handlebars. Oh, and look at the street and the birds in the backdrop!&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Sonnet 4.5 (4 iterations)&lt;/head&gt;
    &lt;p&gt;Sonnet took a more restrained approach. The changes between iterations were subtler—refinements to curves, adding shadows, and movement indicators. Adjustments to positioning. Improving the spokes. Improving the arms and handlebars. The final result is cleaner, but the core composition remained remarkably stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Haiku 4.5 (6 iterations)&lt;/head&gt;
    &lt;p&gt;Haiku took the longest journey—six full iterations. It kept tweaking, kept adjusting. The additional iterations didn’t necessarily produce a dramatically better result, but Haiku seemed determined to get every detail right before calling it done. So the pelican definitely received proper legs and feet.&lt;/p&gt;
    &lt;head rend="h3"&gt;GPT-5 Medium (5 iterations)&lt;/head&gt;
    &lt;p&gt;GPT-5 Medium started with a recognizable pelican-bicycle scene and refined it over five iterations. The improvements were incremental—better proportions, clearer shapes—but the fundamental composition held steady throughout.&lt;/p&gt;
    &lt;head rend="h3"&gt;GPT-5-Codex Medium (5 iterations)&lt;/head&gt;
    &lt;p&gt;Here’s where things get interesting. Its initial attempt was… let’s call it “abstract.” A sort of layer cake of pelican parts. And then, instead of simplifying, it doubled down. The final result added even more layers. More complexity. More parts. Whether this counts as “improvement” is a philosophical question I’m not qualified to answer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini 2.5 Pro (6 iterations)&lt;/head&gt;
    &lt;p&gt;Gemini was the outlier. Most models preserved their initial composition through iterations, making refinements but keeping the core structure mostly intact. Gemini actually changed the fundamental arrangement—the pelican’s pose, the bicycle’s orientation, the spatial relationship between them. Six iterations showed a bigger leap.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Did We Learn?&lt;/head&gt;
    &lt;p&gt;The results are… mixed.&lt;/p&gt;
    &lt;p&gt;The optimistic take: Models like Opus 4.1 made genuinely thoughtful improvements. Adding a bicycle chain isn’t just decoration—it shows understanding of mechanical relationships. The wheel spokes, the adjusted proportions—these are signs of vision-driven refinement working as intended.&lt;/p&gt;
    &lt;p&gt;The skeptical take: Most models didn’t fundamentally change their approach. They tweaked. They adjusted. They added details. But the basic composition—pelican shape, bicycle shape, spatial relationship—was determined in iteration one and largely frozen thereafter.&lt;/p&gt;
    &lt;p&gt;The confusing take: Some models (looking at you, GPT-5-Codex) seemed to mistake “more complex” for “better.” The self-feedback loop amplified their initial artistic direction rather than correcting it. If your first draft is a layer cake of pelican parts, and your self-correction produces an even more elaborate layer cake... did the loop help? Of course, GPT-5-Codex is a fine-tune of GPT-5, optimized for engineering tasks. Could be that its strengths are not in broad visual capabilities.&lt;/p&gt;
    &lt;p&gt;The agentic approach definitely produces different results than zero-shot generation. Whether it produces better results seems to depend heavily on the model’s ability to self-critique. Vision capabilities alone aren’t enough—you need something more: aesthetic judgment, mechanical reasoning, or at least the wisdom to know when to stop adding details.&lt;/p&gt;
    &lt;p&gt;Simon’s zero-shot benchmark reveals how well models handle unusual creative tasks on the first try. The agentic variant reveals something else: how well models can evaluate and improve their own creative output. Turns out, that’s a different skill entirely. But—somehow related?&lt;/p&gt;
    &lt;p&gt;All test code and results available in the GitHub repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891817</guid><pubDate>Tue, 11 Nov 2025 19:40:15 +0000</pubDate></item><item><title>A catalog of side effects</title><link>https://bernsteinbear.com/blog/compiler-effects/</link><description>&lt;doc fingerprint="6202111079d508b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Optimizing compilers like to keep track of each IR instruction’s effects. An instruction’s effects vary wildly from having no effects at all, to writing a specific variable, to completely unknown (writing all state).&lt;/p&gt;
    &lt;p&gt;This post can be thought of as a continuation of What I talk about when I talk about IRs, specifically the section talking about asking the right questions. When we talk about effects, we should ask the right questions: not what opcode is this? but instead what effects does this opcode have?&lt;/p&gt;
    &lt;p&gt;Different compilers represent and track these effects differently. I’ve been thinking about how to represent these effects all year, so I have been doing some reading. In this post I will give some summaries of the landscape of approaches. Please feel free to suggest more.&lt;/p&gt;
    &lt;p&gt;Internal IR effect tracking is similar to the programming language notion of algebraic effects in type systems, but internally, compilers keep track of finer-grained effects. Effects such as “writes to a local variable”, “writes to a list”, or “reads from the stack” indicate what instructions can be re-ordered, duplicated, or removed entirely.&lt;/p&gt;
    &lt;p&gt;For example, consider the following pseodocode for some made-up language that stands in for a snippet of compiler IR:&lt;/p&gt;
    &lt;code&gt;# ...
v = some_var[0]
another_var[0] = 5
# ...
&lt;/code&gt;
    &lt;p&gt;The goal of effects is to communicate to the compiler if, for example, these two IR instructions can be re-ordered. The second instruction might write to a location that the first one reads. But it also might not! This is about knowing if &lt;code&gt;some_var&lt;/code&gt; and &lt;code&gt;another_var&lt;/code&gt; alias—if they are different names that
refer to the same object.&lt;/p&gt;
    &lt;p&gt;We can sometimes answer that question directly, but often it’s cheaper to compute an approximate answer: could they even alias? It’s possible that &lt;code&gt;some_var&lt;/code&gt; and &lt;code&gt;another_var&lt;/code&gt; have different types, meaning that (as long as you
have strict aliasing) the &lt;code&gt;Load&lt;/code&gt; and &lt;code&gt;Store&lt;/code&gt; operations that implement these
reads and writes by definition touch different locations. And if they look
at disjoint locations, there need not be any explicit order enforced.&lt;/p&gt;
    &lt;p&gt;Different compilers keep track of this information differently. The null effect analysis gives up and says “every instruction is maximally effectful” and therefore “we can’t re-order or delete any instructions”. That’s probably fine for a first stab at a compiler, where you will get a big speed up purely based on strength reductions. Over-approximations of effects should always be valid.&lt;/p&gt;
    &lt;p&gt;But at some point you start wanting to do dead code elimination (DCE), or common subexpression elimination (CSE), or loads/store elimination, or move instructions around, and you start wondering how to represent effects. That’s where I am right now. So here’s a catalog of different compilers I have looked at recently.&lt;/p&gt;
    &lt;p&gt;There are two main ways I have seen to represent effects: bitsets and heap range lists. We’ll look at one example compiler for each, talk a bit about tradeoffs, then give a bunch of references to other major compilers.&lt;/p&gt;
    &lt;p&gt;We’ll start with Cinder, a Python JIT, because that’s what I used to work on.&lt;/p&gt;
    &lt;p&gt;Cinder tracks heap effects for its high-level IR (HIR) in instr_effects.h. Pretty much everything happens in the &lt;code&gt;memoryEffects(const Instr&amp;amp; instr)&lt;/code&gt; function, which is expected to know
everything about what effects the given instruction might have.&lt;/p&gt;
    &lt;p&gt;The data representation is a bitset representation of a lattice called an &lt;code&gt;AliasClass&lt;/code&gt; and that is defined in alias_class.h. Each
bit in the bitset represents a distinct location in the heap: reads from and
writes to each of these locations are guaranteed not to affect any of the other
locations.&lt;/p&gt;
    &lt;p&gt;Here is the X-macro that defines it:&lt;/p&gt;
    &lt;code&gt;#define HIR_BASIC_ACLS(X) \
  X(ArrayItem)            \
  X(CellItem)             \
  X(DictItem)             \
  X(FuncArgs)             \
  X(FuncAttr)             \
  X(Global)               \
  X(InObjectAttr)         \
  X(ListItem)             \
  X(Other)                \
  X(TupleItem)            \
  X(TypeAttrCache)        \
  X(TypeMethodCache)

enum BitIndexes {
#define ACLS(name) k##name##Bit,
    HIR_BASIC_ACLS(ACLS)
#undef ACLS
};
&lt;/code&gt;
    &lt;p&gt;Note that each bit implicitly represents a set: &lt;code&gt;ListItem&lt;/code&gt; does not refer to a
specific list index, but the infinite set of all possible list indices. It’s
any list index. Still, every list index is completely disjoint from, say, every
entry in a global variable table.&lt;/p&gt;
    &lt;p&gt;(And, to be clear, an object in a list might be the same as an object in a global variable table. The objects themselves can alias. But the thing being written to or read from, the thing being side effected, is the container.)&lt;/p&gt;
    &lt;p&gt;Like other bitset lattices, it’s possible to union the sets by or-ing the bits. It’s possible to query for overlap by and-ing the bits.&lt;/p&gt;
    &lt;code&gt;class AliasClass {
  // The union of two AliasClass
  AliasClass operator|(AliasClass other) const {
    return AliasClass{bits_ | other.bits_};
  }

  // The intersection (overlap) of two AliasClass
  AliasClass operator&amp;amp;(AliasClass other) const {
    return AliasClass{bits_ &amp;amp; other.bits_};
  }
};
&lt;/code&gt;
    &lt;p&gt;If this sounds familiar, it’s because (as the repo notes) it’s a similar idea to Cinder’s type lattice representation.&lt;/p&gt;
    &lt;p&gt;Like other lattices, there is both a bottom element (no effects) and a top element (all possible effects):&lt;/p&gt;
    &lt;code&gt;#define HIR_OR_BITS(name) | k##name

#define HIR_UNION_ACLS(X)                           \
  /* Bottom union */                                \
  X(Empty, 0)                                       \
  /* Top union */                                   \
  X(Any, 0 HIR_BASIC_ACLS(HIR_OR_BITS))             \
  /* Memory locations accessible by managed code */ \
  X(ManagedHeapAny, kAny &amp;amp; ~kFuncArgs)
&lt;/code&gt;
    &lt;p&gt;Union operations naturally hit a fixpoint at &lt;code&gt;Any&lt;/code&gt; and intersection operations
naturally hit a fixpoint at &lt;code&gt;Empty&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All of this together lets the optimizer ask and answer questions such as:&lt;/p&gt;
    &lt;p&gt;and more.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at an (imaginary) IR version of the code snippet in the intro and see what analyzing it might look like in the optimizer. Here is the fake IR:&lt;/p&gt;
    &lt;code&gt;v0: Tuple = ...
v1: List = ...
v2: Int[5] = ...
# v = some_var[0]
v3: Object = LoadTupleItem v0, 0
# another_var[0] = 5
StoreListItem v1, 0, v2
&lt;/code&gt;
    &lt;p&gt;You can imagine that &lt;code&gt;LoadTupleItem&lt;/code&gt; declares that it reads from the
&lt;code&gt;TupleItem&lt;/code&gt; heap and &lt;code&gt;StoreListItem&lt;/code&gt; declares that it writes to the &lt;code&gt;ListItem&lt;/code&gt;
heap. Because tuple and list pointers cannot be casted into one another and
therefore cannot alias, these are
disjoint heaps in our bitset. Therefore &lt;code&gt;ListItem &amp;amp; TupleItem == 0&lt;/code&gt;, therefore
these memory operations can never interfere! They can (for example) be
re-ordered arbitrarily.&lt;/p&gt;
    &lt;p&gt;In Cinder, these memory effects could in the future be used for instruction re-ordering, but they are today mostly used in two places: the refcount insertion pass and DCE.&lt;/p&gt;
    &lt;p&gt;DCE involves first finding the set of instructions that need to be kept around because they are useful/important/have effects. So here is what the Cinder DCE &lt;code&gt;isUseful&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;bool isUseful(Instr&amp;amp; instr) {
  return instr.IsTerminator() || instr.IsSnapshot() ||
      (instr.asDeoptBase() != nullptr &amp;amp;&amp;amp; !instr.IsPrimitiveBox()) ||
      (!instr.IsPhi() &amp;amp;&amp;amp; memoryEffects(instr).may_store != AEmpty);
}
&lt;/code&gt;
    &lt;p&gt;There are some other checks in there but &lt;code&gt;memoryEffects&lt;/code&gt; is right there at the
core of it!&lt;/p&gt;
    &lt;p&gt;Now that we have seen the bitset representation of effects and an implementation in Cinder, let’s take a look at a different representation and and an implementation in JavaScriptCore.&lt;/p&gt;
    &lt;p&gt;I keep coming back to How I implement SSA form by Fil Pizlo, one of the significant contributors to JavaScriptCore (JSC). In particular, I keep coming back to the Uniform Effect Representation section. This notion of “abstract heaps” felt very… well, abstract. Somehow more abstract than the bitset representation. The pre-order and post-order integer pair as a way to represent nested heap effects just did not click.&lt;/p&gt;
    &lt;p&gt;It didn’t make any sense until I actually went spelunking in JavaScriptCore and found one of several implementations—because, you know, JSC is six compilers in a trenchcoat[citation needed].&lt;/p&gt;
    &lt;p&gt;DFG, B3, DOMJIT, and probably others all have their own abstract heap implementations. We’ll look at DOMJIT mostly because it’s a smaller example and also illustrates something else that’s interesting: builtins. We’ll come back to builtins in a minute.&lt;/p&gt;
    &lt;p&gt;Let’s take a lookat how DOMJIT structures its abstract heaps: a YAML file.&lt;/p&gt;
    &lt;code&gt;DOM:
    Tree:
        Node:
            - Node_firstChild
            - Node_lastChild
            - Node_parentNode
            - Node_nextSibling
            - Node_previousSibling
            - Node_ownerDocument
        Document:
            - Document_documentElement
            - Document_body
&lt;/code&gt;
    &lt;p&gt;It’s a hierarchy. &lt;code&gt;Node_firstChild&lt;/code&gt; is a subheap of &lt;code&gt;Node&lt;/code&gt; is a subheap of…
and so on. A write to any &lt;code&gt;Node_nextSibling&lt;/code&gt; is a write to &lt;code&gt;Node&lt;/code&gt; is a write to
… Sibling heaps are unrelated: &lt;code&gt;Node_firstChild&lt;/code&gt; and &lt;code&gt;Node_lastChild&lt;/code&gt;, for
example, are disjoint.&lt;/p&gt;
    &lt;p&gt;To get a feel for this, I wired up a simplified version of ZJIT’s bitset generator (for types!) to read a YAML document and generate a bitset. It generated the following Rust code:&lt;/p&gt;
    &lt;code&gt;mod bits {
  pub const Empty: u64 = 0u64;
  pub const Document_body: u64 = 1u64 &amp;lt;&amp;lt; 0;
  pub const Document_documentElement: u64 = 1u64 &amp;lt;&amp;lt; 1;
  pub const Document: u64 = Document_body | Document_documentElement;
  pub const Node_firstChild: u64 = 1u64 &amp;lt;&amp;lt; 2;
  pub const Node_lastChild: u64 = 1u64 &amp;lt;&amp;lt; 3;
  pub const Node_nextSibling: u64 = 1u64 &amp;lt;&amp;lt; 4;
  pub const Node_ownerDocument: u64 = 1u64 &amp;lt;&amp;lt; 5;
  pub const Node_parentNode: u64 = 1u64 &amp;lt;&amp;lt; 6;
  pub const Node_previousSibling: u64 = 1u64 &amp;lt;&amp;lt; 7;
  pub const Node: u64 = Node_firstChild | Node_lastChild | Node_nextSibling | Node_ownerDocument | Node_parentNode | Node_previousSibling;
  pub const Tree: u64 = Document | Node;
  pub const DOM: u64 = Tree;
  pub const NumTypeBits: u64 = 8;
}
&lt;/code&gt;
    &lt;p&gt;It’s not a fancy X-macro, but it’s a short and flexible Ruby script.&lt;/p&gt;
    &lt;p&gt;Then I took the DOMJIT abstract heap generator—also funnily enough a short Ruby script—modified the output format slightly, and had it generate its int pairs:&lt;/p&gt;
    &lt;code&gt;mod bits {
  /* DOMJIT Abstract Heap Tree.
  DOM&amp;lt;0,8&amp;gt;:
      Tree&amp;lt;0,8&amp;gt;:
          Node&amp;lt;0,6&amp;gt;:
              Node_firstChild&amp;lt;0,1&amp;gt;
              Node_lastChild&amp;lt;1,2&amp;gt;
              Node_parentNode&amp;lt;2,3&amp;gt;
              Node_nextSibling&amp;lt;3,4&amp;gt;
              Node_previousSibling&amp;lt;4,5&amp;gt;
              Node_ownerDocument&amp;lt;5,6&amp;gt;
          Document&amp;lt;6,8&amp;gt;:
              Document_documentElement&amp;lt;6,7&amp;gt;
              Document_body&amp;lt;7,8&amp;gt;
  */
  pub const DOM: HeapRange = HeapRange { start: 0, end: 8 };
  pub const Tree: HeapRange = HeapRange { start: 0, end: 8 };
  pub const Node: HeapRange = HeapRange { start: 0, end: 6 };
  pub const Node_firstChild: HeapRange = HeapRange { start: 0, end: 1 };
  pub const Node_lastChild: HeapRange = HeapRange { start: 1, end: 2 };
  pub const Node_parentNode: HeapRange = HeapRange { start: 2, end: 3 };
  pub const Node_nextSibling: HeapRange = HeapRange { start: 3, end: 4 };
  pub const Node_previousSibling: HeapRange = HeapRange { start: 4, end: 5 };
  pub const Node_ownerDocument: HeapRange = HeapRange { start: 5, end: 6 };
  pub const Document: HeapRange = HeapRange { start: 6, end: 8 };
  pub const Document_documentElement: HeapRange = HeapRange { start: 6, end: 7 };
  pub const Document_body: HeapRange = HeapRange { start: 7, end: 8 };
}
&lt;/code&gt;
    &lt;p&gt;It already comes with a little diagram, which is super helpful for readability.&lt;/p&gt;
    &lt;p&gt;Any empty range(s) represent empty heap effects: if the start and end are the same number, there are no effects. There is no one &lt;code&gt;Empty&lt;/code&gt; value, but any empty
range could be normalized to &lt;code&gt;HeapRange { start: 0, end: 0 }&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Maybe this was obvious to you, dear reader, but this pre-order/post-order thing is about nested ranges! Seeing the output of the generator laid out clearly like this made it make a lot more sense for me.&lt;/p&gt;
    &lt;p&gt;What about checking overlap? Here is the implementation in JSC:&lt;/p&gt;
    &lt;code&gt;namespace WTF {
// Check if two ranges overlap assuming that neither range is empty.
template&amp;lt;typename T&amp;gt;
constexpr bool nonEmptyRangesOverlap(T leftMin, T leftMax, T rightMin, T rightMax)
{
    ASSERT_UNDER_CONSTEXPR_CONTEXT(leftMin &amp;lt; leftMax);
    ASSERT_UNDER_CONSTEXPR_CONTEXT(rightMin &amp;lt; rightMax);

    return leftMax &amp;gt; rightMin &amp;amp;&amp;amp; rightMax &amp;gt; leftMin;
}

// Pass ranges with the min being inclusive and the max being exclusive.
template&amp;lt;typename T&amp;gt;
constexpr bool rangesOverlap(T leftMin, T leftMax, T rightMin, T rightMax) {
    ASSERT_UNDER_CONSTEXPR_CONTEXT(leftMin &amp;lt;= leftMax);
    ASSERT_UNDER_CONSTEXPR_CONTEXT(rightMin &amp;lt;= rightMax);

    // Empty ranges interfere with nothing.
    if (leftMin == leftMax)
        return false;
    if (rightMin == rightMax)
        return false;

    return nonEmptyRangesOverlap(leftMin, leftMax, rightMin, rightMax);
}
}

class HeapRange {
    bool overlaps(const HeapRange&amp;amp; other) const {
        return WTF::rangesOverlap(m_begin, m_end, other.m_begin, other.m_end);
    }
}
&lt;/code&gt;
    &lt;p&gt;(See also How to check for overlapping intervals and Range overlap in two compares for more fun.)&lt;/p&gt;
    &lt;p&gt;While bitsets are a dense representation (you have to hold every bit), they are very compact and they are very precise. You can hold any number of combinations of 64 or 128 bits in a single register. The union and intersection operations are very cheap.&lt;/p&gt;
    &lt;p&gt;With int ranges, it’s a little more complicated. An imprecise union of &lt;code&gt;a&lt;/code&gt; and
&lt;code&gt;b&lt;/code&gt; can take the maximal range that covers both &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. To get a more
precise union, you have to keep track of both. In the worst case, if you want
efficient arbitrary queries, you need to store your int ranges in an interval
tree. So what gives?&lt;/p&gt;
    &lt;p&gt;I asked Fil if both bitsets and int ranges answer the same question, why use int ranges? He said that it’s more flexible long-term: bitsets get expensive as soon as you need over 128 bits (you might need to heap allocate them!) whereas ranges have no such ceiling. But doesn’t holding sequences of ranges require heap allocation? Well, despite Fil writing this in his SSA post:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The purpose of the effect representation baked into the IR is to provide a precise always-available baseline for alias information that is super easy to work with. […] you can have instructions report that they read/write multiple heaps […] you can have a utility function that produces such lists on demand.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It’s important to note that this doesn’t actually involve any allocation of lists. JSC does this very clever thing where they have “functors” that they pass in as arguments that compress/summarize what they want to out of an instruction’s effects.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at how the DFG (for example) uses these heap ranges in analysis. The DFG is structured in such a way that it can make use of the DOMJIT heap ranges directly, which is neat.&lt;/p&gt;
    &lt;p&gt;Note that &lt;code&gt;AbstractHeap&lt;/code&gt; in the example below is a thin wrapper over the DFG
compiler’s own &lt;code&gt;DOMJIT::HeapRange&lt;/code&gt; equivalent:&lt;/p&gt;
    &lt;code&gt;class AbstractHeapOverlaps {
public:
    AbstractHeapOverlaps(AbstractHeap heap)
        : m_heap(heap)
        , m_result(false)
    {
    }

    void operator()(AbstractHeap otherHeap) const
    {
        if (m_result)
            return;
        m_result = m_heap.overlaps(otherHeap);
    }

    bool result() const { return m_result; }

private:
    AbstractHeap m_heap;
    mutable bool m_result;
};

bool writesOverlap(Graph&amp;amp; graph, Node* node, AbstractHeap heap)
{
    NoOpClobberize noOp;
    AbstractHeapOverlaps addWrite(heap);
    clobberize(graph, node, noOp, addWrite, noOp);
    return addWrite.result();
}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;clobberize&lt;/code&gt; is the function that calls these functors (&lt;code&gt;noOp&lt;/code&gt; or &lt;code&gt;addWrite&lt;/code&gt; in
this case) for each effect that the given IR instruction &lt;code&gt;node&lt;/code&gt; declares.&lt;/p&gt;
    &lt;p&gt;I’ve pulled some relevant snippets of &lt;code&gt;clobberize&lt;/code&gt;, which is quite long, that I
think are interesting.&lt;/p&gt;
    &lt;p&gt;First, some instructions (constants, here) have no effects. There’s some utility in the &lt;code&gt;def(PureValue(...))&lt;/code&gt; call but I didn’t understand fully.&lt;/p&gt;
    &lt;p&gt;Then there are some instructions that conditionally have effects depending on the use types of their operands.1 Taking the absolute value of an Int32 or a Double is effect-free but otherwise looks like it can run arbitrary code.&lt;/p&gt;
    &lt;p&gt;Some run-time IR guards that might cause side exits are annotated as such—they write to the &lt;code&gt;SideState&lt;/code&gt; heap.&lt;/p&gt;
    &lt;p&gt;Local variable instructions read specific heaps indexed by what looks like the local index but I’m not sure. This means accessing two different locals won’t alias!&lt;/p&gt;
    &lt;p&gt;Instructions that allocate can’t be re-ordered, it looks like; they both read and write the &lt;code&gt;HeapObjectCount&lt;/code&gt;. This probably limits the amount of allocation
sinking that can be done.&lt;/p&gt;
    &lt;p&gt;Then there’s &lt;code&gt;CallDOM&lt;/code&gt;, which is the builtins stuff I was talking about. We’ll
come back to that after the code block.&lt;/p&gt;
    &lt;code&gt;template&amp;lt;typename ReadFunctor, typename WriteFunctor, typename DefFunctor, typename ClobberTopFunctor&amp;gt;
void clobberize(Graph&amp;amp; graph, Node* node, const ReadFunctor&amp;amp; read, const WriteFunctor&amp;amp; write, const DefFunctor&amp;amp; def)
{
    // ...

    switch (node-&amp;gt;op()) {
    case JSConstant:
    case DoubleConstant:
    case Int52Constant:
        def(PureValue(node, node-&amp;gt;constant()));
        return;

    case ArithAbs:
        if (node-&amp;gt;child1().useKind() == Int32Use || node-&amp;gt;child1().useKind() == DoubleRepUse)
            def(PureValue(node, node-&amp;gt;arithMode()));
        else
            clobberTop();
        return;

    case AssertInBounds:
    case AssertNotEmpty:
        write(SideState);
        return;

    case GetLocal:
        read(AbstractHeap(Stack, node-&amp;gt;operand()));
        def(HeapLocation(StackLoc, AbstractHeap(Stack, node-&amp;gt;operand())), LazyNode(node));
        return;

    case NewArrayWithSize:
    case NewArrayWithSizeAndStructure:
        read(HeapObjectCount);
        write(HeapObjectCount);
        return;

    case CallDOM: {
        const DOMJIT::Signature* signature = node-&amp;gt;signature();
        DOMJIT::Effect effect = signature-&amp;gt;effect;
        if (effect.reads) {
            if (effect.reads == DOMJIT::HeapRange::top())
                read(World);
            else
                read(AbstractHeap(DOMState, effect.reads.rawRepresentation()));
        }
        if (effect.writes) {
            if (effect.writes == DOMJIT::HeapRange::top()) {
                if (Options::validateDFGClobberize())
                    clobberTopFunctor();
                write(Heap);
            } else
                write(AbstractHeap(DOMState, effect.writes.rawRepresentation()));
        }
        ASSERT_WITH_MESSAGE(effect.def == DOMJIT::HeapRange::top(), "Currently, we do not accept any def for CallDOM.");
        return;
    }
    }
}
&lt;/code&gt;
    &lt;p&gt;(Remember that these &lt;code&gt;AbstractHeap&lt;/code&gt; operations are very similar to DOMJIT’s
&lt;code&gt;HeapRange&lt;/code&gt; with a couple more details—and in some cases even contain DOMJIT
&lt;code&gt;HeapRange&lt;/code&gt;s!)&lt;/p&gt;
    &lt;p&gt;This &lt;code&gt;CallDOM&lt;/code&gt; node is the way for the DOM APIs in the browser—a significant
chunk of the builtins, which are written in C++—to communicate what they do
to the optimizing compiler. Without any annotations, the JIT has to assume that
a call into C++ could do anything to the JIT state. Bummer!&lt;/p&gt;
    &lt;p&gt;But because, for example, &lt;code&gt;Node.firstChild&lt;/code&gt; annotates what
memory it reads from and what it doesn’t write to,
the JIT can optimize around it better—or even remove the access completely.
It means the JIT can reason about calls to known builtins the same way that
it reasons about normal JIT opcodes.&lt;/p&gt;
    &lt;p&gt;(Incidentally it looks like it doesn’t even make a C call, but instead is inlined as a little memory read snippet using a JIT builder API. Neat.)&lt;/p&gt;
    &lt;p&gt;Last, we’ll look at Simple, which has a slightly different take on all of this.&lt;/p&gt;
    &lt;p&gt;Simple is Cliff Click’s pet Sea of Nodes (SoN) project to try and showcase the idea to the world—outside of a HotSpot C2 context.&lt;/p&gt;
    &lt;p&gt;This one is a little harder for me to understand but it looks like each translation unit has a &lt;code&gt;StartNode&lt;/code&gt; that doles out
different classes of memory nodes for each alias class. Each IR node then takes
data dependencies on whatever effect nodes it might uses.&lt;/p&gt;
    &lt;p&gt;Alias classes are split up based on the paper Type-Based Alias Analysis (PDF): “Our approach is a form of TBAA similar to the ‘FieldTypeDecl’ algorithm described in the paper.”&lt;/p&gt;
    &lt;p&gt;The Simple project is structured into sequential implementation stages and alias classes come into the picture in Chapter 10.&lt;/p&gt;
    &lt;p&gt;Because I spent a while spelunking through other implementations to see how other projects did this, here is a list of the projects I looked at. Mostly, they use bitsets.&lt;/p&gt;
    &lt;p&gt;HHVM, a JIT for the Hack language, also uses a bitset for its memory effects. See for example: alias-class.h and memory-effects.h.&lt;/p&gt;
    &lt;p&gt;HHVM has a couple places that use this information, such as a definition-sinking pass, alias analysis, DCE, store elimination, refcount opts, and more.&lt;/p&gt;
    &lt;p&gt;If you are wondering why the HHVM representation looks similar to the Cinder representation, it’s because some former HHVM engineers such as Brett Simmers also worked on Cinder!&lt;/p&gt;
    &lt;p&gt;(note that I am linking an ART fork on GitHub as a reference, but the upstream code is hosted on googlesource)&lt;/p&gt;
    &lt;p&gt;Android’s ART Java runtime also uses a bitset for its effect representation. It’s a very compact class called &lt;code&gt;SideEffects&lt;/code&gt; in nodes.h.&lt;/p&gt;
    &lt;p&gt;The side effects are used in loop-invariant code motion, global value numbering, write barrier elimination, scheduling, and more.&lt;/p&gt;
    &lt;p&gt;CoreCLR mostly uses a bitset for its &lt;code&gt;SideEffectSet&lt;/code&gt;
class. This one is interesting though because it also splits out effects
specifically to include sets of local variables (&lt;code&gt;LclVarSet&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;V8 is also about six completely different compilers in a trenchcoat.&lt;/p&gt;
    &lt;p&gt;Turboshaft uses a struct in operations.h called &lt;code&gt;OpEffects&lt;/code&gt; which is two bitsets for reads/writes of effects. This is used in
value numbering as well a bunch of
other small optimization passes they call “reducers”.&lt;/p&gt;
    &lt;p&gt;Maglev also has this thing called &lt;code&gt;NodeT::kProperties&lt;/code&gt; in their IR
nodes that also looks like a bitset and is used in their various
reducers. It has effect query methods on it such as &lt;code&gt;can_eager_deopt&lt;/code&gt; and
&lt;code&gt;can_write&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Until recently, V8 also used Sea of Nodes as its IR representation, which also tracks side effects more explicitly in the structure of the IR itself.&lt;/p&gt;
    &lt;p&gt;Guile Scheme looks like it has a custom tagging scheme type thing.&lt;/p&gt;
    &lt;p&gt;Both bitsets and int ranges are perfectly cromulent ways of representing heap effects for your IR. The Sea of Nodes approach is also probably okay since it powers HotSpot C2 and (for a time) V8.&lt;/p&gt;
    &lt;p&gt;Remember to ask the right questions of your IR when doing analysis.&lt;/p&gt;
    &lt;p&gt;Thank you to Fil Pizlo for writing his initial GitHub Gist and sending me on this journey and thank you to Chris Gregory, Brett Simmers, and Ufuk Kayserilioglu for feedback on making some of the explanations more helpful.&lt;/p&gt;
    &lt;p&gt;This is because the DFG compiler does this interesting thing where they track and guard the input types on use vs having types attached to the input’s own def. It might be a clean way to handle shapes inside the type system while also allowing the type+shape of an object to change over time (which it can do in many dynamic language runtimes). ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891868</guid><pubDate>Tue, 11 Nov 2025 19:44:49 +0000</pubDate></item><item><title>A modern 35mm film scanner for home</title><link>https://www.soke.engineering/</link><description>&lt;doc fingerprint="4d1a5a5e7b33d889"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - a high-resolution 35 mm film scanner built for photographers who demand speed, quality and control.&lt;/head&gt;
    &lt;head rend="h2"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - a high-resolution 35 mm film scanner built for photographers who demand speed, quality, and control.&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke redefines film scanning by bringing modern imaging, optics, and software into a beautifully engineered device.&lt;/head&gt;
    &lt;head rend="h3"&gt;4064&lt;/head&gt;
    &lt;head rend="h3"&gt;4064&lt;/head&gt;
    &lt;head rend="h3"&gt;DPI Resolution&lt;/head&gt;
    &lt;head rend="h3"&gt;DPI Resolution&lt;/head&gt;
    &lt;head rend="h3"&gt;120 dB&lt;/head&gt;
    &lt;head rend="h3"&gt;120 dB&lt;/head&gt;
    &lt;head rend="h3"&gt;Dynamic Range&lt;/head&gt;
    &lt;head rend="h3"&gt;Dynamic Range&lt;/head&gt;
    &lt;head rend="h3"&gt;48-bit&lt;/head&gt;
    &lt;head rend="h3"&gt;48-bit&lt;/head&gt;
    &lt;head rend="h3"&gt;True Color&lt;/head&gt;
    &lt;head rend="h3"&gt;True Color&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;head rend="h3"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;p&gt;The modern 35 mm film scanner that captures a full roll in under just a few minutes while capturing every frame at 4064 DPI and 48bit colour. Its custom optics and state-of-the-art sensor deliver benchmark setting quality and speed at a price only Knokke can offer.&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;head rend="h2"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;head rend="h3"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;head rend="h2"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;p&gt;Built for the 21st century, Knokke runs on Korova, a lean C++ application that's native to Linux, macOS, and Windowsâso you can forget vintage PCs and enjoy a plug-and-play workflow that lets you focus on your photos.&lt;/p&gt;
    &lt;p&gt;Each frame can have custom scan settings, repeatable across multiple scans for consistent results and tailored workflows. The scanner can also skip directly to requested frames, massively accelerating scanning time and enabling fast access to key shots without unnecessary delay.&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;head rend="h3"&gt;Engineered for Individual Users and Lab Professionals&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Price at Launch&lt;/head&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illumintaed CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illumintaed CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h2"&gt;Price at Launch&lt;/head&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;Frequently &lt;lb/&gt;Asked &lt;lb/&gt;Questions.&lt;/head&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Thank you for your interest! Weâve received an incredible number of requests to join our beta testing program. Weâll be running two separate testing rounds - one in collaboration with selected creators and film labs, and another open to members of our community.&lt;/p&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Thank you for your interest! Weâve received an incredible number of requests to join our beta testing program. Weâll be running two separate testing rounds - one in collaboration with selected creators and film labs, and another open to members of our community.&lt;/p&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;Frequently Asked Questions.&lt;/head&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter in Q1 2026. Follow us on Instagram or subscribe to our newsletter to be among the first notified when pre-orders open.&lt;/p&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter in Q1 2026. Follow us on Instagram or subscribe to our newsletter to be among the first notified when pre-orders open.&lt;/p&gt;
    &lt;head rend="h4"&gt;Engineered for individual and lab use&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illuminated CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illuminated CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;Engineered for individual and lab use&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891907</guid><pubDate>Tue, 11 Nov 2025 19:48:19 +0000</pubDate></item><item><title>Adk-go: code-first Go toolkit for building, evaluating, and deploying AI agents</title><link>https://github.com/google/adk-go</link><description>&lt;doc fingerprint="3d738c52c9d3a6e3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;An open-source, code-first Go toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.&lt;/head&gt;
    &lt;head rend="h3"&gt;Important Links: Docs &amp;amp; Samples &amp;amp; Python ADK &amp;amp; Java ADK &amp;amp; ADK Web.&lt;/head&gt;
    &lt;p&gt;Agent Development Kit (ADK) is a flexible and modular framework that applies software development principles to AI agent creation. It is designed to simplify building, deploying, and orchestrating agent workflows, from simple tasks to complex systems. While optimized for Gemini, ADK is model-agnostic, deployment-agnostic, and compatible with other frameworks.&lt;/p&gt;
    &lt;p&gt;This Go version of ADK is ideal for developers building cloud-native agent applications, leveraging Go's strengths in concurrency and performance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Idiomatic Go: Designed to feel natural and leverage the power of Go.&lt;/item&gt;
      &lt;item&gt;Rich Tool Ecosystem: Utilize pre-built tools, custom functions, or integrate existing tools to give agents diverse capabilities.&lt;/item&gt;
      &lt;item&gt;Code-First Development: Define agent logic, tools, and orchestration directly in Go for ultimate flexibility, testability, and versioning.&lt;/item&gt;
      &lt;item&gt;Modular Multi-Agent Systems: Design scalable applications by composing multiple specialized agents.&lt;/item&gt;
      &lt;item&gt;Deploy Anywhere: Easily containerize and deploy agents, with strong support for cloud-native environments like Google Cloud Run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To add ADK Go to your project, run:&lt;/p&gt;
    &lt;code&gt;go get google.golang.org/adk&lt;/code&gt;
    &lt;p&gt;This project is licensed under the Apache 2.0 License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;The exception is internal/httprr - see its LICENSE file.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45891968</guid><pubDate>Tue, 11 Nov 2025 19:52:55 +0000</pubDate></item><item><title>The terminal of the future</title><link>https://jyn.dev/the-terminal-of-the-future</link><description>&lt;doc fingerprint="37eac95c22846b89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;the terminal of the future&lt;/head&gt;
    &lt;p&gt;This post is part 6 of a multi-part series called “the computer of the next 200 years”.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Terminal internals are a mess. A lot of it is just the way it is because someone made a decision in the 80s and now it’s impossible to change. —Julia Evans&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;This is what you have to do to redesign infrastructure. Rich [Hickey] didn't just pile some crap on top of Lisp [when building Clojure]. He took the entire Lisp and moved the whole design at once. —Gary Bernhardt&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;a mental model of a terminal&lt;/head&gt;
    &lt;p&gt;At a very very high level, a terminal has four parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The "terminal emulator", which is a program that renders a grid-like structure to your graphical display.&lt;/item&gt;
      &lt;item&gt;The "pseudo-terminal" (PTY), which is a connection between the terminal emulator and a "process group" which receives input. This is not a program. This is a piece of state in the kernel.&lt;/item&gt;
      &lt;item&gt;The "shell", which is a program that leads the "process group", reads and parses input, spawns processes, and generally acts as an event loop. Most environments use bash as the default shell.&lt;/item&gt;
      &lt;item&gt;The programs spawned by your shell, which interact with all of the above in order to receive input and send output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I lied a little bit above. "input" is not just text. It also includes signals that can be sent to the running process. Converting keystrokes to signals is the job of the PTY.&lt;/p&gt;
    &lt;p&gt;Similar, "output" is not just text. It's a stream of ANSI Escape Sequences that can be used by the terminal emulator to display rich formatting.&lt;/p&gt;
    &lt;head rend="h2"&gt;what does a better terminal look like?&lt;/head&gt;
    &lt;p&gt;I do some weird things with terminals. However, the amount of hacks I can get up to are pretty limited, because terminals are pretty limited. I won't go into all the ways they're limited, because it's been rehashed many times before. What I want to do instead is imagine what a better terminal can look like.&lt;/p&gt;
    &lt;head rend="h3"&gt;a first try: Jupyter&lt;/head&gt;
    &lt;p&gt;The closest thing to a terminal analog that most people are familiar with is Jupyter Notebook. This offers a lot of cool features that are not possible in a "traditional" VT100 emulator:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;high fidelity image rendering&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a "rerun from start" button (or rerun the current command; or rerun only a single past command) that replaces past output instead of appending to it&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"views" of source code and output that can be rewritten in place (e.g. markdown can be viewed either as source or as rendered HTML)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a built-in editor with syntax highlighting, tabs, panes, mouse support, etc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;some problems&lt;/head&gt;
    &lt;p&gt;Jupyter works by having a "kernel" (in this case, a python interpreter) and a "renderer" (in this case, a web application displayed by the browser). You could imagine using a Jupyter Notebook with a shell as the kernel, so that you get all the nice features of Jupyter when running shell commands. However, that quickly runs into some issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your shell gets the commands all at once, not character-by-character, so tab-complete, syntax highlighting, and autosuggestions don't work.&lt;/item&gt;
      &lt;item&gt;What do you do about long-lived processes? By default, Jupyter runs a cell until completion; you can cancel it, but you can't suspend, resume, interact with, nor view a process while it's running. Don't even think about running &lt;code&gt;vi&lt;/code&gt;or&lt;code&gt;top&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The "rerun cell" buttons do horrible things to the state of your computer (normal Jupyter kernels have this problem too, but "rerun all" works better when the commands don't usually include &lt;code&gt;rm -rf&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Undo/redo do not work. (They don't work in a normal terminal either, but people attempt to use them more when it looks like they should be able to.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It turns out all these problems are solveable.&lt;/p&gt;
    &lt;head rend="h2"&gt;how does that work?&lt;/head&gt;
    &lt;head rend="h3"&gt;shell integration&lt;/head&gt;
    &lt;p&gt;There exists today a terminal called Warp. Warp has built native integration between the terminal and the shell, where the terminal understands where each command starts and stops, what it outputs, and what is your own input. As a result, it can render things very prettily:&lt;/p&gt;
    &lt;p&gt;It does this using (mostly) standard features built-in to the terminal and shell (a custom DCS): you can read their explanation here. It's possible to do this less invasively using OSC 133 escape codes; I'm not sure why Warp didn't do this, but that's ok.&lt;/p&gt;
    &lt;p&gt;iTerm2 does a similar thing, and this allows it to enable really quite a lot of features: navigating between commands with a single hotkey; notifying you when a command finishes running, showing the current command as an "overlay" if the output goes off the screen.&lt;/p&gt;
    &lt;head rend="h3"&gt;long-lived processes&lt;/head&gt;
    &lt;p&gt;This is really three different things. The first is interacting with a long-lived process. The second is suspending the process without killing it. The third is disconnecting from the process, in such a way that the process state is not disturbed and is still available if you want to reconnect.&lt;/p&gt;
    &lt;head rend="h4"&gt;interacting&lt;/head&gt;
    &lt;p&gt;To interact with a process, you need bidirectional communication, i.e. you need a "cell output" that is also an input. An example would be any TUI, like &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;gdb&lt;/code&gt;, or &lt;code&gt;vim&lt;/code&gt; 1.  Fortunately, Jupyter is really good at this!  The whole design is around having interactive outputs that you can change and update.&lt;/p&gt;
    &lt;p&gt;Additionally, I would expect my terminal to always have a "free input cell", as Matklad describes in A Better Shell, where the interactive process runs in the top half of the window and an input cell is available in the bottom half. Jupyter can do this today, but "add a cell" is manual, not automatic.&lt;/p&gt;
    &lt;head rend="h4"&gt;suspending&lt;/head&gt;
    &lt;p&gt;"Suspending" a process is usually called "job control". There's not too much to talk about here, except that I would expect a "modern" terminal to show me all suspended and background processes as a de-emphasized persistent visual, kinda like how Intellij will show you "indexing ..." in the bottom taskbar.&lt;/p&gt;
    &lt;head rend="h4"&gt;disconnecting&lt;/head&gt;
    &lt;p&gt;There are roughly three existing approaches for disconnecting and reconnecting to a terminal session (Well, four if you count reptyr).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Tmux / Zellij / Screen&lt;/p&gt;
        &lt;p&gt;These tools inject a whole extra terminal emulator between your terminal emulator and the program. They work by having a "server" which actually owns the PTY and renders the output, and a "client" that displays the output to your "real" terminal emulator. This model lets you detach clients, reattach them later, or even attach multiple clients at once. You can think of this as a "batteries-included" approach. It also has the benefit that you can program both the client and the server (although many modern terminals, like Kitty and Wezterm are programmable now); that you can organize your tabs and windows in the terminal (although many modern desktop environments have tiling and thorough keyboard shortcuts); and that you get street cred for looking like Hackerman.&lt;/p&gt;
        &lt;p&gt;The downside is that, well, now you have an extra terminal emulator running in your terminal, with all the bugs that implies.&lt;/p&gt;
        &lt;p&gt;iTerm actually avoids this by bypassing the tmux client altogether and acting as its own client that talks directly to the server. In this mode, "tmux tabs" are actually iTerm tabs, "tmux panes" are iTerm panes, and so on. This is a good model, and I would adopt it when writing a future terminal for integration with existing tmux setups.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mosh is a really interesting place in the design space. It is not a terminal emulator replacement; instead it is an ssh replacement. Its big draw is that it supports reconnecting to your terminal session after a network interruption. It does that by running a state machine on the server and replaying an incremental diff of the viewport to the client. This is a similar model to tmux, except that it doesn't support the "multiplexing" part (it expects your terminal emulator to handle that), nor scrollback (ditto). Because it has its own renderer, it has a similar class of bugs to tmux. One feature it does have, unlike tmux, is that the "client" is really running on your side of the network, so local line editing is instant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;alden/shpool/dtach/abduco/diss&lt;/p&gt;
        &lt;p&gt;These all occupy a similar place in the design space: they only handle session detach/resume with a client/server, not networking or scrollback, and do not include their own terminal emulator. Compared to tmux and mosh, they are highly decoupled.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;rerun and undo/redo&lt;/head&gt;
    &lt;p&gt;I'm going to treat these together because the solution is the same: dataflow tracking.&lt;/p&gt;
    &lt;p&gt;Take as an example pluto.jl, which does this today by hooking into the Julia compiler.&lt;/p&gt;
    &lt;p&gt;Note that this updates cells live in response to previous cells that they depend on. Not pictured is that it doesn't update cells if their dependencies haven't changed. You can think of this as a spreadsheet-like Jupyter, where code is only rerun when necessary.&lt;/p&gt;
    &lt;p&gt;You may say this is hard to generalize. The trick here is orthogonal persistence. If you sandbox the processes, track all IO, and prevent things that are "too weird" unless they're talking to other processes in the sandbox (e.g. unix sockets and POST requests), you have really quite a lot of control over the process! This lets you treat it as a pure function of its inputs, where its inputs are "the whole file system, all environment variables, and all process attributes".&lt;/p&gt;
    &lt;head rend="h3"&gt;derived features&lt;/head&gt;
    &lt;p&gt;Once you have these primitives—Jupyter notebook frontends, undo/redo, automatic rerun, persistence, and shell integration—you can build really quite a lot on top. And you can build it incrementally, piece-by-piece:&lt;/p&gt;
    &lt;head rend="h4"&gt;needs a Jupyter notebook frontend&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Runbooks (actually, you can build these just with Jupyter and a PTY primitive).&lt;/item&gt;
      &lt;item&gt;Terminal customization that uses normal CSS, no weird custom languages or ANSI color codes.&lt;/item&gt;
      &lt;item&gt;Search for commands by output/timestamp. Currently, you can search across output in the current session, or you can search across all command input history, but you don't have any kind of smart filters, and the output doesn't persist across sessions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;needs shell integration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Timestamps and execution duration for each command.&lt;/item&gt;
      &lt;item&gt;Local line-editing, even across a network boundary.&lt;/item&gt;
      &lt;item&gt;IntelliSense for shell commands, without having to hit tab and with rendering that's integrated into the terminal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;needs sandboxed tracing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"All the features from sandboxed tracing": collaborative terminals, querying files modified by a command, "asciinema but you can edit it at runtime", tracing build systems.&lt;/item&gt;
      &lt;item&gt;Extend the smart search above to also search by disk state at the time the command was run.&lt;/item&gt;
      &lt;item&gt;Extending undo/redo to a git-like branching model (something like this is already support by emacs undo-tree), where you have multiple "views" of the process tree.&lt;/item&gt;
      &lt;item&gt;Given the undo-tree model, and since we have sandboxing, we can give an LLM access to your project, and run many of them in parallel at the same time without overwriting each others state, and in such a way that you can see what they're doing, edit it, and save it into a runbook for later use.&lt;/item&gt;
      &lt;item&gt;A terminal in a prod environment that can't affect the state of the machine, only inspect the existing state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;ok but how do you build this&lt;/head&gt;
    &lt;p&gt;jyn, you may say, you can't build vertical integration in open source. you can't make money off open source projects. the switching costs are too high.&lt;/p&gt;
    &lt;p&gt;All these things are true. To talk about how this is possible, we have to talk about incremental adoption.&lt;/p&gt;
    &lt;p&gt;if I were building this, I would do it in stages, such that at each stage the thing is an improvement over its alternatives. This is how &lt;code&gt;jj&lt;/code&gt; works and it works extremely well: it doesn't require everyone on a team to switch at once because individual people can use &lt;code&gt;jj&lt;/code&gt;, even for single commands, without a large impact on everyone else.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 1: transactional semantics&lt;/head&gt;
    &lt;p&gt;When people think of redesigning the terminal, they always think of redesigning the terminal emulator. This is exactly the wrong place to start. People are attached to their emulators. They configure them, they make them look nice, they use their keybindings. There is a high switching cost to switching emulators because everything affects everything else. It's not so terribly high, because it's still individual and not shared across a team, but still high.&lt;/p&gt;
    &lt;p&gt;What I would do instead is start at the CLI layer. CLI programs are great because they're easy to install and run and have very low switching costs: you can use them one-off without changing your whole workflow.&lt;/p&gt;
    &lt;p&gt;So, I would write a CLI that implements transactional semantics for the terminal. You can imagine an interface something like &lt;code&gt;transaction [start|rollback|commit]&lt;/code&gt;, where everything run after &lt;code&gt;start&lt;/code&gt; is undoable. There is a lot you can do with this alone, I think you could build a whole business off this.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 2: persistent sessions&lt;/head&gt;
    &lt;p&gt;Once I had transactional semantics, I would try to decouple persistence from tmux and mosh.&lt;/p&gt;
    &lt;p&gt;To get PTY persistence, you have to introduce a client/server model, because the kernel really really expects both sides of a PTY to always be connected. Using commands like alden, or a library like it (it's not that complicated), lets you do this simply, without affecting the terminal emulator nor the programs running inside the PTY session.&lt;/p&gt;
    &lt;p&gt;To get scrollback, the server could save input and output indefinitely and replay them when the client reconnects. This gets you "native" scrollback—the terminal emulator you're already using handles it exactly like any other output, because it looks exactly like any other output—while still being replayable and resumable from an arbitrary starting point. This requires some amount of parsing ANSI escape codes2, but it's doable with enough work.&lt;/p&gt;
    &lt;p&gt;To get network resumption like mosh, my custom server could use Eternal TCP (possibly built on top of QUIC for efficiency). Notably, the persistence for the PTY is separate from the persistence for the network connection. Eternal TCP here is strictly an optimization: you could build this on top of a bash script that runs &lt;code&gt;ssh host eternal-pty attach&lt;/code&gt; in a loop, it's just not as nice an experience because of network delay and packet loss. Again, composable parts allow for incremental adoption.&lt;/p&gt;
    &lt;p&gt;At this point, you're already able to connect multiple clients to a single terminal session, like tmux, but window management is still done by your terminal emulator, not by the client/server. If you wanted to have window management integrated, the terminal emulator could speak the tmux -CC protocol, like iTerm.&lt;/p&gt;
    &lt;p&gt;All parts of this stage can be done independently and in parallel from the transactional semantics, but I don't think you can build a business off them, it's not enough of an improvement over the existing tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 3: structured RPC&lt;/head&gt;
    &lt;p&gt;This bit depends on the client/server model. Once you have a server interposed between the terminal emulator and the client, you can start doing really funny things like tagging I/O with metadata. This lets all data be timestamped3 and lets you distinguish input from output. xterm.js works something like this. When combined with shell integration, this even lets you distinguish shell prompts from program output, at the data layer.&lt;/p&gt;
    &lt;p&gt;Now you can start doing really funny things, because you have a structured log of your terminal session. You can replay the log as a recording, like asciinema4; you can transform the shell prompt without rerunning all the commands; you can import it into a Jupyter Notebook or Atuin Desktop; you can save the commands and rerun them later as a script. Your terminal is data.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 4: jupyter-like frontend&lt;/head&gt;
    &lt;p&gt;This is the very first time that we touch the terminal emulator, and it's intentionally the last step because it has the highest switching costs. This makes use of all the nice features we've built to give you a nice UI. You don't need our &lt;code&gt;transaction&lt;/code&gt; CLI anymore unless you want nested transactions, because your whole terminal session starts in a transaction by default. You get all the features I mention above, because we've put all the pieces together.&lt;/p&gt;
    &lt;head rend="h2"&gt;jyn, what the fuck&lt;/head&gt;
    &lt;p&gt;This is bold and ambitious and I think building the whole thing would take about a decade. That's ok. I'm patient.&lt;/p&gt;
    &lt;p&gt;You can help me by spreading the word :) Perhaps this post will inspire someone to start building this themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;bibliography&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gary Bernhardt, “A Whole New World”&lt;/item&gt;
      &lt;item&gt;Alex Kladov, “A Better Shell”&lt;/item&gt;
      &lt;item&gt;jyn, “how i use my terminal”&lt;/item&gt;
      &lt;item&gt;jyn, “Complected and Orthogonal Persistence”&lt;/item&gt;
      &lt;item&gt;jyn, “you are in a box”&lt;/item&gt;
      &lt;item&gt;jyn, “there's two costs to making money off an open source project…”&lt;/item&gt;
      &lt;item&gt;Rebecca Turner, “Vertical Integration is the Only Thing That Matters”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “New zine: The Secret Rules of the Terminal”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “meet the terminal emulator”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “What happens when you press a key in your terminal?”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “What's involved in getting a "modern" terminal setup?”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Bash scripting quirks &amp;amp; safety tips”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Some terminal frustrations”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Reasons to use your shell's job control”&lt;/item&gt;
      &lt;item&gt;“signal(7) - Miscellaneous Information Manual”&lt;/item&gt;
      &lt;item&gt;Christian Petersen, “ANSI Escape Codes”&lt;/item&gt;
      &lt;item&gt;saoirse, “withoutboats/notty: A new kind of terminal”&lt;/item&gt;
      &lt;item&gt;Jupyter Team, “Project Jupyter Documentation”&lt;/item&gt;
      &lt;item&gt;“Warp: The Agentic Development Environment”&lt;/item&gt;
      &lt;item&gt;“Warp: How Warp Works”&lt;/item&gt;
      &lt;item&gt;“Warp: Completions”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: Proprietary Escape Codes”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: Shell Integration”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: tmux Integration”&lt;/item&gt;
      &lt;item&gt;Project Jupyter, “Jupyter Widgets”&lt;/item&gt;
      &lt;item&gt;Nelson Elhage, “nelhage/reptyr: Reparent a running program to a new terminal”&lt;/item&gt;
      &lt;item&gt;Kovid Goyal, “kitty”&lt;/item&gt;
      &lt;item&gt;Kovid Goyal, “kitty - Frequently Asked Questions”&lt;/item&gt;
      &lt;item&gt;Wez Furlong, “Wezterm”&lt;/item&gt;
      &lt;item&gt;Keith Winstein, “Mosh: the mobile shell”&lt;/item&gt;
      &lt;item&gt;Keith Winstein, “Display errors with certain characters&lt;/item&gt;
      &lt;item&gt;Matthew Skala, “alden: detachable terminal sessions without breaking scrollback”&lt;/item&gt;
      &lt;item&gt;Ethan Pailes, “shell-pool/shpool: Think tmux, then aim... lower”&lt;/item&gt;
      &lt;item&gt;Ned T. Crigler, “crigler/dtach: A simple program that emulates the detach feature of screen”&lt;/item&gt;
      &lt;item&gt;Marc André Tanner, “martanne/abduco: abduco provides session management”&lt;/item&gt;
      &lt;item&gt;yazgoo, “yazgoo/diss: dtach-like program / crate in rust”&lt;/item&gt;
      &lt;item&gt;Fons van der Plas, “Pluto.jl — interactive Julia programming environment”&lt;/item&gt;
      &lt;item&gt;Ellie Huxtable, “Atuin Desktop: Runbooks that Run”&lt;/item&gt;
      &lt;item&gt;Toby Cubitt, “undo-tree”&lt;/item&gt;
      &lt;item&gt;“SIGHUP - Wikipedia”&lt;/item&gt;
      &lt;item&gt;Jason Gauci, “How Eternal Terminal Works”&lt;/item&gt;
      &lt;item&gt;Marcin Kulik, “Record and share your terminal sessions, the simple way - asciinema.org”&lt;/item&gt;
      &lt;item&gt;“Alternate Screen | Ratatui”&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892191</guid><pubDate>Tue, 11 Nov 2025 20:11:33 +0000</pubDate></item><item><title>Collaboration sucks</title><link>https://newsletter.posthog.com/p/collaboration-sucks</link><description>&lt;doc fingerprint="a369965128e686dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Collaboration sucks&lt;/head&gt;
    &lt;head rend="h3"&gt;Be the driver&lt;/head&gt;
    &lt;p&gt;“If you want to go fast, go alone; if you want to go far, go together”&lt;/p&gt;
    &lt;p&gt;This phrase will slowly kill your company and I’m here to prove it.&lt;/p&gt;
    &lt;p&gt;Imagine you are driving a car. It’s often useful to have someone give you directions, point out gas stations, and recommend stops for snacks. This is a helpful amount of collaboration.&lt;/p&gt;
    &lt;p&gt;An unhelpful amount of collaboration is getting out of your car to ask pedestrians if they like your car, swapping drivers every 10 minutes, or having someone constantly commenting on your driving.&lt;/p&gt;
    &lt;p&gt;In the first scenario, you get the right amount of feedback to get to your destination as fast as possible. In the second, you get more feedback, but it slows you down. You run the risk of not making it to the place you want to go.&lt;/p&gt;
    &lt;p&gt;The second scenario is also the one most startups (or companies, really) end up in because of ✨ collaboration ✨.&lt;/p&gt;
    &lt;head rend="h2"&gt;Being good at feedback means knowing when not to give it&lt;/head&gt;
    &lt;p&gt;As PostHog grows, I’ve seen more and more collaboration that doesn’t add value or adds far too little value for the time lost collaborating. So much so we made “collaboration sucks” the topic of the week during a recent company all hands.&lt;/p&gt;
    &lt;p&gt;“You’re the driver” is a key value for us at PostHog. We aim to hire people who are great at their jobs and get out of their way. No deadlines, minimal coordination, and no managers telling you what to do.&lt;/p&gt;
    &lt;p&gt;In return, we ask for extraordinarily high ownership and the ability to get a lot done by yourself. Marketers ship code, salespeople answer technical questions without backup, and product engineers work across the stack.&lt;/p&gt;
    &lt;p&gt;This means there is almost always someone better at what you are doing than you are. It is tempting to get them, or anybody really, involved and ✨ collaborate ✨, but collaboration forces the driver to slow down and explain stuff (background, context, their thinking).&lt;/p&gt;
    &lt;p&gt;This tendency reveals itself in a few key phrases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;“Curious what X thinks”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Would love to hear Y’s take on this”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“We should work with Z on this”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This sometimes leads to valuable insights, but always slows the driver down. It erodes their motivation, confidence, and effectiveness, and ultimately leads us to ship less.&lt;/p&gt;
    &lt;head rend="h2"&gt;If collaboration sucks, why do people do it?&lt;/head&gt;
    &lt;p&gt;Everyone is to blame.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;People want to be helpful. For example, when someone posts their work-in-progress in Slack, others feel obliged to give feedback because we have a culture of feedback.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On the flip side, people don’t ask for feedback from specific people because it doesn’t feel inclusive, even though it would help.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;People aren’t specific enough about what feedback they need. This creates more space for collaboration to sneak in. A discussion about building a specific feature can devolve into reevaluating the entire product roadmap if you let it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When someone has a good idea, the response often defaults to “let’s discuss” rather than “ok, do it.” As proof, we have 175 mentions of “let’s discuss” in Slack.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;People just want to talk about stuff because they&lt;/p&gt;&lt;del rend="overstrike"&gt;are too busy&lt;/del&gt;can’t be bothered to act on it. We drift from our ideal of a pull request to an issue/RFC to Slack (we are mostly here) to “let’s discuss”.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It’s not clear who the owner is (or no one wants to own what’s being discussed).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is annoying, but sometimes a single person can’t ship certain things front to back to a high-enough quality and we can’t just ship and iterate. We can fix broken code, but we can’t resend a newsletter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to crush collaboration (and go farther, faster)&lt;/head&gt;
    &lt;p&gt;So if collaboration is your enemy, how do you defeat it? Here’s what we say:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Default to shipping. Pull requests &amp;gt; issues &amp;gt; Slack messages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every time you see ✨ collaboration ✨ happening, speak up and destroy it. Say “there are too many people involved. X, you are the driver, you decide.” (This is a great way to make friends btw).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Tag who you specifically want input from and what you want from them, not just throw things out there into the void.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prefer to give feedback after something has shipped (but before the next iteration) rather than reviewing it before it ships. Front-loading your feedback can turn it into a quasi-approval process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you are a team lead, or leader of leads, who has been asked for feedback, consider being more you can just do stuff.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When it’s your thing, you are the “informed captain.” Listen to feedback, but know it’s ultimately up to you to decide what to do, not the people giving feedback.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately for me, not all collaboration can be rooted out, and even I will admit that some collaboration is useful. Ian and Andy edited this newsletter after all.&lt;/p&gt;
    &lt;p&gt;The point is, if you aren’t actively attempting to collaborate less, you are probably collaborating too much by default and hurting your ability to go far, fast.&lt;lb/&gt;Words by Charles Cook, who also hates sparkling water, presumably because the bubbles are too collaborative.&lt;/p&gt;
    &lt;head rend="h2"&gt;👷 Jobs at PostHog&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;AI Product Engineer working on PostHog AI, LLM Analytics or Array teams.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Backend Engineer for Feature Flags and Ingestion teams&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Influencer Wrangler on the Marketing team&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;YC Technical Onboarding Specialist on the Onboarding team (San Fran based)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ClickHouse Operations Engineer on the ClickHouse team&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;📖 More good reads&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Workflows are now in Alpha and I already broke mine – Sara Miteva&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your data model is your destiny – Matt Brown&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Spinning Plates – Dylan Martin&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;1000x: The Power of an Interface for Performance (video) – Joran Dirk Greef&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892394</guid><pubDate>Tue, 11 Nov 2025 20:27:38 +0000</pubDate></item><item><title>Meticulous (YC S21) is hiring to redefine software dev</title><link>https://jobs.ashbyhq.com/meticulous/3197ae3d-bb26-4750-9ed7-b830f640515e</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45892773</guid><pubDate>Tue, 11 Nov 2025 21:00:35 +0000</pubDate></item><item><title>X5.1 solar flare, G4 geomagnetic storm watch</title><link>https://www.spaceweatherlive.com/en/news/view/593/20251111-x5-1-solar-flare-g4-geomagnetic-storm-watch.html</link><description>&lt;doc fingerprint="77527d2b7f75e40e"&gt;
  &lt;main&gt;
    &lt;p&gt;Tuesday, 11 November 2025 19:07 UTC&lt;/p&gt;
    &lt;p&gt;Here she blows! Sunspot region 4274 produced its strongest solar flare thus far since it appeared on the east limb and the sixth strongest solar flare of the current solar cycle. An impressive long duration and highly eruptive X5.1 (R3-strong) solar flare peaked this morning at 10:04 UTC.&lt;/p&gt;
    &lt;p&gt;It became quickly clear that the eruption would be followed by an impressive coronal mass ejection (CME). The resulting coronal wave following the solar explosion as well as the coronal dimming observed as the CME was propelled into space were of a spectacular magnitude as can be seen in the animation below provided by halocme.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Another eruption from AR12474, associated with an X5.1 flare. It has become a full halo CME. I am truly impressed by how fast and global this coronal wave is. The CME will arrive on November 13, but because of earlier CMEs it will be challenging to isolate the ICME from this. pic.twitter.com/H6eNjzQUGz&lt;/p&gt;— Halo CME (@halocme) November 11, 2025&lt;/quote&gt;
    &lt;p&gt;Taking a look at coronagraph imagery provided by GOES-19 CCOR-1 we see the gorgeous fast halo coronal mass ejection as it propagates away from the Sun. It doesn't take a rocket scientist to come to the conclusion that this plasma cloud of course has an earth-directed component and it is pretty clear that this will be a strong impact when it arrives at our planet. This rightfully so prompted the NOAA SWPC to issue a G4 or greater geomagnetic storm watch for tomorrow as the cloud could impact our planet as early as 16 UTC on 12 November. Not only is the CME fast but it will also travel trough an area with high ambient solar wind speed and low density thanks to two other CMEs released earlier by this region. More about that below.&lt;/p&gt;
    &lt;p&gt;If the solar wind and interplanetary magnetic field values at Earth are favorable this could result in a geomagnetic storm which is strong enough for aurora to become visible from locations as far south as northern France, Germany, Ukraine, Switzerland and Austria. In the US it could become visible as far south as Nevada and Arkansas. No guarantees of course, this is space weather we are talking about but be sure to download the SpaceWeatherLive app to your mobile device, turn on the alerts and keep an eye on the solar wind data from ACE and DSCOVR!&lt;/p&gt;
    &lt;p&gt;We also want to remind you that we still have two coronal mass ejections on their way to Earth. These are not as impressive as this X5.1 CME but these two plasma clouds will likely arrive within the next 6 to 18 hours. This is a tricky one as they could arrive as one impact or two impacts close intill each other. More information in yesterday's news.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this article! Did you have any trouble with the technical terms used in this article? Our help section is the place to be where you can find in-depth articles, a FAQ and a list with common abbreviations. Still puzzled? Just post on our forum where we will help you the best we can!&lt;/p&gt;
    &lt;p&gt;A lot of people come to SpaceWeatherLive to follow the Solar activity or if there is a chance to see the aurora, but with more traffic comes higher costs to keep the servers online. If you like SpaceWeatherLive and want to support the project you can choose a subscription for an ad-free site or consider a donation. With your help we can keep SpaceWeatherLive online!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Last X-flare&lt;/cell&gt;
        &lt;cell&gt;2025/11/11&lt;/cell&gt;
        &lt;cell&gt;X5.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Last M-flare&lt;/cell&gt;
        &lt;cell&gt;2025/11/11&lt;/cell&gt;
        &lt;cell&gt;M1.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last geomagnetic storm&lt;/cell&gt;
        &lt;cell&gt;2025/11/08&lt;/cell&gt;
        &lt;cell&gt;Kp6+ (G2)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Spotless days&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last spotless day&lt;/cell&gt;
        &lt;cell&gt;2022/06/08&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Monthly mean Sunspot Number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;October 2025&lt;/cell&gt;
        &lt;cell&gt;114.6 -15.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;November 2025&lt;/cell&gt;
        &lt;cell&gt;95.5 -19.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last 30 days&lt;/cell&gt;
        &lt;cell&gt;97.3 -33.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893004</guid><pubDate>Tue, 11 Nov 2025 21:18:26 +0000</pubDate></item><item><title>I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours</title><link>https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/</link><description>&lt;doc fingerprint="b6398d534b2d77a1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours&lt;/head&gt;
    &lt;p&gt;Yesterday after receiving my yearly flu vaccine at the pharmacy I was offered a blood pressure test, which reported a reading that made the young pharmacist who had just given me my vaccine a bit worried.&lt;/p&gt;
    &lt;p&gt;Off the back of this she offered me a 24 hour study, and then strapped a cuff to my arm plumbed into a little device which I had to wear in a little caddy - the cuff would inflate every 30 minutes during the day and every 60 minutes during the night, and then tomorrow I would bring it back for analysis.&lt;/p&gt;
    &lt;p&gt;"Can I read the measurements?" I asked, as it was being strapped to me.&lt;/p&gt;
    &lt;p&gt;"Oh, no, that will just stress you out. We turn that off". Fair enough.&lt;/p&gt;
    &lt;p&gt;Thing is, this device had a little micro-USB port on the side.&lt;/p&gt;
    &lt;head rend="h1"&gt;Doing things the proper way&lt;/head&gt;
    &lt;p&gt;I had started researching the device - a Microlife WatchBP O3 - before I got out of the chemist, and once I'd got back to the office I downloaded the software that's freely available to interact with it, setting up a Bottles instance to run the software since I don't (knowingly) have a Windows machine within 100 metres of me.&lt;/p&gt;
    &lt;p&gt;Unfortunately it didn't seem to be able to access the device, and I had no clue why. In Linux it was just presenting as a standard &lt;code&gt;hidraw&lt;/code&gt; device:&lt;/p&gt;
    &lt;code&gt;[33301.736724] hid-generic 0003:04D9:B554.001E: hiddev96,hidraw1: USB HID v1.11 Device [USB HID UART Bridge] on usb-0000:c5:00.0-1/input0
&lt;/code&gt;
    &lt;p&gt;Fine, I'll install windows.&lt;/p&gt;
    &lt;p&gt;After dodging around Microsoft's idea of UX, and then forwarding the USB device to the VM (I used Gnome Boxes for this, works nicely), I finally got to see WatchBP Analyzer with the data downloaded from the device.&lt;/p&gt;
    &lt;p&gt;But I don't want to open a Virtual Machine running Windows to see this data, and anyway - I'm pretty sure that reverse-engineering this will be good for my blood pressure.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sniffing the traffic&lt;/head&gt;
    &lt;p&gt;Since I'm running this in a Virtual Machine I can just rely on Wireshark in Linux to get the traffic between the host and the device. &lt;code&gt;usbmon&lt;/code&gt; is already installed and we know that the device is on Bus 3, so we can select usbmon3 on startup and start capturing.&lt;/p&gt;
    &lt;p&gt;I'm very much out of my depth at this point but, being one of those who could land a plane in an emergency (why would you talk yourself out of it?!) I decided to crack on regardless. I know that the interesting stuff is sent after I press "Download", and I know that something in there is gonna say "my blood pressure is 137/113" - so let's look for that. Just convert to show bytes as decimal and..&lt;/p&gt;
    &lt;p&gt;..that looks like a blood pressure! Let's copy that out as hex:&lt;/p&gt;
    &lt;code&gt;05 0a 89 71 43 9b
&lt;/code&gt;
    &lt;p&gt;I'm not sure if this is "valid" HID Data (Wireshark seems convinced that only the first byte is the Vendor Data, with the rest being padding) but it seems like the data is being sent in 32-byte "chunks", of which the first byte tells you the number of significant (SIG) following bits in the chunk (I deleted the rest - all zeroes - for clarity). The third byte is my Systolic blood Pressure (SYS), the fourth is my Diastolic blood pressure (DIA), and the fifth is my heart rate (HR) - no clue what the second or last byte is, but let's find all other bytes with my blood pressure in them (in decimal this time, because I can't read hex without help):&lt;/p&gt;
    &lt;code&gt;SIG ??? SYS DIA  HR ??? ??? ???
  5  10 137 113  67 155
  5   0 132  86  68 155
  6   0 126  84  82 155  83
  6  10 128  80  61 155  83
  7   0 148  93  65 155  83  64
  7   0 121  92  74 155  83  94
  7   0 123  83  65 155  83  95
  7   0 123  79  78 155  83 129
&lt;/code&gt;
    &lt;p&gt;Hmm. So we're still looking for the Oscillometric signal peak pressure (OPP)as well as some timestamps (we can calculate Mean arterial pressure - MAP - as &lt;code&gt;(2*DIA+SYS)/3&lt;/code&gt;, according to the manual, and Pulse Pressure (PP) is just &lt;code&gt;SYS-DIA&lt;/code&gt;). We can see the OPP in the packets that come after each of those above, but they don't seem to consistently come in on the same line:&lt;/p&gt;
    &lt;code&gt; 10  82  195   80 *121    0    0    0    0    0    0
 10  82  223   80  *95    0    0    0    0    0    0
  9   1   80  *90    0    0    0    0    0    0
  9  35   80  *86    0    0    0    0    0    0
  8  80 *103    0    0    0    0    0    0
  8  80 *106    0    0    0    0    0    0
  8  80  *90    0    0    0    0    0    0
 10  80  *88    0    0    0    0    0    0   29  251
&lt;/code&gt;
    &lt;p&gt;Oh. Maybe if I stick them together?&lt;/p&gt;
    &lt;code&gt;??? SYS DIA  HR ??? ??? ??? ??? OPP ??? ??? ??? ??? ??? ??? ??? ???
 10 137 113  67 155  82 195  80 121   0   0   0   0   0   0
  0 132  86  68 155  82 223  80  95   0   0   0   0   0   0
  0 126  84  82 155  83   1  80  90   0   0   0   0   0   0
 10 128  80  61 155  83  35  80  86   0   0   0   0   0   0
  0 148  93  65 155  83  64  80 103   0   0   0   0   0   0
  0 121  92  74 155  83  94  80 106   0   0   0   0   0   0
  0 123  83  65 155  83  95  80  90   0   0   0   0   0   0
  0 123  79  78 155  83 129  80  88   0   0   0   0   0   0  29 251
&lt;/code&gt;
    &lt;p&gt;Right, timestamps. I first guessed that the four populated contiguous bytes between &lt;code&gt;HR&lt;/code&gt; and &lt;code&gt;OPP&lt;/code&gt; are a 32-bit unix timestamp, but that would make the first one &lt;code&gt;9B52C350&lt;/code&gt;; either &lt;code&gt;Jul 29 2052&lt;/code&gt; or &lt;code&gt;Dec 08 2012&lt;/code&gt; depending on which endianness the protocol is into. The 8 readings we have here are all from &lt;code&gt;November 10th&lt;/code&gt;, at &lt;code&gt;11:03&lt;/code&gt;, &lt;code&gt;11:31&lt;/code&gt;, &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt;, &lt;code&gt;13:31&lt;/code&gt; and &lt;code&gt;14:01&lt;/code&gt;, which isn't.. isn't that.&lt;/p&gt;
    &lt;p&gt;But note that the number in the 6th column flips from &lt;code&gt;82&lt;/code&gt; to &lt;code&gt;83&lt;/code&gt; when we switch from AM to PM - that's something, and when it does the 7th column resets. And hey - &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;35&lt;/code&gt;, &lt;code&gt;64&lt;/code&gt;, &lt;code&gt;94&lt;/code&gt;, &lt;code&gt;95&lt;/code&gt;.. that seems dangerously close to &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt; and &lt;code&gt;13:31&lt;/code&gt; if you were just to count the minutes. What's going on?&lt;/p&gt;
    &lt;head rend="h1"&gt;Deadlines and dead ends&lt;/head&gt;
    &lt;p&gt;I tried feeding a lot of this into various Als (Kagi gives you access to a few with a nice interface) and I found that they mostly were stupid in ways that made me think. A few times I thought they had "cracked the case" but actually they just made me waste time. But they did remind me e.g. of endianness, so I did get a bit out of them.&lt;/p&gt;
    &lt;p&gt;I also spent quite a bit of time trying to write some Python that emulated the initial handshake and download button of the interface so that it could push out the data as a stream instead of me having to wrestle it out of Wireshark - again, Al had a habit of giving me incorrect code (although it did turn me on to pyhidapi).&lt;/p&gt;
    &lt;p&gt;But ultimately I had a deadline, and I had to return the device even though I wanted to spend more time with it. Possibly for the best - while it did give me some reverse engineering practice (which it turns out I really enjoy), I should do some work instead of procrastinating.&lt;/p&gt;
    &lt;p&gt;My final lesson was a new word - Normotension, normal blood pressure - and a new phrase - White Coat Hypertension, the phenomena of high blood pressure in a clinical setting. Turns out that when you check someone's blood pressure after giving them an injection, it's higher than normal.&lt;/p&gt;
    &lt;p&gt;I don't think I'd recommend getting your blood pressure tested after your next flu jab. But then, I'm not a doctor.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893095</guid><pubDate>Tue, 11 Nov 2025 21:25:19 +0000</pubDate></item><item><title>Heroku Support for .NET 10</title><link>https://www.heroku.com/blog/support-for-dotnet-10-lts-what-developers-need-know/</link><description>&lt;doc fingerprint="df6f5a5e0ea4b3da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Heroku Support for .NET 10 LTS: What Developers Need to Know&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Last Updated: November 11, 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s that time of year for .NET when we get a new major version and a bunch of exciting features. .NET Conf 2025 kicked off earlier today, bringing with it the release of .NET 10, as well as ASP.NET Core 10, C# 14, and F# 10. Congrats (and a big thank you) to the .NET team and everyone who helped get .NET 10 out the door.&lt;/p&gt;
    &lt;p&gt;At Heroku, we believe you should be able to use language and framework releases when they launch, and we prepare accordingly. You can now build and run .NET 10 apps on Heroku, with buildpack support for new SDK features like file-based apps, &lt;code&gt;.slnx&lt;/code&gt; solution files, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration and support timelines&lt;/head&gt;
    &lt;p&gt;This year’s release is significant because .NET 10 is the new Long Term Support (LTS) release, which will be supported for three years. This extended support, including regular updates and security patches, makes it the best release for businesses and developers to build on and migrate to, offering a stable foundation with access to the latest features.&lt;/p&gt;
    &lt;p&gt;With .NET 10 now available, the clock is ticking on previous versions. Both .NET 8 and .NET 9 will reach End of Support on November 10, 2026. In other words, now is a good time to start planning your migration.&lt;/p&gt;
    &lt;p&gt;We will continue to support .NET 8 and .NET 9 with consistent, timely updates alongside .NET 10. Our .NET support follows the official .NET support policy, and we are fully committed to providing a stable and secure platform for your .NET applications.&lt;/p&gt;
    &lt;p&gt;Let’s dive into using .NET 10 on Heroku today!&lt;/p&gt;
    &lt;head rend="h2"&gt;Zero-config deployment with .NET 10 file-based apps&lt;/head&gt;
    &lt;p&gt;One of the most exciting features in .NET 10 is file-based apps – .NET applications defined in a single C# file without project or solution files, making it easier than ever to deploy .NET apps to Heroku.&lt;/p&gt;
    &lt;p&gt;For example, here’s a complete ASP.NET Core 10 web application, &lt;code&gt;HelloHeroku.cs&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// Use the new #sdk directive to pull in the ASP.NET Core SDK
#:sdk Microsoft.NET.Sdk.Web

var builder = WebApplication.CreateBuilder(args);
var app = builder.Build();  
  
app.MapGet("/", () =&amp;gt; "Hello from .NET 10 on Heroku!");

app.Run();
&lt;/code&gt;
    &lt;p&gt;When you push this to Heroku, the platform detects the &lt;code&gt;*.cs&lt;/code&gt; file and uses the .NET buildpack. Since there are no solution or project files, the buildpack treats it as a file-based app, installs the latest .NET SDK, builds and publishes the app, detects and configures it as a web application, and deploys it to serve traffic.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; The result is a simple, zero-config experience to get you started quickly, ideal for prototyping and developers new to .NET. And there’s more coming – check out the .NET SDK repository to see what the .NET team is working on!&lt;/p&gt;
    &lt;p&gt;To learn more about what you can do with file-based apps on Heroku today, see our Dev Center documentation.&lt;/p&gt;
    &lt;head rend="h2"&gt;SLNX: A modern solution for a modern .NET&lt;/head&gt;
    &lt;p&gt;For decades, .NET developers have used &lt;code&gt;.sln&lt;/code&gt; solution files, a proprietary format introduced in 2002 for Visual Studio. Unlike .NET itself, they haven’t changed much since. In a step towards modernization, the .NET 10 SDK is making SLNX the default format. Heroku ensures a seamless deployment experience by fully supporting both formats.&lt;/p&gt;
    &lt;code&gt;&amp;lt;solution&amp;gt;
    &amp;lt;project path="MyApp\MyApp.csproj"&amp;gt;&amp;lt;/project&amp;gt;
&amp;lt;/solution&amp;gt;
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;*.slnx&lt;/code&gt; files are easier to read and edit, less likely to cause merge conflicts, and support a wider range of workflows and environments, from Linux shells to Visual Studio on Windows. To migrate existing &lt;code&gt;.sln&lt;/code&gt; files, run &lt;code&gt;dotnet solution migrate&lt;/code&gt; or see the .NET blog announcement for more details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heroku CI and the Microsoft Testing Platform&lt;/head&gt;
    &lt;p&gt;The .NET 10 SDK integrates the Microsoft Testing Platform (MTP) directly in the &lt;code&gt;dotnet test&lt;/code&gt; command. Since Heroku CI runs &lt;code&gt;dotnet test&lt;/code&gt; by default, your test suite works out of the box after you migrate your apps.&lt;/p&gt;
    &lt;p&gt;For more control over the test setup and execution, you can specify custom test commands in your &lt;code&gt;app.json&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to migrate? We’re here to help&lt;/head&gt;
    &lt;p&gt;To support your .NET 10 migration, we’ve updated all our documentation and resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The .NET Getting Started app now runs on .NET 10.&lt;/item&gt;
      &lt;item&gt;The ASP.NET Core configuration article includes new guidance for ASP.NET Core 10, including migration away from the now-obsolete &lt;code&gt;IPNetwork&lt;/code&gt;and&lt;code&gt;ForwardedHeadersOptions.KnownNetworks&lt;/code&gt;APIs (learn more) often used to integrate with Heroku’s router.&lt;/item&gt;
      &lt;item&gt;Apps currently using .NET 10 RC builds on Heroku (with &lt;code&gt;TargetFramework&lt;/code&gt;set to&lt;code&gt;net10.0&lt;/code&gt;) will automatically be built with the stable .NET 10 release on the next&lt;code&gt;git push&lt;/code&gt;. You can pin to specific SDK versions using a&lt;code&gt;global.json&lt;/code&gt;file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For teams migrating from earlier versions, the .NET 10 breaking changes documentation covers important upgrade considerations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started today&lt;/head&gt;
    &lt;p&gt;We can’t wait to see what you build with .NET 10 on Heroku. From new features like file-based apps to the stability of an LTS release, this is a great time to be a .NET developer.&lt;/p&gt;
    &lt;p&gt;Check out our updated Getting Started with .NET on Heroku guide and please feel free to reach out with any questions or feedback.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Originally Published:&lt;/item&gt;
      &lt;item&gt;.NETBuildpacksLanguagesNextgenProduct Features&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893646</guid><pubDate>Tue, 11 Nov 2025 22:18:17 +0000</pubDate></item><item><title>Four strange places to see London's Roman Wall</title><link>https://diamondgeezer.blogspot.com/2025/11/odd-places-to-see-londons-roman-wall.html</link><description>&lt;doc fingerprint="af4d01017b3c27b4"&gt;
  &lt;main&gt;
    &lt;p&gt;There are manyplaces around the City of London to see its old Roman Wall, notably alongside Noble Street, in Barber Surgeons' Meadow, through the Barbican, in St Alphage Garden and just outside the entrance to Tower Hill station. Here are four of the odder spots.&lt;/p&gt;
    &lt;p&gt;Four strange places to see London's Roman Wall&lt;/p&gt;
    &lt;p&gt;1) From platform 1 at Tower Hill station&lt;/p&gt;
    &lt;p&gt;If you're ever waiting for a westbound train at Tower Hill station, take a walk to the rear of the platform and take a look across the tracks, roughly where the penultimate carriage would stop. High on the far wall is a square recess lined by black tiles, and at the back of that is a dimly-lit surface of chunky irregular blocks. Unlike every single other thing on the Underground, the Romans built that.&lt;/p&gt;
    &lt;p&gt;London's original wall was 2 miles long, 6 metres high and almost 3 metres thick at its base, all the better to keep out uncivilised marauders. It was built around 200 AD, then left to decay and rebuilt in the medieval era, again for defensive purposes. This is one of the original bits, not that you can easily tell by squinting across the tracks. A small metal lamp points inwards but is no longer switched on because heritage illumination is not a TfL priority. There is however a rather nice silver plaque on the pillar opposite, should you step back far enough to notice it.&lt;/p&gt;
    &lt;p&gt;The plaque confirms that the stones here are a continuation of the wall seen (much more clearly) outside. What it doesn't mention is the unavoidable truth that the wall must once have continued across the tracks and platforms but is no longer here. That's because when the Circle line was constructed in 1882 the railway companies had permission to demolish 22 metres of London's wall and duly did, the Victorians never being afraid to destroy ancient heritage. Ian Visits has a photo of navvies standing atop the offending stonework just before they bashed it through. The square hole is no recompense, plus you can't see anything if a train's in the platform, but it is a brilliantly quirky thing to find on the Underground.&lt;/p&gt;
    &lt;p&gt;2) Round the back of the Leonardo Royal Hotel&lt;/p&gt;
    &lt;p&gt;The short walk from Tower Hill station to the rear entrance of Fenchurch Street passes two hotels. The second is the Leonardo Royal, formerly the Grange, whose car port looks like it leads to a cocktail terrace and maybe some parking. Nothing's signed from the street, indeed I'd never thought to duck through before, but at the far end past the umbrellas of Leo's bar is a significant chunk of Roman wall.&lt;/p&gt;
    &lt;p&gt;The upper section has arched windows built for archers and square holes which once supported a timber platform. It's impressive of course merely medieval, part of the rebuild that occurred along much of the wall as the city grew and spread beyond its former border. To see the Roman section stand closer to the rail and look down, this because ground level then was a few metres lower than now. The telltale signs are several distinctive bands of thin red bricks, these added to strengthen and bond the structure, and which look like layers of jam in a particularly lumpy sponge. The entire segment behind the hotel is over 20m long, thus longer than the better-known chunk outside the station.&lt;/p&gt;
    &lt;p&gt;Perhaps the best thing about this bit of wall is that you can walk through it. A couple of steps have been added on each side allowing passage through a low medieval arch, all marked with anachronistic trip hazard markings. If steps aren't your thing you can also pass round the end of the wall on the flat. Round the other side are a glum alley and a staff back-entrance, also an exit into a separate backstreet past a sign that says PRIVATE No Public Right Of Way Beyond This Point Entry At Your Own Risk Absolutely No Liability Is Accepted For Any Reason Whatsoever. Stuff that, there's an actual Roman Wall back here.&lt;/p&gt;
    &lt;p&gt;3) From a cafe terrace&lt;/p&gt;
    &lt;p&gt;I've written about The City Wall at Vine Street before, a free attraction opened in 2023 beneath a block of student flats. Last time I had to battle the Procedural Curmudgeon to gain admittance but I'm pleased to say they've since loosened up and you can now simply gesture at the door, walk in and give your first name to a flunkey with a tablet. He rattled through the key information with all the practised enthusiasm of a call centre employee dictating terms and conditions, then sent me off down the stairs.&lt;/p&gt;
    &lt;p&gt;Two walls are filled with finds from the excavations, including an AD 70s coin and the bones of a 1760s cat. Nobody's quite sure how the ancient Greek tombstone ended up here, given it predates Londinium, but it has pride of place in a central glass case. The 5-minute historical animation is pretty good too, assuming you can read quite fast. But the main draw is the multi-layered towering remnant of wall which here has the benefit of being properly illuminated and protected from the elements. The protruding lower section (which looks much too clean to be so very old) is all that remains of an original postern, and is also unique because all the other towers elsewhere round the City are merely medieval.&lt;/p&gt;
    &lt;p&gt;What's weird is that this large basement space is overlooked by a balcony scattered with small tables at which sit students and businesspeople consuming coffee and all-day brunch. The baristas operate from the cafe upstairs but any food comes from a small kitchen down below, which has the unnerving side effect that while you're wandering around what looks like a museum it smells like an office canteen. If you choose to be tempted by a cappuccino and smashed avocado on your way out you can enjoy extra time with the Roman wall, or indeed skip the walkthrough altogether and focus only on refreshment with an absolutely unique view. I recommend a proper visit though... the visitors book awaits your praise.&lt;/p&gt;
    &lt;p&gt;4) At the rear of a car park&lt;/p&gt;
    &lt;p&gt;This is amazing on many levels, the main level being subterranean. After WW2 so much of the City was in ruins that planners drove a new dual carriageway through the Aldersgate area and called it London Wall. They believed cars were the future and to that end hid a linear car park directly underneath the new road. It's very narrow, very long and pretty grim, indeed precisely the kind of filming location you'd expect a throwback crime thriller to use for a shoot-out or kidnapping. Cars enter down a short spiral ramp and pedestrians through a grubby side door, and the numbered concrete catacombs stretch on and on for almost 400 metres. Keep walking past the white vans, Range Rovers and the attendant's cabin, trying not to attract too much attention, and almost at the far end is... blimey.&lt;/p&gt;
    &lt;p&gt;You can't park in bays 52 and 53 because they're full of Roman remains. A substantial chunk of wall slots in diagonally beneath the joists and pillars, tall enough to incorporate two separate bands of red bricks. It looks quite smooth up front but fairly rubbly round the back, also much thicker at the base than at the top. Obviously it's very risky to have a scheduled ancient monument in a car park so protective concrete blocks have been added to make sure nobody reverses into the stonework by mistake. More recently a glass screen has been added at one end, branded 'City of London' so you know who to thank, but the other end remains accessible for now (not that you should be stepping in or even touching it).&lt;/p&gt;
    &lt;p&gt;It's the contrasts that I found most incongruous. A relic from Roman times penned inbetween a speed hump and a futile pedestrian crossing. A fortification from the 3rd century beside an electric van built last year. A defensive structure that helped see off the Peasants Revolt beside a poster warning what to do in the event of fire. A boundary wall once an intrinsic part of the capital now underground illuminated by strip lights. And all this at the very far end of an oppressive bunker preserved for the benefit of hardly any eyes in a parking facility only a few know to use. Sure you can see chunks of Roman wall all around the City, even from a tube platform, hotel terrace or cafe. But the oddest spot may well be here in the London Wall car park, should you ever have the balls to take a look.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893795</guid><pubDate>Tue, 11 Nov 2025 22:31:41 +0000</pubDate></item><item><title>.NET MAUI is coming to Linux and the browser</title><link>https://avaloniaui.net/blog/net-maui-is-coming-to-linux-and-the-browser-powered-by-avalonia</link><description>&lt;doc fingerprint="f561401a5d0495ec"&gt;
  &lt;main&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45893986</guid><pubDate>Tue, 11 Nov 2025 22:50:32 +0000</pubDate></item><item><title>Why Nietzsche matters in the age of artificial intelligence</title><link>https://cacm.acm.org/blogcacm/why-nietzsche-matters-in-the-age-of-artificial-intelligence/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45894588</guid><pubDate>Tue, 11 Nov 2025 23:59:49 +0000</pubDate></item><item><title>Perkeep – Personal storage system for life</title><link>https://perkeep.org/</link><description>&lt;doc fingerprint="47dd1e4e29b72f56"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Perkeep lets you permanently keep your stuff, for life.&lt;/head&gt;
    &lt;p&gt;Perkeep (née Camlistore) is a set of open source formats, protocols, and software for modeling, storing, searching, sharing and synchronizing data in the post-PC era. Data may be files or objects, tweets or 5TB videos, and you can access it via a phone, browser or FUSE filesystem.&lt;/p&gt;
    &lt;p&gt;Perkeep is under active development. If you're a programmer or fairly technical, you can probably get it up and running and get some utility out of it. Many bits and pieces are actively being developed, so be prepared for bugs and unfinished features.&lt;/p&gt;
    &lt;p&gt;Join the community, consider contributing, or file a bug.&lt;/p&gt;
    &lt;p&gt;Things Perkeep believes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your data is entirely under your control&lt;/item&gt;
      &lt;item&gt;Open Source&lt;/item&gt;
      &lt;item&gt;Paranoid about privacy, everything private by default&lt;/item&gt;
      &lt;item&gt;No SPOF: don't rely on any single party (including yourself)&lt;/item&gt;
      &lt;item&gt;Your data should be alive in 80 years, especially if you are&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Latest Release&lt;/head&gt;
    &lt;p&gt;The latest release is 0.12 ("Toronto"), released 2025-11-11.&lt;/p&gt;
    &lt;p&gt;Follow the download and getting started instructions to set up Perkeep.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video Demo&lt;/head&gt;
    &lt;p&gt;LinuxFest Northwest 2018 [slides] [video]:&lt;/p&gt;
    &lt;p&gt;Or see the other presentations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45896130</guid><pubDate>Wed, 12 Nov 2025 03:34:32 +0000</pubDate></item><item><title>Hard drives on backorder for two years as AI data centers trigger HDD shortage</title><link>https://www.tomshardware.com/pc-components/hdds/ai-triggers-hard-drive-shortage-amidst-dram-squeeze-enterprise-hard-drives-on-backorder-by-2-years-as-hyperscalers-switch-to-qlc-ssds</link><description>&lt;doc fingerprint="2f7cc5998cb29fb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Hard drives on backorder for two years as AI data centers trigger HDD shortage — delays forcing rapid transition to QLC SSDs&lt;/head&gt;
    &lt;p&gt;The AI boom might help QLC overtake TLC in the next two years.&lt;/p&gt;
    &lt;p&gt;The race to achieve AGI (artificial general intelligence) has pushed constituents to invest in and build data centers at a pace far outstripping our ability to make them. Manufacturers are struggling to keep up with AI demand, and the ongoing DRAM shortage is proof of this, with memory kits costing more than double what they did just a few months ago. Now, DigiTimes is reporting that storage is taking a hit, too, with delivery times for enterprise-grade HDDs delayed by two years.&lt;/p&gt;
    &lt;p&gt;That means if a firm wants to buy large-capacity hard drives, the backbone of nearline storage, it has to wait 24 months due to long lead times. As the news cycle suggests, AI money doesn't wait for anyone, so hyperscalers are now switching to QLC NAND-based SSDs to avoid these backorders. Picking QLC over TLC allows them to maintain costs while achieving sufficient endurance for cold storage.&lt;/p&gt;
    &lt;p&gt;However, hoarding QLC NAND creates its own shortage, since every cloud provider in North America and China is now lining up to buy it. This could lead to SSD prices rising worldwide, as most value-oriented models use QLC to save costs. In fact, DigiTimes claims that production capacity for QLC is completely booked through 2026 at some NAND manufacturers.&lt;/p&gt;
    &lt;p&gt;Therefore, given the current situation, QLC NAND is expected to overtake TLC in popularity by early 2027, marking a significant shift in the storage landscape. While enterprise-grade QLC SSDs would entirely power this pivot, Sandisk has already raised NAND prices by 50%, according to another DigiTimes report, after initially warning of a 10% increase two months ago.&lt;/p&gt;
    &lt;p&gt;This unprecedented shortage across memory and storage was largely unforeseen. Still, given the AI ambitions of the world's wealthiest, the overnight whiplash is perhaps the only surprising aspect of these price hikes. Last month, the Adata chairman hinted that the situation would only worsen over time, and it's taken just a few weeks for us to receive confirmation.&lt;/p&gt;
    &lt;p&gt;Every DRAM and NAND manufacturer is now selling capacity to AI customers willing to pay the big bucks. Instead of having 2-3 months of buffer capacity, these firms are down to just a few weeks now. This has led to year-best numbers for many businesses — a sharp turnaround from a few years ago — but, as usual, the short end of the stick trickles down to regular consumers who're now entangled in yet another electronics scarcity.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he’s not working, you’ll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;RTX 2080&lt;/header&gt;I buy BluRay and UHD discs of various films and TV shows I like and Rip them to my PC because 1) long term it is cheaper than paying to maintain 5 different streaming services, 2) the quality is higher, and 3) I prefer owning to subscribing. Consequently, I have a need to keep a large quantity of storage drives on hand.Reply&lt;lb/&gt;A few months ago an Article was written on this very site announcing Seagate’s new 30TB drives. Right up my alley, so I bought one; $549. Fast forward to 3 weeks ago, there were rumblings that the AI build out would soon create a storage shortage. Remembering the grand old days of Chia mining, I bought a second 30TB drive, the price having already increased to $599. Only 2 days later, the 30TB model was out of stock at Seagate and hasn’t come back in stock since. The 28TB model is currently in stock at $569, more than I paid for my first 30TB model. Who knows if even the 28TB model will be consistently in stock even at these elevated prices in the long term.&lt;lb/&gt;It’s rough out there people. It honestly reminds me a lot of trying to buy a RTX 3000 series GPU during COVID; they were either impossible to find or hideously overpriced. Oh well, I’m tempted to buy a RTX 5080 just because I can: a few are available at MSRP.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;yc1&lt;/header&gt;I swear that this is intentional at this point a excuse to raise prices while not improving their product and if the demand doesn't spike allowing a massive price hike they cut production to keep scarcity high and increase prices like what flash memory and ddr 4 manufacturers didReply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;tamalero&lt;/header&gt;Reply&lt;quote/&gt;its a cycle that repeats.. around 4 years ago they complained of RAM being worthless and companies losing money while producing. Aka oversupply of RAM and chips.yc1 said:I swear that this is intentional at this point a excuse to raise prices while not improving their product and if the demand doesn't spike allowing a massive price hike they cut production to keep scarcity high and increase prices like what flash memory and ddr 4 manufacturers did&lt;lb/&gt;Same with hard disks..&lt;lb/&gt;And before, during crypto.. there was also a shortage... in reality is all production manipulation just like the gas/oil production worldwide.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;satoasty&lt;/header&gt;Yep and to be fair the governments never wanted people to own their stuff by 2030. Probably works in their favour too.Reply&lt;lb/&gt;Apart from the stuff I can't prove, is Seagate as reliable as WD I've had a few older drives by Seagate fail on me but 1 was physical on the HDD?&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thesyndrome&lt;/header&gt;So now not only are regular consumers getting screwed on RAM and SSD prices due to the NAND shortage, but now we can't even get mechanical drives for a reasonable price any more? And this is combined with GPU's prices always going up since COVID. It feels like the only PC part that hasn't drastically increased in price is CPUs.Reply&lt;lb/&gt;Computing is starting to become a ridiculous hobby from a financial perspective, and if anything will contribute to the downfall of AI faster than anything else; how can AI ever hope to make a profit if they've priced-out regular consumers from even being able to access it by making computers too expensive for the regular person, bearing in mind that inflation hasn't stopped and household essentials are still going up in price.?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45896707</guid><pubDate>Wed, 12 Nov 2025 05:36:41 +0000</pubDate></item><item><title>Problems with C++ exceptions</title><link>https://marler8997.github.io/blog/bjarne-fix-your-language/</link><description>&lt;doc fingerprint="a1aca10620da550"&gt;
  &lt;main&gt;
    &lt;p&gt;About 23 minutes into his talk about Safe C++ [1], Bjarne shows a slide with this code:&lt;/p&gt;
    &lt;code&gt;void f(const char* p)           // unsafe, naive use
{
    FILE *f = fopen(p, "r");    // acquire
    // use f
    fclose(f);                  // release
}
&lt;/code&gt;
    &lt;p&gt;He shows this code to demonstrate how easy it is to miss resource cleanup. Any code that exits the function inside &lt;code&gt;// use f&lt;/code&gt; that doesn’t also call &lt;code&gt;fclose&lt;/code&gt; results in a leak.  He provides a &lt;code&gt;C++&lt;/code&gt; equivalent with RAII to avoid this footgun:&lt;/p&gt;
    &lt;code&gt;class File_handle {    // belongs in some support library
    FILE *p;
public:
    File_handle(const char *pp, const char *r)
        { p = fopen(pp, r); if (p == 0) throw File_error(pp, r); }
    File_handle(const string&amp;amp; s, const char *r)
        { p = fopen(s.c_str(), r); if (p == 0) throw File_error(pp, r); }
    ~File_handle() { fclose(p); } // destructor
    // copy operations
    // access functions
};


void f(string s)
{
    File_handle fh { s, "r"};     // now: ifstream fh{s}
    // use fh
}
&lt;/code&gt;
    &lt;p&gt;This sample is great at showing the benefits of RAII, but introduces some problems when it comes to error handling. In this example, Bjarne elects to throw an exception to propagate any error from calling &lt;code&gt;fopen&lt;/code&gt;.  Unfortunately, C++ exceptions have 3 problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Correctness: you don’t know if the exception type you’ve caught matches what the code throws&lt;/item&gt;
      &lt;item&gt;Exhaustiveness: you don’t know if you’ve caught all exceptions the code can throw&lt;/item&gt;
      &lt;item&gt;RAISI: try/catch requires a new scope which usually means you need RAISI (Resource Acquisition is Second Initialization)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In most applications it’s expected that failing to open a file is a normal error that should be handled in some way other than throwing an exception all the way up the stack. Let’s assume the proper way to handle the error in this case is to report it to stderr and return from the function. Let’s see how that looks in the original C code:&lt;/p&gt;
    &lt;code&gt;void f(const char* p)
{
    FILE *f = fopen(p, "r");
    if (f == NULL) {
        fprintf(stderr, "failed to open file '%s', error=%d\n", p, errno);
        return;
    }
    // use f
    fclose(f);
}
&lt;/code&gt;
    &lt;p&gt;Most systems-level programmers can look at this code and verify the error has been “caught”. Almost all C functions that return a pointer reserve &lt;code&gt;NULL&lt;/code&gt; for the error case.  This method isn’t perfect, relying on special values to detect errors is a common source of bugs, but let’s contrast this with the C++ example:&lt;/p&gt;
    &lt;code&gt;void f(string s)
{
    try {
        File_handle fh { s, "r"};
        // use fh
    } catch (const File_error&amp;amp; e) {
        fprintf(stderr, "failed to open file '%s', error=%d\n", s.c_str(), errno);
        return;
    }
}
&lt;/code&gt;
    &lt;p&gt;This example introduces a few problems. The first is that our error message may not be correct. It’s possible that the exception we’ve caught was not introduced by opening this file, and, the errno may not reflect the errno at the time &lt;code&gt;fopen&lt;/code&gt; was called.  To fix the first problem, we can limit the code inside our &lt;code&gt;try/catch&lt;/code&gt; to just the code that opens the file, let’s adjust it to do so:&lt;/p&gt;
    &lt;code&gt;void f(string s)
{
    File_handle fh;
    try {
        fh = File_handle(s, "r");
    } catch (const File_error&amp;amp; e) {
        fprintf(stderr, "failed to open file '%s', error=%d\n", s.c_str(), errno);
        return;
    }
    // use fh
}
&lt;/code&gt;
    &lt;p&gt;Unfortunately this won’t compile because the &lt;code&gt;File_handle&lt;/code&gt; type has no default constructor.  Because we can only catch an exception inside a scope, we need to enhance our &lt;code&gt;File_handle&lt;/code&gt; type to cover this transient state of existence before it’s initialized.  In other words, RAII is no longer good enough, now we need RAISI.  We need to introduce our &lt;code&gt;File_handle&lt;/code&gt; object in the outerscope with an initial “null” state, then really initialize it a second time inside the &lt;code&gt;try/catch&lt;/code&gt; scope.&lt;/p&gt;
    &lt;p&gt;One way to accomplish this is to wrap &lt;code&gt;File_handle&lt;/code&gt; in &lt;code&gt;optional&lt;/code&gt;, then update all our &lt;code&gt;//use fh&lt;/code&gt; code to use &lt;code&gt;fh.value()&lt;/code&gt; or &lt;code&gt;*fh&lt;/code&gt; instead of just &lt;code&gt;fh&lt;/code&gt;.  Another way is to enhance &lt;code&gt;File_handle&lt;/code&gt; itself to support a &lt;code&gt;null state&lt;/code&gt;, which would look something like this:&lt;/p&gt;
    &lt;code&gt;// new constructor
File_handle() : p(nullptr) { }

// enhanced destructor
~File_handle() { if (p) fclose(p); }
};
&lt;/code&gt;
    &lt;p&gt;The second problem with our error message is that we don’t know if &lt;code&gt;errno&lt;/code&gt; is correct.  Alot of things have occurred between the time that our call to &lt;code&gt;fclose&lt;/code&gt; failed in the contructor and the exception was caught.  To handle this, we can enhance the &lt;code&gt;File_error&lt;/code&gt; class provided to us by Bjarne by also having it store the errno at the time it’s thrown:&lt;/p&gt;
    &lt;code&gt;// enhanced constructor
File_handle(const char *pp, const char *r)
    { p = fopen(pp, r); if (p == 0) throw File_error(pp, r, errno); }
};
&lt;/code&gt;
    &lt;p&gt;Now we have the tools to report a correct error message. However, if you look at the original C code and the C++ code side-by-side, it doesn’t look pretty. It’s more noisy than the original C example which generates backpressure in changes that attempt to implement proper error handling like this.&lt;/p&gt;
    &lt;p&gt;The bigger problem with our &lt;code&gt;C++&lt;/code&gt; example is that it provides no guarantees about whether we’ve actually caught all the exceptions that could occur when opening a file.  Any nested function/operator/object used inside our File_handle constructor has the ability to throw any other exception type that we haven’t accounted for, and now instead of an error message, our program unintentionally crashes.  This scenario is unlikely in this particular example, but, you can imagine how this problem gets exponentially worse once you start sprinkling exceptions throughout your code base.  It becomes impossible to know whether your code handles all possible exceptions in the places you need to.  In contrast, using return values for error handling localizes the problem only to the function you are calling.  With return values, it’s much more difficult for a change to a function multiple levels down in the call stack to introduce a new error state that you can’t catch/handle.&lt;/p&gt;
    &lt;p&gt;This is the general problem with RAII in C++, how do you pair it with error handling that developers will use without introducing the problems we’ve discussed? One technique you can use today is to do your error handling before your RAII. The example above could be written like this instead:&lt;/p&gt;
    &lt;code&gt;class File_handle {
    FILE *p;
public:
    File_handle(FILE* p) : p(p) { }
    ~File_handle() { if (p) fclose(p); }
};


void f(string s)
{
    File_handle fh(fopen(p, "r"));
    if (!fh.p) {
        fprintf(stderr, "failed to open file '%s', error=%d\n", s.c_str(), errno);
        return;
    }
    // use fh
}
&lt;/code&gt;
    &lt;p&gt;I find that using this technique typically results in these benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;your classes get smaller/simpler&lt;/item&gt;
      &lt;item&gt;you no longer need to define exception types&lt;/item&gt;
      &lt;item&gt;the code that knows how to handle the error has the context it needs to report it propertly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since we’ve removed the abstraction that &lt;code&gt;File_handle&lt;/code&gt; calls &lt;code&gt;fopen&lt;/code&gt;, we now call it ourselves and know that we can get further error information through &lt;code&gt;errno&lt;/code&gt; without propagating it through a new &lt;code&gt;File_error&lt;/code&gt; object.  The general principle here is that anything that can fail, you do outside of a constructor.  By avoiding the need to handle errors inside a constructor, you avoid having the introduce exceptions and the subsequent problems.&lt;/p&gt;
    &lt;p&gt;Another technique to address these problems is to provide a “side-channel” in your constructors for reporting errors. In fact, some of the std library does this. The function &lt;code&gt;std::filesystem::rename&lt;/code&gt; takes a reference to an &lt;code&gt;error_code&lt;/code&gt; which can be used to report errors.  Here’s what out code could look like with that:&lt;/p&gt;
    &lt;code&gt;error_code error_code_from_errno(int);

class File_handle {
    FILE *p;
public:
    File_handle(const char *pp, const char *r, error_code&amp;amp; ec)
        { p = fopen(pp, r); if (p == 0) ec = error_code_from_errno(errno); }
    File_handle(const string&amp;amp; s, const char *r, error_code&amp;amp; ec)
        { p = fopen(s.c_str(), r); if (p == 0) ec = error_code_from_errno(errno); }
    ~File_handle() { if (p) fclose(p); }
};

void f(string s)
{
    std::error_code open_error;
    File_handle fh(s, "r", open_error);
    if (open_error) {
        fprintf(stderr, "failed to open file '%s', error=%d\n", s.c_str(), open_error.value());
        return;
    }
    // use fh
}
&lt;/code&gt;
    &lt;p&gt;Now that we’ve ditched exceptions we’ve avoided the main problem with them. We no longer need RAISI and we are confident we’ve caught/handled the error. It’s still a little more noisy than the C code but it’s less noisy than the original one that used exceptions.&lt;/p&gt;
    &lt;p&gt;Now that we’ve looked at a couple practical techniques, let’s dream about how C++ could look. C++ exceptions actually have a few more problems that I havent mentioned. I highly recommend watching Herb Sutter’s talk about them here:&lt;/p&gt;
    &lt;p&gt;There’s also a paper you can read here:&lt;/p&gt;
    &lt;p&gt;https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0709r4.pdf&lt;/p&gt;
    &lt;p&gt;… go on to talk about removing the required scope to avoid RAISI .. then go on to talk about compile-time enforcement of exception handling…&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45896733</guid><pubDate>Wed, 12 Nov 2025 05:44:10 +0000</pubDate></item></channel></rss>