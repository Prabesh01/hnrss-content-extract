<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 25 Sep 2025 19:08:07 +0000</lastBuildDate><item><title>The Theatre of Pull Requests and Code Review</title><link>https://meks.quest/blogs/the-theatre-of-pull-requests-and-code-review</link><description>&lt;doc fingerprint="3754740beb3d5f9b"&gt;
  &lt;main&gt;&lt;p&gt;We can't find the internet&lt;/p&gt;&lt;p&gt;Attempting to reconnect&lt;/p&gt;&lt;p&gt;Something went wrong!&lt;/p&gt;&lt;p&gt;Hang in there while we get back on track&lt;/p&gt;&lt;head rend="h1"&gt;The Theatre of Pull Requests and Code Review&lt;/head&gt;&lt;head rend="h3"&gt;Meks McClure · September 23, 2025&lt;/head&gt;Photo Credit to Petter Boström&lt;p&gt;I recently attended the Goatmire Elixir Conf and one of the standout talks for me was Saša Jurić's "Tell Me a Story". It was an incredible presentation that combined theatrical storytelling with practical technical advice. Saša performed parts of his talk in character, turning technical topics into a compelling narrative that was part comedy, part tragedy, and fully packed with useful insights I've started implementing myself. The recording will eventually be released online for viewing. I highly recommend that people watch it, and I'll endeavor to add a link to it here when it becomes available.&lt;/p&gt;Photo Credit to Petter Boström&lt;head rend="h2"&gt;The Code Review Challenge&lt;/head&gt;&lt;p&gt;The talk focused on Code Review and Pull Requests (PRs). Saša laid out common problems most software engineers face. Too often, engineers dread code reviews even though they're a significant part of team collaboration. We avoid them because PRs tend to be too large, too complex, too difficult to comprehend, and too painful to test. So we end up commenting "Looks Good To Me" and suggesting a few minor styling improvements to give the appearance of a thorough review.&lt;/p&gt;&lt;p&gt;This is how security leaks happen and codebases become progressively unmaintainable. Since git blame only points to the original author, it's easy to think "if something goes wrong, it's not on me". But we're all responsible for the whole system, regardless of who wrote the individual lines of code.&lt;/p&gt;&lt;head rend="h2"&gt;What Makes a PR Reviewable?&lt;/head&gt;&lt;p&gt;So how do we review something that feels unreviewable? Saša advocates for normalizing the practice of returning difficult-to-understand PRs to the author. This makes logical sense, but it's challenging to implement because it can feel like admitting we're not smart enough to understand the code. However, saying "I don't understand this enough to approve it" is far more valuable than pretending with an empty "LGTM".&lt;/p&gt;&lt;p&gt;If we commit to only reviewing truly reviewable PRs, what does that look like? According to Saša, it should take the average reviewer 5-10 minutes. By 'average reviewer,' he means mid-to-senior developers who understand the domain, business, and tech stack well—not newcomers still learning the system or mythical 10x engineers.&lt;/p&gt;&lt;p&gt;How do you create a PR that can be reviewed in 5-10 minutes? By reducing the scope. A full feature should often be multiple PRs. A good rule of thumb is 300 lines of code changes - once you get above 500 lines, you're entering unreviewable territory.&lt;/p&gt;&lt;head rend="h2"&gt;Telling a Story with Commits&lt;/head&gt;&lt;p&gt;A key part of having a reviewable PR is writing commits that tell a story. Present your changes incrementally and logically so reviewers can follow your thought process. Generic commit messages such as "add dependency," "implement file upload feature," and "address PR feedback" don’t tell much of a story and leave reviewers guessing. Why was the dependency added? What were the specific steps in creating the file uploader feature? What feedback is being addressed?&lt;/p&gt;&lt;head rend="h3"&gt;Story-Telling Commit Messages&lt;/head&gt;&lt;p&gt;After a toast to the demo gods, Saša demonstrated writing story-telling commits with a live coding example, creating a PR that was part of a larger feature. His example PR adds just 152 lines of code, removes 2 lines, but uses 13 thoughtful commits.&lt;/p&gt;&lt;p&gt;While some developers might understand those 152 lines from the final diff alone, I couldn't confidently approve it without the commit story.&lt;/p&gt;&lt;head rend="h3"&gt;Breaking Down the Example&lt;/head&gt;&lt;p&gt; For instance, looking at the overall diff, I didn't understand why he added &lt;code&gt;:runtime_tools&lt;/code&gt;
      to &lt;code&gt;applications&lt;/code&gt;
      in &lt;code&gt;mix.exs&lt;/code&gt;. Following the commit narrative, it's clear this was needed for access to
      &lt;code&gt;:scheduler.get_sample()&lt;/code&gt;
      to collect the samples. Now I can research that context or ask more pointed questions.
    &lt;/p&gt;&lt;head rend="h3"&gt;The Iterative Process&lt;/head&gt;&lt;p&gt;A huge benefit of seeing this live was witnessing the iterative process. In the compute average utilization commit, we initially saw an incorrect implementation that computed averages of all schedulers, including offline ones. When testing revealed unexpected results, Saša went back and updated both the code and the commit that originally implemented that function so the story remained coherent.&lt;/p&gt;&lt;p&gt;A flow that I find to work well for keeping commit history clean is with fixup commits. A fixup is a small commit that’s explicitly marked to be folded into an earlier commit during an interactive rebase. When you run rebase with autosquash, Git automatically pairs each fixup with its target and tucks the changes into the right place, keeping the story coherent without manual reordering.&lt;/p&gt;&lt;p&gt;I sometimes experience creating merge conflicts for myself during this process. Both Saša and I agree that if it becomes too much effort to resolve the conflict, then creating a new commit is ok. Taking the time to put in extra effort to keep the commit history clean and the story coherent makes the PR easier for reviewers to understand.&lt;/p&gt;&lt;head rend="h3"&gt;The Value of Clean History&lt;/head&gt;&lt;p&gt;Keeping the commit history clean connects to advice I've heard about ensuring every commit compiles and keeps the application runnable. I used to follow this loosely, but recent experiences with git bisect emphasized to me its importance. (If you are unfamiliar with git bisect , it's worth checking out; it uses a binary search algorithm to find which commit in your project's history introduced a bug.)&lt;/p&gt;&lt;p&gt;There are a few factors that make narrowing down when and how a regression was introduced more challenging. If a commit doesn't compile, I can't isolate whether the bug first appeared there. If the bug appeared in a commit that had hundreds of lines of code changed, determining which part of the commit is the issue requires significantly more reasoning. A clean commit history with messages that tell a story makes these kinds of investigations easier.&lt;/p&gt;&lt;head rend="h2"&gt;Making Review a Collaborative Success&lt;/head&gt;&lt;p&gt;When we present focused PRs with commits that tell clear stories, we get feedback sooner and our development cycles speed up. When reviewers understand our changes, we're more likely to receive valuable feedback instead of blanket approvals, and we're more likely to ship quality code. When our commits make sense, we can travel back in time as needed to understand how our codebase evolved.&lt;/p&gt;&lt;p&gt;Thanks to Saša's theatrical lesson, I will be more intentional about crafting commit stories. The next time you're preparing a PR, consider: Are you telling a story your reviewers can follow? Start small - maybe focus on just one aspect, like keeping PRs under that 300-line guideline or writing more descriptive commit messages. Your future reviewers (and your future debugging self) will thank you.&lt;/p&gt;&lt;head rend="h3"&gt;Resources&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Git blame for showing which revision and author last modified each line of a file&lt;/item&gt;&lt;item&gt;Git rebase for cleaning up commit history&lt;/item&gt;&lt;item&gt;Git fixup for amending earlier commits&lt;/item&gt;&lt;item&gt;Git bisect for finding when bugs were introduced&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45371283</guid><pubDate>Thu, 25 Sep 2025 10:35:19 +0000</pubDate></item><item><title>The Wind, a Pole, and the Dragon</title><link>https://entropicthoughts.com/the-wind-a-pole-and-the-dragon</link><description>&lt;doc fingerprint="6a4e089e8ea011d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Wind, a Pole, and the Dragon&lt;/head&gt;
    &lt;p&gt;One of my favourite requests for help online comes from the shibboleth-users group, where someone Japanese used machine translation to ask about the following problem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;At often, the goat-time install a error is vomit. To how many times like the wind, a pole, and the dragon? Install 2,3 repeat, spank, vomit blows&lt;/p&gt;14:14:01.869 - INFO [edu.internet2.middleware.shibboleth.common.config.profile.JSPErrorHandlerBeanDefinitionParser:45] Parsing configuration for JSP error handler.&lt;p&gt;Not precise the vomit but with aspect similar, is vomited concealed in fold of goat-time lumber? goat-time see like the wind, pole, and dragon? This insult to father’s stones? JSP error handler with wind, pole, dragon with intercourse to goat-time? Or chance lack of skill with a goat-time?&lt;/p&gt;&lt;p&gt;Please apologize for your stupidity. There are a many thank you&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;I have long wanted to figure out exactly how this went so wrong. Some parts are fairly clear:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;vomit could come from throw (as in throwing an error) or even just output.&lt;/item&gt;
      &lt;item&gt;lumber must clearly reference logs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I have also heard speculation that goat-time means runtime, as in the Java runtime, perhaps. This means we can already figure out how we got to “vomited concealed in fold of goat-time lumber” – it’s an error hidden in the runtime logs.&lt;/p&gt;
    &lt;p&gt;I asked a few llms to assist me with the rest, and they universally think spank is an odd translation of hit, which is apparently used in Japanese to mean something like execute, and skill could be a mistranslation of experience.&lt;/p&gt;
    &lt;p&gt;We can start to put together what the message actually means.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Often when trying to install the runtime an error is thrown. uninterpretable I have tried reinstalling it three times, but when I run it an exception is thrown.&lt;/p&gt;
      &lt;p&gt;This is not the exact exception but something like that. Is the real error hidden in the runtime logs? uninterpretable. uninterpretable arising due to interaction with the runtime? Or perhaps my lack of experience with the runtime?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The llms diverge on the meaning of “insult to father’s stones”. Some suggest the obvious thing, that it’d correspond to an idiomatic expression of frustration. Others seem to think it might be about “problems with the ancestral building blocks”, i.e. software dependencies. I liked that reading, but I have no idea.&lt;/p&gt;
    &lt;p&gt;New here? I apply the same weird curiosity to everything I discover. To learn something new, subscribe for weekly article summaries! If you don't like it, you can unsubscribe any time.&lt;/p&gt;
    &lt;p&gt;Then there’s “the wind, a pole, and the dragon.” I have yet to see anything come close to a reasonable answer. llms produce guesses referring to three parts of the configuration, variable names, dependencies, colloquialisms, descriptions of user interface, or abstract descriptions of how quickly things happen (the wind), a fixed point (a pole), and complexity/power (dragon). But again, I have no idea.&lt;/p&gt;
    &lt;p&gt;If you have more information, please reach out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45371309</guid><pubDate>Thu, 25 Sep 2025 10:39:17 +0000</pubDate></item><item><title>Data Viz Color Palette Generator (For Charts and Dashboards)</title><link>https://www.learnui.design/tools/data-color-picker.html</link><description>&lt;doc fingerprint="32a83ff73ed5803d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Palette Generator&lt;/head&gt;
    &lt;head rend="h3"&gt;Number of Colors&lt;/head&gt;
    &lt;head rend="h3"&gt;Background Color&lt;/head&gt;
    &lt;head rend="h2"&gt;In Context&lt;/head&gt;
    &lt;head rend="h2"&gt;How to Use&lt;/head&gt;
    &lt;p&gt;Use the palette chooser to create a series of colors that are visually equidistant. This is useful for many data visualizations, like pie charts, grouped bar charts, and maps.&lt;/p&gt;
    &lt;p&gt;Note: there are two other modes besides palette mode – check out single-hue scales and divergent scales as well.&lt;/p&gt;
    &lt;p&gt;Creating visually equidistant palettes is basically impossible to do by hand, yet hugely important for data visualizations. Why? When colors are not visually equidistant, it’s harder to (a) tell them apart in the chart, and (b) compare the chart to the key. I’m sure we’ve all looked at charts where you can hardly use the key since the data colors are so similar.&lt;/p&gt;
    &lt;p&gt;For instance, Google Analytics does a terrible job with this:&lt;/p&gt;
    &lt;p&gt;It’s better to use use a range of hues so users can cross-reference with the key easier. It’s far simpler for our brains to distinguish, say, yellow from orange than blue from blue-but-15%-lighter.&lt;/p&gt;
    &lt;p&gt;This color picker allows you to specify both endpoints of the palette. You can choose at least one to be a brand color, which gives you significant flexibility in creating a palette that will work for your visualizations, yet be customized for your brand.&lt;/p&gt;
    &lt;p&gt;Here are a few tips for getting the best palette:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Try picking very different endpoint colors – e.g. one warm, one cool; one bright, one darker – so that your palette covers a wider range&lt;/item&gt;
      &lt;item&gt;If you’re using a brand color for one endpoint, don’t be afraid to modify the saturation and brightness a bit if it creates a more pleasing palette. Users will recognize your brand color by its hue much far more than by it’s exact saturation/brightness.&lt;/item&gt;
      &lt;item&gt;For data visualizations where you’re showing the strength of a single value, try using the Single Hue Palette Generator instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh, and...&lt;/p&gt;
    &lt;head rend="h2"&gt;More on Color&lt;/head&gt;
    &lt;p&gt;If you're new to color in UI design, I highly recommend the following resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The HSB Color System: A Practitioner's Primer&lt;/item&gt;
      &lt;item&gt;Color in UI Design: A Practical Framework&lt;/item&gt;
      &lt;item&gt;Gradient Generator tool, by yours truly, built to be the most fully-featured on the web 😎&lt;/item&gt;
      &lt;item&gt;Design Hacks, my email newsletter where I send original design tips and tactics to 60,000+ of my closest friends.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Anyhow, I've created this to be the tool I wish I had for creating data visualization palettes. Is there another feature you'd like to see in it? Let me know.&lt;/p&gt;
    &lt;head rend="h1"&gt;Single Hue Scale&lt;/head&gt;
    &lt;head rend="h3"&gt;Number of Colors&lt;/head&gt;
    &lt;head rend="h3"&gt;Modify Color Scale&lt;/head&gt;
    &lt;p&gt;Brightness&lt;/p&gt;
    &lt;p&gt;Color Intensity&lt;/p&gt;
    &lt;head rend="h3"&gt;Background Color&lt;/head&gt;
    &lt;head rend="h2"&gt;In Context&lt;/head&gt;
    &lt;head rend="h2"&gt;How to Use&lt;/head&gt;
    &lt;p&gt;The Single Hue Scale generator is most useful for visualizations where you’re showing the value of a single variable. Typically, the darker variation will represent a higher value, and a neutral color (even white) will represent a value closer to zero.&lt;/p&gt;
    &lt;p&gt;In a pie chart or bar chart, size is used to distinguish higher values. But in some visualizations, the size is set and you need to rely on color. Two examples of this are show in the “In Context” section above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A map in which size represents county size; we need to use color to distinguish the value for each county&lt;/item&gt;
      &lt;item&gt;A week-by-week calendar in which each day is an equally sized box; we need to use color to show the value for a particular day&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are a few tips for getting the best single hue scale:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To transition to a flat gray endpoint, set “Color Intensity” to zero&lt;/item&gt;
      &lt;item&gt;To transition to a white endpoint, set “Brightness” to full and “Color Intensity” to zero&lt;/item&gt;
      &lt;item&gt;If your color scale actually shows a variable that transitions from one end to a neutral midpoint to another end, try the Divergent Scale Generator (e.g. Republican to moderate to Democrat; hotter to same-temperature to cooler)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Divergent Color Scale&lt;/head&gt;
    &lt;head rend="h3"&gt;Number of Colors&lt;/head&gt;
    &lt;head rend="h3"&gt;Modify Midpoint Color&lt;/head&gt;
    &lt;p&gt;Brightness&lt;/p&gt;
    &lt;p&gt;Color Intensity&lt;/p&gt;
    &lt;head rend="h3"&gt;Background Color&lt;/head&gt;
    &lt;head rend="h2"&gt;In Context&lt;/head&gt;
    &lt;head rend="h2"&gt;How to Use&lt;/head&gt;
    &lt;p&gt;The Divergent Color Scale generator is most useful for visualizations where you’re showing a transition from (a) one extreme, through a (b) neutral middle, and finally to a (c) opposite extreme.&lt;/p&gt;
    &lt;p&gt;Perhaps the most common example of this is the “how Democrat/Republican is each state in the US” chart.&lt;/p&gt;
    &lt;p&gt;By default, the neutral midpoint is a light gray. You can change it with the "Modify Midpoint Color" sliders to be slightly darker or more colorful. For the best results, set the Color Intensity to the minimum when the two endpoint hues are significantly different – otherwise, the moderate tones will start to blend together (this will be evident in the map).&lt;/p&gt;
    &lt;p&gt;As with the other visualization styles, this will pick colors that are visually equidistant. However, if one of the two endpoint colors is significantly darker or saturated, the swatches on that side will have more color-space between them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45372286</guid><pubDate>Thu, 25 Sep 2025 13:13:25 +0000</pubDate></item><item><title>Video models are zero-shot learners and reasoners</title><link>https://video-zero-shot.github.io/</link><description>&lt;doc fingerprint="cf176b948f11fe6e"&gt;
  &lt;main&gt;
    &lt;p&gt;Veo 3 shows emergent zero-shot abilities across many visual tasks, indicating that video models are on a path to becoming vision foundation models—just like LLMs became foundation models for language.&lt;/p&gt;
    &lt;p&gt;Perception&lt;/p&gt;
    &lt;p&gt;Modeling&lt;/p&gt;
    &lt;p&gt;Manipulation&lt;/p&gt;
    &lt;p&gt;Reasoning&lt;/p&gt;
    &lt;p&gt; The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding?&lt;lb/&gt; We demonstrate that Veo 3 can zero-shot solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and much more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo 3's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models. &lt;/p&gt;
    &lt;p&gt;On a run and want to get a gist of our paper? Listen to the following podcast!&lt;/p&gt;
    &lt;p&gt;Edge detection&lt;/p&gt;
    &lt;p&gt;Segmentation&lt;/p&gt;
    &lt;p&gt;Keypoint localization&lt;/p&gt;
    &lt;p&gt;Super-resolution&lt;/p&gt;
    &lt;p&gt;Blind deblurring&lt;/p&gt;
    &lt;p&gt;Blind denoising&lt;/p&gt;
    &lt;p&gt;Low-light enhancement&lt;/p&gt;
    &lt;p&gt;Conjunctive search / binding problem&lt;/p&gt;
    &lt;p&gt;Dalmatian illusion understanding&lt;/p&gt;
    &lt;p&gt;Shape cue-conflict understanding&lt;/p&gt;
    &lt;p&gt;Rorschach blot interpretation&lt;/p&gt;
    &lt;p&gt;Material properties (flammability)&lt;/p&gt;
    &lt;p&gt;Rigid body transform&lt;/p&gt;
    &lt;p&gt;Soft body transform&lt;/p&gt;
    &lt;p&gt;Gravity (earth)&lt;/p&gt;
    &lt;p&gt;Gravity (moon)&lt;/p&gt;
    &lt;p&gt;Buoyancy (bottle cap)&lt;/p&gt;
    &lt;p&gt;Buoyancy (rock)&lt;/p&gt;
    &lt;p&gt;Visual Jenga&lt;/p&gt;
    &lt;p&gt;Object packing&lt;/p&gt;
    &lt;p&gt;Material optics (glass)&lt;/p&gt;
    &lt;p&gt;Material optics (mirror)&lt;/p&gt;
    &lt;p&gt;Color mixing (additive)&lt;/p&gt;
    &lt;p&gt;Color mixing (subtractive)&lt;/p&gt;
    &lt;p&gt;Categorizing objects&lt;/p&gt;
    &lt;p&gt;Omniglot (recognition)&lt;/p&gt;
    &lt;p&gt;Omniglot (generation)&lt;/p&gt;
    &lt;p&gt;Omniglot (parsing)&lt;/p&gt;
    &lt;p&gt;Memory of world states&lt;/p&gt;
    &lt;p&gt;Background removal&lt;/p&gt;
    &lt;p&gt;Style transfer&lt;/p&gt;
    &lt;p&gt;Colorization&lt;/p&gt;
    &lt;p&gt;Inpainting&lt;/p&gt;
    &lt;p&gt;Outpainting&lt;/p&gt;
    &lt;p&gt;Text manipulation&lt;/p&gt;
    &lt;p&gt;Image editing with doodles&lt;/p&gt;
    &lt;p&gt;Scene composition&lt;/p&gt;
    &lt;p&gt;Novel view synthesis&lt;/p&gt;
    &lt;p&gt;3D-aware reposing&lt;/p&gt;
    &lt;p&gt;Transfiguration&lt;/p&gt;
    &lt;p&gt;Professional headshot&lt;/p&gt;
    &lt;p&gt;Dexterous manipulation (jar)&lt;/p&gt;
    &lt;p&gt;Dexterous manipulation (throw/catch)&lt;/p&gt;
    &lt;p&gt;Dexterous manipulation (baoding balls)&lt;/p&gt;
    &lt;p&gt;Affordance recognition&lt;/p&gt;
    &lt;p&gt;Drawing&lt;/p&gt;
    &lt;p&gt;Visual instruction (burrito)&lt;/p&gt;
    &lt;p&gt;Graph traversal&lt;/p&gt;
    &lt;p&gt;Tree BFS&lt;/p&gt;
    &lt;p&gt;Sequence (dots)&lt;/p&gt;
    &lt;p&gt;Sequence (arrows)&lt;/p&gt;
    &lt;p&gt;Sequence (circles)&lt;/p&gt;
    &lt;p&gt;Sequence (squares)&lt;/p&gt;
    &lt;p&gt;Connecting colors&lt;/p&gt;
    &lt;p&gt;Shape fitting&lt;/p&gt;
    &lt;p&gt;Sorting numbers&lt;/p&gt;
    &lt;p&gt;Tool use&lt;/p&gt;
    &lt;p&gt;Simple sudoku completion&lt;/p&gt;
    &lt;p&gt;Water puzzle solving&lt;/p&gt;
    &lt;p&gt;Maze solving (mouse)&lt;/p&gt;
    &lt;p&gt;Robot navigation&lt;/p&gt;
    &lt;p&gt;Rule extrapolation&lt;/p&gt;
    &lt;p&gt;Analogy (color)&lt;/p&gt;
    &lt;p&gt;Analogy (resize)&lt;/p&gt;
    &lt;p&gt;Analogy (reflect)&lt;/p&gt;
    &lt;p&gt;Analogy (rotate)&lt;/p&gt;
    &lt;p&gt;Maze (5x5)&lt;/p&gt;
    &lt;p&gt;Maze (7x7)&lt;/p&gt;
    &lt;p&gt;Maze (9x9)&lt;/p&gt;
    &lt;p&gt;Maze (irregular)&lt;/p&gt;
    &lt;p&gt;Symmetry (shape)&lt;/p&gt;
    &lt;p&gt;Symmetry (random)&lt;/p&gt;
    &lt;quote&gt;@article{wiedemer2025video, title={Video models are zero-shot learners and reasoners}, author={Wiedemer, Thaddäus and Li, Yuxuan and Vicol, Paul and Gu, Shixiang Shane and Matarese, Nick and Swersky, Kevin and Kim, Been and Jaini, Priyank and Geirhos, Robert}, journal={arXiv preprint arXiv:TBD}, year={2025} }&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45372289</guid><pubDate>Thu, 25 Sep 2025 13:13:34 +0000</pubDate></item><item><title>Launch HN: Webhound (YC S23) – Research agent that builds datasets from the web</title><link>https://news.ycombinator.com/item?id=45373008</link><description>&lt;doc fingerprint="354a104b23ad844a"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;We're the team behind Webhound (&lt;/p&gt;https://webhound.ai&lt;p&gt;), an AI agent that builds datasets from the web based on natural language prompts. You describe what you're trying to find. The agent figures out how to structure the data and where to look, then searches, extracts the results, and outputs everything in a CSV you can export.&lt;/p&gt;&lt;p&gt;We've set up a special no-signup version for the HN community at https://hn.webhound.ai - just click "Continue as Guest" to try it without signing up.&lt;/p&gt;&lt;p&gt;Here's a demo: https://youtu.be/fGaRfPdK1Sk&lt;/p&gt;&lt;p&gt;We started building it after getting tired of doing this kind of research manually. Open 50 tabs, copy everything into a spreadsheet, realize it's inconsistent, start over. It felt like something an LLM should be able to handle.&lt;/p&gt;&lt;p&gt;Some examples of how people have used it in the past month:&lt;/p&gt;&lt;p&gt;Competitor analysis: "Create a comparison table of internal tooling platforms (Retool, Appsmith, Superblocks, UI Bakery, BudiBase, etc) with their free plan limits, pricing tiers, onboarding experience, integrations, and how they position themselves on their landing pages." (https://www.webhound.ai/dataset/c67c96a6-9d17-4c91-b9a0-ff69...)&lt;/p&gt;&lt;p&gt;Lead generation: "Find Shopify stores launched recently that sell skincare products. I want the store URLs, founder names, emails, Instagram handles, and product categories." (https://www.webhound.ai/dataset/b63d148a-8895-4aab-ac34-455e...)&lt;/p&gt;&lt;p&gt;Pricing tracking: "Track how the free and paid plans of note-taking apps have changed over the past 6 months using official sites and changelogs. List each app with a timeline of changes and the source for each." (https://www.webhound.ai/dataset/c17e6033-5d00-4e54-baf6-8dea...)&lt;/p&gt;&lt;p&gt;Investor mapping: "Find VCs who led or participated in pre-seed or seed rounds for browser-based devtools startups in the past year. Include the VC name, relevant partners, contact info, and portfolio links for context." (https://www.webhound.ai/dataset/1480c053-d86b-40ce-a620-37fd...)&lt;/p&gt;&lt;p&gt;Research collection: "Get a list of recent arXiv papers on weak supervision in NLP. For each, include the abstract, citation count, publication date, and a GitHub repo if available." (https://www.webhound.ai/dataset/e274ca26-0513-4296-85a5-2b7b...)&lt;/p&gt;&lt;p&gt;Hypothesis testing: "Check if user complaints about Figma's performance on large files have increased in the last 3 months. Search forums like Hacker News, Reddit, and Figma's community site and show the most relevant posts with timestamps and engagement metrics." (https://www.webhound.ai/dataset/42b2de49-acbf-4851-bbb7-080b...)&lt;/p&gt;&lt;p&gt;The first version of Webhound was a single agent running on Claude 4 Sonnet. It worked, but sessions routinely cost over $1100 and it would often get lost in infinite loops. We knew that wasn't sustainable, so we started building around smaller models.&lt;/p&gt;&lt;p&gt;That meant adding more structure. We introduced a multi-agent system to keep it reliable and accurate. There's a main agent, a set of search agents that run subtasks in parallel, a critic agent that keeps things on track, and a validator that double-checks extracted data before saving it. We also gave it a notepad for long-term memory, which helps avoid duplicates and keeps track of what it's already seen.&lt;/p&gt;&lt;p&gt;After switching to Gemini 2.5 Flash and layering in the agent system, we were able to cut costs by more than 30x while also improving speed and output quality.&lt;/p&gt;&lt;p&gt;The system runs in two phases. First is planning, where it decides the schema, how to search, what sources to use, and how to know when it's done. Then comes extraction, where it executes the plan and gathers the data.&lt;/p&gt;&lt;p&gt;It uses a text-based browser we built that renders pages as markdown and extracts content directly. We tried full browser use but it was slower and less reliable. Plain text still works better for this kind of task.&lt;/p&gt;&lt;p&gt;We also built scheduled refreshes to keep datasets up to date and an API so you can integrate the data directly into your workflows.&lt;/p&gt;&lt;p&gt;Right now, everything stays in the agent's context during a run. It starts to break down around 1000-5000 rows depending on the number of attributes. We're working on a better architecture for scaling past that.&lt;/p&gt;&lt;p&gt;We'd love feedback, especially from anyone who's tried solving this problem or built similar tools. Happy to answer anything in the thread.&lt;/p&gt;&lt;p&gt;Thanks! Moe&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45373008</guid><pubDate>Thu, 25 Sep 2025 14:28:24 +0000</pubDate></item><item><title>Cloudflare Email Service: private beta</title><link>https://blog.cloudflare.com/email-service/</link><description>&lt;doc fingerprint="ee97ad5b3f10bcad"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;If you are building an application, you rely on email to communicate with your users. You validate their signup, notify them about events, and send them invoices through email. The service continues to find new purpose with agentic workflows and other AI-powered tools that rely on a simple email as an input or output.&lt;/p&gt;
      &lt;p&gt;And it is a pain for developers to manage. Itâs frequently the most annoying burden for most teams. Developers deserve a solution that is simple, reliable, and deeply integrated into their workflow.Â &lt;/p&gt;
      &lt;p&gt;Today, we're excited to announce just that: the private beta of Email Sending, a new capability that allows you to send transactional emails directly from Cloudflare Workers. Email Sending joins and expands our popular Email Routing product, and together they form the new Cloudflare Email Service â a single, unified developer experience for all your email needs.&lt;/p&gt;
      &lt;p&gt;With Cloudflare Email Service, weâre distilling our years of experience securing and routing emails, and combining it with the power of the developer platform. Now, sending an email is as easy as adding a binding to a Worker and calling &lt;code&gt;send&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
  async fetch(request, env, ctx) {

    await env.SEND_EMAIL.send({
      to: [{ email: "[email protected]" }],
      from: { email: "[email protected]", name: "Your App" },
      subject: "Hello World",
      text: "Hello World!"
    });

    return new Response(`Successfully sent email!`);
  },
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Email experience is user experience&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Email is a core tenet of your user experience. Itâs how you stay in touch with your users when they are outside your applications. Users rely on email to inform them when they need to take actions such as password resets, purchase receipts, magic login links, and onboarding flows. When they fail, your application fails.&lt;/p&gt;
      &lt;p&gt;That means itâs crucial that emails need to land in your usersâ inboxes, both reliably and quickly. A magic link that arrives ten minutes late is a lost user. An email delivered to a spam folder breaks user flows and can erode trust in your product. Thatâs why weâre focusing on deliverability and time-to-inbox with Cloudflare Email Service.Â &lt;/p&gt;
      &lt;p&gt;To do this, weâre tightly integrating with DNS to automatically configure the necessary DNS records â like SPF, DKIM and DMARC â such that email providers can verify your sending domain and trust your emails. Plus, in true Cloudflare fashion, Email Service is a global service. That means that we can deliver your emails with low latency anywhere in the world, without the complexity of managing servers across regions.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Simple and flexible for developers&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Treating email as a core piece of your application also means building for every touchpoint in your development workflow. Weâre building Email Service as part of the Cloudflare stack to make developing with email feels as natural as writing a Worker.Â &lt;/p&gt;
      &lt;p&gt;In practice, that means solving for every part of the transactional email workflow:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Starting with Email Service is easy. Instead of managing API keys and secrets, you can use the &lt;code&gt;Email&lt;/code&gt; binding to your &lt;code&gt;wrangler.jsonc&lt;/code&gt; and send emails securely and with no risk of leaked credentials.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can use Workers to process incoming mail, store attachments in R2, and add tasks to Queues to get email sending off the hot path of your application. And you can use &lt;code&gt;wrangler&lt;/code&gt; to emulate Email Sending locally, allowing you to test your user journeys without jumping between tools and environments.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;In production, you have clear observability over your emails with bounce rates and delivery events. And, when a user reports a missing email, you can quickly dive into the delivery status to debug issues quickly and help get your user back on track.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Weâre also making sure Email Service seamlessly fits into your existing applications. If you need to send emails from external services, you can do so using either REST APIs or SMTP. Likewise, if youâve been leaning on existing email frameworks (like React Email) to send rich, HTML-rendered emails to users, you can continue to use them with Email Service. Import the library, render your template, and pass it to the `send` method just as you would elsewhere.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { render, pretty, toPlainText } from '@react-email/render';
import { SignupConfirmation } from './templates';

export default {
  async fetch(request, env, ctx) {

    // Convert React Email template to html
    const html = await pretty(await render(&amp;lt;SignupConfirmation url="https://your-domain.com/confirmation-id"/&amp;gt;));

    // Use the Email Sending binding to send emails
    await env.SEND_EMAIL.send({
      to: [{ email: "[email protected]" }],
      from: { email: "[email protected]", name: "Welcome" },
      subject: "Signup Confirmation",
      html,
      text: toPlainText(html)
    });

    return new Response(`Successfully sent email!`);
  }
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Email Routing and Email Sending: Better together&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Sending email is only half the story. Applications often need to receive and parse emails to create powerful workflows. By combining Email Sending with our existing Email Routing capabilities, we're providing a complete, end-to-end solution for all your application's email needs.&lt;/p&gt;
      &lt;p&gt;Email Routing allows you to create custom email addresses on your domain and handle incoming messages programmatically with a Worker, which can enable powerful application flows such as:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Using Workers AI to parse, summarize and even label incoming emails: flagging security events from customers, early signs of a bug or incident, and/or generating automatic responses based on those incoming emails.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Creating support tickets in systems like JIRA or Linear from emails sent to &lt;code&gt;[email protected]&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Processing invoices sent to &lt;code&gt;[email protected]&lt;/code&gt; and storing attachments in R2.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;To use Email Routing, add the &lt;code&gt;email&lt;/code&gt; handler to your Worker application and process it as needed:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
  // Create an email handler to process emails delivered to your Worker
  async email(message, env, ctx) {

    // Classify incoming emails using Workers AI
    const { score, label } = env.AI.run("@cf/huggingface/distilbert-sst-2-int8", { text: message.raw" })

    env.PROCESSED_EMAILS.send({score, label, message});
  },
};  &lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;When you combine inbound routing with outbound sending, you can close the loop entirely within Cloudflare. Imagine a user emails your support address. A Worker can receive the email, parse its content, call a third-party API to create a ticket, and then use the Email Sending binding to send an immediate confirmation back to the user with their ticket number. Thatâs the power of a unified Email Service.&lt;/p&gt;
      &lt;p&gt;Email Sending will require a paid Workers subscription, and we'll be charging based on messages sent. We're still finalizing the packaging, and we'll update our documentation, changelog, and notify users as soon as we have final pricing and long before we start charging. Email Routing limits will remain unchanged.&lt;/p&gt;
      &lt;p&gt;Email is core to your application today, and it's becoming essential for the next generation of AI agents, background tasks, and automated workflows. We built the Cloudflare Email Service to be the engine for this new era of applications, weâll be making it available in private beta this November.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Interested in Email Sending? Sign up to the waitlist here.Â &lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Want to start processing inbound emails? Get started with Email Routing, which is available now, remains free and will be folded into the new email sending APIs coming.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Weâre excited to be adding Email Service to our Developer Platform, and weâre looking forward to seeing how you reimagine user experiences that increasingly rely on emails!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45373081</guid><pubDate>Thu, 25 Sep 2025 14:33:50 +0000</pubDate></item><item><title>This month in Servo: variable fonts, network tools, SVG</title><link>https://servo.org/blog/2025/09/25/this-month-in-servo/</link><description>&lt;doc fingerprint="6286a4978805acc4"&gt;
  &lt;main&gt;
    &lt;p&gt;Another month, another record number of pull requests merged! August flew by, and with it came 447 pull requests from Servo contributors. It was also the final month of our Outreachy cohort; you can read Jerens’ and Uthman’s blogs to learn about how it went!&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlights&lt;/head&gt;
    &lt;p&gt;Our big new feature this month is rendering inline SVG elements (@mukilan, @Loirooriol, #38188, #38603). This improves the appearance of many popular websites.&lt;/p&gt;
    &lt;p&gt;We have implemented named grid line lines and areas (@nicoburns, @loirooriol, #38306, #38574, #38493), still gated behind the &lt;code&gt;layout_grid_enabled&lt;/code&gt; preference (#38306, #38574).&lt;/p&gt;
    &lt;p&gt;Servo now supports CSS ‘font-variation-settings’ on all main desktop platforms (@simonwuelker, @mrobinson, #38642, #38760, #38831). This feature is currently gated behind the &lt;code&gt;layout_variable_fonts_enabled&lt;/code&gt; preference.
We also respect &lt;code&gt;format(*-variations)&lt;/code&gt; inside &lt;code&gt;@font-face&lt;/code&gt; rules (@mrobinson, #38832).
Additionally, Servo now reads data from OpenType Collection (.ttc) system font files on macOS (@nicoburns, #38753), and uses &lt;code&gt;Helvetica&lt;/code&gt; for the ‘system-ui’ font (@dpogue, #39001).&lt;/p&gt;
    &lt;p&gt;Our developer tools continue to make progress! We now have a functional network monitor panel (@uthmaniv, @jdm, #38216, #38601, #38625), and our JS debugger can show potential breakpoints (@delan, @atbrakhi, #38331, #38363, #38333, #38551, #38550, #38334, #38624, #38826, #38797). Additionally, the layout inspector now dims nodes that are not displayed (@simonwuelker, #38575).&lt;/p&gt;
    &lt;p&gt;We’ve fixed a significant source of crashes in the engine: hit testing using outdated display lists (issue #37932). Hit testing in a web rendering engine is the process that determines which element(s) the user’s mouse is hovering over.&lt;/p&gt;
    &lt;p&gt;Previously, this process ran inside of WebRender, which receives a display list representing what should be rendered for a particular page. WebRender runs on a separate thread or process from the actual page content, so display lists are updated asynchronously. By the time we do a hit test, the elements reported may not exist anymore, so we could trigger crashes by (for example) moving the mouse quickly over parts of the page that were rapidly changing.&lt;/p&gt;
    &lt;p&gt;This was fixed by making the hit test operation synchronous and moving it into the same thread as the actual content being tested against, eliminating the possibility of outdated results (@mrobinson, @Loirooriol, @kongbai1996, @yezhizhen, #38480, #38464, #38463, #38884, #38518).&lt;/p&gt;
    &lt;head rend="h2"&gt;Web platform support&lt;/head&gt;
    &lt;head rend="h3"&gt;DOM &amp;amp; JS&lt;/head&gt;
    &lt;p&gt;We’ve upgraded to SpiderMonkey v140 (changelog) (@jdm, #37077, #38563).&lt;/p&gt;
    &lt;p&gt;Numerous pieces of the Trusted Types API are now present in Servo (@TimvdLippe, @jdm, #38595, #37834, #38700, #38736, #38718, #38784, #38871, #8623, #38874, #38872, #38886), all gated behind the &lt;code&gt;dom_trusted_types_enabled&lt;/code&gt; preference.&lt;/p&gt;
    &lt;p&gt;The IndexedDB implementation (gated behind &lt;code&gt;dom_indexeddb_enabled&lt;/code&gt;) is progressing quickly (@arihant2math, @jdm, @rodion, @kkoyung, #28744, #38737, #38836, #38813, #38819, #38115, #38944, #38740, #38891, #38723, #38850, #38735), now reporting errors via &lt;code&gt;IDBRequest&lt;/code&gt; interface and supporting autoincrement keys.&lt;/p&gt;
    &lt;p&gt;A prototype implementation of the CookieStore API is now implemented and gated by the &lt;code&gt;dom_cookiestore_enabled&lt;/code&gt; preference (@sebsebmc, #37968, #38876).&lt;/p&gt;
    &lt;p&gt;Servo now passes over 99.6% of the CSS geometry test suite, thanks to an implementation of matrixTransform() on DOMPointReadOnly, making all geometry interfaces serializable, and adding the SVGMatrix and SVGPoint aliases (@lumiscosity, #38801, #38828, #38810).&lt;/p&gt;
    &lt;p&gt;You can now use the TextEncoderStream API (@minghuaw, #38466). Streams that are piped now correctly pass through &lt;code&gt;undefined&lt;/code&gt; values, too (@gterzian, #38470).
We also fixed a crash in the result of pipeTo() on ReadableStream (@gterzian, #38385).&lt;/p&gt;
    &lt;p&gt;We’ve implemented getModifierState() on MouseEvent (@PotatoCP, #38535), and made a number of changes involving DOM events: ‘mouseleave’ events are fired when the pointer leaves an &amp;lt;iframe&amp;gt; (@mrobinson, @Loirooriol, #38539), pasting from the clipboard into a text input triggers an ‘input’ event (@mrobinson, #37100), focus now occurs after ‘mousedown’ instead of ‘click’ (@yezhizhen, #38589), we ignore ‘mousedown’ and ‘mouseup’ events for elements that are disabled (@yezhizhen, #38671), and removing an event handler attribute like ‘onclick’ clears all relevant event listeners (@TimvdLippe, @kotx, #38734, #39011).&lt;/p&gt;
    &lt;p&gt;Servo now supports scrollIntoView() (@abdelrahman1234567, #38230), and fires a ‘scroll’ event whenever a page is scrolled (@stevennovaryo, #38321). You can now focus an element without scrolling, by passing the &lt;code&gt;{preventScroll: true}&lt;/code&gt; option to focus() (@abdelrahman1234567, #38495).&lt;/p&gt;
    &lt;p&gt;navigator.sendBeacon() is now implemented, gated behind the &lt;code&gt;dom_navigator_sendbeacon_enabled&lt;/code&gt; preference (@TimvdLippe, #38301).
Similarly, the AbortSignal.abort() static method is hidden behind &lt;code&gt;dom_abort_controller_enabled&lt;/code&gt; (@Taym95, #38746).&lt;/p&gt;
    &lt;p&gt;The HTMLDocument interface now exists as a property on the &lt;code&gt;Window&lt;/code&gt; object (@leo030303, #38433).
Meanwhile, the CSS window property is now a WebIDL namespace (@simonwuelker, #38579).
We also implemented the new QuotaExceededError interface (@rmeno12, #38507, #38720), which replaces previous usages of DOMException with the &lt;code&gt;QUOTA_EXCEEDED_ERR&lt;/code&gt; name.&lt;/p&gt;
    &lt;p&gt;Our 2D canvas implementation now supports addPath() on Path2D (@arthmis, #37838) and the restore() methods on CanvasRenderingContext2D and OffscreenCanvas now pop all applied clipping paths (@sagudev, #38496). Additionally, we now support using web fonts in the 2D canvas (@mrobinson, #38979). Meanwhile, the performance continues to improve in the new Vello-based backends (@sagudev, #38406, #38356, #38440, #38437), with asynchronous uploading also showing improvements (@sagudev, @mrobinson, #37776).&lt;/p&gt;
    &lt;p&gt;Muting media elements with the ‘mute’ HTML attribute now works during the initial resource load (@rayguo17, @jschwe, #38462).&lt;/p&gt;
    &lt;p&gt;Modifying stylesheets now integrates better with incremental layout, in both light trees and shadow trees (@coding-joedow, #38530, #38529). Note that calling setProperty() on a readonly CSSStyleDeclaration correctly throws an exception (@simonwuelker, #38677).&lt;/p&gt;
    &lt;head rend="h3"&gt;CSS&lt;/head&gt;
    &lt;p&gt;We’ve upgraded to the upstream Stylo revision as of August 1, 2025.&lt;/p&gt;
    &lt;p&gt;We now support custom CSS properties with the CSS.registerProperty() method (@simonwuelker, #38682), as well as custom element states with the ‘states’ property on ElementInternals (@simonwuelker, #38564).&lt;/p&gt;
    &lt;p&gt;Flexbox cross sizes can no longer end up negative through stretching (@Loirooriol, #38521), while ‘stretch’ on flex items now stretches to the line if possible (@Loirooriol, #38526).&lt;/p&gt;
    &lt;p&gt;Overflow calculations are more accurate, now that we ignore ‘position: fixed’ children of the root element (@stevennovaryo, #38618), compute overflow for &amp;lt;body&amp;gt; separate from the viewport (@shubhamg13, #38825), check for ‘overflow: visible’ in parents and children (@shubhamg13, #38443), and propagate ‘overflow’ to the viewport correctly (@shubhamg13, @Loirooriol, #38598).&lt;/p&gt;
    &lt;p&gt;‘color’ and ‘text-decoration’ properties no longer inherit into the contents of &amp;lt;select&amp;gt; elements (@simonwuelker, #38570).&lt;/p&gt;
    &lt;p&gt;Negative outline offsets work correctly (@lumiscosity, @mrobinson, #38418).&lt;/p&gt;
    &lt;p&gt;Video elements no longer fall back to a preferred aspect ratio of 2 (@Loirooriol, #38705).&lt;/p&gt;
    &lt;p&gt;‘position: sticky’ elements are handled correctly inside CSS transforms (@mrobinson, @Loirooriol, #38391).&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance &amp;amp; Stability&lt;/head&gt;
    &lt;p&gt;We fixed several panics this month, involving IntersectionObserver and missing stacking contexts (@mrobinson, #38473), unpaintable canvases and text (@gterzian, #38664), serializing ‘location’ properties on Window objects (@jdm, #38709), and navigations canceled before HTTP headers are received (@gterzian, #38739).&lt;/p&gt;
    &lt;p&gt;We also fixed a number of performance pitfalls. The document rendering loop is now throttled to 60 FPS (@mrobinson, @Loirooriol, #38431), while animated images do less work when advancing the current frame (@mrobinson, #38857). In addition, elements with CSS images will not trigger page reflow until their image data is fully available (@coding-joedow, #38916).&lt;/p&gt;
    &lt;p&gt;Finally, we made improvements to memory usage and binary size. Inline stylesheets are now deduplicated, which can have a significant impact on pages with lots of form inputs or custom elements with common styles (@coding-joedow, #38540). We also removed many unused pieces of the ICU library, saving 16MB from the final binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Embedding&lt;/head&gt;
    &lt;p&gt;Servo has declared a Minimum Supported Rust Version (1.85.0), and this is verified with every new pull request (@jschwe, #37152).&lt;/p&gt;
    &lt;p&gt;Evaluating JS from the embedding layer now reports an error if the evaluation failed for any reason (@rodio, #38602).&lt;/p&gt;
    &lt;p&gt;Our WebDriver implementation now passes 80% of the implementation conformance tests. This is the result of lots of work on handling user prompts (@PotatoCP, #38591), computing obscured/disabled elements while clicking (@yezhizhen, #38497, #38841, #38436, #38490, #38383), and improving window focus behaviours (@yezhizhen, #38889, #38909). We also implemented the Get Window Handles command (@longvatrong111, @yezhizhen, #38622, #38745), added support for getting element boolean attributes (@kkoyung, #38401), and added more accurate errors for a number of commands (@yezhizhen, @longvatrong111, #38620, #38357). The Element Clear command now clears &lt;code&gt;&amp;lt;input type="file"&amp;gt;&lt;/code&gt; elements correctly (@PotatoCP, #38536), and Element Send Keys now appends to file inputs with the ‘multiple’ attribute.&lt;/p&gt;
    &lt;head rend="h2"&gt;servoshell&lt;/head&gt;
    &lt;p&gt;We now display favicons of each top-level page in the tab bar (@simonwuelker, #36680).&lt;/p&gt;
    &lt;p&gt;Resizing the browser window to a very small dimension no longer crashes the browser (@leo030303, #38461). Element hit testing in full screen mode now works as expected (@yezhizhen, #38328).&lt;/p&gt;
    &lt;p&gt;Various popup dialogs, such as the &amp;lt;select&amp;gt; option chooser dialog, can now be closed without choosing a value (@TimvdLippe, #38373, #38949). Additionally, the browser now responds to a popup closing without any other inputs (@lumiscosity, #39038).&lt;/p&gt;
    &lt;head rend="h2"&gt;Donations&lt;/head&gt;
    &lt;p&gt;Thanks again for your generous support! We are now receiving 5552 USD/month (+18.3% over July) in recurring donations.&lt;/p&gt;
    &lt;p&gt;Historically this has helped cover the cost of our speedy CI servers and Outreachy interns. Thanks to your support, we’re now setting up two new CI servers for benchmarking, and funding the work of our long-time maintainer Josh Matthews (@jdm), with a particular focus on helping more people contribute to Servo.&lt;/p&gt;
    &lt;p&gt;Keep an eye out for further CI improvements in the coming months, including ten-minute WPT builds, macOS arm64 builds, and faster pull request checks.&lt;/p&gt;
    &lt;p&gt;Servo is also on thanks.dev, and already 15 GitHub users (−7 from July) that depend on Servo are sponsoring us there. If you use Servo libraries like url, html5ever, selectors, or cssparser, signing up for thanks.dev could be a good way for you (or your employer) to give back to the community.&lt;/p&gt;
    &lt;p&gt;As always, use of these funds will be decided transparently in the Technical Steering Committee. For more details, head to our Sponsorship page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45373501</guid><pubDate>Thu, 25 Sep 2025 15:02:55 +0000</pubDate></item><item><title>Microsoft blocks Israel's use of its tech. in mass surveillance of Palestinians</title><link>https://www.theguardian.com/world/2025/sep/25/microsoft-blocks-israels-use-of-its-technology-in-mass-surveillance-of-palestinians</link><description>&lt;doc fingerprint="ad20ab19270ca9c2"&gt;
  &lt;main&gt;
    &lt;p&gt;Microsoft has terminated the Israeli military’s access to technology it used to operate a powerful surveillance system that collected millions of Palestinian civilian phone calls made each day in Gaza and the West Bank, the Guardian can reveal.&lt;/p&gt;
    &lt;p&gt;Microsoft told Israeli officials late last week that Unit 8200, the military’s elite spy agency, had violated the company’s terms of service by storing the vast trove of surveillance data in its Azure cloud platform, sources familiar with the situation said.&lt;/p&gt;
    &lt;p&gt;The decision to cut off Unit 8200’s ability to use some of its technology results directly from an investigation published by the Guardian last month. It revealed how Azure was being used to store and process the trove of Palestinian communications in a mass surveillance programme.&lt;/p&gt;
    &lt;p&gt;In a joint investigation with the Israeli-Palestinian publication +972 Magazine and the Hebrew-language outlet Local Call, the Guardian revealed how Microsoft and Unit 8200 had worked together on a plan to move large volumes of sensitive intelligence material into Azure.&lt;/p&gt;
    &lt;p&gt;The project began after a meeting in 2021 between Microsoft’s chief executive, Satya Nadella, and the unit’s then commander, Yossi Sariel.&lt;/p&gt;
    &lt;p&gt;In response to the investigation, Microsoft ordered an urgent external inquiry to review its relationship with Unit 8200. Its initial findings have now led the company to cancel the unit’s access to some of its cloud storage and AI services.&lt;/p&gt;
    &lt;p&gt;Equipped with Azure’s near-limitless storage capacity and computing power, Unit 8200 had built an indiscriminate new system allowing its intelligence officers to collect, play back and analyse the content of cellular calls of an entire population.&lt;/p&gt;
    &lt;p&gt;The project was so expansive that, according to sources from Unit 8200 – which is equivalent in its remit to the US National Security Agency – a mantra emerged internally that captured its scale and ambition: “A million calls an hour.”&lt;/p&gt;
    &lt;p&gt;According to several sources, the enormous repository of intercepted calls – which amounted to as much as 8,000 terabytes of data – was held in a Microsoft datacentre in the Netherlands. Within days of the Guardian publishing the investigation, Unit 8200 appears to have swiftly moved the surveillance data out of the country.&lt;/p&gt;
    &lt;p&gt;According to sources familiar with the huge data transfer outside of the EU country, it occurred in early August. Intelligence sources said Unit 8200 planned to transfer the data to the Amazon Web Services cloud platform. Neither the Israel Defense Forces (IDF) nor Amazon responded to a request for comment.&lt;/p&gt;
    &lt;p&gt;The extraordinary decision by Microsoft to end the spy agency’s access to key technology was made amid pressure from employees and investors over its work for Israel’s military and the role its technology has played in the almost two-year offensive in Gaza.&lt;/p&gt;
    &lt;p&gt;A United Nations commission of inquiry recently concluded that Israel had committed genocide in Gaza, a charge denied by Israel but supported by many experts in international law.&lt;/p&gt;
    &lt;p&gt;The Guardian’s joint investigation prompted protests at Microsoft’s US headquarters and one of its European datacentres, as well as demands by a worker-led campaign group, No Azure for Apartheid, to end all ties to the Israeli military.&lt;/p&gt;
    &lt;p&gt;On Thursday, Microsoft’s vice-chair and president, Brad Smith, informed staff of the decision. In an email seen by the Guardian, he said the company had “ceased and disabled a set of services to a unit within the Israel ministry of defense”, including cloud storage and AI services.&lt;/p&gt;
    &lt;p&gt;Smith wrote: “We do not provide technology to facilitate mass surveillance of civilians. We have applied this principle in every country around the world, and we have insisted on it repeatedly for more than two decades.”&lt;/p&gt;
    &lt;p&gt;The decision brings to an abrupt end a three-year period in which the spy agency operated its surveillance programme using Microsoft’s technology.&lt;/p&gt;
    &lt;p&gt;Unit 8200 used its own expansive surveillance capabilities to intercept and collect the calls. The spy agency then used a customised and segregated area within the Azure platform, allowing for the data to be retained for extended periods of time and analysed using AI-driven techniques.&lt;/p&gt;
    &lt;p&gt;Although the initial focus of the surveillance system was the West Bank, where an estimated 3 million Palestinians live under Israeli military occupation, intelligence sources said the cloud-based storage platform had been used in the Gaza offensive to facilitate the preparation of deadly airstrikes.&lt;/p&gt;
    &lt;p&gt;The revelations highlighted how Israel has relied on the services and infrastructure of major US technology companies to support its bombardment of Gaza, which has killed more than 65,000 Palestinians, mostly civilians, and created a profound humanitarian and starvation crisis.&lt;/p&gt;
    &lt;p&gt;According to a document seen by the Guardian, a senior Microsoft executive told Israel’s ministry of defence late last week: “While our review is ongoing, we have at this juncture identified evidence that supports elements of the Guardian’s reporting.”&lt;/p&gt;
    &lt;p&gt;The executive told Israel officials that Microsoft “is not in the business of facilitating the mass surveillance of civilians” and notified them that it would “disable” access to services that supported the Unit 8200 surveillance project and suspend its use of some AI products.&lt;/p&gt;
    &lt;p&gt;The termination is the first known case of a US technology company withdrawing services provided to the Israeli military since the beginning of its war on Gaza.&lt;/p&gt;
    &lt;p&gt;The decision has not affected Microsoft’s wider commercial relationship with the IDF, which is a longstanding client and will retain access to other services. The termination will raise questions within Israel about the policy of holding sensitive military data in a third-party cloud hosted overseas.&lt;/p&gt;
    &lt;p&gt;Last month’s revelations about Unit 8200’s use of Microsoft technology followed an earlier investigation by the Guardian and its partners into the broader relationship between the company and the Israeli military.&lt;/p&gt;
    &lt;p&gt;That story, published in January and based on leaked files, showed how the IDF’s reliance on Azure and its AI systems surged in the most intensive phase of its Gaza campaign.&lt;/p&gt;
    &lt;p&gt;After that report, Microsoft launched its first review of how the IDF uses its services. It said in May it had “found no evidence to date” the military had failed to comply with its terms of service, or used Azure and its AI technology “to target or harm people” in Gaza.&lt;/p&gt;
    &lt;p&gt;However, the Guardian investigation with +972 and Local Call published in August, which revealed the cloud-based surveillance project had been used to research and identify bombing targets in Gaza, led the company to reassess its conclusions.&lt;/p&gt;
    &lt;p&gt;The disclosures caused alarm among senior Microsoft executives, sparking concerns that some of its Israel-based employees may not have been fully transparent about their knowledge of how Unit 8200 used Azure when questioned as part of the review.&lt;/p&gt;
    &lt;p&gt;The company said its executives, including Nadella, were not aware Unit 8200 planned to use, or ultimately used, Azure to store the content of intercepted Palestinian calls.&lt;/p&gt;
    &lt;p&gt;Microsoft then launched its second and more targeted review, which was overseen by lawyers at the US firm Covington &amp;amp; Burling. In his note to staff, Smith said the inquiry had not accessed any customer data but its findings were based on a review of internal Microsoft documents, emails and messages between staff.&lt;/p&gt;
    &lt;p&gt;“I want to note our appreciation for the reporting of the Guardian,” Smith wrote, noting that it had brought to light “information we could not access in light of our customer privacy commitments”. He added: “Our review is ongoing.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45373564</guid><pubDate>Thu, 25 Sep 2025 15:06:47 +0000</pubDate></item><item><title>FTC Secures Historic $2.5B Settlement Against Amazon</title><link>https://www.ftc.gov/news-events/news/press-releases/2025/09/ftc-secures-historic-25-billion-settlement-against-amazon</link><description>&lt;doc fingerprint="2c79c358257e2788"&gt;
  &lt;main&gt;
    &lt;p&gt;The Federal Trade Commission has secured a historic order with Amazon.com, Inc., as well as Senior Vice President Neil Lindsay and Vice President Jamil Ghani, settling allegations that Amazon enrolled millions of consumers in Prime subscriptions without their consent, and knowingly made it difficult for consumers to cancel. Amazon will be required to pay a $1 billion civil penalty, provide $1.5 billion in refunds back to consumers harmed by their deceptive Prime enrollment practices, and cease unlawful enrollment and cancellation practices for Prime.&lt;/p&gt;
    &lt;p&gt;“Today, the Trump-Vance FTC made history and secured a record-breaking, monumental win for the millions of Americans who are tired of deceptive subscriptions that feel impossible to cancel,” said FTC Chairman Andrew N. Ferguson. “The evidence showed that Amazon used sophisticated subscription traps designed to manipulate consumers into enrolling in Prime, and then made it exceedingly hard for consumers to end their subscription. Today, we are putting billions of dollars back into Americans’ pockets, and making sure Amazon never does this again. The Trump-Vance FTC is committed to fighting back when companies try to cheat ordinary Americans out of their hard-earned pay.”&lt;/p&gt;
    &lt;p&gt;The FTC has charged Amazon and several Amazon executives with knowingly misleading millions of consumers into enrolling in Prime, violating the FTC Act and the Restore Online Shoppers’ Confidence Act (ROSCA). The FTC alleged Amazon created confusing and deceptive user interfaces to lead consumers to enroll in Prime without their knowledge. Compounding these deceptive enrollment practices, Amazon also created a complex and difficult process for consumers seeking to cancel their Prime subscription, with the goal of preventing consumers from cancelling Prime. Amazon documents discovered in the lead up to trial showed that Amazon executives and employees knowingly discussed these unlawful enrollment and cancellation issues, with comments like “subscription driving is a bit of a shady world” and leading consumers to unwanted subscriptions is “an unspoken cancer.”&lt;/p&gt;
    &lt;p&gt;The historic monetary judgment contained in the settlement is only the third ROSCA case in which the FTC has obtained a civil penalty. It includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a $1 billion civil penalty, which is the largest ever in a case involving an FTC rule violation;&lt;/item&gt;
      &lt;item&gt;$1.5 billion in consumer redress, providing full relief for the estimated 35 million consumers impacted by unwanted Prime enrollment or deferred cancellation. This is the second-highest restitution award ever obtained by FTC action.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, the settlement requires Amazon to stop their unlawful practices and make meaningful changes to the Prime enrollment and cancellation flows by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;including a clear and conspicuous button for customers to decline Prime. Amazon can no longer have a button that says, “No, I don’t want Free Shipping.”&lt;/item&gt;
      &lt;item&gt;Including clear and conspicuous disclosures about all material terms of Prime during the Prime enrollment process, such as the cost, the date and frequency of charges to consumers, whether the subscription auto-renews, and cancellation procedures.&lt;/item&gt;
      &lt;item&gt;creating an easy way for consumers to cancel Prime, using the same method that consumers used to sign up. The process cannot be difficult, costly, or time-consuming and must be available using the same method that consumers used to sign up; and&lt;/item&gt;
      &lt;item&gt;paying for an independent, third-party supervisor to monitor Amazon’s compliance with the consumer redress distribution process.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Commission vote approving the stipulated final order was 3-0. The FTC filed the proposed order in the U.S. District Court for the Western District of Washington.&lt;/p&gt;
    &lt;p&gt;NOTE: Stipulated final orders have the force of law when approved and signed by the District Court judge.&lt;/p&gt;
    &lt;p&gt;The Federal Trade Commission works to promote competition and protect and educate consumers. The FTC will never demand money, make threats, tell you to transfer money, or promise you a prize. Learn more about consumer topics at consumer.ftc.gov, or report fraud, scams, and bad business practices at ReportFraud.ftc.gov. Follow the FTC on social media, read consumer alerts and the business blog, and sign up to get the latest FTC news and alerts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45374064</guid><pubDate>Thu, 25 Sep 2025 15:35:19 +0000</pubDate></item><item><title>ChatControl: EU wants to scan all private messages, even in encrypted apps</title><link>https://metalhearf.fr/posts/chatcontrol-wants-your-private-messages/</link><description>&lt;doc fingerprint="b516b17038a1bfe5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction #&lt;/head&gt;
    &lt;p&gt;The 🇪🇺 European Union is advancing legislation that could fundamentally change how we communicate online. ChatControl would require all messaging platforms to automatically scan their users’ private messages and images.&lt;/p&gt;
    &lt;p&gt;Yes, even encrypted ones like Signal, WhatsApp and Telegram. No, you can’t opt out.&lt;/p&gt;
    &lt;p&gt;This isn’t just another privacy policy update you can ignore. If passed, this EU regulation (strongest and most binding legal instrument in EU law) would automatically apply to all member states without any wiggle room for national interpretation. It would even override constitutional protections for communication privacy and establish unprecedented mass surveillance of private communications.&lt;/p&gt;
    &lt;p&gt;The official justification? Fighting child sexual abuse material (CSAM). Protecting children is undeniably crucial, but the proposed methods would eliminate digital privacy for 450 million Europeans and set a global precedent for mass surveillance.&lt;/p&gt;
    &lt;p&gt;This surveillance trend extends beyond Europe: 🇨🇭 Switzerland is advancing metadata retention requirements, the 🇬🇧 UK is implementing comprehensive age verification systems and now the 🇪🇺 EU proposes to scan every private message. Each initiative is positioned as child protection policy, but the implications reach far beyond their stated goals.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is ChatControl #&lt;/head&gt;
    &lt;p&gt;ChatControl is what critics call the EU’s proposed Regulation to Prevent and Combat Child Sexual Abuse, also known as CSAR (Child Sexual Abuse Regulation).&lt;/p&gt;
    &lt;p&gt;The proposal builds on surveillance techniques already deployed by major tech companies. Meta analyzes all Facebook Messenger conversations and unencrypted WhatsApp data (profile photos, group descriptions). Apple announced similar scanning for iCloud content in 2021, though they later suspended the program.&lt;/p&gt;
    &lt;p&gt;This turns voluntary corporate surveillance into mandatory government-ordered scanning. A temporary 2021 EU regulation allowed platforms to scan content voluntarily for three years. That authorization expired in 2024, which is why CSAR was proposed. The temporary regulation merely permitted scanning; CSAR would make detection obligatory under certain conditions.&lt;/p&gt;
    &lt;p&gt;There’s also the Roadmap for Lawful Access to Data which has an even bigger goal: making all our digital data readable by authorities upon request. We’ll dive deeper into this broader surveillance agenda later.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scope and Coverage #&lt;/head&gt;
    &lt;p&gt;CSAR casts an extremely wide net. The regulation would apply to all interpersonal communication service providers, not just obvious targets like Signal, WhatsApp, or Telegram, but also:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Email providers&lt;/item&gt;
      &lt;item&gt;Dating apps&lt;/item&gt;
      &lt;item&gt;Gaming platforms with chat features&lt;/item&gt;
      &lt;item&gt;Social media platforms&lt;/item&gt;
      &lt;item&gt;File hosting services (Google Drive, iCloud, DropBox…)&lt;/item&gt;
      &lt;item&gt;App stores&lt;/item&gt;
      &lt;item&gt;Even small community hosting services run by associations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means virtually any digital service that allows people to communicate or share content would fall under surveillance requirements. The scope extends far beyond what most people imagine when they hear messaging apps.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it Works #&lt;/head&gt;
    &lt;p&gt;ChatControl relies on Client-Side Scanning. Your device becomes a monitoring station that analyzes your content before encryption happens.&lt;/p&gt;
    &lt;p&gt;This represents a fundamental shift away from traditional surveillance that intercepts messages during transmission. With ChatControl, every message gets automatically checked, assuming everyone is guilty until proven innocent and effectively reversing the presumption of innocence.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technical Implementation #&lt;/head&gt;
    &lt;p&gt;The system would automatically scan for three categories of content before encryption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Known illegal content: Images or videos already catalogued by authorities as CSAM. Your device creates hash fingerprints of your content and compares them against databases of known illegal material.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Unknown potential content: Photos or videos that might constitute CSAM but haven’t been previously identified. AI algorithms analyze visual elements (like exposed skin) to flag potentially problematic content based on statistical models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Grooming behavior: Text analysis using AI to identify communication patterns that match predefined indicators of adults soliciting children. This involves scanning the actual content of your private conversations.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If something gets flagged, it automatically gets reported to authorities. No human checks it first, that would be impossible given the billions of daily messages. This would be mandatory for all messaging platforms in 🇪🇺 Europe.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why This Breaks Encryption #&lt;/head&gt;
    &lt;p&gt;ChatControl doesn’t break encryption, it bypasses it entirely. While your messages still get encrypted during transmission, the system defeats the purpose of end-to-end encryption by examining your content before it gets encrypted. True E2EE means only you and your recipient can read messages: no government, no company, no algorithm should peek inside. This surveillance violates that principle by inserting monitoring at the source.&lt;/p&gt;
    &lt;p&gt;Privacy-focused companies like Proton point out this approach might be worse than encryption backdoors. Backdoors give authorities access to communications you share with others. This system examines everything on your device, whether you share it or not.&lt;/p&gt;
    &lt;p&gt;Your encrypted messaging app becomes spyware. Supporters claim this protects privacy because scanning happens locally, but surveillance built into your device makes it impossible to escape.&lt;/p&gt;
    &lt;head rend="h3"&gt;Governance Structure #&lt;/head&gt;
    &lt;p&gt;The proposal would create a centralized EU Centre on Child Sexual Abuse to receive all reports, but EU institutions wouldn’t control the scanning technology itself.&lt;/p&gt;
    &lt;p&gt;Service providers would face additional obligations beyond scanning. They would need to conduct risk assessments to evaluate and minimize the potential for illegal content sharing on their platforms. This requires collecting detailed information about their users (age groups, content types) that many privacy-focused services deliberately avoid gathering.&lt;/p&gt;
    &lt;p&gt;The regulation also pushes for mandatory age verification systems. No viable, privacy-respecting age verification technology currently exists. These systems would eliminate online anonymity, requiring users to prove their identity to access digital services.&lt;/p&gt;
    &lt;head rend="h2"&gt;Real-World Impact #&lt;/head&gt;
    &lt;head rend="h3"&gt;Encryption Concerns #&lt;/head&gt;
    &lt;p&gt;ChatControl fits into a broader political strategy. Since the 1990s crypto wars, certain states have argued that privacy-protecting technologies, especially encryption, obstruct police investigations. These technologies are designed to do exactly that, protect everyone’s ability to control their expression and communication.&lt;/p&gt;
    &lt;p&gt;The European Commission’s Roadmap for Lawful Access to Data wants to make all digital data accessible to authorities by 2030. This involves systematically weakening encryption rather than simply bypassing it.&lt;/p&gt;
    &lt;p&gt;Edward Snowden’s revelations ten years ago led to widespread adoption of encryption and institutional consensus supporting the right to encrypted communication. But governments remain frustrated by their inability to access private communications. We’re seeing a return to authoritarian positions using terrorism, organized crime and child exploitation as justifications for undermining encryption.&lt;/p&gt;
    &lt;p&gt;🇩🇰 Danish Minister of Justice Peter Hummelgaard, chief architect of the current ChatControl proposal, recently stated: “We must break with the totally erroneous perception that it is everyone’s civil liberty to communicate on encrypted messaging services.” Well, there you have it folks: encrypted communication isn’t a civil liberty anymore. You cypherpunks were wrong all along. /s&lt;/p&gt;
    &lt;p&gt;Similarly in 🇫🇷 France, both Bernard Cazeneuve and Emmanuel Macron have explicitly stated their desire to control encrypted messaging, seeking to pierce the privacy of millions who use these services.&lt;/p&gt;
    &lt;p&gt;CSAR provides the perfect opportunity for member states to finally design and implement a generalized surveillance tool for monitoring population communications. Crossing this threshold means eliminating all confidentiality from communications using digital infrastructure.&lt;/p&gt;
    &lt;head rend="h3"&gt;False Positives #&lt;/head&gt;
    &lt;p&gt;These scanning systems get it wrong most of the time. Studies show approximately 80% of algorithmic reports are false positives: innocent content incorrectly flagged as illegal. 🇮🇪 Irish law enforcement confirms this: only 20.3% of 4,192 automated reports actually contained illegal material.&lt;/p&gt;
    &lt;p&gt;Even with hypothetical 99% accuracy (which current systems don’t achieve), scanning billions of daily messages would generate millions of false accusations. Police resources would be overwhelmed investigating innocent families sharing vacation photos while real crimes go uninvestigated.&lt;/p&gt;
    &lt;p&gt;Innocent content regularly triggers these systems: family photos, teenage conversations, educational materials and medical communications. Consider this real case: a father was automatically reported to police after sending photos of his child’s medical condition to their doctor. Google’s algorithms flagged this legitimate medical consultation as potential abuse, permanently closed his account and refused all appeals. His digital life was destroyed by an algorithm that couldn’t distinguish between medical care and criminal activity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scientific Opposition #&lt;/head&gt;
    &lt;p&gt;For the third time in three years, over 600 cryptographers, security researchers and scientists across 35 countries have co-signed an open letter explaining why this mass scanning project is “technically unfeasible”, constitutes a “danger to democracy” and would “completely compromise” the security and privacy of all European citizens.&lt;/p&gt;
    &lt;p&gt;The letter emphasizes that client-side scanning cannot distinguish between legal and illegal content without fundamentally breaking encryption and creating vulnerabilities that malicious actors can exploit.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the Commission has provided no serious studies demonstrating the effectiveness, reliability or appropriateness of these intrusive measures for actually protecting children. Industry claims appear to have taken precedence over evidence-based policy-making.&lt;/p&gt;
    &lt;p&gt;Genuine security emerges through thoughtful design where security measures and civil liberties function as complementary forces, not opposing ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;Easily Defeated #&lt;/head&gt;
    &lt;p&gt;The fundamental flaw in ChatControl becomes clear when examining how easily determined actors can circumvent these scanning systems. Criminals don’t need sophisticated techniques to bypass client-side scanning; they use well-documented public knowledge already employed by malicious actors.&lt;/p&gt;
    &lt;p&gt;Layered Encryption&lt;lb/&gt; Encrypt files with standard tools like GPG before messaging. Hell, even a basic Caesar cipher would be sufficient to bypass detection. Since client-side scanning occurs after user encryption but before transport encryption, pre-encrypted content looks like random data to detection algorithms. Recipients decrypt locally with shared keys.&lt;/p&gt;
    &lt;p&gt;External Platform Bypass&lt;lb/&gt; Upload content to any third-party platform (Dropbox, OneDrive, anonymous file hosts, or obscure hosting services) and share links instead of files. The scanner sees innocent text containing a URL while the actual content sits untouched on external servers.&lt;/p&gt;
    &lt;p&gt;Custom Messaging Clients&lt;lb/&gt; Open-source protocols like XMPP and Matrix allow custom client development. Modified clients can automatically implement cloud storage and encryption workflows transparently. Users experience normal messaging while completely evading surveillance infrastructure.&lt;/p&gt;
    &lt;p&gt;Digital Steganography&lt;lb/&gt; Steganographic techniques embed data within innocent images. Family photos can carry hidden payloads invisible to both human operators and AI systems. Tools like OpenStego make this accessible to average users.&lt;/p&gt;
    &lt;p&gt;Platform Migration&lt;lb/&gt; Criminal networks can shift to decentralized platforms, peer-to-peer networks or services outside EU jurisdiction. Tor-based messaging, blockchain communications or servers in non-compliant countries remain beyond ChatControl’s reach.&lt;/p&gt;
    &lt;p&gt;ChatControl catches only amateur criminals who directly attach problematic content to messages. Professional networks already employ these evasion techniques as standard practice. EU legislation won’t make them forget how computers work.&lt;/p&gt;
    &lt;p&gt;The system fails at protecting children while succeeding at mass civilian monitoring. It’s not a bug, it’s a feature.&lt;/p&gt;
    &lt;head rend="h2"&gt;Business Interests #&lt;/head&gt;
    &lt;head rend="h3"&gt;Industry Players #&lt;/head&gt;
    &lt;p&gt;The child protection narrative masks concerning business interests. The European Commission based its CSAR proposal primarily on claims from industry players rather than independent research.&lt;/p&gt;
    &lt;p&gt;Commercial surveillance companies would manage the technology with guaranteed access to the European market. Organizations like Thorn (co-founded by actor Ashton Kutcher), Microsoft’s PhotoDNA and other tech companies develop these detection systems while simultaneously lobbying for regulations that would require their adoption across Europe.&lt;/p&gt;
    &lt;p&gt;These companies develop the detection technologies and lobby for laws mandating their adoption, creating a profitable feedback loop. The proposal would secure privileged market positions for surveillance companies across hundreds of millions of European users. Pretty nice, isn’t it?&lt;/p&gt;
    &lt;p&gt;These systems would be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Proprietary: Built on closed-source code with methods hidden from public view&lt;/item&gt;
      &lt;item&gt;Unverifiable: Operating without meaningful external examination or accountability&lt;/item&gt;
      &lt;item&gt;Legally powerful: Capable of starting criminal proceedings through algorithmic decisions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Rhetorical Tactics #&lt;/head&gt;
    &lt;p&gt;Commissioner Ylva Johansson consistently emphasizes this narrative in her communications:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“[Privacy defenders make a lot of noise], but someone has to speak for the children.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;“Think of the children” is a well-documented political rhetoric technique that appeals to emotion rather than evidence. While child protection is genuinely important, this approach frames any opposition as being against child welfare, making nuanced discussion more difficult.&lt;/p&gt;
    &lt;p&gt;This creates a false choice. Privacy isn’t a luxury for troublemakers, it’s a fundamental right that protects journalists, whistleblowers, activists and ordinary people from unwarranted intrusion.&lt;/p&gt;
    &lt;p&gt;Critics aren’t opposing child protection. We’re questioning whether undermining privacy rights for 450 million 🇪🇺 Europeans is the most effective approach when targeted alternatives exist that preserve rights.&lt;/p&gt;
    &lt;head rend="h2"&gt;EU Country Positions #&lt;/head&gt;
    &lt;p&gt;Understanding how 🇪🇺 EU member states position themselves on this legislation is crucial, as their votes will determine whether ChatControl becomes reality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vote Breakdown #&lt;/head&gt;
    &lt;p&gt;Countries that support ChatControl (12): 🇧🇬 Bulgaria • 🇭🇷 Croatia • 🇨🇾 Cyprus • 🇩🇰 Denmark • 🇫🇷 France • 🇭🇺 Hungary • 🇮🇪 Ireland • 🇱🇹 Lithuania • 🇲🇹 Malta • 🇵🇹 Portugal • 🇷🇴 Romania • 🇪🇸 Spain&lt;/p&gt;
    &lt;p&gt;Countries that oppose ChatControl (7): 🇦🇹 Austria • 🇨🇿 Czech Republic • 🇪🇪 Estonia • 🇫🇮 Finland • 🇱🇺 Luxembourg • 🇳🇱 Netherlands • 🇵🇱 Poland&lt;/p&gt;
    &lt;p&gt;Countries still undecided (8): 🇧🇪 Belgium • 🇩🇪 Germany • 🇬🇷 Greece • 🇮🇹 Italy • 🇱🇻 Latvia • 🇸🇰 Slovakia • 🇸🇮 Slovenia • 🇸🇪 Sweden&lt;/p&gt;
    &lt;head rend="h3"&gt;National Stances #&lt;/head&gt;
    &lt;head&gt;💪 Strong opposition (the good guys) (click to expand)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;🇦🇹 Austria: Constitutional and privacy concerns.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇨🇿 Czech Republic: Prime Minister explicitly rejects proposals that would allow widespread monitoring of citizens’ private digital communications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇪🇪 Estonia: Acknowledges sincere concerns about child exploitation, but opposes undermining end-to-end encryption and forcing mass surveillance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇫🇮 Finland: Cannot support the latest compromise proposal because it contains a constitutionally problematic identification order.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇱🇺 Luxembourg: Rejects broad surveillance measures like client-side scanning and insists that EU regulation must ensure proportional, targeted detection to protect citizens’ fundamental rights.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇳🇱 Netherlands: Strong privacy protection stance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🇵🇱 Poland: Opposition to mass surveillance measures.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;🤷 Undecided positions (click to expand)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🇧🇪 Belgium: The N-VA party calls ChatControl a “monster that invades your privacy and cannot be tamed”. Despite this, Belgium backed Denmark’s compromise during September meetings. Mixed signals from Brussels.&lt;/item&gt;
      &lt;item&gt;🇩🇪 Germany: Won’t break encryption but wants to find middle ground. They’re trying to craft their own compromise instead of rejecting ChatControl outright. Germany’s fence-sitting could be decisive.&lt;/item&gt;
      &lt;item&gt;🇬🇷 Greece: Still figuring out the technical details. No clear stance yet.&lt;/item&gt;
      &lt;item&gt;🇮🇹 Italy: Has concerns about expanding the scope to cover new CSAM detection. Rome seems hesitant about how far this thing could reach.&lt;/item&gt;
      &lt;item&gt;🇱🇻 Latvia: The government likes what they see on paper but worries about political backlash after summer attention. Classic politicians hedging their bets.&lt;/item&gt;
      &lt;item&gt;🇸🇰 Slovakia: Playing the wait-and-see game. No commitment either way.&lt;/item&gt;
      &lt;item&gt;🇸🇮 Slovenia: Dealing with constitutional headaches around privacy. Another country wrestling with legal implications.&lt;/item&gt;
      &lt;item&gt;🇸🇪 Sweden: Stockholm is still reading the fine print. Taking their time to decide.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Current Status #&lt;/head&gt;
    &lt;p&gt;Current situation: Country positions continue shifting regularly since September 12. With 12 countries supporting, 7 opposing, and 8 undecided, ChatControl supporters still fall short of the 65% EU population threshold needed for a qualified majority. The opposition maintains enough demographic weight to block the proposal for now, but the situation remains fluid as the interim regulation approaches expiration.&lt;/p&gt;
    &lt;head&gt;📅 Timeline of Events (click to expand)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;ChatControl Proposal Introduced&lt;/head&gt;&lt;head rend="h3"&gt;May 11, 2022&lt;/head&gt;&lt;head rend="h4"&gt;European Commission&lt;/head&gt;The European Commission unveils the original ChatControl proposal, requiring all email and messaging providers to scan communications for child sexual abuse material.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Danish Presidency Takes Charge&lt;/head&gt;&lt;head rend="h3"&gt;Jul 1, 2025&lt;/head&gt;&lt;head rend="h4"&gt;EU Council Presidency&lt;/head&gt;🇩🇰 Denmark assumes the EU Council Presidency and immediately reintroduces ChatControl as a top legislative priority, targeting October 14, 2025 for adoption.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Support Momentum Builds&lt;/head&gt;&lt;head rend="h3"&gt;Jul 28, 2025&lt;/head&gt;&lt;head rend="h4"&gt;15 Member States&lt;/head&gt;Fifteen EU member states back the ChatControl proposal, reversing earlier resistance. 🇫🇷 France has shifted its position and now supports the proposal. 🇩🇪 Germany remains the crucial undecided vote.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Opposition Wave Begins&lt;/head&gt;&lt;head rend="h3"&gt;Aug 26, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Czech Republic&lt;/head&gt;🇨🇿 Czech Prime Minister Petr Fiala announces total opposition on behalf of the entire coalition government.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Constitutional Concerns&lt;/head&gt;&lt;head rend="h3"&gt;Aug 29, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Finland&lt;/head&gt;🇫🇮 Finland rejects the compromise proposal due to constitutionally problematic detection requirements.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Blocking Minority Secured&lt;/head&gt;&lt;head rend="h3"&gt;Sep 10, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Germany, Luxembourg, Slovakia&lt;/head&gt;🇩🇪 Germany, 🇱🇺 Luxembourg, and 🇸🇰 Slovakia officially oppose breaking encryption. This creates the blocking minority needed to stop the proposal.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Estonia Joins Opposition&lt;/head&gt;&lt;head rend="h3"&gt;Sep 14, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Privacy Protection&lt;/head&gt;🇪🇪 Estonia acknowledges child exploitation concerns but opposes undermining end-to-end encryption and mass surveillance.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Germany Wavers&lt;/head&gt;&lt;head rend="h3"&gt;Sep 16, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Position Unclear&lt;/head&gt;🇩🇪 Germany refrains from taking a definitive stance during the LEWP meeting, despite previous encryption concerns. Position becomes uncertain.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Three Countries Flip&lt;/head&gt;&lt;head rend="h3"&gt;Sep 23, 2025&lt;/head&gt;&lt;head rend="h4"&gt;Belgium, Latvia, Italy&lt;/head&gt;🇧🇪 Belgium, 🇱🇻 Latvia, and 🇮🇹 Italy have moved away from supporting the proposal and are now undecided. Country positions continue changing regularly since September 12.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Consequences #&lt;/head&gt;
    &lt;p&gt;The effects of these proposals go beyond individual privacy concerns.&lt;/p&gt;
    &lt;p&gt;Cybersecurity gets compromised&lt;lb/&gt; Adding deliberate vulnerabilities to encryption creates weaknesses that everyone can exploit. Any backdoor for authorized access becomes a potential entry point for criminals and foreign intelligence services. In February 2024, the 🇪🇺 European Court of Human Rights already determined that mandating weakened encryption “cannot be regarded as necessary in a democratic society”.&lt;/p&gt;
    &lt;p&gt;Innovation suffers&lt;lb/&gt; 🇪🇺 European cybersecurity companies would face an impossible situation in global markets. How could they credibly sell security solutions when regulations require them to build in access mechanisms that undermine those very protections?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Buy our ultra-secure encrypted stuff!” (Terms and conditions apply, government backdoors included)."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Tech companies will leave Europe&lt;lb/&gt; Privacy-focused services that moved to 🇪🇺 Europe after the Snowden revelations are already signaling they might leave. Signal has explicitly said it would stop operating in 🇪🇺 Europe rather than compromise its security.&lt;/p&gt;
    &lt;p&gt;Even 🇨🇭 Switzerland, traditionally seen as a privacy haven, is facing severe legislative pressures that are forcing tech companies to relocate. Proton has confirmed it has begun moving some of its physical infrastructure out of Switzerland due to “legal uncertainty” over the proposed surveillance law amendments. Lumo, their AI chatbot, became the first product to relocate, moving to Germany instead of Switzerland specifically because of these legislative concerns.&lt;/p&gt;
    &lt;p&gt;The Swiss OSCPT (Ordinance on the Surveillance of Correspondence by Post and Telecommunications) revision would require VPNs and messaging apps to identify users and retain data for up to six months, plus decrypt communications upon authority request. As Proton’s CEO Andy Yen explained, these are proposals that “have been outlawed in the EU” but could soon become reality in Switzerland.&lt;/p&gt;
    &lt;p&gt;Other privacy-focused providers like Tuta have expressed similar concerns and contingency plans to leave 🇨🇭 Switzerland if the surveillance laws pass.&lt;/p&gt;
    &lt;p&gt;Europe might become dependent on US surveillance&lt;lb/&gt; I’m not so sure on this one, but by outsourcing surveillance technology to American companies, 🇪🇺 Europe may create dangerous dependencies. These companies operate under 🇺🇸 US jurisdiction and the CLOUD Act, potentially allowing 🇺🇸 Washington to access data collected on 🇪🇺 European citizens. Under the pretense of child protection, the 🇪🇺 EU risks handing surveillance keys to foreign powers.&lt;/p&gt;
    &lt;p&gt;Social behavior changes&lt;lb/&gt; When people know they’re being watched, they change how they communicate. People start self-censoring, avoiding certain topics and carefully choosing their words even in private conversations.&lt;/p&gt;
    &lt;p&gt;This is called the chilling effect. Rights don’t disappear overnight: they erode gradually as people change their behavior to avoid potential problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Take Action #&lt;/head&gt;
    &lt;p&gt;Here’s how you can contribute to defending our digital freedoms:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Share this article and educate your network: Use hashtags like &lt;code&gt;#ChatControl&lt;/code&gt;or&lt;code&gt;#StopScanningMe&lt;/code&gt;. Forward resources to friends, family and colleagues.&lt;/item&gt;
      &lt;item&gt;Sign the petition: against ChatControl at change.org.&lt;/item&gt;
      &lt;item&gt;Stay informed and follow updates: @[email protected], x.com/nonchatcontrol, patrick-breyer.de and fightchatcontrol.eu.&lt;/item&gt;
      &lt;item&gt;Contact your national representatives to convince your country to oppose ChatControl, if it’s not already the case.&lt;/item&gt;
      &lt;item&gt;Join campaigns and support organizations: stopscanningme.eu for local actions, EFF and EDRi for digital rights advocacy.&lt;/item&gt;
      &lt;item&gt;Adopt privacy tools and infrastructure: Use Signal and other privacy-respecting alternatives. Host your own services or support privacy-focused providers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion #&lt;/head&gt;
    &lt;p&gt;The irony is kinda painful: the continent that built GDPR to protect digital privacy now designs ChatControl to dismantle it systematically. What was once a fundamental right could become mandatory surveillance.&lt;/p&gt;
    &lt;p&gt;ChatControl represents a historic choice for 🇪🇺 Europe. Either we become the first democracy to normalize mass surveillance of private communications or we defend the digital rights that made Europe a global privacy leader.&lt;/p&gt;
    &lt;p&gt;This decision deserves close attention: authoritarian regimes worldwide are watching, ready to justify their own programs with: “Eh, if Europe does it, why shouldn’t we?”&lt;/p&gt;
    &lt;p&gt;The next chapter unfolds on October 14, 2025. 😉&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45374500</guid><pubDate>Thu, 25 Sep 2025 16:01:41 +0000</pubDate></item><item><title>The Harvard-Emory ECG Database</title><link>https://bdsp.io/content/heedb/4.0/</link><description>&lt;doc fingerprint="25dd992d87ba80d0"&gt;
  &lt;main&gt;
    &lt;p&gt;Database Credentialed Access&lt;/p&gt;
    &lt;head rend="h1"&gt;Harvard-Emory ECG Database&lt;/head&gt;
    &lt;p&gt;Zuzana Koscova , Valdery Moura Junior , Matthew Reyna , Shenda Hong , Aditya Gupta , Manohar Ghanta , Reza Sameni , Aaron Aguirre , Qiao Li , Sahar Zafar , Gari Clifford , M Brandon Westover&lt;/p&gt;
    &lt;p&gt;Published: July 28, 2025. Version: 4.0&lt;/p&gt;
    &lt;p&gt; When using this resource, please cite: (show more options) &lt;lb/&gt;Koscova, Z., Moura Junior, V., Reyna, M., Hong, S., Gupta, A., Ghanta, M., Sameni, R., Aguirre, A., Li, Q., Zafar, S., Clifford, G., &amp;amp; Westover, M. B. (2025). Harvard-Emory ECG Database (version 4.0). Brain Data Science Platform. https://doi.org/10.60508/rv6h-7d10. &lt;/p&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;The Harvard-Emory ECG database (HEEDB) is a large collection of 12-lead electrocardiography (ECG) recordings, prepared through a collaboration between Harvard University and Emory University investigators.&lt;/p&gt;
    &lt;p&gt;In version 1.0 of the database, these ECGs from Massachusetts General Brigham hospital sites were provided without labels or metadata, to enable pre-training of ECG analysis models.&lt;/p&gt;
    &lt;p&gt;In version 2.0, metadata is included.&lt;/p&gt;
    &lt;p&gt;In version 3.0, Emory ECGs are included together with metadata, labels from the 12SL ECG analysis program (GE Healthcare ) and ICD-9/10 codes.&lt;/p&gt;
    &lt;p&gt;In version 4.0, typos were corrected in the data description.&lt;/p&gt;
    &lt;p&gt;HEEDB is published as part of the Human Sleep Project (HSP), funded by a grant (R01HL161253) from the National Heart Lung and Blood Institute (NHLBI) of the NIH to Massachusetts General Hospital, Emory University, Stanford University, Kaiser Permanente, Boston Children's Hospital, and Beth Israel Deaconess Medical Center.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;These ECG data include clinical ECGs captured during routine clinical care over several decades. These are intended to be used to determine associations between cardiac abnormalities (e.g. abnormal rhythms) and sleep, sleep-related medical conditions, and health outcomes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methods&lt;/head&gt;
    &lt;p&gt;The dataset consists of standard 12-lead ECG recordings, each 10 seconds long, acquired at sampling rates of 250 or 500 Hz. Collection began in the 1980s and continues to the present day. Version 4 of the database includes 10,471,531 ECGs from 1,818,247 unique patients at institution I0001 and 968,680 ECGs from 349,548 patients at institution I0006. All recordings were obtained as part of routine clinical care.&lt;/p&gt;
    &lt;p&gt;Data preprocessing: Data was de-identified following the Safe Harbor method.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data Description&lt;/head&gt;
    &lt;p&gt;ECG data is stored in WFDB (Waveform Database) and Matlab (V4) compatible format. Each ECG recording includes one waveform data file (.mat for I0001 and .dat for I0006) and one header file (.hea). The waveform data file can be read by WFDB library functions, applications, Toolbox, or be loaded to Matlab directly. Waveform files are 12-lead ECG signals recorded at 250 and 500 Hz for 10 s encoded in 16 bits. The header file specifies the name of the associated waveform file and its attributes including sampling rate, units, channel names and the signal gain. It contains line-oriented and field-oriented ASCII text and can be read by the WFDB library or generic text editors.&lt;/p&gt;
    &lt;p&gt;The directory structure of the HEEDB is organized as follows:&lt;/p&gt;
    &lt;code&gt;ECG/
├── I0006/
│   ├── 12SL_diagnoses/
│   │   ├── diagnoses.csv
│   │   ├── diagnoses_dictionary.csv
│   │   └── README
│   ├── ICD_codes/
│   │   ├── icd9_codes.csv
│   │   ├── icd10_codes.csv
│   │   └── README
│   ├── metadata/
│   │   ├── metadata.csv
│   │   └── README
│   └── WFDB/
│       ├── 2010/
│       ├── 2011/
│       ├── ...
│       └── 2018/
├── I0001/
│   ├── 12SL_diagnoses/
│   │   ├── diagnoses.csv
│   │   ├── diagnoses_dictionary.csv
│   │   └── README
│   ├── ICD_codes/
│   │   ├── icd9_codes.csv
│   │   ├── icd10_codes.csv
│   │   └── README
│   ├── metadata/
│   │   ├── metadata.csv
│   │   └── README
│   └── WFDB/
│       ├── S0001/
│       ├── S0002/
│       ├── S0003/
│       └── S0004/
&lt;/code&gt;
    &lt;p&gt;Each institution (I0001 and I0006) maintains its own subfolders for diagnoses, ICD codes, metadata, and waveform files. The WFDB/ directory contains the ECG waveform data organized either by year (I0006) or by session identifier (I0001).&lt;/p&gt;
    &lt;head rend="h3"&gt;12SL Diagnoses Description&lt;/head&gt;
    &lt;head rend="h3"&gt;The 12SL_diagnoses/ folder contains diagnostic outputs from 12SL (GE Healthcare) software, version 1.&lt;/head&gt;
    &lt;p&gt;File: diagnoses.csv&lt;lb/&gt; This file contains two columns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FileName – Path to the corresponding WFDB file&lt;/item&gt;
      &lt;item&gt;codes – Diagnostic codes, which can be mapped to text labels using diagnoses_dictionary.csv&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;File: diagnoses_dictionary.csv&lt;lb/&gt; This file provides human-readable mappings for 12SL diagnostic codes. It contains the following columns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;codes – Integer codes for diagnoses&lt;/item&gt;
      &lt;item&gt;acronym – Abbreviated diagnosis labels&lt;/item&gt;
      &lt;item&gt;diagnoses – Full textual descriptions of diagnoses&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ICD Codes Description&lt;/head&gt;
    &lt;p&gt;The ICD_codes/ folder contains diagnostic information extracted from Electronic Health Records (EHR) for each patient.&lt;/p&gt;
    &lt;p&gt;File: icd10_codes.csv&lt;lb/&gt; This file contains diagnostic codes from the 10th revision of the International Classification of Diseases (ICD-10), developed by the World Health Organization (WHO). These alphanumeric codes represent diagnoses and health conditions.&lt;/p&gt;
    &lt;p&gt;Columns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BDSPPatientID – Brain Data Science Platform Patient ID&lt;/item&gt;
      &lt;item&gt;RECORDED_DT – Shifted date of the diagnosis&lt;/item&gt;
      &lt;item&gt;DIAGNOSIS_ICD10_CD – Full ICD-10 diagnosis code&lt;/item&gt;
      &lt;item&gt;DIAGNOSIS_ICD10_DESC – Description of the ICD-10 diagnosis code&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;File: icd9_codes.csv&lt;lb/&gt; This file contains diagnostic codes from the 9th revision of the International Classification of Diseases (ICD-9), also developed by the WHO. These codes are also sourced from the EHR system.&lt;/p&gt;
    &lt;p&gt;Columns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BDSPPatientID – Brain Data Science Platform Patient ID&lt;/item&gt;
      &lt;item&gt;RECORDED_DT – Shifted date of the diagnosis&lt;/item&gt;
      &lt;item&gt;DIAGNOSIS_ICD9_CD – Full ICD-9 diagnosis code&lt;/item&gt;
      &lt;item&gt;DIAGNOSIS_ICD9_DESC – Description of the ICD-9 diagnosis code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Metadata Description&lt;/head&gt;
    &lt;p&gt;The metadata/ folder contains demographic and temporal information associated with each ECG recording, including ECG acquisition time, date of birth, date of death, and derived age-related fields.&lt;/p&gt;
    &lt;p&gt;File: metadata.csv&lt;/p&gt;
    &lt;p&gt;Columns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BDSPPatientID – Patient ID&lt;/item&gt;
      &lt;item&gt;FileName – Path to the WFDB file&lt;/item&gt;
      &lt;item&gt;FileID – Basename of the WFDB file&lt;/item&gt;
      &lt;item&gt;PatientRace&lt;/item&gt;
      &lt;item&gt;EthnicGroupDSC&lt;/item&gt;
      &lt;item&gt;MaritalStatusDSC&lt;/item&gt;
      &lt;item&gt;ReligionDSC&lt;/item&gt;
      &lt;item&gt;LanguageDSC&lt;/item&gt;
      &lt;item&gt;VeteranStatusDSC&lt;/item&gt;
      &lt;item&gt;SexDSC&lt;/item&gt;
      &lt;item&gt;PrimaryCauseOfDeathDSC&lt;/item&gt;
      &lt;item&gt;PrimaryCauseOfDeathUNOS&lt;/item&gt;
      &lt;item&gt;FirstContributoryCauseOfDeathDSC&lt;/item&gt;
      &lt;item&gt;FirstContributoryCauseOfDeathUNOS&lt;/item&gt;
      &lt;item&gt;SecondContributoryCauseOfDeathDSC&lt;/item&gt;
      &lt;item&gt;SecondContributoryCauseOfDeathUNOS&lt;/item&gt;
      &lt;item&gt;EducationLevelDSC&lt;/item&gt;
      &lt;item&gt;GenderIdentityDSC&lt;/item&gt;
      &lt;item&gt;SexAssignedAtBirthDSC&lt;/item&gt;
      &lt;item&gt;DateOfDeath&lt;/item&gt;
      &lt;item&gt;DateOfDeathMARegistryData – Massachusetts (MA) state death registry date of death&lt;/item&gt;
      &lt;item&gt;LastKnownVisitDate – Last time the patient had contact with the hospital system&lt;/item&gt;
      &lt;item&gt;ECGAcquisitionTime – Time of ECG acquisition&lt;/item&gt;
      &lt;item&gt;DateOfBirth&lt;/item&gt;
      &lt;item&gt;AgeAtAcquisition – Age at ECG acquisition&lt;/item&gt;
      &lt;item&gt;AgeAtDeath – Age at time of death&lt;/item&gt;
      &lt;item&gt;AgeAtDeathMA – Age at death according to MA state registry&lt;/item&gt;
      &lt;item&gt;AgeAtLastVisit – Age at the last hospital contact&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For I0006, the following columns are missing from the metadata.csv file: EthnicGroupDSC, MaritalStatusDSC, ReligionDSC, LanguageDSC, VeteranStatusDSC, PrimaryCauseOfDeathDSC, PrimaryCauseOfDeathUNOS, FirstContributoryCauseOfDeathDSC, FirstContributoryCauseOfDeathUNOS, SecondContributoryCauseOfDeathDSC, SecondContributoryCauseOfDeathUNOS, EducationLevelDSC, GenderIdentityDSC, SexAssignedAtBirthDSC, and DateOfDeathMARegistryData.&lt;/p&gt;
    &lt;head rend="h2"&gt;Usage Notes&lt;/head&gt;
    &lt;p&gt;HEEDB is intended to support a wide range of ECG studies, in particular those exploring the relationship between ECG conditions and sleep.&lt;/p&gt;
    &lt;p/&gt;
    &lt;head rend="h2"&gt;Release Notes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;v1.0: Initial release containing 10,608,417 ECGs from 1,818,247 subjects (I0001 site only).&lt;/item&gt;
      &lt;item&gt;v2.0: Added additional data files&lt;/item&gt;
      &lt;item&gt;v3.0: Expanded to include two ECG institutions — I0001 (10,608,417 ECGs from 1,818,247 subjects) and I0006 (1,061,598 ECGs from 349,548 patients). Metadata, 12SL diagnostic codes, and ICD-9/10 diagnosis codes were also added. Duplicate ECGs were removed from the I0001 site, and incorrect sampling frequencies in header files were corrected.&lt;/item&gt;
      &lt;item&gt;v4.0: Corrected typos in the data description&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Ethics&lt;/head&gt;
    &lt;p&gt;The study protocol was approved by the Institutional Review Boards of the Massachusetts General Hospital (protocol # 2013P001024) and Beth Israel Deaconess Medical Center (protocol # 2022P000417). The written informed consents were waived, because of the retrospective study design. The study also complied with the Declaration of Helsinki.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Publication of HEEDB is supported by a grant (R01HL161253) from the National Heart Lung and Blood Institute (NHLBI) of the NIH to Massachusetts General Hospital, Emory University, Stanford University, Kaiser Permanente, Boston Children's Hospital, and Beth Israel Deaconess Medical Center&lt;/p&gt;
    &lt;head rend="h2"&gt;Conflicts of Interest&lt;/head&gt;
    &lt;p&gt;Dr. Westover is a co-founder, scientific advisor, consultant to, and has personal equity interest in Beacon Biosignals. The other authors declare that they have no conflicts of interest.&lt;/p&gt;
    &lt;head rend="h5"&gt;Parent Projects&lt;/head&gt;
    &lt;head rend="h5"&gt;Access&lt;/head&gt;
    &lt;p&gt; Access Policy: &lt;lb/&gt; Only credentialed users who sign the DUA can access the files. &lt;/p&gt;
    &lt;p&gt; License (for files): &lt;lb/&gt; BDSP Credentialed Health Data License 1.5.0 &lt;/p&gt;
    &lt;p&gt; Data Use Agreement: &lt;lb/&gt; BDSP Credentialed Health Data Use Agreement &lt;/p&gt;
    &lt;p&gt; Required training: &lt;lb/&gt; CITI Data or Specimens Only Research &lt;/p&gt;
    &lt;head rend="h5"&gt;Discovery&lt;/head&gt;
    &lt;head rend="h5"&gt;Corresponding Author&lt;/head&gt;
    &lt;head rend="h5"&gt;Versions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;4.0 - July 28, 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Files&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;be a credentialed user&lt;/item&gt;
      &lt;item&gt;complete required training:&lt;/item&gt;
      &lt;item&gt;CITI Data or Specimens Only Research You may submit your training here.&lt;/item&gt;
      &lt;item&gt;sign the data use agreement for the project&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45374961</guid><pubDate>Thu, 25 Sep 2025 16:31:02 +0000</pubDate></item><item><title>GDPVal: Measuring the performance of our models on real-world tasks</title><link>https://openai.com/index/gdpval/</link><description>&lt;doc fingerprint="3c50c9fd0dc1852f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Measuring the performance of our models on real-world tasks&lt;/head&gt;
    &lt;p&gt;We’re introducing GDPval, a new evaluation that measures model performance on economically valuable, real-world tasks across 44 occupations.&lt;/p&gt;
    &lt;p&gt;Our mission is to ensure that artificial general intelligence benefits all of humanity. As part of our mission, we want to transparently communicate progress on how AI models can help people in the real world. That’s why we’re introducing GDPval: a new evaluation designed to help us track how well our models and others perform on economically valuable, real-world tasks. We call this evaluation GDPval because we started with the concept of Gross Domestic Product (GDP) as a key economic indicator and drew tasks from the key occupations in the industries that contribute most to GDP.&lt;/p&gt;
    &lt;p&gt;People often speculate about AI’s broader impact on society, but the clearest way to understand its potential is by looking at what models are already capable of doing. History shows that major technologies—from the internet to smartphones—took more than a decade to go from invention to widespread adoption. Evaluations like GDPval help ground conversations about future AI improvements in evidence rather than guesswork, and can help us track model improvement over time.&lt;/p&gt;
    &lt;p&gt;Previous AI evaluations like challenging academic tests and competitive coding challenges have been essential in pushing the boundaries of model reasoning capabilities, but they often fall short of the kind of tasks that many people handle in their everyday work.&lt;/p&gt;
    &lt;p&gt;To bridge this gap, we’ve been developing evaluations that measure increasingly realistic and economically relevant capabilities. This progression has moved from classic academic benchmarks like MMLU (exam-style questions across dozens of subjects), to more applied evaluations like SWE-Bench (software engineering bug-fixing tasks), MLE-Bench (machine learning engineering tasks such as model training and analysis), and Paper-Bench (scientific reasoning and critique on research papers), and more recently to market-based evaluations like SWE-Lancer (freelance software engineering projects based on real payouts).&lt;/p&gt;
    &lt;p&gt;GDPval is the next step in that progression. It measures model performance on tasks drawn directly from the real-world knowledge work of experienced professionals across a wide range of occupations and sectors, providing a clearer picture on how models perform on economically valuable tasks. Evaluating models on realistic occupational tasks helps us understand not just how well they perform in the lab, but how they might support people in the work they do every day.&lt;/p&gt;
    &lt;p&gt;GDPval, the first version of this evaluation, spans 44 occupations selected from the top 9 industries contributing to U.S. GDP. The GDPval full set includes 1,320 specialized tasks (220 in the gold open-sourced set), each meticulously crafted and vetted by experienced professionals with over 14 years of experience on average from these fields. Every task is based on real work products, such as a legal brief, an engineering blueprint, a customer support conversation, or a nursing care plan.&lt;/p&gt;
    &lt;p&gt;GDPval is distinctive both in its realism and diversity of tasks being evaluated. Unlike other evaluations tied to economic value which concentrate on specific domains (e.g., SWE-Lancer), GDPval covers many tasks and occupations. And unlike benchmarks which involve synthetically creating tasks in the style of an academic exam or test (e.g., Humanity’s Last Exam or MMLU), GDPval focuses on tasks based on deliverables that are either an actual piece of work or product that exists today or are a similarly constructed piece of work product.&lt;/p&gt;
    &lt;p&gt;Unlike traditional benchmarks, GDPval tasks are not simple text prompts. They come with reference files and context, and the expected deliverables span documents, slides, diagrams, spreadsheets, and multimedia. This realism makes GDPval a more realistic test of how models might support professionals.&lt;/p&gt;
    &lt;p&gt;GDPval is an early step that doesn’t reflect the full nuance of many economic tasks. While it spans 44 occupations and hundreds of knowledge work tasks, it is limited to one-shot evaluations, so it doesn’t capture cases where a model would need to build context or improve through multiple drafts. Future versions will extend to more interactive workflows and context-rich tasks to better reflect the complexity of real-world knowledge work (see more in our Limitations section below).&lt;/p&gt;
    &lt;p&gt;GDPval covers tasks across 9 industries and 44 occupations, and future versions will continue to expand coverage. The initial 9 industries were chosen based on those contributing over 5% to U.S. GDP, as determined by data from the Federal Reserve Bank of St. Louis. Then, we selected the 5 occupations within each industry that contribute most to total wages and compensation and are predominantly knowledge work occupations, using wage and employment data from the May 2024 US Bureau of Labor Statistics (BLS) occupational employment report(opens in a new window). To determine if the occupations were predominantly knowledge work, we used task data from O*NET(opens in a new window), a database of U.S. occupational information sponsored by the U.S. Department of Labor. We classified whether each task for each occupation in O*NET was knowledge work or physical work/manual labor (requiring actions to be taken in the physical world). An occupation qualified overall as “predominantly knowledge work” if at least 60% of its component tasks were classified as not involving physical work or manual labor. We chose this 60% threshold as a starting point for the first version of GDPval, focusing on occupations where AI could have the highest impact on real-world productivity.&lt;/p&gt;
    &lt;p&gt;This process yielded 44 occupations for inclusion.&lt;/p&gt;
    &lt;p&gt;Real estate and rental and leasing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Concierges&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Property, real estate, and community association managers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real estate sales agents&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real estate brokers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Counter and rental clerks&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Government&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Recreation workers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compliance officers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;First-line supervisors of police and detectives&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Administrative services managers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Child, family, and school social workers&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Manufacturing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Mechanical engineers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Industrial engineers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Buyers and purchasing agents&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shipping, receiving, and inventory clerks&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;First-line supervisors of production and operating workers&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Professional, scientific, and technical services&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Software developers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lawyers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Accountants and auditors&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Computer and information systems managers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Project management specialists&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Health care and social assistance&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Registered nurses&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nurse practitioners&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Medical and health services managers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;First-line supervisors of office and administrative support workers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Medical secretaries and administrative assistants&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finance and insurance&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Customer service representatives&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Financial and investment analysts&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Financial managers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Personal financial advisors&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Securities, commodities and financial services sales agents&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Retail trade&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pharmacists&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;First-line supervisors of retail sales workers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;General and operations managers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Private detectives and investigators&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wholesale trade&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Sales managers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Order clerks&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;First-line supervisors of non-retail sales workers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sales representatives, wholesale and manufacturing, except technical and scientific products&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sales representatives, wholesale and manufacturing, technical and scientific products&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Information&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Audio and video technicians&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Producers and directors&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;News analysts, reporters, and journalists&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Film and video editors&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Editors&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For each occupation, we worked with experienced professionals to create representative tasks that reflect their day-to-day work. These professionals averaged 14 years of experience, with strong records of advancement. We deliberately recruited a breadth of experts—such as lawyers from different practice areas and firms of different sizes—to maximize representativeness.&lt;/p&gt;
    &lt;p&gt;Each task went through a multi-step review process to ensure it was representative of real work, feasible for another professional to complete, and clear for evaluation. On average, each task received 5 rounds of expert review, including checks from other task writers, additional occupational reviewers, and model-based validation.&lt;/p&gt;
    &lt;p&gt;The resulting dataset includes 30 fully reviewed tasks per occupation (full-set) with 5 tasks per occupation in our open-sourced gold set, providing a robust foundation for evaluating model performance on real-world knowledge work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Examples of GDPval tasks&lt;/head&gt;
    &lt;head rend="h3"&gt;Prompt + task context&lt;/head&gt;
    &lt;head rend="h3"&gt;Experienced human deliverable&lt;/head&gt;
    &lt;p&gt;To evaluate model performance on GDPval tasks, we rely on expert “graders”—a group of experienced professionals from the same occupations represented in the dataset. These graders blindly compare model-generated deliverables with those produced by task writers (not knowing which is AI versus human generated), and offer critiques and rankings. Graders then rank the human and AI deliverables and classify each AI deliverable as “better”, “as good as”, or “worse than” one another.&lt;/p&gt;
    &lt;p&gt;Task writers also created detailed scoring rubrics for their occupations, which add consistency and transparency to the grading process. We also built an “automated grader”, an AI system trained to estimate how human experts would judge a given deliverable. In other words, instead of running a full expert review every time, the automated grader can quickly predict which output people would likely prefer. We’re releasing this tool through at evals.openai.com as an experimental research service, but it’s not yet as reliable as expert graders, so we don’t use it to replace them.&lt;/p&gt;
    &lt;p&gt;We found that today’s best frontier models are already approaching the quality of work produced by industry experts. To test this, we ran blind evaluations where industry experts compared deliverables from several leading models—GPT‑4o, o4-mini, OpenAI o3, GPT‑5, Claude Opus 4.1, Gemini 2.5 Pro, and Grok 4—against human-produced work. Across 220 tasks in the GDPval gold set, we recorded when model outputs were rated as better than (“wins”) or on par with (“ties”) the deliverables from industry experts, as shown in the bar chart below. Claude Opus 4.1 was the best performing model in the set, excelling in particular on aesthetics (e.g., document formatting, slide layout), and GPT‑5 excelled in particular on accuracy (e.g., finding domain-specific knowledge). We also see clear progress over time on these tasks. Performance has more than doubled from GPT‑4o (released spring 2024) to GPT‑5 (released summer 2025), following a clear linear trend.&lt;/p&gt;
    &lt;p&gt;In addition, we found that frontier models can complete GDPval tasks roughly 100x faster and 100x cheaper than industry experts. However, these figures reflect pure model inference time and API billing rates, and therefore do not capture the human oversight, iteration, and integration steps required in real workplace settings to use our models. Still, especially on the subset of tasks where models are particularly strong, we expect that giving a task to a model before trying it with a human would save time and money.&lt;/p&gt;
    &lt;p&gt;Finally, we incrementally trained an internal, experimental version of GPT‑5 to assess if we could improve performance on GDPval. We found this process improved performance, creating a pathway for further potential improvement. Other controlled experiments back this up: increasing model size, encouraging more reasoning steps, and giving richer task context each led to measurable gains.&lt;/p&gt;
    &lt;p&gt;You can read the full results in our paper. We’re also releasing a gold subset of GDPval tasks and a public grading service so other researchers can build on this work.&lt;/p&gt;
    &lt;p&gt;As AI becomes more capable, it will likely cause changes in the job market. Early GDPval results show that models can already take on some repetitive, well-specified tasks faster and at lower cost than experts. However, most jobs are more than just a collection of tasks that can be written down. GDPval highlights where AI can handle routine tasks so people can spend more time on the creative, judgment-heavy parts of work. When AI complements workers in this way it can translate into significant economic growth. Our goal is to keep everyone on the “up elevator” of AI by democratizing access to these tools, supporting workers through change, and building systems that reward broad contribution.&lt;/p&gt;
    &lt;p&gt;GDPval is an early step. While it covers 44 occupations and hundreds of tasks, we are continuing to refine our approach to expand the scope of our testing and make the results more meaningful. The current version of the evaluation is also one-shot, so it doesn’t capture cases where a model would need to build context or improve through multiple drafts—for example, revising a legal brief after client feedback or iterating on a data analysis after spotting an anomaly. Additionally, in the real world, tasks aren’t always clearly defined with a prompt and reference files; for example, a lawyer might have to navigate ambiguity and talk to their client before deciding that creating a legal brief is the right approach to help them. We plan to expand GDPval to include more occupations, industries, and task types, with increased interactivity, and more tasks involving navigating ambiguity, with the long-term goal of better measuring progress on diverse knowledge work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you’re an industry expert interested in contributing to GDPval, please show your interest here.&lt;/item&gt;
      &lt;item&gt;If you’re a customer working with OpenAI and you'd like to contribute to a future round of GDPval, please express interest here.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Community participation is essential—we’re excited to build GDPval together with researchers, practitioners, and organizations who share our goal of making AGI more useful for people at work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45375392</guid><pubDate>Thu, 25 Sep 2025 16:55:48 +0000</pubDate></item><item><title>Austria hails 'brain gain' in luring 25 academics away from US after cuts</title><link>https://www.reuters.com/world/austria-hails-brain-gain-luring-25-academics-away-us-after-cuts-2025-09-25/</link><description>&lt;doc fingerprint="b99c9179ae43ce8b"&gt;
  &lt;main&gt;
    &lt;p&gt;VIENNA, Sept 25 (Reuters) - Austria has lured what it calls 25 "top researchers" away from U.S. institutions including Harvard, the Massachusetts Institute of Technology and Princeton with grants set up in response to the Trump administration's funding cuts targeting universities.&lt;/p&gt;
    &lt;p&gt;Recipients of the grants of 500,000 euros ($587,000) each over two years range from post-doctoral researchers to professors and work in fields such as physics, chemistry and life sciences, the Austrian Academy of Sciences said in a statement on Thursday.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;"Thank Trump for this brain gain," the academy's president Heinz Fassmann said.&lt;/p&gt;
    &lt;p&gt;"We have succeeded in bringing these outstanding individuals from the United States to Austria. They bring with them new ideas, new perspectives and international networks. That is a big win for Austrian science," he added, without naming them.&lt;/p&gt;
    &lt;p&gt;U.S. President Donald Trump has cracked down on universities over a range of issues like pro-Palestinian protests against U.S. ally Israel's assault on Gaza, transgender policies, climate initiatives and diversity, equity and inclusion programs.&lt;/p&gt;
    &lt;p&gt;The White House has said even with the cuts, the U.S. would still account for the most global research funding&lt;/p&gt;
    &lt;p&gt;Austria is among the countries that have responded by seeking to lure away academics currently working at universities in the United States. In March, 13 European countries including France, Germany and Spain, urged the EU Commission to move fast to attract academic talent.&lt;/p&gt;
    &lt;p&gt;While Austria is better known for the intellectuals it produced in the 19th and early 20th centuries, such as psychoanalyst Sigmund Freud or quantum physicist Erwin Schroedinger, it currently has four universities in the global Shanghai ranking's top 300.&lt;/p&gt;
    &lt;p&gt;Recipients of the grants will start work this year at Austrian universities or research institutions.&lt;/p&gt;
    &lt;p&gt;"At a time when political interference and authoritarian tendencies are encroaching on research and teaching, we are taking a strong stand against them," Austria's minister for science and research, Eva-Maria Holzleitner of the Social Democrats, said.&lt;/p&gt;
    &lt;p&gt;($1 = 0.8519 euros)&lt;/p&gt;
    &lt;p&gt;Reporting by Francois Murphy; editing by Alexandra Hudson&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45375450</guid><pubDate>Thu, 25 Sep 2025 16:58:58 +0000</pubDate></item><item><title>ChatGPT Pulse</title><link>https://openai.com/index/introducing-chatgpt-pulse/</link><description>&lt;doc fingerprint="9a26a651bae7d2f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing ChatGPT Pulse&lt;/head&gt;
    &lt;p&gt;Now ChatGPT can start the conversation&lt;/p&gt;
    &lt;p&gt;We're building ChatGPT to help you reach your goals. Since ChatGPT launched, that's always meant coming to ask a question. There's magic in being able to simply ask and get answers to help you learn, create or solve problems. However that's limited by what you know to ask for and always puts the burden on you for the next step.&lt;/p&gt;
    &lt;p&gt;Today we're releasing a preview of ChatGPT Pulse to Pro users on mobile. Pulse is a new experience where ChatGPT proactively does research to deliver personalized updates based on your chats, feedback, and connected apps like your calendar. You can curate what ChatGPT researches by letting it know what’s useful and what isn’t. The research appears in Pulse as topical visual cards you can scan quickly or open for more detail, so each day starts with a new, focused set of updates.&lt;/p&gt;
    &lt;p&gt;This is the first step toward a more useful ChatGPT that proactively brings you what you need, helping you make more progress so you can get back to your life. We’ll learn and improve from early use before rolling it out to Plus, with the goal of making it available to everyone.&lt;/p&gt;
    &lt;p&gt;ChatGPT can now do asynchronous research on your behalf. Each night, it synthesizes information from your memory, chat history, and direct feedback to learn what’s most relevant to you, then delivers personalized, focused updates the next day. These could look like follow-ups on topics you discuss often, ideas for quick, healthy dinner to make at home that evening, or next steps toward a longer-term goal such as training for a triathlon.&lt;/p&gt;
    &lt;p&gt;You can also connect Gmail and Google Calendar to provide additional context for more relevant suggestions. When Calendar is connected, ChatGPT might draft a sample meeting agenda, remind you to buy a birthday gift, or surface restaurant recommendations for an upcoming trip. These integrations are off by default and can be turned on or off anytime in settings.&lt;/p&gt;
    &lt;p&gt;Topics shown in Pulse also pass through safety checks to avoid showing harmful content that violates our policies.&lt;/p&gt;
    &lt;p&gt;You can ask for what you’d like ChatGPT to research for you each day. Tap "curate" to request what you want to see in future editions—ask for a Friday roundup of local events, tips for learning a new skill, or something specific like "focus on professional tennis updates tomorrow." You can also give quick feedback with a thumbs up or thumbs down, and easily view or delete your feedback history. Over time, your guidance makes Pulse more personal and useful.&lt;/p&gt;
    &lt;p&gt;Every morning, ChatGPT delivers a curated set of the most relevant updates, giving you the information you need so you can get back to what matters most. Each update is available for that day only unless you save it as a chat or ask a follow-up question, which adds it to your conversation history. Expand any update to dive deeper, request next steps, or save it for later so you can move forward on goals with clear, timely information.&lt;/p&gt;
    &lt;p&gt;We partnered with college students in the ChatGPT Lab to gather early feedback and improve Pulse. One insight in particular we had was that many started to feel its utility once they started telling ChatGPT what they wanted to see. That insight underscored the importance of simple feedback, so we added more ways to share reactions and guide what appears. Here are a few of the students’ favorite personalized updates.&lt;/p&gt;
    &lt;head rend="h3"&gt;Student use cases&lt;/head&gt;
    &lt;head rend="h3"&gt;Actionable recommendations&lt;/head&gt;
    &lt;p&gt;"Received this based on a conversation that I had yesterday that focused on calendar management/structuring PTO for my grant period in Taiwan. What it produced was several logical steps ahead of where I was at in the conversation. The update was incredibly helpful and exposed me to train and commute information I would have never come across or looked for otherwise."&lt;/p&gt;
    &lt;p&gt;Pulse is a preview and won’t always get things right. It aims to show you what’s most relevant and useful but you may still see suggestions that miss the mark. For example, you may get tips for a project you already completed. You can guide what shows up by telling ChatGPT directly. It remembers your feedback for next time and improves as it learns from real use.&lt;/p&gt;
    &lt;p&gt;Pulse is the first step toward a new paradigm for interacting with AI.&lt;/p&gt;
    &lt;p&gt;By combining conversation, memory, and connected apps, ChatGPT is moving from answering questions to a proactive assistant that works on your behalf. Over time, we envision AI systems that can research, plan, and take helpful actions for you—based on your direction—so that progress happens even when you are not asking.&lt;/p&gt;
    &lt;p&gt;Pulse introduces this future in its simplest form: personalized research and timely updates that appear regularly to keep you informed. Soon, Pulse will be able to connect with more of the apps you use so updates capture a more complete picture of your context. We’re also exploring ways for Pulse to deliver relevant work at the right moments throughout the day, whether it’s a quick check before a meeting, a reminder to revisit a draft, or a resource that appears right when you need it.&lt;/p&gt;
    &lt;p&gt;As we expand to more apps and richer actions, ChatGPT will evolve from something you consult into something that quietly accelerates the work and ideas that matter to you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45375477</guid><pubDate>Thu, 25 Sep 2025 16:59:55 +0000</pubDate></item><item><title>Improved Gemini 2.5 Flash and Flash-Lite</title><link>https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/</link><description>&lt;doc fingerprint="bf7738879939489d"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we are releasing updated versions of Gemini 2.5 Flash and 2.5 Flash-Lite, available on Google AI Studio and Vertex AI, aimed at continuing to deliver better quality while also improving the efficiency.&lt;/p&gt;
    &lt;p&gt;The latest version of Gemini 2.5 Flash-Lite was trained and built based on three key themes:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;You can start testing this version today using the following model string: &lt;code&gt;gemini-2.5-flash-lite-preview-09-2025&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This latest 2.5 Flash model comes with improvements in two key areas we heard consistent feedback on:&lt;/p&gt;
    &lt;p&gt;We’re already seeing positive feedback from early testers. As Yichao ‘Peak’ Ji, Co-Founder &amp;amp; Chief Scientist at Manus, an autonomous AI agent, noted: “The new Gemini 2.5 Flash model offers a remarkable blend of speed and intelligence. Our evaluation on internal benchmarks revealed a 15% leap in performance for long-horizon agentic tasks. Its outstanding cost-efficiency enables Manus to scale to unprecedented levels—advancing our mission to Extend Human Reach.”&lt;/p&gt;
    &lt;p&gt;You can start testing this preview version today by using the following model string: &lt;code&gt;gemini-2.5-flash-preview-09-2025&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Over the last year, we’ve learned that shipping preview versions of our models allows you to test our latest improvements and innovations, provide feedback, and build production-ready experiences with the best of Gemini. Today’s releases are not intended to graduate to a new, stable version but will help us shape our future stable releases, and allow us to continue iterating and bring you the best of Gemini.&lt;/p&gt;
    &lt;p&gt;To make it even easier to access our latest models while also reducing the need to keep track of long model string names, we are also introducing a &lt;code&gt;-latest&lt;/code&gt; alias for each model family. This alias always points to our most recent model versions, allowing you to experiment with new features without needing to update your code for each release. You can access the new previews using:&lt;/p&gt;
    &lt;code&gt;gemini-flash-latest&lt;/code&gt;
    &lt;code&gt;gemini-flash-lite-latest&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;To ensure you have time to test new models, we will always provide a 2-week notice (via email) before we make updates or deprecate a specific version behind &lt;code&gt;-latest&lt;/code&gt;. These are just model aliases so the rate limits, cost, and features available may fluctuate between releases.&lt;/p&gt;
    &lt;p&gt;For applications that require more stability, continue to use &lt;code&gt;gemini-2.5-flash&lt;/code&gt; and &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We continue to push the frontier of what is possible with Gemini and this release is just another step in that direction. We will have more to share soon, but in the meantime, happy building!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45375845</guid><pubDate>Thu, 25 Sep 2025 17:20:56 +0000</pubDate></item><item><title>Immich mobile app sync V2</title><link>https://immich.app/blog/sync-v2</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45376516</guid><pubDate>Thu, 25 Sep 2025 18:04:58 +0000</pubDate></item><item><title>Haydex: From Zero to 178,600M rows a second in 30 days</title><link>https://axiom.co/blog/building-haydex</link><description>&lt;doc fingerprint="ef1899554df0cae3"&gt;
  &lt;main&gt;
    &lt;head rend="h5"&gt;Author&lt;/head&gt;
    &lt;head rend="h5"&gt;Tomás Senart&lt;/head&gt;
    &lt;p&gt;Principal Engineer&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I/O architecture determines scale: One large read instead of thousands of small reads changed everything&lt;/item&gt;
      &lt;item&gt;Profiler-driven optimization: 90% of allocations and 70% of CPU were hiding in unexpected places&lt;/item&gt;
      &lt;item&gt;Distributed redesign unlocks speed: Map-reduce Lambda architecture delivered 6x indexing speedup&lt;/item&gt;
      &lt;item&gt;Compound optimizations multiply: Each optimization amplified others to reach 673 billion rows/second&lt;/item&gt;
      &lt;item&gt;Production beats theory: V0's elegant design failed; V1 succeeded by respecting network physics&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nearly every great engineering story starts not with a grand plan, but with a nagging, infuriating problem.&lt;/p&gt;
    &lt;p&gt;Ours was simple: our needle-in-the-haystack queries were too slow. For a database company, that's an existential threat. Our customers, especially giants like Hyperscale Customer, were pushing data at a scale that made our brute-force scanning approach look like trying to find a specific grain of sand on a planet-sized beach with a teaspoon. We had to do something drastic.&lt;/p&gt;
    &lt;p&gt;This is the story of that something. It's the story of a project that had been tried before and shelved, a project that rose from the dead.&lt;/p&gt;
    &lt;p&gt;In a single, caffeine-fueled month between June 9 and July 8, 2025, we took Haydex, our dream of a hyper-fast filtering system, and forged it into a production-hardened reality.&lt;/p&gt;
    &lt;p&gt;It was a journey into the abyss of distributed systems, a battle against memory bottlenecks, API limits, and our own assumptions.&lt;/p&gt;
    &lt;p&gt;We started with a Slack message that read "The Grand Haydex Revival" and ended with a system clocking an effective throughput of 178,600,000,000 rows per second-and peaking at a synergistic 673,850,000,000 rows per second with its caching counterpart.&lt;/p&gt;
    &lt;p&gt;This is how we did it.&lt;/p&gt;
    &lt;p&gt;Background&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In this post we're talking about EventDB, our purpose-built petabyte-scale event datastore which powers Axiom's events, logs, and traces support.&lt;/item&gt;
      &lt;item&gt;EventDB has a custom stateless ingest pipeline, only uses object-storage for storing all ingested data, and has a completely serverless (lambda-powered) query engine.&lt;/item&gt;
      &lt;item&gt;We report effective throughput as &lt;code&gt;effective_rows_per_sec = candidate_rows_before_pruning ÷ wall_clock_seconds&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Numbers in this post come from production runs on Hyperscale Customer’s dataset (with permission, of course).&lt;/item&gt;
      &lt;item&gt;For “Haydex only,” the zero‑matches cache was off; for “Haydex + cache,” it was on.&lt;/item&gt;
      &lt;item&gt;Hardware and cache state were held constant within each comparison. We fix the filter’s target false‑positive rate (FPR) per run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Micro-glossary&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Field‑scoped filter - One large filter per field (e.g., body) spanning thousands of blocks. Inside that filter, each block is the document.&lt;/item&gt;
      &lt;item&gt;Block - The pruning unit; only surviving blocks fan out to workers.&lt;/item&gt;
      &lt;item&gt;FPR (false‑positive rate) - Probability the filter returns “maybe” for a non‑matching block; it never returns false negatives.&lt;/item&gt;
      &lt;item&gt;Zero‑matches cache - Caches known‑empty predicate/interval combos to skip re‑evaluation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The ghost of Haydex past&lt;/head&gt;
    &lt;p&gt;The idea of using probabilistic filters to accelerate queries wasn't new. At its heart, it's a simple, powerful concept: a data structure that can tell you with blistering speed if the needle you're looking for is definitely NOT in a haystack. It might occasionally lie and say something is there when it's not (a false positive), but it will never lie about something being absent. For query planning, that's a perfect trade-off.&lt;/p&gt;
    &lt;p&gt;We had a V0 that tried to do this. It was, for lack of a better word, a train wreck. Not because the core data structure was wrong, but because our initial execution was naive - a perfect monument to Good Ideas on Paper™ that crumble on contact with reality.&lt;/p&gt;
    &lt;p&gt;V0's design seemed logical, almost elegant, in the sterile vacuum of a design document. We would create one filter file per data block. Inside that filter, the individual columns of that block were the "documents" (events) we were indexing. We'd then use this with "adaptive execution," a fancy term for letting the query plan change its mind on the fly based on what these fine-grained filters whispered to it.&lt;/p&gt;
    &lt;p&gt;In practice, this was a catastrophic design. While the filter structure was sound, the granularity was wrong! A query spanning thousands of blocks still had to perform thousands of independent, latency-sensitive reads from S3-one for each block's filter file. We would fan out a query to thousands of Lambda workers, and only then would each worker attempt to fetch its own tiny filter file.&lt;/p&gt;
    &lt;p&gt;It was death by a thousand GETs. The I/O overhead didn't just negate the potential gains; it actively made things slower. We also stumbled into a more insidious problem: false positives born from a flawed document model. V0 treated the entire column within a block as a single document. So the filter could correctly tell us, "Yes, the hashes for 'user' and 'failed' and 'login' are all in this block's message column" but it couldn't tell us they weren't in the same row. We were finding the letters but had no idea if they spelled the right word.&lt;/p&gt;
    &lt;p&gt;The whole thing was too slow, too complex, and ultimately, a dead end. I did the only sensible thing: I shut it down, wrote up the post-mortem, and turned the page. I had learned-the hard way. The ghost of Haydex Past would haunt me, but it would also serve as a constant, nagging reminder of what not to do.&lt;/p&gt;
    &lt;p&gt;The problem, of course, didn't go away. The need to find needles in ever-growing haystacks only became more acute. I knew a solution would be a game-changer. I just needed a much, much better plan.&lt;/p&gt;
    &lt;head rend="h2"&gt;The grand Haydex revival&lt;/head&gt;
    &lt;p&gt;By mid-May 2025, the pressure was immense. Hyperscale Customer, a user operating at a scale that stretches the very definition of "web scale," was pushing us to our absolute limits. Their need for fast, targeted searches wasn't a "nice to have"; it was existential for their use case. The ghost of my failed filter experiment lingered in the backlog, but the problem it was meant to solve had metastasized into a five-alarm fire. It was time for a reckoning.&lt;/p&gt;
    &lt;p&gt;On May 16th, after days spent digging through a mountain of research papers on modern filter designs, I dropped a message into our team Slack channel: "The Grand Haydex Revival". This wasn't just a restart; it was a complete teardown and reimagining, informed by the deep scars of V0.&lt;/p&gt;
    &lt;p&gt;Haydex V1 would be V0's antithesis. I threw out everything that had hurt us.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;From Block-Scoped to Field-Scoped. This was the crucial pivot. Instead of creating one small filter file per block, V1 would create one massive, field-scoped filter that covered a single field (like body) across thousands of blocks. Inside this single filter, each block became a 'document'. This flipped the economics of I/O on its head: checking a term across 10,000 blocks now meant one targeted read from one large file, not 10,000 individual reads from 10,000 tiny files.&lt;/item&gt;
      &lt;item&gt;No more "adaptive execution." That was too clever by half, a premature optimization that cost us dearly. V1 would use brutally effective early pruning. We would make the decision to kill or keep blocks right at the beginning, before fanning out a single request to a worker. Save I/O, save compute, save time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The plan was audacious, borderline reckless: go from concept to a production-ready system, indexing data at scale, in one month. There was no formal design doc, only a vision born from the ashes of failure, a Slack channel that was about to become a war room, and a relentless, unapologetic bias for action.&lt;/p&gt;
    &lt;code&gt;# v0: block-scoped filters
[query]
  ├─&amp;gt; [lambda] ── GET [filter block #1]
  ├─&amp;gt; [lambda] ── GET [filter block #2]
  ├─&amp;gt;  … thousands more …
  └─&amp;gt; [lambda] ── GET [filter block N]
late adaptive execution (pruning after fan‑out)  ⇒ high fan‑out, high latency

# v1: field-scoped filters
[query] ── read [field filter: body] ── early prune (keep/kill block ids)
  ├─&amp;gt; [worker] on surviving blocks
  └─&amp;gt; [worker] …&lt;/code&gt;
    &lt;head rend="h2"&gt;Week 1 (June 9): Quick wins and brutal truths&lt;/head&gt;
    &lt;p&gt;The first week was a frantic blur of hacking. I revived the shelved code from V0 that we could reuse and massaged it into shape. The core data structures were adapted and the initial indexer service was setup. The plan was simple on the surface: a single service running on a beefy machine with fast storage for temporary data. It would wake up, scan for new data blocks, group them into time scoped batches, and build the filters. Simple.&lt;/p&gt;
    &lt;p&gt;Almost immediately, I slammed head-first into a wall. It wasn't what I expected.&lt;/p&gt;
    &lt;p&gt;The metadata catalog, living in Postgres, was grinding to a halt. The queries to figure out which indexes covered which blocks were a bottleneck of monumental proportions. A trace told the brutal truth: most of the time was just the application staring at Postgres, waiting for it to respond. Before I could even think about indexing performance, emergency surgery had to be performed on our catalog schema.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;PR #13517 (+588 −112)&lt;/code&gt; was that surgery. It was a major refactor. We had an inefficient &lt;code&gt;block_nums&lt;/code&gt; array, which was forcing Postgres into painful full scans. It was replaced with a proper mapping table that could be efficiently indexed. An in-memory caching layer was added to shield the database from repetitive lookups. The result was staggering. Catalog query latency plummeted by 94-98%. One bottleneck down.&lt;/p&gt;
    &lt;p&gt;The next one surfaced instantly, like a game of whack-a-mole. Hashing. The simple act of generating hashes for every term in a column was an ecological disaster of CPU cycles and memory allocations. The profiler showed HashColumn allocating memory as if it were free, creating mountains of garbage for the GC to clean up.&lt;/p&gt;
    &lt;p&gt;Time to tear it apart. In &lt;code&gt;PR #13592 (+906 −29)&lt;/code&gt;, I re-wrote HashColumn from the ground up, eliminating all the intermediate object allocations that were killing performance. It added &lt;code&gt;sync. Pool&lt;/code&gt; for our hash sets to aggressively recycle memory and added fast paths for ASCII string processing to avoid expensive UTF-8 machinery when it wasn't needed. The impact was, frankly, explosive. We observed up to a 73.89% reduction in execution time and a mind-boggling 90.74% reduction in memory allocations.&lt;/p&gt;
    &lt;code&gt;goos: darwin
goarch: arm64
cpu: Apple M3 Max
                                     │  before.txt  │            after.txt             │
                                     │    sec/op    │   sec/op     vs base             │
HashColumn/Ints-16                      604.0µ ± 2%   176.9µ ± 2%  -70.72% (p=0.002 n=6)
HashColumn/Floats-16                    631.5µ ± 7%   202.2µ ± 2%  -67.97% (p=0.002 n=6)
HashColumn/Strings-16                   5.857m ± 3%   1.561m ± 0%  -73.35% (p=0.002 n=6)
HashColumn/String_WithNulls-16         190.05µ ± 4%   59.84µ ± 0%  -68.51% (p=0.002 n=6)
HashColumn/String_HighCardinality-16    522.6µ ± 3%   136.4µ ± 0%  -73.89% (p=0.002 n=6)
HashColumn/String_LowCardinality-16    124.08µ ± 0%   48.43µ ± 1%  -60.97% (p=0.002 n=6)
geomean                                 549.5µ        167.5µ       -69.52%

                                     │  before.txt   │             after.txt             │
                                     │     B/op      │     B/op      vs base             │
HashColumn/Ints-16                      670.2Ki ± 0%   160.1Ki ± 0%  -76.10% (p=0.002 n=6)
HashColumn/Floats-16                    670.2Ki ± 0%   160.2Ki ± 0%  -76.10% (p=0.002 n=6)
HashColumn/Strings-16                  7670.0Ki ± 0%   710.3Ki ± 0%  -90.74% (p=0.002 n=6)
HashColumn/String_WithNulls-16         336.71Ki ± 0%   96.08Ki ± 0%  -71.46% (p=0.002 n=6)
HashColumn/String_HighCardinality-16   1012.1Ki ± 0%   120.1Ki ± 0%  -88.13% (p=0.002 n=6)
HashColumn/String_LowCardinality-16    176.31Ki ± 0%   80.20Ki ± 0%  -54.51% (p=0.002 n=6)
geomean                                 769.1Ki        160.1Ki       -79.18%

                                     │  before.txt   │            after.txt             │
                                     │   allocs/op   │  allocs/op   vs base             │
HashColumn/Ints-16                       15.000 ± 0%    4.000 ± 0%  -73.33% (p=0.002 n=6)
HashColumn/Floats-16                     15.000 ± 0%    4.000 ± 0%  -73.33% (p=0.002 n=6)
HashColumn/Strings-16                    50.02k ± 0%   10.00k ± 0%  -80.00% (p=0.002 n=6)
HashColumn/String_WithNulls-16         2008.000 ± 0%    4.000 ± 0%  -99.80% (p=0.002 n=6)
HashColumn/String_HighCardinality-16   5006.000 ± 0%    4.000 ± 0%  -99.92% (p=0.002 n=6)
HashColumn/String_LowCardinality-16    2010.000 ± 0%    4.000 ± 0%  -99.80% (p=0.002 n=6)
geomean                                   781.3         14.74       -98.11%&lt;/code&gt;
    &lt;p&gt;By Friday, June 13th, we had the first taste of real victory. With these core pieces in place, I enabled indexing for Hyperscale Customer's live production logs. The result: an 8.85x speedup on a real-world query. We were on track.&lt;/p&gt;
    &lt;p&gt;The feeling in the channel was electric. I pasted the lyrics to Queen's "Don't Stop Me Now" into Slack. That was the vibe. Haydex was a shooting star, leaping through the sky.&lt;/p&gt;
    &lt;head rend="h2"&gt;Week 2 (June 14): Hitting the scaling wall&lt;/head&gt;
    &lt;p&gt;The high from our early wins was intoxicating, but it masked a deeper, more fundamental problem. Our single-node indexer, for all its beefy specs, was choking. We were trying to index data for a customer with a truly biblical workload. The indexer would lock up, its memory completely exhausted, the garbage collector thrashing so hard it couldn't even trigger an OOMKill. (This is a phenomenon where the system spends more time cleaning up memory than actually doing work.) It was just... stuck. Frozen in a state of silent, high-CPU agony, a zombie process consuming resources but doing no work.&lt;/p&gt;
    &lt;p&gt;On June 14th, it happened again. I posted a weary message to Slack: "Indexer locked up, will kick it. Glad we'll be moving to Lambdas for the hashing soon"&lt;/p&gt;
    &lt;p&gt;That casual "soon" had to become "now." The single-node architecture was a dead end. We needed to distribute the work. An idea I had floated two days earlier became the new rallying cry: a full-scale distributed architecture.&lt;/p&gt;
    &lt;p&gt;I pivoted our design to a map-reduce-like model that completely changed our approach. The "map" phase involved a massive fan-out to parallel workers, each processing a small, independent chunk of data. This distributed workload was key to breaking the memory bottleneck. Once the parallel processing was complete, a centralized service would then perform the "reduce" phase, efficiently assembling the final, massive filter.&lt;/p&gt;
    &lt;p&gt;This was a massive architectural change, but it paid off almost instantly. By June 18th, the new distributed design was deployed. The indexing latency for a large batch of Hyperscale Customer's columns plummeted from around 3.5 minutes to just 30-40 seconds. We were finally able to start a full backfill, indexing an entire week of their production data. It completed in just 6 hours-a task that would have been physically impossible on the old system.&lt;/p&gt;
    &lt;p&gt;The relief was palpable. We were firing on all cylinders. I posted a picture from a brief time off, a photo of myself intensely trying to relax, with the caption: "Guys I'm touching grass". It was a much-needed moment of levity in the middle of the storm.&lt;/p&gt;
    &lt;head rend="h2"&gt;Week 3 (June 21): Death by a thousand paper cuts&lt;/head&gt;
    &lt;p&gt;Scaling is never a clean process. Solving the CPU bottleneck with our new distributed architecture just revealed the next layer of problems. We had slain the beast, only to find it was a hydra. For every head we lopped off, two more grew in its place. Welcome to the wonderful world of distributed systems.&lt;/p&gt;
    &lt;p&gt;First came the mysterious context canceled errors. They were insidious, popping up randomly and bringing the hashing process to a grinding halt. After a long, painful debugging session that stretched across time zones, we traced it back to our object storage client code. We were using an errgroup context that would prematurely cancel ongoing background reads if another goroutine in the group hit an unrelated error. One goroutine would stumble, and the context would yank the rug out from under all the others that were still happily reading data from S3. One targeted fix in &lt;code&gt;PR #13692 (+151 −24)&lt;/code&gt; later, that beast was slain.&lt;/p&gt;
    &lt;p&gt;Then, S3 itself began to fight back. We started hitting the &lt;code&gt;DeleteObjects&lt;/code&gt; API limit. The API can only delete 1000 keys in a single request, and our cleanup process for temporary hash files was naively trying to delete thousands at once. In &lt;code&gt;PR #13705 (+110 −16)&lt;/code&gt;, I refactored our &lt;code&gt;DeleteMany&lt;/code&gt; function to be smarter, splitting the requests into 1000-object chunks and firing them off concurrently. Another head lopped off the hydra. Then came the intermittent &lt;code&gt;InvalidPart&lt;/code&gt; errors on large multipart uploads, which I tackled by making our S3 uploader configuration more robust and tuneable, giving us more control over part sizes and concurrency. Each fix felt like a victory, but the next problem was always just around the corner, waiting.&lt;/p&gt;
    &lt;code&gt;operation error S3: DeleteObjects, https response error StatusCode: 400,
RequestID: 022d5d98230001978435203f0407cd5df403dd06, HostID: akoWM6KLQQfI,
api error MalformedXML: The XML you provided was not well-formed...&lt;/code&gt;
    &lt;code&gt;upload multipart failed...
cause: operation error S3: CompleteMultipartUpload, exceeded maximum number
of attempts, 3,
api error InvalidPart: One or more of the specified parts could not be found.&lt;/code&gt;
    &lt;p&gt;While firefighting, we uncovered another massive optimization, almost by accident. On June 20th, I posted the results of a deep-dive benchmark into our block metadata loading strategy. The results, laid out in a detailed doc, were a slap in the face. We were loading way, way too much data from Postgres before we even got to the pruning stage. The dominant cost was fetching and deserializing the heavy stats data for blocks that we were just going to throw away anyway.&lt;/p&gt;
    &lt;p&gt;This revelation led directly to the "lazy loading" optimization in &lt;code&gt;PR #13737 (+1,807 −2,891)&lt;/code&gt;. It was a simple but profound change in philosophy. I introduced a &lt;code&gt;block.Tiny struct&lt;/code&gt;, a minimal, skeletal representation of a block containing only what was absolutely necessary for pruning. The query runner would load just these tiny structs for all candidate blocks, run them through the Haydex and zero-matches cache pruners, and only then go back to the database to fetch the full, heavy block metadata for the handful of blocks that survived.&lt;/p&gt;
    &lt;code&gt;// Tiny is a minimal representation of a block, used for efficient pruning.
type Tiny struct {
	Num          Num
	CompactionID uint32
}&lt;/code&gt;
    &lt;p&gt;The impact was a staggering 12.7x speedup for queries where pruning was highly effective. I was seeing the effect of compounding optimizations. The faster catalog made queries viable. The distributed indexer made large-scale filters possible. And the lazy loading made the pruning process itself lightning fast. Each hard-won victory amplified the next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Week 4 (June 28): Ludicrous speed&lt;/head&gt;
    &lt;p&gt;By the last week of June, it felt like Haydex had achieved escape velocity. All the pieces were finally clicking into place. The distributed indexer was stable and chewing through backlogs. The query path was hardened and optimized. The low-hanging fruit had been picked, and I had started to climb the tree towards the real rewards..&lt;/p&gt;
    &lt;p&gt;The weekly update email from July 5th told the story. We had achieved 110 billion rows per second effective throughput in production testing against Hyperscale Customer's live, trillion-scale workloads. This wasn't a sterile lab benchmark with synthetic data. This was real-world performance on a massive, messy, production dataset. This was the moment I knew I had built something special.&lt;/p&gt;
    &lt;p&gt;But we kept pushing.&lt;/p&gt;
    &lt;p&gt;On July 6th, I ran a query with Haydex enabled but with our zero-matches cache (a separate, existing sub-system) turned off. The result popped up on my screen: an effective throughput of 178,600,000,000 rows per second. Then, for the grand finale, we re-enabled the zero-matches cache on the already-pruned results. The synergy was incredible. The two systems, working in concert, peaked at an effective throughput of 673,850,000,000 rows per second.&lt;/p&gt;
    &lt;p&gt;These weren't just vanity metrics. They were the result of a month of compounded, hard-won optimizations. The fast catalog lookups, the hyper-efficient hashing, the distributed indexing, the lazy block metadata loading, a new selective query ramp-up strategy, and a more robust filter normalization system. Even the small details mattered, like improving the query progress reporting so the UI wouldn't look like it was frozen during these new, ultra-fast pruning phases. It all added up.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we hammered into our heads&lt;/head&gt;
    &lt;p&gt;This wasn't just building a feature; it was a month-long, high-stakes crash course in systems engineering. Here are the lessons learned in the crucible.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Design docs don't survive first contact. V0 was a beautiful corpse. It was elegant, logical, and theoretically sound. It was also completely wrong. It taught us a brutal lesson: I/O isn't a detail you figure out later; it's the main character. The "death by a thousand GETs" that killed V0 proved that you can't bolt on performance. You have to design for the harsh, unforgiving physics of the network and storage from day one.&lt;/item&gt;
      &lt;item&gt;The profiler is your only god. Our most profound wins weren't moments of genius. It was an act of forensics. We weren't being clever; we were just staring at a flame graph until our eyes bled and we noticed something catastrophically stupid we were doing. The profiler has no opinions. It doesn't care about your elegant design. It just shows you the brutal, unvarnished truth. You just have to be willing to look.&lt;/item&gt;
      &lt;item&gt;Performance is a game of whack-a-mole. This is the relentless, Sisyphean reality of optimization. You push the boulder of one bottleneck up the hill only to have another, bigger one roll right back down at you. We fixed the database, so hashing became the problem. We fixed hashing, so memory became the problem. We fixed memory, so S3 API limits became the problem. There is no glorious, final victory, only the grind of the next profile, the next trace, the next bottleneck to be smashed.&lt;/item&gt;
      &lt;item&gt;Speed is not free. We made a conscious, unapologetic decision to build a turbocharger, not a more efficient sedan. And turbochargers cost money, parts, and expertise. We embraced the cost. The goal wasn't to make something cheap run a little faster; it was to build a premium capability that delivered an order-of-magnitude performance win.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The hydra is slain, but more heads will grow&lt;/head&gt;
    &lt;p&gt;The Haydex saga is far from over. We achieved ludicrous speed, but now the work shifts to taming the beast we've created and making it smarter. The road ahead isn't a checklist; it's the next set of boss battles.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tiered indexing: The current fixed-interval indexing window is a blunt instrument. We want scalpels. The next evolution is a tiered system with "hot" indexes forged for data just seconds old, giving our users near-instant acceleration. These will then be gracefully compacted into "warm" and "cold" tiers over time.&lt;/item&gt;
      &lt;item&gt;The Janitor service: Right now, we create indexes, but we don't clean up after ourselves. An index whose data has been deleted by retention policies is just expensive digital garbage. We're building a janitor service that will relentlessly hunt down and vaporize these orphaned indexes, keeping the system lean and mean.&lt;/item&gt;
      &lt;item&gt;The never-ending hunt: And the search for bottlenecks continues. We're looking at how to teach Haydex to prune even more exotic APL queries. The hydra always grows new heads. We'll be ready.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Built in a month&lt;/head&gt;
    &lt;p&gt;The journey from "The Grand Haydex Revival" to a fully distributed, production-hardened system pushing 178.6 billion rows per second took just over a month. Looking back, it's striking how much ground was covered-wrestling with database performance, memory limits, distributed systems complexity, and the infuriating quirks of third-party APIs. Each problem forced me to measure, learn, and refactor. What emerged wasn't just another feature, but something that quietly shifts what's possible across the entire Axiom platform.&lt;/p&gt;
    &lt;p&gt;None of this would have been possible without standing on the shoulders of giants. Our work relies heavily on brilliant open-source projects. It feels particularly fitting to give a special thanks to Daniel Lemire, whose work inspired this entire project.&lt;/p&gt;
    &lt;p&gt;This is what happens when you combine necessity, obsession, and willingness to chase performance into the deepest corners of the system. Haydex didn't just solve our query speed problem-it fundamentally changed what we thought was possible.&lt;/p&gt;
    &lt;p&gt;From a failed experiment to 178.6 billion rows per second in 30 days. Not bad for a month's work.&lt;/p&gt;
    &lt;p&gt;The real victory isn't the numbers, though they're satisfying. It's that we proved you can take a seemingly impossible problem, break it down into solvable pieces, and build something that shifts the entire performance envelope of your platform. Hyperscale Customer went from struggling with ultra-slow queries to interactive experience on trillion-row datasets. That's the kind of transformation that makes all the late nights and S3 API battles worth it.&lt;/p&gt;
    &lt;p&gt;The hydra will grow new heads. There will always be another bottleneck, another scaling challenge, another "impossible" performance target. But now we know something we didn't know before: with the right approach, enough caffeine, and a willingness to throw out everything that doesn't work, you can build systems that redefine what fast means.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45376559</guid><pubDate>Thu, 25 Sep 2025 18:07:49 +0000</pubDate></item><item><title>Athlon 64: How AMD turned the tables on Intel</title><link>https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/</link><description>&lt;doc fingerprint="e70d5115fd2265ff"&gt;
  &lt;main&gt;
    &lt;p&gt;22 years ago, on September 23, 2003, AMD changed the game for x86 once and for all. They released the Athlon 64 CPU, a chip that did something Intel didn’t want. Intel didn’t want to extend x86 to 64 bits. But when AMD did it, it forced Intel to clone AMD, rather than the other way around.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Intel didn’t want to go 64-bit&lt;/head&gt;
    &lt;p&gt;Even in 2001, x86 had decades of baggage attached to it. It was a 32-bit architecture that had been extended from a 16-bit architecture. But that in turn had been extended from an 8-bit CPU design from 1972 that, believe it or not, originated at Datapoint, not Intel.&lt;/p&gt;
    &lt;p&gt;This was great for backward compatibility. 8-bit applications were very easy to port to x86 in the early 1980s, and those early DOS applications still ran flawlessly on modern systems 30 years later. For that matter, it’s not impossible to get them running even today.&lt;/p&gt;
    &lt;p&gt;Removal of the ability to run 16-bit applications in 64-bit Windows was a design decision, not a technical limitation.&lt;/p&gt;
    &lt;p&gt;Intel wanted to start over to go 64-bit. Without having to worry about backward compatibility, they could design something that would be faster and more efficient. In theory at least, it would be able to scale higher in clock speed. And there was no question a new design would outperform a theoretical 64-bit x86 when running at the same speed because of efficiency.&lt;/p&gt;
    &lt;p&gt;And if you are cynical, there was one more motivation. If Intel could start over, they wouldn’t have to worry about competing CPU designs, at least not for a very long time. The new design would be encumbered with so many patents, it might be 20 years before someone could clone it.&lt;/p&gt;
    &lt;p&gt;Keep in mind that in 2003, not only was AMD in the picture, but Transmeta was still in the picture, and Cyrix was fading but not completely gone.&lt;/p&gt;
    &lt;p&gt;Starting over with a new CPU architecture outright was massively attractive to Intel.&lt;/p&gt;
    &lt;p&gt;This new 64-bit architecture wasn’t theoretical, either. Intel was producing it. It was called Itanium, and Intel first released it in June 2001.&lt;/p&gt;
    &lt;head rend="h2"&gt;AMD’s risky bet and why they made it&lt;/head&gt;
    &lt;p&gt;AMD was well aware of the shortcomings of extending x86 to 64 bits. And they did it anyway. For them, the stakes were completely different.&lt;/p&gt;
    &lt;p&gt;AMD knew that if Itanium caught on, that would be the end for them as a CPU company, unless maybe they wanted to become just another ARM licensee. Being just another ARM licensee is more attractive in 2025 than it was in 2003.&lt;/p&gt;
    &lt;p&gt;But they could see Itanium wasn’t catching on. It had its uses, and it was doing well enough in those niches, but Windows on Itanium was a non-starter. So much so, The Register called it “Itanic.”&lt;/p&gt;
    &lt;p&gt;AMD bet that there would be appeal in a 64-bit architecture that was fully backward compatible with x86 and natively ran 32-bit applications at full speed. People would be able to run 32-bit Windows and 32-bit applications on it if they needed to, and then when they were ready for 64-bit software, the hardware was there and ready to go. And they could continue to run 32-bit apps in 64-bit operating systems as long as needed to ease the transition.&lt;/p&gt;
    &lt;p&gt;The transition to 32 bits took a decade. AMD reasoned more people would be willing to upgrade to 64 bits if they made that transition as similar as the transition from the 286 to the 386 as possible.&lt;/p&gt;
    &lt;p&gt;They believed the market would willingly trade lower 64-bit performance in the long term for better 32-bit performance right away. They also believed that if Microsoft was willing to build Windows on Itanium, they would be willing to take a chance on 64-bit x86 as well.&lt;/p&gt;
    &lt;p&gt;So on September 23, 2003, AMD launched its Athlon 64, the first 64-bit x86 CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why the Athlon 64 was a hit&lt;/head&gt;
    &lt;p&gt;AMD64 was everything AMD hoped it would be. It was backward compatible with 32-bit x86. The 64-bit builds of Windows weren’t available immediately, and they didn’t catch on immediately, but you cannot say nobody used them. People did, in fact, use them. In late 2005, I was in charge of administering the complimentary antivirus software that Charter Communications provided to its subscribers. I’m not going to say say someone called me every day wanting 64-bit antivirus for 64-bit Windows. But it did happen once a week.&lt;/p&gt;
    &lt;p&gt;The transition took at least as long as AMD expected. When I finally bought an Athlon 64 in 2011, I found native 64-bit software was still scarce. I’m an outspoken Firefox fan; the reason I briefly switched to Google Chrome was to get a 64-bit web browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Athlon 64 in the enterprise&lt;/head&gt;
    &lt;p&gt;A few months later, I got a better job with more pay and better growth potential. I can’t talk a lot about the job, but I was administering a mission critical system that ran on Windows, mostly on Dell hardware. I mention Dell because they were exclusively an Intel vendor for years. Cofounder and longtime AMD CEO Jerry Sanders once said of Michael Dell, “I can’t sell him a[n AMD] K6 no matter what I do.”&lt;/p&gt;
    &lt;p&gt;It was the Athlon 64 that made Dell relent and finally start using AMD CPUs. Not only were they using them on desktop systems, but they were putting AMD CPUs in servers, an idea that would have been extremely controversial 5 years before. At least in the circles I ran in.&lt;/p&gt;
    &lt;p&gt;The Athlon 64 caught on because, in spite of its name, it was an outstanding 32-bit CPU. It was faster than an Intel CPU running at the same clock rate, and it used less power as well. The power consumption was the key to getting into the data center. The Intel name was a security blanket, even though AMD had been making x86 CPUs exactly as long as Intel. But certain decision makers bought Intel marketing and saw AMD as a second tier brand.&lt;/p&gt;
    &lt;p&gt;The thing is, when you have a data center with hundreds of systems in it, the money you save on a more efficient CPU really talks.&lt;/p&gt;
    &lt;p&gt;Replacing Intel Prescott-based servers with AMD64 servers was not a universally popular idea. But you could tell a difference when you were standing behind a rack full of Intel-based servers versus a rack full of AMD based servers. The Intels ran hotter.&lt;/p&gt;
    &lt;p&gt;From an uptime perspective, we couldn’t see a difference. The performance metrics I collected showed there was a slight difference, and that difference was in AMD’s favor. So the AMD critics quickly ate their words.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intel giving in and cloning AMD64&lt;/head&gt;
    &lt;p&gt;In 2004, Intel wrote off the Itanium and cloned AMD64. They called it Intel64, but it was a blatant copy of the AMD implementation. A quirk in the agreements that allowed AMD to use the x86 instruction set also gave Intel the rights to use the AMD64 instructions. So there was nothing illegal about what Intel did. Itanium continued to see use in specialized applications, but Intel quietly discontinued it in 2020.&lt;/p&gt;
    &lt;p&gt;AMD and Intel have been chasing and catching each other ever since. One of them will pass the other for a CPU generation or two, and then they will change positions. It’s not terribly different from the situation in 1999 with the original Athlon, when AMD outperformed Intel for the first time. The question in everyone’s mind was whether they would do it a second time. The Athlon 64 was the second time.&lt;/p&gt;
    &lt;p&gt;It was a big step forward. Eight years before, AMD was trying to pass off a high-clocked 486 as a Pentium equivalent. With the Athlon 64, AMD was innovating.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45376605</guid><pubDate>Thu, 25 Sep 2025 18:09:47 +0000</pubDate></item><item><title>Electron-based apps cause system-wide lag on macOS 26 Tahoe</title><link>https://github.com/electron/electron/issues/48311</link><description>&lt;doc fingerprint="c41447174b3717fe"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 16.4k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Maintainer update&lt;/head&gt;
    &lt;p&gt;From @MarshallOfSound (#48311 (comment)):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hey folks, anyone experiencing this issue can you please raise a Feedback (via Feedback Assistant) with Apple. Make sure you send it while the issue is occurring and ensure you include a sysdiagnose with your report (I think that's automatic now, but check the box if there's a box).&lt;/p&gt;
      &lt;p&gt;We need a lot more to go on and this is likely a macOS issue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Preflight Checklist&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I have read the Contributing Guidelines for this project.&lt;/item&gt;
      &lt;item&gt;I agree to follow the Code of Conduct that this project adheres to.&lt;/item&gt;
      &lt;item&gt;I have searched the issue tracker for a bug report that matches the one I want to file, without success.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Electron Version&lt;/head&gt;
    &lt;p&gt;37.3.1&lt;/p&gt;
    &lt;head rend="h3"&gt;What operating system(s) are you using?&lt;/head&gt;
    &lt;p&gt;macOS&lt;/p&gt;
    &lt;head rend="h3"&gt;Operating System Version&lt;/head&gt;
    &lt;p&gt;macOS 26 Tahoe RC&lt;/p&gt;
    &lt;head rend="h3"&gt;What arch are you using?&lt;/head&gt;
    &lt;p&gt;arm64 (including Apple Silicon)&lt;/p&gt;
    &lt;head rend="h3"&gt;Last Known Working Electron version&lt;/head&gt;
    &lt;p&gt;N/A, issue only persists since macOS 26&lt;/p&gt;
    &lt;head rend="h3"&gt;Does the issue also appear in Chromium / Google Chrome?&lt;/head&gt;
    &lt;p&gt;No&lt;/p&gt;
    &lt;head rend="h3"&gt;Expected Behavior&lt;/head&gt;
    &lt;p&gt;Smooth 120fps experience even when Electron-apps are open or not minimized&lt;/p&gt;
    &lt;head rend="h3"&gt;Actual Behavior&lt;/head&gt;
    &lt;p&gt;Using an M1 Max MacBook Pro, having Electron-based apps open / not minimized causes a huge lag.&lt;lb/&gt; CPU and GPU usage remains low, but if I have Discord and VS Code open, moving windows, scrolling is stuttery. It happens even when only Discord is open but it gets worse if I open a second Electron app.&lt;lb/&gt; This is kind of weird because while having Discord open and I'm in Chrome, the lag still occurs, but it's fixed if I minimize Discord (even though Chrome is fully in focus and maximized). This happens since upgrading to macOS 26 RC, macOS 15 didn't have this issue.&lt;lb/&gt; There is a similar lag if I open Settings - Wallpapers, moving the Settings window is laggy then (looks like 60fps instead of 120).&lt;/p&gt;
    &lt;head rend="h3"&gt;Testcase Gist URL&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional Information&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h3"&gt;Assignees&lt;/head&gt;
    &lt;head rend="h3"&gt;Type&lt;/head&gt;
    &lt;head rend="h3"&gt;Projects&lt;/head&gt;
    &lt;p&gt;Status&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45376977</guid><pubDate>Thu, 25 Sep 2025 18:36:20 +0000</pubDate></item><item><title>Tracing JITs in the Real World CPython Core Dev Sprint</title><link>https://antocuni.eu/2025/09/24/tracing-jits-in-the-real-world--cpython-core-dev-sprint/</link><description>&lt;doc fingerprint="23330e932cf522d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tracing JITs in the real world @ CPython Core Dev Sprint¶&lt;/head&gt;
    &lt;p&gt;Last week I got to take part in the CPython Core Developer Sprint in Cambridge, hosted by ARM and brilliantly organized by Diego Russo -- about ~50 core devs and guests were there, and I was excited to join as one of the guests.&lt;/p&gt;
    &lt;p&gt;I had three main areas of focus:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;C API: this was a follow up of what we discussed at the C API summit at EuroPython. The current C API is problematic, so we are exploring ideas for the development of PyNI (Python Native Interface), whose design will likely be heavily inspired by HPy. It's important to underline that this is just the beginning and the entire process will require multiple PEPs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;fancycompleter This is a small PR which I started months ago, to enable colorful tab completions within the Python REPL. I wrote the original version of fancycompleter 15 years ago, but colorful completions work only in combination with PyREPL. Now PyREPL is part of the standard library and enabled by default, so we can finally upstream it. I hope to see it merged soon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"JIT stuff": I spent a considerable amount of time talking to the people who are working on the CPython JIT (in particular Mark, Brandt, Savannah, Ken Jin and Diego). Knowledge transfer worked in both ways: I learned a lot about the internal details of CPython's JIT, and conversely I shared with them some of the experience, pain points and gut feelings which I got by working many years on PyPy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In particular, on the first day I presented a talk titled Tracing JIT and real world Python (slides and source code).&lt;/p&gt;
    &lt;p&gt;What follows is an annotated version of the slides.&lt;/p&gt;
    &lt;p&gt;CPython's new JIT and PyPy's JIT share fundamental similarities, as they're both tracing JITs.&lt;/p&gt;
    &lt;p&gt;I spent ~7 years of my career optimizing existing code for PyPy at a high-frequency trading firm, and I realized that I'm probably one of the few people in the world with actual experience in optimizing real world Python code for a tracing JIT.&lt;/p&gt;
    &lt;p&gt;I expect that some of the challenges which I faced will still be valid also for CPython, and I wanted to share my experience to make sure that CPython core devs are aware of them.&lt;/p&gt;
    &lt;p&gt;One lesson which I learned is that the set of benchmarks in &lt;code&gt;pyperformance&lt;/code&gt; are
a good starting point, but they are not entirely representative of what you
find in the wild.&lt;/p&gt;
    &lt;p&gt;The main goal of the talk is not to present solutions to these problems, but to raise awareness that they exist.&lt;/p&gt;
    &lt;p&gt;Until now CPython's performance has been particularly predictable, there are well established "performance tricks" to make code faster, and generally speaking you can mostly reason about the speed of a given piece of code "locally".&lt;/p&gt;
    &lt;p&gt;Adding a JIT completely changes how we reason about performance of a given program, for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;JITted code can be very fast if your code conforms to the heuristics applied by the JIT compiler, but unexpectedly slow(-ish) otherwise;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the speed of a given piece of code might depend heavily on what happens elsewhere in the program, making it much harder to reason about performance locally.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The end result is that modifying a line of code can significantly impact seemingly unrelated code. This effect becomes more pronounced as the JIT becomes more sophisticated.&lt;/p&gt;
    &lt;p&gt;The CPython JIT is still pretty new and doesn’t give huge speedups yet. I expect that as it gets faster, its performance will start looking more and more like PyPy’s.&lt;/p&gt;
    &lt;p&gt;I delivered this talk at the Core Dev Sprint: I expected my audience to be familiar with CPython's JIT, and wanted to draw parallels with PyPy's one.&lt;/p&gt;
    &lt;p&gt;Since the audience of this blog is different, let me briefly explain CPython's JIT first.&lt;/p&gt;
    &lt;p&gt;The explanations of both JITs are necessarily short, incomplete and highly simplified.&lt;/p&gt;
    &lt;head rend="h4"&gt;CPython JIT 101¶&lt;/head&gt;
    &lt;p&gt;Python source code is turned into bytecode. Bytecode is a sequence of "opcodes" (&lt;code&gt;LOAD_FAST&lt;/code&gt;, &lt;code&gt;BINARY_OP&lt;/code&gt;, etc.), and the CPython VM is an
interpreter for those opcodes. Historically the VM was written by hand, and the
main loop consisted of a big &lt;code&gt;switch&lt;/code&gt; statement which executed the code
corresponding to each opcode.&lt;/p&gt;
    &lt;p&gt;Nowadays things are different: the opcodes are written in a special DSL and the main interpreter loop is generated from this DSL. Additionally, the DSL describes how each opcode can be decomposed into multiple "microops".&lt;/p&gt;
    &lt;p&gt;When the interpreter detects a "hot loop", it starts the JIT. The JIT retroactively looks at the opcodes which were executed in the last iteration of the loop, and creates a "linear trace" which contains the equivalent microops. This process is called trace projection and the result is an unoptimized trace of microops.&lt;/p&gt;
    &lt;p&gt;Then, the JIT can produce an optimized trace, by reordering and removing redundant microops. Finally, the optimized trace is turned into executable code using the "copy &amp;amp; patch" technique.&lt;/p&gt;
    &lt;head rend="h4"&gt;PyPy JIT 101¶&lt;/head&gt;
    &lt;p&gt;CPython's Python interpreter is written in C, and then compiled into an executable by &lt;code&gt;gcc&lt;/code&gt; (or any other C compiler).&lt;/p&gt;
    &lt;p&gt;Similarly, PyPy's Python interpreter is written in RPython, and then compiled into an executable by &lt;code&gt;rpython&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Under the hood, &lt;code&gt;rpython&lt;/code&gt; applies two separate transformations to the source
code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;it turns each function into C code, which is then fed to&lt;/p&gt;&lt;code&gt;gcc&lt;/code&gt;to get the final executable;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;it turns each function into "jitcodes", which is a way to represent RPython's IR (internal representation). For each RPython function, the final&lt;/p&gt;&lt;code&gt;./pypy&lt;/code&gt;executable contains its compiled representation (generated by GCC) and its jitcode representation (embedded as static data into the executable).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In a way, RPython's jitcodes are equivalent to CPython's microops, as they are a low-level representation of the logic of each opcode.&lt;/p&gt;
    &lt;p&gt;When the interpreter detects a hot loop, it enters trace recording mode, which is essentially an interpreter which executes the jitcodes: the result is a linear unoptimized trace of all the jitcodes which were actually executed.&lt;/p&gt;
    &lt;p&gt;Similarly to CPython, PyPy then produces an optimized trace, which is then sent to the JIT backend for actual native code generation.&lt;/p&gt;
    &lt;p&gt;Tracing JITs work by recording a trace of all microops which are executed. The optimizer can then reason about what happens in the trace and remove unneeded operations.&lt;/p&gt;
    &lt;p&gt;However, sometimes we encounter some operation which is a black box from the point of view of the tracer: we call them "trace blocker", because the tracing JIT cannot see through them. In the case of CPython, this happens for example, whenever we call any function implemented in C (because it doesn't have any correspondent "microop").&lt;/p&gt;
    &lt;p&gt;This is a simple function that computes &lt;code&gt;pi&lt;/code&gt;, generated by ChatGPT.  Its
precise content is not important: what matters is that it's a nice purely
numerical loop that the PyPy JIT can optimize very well.&lt;/p&gt;
    &lt;p&gt;Same function as above, with a call to &lt;code&gt;hic_sunt_leones()&lt;/code&gt;. This is actually
an empty function which does absolutely nothing, but annotated in a
special way so that the PyPy JIT cannot "enter" it, so it effectively behaves
as trace blocker.&lt;/p&gt;
    &lt;p&gt;In this example we use the special &lt;code&gt;pypyjit.residual_call&lt;/code&gt; to simulate a trace
blocker, but in real life we get it whenever we have a call to any
non-traceable function, in particular C extensions.&lt;/p&gt;
    &lt;p&gt;The clean version runs 42x faster on PyPy than CPython - that's the JIT working perfectly. But with just one untraceable function call added to the loop, PyPy slows down to only 1.8x faster than CPython. That single line destroyed most of the JIT's effectiveness!&lt;/p&gt;
    &lt;p&gt;This happens because after the call the optimizer no longer knows whether its assumptions about the world are still true, and thus must be much more conservative.&lt;/p&gt;
    &lt;p&gt;I fear that for CPython, this will turn out to be a much bigger problem than for PyPy, for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;nowadays it's virtually impossible to run Python code without using any C extension, either directly or indirectly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;by construction, PyPy's JIT can see much more than CPython's JIT. Remember the slide about "jitcodes": any RPython function gets a "jitcodes" equivalent, which means that the JIT can automatially trace inside builtins and internals of the interpreter, whereas CPython can trace only inside pure python code.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, PyPy's JIT can trace through &lt;code&gt;range()&lt;/code&gt;, &lt;code&gt;zip&lt;/code&gt;, and &lt;code&gt;enumerate()&lt;/code&gt;
automatically. CPython's JIT currently cannot because they are implemented in
C. CPython could add special cases for these common functions, but the
general approach doesn't scale.&lt;/p&gt;
    &lt;p&gt;The second big problem is what I call "data driven control flow". This example has been autogenerated by ChatGPT and it's completely silly, but it's a good representation of what happens in real life code.&lt;/p&gt;
    &lt;p&gt;In this example, &lt;code&gt;fn&lt;/code&gt; takes 9 variables, each of them can be &lt;code&gt;None&lt;/code&gt; or a
number. The function starts with a sequence of &lt;code&gt;if &amp;lt;var&amp;gt; is None: ...&lt;/code&gt;. The
function is then called repeatedly in a loop.&lt;/p&gt;
    &lt;p&gt;One of the assumption of tracing JITs is that control flow tends to stay on the "hot path", and that it's enough to optimize that to get good performance.&lt;/p&gt;
    &lt;p&gt;But in a case like this, each combination of &lt;code&gt;None&lt;/code&gt;ness selects a different
path, and if we assume the data is evenly distributed, we find out that
there is no hot path.&lt;/p&gt;
    &lt;p&gt;Let's see what happens when we execute on CPython and PyPy:&lt;/p&gt;
    &lt;p&gt;PyPy without JIT is "only" 2.3x slower than CPython, but when we enable the JIT, it becomes much worse. This happens because of an exponential explosion of code paths seen by the JIT.&lt;/p&gt;
    &lt;p&gt;In a normal compiler, an &lt;code&gt;if&lt;/code&gt; statement is compiled as a diamond, and the
control flow merges after each &lt;code&gt;if&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;A tracing JIT by definition follows what's happening during a concrete execution, so it sees only a concrete path in the control flow, with "guards" to ensure correctness:&lt;/p&gt;
    &lt;p&gt;When &lt;code&gt;guard(a is None)&lt;/code&gt; fails enough times, we create a "bridge" and record
another linear trace, following again the concrete control flow that happens
now:&lt;/p&gt;
    &lt;code&gt;          guard(a is None) ----&amp;gt; FAIL (side exit)
            /                         \
           /                           \
        a = 0                          pass
           \                             \
            \                             \
    guard(b not None)              guard(b not None)
            /                             /
           /                             /
        b = 0                         b = 0
           \                             \
            \                             \
           ...                           ...
&lt;/code&gt;
    &lt;p&gt;Note how &lt;code&gt;b = 0&lt;/code&gt; is effectively duplicated now. By design, PyPy's JIT never
merges execution flow.&lt;/p&gt;
    &lt;p&gt;Looking inside &lt;code&gt;PYPYLOG&lt;/code&gt; confirms our theory: we get "exponential
tracing". The JIT has to compile separate optimized code for every unique
combination of which parameters are None and which aren't. With 9 parameters,
that could be up to 512 different combinations!&lt;/p&gt;
    &lt;p&gt;One possible mitigation is to rewrite conditional code to be "branchless" - using arithmetic tricks instead of if statements. But this makes code ugly and unreadable, and it's not always possible.&lt;/p&gt;
    &lt;p&gt;Despite years of working on this, I never found a really good solution. There were cases in which we had to continue running some piece of code on CPython because I never managed to make the PyPy version faster.&lt;/p&gt;
    &lt;p&gt;This pattern happens quite a lot, although often is more subtle: in this silly example all the &lt;code&gt;if&lt;/code&gt;s are nicely grouped together at the start, but in a long
trace they can be scattered in multiple places, and any kind of control flow
contributes to the problem, not only &lt;code&gt;if&lt;/code&gt;s. In Python, this includes any kind
of dynamic dispatch, exceptions, etc.&lt;/p&gt;
    &lt;p&gt;One possible solution for CPython's JIT is to try to merge (some) traces to avoid or limit the exponential explosion. However, it is worth underlining that tracing JITs shine precisely when they can optimize a long linear trace: if you try to compile shorter traces, you might quickly end up in a situation which is equivalent to the "trace blocker" problem described earlier.&lt;/p&gt;
    &lt;p&gt;I suspect this might be a fundamental limitation of tracing JITs.&lt;/p&gt;
    &lt;p&gt;Compared to the other two problems, this is less serious, but it's worth mentioning because of prevalence of &lt;code&gt;async&lt;/code&gt; (and thus implicitly generators)
in modern Python.&lt;/p&gt;
    &lt;p&gt;Here's another silly function that counts Pythagorean triples using nested loops. This is our baseline version using plain loops.&lt;/p&gt;
    &lt;p&gt;Here's the same algorithm refactored to use a generator function for the nested iteration. The "state of iteration" is implicitly stored inside the local variables of frame object associated to the &lt;code&gt;range_product&lt;/code&gt; generator.&lt;/p&gt;
    &lt;p&gt;Here's the same functionality implemented as a traditional iterator class. The "state of iteration" is explicitly stored as attributes of &lt;code&gt;RangeProductIter&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;On CPython, the generator version is ~29% slower than the explicit loops. The iterator class is much slower, as one would intuitively expect.&lt;/p&gt;
    &lt;p&gt;However, on PyPy we see different results: &lt;code&gt;RangeProductIter&lt;/code&gt; is basically
same speed as the baseline, while the generator version is slower. This
happens because in the case of &lt;code&gt;RangeProductIter&lt;/code&gt; the JIT is able to see the whole
lifetime of the object and optimize it away entirely: instance variables
become local variables, the call to &lt;code&gt;__next__&lt;/code&gt; is inlined and we get the
equivalent of explicit nested loops.&lt;/p&gt;
    &lt;p&gt;However, generators are required to create a frame object and represent a fundamental case in which the JIT cannot trace through them effectively. In more complex real-world scenarios, we saw much worse slowdowns than these examples show.&lt;/p&gt;
    &lt;p&gt;This is a collection of other miscellaneous problems that I had to deal with. Generally speaking, we lack good support for tooling and profilers. CPython needs to have a good story to explain people how to understand what's happening when the JIT is enabled.&lt;/p&gt;
    &lt;p&gt;Warmup is another big problem: in PyPy, very short programs tend to be slower than CPython because JITting costs. Moreover warmup is not an easily definable phase, as the linked paper shows. This is an area where currently CPython shines, as its JIT is very fast. I think that it will become slightly slower when it tries to optimize more aggressively, but hopefully warmup will overall be a lesser problem than on PyPy.&lt;/p&gt;
    &lt;p&gt;Moreover, it's very easy to accidentally make your code 2x, 5x or even 10x slower by changing seemingly innocent pieces of code. This is another reason why good tooling is essential.&lt;/p&gt;
    &lt;p&gt;Finally, the "long tail of JITting": every loop and every guard gets a counter, and we start JITting when it reaches a threshold. Given a sufficiently long running program, all counters reach the threshold eventually and we end up JITting much more than necessary, using too much memory and/or thrashing the cache. In many cases I found beneficial to just disable the JIT "after a while", with manually tuned heuristics.&lt;/p&gt;
    &lt;p&gt;These are slides which I didn't show during the live presentation, and show a case where a tracing JIT can shine: since the JIT sees a complete trace of an entire loop (including nested calls) it can easily removes a lot of temporary objects which usually penalize Python performance.&lt;/p&gt;
    &lt;p&gt;In many cases, we can get the famous "zero-cost abstractions".&lt;/p&gt;
    &lt;p&gt;Let's look at a concrete example. We need to compute the barycenter of triangles that are serialized in a binary format. Each triangle has three points, each point has x and y coordinates. This simulates real world protocols such as protobuf, capnproto, etc.&lt;/p&gt;
    &lt;p&gt;This is what we use a a baseline: a bare loop, using &lt;code&gt;struct.unpack_from&lt;/code&gt; to read 6 floats at a time.&lt;/p&gt;
    &lt;p&gt;Here's the "proper" object-oriented approach, similar to how modern serialization libraries work. We create &lt;code&gt;Triangle&lt;/code&gt; and &lt;code&gt;Point&lt;/code&gt; classes that
provide a nice API for accessing the binary data. Each property access creates
new objects and calls struct.unpack_from. This is much more readable and
reusable, but creates many temporary objects.&lt;/p&gt;
    &lt;p&gt;Here's how you'd use the object-oriented API. The code is much cleaner and more readable than the bare loop version. But notice how many object creations are happening: one &lt;code&gt;Triangle&lt;/code&gt; object, six &lt;code&gt;Point&lt;/code&gt; objects, plus all the
intermediate tuples from &lt;code&gt;struct.unpack_from&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;As expected, on CPython &lt;code&gt;read_proto&lt;/code&gt; is much slower than the bare one,
roughly 6x slower. However, PyPy can fully optimize away all the
abstraction overhead introduced by &lt;code&gt;Triangle&lt;/code&gt; and &lt;code&gt;Point&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In PyPy jargon we call this form of allocation removal "virtuals" (because we create "virtual objects" whose fields are represented as local variables) and it's probably the single most important optimization that PyPy does.&lt;/p&gt;
    &lt;p&gt;During my week in Cambridge I talked extensively with the CPython JIT devs about this and I hope I convinced them that this is what they should aim for 😊.&lt;/p&gt;
    &lt;p&gt;Note also that &lt;code&gt;read_proto&lt;/code&gt; is actually faster than &lt;code&gt;read_loop&lt;/code&gt;. This
happens because in &lt;code&gt;read_loop&lt;/code&gt; we do a single &lt;code&gt;struct.unpack_from('dddddd', ...)&lt;/code&gt;,
while in &lt;code&gt;read_proto&lt;/code&gt; we do a succession of six individual
&lt;code&gt;struct.unpack_from('d', ...)&lt;/code&gt;. It turns out that the JIT is able to trace
into the second form but not into the first, which means that in &lt;code&gt;read_loop&lt;/code&gt;
we actually need to allocate a pseudo-tuple at each iteration.&lt;/p&gt;
    &lt;p&gt;The funny part is that I did not expect to get this result. I had to take the time to analyze the JIT traces of both versions to understand why &lt;code&gt;read_loop&lt;/code&gt; was slower.  This is probably the best explanation of how
counterintuitive it is to reason about performance in a JITted world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments¶&lt;/head&gt;
    &lt;p&gt;Thanks to Carl Friedrich Bolz-Tereick and Hood Chatham for feedback on the slides and the post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45377030</guid><pubDate>Thu, 25 Sep 2025 18:40:22 +0000</pubDate></item></channel></rss>