<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 08 Oct 2025 16:43:14 +0000</lastBuildDate><item><title>Qualcomm to acquire Arduino</title><link>https://www.qualcomm.com/news/releases/2025/10/qualcomm-to-acquire-arduino-accelerating-developers--access-to-i</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45502541</guid><pubDate>Tue, 07 Oct 2025 13:00:08 +0000</pubDate></item><item><title>Vibe engineering</title><link>https://simonwillison.net/2025/Oct/7/vibe-engineering/</link><description>&lt;doc fingerprint="a3d0c07761f5138d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Vibe engineering&lt;/head&gt;
    &lt;p&gt;7th October 2025&lt;/p&gt;
    &lt;p&gt;I feel like vibe coding is pretty well established now as covering the fast, loose and irresponsible way of building software with AI—entirely prompt-driven, and with no attention paid to how the code actually works. This leaves us with a terminology gap: what should we call the other end of the spectrum, where seasoned professionals accelerate their work with LLMs while staying proudly and confidently accountable for the software they produce?&lt;/p&gt;
    &lt;p&gt;I propose we call this vibe engineering, with my tongue only partially in my cheek.&lt;/p&gt;
    &lt;p&gt;One of the lesser spoken truths of working productively with LLMs as a software engineer on non-toy-projects is that it’s difficult. There’s a lot of depth to understanding how to use the tools, there are plenty of traps to avoid, and the pace at which they can churn out working code raises the bar for what the human participant can and should be contributing.&lt;/p&gt;
    &lt;p&gt;The rise of coding agents—tools like Claude Code (released February 2025), OpenAI’s Codex CLI (April) and Gemini CLI (June) that can iterate on code, actively testing and modifying it until it achieves a specified goal, has dramatically increased the usefulness of LLMs for real-world coding problems.&lt;/p&gt;
    &lt;p&gt;I’m increasingly hearing from experienced, credible software engineers who are running multiple copies of agents at once, tackling several problems in parallel and expanding the scope of what they can take on. I was skeptical of this at first but I’ve started running multiple agents myself now and it’s surprisingly effective, if mentally exhausting!&lt;/p&gt;
    &lt;p&gt;This feels very different from classic vibe coding, where I outsource a simple, low-stakes task to an LLM and accept the result if it appears to work. Most of my tools.simonwillison.net collection (previously) were built like that. Iterating with coding agents to produce production-quality code that I’m confident I can maintain in the future feels like a different process entirely.&lt;/p&gt;
    &lt;p&gt;It’s also become clear to me that LLMs actively reward existing top tier software engineering practices:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automated testing. If your project has a robust, comprehensive and stable test suite agentic coding tools can fly with it. Without tests? Your agent might claim something works without having actually tested it at all, plus any new change could break an unrelated feature without you realizing it. Test-first development is particularly effective with agents that can iterate in a loop.&lt;/item&gt;
      &lt;item&gt;Planning in advance. Sitting down to hack something together goes much better if you start with a high level plan. Working with an agent makes this even more important—you can iterate on the plan first, then hand it off to the agent to write the code.&lt;/item&gt;
      &lt;item&gt;Comprehensive documentation. Just like human programmers, an LLM can only keep a subset of the codebase in its context at once. Being able to feed in relevant documentation lets it use APIs from other areas without reading the code first. Write good documentation first and the model may be able to build the matching implementation from that input alone.&lt;/item&gt;
      &lt;item&gt;Good version control habits. Being able to undo mistakes and understand when and how something was changed is even more important when a coding agent might have made the changes. LLMs are also fiercely competent at Git—they can navigate the history themselves to track down the origin of bugs, and they’re better than most developers at using git bisect. Use that to your advantage.&lt;/item&gt;
      &lt;item&gt;Having effective automation in place. Continuous integration, automated formatting and linting, continuous deployment to a preview environment—all things that agentic coding tools can benefit from too. LLMs make writing quick automation scripts easier as well, which can help them then repeat tasks accurately and consistently next time.&lt;/item&gt;
      &lt;item&gt;A culture of code review. This one explains itself. If you’re fast and productive at code review you’re going to have a much better time working with LLMs than if you’d rather write code yourself than review the same thing written by someone (or something) else.&lt;/item&gt;
      &lt;item&gt;A very weird form of management. Getting good results out of a coding agent feels uncomfortably close to getting good results out of a human collaborator. You need to provide clear instructions, ensure they have the necessary context and provide actionable feedback on what they produce. It’s a lot easier than working with actual people because you don’t have to worry about offending or discouraging them—but any existing management experience you have will prove surprisingly useful.&lt;/item&gt;
      &lt;item&gt;Really good manual QA (quality assurance). Beyond automated tests, you need to be really good at manually testing software, including predicting and digging into edge-cases.&lt;/item&gt;
      &lt;item&gt;Strong research skills. There are dozens of ways to solve any given coding problem. Figuring out the best options and proving an approach has always been important, and remains a blocker on unleashing an agent to write the actual code.&lt;/item&gt;
      &lt;item&gt;The ability to ship to a preview environment. If an agent builds a feature, having a way to safely preview that feature (without deploying it straight to production) makes reviews much more productive and greatly reduces the risk of shipping something broken.&lt;/item&gt;
      &lt;item&gt;An instinct for what can be outsourced to AI and what you need to manually handle yourself. This is constantly evolving as the models and tools become more effective. A big part of working effectively with LLMs is maintaining a strong intuition for when they can best be applied.&lt;/item&gt;
      &lt;item&gt;An updated sense of estimation. Estimating how long a project will take has always been one of the hardest but most important parts of being a senior engineer, especially in organizations where budget and strategy decisions are made based on those estimates. AI-assisted coding makes this even harder—things that used to take a long time are much faster, but estimations now depend on new factors which we’re all still trying to figure out.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re going to really exploit the capabilities of these new tools, you need to be operating at the top of your game. You’re not just responsible for writing the code—you’re researching approaches, deciding on high-level architecture, writing specifications, defining success criteria, designing agentic loops, planning QA, managing a growing army of weird digital interns who will absolutely cheat if you give them a chance, and spending so much time on code review.&lt;/p&gt;
    &lt;p&gt;Almost all of these are characteristics of senior software engineers already!&lt;/p&gt;
    &lt;p&gt;AI tools amplify existing expertise. The more skills and experience you have as a software engineer the faster and better the results you can get from working with LLMs and coding agents.&lt;/p&gt;
    &lt;head rend="h4"&gt;“Vibe engineering”, really?&lt;/head&gt;
    &lt;p&gt;Is this a stupid name? Yeah, probably. “Vibes” as a concept in AI feels a little tired at this point. “Vibe coding” itself is used by a lot of developers in a dismissive way. I’m ready to reclaim vibes for something more constructive.&lt;/p&gt;
    &lt;p&gt;I’ve never really liked the artificial distinction between “coders” and “engineers”—that’s always smelled to me a bit like gatekeeping. But in this case a bit of gatekeeping is exactly what we need!&lt;/p&gt;
    &lt;p&gt;Vibe engineering establishes a clear distinction from vibe coding. It signals that this is a different, harder and more sophisticated way of working with AI tools to build production software.&lt;/p&gt;
    &lt;p&gt;I like that this is cheeky and likely to be controversial. This whole space is still absurd in all sorts of different ways. We shouldn’t take ourselves too seriously while we figure out the most productive ways to apply these new tools.&lt;/p&gt;
    &lt;p&gt;I’ve tried in the past to get terms like AI-assisted programming to stick, with approximately zero success. May as well try rubbing some vibes on it and see what happens.&lt;/p&gt;
    &lt;p&gt;I also really like the clear mismatch between “vibes” and “engineering”. It makes the combined term self-contradictory in a way that I find mischievous and (hopefully) sticky.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI DevDay 2025 live blog - 6th October 2025&lt;/item&gt;
      &lt;item&gt;Embracing the parallel coding agent lifestyle - 5th October 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45503867</guid><pubDate>Tue, 07 Oct 2025 14:55:14 +0000</pubDate></item><item><title>IKEA Catalogs 1951-2021</title><link>https://ikeamuseum.com/en/explore/ikea-catalogue/</link><description>&lt;doc fingerprint="71c3125f48ed4455"&gt;
  &lt;main&gt;
    &lt;p&gt;Good question! We know that a lot of people are curious about what the IKEA catalogue has looked like through the ages. The catalogue has always reflected the age and its views on interior design and everyday living, especially in Sweden, but in recent decades also internationally. The catalogue was in print for 70 years, and by digitising all the catalogues we could make them available to everybody. Making the story of IKEA available to as many people as possible is our main task at IKEA Museum. So we hope that the catalogues will bring some joy and nostalgia, and maybe even a few surprises.&lt;/p&gt;
    &lt;head rend="h1"&gt;IKEA catalogue&lt;/head&gt;
    &lt;p&gt;For over 70 years, the IKEA catalogue was produced in Ãlmhult, constantly growing in number, scope and distribution. From the 1950s when Ingvar Kamprad wrote most of the texts himself, via the poppy, somewhat radical 1970s and all the way into the scaled-down 2000s â the IKEA catalogue always captured the spirit of the time. The 2021 IKEA catalogue was the very last one printed on paper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1950s&lt;/item&gt;
      &lt;item&gt;1960s&lt;/item&gt;
      &lt;item&gt;1970s&lt;/item&gt;
      &lt;item&gt;1980s&lt;/item&gt;
      &lt;item&gt;1990s&lt;/item&gt;
      &lt;item&gt;2000s&lt;/item&gt;
      &lt;item&gt;2010s&lt;/item&gt;
      &lt;item&gt;2020s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just like the perception of the home, the catalogue has changed dramatically since 1951, when it was first published. Look in the older catalogues and youâll be amazed at what you find. In fact, youâll probably even have a giggle or two. In the 1950s and 1960s, there are rarely any people in the pictures, and never any children. But in the 1970s there are children playing all over the home, you can see adults smoking and even the occasional political poster on the wall. Browse on to the 1980s IKEA catalogues and the trends have changed again, with shiny fabrics and other fancy materials. In the 1990s homes become more scaled-down and clearly inspired by a Scandinavian tradition. In this way, the IKEA catalogues are a kind of time capsule for you to travel in. And who knows? When we look back at the most recent catalogues in 10 or 20 yearsâ time, weâll probably shake our heads and give a sigh.&lt;/p&gt;
    &lt;p&gt;IKEA Museum decided to start with the Swedish catalogue as it has been around the longest. In the future, we hope to be able to digitise catalogues from more countries in more languages.&lt;/p&gt;
    &lt;p&gt;No. The IKEA catalogue has always only shown a selection of whatâs available in the stores. The catalogues from the 1970s and onwards show around 30â50 per cent of the entire range. The products that are not featured are generally smaller ones in textiles, decorations and lighting. Temporary collections are rarely included either. But the farther back you go, the higher a percentage of the range can be found in the catalogue.&lt;/p&gt;
    &lt;p&gt;Yes, but the older a product is, the harder it may be to find information about it. If you have a specific question about a product, weâre happy to help you out if we can. But 70 years is a long time, so we canât promise anything. While youâre waiting for our response you can always browse through the catalogues â the product texts that are there are quite detailed. You can search in the catalogues by product name and product type. There are also various stories about different products on our site, and more are constantly being added.&lt;lb/&gt; Browse through stories about IKEA products from 7 decades. &lt;/p&gt;
    &lt;p&gt;IKEA was founded in the 1940s, so why are you showing no catalogues from before 1951?&lt;lb/&gt; The first catalogue did not come out until 1951. Before that, IKEA was a mail order company that didnât sell furniture, but pens, clocks, electric razors, wallets and bags. At that time, the range was only presented in a small mail order brochure called ikÃ©a-nytt (literally ikÃ©a news). Sometimes it was distributed as a supplement in farming paper Jordbrukarnas FÃ¶reningsblad, which reached hundreds of thousands of people in the Swedish countryside. From autumn 1948 Ingvar Kamprad started including furniture in the range, and things quickly grew from there. In the 1950 ikÃ©a-nytt, as many as six of the 18 pages featured furniture. And when you look at the 1951 catalogue, youâll see that there are no more pens and wallets. Ingvar Kamprad was now truly focusing on home furnishing, and shelving the rest.&lt;lb/&gt; Browse through all issues of ikÃ©a-nytt.&lt;/p&gt;
    &lt;p&gt;Not really. We do have a few copies of each yearâs IKEA catalogue in our archives, which weâre saving for posterity. They should be handled as little as possible to keep them in good condition, so weâve made the catalogues available digitally, both online and on monitors at IKEA Museum. You can browse through those as much as you like!&lt;/p&gt;
    &lt;p&gt;Yes you can. The easiest way to share the catalogues is to click on the arrow at the bottom left corner for each catalogue, or in the left-hand menu once youâve started browsing through. This will copy a link which you can share on a website or social media. If you would like to download and publish on your own digital platform, you can share a maximum of three complete digital catalogues. Donât forget to state the copyright details, “Â© Inter IKEA Systems B.V.”, the catalogue year, and the link /en/explore/ikea-catalogue/ so that anyone interested can find out more. You may not publish the digital catalogues for commercial purposes.&lt;/p&gt;
    &lt;p&gt;Absolutely! You can share up to 30 images from the catalogues on your own digital platform, such as a blog, on Instagram or similar (as long as itâs not for commercial purposes). Donât forget to state the copyright details, “Â© Inter IKEA Systems B.V.”, the catalogue year, and the link /en/explore/ikea-catalogue/ so that anyone interested can find out more.&lt;/p&gt;
    &lt;p&gt;Yes! You can find all press material, including images, information about current exhibitions and much more, in the IKEA Museum press room.&lt;/p&gt;
    &lt;p&gt;At the moment we have a good amount of catalogues in all languages at the museum, and do not need any more. Having said that, please contact us anyway if youâve been collecting catalogues for several decades, or if you have any other material you think might be of interest to IKEA Museum.&lt;/p&gt;
    &lt;p&gt;Unfortunately not. We sometimes wish we did, as we handle quite a lot of old products that may need putting together and taking apart.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45504470</guid><pubDate>Tue, 07 Oct 2025 15:35:48 +0000</pubDate></item><item><title>Show HN: Timelinize – Privately organize your own data from everywhere, locally</title><link>https://timelinize.com</link><description>&lt;doc fingerprint="e64842e6eda1dcc1"&gt;
  &lt;main&gt;
    &lt;p&gt;Timelinize ("time-lynn-eyes") is an open source personal archival suite, designed for modern family history. It organizes all your data onto a single, unified timeline on your own computer.&lt;/p&gt;
    &lt;p&gt;Photos, videos, text messages, locations, chats, social media, and more. Timelinize unifies it all.&lt;/p&gt;
    &lt;p&gt;By adding all your data, Timelinize documents your family's life with more detail and privacy, and gives you a more complete view of your story, than standard photo library and journaling apps.&lt;/p&gt;
    &lt;p&gt;Most apps store your data "in the cloud" and out of your control. What if you lost access to your Google/Apple/Facebook accounts, or your phone? By bringing that data home to your own computer, Timelinize preserves a richer story than any one app or service can do alone.&lt;/p&gt;
    &lt;p&gt;Timelinize isn't a replacement for the apps and services you already use, so you don't need to disrupt your way of life. Instead, it "sits behind" what you already use to become the permanent private archive of your working copy from:&lt;/p&gt;
    &lt;p&gt;With several projections for your data, it's easy to keep moments alive that would otherwise be forgotten, rotting on a hard drive in your closet... or in a bigcorp's cloud.&lt;/p&gt;
    &lt;p&gt;The timeline view semantically groups all your data into a single linear layout. Easily see what occurred on a specific day in the order it happened.&lt;/p&gt;
    &lt;p&gt;Visualize your data on a huge, beautiful map of the world that plots points when and where they happened, even for data that doesn't have coordinates (like text messages and emails).&lt;/p&gt;
    &lt;p&gt;Follow connections with people across all kinds of chats and messages. Combine conversations with people across platforms into one view.&lt;/p&gt;
    &lt;p&gt;Browse through a rich display of photos and videos from photo libraries, messages sent and received, and other sources.&lt;/p&gt;
    &lt;p&gt;Add millions of data points to your timeline in a matter of minutes. You get full control over background jobs like imports, thumbnails, and embeddings.&lt;/p&gt;
    &lt;p&gt;Timelinize supports playing "live photos" (or "motion photos") for photos taken on Apple, Google, and Samsung devices.&lt;/p&gt;
    &lt;p&gt;Timelinize specializes in combining data from multiple sets and sources. It can identify people and other entities across data sources by their attributes. If a person or contact appears in multiple data sets, it will automatically merge them if possible. If not, you can easily merge entities with the click of a button.&lt;/p&gt;
    &lt;p&gt;Because Timelinize is entity-aware, it can project data points onto a map even without coordinate data. If a geolocated point is known for an entity around the same time of others of that entity's data points, it will appear on the map.&lt;/p&gt;
    &lt;p&gt;Customize the map to change its theme, layers, and even make it 3D.&lt;/p&gt;
    &lt;p&gt;The heatmap shows where your data is concentrated. It smoothly blends as you zoom in and out.&lt;/p&gt;
    &lt;p&gt;Customize what defines a duplicate item, and how to handle that, with a fine degree of control—perfect for merging separate, disparate data sets.&lt;/p&gt;
    &lt;p&gt;An implicit conversation is discovered when a data source links items and entities with a "sent to" relation. You can easily view conversations between entities across modalities in a single scroll: chats, emails, messages, texts, and more.&lt;/p&gt;
    &lt;p&gt;Since I need this to function well for my own family, I have tried to give special attention to less-visible aspects of this application, such as:&lt;/p&gt;
    &lt;p&gt;Timelinize deduplicates, denoises, clusters, and simplifies location data for optimal preservation, with an algorithm that subjectively performs better than Google Maps Timeline.&lt;/p&gt;
    &lt;p&gt;For nerds like me: you can use Timelinize through its CLI, which mirrors all the functions of the HTTP API used by the frontend.&lt;/p&gt;
    &lt;p&gt;Search for pictures and messages by describing them, or find similar items to what you're viewing.&lt;/p&gt;
    &lt;p&gt;All items are stored verbatim, then thumbnails are generated for all images and video media, which are stored separately. Your original data is not modified.&lt;/p&gt;
    &lt;p&gt;The database schema has been meticulously designed and refined to be as adaptable as possible.&lt;/p&gt;
    &lt;p&gt;Timelinize will continue to develop and evolve. In the future, I anticipate the following capabilities:&lt;/p&gt;
    &lt;p&gt;Annotate your timeline, write rich stories with live embeddings from your timeline data, or make physical media like photo books (but with more than just photos!).&lt;/p&gt;
    &lt;p&gt;Add context to your timeline with additional public timelines which have weather, local/regional news, and global events.&lt;/p&gt;
    &lt;p&gt;Securely and privately share parts of your timeline with trusted friends and family members, directly from your computer to theirs.&lt;/p&gt;
    &lt;p&gt;Right now, Timelinize sits "behind" the apps and platforms you already use. But in the future, you could sync data directly to your timeline as it is originated.&lt;/p&gt;
    &lt;p&gt;Collect your data from various sources. Import it with a few clicks. Within minutes, explore millions of your data points in several intuitive ways.&lt;/p&gt;
    &lt;p&gt;Imported data is copied into your timeline folder, ensuring long-term stability and integrity. Timelines are portable—you can copy them or move them to other devices and computers.&lt;/p&gt;
    &lt;p&gt;Your timeline is simply a folder on disk containing a SQLite database alongside your data files. You can freely explore it with other tooling, so you're not locked into Timelinize.&lt;/p&gt;
    &lt;p&gt;Unlike writing a journal, you don't have to take extra time to create content. You're already making the data your timeline can display! And it doesn't replace your current workflow or apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45504973</guid><pubDate>Tue, 07 Oct 2025 16:10:22 +0000</pubDate></item><item><title>Seeing like a software company</title><link>https://www.seangoedecke.com/seeing-like-a-software-company/</link><description>&lt;doc fingerprint="2615d9142c2d05e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Seeing like a software company&lt;/head&gt;
    &lt;p&gt;The big idea of James C. Scott’s Seeing Like A State can be expressed in three points:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Modern organizations exert control by maximising “legibility”: by altering the system so that all parts of it can be measured, reported on, and so on.&lt;/item&gt;
      &lt;item&gt;However, these organizations are dependent on a huge amount of “illegible” work: work that cannot be tracked or planned for, but is nonetheless essential.&lt;/item&gt;
      &lt;item&gt;Increasing legibility thus often actually lowers efficiency - but the other benefits are high enough that organizations are typically willing to do so regardless.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By “legible”, I mean work that is predictable, well-estimated, has a paper trail, and doesn’t depend on any contingent factors (like the availability of specific people). Quarterly planning, OKRs, and Jira all exist to make work legible. Illegible work is everything else: asking for and giving favors, using tacit knowledge that isn’t or can’t be written down, fitting in unscheduled changes, and drawing on interpersonal relationships. As I’ll argue, tech companies need to support both of these kinds of work.&lt;/p&gt;
    &lt;p&gt;Thinking in terms of legibility and illegibility explains so many of the things that are confusing about large software companies. It explains why companies do many things that seem obviously counter-productive, why the rules in practice are so often out of sync with the rules as written, and why companies are surprisingly willing to tolerate rule-breaking in some contexts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Seeing like a state&lt;/head&gt;
    &lt;p&gt;James C. Scott was writing about the “high modernist” movement in governance that produced (among other things) the tidy German forests of the 19th century1. In order to produce wood at scale, the German state demanded legibility: forests that an inspector could visit to tally up the amount of healthy trees. That means that you must be able to walk through the forest - i.e. the underbrush must be controlled - and the trees ought to be ideally laid out in neat rows of a single type.&lt;/p&gt;
    &lt;p&gt;Proponents of legibility often describe their processes as “efficiency measures” or ways to “avoid waste”. But overall, the new “efficient” forests were in fact far less efficient than the old, illegible forests. They produced less wood per year and required more effort to fight disease, because the underbrush proved surprisingly load-bearing to the health of the soil, and the variety of species turned out to have been an asset. The new homogeneous forests could be wiped out by a single parasite or disease in a way that the older, more varied forests could not.&lt;/p&gt;
    &lt;p&gt;However, the advantages of legibility are enormous. Once you know exactly how many trees you have, you can plan ahead, make large trade deals, avoid graft, and so on. To me, this is the most interesting point Scott makes. Large organizations did genuinely think that more legibility would necessarily increase efficiency2. But even when it became clear that that was false, those organizations continued pushing for legibility anyway, because the other advantages were too powerful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Seeing like a software company&lt;/head&gt;
    &lt;p&gt;It’s the same way in software companies. It’s almost a truism among software engineers that a single engineer can be more efficient alone than they can by working as part of a team. That’s why there are so many anecdotes about engineers taking leave to finally get some work done, or about productive work being done on nights and weekends.&lt;/p&gt;
    &lt;p&gt;Likewise, it should be obvious to any practicing engineer that engineer-driven work goes far more swiftly than work that is mandated from above. Engineer-driven work doesn’t need to be translated into something that makes sense, doesn’t need to be actively communicated in all directions, and can in general just be done in the most straightforward and efficient way.&lt;/p&gt;
    &lt;p&gt;This is why tiny software companies are often much better than large software companies at delivering software: it doesn’t matter that the large company is throwing ten times the number of engineers at the problem if the small company is twenty times more efficient3.&lt;/p&gt;
    &lt;p&gt;Why don’t large companies react to this by doing away with all of their processes? Are they stupid? No. The processes that slow engineers down are the same processes that make their work legible to the rest of the company. And that legibility (in dollar terms) is more valuable than being able to produce software more efficiently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why legibility is valuable to tech companies&lt;/head&gt;
    &lt;p&gt;What does legibility mean to a tech company, in practice? It means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The head of a department knows, to the engineer, all the projects the department is currently working on&lt;/item&gt;
      &lt;item&gt;That head also knows (or can request) a comprehensive list of all the projects the department has shipped in the last quarter&lt;/item&gt;
      &lt;item&gt;That head has the ability to plan work at least one quarter ahead (ideally longer)&lt;/item&gt;
      &lt;item&gt;That head can, in an emergency, direct the entire resources of the department at immediate work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that “shipping high quality software” or “making customers happy” or even “making money” is not on this list. Those are all things tech companies want to do, but they’re not legibility.&lt;/p&gt;
    &lt;p&gt;Our small-but-efficient software company meets only one of these criteria: the ability to pivot to some immediate problem that needs solving. The other information is all locked up in various engineers’ heads, who may or may not remember what they did two months ago (and who certainly won’t be willing to commit to work two months from now). That’s not necessarily a problem, so long as everyone’s on the same page about what needs doing and the product is continuing to improve.&lt;/p&gt;
    &lt;p&gt;A typical large software company meets almost all of these criteria - I say almost, because in some companies or departments the ability to direct immediate work has atrophied (more on that later). But aside from that, large companies are usually very good at cataloguing what is being worked on, remembering what’s been shipped in the past, and planning work in the medium-to-long-term.&lt;/p&gt;
    &lt;p&gt;Why are these capabilities so valuable to a large software company, when small software companies can do without them? This is leaving my area of expertise somewhat, but I’m pretty sure the main answer is large enterprise deals. Making deals with large enterprise customers is fantastically profitable. Any sufficiently large SaaS will thus pivot from small customers to enterprise customers, if it can4. But enterprise deals (a) can take many, many months to set up, and (b) require making long-term feature commitments. An illegible company is not configured to be able to stick with a boring enterprise deal for many months, constantly answering questions and delivering features. Large enterprise customers simply won’t trust a small software company to deliver the things they need over the next year or two.&lt;/p&gt;
    &lt;p&gt;Customers like this typically value legibility very highly, and so demand that their vendors also be legible. In fact, highly legible organizations struggle to communicate at all with organizations that are less legible (and vice versa). They don’t have access to the right bona fides, they don’t talk the same language, and so on. This puts real pressure on growing tech companies to become more legible, even if it hurts their ability to deliver software.&lt;/p&gt;
    &lt;head rend="h3"&gt;Legible assumptions&lt;/head&gt;
    &lt;p&gt;In the pursuit of legibility, large tech companies make simplifying assumptions about the nature of tech work. For instance, they assume:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any engineers with the same job title perform roughly the same.&lt;/item&gt;
      &lt;item&gt;Engineers can be shuffled and reorganized without substantial loss of productivity.&lt;/item&gt;
      &lt;item&gt;A team will maintain the same level of productivity over time, if it has the same number of engineers.&lt;/item&gt;
      &lt;item&gt;Projects can be estimated ahead of time, albeit with some margin for error. The more time spent estimating a project, the more accurate the estimate will become.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, all of these are false. Within the same job title, there is significant variance in engineering ability. Engineers have different skillsets and interests, and will work much more productively on projects that are a good fit for them. Because of this, the productivity of a team has a weak relationship to the number of engineers on the team.&lt;/p&gt;
    &lt;p&gt;Project estimates are largely fantasy. More accurately, they’re performative: the initial estimate determines the kind of engineering work that gets done to deliver by that estimate, not the other way around. For this reason, breaking down a project into parts and estimating each part often delivers a less accurate estimate, because it makes it harder for engineers to align with the overall ship date.&lt;/p&gt;
    &lt;p&gt;However, these assumptions are true enough for their purpose, which is to provide legibility to the executives in charge of the company. Whether the project estimate is accurate or not, it can be used to plan and to communicate with other large organizations (who are themselves typically aware that these estimates ought not to be taken completely seriously).&lt;/p&gt;
    &lt;head rend="h3"&gt;Temporary sanctioned zones of illegibility&lt;/head&gt;
    &lt;p&gt;I mentioned above that large companies sometimes lose the ability to prioritize immediate work. This is because the processes that make work legible also impose a serious delay. Consider the steps that a hypothetical large company might take before beginning to write code on a problem:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Somebody has a product idea.&lt;/item&gt;
      &lt;item&gt;They take that idea to the Product org, where it goes into the “planning” stage. Meetings are had about the idea.&lt;/item&gt;
      &lt;item&gt;Once the Product org formally decide they want to do it, the idea then passes to the Engineering org: into the hands of some council of engineering architects, who are tasked with the initial technical review. They figure out how it fits into the general engineering priorities and give it a very rough time estimate.&lt;/item&gt;
      &lt;item&gt;The VPs and senior managers in the engineering org then negotiate which team will own the work. Often this is a semi-technical, semi-organizational decision (because which service the work should fall into is at least partly a technical question).&lt;/item&gt;
      &lt;item&gt;Finally the work lands on the team. It enters the team planning backlog, where the team technical lead breaks it out into smaller pieces of work.&lt;/item&gt;
      &lt;item&gt;Those smaller pieces of work enter the team ticket backlog, and are estimated in the team’s weekly planning meeting.&lt;/item&gt;
      &lt;item&gt;Finally some of those pieces of work make it into the next sprint, and are picked up by an engineer who can actually do it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m leaving out many crucial parts of this process: the updates on each ticket, which then roll up to higher levels of management, legal and design review, which can themselves take weeks, and then the final steps involved in shipping the change to customers. All of this makes the work very legible, but none of this is compatible with work that has to be done right now. What do you do when there’s a sudden, urgent technical problem - maybe you’re about to overflow your &lt;code&gt;int&lt;/code&gt; ID column on the users table, or some very large customer is experiencing a show-stopping bug?5&lt;/p&gt;
    &lt;p&gt;To solve this kind of problem, tech companies often reserve the right to create temporary zones where illegible work is allowed. Sometimes these are called “virtual teams”, or “strike teams” (or even the colourful name “tiger teams”). They are composed of hand-picked engineers who are trusted by the organization. Often there is no manager assigned at all, but instead some very senior engineer who’s tasked with running the project. These teams are given a loose mandate - like “stop the database from falling over every few days” - and allowed to do basically whatever it takes to get it done.&lt;/p&gt;
    &lt;p&gt;This is a smart compromise between complete illegibility, which as I discussed above would make the company unable to make deals with its richest customers, and complete legibility, which would force even urgent company-killing issues to go through the entire laborious process of scoping, planning and estimating.&lt;/p&gt;
    &lt;p&gt;Even when siloed to a temporary team, sanctioned illegibility still coexists awkwardly with the rest of the organization. Engineers outside the team don’t like seeing other engineers given the freedom to work without the burden of process: either because they’re jealous, or because they’re believers in process and think that such work is unacceptably dangerous. Managers also don’t like extending that level of trust. That’s why sanctioned efforts like this are almost always temporary. The majority of the illegible work that occurs in large organizations is still unsanctioned.&lt;/p&gt;
    &lt;head rend="h3"&gt;Permanent zones of unsanctioned illegibility&lt;/head&gt;
    &lt;p&gt;If you’re an engineer on team A, and you need team B to do some kind of work for you, the formal way to do this is to create an issue in their “planning” backlog and wait for it to go through the entire twelve-step process before it finally makes its way into one of their sprints, where hopefully somebody will pick it up and do it. This can take weeks to months. When what you want is a one-line change, it’s incredibly frustrating to watch your requested work item go through all these steps - each one of which takes many times longer than it would take to simply do the work.&lt;/p&gt;
    &lt;p&gt;The official way around this problem is that team A should anticipate in their planning process that team B will need to do this work, so that piece for team B can enter their backlog at the same time as it enters team A’s backlog. That way (in theory) they should be complete at around the same time6. Any practicing software engineer knows how ridiculous this idea is. You can never anticipate every change that has to be made months before you start writing code.&lt;/p&gt;
    &lt;p&gt;The actual way around this problem is illegible backchannels. An engineer on team A reaches out to an engineer on team B asking “hey, can you make this one-line change for me”. That engineer on team B then does it immediately, maybe creating a ticket, maybe not. Then it’s done! This works great, but it’s illegible because the company can’t expect it or plan for it - it relies on the interpersonal relationships between engineers on different teams, which are very difficult to quantify. If you’re a well-liked engineer, your ability to pull on these backchannels is significantly greater than if you’re brand-new or have a bad reputation. But how well-liked you are is not something companies can officially use when they’re planning projects.&lt;/p&gt;
    &lt;p&gt;Backchannels are a constant presence at all levels of the company. As well as engineer-engineer cross-team backchannels, there are backchannels inside teams, between managers, product managers, and so on. Often when a question is asked formally in a public space, it’s already been rehearsed and workshopped privately with the person who’s answering the question. None of this is or can be documented as part of the formal processes of the company, but it’s load-bearing nonetheless. Many formal processes simply cannot function without the consensus mechanisms or safety valves offered by backchannels.&lt;/p&gt;
    &lt;p&gt;Sometimes backchannels can go badly. Earlier this year I wrote Protecting your time from predators in large tech companies about how some people use backchannels to benefit themselves at the expense of the naive engineers they’re requesting work from. And it never feels good when you get the sense that everyone in a meeting has privately discussed the topic ahead of time except for you. For these reasons, some people think that backchannels themselves are a bad thing, and that all communication should go via formal, legible channels.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sociopaths, clueless, and losers&lt;/head&gt;
    &lt;p&gt;There’s another text which has been as influential to many as Seeing Like A State. This one isn’t a book, but a blog post: The Gervais Principle by Venkatesh Rao. Rao divides organizations into three groups. At the top are the “sociopaths”, who cynically use organizational rules for their own benefit. In middle management are the “clueless”, who are bought into the formal rules of the organization and don’t realise that there’s a deeper game being played above their heads. Below them are the “losers”, who realise there’s a game being played but don’t want to play it. The name “losers” is not a value judgement - I think it’s meant to affectionately pick out people like the leads in Clerks, who are too authentic to get involved in the corporate game.&lt;/p&gt;
    &lt;p&gt;I don’t agree with everything in The Gervais Principle, though I think it’s worth a read (if you’re interested in this stuff, you should also read the excellent Moral Mazes). But the categories here can be very naturally read in terms of legibility. Both sociopaths and losers are engaged with the illegible world of the organization. Sociopaths use this world to climb the ladder, while losers use it to carve out a cosy low-effort niche for themselves.&lt;/p&gt;
    &lt;p&gt;The “clueless” are only engaged with legible processes. They’re the people who, when they want to get promoted, go and look up the formal job ladder and make a plan for how they can exemplify each of the values at the next level. They’re concerned with doing everything by the book. When they’re forced into an encounter with the illegible world, their reaction is to shake their heads and start drafting updates to the legible process that can accommodate some pale approximation of the more-efficient illegible process.&lt;/p&gt;
    &lt;p&gt;I think it’s far too cynical to call these people clueless. Legible process is still very important - after all, it’s the large part of what the organization does. Improving formal processes is still very high-leverage work, even if formal processes can’t ever describe the entirety of how an organization operates. People who are invested in legibility have real value to any tech company.&lt;/p&gt;
    &lt;p&gt;However, thinking about people in Rao’s categories - people who exploit illegibility, people who find it distasteful, and people who use it casually - can be illuminating. Many frequent areas of conflict in software companies stem from the friction between these groups of people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;I write a lot about recognizing and using illegibility in tech companies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Breaking the (formal, legible) rules is sometimes the right thing to do&lt;/item&gt;
      &lt;item&gt;Beware of savvy product managers (and others) exploiting illegible channels to chisel work out of naive engineers&lt;/item&gt;
      &lt;item&gt;Competent engineers should work on “side bets” that are outside the normal planning process&lt;/item&gt;
      &lt;item&gt;Getting promoted to Staff and above has very little to do with the formal job ladder&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In general, advice about illegible processes is what I call “dangerous advice”. It’s dangerous because if you make it legible - for instance, if you announce publicly that you’re getting a piece of work done through backchannels instead of the formal process - you will be punished by the organization even if your management chain wanted you to do it. You can’t speak too loudly about it. It has to stay illegible.&lt;/p&gt;
    &lt;p&gt;I get a lot of negative feedback on these posts from people who say that you should never sidestep the formal process. According to them, if it needs changing, you should change the process instead of going around it. In other words, everything that goes on in a tech company should be legible, and illegible processes should be stamped out and converted to legible ones.&lt;/p&gt;
    &lt;p&gt;I think this view is naive. All organizations - tech companies, social clubs, governments - have both a legible and an illegible side. The legible side is important, past a certain size. It lets the organization do things that would otherwise be impossible: long-term planning, coordination with other very large organizations, and so on. But the illegible side is just as important. It allows for high-efficiency work, offers a release valve for processes that don’t fit the current circumstances, and fills the natural human desire for gossip and soft consensus.&lt;/p&gt;
    &lt;p&gt;edit: this got some comments on Hacker News. Some commenters agree with everything except the idea that large enterprise deals are the primary driver for legibility - they think it’s communication at scale, or being able to target market share, or internal control.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;This is the first example Scott gives, but I promise I did read the whole book. Other examples: the construction of Brasília, Operation Vijiji in Tanzania, and the Soviet attempt to replace individual peasant farms with state-run collectives.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is a very common false belief today among software engineers.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I don’t think small companies just work harder; plenty of people at large companies work very hard. I also don’t think that small companies just have better engineers - what advantage they have in enthusiasm is often outweighed by the fact that they can’t afford to pay as well.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I was at Zendesk during the height of its pivot.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ironically, the most urgent types of problem typically can be solved via a normal “incident” process - but this itself is usually a zone where the rules are relaxed a bit in order to resolve the incident as quickly as possible. Anyway, here I’m not talking about incidents but about projects that will take a couple of weeks to resolve.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The other, healthier official way is to allow teams to make small changes to other teams’ services themselves. But this only goes so far - the other team will always be the gatekeepers for changes like this, and are always in a position to slow down the change by days or weeks.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;September 3, 2025 │ Tags: tech companies&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45505539</guid><pubDate>Tue, 07 Oct 2025 16:49:09 +0000</pubDate></item><item><title>Gemini 2.5 Computer Use model</title><link>https://blog.google/technology/google-deepmind/gemini-computer-use-model/</link><description>&lt;doc fingerprint="b97269db1c538405"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing the Gemini 2.5 Computer Use model&lt;/head&gt;
    &lt;p&gt;Earlier this year, we mentioned that we're bringing computer use capabilities to developers via the Gemini API. Today, we are releasing the Gemini 2.5 Computer Use model, our new specialized model built on Gemini 2.5 Pro’s visual understanding and reasoning capabilities that powers agents capable of interacting with user interfaces (UIs). It outperforms leading alternatives on multiple web and mobile control benchmarks, all with lower latency. Developers can access these capabilities via the Gemini API in Google AI Studio and Vertex AI.&lt;/p&gt;
    &lt;p&gt;While AI models can interface with software through structured APIs, many digital tasks still require direct interaction with graphical user interfaces, for example, filling and submitting forms. To complete these tasks, agents must navigate web pages and applications just as humans do: by clicking, typing and scrolling. The ability to natively fill out forms, manipulate interactive elements like dropdowns and filters, and operate behind logins is a crucial next step in building powerful, general-purpose agents.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;The model’s core capabilities are exposed through the new `computer_use` tool in the Gemini API and should be operated within a loop. Inputs to the tool are the user request, screenshot of the environment, and a history of recent actions. The input can also specify whether to exclude functions from the full list of supported UI actions or specify additional custom functions to include.&lt;/p&gt;
    &lt;p&gt;Gemini 2.5 Computer Use Model flow&lt;/p&gt;
    &lt;p&gt;The model then analyzes these inputs and generates a response, typically a function call representing one of the UI actions such as clicking or typing. This response may also contain a request for an end user confirmation, which is required for certain actions such as making a purchase. The client-side code then executes the received action.&lt;/p&gt;
    &lt;p&gt;After the action is executed, a new screenshot of the GUI and the current URL are sent back to the Computer Use model as a function response restarting the loop. This iterative process continues until the task is complete, an error occurs or the interaction is terminated by a safety response or user decision.&lt;/p&gt;
    &lt;p&gt;The Gemini 2.5 Computer Use model is primarily optimized for web browsers, but also demonstrates strong promise for mobile UI control tasks. It is not yet optimized for desktop OS-level control.&lt;/p&gt;
    &lt;p&gt;Check out a few demos below to see the model in action (shown here at 3X speed).&lt;/p&gt;
    &lt;p&gt;Prompt: “From https://tinyurl.com/pet-care-signup, get all details for any pet with a California residency and add them as a guest in my spa CRM at https://pet-luxe-spa.web.app/. Then, set up a follow up visit appointment with the specialist Anima Lavar for October 10th anytime after 8am. The reason for the visit is the same as their requested treatment.”&lt;/p&gt;
    &lt;p&gt;Prompt: “My art club brainstormed tasks ahead of our fair. The board is chaotic and I need your help organizing the tasks into some categories I created. Go to sticky-note-jam.web.app and ensure notes are clearly in the right sections. Drag them there if not.”&lt;/p&gt;
    &lt;head rend="h2"&gt;How it performs&lt;/head&gt;
    &lt;p&gt;The Gemini 2.5 Computer Use model demonstrates strong performance on multiple web and mobile control benchmarks. The table below includes results from self-reported numbers, evaluations run by Browserbase and evaluations we ran ourselves. Evaluation details are available in the Gemini 2.5 Computer Use evaluation info and in Browserbase’s blog post. Unless otherwise indicated, scores shown are for computer use tools exposed via API.&lt;/p&gt;
    &lt;p&gt;Gemini 2.5 Computer Use outperforms leading alternatives on multiple benchmarks&lt;/p&gt;
    &lt;p&gt;The model offers leading quality for browser control at the lowest latency, as measured by performance on the Browserbase harness for Online-Mind2Web.&lt;/p&gt;
    &lt;p&gt;Gemini 2.5 Computer Use delivers high accuracy while maintaining low latency&lt;/p&gt;
    &lt;head rend="h2"&gt;How we approached safety&lt;/head&gt;
    &lt;p&gt;We believe that the only way to build agents that will benefit everyone is to be responsible from the start. AI agents that control computers introduce unique risks, including intentional misuse by users, unexpected model behavior, and prompt injections and scams in the web environment. Thus, it is critical to implement safety guardrails with care.&lt;/p&gt;
    &lt;p&gt;We have trained safety features directly into the model to address these three key risks (described in the Gemini 2.5 Computer Use System Card).&lt;/p&gt;
    &lt;p&gt;Further, we also provide developers with safety controls, which empower developers to prevent the model from auto-completing potentially high-risk or harmful actions. Examples of these actions include harming a system's integrity, compromising security, bypassing CAPTCHAs, or controlling medical devices. The controls:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Per-step safety service: An out-of-model, inference-time safety service that assesses each action the model proposes before it’s executed.&lt;/item&gt;
      &lt;item&gt;System instructions: Developers can further specify that the agent either refuses or asks for user confirmation before it takes specific kinds of high-stakes actions. (Example in documentation).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additional recommendations for developers on safety measures and best practices can be found in our documentation. While these safeguards are designed to reduce risk, we urge all developers to thoroughly test their systems before launch.&lt;/p&gt;
    &lt;head rend="h2"&gt;How early testers have used it&lt;/head&gt;
    &lt;p&gt;Google teams have already deployed the model to production for use cases including UI testing, which can make software development signficantly faster. Versions of this model have also been powering Project Mariner, the Firebase Testing Agent, and some agentic capabilities in AI Mode in Search.&lt;/p&gt;
    &lt;p&gt;Users from our early access program have also been testing the model to power personal assistants, workflow automation, and UI testing, and have seen strong results. In their own words:&lt;/p&gt;
    &lt;head rend="h2"&gt;How to get started&lt;/head&gt;
    &lt;p&gt;Starting today, the model is available in public preview, accessible via the Gemini API on Google AI Studio and Vertex AI.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Try it now: In a demo environment hosted by Browserbase.&lt;/item&gt;
      &lt;item&gt;Start building: Dive into our reference and documentation (see Vertex AI docs for enterprise use) to learn how to build your own agent loop locally with Playwright or in a cloud VM with Browserbase.&lt;/item&gt;
      &lt;item&gt;Join the community: We’re excited to see what you build. Share feedback and help guide our roadmap in our Developer Forum.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45507936</guid><pubDate>Tue, 07 Oct 2025 19:49:11 +0000</pubDate></item><item><title>Show HN: Oh Yah – Routine management app I built for my sons</title><link>https://ohyahapp.com</link><description>&lt;doc fingerprint="3c11f5fd244164e3"&gt;
  &lt;main&gt;
    &lt;p&gt;Each task has a dedicated timer in a distraction-free environment. Navigation is disabled during focus time to maintain attention and encourage task completion.&lt;/p&gt;
    &lt;p&gt;Kids can submit photo evidence of completed tasks, building accountability whilst giving parents peace of mind about task completion.&lt;/p&gt;
    &lt;p&gt;Ordered tasks provide structure and sequence, whilst unordered tasks offer flexibility. Clear priorities help kids understand what needs to be done when.&lt;/p&gt;
    &lt;p&gt;Parents review individual tasks but award stars for the complete schedule. Auto-approval with full stars occurs after 24 hours if all tasks are submitted and no manual award is given.&lt;/p&gt;
    &lt;p&gt;Manage up to 8 child profiles with individual schedules and tasks. Perfect for families with multiple children.&lt;/p&gt;
    &lt;p&gt;Clean interface with simple navigation. Kids tap their profile and start immediately - no complex menus or overwhelming choices.&lt;/p&gt;
    &lt;p&gt;Oh Yah! is for families who need routine management that works with their child's unique needs — not generic productivity apps that overwhelm.&lt;/p&gt;
    &lt;p&gt;Child taps their profile and sees today's tasks in a consistent order set by parents.&lt;/p&gt;
    &lt;p&gt;Each task opens in a focused timer modal with no distractions or navigation.&lt;/p&gt;
    &lt;p&gt;Take a photo as proof and submit the completed task for parent review.&lt;/p&gt;
    &lt;p&gt;Parents review tasks and award stars based on quality. Unreviewed tasks auto-approve after 24 hours, keeping parents accountable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45513459</guid><pubDate>Wed, 08 Oct 2025 08:15:49 +0000</pubDate></item><item><title>Synology reverses policy banning third-party HDDs after sales allegedly plummet</title><link>https://www.guru3d.com/story/synology-reverses-policy-banning-thirdparty-hdds-after-nas-sales-plummet/</link><description>&lt;doc fingerprint="7da303bb4dfe4cfc"&gt;
  &lt;main&gt;
    &lt;p&gt;Now, with the release of DSM 7.3, Synology has quietly walked the policy back. Third-party hard drives and 2.5-inch SATA SSDs can once again be used without triggering warning messages or reduced functionality. Drives from Seagate, WD, and others will work exactly as they did before—complete with full monitoring, alerts, and storage features.&lt;/p&gt;
    &lt;p&gt;For users, this means more choice and lower costs when building or upgrading a NAS. For Synology, it’s a much-needed course correction after months of backlash. While the company hasn’t publicly admitted fault, it’s clear that sales pressure and community outrage played a major role in reversing the decision.&lt;/p&gt;
    &lt;p&gt;Critics say the entire episode has damaged Synology’s reputation. The company seemed to believe that after QNAP’s well-known ransomware troubles, it could tighten control of the market without losing customers. Instead, the plan backfired—hard. Many loyal users have since turned to alternative brands or expressed hesitation about buying another Synology product.&lt;/p&gt;
    &lt;p&gt;Still, the return of open drive support is good news for anyone running a Synology NAS. It restores the flexibility that made the brand so popular in the first place. Whether this move is enough to win back frustrated users remains to be seen, but for now, DSM 7.3 brings a welcome dose of freedom back to the platform.&lt;/p&gt;
    &lt;p&gt;Source: Synology / nascompares&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45513485</guid><pubDate>Wed, 08 Oct 2025 08:19:36 +0000</pubDate></item><item><title>Nobel Prize in Chemistry 2025</title><link>https://www.nobelprize.org/prizes/chemistry/2025/popular-information/</link><description>&lt;doc fingerprint="37d0923c8f250160"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Popular information&lt;/head&gt;
    &lt;p&gt;Popular science background: They have created new rooms for chemistry (pdf)&lt;lb/&gt;Populärvetenskaplig information: De har skapat nya rum för kemi (pdf)&lt;/p&gt;
    &lt;head rend="h2"&gt;They have created new rooms for chemistry&lt;/head&gt;
    &lt;p&gt;Susumu Kitagawa, Richard Robson and Omar M. Yaghi are awarded the Nobel Prize in Chemistry 2025 for the development of a new type of molecular architecture. The constructions they created – metal–organic frameworks – contain large cavities in which molecules can flow in and out. Researchers have used them to harvest water from desert air, extract pollutants from water, capture carbon dioxide and store hydrogen.&lt;/p&gt;
    &lt;p&gt;An attractive and very spacious studio apartment, specifically designed for your life as a water molecule – this is how an estate agent might describe one of all the metal–organic frameworks that laboratories around the world have developed in recent decades. Other constructions of this type are tailormade for capturing carbon dioxide, separating PFAS from water, delivering pharmaceuticals in the body or managing extremely toxic gases. Some can trap the ethylene gas from fruit – so they ripen more slowly – or encapsulate enzymes that break down traces of antibiotics in the environment.&lt;/p&gt;
    &lt;p&gt;Simply stated, metal–organic frameworks are exceptionally useful. Susumu Kitagawa, Richard Robson and Omar Yaghi are awarded the Nobel Prize in Chemistry 2025 because they created the first metal–organic frameworks (MOF) and demonstrated their potential. Thanks to the laureates’ work, chemists have been able to design tens of thousands of different MOFs, facilitating new chemical wonders.&lt;/p&gt;
    &lt;p&gt;As so often in the sciences, the story of the Nobel Prize in Chemistry 2025 begins with someone who thought outside the box. This time, inspiration came during preparations for a classic chemistry lesson, in which the students were to build molecules from rods and balls.&lt;/p&gt;
    &lt;head rend="h3"&gt;A simple wooden model of a molecule generates an idea&lt;/head&gt;
    &lt;p&gt;It was 1974. Richard Robson, who was teaching at the University of Melbourne, Australia, had been tasked with turning wooden balls into models of atoms, so students could create molecular structures. For this to work, he needed the university’s workshop to drill holes in them, so that wooden rods – the chemical bonds – could be attached to the atoms. However, the holes could not be randomly placed. Each atom – such as carbon, nitrogen or chlorine – forms chemical bonds in a specific way. Robson needed to mark out where the holes should be drilled.&lt;/p&gt;
    &lt;p&gt;When the workshop returned the wooden balls, he tested building some molecules. This was when he had a moment of insight: there was a vast amount of information baked into the holes’ positioning. The model molecules automatically had the correct form and structure, because of where the holes were situated. This insight led to his next idea: what would happen if he utilised the atoms’ inherent properties to link together different types of molecules, rather than individual atoms? Could he design new types of molecular constructions?&lt;/p&gt;
    &lt;head rend="h3"&gt;Robson builds innovative chemical creations&lt;/head&gt;
    &lt;p&gt;Every year, when Robson brought out the wooden models to teach new students, the same idea occurred to him. However, more than a decade passed before he decided to test it out. He started with a very simple model, inspired by the structure of a diamond, in which each carbon atom bonds to four others, forming a tiny pyramid (figure 2). Robson’s aim was to build a similar structure, but his would be based on positively charged copper ions, Cu+. Like carbon, they prefer to have four other atoms around them.&lt;/p&gt;
    &lt;p&gt;He combined the copper ions with a molecule that has four arms: 4′,4″,4”’,4””-tetracyanotetraphenylmethane. There’s no need to remember its complicated name, but it is important that the molecule at the end of each arm had a chemical group, nitrile, that was attracted to the positively charged copper ions (figure 2).&lt;/p&gt;
    &lt;p&gt;At that time, most chemists would have assumed that combining copper ions with the four-armed molecules would result in a bird’s nest of ions and molecules. But things went Robson’s way. As he had predicted, the ions and molecules inherent attraction to each other mattered, so they organised themselves into a large molecular construction. Just like carbon atoms in a diamond, they formed a regular crystalline structure. However, unlike diamond – which is a compact material – this crystal contained a vast number of large cavities (figure 2).&lt;/p&gt;
    &lt;p&gt;In 1989, Robson presented his innovative chemical creation in the Journal of the American Chemical Society. In his article, he speculates about the future and suggests that this could offer a new way to construct materials. These, he writes, could be given never previously seen properties, potentially beneficial ones.&lt;/p&gt;
    &lt;p&gt;As it turned out, he had foreseen the future.&lt;/p&gt;
    &lt;head rend="h3"&gt;Robson brings about a pioneering spirit in chemistry&lt;/head&gt;
    &lt;p&gt;As soon as the year after his pioneering work was published, Robson presented several new types of molecular constructions with cavities that were filled with various substances. He used one of them to exchange ions. He submerged the ion-filled construction in a fluid that contained a different type of ion. The result was that the ions changed places, demonstrating that substances could flow in and out of the construction.&lt;/p&gt;
    &lt;p&gt;In his experiments, Robson showed that rational design can be utilised for building crystals with spacious interiors that are optimised for specific chemicals. He suggested that this new form of molecular construction – when correctly designed – could be used to catalyse chemical reactions, for example.&lt;/p&gt;
    &lt;p&gt;However, Robson’s constructions were quite rickety and tended to fall apart. Many chemists thought they were useless, but some could see that he was onto something and, for them, his ideas about the future awakened a pioneering spirit. Those who would come to lay a stable foundation for his visions were Susumu Kitagawa and Omar Yaghi. Between 1992 and 2003 they made – separately – a series of groundbreaking discoveries. We will begin in the 1990s, with Kitagawa, who was working at Kindai University, Japan.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kitagawa’s motto: even useless things can become useful&lt;/head&gt;
    &lt;p&gt;Throughout his research career, Susumu Kitagawa has followed an important principle: to try to see “the usefulness of useless.” As a young student, he read a book by the Nobel Prize laureate Hideki Yukawa. In it, Yukawa refers to an ancient Chinese philosopher, Zhuangzi, who says that we must question what we believe to be useful. Even if something does not bring immediate benefit, it may still turn out to be valuable.&lt;/p&gt;
    &lt;p&gt;Accordingly, when Kitagawa began to investigate the potential for creating porous molecular structures, he did not believe they had to have a specific purpose. When he presented his first molecular construction in 1992, it was indeed not particularly useful: a two-dimensional material with cavities in which acetone molecules could hide. However, it had resulted from a new way of thinking about the art of building with molecules. Like Robson, he used copper ions as cornerstones that were linked together by larger molecules.&lt;/p&gt;
    &lt;p&gt;Kitagawa wanted to continue experimenting with this new construction technology, but when he applied for grants, research funders did not think there was any particular point to his ambitions. The materials he created were unstable and had no purpose, so many of his proposals were rejected.&lt;/p&gt;
    &lt;p&gt;However, he did not give up and in 1997 he had his first major breakthrough. Using cobalt, nickel or zinc ions and a molecule called 4,4′-bipyridine, his research group created three-dimensional metal–organic frameworks that were intersected by open channels (figure 3). When they dried one of these materials – emptying it of water – it was stable and the spaces could even be filled with gases. The material could absorb and release methane, nitrogen and oxygen, without changing shape.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kitagawa sees the uniqueness of his creations&lt;/head&gt;
    &lt;p&gt;Kitagawa’s constructions were both stable and had a function, but research funders were still unable to see their charm. One reason was that chemists already had zeolites, stable and porous materials, which they could build from silicon dioxide. These can absorb gases, so why would anyone develop a similar material that did not work as well?&lt;/p&gt;
    &lt;p&gt;Susumu Kitagawa understood that if he were to receive any major grants, he had to define what made metal–organic frameworks unique. So, in 1998, he described his vision in the Bulletin of the Chemical Society of Japan. He presented several advantages with MOFs. For example, they can be created from many types of molecules, so there is enormous potential for integrating different functions. Also – and this is important – he realised that MOFs can form soft materials. Unlike zeolites, which are usually hard materials, MOFs contain flexible molecular building blocks (figure 4) that can create a pliant material.&lt;/p&gt;
    &lt;p&gt;After this, all he had to do was to put his ideas into practice. Kitagawa, along with other researchers, started developing flexible MOFs. While they work on this, we will move our focus to the US, where Omar Yaghi was also occupied with taking molecular architecture to new heights.&lt;/p&gt;
    &lt;head rend="h3"&gt;A secret library visit opens Yaghi’s eyes to chemistry&lt;/head&gt;
    &lt;p&gt;Studying chemistry was not an obvious choice for Omar Yaghi. He and his many siblings were raised in a single room in Amman, Jordan, with no electricity or running water. School was a refuge from his otherwise challenging life. One day, when he was ten years old, he sneaked into the school library, which was usually locked, and picked a book at random from the shelf. On opening it, his eyes were drawn to unintelligible but captivating pictures – his first encounter with molecular structures.&lt;/p&gt;
    &lt;p&gt;At the age of 15 – and on his father’s stern instruction – Yaghi moved to the US to study. He was attracted by chemistry and eventually by the art of designing new materials, but found the traditional way of building new molecules too unpredictable. Normally, chemists combine substances that are to react with each other in a container. Then, to start the chemical reaction, they heat the container. The desired molecule forms, but is also often accompanied by a range of contaminating side products.&lt;/p&gt;
    &lt;p&gt;In 1992, when Yaghi started his first position as research group leader, at Arizona State University, he wanted to find more controlled ways in which to create materials. His aim was to use rational design to connect different chemical constituents, like pieces of Lego, to make large crystals. This turned out to be challenging, but they finally succeeded when the research group started combining metal ions with organic molecules. In 1995, Yaghi published the structure of two different two-dimensional materials; these were like nets and were held together by copper or cobalt. The latter could host guest molecules in its spaces and, when these were fully occupied, it was so stable that it could be heated to 350°C without collapsing. Yaghi describes this material in an article in Nature where he coins the name “metal–organic framework;” this term is now used to describe extended and ordered molecular structures that potentially contain cavities, and are built from metals and organic (carbon-based) molecules.&lt;/p&gt;
    &lt;head rend="h3"&gt;Just a few grams of Yaghi’s framework can contain a football pitch&lt;/head&gt;
    &lt;p&gt;Yaghi established the next milestone in the development of metal–organic frameworks in 1999, when he presented MOF-5 to the world. This material has become a classic in the field. It is an exceptionally spacious and stable molecular construction. Even when empty, it can be heated to 300°C without collapsing.&lt;/p&gt;
    &lt;p&gt;However, what caused many researchers to raise their eyebrows was the enormous area hiding inside the material’s cubic spaces. A couple of grams of MOF-5 holds an area as big as a football pitch, which means it can absorb much more gas than a zeolite could (figure 5).&lt;/p&gt;
    &lt;p&gt;Speaking of the differences between zeolites and MOFs, it took just a few years for researchers to succeed in developing soft MOFs. One of those who was able to present a flexible material was Susumu Kitagawa himself. When his material was filled with water or methane, it changed shape, and when it was emptied, it returned to its original form. The material behaved somewhat like a lung that can breathe gas in and out, changeable but stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Yaghi’s research group conjures drinking water from desert air&lt;/head&gt;
    &lt;p&gt;Omar Yaghi laid the final bricks in the foundation of metal–organic frameworks in 2002 and 2003. In two articles, in Science and Nature, he shows that it is possible to modify and change MOFs in a rational manner, giving them different properties. One thing he did was to produce 16 variants of MOF-5, with cavities that were both larger and smaller than those in the original material (figure 6). One variant could store huge volumes of methane gas, which Yaghi suggested could be used in RNG-fuelled vehicles.&lt;/p&gt;
    &lt;p&gt;Subsequently, metal–organic frameworks have taken the world by storm. Researchers have developed a molecular kit with a wide range of different pieces that can be used to create new MOFs. These have different shapes and characters, providing incredible potential for the rational – or AI-based – design of MOFs for different purposes. Figure 7 provides examples of how MOFs can be utilised. For instance, Yaghi’s research group has harvested water from the desert air of Arizona. During the night, their MOF material captured water vapour from the air. When dawn came and the sun heated the material, they were able to collect the water.&lt;/p&gt;
    &lt;head rend="h3"&gt;MOF materials that capture carbon dioxide and toxic gases&lt;/head&gt;
    &lt;p&gt;Researchers have created numerous different and functional MOFs. So far, in most cases, the materials have only been used on a small scale. To harness the benefits of MOF materials for humanity, many companies are now investing in their mass production and commercialisation. Some have succeeded. For example, the electronics industry can now use MOF materials to contain some of the toxic gases required to produce semiconductors. Another MOF can instead break down harmful gases, including some that can be used as chemical weapons. Numerous companies are also testing materials that can capture carbon dioxide from factories and power stations, to reduce greenhouse gas emissions.&lt;/p&gt;
    &lt;p&gt;Some researchers believe that metal–organic frameworks have such huge potential that they will be the material of the twenty-first century. Time will tell, but through the development of metal–organic frameworks, Susumu Kitagawa, Richard Robson and Omar Yaghi have provided chemists with new opportunities for solving some of the challenges we face. They have thus – as Alfred Nobel’s will states – brought the greatest benefit to humankind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further reading&lt;/head&gt;
    &lt;p&gt;Additional information on this year’s prizes, including a scientific background in English, is available on the website of the Royal Swedish Academy of Sciences, www.kva.se, and at www.nobelprize.org, where you can watch video from the press conferences, the Nobel Prize lectures and more. Information on exhibitions and activities related to the Nobel Prizes and the prize in economic sciences is available at www.nobelprizemuseum.se.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Chemistry 2025 to&lt;/head&gt;
    &lt;p&gt;SUSUMU KITAGAWA&lt;lb/&gt;Born 1951 in Kyoto, Japan. PhD 1979 from Kyoto University, Japan. Professor at Kyoto University, Japan.&lt;/p&gt;
    &lt;p&gt;RICHARD ROBSON&lt;lb/&gt;Born 1937 in Glusburn, UK. PhD 1962 from University of Oxford, UK. Professor at University of Melbourne, Australia.&lt;/p&gt;
    &lt;p&gt;OMAR M. YAGHI&lt;lb/&gt;Born 1965 in Amman, Jordan. PhD 1990 from University of Illinois Urbana-Champaign, USA. Professor at University of California, Berkeley, USA.&lt;/p&gt;
    &lt;p&gt;“for the development of metal–organic frameworks”&lt;/p&gt;
    &lt;p&gt;Science Editors: Peter Brzezinski, Heiner Linke, Olof Ramström and Xiaodong Zou, the Nobel Committee for Chemistry&lt;lb/&gt;Text: Ann Fernholm&lt;lb/&gt;Translation: Clare Barnes&lt;lb/&gt;Illustrations: Johan Jarnestad&lt;lb/&gt;Editor: Alicia Hegner&lt;lb/&gt;© The Royal Swedish Academy of Sciences&lt;/p&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6–13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45514164</guid><pubDate>Wed, 08 Oct 2025 09:49:36 +0000</pubDate></item><item><title>Legal Contracts Built for AI Agents</title><link>https://paid.ai/blog/ai-agents/paid-gitlaw-introducing-legal-contracts-built-for-ai-agents</link><description>&lt;doc fingerprint="3d14491969fbea31"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Insights straight to your inbox&lt;/head&gt;
    &lt;p&gt;Join 10,000+ subscribers getting the latest insights on AI monetization.&lt;/p&gt;
    &lt;p&gt;We've partnered with GitLaw to launch something that should have existed from day one: a Master Services Agreement specifically designed for AI agents.&lt;/p&gt;
    &lt;p&gt;Not because we love legal documents (we don’t), but because the contracts most agent companies are using create problems they don't see until something breaks.&lt;/p&gt;
    &lt;p&gt;Most AI agent companies are still using SaaS contracts which makes no sense.&lt;/p&gt;
    &lt;p&gt;Your software isn't just sitting there helping someone fill out a form. It's booking meetings, writing code, making decisions. When something goes wrong, who's liable?&lt;/p&gt;
    &lt;p&gt;Standard contracts don't answer that question. They assume software waits for human instructions. Click button, thing happens, done.&lt;/p&gt;
    &lt;p&gt;But your agent operates differently. It decides which prospects to contact. It writes outreach messages. It follows up based on response patterns. It learns from interactions and adjusts behavior over time.&lt;/p&gt;
    &lt;p&gt;Those are autonomous actions. And when your agent does something unexpected, the gap between what your contract says and what your product does creates legal exposure you can't price for.&lt;/p&gt;
    &lt;p&gt;Your workflow agent doesn't suggest next steps. It executes them. Sends emails. Updates records. Moves data between systems. No human clicking approve at every stage.&lt;/p&gt;
    &lt;p&gt;Traditional software processes tasks one at a time when asked. Agents run 24/7, making hundreds of micro-decisions. Remember the Ford dealership chatbot that hallucinated a free truck offer? That's what happens when autonomous systems operate under contracts written for passive tools.&lt;/p&gt;
    &lt;p&gt;Static software behaves the same way every deployment. Agents learn from context, adjust to patterns, change behavior based on accumulated data. The system you shipped six months ago operates differently today.&lt;/p&gt;
    &lt;p&gt;Your SaaS contract wasn't built for any of this.&lt;/p&gt;
    &lt;p&gt;Working with Nick and the GitLaw team, we identified the contract gaps that create the most exposure for agent companies. The new MSA addresses three critical areas:&lt;/p&gt;
    &lt;p&gt;The contract establishes that your agent functions as a sophisticated tool, not an autonomous employee. When a customer's agent books 500 meetings with the wrong prospect list, the answer to "who approved that?" cannot be "the AI decided."&lt;/p&gt;
    &lt;p&gt;It has to be "the customer deployed the agent with these parameters and maintained oversight responsibility."&lt;/p&gt;
    &lt;p&gt;The MSA includes explicit language in Section 1.2 that protects you from liability for autonomous decisions while clarifying customer responsibility.&lt;/p&gt;
    &lt;p&gt;AI agents hallucinate. They produce confident outputs that turn out wrong. The MSA includes explicit disclaimers that agent outputs require human verification before material business decisions.&lt;/p&gt;
    &lt;p&gt;It also includes damage caps appropriate for unpredictable systems. Typically 12 months of fees with exclusions for indirect losses. Not being difficult. Acknowledging you can't predict every edge case in software that learns and adapts.&lt;/p&gt;
    &lt;p&gt;Section 7 covers liability limitations with AI-specific disclaimers about output accuracy in Section 4.1.&lt;/p&gt;
    &lt;p&gt;This kills more deals than any other contract issue. Your agent ingests customer data and generates outputs. You might want to use those interactions to improve your models.&lt;/p&gt;
    &lt;p&gt;Customers panic when they hear that. They imagine their proprietary data training models that help competitors.&lt;/p&gt;
    &lt;p&gt;The MSA establishes that customers own their data and any agent outputs. Then it provides separate, customizable language about using de-identified, aggregated data for training purposes. With clear opt-out options.&lt;/p&gt;
    &lt;p&gt;Most customers accept training use when it's explained clearly. Trying to slip it in through vague language destroys trust.&lt;/p&gt;
    &lt;p&gt;Section 2.1 covers ownership with customizable training permissions in the cover page variables.&lt;/p&gt;
    &lt;p&gt;At Paid, we solve billing and cost tracking for AI agents. But we kept hearing the same problem before companies even got to pricing.&lt;/p&gt;
    &lt;p&gt;Founders would tell us they couldn't figure out how to charge for agents. Then we'd look at their contracts. They were trying to price outcome-based work using terms written for seat-based software.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Most AI agent companies are still using SaaS contract language which makes no sense. Your software isn't just sitting there helping someone fill out a form. It's booking meetings, writing code, making decisions. When something goes wrong, who's liable? The standard contracts don't answer that. We kept hearing this from founders, so when GitLaw said they were building an agent-specific MSA, we jumped in. Builders need legal frameworks that match what their agents actually do".&lt;/p&gt;&lt;lb/&gt;Manny Medina, Paid CEO&lt;/quote&gt;
    &lt;p&gt;You can't bill for outcomes if your contract only covers usage. You can't price based on value delivered if your liability framework assumes predictable, passive behavior. You can't protect your margins when the legal foundation doesn't match what your product does.&lt;/p&gt;
    &lt;p&gt;The contract shapes everything that comes after. Get it wrong and your entire business model sits on shaky ground.&lt;/p&gt;
    &lt;p&gt;The MSA is open source and free to use. You can access it directly in the GitLaw Community or ask the GitLaw AI Agent to generate a customized version for your specific needs.&lt;/p&gt;
    &lt;p&gt;Because the law around AI agents is evolving rapidly, treat this as a starting point, not a substitute for legal advice. Work with a commercial lawyer to customize it for your situation.&lt;/p&gt;
    &lt;p&gt;The template uses CommonPaper's Software Licensing Agreement and AI Addendum as a foundation, adapted for the unique characteristics of AI agents.&lt;/p&gt;
    &lt;p&gt;Nick and the GitLaw team built this based on patterns from reviewing hundreds of agent contracts. We contributed our research from working with dozens of agent companies on monetization challenges.&lt;/p&gt;
    &lt;p&gt;Together, we're building the infrastructure the agent economy needs. Legal frameworks that match how agents actually work. Billing systems that align pricing with value delivery. Cost tracking that protects margins.&lt;/p&gt;
    &lt;p&gt;Because agents aren't just another SaaS feature. They're a fundamentally different product category that needs different infrastructure.&lt;/p&gt;
    &lt;p&gt;Legal frameworks always lag behind technology. Right now that lag creates real risk for anyone building agents.&lt;/p&gt;
    &lt;p&gt;You can ignore it and hope nothing breaks. Or you can use contracts built for what agents actually do, not what software did ten years ago.&lt;/p&gt;
    &lt;p&gt;Most founders choose hope. The ones who survive choose better infrastructure - and you'll soon find these MSAs baked into Paid's offering too.&lt;/p&gt;
    &lt;p&gt;→ Read the announcement on GitLaw&lt;/p&gt;
    &lt;p&gt;→ Listen to our conversation with Nick about building legal infrastructure for the agent economy.&lt;/p&gt;
    &lt;p&gt;Join 10,000+ subscribers getting the latest insights on AI monetization.&lt;/p&gt;
    &lt;p&gt;Price smarter. Protect margins. Grow revenue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45515640</guid><pubDate>Wed, 08 Oct 2025 12:55:01 +0000</pubDate></item><item><title>The email they shouldn't have read</title><link>https://it-notes.dragas.net/2025/10/08/the-email-they-shouldnt-have-read/</link><description>&lt;doc fingerprint="764a911c87a67cf9"&gt;
  &lt;main&gt;
    &lt;p&gt;Author's Note: Before we begin, an important clarification. What follows is a horror story based on real events from my career. However, to protect the privacy of the people and companies involved, I have deliberately mixed things up: technologies, contexts, and specific details have been modified or merged with other experiences. I therefore invite you to read this story not as a strict chronicle of a single event, but as an archetype of a widespread problem in the IT world: vendor lock-in and predatory business practices. Any attempt to identify the specific company or software described would lead to an incorrect conclusion.&lt;/p&gt;
    &lt;p&gt;When the phone rang, I was in a meeting - so I didnât answer. But I recognized the number and sent a quick message:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Iâm with a client right now. If itâs not urgent, please send me an email - otherwise Iâll call you ASAP".&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The reply, via SMS, left me speechless:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Thatâs exactly the problem. I canât send you an email. Call me as soon as you can".&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;From that moment on, my perception of a certain kind of world changed forever.&lt;/p&gt;
    &lt;p&gt;A few years earlier, a major public institution - letâs call it Agency A - was still running an ancient Exchange mail server. It hadnât received security updates for ages, the anti-spam was completely ineffective, and the new regulations were clear: embrace Open Source solutions whenever possible.&lt;/p&gt;
    &lt;p&gt;They had already received a proposal - expensive but, when compared to similar offers made to other organizations, apparently reasonable - for a managed service hosted by an external provider and based on an open source mail stack. The company offered a managed version with its own proprietary additions and enterprise support.&lt;/p&gt;
    &lt;p&gt;The catch? While such pricing had become almost "normal" in the market, it was still wildly inflated considering what was actually being delivered. Agency A already had solid infrastructure - reputable IP classes, redundant datacenters, everything running smoothly. We had built and maintained that environment for years, and it was still performing perfectly.&lt;/p&gt;
    &lt;p&gt;The request was simple: âEvaluate this solution, and if itâs suitable, weâll migrate.â. About 500 active mailboxes, roughly the same number of aliases. Manageable, but far from trivial.&lt;/p&gt;
    &lt;p&gt;So I started experimenting. I had heard of that stack before but never used it directly. I deployed it in some non-critical environments - ours, and a few test clients who agreed to try it at a discounted rate. Everything worked flawlessly for almost a year. I began to appreciate its design and flexibility. Confident, I told Agency A we could proceed with a pilot migration.&lt;/p&gt;
    &lt;p&gt;We built a new server, deployed the stack, and assigned a few secondary domains for early adopters. The feedback was great - so good that users started pushing for a full migration.&lt;lb/&gt; The IT team planned carefully: created accounts and aliases, migrated selected mailboxes, and kept the old Exchange server online (hidden, for legacy access).&lt;/p&gt;
    &lt;p&gt;The morning after the MX switch I was tense, waiting for trouble - but it never came. A couple of small questions, nothing serious. The internal team handled everything perfectly. It was a success.&lt;/p&gt;
    &lt;p&gt;Word spread quickly.&lt;/p&gt;
    &lt;p&gt;Agency B - smaller, but in some ways more influential - contacted me. They were customers of the same managed-service company that had pitched to Agency A. Once they saw the potential savings (at less than a tenth of the annual cost), the stability, and the freedom of keeping their data on their own servers, they became very interested. Their contract, however, was a five-year deal with automatic renewal - two years left. The legal office said the notice period was six months, so there was time.&lt;/p&gt;
    &lt;p&gt;They wanted to prepare silently. Their supplier was known for aggressive commercial behavior and often retaliated when customers tried to leave. So we built everything quietly - users, aliases, test setups - and froze the system, waiting for the official termination notice.&lt;/p&gt;
    &lt;p&gt;That day finally came. The notice was sent, about eight months before expiration. Migration would begin upon confirmation of receipt - or the following month at the latest.&lt;/p&gt;
    &lt;p&gt;Meanwhile, I learned that Agency C - another institution - was also planning to leave the same provider. They wanted to keep the same software stack for consistency, so I told them about our experience. They asked for a quote, which I prepared (without mentioning Agency B, of course). My margin would have been small, but the project made sense: it was about owning your data, not making money.&lt;/p&gt;
    &lt;p&gt;Everything seemed to move smoothly - until that SMS.&lt;/p&gt;
    &lt;p&gt;I called back immediately.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Thereâs been a problem with the termination" said the IT manager of Agency B.&lt;/p&gt;&lt;lb/&gt;"Somehow they found out what we were doing. There are hidden clauses we didnât know about, and now we canât leave - at least not for another five years. They know everything. Even your quote.".&lt;/quote&gt;
    &lt;p&gt;I was stunned. How could they possibly know?&lt;/p&gt;
    &lt;p&gt;Minutes later, my phone rang again - Agency C this time.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Forget the proposal", they said. "They called us. Threatened us, actually. They even mentioned your name and said they might take legal action against you for unfair competition - claiming theyâre the only âauthorizedâ installers of that software. Which is absurd, of course. Itâs open source. But our director doesnât want trouble.".&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Sometimes, when public money is involved, people prefer avoiding troubles over doing whatâs right.&lt;/p&gt;
    &lt;p&gt;Something didnât add up.&lt;lb/&gt; Then someone at Agency C noticed a clue: a former interim IT manager still had an email client connected via token authentication - with access to all messages. And that person had signed the original contract with the provider years before. Informally questioned, he admitted contacting them "to warn them" but claimed it was harmless. He never mentioned me - supposedly.&lt;/p&gt;
    &lt;p&gt;That still didnât explain how they knew about Agency Bâs internal steps. To test a theory, we set a trap: I asked a friend abroad to send Agency B a fake quote, from a company outside the EU.&lt;lb/&gt; The following Monday, the provider called Agency B and said, "We advise against working with non-EU companies - compliance can get tricky.".&lt;/p&gt;
    &lt;p&gt;That strongly suggested it: it looked as if they might have been reading the emails.&lt;/p&gt;
    &lt;p&gt;The IT manager exploded and confronted them. The response was chilling:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Iâm not saying we do - but we could. Itâs in the contract. You should read the fine print, especially the unilateral amendment from two years ago.".&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That amendment, quietly accepted, included horrifying clauses:&lt;lb/&gt; - notice period extended from 6 to 12 months&lt;lb/&gt; - formerly free services could become paid at the providerâs discretion&lt;lb/&gt; - and, "for security reasons", they could disable any access other than the webmail - which they promptly did.&lt;/p&gt;
    &lt;p&gt;All of this happened before the GDPR era, when certain practices could still slip through.&lt;/p&gt;
    &lt;p&gt;I tried to contact the companyâs owner directly - no reply. Calls, emails, nothing. Their support lines were "not authorized to forward requests". I wanted to confront them about the ânot accreditedâ nonsense and the so-called unfair competition. But bullies never like a fair conversation.&lt;/p&gt;
    &lt;p&gt;I urged Agency B and C to investigate - not only legally, but ethically.&lt;lb/&gt; They were horrified, yes - but in the end, nothing changed.&lt;lb/&gt; Worse: the provider, invoking that same contract amendment, made previously free features paid ones, increasing their costs by another 30%.&lt;lb/&gt; Management wasnât outraged by the abuse - just by the extra expense, "hard to justify in the budget".&lt;/p&gt;
    &lt;p&gt;Years later, those directors were gone. The technical staff remained - older, wiser, and determined not to repeat the mistake. They eventually switched providers, though to something "safer", not necessarily better.&lt;/p&gt;
    &lt;p&gt;I couldnât solve that problem. The battle had to come from them, and I would have supported them all the way - not for profit, but for principle.&lt;lb/&gt; Because when a company that claims to âsupport open sourceâ behaves like that, we all lose.&lt;lb/&gt; We all get labeled the same way.&lt;/p&gt;
    &lt;p&gt;And thatâs the real horror of the story - not the software, but what people do with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45515657</guid><pubDate>Wed, 08 Oct 2025 12:56:54 +0000</pubDate></item><item><title>The weaponization of travel blacklists</title><link>https://papersplease.org/wp/2025/10/06/the-weaponization-of-travel-blacklists/</link><description>&lt;doc fingerprint="7712fb12443571b9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The weaponization of travel blacklists&lt;/head&gt;&lt;p&gt;Coming just hours before the partial shutdown of Federal government operations, this hearing was sparsely attended, even by members of the committee, and got little press attention. The hearing opened with the Chair and Ranking Minority Member of the committee talking over each other at length.&lt;/p&gt;&lt;p&gt;Much of the argument between Senators and the questioning of witnesses focused not on the general problems of the Quiet Skies traveler surveillance program program or government travel blacklists (referred to euphemistically as “watchlists” throughout the hearing) but on whether these programs have been weaponized to a greater extent under Democratic or Republican administrations.&lt;/p&gt;&lt;p&gt;But if we — and, we hope, members of Congress — can look past the partisan polemics, the testimony and documents introduced into the record of this hearing provide important guidance on what can and should be done to protect all travelers — regardless of our party affiliation (if any), ethnicity, religious beliefs, or political opinions — against the weaponization of travel blacklists by whatever government is in power.&lt;/p&gt;&lt;p&gt;The Quiet Skies [sic] program assigned officers from the Federal Air Marshals Service (one of the police agencies within the Transportation Security Administration) to accompany and surveil pre-selected airline passengers on flights and in airports.&lt;/p&gt;&lt;p&gt;Quiet Skies is just one of many travel surveillance and blacklisting programs. According to another report by the same Senate committee last yea (under a chair from the opposite side of the aisle), “There are at least 22 different mechanisms that might lead Americans to receive additional screening at airports and other ports of entry or be denied the ability to travel.”&lt;/p&gt;&lt;p&gt;According to precious whistleblower reports and documents obtained by the Senate HSGAC Committee and entered into the record of the hearing last week, Quite Skies targets were selected by a ruleset that incorporated other US government blacklists and watchlists as well as profile-based rules that factored in ethnicity and countries previously visited.&lt;/p&gt;Targets of blacklisting and watchlisting have ranged from people selected based on having used a credit card at a Washington-area airport on or around January 6, 2021, to critics of Israel’s military actions in Gaza, among other activities protected by the First Amendment.&lt;p&gt;The Quiet Skies program was ended in June 2025 by the Secretary of Homeland Security, Kristi Noehm. The same day as the Senate hearing on Quiet Skies last week, the TSA announced that Secretary Noehm was firing five senior TSA officials associated with the Quite Skies program, including the TSA’s executive assistant administrator for operations support and the deputy assistant administrator for intelligence and analysis.&lt;/p&gt;&lt;p&gt;News reports, presumably based on DHS statements, described Quiet Skies as a “Biden-era” program even though its largest expansion came in 2018 during the first Trump administration. And according to the TSA’s press release last week:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The TSA administrator has legal authority to prevent an individual from boarding an aircraft, task Federal Air Marshals with surveilling an individual, or place them on a watchlist, but it is only intended to be used to track and prevent dangerous criminals and terrorists from carrying out attacks on Americans.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We question whether the TSA Administrator has the authority to prevent someone from traveling by common carrier without getting an order from a judge. And the TSA and DHS have consistently asserted authority and exercised power — legally or not — to blacklist and surveil travelers without needing any suspicion of criminality.&lt;/p&gt;&lt;p&gt;The bipartisanship of violations of the right to travel through blacklisting and watchlisting was brought out in the response of Mr. Abed Ayoub, Executive Director of the American-Arab Anti-Discrimination Committee (ADC) to one of the Senators’ questions about the weaponization of watchlists and blacklists against disfavored groups and individuals:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;That’s not a partisan problem. That’s a due process problem. When the government can quietly tag one group, it can quietly tag any group, left, right, neither, just depending on who holds power. The answer is the same for all of us: end the watchlists….&lt;/p&gt;&lt;p&gt;Quiet Skies captures only part of the story. It’s a window into a faulty surveillance architecture. The watchlist isn’t about who you vote for. It’s about whether the government has to prove its case when it takes away your rights….&lt;/p&gt;&lt;p&gt;Our community, for the past quarter of a century, we’re… targeted by all the administrations, so we can tell you the experience under Republican and Democratic administrations.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Further guidance about what Congress can and should do was provided by another of the witnesses, Mr. Jim Harper, a Senior Nonresident Fellow of the American Enterprise Institute:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We intuitively feel we have the right to travel, and there are cases from the Supreme Court that say you have a right to travel: You can move from one state to another and they can’t deny you Welfare benefits when you get there.&lt;/p&gt;&lt;p&gt;There are other cases: A man in San Francisco wanted to fly to Washington, DC, to meet with his Representative, and the 9th Circuit Court of Appeals said that you don’t have a right to travel on your preferred mode of transportation. They said, no, you can take the train, you can drive, if you want to go to Washington, DC.&lt;/p&gt;&lt;p&gt;A clear right to travel — Americans believe they have it, but the courts haven’t accorded it to them — would give them a position when they went to court.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Court cases challenging the no-fly list (ongoing since 2013, and now on remand following a unanimous favorable Supreme Court decision in 2024) and other travel blacklists and watchlists (ongoing since 2018) have been dragged out for years by the government. The government continues to claim that “merely” being delayed for a few hours or days isn’t sufficiently harmful to a traveler’s rights to provide a basis for a lawsuit.&lt;/p&gt;&lt;p&gt;The Freedom to Travel Act that was introduced in 2021 in the House of Representatives would provide both an explicit statutory right to travel by common carrier and a cause of action for violations of that right by government agencies, carriers, or other entities.&lt;/p&gt;&lt;p&gt;We hope Senators and Representatives moved to act on the issues raised in last week’s hearing on the TSA will re-introduce and co-sponsor the Freedom to Travel Act in this session of Congress, hold committee hearings on it, and incorporate it into appropriate omnibus legislation that has a better chance of passage than a standalone bill.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45515783</guid><pubDate>Wed, 08 Oct 2025 13:10:19 +0000</pubDate></item><item><title>We found a bug in Go's ARM64 compiler</title><link>https://blog.cloudflare.com/how-we-found-a-bug-in-gos-arm64-compiler/</link><description>&lt;doc fingerprint="3b06c011ce263a54"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Every second, 84 million HTTP requests are hitting Cloudflare across our fleet of data centers in 330 cities. It means that even the rarest of bugs can show up frequently. In fact, it was our scale that recently led us to discover a bug in Go's arm64 compiler which causes a race condition in the generated code.&lt;/p&gt;
      &lt;p&gt;This post breaks down how we first encountered the bug, investigated it, and ultimately drove to the root cause.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Investigating a strange panic&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We run a service in our network which configures the kernel to handle traffic for some products like Magic Transit and Magic WAN. Our monitoring watches this closely, and it started to observe very sporadic panics on arm64 machines.&lt;/p&gt;
      &lt;p&gt;We first saw one with a fatal error stating that traceback did not unwind completely. That error suggests that invariants were violated when traversing the stack, likely because of stack corruption. After a brief investigation we decided that it was probably rare stack memory corruption. This was a largely idle control plane service where unplanned restarts have negligible impact, and so we felt that following up was not a priority unless it kept happening.&lt;/p&gt;
      &lt;p&gt;And then it kept happening.Â &lt;/p&gt;
      &lt;p&gt;When we first saw this bug we saw that the fatal errors correlated with recovered panics. These were caused by some old code which used panic/recover as error handling.Â &lt;/p&gt;
      &lt;p&gt;At this point, our theory was:Â &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;All of the fatal panics happen within stack unwinding.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We correlated an increased volume of recovered panics with these fatal panics.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Recovering a panic unwinds goroutine stacks to call deferred functions.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;A related Go issue (#73259) reported an arm64 stack unwinding crash.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Letâs stop using panic/recover for error handling and wait out the upstream fix?&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;So we did that and watched as fatal panics stopped occurring as the release rolled out. Fatal panics gone, our theoretical mitigation seemed to work, and this was no longer our problem. We subscribed to the upstream issue so we could update when it was resolved and put it out of our minds.&lt;/p&gt;
      &lt;p&gt;But, this turned out to be a much stranger bug than expected. Putting it out of our minds was premature as the same class of fatal panics came back at a much higher rate. A month later, we were seeing up to 30 daily fatal panics with no real discernible cause; while that might account for only one machine a day in less than 10% of our data centers, we found it concerning that we didnât understand the cause. The first thing we checked was the number of recovered panics, to match our previous pattern, but there were none. More interestingly, we could not correlate this increased rate of fatal panics with anything. A release? Infrastructure changes? The position of Mars? &lt;/p&gt;
      &lt;p&gt;At this point we felt like we needed to dive deeper to better understand the root cause. Pattern matching and hoping was clearly insufficient.Â &lt;/p&gt;
      &lt;p&gt;We saw two classes of this bug -- a crash while accessing invalid memory and an explicitly checked fatal error.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 153 gp=0x4000105340 m=324 mp=0x400639ea08 [GC worker (active)]:
/usr/local/go/src/runtime/asm_arm64.s:244 +0x6c fp=0x7ff97fffe870 sp=0x7ff97fffe860 pc=0x55558d4098fc
runtime.systemstack(0x0)
       /usr/local/go/src/runtime/mgc.go:1508 +0x68 fp=0x7ff97fffe860 sp=0x7ff97fffe810 pc=0x55558d3a9408
runtime.gcBgMarkWorker.func2()
       /usr/local/go/src/runtime/mgcmark.go:1102
runtime.gcDrainMarkWorkerIdle(...)
       /usr/local/go/src/runtime/mgcmark.go:1188 +0x434 fp=0x7ff97fffe810 sp=0x7ff97fffe7a0 pc=0x55558d3ad514
runtime.gcDrain(0x400005bc50, 0x7)
       /usr/local/go/src/runtime/mgcmark.go:212 +0x1c8 fp=0x7ff97fffe7a0 sp=0x7ff97fffe6f0 pc=0x55558d3ab248
runtime.markroot(0x400005bc50, 0x17e6, 0x1)
       /usr/local/go/src/runtime/mgcmark.go:238 +0xa8 fp=0x7ff97fffe6f0 sp=0x7ff97fffe6a0 pc=0x55558d3ab578
runtime.markroot.func1()
       /usr/local/go/src/runtime/mgcmark.go:887 +0x290 fp=0x7ff97fffe6a0 sp=0x7ff97fffe560 pc=0x55558d3acaa0
runtime.scanstack(0x4014494380, 0x400005bc50)
       /usr/local/go/src/runtime/traceback.go:447 +0x2ac fp=0x7ff97fffe560 sp=0x7ff97fffe4d0 pc=0x55558d3eeb7c
runtime.(*unwinder).next(0x7ff97fffe5b0?)
       /usr/local/go/src/runtime/traceback.go:566 +0x110 fp=0x7ff97fffe4d0 sp=0x7ff97fffe490 pc=0x55558d3eed40
runtime.(*unwinder).finishInternal(0x7ff97fffe4f8?)
       /usr/local/go/src/runtime/panic.go:1073 +0x38 fp=0x7ff97fffe490 sp=0x7ff97fffe460 pc=0x55558d403388
runtime.throw({0x55558de6aa27?, 0x7ff97fffe638?})
runtime stack:
fatal error: traceback did not unwind completely
       stack=[0x4015d6a000-0x4015d8a000
runtime: g8221077: frame.sp=0x4015d784c0 top=0x4015d89fd0&lt;/code&gt;
      &lt;/quote&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 187 gp=0x40003aea80 m=13 mp=0x40003ca008 [GC worker (active)]:
       /usr/local/go/src/runtime/asm_arm64.s:244 +0x6c fp=0x7fff2afde870 sp=0x7fff2afde860 pc=0x55557e2d98fc
runtime.systemstack(0x0)
       /usr/local/go/src/runtime/mgc.go:1489 +0x94 fp=0x7fff2afde860 sp=0x7fff2afde810 pc=0x55557e279434
runtime.gcBgMarkWorker.func2()
       /usr/local/go/src/runtime/mgcmark.go:1112
runtime.gcDrainMarkWorkerDedicated(...)
       /usr/local/go/src/runtime/mgcmark.go:1188 +0x434 fp=0x7fff2afde810 sp=0x7fff2afde7a0 pc=0x55557e27d514
runtime.gcDrain(0x4000059750, 0x3)
       /usr/local/go/src/runtime/mgcmark.go:212 +0x1c8 fp=0x7fff2afde7a0 sp=0x7fff2afde6f0 pc=0x55557e27b248
runtime.markroot(0x4000059750, 0xb8, 0x1)
       /usr/local/go/src/runtime/mgcmark.go:238 +0xa8 fp=0x7fff2afde6f0 sp=0x7fff2afde6a0 pc=0x55557e27b578
runtime.markroot.func1()
       /usr/local/go/src/runtime/mgcmark.go:887 +0x290 fp=0x7fff2afde6a0 sp=0x7fff2afde560 pc=0x55557e27caa0
runtime.scanstack(0x40042cc000, 0x4000059750)
       /usr/local/go/src/runtime/traceback.go:458 +0x188 fp=0x7fff2afde560 sp=0x7fff2afde4d0 pc=0x55557e2bea58
runtime.(*unwinder).next(0x7fff2afde5b0)
goroutine 0 gp=0x40003af880 m=13 mp=0x40003ca008 [idle]:
PC=0x55557e2bea58 m=13 sigcode=1 addr=0x118
SIGSEGV: segmentation violation&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now we could observe some clear patterns. Both errors occur when unwinding the stack in &lt;code&gt;(*unwinder).next&lt;/code&gt;. In one case we saw an intentional fatal error as the runtime identified that unwinding could not complete and the stack was in a bad state. In the other case there was a direct memory access error that happened while trying to unwind the stack. The segfault was discussed in the GitHub issue and a Go engineer identified it as dereference of a go scheduler struct, m, when unwinding.Â   &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;A review of Go scheduler structs&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Go uses a lightweight userspace scheduler to manage concurrency. Many goroutines are scheduled on a smaller number of kernel threads â this is often referred to as M:N scheduling. Any individual goroutine can be scheduled on any kernel thread. The scheduler has three core types â &lt;code&gt;g&lt;/code&gt;Â  (the goroutine), &lt;code&gt;m&lt;/code&gt; (the kernel thread, or âmachineâ), and &lt;code&gt;p&lt;/code&gt; (the physical execution context, orÂ  âprocessorâ). For a goroutine to be scheduled a free &lt;code&gt;m&lt;/code&gt; must acquire a free &lt;code&gt;p&lt;/code&gt;, which will execute a g. Each &lt;code&gt;g&lt;/code&gt; contains a field for its m if it is currently running, otherwise it will be nil. This is all the context needed for this post but the go runtime docs explore this more comprehensively.Â &lt;/p&gt;
      &lt;p&gt;At this point we can start to make inferences on whatâs happening: the program crashes because we try to unwind a goroutine stack which is invalid. In the first backtrace, if a return address is null, we call &lt;code&gt;finishInternal&lt;/code&gt; and abort because the stack was not fully unwound. The segmentation fault case in the second backtrace is a bit more interesting: if instead the return address is non-zero but not a function then the unwinder code assumes that the goroutine is currently running. It'll then dereference m and fault by accessing &lt;code&gt;m.incgo&lt;/code&gt; (the offset of &lt;code&gt;incgo&lt;/code&gt; into &lt;code&gt;struct m&lt;/code&gt; is 0x118, the faulting memory access). &lt;/p&gt;
      &lt;p&gt;What, then, is causing this corruption? The traces were difficult to get anything useful from â our service has hundreds if not thousands of active goroutines. It was fairly clear from the beginning that the panic was remote from the actual bug. The crashes were all observed while unwinding the stack and if this were an issue any time the stack was unwound on arm64 we would be seeing it in many more services. We felt pretty confident that the stack unwinding was happening correctly but on an invalid stack.Â &lt;/p&gt;
      &lt;p&gt;Our investigation stalled for a while at this point â making guesses, testing guesses, trying to infer if the panic rate went up or down, or if nothing changed. There was a known issue on Goâs GitHub issue tracker which matched our symptoms almost exactly, but what they discussed was mostly what we already knew. At some point when looking through the linked stack traces we realized that their crash referenced an old version of a library that we were also using â Go Netlink.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 1267 gp=0x4002a8ea80 m=nil [runnable (scan)]:
runtime.asyncPreempt2()
        /usr/local/go/src/runtime/preempt.go:308 +0x3c fp=0x4004cec4c0 sp=0x4004cec4a0 pc=0x46353c
runtime.asyncPreempt()
        /usr/local/go/src/runtime/preempt_arm64.s:47 +0x9c fp=0x4004cec6b0 sp=0x4004cec4c0 pc=0x4a6a8c
github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(0x14360300000000?)
        /go/pkg/mod/github.com/!data!dog/[email protected]/nl/nl_linux.go:803 +0x130 fp=0x4004cfc710 sp=0x4004cec6c0 pc=0xf95de0
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;We spot-checked a few stack traces and confirmed the presence of this Netlink library. Querying our logs showed that not only did we share a library â every single segmentation fault we observed had happened while preempting &lt;code&gt;NetlinkSocket.Receive&lt;/code&gt;.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Whatâs (async) preemption?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;In the prehistoric era of Go (&amp;lt;=1.13) the runtime was cooperatively scheduled. A goroutine would run until it decided it was ready to yield to the scheduler â usually due to explicit calls to &lt;code&gt;runtime.Gosched()&lt;/code&gt; or injected yield points at function calls/IO operations. Since Go 1.14 the runtime instead does async preemption. The Go runtime has a thread &lt;code&gt;sysmon&lt;/code&gt; which tracks the runtime of goroutines and will preempt any that run for longer than 10ms (at time of writing). It does this by sending &lt;code&gt;SIGURG&lt;/code&gt; to the OS thread and in the signal handler will modify the program counter and stack to mimic a call to &lt;code&gt;asyncPreempt&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;At this point we had two broad theories:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;This is a Go Netlink bug â likely due to &lt;code&gt;unsafe.Pointer&lt;/code&gt; usage which invoked undefined behavior but is only actually broken on arm64&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;This is a Go runtime bug and we're only triggering it in &lt;code&gt;NetlinkSocket.Receive&lt;/code&gt; for some reason&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;After finding the same bug publicly reported upstream, we were feeling confident this was caused by a Go runtime bug. However, upon seeing that both issues implicated the same function, we felt more skeptical â notably the Go Netlink library uses unsafe.Pointer so memory corruption was a plausible explanation even if we didn't understand why.&lt;/p&gt;
      &lt;p&gt;After an unsuccessful code audit we had hit a wall. The crashes were rare and remote from the root cause. Maybe these crashes were caused by a runtime bug, maybe they were caused by a Go Netlink bug. It seemed clear that there was something wrong with this area of the code, but code auditing wasnât going anywhere. &lt;/p&gt;
      &lt;p&gt;At this point we had a fairly good understanding of what was crashing but very little understanding of why it was happening. It was clear that the root cause of the stack unwinder crashing was remote from the actual crash, and that it had to do with &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt;, but why? We were able to capture a coredump of a production crash and view it in a debugger. The backtrace confirmed what we already knew â that there was a segmentation fault when unwinding a stack. The crux of the issue revealed itself when we looked at the goroutine which had been preempted while calling &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt;.Â     &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;(dlv) bt
0  0x0000555577579dec in runtime.asyncPreempt2
   at /usr/local/go/src/runtime/preempt.go:306
1  0x00005555775bc94c in runtime.asyncPreempt
   at /usr/local/go/src/runtime/preempt_arm64.s:47
2  0x0000555577cb2880 in github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive
   at
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:779
3  0x0000555577cb19a8 in github.com/vishvananda/netlink/nl.(*NetlinkRequest).Execute
   at 
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:532
4  0x0000555577551124 in runtime.heapSetType
   at /usr/local/go/src/runtime/mbitmap.go:714
5  0x0000555577551124 in runtime.heapSetType
   at /usr/local/go/src/runtime/mbitmap.go:714
...
(dlv) disass -a 0x555577cb2878 0x555577cb2888
TEXT github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(SB) /vendor/github.com/vishvananda/netlink/nl/nl_linux.go
        nl_linux.go:779 0x555577cb2878  fdfb7fa9        LDP -8(RSP), (R29, R30)
        nl_linux.go:779 0x555577cb287c  ff430191        ADD $80, RSP, RSP
        nl_linux.go:779 0x555577cb2880  ff434091        ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
        nl_linux.go:779 0x555577cb2884  c0035fd6        RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;The goroutine was paused between two opcodes in the function epilogue. Since the process of unwinding a stack relies on the stack frame being in a consistent state, it felt immediately suspicious that we preempted in the middle of adjusting the stack pointer. The goroutine had been paused at 0x555577cb2880, between&lt;code&gt; ADD $80, RSP, RSP and ADD $(16&amp;lt;&amp;lt;12), RSP, RSP&lt;/code&gt;.Â &lt;/p&gt;
      &lt;p&gt;We queried the service logs to confirm our theory. This wasnât isolated â the majority of stack traces showed that this same opcode was preempted. This was no longer a weird production crash we couldnât reproduce. A crash happened when the Go runtime preempted between these two stack pointer adjustments. We had our smoking gun.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Building a minimal reproducer&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;At this point we felt pretty confident that this was actually just a runtime bug and it should be reproducible in an isolated environment without any dependencies. The theory at this point was:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Stack unwinding is triggered by garbage collection&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Async preemption between a split stack pointer adjustment causes a crash&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;What if we make a function which splits the adjustment and then call it in a loop?&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;quote&gt;
        &lt;code&gt;package main

import (
	"runtime"
)

//go:noinline
func big_stack(val int) int {
	var big_buffer = make([]byte, 1 &amp;lt;&amp;lt; 16)

	sum := 0
	// prevent the compiler from optimizing out the stack
	for i := 0; i &amp;lt; (1&amp;lt;&amp;lt;16); i++ {
		big_buffer[i] = byte(val)
	}
	for i := 0; i &amp;lt; (1&amp;lt;&amp;lt;16); i++ {
		sum ^= int(big_buffer[i])
	}
	return sum
}

func main() {
	go func() {
		for {
			runtime.GC()
		}
	}()
	for {
		_ = big_stack(1000)
	}
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This function ends up with a stack frame slightly larger than can be represented in 16 bits, and so on arm64 the Go compiler will split the stack pointer adjustment into two opcodes. If the runtime preempts between these opcodes then the stack unwinder will read an invalid stack pointer and crash.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; epilogue for main.big_stack
ADD $8, RSP, R29
ADD $(16&amp;lt;&amp;lt;12), R29, R29
ADD $16, RSP, RSP
; preemption is problematic between these opcodes
ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;After running this for a few minutes the program panicked as expected!&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;SIGSEGV: segmentation violation
PC=0x60598 m=8 sigcode=1 addr=0x118

goroutine 0 gp=0x400019c540 m=8 mp=0x4000198708 [idle]:
runtime.(*unwinder).next(0x400030fd10)
        /home/thea/sdk/go1.23.4/src/runtime/traceback.go:458 +0x188 fp=0x400030fcc0 sp=0x400030fc30 pc=0x60598
runtime.scanstack(0x40000021c0, 0x400002f750)
        /home/thea/sdk/go1.23.4/src/runtime/mgcmark.go:887 +0x290 

[...]

goroutine 1 gp=0x40000021c0 m=nil [runnable (scan)]:
runtime.asyncPreempt2()
        /home/thea/sdk/go1.23.4/src/runtime/preempt.go:308 +0x3c fp=0x40003bfcf0 sp=0x40003bfcd0 pc=0x400cc
runtime.asyncPreempt()
        /home/thea/sdk/go1.23.4/src/runtime/preempt_arm64.s:47 +0x9c fp=0x40003bfee0 sp=0x40003bfcf0 pc=0x75aec
main.big_stack(0x40003cff38?)
        /home/thea/dev/stack_corruption_reproducer/main.go:29 +0x94 fp=0x40003cff00 sp=0x40003bfef0 pc=0x77c04
Segmentation fault (core dumped)

real    1m29.165s
user    4m4.987s
sys     0m43.212s&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;A reproducible crash with standard library only? This felt like conclusive evidence that our problem was a runtime bug.&lt;/p&gt;
      &lt;p&gt;This was an extremely particular reproducer! Even now with a good understanding of the bug and its fix, some of the behavior is still puzzling. It's a one-instruction race condition, so itâs unsurprising that small changes could have large impact. For example, this reproducer was originally written and tested on Go 1.23.4, but did not crash when compiled with 1.23.9 (the version in production), even though we could objdump the binary and see the split ADD still present! We donât have a definite explanation for this behavior â even with the bug present there remain a few unknown variables which affect the likelihood of hitting the race condition.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;A single-instruction race condition window&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;arm64 is a fixed-length 4-byte instruction set architecture. This has a lot of implications on codegen but most relevant to this bug is the fact that immediate length is limited. &lt;code&gt;add&lt;/code&gt; gets a 12-bit immediate, &lt;code&gt;mov&lt;/code&gt; gets a 16-bit immediate, etc. How does the architecture handle this when the operands don't fit? It depends â &lt;code&gt;ADD&lt;/code&gt; in particular reserves a bit for "shift left by 12" so any 24 bit addition can be decomposed into two opcodes. Other instructions are decomposed similarly, or just require loading an immediate into a register first.Â &lt;/p&gt;
      &lt;p&gt;The very last step of the Go compiler before emitting machine code involves transforming the program into &lt;code&gt;obj.Prog&lt;/code&gt; structs. It's a very low level intermediate representation (IR) that mostly serves to be translated into machine code.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;//https://github.com/golang/go/blob/fa2bb342d7b0024440d996c2d6d6778b7a5e0247/src/cmd/internal/obj/arm64/obj7.go#L856

// Pop stack frame.
// ADD $framesize, RSP, RSP
p = obj.Appendp(p, c.newprog)
p.As = AADD
p.From.Type = obj.TYPE_CONST
p.From.Offset = int64(c.autosize)
p.To.Type = obj.TYPE_REG
p.To.Reg = REGSP
p.Spadj = -c.autosize
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Notably, this IR is not aware of immediate length limitations. Instead, this happens in asm7.go when Go's internal intermediate representation is translated into arm64 machine code. The assembler will classify an immediate in conclass based on bit size and then use that when emitting instructions â extra if needed.&lt;/p&gt;
      &lt;p&gt;The Go assembler uses a combination of (&lt;code&gt;mov, add&lt;/code&gt;) opcodes for some adds that fit in 16-bit immediates, and prefers (&lt;code&gt;add, add + lsl 12&lt;/code&gt;) opcodes for 16-bit+ immediates.Â   &lt;/p&gt;
      &lt;p&gt;Compare a stack of (slightly larger than) &lt;code&gt;1&amp;lt;&amp;lt;15&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; //go:noinline
; func big_stack() byte {
; 	var big_stack = make([]byte, 1&amp;lt;&amp;lt;15)
; 	return big_stack[0]
; }
MOVD $32776, R27
ADD R27, RSP, R29
MOVD $32784, R27
ADD R27, RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;With a stack of &lt;code&gt;1&amp;lt;&amp;lt;16&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;; //go:noinline
; func big_stack() byte {
; 	var big_stack = make([]byte, 1&amp;lt;&amp;lt;16)
; 	return big_stack[0]
; } 
ADD $8, RSP, R29
ADD $(16&amp;lt;&amp;lt;12), R29, R29
ADD $16, RSP, RSP
ADD $(16&amp;lt;&amp;lt;12), RSP, RSP
RET
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In the larger stack case, there is a point between &lt;code&gt;ADD x, RSP, RSP&lt;/code&gt; opcodes where the stack pointer is not pointing to the tip of a stack frame. We thought at first that this was a matter of memory corruption â that in handling async preemption the runtime would push a function call on the stack and corrupt the middle of the stack. However, this goroutine is already in the function epilogue â any data we corrupt is actively in the process of being thrown away. What's the issue then?  &lt;/p&gt;
      &lt;p&gt;The Go runtime often needs to unwind the stack, which means walking backwards through the chain of function calls. For example: garbage collection uses it to find live references on the stack, panicking relies on it to evaluate &lt;code&gt;defer&lt;/code&gt; functions, and generating stack traces needs to print the call stack. For this to work the stack pointer must be accurate during unwinding because of how golang dereferences sp to determine the calling function. If the stack pointer is partially modified, the unwinder will look for the calling function in the middle of the stack. The underlying data is meaningless when interpreted as directions to a parent stack frame and then the runtime will likely crash.Â &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;//https://github.com/golang/go/blob/66536242fce34787230c42078a7bbd373ef8dcb0/src/runtime/traceback.go#L373

if innermost &amp;amp;&amp;amp; frame.sp &amp;lt; frame.fp || frame.lr == 0 {
    lrPtr = frame.sp
    frame.lr = *(*uintptr)(unsafe.Pointer(lrPtr))
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;When async preemption happens it will push a function call onto the stack but the parent stack frame is no longer correct because sp was only partially adjusted when the preemption happened. The crash flow looks something like this: &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Async preemption happens between the two opcodes that &lt;code&gt;add x, rsp&lt;/code&gt; expands to&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Garbage collection triggers stack unwinding (to check for heap object liveness)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The unwinder starts traversing the stack of the problematic goroutine and correctly unwinds up to the problematic function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The unwinder dereferences &lt;code&gt;sp&lt;/code&gt; to determine the parent function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Almost certainly the data behind &lt;code&gt;sp&lt;/code&gt; is not a function&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Crash&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;We saw earlier a faulting stack trace which ended in &lt;code&gt;(*NetlinkSocket).Receive&lt;/code&gt; â in this case stack unwinding faulted while it was trying to determine the parent frame.Â    &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;goroutine 90 gp=0x40042cc000 m=nil [preempted (scan)]:
runtime.asyncPreempt2()
/usr/local/go/src/runtime/preempt.go:306 +0x2c fp=0x40060a25d0 sp=0x40060a25b0 pc=0x55557e299dec
runtime.asyncPreempt()
/usr/local/go/src/runtime/preempt_arm64.s:47 +0x9c fp=0x40060a27c0 sp=0x40060a25d0 pc=0x55557e2dc94c
github.com/vishvananda/netlink/nl.(*NetlinkSocket).Receive(0xff48ce6e060b2848?)
/vendor/github.com/vishvananda/netlink/nl/nl_linux.go:779 +0x130 fp=0x40060b2820 sp=0x40060a27d0 pc=0x55557e9d2880
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Once we discovered the root cause we reported it with a reproducer and the bug was quickly fixed. This bug is fixed in go1.23.12, go1.24.6, and go1.25.0. Previously, the go compiler emitted a single &lt;code&gt;add x, rsp&lt;/code&gt; instruction and relied on the assembler to split immediates into multiple opcodes as necessary. After this change, stacks larger than 1&amp;lt;&amp;lt;12 will build the offset in a temporary register and then add that to &lt;code&gt;rsp&lt;/code&gt; in a single, indivisible opcode. A goroutine can be preempted before or after the stack pointer modification, but never during. This means that the stack pointer is always valid and there is no race condition.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;LDP -8(RSP), (R29, R30)
MOVD $32, R27
MOVK $(1&amp;lt;&amp;lt;16), R27
ADD R27, RSP, RSP
RET&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This was a very fun problem to debug. We donât often see bugs where you can accurately blame the compiler. Debugging it took weeks and we had to learn about areas of the Go runtime that people donât usually need to think about. Itâs a nice example of a rare race condition, the sort of bug that can only really be quantified at a large scale.&lt;/p&gt;
      &lt;p&gt;Weâre always looking for people who enjoy this kind of detective work. Our engineering teams are hiring. &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45516000</guid><pubDate>Wed, 08 Oct 2025 13:33:15 +0000</pubDate></item><item><title>Testing a compiler-driven full-stack web framework</title><link>https://wasp.sh/blog/2025/10/07/how-we-test-a-web-framework</link><description>&lt;doc fingerprint="b5019a1367551ed1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How we test a web framework&lt;/head&gt;
    &lt;p&gt;Wasp is a compiler-driven full-stack web framework; it takes configuration and source files with your unique logic, and it generates the complete source code your the web app. Think of a Rails-like framework for React, Node.js and Prisma.&lt;/p&gt;
    &lt;p&gt;As a result of our approach and somewhat unique design, we have a large surface area to test. Every layer can break in its own creative way, and a strong suite of automated tests is what keeps us (somewhat) sane.&lt;/p&gt;
    &lt;p&gt;In this article, our goal is to demonstrate the practical side of testing in a compiler-driven full-stack framework, where traditional testing intersects with code generation and developer experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our approach to tests&lt;/head&gt;
    &lt;p&gt;If we wanted to reduce our principle to a single sentence, it would be: We believe that test code deserves the same care as production code.&lt;/p&gt;
    &lt;p&gt;Bad tests slow you down. They make you afraid to change things. So our principle is simple: if a piece of test code matters enough to catch a bug, it matters enough to be well-designed. We refactor it. We name things clearly. We make it easy to read and reason about.&lt;/p&gt;
    &lt;p&gt;It’s not new or revolutionary; it’s just consistent care, applied where most people stop caring.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tests that explain themselves&lt;/head&gt;
    &lt;p&gt;Our guiding principle is that tests should be readable at a glance, without requiring an understanding of the machinery hiding underneath. That’s why we write them so that the essence of the test, the input and expected output, comes first. Supporting logic and setup details follow afterward, only for those who need to understand the details.&lt;/p&gt;
    &lt;code&gt;spec_kebabToCamelCase :: Spec&lt;/code&gt;
    &lt;p&gt;That rule naturally connects to the next one: tests should be descriptive enough that you can understand their essence without additional comments. That’s why sometimes we end up with beautifully long descriptions like this:&lt;/p&gt;
    &lt;code&gt;spec_WriteFileDrafts :: Spec&lt;/code&gt;
    &lt;p&gt;The nice thing about writing tests in Haskell is how easy it is to build tiny DSLs that make tests readable. And for us, reading code is much more important than writing it; we even leaned into Unicode operators for math operations. But the boundary between clarity and productivity can be tricky when you realize nobody remembers how to type “⊆”.&lt;/p&gt;
    &lt;code&gt;  describe "isSubintervalOf" $ do&lt;/code&gt;
    &lt;head rend="h3"&gt;Courage not coverage&lt;/head&gt;
    &lt;p&gt;Chasing 100% coverage is fun. It’s complete. But it’s also hard to do. It can push you to spend time testing code paths that don’t really matter. It looks good in the report, but while getting there, you miss out on testing potentially important stuff.&lt;/p&gt;
    &lt;p&gt;The goal is that our combined tests catch nearly all meaningful errors. We aim for “courage”. Confidence that if something breaks, we’ll know fast.&lt;/p&gt;
    &lt;head rend="h3"&gt;TDD (but not the one you think)&lt;/head&gt;
    &lt;p&gt;We've always liked the idea of test-driven development, but it never really stuck for us. In practice, we’d start coding and only after something worked, would we add tests.&lt;/p&gt;
    &lt;p&gt;One thing we love is strong typing (we use TypeScript and Haskell), describing what the feature should look like and how data should flow. Once the types make sense, the implementation becomes straightforward. It’s leaning on the compiler to guide you along the way. For us, that rhythm feels more natural, the Type-Driven Development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing the compiler&lt;/head&gt;
    &lt;p&gt;At the core of our framework sits the compiler, written in Haskell. It takes a configuration file and user source code as input, and it assembles a full-stack web app as output.&lt;/p&gt;
    &lt;p&gt;Although Haskell has excellent reliability and type safety (e.g., check out our library for type-safe paths), tests are still necessary. We use unit tests to ensure our compiler’s logic is correct. But the compiler’s most important product is the generated code that exists outside the Haskell domain. To verify the generated code, we use the end-to-end (e2e) tests.&lt;/p&gt;
    &lt;head rend="h3"&gt;Our E2E tests story&lt;/head&gt;
    &lt;p&gt;The purpose of our e2e tests is to verify that the Wasp binary works as expected. We are not concerned with the internal implementation, only its interface and outputs.&lt;/p&gt;
    &lt;p&gt;The interface is the Wasp CLI (called &lt;code&gt;waspc&lt;/code&gt;). Every command is treated as a black box: we feed it input, observe its side effects, and verify the output.&lt;/p&gt;
    &lt;p&gt;The primary output of &lt;code&gt;waspc&lt;/code&gt; is a Wasp app. So we validate that each command correctly generates or modifies an app. Secondary outputs are installer behavior, uninstall flow, &lt;code&gt;bash&lt;/code&gt; completions, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tracking each and every change&lt;/head&gt;
    &lt;p&gt;Wasp generates a considerable amount of code, and even small compiler tweaks can cause the weirdest changes in the output — a real-life butterfly effect. We want to be sure that each PR doesn't cause any unexpected changes.&lt;/p&gt;
    &lt;p&gt;Snapshot tests are the crown jewel of our e2e story. We use it to track the compiler’s code generation changes in the form of golden vs current snapshots. We test the actual (current) output vs. the expected (golden) output.&lt;/p&gt;
    &lt;p&gt;They are an efficient way to gain high confidence in the generated output with relatively little test code, a good fit for code generation. Because we track golden snapshots with Git, every pull request clearly shows how the generated code changes.&lt;/p&gt;
    &lt;p&gt;To make it clear what we are testing, we build our test cases from simple:&lt;/p&gt;
    &lt;code&gt;waspNewSnapshotTest :: SnapshotTest&lt;/code&gt;
    &lt;p&gt;To more complex ones, feature by feature (command by command):&lt;/p&gt;
    &lt;code&gt;waspMigrateSnapshotTest :: SnapshotTest&lt;/code&gt;
    &lt;p&gt;What does this look like in practice? Suppose while modifying a feature, we accidentally added a stray character (e.g., a dot) while editing a Mustache template, which means it will also appear in the generated code. If we now run snapshot tests to compare the current output of the compiler with the golden (expected) one, it will detect the change in the generated files and ask us to review it:&lt;/p&gt;
    &lt;p&gt;Now we can check the change and accept it if it was expected, or fix it if not. Finally, when we are satisfied with the current snapshot, we record it as a new golden snapshot.&lt;/p&gt;
    &lt;head rend="h3"&gt;Untangling TypeScript from Mustache&lt;/head&gt;
    &lt;p&gt;Mustache templates make up the core of our code generation. Any file with dynamic content is a Mustache template, be it TypeScript, HTML, or a Dockerfile. It made sense as we need the compiler to inject them with relevant data.&lt;/p&gt;
    &lt;p&gt;While this gave us a lot of control and flexibility while generating the code, it also created development challenges. Mustache templates aren’t valid TypeScript, so they broke TypeScript’s own ecosystem: linters, formatters, and tests.&lt;/p&gt;
    &lt;p&gt;This inconvenience made our usual development workflow consist of generating a Wasp app, making modifications to the generated files, and carrying over the changes back to the template. Repeat the process until we get it right.&lt;/p&gt;
    &lt;p&gt;That is why we’re migrating most of the TypeScript logic from Mustaches templates into dedicated &lt;code&gt;npm&lt;/code&gt; packages. This will leave templates as mostly simple &lt;code&gt;import&lt;/code&gt;/&lt;code&gt;export&lt;/code&gt; wrappers, while allowing us to build and test the TypeScript side of the source code with full type safety and normal toolqing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing the Wasp apps&lt;/head&gt;
    &lt;p&gt;Besides the compiler, we also ship many Wasp apps ourselves, including starter templates and example apps. We maintain them and update them together with the compiler. Our goal is to test the Wasp apps in runtime, and we use &lt;code&gt;playwright&lt;/code&gt; e2e tests for that.&lt;/p&gt;
    &lt;head rend="h3"&gt;Starter templates&lt;/head&gt;
    &lt;p&gt;Typically, every Wasp app starts from a starter template. They are prebuilt Wasp apps that you generate through Wasp CLI to get you started. As they are our first line of UX (or DX?), it's essential to keep the experience as smooth and flawless as possible.&lt;/p&gt;
    &lt;p&gt;What is most important is to test the starter templates themselves. Each starter represents a different promise that we have to validate. We test their domains rather than the framework itself.&lt;/p&gt;
    &lt;p&gt;Interestingly, since starter templates are Mustache templates, we can’t test them directly. Instead, we must initialize new projects through the Wasp CLI, on which we run the prebuilt &lt;code&gt;playwright&lt;/code&gt; e2e tests.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example apps&lt;/head&gt;
    &lt;p&gt;Starter templates get you started, but Wasp features so many more features. To test the entire framework end-to-end, we had to build additional Wasp apps — example apps. They serve a dual purpose: to serve as a public examples of what can be built with Wasp and how, but also as a testing suite on which we run extensive tests.&lt;/p&gt;
    &lt;p&gt;We test each framework feature with &lt;code&gt;playwright&lt;/code&gt;. On each PR, we build the development version of Wasp, and each example app runs its e2e tests in isolation. While golden snapshots provide clarity into code generation changes, these tests serve to ensure none of the framework features' expectations were broken.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kitchen sink&lt;/head&gt;
    &lt;p&gt;The kitchen sink app is the "holy grail" of example apps. We test most of the framework features in this single application (smartly named &lt;code&gt;kitchen-sink&lt;/code&gt; ). If you’re not familiar with the term “kitchen sink application”, think of it like a Swiss knife for framework features&lt;/p&gt;
    &lt;p&gt;Kitchen sink is also one of the applications we snapshot in our snapshot tests. So, &lt;code&gt;kitchen-sink&lt;/code&gt; not only serves to test that the code works in the runtime, but also tracks any changes to the code generation.&lt;/p&gt;
    &lt;p&gt;We have one golden rule when modifying/adding framework features: “There must be a test in the example applications which covers this feature.”&lt;/p&gt;
    &lt;head rend="h3"&gt;When Kitchen Sink is not enough (or too much)&lt;/head&gt;
    &lt;p&gt;Previously, I mentioned that &lt;code&gt;kitchen-sink&lt;/code&gt; tests most of the framework features. Most — because Wasp has mutually exclusive features. For example, &lt;code&gt;usernameAndPassword&lt;/code&gt; authentication vs &lt;code&gt;email&lt;/code&gt; authentication (yes, &lt;code&gt;email&lt;/code&gt; authentication also uses a password, I didn’t design the name). So we try to pick up the scraps with the rest of the smaller example apps.&lt;/p&gt;
    &lt;p&gt;While the &lt;code&gt;kitchen-sink&lt;/code&gt; application is suitable for showcasing the framework's power to users, it’s impossible to test all of the features in a single application. Nor is it the proper way to test Wasp end-to-end.&lt;/p&gt;
    &lt;p&gt;This is how our “variants” idea sparked. The idea is to build variants on top of the &lt;code&gt;minimal&lt;/code&gt; starter. E.g., “Wasp app but using SendGrid email sender”, “Wasp app but using Mailgun email sender”…&lt;/p&gt;
    &lt;p&gt;For each possible feature that exists, a Wasp application of that feature should exist. It's something we haven't yet solved, but we plan to address it as we approach the Wasp 1.0 release. For now, the &lt;code&gt;kitchen-sink&lt;/code&gt; app serves us well enough.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building tools for your tests&lt;/head&gt;
    &lt;p&gt;Wasp applications are complex systems with many parts: front-end, back-end, database, and specific requirements and differences between the &lt;code&gt;development&lt;/code&gt; and &lt;code&gt;production&lt;/code&gt; versions of the application. This makes test automation cumbersome.&lt;/p&gt;
    &lt;p&gt;You can do it, but you really don’t want to repeat the process. So we’ve packaged it into our own driver called &lt;code&gt;wasp-app-runner&lt;/code&gt;. It exports two simple commands: &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;build&lt;/code&gt;. It’s not suitable for development purposes (nor deployment), but for testing, it’s perfect. Tooling for your tests is tooling for your sanity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing the deployment&lt;/head&gt;
    &lt;p&gt;Wasp CLI can automatically deploy your Wasp applications to certain supported providers. You set the production environment variables, and the command does everything else.&lt;/p&gt;
    &lt;p&gt;To ensure deployment continues to work correctly, each code merge on the Wasp repository triggers a test deployment of the &lt;code&gt;kitchen-sink&lt;/code&gt; example app using the development version of Wasp, followed by basic smoke tests on the client and server to confirm everything runs smoothly. Finally, we clean up the deployed app.&lt;/p&gt;
    &lt;p&gt;When releasing a new version of our framework, we follow the same procedure described above, but for all the example apps, not just the &lt;code&gt;kitchen-sink&lt;/code&gt; one: we redeploy their test deployments using this new version of the framework. However, these deployments remain permanent, as we use example apps to showcase Wasp to users.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing the docs (kind of)&lt;/head&gt;
    &lt;p&gt;APIs change fast, in a startup building a pre-1.0 framework. Documentation lags even faster. You tweak a feature, push the code, and somewhere, a forgotten code example still lies.&lt;/p&gt;
    &lt;p&gt;We’re careful about updating documentation when features change, but some references hide in unexpected corners. It’s a recurring pain: docs are the primary way developers experience your tool, yet they’re often the easiest part to let rot. So we started treating documentation more like code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping code examples honest&lt;/head&gt;
    &lt;p&gt;You modify a feature, update the API, but some part of the docs still shows an old example. Users (and I) prefer copy-pasting examples over reading API documentation. We copy and paste broken snippets and expect things to work, but they don’t.&lt;/p&gt;
    &lt;p&gt;Wouldn’t it be nice if docs’ code examples were also tested like Wasp app examples? Why not combine the two?&lt;/p&gt;
    &lt;p&gt;We agreed that the docs examples must reference the source code of example apps. Each code snippet in the docs must declare a source file in one of the example apps where that same code resides (with some caveats). We can automatically verify that the reference is correct and the code matches; if not, the CI fails.&lt;/p&gt;
    &lt;p&gt;We are implementing this as a &lt;code&gt;Docusaurus&lt;/code&gt; plugin called &lt;code&gt;code-ref-checker&lt;/code&gt;. It’s still a work in progress, but we’re happy with the early results (notice the code ref in the header):&lt;/p&gt;
    &lt;code&gt;```ts title="src/auth.ts" ref="waspc/examples/todoApp/src/auth/signup.ts:L1-14"&lt;/code&gt;
    &lt;p&gt;An additional benefit is that, besides ensuring code examples in the docs don't become stale, it forces us to test every feature, because when we write documentation and add a code example, it can’t exist without implementing it first inside an example app.&lt;/p&gt;
    &lt;head rend="h3"&gt;Making tutorials testable&lt;/head&gt;
    &lt;p&gt;We have a “Todo App” tutorial in our documentation that, before every release, we would manually review and verify to ensure it was still valid. Someone would have to execute all the steps, and once they finally finish them, they would still have to test the resulting Wasp app.&lt;/p&gt;
    &lt;p&gt;While &lt;code&gt;code-ref-checker&lt;/code&gt; solved the examples drift, tutorials add a time dimension. They evolve as the reader builds the app: files appear, disappear, and change with each step. So we opted for a new solution.&lt;/p&gt;
    &lt;p&gt;Looking at our tutorial, each step changes the project: run a CLI command, apply a diff, and move on. We realized the tutorial basically repeats those two actions over and over.&lt;/p&gt;
    &lt;p&gt;So we built a small CLI tool integrating with the &lt;code&gt;Docusaurus&lt;/code&gt; plugin to formalize that process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Each step defines an action.&lt;/item&gt;
      &lt;item&gt;The CLI can replay all steps to rebuild the final app automatically.&lt;/item&gt;
      &lt;item&gt;Steps are easily editable in isolation.&lt;/item&gt;
      &lt;item&gt;That final app is then tested like any other Wasp app.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We call it &lt;code&gt;TACTE&lt;/code&gt;, the Tutorial Action Executor.&lt;/p&gt;
    &lt;p&gt;In &lt;code&gt;TACTE&lt;/code&gt;, each step is declared via a JSX component that lives next to the tutorial content itself, and the CLI helps us define the actions to make the process work.&lt;/p&gt;
    &lt;code&gt;To setup a new Wasp project, run the following command in your terminal:&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;TACTE&lt;/code&gt; is still in development, but we are planning to publish it as a library in the near future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45516134</guid><pubDate>Wed, 08 Oct 2025 13:45:30 +0000</pubDate></item><item><title>Now open for building: Introducing Gemini CLI extensions</title><link>https://blog.google/technology/developers/gemini-cli-extensions/</link><description>&lt;doc fingerprint="994b9cc09aaba6b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Now open for building: Introducing Gemini CLI extensions&lt;/head&gt;
    &lt;p&gt;The best tools are the ones that adapt to you, not the other way around. For developers whose work is becoming more complex every day, the need for personalized, intelligent assistance has never been greater.&lt;/p&gt;
    &lt;p&gt;That’s why we’re announcing Gemini CLI extensions, a new framework that allows you to customize Gemini CLI and connect it to the tools you use most, all from the command line. Instead of context-switching between your terminal and other tools, you can now bring those tools directly into your workflow.&lt;/p&gt;
    &lt;p&gt;In just three months since our launch, more than one million developers are building with Gemini CLI. And they can now access a new ecosystem of extensions from Google, plus industry leaders like Dynatrace, Elastic, Figma, Harness, Postman, Shopify, Snyk and Stripe, and the broader open-source community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Personalize your command line with Gemini CLI extensions&lt;/head&gt;
    &lt;p&gt;Gemini CLI is an open-source, AI-powered agent for your terminal, and extensions are its power-ups — pre-packaged, easily installable integrations that connect it to external tools including everything from databases and design platforms to payment services.&lt;/p&gt;
    &lt;p&gt;Each extension contains a built-in “playbook” that instantly teaches the AI how to use the new tools effectively. This means you get meaningful results from the very first command, no complex setup required, allowing you to tailor your experience with the tools most valuable to you.&lt;/p&gt;
    &lt;p&gt;It’s easy to install an extension — simply type: “gemini extensions install &amp;lt;add your GitHub URL or local path&amp;gt;” from your command line.&lt;/p&gt;
    &lt;p&gt;Easily install extensions from Gemini CLI's open ecosystem&lt;/p&gt;
    &lt;head rend="h2"&gt;Access an open, growing ecosystem of partners and builders&lt;/head&gt;
    &lt;p&gt;Extensions put Gemini CLI at the center of an open ecosystem in which anyone can build integrations. That’s why in addition to our own set of Google-created extensions, we’re launching with a strong group of partners and open-source contributors.&lt;/p&gt;
    &lt;p&gt;To make extensions easy to find and use, we’re also launching a new Gemini CLI Extensions page. Here, you can discover a growing catalog of community, partner and Google-built extensions, ranked by popularity by GitHub stars.&lt;/p&gt;
    &lt;p&gt;You can get started with extensions from a wide range of launch partners and more coming soon. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dynatrace: Get real-time insights into application performance, availability and root-cause analysis directly from your CLI to accelerate debugging.&lt;/item&gt;
      &lt;item&gt;Elastic: Search, retrieve and analyze Elasticsearch data in developer and agentic workflows. Connects directly to an Elastic MCP server hosted in Elastic Cloud Serverless.&lt;/item&gt;
      &lt;item&gt;Figma: Generate code from frames, extract design context, retrieve resources and ensure design system consistency with your codebase.&lt;/item&gt;
      &lt;item&gt;Harness: Bring AI-powered intelligence to CI/CD by analyzing pipeline execution data, surfacing cost insights, detecting failure patterns and automatically remediating issues to accelerate software delivery.&lt;/item&gt;
      &lt;item&gt;Postman: Have AI agents access Postman workspaces, manage collections and environments, evaluate APIs and automate workflows through natural language interactions.&lt;/item&gt;
      &lt;item&gt;Shopify: Connect to Shopify's developer ecosystem with tools to search docs, explore API schemas, and build serverless Shopify functions.&lt;/item&gt;
      &lt;item&gt;Snyk: Seamlessly integrate Snyk's comprehensive security capabilities into your development process to ensure that code is secure at inception.&lt;/item&gt;
      &lt;item&gt;Stripe: Define a set of tools that AI agents can use to interact with the Stripe API and search the knowledge base.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;More than a connection: See how extensions add intelligence&lt;/head&gt;
    &lt;p&gt;Developers get more from Gemini CLI by integrating Model Context Protocol (MCP) tools, and extensions build on this by enabling even smarter interactions. While MCP provides the raw connection to a tool, a Gemini CLI extension takes the basic ability to use that tool and wraps it in a layer of intelligence and personalization. This makes the experience seamless for developers.&lt;/p&gt;
    &lt;p&gt;Gemini CLI extensions are easy to install and have a simple “playbook” — a set of tools it knows how to use, like a local script or a third-party API. When you run a command, Gemini CLI consults this playbook and uses the context from your environment (like your local files and git status) to execute the right tool for the job, exactly how you intended.&lt;/p&gt;
    &lt;p&gt;If you want to look under the hood, Gemini CLI extensions package instructions, MCP servers and custom commands into a familiar and user-friendly format. Extensions can bundle any combination of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One or more MCP servers: To connect with external tools and services.&lt;/item&gt;
      &lt;item&gt;Context files: Like GEMINI.md or bring your own context file type(s), to provide specific instructions and guidelines to the model.&lt;/item&gt;
      &lt;item&gt;Excluded tools: Useful for disabling built-in tools or offering alternative implementations.&lt;/item&gt;
      &lt;item&gt;Custom commands: To encapsulate complex prompts into simple slash commands.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use Gemini CLI to access all kinds of extensions, including one for image generation with Nano Banana&lt;/p&gt;
    &lt;head rend="h2"&gt;Discover Google-created extensions&lt;/head&gt;
    &lt;p&gt;Googlers have also been building a suite of extensions for Gemini CLI. Give them a try; they just might help you solve some common developer pain points, deepen integration with other Google offerings or just have fun:&lt;/p&gt;
    &lt;p&gt;For cloud-native deployments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go from local code to a live public URL in a single step, with the Cloud Run extension.&lt;/item&gt;
      &lt;item&gt;Manage your Google Kubernetes Engine (GKE) clusters, from checking node health to deploying applications with our GKE extension.&lt;/item&gt;
      &lt;item&gt;Give Gemini CLI the ability to easily interact with your Google Cloud environment by using the gcloud extension.&lt;/item&gt;
      &lt;item&gt;Understand, manage and troubleshoot your Google Cloud environment with the Google Cloud Observability extension.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For app builders:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Perform code reviews on your codebase with the Code Review extension.&lt;/item&gt;
      &lt;item&gt;Perform AI-powered vulnerability detection on your code changes with the Security extension.&lt;/item&gt;
      &lt;item&gt;Retrieve location-based data from Google and embed Google Maps imagery into applications with the Google Maps Platform extension.&lt;/item&gt;
      &lt;item&gt;Create, build, refactor, debug and maintain Flutter applications with the Flutter extension.&lt;/item&gt;
      &lt;item&gt;Control and inspect a live Chrome browser for reliable automation, in-depth debugging and performance analysis with the Chrome DevTools extension.&lt;/item&gt;
      &lt;item&gt;Set up and manage your Firebase backend with the Firebase extension.&lt;/item&gt;
      &lt;item&gt;Enhance the user experience for building GenAI-powered apps with the Genkit extension.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For generative AI and data interaction:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For a bit of fun, generate and edit images with the Nano Banana extension 🍌.&lt;/item&gt;
      &lt;item&gt;Explore and visualize your business data with the Looker extension.&lt;/item&gt;
      &lt;item&gt;Build applications and analyze trends with services like Cloud SQL, AlloyDB BigQuery and more with our Data Cloud extensions.&lt;/item&gt;
      &lt;item&gt;Connect to enterprise data easily and securely using the MCP Toolbox for Databases extension.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Build the CLI of your dreams&lt;/head&gt;
    &lt;p&gt;Gemini CLI extensions put you in control. You can combine extensions, chain commands and build a personalized toolchain that perfectly fits the way you work.&lt;/p&gt;
    &lt;p&gt;Whether you want to streamline a personal workflow or integrate a company's internal tools, you now have the power to create the command-line experience you've always wanted.&lt;/p&gt;
    &lt;p&gt;Ready to get started? Visit the new Gemini CLI Extensions page to explore community tools, and check out our templates and a step-by-step guide to help you build your first extension and share it with the community.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45516426</guid><pubDate>Wed, 08 Oct 2025 14:13:30 +0000</pubDate></item><item><title>Show HN: Recall: Give Claude perfect memory with Redis-backed persistent context</title><link>https://www.npmjs.com/package/@joseairosa/recall</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45516584</guid><pubDate>Wed, 08 Oct 2025 14:28:06 +0000</pubDate></item><item><title>Vectrex Mini</title><link>https://vectrex.com/vectrex-mini-details/</link><description>&lt;doc fingerprint="27d74f9593ed6a7c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Vectrex Mini Details&lt;/head&gt;
    &lt;p&gt;Experience the spirit of the original Vectrex in a modern, compact format. &lt;lb/&gt;After three years of development and refinement, the Vectrex Mini is ready for production.&lt;/p&gt;
    &lt;p&gt;The Kickstarter goes live on November 3rd 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Console&lt;/head&gt;
    &lt;p&gt;The Vectrex Mini captures the full spirit of the original Vectrex in a case half the size. Its plastic shell is made using injection molding, just like the console of the time, and each unit will come in packaging inspired by the original box.&lt;/p&gt;
    &lt;p&gt;On the technical side, the console features a built-in 5-inch AMOLED display with a resolution of 800×600, delivering sharp and bright vector graphics. Power is supplied via USB-C, so it can run on a wall outlet as well as an external battery. The Vectrex Mini is powered by an ESP32, a modern and reliable microprocessor, powerful enough to run the entire Vectrex game library with performance beyond many arcade machines of the era.&lt;/p&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;p&gt;The console will include 12 built-in classic games, mostly from the General Consumer Electronics catalog (the final selection is still being completed, with the official list to be announced later). Each game will come with its own physical overlay to recreate the visual experience of the 1980s.&lt;/p&gt;
    &lt;p&gt;Sticker sheets will also be included, featuring different official logos (current, European, American, and Japanese), so players can customize their machine.&lt;/p&gt;
    &lt;p&gt;A Vector Clock mode will turn the console into a clock displaying the time, date, and weather thanks to Wi-Fi connectivity.&lt;/p&gt;
    &lt;p&gt;An alarm can also be set, allowing the Vectrex Mini to double as a stylish bedside clock or desk companion.&lt;/p&gt;
    &lt;p&gt;The Vectrex Mini offers several connection options. A micro-SD slot will allow the addition of games or homebrews (a card will likely not be included).&lt;/p&gt;
    &lt;p&gt;A Bluetooth controller with four action buttons and a self-centering analog joystick will be provided and can be stored in the console as with the original. A second controller can also be connected for multiplayer.&lt;/p&gt;
    &lt;p&gt;Finally, a video output will be integrated at the back of the unit (the final choice between HDMI or USB-C will be confirmed during production).&lt;/p&gt;
    &lt;head rend="h2"&gt;Special Edition&lt;/head&gt;
    &lt;p&gt;A limited run of 250 units will be produced. The White Edition has all the features of the standard version but stands out with a fully white finish, a unique serial number, and a certificate of authenticity. Aimed at collectors, it will come in its own box.&lt;/p&gt;
    &lt;head rend="h2"&gt;Merchandise&lt;/head&gt;
    &lt;p&gt;Several items will be available to accompany the release of the Vectrex Mini:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A second controller can be purchased separately and will include a dongle to connect it to the original console.&lt;/item&gt;
      &lt;item&gt;Four T-shirt designs will be released, featuring artwork of the console itself, a Vectrex-style Superman, Minestrom, and Spike.&lt;/item&gt;
      &lt;item&gt;A book written by Douglas Alves will also be part of the collection. A well-known French specialist in video game history, longtime journalist, MO5 association member, and teacher in the field, he retraces the story of the Vectrex in this work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Kickstarter Campaign&lt;/head&gt;
    &lt;p&gt;After three years dedicated to prototyping and preparing for production, the project is now ready. We have set a clear timeline with defined milestones for testing, final adjustments, and assembly. Manufacturing will be carried out in partnership with teams based in Taiwan.&lt;/p&gt;
    &lt;p&gt;The Kickstarter campaign aims to finance this production phase. We waited until we had a fully functional prototype, strong partners, and a realistic schedule before launching. See you on November 3 on Kickstarter to discover all the details and join the adventure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45516690</guid><pubDate>Wed, 08 Oct 2025 14:38:23 +0000</pubDate></item><item><title>After 2 decades of tinkering, MAME cracks the Hyper Neo Geo 64</title><link>https://www.readonlymemo.com/mame-hyper-neo-geo-support-sound-emulation/</link><description>&lt;doc fingerprint="372e9953cb024b11"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;After 2 decades of tinkering, MAME finally cracks the Hyper Neo Geo 64&lt;/head&gt;
    &lt;p&gt;How MAME devs finally got sound working for the 3D arcade system. Plus: PC Engine LaserActive support gets fast-tracked.&lt;/p&gt;
    &lt;p&gt;Let me test out a theory here: If you're into emulation, you're into older video games, ergo you're into old stuff of all kinds. That means you, savvy, good-taste-having reader, will love this spread of photos I took in Tokyo last week at the National Film Archive of Japan, which has a small but lovely set of exhibits from the history of Japanese film. Since you like playing Super Nintendo games this is absolutely your shit, right? Right??&lt;/p&gt;
    &lt;p&gt;Okay, I'll throw in a pic of some games to sweeten the deal.&lt;/p&gt;
    &lt;p&gt;This issue is coming a week late as I was off to Japan last week for my first-ever visit to the Tokyo Game Show, and too busy working (and working at eating sushi) to squeeze in a newsletter. And it's coming late in the day Sunday — apologies! But patience pays off!!&lt;/p&gt;
    &lt;p&gt;This issue's main story has been cookin' for a minute: last month the news landed that MAME had finally properly cracked Hyper Neo Geo 64 support, but the celebration was a little bit premature. The arcade system was playable in MAME, yes, but sound was in really shoddy shape — it wasn't yet a particularly good experience.&lt;/p&gt;
    &lt;p&gt;Over the last month or so that's been changing, and changing fast, with frequent improvements checked in by a pair of regular MAME contributors. So now is the time to talk about it, and soon (with the very next MAME release!) it will be time to actually play it. Considering there are only seven Hyper Neo Geo 64 games, well, that's a week's worth of evenings sorted.&lt;/p&gt;
    &lt;p&gt;As with every trip to Tokyo I took a few hours to stop by Akihabara this time, but its pull has certainly lessened over the years as retro prices have skyrocketed from where they were a decade ago and the selection has gotten thinner and thinner. Still, browsing the stores is a fun time and there are great finds to be found as long as you're not looking for anything too in-demand. I picked up one game: Kamiwaza, a PS2 "stealth" game where you play as a thief in feudal Japan stealing hella stuff.&lt;/p&gt;
    &lt;p&gt;As you might guess, it's more silly than stealthy.&lt;/p&gt;
    &lt;p&gt;Shout out to my shopping partner in crime, Paradise Killer's Oli Clarke Smith, for the recommendation. I've got a feature on the way in the coming weeks over at PC Gamer based on some of the games we picked up and how they speak to the "identity" of particular retro consoles. I'm hoping it'll be a fun read!&lt;/p&gt;
    &lt;p&gt;For now, let's hop into MAME; then stick around for an update on Pioneer LaserActive emulation!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Big Two&lt;/head&gt;
    &lt;head rend="h3"&gt;1. The Hyper Neo Geo comes to MAME: Now with working sound!&lt;/head&gt;
    &lt;p&gt;21 years ago, David "MameHaze" Haywood started looking into what it would take to add support for the Hyper Neo Geo 64 arcade system — then just five years past the end of its short lifespan — to MAME. "When I started looking at the system back in 2004 MAME didn't really do much 3D stuff at all, even things like the MIPS (main CPU) core were in a much rougher shape, there were no dumps of the I/O MCU at all (happened only a few years ago) and the PC I had at the time barely had enough memory to load and decode even the 2D graphics," he says.&lt;/p&gt;
    &lt;p&gt;"It was also pre-YouTube, and even in the early days of YouTube you didn't really get much in the way of good reference material. Kinda crazy to think that a lot of people who are probably interested in the emulation of the platform now as younger adults weren't even born when emulation work first started on it!"&lt;/p&gt;
    &lt;p&gt;A few weeks ago, two decades after he started looking into the system, Haywood finally promoted it to "working" status in MAME. But that move was a bit of a formality, or a bit sneaky, depending on how you look at it. Though the promotion got some buzz, it wasn't truly finished: proper sound emulation was still missing. Haywood actually hadn't worked on the core since 2023, and decided, well, people had been playing the games for long enough without sound, he might as well slap the "working" label on. It turned out to be the final push other MAME contributors needed to take a crack at tuning up the sound.&lt;/p&gt;
    &lt;p&gt;What's the Hyper Neo Geo's whole deal, anyway? Well, it makes some sense that the system would be more a curiosity for younger folks to discover than an object of intense nostalgia like some of MAME's more high-profile cores or the original Neo Geo; it was only active in arcades for two years from 1997 to 1999 during the awkward transitional period to 3D, with just seven games released for it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Road's Edge&lt;/item&gt;
      &lt;item&gt;Samurai Shodown 64&lt;/item&gt;
      &lt;item&gt;Xtreme Rally&lt;/item&gt;
      &lt;item&gt;Beat Busters: Second Nightmare&lt;/item&gt;
      &lt;item&gt;Samurai Shodown 64: Warriors Rage&lt;/item&gt;
      &lt;item&gt;Fatal Fury: Wild Ambition&lt;/item&gt;
      &lt;item&gt;Buriki One&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here's a nice write-up of the system from Nicole Express that delves into the hardware:&lt;/p&gt;
    &lt;p&gt;None of these games have the Bloodborne-style pull to individually inspire interest in emulation, but the MAME team's all-consuming drive to reverse-engineer and archive every arcade system in existence kept it on the radar. Haywood's initial investigation into it predated even MAME's high profile support of Capcom's CPS3 boards, for instance, but once that platform was decrypted support was added quite quickly because hell yeah people wanted to play Street Fighter III. By comparison, "Hyper64 has been a 21 year on-and-off slog," he says.&lt;/p&gt;
    &lt;p&gt;It's a perfect representation one of the eternal frustrations of emulation development: People often ask why no one's working on something when they actually are. Just invisibly.&lt;/p&gt;
    &lt;quote&gt;"The sheer number of times I picked up the Hyper64 driver and pumped weeks of work in it only to not be able to make any progress at all was frustrating at the best of times. Just trying to gain an understanding of it all, but ending up not making any headway at all. That's something I don't think people really appreciate when it comes to emulation, the amount of time that you have to put in which often yields no positive results at all, where all you can really conclude is it doesn't work the way you were hoping it would work."&lt;/quote&gt;
    &lt;p&gt;A few years ago, Haywood finally made substantial progress: Someone dumped the I/O microcontroller, and he was able to write a CPU core to emulate it. "The inputs finally started working in a bunch of the games, which allowed me to explore them further and make video improvements," he said.&lt;/p&gt;
    &lt;p&gt;"Other components improving in MAME over the years has really helped too. When I started MAME didn't have a CPU core for the V53 either (which is a V33 CPU with variolus peripherals) and is the CPU driving the sound DSP. At some point in MAME's history the V53 support got fleshed out (for other systems) which has really come in handy now, as proper sound emulation requires that to be running properly."&lt;/p&gt;
    &lt;p&gt;When Haywood marked the platform as working, it caught the attention of another longtime MAME contributor: R. Belmont. For the last month or so, Belmont, as well as two other devs, Happy and O. Galibert, have been chipping away at making the games sound like they're supposed to.&lt;/p&gt;
    &lt;p&gt;"Haze marking them working did provide a push, and Happy had done a detailed disassembly of the sound CPU program which was quite useful," Belmont recently posted on Reddit. Belmont and Galibert have both been working on synthesizer support in MAME, and the Hyper Neo Geo 64's sound chip happens to be used it one, providing them with some convenient overlap in interest/speciality.&lt;/p&gt;
    &lt;p&gt;The current MAME release, 0.281, includes a series of rapid-fire improvements as documented by Belmont in a few videos on YouTube:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Very early work in progress on better audio for the Hyper Neo Geo 64 system. There is a loooong way to go, this is just some very basic fixes so far."&lt;/item&gt;
      &lt;item&gt;"Since the first video we've got the basic sample starts and stops working the actual correct way they're supposed to and added a preliminary support for volume envelopes, which also helps the audio balance. Still a lot of work to go though."&lt;/item&gt;
      &lt;item&gt;"More progress today! I figured out how the volume envelopes really work, and that made Buriki One's intro mostly awesome."&lt;/item&gt;
      &lt;item&gt;"Barring any last minute adjustments, this is what HNG64 audio will sound like in MAME 0.281. Since the last video, the per-voice low pass filter was added, which cleans up some of the high frequency 'hash' audible previously and makes the sound a bit cleaner."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It was a ton of progress for a month, taking Hyper Neo Geo 64 sound support from messy and inaccurate to, at least, broadly playable without assaulting your ears. But the real refinements have been coming in just the last few days since 0.281's release in late September.&lt;/p&gt;
    &lt;p&gt;But the next build is gonna be the big one. October's upcoming MAME 0.282 release will notably fix up the audio issues in one of the the trickiest games, Xtreme Rally, while really polishing up the rest. Here's what Belmont's noted in update notes for 0.282 so far:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Olivier Galibert figured out how they squeezed 12 bits of dynamic range into 8 bits (presumably this is the format from Roger Linn's original MPC60 design) and replaced the biquad low-pass filter with a more likely Chamberlin one that fits the parameters better. Also there were some improvements to the filter envelope. All this results in much clearer and higher-fidelity sound."&lt;/item&gt;
      &lt;item&gt;"This time we've figured out how looping samples actually work, fixed the final mixdown to not introduce any distortion, and fixed the filter envelope. The result? A dramatic improvement to Beast Busters Second Nightmare's intro."&lt;/item&gt;
      &lt;item&gt;"So the previous fixes seem to have solved Samurai Shodown 64 and SS64 2, but Xtreme Rally (aka Off Beat Racer) was still extremely broken. The engine sound barely worked, sounds were missing, and some sounds would stick looping forever. This time the problem wasn't actually in the sound emulation itself; Xtreme Rally has unique code among the 7 HNG64 games that tries to push sound commands to the sound CPU as quickly as possible. This resulted in as many as 2/3rds of the commands getting dropped on the floor. I have fixed that issue so that all of the commands make it, and Xtreme Rally now sounds great."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since Belmont made that post he dialed in a few more improvements where the wrong sounds were playing in certain instances in the game. Here's a video from Haywood showing off the vastly improved audio (though he notes "some graphical issues, such as the fog in the tunnel still need addressing eventually.")&lt;/p&gt;
    &lt;p&gt;Once MAME 0.282 releases, the Hyper Neo Geo 64 will well and truly be worthy of the "working" label. Turns out it just needed that last little push.&lt;/p&gt;
    &lt;p&gt;In a perfect encapsulation of how these sorts of collaborative projects come together, Galibert noted on Reddit that despite their contributions to the core stemming from an interest in emulating synthesizers, "amusingly, the synth itself (MPC3000) is not working at all yet." Some parts of the synthesizer remain undumped and undocumented, just as parts of the Hyper Neo Geo once were; sometimes while you're waiting for all the pieces to fall into place, it turns out one of the pieces you do have happens to fit into another puzzle entirely.&lt;/p&gt;
    &lt;p&gt;"It's just been a long slow process," Haywood said. "Things have inched forward a little bit over the years, and the surrounding code in MAME has become better / more capable, allowing for more progress to be made, step by step."&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Can't stop Pioneering: NEC support and big LaserActive performance improvements arrive in the latest Ares nightlies&lt;/head&gt;
    &lt;p&gt;Often I end a big story, like the August issue's deep dive into the 16 years it took to emulate the Pioneer LaserActive, with the door open to a follow-up many months or years down the road. In this story, our hero — emudev Nemesis — finished work on one of two modules for the Laserdisc-based gaming console, making it possible to play Sega's Mega LD games via emulation for the first time.&lt;/p&gt;
    &lt;p&gt;As of publication time, Nemesis was juuuust starting to take a look at the work required to do the same for the other "pak" players could slot into the LaserActive to play NEC PC Engine games, but who knew how long that would take?&lt;/p&gt;
    &lt;p&gt;Maybe we'd come back to it before the end of 2026, or maybe next year, or maybe in half a deca-&lt;/p&gt;
    &lt;p&gt;Oh. He already did it.&lt;/p&gt;
    &lt;p&gt;"NEC LDROM2 support is functioning on nightly builds of the v147 prerelease, and will be included in the next official Ares release," Nemesis recently wrote on his website. It took less than three weeks. While you can grab the nightly build anytime, when the next stable build of Ares releases, it'll be all official-like.&lt;/p&gt;
    &lt;p&gt;Considering the bulky size of the LaserActive game rips — they can take up dozens of gigabytes — some performance optimizations Nemesis has implemented in the last few days are almost as exciting as the second console support. Because now you should be able to run the images off a decent HDD without performance issues. Here's the breakdown on Github:&lt;/p&gt;
    &lt;quote&gt;"This change brings speed enhancements to LaserActive games. The linear resampling coefficient precalculation reduces overall CPU overhead by approximately 30%, making slower CPUs much more likely to achieve full framerate. Additionally, frame prefetch using a background thread makes games much more tolerant of IO latency, making it possible to play games back from platter drives over SATA3.&lt;lb/&gt;This should be sufficient to make emulation performance acceptable on 95%+ of systems, and I don't have any further optimizations planned at this stage."&lt;/quote&gt;
    &lt;p&gt;So then — LaserActive support is more or less feature complete. What can you play? What should you play? Seeing as the system's deader than dead and nobody's likely to be playing copyright cop, the Laserdisc rips are being freely uploaded and shared here. Not every game is available yet, but here's where you should probably start:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vajra and Vajra 2 - A pair of LaserActive-exclusive rail shooters by Data West&lt;/item&gt;
      &lt;item&gt;Triad Stone - An FMV game in the Dragon's Lair game&lt;/item&gt;
      &lt;item&gt;J.B. Harold - Blue Chicago Blues - As described by Nemesis, an "FMV murder mystery detective game, with a surprising amount of freedom. You have control over where to go, what actions to take, and what questions to ask. This title came on a double-sided CLV disc, giving it four times the video content of a typical single-sided CAV LaserActive title. The game also used separate video streams per field, to squeeze a whopping 4 hours of footage into one disc."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are a couple other cool games and curiosities on the system, including more rail shooters, some prototypes of Myst, and a German TV movie that lets you swap between different perspectives — but the above should give you a taste for that sweet sweet (or fuzzy, fuzzy) '90s laser gaming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Patching In&lt;/head&gt;
    &lt;p&gt;MiSTer's Taito F2 core pulls off a Hat Trick – Taito's 1990 game Football Champ, aka Hat Trick Hero, is now playable on the MiSTer's arcade core, as is the baseball game Ah Eikou no Koshien. The latter's surprisingly expressive for its era and looks like a lot more fun than I expect from '90s baseball games.&lt;/p&gt;
    &lt;p&gt;MiSTer's CDi core once again threatens you by functioning – In the latest unstable nightly build of the CDi core, developer Andre Zeps has committed several crimes of CDi improvement, including: "Fix dual SDRAM mode," "Add support for chroma subcarrier for clean composite video from external RGB converters," and "- Add bob deinterlacing to ascal." Go on then, but don't blame me if you start bleeding from every orifice while playing Wind of Gamelon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Core Report&lt;/head&gt;
    &lt;p&gt;Windows builds of RPCS3 are back in business – Due to a compiler issue, RPCS3's latest builds haven't been available on Windows since back in June, but they're back and working again now. Meanwhile, contributor Whatcookie has created a surprisingly detailed breakdown of how hard it is to do nothing, efficiently.&lt;/p&gt;
    &lt;p&gt;Eden is off the Play Store, for now – Well, so much for that. After launching on Android a few weeks ago, the Switch emulator has been taken down, though you can still find builds, including for Android, on the Github. Aren't DMCA takedowns lovely?&lt;/p&gt;
    &lt;p&gt;Speedrunning-focused emulator BizHawk gets hexed – But in a good way! The source port DSDA-Doom has been integrated into BizHawk, supporting Doom, Heretic and Hexen. It also now has an integrated DOSBox-X core, as well as Opera, for the 3DO.&lt;/p&gt;
    &lt;p&gt;ShadPS4 gets more Unreal – The latest build of ShadPS4 marks a significant milestone: some Unreal Engine games for the console are now playable, and even more are bootable. Look at all these games that work!&lt;/p&gt;
    &lt;head rend="h2"&gt;Translation Station&lt;/head&gt;
    &lt;p&gt;Tis the season for brains... Dead of the Brain (2) – It's spooky season, which means the crew behind the translation of PC-98 adventure game Dead of the Brain is back with the sequel two years after the first! "Like the original game, this is also a point-and-click adventure involving zombies, but this time the gameplay is much simpler, but there's still a degree of brute force required," translation crew WINE says. Playing this one might be a bit tedious, but the art is :chefskiss:&lt;/p&gt;
    &lt;p&gt;Virtual-On, on PS3 – The PS3 re-release of this mecha game had English dialogue etc., but its UI was in Japanese. This translation patch changes that.&lt;/p&gt;
    &lt;p&gt;Wizardy VI, on Saturn – There are at least nine platforms you can play Wizardry: Bane of the Cosmic Forge on, from the 1990 DOS original to the Amiga and FM Towns and modern ports, but the Japanese-only Saturn port is unique, incorporating features from Wizardry 7, and now playable with the original English script. This version has "more spells, more traps, and more skills (though most of the extra skills do nothing in 6, unfortunately), and the art style too is very reminiscent of 7 ... it’s also got a much easier early game, which a lot of new players have notoriously struggled with when playing the other versions," hacker and fan Remisse told Sega Saturn Shiro.&lt;/p&gt;
    &lt;p&gt;Undercover Cops play board games on the GB – Prolific translation group Stardust Crusaders is back with a Game Boy board/card game based on the arcade game. I'm not gonna say it's one of Irem's all-timers (find me playing Ninja Baseball Bat Man instead), but it's cute!&lt;/p&gt;
    &lt;head rend="h2"&gt;Good pixels&lt;/head&gt;
    &lt;p&gt;It's early October which means it's basically Halloween, right? Here's a load of screenshots from the first couple hours of Dead of the Brain 2. 👻&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45516968</guid><pubDate>Wed, 08 Oct 2025 15:01:38 +0000</pubDate></item><item><title>A deep dive into the RSS feed reader landscape</title><link>https://lighthouseapp.io/blog/feed-reader-deep-dive</link><description>&lt;doc fingerprint="be149c54c0323cec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A deep dive into the rss feed reader landscape&lt;/head&gt;
    &lt;p&gt;RSS feeds and, in one form or another, feed readers, have existed for more than 20 years. Their main purpose is enabling their users to consume content from various sources in one place. And especially in recent years, also helping users deal with content overload.&lt;/p&gt;
    &lt;p&gt;Back then there were only a handful of relevant products. Today it's different, there are products for many different situations and use-cases. When first getting into RSS and feed readers, it can be difficult to find out which of this wide range of products are the right ones to use.&lt;/p&gt;
    &lt;p&gt;This article describes the landscape so you can find out which product fits best for your use-case.&lt;/p&gt;
    &lt;p&gt;Side-note: I'm using RSS as a synonym for all web feed standards (Atom, JSON Feed)&lt;/p&gt;
    &lt;head rend="h2"&gt;Classification&lt;/head&gt;
    &lt;p&gt;I attempted a classification of feed readers based on 2 axes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deployment model: local (phone or PC), browser extension, self-hosted, hosted&lt;/item&gt;
      &lt;item&gt;Business model: free, one-time payment, SAAS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The deployment model is based on where data is stored and feed fetching happens. Some products have a web app and mobile apps, but feed fetching happens on the server. In this case it's classified as &lt;code&gt;hosted&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The business model categorization is based on the cheapest option that gives access to the full feature set of the product. A self-hostable product with a hosted option is categorized into &lt;code&gt;free&lt;/code&gt;. A freemium SAAS product is categorized as &lt;code&gt;paid (SAAS)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And here the image in table format, for easily clickable links:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Pricing&lt;/cell&gt;
        &lt;cell role="head"&gt;On-device (phone or PC)&lt;/cell&gt;
        &lt;cell role="head"&gt;Browser extension&lt;/cell&gt;
        &lt;cell role="head"&gt;Self-hosted&lt;/cell&gt;
        &lt;cell role="head"&gt;Hosted&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Free&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Paid (one-time)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Hosting models&lt;/head&gt;
    &lt;head rend="h3"&gt;Browser extension&lt;/head&gt;
    &lt;p&gt;Feed readers deployed as browser extensions can be installed through the respective stores of the browsers, usually either the Chrome Web Store or the Firefox Add-ons.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Setup typically only requires installing the extension, not even an account is needed. There is no maintenance required beyond the occasional update, which usually happens automatically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Data is stored locally, with browser storage functionality (local storage or IndexedDB). How much data can be stored depends on the storage of the device. Browser extensions can only store as much data as the browser allows itself to use. But for most users this is easily enough.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the device itself. Because the extension only runs if the browser runs, feeds are only fetched if the browser is open. This can lead to missed posts, depending on the publishing frequency of the feed and how often the browser is active.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Since all data is stored on the device, by default it's available only on the device where the extension is installed. If users enable it, the extension supports it, and the user is signed in, browsers can sync data to other devices that have the extension. Storing all data on the device also means that it is accessible offline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: Extensions can integrate deeply with the browser, and offer features like automatic feed finding of visited websites. Extensions can also provide a comprehensive feature set. Their only limitations are more compute-intensive features (e.g. requiring machine learning), or features that require special infrastructure to run (e.g. emails).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products (free)&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In this category I only found one product. Smart-RSS was another one, but was discontinued in February.&lt;/p&gt;
    &lt;head rend="h3"&gt;On-device&lt;/head&gt;
    &lt;p&gt;On-device products are separate applications and installed on the device you want to use them. Either on your iOS or Android phone or tablet, or on your Windows, Mac, or Linux computer. They fetch feeds and store data on that device.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: In general setup is done by installing the application. Some products may require an account, but that is not the norm. Maintenance is similar to browser extensions, the occasional updates. Some applications require manual update installations, others do that automatically.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Data is stored locally on the device, which gives total control over the data. The maximum amount of feeds and articles stored only depend on the available storage of the device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the device. For that to happen the application must be running. Some products may implement a background fetching service, in that case only the device must be turned on. Similar to browser extensions, this limitation may lead to missed posts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Typically the data of on-device feed readers are only available on the device where it's installed. Data sync would need to be done manually or via operating system specific mechanisms (e.g. iCloud), which not all products support. Since data is stored on the device, it's also available offline.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: On-device products can use the full computing power of the device. Combined with the comparatively small storage requirements of one user, it means that some features can be significantly faster than other categories. The exact features depend on the application, and mobile apps are typically less powerful than desktop apps. Limitations are only features that require multiple users (e.g. recommendations) or specific infrastructure (e.g. newsletter subscriptions).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Self-hosted&lt;/head&gt;
    &lt;p&gt;Self-hosted products all fall into the open-source and free category. They are designed to be installed on a server to run continuously, though it is possible to install them on a PC as well. They fetch feeds and store data on the server, and make it available through a web interface.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Setup requires a server, installing the application, and depending on how it should be accessible, possibly also domain and reverse proxy setup. This needs significantly more technical knowledge than the other options, but with ChatGPT (or others) it should be possible also for fairly non-technical people. This setup is generally more complex with more failure points than the other options, but with a correct setup failures happen rarely, if at all.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: The application stores data on the server it runs on. Since users control their server, they also have total control over the data stored on it. Applications typically use well-established databases, which allows users to inspect and change data with common tools. The amount of data that can be stored is only limited by the available storage on the server. Most servers start at 20 GB, which is enough for most feed reading use-cases. Often, storage can be dynamically extended if more space is needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Feeds are fetched on the server. Since it runs continuously, there is no break in feed fetching, and even high frequency feeds are no problem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: The application and all its data is accessible from any device with a web browser by navigating to the server's URL. Data is stored on the server, therefore automatically synced between all devices that use it, but also require internet access.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: On average self-hosted products have a wider feature set than browser extension or on-device feed readers. There is no theoretical limit on the feature set, but typically they keep the infrastructure as simple as possible, to keep setup and maintenance straightforward. This makes them slightly less powerful than hosted alternatives.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;p&gt;Self-hosted products are all open-source and free. The only costs are for the server, and the cheapest VPS plans start at ~2$/month.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hosted&lt;/head&gt;
    &lt;p&gt;Hosted feed readers are managed services. It's required to create an account to get access to them. All of them are paid SAAS products, and most offer a free plan.&lt;/p&gt;
    &lt;p&gt;On average they have the most polished user experience and most comprehensive feature sets, since they're backed by companies that invest in continuous development.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Setup and maintenance: Beyond creating an account, no setup is required. The same for maintenance. Service providers make sure the products work as intended, and deal with everything required to keep the service running, including infrastructure setup, monitoring, backups, etc.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Data control and storage: Since data is stored on the servers and databases of the company running the product, users don't have direct control. But through laws like GDPR users have the right to export all their data, or request deletion. Storage is essentially unlimited, though most products have limits to avoid abuse. These limits are mentioned on their pricing page.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Feed fetching: Similar to self-hosted products, feeds are fetched on the server, which runs continuously and ensures that even high-frequency feeds don't pose a problem. The fetching interval is usually dynamic, based on the popularity of a feed and the publishing frequency.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Availability: Hosted products are primarily available as web application, and some offer native apps as well. Since data is stored on the server, it's natively synced between devices. Some hosted feed readers also offer offline support.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Functionality: Hosted feed readers serve many customers at the same time, which makes it necessary to have more complex infrastructure setups. This also enables features that other types of products cannot do, like email receiving, recommendations, or historic feed contents. Consequently hosted options are generally more powerful than other categories, though the specific feature set depends on the product.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Products&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (one-time)&lt;/cell&gt;
        &lt;cell role="head"&gt;Paid (SAAS)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All hosted products are SAAS products and charge monthly. &lt;code&gt;Folo&lt;/code&gt; is an outlier, they currently don't have any paid features, but the terms of service indicate that this will come in the future. At that point it will join the others in the &lt;code&gt;Paid (SAAS)&lt;/code&gt; category.&lt;/p&gt;
    &lt;head rend="h2"&gt;App support and offline access for products that don't offer it natively&lt;/head&gt;
    &lt;p&gt;Many of the self-hosted or hosted products don't provide apps or offline access by themselves. However, with the right setup, it's still possible to access the articles offline.&lt;/p&gt;
    &lt;p&gt;Most of these products provide APIs. This can be a proprietary API, or an API based on the old Google Reader API.&lt;/p&gt;
    &lt;p&gt;Mobile apps, for example ReadKit or Fiery Feeds, can not only subscribe to feeds, but also connect to other products through their APIs. They download contents, store them locally, and sync changes back to the connected products.&lt;/p&gt;
    &lt;p&gt;These apps make it possible to use a native mobile app for products that don't provide an app themselves, and get offline access at the same time.&lt;/p&gt;
    &lt;p&gt;FreshRSS, for example, has a list of native apps that support it.&lt;/p&gt;
    &lt;head rend="h2"&gt;A note on newsletters&lt;/head&gt;
    &lt;p&gt;Newsletters are a growing mechanism for delivering (blog) content. Some, but not all, hosted feed readers support newsletters out of the box. But on-device products can't do that at all because of their infrastructure limitations.&lt;/p&gt;
    &lt;p&gt;Even if a feed reader doesn't support newsletters by itself, it's still possible to get newsletters into the feed reader, through services that convert newsletters into RSS feeds.&lt;/p&gt;
    &lt;p&gt;These services generate an email address and give you a corresponding feed URL. Emails to the generated address are added to the feed as new entry. So after signing up to the newsletter and adding the feed URL, the newsletter is added to the feed reader even without native email support.&lt;/p&gt;
    &lt;p&gt;Services that do this are Kill the Newsletter and the Lighthouse Newsletter to RSS tool. Both are free.&lt;/p&gt;
    &lt;head rend="h2"&gt;Products&lt;/head&gt;
    &lt;p&gt;The descriptions highlight the defining and unique aspects of the products, for the full feature list please go to their respective websites. Where available I used the screenshots for their websites, to represent the products as best as possible (test account screenshots wouldn't be as good).&lt;/p&gt;
    &lt;head rend="h3"&gt;NetNewsWire (on-device, free)&lt;/head&gt;
    &lt;p&gt;NetNewsWire is a free, on-device feed reader for Mac and iOS. By default it stores data on the device itself, but can sync via iCloud and using other products as storage. And it provides a Safari extension for easy feed-adding. Notably, it supports AppleScript, which makes it possible to automate certain workflows.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fiery Feeds (on device, paid SAAS)&lt;/head&gt;
    &lt;p&gt;Fiery feeds is a Mac and iOS app. The download is free, and to get the premium features it costs ~15$/year (varies by region). By default it stores data on the device itself, but can sync via iCloud and using the API of other products (e.g. FreshRSS). The main distinguishing features are the customization options of the app, like custom themes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reeder (on-device, SAAS)&lt;/head&gt;
    &lt;p&gt;The current version of Reeder is a Mac and iOS app. The download is free, and to get the premium features it costs ~10$/year. The previous version of Reeder, now called Reeder Classic, is still available as one-time purchase. It also stores data on-device, and can sync via iCloud.&lt;/p&gt;
    &lt;p&gt;The main distinguishing feature is the unified timeline, which includes RSS, podcasts, social media, and more.&lt;/p&gt;
    &lt;head rend="h3"&gt;FreshRSS (self-hosted, free)&lt;/head&gt;
    &lt;p&gt;FreshRSS is a self-hosted feed reader and once set up, it's available as a web app. It is quite powerful, and in addition to normal feed subscriptions offers WebSub as well. It's possible to customize it with themes and extensions, and it's translated into more than 15 languages.&lt;/p&gt;
    &lt;p&gt;Together with Miniflux they're the most-recommended self-hosted options.&lt;/p&gt;
    &lt;head rend="h3"&gt;Miniflux&lt;/head&gt;
    &lt;p&gt;Miniflux is a self-hosted feed reader as well, and available as web app after setup. It's focus is on staying simple and fast instead of adding fancy features. It also offers a hosted version, which is 15$/year and has a 15 day trial period.&lt;/p&gt;
    &lt;p&gt;Together with FreshRSS they're the most-recommended self-hosted options.&lt;/p&gt;
    &lt;head rend="h3"&gt;Folo (hosted, free)&lt;/head&gt;
    &lt;p&gt;Folo is a relatively new product developed by the creators of RSS Hub. It's a free hosted feed reader, with apps available for every major platform. Their terms of service suggest that at some point they will charge a monthly fee for some premium features, but at the time of writing there were no paid features.&lt;/p&gt;
    &lt;p&gt;Folo is also open-source and therefore theoretically self-hostable, but I haven't found documentation for it.&lt;/p&gt;
    &lt;p&gt;It has a huge feature set, including newsletter support, transforming websites into feeds, AI summaries, and much more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feedly (hosted, SAAS)&lt;/head&gt;
    &lt;p&gt;Feedly is the most widely-known product and has the most users of all feed readers. It has a comprehensive features set, and offers a free plan as well. For the past couple of years, it seems that Feedly has focused more on AI features and enterprise customers, but their core product remains solid.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inoreader&lt;/head&gt;
    &lt;p&gt;Inoreader is the second most widely-known product, after Feedly. It too offers a free plan. Their feature list is impressive, with social media support for subscriptions, automation and AI features, public API and integrations. And of course much more.&lt;/p&gt;
    &lt;p&gt;What stands out on Reddit are complaints about price hikes. Though Inoreader probably has tens or hundreds of thousands of users who don't have issues and think the product is great.&lt;/p&gt;
    &lt;head rend="h3"&gt;Readwise Reader&lt;/head&gt;
    &lt;p&gt;Readwise Reader is a relatively new product, from the creators of the original Readwise. It's a great feed reader, though were it really shines is the reading experience. They also have reading views for PDFs, eBooks, and more.&lt;/p&gt;
    &lt;p&gt;The website mentions that it's in beta, but it has been in beta for years now and at this point is a stable product, and has been for awhile.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tiny Tiny RSS&lt;/head&gt;
    &lt;p&gt;Tiny Tiny RSS deserves an honorable mention in this list. It was, and still is, used by many, and has been for a long time, and was often recommended on the RSS subreddit.&lt;/p&gt;
    &lt;p&gt;On October 3rd the maintainer announced that he's going to stop working on it, and will remove all infrastructure on November 1st. Forks of the project with other maintainers may pop up, but at the moment it's too soon to tell what the future of Tiny Tiny RSS will be.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lighthouse&lt;/head&gt;
    &lt;p&gt;Lighthouse is also a relatively new product. It's in beta, which refers to the fact that it doesn't yet have all features necessary to fulfill its vision, which goes beyond simple feed reading.&lt;/p&gt;
    &lt;p&gt;The main differentiator is that it focuses on articles over feeds, and has a separate view for curating articles (the inbox). It's focused on finding high-value content.&lt;/p&gt;
    &lt;head rend="h2"&gt;Similar product categories&lt;/head&gt;
    &lt;p&gt;Feed readers are powerful products, which require manual setup to make them work well. Subscribing to feeds is mandatory, adding tags, filtered views, and rules can make them work even better. But for some use-cases, other product categories are simpler and might work better.&lt;/p&gt;
    &lt;head rend="h3"&gt;News aggregators&lt;/head&gt;
    &lt;p&gt;News aggregators automatically collect and curate news from a large variety of sources. They usually provide some customization options, but focus on news and don't allow arbitrary feed subscriptions like feed readers do.&lt;/p&gt;
    &lt;p&gt;They focus on providing an overview of the most relevant news stories.&lt;/p&gt;
    &lt;p&gt;Examples are Kagi News, Ground News, and SmartNews.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reading lists&lt;/head&gt;
    &lt;p&gt;Reading lists, also called read-it-later apps, are for saving and organizing links you found somewhere on the web. Many feed readers can do the same, but reading lists are optimized for that purpose.&lt;/p&gt;
    &lt;p&gt;Examples are Instapaper and Matter. Karakeep is a self-hosted option.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to choose&lt;/head&gt;
    &lt;p&gt;For most people, going with one of the hosted products is the best option. They're generally the most polished and most powerful products, owing to the fact that they have companies behind them with full-time engineers. They generally also offer a free plan, so if you stay within the limits of those, they will even be free to use.&lt;/p&gt;
    &lt;p&gt;For more specific requirements, products of the other categories can work better. They have different characteristics, and can work better in some situations. For example, if you want total control over your data, self-hosted options are the way to go.&lt;/p&gt;
    &lt;p&gt;It's probably easiest to first decide on the category, and then check out multiple products, or maybe even try them out. Virtually all products support OPML import and export, so moving feed subscriptions from one product to the next is almost zero effort.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45517134</guid><pubDate>Wed, 08 Oct 2025 15:17:36 +0000</pubDate></item><item><title>GitHub Will Prioritize Migrating to Azure over Feature Development</title><link>https://thenewstack.io/github-will-prioritize-migrating-to-azure-over-feature-development/</link><description>&lt;doc fingerprint="9a4a3619e3a20acb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GitHub Will Prioritize Migrating to Azure Over Feature Development&lt;/head&gt;
    &lt;p&gt;After acquiring GitHub in 2018, Microsoft mostly let the developer platform run autonomously. But in recent months, that’s changed. With GitHub CEO Thomas Dohmke leaving the company this August, and GitHub being folded more deeply into Microsoft’s organizational structure, GitHub lost that independence. Now, according to internal GitHub documents The New Stack has seen, the next step of this deeper integration into the Microsoft structure is moving all of GitHub’s infrastructure to Azure, even at the cost of delaying work on new features.&lt;/p&gt;
    &lt;p&gt;In a message to GitHub’s staff, CTO Vladimir Fedorov notes that GitHub is constrained on capacity in its Virginia data center. “It’s existential for us to keep up with the demands of AI and Copilot, which are changing how people use GitHub,” he writes.&lt;/p&gt;
    &lt;p&gt;The plan, he writes, is for GitHub to completely move out of its own data centers in 24 months. “This means we have 18 months to execute (with a 6 month buffer),” Fedorov’s memo says. He acknowledges that since any migration of this scope will have to run in parallel on both the new and old infrastructure for at least six months, the team realistically needs to get this work done in the next 12 months.&lt;/p&gt;
    &lt;p&gt;To do so, he is asking GitHub’s teams to focus on moving to Azure over virtually everything else. “We will be asking teams to delay feature work to focus on moving GitHub. We have a small opportunity window where we can delay feature work to focus, and we need to make that window as short as possible,” writes Fedorov.&lt;/p&gt;
    &lt;p&gt;While GitHub had previously started work on migrating parts of its service to Azure, our understanding is that these migrations have been halting and sometimes failed. There are some projects, like its data residency initiative (internally referred to as Project Proxima) that will allow GitHub’s enterprise users to store all of their code in Europe, that already solely use Azure’s local cloud regions.&lt;/p&gt;
    &lt;p&gt;“We have to do this,” Fedorov writes. “It’s existential for GitHub to have the ability to scale to meet the demands of AI and Copilot, and Azure is our path forward. We have been incrementally using more Azure capacity in places like Actions, search, edge sites and Proxima, but the time has come to go all-in on this move and finish it.”&lt;/p&gt;
    &lt;p&gt;GitHub has recently seen more outages, in part because its central data center in Virginia is indeed resource-constrained and running into scaling issues. AI agents are part of the problem here. But it’s our understanding that some GitHub employees are concerned about this migration because GitHub’s MySQL clusters, which form the backbone of the service and run on bare metal servers, won’t easily make the move to Azure and lead to even more outages going forward.&lt;/p&gt;
    &lt;p&gt;In a statement, a GitHub spokesperson confirmed our reporting and told us that “GitHub is migrating to Azure over the next 24 months because we believe it’s the right move for our community and our teams. We need to scale faster to meet the explosive growth in developer activity and AI-powered workflows, and our current infrastructure is hitting its limits. We’re prioritizing this work now because it unlocks everything else. For us, availability is job #1, and this migration ensures GitHub remains the fast, reliable platform developers depend on while positioning us to build more, ship more, and scale without limits. This is about ensuring GitHub can grow with its community, at the speed and scale the future demands.”&lt;/p&gt;
    &lt;p&gt;For some open source developers, having GitHub linked even closer to Microsoft and Azure may also be a problem; though for the most part, some of the recent outages and rate limits developers have been facing have been the bigger issue for the service. Microsoft has long been a good steward of GitHub’s fortunes, but in the end, no good service can escape the internal politics of a giant machine like Microsoft, where executives will always want to increase the size of their fiefdoms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45517173</guid><pubDate>Wed, 08 Oct 2025 15:21:12 +0000</pubDate></item></channel></rss>