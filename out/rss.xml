<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 12 Oct 2025 12:16:48 +0000</lastBuildDate><item><title>Microsoft only lets you opt out of AI photo scanning 3x a year</title><link>https://hardware.slashdot.org/story/25/10/11/0238213/microsofts-onedrive-begins-testing-face-recognizing-ai-for-photos-for-some-preview-users</link><description>&lt;doc fingerprint="27d138be7ae4dabe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft's OneDrive Begins Testing Face-Recognizing AI for Photos (for Some Preview Users) (microsoft.com) 55&lt;/head&gt;
    &lt;p&gt;And...&lt;/p&gt;
    &lt;p&gt;"You can only turn off this setting 3 times a year."&lt;/p&gt;
    &lt;p&gt;If I moved the slidebar for that setting to the left (for "No"), it moved back to the right, and said "Something went wrong while updating this setting." (Apparently it's not one of those three times of the year.)&lt;/p&gt;
    &lt;p&gt;The feature is already rolling out to a limited number of users in a preview, a Microsoft publicist confirmed to Slashdot. (For the record, I don't remember signing up for this face-recognizing "preview".) But there's a link at the bottom of the screen for a "Microsoft Privacy Statement" that leads to a Microsoft support page, which says instead that "This feature is coming soon and is yet to be released." And in the next sentence it's been saying "Stay tuned for more updates" for almost two years...&lt;/p&gt;
    &lt;p&gt;A Microsoft publicist agreed to answer Slashdot's questions...&lt;/p&gt;
    &lt;p&gt;Slashdot: What's the reason OneDrive tells users this setting can only be turned off 3 times a year? (And are those any three times — or does that mean three specific days, like Christmas, New Year's Day, etc.)&lt;/p&gt;
    &lt;p&gt;[Microsoft's publicist chose not to answer this question.]&lt;/p&gt;
    &lt;p&gt;Slashdot: If I move the slidebar to the left (for "No"), it moves back to the right, and says "Something went wrong while updating this setting." So is it correct to say that there's no way for users to select "No" now?&lt;/p&gt;
    &lt;p&gt;Microsoft: We haven't heard about the experience you are having with toggling, but our Microsoft contacts would like to investigate why this is happening for you. Can you share what type of device you are using, so we can put you in touch with the right team?&lt;/p&gt;
    &lt;p&gt;Slashdot: Is this feature really still "coming soon"? Can you give me more specific details on when "soon" will be?&lt;/p&gt;
    &lt;p&gt;Microsoft: This feature is currently rolling out to limited users in a preview so we can learn and improve. We have nothing more to share at this time.&lt;/p&gt;
    &lt;p&gt;Slashdot: I want to confirm something about how this feature is "yet to be released." Does this mean that currently OneDrive is not (and has never) used AI to "recognize" faces in photos?&lt;/p&gt;
    &lt;p&gt;Microsoft: Privacy is built into all Microsoft OneDrive experiences. Microsoft OneDrive services adhere to the Microsoft Privacy Statement and follow Microsoft's compliance with General Data Protection Regulation and the Microsoft EU Data Boundary.&lt;/p&gt;
    &lt;p&gt;Slashdot: Some privacy advocates prefer "opt-in" features, but it looks like here OneDrive is planning a (limited) opt-out feature. What is the reasoning for going with opt-out rather than opt-in?&lt;/p&gt;
    &lt;p&gt;Microsoft: Microsoft OneDrive inherits privacy features and settings from Microsoft 365 and SharePoint, where applicable.&lt;/p&gt;
    &lt;p&gt;Slashdot also spoke to EFF security/privacy activist Thorin Klosowski, who expressed concerns. "Any feature related to privacy really should be opt-in and companies should provide clear documentation so its users can understand the risks and benefits to make that choice for themselves."&lt;/p&gt;
    &lt;p&gt;Microsoft's "three times a year" policy also seemed limiting to Klosowski. "People should also be able to change those settings at-will whenever possible because we all encounter circumstances were we need to re-evaluate and possibly change our privacy settings."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45551504</guid><pubDate>Sat, 11 Oct 2025 18:36:51 +0000</pubDate></item><item><title>A Guide for WireGuard VPN Setup with Pi-Hole Adblock and Unbound DNS</title><link>https://psyonik.tech/posts/a-guide-for-wireguard-vpn-setup-with-pi-hole-adblock-and-unbound-dns/</link><description>&lt;doc fingerprint="b678e959f90921c1"&gt;
  &lt;main&gt;&lt;p&gt;I’ve used Mullvad as my VPN provider for a few years. Their service is good, they provide keys for 5 devices, rely on the Wireguard protocol, and offer alternative configurations as well. Despite that, my needs have changed and I wanted to be able to have granular control over DNS queries and the ability to connect my various devices on my network through simple addresses, such as http://emby.home.server. Enter Wireguard, Pi-Hole and Unbound.&lt;/p&gt;&lt;p&gt;Of course, there are many tools that might achieve the same results with a lot less work, such as Tailscale. I would reach devices on the network easily, one of the nodes could be set to act as an exit node and you could also apply network-wide ad-block with Next DNS (I believe that is, at the time of writing, the solution they offer). Outside of Tailscale there are other fully open source alternatives, such as Headscale, Nebula and others. However, I chose to set up my own Wireguard network, as it gave me the opportunity to learn a lot and helped me better understand everything involved in the process.&lt;/p&gt;&lt;p&gt;For devices, I have the following configuration, and the below IPs will be used throughout the article. They represent a starting point, but should be sufficient for anyone to build their network according to their needs.&lt;/p&gt;&lt;code&gt;10.10.10.1&lt;/code&gt;) - running on a VPS (Virtual Private Server) from a provider that meets my requirements (close physical proximity, high bandwidth and at least 1 Gbps transfer speeds)&lt;code&gt;10.10.10.10&lt;/code&gt;) - running on my home network and providing various service (media server, personal cloud, torrent client, etc.)&lt;code&gt;10.10.10.11&lt;/code&gt;) - running on my home network&lt;code&gt;10.10.10.12&lt;/code&gt;) - running on my home network, but might connect through outside networks (i.e.: coffee shops, libraries, airports, etc.)&lt;code&gt;10.10.10.100&lt;/code&gt;) - running on my home network, mobile data, mobile data roaming or public networks (coffee shops, libraries, airports, etc.)&lt;code&gt;10.10.10.101&lt;/code&gt;) - same as above&lt;p&gt;I will use a hub and spoke network topology where the VPS will act as the in-between for all inter-device communications on the network as well as the exit node for all Internet-facing communications. For example, if my Desktop connects to the Internet it will do so by navigating to the VPS, which will then resolve the query and block any domains that are on the block list. If it wants to connect to the Home Server it will do so without the VPS, since the Home Server and Desktop are on the same local network, connected via a router. If Smartphone 1 wants to connect to the Internet, the query is resolved by the VPS, which will block any domains on the block list. If it wants to connect to the Home Server, the VPS will receive the request and route it to the IP of the Home Server. It’s important to keep in mind that the contents of the request become visible as they leave the Wireguard tunnel between Smartphone 1 and the VPS. On the VPS contents will be encrypted again and then forwarded to the Home Server.&lt;/p&gt;&lt;p&gt;I chose a VPS provider that is physically near my location, has minimum 1 Gbps speed (up/down), unlimited bandwidth and good performance while gaming (i.e.: specifically no packet loss). There are many options; you can check resources like LowEndBox and VPSBenchmarks to get an idea of price ranges, availability and regions. For your own needs, start by defining a set of requirements and see which of these match your needs.&lt;/p&gt;&lt;p&gt;My only recommendation is to initially opt for a cheaper provider, that offers hourly billing and understand the end-to-end-process before committing. Keep in mind that various providers have different configurations and settings, so setups can vary.&lt;/p&gt;&lt;p&gt;If you are completely new to setting up servers using a command line interface (CLI) only, I recommend checking out the Digital Ocean cloud computing series. Most providers have their own guides, so for specific steps check their respective documentation. Same goes for any chosen OS. For this guide I will be using Ubuntu 24.04. For the remainder of the article I will assume you are running an OS with access to a terminal that can send commands to a CLI that can generate SSH keys, establish SSH connections and can run a text editor that you can use to create, edit, update and delete text files. I will be using BASH/Fish with &lt;code&gt;nano&lt;/code&gt; as my text editor.&lt;/p&gt;&lt;p&gt;To start, create an SSH key on your local machine and upload it to your chosen cloud provider. This first step already greatly improves security of the VPS and simplifies connection to it as you don’t need to remember a password.&lt;/p&gt;&lt;code&gt;# Run inside your terminal on your client machine 
# https://www.man7.org/linux/man-pages/man1/ssh-keygen.1.html
$ ssh-keygen -t ed25519
&lt;/code&gt;&lt;p&gt;Follow the on-screen prompts to provide a name, set a passphrase (recommended) and have the private and public files created at the given path in the prompt.&lt;/p&gt;&lt;p&gt;Next, copy the contents of the .pub file, or the file itself to your chosen VPS provider’s server setup page. Complete the initial server setup as per your provider’s steps. Once ready, you will have a public IPv4 address to which you can connect (this is a requirement in this guide, and some providers might only offer IPv6 addresses on their cheapest VPS). A connection can now be established with:&lt;/p&gt;&lt;code&gt;# replace root with the username your provider created by default; could be ubuntu as well
# replace 123.123.123.123 with the IP address of your VPS
$ ssh -i .ssh/article root@123.123.123.123
&lt;/code&gt;&lt;p&gt;You are now connected to the server, as the &lt;code&gt;root&lt;/code&gt; user. If you connected via another user that has &lt;code&gt;sudo&lt;/code&gt; access, then prepend all commands with &lt;code&gt;sudo&lt;/code&gt;. If I write &lt;code&gt;ufw status numbered&lt;/code&gt; use &lt;code&gt;sudo ufw status numbered&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;First, make sure you have &lt;code&gt;unattended-upgrades&lt;/code&gt; installed on your VPS.&lt;/p&gt;&lt;code&gt;$ apt install unattended-upgrades
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
unattended-upgrades is already the newest version (2.9.1+nmu4ubuntu1).
0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.
&lt;/code&gt;&lt;p&gt;Next, prepare the settings you want to apply. Because the application reads through different configuration files in order, I made a copy of /etc/apt/apt.conf.d/50unattended-upgrades and called it /etc/apt/apt.conf.d/51unattended-upgrades. This means that even after an update, while the original file could be overwritten, my settings will persist. The main settings I changed in my copy are found below. Read through the different options and adjust to your preferences.&lt;/p&gt;&lt;code&gt;# Make a copy and give a new name
$ cp /etc/apt/apt.conf.d/50unattended-upgrades /etc/apt/apt.conf.d/51unattended-upgrades
# Edit the file 
$ nano /etc/apt/apt.conf.d/51unattended-upgrades
# File contents below are edited for brevity
Unattended-Upgrade::Allowed-Origins {
        "${distro_id}:${distro_codename}";
        "${distro_id}:${distro_codename}-security";
        // Extended Security Maintenance; doesn't necessarily exist for
        // every release and this system may not have it installed, but if
        // available, the policy for updates is such that unattended-upgrades
        // should also install from here by default.
        "${distro_id}ESMApps:${distro_codename}-apps-security";
        "${distro_id}ESM:${distro_codename}-infra-security";
        "${distro_id}:${distro_codename}-updates";
};

// Remove unused automatically installed kernel-related packages
// (kernel images, kernel headers and kernel version locked tools).
Unattended-Upgrade::Remove-Unused-Kernel-Packages "true";

// Do automatic removal of newly unused dependencies after the upgrade
Unattended-Upgrade::Remove-New-Unused-Dependencies "true";

// Do automatic removal of unused packages after the upgrade
// (equivalent to apt-get autoremove)
Unattended-Upgrade::Remove-Unused-Dependencies "true";

// Automatically reboot *WITHOUT CONFIRMATION* if
// the file /var/run/reboot-required is found after the upgrade
Unattended-Upgrade::Automatic-Reboot "true";

// If automatic reboot is enabled and needed, reboot at the specific
// time instead of immediately
// Default: "now" - change this to whatever works for you!
Unattended-Upgrade::Automatic-Reboot-Time "00:00";
&lt;/code&gt;&lt;p&gt;Next, update the periodic file to provide &lt;code&gt;unattended-upgrades&lt;/code&gt; with the intervals at which it checks for updates and runs the clean command. Create a new file at /etc/apt/apt.conf.d/21periodic (you can name it anything else, as these are parsed in order, so 22periodic, 23…). The below are the values for the purposes of this server:&lt;/p&gt;&lt;code&gt;APT::Periodic::Update-Package-Lists "1";
APT::Periodic::Download-Upgradeable-Packages "1";
APT::Periodic::AutocleanInterval "7";
APT::Periodic::Unattended-Upgrade "1";
&lt;/code&gt;&lt;p&gt;Number 1 means the option is enabled and runs daily. Zero means the option is turned off. Any other number shows the frequency with which the step is completed (e.g.: 7 - the action is performed once a week).&lt;/p&gt;&lt;p&gt;Run &lt;code&gt;unattended-upgrade -d&lt;/code&gt; to quickly debug your current app configuration and confirm the settings you have setup so far.&lt;/p&gt;&lt;p&gt;Next off, I configure the system hostname. The hostname can then be used as part of a fully qualified domain name for the system (e.g.: hostname = pihole, fqdn = pihole.psyonik.com)&lt;/p&gt;&lt;code&gt;# you can check current hostname with
hostname
# set hostname to something else
hostnamectl set-hostname pihole
# edit hosts file to create static associations between IP addresses and hostnames/domains
# which the system prioritizes before DNS for name resolution
nano /etc/hosts
# The contents of the file might be similar, adjust them to match what I have below 
# replacing &amp;lt;pihole&amp;gt; and &amp;lt;psyonik&amp;gt; with your settings
127.0.0.1 localhost
127.0.0.1 pihole
127.0.1.1 pihole.psyonik.com pihole
# replace the below address with your VPS actual IP address
123.123.123.113 pihole.psyonik.com pihole
&lt;/code&gt;&lt;p&gt;I check if the hostname is setup correctly by calling &lt;code&gt;ping pihole.psyonik.com&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;As mentioned previously, I connect to the server by using &lt;code&gt;ssh -i .ssh/article root@123.123.123.123&lt;/code&gt;. The command passes in the path to the SSH key I use, sets the username I want to connect as, and sets the IP address of the server. By default, the SSH service runs on port 22. This is not explicitly stated in the connection command, it is just assumed as being set as such. One of the first things I like to change on a new VPS is the default port to something else. On Ubuntu 24.04 you will have this option in the sshd_config file, but the actual port used will be picked up from the &lt;code&gt;ssh.socket&lt;/code&gt; service. This can be confusing, as the setting is still present in the sshd_config file.&lt;/p&gt;&lt;p&gt;The first step is to make a copy of the configuration file and then edit the following lines in the original file.&lt;/p&gt;&lt;code&gt;$ cp /etc/ssh/sshd_config /etc/ssh/sshd_config.1
# Edit the SSH configuration - disable root login, password auth, enable pubkey login
$ nano /etc/ssh/sshd_config

# Find and edit the below values in the file
AddressFamily inet # SSH Service will only listen to IPv4 addresses
PermitRootLogin no # disable root login
PubkeyAuthentication yes # only allow SSH key-based authentication  
AuthorizedKeysFile .ssh/authorized_keys # file that contains allowed public keys  
PasswordAuthentication no # do not allow password auth  
PermitEmptyPasswords no # do not allow empty passwords 
ChallengeResponseAuthentication no # Specifies if challenge-response auth is allowed
UsePAM no # disable authentication through PAM (Pluggable Authentication Module)
&lt;/code&gt;&lt;p&gt;Next, I create a folder and a configuration file to change the SSH port for the &lt;code&gt;ssh.socket&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;$ mkdir /etc/systemd/system/ssh.socket.d
# Create and add the values into the listen.conf file
$ nano /etc/systemd/system/ssh.socket.d/listen.conf
[Socket]
ListenStream=
ListenStream=12345
# Save and exit file
$ systemctl daemon-reload
# Restart the SSH Socket
$ systemctl restart ssh.socket
# Restart the SSH Service
$ systemctl restart ssh.service
# Check the status of the service after restart; you should see the new port number displayed
$ systemctl status ssh.service
# Output for my configuration
● ssh.service - OpenBSD Secure Shell server
     Loaded: loaded (/usr/lib/systemd/system/ssh.service; disabled; preset: enabled)
     Active: active (running) since Sun 1999-01-01 12:01:04 UTC; 5s ago
TriggeredBy: ● ssh.socket
       Docs: man:sshd(8)
             man:sshd_config(5)
    Process: 12928 ExecStartPre=/usr/sbin/sshd -t (code=exited, status=0/SUCCESS)
   Main PID: 12929 (sshd)
      Tasks: 1 (limit: 4543)
     Memory: 1.2M (peak: 1.3M)
        CPU: 36ms
     CGroup: /system.slice/ssh.service
             └─12929 "sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups"

Jan 01 12:01:04 pihole systemd[1]: Starting ssh.service - OpenBSD Secure Shell server...
Jan 01 12:01:04 pihole sshd[12929]: Server listening on :: port 12345.
Jan 01 12:01:04 pihole systemd[1]: Started ssh.service - OpenBSD Secure Shell server.
&lt;/code&gt;&lt;p&gt;Now, in a new tab, if I try to connect with the previous command, this should not work. I would now also need to specify the port, but if I try to connect as &lt;code&gt;root&lt;/code&gt; this should get denied.&lt;/p&gt;&lt;code&gt;$ ssh -i .ssh/article -p 12345 root@123.123.123.123 # this should return an error
&lt;/code&gt;&lt;p&gt;While keeping my session open, I will create a new user, add the user to &lt;code&gt;sudo&lt;/code&gt;, copy the .ssh/authorized_keys file over to the new users home directory and try to connect with that user. I then complete the remaining configuration using &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;# Create a new user and follow on-screen prompts
$ adduser psyonik
# Add user to sudo
$ usermod -aG sudo psyonik
# Copy authorizedkeys file to the new user folder and change ownership 
$ cp -r .ssh/ /home/psyonik/
$ chown psyonik:psyonik -R /home/psyonik/.ssh
# Ensure ownership is set correctly
$ ls -alh /home/psyonik/.ssh/
total 12K
drwx------ 2 psyonik psyonik 4.0K Jan  1 12:09 .
drwxr-x--- 3 psyonik psyonik 4.0K Jan  1 12:09 ..
-rw------- 1 psyonik psyonik  162 Jan  1 12:09 authorized_keys
&lt;/code&gt;&lt;p&gt;When I connect from my local machine to the VPS using this new username, the connection should be established correctly.&lt;/p&gt;&lt;code&gt;$ ssh -i .ssh/article -p 12345 psyonik@123.123.123.123
&lt;/code&gt;&lt;p&gt;This completes the setup of SSH. Next up, we setup the firewall. If you encounter issues, you can use &lt;code&gt;ssh -vvv&lt;/code&gt; to have verbose logging enabled during the SSH connection. This should highlight any issues such as access denied, incorrect keys or anything else of the sort.&lt;/p&gt;&lt;p&gt;Although the server can now do automatic updates, doesn’t rely on the default port to allow incoming connections and has &lt;code&gt;root&lt;/code&gt; login disabled, we still need to configure the firewall. There are various ways to do this, but I like to use &lt;code&gt;ufw&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Uncomplicated firewall is a great way to setup some basic rules and security on the server. I allow only 2 ports open on IPv4 on the server at this stage, enable logging, and deny all other incoming requests while allowing all outgoing requests.&lt;/p&gt;&lt;code&gt;# the SSH port should be replaced with whatever you wish to use instead
$ ufw allow from 0.0.0.0/0 to any port 12345 comment "SSH"
$ ufw allow from 0.0.0.0/0 to any port 51820 comment "Wireguard"
# block all incoming connections except those going to the SSH/Wireguard ports
$ ufw default deny incoming
# allow 
$ ufw default allow outgoing
# enable logging
$ ufw logging on
# enable ufw and display the rules in a numbered list
$ ufw enable
$ ufw status numbered
Status: active

     To                         Action      From
     --                         ------      ----
[ 1] 12345                      ALLOW IN    Anywhere                   # SSH
[ 2] 51820                      ALLOW IN    Anywhere                   # Wireguard
&lt;/code&gt;&lt;p&gt;Each time you edit a rule you can call &lt;code&gt;ufw reload&lt;/code&gt; to refresh the new rules. You can then check the status of the rules with &lt;code&gt;ufw status numbered&lt;/code&gt;. This also gives you an easy way to delete any unused rules.&lt;/p&gt;&lt;p&gt;Lastly, there is one more change that can be done so that the server will not respond to ping requests.&lt;/p&gt;&lt;code&gt;sudo nano /etc/ufw/before.rules
# Look for the following row and change from ACCEPT to DROP
-A ufw-before-input -p icmp --icmp-type echo-request -j DROP
&lt;/code&gt;&lt;p&gt;This completes a basic server configuration for this machine. Other settings can be changed and if I were to do this often, I would opt for a script file as a minimum, or ideally a cloud-init script. Feel free to explore that once you feel comfortable with this setup.&lt;/p&gt;&lt;p&gt;Next, I’ll setup Wireguard, Pi-Hole and Unbound.&lt;/p&gt;&lt;p&gt;Unlike traditional VPN services, where a central server acts as a point of control and has clients, Wireguard uses the concept of peers. Peers can connect directly to each other, thus allowing for lower latency for connections and removing the single point of failure of a server. However, for my purposes, I configured one of the peers, in Wireguard parlance, to act as a server due to the hub-and-spoke network topology I employ in this setup. As such, the VPS will be referred to as the ‘server’ and all other devices as ‘clients’.&lt;/p&gt;&lt;p&gt;I will expand this article once I have a configuration in which clients that need to be able to connect to each other will do so directly, without a VPS in-between.&lt;/p&gt;&lt;p&gt;Next, to setup each client and the server, I need to create keys for all devices and create configuration files using these keys. The below steps will create public, private and pre-shared keys for all devices.&lt;/p&gt;&lt;code&gt;# Install or make sure Wireguard is installed
$ apt install wireguard

# Create a folder to keep client keys and one for client configurations
# This is not strictly needed, as once clients are added, the keys can be removed from the server
# Except for the preshared keys 
$ mkdir /etc/wireguard/clients
$ mkdir /etc/wireguard/clientconfs

# Create VPS keys - key, pub, psk
$ wg genkey &amp;gt; vps.key # I use .key to show we're dealing with a private key
$ wg pubkey &amp;lt; vps.key &amp;gt; vps.pub # I use .pub to show this is a public key

$ wg genkey &amp;gt; /etc/wireguard/clients/homeserver.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/homeserver.key &amp;gt; /etc/wireguard/clients/homeserver.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/homeserver.psk # I use .psk to show this is a preshared key

$ wg genkey &amp;gt; /etc/wireguard/clients/desktop.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/desktop.key &amp;gt; /etc/wireguard/clients/desktop.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/desktop.psk

$ wg genkey &amp;gt; /etc/wireguard/clients/laptop.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/laptop.key &amp;gt; /etc/wireguard/clients/laptop.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/laptop.psk

$ wg genkey &amp;gt; /etc/wireguard/clients/mobile1.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/mobile1.key &amp;gt; /etc/wireguard/clients/mobile1.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/mobile1.psk

$ wg genkey &amp;gt; /etc/wireguard/clients/mobile2.key
$ wg pubkey &amp;lt; /etc/wireguard/clients/mobile2.key &amp;gt; /etc/wireguard/clients/mobile2.pub
$ wg genpsk &amp;gt; /etc/wireguard/clients/mobile2.psk
&lt;/code&gt;&lt;p&gt;With the keys created, I prepare the VPS configuration file. Each peer that wants to join the VPN needs its own configuration file. In Wireguard, the configuration file is split into two main sections: &lt;code&gt;[Interface]&lt;/code&gt; and &lt;code&gt;[Peer]&lt;/code&gt;. Each configuration file can contain zero, one or multiple peers that I can connect to. If I have one peer in my config, I’m connecting to that one device, if I have more, it means I can directly connect to all those peers.&lt;/p&gt;&lt;p&gt;In this case, the server will have multiple peers while the peers will each have a single peer: the server. The term interface is used because Wireguard creates a new network interface which is named according to the name of the configuration file.&lt;/p&gt;&lt;p&gt;Following this, I create a new configuration file named wg0, which means that once I start Wireguard with this configuration, I will have a new network interface called &lt;code&gt;wg0&lt;/code&gt;. This can be renamed to anything else.&lt;/p&gt;&lt;code&gt;$ nano /etc/wireguard/wg0.conf
# Next I will populate the file with the below information
[Interface]
# Replace the contents with the value you got in vps.key
PrivateKey = QELYVAdCPCaVQfoyE3KrADMZBBdoNotUseAsKeyThisIsMine+eWUA=
Address = 10.10.10.1/24 # the IP address the server will have inside the wg0 network 
ListenPort = 51820 # the port on which the server will listen for incoming connections
SaveConfig = true # https://manpages.debian.org/unstable/wireguard-tools/wg-quick.8.en.html
PreUp = sysctl -w net.ipv4.ip_forward=1 # enables ipv4 forwarding on the VPS
# PreUp = sysctl -w net.ipv6.conf.all.forward=1 if you use ipv6 instead
&lt;/code&gt;&lt;p&gt;At this stage I have the base for my Wireguard network on the server. But because I want to hide traffic from peers behind the server’s IP address I need to apply masquerading to the incoming requests from the clients, a function similar to NAT. To achieve this, we need to get the data coming in on the network interface Wireguard creates out to the network interface of the server and allow the server to alter the source IP.&lt;/p&gt;&lt;p&gt;The first step is finding the default network interface on the server, so I run the below command:&lt;/p&gt;&lt;code&gt;$ ip route list default
# Output could be something like: 
# default via 123.123.123.123 dev eth0 proto dhcp src 123.123.123.123 metric 100
&lt;/code&gt;&lt;p&gt;The important bit here is to see the device (in my sample above it is &lt;code&gt;dev eth0&lt;/code&gt;). This means I want to route requests that I receive out &lt;code&gt;eth0&lt;/code&gt; and apply masquerading, which makes it look like a request received from a smartphone originated from the server.&lt;/p&gt;&lt;p&gt;I will go back to the configuration and continue applying a few more settings to enable masquerading out the &lt;code&gt;eth0&lt;/code&gt; interface (again, this can be different on your machine) and allow traffic that comes in on the &lt;code&gt;wg0&lt;/code&gt; interface (named after the configuration file) to go out on &lt;code&gt;eth0&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;$ nano /etc/wireguard/wg0.conf
# Next I will populate the file with the below information
[Interface]
PrivateKey = QELYVAdCPCaVQfoyE3KrADMZBBdoNotUseAsKeyThisIsMine+eWUA=
Address = 10.10.10.1/24 # the IP address the server will have inside the wg0 network 
ListenPort = 51820 # the port on which the server will listen for incoming connections
SaveConfig = true # https://manpages.debian.org/unstable/wireguard-tools/wg-quick.8.en.html
PreUp = sysctl -w net.ipv4.ip_forward=1 # enables ipv4 forwarding on the VPS
# PreUp = sysctl -w net.ipv6.conf.all.forward=1 if you use ipv6 instead
PostUp= ufw route allow in on wg0 out on eth0
PostUp= iptables -t nat -I POSTROUTING -o eth0 -j MASQUERADE
PreDown= ufw route delete allow in on wg0 out on eth0
PreDown= iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE
&lt;/code&gt;&lt;p&gt;This completes the configuration of the Wireguard interface. After exiting and saving the file, I enable a &lt;code&gt;systemd&lt;/code&gt; service that follows the name of the config file/interface and check to make sure it runs.&lt;/p&gt;&lt;code&gt;# Enable Wireguard service at system startup 
$ systemctl enable wg-quick@wg0.service
# Start Wireguard service
$ systemctl start wg-quick@wg0.service
# Check service status 
$ systemctl status wg-quick@wg0.service
● wg-quick@wg0.service - WireGuard via wg-quick(8) for wg0
     Loaded: loaded (/usr/lib/systemd/system/wg-quick@.service; enabled; preset: enabled)
     Active: active (exited) since Sun 2024-01-01 12:12:42 UTC; 1min 29s ago
       Docs: man:wg-quick(8)
             man:wg(8)
             https://www.wireguard.com/
             https://www.wireguard.com/quickstart/
             https://git.zx2c4.com/wireguard-tools/about/src/man/wg-quick.8
             https://git.zx2c4.com/wireguard-tools/about/src/man/wg.8
    Process: 13987 ExecStart=/usr/bin/wg-quick up wg0 (code=exited, status=0/SUCCESS)
   Main PID: 13987 (code=exited, status=0/SUCCESS)
        CPU: 325ms

Jan 01 12:12:41 pihole wg-quick[13997]: net.ipv4.ip_forward = 1
Jan 01 12:12:41 pihole wg-quick[13987]: [#] ip link add wg0 type wireguard
Jan 01 12:12:41 pihole wg-quick[13987]: [#] wg setconf wg0 /dev/fd/63
Jan 01 12:12:41 pihole wg-quick[13987]: [#] ip -4 address add 10.10.10.1/24 dev wg0
Jan 01 12:12:41 pihole wg-quick[13987]: [#] ip link set mtu 1420 up dev wg0
Jan 01 12:12:41 pihole wg-quick[13987]: [#] ufw route allow in on wg0 out on eth0
Jan 01 12:12:41 pihole wg-quick[14017]: Rule added
Jan 01 12:12:41 pihole wg-quick[14017]: Rule added (v6)
Jan 01 12:12:42 pihole wg-quick[13987]: [#] iptables -t nat -I POSTROUTING -o eth0 -j MASQUERADE
Jan 01 12:12:42 pihole systemd[1]: Finished wg-quick@wg0.service - WireGuard via wg-quick(8) for wg0.
# Check wireguard status directly and verify the key
$ wg
interface: wg0
  public key: qUo/OLUZadoNotUseAsKeyThisIsMineokE6T3pYl0c=
  private key: (hidden)
  listening port: 51820
# Show network interfaces
$ ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever  
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 1f:1f:1f:1f:1f:1f brd ff:ff:ff:ff:ff:ff
    inet 123.123.123.123/32 metric 100 scope global dynamic eth0
       valid_lft 60719sec preferred_lft 60719sec
4: wg0: &amp;lt;POINTOPOINT,NOARP,UP,LOWER_UP&amp;gt; mtu 1420 qdisc noqueue state UNKNOWN group default qlen 1000
    link/none 
    inet 10.10.10.1/24 scope global wg0
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;p&gt;After this, I setup the first peer - remember, I need a configuration file for this. The private key field holds the private key of the peer, the pre-shared key is the value of the pre-shared key I created for the peer, and the public key section in the file holds the public key of the VPS.&lt;/p&gt;&lt;code&gt;# Just naming it dwg0 as for me it will be easy to see that this is the desktop config
# To bring this interface up on my desktop I would use `sudo wg-quick up dwg0`
$ nano dwg0.conf
[Interface]
# Replace with the private key file contents for THIS particular device, in my case desktop
PrivateKey = uIwALjBCXdoNotUseAsKeyThisIsMine1Vb/kI3XGXk=
# The address I had mentioned initially in the topology overview
Address = 10.10.10.11/32
# The VPS will act as a DNS server for this device
DNS = 10.10.10.1

[Peer]
# Replace with the public key file contents for your VPS, in my case vps.pub
PublicKey = qUo/OLUdoNotUseAsKeyThisIsMineWgokE6T3pYl0c=
# Replace with the value you got in your .psk file for THIS particular device, in my case desktop
PresharedKey = pMTDdoNotUseAsKeyThisIsMineeaGilNZRO9OGy3Q=
# https://www.procustodibus.com/blog/2021/03/wireguard-allowedips-calculator/#background
AllowedIPs = 0.0.0.0/0
# Replace 123.123.123.123:51820 with the IP of your VPS and the port you used if different
Endpoint = 123.123.123.123:51820
PersistentKeepalive = 25 # https://wiki.archlinux.org/title/WireGuard#Unable_to_establish_a_persistent_connection_behind_NAT_/_firewall
&lt;/code&gt;&lt;p&gt;At this stage, if I enable the configuration on my desktop, it will not connect to the server. First, I need to add the desktop as a peer of the VPS. I can do that by either bringing down the &lt;code&gt;wg0&lt;/code&gt; interface or by using the &lt;code&gt;wg set&lt;/code&gt; command. I use the latter, which edits the /etc/wireguard/wg0.conf file and adds a new entry under the &lt;code&gt;[Peer]&lt;/code&gt; section.&lt;/p&gt;&lt;code&gt;# The key that I provide to the command is the public key of the desktop 
$ wg set wg0 peer KKdoNotUseAsKeyThisIsMineRFCyZVorUW8m7E/3S0= preshared-key /etc/wireguard/clients/desktop.psk allowed-ips 10.10.10.11
# Check the peer has been added to the interface
$ wg
interface: wg0
  public key: qUo/OLUdoNotUseAsKeyThisIsMineWgokE6T3pYl0c=
  private key: (hidden)
  listening port: 51820

peer: KKdoNotUseAsKeyThisIsMineRFCyZVorUW8m7E/3S0=
  preshared key: (hidden)
  allowed ips: 10.10.10.11/32
&lt;/code&gt;&lt;p&gt;This completes this part of the setup. To use the server to resolve DNS queries and block ads, I setup Pi-Hole and Unbound next.&lt;/p&gt;&lt;p&gt;I installed Pi-Hole using the automated script from their website. I won’t go into the details of piping a script to bash directly, but for those of you who wish to dissect the script or use alternative methods, you can clone the repo and run the script that way, or you can download the installer file and run it. You can also use Docker to deploy it, but I won’t cover those in this guide.&lt;/p&gt;&lt;code&gt;$ curl -sSL https://install.pi-hole.net | bash
&lt;/code&gt;&lt;p&gt;You’ll be greeted by a step by step graphical install. Assuming you’re using a VPS with a dedicated IP, you can confirm that you do have a Static IP and select Continue. You’ll next be asked for the interface, at this stage select &lt;code&gt;wg0&lt;/code&gt; as we want Pi-Hole to run for those connected on the Wireguard interface.&lt;/p&gt;&lt;p&gt;For upstream DNS provider I selected Quad9 for now. Next you’ll be asked to confirm using the suggested block list, select Yes and continue the install. Select Yes for the admin interface and select Yes to install &lt;code&gt;lighttpd&lt;/code&gt; and the required PHP modules (adjust if you don’t wish to use these). It is up to yourself if you wish to enable query logging or not. You can turn this off later on, if you enable it now. Select the privacy level for the query log.&lt;/p&gt;&lt;p&gt;Once the install finishes you should see the IP on which Pi-Hole is running, the interface on which it is running and the password. Make sure to save the password.&lt;/p&gt;&lt;p&gt;With both the server and the client configured and Pi-Hole installed, we’re close to turning on the client. Before that, there is one thing to consider. The client configuration sets the server as the DNS resolver. However, the server currently only allows access to port 12345 (SSH) and port 51820 for Wireguard. Ubuntu has &lt;code&gt;systemd-resolved&lt;/code&gt; using port 53 for DNS resolution, but access to this port is currently restricted, so I’ll add one more rule to UFW on the server, to allow requests to port 53 coming from the Wireguard IP range I defined earlier.&lt;/p&gt;&lt;p&gt;I also want to be able to configure Pi-Hole once I bring the Wireguard interface up, so for that I need to tell UFW to allow connections to port 80 (HTTP) on the Wireguard IP range.&lt;/p&gt;&lt;code&gt;# Allow requests to port 53 from any ip in the 10.10.10.1 - 10.10.10.255 range
$ ufw allow from 10.10.10.0/24 to any port 53
# Allow requests to port 80 from any ip in the 10.10.10.1 - 10.10.10.255 range
$ ufw allow from 10.10.10.0/24 to any port 80
# Reload firewall to apply settings
$ ufw reload
# Check rules are showing up
$ ufw status numbered
&lt;/code&gt;&lt;p&gt;At this point, I can copy the contents of the client config to my desktop, bring up the interface on the desktop (using &lt;code&gt;wg-quick&lt;/code&gt;) and navigate to http://10.10.10.1/admin and change any settings on the Pi-Hole instance running on the VPS server. But before doing that, I want to have Unbound setup on the server too, and then bring up the interface on my desktop and change the DNS resolver on Pi-Hole from Quad9 to Unbound.&lt;/p&gt;&lt;p&gt;Unbound is a recursive, caching DNS resolver. During the Pi-Hole setup, I had picked an upstream DNS resolver, but the issue is, from a privacy standpoint, that the upstream server (in this case Quad9) knows my queries and the queries of everyone using VPS as their DNS, because it’s not VPS resolving the query, it just forwards it to the upstream provider. More information on this, along with the Unbound configuration and setup can be found on the Pi-Hole website.&lt;/p&gt;&lt;p&gt;In this guide I’ll just walk you through the setup of Unbound and the configuration I use. To start off, I’ll install unbound and configure it.&lt;/p&gt;&lt;code&gt;$ apt install unbound
&lt;/code&gt;&lt;p&gt;Chances are that at this point when you check the status of the Unbound service, it shows as failed. The reason is that you have already a resolver running on port 53. To get around this, I setup a new configuration for Unbound in line with the one provided in the Pi-Hole documentation, with a few tweaks to match my VPS and IP ranges.&lt;/p&gt;&lt;p&gt;I create the new configuration file at /etc/unbound/unbound.conf.d/pi-hole.conf, and more details on options and how to change them can be found at the Unbound configuration documentation.&lt;/p&gt;&lt;code&gt;$ nano /etc/unbound/unbound.conf.d/pi-hole.conf
# Add the below to the configuration file; adjust according to your needs and server capabilities  
server:
  num-threads: 4

  # Enable operation information logging; up to 5
  verbosity: 1

  # Listen to queries on all interfaces
  interface: 127.0.0.1
  port: 5353

  # Disable ipv6
  do-ip6: no

  # IP range authorized to send queries to DNS
  access-control: 0.0.0.0/0 refuse
  access-control: 127.0.0.1/32 allow
  access-control: 10.10.10.0/24 allow

  # Hide id.server and hostname.bind
  hide-identity: yes

  # Hide version.server and version.bind
  hide-version: yes

  # Hide addresses on the private network
  private-address: 10.0.0.0/8

  # A total number of unwanted replies is kept track of; when reached cache is cleared to prevent DNS Poisoning
  unwanted-reply-threshold: 10000000

  # Because my server has low traffic/usage I enable prefetch; this adds load but cache elements are prefetched before expiry
  prefetch: yes
  prefetch-key: yes

  # Add minimum cache lifetime in seconds
  cache-min-ttl: 1800
  cache-max-ttl: 14400

  # Secure DNS and use DNSSEC
  harden-glue: yes
  harden-dnssec-stripped: yes
&lt;/code&gt;&lt;p&gt;Next, I restart the service and run a test query to make sure it’s resolving it.&lt;/p&gt;&lt;code&gt;$ systemctl restart unbound.service
$ systemctl status unbound.service
$ dig news.ycombinator.com/ @127.0.0.1 -p 5353
; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.18.28-0ubuntu0.24.04.1-Ubuntu &amp;lt;&amp;lt;&amp;gt;&amp;gt; news.ycombinator.com/ @127.0.0.1 -p 5353
;; global options: +cmd
;; Got answer:
;; -&amp;gt;&amp;gt;HEADER&amp;lt;&amp;lt;- opcode: QUERY, status: NXDOMAIN, id: 31572
;; flags: qr rd ra ad; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;news.ycombinator.com/.		IN	A

;; AUTHORITY SECTION:
.			3563	IN	SOA	a.root-servers.net. nstld.verisign-grs.com. 2024120801 1800 900 604800 86400

;; Query time: 18 msec
;; SERVER: 127.0.0.1#5353(127.0.0.1) (UDP)
;; WHEN: Sun Jan 01 12:14:17 UTC 2024
;; MSG SIZE  rcvd: 125
&lt;/code&gt;&lt;p&gt;The query should now be resolved by Unbound as the &lt;code&gt;SERVER&lt;/code&gt; in the response gives &lt;code&gt;127.0.0.1#5353&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;With this, we’re almost done. I now turn on the Wireguard interface on the desktop as I have the configuration file prepared previously, and I added the client to the VPS’s list of peers.&lt;/p&gt;&lt;p&gt;This will disconnect the current SSH connection, but assuming all previous steps went OK, I am able to reconnect to the VPS server and also access http://10.10.10.1/admin to reach the admin interface for the current Pi-Hole installation.&lt;/p&gt;&lt;code&gt;# On my desktop I will enable the dwg0 service
$ sudo systemctl enable wg-quick@dwg0.service
# Start the dwg0 service which will create the dwg0 interface on my desktop 
# (I could also name this wg0, I just opted to rename it 
# to make it easier when working with multiple configs)
$ sudo systemctl start wg-quick@dwg0.service
# Check the status of the service and make sure it is active and running
$ sudo systemctl status wg-quick@dwg0.service
&lt;/code&gt;&lt;p&gt;Now I can go to the admin interface of the Pi-Hole installation I had setup earlier and change the DNS provider from Quad9 to the instance of Unbound that runs on port 5353 on the VPS.&lt;/p&gt;&lt;p&gt;Once I log into the interface using the password displayed at the end of the Pi-Hole installation, I next go to Settings, select the DNS tab and here uncheck the Quad9 section in the Upstream DNS Servers part of the page. Then, I check the Custom 1 (IPv4) checkbox and enter &lt;code&gt;127.0.0.1#5353&lt;/code&gt;, make sure that Bind only to interface wg0 is selected and then Save the settings.&lt;/p&gt;&lt;p&gt;Now I can visit DNS Leak Test and run an extended test. If everything runs as expected, the only result here should be the IP address of the VPS. If you are getting different servers showing up it means you are leaking DNS queries. There are a multitude of reasons for this, on Ubuntu derived distributions it might be down to Network Manager changing your DNS settings or you having different DNS servers configured which override the Wireguard DNS server.&lt;/p&gt;&lt;p&gt;This can be difficult to debug and it could take some time for you to chase it down, but some tips to get started would be to run &lt;code&gt;resolvectl dns&lt;/code&gt; which will show you the global DNS (if this was configured in /etc/systemd/resolved.conf) or if a global DNS is picked up from somewhere else like /run/systemd/resolve/resolved.conf or Network Manager. For Network Manager, check /etc/NetworkManager/system-connections and look for the name of your connection. The file in there will have a setting in the &lt;code&gt;[ipv4]&lt;/code&gt; section called &lt;code&gt;dns&lt;/code&gt;. This supports multiple DNS settings separated by semicolons.&lt;/p&gt;&lt;p&gt;There are further steps you could take and I recommend the following article by Andrea Corbellini. You can also check to see if it was indeed Pi-Hole that listened and resolved your query by running &lt;code&gt;lsof -i -P -n | grep LISTEN&lt;/code&gt; on the VPS and check to see if pihole is actually listening on port 53 or if there is another resolver that is using the port and responding to your queries.&lt;/p&gt;&lt;p&gt;While getting a configuration file from the VPS to a device with a keyboard is simple, Android and iOS devices aren’t that straightforward. To get a configuration over to a mobile device, I use &lt;code&gt;qrencode&lt;/code&gt;. The application creates a QR Code from a given configuration file.&lt;/p&gt;&lt;code&gt;# Install qrencode
$ apt install qrencode
# Create a QR Code from the previously created dwg0.conf file  
$ qrencode -t ansiutf8 &amp;lt; dwg0.conf
# If the file was created by root and you are now signed in with a non-root account then use 
$ sudo sh -c 'qrencode -t ansiutf8 &amp;lt; dwg0.conf'
&lt;/code&gt;&lt;p&gt;The above command outputs a QR code to the terminal directly, which you can then scan with the Wireguard application on iOS or Android. Don’t forget that you need different configuration files for each new device you wish to add, so don’t try to reuse the same configuration file across multiple devices.&lt;/p&gt;&lt;p&gt;One more goal that I wanted to achieve was that of being able to access my home server resources while I’m not on my home network. The final setup is a bit clunky, but it works for now. I’d be curious about any improvements any of you out there can think of. Feel free to e-mail me with any alternatives or ideas on the vpn email address for this domain.&lt;/p&gt;&lt;p&gt;First, go to the Pi-Hole administrator website, select Local DNS and then DNS Records. Then, add a number of domain/IP addresses to cover your use cases. While the VPN IP address range covers &lt;code&gt;10.10.10.0/24&lt;/code&gt; the local home network IP address range covers &lt;code&gt;192.168.1.0/24&lt;/code&gt;. Assuming these IP ranges, I ended up with the following list, assuming the home server runs on &lt;code&gt;10.10.10.10&lt;/code&gt; within the Wireguard network and &lt;code&gt;192.168.1.10&lt;/code&gt; on the local network:&lt;/p&gt;&lt;code&gt;192.168.1.10&lt;/code&gt;&lt;code&gt;10.10.10.10&lt;/code&gt;&lt;code&gt;192.168.1.10&lt;/code&gt;&lt;code&gt;10.10.10.10&lt;/code&gt;&lt;p&gt;The home server is running a reverse proxy in front of these services, so each call gets resolved by the reverse proxy to their respective services. The VPS is already allowing traffic to port &lt;code&gt;80&lt;/code&gt;, but if you are using HTTPS on any of these services, you would also need a rule to allow traffic to port &lt;code&gt;443&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Lastly, any request coming in on the VPS &lt;code&gt;wg0&lt;/code&gt; interface needs to be forwarded out on the same interface, so a couple more &lt;code&gt;ufw&lt;/code&gt; rules need added to allow any device to easily reach these services whether they sit on the same LAN, or connect via Wireguard.&lt;/p&gt;&lt;code&gt;# Set the following ufw rules on the VPS - this allows forwarding of requests received on wg0 on wg0
$ ufw route allow in on wg0 out on wg0
# Set the rule to allow HTTPS traffic if your services are running with HTTPS
$ ufw allow from 10.10.10.0/24 to any port 443
$ ufw reload
&lt;/code&gt;&lt;p&gt;An easier way to persist the rules is to add them to the &lt;code&gt;PostUp&lt;/code&gt; and &lt;code&gt;PreDown&lt;/code&gt; sections of the Wireguard configuration file. The above settings also mean that if I’m on my home network, I can use emby.home.server and access my home server with its local IP address and when I’m outside my home network, then I can access it via emby.travel.server. It’s not ideal, but this seemed to work most consistently across various OSes, devices and apps.&lt;/p&gt;&lt;p&gt;With this change, the travel or external domain should work, but the local one might not, depending on the device you try to access it from. That is because the &lt;code&gt;AllowedIPs&lt;/code&gt; setting tells Wireguard to route all traffic through the tunel, but the IP 192.168.1.10 is not reachable through the Wireguard network, as it is an internal network IP address. On most Linux distributions, there are default rules in place to route traffic to these internal or default IP ranges. On Linux you can check the default routes with &lt;code&gt;ip route show table main&lt;/code&gt;. On other devices, you need to adjust the &lt;code&gt;AllowedIPs&lt;/code&gt; setting, so that all traffic, except for that particular IP range, is routed through the Wireguard tunnel.&lt;/p&gt;&lt;p&gt;I used this Wireguard AllowedIPs Calculator to update the IP range for my mobile devices. By setting Allowed IPs on the page to &lt;code&gt;0.0.0.0/0&lt;/code&gt; and the Disallowed IPs to &lt;code&gt;192.168.1.0/24&lt;/code&gt;, I would get a list of &lt;code&gt;AllowedIPs&lt;/code&gt; like such:&lt;/p&gt;&lt;code&gt;AllowedIPs = 0.0.0.0/1, 128.0.0.0/2, 192.0.0.0/9, 192.128.0.0/11, 192.160.0.0/13, 192.168.0.0/24, 192.168.2.0/23, 192.168.4.0/22, 192.168.8.0/21, 192.168.16.0/20, 192.168.32.0/19, 192.168.64.0/18, 192.168.128.0/17, 192.169.0.0/16, 192.170.0.0/15, 192.172.0.0/14, 192.176.0.0/12, 192.192.0.0/10, 193.0.0.0/8, 194.0.0.0/7, 196.0.0.0/6, 200.0.0.0/5, 208.0.0.0/4, 224.0.0.0/3
&lt;/code&gt;&lt;p&gt;On mobile clients you can also go directly into the Wireguard app, click on edit and select the checkbox Exclude private IPs then save the changes to the configuration file. With these changes you should now be able to reach applications on your local network or through the tunnel using the above domain names.&lt;/p&gt;&lt;p&gt;If after the setup, you notice that your connection isn’t that great or that you see a significant drop in connection performance, you can test the connection speed between the VPS and a client that is running Linux using &lt;code&gt;iperf3&lt;/code&gt;. This is a good way to check if your speed is slow because of the VPS or some other factors. I would recommend using the Cloudflare Speedtest on a client to get an idea of your current speed without the Wireguard tunnel enabled.&lt;/p&gt;&lt;p&gt;You can then use the &lt;code&gt;speedtest-cli&lt;/code&gt; application on the VPS to test its upload and download connection and then use &lt;code&gt;iperf3&lt;/code&gt; to test the speed of the connection between client and VPS to find any bottlenecks.&lt;/p&gt;&lt;p&gt;On the VPS I install &lt;code&gt;iperf3&lt;/code&gt; and allow connections on port &lt;code&gt;5201&lt;/code&gt; to run the speed test. I then start &lt;code&gt;iperf3&lt;/code&gt; in server mode, which sets it up to listen for incoming requests.&lt;/p&gt;&lt;code&gt;# Install iperf3 if not already installed
$ apt install iperf3
# Allow connections on port 5201 from the Wireguard IP range
$ ufw allow from 10.10.10.0/24 to any port 5201
$ ufw reload
# Start iperf3 in server mode
$ iperf3 --server
# Install iperf3 on client too and then start a test by defining the ip of the VPS on the Wireguard network
# The below command will test the upload speed from client to VPS
$ iperf3 --client 10.10.10.1
Connecting to host 10.10.10.1, port 5201
[  5] local 10.10.10.11 port 56590 connected to 10.10.10.1 port 5201
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  1.21 MBytes  10.1 Mbits/sec    0   81.5 KBytes       
[  5]   1.00-2.00   sec  2.39 MBytes  20.1 Mbits/sec    0    195 KBytes       
[  5]   2.00-3.00   sec  2.94 MBytes  24.7 Mbits/sec    0    331 KBytes       
[  5]   3.00-4.00   sec  3.37 MBytes  28.3 Mbits/sec    0    484 KBytes       
[  5]   4.00-5.00   sec  3.86 MBytes  32.4 Mbits/sec    0    640 KBytes       
[  5]   5.00-6.00   sec  2.45 MBytes  20.5 Mbits/sec    0    786 KBytes       
[  5]   6.00-7.00   sec  3.86 MBytes  32.4 Mbits/sec    0    945 KBytes       
[  5]   7.00-8.00   sec  3.50 MBytes  29.3 Mbits/sec    0   1.08 MBytes       
[  5]   8.00-9.00   sec  3.75 MBytes  31.5 Mbits/sec    0   1.23 MBytes       
[  5]   9.00-10.00  sec  2.50 MBytes  21.0 Mbits/sec    0   1.35 MBytes       
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.00  sec  29.8 MBytes  25.0 Mbits/sec    0             sender
[  5]   0.00-10.61  sec  27.6 MBytes  21.8 Mbits/sec                  receiver

iperf Done.
# The below command will test the download speed from VPS to the client
$ iperf3 --client 10.10.10.1 --reverse
Connecting to host 10.10.10.1, port 5201
Reverse mode, remote host 10.10.10.1 is sending
[  5] local 10.10.10.11 port 34346 connected to 10.10.10.1 port 5201
[ ID] Interval           Transfer     Bitrate
[  5]   0.00-1.00   sec  3.50 MBytes  29.4 Mbits/sec                  
[  5]   1.00-2.00   sec  7.35 MBytes  61.7 Mbits/sec                  
[  5]   2.00-3.00   sec  11.7 MBytes  98.0 Mbits/sec                  
[  5]   3.00-4.00   sec  23.6 MBytes   198 Mbits/sec                  
[  5]   4.00-5.00   sec  39.6 MBytes   332 Mbits/sec                  
[  5]   5.00-6.00   sec  44.4 MBytes   372 Mbits/sec                  
[  5]   6.00-7.00   sec  40.6 MBytes   340 Mbits/sec                  
[  5]   7.00-8.00   sec  40.5 MBytes   340 Mbits/sec                  
[  5]   8.00-9.00   sec  38.4 MBytes   322 Mbits/sec                  
[  5]   9.00-10.00  sec  40.8 MBytes   342 Mbits/sec                  
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-10.05  sec   294 MBytes   245 Mbits/sec    0             sender
[  5]   0.00-10.00  sec   290 MBytes   244 Mbits/sec                  receiver

iperf Done.
&lt;/code&gt;&lt;p&gt;In the above tests I can see that my upload speed to this test VPS is between 10 and 30 Mbps and my download speed is between 30 and 340 Mbps. After completing the test, I can stop the server on the VPS and even close down the port. For more information on tuning the performance of Wireguard, I recommend the detailed article on the Pro Custodibus website.&lt;/p&gt;&lt;p&gt;At this stage, you should have your VPS setup, your first two devices connected and you should also be able to access any remote resources.&lt;/p&gt;&lt;p&gt;Most of this could not be done without the articles below. These were great resources and I would greatly recommend them for further reading of a particular topic:&lt;/p&gt;&lt;p&gt;There are plenty more websites that I read through for quick fixes and I apologise for not recording those consistently, as the information there helped me fix some local issues on my Pop OS desktop.&lt;/p&gt;&lt;p&gt;In any case, I hope this article was useful and helped you setup your own Wireguard VPN server, access resources on your home network and provide network-wide ad-block for all your devices. By using a VPS you can generally cover a variety of devices for around $10 a month. This beats a lot of the providers out there and it does so while offering you full control over blocking lists and resources on the network.&lt;/p&gt;&lt;p&gt;If this guide was useful in any way, please make sure to support the Wireguard project, the Pi-Hole project, Unbound and all the other open source projects that allow us to gain some modicum of control over out digital lives!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45552049</guid><pubDate>Sat, 11 Oct 2025 19:41:27 +0000</pubDate></item><item><title>Ask HN: Abandoned/dead projects you think died before their time and why?</title><link>https://news.ycombinator.com/item?id=45553132</link><description>&lt;doc fingerprint="37a899571fa72dd9"&gt;
  &lt;main&gt;
    &lt;p&gt;It's the closest thing to a Unix successor we ever got, taking the "everything is a file" philosophy to another level and allowing to easily share those files over the network to build distributed systems. Accessing any remote resources is easy and robust on Plan9, meanwhile on other systems we need to install specialized software with bad interoperability for each individual use case.&lt;/p&gt;
    &lt;p&gt;Plan9 also had some innovative UI features, such as mouse chording to edit text, nested window managers, the Plumber to run user-configurable commands on known text patterns system-wide, etc.&lt;/p&gt;
    &lt;p&gt;Its distributed nature should have meant it's perfect for today's world with mobile, desktop, cloud, and IoT devices all connected to each other. Instead, we're stuck with operating systems that were never designed for that.&lt;/p&gt;
    &lt;p&gt;There are still active forks of Plan9 such as 9front, but the original from Bell Labs is dead. The reasons it died are likely:&lt;/p&gt;
    &lt;p&gt;- Legal challenges (Plan9 license, pointless lawsuits, etc.) meant it wssn't adopted by major players in the industry.&lt;/p&gt;
    &lt;p&gt;- Plan9 was a distributed OS during a time when having a local computer became popular and affordable, while using a terminal to access a centrally managed computer fell out of fashion (though the latter sort of came back in a worse fashion with cloud computing).&lt;/p&gt;
    &lt;p&gt;- Bad marketing and posing itself as merely a research OS meant they couldn't capitalize on the .com boom.&lt;/p&gt;
    &lt;p&gt;- AT&amp;amp;T lost its near endless source of telephone revenue. Bell Labs was sold multiple times over the coming years, a lot of the Unix/Plan9 guys went to other companies like Google.&lt;/p&gt;
    &lt;p&gt;- Photon, the graphical interface for QNX. Oriented more towards real time (widgets included gauges) but good enough to support two different web browsers. No delays. This was a real time operating system.&lt;/p&gt;
    &lt;p&gt;- MacOS 8. Not the Linux thing, but Copeland. This was a modernized version of the original MacOS, continuing the tradition of no command line. Not having a command line forces everyone to get their act together about how to install and configure things. Probably would have eased the tradition to mobile. A version was actually shipped to developers, but it had to be covered up to justify the bailout of Next by Apple to get Steve Jobs.&lt;/p&gt;
    &lt;p&gt;- Transaction processing operating systems. The first one was IBM's Customer Information Control System. A transaction processor is a kind of OS where everything is like a CGI program - load program, do something, exit program. Unix and Linux are, underneath, terminal oriented time sharing systems.&lt;/p&gt;
    &lt;p&gt;- IBM MicroChannel. Early minicomputer and microcomputer designers thought "bus", where peripherals can talk to memory and peripherals look like memory to the CPU. Mainframes, though, had "channels", simple processors which connected peripherals to the CPU. Channels could run simple channel programs, and managed device access to memory. IBM tried to introduce that with the PS2, but they made it proprietary and that failed in the marketplace. Today, everything has something like channels, but they're not a unified interface concept that simplifies the OS.&lt;/p&gt;
    &lt;p&gt;- CPUs that really hypervise properly. That is, virtual execution environments look just like real ones. IBM did that in VM, and it worked well because channels are a good abstraction for both a real machine and a VM. Storing into device registers to make things happen is not. x86 has added several layers below the "real machine" layer, and they're all hacks.&lt;/p&gt;
    &lt;p&gt;- The Motorola 680x0 series. Should have been the foundation of the microcomputer era, but it took way too long to get the MMU out the door. The original 68000 came out in 1978, but then Motorola fell behind.&lt;/p&gt;
    &lt;p&gt;- Modula. Modula 2 and 3 were reasonably good languages. Oberon was a flop. DEC was into Modula, but Modula went down with DEC.&lt;/p&gt;
    &lt;p&gt;- XHTML. Have you ever read the parsing rules for HTML 5, where the semantics for bad HTML were formalized? Browsers should just punt at the first error, display an error message, and render the rest of the page in Times Roman. Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;- Word Lens. Look at the world through your phone, and text is translated, standalone, on the device. No Internet connection required. Killed by Google in favor of hosted Google Translate.&lt;/p&gt;
    &lt;p&gt;&amp;gt;- XHTML. [...] Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;XHTML appeals to the intuition that there should be a Strict Right Way To Do Things ... but you can't use that unforgiving framework for web documents that are widely shared.&lt;/p&gt;
    &lt;p&gt;The "real world" has 2 types of file formats:&lt;/p&gt;
    &lt;p&gt;(1) file types where consumers cannot contact/control/punish the authors (open-loop) : HTML, pdf, zip, csv, etc. The common theme is that the data itself is more important that the file format. That's why Adobe Reader will read malformed pdf files written by buggy PDF libraries. And both 7-Zip and Winrar can read malformed zip files with broken headers (because some old buggy Java libraries wrote bad zip files). MS Excel can import malformed csv files. E.g. the Citi bank export to csv wrote a malformed file and it was desirable that MS Excel imported it anyway because the raw data of dollar amounts was more important than the incorrect commas in the csv file -- and -- I have no way of contacting the programmer at Citi to tell them to fix their buggy code that created the bad csv file.&lt;/p&gt;
    &lt;p&gt;(2) file types where the consumer can control the author (closed-loop): programming language source code like .c, .java, etc or business interchange documents like EDI. There's no need to have a "lenient forgiving" gcc/clang compiler to parse ".c" source code because the "consumer-and-author" will be the same person. I.e. the developer sees the compiler stop at a syntax error so they edit and fix it and try to re-compile. For business interchange formats like EDI, a company like Walmart can tell the vendor to fix their broken EDI files.&lt;/p&gt;
    &lt;p&gt;XHTML wants to be in group (2) but web surfers can't control all the authors of .html so that's why lenient parsing of HTML "wins". XHTML would work better in a "closed-loop" environment such as a company writing internal documentation for its employees. E.g. an employee handbook can be written in strict XHTML because both the consumers and authors work at the same company. E.g. can't see the vacation policy because the XHTML syntax is wrong?!? Get on the Slack channel and tell the programmer or content author to fix it.&lt;/p&gt;
    &lt;p&gt;The problem is that group (1) results in a nightmarish race-to-the-bottom. File creators have zero incentive to create spec-compliant files, because there's no penalty for creating corrupted files. In practice this means a large proportion of documents are going to end up corrupt. Does it open in Chrome? Great, ship it! The file format is no longer the specification, but it has now become a wild guess at whatever weird garbage the incumbent is still willing to accept. This makes it virtually impossible to write a new parser, because the file format suddenly has no specification.&lt;/p&gt;
    &lt;p&gt;On the other hand, imagine a world where Chrome would slowly start to phase out its quirks modes. Something like a yellow address bar and a "Chrome cannot guarantee the safety of your data on this website, as the website is malformed" warning message. Turn it into a red bar and a "click to continue" after 10 years, remove it altogether after 20 years. Suddenly it's no longer that one weird customer who is complaining, but everyone - including your manager. Your mistakes are painfully obvious during development, so you have a pretty good incentive to properly follow the spec. You make a mistake on a prominent page and the CTO sees it? Well, guess you'll be adding an XHTML validator to your CI pipeline next week!&lt;/p&gt;
    &lt;p&gt;It is very tempting to write a lenient parser when you are just one small fish in a big ecosystem, but over time it will inevitably lead to the degradation of that very ecosystem. You need some kind of standards body to publish a validating reference parser. And like it or not, Chrome is big enough that it can act as one for HTML.&lt;/p&gt;
    &lt;p&gt;I’d argue a good comparison here is HTTPS. Everyone decided it would be good for sites to move over to serving via HTTPS so browsers incentivised people to move by gating newer features to HTTPS only. They could have easily done the same with XHTML had they wanted.&lt;/p&gt;
    &lt;p&gt;&amp;gt; - XHTML. Have you ever read the parsing rules for HTML 5, where the semantics for bad HTML were formalized? Browsers should just punt at the first error, display an error message, and render the rest of the page in Times Roman. Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;We stop at the first sign of trouble for almost every other format, we do not need lax parsing for HTML. This has caused a multitude of security vulnerabilities and only makes it more difficult for pretty much everybody.&lt;/p&gt;
    &lt;p&gt;The attitude towards HTML5 parsing seemed to grow out of this weird contrarianism that everybody who wanted to do better than whatever Internet Explorer did had their head in the clouds and that the role of a standard was just to write down all the bugs.&lt;/p&gt;
    &lt;p&gt;Just to remind you that &amp;lt;bold&amp;gt; &amp;lt;italic&amp;gt; text &amp;lt;/bold&amp;gt; &amp;lt;/italic&amp;gt; that has been working for ages in every browser ever, is NOT a valid XHTML, and should be rejected by GP's proposal.&lt;/p&gt;
    &lt;p&gt;XHTML allows you to use XML and &amp;lt;bold&amp;gt; &amp;lt;italic&amp;gt; are just XML nodes with no schema. The correct form has been and will always be &amp;lt;b&amp;gt; and &amp;lt;i&amp;gt;. Since the beginning.&lt;/p&gt;
    &lt;p&gt;That caused plenty of incompatibilities in the past. At one point, Internet Explorer would parse that and end up with something that wasn’t even a tree.&lt;/p&gt;
    &lt;p&gt;HTML is not a set of instructions that you follow. It’s a terrible format if you treat it that way.&lt;/p&gt;
    &lt;p&gt;- I think without the move to NeXT, even if Jobs had come back to Apple, they would never have been able to get to the iPhone. iOS was - and still is - a unix-like OS, using unix-like philosophy, and I think that philosophy allowed them to build something game-changing compared to the SOTA in mobile OS technology at the time. So much so, Android follows suit. It doesn't have a command line, and installation is fine, so I'm not sure your line of reasoning holds strongly. One thing I think you might be hinting at though that is a missed trick: macOS today could learn a little from the way iOS and iPadOS is forced to do things and centralise configuration in a single place.&lt;/p&gt;
    &lt;p&gt;- I think transaction processing operating systems have been reinvented today as "serverless". The load/execute/quit cycle you describe is how you build in AWS Lambdas, GCP Cloud Run Functions or Azure Functions.&lt;/p&gt;
    &lt;p&gt;- Most of your other ideas (with an exception, see below), died either because of people trying to grab money rather than build cool tech, and arguably the free market decided to vote with its feet - I do wonder when we might next get a major change in hardware architectures again though, it does feel like we've now got "x86" and "ARM" and that's that for the next generation.&lt;/p&gt;
    &lt;p&gt;- XHTML died because it was too hard for people to get stuff done. The forgiving nature of the HTML specs is a feature, not a bug. We shouldn't expect people to be experts at reading specs to publish on the web, nor should it need special software that gatekeeps the web. It needs to be scrappy, and messy and evolutionary, because it is a technology that serves people - we don't want people to serve the technology.&lt;/p&gt;
    &lt;p&gt;&amp;gt; XHTML died because it was too hard for people to get stuff done.&lt;/p&gt;
    &lt;p&gt;This is not true. The reason it died was because Internet Explorer 6 didn’t support it, and that hung around for about a decade and a half. There was no way for XHTML to succeed given that situation.&lt;/p&gt;
    &lt;p&gt;The syntax errors that cause XHTML to stop parsing also cause JSX to stop parsing. If this kind of thing really were a problem, it would have killed React.&lt;/p&gt;
    &lt;p&gt;People can deal with strict syntax. They can manage it with JSX, they can manage it with JSON, they can manage it with JavaScript, they can manage it with every back-end language like Python, PHP, Ruby, etc. The idea that people see XHTML being parsed strictly and give up has never had any truth to it.&lt;/p&gt;
    &lt;p&gt;On XHTML, I think there was room for both HTML and a proper XHTML that barks on errors. If you're a human typing HTML or using a language where you build your HTML by concatenation like early PHP, sure it makes sense to allow loosey goosey HTML but if you're using any sort of simple DOM builder which should preclude you from the possibility of outputting invalid HTML, strict XHTML makes a lot more sense.&lt;/p&gt;
    &lt;p&gt;Honestly I'm disappointed the promised XHTML5 never materialized along side HTML5. I guess it just lost steam.&lt;/p&gt;
    &lt;p&gt;But a HTML5 parser will obviously parse "strict" HTML5 just fine too, what value is there to special-case the "this was generated by a DOM builder" path client-side?&lt;/p&gt;
    &lt;p&gt;&amp;gt; Honestly I'm disappointed the promised XHTML5 never materialized along side HTML5. I guess it just lost steam.&lt;/p&gt;
    &lt;p&gt;The HTML Standard supports two syntaxes, HTML and XML. All browsers support XML syntax just fine—always have, and probably always will. Serve your file as application/xhtml+xml, and go ham.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Would it kill people to have to close their tags properly&lt;/p&gt;
    &lt;p&gt;It would kill the approachability of the language.&lt;/p&gt;
    &lt;p&gt;One of the joys of learning HTML when it tended to be hand-written was that if you made a mistake, you'd still see something just with distorted output.&lt;/p&gt;
    &lt;p&gt;That was a lot more approachable for a lot of people who were put off "real" programming languages because they were overwhelmed by terrible error messages any time they missed a bracket or misspelled something.&lt;/p&gt;
    &lt;p&gt;If you've learned to program in the last decade or two, you might not even realise just how bad compiler errors tended to be in most languages.&lt;/p&gt;
    &lt;p&gt;The kind of thing where you could miss a bracket on line 47 but end up with a compiler error complaining about something 20 lines away.&lt;/p&gt;
    &lt;p&gt;Rust ( in particular ) got everyone to bring up their game with respect to meaningful compiler errors.&lt;/p&gt;
    &lt;p&gt;But in the days of XHTML? Error messages were arcane, you had to dive in to see what the problem actually was.&lt;/p&gt;
    &lt;p&gt;If you forget a closing quote on an attribute in html, all content until next quote is ignored and not rendered - even if it is the rest of the page. I dont think this is more helpful than an error message. It was just simpler to implement.&lt;/p&gt;
    &lt;p&gt;For reference, observe what happens if you try opening this malformed document in a browser: save it with a .xhtml extension, or serve it with MIME type application/xhtml+xml.&lt;/p&gt;
    &lt;p&gt;Firefox displays naught but the error:&lt;/p&gt;
    &lt;p&gt;XML Parsing Error: mismatched tag. Expected: &amp;lt;/b&amp;gt;. Location: file:///tmp/x.xhtml Line Number 22, Column 3: &amp;lt;/p&amp;gt; --^&lt;/p&gt;
    &lt;p&gt;Chromium displays this banner on top of the document up to the error:&lt;/p&gt;
    &lt;p&gt;This page contains the following errors: error on line 22 at column 5: Opening and ending tag mismatch: b line 19 and p Below is a rendering of the page up to the first error.&lt;/p&gt;
    &lt;p&gt;Thanks for showing these. We can see Firefox matches the same style of accurate but unhelpful error message.&lt;/p&gt;
    &lt;p&gt;Chromium is much more helpful in the error message, directing the user to both line 19 and 22. It also made the user-friendly choice to render up to the error.&lt;/p&gt;
    &lt;p&gt;In the context of XHTML, we should also keep in mind that Chrome post-dates XHTML by almost a decade.&lt;/p&gt;
    &lt;p&gt;If, on the other hand, you have some sorts of XSLT errors, Firefox gives you a reasonably helpful error message in the dev tools, whereas Chromium gives you a blank document and nothing else… unless you ran it in a terminal. I’m still a little surprised that I managed to discover that it was emitting XSLT errors to stdout or stderr (don’t remember which).&lt;/p&gt;
    &lt;p&gt;Really, neither has particularly great handling of errors in anything XML. None of it is better than minimally maintained, a lot of it has simply been unmaintained for a decade or more.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Would it kill people to have to close their tags properly?&lt;/p&gt;
    &lt;p&gt;Probably not, but what would be the benefit of having more pages fail to render? If xhtml had been coupled with some cool features which only worked in xhtml mode, it might have become successful, but on its own it does not provide much value.&lt;/p&gt;
    &lt;p&gt;&amp;gt; but what would be the benefit of having more pages fail to render?&lt;/p&gt;
    &lt;p&gt;I think those benefits are quite similar to having more programs failing to run (due to static and strong typing, other static analysis, and/or elimination of undefined behavior, for instance), or more data failing to be read (due to integrity checks and simply strict parsing): as a user, you get documents closer to valid ones (at least in the rough format), if anything at all, and additionally that discourages developers from shipping a mess. Then parsers (not just those in viewers, but anything that does processing) have a better chance to read and interpret those documents consistently, so even more things work predictably.&lt;/p&gt;
    &lt;p&gt;Sure, authoring tools should help authors avoid mistakes and produce valid content. But the browser is a tool for the consumer of content, and there is no benefit for the user if it fails to to render some existing pages.&lt;/p&gt;
    &lt;p&gt;It is like Windows jumping through hoops to support backwards compatibility even with buggy software. The interest of the customer is that the software runs.&lt;/p&gt;
    &lt;p&gt;if developer accidentally left opening comment at the start of the html.&lt;/p&gt;
    &lt;p&gt;Rhetorical question: Should the browser display page even if it is commented out?&lt;/p&gt;
    &lt;p&gt;There is some bar for what is expected to work.&lt;/p&gt;
    &lt;p&gt;If all browsers would consistently error out on unclosed tags, then it would definitely force developers to close tags, it would force it become common knowledge, second nature.&lt;/p&gt;
    &lt;p&gt;&amp;gt; there is no benefit for the user if it fails to to render some existing pages&lt;/p&gt;
    &lt;p&gt;What if the browser renders it incorrectly? If a corrupt tag combination leads to browser X parsing "&amp;lt;script&amp;gt;" as inline text but browser Y parsing it as a script tag, that could lead to serious security issues!&lt;/p&gt;
    &lt;p&gt;Blindly guessing at the original author's intent whenever you encounter buggy content is a recipe for disaster. Sometimes it is to the user's benefit to just refuse to render it.&lt;/p&gt;
    &lt;p&gt;HTML5 was the answer for the consistency part: where before browsers did different things to recover from "invalid" HTML, HTML5 standardizes it because it doesn't care about valid/invalid as much, it just describes behavior anyways.&lt;/p&gt;
    &lt;p&gt;XHTML is XML. XML-based markup for content can be typeset into PDF, suitable for print media. I invite you to check out the PDFs listed in the intro to my feature matrix comparison page, all being sourced from XHTML:&lt;/p&gt;
    &lt;p&gt;The reason XHTML failed is because the spec required it to be sent with a new MIME type (application/xml+xhtml I believe) which no webserver did out of the box. Everything defaulted to text/html, which all browsers would interpret as HTML, and given the mismatching doctype, would interpret as tag soup (quirks mode/lenient).&lt;/p&gt;
    &lt;p&gt;Meanwhile, local files with the doctype would be treated as XHTML, so people assumed the doctype was all you needed. So everyone who tried to use XHTML didn't realize that it would go back to being read as HTML when they upload it to their webserver/return it from PHP/etc. Then, when something went wrong/worked differently than expected, the author would blame XHTML.&lt;/p&gt;
    &lt;p&gt;Edit: I see that I'm getting downvoted here; if any of this is factually incorrect I would like to be educated please.&lt;/p&gt;
    &lt;p&gt;&amp;gt; The reason XHTML failed is because the spec required it to be sent with a new MIME type (application/xml+xhtml I believe) which no webserver did out of the box. Everything defaulted to text/html, which all browsers would interpret as HTML, and given the mismatching doctype, would interpret as tag soup (quirks mode/lenient).&lt;/p&gt;
    &lt;p&gt;None of that is correct.&lt;/p&gt;
    &lt;p&gt;It was perfectly spec. compliant to label XHTML as text/html. The spec. that covers this is RFC 2854 and it states:&lt;/p&gt;
    &lt;p&gt;&amp;gt; The text/html media type is now defined by W3C Recommendations; the latest published version is [HTML401]. In addition, [XHTML1] defines a profile of use of XHTML which is compatible with HTML 4.01 and which may also be labeled as text/html.&lt;/p&gt;
    &lt;p&gt;There’s no spec. that says you need to parse XHTML served as text/html as HTML not XHTML. As the spec. says, text/html covers both HTML and XHTML. That’s something that browsers did but had no obligation to.&lt;/p&gt;
    &lt;p&gt;The mismatched doctype didn’t trigger quirks mode. Browsers don’t care about that. The prologue could, but XHTML 1.0 Appendix C told you not to use that anyway.&lt;/p&gt;
    &lt;p&gt;Even if it did trigger quirks mode, that makes no difference in terms of tag soup. Tag soup is when you mis-nest tags, for instance &amp;lt;strong&amp;gt;&amp;lt;em&amp;gt;&amp;lt;/strong&amp;gt;&amp;lt;/em&amp;gt;. Quirks mode was predominantly about how it applied CSS layout. There are three different concepts being mixed up here: being parsed as HTML, parsing tag soup, and doctype switching.&lt;/p&gt;
    &lt;p&gt;The problem with serving application/xhtml+xml wasn’t anything to do with web servers. The problem was that Internet Explorer 6 didn’t support it. After Microsoft won the browser wars, they discontinued development and there was a five year gap between Internet Explorer 6 and 7. Combined with long upgrade cycles and operating system requirements, this meant that Internet Explorer 6 had to be supported for almost 15 years globally.&lt;/p&gt;
    &lt;p&gt;Obviously, if you can’t serve XHTML in a way browsers will parse as XML for a decade and a half, this inevitably kills XHTML.&lt;/p&gt;
    &lt;p&gt;Yes, I covered that; everyone assumed that you only needed to specify the doctype, but in practice browsers only accepted it for local files or HTTP responses with Content-Type: application/xml+xhtml. I've edited the comment to make that more explicit.&lt;/p&gt;
    &lt;p&gt;I love this mismatched list of grievances and I find myself agreeing with most of them. XHTML and proper CPU hypervisors in particular.&lt;/p&gt;
    &lt;p&gt;People being too lazy to close the &amp;lt;br /&amp;gt; tag was apparently a gateway drug into absolute mayhem. Modern HTML is a cesspool. I would hate to have to write a parser that's tolerant enough to deal with all the garbage people throw at it. Is that part of the reason why we have so few browsers?&lt;/p&gt;
    &lt;p&gt;&amp;gt; People being too lazy to close the &amp;lt;br /&amp;gt; tag was apparently a gateway drug into absolute mayhem.&lt;/p&gt;
    &lt;p&gt;Your chronology is waaaaaaaaaaaay off.&lt;/p&gt;
    &lt;p&gt;&amp;lt;BR&amp;gt; came years before XML was invented. It was a tag that didn’t permit children, so writing it &amp;lt;BR&amp;gt;&amp;lt;/BR&amp;gt; would have been crazy, and inventing a new syntax like &amp;lt;BR// or &amp;lt;BR/&amp;gt; would have been crazy too. Spelling it &amp;lt;BR&amp;gt; was the obvious and reasonable choice.&lt;/p&gt;
    &lt;p&gt;The &amp;lt;br /&amp;gt; or &amp;lt;br/&amp;gt; spelling was added to HTML after XHTML had already basically lost, as a compatibility measure for porting back to HTML, since those enthusiastic about XHTML had taken to writing it and it was nice having a compatible spelling that did the same in both. (In XHTML you could also write &amp;lt;br&amp;gt;&amp;lt;/br&amp;gt;, but that was incorrect in HTML; and if you wrote &amp;lt;br /&amp;gt; in HTML it was equivalent to &amp;lt;br /=""&amp;gt;, giving you one attribute with name "/" and value "". There were a few growing pains there, such as how &amp;lt;input checked&amp;gt; used to mean &amp;lt;input checked="checked"&amp;gt;—it was actually the attribute name that was being omitted, not the value!—except… oh why am I even writing this, messy messy history stuff, engines doing their own thing blah blah blah, these days it’s &amp;lt;input checked=""&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Really, the whole &amp;lt;… /&amp;gt; thing is more an artefact of an arguably-misguided idea after a failed reform. The absolute mayhem came first, not last.&lt;/p&gt;
    &lt;p&gt;&amp;gt; I would hate to have to write a parser that's tolerant enough to deal with all the garbage people throw at it.&lt;/p&gt;
    &lt;p&gt;The HTML parser is magnificent, by far the best spec for something reasonably-sized that I know of. It’s exhaustively defined in terms of state machines. It’s huge, far larger than one would like it to be because of all this compatibility stuff, but genuinely easy to implement if you have the patience. Seriously, go read it some time, it’s really quite approachable.&lt;/p&gt;
    &lt;p&gt;Adobe Flash / Shockwave. After all these decades, I've yet to see a tool that makes it as easy to make games or multimedia as Flash did. One of many reminders recently (many others in politics) that humanity doesn't just inevitably or linearly move forward in any domain, or even 2 steps forward 1 step back. Some things are just lost to time - maybe rediscovered in a century, maybe never.&lt;/p&gt;
    &lt;p&gt;Godot is pretty awesome. Easy to learn, can do 2D or 3D, and can export to HTML5/webasm that works across all major OSes and browsers including mobile.&lt;/p&gt;
    &lt;p&gt;It’s far from perfect but I’ve been enjoying playing with it even for things that aren’t games and it has come a long way just in the last year or two. I feel like it’s close to (or is currently) having its Blender moment.&lt;/p&gt;
    &lt;p&gt;Even if Adobe had gotten their act together and fixed all security holes, Apple would have still killed it. It was always a threat as a popular design tool. And decades later, with the HTML canvas hype faded, there's still no replacement to what Adobe Flash could do - any designer could create stellar, interactive design that can be embedded into any website...without a monthly subscription.&lt;/p&gt;
    &lt;p&gt;I don't think thats the case. For the longest while flash was faster than js at doing anything vaguely graphic based. The issue for apple was that the CPU in the iphone wasn't fast enough to do flash and anything else. Moreover Adobe didn't get on with jobs when they were talking about custom versions.&lt;/p&gt;
    &lt;p&gt;You have to remember that "apps" were never meant to be a thing on the iphone, it was all about "desktop" like web performance.&lt;/p&gt;
    &lt;p&gt;Flash performance is still better than current web stack's. Probably will always be - you could write non trivial games that would work on 128MB memory machine. Currently single browser tab with simple page can take more than that.&lt;/p&gt;
    &lt;p&gt;Performance was way better than what we have now with modern web stacks, we just have more powerful computers.&lt;/p&gt;
    &lt;p&gt;I agree on security and bugs, but bugs can be fixed. It just shows neglect by Adobe, which was, I think, the real problem. I think that if Adobe seriously wanted to, it could have been a web standard.&lt;/p&gt;
    &lt;p&gt;Macromedia Fireworks was an outstanding piece of software.&lt;/p&gt;
    &lt;p&gt;The 20 most common things you’d do with the tool were there for you in obvious toolbars. It had a lot of advanced features for image editing. It had a scripting language, so you could do bulk editing operations. It supported just about every file extension you could think of.&lt;/p&gt;
    &lt;p&gt;Most useful feature of all was that it’d load instantly. You’d click the icon on the desktop, and there’d be the Fireworks UI before you could finish blinking. Compared to 2025 Adobe apps, where you click the desktop icon and make a coffee while it starts, it’s phenomenal performance.&lt;/p&gt;
    &lt;p&gt;Personal pet peeve, but as someone who still makes gifs, Image Ready. Adobe kind of absorbed Image Ready into Photoshop and it's just never lived up to how easy it was to make simple gifs in Image Ready&lt;/p&gt;
    &lt;p&gt;Yes. I never used flash personally, but I loved those little games people created with them. There was the whole scene of non developers creating little games of all kinds and it just ceased to exist.&lt;/p&gt;
    &lt;p&gt;Kids now create games in Roblox. More constrained, more commercial, more exploitative- but there is still a huge scene of non-developers creating games if you care to look.&lt;/p&gt;
    &lt;p&gt;The Ricochet network. A packet mesh network providing ISDN speeds in the dialup era, wirelessly.&lt;/p&gt;
    &lt;p&gt;They burned through $5B of 1999 dollars, building out a network in 23 cities, and had effectively zero customers. Finally shut down in 2001.&lt;/p&gt;
    &lt;p&gt;All their marketing was focused on "mobile professionals", whoever those were, while ignoring home users who were clamoring for faster internet where other ISPs dragged their feet.&lt;/p&gt;
    &lt;p&gt;Today, 5G femtocells have replicated some of the concept (radically small cell radius to increase geographic frequency reuse), but without the redundancy -- a femtocell that loses its uplink is dead in the water, not serving as a relay node. A Ricochet E-radio that lost its uplink (but still had power) would simply adjust its routing table and continue operating.&lt;/p&gt;
    &lt;p&gt;Edit: you asked why. I first saw it at SELF where Chris DiBona showed it to me and a close friend. It was awesome. Real time translation, integration of various types of messaging, tons of cool capabilities, and it was fully open source. What made it out of Google was a stripped down version of what I was shown, the market rejected it, and it was a sad day. Now, I am left with JIRA, Slack, and email. It sucks.&lt;/p&gt;
    &lt;p&gt;I managed trips with friends and it was a great form factor for ad-hoc discussions with docs and links included. I thought it was the future and in my very early programming days wrote probably the most insecure plugin ever to manage your servers.&lt;/p&gt;
    &lt;p&gt;Google wave was built on an awesome technology layer, and they they totally blew in on the user interface.... deciding to treat it as a set of separate items instead of a single document everyone everywhere all at once could edit.... killed it.&lt;/p&gt;
    &lt;p&gt;It make it seem needlessly complicated, and effectively erased all the positives.&lt;/p&gt;
    &lt;p&gt;I was blown away by the demo but then after I thought about it, it seemed like a nightmare to me. All the problems of slack of having to manually check channels for updates except X 100 (yea, I get that slack wasn't available then. My point is I saw that it seemed impossible to keep up with nested constantly updated hierarchical threads. Keeping up with channels on slack is bad enough so imagine if Wave had succeeded. It'd be even worse.&lt;/p&gt;
    &lt;p&gt;Wave was great for conversation with one or two other people on a specific project, which I'm sure most people here used it for. I can't imagine it scaling well beyond that.&lt;/p&gt;
    &lt;p&gt;Google Wave had awesome tech but if you look at the demo in hindsight you can tell it’s just not a very good product. They tried making an all-in-one kind of product which just doesn’t work.&lt;/p&gt;
    &lt;p&gt;In a sense Wave still exists but was split into multiple products, so I wouldn’t say it’s “dead”. The tech that powered it is still used today in many of Google’s popular products. It turns out that having separate interfaces for separate purposes is just more user friendly than an all-in-one.&lt;/p&gt;
    &lt;p&gt;Even the watered-down version of wave was something I used at my host startup, it was effectively our project management tool. And it was amazing at that.&lt;/p&gt;
    &lt;p&gt;I don't know how it would fare compared to the options available today, but back then, it shutting down was a tremendous loss.&lt;/p&gt;
    &lt;p&gt;It's indeed not a good one. Discord refined instant messaging and bolts other things on top like forums but isn't fundamentally different. Google Wave was (and still is) a completely different paradigm. Everything was natively collaborative: it mixed instant messaging with document edition (like Google Docs or pads) and any widget you could think of (polls, calendars, playing music, drawing, ...) could be added by users through sandboxed Javascript. The current closest I can think of is DeltaChat's webxdc.&lt;/p&gt;
    &lt;p&gt;Google sucked/s at executive function because they completely lack appreciation for proper R&amp;amp;D and long-term investment and also kill things people use and love.&lt;/p&gt;
    &lt;p&gt;Yep. And rather than ask people, focus group, or look at the evidence, they just guess or do whatever they want. Not much leadership or community engagement appears to be involved.&lt;/p&gt;
    &lt;p&gt;Optane persistent memory had a fascinating value proposition: stop converting data structures for database storage and just persist the data directly. No more booting or application launch or data load: just pick up where you left off. Died because it was too expensive, but probably long after it should have.&lt;/p&gt;
    &lt;p&gt;VM's persist memory snapshots (as do Apple's containers, for macOS at least), so there's still room for something like that workflow.&lt;/p&gt;
    &lt;p&gt;Systems are stuck in old ways in how they model storage, so they weren't ready for something that is neither really RAM nor disk. Optane did inspire quite a few research projects for a while though. A few applications emerged in the server space, in particular.&lt;/p&gt;
    &lt;p&gt;The world had already caught up. By the time it was released, flash memory was already nearing it's speed and latency, to the point that the difference want with the cost.&lt;/p&gt;
    &lt;p&gt;&amp;gt;flash memory was already nearing it's speed and latency&lt;/p&gt;
    &lt;p&gt;Kinda, but for small writes it's still nowhere near.&lt;/p&gt;
    &lt;p&gt;Samsung 990 Pro - IOPS 4KQD1 113 MBytes/Sec&lt;/p&gt;
    &lt;p&gt;P4800X optane - IOPS 4KQD1 206 MBytes/Sec&lt;/p&gt;
    &lt;p&gt;And that's a device 5 years newer and on a faster pcie generation.&lt;/p&gt;
    &lt;p&gt;It disappeared because the market that values above attribute is too small and its hard to market because at first glance they look about the same on a lot of metrics as you say&lt;/p&gt;
    &lt;p&gt;How does that work? It loads kernel from drive to ram?&lt;/p&gt;
    &lt;p&gt;Isn't windows fast boot something like that (only slower, depending on ssd)? It semi-hibernates, stores kernel part of memory on disk for faster startup.&lt;/p&gt;
    &lt;p&gt;This one would have behaved more like suspend to RAM. In suspend to RAM, the RAM is kept powered, while everything else is shut down. The recovery would be near instant, since all the execution contexts are preserved on the RAM.&lt;/p&gt;
    &lt;p&gt;Optane was nearly as fast as RAM, but also persistent like a storage device. So you do a suspend to RAM, without the requirement to keep it powered like a RAM.&lt;/p&gt;
    &lt;p&gt;Not only because of price. The 'ecosystem' infrastructure wasn't there, or at least not spread wide enough. The 'mindshare'/thinking of ways how to do, neither. This is more aligned with (live) 'image-based' working environments like early Lisp and Smalltalk systems. Look at where they are now...&lt;/p&gt;
    &lt;p&gt;A few more thoughts about that, since I happen to have some of the last systems who actually had systems level support for that in their firmware, and early low-capacity optanes designed for that sort of use. It's fascinating to play with these, but they are low capacity, and bound to obsolete operating systems.&lt;/p&gt;
    &lt;p&gt;Given enough RAM, you can emulate that with working suspend and resume to/and from RAM.&lt;/p&gt;
    &lt;p&gt;Another avenue are the ever faster and larger SSDs, in practice, with some models it makes almost no difference anymore, since random access times are so fast, and transfer speeds insane. Maybe total and/or daily TBW remains a concern.&lt;/p&gt;
    &lt;p&gt;Google Reader: I will forever be salty about how Google killed something that likely required very little maintenance in the long run. It could have stayed exactly the same for a decade and I wouldn't have cared because I use an RSS reader exactly the same way I do that I did back in 2015.&lt;/p&gt;
    &lt;p&gt;Yes. That was the single worst business decision in Google history, as somebody correctly noted. It burned an enormous amount of goodwill for no gain whatsoever.&lt;/p&gt;
    &lt;p&gt;Killing Google Reader affected a relatively small number of users, but these users disporportionately happened to be founders, CTOs, VPs of engineering, social media luminaries, and people who eventually became founders, CTOs, etc. They had been painfully taught to not trust Google, and, since that time, they didn't. And still don't.&lt;/p&gt;
    &lt;p&gt;Just think of the data mining they could have had there.&lt;/p&gt;
    &lt;p&gt;They had a core set of ultra-connected users who touched key aspects of the entire tech industry. The knowledge graph you could have built out of what those people read and shared…&lt;/p&gt;
    &lt;p&gt;They could have just kept the entire service running with, what, 2 software engineers? Such a waste.&lt;/p&gt;
    &lt;p&gt;This would require the decision-maker to think and act at the scale and in interests of the entire company. Not at the scale of a promo packet for next perf: "saved several millions in operation costs by shutting down a low-impact, unprofitable service."&lt;/p&gt;
    &lt;p&gt;There is some truth in this. I fit into a few of these buckets and I don’t think I could ever recommend their enterprise stuff after having my favourite consumer products pulled.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Google Play Music: I had uploaded thousands of MP3 files there. They killed it. I won't waste my time uploading again.&lt;/p&gt;
    &lt;p&gt;You can argue whether it's as good as GPM or not, but it's false to imply that your uploaded music disappeared when Google moved to YouTube Music. I made the transition, and all of my music moved without a new upload.&lt;/p&gt;
    &lt;p&gt;I still use PICASA it works fine. However, when google severed the gdrive-photo linking it meant my photos didn’t automatically download from google to my PC. This is what killed google for me.&lt;/p&gt;
    &lt;p&gt;Hmm, good to know. But given Google's history, I assumed that it would stop working.&lt;/p&gt;
    &lt;p&gt;I also need to sell my Google Chromecast with Google TV 4K. Brand new, still in its shrink wrap. Bought it last year, to replace a flaky Roku. It was a flaky HDMI cable instead. I trust Roku more than Google for hardware support.&lt;/p&gt;
    &lt;p&gt;In absolutely shocking news, it did stop working and then Google went out of their way to fix it.&lt;/p&gt;
    &lt;p&gt;I genuinely thought all the chromecast audios I owned were useless bricks and was looking around for replacements and then they just started working again from an OTA update. Astounding. I assume someone got fired for taking time away from making search worse to do this.&lt;/p&gt;
    &lt;p&gt;I'm still amused that they killed Google Notebook and then a few years later created Google Keep, an application with basically the same exact feature set.&lt;/p&gt;
    &lt;p&gt;You can say that for a fair few of the services mentioned by GP.&lt;/p&gt;
    &lt;p&gt;Google killed a lot of things to consolidate them into more "integrated" (from their perspective) product offerings. Picasa -&amp;gt; Photos, Hangounts -&amp;gt; Meet, Music -&amp;gt; YT Premium.&lt;/p&gt;
    &lt;p&gt;No idea what NFC Wallet was, other than the Wallet app on my phone that still exists and works?&lt;/p&gt;
    &lt;p&gt;The only one I'm not sure about is Chromecast - a while back my ones had an "update" to start using their newer AI Assistant system for managing it. Still works.&lt;/p&gt;
    &lt;p&gt;That was probably me, when I stopped using Google Search some years ago. :-) Got tired of the ads, the blog spam, and AI-generated content crap floating to the top of their results page.&lt;/p&gt;
    &lt;p&gt;The https://udm14.com/ flavor of Google is quite usable, though, esp with notable operators like inurl:this-or-that. But, all in all, yeah, gimme back vanilla Google search from 2008-2010 or so. Back then it was definitely a tool (I worked in investigative journalism at the time), whereas currently "searching" stands for sitting fingers crossed and hoping for the better. But, oh well. &amp;lt;/rant&amp;gt;&lt;/p&gt;
    &lt;p&gt;That's more what I meant. Sure, lots of people still type stuff into the URL bar that takes them to www.google.com/search. But whatever you want to call that results page now, it's no longer Google Search in anything but name.&lt;/p&gt;
    &lt;p&gt;same can be said if you compare www.google.com search from 2012 and 2022, times are changing… I am not defending google search here, I haven’t used it except by accident in long time now but to say google search is “dying” like you often hear (especially here on HN) is a serious detachment from reality&lt;/p&gt;
    &lt;p&gt;Picasa was awesome, they had face recognition years before almost everything else, in a nice offline package.&lt;/p&gt;
    &lt;p&gt;Unfortunately the last public version has a bug that randomly swaps face tags, so you end up training on the wrong persons faces just enough to throw it all off, and the recognition becomes effectively worthless on thousands of family photos. 8(&lt;/p&gt;
    &lt;p&gt;Digikam is a weak sauce replacement that barely gets the job done.&lt;/p&gt;
    &lt;p&gt;Which particular thing called Hangouts? There were at least two, frankly I’d say more like four.&lt;/p&gt;
    &lt;p&gt;Google and Microsoft are both terrible about reusing names for different things in confusing ways.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Can't keep track of all the Google chat apps.&lt;/p&gt;
    &lt;p&gt;And Hangouts was part of that problem. Remember Google Talk/Chat? That was where things began, and in my family we never wanted Hangouts, Talk/Chat was better.&lt;/p&gt;
    &lt;p&gt;Allo, Chat, Duo, Hangouts, Meet, Messenger, Talk, Voice… I’ve probably forgotten at least two more names, knowing Google. Most of these products have substantial overlap with most of the rest.&lt;/p&gt;
    &lt;p&gt;I use this free and extremely bare bones app made by a friend: https://apps.apple.com/us/app/max-where/id1579123291. It tracks your location constantly, has a basic viewer, and lets you export to CSV. That’s about it but it’s all I need.&lt;/p&gt;
    &lt;p&gt;Check out Dawarich, it has an official iOS app and you can use a number of 3rd party mobile apps to track your data and then upload it to server: either ran on your own hardware (FOSS self-hosted) or to the Dawarich Cloud one: https://dawarich.app&lt;/p&gt;
    &lt;p&gt;I’m still using - free g suite - play music - finance - nfc wallet is just google wallet isn’t it? - chromecast, video and audio-only I guess play music is now YouTube music, and doesn't have uploads, so that can be considered dead, but the others seem alive to me.&lt;/p&gt;
    &lt;p&gt;I used Picasa and loved it, until I realized I want all my photos available from all my devices at all times and so gave in to Google Photos (for access, not backup)&lt;/p&gt;
    &lt;p&gt;I use SyncThing for that purpose. It syncs across my phone, my laptops, and my Synologies. But I don't sync all my photos.&lt;/p&gt;
    &lt;p&gt;I don't like the thought of providing Google thousands of personal photos for their AI training. Which will eventually leak to gov't agencies, fraudsters, and criminals.&lt;/p&gt;
    &lt;p&gt;I used Google Talk than Hangouts, but once they switched to Meet, I gave up on them. By then my family was all using Hangouts, and we never settled on a new service, because one of my siblings didn't want to support any chat services that don't freely give user information to the government, and the rest of us didn't want to use a chat platform that does freely give user information to the government.&lt;/p&gt;
    &lt;p&gt;Am I the only one salty about Google Podcasts? For me that was the straw that broke the camel’s back… I dropped Android, switched to iOS, and slowly phasing out the Google products in my life.&lt;/p&gt;
    &lt;p&gt;From what I can tell (since I am just finding out about this today), they stopped manufacturing the old Chromecast hardware, and at some point, will stop supporting the old devices. The old devices may stop working in the future, for example, because they sunset the servers. Like their thermostats. Who knows?&lt;/p&gt;
    &lt;p&gt;Yahoo pipes. It was so great at creating rss feeds and custom workflows. There are replacements now like Zapier and n8n but loved that. Also google reader which is mentioned multiple times already.&lt;/p&gt;
    &lt;p&gt;Yahoo Pipes was what internet should have been. We're so many decades into computing and that kind of inter-tool linking has only barely been matched by unix pipes.&lt;/p&gt;
    &lt;p&gt;Many companies are working very hard to make that impossible unfortunately. For example you can't get posts from public Facebook groups automatically, although that would be a really good source candidate. They used to allow it, but... not anymore.&lt;/p&gt;
    &lt;p&gt;I loved pipes. I had rss feeds from all the sites where I was sharing content collected up and formatted via pipes into a single rss feed that was pulled into a php blog.&lt;/p&gt;
    &lt;p&gt;Then all those sites I used to post on stopped supporting rss one by one and finally pipes was killed off.&lt;/p&gt;
    &lt;p&gt;For a while I used a python library called riko that did the same thing as pipes without the visual editor. I have to thank it for getting me off php and into python.&lt;/p&gt;
    &lt;p&gt;If anyone with time, money and resources wants to revive the ideas of Yahoo! Pipes then I would suggest using Node-RED[^1] as a good starting point.&lt;/p&gt;
    &lt;p&gt;It has the advantage of being open source, has well defined and stable APIs and a solid backend. Plus 10+ years of constant development with many learnings around how to implement flow based programming visually.&lt;/p&gt;
    &lt;p&gt;I used the Node-RED frontend to create Browser-Red[^2] which is a Node-RED that solely executes in the browser, no server required. It does not support all Node-RED functionality but gives a good feel for using Node-RED and flow based programming.&lt;/p&gt;
    &lt;p&gt;The second project with which I am using Node-RED frontend is Erlang-Red[^3] which is Node-RED with an Erlang backend. Erlang is better suited to flow based programming than NodeJS, hence this attempt to demonstrate that!&lt;/p&gt;
    &lt;p&gt;Node-RED makes slightly different assumptions than Yahoo! Pipes - input ports being the biggest: all nodes in Node-RED have either zero or one input wires, nodes in Yahoo! Pipes had multiple input wires.&lt;/p&gt;
    &lt;p&gt;A good knowledge of jQuery is required but that makes it simpler to get into the frontend code - would be my argument ;) I am happy to answer questions related to Node-RED, email in bio.&lt;/p&gt;
    &lt;p&gt;I can recommend Apache Camel (https://camel.apache.org) for similar data integration pipelines and even agentic workflows. There are even visual editors for Camel today, which IMHO make it extremely user friendly to build any kind of pipeline quickly.&lt;/p&gt;
    &lt;p&gt;I missed Yahoo Pipes a lot so I built something similar recently for myself :) I know there are a few alternatives out there, but had to scratch my own itch.&lt;/p&gt;
    &lt;p&gt;Pascal/Delphi - especially in the educational context.&lt;/p&gt;
    &lt;p&gt;Crazy fast compiler so doesn't frustrate trial &amp;amp; erroring students, decent type system without the wildness of say rust and all the basic programming building blocks you want students to grasp are present without language specific funkiness.&lt;/p&gt;
    &lt;p&gt;Iirc Delphi didn’t have threads, sockets, or OS integration (signals, file watching …). So it wasn’t suited to systems programming ie servers and services. It nailed gui applications, and that was a lot. Maybe freepascal has threads and sockets but imo it was too late.&lt;/p&gt;
    &lt;p&gt;Sandstorm: it seemed quite nice with a lot of possibilities when it launched in 2014, but it didn’t really take off and then it moved to sandstorm.org.&lt;/p&gt;
    &lt;p&gt;The actual problem with Sandstorm wasn't the era in which it was released. It will probably have the same problems even if released today. The problem was its application isolation mechanism - especially the data isolation (I think they were called grains). The mechanism is technically brilliant. But it's a big departure from how apps are developed today. It means that you have to do non-trivial modifications to web applications before they can run on the platform. The platform is better for applications designed to run on it in the start. It should have been marketed as a platform for building web applications, rather than as one for just deploying them.&lt;/p&gt;
    &lt;p&gt;Sandstorm was a great idea, but in my opinion it was targeted wrong. It should have been a platform and marketplace for B2B SaaS, not B2C SaaS. Specifically, all the third-party services which typical web apps use could have been Sandstorm apps, like analytics, logging, email, customer service etc.&lt;/p&gt;
    &lt;p&gt;Midori, Microsoft's capability-based security OS[1]. Rumor has it that it was getting to the point where it was able to run Windows code, so it was killed through internal politics, but who knows! It was the Fuchsia of its time...&lt;/p&gt;
    &lt;p&gt;I've heard someone at Microsoft describe it as a moonshot but also a retention project; IIRC it had a hundred plus engineers on it at one time, including a lot of very senior people.&lt;/p&gt;
    &lt;p&gt;Apparently a bunch of research from Midori made it into .NET so it wasn't all lost, but still...&lt;/p&gt;
    &lt;p&gt;The technical foundation seems interesting, but knowing Microsoft this would have just become yet another bloated mess with it's own new set of problems. And by now it would have equally become filled with spyware and AI "features" users don't want.&lt;/p&gt;
    &lt;p&gt;I really liked Google Circles, a feature of Google+ social media. It allowed you to target content to specific groups of users. You could have a "family" circle or a "work" circle and not have to worry about cross posting something accidentally. It was a small thing but it made it really easy to manage your posts.&lt;/p&gt;
    &lt;p&gt;Vine. It was already pretty big back in 2013 but Twitter had no idea what to do with it. TikTok actually launched just a few months before Vine was shut down and erased from the internet.&lt;/p&gt;
    &lt;p&gt;Whoever took the decision to kill Vine was an absolute moron, even without hindsight. It was square videos, how hard could it have been to shove an ads banner above it and call it a day? Incredible&lt;/p&gt;
    &lt;p&gt;Quartz Composer - Apple's "patch-based" visual programming environment. Drag out a bunch of nodes, wire them together, build a neat little GUI.&lt;/p&gt;
    &lt;p&gt;10+ years ago I'd regularly build all sorts of little utilities with it. It was surprisingly easy to use it to tap into things that are otherwise a lot more work. For instance I used it to monitor the data coming from a USB device. Like 3 nodes and 3 patches to make all of that work. Working little GUI app in seconds.&lt;/p&gt;
    &lt;p&gt;Apple hasn't touched it since 2016, I kind of hope it makes a comeback given Blender and more so Unreal Engine giving people a taste of the node based visual programming life.&lt;/p&gt;
    &lt;p&gt;You can still download it from Apple, and it still technically works but a lot of the most powerful nodes are broken in the newer OS's. I'd love to see the whole thing revitalized.&lt;/p&gt;
    &lt;p&gt;Heroku? I know it's still around, though IDK who uses it, but I miss those days when it was thriving. One language, one deployment platform, one database, a couple plugins to choose from, everything simple and straightforward, no decision fatigue.&lt;/p&gt;
    &lt;p&gt;I often wonder, if AI had come 15 years earlier, would it have been a ton better because there weren't a billion different ways to do things? Would we have ever bothered to come up with all the different tech, if AI was just chugging through features efficiently, with consistent training data etc.?&lt;/p&gt;
    &lt;p&gt;As soon as they put a persistent Salesforce brand banner across the top which did nothing but waste space and put that ugly logo in our face every day, my team started our transition off Heroku pretty much right away.&lt;/p&gt;
    &lt;p&gt;I use the core product for my SaaS apps. Great platform, does what it needs to do. Haven’t felt the need to switch. Sometimes tempted to move to a single VPS with Coolify or Dokku, but not interested in taking on the server admin burden.&lt;/p&gt;
    &lt;p&gt;My company still uses Heroku in production actually. Every time I see the Salesforce logo show up I wince, but we haven't had any issues at all. It continues to make deployment very easy.&lt;/p&gt;
    &lt;p&gt;Didn't they offer free compute? IIRC all free compute on the Internet went away with the advent of cryptocurrencies as it became practical to abuse the compute and translate it directly into money.&lt;/p&gt;
    &lt;p&gt;I think their main failure points were the following:&lt;/p&gt;
    &lt;p&gt;- not lowering prices as time went off. They probably kept a super-huger margin profit, but they’re largely irrelevant today&lt;/p&gt;
    &lt;p&gt;- not building their own datacenters and staying in aws. That would have allowed them to lower prices and gain even more market share. Everyone that has been in amazon/aws likely has seen the internal market rate for ec2 instances and know there’s a HUGE profit margin deriving by building datacenters. Add the recent incredible improvements to compute density (you can easily get 256c/512t and literally terabytes of memory in a 2u box) and you get basically an infinite money glitch.&lt;/p&gt;
    &lt;p&gt;The internet before advertising, artificial intelligence, social media and bots. When folks created startups in their bedrooms or garages. The days when google slogan was “don’t be evil”.&lt;/p&gt;
    &lt;p&gt;I really miss the like 8 year ago push where a lot of major projects were moving to IRC. It's too bad Freenode took the opportunity to jump the shark and killed the momentum.&lt;/p&gt;
    &lt;p&gt;I mean, they're intentionally buried in the name of capital. If you need more than a Google search to find them, of course no one will go to them.&lt;/p&gt;
    &lt;p&gt;I don't like the siloing our information to Discord being a comparison to old internet. We had indexable information in forums that is "lost", not in the literal sense, but because you wouldn't be able to find it without obsessive digging to find it again. Conversations in Discord communities are very surface level and cyclical because it's far less straight forward to keep track of and link to answers from last week let alone two years ago. It is profoundly sad, to be honest.&lt;/p&gt;
    &lt;p&gt;I was a hold out on smartphones for a while and I used to print out k5 articles to read while afk... Just such an amazing collection of people sharing ideas and communal moderation, editing and up voting.&lt;/p&gt;
    &lt;p&gt;I learned about so many wierd and wonderful things from that site.&lt;/p&gt;
    &lt;p&gt;Looking at firefox memory usage, i’m afraid the issue there is not memory safety but rather the average javascript developer being completely and blissfully unaware of and careless about memory memory usage of the software they write&lt;/p&gt;
    &lt;p&gt;I liked del.icio.us, it was online bookmark sharing, but with actual people I knew, and it had genuinely useful category tagging. I guess it was basically replaced with https://old.reddit.com and maybe twitter.&lt;/p&gt;
    &lt;p&gt;Isn’t Pinboard (Who bought delicious) very similar? I also see bookmarks of my friend there, recently switched to Raindrop though as it’s much more maintained.&lt;/p&gt;
    &lt;p&gt;Full vector dpi aware UI, with grid, complex animation, and all other stuff that html5/css didn’t have in 2018 but silverlight had even in 2010 (probable even earlier).&lt;/p&gt;
    &lt;p&gt;MVVM pattern, two-way bindings. Expression Blend (basically figma) that allowed designers create UI that was XAML, had sample data, and could be used be devs as is with maybe some cleanup.&lt;/p&gt;
    &lt;p&gt;Excellent tooling, static analysis, debugging, what have you.&lt;/p&gt;
    &lt;p&gt;Rendered and worked completely the same in any browser (safari, ie, chrome, opera, firefox) on mac and windows&lt;/p&gt;
    &lt;p&gt;If that thing still worked, boy would we be in a better place regarding web apps.&lt;/p&gt;
    &lt;p&gt;Unfortunately, iPhone killed adobe flash and Silverlight as an aftermath. Too slow processor, too much energy consumption.&lt;/p&gt;
    &lt;p&gt;I am happy this one died. It was just another attempt by Microsoft to sidestep open web standards in favor of a proprietary platform. The other notorious example is Flash, and both should be considered malware.&lt;/p&gt;
    &lt;p&gt;Open web standards are great but consider where we could have been if competition drove them a different way? We're still stuck with JavaScript today (wasm still needs it). Layout/styling is caught up now but where would we be if that came sooner?&lt;/p&gt;
    &lt;p&gt;&amp;gt; Open web standards are great but consider where we could have been if competition drove them a different way? We're still stuck with JavaScript today (wasm still needs it). Layout/styling is caught up now but where would we be if that came sooner?&lt;/p&gt;
    &lt;p&gt;Why do you think JavaScript is a problem? And a big enough problem to risk destroying open web standards.&lt;/p&gt;
    &lt;p&gt;TypeScript exists for the same reason things like mypy exists, and no one in their right mind claims that python's openness should be threatened just because static typing is convenient.&lt;/p&gt;
    &lt;p&gt;&amp;gt; A remote code execution vulnerability exists when Microsoft Silverlight decodes strings using a malicious decoder that can return negative offsets that cause Silverlight to replace unsafe object headers with contents provided by an attacker. In a web-browsing scenario, an attacker who successfully exploited this vulnerability could obtain the same permissions as the currently logged-on user. If a user is logged on with administrative user rights, an attacker could take complete control of the affected system. An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights. Users whose accounts are configured to have fewer user rights on the system could be less impacted than users who operate with administrative user rights.&lt;/p&gt;
    &lt;p&gt;Back in the day Microsoft sent someone to our university to demo all of their new and upcoming products. I remember Vista (then named Longhorn) and Silverlight being among them. I also remember people being particularly impressed by the demo they gave of the latter, but everything switfly falling apart when someone queried whether it worked in other browsers. This was at a time when IE was being increasingly challenged by browsers embracing open standards. So there was an element of quiet amusement/frustration in seeing them continue to not get it.&lt;/p&gt;
    &lt;p&gt;I loved silverlight. Before I got a “serious” job, I was a summer intern at a small civil engineering consultancy that had gradually moved into developing custom software that it sold mostly to local town/city/county governments in Arizona (mostly custom mapping applications; for example, imagine Google Maps but you can see an overlay of all the street signs your city owns and click on one to insert a note into some database that a worker needs to go repair it… stuff like that).&lt;/p&gt;
    &lt;p&gt;Lots of their stuff was delivered as Silverlight apps. It turns out that getting office workers to install a blessed plugin from Microsoft and navigate to a web page is much easier than distributing binaries that you have to install and keep up to date. And developing for it was pure pleasure; you got to use C# and Visual Studio, and a GUI interface builder, rather than the Byzantine HTML/JS/CSS ecosystem.&lt;/p&gt;
    &lt;p&gt;I get why it never took off, but in this niche of small-time custom software it was really way nicer than anything else that existed at the time. Web distribution combined with classic desktop GUI development.&lt;/p&gt;
    &lt;p&gt;Geocities ; It was a "put your html here" Free web hosting back when people barely knew what html was. Today you have to be a rocket scientist to find a way to host a free static "simple" page online.&lt;/p&gt;
    &lt;p&gt;tumblr is nothing like a webpage. LLMs were just invented 5 minutes ago and are losing money hand over fist until people are dependent, then will be very expensive to use; and you still have to figure out how to host, where to host, and how much it's going to cost you. So, I have no idea what you're getting at.&lt;/p&gt;
    &lt;p&gt;You could have said Wordpress.com or something. It's not quite a website, but it's close. It's also probably going to be Typepad (i.e. defunct) in a few years and Blogger is probably going to be there quicker than that.&lt;/p&gt;
    &lt;p&gt;ReactOS, the effort to create a free and open source Windows NT reimplementation.&lt;/p&gt;
    &lt;p&gt;It has been in existence in some form or another for nearly 30 years, but did not gain the traction it needed and as of writing it's still not in a usable state on real hardware. It's not abandoned, but progress on it is moving so slow that I doubt we'll ever see it be released in a state that's useful for real users.&lt;/p&gt;
    &lt;p&gt;It's too bad, because a drop in Windows replacement would be nice for all the people losing Windows 10 support right now.&lt;/p&gt;
    &lt;p&gt;On the other hand, I think people underestimate the difficulty involved in the project and compare it unfavorably to Linux, BSD, etc. Unix and its source code was pretty well publicly documented and understood for decades before those projects started, nothing like that ever really existed for Windows.&lt;/p&gt;
    &lt;p&gt;They had no chance. Look how long it tooks for Wine to get where they are. Their project is Wine + a kernel + device drivers compatibility, and a moving target.&lt;/p&gt;
    &lt;p&gt;&amp;gt; I think people underestimate the difficulty involved in the project&lt;/p&gt;
    &lt;p&gt;I don't think people do, it sounds like a nearly impossible struggle, and at the end you get a Windows clone. I can't imagine hating yourself enough to work on it for an extended period of time for no money and putting yourself and your hard work in legal risk. It's a miracle we have Wine and serious luck that we have Proton.&lt;/p&gt;
    &lt;p&gt;People losing Windows 10 support need to move on. There's Linux if you want to be free, and Apple if you still prefer to be guided. You might lose some of your video games. You can still move to Windows 11 if you think that people should serve their operating systems rather than vice versa.&lt;/p&gt;
    &lt;p&gt;&amp;gt; ReactOS, the effort to create a free and open source Windows NT reimplementation.&lt;/p&gt;
    &lt;p&gt;Some projects creep along slowly until something triggers an interest and suddenly they leap ahead.&lt;/p&gt;
    &lt;p&gt;MAME's Tandy 2000 implementation was unusable, until someone found a copy of Windows 1.0 for the Tandy 2000, then the emulation caught up until Windows ran.&lt;/p&gt;
    &lt;p&gt;Maybe ReactOS will get a big influx of activity after Windows 10 support goes offline in a couple days, or even shortly after when you can't turn AI spying off, not even three times a year.&lt;/p&gt;
    &lt;p&gt;Not so long ago there was a leak of windows’ source code, up to xp and 2003 server… the leak was so complete there are videos on YouTube about people building and booting (!!!) windows from there.&lt;/p&gt;
    &lt;p&gt;And yet, no big leap in ReactOS (at least for now).&lt;/p&gt;
    &lt;p&gt;They need to train an LLM with the windows source code and ask it to write an windows clone.&lt;/p&gt;
    &lt;p&gt;Apparently copyright law only applies for humans, generative AI gets away with stealing because there is too much monetary interest involved in looking the other way.&lt;/p&gt;
    &lt;p&gt;I've heard people say this, and believed it myself for a long time, but recently I set up a windows XP VM and was shocked by how bad the quality of life was.&lt;/p&gt;
    &lt;p&gt;I think nostalgia is influencing this opinion quite a bit, and we don't realize the mountain of tiny usability improvements that have been made since XP&lt;/p&gt;
    &lt;p&gt;Wine, Proton and virtualization all got good enough that there's no need for a half-baked binary-compatible Windows reimplementation, and I think that took a lot of the oxygen out of what could have been energy towards ReactOS. It's a cool concept but not really a thing anybody requires.&lt;/p&gt;
    &lt;p&gt;I loved my N900, and my N800 before that, and I would have loved to have seen successors. Ultimately, I ended up switching to Android because I was tired of things only available as apps. Since then, web technologies have gotten better, and it's become much more feasible to use almost exclusively websites.&lt;/p&gt;
    &lt;p&gt;They should have partnered not only with Intel, but with Palm, RIM or whatever other then-giant to rival Android. Those two went their own ways with WebOS and buying QNX, so maybe they could have agreed to form a consortium for an open and interoperable mobile OS&lt;/p&gt;
    &lt;p&gt;MS Sidewinder Force Feedback Pro (1997) and Sidewinder Force Feedback 2 (USB). You can buy similar today, but nowhere near the pricepoint. Also the out of the box support by Windows has vanished, and therefore the incentive of game developers to include force feedback.&lt;/p&gt;
    &lt;p&gt;Microsoft Songsmith is another one that deserved a second life. It let you hum or sing a melody and would auto-generate full backing tracks, guitar, bass, drums, chords, in any style you chose.&lt;/p&gt;
    &lt;p&gt;It looked a bit goofy in the promo videos, but under the hood it was doing real-time chord detection and accompaniment generation. Basically a prototype of what AI music tools like Suno, Udio, or Mubert are doing today, fifteen years too early.&lt;/p&gt;
    &lt;p&gt;If Microsoft had kept iterating on it with modern ML models, it could’ve become the "GarageBand for ideas that start as a hum."&lt;/p&gt;
    &lt;p&gt;It was a series of experiments with new approaches to programming. Kind of reminded me of the research that gave us Smalltalk. It would have been interesting to see where they went with it, but they wound down the project.&lt;/p&gt;
    &lt;p&gt;The Lockheed D-21 drone. Supersonic ramjet without the complexity of scramjet or the cost of turbojet, hamstrung by the need for a manned launch platform (making operations safety-critical… with predictable results) and recovery to get data off it. Twenty or forty years later it would have been paired by a small number of high-cost launcher UAVs and had its cost driven down to disposable, with data recovery over radio comms… but twenty to forty years later there’s nothing like it, and the maturation of satellites means there almost certainly never will be.&lt;/p&gt;
    &lt;p&gt;In 2011, before TypeScript, Next.js or even React, they had seamless server-client code, in a strongly typed functional language with support for features like JSX-like inline HTML, async/await, string interpolation, built-in MongoDB ORM, CSS-in-JS, and many syntax features that were added to ECMAScript since then.&lt;/p&gt;
    &lt;p&gt;I find it wild how this project was 90%+ correct on how we will build web apps 14 years later.&lt;/p&gt;
    &lt;p&gt;Boot2Gecko or whatever the browser as Operating system was called. This was a project that should have focused on providing whatever its current users needed expanding and evolving to do whatever those users wanted it to do better.&lt;/p&gt;
    &lt;p&gt;Instead it went chasing markets, abandoning existing users as it did so, in favour of potential larger pools of users elsewhere. In the end it failed to find a niche going forward while leaving a trail of abandoned niches behind it.&lt;/p&gt;
    &lt;p&gt;For a few short months circa 2016 or 2017, KaiOS was the number one mobile OS in India. This was probably because of all the ultra-cheap KaiOS-powered Reliance Jio phones flooding the Indian market at the time.&lt;/p&gt;
    &lt;p&gt;I noticed the trend when I was working on a major web property for the Aditya Birla conglomerate. My whole team was pleasantly surprised, and we made sure to test everything in Firefox for that project. But everyone switched to Android + Chrome over the next few years, which was a shame.&lt;/p&gt;
    &lt;p&gt;I adored my Firefox Phones. Writing apps was so easy I built myself dozens of little one-offs. Imagine if it had survived to today, its trivial html/css/js apps could be vibe coded on-device and be the ultimate personalized phone.&lt;/p&gt;
    &lt;p&gt;Luckily it wasn't long after Mozilla abandoned it that PWAs were introduced and I could port the apps I cared about.&lt;/p&gt;
    &lt;p&gt;RethinkDB. Technically it still exists (under The Linux Foundation), but (IMO) the original company's widening scope (the Horizon BaaS) that eventually led to its demise killed its momentum.&lt;/p&gt;
    &lt;p&gt;Non Daw. Its breaking up each function of the DAW into its own application gave a better experience in each of those functions, especially when you only needed that aspect, you were not working around everything else that the DAW offers. The integration between the various parts was not all that it could be but I think the idea has some real potential.&lt;/p&gt;
    &lt;p&gt;I've never heard of this software before. Any idea why it's discontinued? There are a bunch of weird messages that point to sort of a hostile take over of the project by forking, but it doesn't say anything about why or how it was discontinued.&lt;/p&gt;
    &lt;p&gt;Thought about Non immediately, but I figured it must have (had) about 2 other users amongst HNers, though. :) Nice to see it mentioned.&lt;/p&gt;
    &lt;p&gt;I used it quite a bit to produce radio shows for my country's public broadcasting. Because Non's line-oriented session format was so easy to parse with classic Unix tools, I wrote a bunch of scripts for it with Awk etc. (E.g. calculating the total length of clips highlighted with brown color in the DAW -- which was stuff meant for editing out; or creating a poor man's "ripple editing" feature by moving loosely-placed clips precisely side by side; or, eventually, converting the sessions to Samplitude EDL format, and, from there, to Pro Tools via AATranslator [1] (because our studio was using PT), etc. Really fun times!)&lt;/p&gt;
    &lt;p&gt;Macromedia Flash. Its scope and security profile was too big. It gave way to HTML’s canvas. But man, the tooling is still no where near as good. Movieclips, my beloved. I loved it all.&lt;/p&gt;
    &lt;p&gt;The iPhone killed Flash, probably because it would've been a way to create apps for it, more probably because it would've been laggy in the 2007 hardware, and people would've considered the iPhone "a piece of junk".&lt;/p&gt;
    &lt;p&gt;Interesting how Flash became the almost universal way to play videos in the browser, in the latter half of the 2000's (damn I'm old...).&lt;/p&gt;
    &lt;p&gt;It's incredible to me that they killed the whole tool instead of making a JS/Canvas port. Even without "full flash websites", there's still need for vectorial animations on the web.&lt;/p&gt;
    &lt;p&gt;As a Linux user, I hated Flash with a passion. It mostly didn't work despite several Linux implementations. About the time they sorted all the bugs out, it went away. Good riddance.&lt;/p&gt;
    &lt;p&gt;Was recently reading about Project Ara, the modular smartphone project by Google/Motorola [1]. Would have liked to see a few more iterations of the idea. Something more customizable than what we have today without having to take the phone apart.&lt;/p&gt;
    &lt;p&gt;BT had this grand vision for basically providing rich multi-media through the phone line, but in ~1998. Think a mix of on-demand cable and "teleconferencing" with TV based internet (ceefax/red button on steriods)&lt;/p&gt;
    &lt;p&gt;It would have been revolutionary and kick started the UK's jump into online rich media.&lt;/p&gt;
    &lt;p&gt;However it wouldnt have got past the regulators as both sky and NTL(now virgin) would have protested loudly.&lt;/p&gt;
    &lt;p&gt;ICQ ; It was the first instant messenger, the technology could have adopted voice (and not get disrupted by Skype) and mobile (and not get disrupted by whatsapp) and group chat (and not get disrupted by slack/discord). But they didn't even try and put up a fight.&lt;/p&gt;
    &lt;p&gt;They got bought by AOL in 98, long before most/all of this innovation happened?&lt;/p&gt;
    &lt;p&gt;Edit: in fact I'd say they were irrelevant before pretty much all of those innovations. By the time AIM or MSN Messenger really became popular, ICQ didn't matter anymore.&lt;/p&gt;
    &lt;p&gt;Skype ; Because my R.I.P. grandma was using it to talk to her relatives overseas just like she would use a phone, but it didn't cost an arm and a leg (unlike phone calls).&lt;/p&gt;
    &lt;p&gt;One of the best P2P software at the time. It was so simple and effective and allowed people to call real phones with Skype credit.&lt;/p&gt;
    &lt;p&gt;A genius product ripped my Microsoft. Have you used Microsoft Teams recently? Bad UI, hard to configure external hardware and good level of incompatibility, missing the good old "Echo / Sound Test Service". At a point I even installed Skype of my old Android but was sucking up too much battery.&lt;/p&gt;
    &lt;p&gt;I tried it twice and the onboarding experience was insurmountable. Never managed to achieve a critical mass of followers or whatever they call it, so things were permanently read-only for me. I'd reply but nobody saw it.&lt;/p&gt;
    &lt;p&gt;It was a fascinating protocol underneath, but the social follow structure seemed to select strongly for folks who already had a following or something.&lt;/p&gt;
    &lt;p&gt;Drama has killed the technological progress in open source, if you ask me.&lt;/p&gt;
    &lt;p&gt;Having seen what goes on in the foss world and what goes on in the large faang-size corporate world, no wonder the corporate world is light-years ahead.&lt;/p&gt;
    &lt;p&gt;It's been a number of years but my understanding was they kind of killed all the momentum it had by removing support for custom operators which broke everyone's code?&lt;/p&gt;
    &lt;p&gt;Yeah, Opa was wildly ahead of its time, I actually just wrote a top level comment about it. Basically Next.js+TypeScript+modern ECMAScript features, but in 2011.&lt;/p&gt;
    &lt;p&gt;A simple UI programming pattern, with a circular, unidirectional data flow. It is very rigid by design, to be side-effect free, functional, unidirectional:&lt;/p&gt;
    &lt;p&gt;It might be too soon to call it abandoned, but I was very intrigued by the Austral [1] language. The spec [2] is worth reading, it has an unusual clarity of thought and originality, and I was hoping that it would find some traction. Unfortunately it seems that the author is no longer actively working on it.&lt;/p&gt;
    &lt;p&gt;I played with Austral about a year ago and really wanted to use it for my projects, but as a hobbyist and mostly inept programmer it lacked the community and ecosystem I require. I found it almost intuitive and the spec does an amazing job of explaining the language. Would love to see it get a foothold.&lt;/p&gt;
    &lt;p&gt;The author got hired by Modular, the AI startup founded by the creators of LLVM and Swift, and is now working on the new language Mojo. He’s been bringing a bunch of ideas from Vale to Mojo&lt;/p&gt;
    &lt;p&gt;Oh nice! I just had an excuse to try mojo via max inference, it was pretty impressive. Basically on par with vllm for some small benchmarks, bit of variance in ttft and tpot. Very cool!&lt;/p&gt;
    &lt;p&gt;CLPM, the Common Lisp Package Manager. The Quicklisp client doesn't do HTTPS, ql-https doesn't do Ultralisp, and OCICL (which I'm currently using) doesn't do system-wide packages. CLPM is a great project, but it's gone neglected long enough that it's bitrotted and needs some thorough patching to be made usable. Fortunately Common Lisp is still as stable as it has been for 31 years, so it's just the code which interacts with 3rd-party libraries that needs updating.&lt;/p&gt;
    &lt;p&gt;Yeah I felt that Quicklisp doesn't have the same features as package managers in other languages, and https is one of them. Also it's run by a single person which doesn't have too much time to constantly update the libraries.&lt;/p&gt;
    &lt;p&gt;In comparison I found Clojars^[0] for Clojure better and community driven like NPM. But obv Clojure has more business adoption than CL.&lt;/p&gt;
    &lt;p&gt;Lazarus is nice but both its apis and the ui feel like they're still stuck in the early 00's. It's not enough to look like VB6 / Delphi these days; you've got to keep up with what kinds of conventions we expect now.&lt;/p&gt;
    &lt;p&gt;10/GUI did some deep thinking about the limitations and potential of the (then-fairly new) multi touch input method. I wished something more had come out of it, instead it stayed a niche concept art video that is mostly forgotten now.&lt;/p&gt;
    &lt;p&gt;I’m not arguing the solutions it outlined are good, but I think some more discussion around how we interact with touch screens would be needed. Instead, we are still typing on a layout that was invented for mechanical typewriters - in 2025, on our touch screens.&lt;/p&gt;
    &lt;p&gt;The TUNES [1] operating system and programming language project. The reason for its failure are described perfectly on the archival website:&lt;/p&gt;
    &lt;p&gt;&amp;gt; TUNES started in 1992-95 as an operating system project, but was never clearly defined, and it succumbed to design-by-committee syndrome and gradually failed. Compared to typical OS projects it had very ambitious goals, which you may find interesting.&lt;/p&gt;
    &lt;p&gt;I've argued this for years on this site...but AOL.&lt;/p&gt;
    &lt;p&gt;At its best, having IM, email, browser, games, keywords, chats, etc. was a beautiful idea IMO. That they were an ISP seemed secondary or even unrelated to the idea. But they chose to charge for access even in the age of broadband, and adopt gym level subscription tactics to boot, and people decided they'd rather not pay it which is to be expected. I often wonder if they'd have survived as a software company otherwise.&lt;/p&gt;
    &lt;p&gt;They were basically a better thought out Facebook before Facebook, in my opinion.&lt;/p&gt;
    &lt;p&gt;RAM Disks. Basically extremely fast storage using RAM sticks slotted into a specially made board that fit in a PCIe slot. Not sure what happened to the project exactly but the website disappeared sometime in 2023.&lt;/p&gt;
    &lt;p&gt;The idea that you could read and write data at RAM speeds was really exciting to me. At work it's very common to see microscope image sets anywhere from 20 to 200 GB and file transfer rates can be a big bottleneck.&lt;/p&gt;
    &lt;p&gt;Products to attach RAM to expansion slots have long existed and continue to be developed. It's a matter of adding more memory once all of the DIMMs are full.&lt;/p&gt;
    &lt;p&gt;What to do with it, once it's there, is a concern of software, but specialized hardware is needed to get it there.&lt;/p&gt;
    &lt;p&gt;It's a real shame its raster functionality wasn't integrated into Illustrator. Adobe really butchered the whole Macromedia portfolio, didn't they?&lt;/p&gt;
    &lt;p&gt;(For those unfamiliar, Illustrator is a pure vector graphics editor; once you rasterize its shapes, they become uneditable fixed bitmaps. Fireworks was a vector graphics editor that rendered at a constant DPI, so it basically let you edit raster bitmaps like they were vectors. It was invaluable for pixel-perfect graphic design. Nothing since lets you do that, though with high-DPI screens and resolution-independent UIs being the norm these days, this functionality is less relevant than it used to be.)&lt;/p&gt;
    &lt;p&gt;At my last job m our designer was a Fireworks holdout. It was very pleasant. As someone who has to implement UIs, I greatly preferred it to Figma, though with today's flat boring designs there's a lot less slicing.&lt;/p&gt;
    &lt;p&gt;Nokia Maps. There was a brief period in the early 2010s where Nokia had the best mapping product on the planet, and it was given away for free on Lumia phones at a time when TomTom and Garmin were still charging $60+ for navigation apps.&lt;/p&gt;
    &lt;p&gt;VPRI, I was really hoping it would profoundly revolutionise desktop application development and maybe even lead to a new desktop model, and instead they wound up the project without having achieved the kind of impact I was dreaming of.&lt;/p&gt;
    &lt;p&gt;Dreamweaver or some other real WYSISYG web page editor that could maybe deal with very basic JavaScript.&lt;/p&gt;
    &lt;p&gt;I just wanna make a mostly static site with links in and out of my domain. Maybe a light bit of interactivity for things like search that autocompletes.&lt;/p&gt;
    &lt;p&gt;I always thought Microsoft Popfly had huge potential and was way ahead of its time. It made building web mashups feel like playing with Lego blocks, drag, drop, connect APIs, and instantly see the result.&lt;/p&gt;
    &lt;p&gt;If something like that existed today, powered by modern APIs and AI, it could become the ultimate no-code creativity playground.&lt;/p&gt;
    &lt;p&gt;Connect your phone to a display, mouse, keyboard and get a full desktop experience.&lt;/p&gt;
    &lt;p&gt;At the time smartphones were not powerful enough, cables were fiddly (adapters, HDMI, USB A instead of a single USB c cable) and virtualization and containers not quite there.&lt;/p&gt;
    &lt;p&gt;Today, going via pkvm seems like promising approach. Seamless sharing of data, apps etc. will take some work, though.&lt;/p&gt;
    &lt;p&gt;People talk so much about how you need to write code that fits well within the rest of the codebase, but what tools do we have to explore codebases and see what is connected to what? Clicking through files feels kind of stupid because if you have to work with changes that involve 40 files, good luck keeping any of that in your working memory. In my experience, the JetBrains dependency graphs also aren't good enough.&lt;/p&gt;
    &lt;p&gt;Sourcetrail was a code visualization tool that allowed you to visualize those dependencies and click around the codebase that way, see what methods are connected to what and so on, thanks to a lovely UI. I don't think it was enough alone, but I absolutely think we need something like this: https://www.dbvis.com/features/database-management/#explore-... but for your code, especially for codebases with hundreds of thousands or like above a million SLoC.&lt;/p&gt;
    &lt;p&gt;I yearn to some day view entire codebases as graphs with similarly approachable visualization, where all the dependencies are highlighted when I click an element. This could also go so, so much further - you could have a debugger breakpoint set and see the variables at each place, alongside being able to visually see how code is called throughout the codebase, or hell, maybe even visualize every possible route that could be taken.&lt;/p&gt;
    &lt;p&gt;Flickr - that was the future of photo storage, sharing, discovery.&lt;/p&gt;
    &lt;p&gt;What was the bookmarks social tool called from 00’s? I loved it and it fell off the earth. You could save your bookmarks, “publish” them to the community, share etc..&lt;/p&gt;
    &lt;p&gt;What ever happened to those build your own homepage apps like startpage (I think)? I always thought those would take off&lt;/p&gt;
    &lt;p&gt;Anyone remember Openmoko, the first commercialised open source smart phone. Was heaps buggy though, not really polished, etc. It’s only redeeming feature was the open source software and hardware (specs?).&lt;/p&gt;
    &lt;p&gt;There was the https://en.wikipedia.org/wiki/PinePhone and it's successor PinePhonePro. Bugginess and general impracticalities brought up to more recent standards. Inflation-adjusted, of course!&lt;/p&gt;
    &lt;p&gt;&amp;gt;This presentation introduces Via, a virtual file system designed to address the challenges of large game downloads and storage. Unlike cloud gaming, which suffers from poor image quality, input latency, and high hosting costs, Via allows games to run locally while only downloading game data on demand. The setup process is demonstrated with Halo Infinite, showing a simple installation that involves signing into Steam and allocating storage space for Via's cache.&lt;/p&gt;
    &lt;p&gt;&amp;gt;Via creates a virtual Steam library, presenting all owned games as installed, even though their data is not fully downloaded. When a game is launched, Via's virtual file system intercepts requests and downloads only the necessary game content as it's needed. This on-demand downloading is integrated with the game's existing streaming capabilities, leveraging features like level-of-detail and asset streaming. Performance metrics are displayed, showing download rates, server ping, and disk commit rates, illustrating how Via fetches data in real-time.&lt;/p&gt;
    &lt;p&gt;&amp;gt;The system prioritizes caching frequently accessed data. After an initial download, subsequent play sessions benefit from the on-disk cache, significantly reducing or eliminating the need for network downloads. This means the actual size of a game becomes less relevant, as only a portion of it needs to be stored locally. While server locations are currently limited, the goal is to establish a global network to ensure low ping. The presentation concludes by highlighting Via's frictionless user experience, aiming for a setup so seamless that users are unaware of its presence. Via is currently in early access and free to use, with hopes of future distribution partnerships.&lt;/p&gt;
    &lt;p&gt;I'm amazed the video still has under 4,000 views. Sadly, Flaherty got hired by XAI and gave up promoting the project.&lt;/p&gt;
    &lt;p&gt;Wait until you hear that almost all Unity games don't really have asset streaming because the engine loads things eagerly by default.&lt;/p&gt;
    &lt;p&gt;I don't see how this could take off. Internet speeds are getting quicker, disk space is getting cheaper, and this will slow down load times. And what's worse is the more you need this tech the worse experience you have.&lt;/p&gt;
    &lt;p&gt;Just on principle, I'd have liked to see it on the market for more than 49 days! It pains me as an engineer to think of the effort to bring a hardware device to market for such a minuscule run.&lt;/p&gt;
    &lt;p&gt;CueCat it was an affordable barcode scanner that anyone could have connected to their computer, and it scanned barcodes. It took almost two decades before we could finally do it again with our mobile phones.&lt;/p&gt;
    &lt;p&gt;The IBM school's computer. Developed by IBM Hursley in 1967, it was years ahead in its design, display out to a television and storage on normal audio tape. Would have kick started an educational revolution if it had been launched beyond the 10 prototype machines.&lt;/p&gt;
    &lt;p&gt;XenClient. I would really love to have some minimal OS HyperVisor running, and then you slap multiple OSes on top of that w/ easy full GUI switching via some hotkeys like Ctrl+Shift+F1. Additionaly, special drivers to virtualize Gfx and Sfx devices so every VM have full desktop capabilities and low latency.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it died because its very niche and also they couldnt keep up with development of drivers for desktops.. This is even worse today...&lt;/p&gt;
    &lt;p&gt;I came to say Opa too. I liked the language but the meteor-like framework it was bundled with, while nice for prototyping, was a pain to work around when it didn't do what you needed.&lt;/p&gt;
    &lt;p&gt;That said, frameworks were all the buzz back in the day, so the language alone probably wouldn't have gone anywhere without it.&lt;/p&gt;
    &lt;p&gt;Mozilla heka. As far as data collection and processing goes, we are still stuck with Logstash after all of these years. Heka promised a much more efficient solution, being implemented with Go and Lua plugins.&lt;/p&gt;
    &lt;p&gt;Pivotal Tracker ; Users loved it, it had an excellent model for tracking work and limiting work in progress on software projects. There is no real good alternative and the usual suspects for tracking project work are horrible in comparison.&lt;/p&gt;
    &lt;p&gt;In the late 90s there was a website called fuckedcompany which was a place where people could spill the beans about startups (mainly in silicon valley). It was anonymous and a pretty good view into the real state of tech. Now there is twitter/x but it's not as focused on this niche.&lt;/p&gt;
    &lt;p&gt;The closest sites I've found are Web3 is Going Just Great and Pivot to AI, which are newsfeeds of various car crashes in their respective hype arenas, although without any insider scoops/gossip.&lt;/p&gt;
    &lt;p&gt;wua.la … the original version. You share part of your storage to get the same amount back as resilient cloud storage from others. Was bought and killed by LaCie (now Seagate). They later provided paid-for cloud storage under the same name but it didn’t take off.&lt;/p&gt;
    &lt;p&gt;https://www.kite.com for python i first learned about it when i was working in an university group and had the task to transform a windowing algorithm already working on matlab to python. it felt like a modern linter and lsp with additional support through machine learning. i don't quite know why it got comparative small recognition, but perhaps enough to remain an avantgarde pioneering both python and machine learning support for further generations and wider applications.&lt;/p&gt;
    &lt;p&gt;i first learned about it when i was working in an university group and had the task to transform a windowing algorithm already working on matlab to python. it felt like a modern linter and lsp with additional support through machine learning. i don't quite know why it got comparative small recognition, but perhaps enough to remain an avantgarde pioneering both python and machine learning support for further generations and wider applications.&lt;/p&gt;
    &lt;p&gt;Google Wave ; It had a bunch of agents participating in editing the text together with you, making spelling fixes, finding additional information to enrich your content, and so much more.&lt;/p&gt;
    &lt;p&gt;I'm booting and running Haiku on my Thinkpad. It's a from-scratch workalike of BeOS, and able to run Be software. Though, frankly, Be software is totally 1990s, so a lot of Linux software written for Qt has been ported to Haiku.&lt;/p&gt;
    &lt;p&gt;In the end I wound up with basically the same application software as on my Debian desktop, except running on Haiku instead of Linux. Haiku is noticeably snappier and more responsive than Linux+X+Qt+KDE, though.&lt;/p&gt;
    &lt;p&gt;In late September or early October 1996, Fry's Electronics places a full page promo ad on the back of the business section of the San Jose Mercury News for OS/2 4.0 "WRAP [sic]" in 256 pt font in multiple places. Oops!&lt;/p&gt;
    &lt;p&gt;Nah, that time has passed and there's not much to miss from the base OS. What would be interesting is for IBM to publish the source to the Workplace Shell and the underlying SOM code so it might get a new life running on one of the free *nixes.&lt;/p&gt;
    &lt;p&gt;I could think of many examples, but I'll talk about the top four that I have in mind, that I'd like to see re-evaluated for today's times.&lt;/p&gt;
    &lt;p&gt;1. When Windows Vista was being developed, there were plans to replace the file system with a database, allowing users to organize and search for files using database queries. This was known as WinFS (https://en.wikipedia.org/wiki/WinFS). I was looking forward to this in the mid-2000s. Unfortunately Vista was famously delayed, and in an attempt to get Vista released, Microsoft pared back features, and one of these features was WinFS. Instead of WinFS, we ended up getting improved file search capabilities. It's unfortunate that there's been no proposals for database file systems for desktop operating systems since.&lt;/p&gt;
    &lt;p&gt;2. OpenDoc (https://en.wikipedia.org/wiki/OpenDoc) was an Apple technology from the mid-1990s that promoted component-based software. Instead of large, monolithic applications such as Microsoft Excel and Adobe Photoshop, functionality would be offered in the form of components, and users and developers can combine these components to form larger solutions. For example, as an alternative to Adobe Photoshop, there would be a component for the drawing canvas, and there would be separate components for each editing feature. Components can be bought and sold on an open marketplace. It reminds me of Unix pipes, but for GUIs. There's a nice promotional video at https://www.youtube.com/watch?v=oFJdjk2rq4E.&lt;/p&gt;
    &lt;p&gt;OpenDoc was a radically different paradigm for software development and distribution, and I think this was could have been an interesting contender against the dominance that Microsoft and Adobe enjoys in their markets. OpenDoc actually did ship, and there were some products made using OpenDoc, most notably Apple's Cyberdog browser (https://en.wikipedia.org/wiki/Cyberdog).&lt;/p&gt;
    &lt;p&gt;Unfortunately, Apple was in dire straits in the mid-1990s. Windows 95 was a formidable challenger to Mac OS, and cheaper x86 PCs were viable alternatives to Macintosh hardware. Apple was an acquisition target; IBM and Apple almost merged, and there was also an attempt to merge Apple with Sun. Additionally, the Macintosh platform depended on the availability of software products like Microsoft Office and Adobe Photoshop, the very types of products that OpenDoc directly challenged. When Apple purchased NeXT in December 1996, Steve Jobs returned to Apple, and all work on OpenDoc ended not too long afterward, leading to this now-famous exchange during WWDC 1997 between Steve Jobs and an upset developer (https://www.youtube.com/watch?v=oeqPrUmVz-o).&lt;/p&gt;
    &lt;p&gt;I don't believe that OpenDoc fits in with Apple's business strategy, even today, and while Microsoft offers component-based technologies that are similar to OpenDoc (OLE, COM, DCOM, ActiveX, .NET), the Windows ecosystem is still dominated by monolithic applications.&lt;/p&gt;
    &lt;p&gt;I think it would have been cool had the FOSS community pursued component-based software. It would have been really cool to apt-get components from remote repositories and link them together, either using GUI tools, command-line tools, or programmatically to build custom solutions. Instead, we ended up with large, monolithic applications like LibreOffice, Firefox, GIMP, Inkscape, Scribus, etc.&lt;/p&gt;
    &lt;p&gt;3. I am particularly intrigued by Symbolics Genera (https://en.wikipedia.org/wiki/Genera_(operating_system)), an operating system designed for Symbolics Lisp machines (https://en.wikipedia.org/wiki/Symbolics). In Genera, everything is a Lisp object. The interface is an interesting hybrid of early GUIs and the command line. To me, Genera could have been a very interesting substrate for building component-based software; in fact, it would have been far easier building OpenDoc on top of Common Lisp than on top of C or C++. Sadly, Symbolics' fortunes soured after the AI winter of the late 1980s/early 1990s, and while Genera was ported to other platforms such as the DEC Alpha and later the x86-64 via the creation of a Lisp machine emulator, it's extremely difficult for people to obtain a legal copy, and it was never made open source. The closest things to Genera we have are Xerox Interlisp, a competing operating system that was recently made open source, and open-source descendants of Smalltalk-80: Squeak, Pharo, and Cuis-Smalltalk.&lt;/p&gt;
    &lt;p&gt;4. Apple's "interregnum" years between 1985 and 1996 were filled with many intriguing projects that were either never commercialized, were cancelled before release, or did not make a splash in the marketplace. One of the most interesting projects during the era was Bauhaus, a Lisp operating system developed for the Newton platform. Mikel Evins, a regular poster here, describes it here (https://mikelevins.github.io/posts/2021-07-12-reimagining-ba...). It would have been really cool to have a mass-market Lisp operating system, especially if it had the same support for ubiquitous dynamic objects like Symbolic Genera.&lt;/p&gt;
    &lt;p&gt;Re: obtaining a legal copy of Genera, as of 2023 Symbolics still existed as a corporate entity and they continued to sell x86-64 laptops with "Portable Genera 2.0". I bought one from them then, and occasionally see them listing some on Ebay. (This isn't intended as an advertisement or endorsement, just a statement. I think it's quite unfortunate that Symbolics's software hasn't been made freely available, since it's now really only of historical interest.)&lt;/p&gt;
    &lt;p&gt;I'm intrigued by Symbolics Genera too. It would have been interesting seeing further development of Lisp OS, especially when they would have had internet connection. Rewriting part of your OS and see the changes in real time? Maybe web apps could have been just software written in Lisp, downloaded on the machine and directly being executed in a safe environment on top of the Genera image. Big stuff.&lt;/p&gt;
    &lt;p&gt;OpenDoc was mostly given to Taligent (the Apple and IBM joint venture) to develop. It was full-on OO: about 35 files for a minimal application, which meant that Erich Gamma had to build a whole new type of IDE which was unusable. He likely learned his lesson: it's pretty hard to define interfaces between unknown components without forcing each one to know about all the others.&lt;/p&gt;
    &lt;p&gt;MIME types for mail addressed much of the demand for pluggable data types.&lt;/p&gt;
    &lt;p&gt;Fro me, DESQview. Microsoft tried to buy it in order to use its tech in their windows system. I wonder how things would be today if they were able to purchase it. But DESQview said "no".&lt;/p&gt;
    &lt;p&gt;Instead it went into a slow death spiral due to Windows 95.&lt;/p&gt;
    &lt;p&gt;Love seeing this one. My uncle was co-founder of Quarterdeck, and I grew up in a world of DESQview and QEMM. It was a big influence on me as a child.&lt;/p&gt;
    &lt;p&gt;Got a good family story about that whole acquisition attempt, but I don't want to speak publicly on behalf of my uncle. I know we've talked at length about the what-ifs of that moment.&lt;/p&gt;
    &lt;p&gt;I do have a scattering of some neat Quarterdeck memorabilia I can share, though:&lt;/p&gt;
    &lt;p&gt;DESQview/X sucked the wind out of DESQview's sails. It was, on paper, a massive upgrade. I had been running DESQview for years, with a dial-up BBS in the background.&lt;/p&gt;
    &lt;p&gt;But you couldn't actually buy /X. After trying to buy a copy, my publisher even contacted DESQ's marketing people to get a copy for me, and they wouldn't turn one over. Supposedly there were some copies actually sold, but too few, too late, and then /X was dropped. There was at least one more release of plain DESQview after that, but by then Windows was eating its lunch.&lt;/p&gt;
    &lt;p&gt;OSI's session layer did very little more than TCP/UDP port numbers; in the OSI model you would open a connection to a machine, then use that connection to open a session to a particular application.&lt;/p&gt;
    &lt;p&gt;X.400 was a nice idea, but the ideal of having a single global directory predates security. I can understand why it never happened&lt;/p&gt;
    &lt;p&gt;On X.509, the spec spends two chapters on attribute certificates, which I've never seen used in the wild. It's a shame; identity certificates do a terrible job at authentication&lt;/p&gt;
    &lt;p&gt;Windows Phone's UI is still with us, from Windows 8 onwards. Everything on 8, 10, and 11 is optimized for a touch interface on a small screen, which is ridiculous on a modern desktop with a 32" or so monitor and a trackball or mouse.&lt;/p&gt;
    &lt;p&gt;False. The Metro design was abandoned long ago. No live tiles, no typography-first minimal UIs in windows 10/11. I pin an email app to taskbar/start, I don't see the unread count.&lt;/p&gt;
    &lt;p&gt;From Windows 10, there is a switch between desktop and touch mode.&lt;/p&gt;
    &lt;p&gt;They stopped supporting small tablets some years ago though, and made it worse with every Windows update. I can only surmise that it was to make people stop using them. Slow GUI, low contrast, killed apps.&lt;/p&gt;
    &lt;p&gt;Fortress language. It suffered from being too Haskell-like in terms of too many, non-orthogonal features. Rust and Go applied lessons from it perhaps indirectly.&lt;/p&gt;
    &lt;p&gt;their operator precedence system was one of my favourite pieces of language design. the tl;dr was that you could group operators into precedence sets, and an expression involving operators that all came from the same set would have that set's precedence rules applied, but if you had an expression involving mixed sets you needed to add the parentheses. crucially, they also supported operator overloading, and the same operator could be used in a different set as long as everything could be parsed unambiguously. (caveat, I never used the language, I just read about the operator design in the docs and it was very eye opening in the sense that every other language's operator precedence system suddenly felt crude and haphazard)&lt;/p&gt;
    &lt;p&gt;LSR, the "Linux Screen Reader", an ambitiousy designed Python implementation of a GUI screen reader developed by IBM starting around 2006 or so. The project was ended 2008 when IBM ended all its Accessibility involvement in FLOSS.&lt;/p&gt;
    &lt;p&gt;Humane AI Pin. I think they launched 2 years too early and were too greedy with device pricing and subscription. Also if they focused as accessory for Android/iPhone they could reduce power usage and cost as well.&lt;/p&gt;
    &lt;p&gt;Their execution was of course bad but I think today current LLM models are better and faster and there is much more OSS models to reduce costs. Hardware though looked nice and pico projector interesting concept even though not the best executed.&lt;/p&gt;
    &lt;p&gt;Wine predates ReactOS. It was basically a FOSS duplicate of Sun's WABI.&lt;/p&gt;
    &lt;p&gt;I wrote a bunch of software in Borland Delphi, which ran in Windows, Wine, and ReactOS with no problems. Well, except for ReactOS' lack of printing support.&lt;/p&gt;
    &lt;p&gt;As long as you stay within the ECMA or published Windows APIs, everything runs fine in Wine and ReactOS. But Microsoft products are full of undocumented functions, as well as checks to see if they're running on real Windows. That goes back to the Windows 3.1 days, when 3.1 developers regularly used OS/2 instead of DOS, and Microsoft started adding patches to fail under OS/2 and DR-DOS. So all that has to be accounted for by Wine and ReactOS. A lot of third-party software uses undocumented functions as well, especially stuff written back during the days when computer magazines were a thing, and regularly published that kind of information. A lot of programmers found the lure of undocumented calls to be irresistible, and they wound up in all kinds of commercial applications where they really shouldn't have been.&lt;/p&gt;
    &lt;p&gt;In my experience anything that will load under Wine will run with no problems. ReactOS has some stability problems, but then the developers specifically call it "alpha" software. Despite that, I've put customers on ReactOS systems after verifying all their software ran on it. It gets them off the Microsoft upgrade treadmill. Sometimes there are compatibility problems and I fall back to Wine on Linux. Occasionally nothing will do but real Windows.&lt;/p&gt;
    &lt;p&gt;Hard disagree. The Humane AI Pin ad was a classic silicon valley ad that screamed B2VC and demonstrated nothing actually useful that couldn't be done with an all-in-one phone app (or even the ChatGPT app) and bluetooth earbuds that you already have.&lt;/p&gt;
    &lt;p&gt;Which reduces its innovation level to nothing more than a chest-mounted camera.&lt;/p&gt;
    &lt;p&gt;You want real B2C products that people would actually buy? Look at the Superbowl ads instead. Then watch the Humane ad again. It's laughable.&lt;/p&gt;
    &lt;p&gt;Ceylon, JVM language, developed by Red Hat, now abandoned at Eclipse. Lost the race with Kotlin but proposed more than just syntax sugar over Java. Anonymous union types, comprehensions, proper module system...&lt;/p&gt;
    &lt;p&gt;nah, glass was impressive for a such a big org like google, but smartphones are popular because people use them like portable televisions. glanceable info and walking directions are more like an apple watch sized market, without the fashion element. meta is about to find out.&lt;/p&gt;
    &lt;p&gt;google glass sucks though and glasses will never be a thing. google and meta and … can spend $8T and come up with the most insane tech etc but no one will be wearing f’ing glasses :)&lt;/p&gt;
    &lt;p&gt;Apple’s scanning system for CSAM. The vast majority of the debate was dominated by how people imagined it worked, which was very different to how it actually worked.&lt;/p&gt;
    &lt;p&gt;It was an extremely interesting effort where you could tell a huge amount of thought and effort went into making it as privacy-preserving as possible. I’m not convinced it’s a great idea, but it was a substantial improvement over what is in widespread use today and I wanted there to be a reasonable debate on it instead of knee-jerk outrage. But congrats, I guess. All the cloud hosting systems scan what they want anyway, and the one that was actually designed with privacy in mind got screamed out of existence by people who didn’t care to learn the first thing about it.&lt;/p&gt;
    &lt;p&gt;Good riddance to a system that would have provided precedent for client-side scanning for arbitrary other things, as well as likely false positives.&lt;/p&gt;
    &lt;p&gt;&amp;gt; I wanted there to be a reasonable debate on it&lt;/p&gt;
    &lt;p&gt;I'm reminded of a recent hit-piece about Chat Control, in which one of the proponent politicians was quoted as complaining about not having a debate. They didn't actually want a debate, they wanted to not get backlash. They would never have changed their minds, so there's no grounds for a debate.&lt;/p&gt;
    &lt;p&gt;We need to just keep making it clear the answer is "no", and hopefully strengthen that to "no, and perhaps the massive smoking crater that used to be your political career will serve as a warning to the next person who tries".&lt;/p&gt;
    &lt;p&gt;This. No matter how cool the engineering might have been, from the perspective of what surveillance policies it would have (and very possibly did) inspire/set precedent for… Apple was very much creating the Torment Nexus from “Don’t Create the Torment Nexus.”&lt;/p&gt;
    &lt;p&gt;&amp;gt; from the perspective of what surveillance policies it would have (and very possibly did) inspire/set precedent for…&lt;/p&gt;
    &lt;p&gt;I can’t think of a single thing that’s come along since that is even remotely similar. What are you thinking of?&lt;/p&gt;
    &lt;p&gt;I think it’s actually a horrible system to implement if you want to spy on people. That’s the point of it! If you wanted to spy on people, there are already loads of systems that exist which don’t intentionally make it difficult to do so. Why would you not use one of those models instead? Why would you take inspiration from this one in particular?&lt;/p&gt;
    &lt;p&gt;The problem isn’t the system as implemented; the problem is the very assertion “it is possible to preserve the privacy your constituents want, while running code at scale that can detect Bad Things in every message.”&lt;/p&gt;
    &lt;p&gt;Once that idea appears, it allows every lobbyist and insider to say “mandate this, we’ll do something like what Apple did but for other types of Bad People” and all of a sudden you have regulations that force messaging systems to make this possible in the name of Freedom.&lt;/p&gt;
    &lt;p&gt;Remember: if a model can detect CSAM at scale, it can also detect anyone who possesses any politically sensitive image. There are many in politics for whom that level of control is the actual goal.&lt;/p&gt;
    &lt;p&gt;&amp;gt; the problem is the very assertion “it is possible to preserve the privacy your constituents want, while running code at scale that can detect Bad Things in every message.”&lt;/p&gt;
    &lt;p&gt;Apple never made that assertion, and the system they designed is incapable of doing that.&lt;/p&gt;
    &lt;p&gt;&amp;gt; if a model can detect CSAM at scale, it can also detect anyone who possesses any politically sensitive image.&lt;/p&gt;
    &lt;p&gt;Apple’s system cannot do that. If you change parts of it, sure. But the system they proposed cannot.&lt;/p&gt;
    &lt;p&gt;To reiterate what I said earlier:&lt;/p&gt;
    &lt;p&gt;&amp;gt; The vast majority of the debate was dominated by how people imagined it worked, which was very different to how it actually worked.&lt;/p&gt;
    &lt;p&gt;So far, you are saying that you don’t have a problem with the system Apple designed, and you do have a problem with some other design that Apple didn’t propose, that is significantly different in multiple ways.&lt;/p&gt;
    &lt;p&gt;Also, what do you mean by “model”? When I used the word “model” it was in the context of using another system as a model. You seem to be using it in the AI sense. You know that’s not how it worked, right?&lt;/p&gt;
    &lt;p&gt;&amp;gt; Chat Control, and other proposals that advocate backdooring individual client systems.&lt;/p&gt;
    &lt;p&gt;Chat Control is older than Apple’s CSAM scanning and is very different from it.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Clients should serve the user.&lt;/p&gt;
    &lt;p&gt;Apple’s system only scanned things that were uploaded to iCloud.&lt;/p&gt;
    &lt;p&gt;You missed the most important part of my comment:&lt;/p&gt;
    &lt;p&gt;&amp;gt; I think it’s actually a horrible system to implement if you want to spy on people. That’s the point of it! If you wanted to spy on people, there are already loads of systems that exist which don’t intentionally make it difficult to do so. Why would you not use one of those models instead? Why would you take inspiration from this one in particular?&lt;/p&gt;
    &lt;p&gt;I don’t think you can accurately describe it as client-side scanning and false positives were not likely. Depending upon how you view it, false positives were either extremely unlikely, or 100% guaranteed for practically everybody. And if you think the latter part is a problem, please read up on it!&lt;/p&gt;
    &lt;p&gt;&amp;gt; I'm reminded of a recent hit-piece about Chat Control, in which one of the proponent politicians was quoted as complaining about not having a debate. They didn't actually want a debate, they wanted to not get backlash. They would never have changed their minds, so there's no grounds for a debate.&lt;/p&gt;
    &lt;p&gt;Right, well I wanted a debate. And Apple changed their minds. So how is it reminding you of that? Neither of those things apply here.&lt;/p&gt;
    &lt;p&gt;No, but I have a hard time imagining a bug that would meaningfully compromise this kind of system. Can you give an example?&lt;/p&gt;
    &lt;p&gt;&amp;gt; How about making Apple vulnerable to demands from every government where they do business?&lt;/p&gt;
    &lt;p&gt;They already are. So are Google, Meta, Microsoft, and all the other giants we all use. And all those other companies are already scanning your stuff. Meta made two million reports in 2024Q4 alone.&lt;/p&gt;
    &lt;p&gt;There is no place for spyware of any kind on my phone. Saying that it is to "protect the children" and "to catch terrorists" does not make it any more acceptable.&lt;/p&gt;
    &lt;p&gt;I believe my retro Nokia phones s60/s90 does not have any spyware. I believe earlier Nokia models like s40 or monochrome does not even have an ability to spy on me (but RMS considers triangulation as spyware). I don't believe any products from the duopoly without even root access are free from all kinds of vendor's rootkits.&lt;/p&gt;
    &lt;p&gt;Apple designed a system. People guessed at what it did. Their guesses were way off the mark. This poisoned all rational discussion on the topic. If you imagine a system that works differently to Apple’s system, you can complain about that imaginary system all you want, but it won’t be meaningful, it’s just noise.&lt;/p&gt;
    &lt;p&gt;You understand it just fine, you're just trying to pass you fantasy pod immutable safe future as rational while painting the obvious objections based on the real world as meaningless noise.&lt;/p&gt;
    &lt;p&gt;Your point did not come across. It still isn’t. I don’t know what you mean by “pass you fantasy pod immutable safe future as rational”. You aren’t making sense to me. I absolutely do not “understand it just fine”.&lt;/p&gt;
    &lt;p&gt;Founder perspective: “avoid patents by staying 20 years behind” is the tragedy. I published a 2-page CC0 initiative that splits protection into two layers: • GLOBAL layer — fast, low-friction recognition for non-strategic inventions • LOCAL-STRATEGIC layer — conventional national control for sensitive tech Goal: cut admin drag/time-to-market while keeping sovereignty intact.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553132</guid><pubDate>Sat, 11 Oct 2025 22:16:18 +0000</pubDate></item><item><title>CamoLeak: Critical GitHub Copilot Vulnerability Leaks Private Source Code</title><link>https://www.legitsecurity.com/blog/camoleak-critical-github-copilot-vulnerability-leaks-private-source-code</link><description>&lt;doc fingerprint="9d071f1f98be66c7"&gt;
  &lt;main&gt;
    &lt;p&gt;Get details on our discovery of a critical vulnerability in GitHub Copilot Chat.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR:&lt;/head&gt;
    &lt;p&gt;In June 2025, I found a critical vulnerability in GitHub Copilot Chat (CVSS 9.6) that allowed silent exfiltration of secrets and source code from private repos, and gave me full control over Copilot’s responses, including suggesting malicious code or links.&lt;/p&gt;
    &lt;p&gt;The attack combined a novel CSP bypass using GitHub’s own infrastructure with remote prompt injection. I reported it via HackerOne, and GitHub fixed it by disabling image rendering in Copilot Chat completely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;GitHub Copilot Chat is an AI assistant built into GitHub that helps developers by answering questions, explaining code, and suggesting implementations directly in their workflow.&lt;/p&gt;
    &lt;p&gt;Copilot Chat is context-aware: it can use information from the repository (such as code, commits, or pull requests) to provide tailored answers.&lt;/p&gt;
    &lt;p&gt;As always, more context = more attack surface.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the prompt injection&lt;/head&gt;
    &lt;p&gt;As mentioned earlier, GitHub Copilot is context-aware - so I set out to make it notice me. To do this, I embedded a prompt directed at Copilot inside a pull request description.&lt;/p&gt;
    &lt;p&gt;But what’s the point if everyone can see it? Luckily, GitHub came to the rescue with a proper solution: invisible comments are an official feature! 🎉&lt;/p&gt;
    &lt;p&gt;You can find more details in their documentation: Hiding content with comments. By simply putting the content you want to hide inside:&lt;/p&gt;
    &lt;p&gt;I tried the same prompt but this time as a hidden comment inside the PR description, and it worked!&lt;/p&gt;
    &lt;p&gt;Interestingly, posting a hidden comment triggers the usual PR notification to the repo owner, but the content of the hidden comment isn’t revealed anywhere.&lt;/p&gt;
    &lt;p&gt;I attempted logging in with a different user and visited the pull request page. The prompt was injected into my context as well! &lt;/p&gt;
    &lt;p&gt;I then replaced the original “HOORAY” prompt with far more complex instructions, including code suggestions and Markdown rendering, and to my surprise, they worked flawlessly!&lt;/p&gt;
    &lt;p&gt;For instance, notice how effortlessly Copilot suggests this malicious Copilotevil package.&lt;/p&gt;
    &lt;p&gt;* Notice that the user who asked Copilot Chat to explain the PR is different from the user who posted the invisible prompt, demonstrating that the prompt can affect any user who visits the page.&lt;/p&gt;
    &lt;p&gt;Copilot operates with the same permissions as the user making the request, but it obviously needs access to the user’s private repositories to respond accurately. We can exploit this by including instructions in our injected prompt to access a victim user’s private repository, encode its contents in base16, and append it to a URL. Then, when the user clicks the URL, the data is exfiltrated back to us.&lt;/p&gt;
    &lt;p&gt;* Notice that the repository https://github.com/LegitSecurity/issues-service is a private repo inside a private GitHub organization!&lt;/p&gt;
    &lt;head rend="h2"&gt;Recap: What We Can Do&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Influence the responses generated by another user’s Copilot&lt;/item&gt;
      &lt;item&gt;Inject custom Markdown, including URLs, code, and images&lt;/item&gt;
      &lt;item&gt;Exploit the fact that Copilot runs with the same permissions as the victim user&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Bypassing Content Security Policy (CSP)&lt;/head&gt;
    &lt;p&gt;This is where things get tricky. If you’ve followed along so far, you’re probably thinking — just inject an HTML &amp;lt;img&amp;gt; tag into the victim’s chat, encode their private data as a parameter, and once the browser tries to render it, the data will be leaked.&lt;/p&gt;
    &lt;p&gt;Not so fast. GitHub enforces a very restrictive Content Security Policy (CSP), which blocks fetching images and other content types from domains that aren’t explicitly owned by GitHub. So, our “simple” &amp;lt;img&amp;gt; trick won’t work out of the box.&lt;/p&gt;
    &lt;p&gt;You’re probably asking yourself - wait, how does my fancy README manage to show images from third-party sites?&lt;/p&gt;
    &lt;p&gt;When you commit a README or any Markdown file containing external images, GitHub automatically processes the file, during this process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;GitHub parses the Markdown and identifies any image URLs pointing to domains outside of GitHub.&lt;/item&gt;
      &lt;item&gt;URL rewriting via Camo: Each external URL is rewritten to a Camo proxy URL. This URL includes a HMAC-based cryptographic signature and points to https://camo.githubusercontent.com/….&lt;/item&gt;
      &lt;item&gt;Signed request verification: When a browser requests the image, the Camo proxy verifies the signature to ensure it was generated by GitHub. Only valid, signed URLs are allowed.&lt;/item&gt;
      &lt;item&gt;Content fetching: If the signature is valid, Camo fetches the external image from its original location and serves it through GitHub’s servers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This process ensures that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Attackers cannot craft arbitrary URLs to exfiltrate dynamic data.&lt;/item&gt;
      &lt;item&gt;All external images go through a controlled proxy, maintaining security and integrity.&lt;/item&gt;
      &lt;item&gt;The end user sees the image seamlessly in the README, but the underlying URL never exposes the original domain directly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More information about Camo can be found here.&lt;/p&gt;
    &lt;p&gt;Let’s look at an example: Committing a README file to GitHub that contains this URL:&lt;/p&gt;
    &lt;p&gt;Will be automatically changed inside the README into:&lt;/p&gt;
    &lt;p&gt;Rather than doing it manually through the website, you can use GitHub’s REST API to submit raw Markdown and receive it back with all external image URLs automatically converted to Camo proxy URLs.&lt;/p&gt;
    &lt;p&gt;Alright, so we can’t generate Camo URLs on the fly — without code execution, every &amp;lt;img&amp;gt; tag we inject into the victim’s chat must include a valid Camo URL signature that was pre-generated. Otherwise, GitHub’s reverse proxy won’t fetch the content.&lt;/p&gt;
    &lt;head rend="h2"&gt;The discovery&lt;/head&gt;
    &lt;p&gt;I spent a long time thinking about this problem before this crazy idea struck me.&lt;lb/&gt;If I create a dictionary of all letters and symbols in the alphabet, pre-generate their corresponding Camo URLs, embed this dictionary into the injected prompt, and then ask Copilot to play a “small game” by rendering the content I want to leak as “ASCII art” composed entirely of images, will Copilot inject valid Camo images that the browser will render by their order? Yes, it will.&lt;/p&gt;
    &lt;p&gt;I quickly got to work. First, I set up a web server that responds to every request with a 1x1 transparent pixel. This way, when GitHub’s Camo reverse proxy fetches the images from my server, they remain invisible in the victim’s chat.&lt;/p&gt;
    &lt;p&gt;Next, by using GitHub’s API, I created a valid Camo URL dictionary of all the letters and symbols that may be used to leak source code / issues content:&lt;/p&gt;
    &lt;p&gt;Turns into:&lt;/p&gt;
    &lt;p&gt;And finally, I created the prompt:&lt;/p&gt;
    &lt;p&gt;* I added "random" parameter at the end of each Camo URL and requested Copilot to generate each time a new random number and append it to the URL, this way caching is not a problem.&lt;/p&gt;
    &lt;p&gt;Our target: the description of a zero-day vulnerability inside an issue of a private project.&lt;/p&gt;
    &lt;p&gt;The result: Stealing zero days from private repositories.&lt;/p&gt;
    &lt;p&gt;PoC showcasing the full attack (Only if you have 4 minutes):&lt;/p&gt;
    &lt;p&gt;I also managed to get Copilot to search the victim’s entire codebase for the keyword "AWS_KEY" and exfiltrate the result.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub’s Response&lt;/head&gt;
    &lt;p&gt;GitHub reports that the vulnerability was fixed as of August 14.&lt;/p&gt;
    &lt;head rend="h2"&gt;To learn more&lt;/head&gt;
    &lt;p&gt;Get details on a previous vulnerability we unearthed in GitLab Duo.&lt;/p&gt;
    &lt;p&gt;Get our thoughts on AppSec in the age of AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553422</guid><pubDate>Sat, 11 Oct 2025 22:58:30 +0000</pubDate></item><item><title>Meta Superintelligence's surprising first paper</title><link>https://paddedinputs.substack.com/p/meta-superintelligences-surprising</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553577</guid><pubDate>Sat, 11 Oct 2025 23:16:05 +0000</pubDate></item><item><title>Paper2Video: Automatic Video Generation from Scientific Papers</title><link>https://arxiv.org/abs/2510.05096</link><description>&lt;doc fingerprint="341cba0b81f1c4ee"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 6 Oct 2025 (v1), last revised 9 Oct 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Paper2Video: Automatic Video Generation from Scientific Papers&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at this https URL.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Zeyu Zhu Mr. [view email]&lt;p&gt;[v1] Mon, 6 Oct 2025 17:58:02 UTC (6,815 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 9 Oct 2025 17:29:00 UTC (6,817 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.CV&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553701</guid><pubDate>Sat, 11 Oct 2025 23:32:25 +0000</pubDate></item><item><title>Vancouver Stock Exchange: Scam capital of the world (1989) [pdf]</title><link>https://scamcouver.wordpress.com/wp-content/uploads/2012/04/scam-capital.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553783</guid><pubDate>Sat, 11 Oct 2025 23:43:42 +0000</pubDate></item><item><title>LineageOS 23</title><link>https://lineageos.org/Changelog-30/</link><description>&lt;doc fingerprint="1f3a29d3d2b25be8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Changelog 30 - Sleek Sixteen, Streamlined Suite, Future Flow&lt;/head&gt;
    &lt;head rend="h6"&gt;Written on October 11, 2025 by Nolen Johnson (npjohnson)&lt;/head&gt;
    &lt;head rend="h2"&gt;23 - Primetime Release&lt;/head&gt;
    &lt;p&gt;Hey there! Welcome back!&lt;/p&gt;
    &lt;p&gt;This last year has been a whirlwind to say the least, but we have remained dedicated to bringing an updated LineageOS based on Android 16 to the masses!&lt;/p&gt;
    &lt;p&gt;We’ve been hard at work rebasing all of our changes and features since Android 16’s release in June. Android 16’s first release mainly contained iterative improvements and some UI/UX refinements, but due to our previous efforts adapting to Google’s UI-centric adjustments in Android 12 through 14, we were able to rebase onto Android 16’s code-base faster than anticipated. Yes you read that right: We’re early this year!&lt;/p&gt;
    &lt;p&gt;Other components have complicated our release and security patching efforts, but we’ll get to that shortly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Google’s Patch Cadence, and LineageOS Going Forward&lt;/head&gt;
    &lt;head rend="h4"&gt;Firstly, what even is an ASB? Or a QPR?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ASB (Android Security Bulletin) – Google’s monthly roundup of newly fixed security vulnerabilities across the Android ecosystem. Distributed as a list of patches and security branch updates for older but still supported Android versions, with the current version tagged monthly. These updates let projects like ours and OEMs stay aligned with Google’s security baseline.&lt;/item&gt;
      &lt;item&gt;QPR (Quarterly Platform Release) – Mid-cycle updates to a given Android version, landing every few months. QPRs bring not just security fixes but also bug fixes, performance improvements, and feature changes (like Material 3 Expressive in 16 QPR1).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Okay, why are you telling me this?&lt;/head&gt;
    &lt;p&gt;You’ll notice we are choosing to release LineageOS 23.0, and not 23.1. That’s because it’s based on Android 16’s initial release (what we’ll call QPR0), even though QPR1 has already rolled out to Pixels. The catch? Google never pushed QPR1’s source to AOSP. They’ve said it’ll come “in the coming weeks” (source), but right now only contracted partners have access. And to answer the immediate question, the likelihood that any custom ROM will ever be a contracted Google partner is near 0.&lt;/p&gt;
    &lt;p&gt;On top of that, Google’s handling of ASBs has shifted (source). July was empty for the first time since the program began, August had a single patch, and September omitted patches for several listed vulnerabilities, with fixes shared privately to partners under embargo. The result is that AOSP security updates are no longer released in full on a monthly basis. Instead, only vulnerabilities deemed “high risk” (i.e. actively exploited in the wild) will be published by Google in the monthly ASBs, and even then, the underlying patches are often not made public immediately.&lt;/p&gt;
    &lt;p&gt;On a quarterly cadence, Google now issues larger security bulletins that include patches for vulnerabilities originally discovered in prior months. These quarterly bulletins coincide with QPRs (Quarterly Platform Releases), which bundles those security fixes together with feature updates, but have so far not been pushed to AOSP at the time of release.&lt;/p&gt;
    &lt;p&gt;This is why you’ve seen the LineageOS 22.2 security patch level remain reflective of August well into September.&lt;/p&gt;
    &lt;p&gt;In short: this cadence is now the norm, and we need to adapt.&lt;/p&gt;
    &lt;head rend="h4"&gt;And I heard that Google stopped pushing Pixel source?&lt;/head&gt;
    &lt;p&gt;Yes, Google has pulled back here too. Pixel kernels are now only offered as history-stripped tarballs, available privately on request, with no device trees, HALs, or configs. Thanks to projects like CalyxOS, Pixels will likely remain well supported, but they’re no longer guaranteed “day one” devices for LineageOS. Pixel devices are now effectively no easier to support than any other OEM’s devices.&lt;/p&gt;
    &lt;p&gt;In short, this just makes things harder, not impossible.&lt;/p&gt;
    &lt;head rend="h4"&gt;How does this affect LineageOS? And me?&lt;/head&gt;
    &lt;p&gt;It means we adapt. Instead of waiting indefinitely for QPR1’s source, we’re shipping 23.0 now on “QPR0”, with the publicly available ASB patches applied, and we’ll only attest to a security patch level once we have access to all of its fixes. When QPR1 (and future QPRs) eventually land in AOSP, we’ll merge or rebase as appropriate.&lt;/p&gt;
    &lt;p&gt;This does mean some features (like Material 3 Expressive) aren’t here yet. But it ensures users get timely builds, the most complete security fixes we can legally access, and a clear path forward without being stuck in limbo.&lt;/p&gt;
    &lt;p&gt;This will likely be the expected norm for Android 17 and beyond, so expect more &lt;code&gt;.0&lt;/code&gt; releases in the
future!&lt;/p&gt;
    &lt;head rend="h4"&gt;TL;DR:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Google no longer pushes monthly tags and patches to AOSP; and most fixes/security patches are now pushed quarterly, if at all.&lt;/item&gt;
      &lt;item&gt;Security patch levels may occasionally lag: we only increment them once all patches are public.&lt;/item&gt;
      &lt;item&gt;LineageOS 23.0 is based on Android 16 “QPR0” because QPR1’s source isn’t public yet. &lt;list rend="ul"&gt;&lt;item&gt;Because of this some headline features (like Material 3 Expressive) will come later, once sources drop.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Pixel support continues, but with reduced source access, they’re no longer guaranteed, let alone on day one.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Legacy Devices&lt;/head&gt;
    &lt;p&gt;The bad news extends a bit further here, though it’s less surprising than the earlier sections.&lt;/p&gt;
    &lt;p&gt;Google’s increased reliance on newer eBPF features has made supporting devices with older Linux kernels increasingly difficult. Android 16 “QPR0” “requires Linux 5.4 and above, and at the time of writing, the necessary features have only been properly backported as far back as &lt;code&gt;4.14&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Unfortunately, LineageOS 22.2 still supports many devices running &lt;code&gt;4.4&lt;/code&gt; and &lt;code&gt;4.9&lt;/code&gt;. As of
now, no complete backports of the required features exist for these kernels. The silver lining is
that, unlike the massive device loss we saw moving off LineageOS 18.1, these versions could be
salvaged if someone were to take on the work of adapting the backports. If you do succeed, please
reach out to devrel(at)lineageos.org, we’d be happy to review it!&lt;/p&gt;
    &lt;p&gt;We’re currently targeting only shipping kernels that have 1:1 eBPF backports to make them feature equivalent to Linux 5.4 from here on out to avoid compatibility issues.&lt;/p&gt;
    &lt;head rend="h3"&gt;Back to the Good Stuff!&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Security patches from September 2024 to August 2025 have been merged to LineageOS 20 through 23.0.&lt;/item&gt;
      &lt;item&gt;SeedVault and Etar have both been updated to their newest respective upstream version, with multiple fixes having been sent back to the relevant upstream repos!&lt;/item&gt;
      &lt;item&gt;WebView has been updated to Chromium 140.0.7339.51.&lt;/item&gt;
      &lt;item&gt;Contributor demon000 (Cosmin Tanislav) has started work on an awesome set of tools to assist maintainers in device bringups from scratch! They’re still in-progress, but are staged to make a lot of our efforts significantly easier and more streamlined - so stay tuned! Maybe a rare non-yearly blog post? ;)&lt;/item&gt;
      &lt;item&gt;Contributor 0xCAFEBABE has extended support for various VirtIO configurations (QEMU/crosvm/UTM/libvirt, etc.) targets! Though these aren’t supported officially, there is an awesome, comprehensive guide for building and utilizing these targets on the Wiki. There is even a newer one allowing you to run LineageOS in a UTM virtual machine on an Apple Silicon Mac!&lt;/item&gt;
      &lt;item&gt;Contributor 0xCAFEBABE has added support for Cuttlefish targets!&lt;/item&gt;
      &lt;item&gt;Contributor 0xCAFEBABE has extended support for devices booting Android on the mainline Linux kernel! This will allow us to in theory boot LineageOS on almost all devices supported by the Linux kernel. It’s in early phases, but very promising, with several successful device ports already available on the LineageOS GitHub organization! Check the search term “mainline” on the organization’s search bar.&lt;/item&gt;
      &lt;item&gt;LineageOS is now nearly &lt;code&gt;Android.mk&lt;/code&gt;free! Google announced their move from&lt;code&gt;make&lt;/code&gt;to&lt;code&gt;soong&lt;/code&gt;many years ago, pushing developers to migrate from Android.mk to Android.bp, and has started blocking Android.mk in many locations of the source tree.&lt;list rend="ul"&gt;&lt;item&gt;While converting these is a seemingly simple task - in practice it involved countless hours of converting conditional checks, regression testing, and thousands of individual patches. As of this writing, LineageOS introduces less than 10 Android.mk files at a platform level, and many of these are in the process of being converted. In short - we’re ready for when Google kills support for Android.mk.&lt;/item&gt;&lt;item&gt;0xCAFEBABE also created a build target for converting from mk to bp, a WIP feature to assist developers in migrating to soong!&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Both the Charging Control, and Fast Charge Control features have received many updates and improvements.&lt;/item&gt;
      &lt;item&gt;A new set of ringtones and alarms from Plasma Mobile have been included.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Application Updates&lt;/head&gt;
    &lt;p&gt;LineageOS isn’t just about the OS itself: our suite of core apps continues to evolve as well. This cycle brings some major improvements:&lt;/p&gt;
    &lt;head rend="h4"&gt;Aperture (Camera)&lt;/head&gt;
    &lt;p&gt;Our camera app, Aperture, has been rewritten from the ground up. The rewrite makes the app much easier to maintain, while also bringing new features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for JPEG Ultra HDR, RAW, and simultaneous RAW+JPEG capture.&lt;/item&gt;
      &lt;item&gt;A redesigned notification island with dynamic colors, and new indicators (JPEG Ultra HDR, RAW, battery, thermal throttling).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Keep your system updated (or keep updating the app if you’re using the app standalone), since we plan to introduce more features and improvements over time (believe it or not, nowadays the only obstacle is Google’s CameraX library, which has slowed down the development of certain components which we use in Aperture). We do have some plans to overcome this though.&lt;/p&gt;
    &lt;head rend="h4"&gt;Twelve (Music Player)&lt;/head&gt;
    &lt;p&gt;Our music player, Twelve, didn’t need a full overhaul this year, but it did get some polish and some new features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Added a “Play random songs” button for quicker library playback.&lt;/item&gt;
      &lt;item&gt;Updated the Now Playing screen with playback statistics (for the nerds and audiophiles out there).&lt;/item&gt;
      &lt;item&gt;Added the ability to rescan the local media store, so newly added music shows up without needing a reboot.&lt;/item&gt;
      &lt;item&gt;Expanded Jellyfin integration, including suggestions, favorites, and better thumbnail handling.&lt;/item&gt;
      &lt;item&gt;Added MIDI playback support.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;New App, Again?&lt;/head&gt;
    &lt;p&gt;We’re excited to debut Catapult, our brand-new custom launcher for Android TV. Catapult is built with the same principles we bring to the rest of LineageOS: clean, simple, functional design, with thoughtful user experience at its core.&lt;/p&gt;
    &lt;p&gt;Why build a new launcher? For years, Android TV and Google TV users have been stuck with preloaded launchers that aggressively push advertising and recommendations users can’t control. Catapult changes that. It strips away the clutter, gives you back your home screen, and lets you decide what belongs front and center.&lt;/p&gt;
    &lt;p&gt;With Catapult, you get a fast, intuitive interface that focuses on your apps and your content: no forced feeds, no “sponsored” rows, just a launcher that works the way you expect.&lt;/p&gt;
    &lt;p&gt;We’re also planning to add more features in the future, you’ll see them appear as you keep your device up to date, stay tuned!&lt;/p&gt;
    &lt;head rend="h3"&gt;Extended QEMU-Based Virtual Machine Support&lt;/head&gt;
    &lt;p&gt;LineageOS has long been a favorite for developers and tinkerers, and with 23.0 we’ve expanded support for virtualized environments. Thanks to extended QEMU integration spearheaded by developer 0xCAFEBABE, it’s now easier than ever to run LineageOS in VMs for testing, debugging, or just exploration. This means developers can spin up consistent environments on their desktops without needing dedicated hardware, and testers can reproduce tricky issues with greater reliability. Whether you’re validating patches or just curious to see how LineageOS runs under the hood, the tooling is smoother and more accessible.&lt;/p&gt;
    &lt;p&gt;If interested, take a look on the Wiki. You can run LineageOS via libvirt on Linux/Windows, and on an Apple Silicon Mac with UTM.&lt;/p&gt;
    &lt;p&gt;Additionally, LineageOS now supports Cuttlefish build configurations, which are similar to the &lt;code&gt;emulator&lt;/code&gt; family of targets, but has extra
emulated peripherals, so as to act more like a real device! You can view a list of all the differences
here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mainline Kernel Support&lt;/head&gt;
    &lt;p&gt;Another big milestone in 23.0 is our improved support for devices running Linux mainline kernels.&lt;/p&gt;
    &lt;p&gt;While Android has historically relied on heavily modified vendor kernels, the ecosystem is shifting toward mainline for long-term stability and maintainability.&lt;/p&gt;
    &lt;p&gt;With 23.0, developer 0xCAFEBABE has spearheaded an effort to extend compatibility for devices capable of booting the mainline Linux kernel, and we’ve made it easier for maintainers to bring their devices closer to upstream with inheritable common trees. The end result? For now, nothing, but in the future, we will hopefully be able to boot Android on almost any device that is supported by the mainline Linux kernel.&lt;/p&gt;
    &lt;p&gt;This effort should help keep devices alive well past the point where their proprietary components stop working with newer Android releases.&lt;/p&gt;
    &lt;p&gt;See the following repos if interested! (1, 2, 3, 4)&lt;/p&gt;
    &lt;head rend="h3"&gt;Careful Commonization&lt;/head&gt;
    &lt;p&gt;Several of our developers have worked hard on SoC-specific common kernels to base on that can be merged on a somewhat regular basis to pull in the latest features/security patches to save maintainers additional effort.&lt;/p&gt;
    &lt;p&gt;Go check them out and consider basing your device kernels on them!&lt;/p&gt;
    &lt;p&gt;Supported SoCs right now are:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;SoC (system-on-chip)&lt;/cell&gt;
        &lt;cell role="head"&gt;Kernel Version&lt;/cell&gt;
        &lt;cell role="head"&gt;Android Version&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm MSM8998/MSM8996&lt;/cell&gt;
        &lt;cell&gt;4.4&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm MSM8953&lt;/cell&gt;
        &lt;cell&gt;4.9&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm SDM845&lt;/cell&gt;
        &lt;cell&gt;4.9&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm SM8150&lt;/cell&gt;
        &lt;cell&gt;4.14&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15, 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm SDM660&lt;/cell&gt;
        &lt;cell&gt;4.19&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15, 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm SM8250&lt;/cell&gt;
        &lt;cell&gt;4.19&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15, 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm SM8350&lt;/cell&gt;
        &lt;cell&gt;5.4&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15, 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm SM8450&lt;/cell&gt;
        &lt;cell&gt;5.10&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15, 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm SM8550&lt;/cell&gt;
        &lt;cell&gt;5.15&lt;/cell&gt;
        &lt;cell&gt;13, 14, 15, 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qualcomm SM8650&lt;/cell&gt;
        &lt;cell&gt;6.1&lt;/cell&gt;
        &lt;cell&gt;14, 15, 16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qualcomm SM8750&lt;/cell&gt;
        &lt;cell&gt;6.6&lt;/cell&gt;
        &lt;cell&gt;15, 16&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Moreover, many legacy devices require interpolating libraries that we colloquially refer to as “shims” - these have long been device and maintainer managed, but this cycle we have decided to commonize them to make the effort easier on everyone and not duplicate effort!&lt;/p&gt;
    &lt;p&gt;You can check it out here and contribute shims that you think other devices may need or add additional components to additional shims and compatibility layers provided via Gerrit!&lt;/p&gt;
    &lt;head rend="h3"&gt;Deprecations&lt;/head&gt;
    &lt;p&gt;Overall, we feel that the 23.0 branch has reached feature and stability parity with 22.2 and is ready for initial release.&lt;/p&gt;
    &lt;p&gt;We will allow new LineageOS 21 submissions to be forked to the organization, but we will no longer allow newly submitted LineageOS 21 devices to ship.&lt;/p&gt;
    &lt;p&gt;LineageOS 23.0 will launch building for a decent selection of devices, with additional devices to come as they meet the requirements specified by the Charter and are made ready for builds by their maintainer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Upgrading to LineageOS 23.0&lt;/head&gt;
    &lt;p&gt;To upgrade, please follow the upgrade guide for your device by clicking on it here and then on “Upgrade to a higher version of LineageOS”.&lt;/p&gt;
    &lt;p&gt;If you’re coming from an unofficial build, you need to follow the good ole’ install guide for your device, just like anyone else looking to install LineageOS for the first time. These can be found at the same place here by clicking on your device and then on “Installation”.&lt;/p&gt;
    &lt;p&gt;Please note that if you’re currently on an official build, you DO NOT need to wipe your device, unless your device’s wiki page specifically dictates otherwise, as is needed for some devices with massive changes, such as a repartition.&lt;/p&gt;
    &lt;head rend="h3"&gt;Download portal&lt;/head&gt;
    &lt;p&gt;While it has been in the making for quite a while and already released two years ago, it’s still relevant to this blog post. Our download portal has been redesigned and gained a few functional improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dark mode&lt;/item&gt;
      &lt;item&gt;Downloads of additional images (shown for all devices but not used on all of them, read the instructions to know which ones you need for your device’s installation!)&lt;/item&gt;
      &lt;item&gt;Verifying downloaded files (see here) - if you go with any download not obtained from us, you can still verify it was originally signed by us and thus untampered with&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A reminder: Follow the LineageOS Wiki to the letter! If an image is listed on the download portal that the wiki doesn’t ask you to use, ignore it! It is likely there so the maintainer can point power-users at it if needed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Wiki&lt;/head&gt;
    &lt;p&gt;The LineageOS Wiki has also been expanded over the years and now offers, in addition to the known and tested instructions for all supported devices, some improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The device overview allows filtering for various attributes you might be interested in a device (please note that choosing a device only based on that list still does not guarantee any device support beyond the point of when you chose it)&lt;/item&gt;
      &lt;item&gt;The device overview now lists variants of a device and other known marketing names in a more visible way, also allowing for different device information and instructions per variant to be shown&lt;/item&gt;
      &lt;item&gt;The installation instructions have been paginated, giving users less chance to skip a section involuntarily&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition to that we’d like to take this time to remind users to follow instructions on their device’s respective Wiki Page given the complexity introduced by AOSP changes like System-As-Root, A/B Partition Scheme, Dynamic Partitions, and most recently Virtual A/B found on the Pixel 5 and other devices launching with Android 11, the instructions many of you are used to following from memory are either no longer valid or are missing very critical steps. As of 16.0, maintainers have been expected to run through the full instructions and verify they work on their devices. The LineageOS Wiki was recently further extended, and maintainers were given significantly more options to customize their device’s specific installation, update, and upgrade instructions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Developers, Developers, Developers&lt;/head&gt;
    &lt;p&gt;Or, in this case, maintainers, maintainers, maintainers. We want your device submissions!&lt;/p&gt;
    &lt;p&gt;If you’re a developer and would like to submit your device for officials, it’s easier than ever. Just follow the instructions here.&lt;/p&gt;
    &lt;p&gt;The above also applies to people looking to bring back devices that were at one point official but are no longer supported - seriously - even if it’s not yet completely compliant, submit it! Maybe we can help you complete it.&lt;/p&gt;
    &lt;p&gt;After you submit, within generally a few weeks, but in most cases a week, you’ll receive some feedback on your device submission; and if it’s up to par, you’ll be invited to our communications instances and your device will be forked to LineageOS’s official repositories.&lt;/p&gt;
    &lt;p&gt;Don’t have the knowledge to maintain a device, but want to contribute to the platform? We have lots of other things you can contribute to. For instance, our apps suite is always looking for new people to help improve them, or you can contribute to the wiki by adding more useful information &amp;amp; documentation. Gerrit is always open for submissions! Once you’ve contributed a few things, send an email to devrel(at)lineageos.org detailing them, and we’ll get you in the loop.&lt;/p&gt;
    &lt;p&gt;Also, if you sent a submission that didn’t get a response in the last few months, please follow up, we’ve swapped providers again!&lt;/p&gt;
    &lt;head rend="h3"&gt;Generic Targets&lt;/head&gt;
    &lt;p&gt;We’ve talked about these before, but these are important, so we will cover them again.&lt;/p&gt;
    &lt;p&gt;Although we’ve had buildable generic targets since 2019, to make LineageOS more accessible to developers, and really anyone interested in giving LineageOS a try, we’ve documented how to use them in conjunction with the Android Emulator/Android Studio!&lt;/p&gt;
    &lt;p&gt;Additionally, similar targets can now be used to build GSI in mobile, Android TV configurations, and Android Automotive making LineageOS more accessible than ever to devices using Google’s Project Treble. We won’t be providing official builds for these targets, due to the fact the user experience varies entirely based on how well the device manufacturer complied with Treble’s requirements, but feel free to go build them yourself and give it a shot!&lt;/p&gt;
    &lt;p&gt;Please note that Android 12 (and by proxy all later Android versions) diverged GSI and Emulator targets. Emulator targets reside in &lt;code&gt;lineage_sdk_$arch&lt;/code&gt;, while GSI targets reside in
&lt;code&gt;lineage_gsi_$arch&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Additionally, experimental targets now exist for QEMU-based virtual machine software (libvirt, UTM, etc). Instructions on building and utilizing these targets can be found on the Wiki.&lt;/p&gt;
    &lt;head rend="h3"&gt;Translations&lt;/head&gt;
    &lt;p&gt;Bilingual? Trilingual? Anything-lingual?&lt;/p&gt;
    &lt;p&gt;If you think you can help translate LineageOS to a different language, jump over to our wiki and have a go! If your language is not supported natively in Android, reach out to us on Crowdin and we’ll take the necessary steps to include your language. For instance, LineageOS is the first Android custom distribution that has complete support for the Welsh (Cymraeg) language thanks to its community of translators.&lt;/p&gt;
    &lt;p&gt;Please, contribute to translations only if you are reasonably literate in the target language; poor translations waste both our time and yours.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build roster&lt;/head&gt;
    &lt;head rend="h4"&gt;Added 23 devices&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Device name&lt;/cell&gt;
        &lt;cell role="head"&gt;Wiki&lt;/cell&gt;
        &lt;cell role="head"&gt;Maintainers&lt;/cell&gt;
        &lt;cell role="head"&gt;Moved from&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;F(x)tec Pro¹ X&lt;/cell&gt;
        &lt;cell&gt;pro1x&lt;/cell&gt;
        &lt;cell&gt;BadDaemon, bgcngm, mccreary, npjohnson, qsnc, tdm&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Fairphone 4&lt;/cell&gt;
        &lt;cell&gt;FP4&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Fairphone 5&lt;/cell&gt;
        &lt;cell&gt;FP5&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Google Pixel 4a 5G&lt;/cell&gt;
        &lt;cell&gt;bramble&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Google Pixel 5&lt;/cell&gt;
        &lt;cell&gt;redfin&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Google Pixel 5a&lt;/cell&gt;
        &lt;cell&gt;barbet&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Lenovo Z5 Pro GT&lt;/cell&gt;
        &lt;cell&gt;heart&lt;/cell&gt;
        &lt;cell&gt;themard, optionaltoast&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Lenovo Z6 Pro&lt;/cell&gt;
        &lt;cell&gt;zippo&lt;/cell&gt;
        &lt;cell&gt;Lucchetto, themard, einargednochsson&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola defy 2021&lt;/cell&gt;
        &lt;cell&gt;bathena&lt;/cell&gt;
        &lt;cell&gt;Deivid Ignacio Parra, Francisco Sanchez&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola edge 20&lt;/cell&gt;
        &lt;cell&gt;berlin&lt;/cell&gt;
        &lt;cell&gt;npjohnson, SGCMarkus&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola edge 2021&lt;/cell&gt;
        &lt;cell&gt;berlna&lt;/cell&gt;
        &lt;cell&gt;SyberHexen&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola edge 30 fusion&lt;/cell&gt;
        &lt;cell&gt;tundra&lt;/cell&gt;
        &lt;cell&gt;themard, electimon&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola edge 30 neo&lt;/cell&gt;
        &lt;cell&gt;miami&lt;/cell&gt;
        &lt;cell&gt;marcost2&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola edge 30&lt;/cell&gt;
        &lt;cell&gt;dubai&lt;/cell&gt;
        &lt;cell&gt;themard, sb6596, Demon000&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola edge 40 pro / Motorola moto X40 / Motorola edge+ (2023)&lt;/cell&gt;
        &lt;cell&gt;rtwo&lt;/cell&gt;
        &lt;cell&gt;sgcmarkus, themard&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto e7 plus / Lenovo K12&lt;/cell&gt;
        &lt;cell&gt;guam&lt;/cell&gt;
        &lt;cell&gt;Rajin Gangadharan, Deivid Ignacio Parra&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g 5G - 2024&lt;/cell&gt;
        &lt;cell&gt;fogo&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g power 2021&lt;/cell&gt;
        &lt;cell&gt;borneo&lt;/cell&gt;
        &lt;cell&gt;Syed Fawwaz Hussain (Fazwalrus), Deivid Ignacio Parra&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g stylus 5G (2022)&lt;/cell&gt;
        &lt;cell&gt;milanf&lt;/cell&gt;
        &lt;cell&gt;AnierinBliss&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g stylus 5G&lt;/cell&gt;
        &lt;cell&gt;denver&lt;/cell&gt;
        &lt;cell&gt;Vivekachooz&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g10 / Motorola moto g10 power / Lenovo K13 Note&lt;/cell&gt;
        &lt;cell&gt;capri&lt;/cell&gt;
        &lt;cell&gt;Deivid Ignacio Parra, Sultanahamer&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g200 5G / Motorola Edge S30&lt;/cell&gt;
        &lt;cell&gt;xpeng&lt;/cell&gt;
        &lt;cell&gt;themard, rogers2602&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g30 / Lenovo K13 Pro&lt;/cell&gt;
        &lt;cell&gt;caprip&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g32&lt;/cell&gt;
        &lt;cell&gt;devon&lt;/cell&gt;
        &lt;cell&gt;Dhina17, mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g34 5G / Motorola moto g45 5G&lt;/cell&gt;
        &lt;cell&gt;fogos&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g42&lt;/cell&gt;
        &lt;cell&gt;hawao&lt;/cell&gt;
        &lt;cell&gt;Dhina17, mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g52&lt;/cell&gt;
        &lt;cell&gt;rhode&lt;/cell&gt;
        &lt;cell&gt;Dhina17, mikeioannina, tomoms&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g82 5G&lt;/cell&gt;
        &lt;cell&gt;rhodep&lt;/cell&gt;
        &lt;cell&gt;AnandSuresh02, sevenrock&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g84 5G&lt;/cell&gt;
        &lt;cell&gt;bangkk&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g9 play / Motorola moto g9 / Lenovo K12 Note&lt;/cell&gt;
        &lt;cell&gt;guamp&lt;/cell&gt;
        &lt;cell&gt;DelightReza, Deivid Ignacio Parra&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g9 power / Lenovo K12 Pro&lt;/cell&gt;
        &lt;cell&gt;cebu&lt;/cell&gt;
        &lt;cell&gt;Deivid Ignacio Parra&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nothing Phone (2)&lt;/cell&gt;
        &lt;cell&gt;Pong&lt;/cell&gt;
        &lt;cell&gt;chandu078&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nubia Mini 5G&lt;/cell&gt;
        &lt;cell&gt;TP1803&lt;/cell&gt;
        &lt;cell&gt;ArianK16a, npjohnson&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 11 5G&lt;/cell&gt;
        &lt;cell&gt;salami&lt;/cell&gt;
        &lt;cell&gt;bgcngm&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 12R&lt;/cell&gt;
        &lt;cell&gt;aston&lt;/cell&gt;
        &lt;cell&gt;inferno0230&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 7 Pro / OnePlus 7 Pro (T-Mobile)&lt;/cell&gt;
        &lt;cell&gt;guacamole&lt;/cell&gt;
        &lt;cell&gt;LuK1337, Tortel&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 7&lt;/cell&gt;
        &lt;cell&gt;guacamoleb&lt;/cell&gt;
        &lt;cell&gt;shantanu-sarkar&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 7T / OnePlus 7T (T-Mobile)&lt;/cell&gt;
        &lt;cell&gt;hotdogb&lt;/cell&gt;
        &lt;cell&gt;LuK1337&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 7T Pro&lt;/cell&gt;
        &lt;cell&gt;hotdog&lt;/cell&gt;
        &lt;cell&gt;qsnc&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 8 / OnePlus 8 (T-Mobile)&lt;/cell&gt;
        &lt;cell&gt;instantnoodle&lt;/cell&gt;
        &lt;cell&gt;jabashque&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 8 Pro&lt;/cell&gt;
        &lt;cell&gt;instantnoodlep&lt;/cell&gt;
        &lt;cell&gt;LuK1337&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 8T / OnePlus 8T (T-Mobile)&lt;/cell&gt;
        &lt;cell&gt;kebab&lt;/cell&gt;
        &lt;cell&gt;LuK1337, mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 9 / OnePlus 9 (T-Mobile)&lt;/cell&gt;
        &lt;cell&gt;lemonade&lt;/cell&gt;
        &lt;cell&gt;mikeioannina, tangalbert919, ZVNexus&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 9 Pro / OnePlus 9 Pro (T-Mobile)&lt;/cell&gt;
        &lt;cell&gt;lemonadep&lt;/cell&gt;
        &lt;cell&gt;LuK1337, bgcngm, mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 9R&lt;/cell&gt;
        &lt;cell&gt;lemonades&lt;/cell&gt;
        &lt;cell&gt;mikeioannina&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 9RT&lt;/cell&gt;
        &lt;cell&gt;martini&lt;/cell&gt;
        &lt;cell&gt;mikeioannina, basamaryan&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus Nord CE 2 Lite 5G&lt;/cell&gt;
        &lt;cell&gt;oscaro&lt;/cell&gt;
        &lt;cell&gt;Vivekachooz&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus Nord CE 3 Lite 5G / OnePlus Nord N30 5G&lt;/cell&gt;
        &lt;cell&gt;larry&lt;/cell&gt;
        &lt;cell&gt;Vivekachooz&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus Nord CE4&lt;/cell&gt;
        &lt;cell&gt;benz&lt;/cell&gt;
        &lt;cell&gt;inferno0230&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus Pad 2 Pro / OnePlus Pad 3&lt;/cell&gt;
        &lt;cell&gt;erhai&lt;/cell&gt;
        &lt;cell&gt;LuK1337, bgcngm&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Realme 10 Pro 5G&lt;/cell&gt;
        &lt;cell&gt;luigi&lt;/cell&gt;
        &lt;cell&gt;Vivekachooz&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Realme 9 Pro 5G / Realme 9 5G / Realme Q5&lt;/cell&gt;
        &lt;cell&gt;oscar&lt;/cell&gt;
        &lt;cell&gt;Vivekachooz&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy A21s&lt;/cell&gt;
        &lt;cell&gt;a21s&lt;/cell&gt;
        &lt;cell&gt;DaemonMCR&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy F62 / Samsung Galaxy M62&lt;/cell&gt;
        &lt;cell&gt;f62&lt;/cell&gt;
        &lt;cell&gt;Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Note10 5G&lt;/cell&gt;
        &lt;cell&gt;d1x&lt;/cell&gt;
        &lt;cell&gt;Rocky7842, Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Note10&lt;/cell&gt;
        &lt;cell&gt;d1&lt;/cell&gt;
        &lt;cell&gt;Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Note10+ 5G&lt;/cell&gt;
        &lt;cell&gt;d2x&lt;/cell&gt;
        &lt;cell&gt;Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Note10+&lt;/cell&gt;
        &lt;cell&gt;d2s&lt;/cell&gt;
        &lt;cell&gt;Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy S10 5G&lt;/cell&gt;
        &lt;cell&gt;beyondx&lt;/cell&gt;
        &lt;cell&gt;Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy S10&lt;/cell&gt;
        &lt;cell&gt;beyond1lte&lt;/cell&gt;
        &lt;cell&gt;Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy S10+&lt;/cell&gt;
        &lt;cell&gt;beyond2lte&lt;/cell&gt;
        &lt;cell&gt;Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy S10e&lt;/cell&gt;
        &lt;cell&gt;beyond0lte&lt;/cell&gt;
        &lt;cell&gt;Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy S20 FE / Samsung Galaxy S20 FE 5G&lt;/cell&gt;
        &lt;cell&gt;r8q&lt;/cell&gt;
        &lt;cell&gt;ata-kaner&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Tab A7 10.4 2020 (LTE)&lt;/cell&gt;
        &lt;cell&gt;gta4l&lt;/cell&gt;
        &lt;cell&gt;chrmhoffmann&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Tab A7 10.4 2020 (Wi-Fi)&lt;/cell&gt;
        &lt;cell&gt;gta4lwifi&lt;/cell&gt;
        &lt;cell&gt;chrmhoffmann&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Tab S6 Lite (LTE)&lt;/cell&gt;
        &lt;cell&gt;gta4xl&lt;/cell&gt;
        &lt;cell&gt;haggertk, Linux4&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Tab S6 Lite (Wi-Fi)&lt;/cell&gt;
        &lt;cell&gt;gta4xlwifi&lt;/cell&gt;
        &lt;cell&gt;Linux4, haggertk&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Tab S7 (LTE)&lt;/cell&gt;
        &lt;cell&gt;gts7l&lt;/cell&gt;
        &lt;cell&gt;bgcngm&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Tab S7 (Wi-Fi)&lt;/cell&gt;
        &lt;cell&gt;gts7lwifi&lt;/cell&gt;
        &lt;cell&gt;bgcngm&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Solana Saga&lt;/cell&gt;
        &lt;cell&gt;ingot&lt;/cell&gt;
        &lt;cell&gt;mikeioannina, npjohnson, tomoms&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 1 II&lt;/cell&gt;
        &lt;cell&gt;pdx203&lt;/cell&gt;
        &lt;cell&gt;hellobbn&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 1 III&lt;/cell&gt;
        &lt;cell&gt;pdx215&lt;/cell&gt;
        &lt;cell&gt;hellobbn&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 1 V&lt;/cell&gt;
        &lt;cell&gt;pdx234&lt;/cell&gt;
        &lt;cell&gt;hellobbn&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 10 IV&lt;/cell&gt;
        &lt;cell&gt;pdx225&lt;/cell&gt;
        &lt;cell&gt;LuK1337, jmpfbmx&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 10 Plus&lt;/cell&gt;
        &lt;cell&gt;mermaid&lt;/cell&gt;
        &lt;cell&gt;LuK1337&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 10 V&lt;/cell&gt;
        &lt;cell&gt;pdx235&lt;/cell&gt;
        &lt;cell&gt;jmpfbmx, LuK1337&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 10&lt;/cell&gt;
        &lt;cell&gt;kirin&lt;/cell&gt;
        &lt;cell&gt;LuK1337&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 5 II&lt;/cell&gt;
        &lt;cell&gt;pdx206&lt;/cell&gt;
        &lt;cell&gt;kyasu, hellobbn&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 5 III&lt;/cell&gt;
        &lt;cell&gt;pdx214&lt;/cell&gt;
        &lt;cell&gt;kyasu, hellobbn&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia 5 V&lt;/cell&gt;
        &lt;cell&gt;pdx237&lt;/cell&gt;
        &lt;cell&gt;kyasu, hellobbn&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia XA2 Plus&lt;/cell&gt;
        &lt;cell&gt;voyager&lt;/cell&gt;
        &lt;cell&gt;LuK1337&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia XA2 Ultra&lt;/cell&gt;
        &lt;cell&gt;discovery&lt;/cell&gt;
        &lt;cell&gt;LuK1337&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sony Xperia XA2&lt;/cell&gt;
        &lt;cell&gt;pioneer&lt;/cell&gt;
        &lt;cell&gt;LuK1337, jmpfbmx&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi 12 Pro&lt;/cell&gt;
        &lt;cell&gt;zeus&lt;/cell&gt;
        &lt;cell&gt;Adrianyyy, ArianK16a&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi 12&lt;/cell&gt;
        &lt;cell&gt;cupid&lt;/cell&gt;
        &lt;cell&gt;Adrianyyy, ArianK16a&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi 12S Pro&lt;/cell&gt;
        &lt;cell&gt;unicorn&lt;/cell&gt;
        &lt;cell&gt;Adrianyyy, ArianK16a&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi 12S Ultra&lt;/cell&gt;
        &lt;cell&gt;thor&lt;/cell&gt;
        &lt;cell&gt;Adrianyyy, ArianK16a&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi 12S&lt;/cell&gt;
        &lt;cell&gt;mayfly&lt;/cell&gt;
        &lt;cell&gt;Flower Sea&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi 12T Pro / Xiaomi Redmi K50 Ultra&lt;/cell&gt;
        &lt;cell&gt;diting&lt;/cell&gt;
        &lt;cell&gt;Adrianyyy, ArianK16a&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi 13 Pro&lt;/cell&gt;
        &lt;cell&gt;nuwa&lt;/cell&gt;
        &lt;cell&gt;Minus&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi 13&lt;/cell&gt;
        &lt;cell&gt;fuxi&lt;/cell&gt;
        &lt;cell&gt;lolipuru&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi 9T / Xiaomi Redmi K20 (China) / Xiaomi Redmi K20 (India)&lt;/cell&gt;
        &lt;cell&gt;davinci&lt;/cell&gt;
        &lt;cell&gt;ArianK16a&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi A3&lt;/cell&gt;
        &lt;cell&gt;laurel_sprout&lt;/cell&gt;
        &lt;cell&gt;Skyblueborb&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi POCO F5 (Global) / Xiaomi POCO F5 (India) / Xiaomi Redmi Note 12 Turbo&lt;/cell&gt;
        &lt;cell&gt;marble&lt;/cell&gt;
        &lt;cell&gt;Adrianyyy, ArianK16a&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi POCO F5 Pro / Xiaomi Redmi K60&lt;/cell&gt;
        &lt;cell&gt;mondrian&lt;/cell&gt;
        &lt;cell&gt;Adrianyyy, ArianK16a&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Redmi 12C / Xiaomi Redmi 12C NFC / Xiaomi POCO C55&lt;/cell&gt;
        &lt;cell&gt;earth&lt;/cell&gt;
        &lt;cell&gt;surblazer&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Redmi 3S / Xiaomi Redmi 3X / Xiaomi Redmi 4 (India) / Xiaomi Redmi 4X / Xiaomi Redmi Note 5A Prime / Xiaomi Redmi Y1 Prime&lt;/cell&gt;
        &lt;cell&gt;Mi8937&lt;/cell&gt;
        &lt;cell&gt;0xCAFEBABE&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Redmi 4A / Xiaomi Redmi 5A / Xiaomi Redmi Note 5A Lite / Xiaomi Redmi Y1 Lite&lt;/cell&gt;
        &lt;cell&gt;Mi8917&lt;/cell&gt;
        &lt;cell&gt;0xCAFEBABE&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Redmi 7A / Xiaomi Redmi 8 / Xiaomi Redmi 8A / Xiaomi Redmi 8A Dual&lt;/cell&gt;
        &lt;cell&gt;Mi439&lt;/cell&gt;
        &lt;cell&gt;0xCAFEBABE&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Redmi K60 Pro&lt;/cell&gt;
        &lt;cell&gt;socrates&lt;/cell&gt;
        &lt;cell&gt;WenHao2130&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Redmi Note 10 Pro / Xiaomi Redmi Note 10 Pro (India) / Xiaomi Redmi Note 10 Pro Max (India)&lt;/cell&gt;
        &lt;cell&gt;sweet&lt;/cell&gt;
        &lt;cell&gt;basamaryan, danielml3&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Redmi Note 10S / Xiaomi Redmi Note 10S NFC / Xiaomi Redmi Note 10S Latin America / Xiaomi POCO M5s&lt;/cell&gt;
        &lt;cell&gt;rosemary&lt;/cell&gt;
        &lt;cell&gt;surblazer&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Redmi Note 7 Pro&lt;/cell&gt;
        &lt;cell&gt;violet&lt;/cell&gt;
        &lt;cell&gt;0xCAFEBABE&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Xiaomi Redmi Note 8 / Xiaomi Redmi Note 8T&lt;/cell&gt;
        &lt;cell&gt;ginkgo&lt;/cell&gt;
        &lt;cell&gt;Skyblueborb, mikeioannina, programminghoch10&lt;/cell&gt;
        &lt;cell&gt;22.2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Added 22.2 devices&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Device name&lt;/cell&gt;
        &lt;cell role="head"&gt;Wiki&lt;/cell&gt;
        &lt;cell role="head"&gt;Maintainers&lt;/cell&gt;
        &lt;cell role="head"&gt;Moved from&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ASUS ZenFone 8&lt;/cell&gt;
        &lt;cell&gt;sake&lt;/cell&gt;
        &lt;cell&gt;DD3Boh, mikooomich&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ASUS Zenfone 5Z (ZS620KL)&lt;/cell&gt;
        &lt;cell&gt;Z01R&lt;/cell&gt;
        &lt;cell&gt;ThEMarD&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;LG Style3&lt;/cell&gt;
        &lt;cell&gt;style3lm&lt;/cell&gt;
        &lt;cell&gt;rtx4d&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;LG Velvet&lt;/cell&gt;
        &lt;cell&gt;caymanslm&lt;/cell&gt;
        &lt;cell&gt;rtx4d&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g 5G / Motorola moto one 5G ace&lt;/cell&gt;
        &lt;cell&gt;kiev&lt;/cell&gt;
        &lt;cell&gt;basamaryan, Jleeblanch, SyberHexen, Vivekachooz&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Motorola moto g 5G plus / Motorola moto one 5G&lt;/cell&gt;
        &lt;cell&gt;nairo&lt;/cell&gt;
        &lt;cell&gt;ItsVixano, Ivanmeler, SyberHexen, zlewchan&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nokia 6.1 (2018)&lt;/cell&gt;
        &lt;cell&gt;PL2&lt;/cell&gt;
        &lt;cell&gt;npjohnson&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nokia 7 plus&lt;/cell&gt;
        &lt;cell&gt;B2N&lt;/cell&gt;
        &lt;cell&gt;Tuan Anh&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nokia 8&lt;/cell&gt;
        &lt;cell&gt;NB1&lt;/cell&gt;
        &lt;cell&gt;Tuan Anh&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nothing Phone (1)&lt;/cell&gt;
        &lt;cell&gt;Spacewar&lt;/cell&gt;
        &lt;cell&gt;zlewchan, ko_ko_konb&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nubia Red Magic 5G (Global) / Nubia Red Magic 5G (China) / Nubia Red Magic 5S (Global) / Nubia Red Magic 5S (China)&lt;/cell&gt;
        &lt;cell&gt;nx659j&lt;/cell&gt;
        &lt;cell&gt;zlewchan&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nubia Red Magic Mars&lt;/cell&gt;
        &lt;cell&gt;nx619j&lt;/cell&gt;
        &lt;cell&gt;Cyborg2017, rtx4d&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nubia Z17&lt;/cell&gt;
        &lt;cell&gt;nx563j&lt;/cell&gt;
        &lt;cell&gt;BeYkeRYkt&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nubia Z18&lt;/cell&gt;
        &lt;cell&gt;nx606j&lt;/cell&gt;
        &lt;cell&gt;Cyborg2017, rtx4d&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 12&lt;/cell&gt;
        &lt;cell&gt;waffle&lt;/cell&gt;
        &lt;cell&gt;chandu078&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 5&lt;/cell&gt;
        &lt;cell&gt;cheeseburger&lt;/cell&gt;
        &lt;cell&gt;qsnc&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus 5T&lt;/cell&gt;
        &lt;cell&gt;dumpling&lt;/cell&gt;
        &lt;cell&gt;qsnc&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus Nord N20&lt;/cell&gt;
        &lt;cell&gt;gunnar&lt;/cell&gt;
        &lt;cell&gt;tangalbert919&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OnePlus Nord N200&lt;/cell&gt;
        &lt;cell&gt;dre&lt;/cell&gt;
        &lt;cell&gt;tangalbert919&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy A52 4G&lt;/cell&gt;
        &lt;cell&gt;a52q&lt;/cell&gt;
        &lt;cell&gt;Simon1511&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy A52s 5G&lt;/cell&gt;
        &lt;cell&gt;a52sxq&lt;/cell&gt;
        &lt;cell&gt;Simon1511&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy A72&lt;/cell&gt;
        &lt;cell&gt;a72q&lt;/cell&gt;
        &lt;cell&gt;Simon1511&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy A73 5G&lt;/cell&gt;
        &lt;cell&gt;a73xq&lt;/cell&gt;
        &lt;cell&gt;Simon1511&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy M52 5G&lt;/cell&gt;
        &lt;cell&gt;m52xq&lt;/cell&gt;
        &lt;cell&gt;Simon1511&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Samsung Galaxy Tab A 8.0 (2019)&lt;/cell&gt;
        &lt;cell&gt;gtowifi&lt;/cell&gt;
        &lt;cell&gt;lifehackerhansol&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Vsmart Joy 3 / Vsmart Joy 3+&lt;/cell&gt;
        &lt;cell&gt;casuarina&lt;/cell&gt;
        &lt;cell&gt;Tuan Anh, nhglong&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Black Shark&lt;/cell&gt;
        &lt;cell&gt;shark&lt;/cell&gt;
        &lt;cell&gt;rtx4d, tdrkDev&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi MIX Fold 2&lt;/cell&gt;
        &lt;cell&gt;zizhan&lt;/cell&gt;
        &lt;cell&gt;Adrianyyy, ArianK16a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi 11i / Xiaomi Redmi K40 Pro / Xiaomi Redmi K40 Pro+ / Xiaomi Mi 11X Pro&lt;/cell&gt;
        &lt;cell&gt;haydn&lt;/cell&gt;
        &lt;cell&gt;ikeramat&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi 8 Explorer Edition&lt;/cell&gt;
        &lt;cell&gt;ursa&lt;/cell&gt;
        &lt;cell&gt;bgcngm&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi 8 Pro&lt;/cell&gt;
        &lt;cell&gt;equuleus&lt;/cell&gt;
        &lt;cell&gt;bgcngm&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi 8&lt;/cell&gt;
        &lt;cell&gt;dipper&lt;/cell&gt;
        &lt;cell&gt;infrag&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi MIX 2S&lt;/cell&gt;
        &lt;cell&gt;polaris&lt;/cell&gt;
        &lt;cell&gt;bgcngm&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi MIX 3&lt;/cell&gt;
        &lt;cell&gt;perseus&lt;/cell&gt;
        &lt;cell&gt;bgcngm, rtx4d&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi Mi Note 10 / Xiaomi Mi Note 10 Pro / Xiaomi Mi CC9 Pro&lt;/cell&gt;
        &lt;cell&gt;tucana&lt;/cell&gt;
        &lt;cell&gt;SanyaPilot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi POCO F1&lt;/cell&gt;
        &lt;cell&gt;beryllium&lt;/cell&gt;
        &lt;cell&gt;bgcngm, warabhishek&lt;/cell&gt;
        &lt;cell&gt;22.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi POCO X3 NFC&lt;/cell&gt;
        &lt;cell&gt;surya&lt;/cell&gt;
        &lt;cell&gt;Shimitar, TheStrechh, ikeramat&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Xiaomi POCO X3 Pro&lt;/cell&gt;
        &lt;cell&gt;vayu&lt;/cell&gt;
        &lt;cell&gt;SebaUbuntu&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ZTE Axon 9 Pro&lt;/cell&gt;
        &lt;cell&gt;akershus&lt;/cell&gt;
        &lt;cell&gt;rtx4d, tdrkDev&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553835</guid><pubDate>Sat, 11 Oct 2025 23:53:17 +0000</pubDate></item><item><title>Show HN: Rift – A tiling window manager for macOS</title><link>https://github.com/acsandmann/rift</link><description>&lt;doc fingerprint="b650c0c0f935c118"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple layout styles &lt;list rend="ul"&gt;&lt;item&gt;Tiling (i3/sway-like)&lt;/item&gt;&lt;item&gt;Binary Space Partitioning (bspwm-like)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Menubar icon that shows all of the workspaces and the layouts within&lt;/item&gt;
      &lt;item&gt;MacOS-style mission control that allows you to visually navigate between workspaces&lt;/item&gt;
      &lt;item&gt;Focus follows the mouse with auto raise&lt;/item&gt;
      &lt;item&gt;Drag windows over one another to swap positions&lt;/item&gt;
      &lt;item&gt;Performant animations (as seen in the demo)&lt;/item&gt;
      &lt;item&gt;Switch to next/previous workspace with trackpad gestures (just like native macOS)&lt;/item&gt;
      &lt;item&gt;Hot reloadable configuration&lt;/item&gt;
      &lt;item&gt;Interop with third-party programs (ie Sketchybar) &lt;list rend="ul"&gt;&lt;item&gt;Requests can be made to rift via the cli or the mach port exposed (lua client here)&lt;/item&gt;&lt;item&gt;Signals can be sent on startup, workspace switches, and when the windows within a workspace change. These signals can be sent via a command(cli) or through a mach connection&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Does not require disabling SIP&lt;/item&gt;
      &lt;item&gt;Works with “Displays have separate Spaces” enabled (unlike all other major WMs)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Get up and running via the wiki: &lt;/p&gt;
    &lt;p&gt;Rift is in active development but is still generally stable. There is no official release yet; expect ongoing changes.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Issues and PRs are very welcome.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Aerospace worked well for me, but I missed animations and the ability to use fullscreen on one display while working on the other. I also prefer leveraging private/undocumented APIs as they tend to be more reliable (due to the OS being built on them and all the public APIs) and performant. for more on why rift exists and what rift thrives to do, see the manifesto&lt;/p&gt;
    &lt;p&gt;Rift began as a fork (and is licensed as such) of glide-wm but has since diverged significantly. It uses private APIs reverse engineered by yabai and other projects. It is not affiliated with glide-wm or yabai.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45553995</guid><pubDate>Sun, 12 Oct 2025 00:22:15 +0000</pubDate></item><item><title>Coral Protocol: Open infrastructure connecting the internet of agents</title><link>https://arxiv.org/abs/2505.00749</link><description>&lt;doc fingerprint="49bea48bc7dd808e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Multiagent Systems&lt;/head&gt;&lt;p&gt; [Submitted on 30 Apr 2025 (v1), last revised 17 Jul 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Coral Protocol: Open Infrastructure Connecting The Internet of Agents&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Coral Protocol is an open and decentralized collaboration infrastructure that enables communication, coordination, trust and payments for The Internet of Agents. It addresses the growing need for interoperability in a world where organizations are deploying multiple specialized AI agents that must work together across domains and vendors. As a foundational platform for multi-agent AI ecosystems, Coral establishes a common language and coordination framework allowing any agent to participate in complex workflows with others. Its design emphasizes broad compatibility, security, and vendor neutrality, ensuring that agent interactions are efficient and trustworthy. In particular, Coral introduces standardized messaging formats for agent communication, a modular coordination mechanism for orchestrating multi-agent tasks, and secure team formation capabilities for dynamically assembling trusted groups of agents. Together, these innovations position Coral Protocol as a cornerstone of the emerging "Internet of Agents," unlocking new levels of automation, collective intelligence, and business value through open agent collaboration.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Önder Gürcan [view email]&lt;p&gt;[v1] Wed, 30 Apr 2025 22:17:13 UTC (765 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 17 Jul 2025 08:34:37 UTC (785 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45555012</guid><pubDate>Sun, 12 Oct 2025 03:41:29 +0000</pubDate></item><item><title>Pipelining in psql (PostgreSQL 18)</title><link>https://postgresql.verite.pro/blog/2025/10/01/psql-pipeline.html</link><description>&lt;doc fingerprint="f799b5d2b8b01ede"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pipelining in psql (PostgreSQL 18)&lt;/head&gt;
    &lt;head rend="h2"&gt;What is pipelining in Postgres?&lt;/head&gt;
    &lt;p&gt;Pipelining is a client-side feature supported by the network protocol that basically consists of not waiting for the results of previously sent queries before sending the next. This increases the throughput in two ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The client, network and server can work in parallel. For instance, the network may transmit the results of the (N-1)th query while the server executes the Nth query and the client sends the (N+1)th query, all this at the same time.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The network is better utilized because successive queries can be grouped in the same network packets, resulting in less packets overall.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pipelining is possible since version 7.4 (released in 2003), which introduced the extended query protocol. But itâs only since 2021, with PostgreSQL 14, that it can be used through libpq, the client-side C library. Since then, some libpq-based drivers like psycopg3 have started to support it.&lt;/p&gt;
    &lt;p&gt;With PostgreSQL 18, released last week, &lt;code&gt;psql&lt;/code&gt;, the command line client
comes equipped with commands to use pipelining in SQL scripts, making it
even more accessible.
While this addition is not part of the highlighted features of that release,
it can provide huge gains in query throughput, as weâre going to see in a simple
test.&lt;/p&gt;
    &lt;head rend="h2"&gt;psql commands&lt;/head&gt;
    &lt;p&gt;The pipeline is started with &lt;code&gt;\startpipeline&lt;/code&gt;, and in the most simple case, followed
by the SQL queries and ended with &lt;code&gt;\endpipeline&lt;/code&gt;.
If intermediate results are needed, we can use &lt;code&gt;\syncpipeline&lt;/code&gt; to force a 
synchronisation point and &lt;code&gt;\getresults&lt;/code&gt; to fetch all results up to that point.
Also, starting a pipeline creates an implicit transaction. If a query fails,
all the changes since the start (or before the last synchronization point) will
be rolled back.&lt;/p&gt;
    &lt;p&gt;If you know about the &lt;code&gt;\;&lt;/code&gt; syntax to group several queries in the same request, there are similarities between this technique and pipelining: theyâre both used to reduce server round-trips and have the same semantics with regard to transactions. In a way, pipelining is the evolution in the extended query protocol of what multi-statement queries (&lt;code&gt;\;&lt;/code&gt; in psql) are in the simple query protocol.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance test&lt;/head&gt;
    &lt;p&gt;Letâs do a simple test where data from devices are imported with &lt;code&gt;INSERT ... ON CONFLICT&lt;/code&gt; queries. Same-device same-date does update the row,
otherwise it inserts a new row.
Note that if we wanted to unconditionally append all rows,
&lt;code&gt;COPY&lt;/code&gt; would be preferable and pipelining not necessary, which is why
the more sophisticated insert-or-update is chosen for that test.&lt;/p&gt;
    &lt;p&gt;The following bash code imports the (random) data, with or without the pipelining depending on a parameter.&lt;/p&gt;
    &lt;code&gt;function import_data
{
  local count=$1  # how many rows?
  local pipeline=$2 # 1 or 0
  local now_ts=$(date +%s)

  (
    echo 'PREPARE s AS insert into events(device, recorded_at, measure)
values($1, to_timestamp($2), $3) on conflict(device,recorded_at) do update set measure=excluded.measure;'
    echo "BEGIN;"
    [[ $pipeline = 1 ]] &amp;amp;&amp;amp; echo "\\startpipeline"
    for i in $(seq 1 $count)
    do
      device=$RANDOM
      secs=$(($now_ts + $RANDOM*50))
      measure=${RANDOM}"."${RANDOM}
      echo "execute s($device, '$secs', $measure);"
    done
    [[ $pipeline = 1 ]] &amp;amp;&amp;amp; echo "\\endpipeline"
    echo "COMMIT;"
  ) | $psql -q -v ON_ERROR_STOP=1
}
&lt;/code&gt;
    &lt;p&gt;Letâs try this with batches of 100, 1000, 5000, 10000, 50000, 100000 rows, with and without pipelining, and compare how fast these batches are processed.&lt;/p&gt;
    &lt;p&gt;Also, since the network speed matters a lot here, letâs try with three typical kinds of network connections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;localhost (ping time ~ 0.04ms): client and server are on the same host.&lt;/item&gt;
      &lt;item&gt;LAN (ping time ~ 1ms): client and server are separated only by an Ethernet 1GB/s switch.&lt;/item&gt;
      &lt;item&gt;WAN (ping time ~ 4ms): the server is reached through a public Internet connection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, each case is run 5 times and we keep only the median time of the runs.&lt;/p&gt;
    &lt;p&gt;On the same host, the pipeline acceleration ranges from 1.5x for the smallest batch size, up to 5x.&lt;/p&gt;
    &lt;p&gt;On a local network connection, the smallest batch size is accelerated by 2.6x, and it goes up to 42x with the bigger sizes.&lt;/p&gt;
    &lt;p&gt;On the slowest network, itâs even more impressive. The acceleration is between 5.4x and 71x !&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;These accelerations show how under-utilized the network is when we send batches of small queries without pipelining: the network packets are like 50 seater buses that ride with only one passenger.&lt;/p&gt;
    &lt;p&gt;In our example, all we have to do to optimize on that front is to add a pair of &lt;code&gt;\startpipeline&lt;/code&gt; and &lt;code&gt;\endpipeline&lt;/code&gt;.
Thatâs because our queries do not depend on the results of previous queries
of the same batch, except in the sense that if one fails, the entire batch fails.&lt;/p&gt;
    &lt;p&gt;Without pipelining, we could still optimize our test by adding many rows to the &lt;code&gt;VALUES&lt;/code&gt; clauses for each query instead of one row per
query. But itâs not easy to find the sweet spot for how many data rows
there needs to be per query, and large queries with thousands of parameters
are not the nicest to handle on the server side.
Also, if the client-side logic is more complicated, for instance
conditionally targeting several tables, running simple statements in
a pipeline while using row-by-row logic might be much easier.&lt;/p&gt;
    &lt;p&gt;The pipelining meta-commands were added in psql version 18, but they do not require PostgreSQL 18 on the server side. For those interested in this feature who canât upgrade their server soon, you can still upgrade to the latest version of psql: itâs backward-compatible as much as possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45555308</guid><pubDate>Sun, 12 Oct 2025 04:46:57 +0000</pubDate></item><item><title>Show HN: I made an esoteric programming language that's read like a spellbook</title><link>https://github.com/sirbread/spellscript</link><description>&lt;doc fingerprint="943a9b8b627ac612"&gt;
  &lt;main&gt;
    &lt;p&gt;an esoteric programming language where code reads like magical incantations from an ancient spellbook. every program is a "spell" written in a "grimoire," so theorhetically, you can write all your code english essay style due to its no newline/indentation requirement.&lt;/p&gt;
    &lt;p&gt;write code that looks like this:&lt;/p&gt;
    &lt;code&gt;begin the grimoire.
summon the power with essence of 7.
conjure ritual named amplify with value to return value multiplied by value.
summon the result with essence of through ritual amplify with power.
inscribe whispers of "the power is amplified: " bound with result.
close the grimoire.
&lt;/code&gt;
    &lt;p&gt;output: &lt;code&gt;the power is amplified: 49&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;variables&lt;/item&gt;
      &lt;item&gt;dynamic typing&lt;/item&gt;
      &lt;item&gt;arrays&lt;/item&gt;
      &lt;item&gt;functions&lt;/item&gt;
      &lt;item&gt;conditionals/loops&lt;/item&gt;
      &lt;item&gt;string manipulation&lt;/item&gt;
      &lt;item&gt;type conversion&lt;/item&gt;
      &lt;item&gt;user input&lt;/item&gt;
      &lt;item&gt;output&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;clone thy repo&lt;/item&gt;
      &lt;item&gt;make sure you have python 3.6 +&lt;/item&gt;
      &lt;item&gt;create a file called &lt;code&gt;&amp;lt;filename&amp;gt;.spell&lt;/code&gt;:&lt;/item&gt;
      &lt;item&gt;then run &lt;code&gt;python spellscript.py your-spell.spell&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;concept&lt;/cell&gt;
        &lt;cell role="head"&gt;spellscript&lt;/cell&gt;
        &lt;cell role="head"&gt;traditional&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;declare variable&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;summon the x with essence of 10&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;x = 10&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;modify variable&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;enchant x with 20&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;x = 20&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;inscribe x&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;print(x)&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;input&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;inquire whispers of "prompt" into x&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;x = input("prompt")&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;string&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;whispers of "text"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"text"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;array&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;collection holding 1 and 2 and 3&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[1, 2, 3]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;if statement&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;if the signs show x equals 5 then&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;if x == 5:&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;loop&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;repeat the incantation 5 times to begin:&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;for i in range(5):&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;function&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;conjure ritual named add with a and b to&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;def add(a, b):&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;return&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;return result&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;return result&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;operation&lt;/cell&gt;
        &lt;cell role="head"&gt;spellscript&lt;/cell&gt;
        &lt;cell role="head"&gt;traditional&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;addition&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a greater by b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a + b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;subtraction&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a lesser by b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a - b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;multiplication&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a multiplied by b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a * b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;division&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a divided by b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a / b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;equals&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a equals b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a == b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;greater than&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a greater than b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a &amp;gt; b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;less than&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a less than b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a &amp;lt; b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;and&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a and b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a and b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;or&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a or b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;a or b&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;not&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;not a&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;not a&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;resources/documentation.md - feature documentation&lt;/item&gt;
      &lt;item&gt;resources/examples - example programs&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;no nested arrays&lt;/item&gt;
      &lt;item&gt;no string indexing (use character arrays)&lt;/item&gt;
      &lt;item&gt;no modulo operator&lt;/item&gt;
      &lt;item&gt;no break/continue in loops&lt;/item&gt;
      &lt;item&gt;no comments&lt;/item&gt;
      &lt;item&gt;no recursion (use iteration)&lt;/item&gt;
      &lt;item&gt;functions must have at least one parameter&lt;/item&gt;
      &lt;item&gt;no null concept&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;idea from the muffin programming language&lt;/item&gt;
      &lt;item&gt;ai was used for debugging some inperpreter issues, which included rituals and conditionals.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45555523</guid><pubDate>Sun, 12 Oct 2025 05:31:00 +0000</pubDate></item><item><title>Show HN: Sober not Sorry – free iOS tracker to help you quit bad habits</title><link>https://sobernotsorry.app/</link><description>&lt;doc fingerprint="bdb918e92c1daff9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Sober not Sorry&lt;/head&gt;&lt;p&gt;Simple Sobriety Tracker&lt;/p&gt;&lt;p&gt;The mindful way to quit bad habits and stay free â one day at a time&lt;/p&gt;&lt;p&gt;Download for iOS&lt;/p&gt;&lt;head rend="h2"&gt;Simplicity first&lt;/head&gt;&lt;p&gt;Sober not Sorry is designed for calm focus. No noise, no pressure â just a clean space to track your journey to freedom&lt;/p&gt;&lt;head rend="h2"&gt;Everything that matters&lt;/head&gt;&lt;p&gt;Counters, achievements, and a clear view of how your health improves â all you need to stay motivated and consistent&lt;/p&gt;&lt;head rend="h2"&gt;Always with you&lt;/head&gt;&lt;p&gt;Elegant home and Lock Screen widgets keep your motivation close â a gentle reminder of how far you've come&lt;/p&gt;&lt;head rend="h2"&gt;Sober not Sorry is clarity. Nothing more.&lt;/head&gt;&lt;p&gt;Free iOS sobriety tracker&lt;/p&gt;Download for iOS&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45555873</guid><pubDate>Sun, 12 Oct 2025 06:46:58 +0000</pubDate></item><item><title>4x faster LLM inference (Flash Attention guy's company)</title><link>https://www.together.ai/blog/adaptive-learning-speculator-system-atlas</link><description>&lt;doc fingerprint="e3dd3bba699a0e85"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AdapTive-LeArning Speculator System (ATLAS): A New Paradigm in LLM Inference via Runtime-Learning Accelerators&lt;/head&gt;
    &lt;p&gt;ATLAS delivers up to 4x faster LLM inference, powered by Together Turboâs latest research.&lt;/p&gt;
    &lt;p&gt;At Together AI, the AIÂ Native Cloud, weâre obsessed with performance. Making large language models faster, cheaper, and more efficient is not a one-trick problem â it requires optimizing along multiple axes. That is the philosophy behind Together Turbo, our suite of inference innovations that draw from research in algorithms, architectures, and modeling recipes. Weâre excited to introduce the AdapTive-LeArning Speculator System (ATLAS), the first speculator of its kind that gives automatic performance improvements without any manual tuning.&lt;/p&gt;
    &lt;p&gt;ATLAS offers a new way of doing speculative decoding â one that dynamically improves at runtime â and it fits seamlessly alongside our other Turbo techniques like the proprietary Together Turbo Speculator or Custom Speculators. But why create an adaptive-learning speculator system?&lt;/p&gt;
    &lt;p&gt;Standard speculators are trained for general workloads. Custom speculators are trained on your specific data, but only for a specific snapshot in time. However, as the workload evolves (codebase grows, traffic patterns shift, request distributions change), even highly customized speculators can fall behind. In contrast, ATLAS evolves automatically with usage, learning from both historical patterns and live traffic to continuously align with the target modelâs behaviors in real time. This means the more you use our inference service, the better ATLAS will perform!Â&lt;/p&gt;
    &lt;p&gt;Built on top of Together Turbo Speculator, ATLAS reaches up to 500 TPS on DeepSeek-V3.1 and up to 460 TPS on Kimi-K2 in a fully adapted scenario â 2.65x faster than standard decoding, outperforming even specialized hardware like Groq (Figure 1).&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Speculative Decoding&lt;/head&gt;
    &lt;p&gt;Speculative decoding is one of the most powerful levers for accelerating inference.2 Instead of having the target model generate every token step by step, a faster speculator (also known as the draft model) proposes multiple tokens ahead, and the target model verifies them in parallel in a single forward pass. The verification process ensures that the quality of the output matches the distribution of non-speculative decoding, while achieving speedups by accepting many tokens at a time.&lt;/p&gt;
    &lt;p&gt;The overall speed is influenced by the acceptance rate $Î±$ (i.e., how often the target model agrees with the drafted tokens from the speculator) and the relative latency $c$ of the draft versus the target. Typically, larger speculators with more parameters yield higher acceptance rates due to their higher capacity but are slower to generate draft tokens. Progress therefore comes from both sides: aligning draft and target models to increase $Î±$ (training objectives, data, and algorithms) and designing draft models/kernels that keep $c$ low while maintaining $Î±$ (sparsity, quantization, lightweight &amp;amp; kernel-efficient architectures). The sweet spot is where a high $Î±$ meets a low $c$, minimizing end-to-end latency.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;p&gt;At Together AI, the Turbo team has developed high-performance speculators that have achieved the worldâs fastest decoding speeds on NVIDIA Blackwell by drawing on advances across architecture, sparsity, algorithms, post-training recipes, and data [1-9]. Weâve built a speculator design and selection framework that determines the optimal speculator architecture (width/depth, lookahead, sparsity/quantization, KV reuse) and a scalable training system that brings up speculators for the largest and most challenging open-source targets quickly and reproducibly (e.g., DeepSeek-V3.1 and Kimi-K2). For instance, while Kimi ships without a ready-to-use speculator, we can train and deploy one rapidly and take Kimi from ~150 TPS out of the box to 270+ TPS on the same hardware and batch settings, while preserving target-model quality (see Figure 1, yellow bars). This pipeline powers Turbo Speculators that deliver state-of-the-art decoding latency, and it sets the stage for what comes next: an Adaptive-Learning Speculator System that adjusts token drafting to the workload in real time.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Introducing Turboâs Adaptive-Learning Speculator System&lt;/head&gt;
    &lt;p&gt;At Together AI, we power a broad range of inference workloads. But todayâs speculative decoding methods are constrained to using a static speculator, trained on a fixed dataset. Once deployed, the speculator cannot adapt, leading to degrading performance if the input distribution evolves. This problem is particularly pronounced in serverless, multi-tenant environments, where input diversity is sky-high. New users continuously arrive, and bring with them unique workloads that the fixed speculator may not have seen during training. Furthermore, these speculators typically use a fixed lookahead, predicting the same number of tokens regardless of the speculatorâs confidence. Put simply, a static speculator cannot keep up.&lt;/p&gt;
    &lt;p&gt;To address these limitations, we designed the Adaptive-Learning Speculative System with two cooperating speculators, as shown in Figure 3:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A heavyweight static speculator trained on a broad corpus that provides strong, general speculation.&lt;/item&gt;
      &lt;item&gt;A lightweight adaptive speculator that allows for rapid, low-overhead updates from real-time traffic, specializing on-the-fly to emerging domains.&lt;/item&gt;
      &lt;item&gt;A confidence-aware controller that chooses which speculator to trust at each step and what speculation lookahead to use, using longer speculations when the speculator has high confidence.Â&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Efficiency Guardrail via Static Speculator. The static Turbo Speculator serves as an always-on speed floor: it is trained on a broad corpus and remains stable across workloads, so TPS does not collapse when traffic shifts or the adaptive path is cold. In ATLAS, we use it to jump-start speed and provide a fail-safe fallbackâif confidence drops or drift is detected, the controller shortens lookahead or routes back to the static path to preserve latency while the adaptive speculator relearns.&lt;/p&gt;
    &lt;p&gt;Customized Speculator vs. Adaptive-Learning. We know from our previous studies that a customized speculator trained on samples from real traffic that mirror expected usage delivers an additional speed boost. The Adaptive-Learning Speculator enables us to be even more customized in real time. For instance, during a vibe-coding session, the adaptive system can specialize a lightweight speculator for the relevant code files being edited and not seen during training, further increasing the acceptance rate and decoding speed. This kind of on-the-fly specialization is hard to achieve with static speculators.&lt;/p&gt;
    &lt;p&gt;Accelerating RL Training. Reinforcement learning (RL) alternates between two phases: (1) a rollout phase, where the current policy generates trajectories and receives rewards, and (2) an update phase, where we use the rewards to update the policy. In practice, rollouts are often the bottleneck, accounting for roughly 70% of total wall-clock time3. In general, because the policy distribution shifts throughout training, static speculators quickly fall out of alignment with the target policy, resulting in sub-optimal throughput.4 ATLAS addresses this by adapting online to the evolving policy and the specific RL domain, maintaining alignment and reducing the overall rollout time. The domain-specific, iterative nature of RL further enables rapid adaptation, yielding sustained and growing speedups. As shown in Figure 4, applying ATLAS to the RL-MATH pipeline produces increasing speedups as training progresses.&lt;/p&gt;
    &lt;p&gt;Built as part of the Turbo optimization suite. The Adaptive-Learning Speculator System is a core component of the broader Turbo optimization suite, where each layer of optimization compounds the benefits of the others. As illustrated in Figure 5, performance progressively improves through near-lossless quantization (calibrated to preserve quality), the Turbo Speculator, and finally the Adaptive-Learning Speculator System. Additional optimizations in the suite include TurboBoost-TTFT (not shown) for reducing time-to-first-token latency, further contributing to end-to-end acceleration.&lt;/p&gt;
    &lt;p&gt;Extreme Peak Efficiency. When the input distribution is narrow and outputs closely echo previously seen tokens, the adaptive system specializes quickly. In this scenario, the controller becomes confident in utilizing more tokens from the lightweight speculator and lengthens lookahead tokens. This yields consistently higher TPS than static or one-off custom speculators can maintain. As shown in Figures 1 and 5, once fully adapted to Arena-Hard traffic, DeepSeek achieves up to 500 tokens per second for batch size 1 on 4 B200 GPUs, delivering a 400% speedup over the FP8 baseline (improvement from 105 TPS to 501 TPS).&lt;/p&gt;
    &lt;p&gt;{{custom-cta-1}}&lt;/p&gt;
    &lt;head rend="h2"&gt;Build the Future of Efficient AI&lt;/head&gt;
    &lt;p&gt;In parallel to making models smarter and more capable, advancements in inference efficiency are just as transformative â because intelligence only matters when you can deliver it swiftly, cost-effectively, and at scale. At Together AI, our Turbo team turns cutting-edge research (algorithms, quantization, sparsity, distillation, architectures, model pruning, and post-training recipes) into production systems that cut costs and unlock entirely new product experiences. If you love turning elegant ideas into billions of faster tokens, obsess over optimizing efficiency-quality frontier, and want your research to land in real usersâ hands quickly, come build with us. Weâre hiring exceptional research scientists and engineers who can push the frontier of efficient AI. Apply to Together and help define how intelligence is deployed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;1. Each benchmark burst contained 32 arena-hard prompts (â 3,000 token completions on average). Since the observed TPS depends on the prompts while speculative decoding is used, we report tokens per second (TPS) as the mean across requests. In the Adaptive-Learning Speculator System scenario, we show the peak speed where the system is fully adapted to Arena Hard traffic. We use TP=4 for DeepSeek-V3.1 and TP=8 for Kimi-K2-0905 on NVIDIA B200. This is different from our previous blog where we used TP=8, EP=2 for DeepSeek.&lt;lb/&gt;2. Fast Inference from Transformers via Speculative Decoding.&lt;lb/&gt;3. DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL&lt;lb/&gt;4. Speculative decoding is typically not beneficial in pure throughput- or compute-bound settings, where GPUs are fully saturated. However, in reinforcement learning (RL) training, the situation can be different. Agent RL training often operates with small batch sizes and CPU-driven environment steps, where each agent waits for model outputs to do the next action. In this regime, endpoint latency (tokens per second per request) becomes the bottleneck. This makes speculative decoding highly applicable for RL training, if there is a suitable and high performant speculator. Faster decoding pipeline can improve CPU utilization and overall sample throughput.Â &lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;p&gt;[1] Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads&lt;/p&gt;
    &lt;p&gt;[2] SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices&lt;/p&gt;
    &lt;p&gt;[3] Ladder-Residual: Parallelism-Aware Architecture for Accelerating Large Model Inference with Communication OverlappingÂ&lt;/p&gt;
    &lt;p&gt;[4] TEAL: Training-Free Activation Sparsity in Large Language Model&lt;/p&gt;
    &lt;p&gt;[5] The Mamba in the Llama: Distilling and Accelerating Hybrid Models&lt;/p&gt;
    &lt;p&gt;[6] SEQUOIA: Scalable and Robust Speculative Decoding&lt;/p&gt;
    &lt;p&gt;[7] Mixture-of-Agents Alignment: Harnessing the Collective Intelligence of Open-Source LLMs to Improve Post-Training&lt;/p&gt;
    &lt;p&gt;[8] Boosting DeepSeek-R1âs Speed with Customized Speculative Decoding&lt;/p&gt;
    &lt;p&gt;[9] DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;LOREM IPSUM&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LOREM IPSUM&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Value Prop #1&lt;/p&gt;
        &lt;p&gt;Body copy goes here lorem ipsum dolor sit amet&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Value Prop #1&lt;/p&gt;
        &lt;p&gt;Body copy goes here lorem ipsum dolor sit amet&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Value Prop #1&lt;/p&gt;
        &lt;p&gt;Body copy goes here lorem ipsum dolor sit amet&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
          &lt;item&gt;Bullet point goes here lorem ipsum Â&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;List Item Â #1&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;List Item Â #1&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;List Item Â #1&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
          &lt;item&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;List Item Â #2&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;List Item #2&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;List Item #3&lt;/p&gt;
        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;article&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45556474</guid><pubDate>Sun, 12 Oct 2025 08:37:01 +0000</pubDate></item><item><title>Why it took 4 years to get a lock files specification</title><link>https://snarky.ca/why-it-took-4-years-to-get-a-lock-files-specification/</link><description>&lt;doc fingerprint="1700b97754a785d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why it took 4 years to get a lock files specification&lt;/head&gt;
    &lt;p&gt;(This is the blog post version of my keynote from EuroPython 2025 in Prague, Czechia.)&lt;/p&gt;
    &lt;p&gt;We now have a lock file format specification. That might not sound like a big deal, but for me it took 4 years of active work to get us that specification. Part education, part therapy, this post is meant to help explain what make creating a lock file difficult and why it took so long to reach this point.&lt;/p&gt;
    &lt;head rend="h1"&gt;What goes into a lock file&lt;/head&gt;
    &lt;p&gt;A lock file is meant to record all the dependencies your code needs to work along with how to install those dependencies.&lt;/p&gt;
    &lt;p&gt;That involves The "how" is source trees, source distributions (aka sdists), and wheels. With all of these forms, the trick is recording the right details in order to know how to install code in any of those three forms. Luckily we already had the &lt;code&gt;direct_url.json&lt;/code&gt; specification that just needed translation into TOML for source trees. As for sdists and wheels, it's effectively recording what an index server provides you when you look at a project's release.&lt;/p&gt;
    &lt;p&gt;The much trickier part is figuring what to install when. For instance, let's consider where your top-level, direct dependencies come from. In &lt;code&gt;pyproject.toml&lt;/code&gt; there's &lt;code&gt;project.dependencies&lt;/code&gt; for dependencies you always need for your code to run, &lt;code&gt;project.optional-dependencies&lt;/code&gt; (aka extras), for when you want to offer your users the option to install additional dependencies, and then there's &lt;code&gt;dependency-groups&lt;/code&gt; for dependencies that are not meant for end-users (e.g. listing your test dependencies).&lt;/p&gt;
    &lt;p&gt;But letting users control what is (not) installed isn't the end of things. There's also the specifiers you can add to any of your listed dependencies. They allow you to not only restrict what versions of things you want (i.e. setting a lower-bound and not setting an upper-bound if you can help it), but also when the dependency actually applies (e.g. is it specific to Windows?).&lt;/p&gt;
    &lt;p&gt;Put that all together and you end up with a graph of dependencies who edges dictate whether a dependency applies on some platform. If you manage to write it all out then you have multi-use lock files which are portable across platforms and whatever options the installing users selects, compared to single-use lock files that have a specific applicability due to only supporting a single platform and set of input dependencies.&lt;/p&gt;
    &lt;p&gt;Oh, and even getting the complete list of dependencies in either case is an NP-complete problem.&lt;/p&gt;
    &lt;p&gt;And it make makes things "interesting", I also wanted the file format to be written by software but readable by people, secure by default, fast to install, and allow the locker which write the lock file to be different from the installer that performs the install (and either be written in a language other than Python).&lt;/p&gt;
    &lt;p&gt;In the end, it all worked out (luckily); you can read the spec for all the nitty-gritty details about &lt;code&gt;pylock.toml&lt;/code&gt; or watch the keynote where I go through the spec. But it sure did take a while to get to this point.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why it took (over) 4 years&lt;/head&gt;
    &lt;p&gt;I'm not sure if this qualifies as the longest single project I have ever taken on for Python (rewriting the import system might still hold that record for me), but it definitely felt the most intense over a prolonged period of time.&lt;/p&gt;
    &lt;p&gt;The oldest record I have that I was thinking about this problem is a tweet from Feb 2019:&lt;/p&gt;
    &lt;head rend="h2"&gt;2019&lt;/head&gt;
    &lt;p&gt;That year there were 106 posts on discuss.python.org about a &lt;code&gt;requirements.txt&lt;/code&gt; v2 proposal. It didn't come to any specific conclusion that I can recall, but it at least got the conversation started.&lt;/p&gt;
    &lt;head rend="h2"&gt;2020&lt;/head&gt;
    &lt;p&gt;The next year, the conversation continued and generated 43 posts. I was personally busy with PEP 621 and the &lt;code&gt;[project]&lt;/code&gt; table in &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;2021&lt;/head&gt;
    &lt;p&gt;In January of 2021 Tzu-Ping Chung, Pradyun Gedam, and myself began researching how other language ecosystems did lock files. It culminated in us writing PEP 665 and posting it in July. That led to 359 posts that year.&lt;/p&gt;
    &lt;p&gt;The goal of PEP 665 was a very secure lock file which partially achieved that goal by only supporting wheels. With no source trees or sdists to contend with, it meant installation didn't involve executing a build back-end which can be slow, be indeterminate, and a security risk simply due to running more code. We wrote the PEP with the idea that any source trees or sdists would be built into wheels out-of-band so you could then lock against those wheels.&lt;/p&gt;
    &lt;head rend="h2"&gt;2022&lt;/head&gt;
    &lt;p&gt;In the end, PEP 665 was rejected in January of 2022, generating 106 posts on the subject both before and after the rejection. It turns out enough people had workflows dependent on sdists that they balked at having the added step of building wheels out-of-band. There was also some desire to also lock the build back-end dependencies.&lt;/p&gt;
    &lt;head rend="h2"&gt;2023&lt;/head&gt;
    &lt;p&gt;After the failure of PEP 665, I decided to try to tackle the problem again entirely on my own. I didn't want to drag other poor souls into this again and I thought that being opinionated might make things a bit easier (compromising to please everyone can lead to bad outcomes when a spec if large and complicated like I knew this would be).&lt;/p&gt;
    &lt;p&gt;I also knew I was going to need a proof-of-concept. That meant I needed code that could get metadata from an index server, resolve all the dependencies some set of projects needed (at least from a wheel), and at least know what I would install on any given platform. Unfortunately a lot of that didn't exist as some library on PyPI, so I had to write a bunch of it myself. Luckily I had already started the journey before with my mousebender project, but that only covered the metadata from an index server. I still needed to be able to read &lt;code&gt;MEtADATA&lt;/code&gt; files from a wheel and do the resolution. The former Donald Stufft had taken a stab at and which I picked up and completed, leading to &lt;code&gt;packaging.metadata&lt;/code&gt;. I then used resolvelib to create a resolver.&lt;/p&gt;
    &lt;p&gt;As such there were only 54 posts about lock files that were general discussion. The key outcome there was trying to lock for build back-ends confused people too much, and so I dropped that feature request from my thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;2024&lt;/head&gt;
    &lt;p&gt;Come 2024, I was getting enough pieces together to actually have a proof-of-concept. And then uv came out in February. That complicated things a bit as it did/planned to do things I had planned to help entice people to care about lock files. I also knew I couldn't keep up with the folks at Astral as I didn't get to work on this full-time as a job (although I did get a lot more time starting in September of 2024).&lt;/p&gt;
    &lt;p&gt;I also became a parent in April which initially gave me a chunk of time (babies for the first couple of months sleep a lot, so if gives you a bit of time). And so in July I posted the first draft of PEP 751. It was based on &lt;code&gt;pdm.lock&lt;/code&gt; (which itself is based on &lt;code&gt;poetry.lock&lt;/code&gt;). It covered sdists and wheels and was multi-use, all by recording the projects to install as a set which made installation fast.&lt;/p&gt;
    &lt;p&gt;But uv's popularity was growing and they had extra needs that PDM and Poetry– the other major participants in the PEP discussions --didn't. And do I wrote another draft where I pivoted from a set of projects to a graph of projects. But otherwise the original feature set was all there.&lt;/p&gt;
    &lt;p&gt;And then Hynek came by with what seemed like an innocuous request about making the version of a listed project optional instead of required (which was done because the version is required in &lt;code&gt;PKG-INFO&lt;/code&gt; in sdists and &lt;code&gt;METADATA&lt;/code&gt; in wheels).&lt;/p&gt;
    &lt;p&gt;Unfortunately the back-and-forth on that was enough to cause the Astral folks to want to scale the whole project back all the way to the &lt;code&gt;requirements.txt&lt;/code&gt; v2 solution.&lt;/p&gt;
    &lt;p&gt;While I understood their reasoning and motivation, I would be lying if I said it wasn't disappointing. I felt we were extremely close up to that point in reaching an agreement on the PEP, and then having to walk back so much work and features did not exactly make me happy.&lt;/p&gt;
    &lt;p&gt;This was covered by 974 posts on discuss.python.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;2025&lt;/head&gt;
    &lt;p&gt;But to get consensus among uv, Poetry, and PDM, I did a third draft of PEP 751. This went back to the set of projects to install, but was single-use only. I also became extremely stringent with timelines on when people could provide feedback as well as what would be required to add/remove anything. At this point I was fighting burn-out on this subject and my own wife had grown tired of the subject and seeing me feel dejected every time there was a setback. And so I set a deadline of the end of March to get things done, even if I had to drop features to make it happen.&lt;/p&gt;
    &lt;p&gt;And in February I thought we had reached and agreement on this third draft. But then Frost Ming, the maintainer of PDM, asked why did we drop multi-use lock files when they thought the opposition wasn't that strong?&lt;/p&gt;
    &lt;p&gt;And so, with another 150 posts and some very strict deadlines for feedback, we managed to bring back multi-use lock files and get PEP 751 accepted-- with no changes! -- on March 31.&lt;/p&gt;
    &lt;head rend="h2"&gt;2 PEPs and 6 years later ...&lt;/head&gt;
    &lt;p&gt;If you add in some ancillary discussions, the total number of posts on the subject of lock files since 2019 comes to over 1.8K. But as I write this post, less than 7 months since PEP 751 was accepted, PDM has already been updated to allow users to opt into using &lt;code&gt;pylock.toml&lt;/code&gt; over &lt;code&gt;pdm.lock&lt;/code&gt; (which shows that the lock file format works and meets the needs of at least one of the three key projects I tried to make happy). Uv and pip also have some form of support.&lt;/p&gt;
    &lt;p&gt;I will say, though, that I think I'm done with major packaging projects (work has also had me move on from working on packaging since April, so any time at this point would be my free time, which is scant when you have a toddler). Between &lt;code&gt;pyproject.toml&lt;/code&gt; and &lt;code&gt;pylock.toml&lt;/code&gt;, I'm ready to move on to the next area of Python where I think I could be the most useful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45556741</guid><pubDate>Sun, 12 Oct 2025 09:21:29 +0000</pubDate></item><item><title>Nostr and ATProto (2024)</title><link>https://shreyanjain.net/2024/07/05/nostr-and-atproto.html</link><description>&lt;doc fingerprint="231dd9512d25a3d4"&gt;
  &lt;main&gt;
    &lt;p&gt;This post could’ve been titled “Nostr vs ATProto”, but that really isn’t what I wanted to do here. While I will be comparing and contrasting them a lot, and that’s kind of even the point of writing this, I didn’t want to really pit the two against each other at all, and especially not with the title. I also want to try avoiding commenting on the differences between the communities that have formed on the protocols and their apps, although I definitely will be looking at the philosophical differences between the two a lot - also kind of the point of writing this. This also isn’t a super deep technical post, though it assumes familiarity with technical concepts. I also might come back to edit parts of it and add more later.&lt;/p&gt;
    &lt;p&gt;You can read and leave comments on this post here on Bluesky, or here on Nostr, or even here on Mastodon.&lt;/p&gt;
    &lt;p&gt;So I wrote a paragraph mostly about what this post isn’t about, with a little bit about what I will talk about in it, but I haven’t really explained what this post is, or why I’m writing it. Honestly, I’m not completely sure of the first one yet either; I’m figuring that out as I write it. The paragraph at the top are really serving as guidelines for myself as I write this.&lt;/p&gt;
    &lt;p&gt;However, I can explain how this post came to be. It started with a showerthought (I was literally in the shower) about how similar ATProto and Nostr really are. This thought came to me after ruminating on ATProto Relays and Nostr Relays, and thinking about how my favorite feature of Nostr Relays (spoiler: it’s filtering) could be added to ATProto Relays, and why you would want to do that. More broadly, this made me think that the two protocols are similar enough that they are likely to slowly converge over time as they learn from each other.&lt;/p&gt;
    &lt;p&gt;A direct result of those thoughts (after getting out of the shower, of course) was to search the internet for a good comparison of Nostr and ATProto. A direct result of my failure to find any was this Bluesky skoot (There’s a lot of good replies and thoughts in that thread as well—you probably want to read it before continuing with this post). A direct result of my skooting that was this reply. Before, I’d been tentatively considering writing a purely technical comparison after not finding any, but that reply really set the stage for deciding what I wanted to do in this post.&lt;/p&gt;
    &lt;p&gt;So, to start, let’s look at…&lt;/p&gt;
    &lt;head rend="h2"&gt;How we got here&lt;/head&gt;
    &lt;head rend="h3"&gt;A Caged Bird&lt;/head&gt;
    &lt;head rend="h4"&gt;or, Twitter&lt;/head&gt;
    &lt;p&gt;Twitter here could, in theory, be replaced here by just “Centralized Social Media”, but really it was Twitter that got us here. Both ATProto and Nostr exist because of Twitter - the AT Protocol very directly so, Nostr as a response to “censorship” (real or perceived) on Twitter. ATProto is the result of Bluesky’s original mission - to build a decentralized protocol Twitter could adopt. Post-Elon, who knows if that will ever happen, but, well, that is how it started.&lt;/p&gt;
    &lt;p&gt;Twitter sprang into existence in 2007, as a small, SMS-based service that allowed people to post short status updates - tweets, as they became known. Who knows if it was the first of its kind? Well, it certainly became the most popular. It really was the service that was able to popularize the concept of microblogging. It developed a multitude of subcultures, each with their own unique characteristics, often intersecting with each other in fascinating, unpredictable places and ways. And while Twitter certainly never became as popular as some of its big tech companions, it may have had the greatest cultural impact - it was one of the only places in existence where an average person (you!!) could, say, ratio a presidential candidate or give interesting new details on a story to some famous journalist (I don’t know, I just made those up). Some have said it was the first “global town square”.&lt;/p&gt;
    &lt;p&gt;Over the years of Twitter’s existence, lots of things happened to Twitter. Moderation issues including Donald Trump, authoritarian governments around the world, all sorts of mini community wars and harassment, etc. Twitter, as beautiful as it was, well… kind of sucked, and people drew many different (not mutually exclusive and often overlapping!!) conclusions about why. Some, like Christopher Bouzy of Spoutible, concluded that the platform’s moderation simply wasn’t enough for what the platform had become, and people needed a smaller, more closed space with stricter moderation policies. Others concluded that a global-scale social network is simply an inherently bad idea and people should stick to smaller, more tight-knit communities. But one of the most popular conclusions was that something as important as Twitter - whether you considered it a “global town square” or a place to make connections with your community or Whatever Else - simply could not and should not be controlled by a single corporation. Indeed, this was the conclusion that Twitter themselves came to! This is the conclusion that both ATProto and Nostr are founded upon - the idea of a move from closed, centralized, corporate-owned social platforms to a world of open, decentralized social protocols.&lt;/p&gt;
    &lt;p&gt;But ATProto and Nostr don’t exist in a vacuum. They weren’t the only ones to come to this conclusion. They weren’t even the first. And that brings us to…&lt;/p&gt;
    &lt;head rend="h3"&gt;The Mastodon in the Room&lt;/head&gt;
    &lt;head rend="h4"&gt;or, ActivityPub and the Fediverse&lt;/head&gt;
    &lt;p&gt;⚠️ I am not an expert on ActivityPub. Take everything in this section with a grain of salt. If I get something wrong, please correct me. ⚠️&lt;/p&gt;
    &lt;p&gt;ActivityPub is kind of a big deal in the decentralized social protocols world. It’s not the first, either - it would be extremely hard to really find a first. But it is, at least for now, the largest, and realistically is about to become a lot larger, at least if Meta Threads federates with it.&lt;/p&gt;
    &lt;p&gt;It’s also got an entirely different philosophy to either Nostr or ATProto - while both of the latter are based on a more individualistic approach to decentralization, ActivityPub opted for a more collectivist approach, one that favors tight-knit communities over a global network (that hasn’t stopped people from trying to build global networks with it, though.)&lt;/p&gt;
    &lt;p&gt;(Side-note: I should also mention that whether the Fediverse should focus on smaller communities or mass-interconnection has been a debate even within the Fediverse since right about the beginning, which a lot of the differing viewpoints around this topic explained brilliantly by Evan Podromou. Since Small Fedi seems to be the dominant philosophy shaping the current Fediverse, I’ve mostly focused on Small Fedi when talking about ActivityPub here.)&lt;/p&gt;
    &lt;p&gt;There are many different server implementations of the ActivityPub Spec, each adding their own unique flair to the ecosystem. The most popular of these implementations is Mastodon. ActivityPub is also, like I said above, kind of a big deal in the decentralized social protocols world. Almost everyone working on decentralized protocols after ActivityPub has been forced to acknowledge its existence, draw comparisons to it, and often been bridged to it. In fact, when Jack Dorsey fired off his famous tweet thread announcing Bluesky, he was definitely aware of ActivityPub, given that in a reply to a reply to that thread, he stated “ActivityPub is great.”&lt;/p&gt;
    &lt;p&gt;Because ActivityPub uses a federation model centered around small community servers, it has a lot of the benefits of centralized social media. For example, it makes it relatively easy to support private content, since it’s a push-based protocol - only those whose inboxes you push content to can view it (there’s also an “Everyone” option that makes your content fetchable, I think). This is also why the Fediverse has things like Follow Requests, server-to-server DMs (though your instance admin can view them - ActivityPub kind of assumes you trust them), and real blocks that mostly work.&lt;/p&gt;
    &lt;p&gt;However, many of the more collectivist choices made in ActivityPub were concluded to not be conductive to a “decentralized Twitter”, and both ATProto and Nostr exist in large part because of this. In fact, both ATProto and Nostr strayed from ActivityPub for the same reasons - identity is extremely tied to your initial server. There are good reasons for this, given that ActivityPub is largely used by smaller communities who federate with each other, but it does have an important consequence:&lt;/p&gt;
    &lt;p&gt;Your data is not really portable. You can move accounts to another server, and if your old server is well-behaved it can add a redirect to your new account, which will help automatically transfer your old social connections over to your new account, but this doesn’t include any of your data except your follows and followers, and falls apart if your old server goes offline, is adversarial to you or your current server, or in basically any situation where you can’t get that redirect.&lt;/p&gt;
    &lt;p&gt;There are many other philosophical differences between the ActivityPub camp and the Nostr and ATProto camp, but this one is the most important one, at least in my opinion - both ATProto and Nostr have sections explaining “Why not just go with ActivityPub?” that state this as their primary reason. Both ATProto and Nostr have real account portability by design.&lt;/p&gt;
    &lt;p&gt;Both of these protocols don’t have much in common with ActivityPub, so I won’t talk about ActivityPub too much here. But there is one older protocol that both of them extensively draw inspiration from…&lt;/p&gt;
    &lt;head rend="h3"&gt;Secure Scuttlebutt&lt;/head&gt;
    &lt;p&gt;This is where things start to get pretty interesting. In 2014, a New Zealand programmer named Dominic Tarr was living on a sailboat. As you might assume, such a life includes little internet, and when it comes, in sporadic bursts. Centralized social media, like Twitter, wants you to be connected at all times, scrolling your feed and looking at ads. Tarr didn’t want that. The result? He designed a protocol designed for offline-first, intentional, slow communication, free from Big Tech. Its name? Secure Scuttlebutt.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt uses an append-only log of cryptographically signed messages. Your identity is an Ed25519 keypair and is pretty much tied to a single device. One consequence of this is that, as the Scuttlebutt developer docs themselves acknowledge, “If a user loses their secret key or has it stolen, they will need to generate a new identity, and tell people to use their new one instead.”&lt;/p&gt;
    &lt;p&gt;Because it’s an append-only log, every message must contain a reference to the previous message - a bit like a blockchain. That also means that deletes are straight-up impossible. This is also not necessarily a bad thing, just a trade-off.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt started as a purely peer-to-peer protocol, using a gossip model - in fact, that’s where its name comes from; in sailor-slang, scuttlebutt means “water-cooler gossip”. The first popular Scuttlebutt client was an app called Patchwork, authored by Paul Frazee (keep this guy in mind, he’s gonna be important later), and initially the protocol and client often evolved together, adapting to each other’s needs.&lt;/p&gt;
    &lt;p&gt;By default, when you add to your append-only log, that addition only exists on your device; but the next time you connect to a peer running a Scuttlebutt client, your two clients will sync with each others’ logs, and then verify them against each others’ public keys. And to verify the newest part of a Scuttlebutt log, you need the whole log - this ensures that if someone gets part of your content, they get all of it.&lt;/p&gt;
    &lt;p&gt;But you don’t just sync each others’ content - your clients sync all the logs they have locally. That’s why it’s called the gossip model - once you put out a post, as long as you’re connected to a few peers every once in a while, your post will spread as fast as gossip to the friends of your friends. It usually takes time for that information to spread to everywhere, which keeps the pace of Scuttlebutt life somewhat slow and relaxed, with the most active communities being, again, small and tight-knit. Scuttlebutt is definitely not a global social network. The gossip model was driven by the social graph, allowing users to sync with others based on who they follow and who their connections follow. This mechanism relied on cloud bot users, known as “pubs,” acting as connectors and community hubs.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt syncing took time due to the necessity of syncing all activity. Pubs played a crucial role in facilitating connectivity within the network, ensuring that users could discover others either by sharing a pub or by following users who were connected to them.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt’s evolution was influenced by the desire for decentralized communication, distinct from the centralized nature of platforms like Twitter. It offered an alternative for those seeking intentional, offline-first communication free from the constraints of Big Tech. While initially designed for smaller, tight-knit communities, the ideas and learnings from Scuttlebutt inspired later attempts to build decentralized networks suitable for global networking.&lt;/p&gt;
    &lt;p&gt;So, now the stage is mostly set. Twitter was the first “global town square”, a social network connecting people and ideas worldwide - but not without a myriad of problems, which many concluded were due to its centralized nature. ActivityPub and Scuttlebutt (and others) experimented with decentralizing the social world, mostly with a focus on smaller communities, though as they evolved people tried to make them more suitable for global networking. Neither of them would prove viable for global social networks, but the learnings from them would help develop the next generation of social protocols.&lt;/p&gt;
    &lt;head rend="h3"&gt;Freeing the Bird&lt;/head&gt;
    &lt;head rend="h4"&gt;or, where ATProto and Nostr came from&lt;/head&gt;
    &lt;p&gt;All of this is important background for understanding the motivation behind these two protocols. Twitter started it all by showing us what microblogging at scale - a “global town square” - looks like. It showed us how many problems there are with it, and to some, that the only way to fix them is to remove corporate control. ActivityPub and Scuttlebutt showed us two very different ways of doing so, each with their own major benefits and major drawbacks. But there’s still a long way to go from these experiments, which were largely paving the way in the late 2010s, to where we are now, almost halfway into the third decade of the 21st century. To fill in these gaps, we can start towards the end of the second decade of the 21st century.&lt;/p&gt;
    &lt;p&gt;It wasn’t just people outside Twitter who were aware of the multitude of issues with Twitter - of course Twitter noticed them too. Twitter had started as a much more open company than it was at this point in December of 2019 - over the years, they’d taken, for a variety of reasons, a more centralized path, facing investor pressure for returns, and other such things. Twitter knew that, in the words of founder then-CEO Jack Dorsey, “centralized enforcement of global policy to address abuse and misleading information is unlikely to scale over the long-term without placing far too much burden on people.” Jack and the rest of Twitter drew the same conclusion as ActivityPub and Scuttlebutt had before - corporate control of social media was simply bad for everyone. Twitter was a company full of people who realized the service was just in a shitty position no matter how you looked at it, and who were doing everything in their power to keep things healthy despite it all - and they saw a way out: to build on, or build, an open protocol for a global social network. And for all the reasons we talked about before, about ActivityPub and Scuttlebutt, neither of those protocols were up to the task.&lt;/p&gt;
    &lt;p&gt;So the Bluesky initiative began. The early history of the project is much better documented elsewhere, but one of the most interesting things to come out of it at this early stage was an ecosystem review of existing decentralized protocols. It was authored by a Zcash developer named Jay Graber, who would go on to become CEO of Bluesky. It included contributions from several notable people in the decentralization space, including Christine Lemmer-Webber, co-author of the ActivityPub spec, Paul Frazee of Patchwork (and at the time now working on Beaker Browser and Dat), Whyrusleeping from IPFS, and Rabble of early Twitter (at the time working on planetary.social, a Scuttlebutt client). It lays out the state of numerous decentralized protocols, including ActivityPub and Scuttlebutt, and explains how user discovery, moderation, etc works in each of them.&lt;/p&gt;
    &lt;p&gt;At the end of all this ecosystem review, Bluesky concluded that none of these existing protocols was really suitable for their goal - a decentralized protocol Twitter, a global social network, could run on. So they decided to create their own - ATProto - and incorporated into a Public Benefit LLC to help achieve this goal. And when their initial team was hired, it included none other than Paul Frazee of Patchwork, in addition to Aaron Goldman, a former security engineer at Twitter, and Daniel Holmgren, an engineer with experience building on IPFS.&lt;/p&gt;
    &lt;p&gt;Now, while all of this was happening, a Bitcoin enthusiast under the pseudonym Fiatjaf was working on his own little thing. His idea was a non-peer-to-peer reimagining of Scuttlebutt and what it would take to make a similar protocol usable on a global scale. And on November 7th, 2020, the first basic working code for his idea of “Relays” quietly slipped onto the scene. Nostr’s initial description even cites Scuttlebutt as an inspiration - the main design differences between the two (at a high level) are that Nostr moves from a p2p network, with pubs as an afterthought, to a purely client-relay model, and that Nostr events are all separate units that do not form a chain.&lt;/p&gt;
    &lt;p&gt;His motivation for creating this protocol was, somewhat similarly to Bluesky, problems with Twitter. Bluesky was motivated by the idea that content moderation at scale is impossible to do well, and centralizing it in the hands of a single company was a bad idea. Nostr, meanwhile, views moderation itself as an enemy - as censorship that the protocol should be resistant to. While in reality, even Nostr has ultimately ended up exploring different forms of communal moderation, the primary motivation behind Nostr’s design choices is an idea of extremely high censorship resistance. This implies that the design, rather than optimizing for consistency, should optimize for availability - if someone wants to see your content, they should be guaranteed to be able to get it from somewhere. The protocol design is pretty conducive to this.&lt;/p&gt;
    &lt;p&gt;Both of these efforts were toiling away in the darkness, waiting for their moment in order to replace centralized social media with a decentralized future. Then in late 2022, something remarkable happened. Centralized social media fell prey to one of its prime weaknesses, right where everyone could see, thanks to one very famous billionaire. Elon Musk payed 44 billion dollars for Twitter, released the so-called “Twitter Files”, and Jack Dorsey, who had earlier kicked off the Bluesky initiative with 13 million dollars, put out a little manifesto in response, titled a native internet protocol for social media. Within a few hours, someone responded pointing him to the Nostr protocol, and he grew very interested, soon giving fiatjaf 14 Bitcoin to help fund Nostr development. A few months later, Bluesky launched their reference app for the AT Protocol. About a year later, Jack Dorsey left the Bluesky board, having chosen to focus on Nostr instead, as it aligned with his “free-speech-Bitcoin-vibes” ethos better. This was despite the fact that ATProto basically does everything he wants in a decentralized social protocol, but he prefers the more Bitcoin-y community of Nostr.&lt;/p&gt;
    &lt;p&gt;Okay, so that’s how we got here. Now we’ve arrived, back in the present. Let’s look at…&lt;/p&gt;
    &lt;head rend="h2"&gt;Where we are&lt;/head&gt;
    &lt;p&gt;Both Nostr and ATProto follow a similar pattern: adapting peer-to-peer data models to work in a client-server model (that isn’t quite federation). The peer-to-peer world had to deal with a unique problem: because there were no servers, there was no canonical source for data where you could go to verify its integrity. Thanks to the wonders of modern cryptography, efforts like Scuttlebutt, IPFS, and Dat all were able to use self-certifying data structures that could be verified independently of any third-party authority. A good example of this is a Merkle Tree, which is a data structure that ATProto also uses (be sure to watch that video, it’s very good and explains well why peer-to-peer networks need this).&lt;/p&gt;
    &lt;p&gt;As it turned out, these data structures and their benefits would help solve many of the problems the federated world faces. Specifically, the federated world, while no longer reliant on a single central server, often ends up simply shifting this reliance to smaller centralized servers that are the only canonical source for user data. When done correctly, applying peer-to-peer data models to the server would reduce this reliance and make data more independent of servers, while also allowing the big-world networking that only servers can achieve.&lt;/p&gt;
    &lt;p&gt;This sounds like a perfect solution, but it’s worth mentioning that it does have some important tradeoffs compared to a pure federation approach like ActivityPub’s. For example, while deletes are still possible on both protocols (though rather difficult on Nostr, which you might be able to piece together why), if someone has your data saved from before your deletion, it is much easier to prove that you said it and hold it up as yours than it is on a protocol that doesn’t have you cryptographically sign everything. And since both protocols heavily optimize for public content, things like Direct Messaging become much more difficult - in fact, on Nostr, DMs are public like everything else (their content is encrypted so no one else can read them). In general, trying to keep data private becomes extremely difficult; these protocols have delivery models which both center around the same self-certifying data being replicated in many places so anyone who wants it can get at it. With this, things like blocking other users become basically impossible, since there’s no canonical source to restrict content from.&lt;/p&gt;
    &lt;p&gt;Now let’s look at a few different protocol building blocks and how each protocol handles them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Identity&lt;/head&gt;
    &lt;p&gt;Identity in networks is a difficult problem. Ideally, you want identifiers to be human-meaningful - for example, a Twitter handle. If I see the Twitter handle @jack, I can be fairly sure that that’s Jack Dorsey. You also want them to be secure - only @jack should be able to create a post that says it’s from @jack, and I shouldn’t easily be able to take over the account @jack without gaining access to some kind of key. And you probably also want them to be decentralized, so that @jack isn’t beholden to anyone else to hold his identity, and can move around.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it’s not easy to have all three of these nice properties - Secure, Human-Meaningful, and Decentralized - at once. Almost every system which tries to have all three has to end up compromising on one of them. This trilemna is known as Zooko’s Triangle. As examples:&lt;/p&gt;
    &lt;p&gt;Twitter usernames are secure - I can’t just put out a tweet that looks like it’s from @jack - and human-meaningful - a guy with the handle @jack is probably named Jack. But they’re obviously not decentralized - they are all reliant on Twitter’s servers, and it’s Twitter who decides that @jack points to Jack Dorsey’s account. If they, say, wanted to rebrand to X, and someone was using the @x handle, Twitter could easily take it from them and make their own handle @X.&lt;/p&gt;
    &lt;p&gt;Scuttlebutt, meanwhile, has identity that’s decentralized - it’s just your private key, essentially a random number - and your public key, the part other people can see. It’s also secure - I need to actually have your private key to pretend to be you. But a public key, which is also just a number (derived from your private key), is not very human meaningful.&lt;/p&gt;
    &lt;p&gt;If you’re familiar with ActivityPub, you might argue that ActivityPub usernames are all three. This isn’t really true - ActivityPub usernames behave like Twitter usernames, except instead of just one big central Twitter server deciding what username points to what, this is handled in smaller centralized servers which federate with each other.&lt;/p&gt;
    &lt;p&gt;Nostr and ATProto also experience this problem, and they both share a few views around identity, listed out here so each one corresponds to a side of Zooko’s Triangle:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Your identity should not be permanently tied to a single server - Decentralization&lt;/item&gt;
      &lt;item&gt;Your data should be cryptographically verifiable as coming from your identity - Security&lt;/item&gt;
      &lt;item&gt;There are two “layers” of identity - a permanent computer-oriented one and a changeable human-friendly one - Human-Meaningful.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with these similarities, how that really plays out in both protocols looks extremely different. The idea that your data is cryptographically verifiable as yours implies a keypair somewhere. In Nostr, that’s exactly it - your identity is just a secp256k1 keypair. Nothing more, nothing less.&lt;/p&gt;
    &lt;p&gt;That sounds very much like the permanent computer-oriented layer of identity. So the human-friendly identity is handled by a Nostr event of the profile type - this contains stuff like your bio, display name, and avatar. There’s also NIP-05, which allows using the .well-known/nostr.json path on a domain to get email-style usernames, like &lt;code&gt;jack@cash.app&lt;/code&gt; - and this includes a special case, &lt;code&gt;_@domain&lt;/code&gt;, that gets treated by clients as just &lt;code&gt;@domain&lt;/code&gt;. When you &lt;code&gt;@mention&lt;/code&gt; someone in a Nostr note, it’s just &lt;code&gt;@&amp;lt;their public key&amp;gt;&lt;/code&gt;, which clients then simply display as their display names. Notably, having either a display name or even a real NIP-05 username is completely optional under Nostr, and your public key really is your identity.&lt;/p&gt;
    &lt;p&gt;This looks like mostly a success, at least in terms of taking those views and treating them as criteria. Nostr actually takes the first point - identity should not be permanently tied to a single server - and goes slightly further: in Nostr’s model, where your identity really is just your keypair, no servers are involved in identity at all. Why would you want that? A major benefit of this approach is that if any of the servers involved in the system goes down or is no longer friendly with you, your identity doesn’t even need to be “recovered” - it’s just there, the same as before. This works well with the Nostr Relay model, which we’ll discuss in the next section.&lt;/p&gt;
    &lt;p&gt;The drawbacks of this approach are the same as Scuttlebutt. Thanks to the relay model, your identity is no longer tied to a single client on a single device - you can easily move around, between relays, between clients, between devices. This, by itself, for most people, is a good thing, but it comes with an entirely different kind of problem:&lt;/p&gt;
    &lt;p&gt;Managing a cryptographic keypair is simply not very user-friendly. You simply can’t expect most people to write it down and keep it in a safe place or even take the time to understand what it means. People expect username-password systems, and sure, newer technology like passkeys is actually more secure and potentially easier - but that comes with actual benefits over username-password for most people! Managing a keypair is not only unfriendly, it’s incredibly risky. Since the entirety of your identity is your keypair, and to sign in to Nostr clients is to give them your private key - well, you can probably see where this is going. And again, since your identity is just your keypair, just like with Scuttlebutt, if an attacker gets a hold of your private key, that identity is gone. No longer yours. There’s no-one you can go to for help, no-one who can recover that account, no password reset link.&lt;/p&gt;
    &lt;p&gt;That sounds very negative, but it is worth noting that at least for web Nostr clients, there is a (relatively) good solution to the sign-in problem - NIP-07. In the NIP-07 world, you don’t give every client your private key - you give it once to a browser extension, and then every time a web client wants to do something on your behalf, instead of directly using your private key to sign messages etc, it delegates that to your trusted extension. This is a lot better than giving your private key out to every client that has some cool new feature you want to try. Of course, this doesn’t help with recoverability - if you lose your private key, whether to your memory or to an attacker, it’s still gone. There are attempts to solve this, too, which I’ll talk about in “Where we’re going” because it has interesting future implications.&lt;/p&gt;
    &lt;p&gt;ATProto looks at things a little differently. Because of the aforementioned difficulties involved with users managing their own private keys, Bluesky chose to have your signing keypair live on a server - your Personal Data Server, or PDS. Your PDS is responsible for serving your Data Repository to other services on the network, and serves as more-or-less the canonical source for your content. However, your Repository is fully self-certifiable (that means someone can check whether or not you created the content in a copy of your Repo without needing a third party to verify), and so is not permanently tied to your PDS. This is because your PDS is not the canonical source for your identity - but your identity is also not something as small as a keypair here, and does not live entirely client side.&lt;/p&gt;
    &lt;p&gt;Instead, ATProto uses their own homegrown DID (Decentralized IDentifiers, W3C spec with the aim of helping, well, decentralize identity) method called did:plc, for PLaCeholder. Why is it named “placeholder”? Well, because as of now, it’s centralized. That’s right, the supposedly “Decentralized” Identifier is centralized - and Bluesky actively doesn’t want it to be that way. did:plc was initially intended to be a placeholder until a decentralized method was able to meet their requirements - “a strongly consistent, highly available, recoverable, and cryptographically secure method with fast and cheap propagation of updates”. did:plc has all of these at one major cost - it’s centralized. However, the data in a did:plc is self-certifying (you don’t need to trust/rely on plc.directory to verify the information), so it’s conceivable for it to become more decentralized in the future. (You can also use a did:web, which removes this centralization but forces you to manage everything yourself and relies permanently on your control of a web host on a domain, thus removing most of PLC’s benefits. This is pretty niche, so I won’t talk about it in detail here.)&lt;/p&gt;
    &lt;p&gt;A did:plc: contains two public keys - your rotation key and your signing key. This signing key is the aforementioned key that the PDS uses to sign your data. The rotation key is important because it manages your did:plc: and thus is needed to sign updates to your DID document, such as when migrating PDSes. The canonical source for your current PDS, valid signing key, handle, and rotation keys (which can also be rotated) are all your DID document. In this way, a DID serves as a “Theseus Identity”, an idea Aaron Goldman laid out well in this YouTube video.&lt;/p&gt;
    &lt;p&gt;The canonical source of your identity is your DID doc, and all the information in it, i.e. your handle and current PDS must be a two-way connection - your handle is a domain with a dns txt record or ./well-known/atproto-did that must point to your DID, providing two way verification, and whatever PDS your DID document points to must actually have your account on it. Meanwhile, the PDS handles data, and implements a standard, user-friendly login system, and signs your updates with your key on the server side.&lt;/p&gt;
    &lt;p&gt;Here, there was a trade-off between principles of security, recoverability, and user-friendliness, and a principle of max-decentralization - low-friction identity, with no centralizing points of control at all, extreme takeover resistance. Notice that&lt;/p&gt;
    &lt;p&gt;Where ATProto chooses user-friendliness, Nostr chooses max-decentralization. This is a trend that repeats in many other parts of each protocol’s design, as we’ll see.&lt;/p&gt;
    &lt;head rend="h3"&gt;Data&lt;/head&gt;
    &lt;p&gt;In the traditional federated world of protocols like ActivityPub, there had never been much of an emphasis on data, and the formats and structures it’s stored in. The federated world thought much more about how servers should communicate messages rather than how they should store data - this difference is laid out well by Bryan Newbold, who incidentally now works on protocol design at Bluesky. This emphasis on communication standards rather than data standards is a big part of why there’s no standard “fediverse repo” that you can transfer between servers, and other such problems in the federated world.&lt;/p&gt;
    &lt;p&gt;The peer-to-peer world, as we looked at earlier, couldn’t afford to define pure transport protocols - they had to design standardized data structures that were self-certifying and self-contained. An example of such a data structure is a blockchain, and indeed, the peer-to-peer community and the blockchain community learned much more from each other than either of them and federation did from each other.&lt;/p&gt;
    &lt;p&gt;This was the status quo until ATProto and Nostr came along and broke the mold by bringing these self-certifying data structures into the client-server world. They both use asymmetric cryptography to make this data self-certifying, but the similarities basically end there.&lt;/p&gt;
    &lt;p&gt;In the Nostr model, servers are dumb. They have basically one job - transmit data. There’s only one kind of server in Nostr - a Relay, and a Relay does only three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Receive data to store&lt;/item&gt;
      &lt;item&gt;Return that data when asked for it&lt;/item&gt;
      &lt;item&gt;Provide a continuous stream of the data being placed on that Relay&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notably, Relays store data. Data is placed on Relays. All this data is created on the client-side. Relays don’t manage identity or any of that. Your keys live with your client, and it’s your client who signs your &lt;code&gt;events&lt;/code&gt; (a piece of data in Nostr terminology.) When you fetch data from a Relay, it comes back with signatures and all - which, guess what, your client verifies. Your client almost operates under the assumption that Relays will try to do weird stuff, people will submit fake events, etc - and so Nostr removed the requirement of trust, by making clients verify everything themselves. A trade-off!&lt;/p&gt;
    &lt;p&gt;Nostr, by optimizing for censorship resistance, needs to remove as much rigidity from its design as possible. Data needs to be cheap to create and transmit and store. So Nostr events all exist as individual units following a fixed JSON format with a strict signing convention. Unlike Scuttlebutt, these events don’t need to form a chain - they are purely self-contained. Like your identity, there’s no canonical source for them either - by design, you’re supposed to be able to get them from pretty much any relay that has them. When you create the event, your client signs it and then just publishes it to as many relays as possible, from where it will circulate into other Relays, consuming clients will republish them, etc. Because they are signed against your public and are fully self-contained, it’s trivial to verify them too, removing the necessity of trust in the Relay you get the event from.&lt;/p&gt;
    &lt;p&gt;ATProto data is also very portable, but it is slightly more rigid than Nostr data is. Instead of using these one-off events which are fully self-certifying, ATProto stores your data as records in what it calls a repo. These records live under a collection like &lt;code&gt;app.bsky.feed.post&lt;/code&gt; and are given an &lt;code&gt;rkey&lt;/code&gt; (record key). Together, this forms a URI for any given record that looks like &lt;code&gt;at://did/collection/rkey&lt;/code&gt;. Importantly, records are mutable, unlike nostr events, and the contents an at:// uri points to may change. However, all the commits to your repo, which contain changes like record creation, editing, and deletion, are content-addressed using a CID, and these are immutable, and are all signed using your repo’s signing key (the one from your DID doc, remember?) Your commits can also optionally form a chain if you want, but when they don’t, deletes are easier. (If all of that flew over your head, don’t worry. All you need to know is that ATProto allows deletes and edits, while Nostr can’t.) Because your data all lives in this repo, unlike Nostr, ATProto actually has a canonical source for your data.&lt;/p&gt;
    &lt;p&gt;There’s also a single place where your repo lives, instead of being scattered as a bunch of events across Relays like in Nostr. Your repo lives in your Personal Data Server - as the name implies, a PDS is designed to store your personal data. While Nostr Relays are dumb pipes, PDSes are more like a user agent, which really performs almost all actions on the user’s behalf. It’s responsible for signing and storing commits to your repo and wrapping them in a nice API that’s easy for clients to use.&lt;/p&gt;
    &lt;p&gt;Actually, we should probably take a minute just to talk about deletes and edits. When I said Nostr can’t allow deletes and edits, that wasn’t completely true: Nostr does have a way to request deletes from Relays, which most but not all Relays support, but the real trouble is figuring out what a delete even means (and edits are straight-up impossible since Nostr event IDs are fully content-addressed). Nostr’s model is fundamentally based on an idea of events flowing from the creator into Relays, which then flow into other people’s clients, which cache them and republish them to other Relays, and so on. An event doesn’t have a location to be deleted from - it could be (and in Nostr’s model, should be!) anywhere and everywhere.&lt;/p&gt;
    &lt;p&gt;In ATProto, your repo actually has a place where it lived - your PDS, as specified in your DID doc. And at:// uris are mutable, so a commit can actually change the content it points to. Deletes remove content from your repo - although anybody who has a copy of your content pre-delete will still have it and can very easily cryptographically prove that it’s your content.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trust&lt;/head&gt;
    &lt;p&gt;Nostr and ATProto have relatively similar approaches to trust, though with some important differences. Nostr trusts nobody, and is built accordingly, with clients verifying everything themselves. ATProto assumes you trust somebody, but lets you choose whom you trust, and provides the mechanisms needed to verify that trust is placed correctly (although this could be improved).&lt;/p&gt;
    &lt;p&gt;Nostr, as mentioned earlier, was designed to basically eliminate the necessity of trust in the first place. Because everything is verified client-side, and essentially functions as a bunch of self-authenticated units of data traveling between relays and clients, there really is no one to trust. Relays can choose not to carry content, but other relays might have them instead. However, the fact that all data moves as individual units means that it would be harder to spot if only certain events are available.&lt;/p&gt;
    &lt;p&gt;Since every user is assumed to be pointing their client at more than one relay, it doesn’t really matter if one relay chooses not to carry someone’s content; there’s a high likelihood another one is. If many relays agree to hide something from the network, then it won’t show up, but that’s pretty unlikely to happen. As for trusting the authenticity of the content delivered by the relay, because it’s cryptographically verifiable as coming from the attached pubkey, any shenanigans will be spotted quickly. And verifying a pubkey’s identity is done by attaching it to a trusted NIP-05, i.e. @jack@cash.app or @jb55.com.&lt;/p&gt;
    &lt;p&gt;ATProto isn’t that different, all things considered, but there’s multiple other hops between the source of data and the client you view it in. Each ATProto PDS puts out a cryptographically verifiable stream of commits being pushed to repos on the PDS, carrying every bit of data to the subscribers, called the firehose. Because there are a lot of PDSes, an optimization also called a Relay was introduced, which basically aggregates PDS firehoses into its own giant firehose. In a way, this Relay could be considered its own centralization point where bad untrustworthy things could happen, but once more than one Relay exists this should be less of a problem. At the Relay and PDS, everything is cryptographically verifiable, and as a bonus because of ATProto’s repo structure, you can tell if you’re not getting the whole picture.&lt;/p&gt;
    &lt;p&gt;After the Relay, things get a bit murkier, because as an optimization ATProto applications use something called the AppView. The AppView reads in the firehose from the Relay constantly and pieces it together into fully hydrated and speedy APIs which make clients’ lives much easier. The thing about the AppView is that it’s basically centralized, and though it’s not super difficult to spot inconsistencies between what the AppView gives you and the true state of the network, the AppView doesn’t even provide the cryptographic signatures that were passed into it, making its trustworthiness a bit murky at some unknown time in the future, at which point other contenders will hopefully exist to replace it, based on analysis of which one is more trustworthy by comparing the data each AppView gives you with what actually exists on the Relay and PDSes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Privacy&lt;/head&gt;
    &lt;p&gt;Everything is completely public on both protocols and in fact being actively broadcasted to loads of consumers, not just sitting around waiting to be stepped on and found. Nothing you do is really hideable from anyone.&lt;/p&gt;
    &lt;p&gt;However, at least on ATProto, there have been attempts to add some semblance of privacy to the network. For example, there are AppView-enforced blocks, but they can be bypassed very easily. There is also a setting which asks the client to not show your posts to logged-out users, but this is superficial at best, since only some clients really follow it anyways, and the “official” popular client does so it does kind of work. But overall these measures both run a risk of making people feel like their posts and other activity are hidden and safe, lulling them into acting with less precaution than they should, especially since there is a lack of user awareness around the all-public nature of data on the network.&lt;/p&gt;
    &lt;p&gt;No such attempts have been made on Nostr. This is on the one hand unfortunate, but on the other hand possibly better since it is more honest about the true nature of how public everything is on the network.&lt;/p&gt;
    &lt;head rend="h3"&gt;Development&lt;/head&gt;
    &lt;p&gt;Due to the Bluesky devs’ past experiences with developing on peer-to-peer and federated protocols, many of them felt burnt by a Scuttlebutt-and-Nostr-style approach to development, where specifications were loose and implementations varied wildly. Because of these past experiences, Bluesky chose to go with a slightly more slow, intentional, and centralized development model. The protocol is mostly developed within Bluesky the company, though often adapts to the needs and feedback from the wider ATProto developer community, and community members often contribute to both the protocol and the clients. The rollout of core features like federation and stackable moderation has also been much more slow on ATProto than similar features in Nostr implementations, because in general Bluesky prefers to take their time and “get it right” and standardized before letting things out into the wild. Also, despite the existence of third-party clients, the “official” Bluesky app and service is still the most popular one by a huge margin, due to its being the default (and basically only) inroad into the protocol and ecosystem. There are other up-and-coming AT Protocol projects that aren’t just Twitter clones, like WhiteWind for blogging, but overall the ecosystem remains sparse compared to Nostr.&lt;/p&gt;
    &lt;p&gt;Nostr, meanwhile, takes the same approach as these previous projects - the protocol itself just exists, very small, letting anyone expand on it. When an extension wants to become standardized, it’s reviewed by a small team including fiatjaf and a few others, and becomes part of the NIPs repository (Nostr Implementation Possibilities). This is basically classic BDFL open-source. However, clients and relays are free to try their own wild things without being “official” NIPs, and any NIP proposal must be adopted by a few clients and relays before it can be considered for “official” status. So it’s a much wilder, freer ecosystem so far.&lt;/p&gt;
    &lt;head rend="h3"&gt;Applications&lt;/head&gt;
    &lt;p&gt;One of the places where ATProto and Nostr differ greatly is their model for building applications.&lt;/p&gt;
    &lt;p&gt;ATProto takes the AppView approach. An AppView is basically a service that reads in the firehose of all the public data on the network, and indexes it into hydrated “views” as an API which clients then use. AppViews are pretty resource-intensive to run and functionally centralized in nature. If you want to make a new ATProto app, you first design your schemas for content in a DSL called Lexicon. Then you make a client that can start publishing your record type, and retrieving and displaying it. For the retrieval and displaying, you create an AppView which monitors the firehose for your record types and indexes them into hydrated views, which your client can then fetch from and display nicely and neatly. This is, for example, how the Bluesky app can show a list of users who liked a post; because instead of the client having to crawl the entire network itself and figure out which likes are for the post you just viewed and then get the DID and fetch each of that user’s profiles and whether or not you’re following them by checking your own repo, and whether or not they’re following you by looking all over their follow lists, the client just makes one HTTP request and makes the result human-readable. Nice and fast. Of course, the relief that comes to the client means a lot of responsibility is thrusted onto the AppView, which becomes very resource-intensive to run.&lt;/p&gt;
    &lt;p&gt;The first steps to the Nostr model look similar at first, but rapidly diverge. With Nostr, you also start with defining event kinds, and then creating a client which can publish them, and then adding fetching and displaying. The key difference is in how events are fetched. With ATProto, you write an AppView to do the heavy lifting; with Nostr, the heavy lifting is shared between the Relay and the Client. When defining your event kinds, you make sure to also define how to use the “tags” field for that event kind, which is an array of key-value pairs with single letter keys which are indexed by the relays the events are sent to. Basically, if you want to do any kind of linking between events, or inserting any kind of indexable data, that’s where you want to do it.&lt;/p&gt;
    &lt;p&gt;Then for the fetching of the data, we use Nostr’s filtering system. With Nostr, there are two kinds of communication between the client and the relays; publishing events, which pushes the signed client-created event into the relay’s data store, and subscription. Subscription is the interesting part we’re looking at here.&lt;/p&gt;
    &lt;p&gt;Nostr clients can request a subscription to a stream of events from the relays they’re connected to, and this stream subscription can have filters attached. A filter is fully specified using the following attributes, all optional:&lt;/p&gt;
    &lt;code&gt;{
  "ids": &amp;lt;a list of event ids&amp;gt;,
  "authors": &amp;lt;a list of lowercase pubkeys, the pubkey of an event must be one of these&amp;gt;,
  "kinds": &amp;lt;a list of a kind numbers&amp;gt;,
  "#&amp;lt;single-letter (a-zA-Z)&amp;gt;": &amp;lt;a list of tag values, for #e — a list of event ids, for #p — a list of pubkeys, etc.&amp;gt;,
  "since": &amp;lt;an integer unix timestamp in seconds, events must be newer than this to pass&amp;gt;,
  "until": &amp;lt;an integer unix timestamp in seconds, events must be older than this to pass&amp;gt;,
  "limit": &amp;lt;maximum number of events relays SHOULD return in the initial query&amp;gt;
}
&lt;/code&gt;
    &lt;p&gt;By adding multiple filters, you can get all the events matching any of the filters. By adding multiple attributes to a single filter, you add multiple conditions that all have to be fulfilled for events to make it through that filter. Filters are expressly the mechanism for fetching content, since subscriptions are supposed to start by backfilling everything that meets the criteria, and then pushing any new events that meet the filters’ requirements to the client.&lt;/p&gt;
    &lt;p&gt;By studying the filter specification, it’s clear that basically every behavior of ATProto AppViews can be recreated through filters on the client-side, knowing how tags allow extensibility as well. There’s an obvious cost though: clients must be very complex and do a lot of work themselves, and for big events duplicating a lot of effort that could be handled by something akin to an AppView. The benefit of this is that it is very generic and means that any relay can generally be used for any functionality since everything you need is baked into the core protocol, and the speed of development is basically only constrained by the client, and not an AppView. And by not spending any resources on building a giant indexer yourself, you basically shift the cost onto the Relays instead. It’s another example of the more “bazaar” philosophy of Nostr compared to a more “cathedral” approach from ATProto.&lt;/p&gt;
    &lt;p&gt;So, all in all, this gives a pretty good picture of where the two protocols are now. But exciting things are on the horizon for both. We’re heading into uncharted territory…&lt;/p&gt;
    &lt;head rend="h2"&gt;Where we’re going&lt;/head&gt;
    &lt;p&gt;When Jack Dorsey wrote a native internet protocol for social media, he wrote that “As far as the free and open social media protocol goes, there are many competing projects: @bluesky is one with the AT Protocol, nostr another, Mastodon yet another, Matrix yet another…and there will be many more. One will have a chance at becoming a standard like HTTP or SMTP.”&lt;/p&gt;
    &lt;p&gt;That’s one way of thinking about it, as a competition for the final spot of “the standard for social”. But as you’ve probably noticed from reading this post up to here, I don’t really agree with this viewpoint. ATProto, Nostr, ActivityPub, Scuttlebutt, Matrix, IPFS, Dat, Holepunch, and others all share similar goals, yet have vastly different perspectives about how to accomplish them. Maybe these different perspectives will all lose! Maybe, as Jack says, one of them will win, becoming a standard that everyone adopts. Or maybe they will all learn from each other and slowly begin to converge. And it’s not hard to make the case that that last possibility will happen for at least two of these protocols - of course, Nostr and ATProto. In fact, that’s already happening.&lt;/p&gt;
    &lt;head rend="h3"&gt;Convergence&lt;/head&gt;
    &lt;p&gt;Because a lot of core ideas in the protocols were already very similar, they can quite easily borrow ideas from each other in order to improve themselves. By making nearly opposite compromises, they now face roughly opposite problems as well - but often, the other protocol already has a solution waiting for them. So first let’s look at some of the ways Nostr is becoming more like ATProto.&lt;/p&gt;
    &lt;p&gt;First, the idea of keys in a server, instead of purely client-side. As mentioned earlier, one of the dangers of Nostr keys is that by giving them to lots of random clients you try, they might accidentally end up in the hands of bad actors. One of the solutions to this was NIP-07 browser extensions; another one is the idea of an NSecBunker, for Nostr Secret Key Bunker. The idea is that this is a server, similar to a PDS, which holds your Nostr private key, and when your client wants to sign an event, it makes a request to your NSecBunker to sign that event using your private key, which stays safe in your Bunker. These requests usually are authenticated using measures like OAuth. It allows Nostr to bring back at least one part of the user experience people are familiar with.&lt;/p&gt;
    &lt;p&gt;Another idea that Nostr is ending up trying is something similar to AppViews. This is particularly divisive within the community, with many feeling that only the relay-based filtering mechanisms should be used to build clients. But because this is often inefficient, clients like Primal have begun doing their own pre-indexing of many users and posts in order to improve their UX. Unfortunately, Primal’s is proprietary, and only Primal can interact with it, due to the lack of any built-in support for AppView-style services in the Nostr protocol, vs. ATProto’s numerous mechanisms to provide explicit support for this use case.&lt;/p&gt;
    &lt;p&gt;Meanwhile, some Nostr ideas are naturally going to the ATProto world as well. The idea of keys directly owned by the users has long been floated, and at this point developers can get control of their did:plc and its rotationKeys (fun fact: I set one of my plc rotationKeys to my Nostr pubkey). Unfortunately no nice UI exists for this yet. And as for signing keys, with commits that could be pushed to a PDS instead of made there, that would rely on a PDS supporting this use case. No PDS implementation currently supports this, but there is one in development which hopes to at some point ;)&lt;/p&gt;
    &lt;p&gt;Another idea which I hope to see adopted in the ATProto world is something similar to Nostr’s filters model. While the AppView model is nice for production apps, something like Nostr filters could help a lot early in development to just play with an idea and try it out. And it could help those with concerns about the trustworthiness of AppViews quickly verify it against certain queries. You can do a shocking amount with backlinks alone.&lt;/p&gt;
    &lt;p&gt;Of course, the slow convergence of both protocols isn’t the only way the divide between them is being bridged…&lt;/p&gt;
    &lt;head rend="h3"&gt;Bridging&lt;/head&gt;
    &lt;p&gt;Recently, Bridgy Fed started bridging the Fediverse and the ATmospherewith each other. For a while, services like Mostrhave been bridging the Fediverse and Nostr with each other. Now, if you visit the Mostr homepage and scroll down, you can probably see where this is going…&lt;/p&gt;
    &lt;p&gt;Soon after Bridgy Fed started bridging the Fediverse and the ATmosphere, Nostr users experimented with this to bridge between Nostr and Bluesky. Very much an indirect hack, but also a glimpse at the future.&lt;/p&gt;
    &lt;p&gt;One of the most important promises of decentralized social media was that no matter what service you signed up on and post on, you would be able to see content from and interact with anyone, no matter which service they used either. Now, all this would work, if every service signed on to the same decentralized social protocol. However, instead, we have many, and none of them show much of a sign of becoming the singular standard for social media. Instead of Jack’s vision of one winner, bridges offer a vision of a world where every protocol can win, and it truly won’t matter which protocol your service uses, either.&lt;/p&gt;
    &lt;p&gt;While the bridging I talked about above was very indirect, Bridgy Fed itself may soon have native Nostr support. Soon all three major decentralized protocols may be able to talk to each other, and easily too.&lt;/p&gt;
    &lt;p&gt;So. Let’s recap what we’ve been through in this post so far. In the beginning, there was Twitter. Twitter’s problems caused them to look to decentralization as a way to make social media more fair. This caused many new decentralized protocols to emerge, taking inspiration from older ones. Of these new protocols, two of them, Nostr and ATProto, evolved in similar directions, yet unaware of each other made many opposite compromises. And now they are evolving back towards each other, converging in potentially very interesting ways, with bridging offering to make social media not just platform- but protocol-agnostic.&lt;/p&gt;
    &lt;p&gt;The future is looking good for decentralized social media.&lt;/p&gt;
    &lt;p&gt;You can join the conversation on Bluesky here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments from Bluesky:&lt;/head&gt;
    &lt;p&gt;Or on Nostr:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45556763</guid><pubDate>Sun, 12 Oct 2025 09:24:43 +0000</pubDate></item><item><title>Macro Gaussian Splats</title><link>https://danybittel.ch/macro.html</link><description>&lt;doc fingerprint="7bcdcf46dcb0d210"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Macro Splats 2025&lt;/head&gt;
    &lt;p&gt; A Gaussian splat is essentially a bunch of blurry ellipsoids. Each one has a view-dependent color, using a process similar to training an AI model, you can optimize until it converges to the photos you feed in. The result is a sort of 3D photograph that can be viewed freely from any angle. &lt;lb/&gt; Captivated by this possibility, I wanted to create splats of macro subjects. The hairy, fuzzy textures and complex structures of insects make them a perfect fit for this technique. &lt;lb/&gt; In theory, creating a splat is as simple as filming the object from all sides. Unfortunately, the extremely shallow depth of field in macro photography completely throws this process off. If you feed unsharp photos into it, the resulting model will contain unsharp areas as well. &lt;lb/&gt; Thankfully, there’s a common technique in macro photography called focus stacking, where multiple images taken from the same angle but with slightly different focal points are combined into one fully sharp photo. A single stack usually contains anywhere from 50 to 500 images. Since I needed to photograph the subject from many angles, I optimized the process to use as few photos per stack as possible and settled on 16. I shot at a small aperture of f/18 to maximize depth of field. The diffraction introduced by this setup can be minimized later in post. &lt;lb/&gt; To capture the specimen from all angles, covering a bit more than half a hemisphere, I mounted the insect on a rotary disk and tilted the camera up and down on a boom arm. A script rotated the disk by fixed increments, and each focus stack was captured using a WeMacro automated focus rail. The vertical angle was adjusted manually (only eight times), so it wasn’t a big issue. In total, I captured 111 perspectives. A full session of 1776 photos took about four hours. The main bottleneck is my Nikon D810, which isn’t built for such continuous shooting, it slows down to one frame every one or two seconds once the buffer fills up. I used a Tamron 90mm lens with a 20mm extension and shot in DX (cropped sensor) mode. Shorter lenses would change the perspective too much between focus areas, making image alignment impossible. &lt;lb/&gt; After batch focus-stacking all the photos, I ended up with 111 fully sharp images. The camera positions could then be reconstructed in COLMAP. I performed some color correction and background masking before feeding the data into training with Postshot. Out comes the splat, requiring only minimal retouching to remove the mounting. &lt;/p&gt;
    &lt;head rend="h1"&gt;See it in 3D&lt;/head&gt;
    &lt;p&gt; You can view all the insects on my superspl.at page. &lt;lb/&gt; I’m also releasing the cluster fly model for free under a CC BY license: Download here. You’re free to use this model for both commercial and non-commercial purposes, as long as you provide credit. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45556952</guid><pubDate>Sun, 12 Oct 2025 10:08:02 +0000</pubDate></item><item><title>Why Wikipedia cannot claim the Earth is not flat</title><link>https://en.wikipedia.org/wiki/Wikipedia:Why_Wikipedia_cannot_claim_the_Earth_is_not_flat</link><description>&lt;doc fingerprint="b7005a2b3066dd33"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Wikipedia:Why Wikipedia cannot claim the Earth is not flat&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;This is an essay.&lt;/p&gt;&lt;p&gt;It contains the advice or opinions of one or more Wikipedia contributors. This page is not an encyclopedia article, nor is it one of Wikipedia's policies or guidelines, as it has not been thoroughly vetted by the community. Some essays represent widespread norms; others only represent minority viewpoints.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;This page in a nutshell: There are many types of argument that fringe authors (for example, people who think the world is flat) might use to argue that their point is unduly correct. This page describes techniques to recognise and defuse them.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;It is unlikely that you will ever happen upon an editor who will argue that Wikipedia cannot claim that the Earth is not flat. But you may indeed encounter some who will strenuously maintain that a particular "breakthrough", or a "notable" or "controversial" idea, belief, or theory deserves more consideration than it has received in the academic world. Using the Flat Earth example (below) as a metaphor, this essay will examine ten types of arguments commonly used by advocates of fringe concepts and advise the neutrally-minded editor or administrator on how to defuse them.&lt;/p&gt;&lt;head rend="h2"&gt;Policy and practices&lt;/head&gt;[edit]&lt;p&gt;It is the stated goal of Wikipedia to mirror the current consensus of mainstream scholarship – in the words of WP:NOT, "accepted knowledge". Self-evidently, the mainstream view of what is accepted knowledge in a discipline has the largest following and as such the most due weight in the literature. The encyclopedia does not act as an advocate for, or passionately promote, pioneering minority theories that are currently controversial (i.e soapboxing), even if there is a slim chance beliefs on the margin may eventually gain wide consensus (as happened with the proposals of the round Earth in archaic periods and continental drift before the mechanism of plate tectonics, two classic examples of cutting edge views once deemed fringe theories that turned out to be justified). Wikipedia acknowledges diverse viewpoints on contemporary controversies, but represents them in proportion to their prevalence (or due weight) among serious scholars and reporters with reputations of responsibility and reliability. Wikipedia may in some cases limit its mention of theories understood to be fringe to specific articles about those theories, and remove their mention from other articles, per the one way principle.&lt;/p&gt;&lt;p&gt;In summary, Wikipedia is not a soapbox for people to advocate pet points of view, nor is Wikipedia in the business of adjudicating which pet points of view have a potential for subsequent wide acceptance in the future. Some marginal theories are fringe science and some are pseudo-science, but Wikipedia is not in the business of calling the shots as to where these stand except where reliable sources clarify those differences. Thus, Wikipedia is academically conservative, as is fitting for a standard reference work.&lt;/p&gt;&lt;head rend="h3"&gt;Wikipedia's role as a reference work&lt;/head&gt;[edit]&lt;p&gt;The threshold for including material in Wikipedia is that it is verifiable, not merely that we think it is true. That is, readers must be able to check that the material has already been published by a reliable source. Editors should provide a reliable source for quotations and for any material that is challenged or likely to be challenged, or the material may be removed. Verifiability is one of Wikipedia's core content policies.&lt;/p&gt;&lt;p&gt;Therefore, Wikipedia is not worried per se about whether the theory that the Earth is flat is true. There must be current, reliable and independent sources substantiating claims that the Earth is flat. But there are no such sources[1] that are current (almost no scientists have thought the Earth was flat since about the fourth century BC), that are reliable (reliable sources are reviewed for accuracy), or independent (a journal published by a Flat Earth Society would not be independent.[2])&lt;/p&gt;&lt;p&gt;If Wikipedia had been available around the sixth century BC, it would have reported the view that the Earth is flat as a fact without qualification. It would have also reported the views of Eratosthenes (who correctly determined the Earth's circumference in 240 BC) either as controversial or a fringe view. Similarly if available in Galileo's time, it would have reported the view that the Sun goes round the Earth as a fact, and if Galileo had been a Vicipaedia editor, his view would have been rejected as "originale investigationis". Of course, if there is a popularly held or notable view that the Earth is flat, Wikipedia reports this view. But it does not report it as true. It reports only on what its adherents believe, the history of the view, and its notable or prominent adherents. Wikipedia is inherently a non-innovative reference work: it stifles creativity and free thought, which is a Good Thing.&lt;/p&gt;&lt;head rend="h3"&gt;Close encounters of the fringe kind&lt;/head&gt;[edit]&lt;p&gt;Occasionally, civic-minded Wikipedia editors must act to mitigate, redesign, and sometimes destroy the offerings of users who think that a particular 'breakthrough' or 'notable' or 'controversial' idea or theory deserves more consideration than it has received in the academic world. Since Wikipedia is an open project that "anyone can edit", good editors don't take such encounters personally. They do not automatically view supporters of fringe theories as "the enemy". They know that sometimes these fallacies are propagated not out of malice, but ignorance. Humans are fallible creatures, and there are many more ways to be wrong than right. Science is stodgy, typically not glamorous, and entails hard work.&lt;/p&gt;&lt;p&gt;By contrast, speculation on "amazing new ideas" is stimulating, easy, and fun. It's more exciting to see yourself as a re-discoverer of ancient truths or in the vanguard of a revolutionary scientific breakthrough. Belonging to a small club with a particular belief can be very fulfilling. The world would be a more exciting place if there were malevolent aliens abducting humans, if dead people could send us messages, if exotic plants were able to miraculously cure all disease, if free energy were readily available to anyone, or if our dreams could foretell the future. In addition, popular culture can often confuse the general public with uncritical or credulous presentations of such concepts on the internet, in books, radio talk shows, TV, news, and films. It's little wonder that Wikipedia attracts individuals who feel the encyclopedia should include sympathetic coverage of these types of subjects.&lt;/p&gt;&lt;head rend="h3"&gt;Dealing with dedicated fringe advocates&lt;/head&gt;[edit]&lt;p&gt;Unfortunately, Wikipedia can attract some extremely dedicated individuals whose aim is to promote pseudoscience, crankery, conspiracy theories, marginal nationalist or historic viewpoints and the like, together with other theories entirely unrecognised by academia. These enthusiasts often edit primarily or entirely on one topic or theme. They attempt to water down language and unreasonably exclude, marginalize or push views beyond the requirements of Neutral point of view, especially by giving undue weight to their preferred theories.&lt;/p&gt;&lt;p&gt;Such grandstanding is forbidden by a variety of Wikipedia policies and guidelines (Verifiability, Neutral point of view, What Wikipedia is not and Fringe theories to name just a few). These policies, correctly understood and correctly used, will successfully exclude non-notable or fringe views. But many dedicated fringe advocates are familiar with these policies, and have become expert at gaming them or even using them against neutrally-minded but inexpert editors. The latter often find their efforts subverted at every step by advocates who revert war over edits, frivolously request citations for obvious or well known information, argue endlessly about the neutral-point-of-view policy and particularly try to undermine the undue weight clause.&lt;/p&gt;&lt;p&gt;This maneuvering and filibustering is soon likely to exhaust the patience of any reasonable person who naturally prefers not to reason with the unreasonable, and who, unlike the advocate, has no special interest or passion other than striving to maintain neutrality. Additionally, by continually engaging fringe advocates in endless argument, you run the risk of turning Wikipedia into a battleground or a debating society. At the present time, Wikipedia does not have an effective means to address superficially polite but tendentious, long-term, fringe advocacy. Some contend that this is a main flaw of Wikipedia; that unlike conventional encyclopedias, fanatics can always get their way if they stay around long enough and make enough edits and reversions.[3] In this sense, Wikipedia's 'commitment to amateurism' does not always work for the best interests of the project.&lt;/p&gt;&lt;head rend="h2"&gt;Ten types of arguments&lt;/head&gt;[edit]&lt;p&gt;Arguments commonly used by fringe advocates to support inclusion of marginal viewpoints against official policies fall into a small number of easily recognizable categories. Here are the top ten approaches that might be used by our allegorical Flat Earth advocate to argue that Wikipedia cannot claim "The Earth is not flat":&lt;/p&gt;&lt;head rend="h3"&gt;1. Personalisation&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Your bias against the Earth being flat is too strong to be objective.&lt;/item&gt;&lt;item&gt;Your arguments against the flat Earth theory so resemble the arguments of editor X that you must be their sockpuppet.&lt;/item&gt;&lt;item&gt;The flat Earth article is being degraded by those who don't like the flat Earth theory.&lt;/item&gt;&lt;item&gt;Ignoring users with differing opinions does nothing to help the further development of this page.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;Personalisation is easily the most common form of attack on neutrally-minded editors. Personalisation is ignoring the basis for inclusion altogether, and making the argument personal. For example, they argue that an editor is biased towards the mainstream, or that editors are ganging up because their arguments are so similar (even though they would be similar – the main argument against the Earth being flat is topographical, and it is hard to argue against it without repeating the argument). Or they may claim that to disagree with an editor with a fringe agenda is uncivil, a personal attack, a violation of Do not bite the newcomers or a violation of Assume good faith. It may even be claimed that sources that disagree with the fringe point of view cannot be used if they reflect poorly on any living people who are proponents of the fringe point of view (such as critical book reviews, etc).&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;p&gt;Ignore any personal attack altogether – and particularly do not make a personal attack yourself, however tempting it may be. Also try to ignore the arguments and reasons used by mainstream science itself. Your opponents will love this and turn the talk page into a battlefield of competing claims and counterclaims. Simply stick to the principles: if mainstream science holds that the Earth is round, and there are reliable sources establishing this as a fact, that is sufficient.&lt;/p&gt;&lt;head rend="h3"&gt;2. Sourcing&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Essex local authorities trained employees in flat Earth theory in 1993.&lt;/item&gt;&lt;item&gt;The statement that the Earth is flat is reliably sourced from Flat Earth magazine, which is peer-reviewed by top flat Earth experts.&lt;/item&gt;&lt;item&gt;There are published sources (including PubMed) that back up the view that people use Flat Earth theory as an adjunct to their existing qualifications and businesses.&lt;/item&gt;&lt;item&gt;How do you explain the results the US Army gets by using techniques that are talked about in Flat Earth literature? If it's a bunch of hogwash, then the TRADOC's results should be in shambles. Instead, we have the most successful, motivated force on the planet.&lt;/item&gt;&lt;item&gt;Since established scientists attended a flat-Earth conference, it follows they take the theory seriously.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;After you have insisted on the use of reliable sources, supporters of the marginal view will then try to exploit the definition of 'reliable source'. They will argue for the inclusion of material of dubious reliability; for example, using commentary from partisan think-tanks rather than from the scientific literature. Occasionally, they will discover that they can get more attention if they make appeals to authority by presenting supporters who have academic credentials. Typical pseudoscience sources include:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Dedicated websites (normally registered under a .com or .org – rarely under .edu though there are occasions where this may be possible)&lt;/item&gt;&lt;item&gt;Dedicated periodicals&lt;/item&gt;&lt;item&gt;Self-published sources&lt;/item&gt;&lt;item&gt;Publications made outside the typical scientific presses&lt;/item&gt;&lt;item&gt;In-house journals (The Flat Earth Institute's Journal not to be confused with academic journals)&lt;/item&gt;&lt;item&gt;Occasional peer-reviewed articles – often in more obscure journals&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;p&gt;The best sources to use when describing fringe theories or determining their notability and prominence are independent reliable sources. It's impossible to write a balanced article or describe a fringe theory in an objective way if the sources being used have a stake in promoting a specific fringe theory. Independent sources are also necessary to determine the relationship of a fringe theory to mainstream scholarly discourse. And arguments for inclusion of fringe theories based on a proponent's credentials alone are unwarranted. Attempts to insert language that showcases a proponent's academic degrees or honorification should be treated as promotionalism.&lt;/p&gt;&lt;head rend="h3"&gt;3. Balance&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You must not say 'the Earth is not flat' but 'according to critics of the flat Earth theory, the Earth is not flat'.&lt;/item&gt;&lt;item&gt;There should be no criticism of the flat Earth theory in the introduction to the article. There is already criticism of the theory in the article, section 94.&lt;/item&gt;&lt;item&gt;So what if the article on flat Earth theory is 250k, and the round Earth article only 8k? The answer is not to fix the balance by writing less about the flat Earth, that only makes Wikipedia worse, but to add more information about the Earth being round.&lt;/item&gt;&lt;item&gt;Is this an encyclopedia for academics or for the general public?&lt;/item&gt;&lt;item&gt;Criticism of the flat Earth theory should be balanced by criticism of the round Earth theory.&lt;/item&gt;&lt;item&gt;The article lead should begin with a pure definition. Criticism should come second, e.g.:"Flat Earth refers to the Earth's flat shape. Skeptics say the Earth is round."&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;Even when supporters of fringe viewpoints recognise the mainstream view as mainstream and established, and agree that Wikipedia may state the mainstream view without qualification, they will still challenge the relative prominence accorded to the mainstream over the fringe viewpoint, and make all sorts of arguments about balance. It is often seriously claimed that the "N" in NPOV (Neutral point of view) means that no negative or critical or mainstream material can appear at all in the article, since it is not neutral, or that Wikipedia is not for advocacy, and so advocates of 'scientific points of view' should not overstate their case.&lt;/p&gt;&lt;p&gt;It is claimed that the reader will not understand the idea unless it is described without criticism, since Wikipedia is an encyclopedia for the general public, not a technical journal. Reversing this argument, they will state that readers are smart enough to know that fringe ideas are nonsense without including any negative or critical material or sources. They will propose that negative material be forked off into another article, or relegated into a "criticism ghetto" or criticism section or removed from the Lead section. They may argue that one must always state the idea first before criticizing it, or that any sources that disagree with the fringe point of view cannot be used since they violate the Neutral point of view.&lt;/p&gt;&lt;p&gt;They may insist that a statement by an advocate of a fringe theory saying they "do not agree that it is a fringe theory" must be included in the article text. They may claim that any critical or negative material cannot appear in an article since it is biased. Or that any negative or critical material is unusable since it is just opinion and not fact. Some of them will even claim that there are no facts, arguing that if a fringe minority, not present in any reliable sources, disagrees with a widely accepted fact, it violates Neutral point of view to state it as a fact in the article. They may demand that every statement of fact should be attributed, no matter how universally accepted.&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;p&gt;Wikipedia's neutrality is often misunderstood as giving equal validity to mainstream and fringe views. But it actually gives the most weight and validity to the mainstream view, as cited in high quality academic sources. The fact that fringe theory promoters do not think their theory is fringe is a given. Every pseudoscience advocate strenuously objects to their avocation being called pseudoscience, it's an unremarkable detail (see WP:MANDY), and unless these objections are notable in WP:FRIND sources, they don't belong in the article. At the root of many of these arguments are intentional (or unintentional) misinterpretations of Neutral point of view, particularly undue weight, although certain kinds of deliberate pettifogging can also be a sign of gaming the system. See #6 below, "Gaming".&lt;/p&gt;&lt;head rend="h3"&gt;4. Conspiracy&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The flat Earth theory has been marginalised by the scientific establishment in order to protect its interests.&lt;/item&gt;&lt;item&gt;Any scientist who tried to study flat Earth theory would lose his research funding. Dissent is being suppressed by the scientific establishment.[4]&lt;/item&gt;&lt;item&gt;Rosencrantz was tremendously rude about scientists who claimed the Earth was round. If the scientific establishment has marginalized him this is not really surprising.&lt;/item&gt;&lt;item&gt;As a professional astronomer you have a clear conflict of interest.&lt;/item&gt;&lt;item&gt;X, Y and Z are hard-line skeptics about flat-Earthism. They often publish in skeptics magazines and take a hard line with any approach to any theory which is not empirically verified.&lt;/item&gt;&lt;item&gt;The scientific establishment (peer-reviewed journals, universities) are trying to suppress the Truth about flat Earth theory; they refuse to allow flat Earth papers at conferences and will not publish flat Earth research.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;The next tactic is to appeal to your ideas about free speech and distrust of censorship and the establishment. All theories that are not generally accepted have a part of the theory devoted to explaining why this is. Fringe theories are no exception. They will claim that the scientific establishment is afraid of being proved wrong, and hence is trying to suppress the truth. This is a classic conspiracy theory. Their theory is not accepted because the black suits in the Scientific Establishment are not concerned about the pursuit of truth, but are much more concerned about not rocking the boat in order to protect their vested interests. The round-earth theorists have the backing of the major media who also have vested interests which they must protect. This explains why the discoveries of 'edges' round the Earth into which planes have gone missing, reports of travelers who have looked into the abyss, are receiving no coverage whatsoever by the major newspapers or the major TV networks.&lt;/p&gt;&lt;p&gt;Thus, it is claimed that trying to balance positive content with negative content for due weight is censorship. It is claimed that there is a conspiracy against the fringe position and anyone who opposes an uncritical article about the fringe position is in on the conspiracy, has been bought off, is breaking the rules of Wikipedia, is just plain evil, etc.&lt;/p&gt;&lt;p&gt;It is claimed that any source that has not written articles that are supportive and uncritical of fringe positions are not suitable as tertiary sources. For example, recently at a controversial article, it was once argued 'Actually, those really shouldn't be used as sources on this topic because (to my knowledge) they haven't written anything pro-X, and hence really can't be considered third party.'&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"The following list has been compiled from the wealth of research I have put together over the last ten years. I would suggest that all of these are reptilian bloodline, but I only mention shapeshifting where it has been witnessed"&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The easiest reply to these arguments is to humour them. You can agree to their ludicrous claims, but point out that Wikipedia is not here to right wrongs, or address grievances. Point out (see above) that if Wikipedia had been around at the time of Galileo, it would have had a duty to report the claims of the Catholic church as fact, without qualification, despite the conspiracy that undoubtedly existed.&lt;/p&gt;&lt;head rend="h3"&gt;5. Reversed burden of proof&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;X's paper on 'scientific fallacies' contains only passing reference to the 'flat Earth fallacy'. WP:NPOV says "Even with well-sourced material ... if you use it out of context or to advance a position that is not directly and explicitly supported by the source used, you as an editor are engaging in original research."&lt;/item&gt;&lt;item&gt;"You are taking lack of discussion of whether the Earth is flat as evidence an author picks a side on the issue.... The evidence we should consider are those who consider the Earth is flat, and those who explicitly reject this view. Sources that remain silent on the issue should be discarded."&lt;/item&gt;&lt;item&gt;There is no reliable source for the statement that 'flat-Earthism has entirely been ignored by reliable sources'.&lt;/item&gt;&lt;item&gt;The statement 'there is no scientific consensus for the flat-Earth view' has no scientific consensus.&lt;/item&gt;&lt;item&gt;There has been no serious study of whether the Earth is flat since 1493.[5] Therefore we cannot claim in Wikipedia that Earth is not flat, only that a study in 1493 came to this conclusion.&lt;/item&gt;&lt;item&gt;X's statement "Informal soundings amongst scientists revealed an almost total absence of awareness of the flat Earth theory" is mere opinion. X is using personal experience as evidence. This is not a scientific evidence and is therefore mere opinion.&lt;/item&gt;&lt;item&gt;You can't say "modern geologists reject Rosencranz's theories." Very few scholars have even read Rosencranz or care, so don't extrapolate that to the whole field as if they have rigorously investigated his work as a group. (recently from Ancient astronauts)&lt;/item&gt;&lt;item&gt;"Prove that there are no ______." You can't prove that there are none, only that we haven't found one yet.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;We move to the most powerful weapon in the fringe armoury: the argument from reversed burden of proof. Instead of them having to prove that their view is supported by reliable and independent sources, they will shift the burden of proof over to you, so you have to prove either that their view is not supported, or even that it is refuted by reliable and independent sources. This is difficult for two reasons. First, it is always difficult to prove a negative existential statement (which is in effect a claim about everything there is). Second, because science generally ignores pseudoscience, it is often very difficult to find reliable sources that describe some pseudoscientific view as pseudoscientific.&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;p&gt;This argument is often difficult to address. However, you should always recognise the shifting of the burden for what it is, the second that ball comes thundering down the court at 80 mph. Slam it back. Insist that the burden is theirs.&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dd-1"&gt;"When two or more theories are in competition, it is common for one of them to be treated as the established position – the default option, as it were – and the others to be treated as challengers. A challenging theory is normally expected to bear the burden or onus of proof. In other words, advocates of the challenging theory are expected to provide highly convincing evidence and arguments before the theory can be taken seriously. To use a different metaphor, it is assumed that the established theory has jumped over a very high hurdle to gain its leading position and that any challenger must jump over an equally high hurdle before being in contention for the remainder of the race."[6]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Also, in such cases it is legitimate to source from non-promotional descriptions of pseudoscience that can only be obtained from second- and third-party sources. Although most of these sources will not be peer-reviewed simply because science tends to ignore pseudoscience, they are still independent. Thus, the following are reliable sources for describing pseudoscience:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Committee for Skeptical Inquiry&lt;/item&gt;&lt;item&gt;Encyclopedia of Pseudoscience&lt;/item&gt;&lt;item&gt;An Encyclopedia of Claims, Frauds, and Hoaxes of the Occult and Supernatural&lt;/item&gt;&lt;item&gt;Skeptic's Dictionary&lt;/item&gt;&lt;item&gt;Skeptical Inquirer&lt;/item&gt;&lt;item&gt;talk.origins archive&lt;/item&gt;&lt;item&gt;Bad Astronomy&lt;/item&gt;&lt;item&gt;Quackwatch&lt;/item&gt;&lt;item&gt;Mainstream media reports&lt;/item&gt;&lt;item&gt;Skeptical scientists speaking extemporaneously (whether it be in person, letters, personal websites, blogs, etc.)&lt;/item&gt;&lt;item&gt;Statements from scientific societies&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;6. Gaming&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The statement 'The Earth is round' has reliable sources in scientific literature. The statement 'If the X is round, X is not flat' is a valid inference that can be sourced from any reliable logic textbook. But 'The Earth is not flat', while a conclusion validly yielded by these two reliably-sourced premises, is a violation of WP:SYNTH: "Even if published by reliable sources, material must not be connected together in such a way that it constitutes original research".&lt;/item&gt;&lt;item&gt;One should use only primary sources. Relying on secondary sources is POV.&lt;/item&gt;&lt;item&gt;Words like "alleged", "supposed", and "purported" when used to describe the characteristics of the Flat Earth are WP:WTA and unduly prejudice the reader against the subject. Words that can be interpreted ambiguously by the reader (such as "apparent") are better suited to a neutral presentation.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"If the facts are against you, argue the law. If the law is against you, argue the facts. If the law and the facts are against you, pound the table and yell like hell."&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;You have kept the marginal and fringe viewpoint at bay for some months or years. But now they have got wise, and expert in the ways of Wikipedia. They have read the policies carefully, and have worked out the various loopholes in it, and the endless games they can play with it.&lt;/p&gt;&lt;p&gt;They now claim that only the proponents of the FRINGE position understand NPOV or NOR or RS, not the experienced editors with tens of thousands of edits, and FAs and GAs to their credit. They will 'wikilawyer' to try to redefine a FRINGE position as nonFRINGE, or the mainstream position as the FRINGE position instead. They will attempt to use mainly primary sources, and to reject secondary and tertiary sources, or to redefine the preferences for secondary and tertiary sources in policy.&lt;/p&gt;&lt;p&gt;Worst of all, it is now many months since you tidied up the article. You have no inherent interest in the Flat Earth theory, and you have moved on to another area of pseudoscience (let's say the Geocentric theory). But the Flat Earth supporters are interested in nothing else than their pet theory. They will come back when you are gone and revert when you do not notice. The arguments that you successfully rebutted and dismissed, sometimes with extensive references, will be repeated over and over and over, sometimes just with a cut and paste approach. Sometimes they will be presented by the same person dozens and dozens of times over days and weeks and months. They will try to add information that is (at best) peripherally relevant on the grounds that 'it is verifiable, so it should be in'. They repeatedly use the talk page for soapboxing, or to re-raise the same issues that have already been discussed numerous times. They hang around forever wearing down more serious editors and become expert in an odd kind of way on their niche POV.&lt;/p&gt;&lt;p&gt;They will make a series of silly and time-wasting requests for comment, mediation or arbitration again to try to wear you down. They will add [citation needed] tags repeatedly to well-known material, or material that is fully referenced on wikilinked articles that discuss that point in more detail. Assorted templates branding the article are thrown on the article repeatedly, such as the claim that an NPOV dispute is going on, when it is more accurate to describe the discussion as revolving around some editor's idiosyncratic interpretation of NPOV to satisfy their own personal agenda. Accusations that a group editing the article own the article since they will not change the consensus to satisfy one malcontent are common.&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;p&gt;If you are unable to discourage a fringe advocate from willfully and knowingly misusing policy you might seek support from the community via mediation or arbitration. However, many fringe advocates thrive on the increased attention and actually welcome these forums as a soapbox from which to further argue their viewpoint. Finding themselves in the spotlight, it is not unusual for dedicated fringe advocates to suddenly disavow any former or present interest/connection with the subject of their advocacy ("Gosh, I don't believe the Earth is flat, I'm just here to uphold NPOV") and profess that they are only fighting "for the good of Wikipedia". The risks of continued involvement with disputes that escalate to this level should be carefully considered, especially if accompanied by obsessive/compulsive behavior.&lt;/p&gt;&lt;head rend="h3"&gt;7. Amenability&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The flat Earth theory is not amenable to scientific approaches and methods.&lt;/item&gt;&lt;item&gt;Flat-earth theorists are pragmatic. They are not interested in what is 'true', they are interested in 'what works'.&lt;/item&gt;&lt;item&gt;Rosencrantz never claimed nor explicitly stated that the Flat Earth Theory is a 'science'.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;Another way of evading NPOV is to avoid the requirement for reliable sourcing altogether. They will claim that the view in question is simply not amenable to scientific treatment. Source X was from a scientific journal, it attempted to address the Flat Earth theory in a way that science could deal with it. But Flat Earth theory is not amenable to scientific treatment. Source X misunderstood what the theory was really saying. The Flat Earth theory is not something that is really a 'fact' in the scientific sense. (See the archived talk pages of the article Neuro-linguistic programming for endless repetitions and varieties of this argument).&lt;/p&gt;&lt;p&gt;Or they claim that writing material using facts in the same context as in reliable sources violates NPOV since they are following a "narrative", and we must instead choose facts which no source describes as relevant to allow our readers to decide which "narrative" should be chosen.&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;p&gt;Stick to your guns. This is merely a philosophically naive means of evading justification and substantiation. All theories make claims of some sort, otherwise they would not have 'proponents' (a proponent literally 'puts forward' a certain view that is susceptible of truth or falsity). The Flat Earth theory claims that the Earth is flat, not round. That is a statement with a binary truth-value. And it is capable of confirmation or refutation, it is verifiable. For example, topography (measuring the distances between defined points on the Earth's surface) shows the shape of the Earth. Therefore, the theory is amenable to scientific treatment.&lt;/p&gt;&lt;head rend="h3"&gt;8. Special pleading&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Scientist X, who claimed the flat Earth theory was nonsense, clearly had not read the literature on the flat Earth theory.&lt;/item&gt;&lt;item&gt;Scientist X was not trained in flat Earth theory, and therefore could not make an expert judgment.&lt;/item&gt;&lt;item&gt;The criticisms made by scientist X were valid only against Rosencrantz' version of the flat Earth theory, long since outmoded. They fail to address Guildenstern's improved version of the theory.&lt;/item&gt;&lt;item&gt;Your arguments assume there is a mainstream flat Earth view. There is no mainstream 'flat Earth' view, therefore your criticisms are misplaced.&lt;/item&gt;&lt;item&gt;You haven't read any of Rosencrantz' work.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;Special pleading is when the advocates of a fringe viewpoint argue that you have no expertise in the theory (which may, they argue, take years to fully master). You do not understand the theory, and therefore you cannot make your claims. Another version of this argument is to claim there are many different types of the theory, and that while version X and version Y are clearly nonsense, the most recent version Z (which of course you have never heard of) is scientifically impeccable. They may even claim there is no such 'version' of the theory, and that you are attacking a straw man.&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;p&gt;The only people qualified enough to understand flat Earth theory are those who just happen to support flat Earth theory? Ridiculous. Advocates, promoters, and self proclaimed "flat Earth theory experts" are not independent, objective sources of fact about whether or not the Earth is flat, or whether or not flat Earth theory is valid. Also bear in mind that any "new and improved" versions of flat Earth theory must be notable enough to have attracted review and comment by independent, objective sources.&lt;/p&gt;&lt;head rend="h3"&gt;9. Controversy&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The flat Earth theory is clearly controversial. This is proof that scientists take it seriously.&lt;/item&gt;&lt;item&gt;The more controversial or fringe a subject, the less the article should tell the reader what to believe. The reader should be allowed to make up his/her own mind concerning the subject. e.g.: "Flat Earthism is a highly controversial subject, and its scientific validity is often questioned."&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;Although supporters of the marginal view cannot overcome Wikipedia policies, they will try to distort and alter an article's language in a way that represents their view as less marginal. The most well-known and often-used tactic is to claim that their viewpoint is 'controversial', as though there were a minority but substantial view held by serious scientists or academics, actively engaged by the mainstream, and which is reported as controversial by reliable sources.&lt;/p&gt;&lt;p&gt;They will try to exploit equivocation in the description of pseudoscience. For example, instead of simply stating: "the Flat Earth theory violates the known laws of geometry", a proponent may argue for the equivocal statement: "some geometers claim that the Flat Earth theory violates the known laws of geometry", perhaps adding "but there is considerable controversy over the matter."&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"The fact that some geniuses were laughed at does not imply that all who are laughed at are geniuses. They laughed at Columbus, they laughed at Fulton, they laughed at the Wright brothers. But they also laughed at Bozo the Clown."&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Pseudoscience should not be described on its own terms. The goal of writing an article on pseudoscience should be to present the ideas that are most commonly seen in relation to that pseudoscientific idea. This means that when writing an article on pseudoscience, popularity of ideas is a major rationale for inclusion or exclusion. Obscure iterations of pseudoscience should be eliminated, even if so-called "experts" in the subject believe such ideas to be of the utmost importance. The best way to write an article on pseudoscience is to approach it from the perspective of what topics are most prevalent in the popular culture about the subject.&lt;/p&gt;&lt;p&gt;All claims that are made about observable reality which are directly contradicted by mainstream science must be represented as such. Per the rules of reliable sourcing and not unduly weighting fringe opinions, an article about a mainstream topic should marginalize all related pseudoscience topics relative to the prominence seen in secondary and tertiary sources about the mainstream topic. A pseudoscientific topic should not be mentioned in an article about a mainstream topic unless there are independent mainstream sources that connect the topics. For example, there are plenty of mainstream sources which describe how astronomy is not astrology, and so a decent article on the former may mention the latter. However, there are no mainstream sources about special relativity which also mention autodynamics, and so a decent article on the former should not mention the latter. This approach is outlined in the guideline WP:ONEWAY.&lt;/p&gt;&lt;p&gt;If it is deemed necessary to exclude pseudoscience from a certain article, there should not even be a link through a see also section. Often pseudoscience articles must link to science articles. Rarely will science articles link to pseudoscience articles. That is the principle of one-way linking.&lt;/p&gt;&lt;head rend="h3"&gt;10. But some of the theory is true&lt;/head&gt;[edit]&lt;p&gt;Examples&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Rosencrantz claimed many times that the sky is blue, that grass is green. These facts are well-established by reliable sources.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How to recognise&lt;/p&gt;&lt;p&gt;The last weapon in the pseudoscience arsenal is something you cannot deny: parts of the theory may be true. Proponents will ignore the many bogus and patently untrue claims of the theory, and perhaps not even mention them in the article, but will go on at length about the parts of the theory that are true. Often these are platitudinous, or are statements that are better and more clearly covered in reliable sources. Worse, they will cite reliable sources which make these true claims, but which do not mention the fringe theory, as though they supported the theory.&lt;/p&gt;&lt;p&gt;How to reply&lt;/p&gt;&lt;p&gt;Rosencrantz may have said that the sky is blue and grass is green but he most likely isn't considered an authority or reliable source for such information. Attention to such details is only warranted if there is significant third-party coverage of them.&lt;/p&gt;&lt;head rend="h2"&gt;Historical note&lt;/head&gt;[edit]&lt;p&gt;By the 1950s historians had established that in the High Middle Ages the educated classes had recovered the ancient Greeks' discovery that the world is a sphere, even though their world-view was geocentric, and that the notion that Columbus's voyage was to prove the world round is a piece of fiction introduced by Washington Irving. (See Myth of the Flat Earth.) Although it was presumed before Copernicus that the Earth was the centre of the Universe, with the Sun revolving around it, we now know that the educated classes still understood the Earth to be a sphere with gravity acting towards its centre, contrary to the widespread 19th-century assumption that most mediaeval people believed it to be flat. (For example, the final two cantos of Dante Alighieri's Inferno assume a spherical Earth.) However, this essay uses the flat Earth as a metaphor for explaining Wikipedia policy, not to describe any authentic historical controversy.&lt;/p&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Civil POV pushing (essay)&lt;/item&gt;&lt;item&gt;Creating controversial content (essay)&lt;/item&gt;&lt;item&gt;Verifiability, not truth (essay)&lt;/item&gt;&lt;item&gt;Wikipedia is not RationalWiki (essay)&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Notes&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ notwithstanding The World Is Flat by Thomas Friedman&lt;/item&gt;&lt;item&gt;^ An independent source is one that has no interest, ideological, financial or otherwise, in preferring one view over another.&lt;/item&gt;&lt;item&gt;^ This is sometimes known as the "Most Insane Person Always Wins" theory of Internet debate. See Dogbert: "Reality is always controlled by the people who are most insane." (Scott Adams)&lt;/item&gt;&lt;item&gt;^ Thanks to User:MastCell&lt;/item&gt;&lt;item&gt;^ Yes, as all pedants know, they already knew the Earth was round before 1493. Get a life.&lt;/item&gt;&lt;item&gt;^ Martin, B., "The burden of proof and the origin of acquired immune deficiency syndrome" Philosophical Transactions of the Royal Society of London, Series B, Vol. 356, 2001, pp. 939–944&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45557013</guid><pubDate>Sun, 12 Oct 2025 10:19:34 +0000</pubDate></item><item><title>A New Algorithm Makes It Faster to Find the Shortest Paths</title><link>https://www.wired.com/story/new-method-is-the-fastest-way-to-find-the-best-routes/</link><description>&lt;doc fingerprint="ff98b38025340879"&gt;
  &lt;main&gt;
    &lt;p&gt;The original version of this story appeared in Quanta Magazine.&lt;/p&gt;
    &lt;p&gt;If you want to solve a tricky problem, it often helps to get organized. You might, for example, break the problem into pieces and tackle the easiest pieces first. But this kind of sorting has a cost. You may end up spending too much time putting the pieces in order.&lt;/p&gt;
    &lt;p&gt;This dilemma is especially relevant to one of the most iconic problems in computer science: finding the shortest path from a specific starting point in a network to every other point. It’s like a souped-up version of a problem you need to solve each time you move: learning the best route from your new home to work, the gym, and the supermarket.&lt;/p&gt;
    &lt;p&gt;“Shortest paths is a beautiful problem that anyone in the world can relate to,” said Mikkel Thorup, a computer scientist at the University of Copenhagen.&lt;/p&gt;
    &lt;p&gt;Intuitively, it should be easiest to find the shortest path to nearby destinations. So if you want to design the fastest possible algorithm for the shortest-paths problem, it seems reasonable to start by finding the closest point, then the next-closest, and so on. But to do that, you need to repeatedly figure out which point is closest. You’ll sort the points by distance as you go. There’s a fundamental speed limit for any algorithm that follows this approach: You can’t go any faster than the time it takes to sort.&lt;/p&gt;
    &lt;p&gt;Forty years ago, researchers designing shortest-paths algorithms ran up against this “sorting barrier.” Now, a team of researchers has devised a new algorithm that breaks it. It doesn’t sort, and it runs faster than any algorithm that does.&lt;/p&gt;
    &lt;p&gt;“The authors were audacious in thinking they could break this barrier,” said Robert Tarjan, a computer scientist at Princeton University. “It’s an amazing result.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Frontier of Knowledge&lt;/head&gt;
    &lt;p&gt;To analyze the shortest-paths problem mathematically, researchers use the language of graphs—networks of points, or nodes, connected by lines. Each link between nodes is labeled with a number called its weight, which can represent the length of that segment or the time needed to traverse it. There are usually many routes between any two nodes, and the shortest is the one whose weights add up to the smallest number. Given a graph and a specific “source” node, an algorithm’s goal is to find the shortest path to every other node.&lt;/p&gt;
    &lt;p&gt;The most famous shortest-paths algorithm, devised by the pioneering computer scientist Edsger Dijkstra in 1956, starts at the source and works outward step by step. It’s an effective approach, because knowing the shortest path to nearby nodes can help you find the shortest paths to more distant ones. But because the end result is a sorted list of shortest paths, the sorting barrier sets a fundamental limit on how fast the algorithm can run.&lt;/p&gt;
    &lt;p&gt;In 1984, Tarjan and another researcher improved Dijkstra’s original algorithm so that it hit this speed limit. Any further improvement would have to come from an algorithm that avoids sorting.&lt;/p&gt;
    &lt;p&gt;In the late 1990s and early 2000s, Thorup and other researchers devised algorithms that broke the sorting barrier, but they needed to make certain assumptions about weights. Nobody knew how to extend their techniques to arbitrary weights. It seemed they’d hit the end of the road.&lt;/p&gt;
    &lt;p&gt;“The research stopped for a very long time,” said Ran Duan, a computer scientist at Tsinghua University in Beijing. “Many people believed that there’s no better way.”&lt;/p&gt;
    &lt;p&gt;Duan wasn’t one of them. He’d long dreamed of building a shortest-paths algorithm that could break through the sorting barrier on all graphs. Last fall, he finally succeeded.&lt;/p&gt;
    &lt;head rend="h2"&gt;Out of Sorts&lt;/head&gt;
    &lt;p&gt;Duan’s interest in the sorting barrier dates back nearly 20 years to his time in graduate school at the University of Michigan, where his adviser was one of the researchers who worked out how to break the barrier in specific cases. But it wasn’t until 2021 that Duan devised a more promising approach.&lt;/p&gt;
    &lt;p&gt;The key was to focus on where the algorithm goes next at each step. Dijkstra’s algorithm takes the region that it has already explored in previous steps. It decides where to go next by scanning this region’s “frontier”—that is, all the nodes connected to its boundary. This doesn’t take much time at first, but it gets slower as the algorithm progresses.&lt;/p&gt;
    &lt;p&gt;Duan instead envisioned grouping neighboring nodes on the frontier into clusters. He would then only consider one node from each cluster. With fewer nodes to sift through, the search could be faster at each step. The algorithm also might end up going somewhere other than the closest node, so the sorting barrier wouldn’t apply. But ensuring that this clustering-based approach actually made the algorithm faster rather than slower would be a challenge.&lt;/p&gt;
    &lt;p&gt;Duan fleshed out this basic idea over the following year, and by fall 2022 he was optimistic that he could surmount the technical hurdles. He roped in three graduate students to help work out the details, and a few months later they arrived at a partial solution—an algorithm that broke the sorting barrier for any weights, but only on so-called undirected graphs.&lt;/p&gt;
    &lt;p&gt;In undirected graphs, every link can be traversed in both directions. Computer scientists are usually more interested in the broader class of graphs that feature one-way paths, but these “directed” graphs are often trickier to navigate.&lt;/p&gt;
    &lt;p&gt;“There could be a case that A can reach B very easily, but B cannot reach A very easily,” said Xiao Mao, a computer science graduate student at Stanford University. “That’s going to give you a lot of trouble.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Promising Paths&lt;/head&gt;
    &lt;p&gt;In the summer of 2023, Mao heard Duan give a talk about the undirected-graph algorithm at a conference in California. He struck up a conversation with Duan, whose work he’d long admired.&lt;/p&gt;
    &lt;p&gt;“I met him for the first time in real life,” Mao recalled. “It was very exciting.”&lt;/p&gt;
    &lt;p&gt;After the conference, Mao began thinking about the problem in his spare time. Meanwhile, Duan and his colleagues were exploring new approaches that could work on directed graphs. They took inspiration from another venerable algorithm for the shortest-paths problem, called the Bellman-Ford algorithm, that doesn’t produce a sorted list. At first glance, it seemed like an unwise strategy, since the Bellman-Ford algorithm is much slower than Dijkstra’s.&lt;/p&gt;
    &lt;p&gt;“Whenever you do research, you try to take a promising path,” Thorup said. “I would almost call it anti-promising to take Bellman-Ford, because it looks completely like the stupidest thing you could possibly do.”&lt;/p&gt;
    &lt;p&gt;Duan’s team avoided the slowness of the Bellman-Ford algorithm by running it for just a few steps at a time. This selective use of Bellman-Ford enabled their algorithm to scout ahead for the most valuable nodes to explore in later steps. These nodes are like intersections of major thoroughfares in a road network.&lt;/p&gt;
    &lt;p&gt;“You have to pass through [them] to get the shortest path to a lot of other stuff,” Thorup said.&lt;/p&gt;
    &lt;p&gt;In March 2024, Mao thought of another promising approach. Some key steps in the team’s original approach had used randomness. Randomized algorithms can efficiently solve many problems, but researchers still prefer nonrandom approaches. Mao devised a new way to solve the shortest-paths problem without randomness. He joined the team, and they worked together over the following months via group chats and video calls to merge their ideas. Finally, in the fall, Duan realized they could adapt a technique from an algorithm he’d devised in 2018 that broke the sorting barrier for a different graph problem. That technique was the last piece they needed for an algorithm that ran faster than Dijkstra’s on both directed and undirected graphs.&lt;/p&gt;
    &lt;p&gt;The finished algorithm slices the graph into layers, moving outward from the source like Dijkstra’s. But rather than deal with the whole frontier at each step, it uses the Bellman-Ford algorithm to pinpoint influential nodes, moves forward from these nodes to find the shortest paths to others, and later comes back to other frontier nodes. It doesn’t always find the nodes within each layer in order of increasing distance, so the sorting barrier doesn’t apply. And if you chop up the graph in the right way, it runs slightly faster than the best version of Dijkstra’s algorithm. It’s considerably more intricate, relying on many pieces that need to fit together just right. But curiously, none of the pieces use fancy mathematics.&lt;/p&gt;
    &lt;p&gt;“This thing might as well have been discovered 50 years ago, but it wasn’t,” Thorup said. “That makes it that much more impressive.”&lt;/p&gt;
    &lt;p&gt;Duan and his team plan to explore whether the algorithm can be streamlined to make it even faster. With the sorting barrier vanquished, the new algorithm’s runtime isn’t close to any fundamental limit that computer scientists know of.&lt;/p&gt;
    &lt;p&gt;“Being an optimist, I would not be surprised if you could take it down even further,” Tarjan said. “I certainly don’t think this is the last step in the process.”&lt;/p&gt;
    &lt;p&gt;Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45557256</guid><pubDate>Sun, 12 Oct 2025 11:06:50 +0000</pubDate></item><item><title>Blood test detecting Long Covid in kids with 94% accuracy microclots</title><link>https://www.researchsquare.com/article/rs-7483367/v1</link><description>&lt;doc fingerprint="2c9b11db8577b373"&gt;
  &lt;main&gt;
    &lt;p&gt;Long COVID (LC) impacts one in five children after an acute SARS-CoV-2 infection. Typical LC symptoms include fatigue, brain fog, pain, and shortness of breath, which can significantly impact individuals and society. Moreover, LC may impair school performance and have long-term health and development consequences. However, the diagnosis of LC is often imprecise and cumbersome, delaying appropriate care. To address the LC diagnosis challenges, we focused on fibrinaloid clots (microclots), recently proposed as contributing to LC’s underlying mechanisms. We overcame the limitations of current microclot assessment methods, which are qualitative, by developing a microfluidic device to quantify the number of microclots in patient blood samples. We found significantly higher microclot levels in samples from pediatric LC patients than from healthy children. We evaluated the diagnostic power of the device in a cohort of 45 LC patients and 14 healthy pediatric donors. We estimated a 94% accuracy for the microclot count using the devices, significantly higher than the traditional counting of microclots on slides (66% accuracy). Intriguingly, we found the highest microclot counts in samples from patients with persistent SARS-CoV-2 spike protein in the blood. Further studies will evaluate the utility of the assay as a screening test for Long COVID in larger populations and for assessing treatment responses.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45557267</guid><pubDate>Sun, 12 Oct 2025 11:08:58 +0000</pubDate></item></channel></rss>