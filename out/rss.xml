<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 22 Nov 2025 03:47:40 +0000</lastBuildDate><item><title>Olmo 3: Charting a path through the model flow to lead open-source AI</title><link>https://allenai.org/blog/olmo3</link><description>&lt;doc fingerprint="ad5e3b38243b8f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Olmo 3: Charting a path through the model flow to lead open-source AI&lt;/head&gt;
    &lt;p&gt;November 20, 2025&lt;/p&gt;
    &lt;p&gt;Ai2&lt;/p&gt;
    &lt;p&gt;Language models are often treated as snapshots—brief captures of a long and carefully curated development process. But sharing only the end result obscures the rich context needed to modify, adapt, and extend a model's capabilities. Many meaningful adjustments require integrating domain-specific knowledge deep within the development pipeline, not merely at the final stage. To truly advance open AI development and research, the entire model flow – not just its endpoint – should be accessible and customizable. The model flow is the full lifecycle of an LM: every stage, checkpoint, dataset, and dependency required to create and modify it. By exposing this complete process, the goal is to engender greater trust and enable more effective adaptation, collaboration, and innovation.&lt;/p&gt;
    &lt;p&gt;With today's release of Olmo 3, we're empowering the open source community with not only state-of-the-art open models, but the entire model flow and full traceability back to training data.&lt;/p&gt;
    &lt;p&gt;At its center is Olmo 3-Think (32B), the best fully open 32B-scale thinking model that for the first time lets you inspect intermediate reasoning traces and trace those behaviors back to the data and training decisions that produced them. Olmo 3 is a family of compact, dense models at 7 billion and 32 billion parameters that can run on everything from laptops to research clusters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo 3-Base (7B, 32B) is our most powerful base model yet. When evaluated on our expanded, diverse evaluation suite, Olmo 3-Base delivers the strongest performance among fully open base models – where training data, code, and weights are all publicly available, like Stanford's Marin and Swiss AI's Apertus – and achieves competitive performance with some of the best open-weights base models of comparable size and architecture, including Qwen 2.5 and Gemma 3. Achieving strong results in programming, reading comprehension, and math problem solving, Olmo 3-Base maintains performance at extended context lengths (~up to 65K tokens)—providing a versatile foundation for continued pretraining, targeted fine-tuning, and reinforcement learning and making it easy to build in specialized capabilities like reasoning, tool use (function calling), and instruction following through post-training.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Think (7B, 32B) is our flagship post-trained reasoning set built on Olmo 3-Base. At a time when few organizations are releasing truly open models at this scale, Olmo 3-Think (32B) serves as a workhorse for RL research, long-horizon reasoning, and other advanced experiments that require substantial compute. On our suite of reasoning benchmarks (discussed below), it's the strongest fully open thinking model we're aware of, narrowing the gap to the best open-weight models of similar scale – such as Qwen 3 32B – while training on roughly 6x fewer tokens. Olmo 3-Think (7B) brings the same design and training approach to an even more efficient form factor, surfacing intermediate thinking steps for complex prompts while making open, inspectable reasoning accessible on more modest hardware.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Instruct (7B) is a chat and quick-response focused post-train of Olmo 3-Base that handles multi-turn, instruction-following, tool use, and more. In our evaluations, it matches or outperforms open-weight models including Qwen 2.5, Gemma 3, and Llama 3.1, and narrows the gap with Qwen 3 model families at a similar scale—delivering a strong, fully open alternative for high-quality conversational and tool-using agents.&lt;/item&gt;
      &lt;item&gt;Olmo 3-RL Zero (7B), is a fully open reinforcement learning pathway built on Olmo 3-Base, designed to bootstrap complex reasoning behaviors and enable clear benchmarking of RL algorithms. We release four series of checkpoints from domain-focused training on math, code, instruction following, and general chat, enabling careful study of reinforcement learning with verifiable rewards (RLVR).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of a single set of frozen weights, Olmo 3 offers multiple, fully documented paths through development: the Instruct path for everyday chat and tool use, the RL Zero path for RL experimentation from base models, and the Think/reasoning path for models that leverage inference-time scaling to unlock complex reasoning and agentic behaviors. Each path is a concrete example of how to shape behavior from the same base model, and you’re free to fork or remix them—start with Olmo 3-Base, explore your own supervised fine-tuning (SFT) or direct preference optimization (DPO) recipe for instruct-style use cases, or plug in a new RL objective to probe different tradeoffs. The flow itself becomes a rich, reusable object—not just a record of how we built Olmo 3, but a scaffold for how you can build your own systems.&lt;/p&gt;
    &lt;p&gt;Explore the Model Flow&lt;/p&gt;
    &lt;p&gt;Click on any stage to learn more about it and download artifacts.&lt;/p&gt;
    &lt;p&gt;The Olmo 3 checkpoints we're releasing represent our initial paths targeting our goals around reasoning, tool use, and general capabilities – we have exciting plans for other ways to leverage Olmo 3-Base 32B. But because we're releasing the entire flow, you can intervene at any point: swap in domain-specific data during mid-training, adjust post-training for your use case, or build on an earlier checkpoint that better suits your needs.&lt;/p&gt;
    &lt;p&gt;As with Olmo and Olmo 2, we’re releasing all components of the Olmo 3 flow – data, code, model weights, and checkpoints – under permissive open source licenses.&lt;/p&gt;
    &lt;p&gt;Try Olmo 3 on the Ai2 Playground | Use Olmo 3 via OpenRouter | Download the models &amp;amp; data | Read the report&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong performance across the board&lt;/head&gt;
    &lt;p&gt;We run the Olmo 3 checkpoints through a broad, updated benchmark suite, grouping dozens of industry-standard tasks (plus a few new ones we introduce) into several capability clusters. Together, the clustered suite and these held-out tasks give us a capability profile of Olmo 3—a clear picture of how well it solves math problems, codes, uses tools, answers general-knowledge questions, and more.&lt;/p&gt;
    &lt;p&gt;At a high level, the Olmo 3 family delivers the strongest fully open base and thinking models we’re aware of. Olmo 3-Base 32B outperforms other fully open base models, and Olmo 3-Think 32B emerges as the strongest fully open thinking model.&lt;/p&gt;
    &lt;p&gt;Our results were made possible by rigorous data curation at every stage of training, a carefully designed training recipe for each model, and a set of new algorithmic and infrastructure advances across data processing, training, and reinforcement learning. We also introduce an enhanced reinforcement learning framework that guides the development of our models and is particularly essential for our thinking models. To design the training recipe and coordinate targeted improvements across a wide range of capabilities at each stage of the model training pipeline, our development framework balances distributed innovation with centralized evaluation.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Base, with a training pipeline that first focuses on broad coverage over diverse text, code, and math, then concentrates on harder distributions to sharpen programming, quantitative reasoning, and reading comprehension, is clearly the strongest set of fully open base models in our evaluations. It’s also arguably the best 32B model in the entire ecosystem of models with open weights, performing impressively in programming, reading comprehension, math problem solving, and long-context benchmarks like RULER, which tests information retrieval from lengthy texts. Olmo 3-Base (7B) and Olmo 3-Base (32) maintain quality at extended context lengths and integrate cleanly with RL workflows, providing a robust foundation for continued pretraining and post-training.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Think, which turns the Base into a reasoning model by training on multi-step problems spanning math, code, and general problem solving, then running the thinking SFT → thinking DPO → RLVR model flow to elicit high-quality reasoning traces, competes with or exceeds several open-weight reasoning models of similar sizes. On math benchmarks, Olmo 3-Think (7B) matches Qwen 3 8B on MATH and comes within a few points on AIME 2024 and 2025, and also leads all comparison models on HumanEvalPlus for coding—performing strongly on MBPP and LiveCodeBench to demonstrate particular strength in code-intensive reasoning. On broader reasoning tasks like BigBench Hard and AGI Eval English, Olmo 3-Think (7B) remains competitive with Qwen 3 8B reasoning and Qwen 3 VL 8B Thinker while staying fully open and slightly smaller.&lt;/p&gt;
    &lt;p&gt;For the 32B model, Olmo 3-Think scales these trends up and becomes one of the strongest fully open reasoning models in its class. Olmo 3-Think (32B) either wins or sits within roughly two points of the best open-weight model on MATH, OMEGA, BigBenchHard, HumanEvalPlus, PopQA, and IFEval. It ties Qwen 3 VL 32B Thinking for the top score on the OMEGA suite while staying clearly ahead of Gemma 3 27B Instruct and competitive with DeepSeek R1 Distill 32B on math and reasoning. On broader knowledge and QA, Olmo 3-Think (32B) is effectively neck-and-neck with the Qwen 3 models on PopQA. And in instruction following, Olmo 3-Think (32B) tops this subset on IFEval and remains solid on IFBench and AlpacaEval 2 LC—offering a strong default for reasoning workloads at the 32B scale.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Instruct, which produces shorter sequences than the corresponding Olmo 3-Think models to improve inference efficiency and is designed to focus on general chat, tool use, and synthetic data generation, outperforms comparably-sized open-weight models. Olmo 3-Instruct ties or surpasses Qwen 2.5, Gemma 3, and Llama 3.1 in our evaluations, and competes with the Qwen 3 family at similar scale, delivering strong function calling performance and instruction-following capabilities in a fully open 7B model.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Olmo 3 architecture and training stages&lt;/head&gt;
    &lt;p&gt;Olmo 3 uses a decoder-only transformer architecture and multi-stage training pipeline. Pretraining runs in three stages—an initial large-scale training run that builds broad capabilities; a mid-training phase that focuses on harder material like math, code, and reading comprehension; and a final long-context extension stage that trains the model on very long documents. Together with architectural enhancements, this yields a more capable, efficient base for the Olmo 3 family.&lt;/p&gt;
    &lt;p&gt;Post-training then specializes the pretrained model for different use cases. Building on Olmo 2, each pathway follows a three-stage recipe – SFT, preference tuning with DPO, and RLVR – but in Olmo 3, we expose this as a fully documented model flow with complete customization over each training stage and dataset mix.&lt;/p&gt;
    &lt;p&gt;Instead of releasing only the final weights, we provide checkpoints from each major training milestone: the base pretrained model, the mid-trained model after targeted skill enhancement, the long-context-extended version, plus post-training checkpoints for the Olmo 3-Think, Olmo 3-Instruct, and Olmo 3-RL Zero flows. You can study how capabilities emerge over time, run ablations on specific stages, and fork the model at whatever point best fits your data, compute, and goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expanded training data&lt;/head&gt;
    &lt;p&gt;Compared to Olmo 2, we scaled data collection and significantly strengthened our dataset curation methods. Continuing our commitment to full transparency, we’re releasing several new, higher-quality datasets that cover every stage of base model training and post-training—from initial learning to specialized skills like complex reasoning and long-context understanding. This means anyone can see exactly what data shaped the model’s capabilities, reproduce our results, and reuse these datasets to train their own AI systems.&lt;/p&gt;
    &lt;p&gt;Olmo 3 is pretrained on Dolma 3, a new ~9.3-trillion-token corpus drawn from web pages, science PDFs processed with olmOCR, codebases, math problems and solutions, and encyclopedic text. From this pool, we construct Dolma 3 Mix, a 5.9-trillion-token (~6T) pretraining mix with a higher proportion of coding and mathematical data than earlier Dolma releases, plus much stronger decontamination via extensive deduplication, quality filtering, and careful control over data mixing. We follow established web standards in collecting training data and don’t collect from sites that explicitly disallow it, including paywalled content.&lt;/p&gt;
    &lt;p&gt;On top of this, we introduce two Dolma 3-based mixes for later stages of base model training. Dolma 3 Dolmino is our mid-training mix: 100B training tokens sampled from a ~2.2T-token pool of high-quality math, science, code, instruction-following, and reading-comprehension data, including reasoning traces that also enable RL directly on the base model. Dolma 3 Longmino is our long-context mix: ~50B training tokens drawn from a 639B-token pool of long documents combined with mid-training data to teach Olmo 3 to track information over very long inputs (like reports, logs, and multi-chapter documents).&lt;/p&gt;
    &lt;p&gt;We also introduce Dolci, a new post-training data suite tailored specifically for reasoning, tool use, and instruction following. Dolci provides separate mixes for each stage of post-training: SFT, DPO, and RLVR. For SFT, Dolci aggregates state-of-the-art datasets that advance step-by-step reasoning, tool use, and high-quality conversational behavior; for DPO, it supplies high-quality contrastive preference data; and for RL, it includes hard, diverse prompts across math, coding, instruction following, and general chat.&lt;/p&gt;
    &lt;p&gt;Together, Dolma 3 and Dolci give Olmo 3 a fully open data curriculum from first token to final post-trained checkpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;Efficient training stack&lt;/head&gt;
    &lt;p&gt;We pretrained Olmo 3 on a cluster of up to 1,024 H100 GPUs; we achieved training throughput of 7.7K tokens per device per second for Olmo 3-Base (7B). We mid-trained on 128 H100 GPUs, and post-trained on a set of 256 H100s.&lt;/p&gt;
    &lt;p&gt;For Olmo 3, building on the work we did for Olmo 2, we were able to significantly improve the efficiency of our post-training code. By moving SFT from Open Instruct (our post-training codebase, prioritizing flexibility) to Olmo Core (our pretraining codebase, designed to maximize efficiency), we increased throughput (tokens/second) by 8x. Similarly, by incorporating in-flight weight updates, continuous batching, and a lot of threading improvements, we made our RL training 4x more efficient—resulting in training runs that are significantly cheaper and faster.&lt;/p&gt;
    &lt;p&gt;A note on our 32B models: We believe 32B sits in a sweet spot for research and tinkering. 32B models are big enough to support strong, competitive performance, but still small enough that a wide audience can fine-tune and deploy them on accessible hardware.&lt;/p&gt;
    &lt;p&gt;For more details, including ablations, please read our technical report.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency at the core&lt;/head&gt;
    &lt;p&gt;A core goal of Olmo 3 is not just to open the model flow, but to make it actionable for people who want to understand and improve model behavior. Olmo 3 integrates with OlmoTrace, our tool for tracing model outputs back to training data in real time.&lt;/p&gt;
    &lt;p&gt;For example, in the Ai2 Playground, you can ask Olmo 3-Think (32B) to answer a general-knowledge question, then use OlmoTrace to inspect where and how the model may have learned to generate parts of its response. This closes the gap between training data and model behavior: you can see not only what the model is doing, but why—and adjust data or training decisions accordingly.&lt;/p&gt;
    &lt;p&gt;To further promote transparency and explainability, we’re making every training and fine-tuning dataset available for download, all under a permissive license that allows for custom deployment and reuse. The datasets come in a range of mixes to accommodate different storage and hardware constraints, from several billion tokens all the way up to 6 trillion.&lt;/p&gt;
    &lt;p&gt;Our new tooling for data processing allows you to de-contaminate, tokenize, and de-duplicate data in the same way we did for Olmo 3’s corpora. All the tooling is open source, enabling you to replicate our training curves or run controlled ablations across data mixes and objectives.&lt;/p&gt;
    &lt;p&gt;Our Olmo utilities and software cover the whole development cycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo-core is a state-of-the-art framework for distributed model training.&lt;/item&gt;
      &lt;item&gt;Open Instruct is our post-training pipeline.&lt;/item&gt;
      &lt;item&gt;datamap-rs is a pure-Rust toolkit for large-scale cleaning.&lt;/item&gt;
      &lt;item&gt;duplodocus for ultra-efficient fuzzy de-duplication.&lt;/item&gt;
      &lt;item&gt;OLMES is a toolkit for reproducible evals. It includes our brand-new eval collection OlmoBaseEval, which we used for Olmo 3 base model development.&lt;/item&gt;
      &lt;item&gt;decon removes test sets from training data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Importantly, our tooling allows you to instrument complex tasks and analyze intermediate traces to understand where the models succeed—or struggle. Because the Olmo 3 data recipes, training pipeline, and checkpoints are open, independent teams can connect model behavior back to measurable properties.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to deploy and use&lt;/head&gt;
    &lt;p&gt;Together, the Olmo 3 family makes it easier to build trustworthy features quickly, whether for research, education, or applications. By making every development step available and inspectable, we're enabling entirely new categories of research. You can run experiments on any training phase, understand exactly how different techniques contribute to model capabilities, and build on our work at whatever stage makes sense for your project.&lt;/p&gt;
    &lt;p&gt;For scientists, the fully open flow exposes the model’s inner workings, so you can instrument experiments across coding, reasoning, RL, and tool use.&lt;/p&gt;
    &lt;p&gt;If you care about AI you can study, audit, and improve, Olmo 3 is for you. Try the demos in the Ai2 Playground, explore the documentation, and build on the released weights and checkpoints. Then tell us what you discover—we invite the community to validate, critique, and extend our findings.&lt;/p&gt;
    &lt;p&gt;True openness in AI isn't just about access—it's about trust, accountability, and shared progress. We believe the models shaping our future should be fully inspectable, not black boxes. Olmo 3 represents a different path: one where anyone can understand, verify, and build upon the AI systems that increasingly influence our world. This is what open-first means—not just releasing weights, but sharing the complete knowledge needed to advance AI responsibly: the flow.&lt;/p&gt;
    &lt;p&gt;Try Olmo 3 on the Ai2 Playground | Use Olmo 3 via OpenRouter | Download the models &amp;amp; data | Read the report&lt;/p&gt;
    &lt;p&gt;Deep dive with Olmo lead researchers Hanna Hajishirzi and Noah Smith on how – and why – we built Olmo 3, and what comes next:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46001889</guid><pubDate>Fri, 21 Nov 2025 06:50:14 +0000</pubDate></item><item><title>FAWK: LLMs can write a language interpreter</title><link>https://martin.janiczek.cz/2025/11/21/fawk-llms-can-write-a-language-interpreter.html</link><description>&lt;doc fingerprint="5ff690b5ea490edb"&gt;
  &lt;main&gt;
    &lt;p&gt;After reading the book The AWK Programming Language (recommended!), I was planning to try AWK out on this year’s Advent of Code. Having some time off from work this week, I tried to implement one of the problems in it to get some practice, set up my tooling, see how hard AWK would be, and… I found I’m FP-pilled.&lt;/p&gt;
    &lt;p&gt;I knew I’m addicted to the combination of algebraic data types (tagged unions) and exhaustive pattern matching, but what got me this time was immutability, lexical scope and the basic human right of being allowed to return arrays from functions.&lt;/p&gt;
    &lt;p&gt;Part 1 of the Advent of Code problem was easy enough, but for part 2 (basically a shortest path search with a twist, to not spoil too much), I found myself unable to switch from my usual functional BFS approach to something mutable, and ended up trying to implement my functional approach in AWK.&lt;/p&gt;
    &lt;p&gt;It got hairy very fast: I needed to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hashing of strings and 2D arrays (by piping to &lt;code&gt;md5sum&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;a global &lt;del rend="overstrike"&gt;set&lt;/del&gt;array of seen states&lt;/item&gt;
      &lt;item&gt;a way to serialize and deserialize a 2D array to/from a string&lt;/item&gt;
      &lt;item&gt;and a few associative arrays for retrieving this serialized array by its hash.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was very lost by the time I had all this; I spent hours just solving what felt like accidental complexity; things that I’d take for granted in more modern languages.&lt;/p&gt;
    &lt;p&gt;Now, I know nobody said AWK is modern, or functional, or that it promises any convenience for anything other than one-liners and basic scripts that fit under a handful of lines. I don’t want to sound like I expect AWK to do any of this; I knew I was stretching the tool when going in. But I couldn’t shake the feeling that there’s a beautiful AWK-like language within reach, an iteration on the AWK design (the pattern-action way of thinking is beautiful) that also gives us a few of the things programming language designers have learnt over the 48 years since AWK was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dreaming of functional AWK&lt;/head&gt;
    &lt;p&gt;Stopping my attempts to solve the AoC puzzle in pure AWK, I wondered: what am I missing here?&lt;/p&gt;
    &lt;p&gt;What if AWK had first-class arrays?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # array literals
  normal   = [1, 2, 3]
  nested   = [[1,2], [3,4]]
  assoc    = ["foo" =&amp;gt; "bar", "baz" =&amp;gt; "quux"]
  multidim = [(1,"abc") =&amp;gt; 999]

  five = range(1,5)
  analyze(five)
  print five  # --&amp;gt; still [1, 2, 3, 4, 5]! was passed by value
}

function range(a,b) {
  r = []
  for (i = a; i &amp;lt;= b; i++) {
    r[length(r)] = i
  }
  return r  # arrays can be returned!
}

function analyze(arr) {
  arr[0] = 100
  print arr[0]  # --&amp;gt; 100, only within this function
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had first-class functions and lambdas?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # construct anonymous functions
  double = (x) =&amp;gt; { x * 2 }
  add = (a, b) =&amp;gt; { c = a + b; return c }

  # functions can be passed as values
  apply = (func, value) =&amp;gt; { func(value) }

  print apply(double,add(1,3))  # --&amp;gt; 8
  print apply(inc,5)  # --&amp;gt; 6
}

function inc(a) { return a + 1 }
&lt;/code&gt;
    &lt;p&gt;What if AWK had lexical scope instead of dynamic scope?&lt;/p&gt;
    &lt;code&gt;# No need for this hack anymore ↓     ↓
#function foo(a, b         ,local1, local2) {
function foo(a, b) {
  local1 = a + b
  local2 = a - b
  return local1 + local2
}

BEGIN {
  c = foo(1,2)
  print(local1)  # --&amp;gt; 0, the local1 from foo() didn't leak!
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had explicit globals, and everything else was local by default?&lt;/p&gt;
    &lt;code&gt;BEGIN { global count }
END {
  foo()
  print count  # --&amp;gt; 1
  print mylocal # --&amp;gt; 0, didn't leak
}
function foo() { count++; mylocal++ }
&lt;/code&gt;
    &lt;p&gt;(This one, admittedly, might make programs a bit more verbose. I’m willing to pay that cost.)&lt;/p&gt;
    &lt;p&gt;What if AWK had pipelines? (OK, now I’m reaching for syntax sugar…)&lt;/p&gt;
    &lt;code&gt;BEGIN {
  result = [1, 2, 3, 4, 5] 
      |&amp;gt; filter((x) =&amp;gt; { x % 2 == 0 })
      |&amp;gt; map((x) =&amp;gt; { x * x })
      |&amp;gt; reduce((acc, x) =&amp;gt; { acc + x }, 0)

  print "Result:", result
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Making it happen&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TL;DR:&lt;/p&gt;&lt;code&gt;Janiczek/fawk&lt;/code&gt;on GitHub&lt;/quote&gt;
    &lt;p&gt;Now for the crazy, LLM-related part of the post. I didn’t want to spend days implementing AWK from scratch or tweaking somebody else’s implementation. So I tried to use Cursor Agent for a larger task than I usually do (I tend to ask for very small targeted edits), and asked Sonnet 4.5 for a README with code examples, and then a full implementation in Python.&lt;/p&gt;
    &lt;p&gt;And it did it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: I also asked for implementations in C, Haskell and Rust at the same time, not knowing if any of the four would succeed, and they all seem to have produced code that at least compiles/runs. I haven’t tried to test them or even run them though. The PRs are here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was very impressed—I still am! I expected the LLM to stumble and flail around and ultimately get nothing done, but it did what I asked it for (gave me an interpreter that could run those specific examples), and over the course of a few chat sessions, I guided it towards implementing more and more of “the rest of AWK”, together with an excessive amount of end-to-end tests.&lt;/p&gt;
    &lt;p&gt;The only time I could see it struggle was when I asked it to implement arbitrary precision floating point operations without using an external library like &lt;code&gt;mpmath&lt;/code&gt;. It attempted to use Taylor series, but couldn’t get it right for at
least a few minutes. I chickened out and told it to &lt;code&gt;uv add mpmath&lt;/code&gt; and simplify
the interpreter code. In a moment it was done.&lt;/p&gt;
    &lt;p&gt;Other things that I thought it would choke on, like &lt;code&gt;print&lt;/code&gt; being both a
statement (with &lt;code&gt;&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; redirection support) and an expression, or
multi-dimensional arrays, or multi-line records, these were all implemented
correctly. Updating the test suite to also check for backwards compatibility
with GAWK - not an issue. Lexical scoping
and tricky closure environment behaviour - handled that just fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;As the cool kids say, I have to update my priors. The frontier of what the LLMs can do has moved since the last time I tried to vibe-code something. I didn’t expect to have a working interpreter the same day I dreamt of a new programming language. It now seems possible.&lt;/p&gt;
    &lt;p&gt;The downside of vibe coding the whole interpreter is that I have zero knowledge of the code. I only interacted with the agent by telling it to implement a thing and write tests for it, and I only really reviewed the tests. I reckon this would be an issue in the future when I want to manually make some change in the actual code, because I have no familiarity with it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This also opened new questions for me wrt. my other projects where I’ve previously run out of steam, eg. trying to implement a Hindley-Milner type system for my dream forever-WIP programming language Cara. It seems I can now just ask the LLM to do it, and it will? But then, I don’t want to fall into the trap where I am no longer able to work on the codebase myself. I want to be familiar with and able to tinker on the code. I’d need to spend my time reviewing and reading code instead of writing everything myself. Perhaps that’s OK.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Performance of FAWK might be an issue as well, though right now it’s a non-goal, given my intended use case is throwaway scripts for Advent of Code, nothing user-facing. And who knows, based on what I’ve seen, maybe I can instruct it to rewrite it in Rust and have a decent chance of success?&lt;/p&gt;
    &lt;p&gt;For now, I’ll go dogfood my shiny new vibe-coded black box of a programming language on the Advent of Code problem (and as many of the 2025 puzzles as I can), and see what rough edges I can find. I expect them to be equal parts “not implemented yet” and “unexpected interactions of new PL features with the old ones”.&lt;/p&gt;
    &lt;p&gt;If you’re willing to jump through some Python project dependency hoops, you can try to use FAWK too at your own risk, at &lt;code&gt;Janiczek/fawk&lt;/code&gt; on
GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46003144</guid><pubDate>Fri, 21 Nov 2025 10:28:49 +0000</pubDate></item><item><title>We should all be using dependency cooldowns</title><link>https://blog.yossarian.net/2025/11/21/We-should-all-be-using-dependency-cooldowns</link><description>&lt;doc fingerprint="af811b9b5ac90ef0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 21, 2025 Tags: oss, security&lt;/p&gt;
    &lt;p&gt;TL;DR: Dependency cooldowns are a free, easy, and incredibly effective way to mitigate the large majority of open source supply chain attacks. More individual projects should apply cooldowns (via tools like Dependabot and Renovate) to their dependencies, and packaging ecosystems should invest in first-class support for cooldowns directly in their package managers.&lt;/p&gt;
    &lt;p&gt;âSupply chain securityâ is a serious problem. Itâs also seriously overhyped, in part because dozens of vendors have a vested financial interest in convincing your that their framing of the underlying problem1 is (1) correct, and (2) worth your money.&lt;/p&gt;
    &lt;p&gt;Whatâs consternating about this is that most open source supply chain attacks have the same basic structure:&lt;/p&gt;
    &lt;p&gt;An attacker compromises a popular open source project, typically via a stolen credential or CI/CD vulnerabilty (such as âpwn requestsâ in GitHub Actions).&lt;/p&gt;
    &lt;p&gt;The attacker introduces a malicious change to the project and uploads it somewhere that will have maximum effect (PyPI, npm, GitHub releases, &amp;amp;c., depending on the target).&lt;/p&gt;
    &lt;p&gt;At this point, the clock has started, as the attacker has moved into the public.&lt;/p&gt;
    &lt;p&gt;Users pick up the compromised version of the project via automatic dependency updates or a lack of dependency pinning.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the aforementioned vendors are scanning public indices as well as customer repositories for signs of compromise, and provide alerts upstream (e.g. to PyPI).&lt;/p&gt;
    &lt;p&gt;Notably, vendors are incentivized to report quickly and loudly upstream, as this increases the perceived value of their services in a crowded field.&lt;/p&gt;
    &lt;p&gt;Upstreams (PyPI, npm, &amp;amp;c.) remove or disable the compromised package version(s).&lt;/p&gt;
    &lt;p&gt;End-user remediation begins.&lt;/p&gt;
    &lt;p&gt;The key thing to observe is that the gap between (1) and (2) can be very large2 (weeks or months), while the gap between (2) and (5) is typically very small: hours or days. This means that, once the attacker has moved into the actual exploitation phase, their window of opportunity to cause damage is pretty limited.&lt;/p&gt;
    &lt;p&gt;We can see this with numerous prominent supply chain attacks over the last 18 months3:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attack&lt;/cell&gt;
        &lt;cell role="head"&gt;Approx. Window of Opportunity&lt;/cell&gt;
        &lt;cell role="head"&gt;References&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xz-utils&lt;/cell&gt;
        &lt;cell&gt;â 5 weeks4&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 1)&lt;/cell&gt;
        &lt;cell&gt;12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 2)&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tj-actions&lt;/cell&gt;
        &lt;cell&gt;3 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;chalk&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nx&lt;/cell&gt;
        &lt;cell&gt;4 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rspack&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num2words&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kong Ingress Controller&lt;/cell&gt;
        &lt;cell&gt;â 10 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;web3.js&lt;/cell&gt;
        &lt;cell&gt;5 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(Each of these attacks has significant downstream effect, of course, but only within their window of opportunity. Subsequent compromises from each, like Shai-Hulud, represent new windows of opportunity where the attackers regrouped and pivoted onto the next set of compromised credentials.)&lt;/p&gt;
    &lt;p&gt;My takeaway from this: some windows of opportunity are bigger, but the majority of them are under a week long. Consequently, ordinary developers can avoid the bulk of these types of attacks by instituting cooldowns on their dependencies.&lt;/p&gt;
    &lt;p&gt;A âcooldownâ is exactly what it sounds like: a window of time between when a dependency is published and when itâs considered suitable for use. The dependency is public during this window, meaning that âsupply chain securityâ vendors can work their magic while the rest of us wait any problems out.&lt;/p&gt;
    &lt;p&gt;I love cooldowns for several reasons:&lt;/p&gt;
    &lt;p&gt;Theyâre empirically effective, per above. They wonât stop all attackers, but they do stymie the majority of high-visibiity, mass-impact supply chain attacks that have become more common.&lt;/p&gt;
    &lt;p&gt;Theyâre incredibly easy to implement. Moreover, theyâre literally free to implement in most cases: most people can use Dependabotâs functionality, Renovateâs functionality, or the functionality build directly into their package manager5.&lt;/p&gt;
    &lt;p&gt;This is how simple it is in Dependabot:&lt;/p&gt;
    &lt;p&gt;(Rinse and repeat for other ecosystems as needed.)&lt;/p&gt;
    &lt;p&gt;Cooldowns enforce positive behavior from supply chain security vendors: vendors are still incentivized to discover and report attacks quickly, but are not as incentivized to emit volumes of blogspam about âcriticalâ attacks on largely underfunded open source ecosystems.&lt;/p&gt;
    &lt;p&gt;In the very small sample set above, 8/10 attacks had windows of opportunity of less than a week. Setting a cooldown of 7 days would have prevented the vast majority of these attacks from reaching end users (and causing knock-on attacks, which several of these were). Increasing the cooldown to 14 days would have prevented all but 1 of these attacks6.&lt;/p&gt;
    &lt;p&gt;Cooldowns are, obviously, not a panacea: some attackers will evade detection, and delaying the inclusion of potentially malicious dependencies by a week (or two) does not fundamentally alter the fact that supply chain security is a social trust problem, not a purely technical one. Still, an 80-90% reduction in exposure through a technique that is free and easy seems hard to beat.&lt;/p&gt;
    &lt;p&gt;Related to the above, itâs unfortunate that cooldowns arenât baked directly into more packaging ecosystems: Dependabot and Renovate are great, but even better would be if the package manager itself (as the source of ground truth) could enforce cooldowns directly (including of dependencies not introduced or bumped through automated flows).&lt;/p&gt;
    &lt;p&gt;The problem being, succinctly: modern software stacks are complex and opaque, with little to no difference in privilege between first-party code and third-party dependencies.Â ↩&lt;/p&gt;
    &lt;p&gt;In part because of the prevalence of long-lived, overscoped credentials. Long-lived credentials let attackers operate on their own (comfortable) timelines; this is why Trusted Publishing is such a useful (but not wholly sufficient) technique for reducing the attackerâs attack staging window.Â ↩&lt;/p&gt;
    &lt;p&gt;Filippo Valsorda has an excellent compilation of recent supply chain compromises here.Â ↩&lt;/p&gt;
    &lt;p&gt;The xz-utils attack is a significant outlier, both in its scope and the length of its window of opportunity. In this case, Iâve measured from the attackerâs first backdoored release (v5.6.0, 2024-02-24) to the time of rollback within Debian (2024-03-28).Â ↩&lt;/p&gt;
    &lt;p&gt;For example, pnpmâs &lt;code&gt;minimumReleaseAge&lt;/code&gt;.
           uv also has &lt;code&gt;exclude-newer&lt;/code&gt;, 
           although this specifies an absolute cutoff rather than a rolling cooldown.Â ↩&lt;/p&gt;
    &lt;p&gt;Notably, the only attack that would have stymied a 14-day cooldown is xz-utils, which is also the most technically, logistically, and socially advanced of all of the attacks.Â ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005111</guid><pubDate>Fri, 21 Nov 2025 14:50:36 +0000</pubDate></item><item><title>Make product worse, get money</title><link>https://dynomight.net/worse/</link><description>&lt;doc fingerprint="652b19d9bbe30db8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Make product worse, get money&lt;/head&gt;
    &lt;p&gt;I recently asked why people seem to hate dating apps so much. In response, 80% of you emailed me some version of the following theory:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The thing about dating apps is that if they do a good job and match people up, then the matched people will quit the app and stop paying. So they have an incentive to string people along but not to actually help people find long-term relationships.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;May I explain why I don’t find this type of theory very helpful?&lt;/p&gt;
    &lt;p&gt;I’m not saying that I think it’s wrong, mind you. Rather, my objection is that while the theory is phrased in terms of dating apps, the same basic pattern applies to basically anyone who is trying to make money by doing anything.&lt;/p&gt;
    &lt;p&gt;For example, consider a pizza restaurant. Try these theories on for size:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza: “The thing about pizza restaurants is that if they use expensive ingredients or labor-intensive pizza-making techniques, then it costs more to make pizza. So they have an incentive to use low-cost ingredients and labor-saving shortcuts.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza II: “The thing about pizza restaurants is that if they have nice tables separated at a comfortable distance, then they can’t fit as many customers. So they have an incentive to use tiny tables and cram people in cheek by jowl.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza III: “The thing about pizza restaurants is that if they sell big pizzas, then people will eat them and stop being hungry, meaning they don’t buy additional pizza. So they have an incentive to serve tiny low-calorie pizzas.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See what I mean? You can construct similar theories for other domains, too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Cars: “The thing about automakers is that making cars safe is expensive. So they have an incentive to make unsafe cars.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Videos: “The thing about video streaming is that high-resolution video uses more expensive bandwidth. So they have an incentive to use low-resolution.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Blogging: “The thing about bloggers is that research is time-consuming. So they have an incentive to be sloppy about the facts.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Durability: “The thing about {lightbulb, car, phone, refrigerator, cargo ship} manufacturing is that if you make a {lightbulb, car, phone, refrigerator, cargo ship} that lasts a long time, then people won’t buy new ones. So there’s an incentive to make {lightbulbs, cars, phones, refrigerators, cargo ships} that break quickly.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All these theories can be thought of as instances of two general patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Make product worse, get money: “The thing about selling goods or services is that making goods or services better costs money. So people have an incentive to make goods and services worse.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Raise price, get money: “The thing about selling goods and services is that if you raise prices, then you get more money. So people have an incentive to raise prices.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Are these theories wrong? Not exactly. But it sure seems like something is missing.&lt;/p&gt;
    &lt;p&gt;I’m sure most pizza restauranteurs would be thrilled to sell lukewarm 5 cm cardboard discs for $300 each. They do in fact have an incentive to do that, just as predicted by these theories! Yet, in reality, pizza restaurants usually sell pizzas that are made out of food. So clearly these theories aren’t telling the whole story.&lt;/p&gt;
    &lt;p&gt;Say you have a lucrative business selling 5 cm cardboard discs for $300. I am likely to think, “I like money. Why don’t I sell pizzas that are only mostly cardboard, but also partly made of flour? And why don’t I sell them for $200, so I can steal Valued Reader’s customers?” But if I did that, then someone else would probably set prices at only $100, or even introduce cardboard-free pizzas, and this would continue until hitting some kind of equilibrium.&lt;/p&gt;
    &lt;p&gt;Sure, producers want to charge infinity dollars for things that cost them zero dollars to make. But consumers want to pay zero dollars for stuff that’s infinitely valuable. It’s in the conflict between these desires that all interesting theories live.&lt;/p&gt;
    &lt;p&gt;This is why I don’t think it’s helpful to point out that people have an incentive to make their products worse. Of course they do. The interesting question is, why are they able to get away with it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Reasons stuff is bad&lt;/head&gt;
    &lt;p&gt;First reason stuff is bad: People are cheap&lt;/p&gt;
    &lt;p&gt;Why are seats so cramped on planes? Is it because airlines are greedy? Sure. But while they might be greedy, I don’t think they’re dumb. If you do a little math, you can calculate that if airlines were to remove a single row of seats, they could add perhaps 2.5 cm (1 in) of extra legroom for everyone, while only decreasing the number of paying customers by around 3%. (This is based on a 737 with single-class, but you get the idea.)&lt;/p&gt;
    &lt;p&gt;So why don’t airlines rip out a row of seats, raise prices by 3% and enjoy the reduced costs for fuel and customer service? The only answer I can see is that people, on average, aren’t actually willing to pay 3% more for 2.5 cm more legroom. We want a worse but cheaper product, and so that’s what we get.&lt;/p&gt;
    &lt;p&gt;I think this is the most common reason stuff is “bad”. It’s why Subway sandwiches are so soggy, why video games are so buggy, and why IKEA furniture and Primark clothes fall apart so quickly.&lt;/p&gt;
    &lt;p&gt;It’s good when things are bad for this reason. Or at least, that’s the premise of capitalism: When companies cut costs, that’s the invisible hand redirecting resources to maximize social value, or whatever. Companies may be motivated by greed. And you may not like it, since you want to pay zero dollars for infinite value. But this is markets working as designed.&lt;/p&gt;
    &lt;p&gt;Second reason stuff is bad: Information asymmetries&lt;/p&gt;
    &lt;p&gt;Why is it that almost every book / blog / podcast about longevity is such garbage? Well, we don’t actually know many things that will reliably increase longevity. And those things are mostly all boring / hard / non-fun. And even if you do all of them, it probably only adds a couple of years in expectation. And telling people these facts is not a good way to find suckers who will pay you lots of money for your unproven supplements / seminars / etc.&lt;/p&gt;
    &lt;p&gt;True! But it doesn’t explain why all longevity stuff is so bad. Why don’t honest people tell the true story and drive all the hucksters out of business? I suspect the answer is that unless you have a lot of scientific training and do a lot of research, it’s basically impossible to figure out just how huckstery all the hucksters really are.&lt;/p&gt;
    &lt;p&gt;I think this same basic phenomenon explains why some supplements contain heavy metals, why some food contains microplastics, why restaurants use so much butter and salt, why rentals often have crappy insulation, and why most cars seem to only be safe along dimensions included in crash test scores. When consumers can’t tell good from evil, evil triumphs.&lt;/p&gt;
    &lt;p&gt;Third reason stuff is bad: People have bad taste&lt;/p&gt;
    &lt;p&gt;Sometimes stuff is bad because people just don’t appreciate the stuff you consider good. Examples are definitionally controversial, but I think this includes restaurants in cities where all restaurants are bad, North American tea, and travel pants. This reason has a blurry boundary with information asymmetries, as seen in ultrasonic humidifiers or products that use Sucralose instead of aspartame for “safety”.&lt;/p&gt;
    &lt;p&gt;Fourth reason stuff is bad: Pricing power&lt;/p&gt;
    &lt;p&gt;Finally, sometimes stuff is bad because markets aren’t working. Sometimes a company is selling a product but has some kind of “moat” that makes it hard for anyone else to compete with them, e.g. because of some technological or regulatory barrier, control of some key resource or location, intellectual property, a beloved brand, or network effects.&lt;/p&gt;
    &lt;p&gt;If that’s true, then those companies don’t have to worry as much about someone else stealing their business, and so (because everyone is axiomatically greedy) they will find ways to make their product cheaper and/or raise prices up until the price is equal to the full value it provides to the marginal consumer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Why is food so expensive at sporting events? Yes, people have no alternatives. But people know food is expensive at sporting events. And they don’t like it. Instead of selling water for $17, why don’t venues sell water for $2 and raise ticket prices instead? I don’t know. Probably something complicated, like that expensive food allows you to extract extra money from rich people without losing business from non-rich people.&lt;/p&gt;
    &lt;p&gt;So of course dating apps would love to string people along for years instead of finding them long-term relationships, so they keep paying money each month. I wouldn’t be surprised if some people at those companies have literally thought, “Maybe we should string people along for years instead of finding them long-term relationships, so they keep paying money each month, I love money so much.”&lt;/p&gt;
    &lt;p&gt;But if they are actually doing that (which is unclear to me) or if they are bad in some other way, then how do they get away with it? Why doesn’t someone else create a competing app that’s better and thereby steal all their business? It seems like the answer has to be either “because that’s impossible” or “because people don’t really want that”. That’s where the mystery begins.&lt;/p&gt;
    &lt;p&gt;Dating: A mysterious constellation of facts · life economics&lt;/p&gt;
    &lt;p&gt;So much blood · conspiracy economics&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005388</guid><pubDate>Fri, 21 Nov 2025 15:23:20 +0000</pubDate></item><item><title>Show HN: Wealthfolio 2.0- Open source investment tracker. Now Mobile and Docker</title><link>https://wealthfolio.app/?v=2.0</link><description>&lt;doc fingerprint="27ab40bb69b94b92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grow Wealth. Keep Control.&lt;/head&gt;
    &lt;head rend="h2"&gt;A beautiful, Private and Open-Source investment tracker that runs locally on all your devices.&lt;/head&gt;
    &lt;head rend="h2"&gt;WHY CHOOSE WEALTHFOLIO?&lt;/head&gt;
    &lt;p&gt;A beautiful portfolio tracker that respects your privacy and your data&lt;/p&gt;
    &lt;head rend="h3"&gt;Privacy-First Approach&lt;/head&gt;
    &lt;p&gt;Your data never leaves your device. As an open-source project, we prioritize security and transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple and Beautifully Crafted&lt;/head&gt;
    &lt;p&gt;Powerful features wrapped in an elegant, easy-to-use interface. Simplicity meets sophistication.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Hidden Costs&lt;/head&gt;
    &lt;p&gt;Free to use with optional one-time payment. No subscriptions or recurring fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE ESSENTIALS YOU NEED TO TRACK YOUR WEALTH&lt;/head&gt;
    &lt;p&gt;No More Messy Spreadsheets or Privacy Concerns - Just You and Your Secure, Personal Wealth Companion Application&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Aggregation&lt;/head&gt;
    &lt;p&gt;Gather all your investment and savings accounts in one place. See everything at a glance, from stocks to savings! Import your CSV statements from your broker or bank.&lt;/p&gt;
    &lt;head rend="h4"&gt;Comprehensive View&lt;/head&gt;
    &lt;p&gt;See all your accounts in one place.&lt;/p&gt;
    &lt;head rend="h4"&gt;CSV Import&lt;/head&gt;
    &lt;p&gt;Easily import your CSV statements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Holdings Overview&lt;/head&gt;
    &lt;p&gt;Get a clear picture of what's in your portfolio. Stocks, ETFs, or Cryptocurrencies - know what you have and how it's performing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Portfolio Insights&lt;/head&gt;
    &lt;p&gt;Understand your asset allocation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance Tracking&lt;/head&gt;
    &lt;p&gt;Monitor how your investments are doing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance Dashboard&lt;/head&gt;
    &lt;p&gt;See how your investments stack up, all in one place. Compare your accounts side by side, check if you are beating the S&amp;amp;P 500, and track your favorite ETFs without the hassle. No fancy jargon - just clear, useful charts that help you understand how your money is really doing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compare Your Accounts&lt;/head&gt;
    &lt;p&gt;See which accounts are doing best.&lt;/p&gt;
    &lt;head rend="h4"&gt;Beat the Market?&lt;/head&gt;
    &lt;p&gt;Check how you stack up against some popular indexes and ETFs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Income Tracking&lt;/head&gt;
    &lt;p&gt;Monitor dividends and interest income across your entire portfolio. Get a clear view of your passive income streams, helping you make informed decisions about your investments.&lt;/p&gt;
    &lt;head rend="h4"&gt;Dividend Monitoring&lt;/head&gt;
    &lt;p&gt;Track your dividend income.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interest Income&lt;/head&gt;
    &lt;p&gt;Keep an eye on interest earnings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Performance&lt;/head&gt;
    &lt;p&gt;Track your accounts' holdings and performance over time. See how a particular account is performing, and how it's changing over time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Historical Data&lt;/head&gt;
    &lt;p&gt;View past performance trends.&lt;/p&gt;
    &lt;head rend="h4"&gt;Account Analysis&lt;/head&gt;
    &lt;p&gt;Analyze individual account performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goals Tracking&lt;/head&gt;
    &lt;p&gt;Set your savings targets clearly. Distribute your funds across these objectives, assigning a specific percentage to each. Keep an eye on your progress.&lt;/p&gt;
    &lt;head rend="h4"&gt;Target Setting&lt;/head&gt;
    &lt;p&gt;Define your financial goals.&lt;/p&gt;
    &lt;head rend="h4"&gt;Progress Monitoring&lt;/head&gt;
    &lt;p&gt;Track your progress towards goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Contribution Rooms and Limit Tracking&lt;/head&gt;
    &lt;p&gt;Stay on top of your contribution limits for tax-advantaged accounts like IRAs, 401(k)s, or TFSAs. Track your available contribution room and avoid over-contributing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Limit Awareness&lt;/head&gt;
    &lt;p&gt;Know your contribution limits.&lt;/p&gt;
    &lt;head rend="h4"&gt;Avoid Over-Contribution&lt;/head&gt;
    &lt;p&gt;Prevent excess contributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extend Wealthfolio with Powerful Add-ons&lt;/head&gt;
    &lt;head rend="h3"&gt;Investment Fees Tracker&lt;/head&gt;
    &lt;p&gt;Track and analyze investment fees across your portfolio with detailed analytics and insights&lt;/p&gt;
    &lt;head rend="h3"&gt;Goal Progress Tracker&lt;/head&gt;
    &lt;p&gt;Track your investment progress towards target amounts with a visual representation&lt;/p&gt;
    &lt;head rend="h3"&gt;Stock Trading Tracker&lt;/head&gt;
    &lt;p&gt;Simple swing stock trading tracker with performance analytics and calendar views&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006016</guid><pubDate>Fri, 21 Nov 2025 16:34:52 +0000</pubDate></item><item><title>You can make PS2 games in JavaScript</title><link>https://jslegenddev.substack.com/p/you-can-now-make-ps2-games-in-javascript</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006082</guid><pubDate>Fri, 21 Nov 2025 16:42:19 +0000</pubDate></item><item><title>Pivot Robotics (YC W24) Is Hiring for an Industrial Automation Hardware Engineer</title><link>https://www.ycombinator.com/companies/pivot-robotics/jobs/7xG9Dc6-mechanical-engineer-controls</link><description>&lt;doc fingerprint="6e34f445ca77fde2"&gt;
  &lt;main&gt;
    &lt;p&gt;AI for Robot Arms in Factories&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Pivot Robots (YC W24) is building the AI brain for robot arms for high-mix manufacturing.&lt;/p&gt;
    &lt;p&gt;Pivot Robots combines off-the-shelf robots and vision sensors with recent breakthroughs in foundation vision models to give industrial robot arms the power to adapt. Our first product directly addresses the dangerous and unpopular task of grinding metal parts. Currently, our software is being deployed on 10+ robots at a large cast iron foundry.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006250</guid><pubDate>Fri, 21 Nov 2025 17:00:00 +0000</pubDate></item><item><title>Solving Fizz Buzz with Cosines</title><link>https://susam.net/fizz-buzz-with-cosines.html</link><description>&lt;doc fingerprint="12d799356860fab4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Solving Fizz Buzz with Cosines&lt;/head&gt;
    &lt;p&gt;Fizz Buzz is a counting game that has become oddly popular in the world of computer programming as a simple test of basic programming skills. The rules of the game are straightforward. Players say the numbers aloud in order beginning with one. Whenever a number is divisible by 3, they say 'Fizz' instead. If it is divisible by 5, they say 'Buzz'. If it is divisible by both 3 and 5, the player says both 'Fizz' and 'Buzz'. Here is a typical Python program that prints this sequence:&lt;/p&gt;
    &lt;code&gt;for n in range(1, 101):
    if n % 15 == 0:
        print('FizzBuzz')
    elif n % 3 == 0:
        print('Fizz')
    elif n % 5 == 0:
        print('Buzz')
    else:
        print(n)
&lt;/code&gt;
    &lt;p&gt;Here is the output: fizz-buzz.txt. Can we make the program more complicated? The words 'Fizz', 'Buzz' and 'FizzBuzz' repeat in a periodic manner throughout the sequence. What else is periodic? Trigonometric functions! Perhaps we can use trigonometric functions to encode all four rules of the sequence in a single closed-form expression. That is what we are going to explore in this article. By the end, we will obtain a discrete Fourier series that can take any integer \( n \) and select the corresponding text to be printed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Definitions&lt;/head&gt;
    &lt;p&gt;Before going any further, we establish a precise mathematical definition for the Fizz Buzz sequence. We begin by introducing a few functions that will help us define the Fizz Buzz sequence later.&lt;/p&gt;
    &lt;head rend="h3"&gt;Symbol Functions&lt;/head&gt;
    &lt;p&gt;We define a set of four functions \( \{ s_0, s_1, s_2, s_3 \} \) for integers \( n \) by: \begin{align*} s_0(n) &amp;amp;= n, \\ s_1(n) &amp;amp;= \mathtt{Fizz}, \\ s_2(n) &amp;amp;= \mathtt{Buzz}, \\ s_3(n) &amp;amp;= \mathtt{FizzBuzz}. \end{align*} We call these the symbol functions because they produce every term that appears in the Fizz Buzz sequence. The symbol function \( s_0 \) returns \( n \) itself. The functions \( s_1, \) \( s_2 \) and \( s_3 \) are constant functions that always return the literal words \( \mathtt{Fizz}, \) \( \mathtt{Buzz} \) and \( \mathtt{FizzBuzz} \) respectively, no matter what the value of \( n \) is.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fizz Buzz Sequence&lt;/head&gt;
    &lt;p&gt;Now we can define the Fizz Buzz sequence as the sequence \[ (s_{f(n)}(n))_{n = 1}^{\infty} \] where \[ f(n) = \begin{cases} 1 &amp;amp; \text{if } 3 \mid n \text{ and } 5 \nmid n, \\ 2 &amp;amp; \text{if } 3 \nmid n \text{ and } 5 \mid n, \\ 3 &amp;amp; \text{if } 3 \mid n \text{ and } 5 \mid n, \\ 0 &amp;amp; \text{otherwise}. \end{cases} \] The notation \( m \mid n \) means that the integer \( m \) divides the integer \( n, \) i.e. \( n \) is a multiple of \( m. \) Equivalently, there exists an integer \( c \) such that \( n = cm . \) Similarly, \( m \nmid n \) means that \( m \) does not divide \( n, \) i.e. \( n \) is not a multiple of \( m. \) With the above definitions in place, we can expand the first few terms of the sequence explicitly as follows: \begin{align*} (s_{f(n)}(n))_{n = 1}^{\infty} &amp;amp;= (s_{f(1)}(1), \; s_{f(2)}(2), \; s_{f(3)}(3), \; s_{f(4)}(4), \; s_{f(5)}(5), \; s_{f(6)}(6), \; s_{f(7)}(7), \; \dots) \\ &amp;amp;= (s_0(1), \; s_0(2), \; s_1(3), \; s_0(4), s_2(5), \; s_1(6), \; s_0(7), \; \dots) \\ &amp;amp;= (1, \; 2, \; \mathtt{Fizz}, \; 4, \; \mathtt{Buzz}, \; \mathtt{Fizz}, \; 7, \; \dots). \end{align*} Note how the function \( f(n) \) produces an index \( i \) which we then use to select the symbol function \( s_i(n) \) to produce the \( n \)th term of the sequence. We therefore call \( f(n) \) the index function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicator Functions&lt;/head&gt;
    &lt;p&gt;Here is the index function \( f(n) \) from the previous section with its cases and conditions rearranged to make it easier to spot interesting patterns: \[ f(n) = \begin{cases} 0 &amp;amp; \text{if } 5 \nmid n \text{ and } 3 \nmid n, \\ 1 &amp;amp; \text{if } 5 \nmid n \text{ and } 3 \mid n, \\ 2 &amp;amp; \text{if } 5 \mid n \text{ and } 3 \nmid n, \\ 3 &amp;amp; \text{if } 5 \mid n \text{ and } 3 \mid n. \end{cases} \] This function helps us to select another function \( s_{f(n)}(n) \) which in turn determines the \( n \)th term of the Fizz Buzz sequence. Our goal now is to replace this piecewise formula with a single closed-form expression. To do so, we first define indicator functions \( I_m(n) \) as follows: \[ I_m(n) = \begin{cases} 1 &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] The formula for \( f(n) \) can now be written as: \[ f(n) = \begin{cases} 0 &amp;amp; \text{if } I_5(n) = 0 \text{ and } I_3(n) = 0, \\ 1 &amp;amp; \text{if } I_5(n) = 0 \text{ and } I_3(n) = 1, \\ 2 &amp;amp; \text{if } I_5(n) = 1 \text{ and } I_3(n) = 0, \\ 3 &amp;amp; \text{if } I_5(n) = 1 \text{ and } I_3(n) = 1. \end{cases} \] Do you see a pattern? Here is the same function written as a table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\( I_5(n) \)&lt;/cell&gt;
        &lt;cell role="head"&gt;\( I_3(n) \)&lt;/cell&gt;
        &lt;cell role="head"&gt;\( f(n) \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 2 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 3 \)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Do you see it now? If we treat the values in the first two columns as binary digits and the values in the third column as decimal numbers, then in each row the first two columns give the binary representation of the number in the third column. For example, \( 3_{10} = 11_2 \) and indeed in the last row of the table, we see the bits \( 1 \) and \( 1 \) in the first two columns and the number \( 3 \) in the last column. In other words, writing the binary digits \( I_5(n) \) and \( I_3(n) \) side by side gives us the binary representation of \( f(n). \) Therefore \[ f(n) = 2 \, I_5(n) + I_3(n). \] We can now write a small program to demonstrate this formula:&lt;/p&gt;
    &lt;code&gt;
 for n in range(1, 101):
    s = [n, 'Fizz', 'Buzz', 'FizzBuzz']
    i = (n % 3 == 0) + 2 * (n % 5 == 0)
    print(s[i])
&lt;/code&gt;
    &lt;p&gt;We can make it even shorter at the cost of some clarity:&lt;/p&gt;
    &lt;code&gt;
 for n in range(1, 101):
    print([n, 'Fizz', 'Buzz', 'FizzBuzz'][(n % 3 == 0) + 2 * (n % 5 == 0)])
&lt;/code&gt;
    &lt;p&gt;What we have obtained so far is pretty good. While there is no universal definition of a closed-form expression, I think most people would agree that the indicator functions as defined above are simple enough to be permitted in a closed-form expression.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complex Exponentials&lt;/head&gt;
    &lt;p&gt;In the previous section, we obtained the formula \[ f(n) = I_3(n) + 2 \, I_5(n) \] which we then used as an index to look up the text to be printed. We also argued that this is a pretty good closed-form expression already.&lt;/p&gt;
    &lt;p&gt;However, in the interest of making things more complicated, we must ask ourselves: What if we are not allowed to use the indicator functions? What if we must adhere to the commonly accepted meaning of a closed-form expression which allows only finite combinations of basic operations such as addition, subtraction, multiplication, division, integer exponents and roots with integer index as well as functions such as exponentials, logarithms and trigonometric functions. It turns out that the above formula can be rewritten using only addition, multiplication, division and the cosine function. Let us begin the translation. Consider the sum \[ S_m(n) = \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m}, \] where \( i \) is the imaginary unit and \( n \) and \( m \) are integers. This is a geometric series in the complex plane with ratio \( r = e^{2 \pi i n / m}. \) If \( n \) is a multiple of \( m , \) then \( n = cm \) for some integer \( c \) and we get \[ r = e^{2 \pi i n / m} = e^{2 \pi i c} = 1. \] Therefore, when \( n \) is a multiple of \( m, \) we get \[ S_m(n) = \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m} = \sum_{k = 0}^{m - 1} 1^k = m. \] If \( n \) is not a multiple of \( m, \) then \( r \ne 1 \) and the geometric series becomes \[ S_m(n) = \frac{r^m - 1}{r - 1} = \frac{e^{2 \pi i n} - 1}{e^{2 \pi i n / m} - 1} = 0. \] Therefore, \[ S_m(n) = \begin{cases} m &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] Dividing both sides by \( m, \) we get \[ \frac{S_m(n)}{m} = \begin{cases} 1 &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] But the right-hand side is \( I_m(n). \) Therefore \[ I_m(n) = \frac{S_m(n)}{m} = \frac{1}{m} \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m}. \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Cosines&lt;/head&gt;
    &lt;p&gt;We begin with Euler's formula \[ e^{i x} = \cos x + i \sin x \] where \( x \) is a real number. From this formula, we get \[ e^{i x} + e^{-i x} = 2 \cos x. \] Therefore \begin{align*} I_3(n) &amp;amp;= \frac{1}{3} \sum_{k = 0}^2 e^{2 \pi i k n / 3} \\ &amp;amp;= \frac{1}{3} \left( 1 + e^{2 \pi i n / 3} + e^{4 \pi i n / 3} \right) \\ &amp;amp;= \frac{1}{3} \left( 1 + e^{2 \pi i n / 3} + e^{-2 \pi i n / 3} \right) \\ &amp;amp;= \frac{1}{3} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right). \end{align*} The third equality above follows from the fact that \( e^{4 \pi i n / 3} = e^{6 \pi i n / 3} e^{-2 \pi i n / 3} = e^{2 \pi i n} e^{-2 \pi i n/3} = e^{-2 \pi i n / 3}. \)&lt;/p&gt;
    &lt;p&gt;The function above is defined for integer values of \( n \) but we can extend its formula to real \( x \) and plot it to observe its shape between integers. As expected, the function takes the value \( 1 \) whenever \( x \) is an integer multiple of \( 3 \) and \( 0 \) whenever \( x \) is an integer not divisible by \( 3. \)&lt;/p&gt;
    &lt;p&gt;Similarly, \begin{align*} I_5(n) &amp;amp;= \frac{1}{5} \sum_{k = 0}^4 e^{2 \pi i k n / 5} \\ &amp;amp;= \frac{1}{5} \left( 1 + e^{2 \pi i n / 5} + e^{4 \pi i n / 5} + e^{6 \pi i n / 5} + e^{8 \pi i n / 5} \right) \\ &amp;amp;= \frac{1}{5} \left( 1 + e^{2 \pi i n / 5} + e^{4 \pi i n / 5} + e^{-4 \pi i n / 5} + e^{-2 \pi i n / 5} \right) \\ &amp;amp;= \frac{1}{5} + \frac{2}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{2}{5} \cos \left( \frac{4 \pi n}{5} \right). \end{align*} Extending this expression to real values of \( x \) allows us to plot its shape as well. Once again, the function takes the value \( 1 \) at integer multiples of \( 5 \) and \( 0 \) at integers not divisible by \( 5. \)&lt;/p&gt;
    &lt;p&gt;Recall that we expressed \( f(n) \) as \[ f(n) = I_3(n) + 2 \, I_5(n). \] Substituting these trigonometric expressions yields \[ f(n) = \frac{1}{3} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + 2 \cdot \left( \frac{1}{5} + \frac{2}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{2}{5} \cos \left( \frac{4 \pi n}{5} \right) \right). \] A straightforward simplification gives \[ f(n) = \frac{11}{15} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right). \] We can extend this expression to real \( x \) and plot it as well. The resulting curve takes the values \( 0, 1, 2 \) and \( 3 \) at integer points, as desired.&lt;/p&gt;
    &lt;p&gt;Now we can write our Python program as follows:&lt;/p&gt;
    &lt;code&gt;
 from math import cos, pi
for n in range(1, 101):
    s = [n, 'Fizz', 'Buzz', 'FizzBuzz']
    i = round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3)
                      + (4 / 5) * cos(2 * pi * n / 5)
                      + (4 / 5) * cos(4 * pi * n / 5))
    print(s[i])
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discrete Fourier Transform&lt;/head&gt;
    &lt;p&gt;The keen-eyed might notice that the expression we obtained for \( f(n) \) is a finite Fourier series. This is not surprising, since the output of a Fizz Buzz programme depends only on \( n \bmod 15 . \) Any function on a finite cyclic group can be written exactly as a finite Fourier expansion. In this section, we recover the same function \( f(n) \) using the discrete Fourier transform. It is worth mentioning here that the calculations presented here are quite tedious to do by hand. Nevertheless, this section offers a glimpse of how such coefficients are calculated. By the end, we will arrive at exactly the same \( f(n) \) as before. There is nothing new to discover here. We simply obtain the result by a different, more direct and noticeably tedious method. If this doesn't sound interesting to you, you may safely skip this section.&lt;/p&gt;
    &lt;p&gt;We know that \( f(n) \) is a periodic function with period \( 15. \) To apply the discrete Fourier transform, we look at one complete period using the indices \( n = 1, 2, \dots, 15. \) Over this period, the values are: \begin{array}{c|ccccccccccccccc} n &amp;amp; 1 &amp;amp; 2 &amp;amp; 3 &amp;amp; 4 &amp;amp; 5 &amp;amp; 6 &amp;amp; 7 &amp;amp; 8 &amp;amp; 9 &amp;amp; 10 &amp;amp; 11 &amp;amp; 12 &amp;amp; 13 &amp;amp; 14 &amp;amp; 15 \\ \hline f(n) &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 2 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 3 \end{array} The discrete Fourier transform gives us the constants defined by: \[ c_k = \frac{1}{15} \sum_{n = 1}^{15} f(n) e^{-2 \pi i k n / 15}, \] for \( k = 1, 2, \dots, 15. \) These constants reconstruct \( f(n) \) via the inverse transform: \[ f(n) = \sum_{k = 1}^{15} c_k e^{2 \pi i k n / 15} \] where \( n \in \mathbb{Z}. \) Let us compute the first constant: \[ c_1 = \frac{1}{15}\left( \begin{aligned} e^{-2 \pi i \cdot 3/15} &amp;amp;+ 2 e^{-2 \pi i \cdot 5/15} + e^{-2 \pi i \cdot 6/15} + e^{-2 \pi i \cdot 9/15} \\ &amp;amp;+ 2 e^{-2 \pi i \cdot 10/15} + e^{-2 \pi i \cdot 12/15} + 3 e^{-2 \pi i \cdot 15/15} \end{aligned} \right) = 0. \] Similarly, \[ c_2 = \frac{1}{15}\left( \begin{aligned} e^{-2 \pi i \cdot 6/15} &amp;amp;+ 2 e^{-2 \pi i \cdot 10/15} + e^{-2 \pi i \cdot 12/15} + e^{-2 \pi i \cdot 18/15} \\ &amp;amp;+ 2 e^{-2 \pi i \cdot 20/15} + e^{-2 \pi i \cdot 24/15} + 3 e^{-2 \pi i \cdot 30/15} \end{aligned} \right) = 0. \] and \[ c_3 = \frac{1}{15}\left( \begin{aligned} e^{-2 \pi i \cdot 9/15} &amp;amp;+ 2 e^{-2 \pi i \cdot 15/15} + e^{-2 \pi i \cdot 18/15} + e^{-2 \pi i \cdot 27/15} \\ &amp;amp;+ 2 e^{-2 \pi i \cdot 30/15} + e^{-2 \pi i \cdot 36/15} + 3 e^{-2 \pi i \cdot 45/15} \end{aligned} \right) = \frac{2}{5}. \] Continuing in this manner, we find all the constants: \begin{align*} c_1 &amp;amp;= c_2 = c_4 = c_7 = c_8 = c_{11} = c_{13} = c_{14} = 0, \\ c_3 &amp;amp;= c_6 = c_9 = c_{12} = \frac{2}{5}, \\ c_5 &amp;amp;= c_{10} = \frac{1}{3}, \\ c_{15} &amp;amp;= \frac{11}{15}. \end{align*} Using the inverse transform, we get \begin{align*} f(n) &amp;amp;= \sum_{k = 1}^{15} c_k \, e^{2 \pi i k n / 15} \\ &amp;amp;= \frac{11}{15} + \frac{2}{5} \left( e^{2 \pi i \cdot 3n/15} + e^{2 \pi i \cdot 6n/15} + e^{2 \pi i \cdot 9n/15} + e^{2 \pi i \cdot 12n/15} \right) \\ &amp;amp;\phantom{=} \phantom{\frac{11}{15}} + \frac{1}{3} \left( e^{2 \pi i \cdot 5n/15} + e^{2 \pi i \cdot 10n/15} \right) \\ &amp;amp;= \frac{11}{15} + \frac{2}{5} \left( e^{6 \pi i n/15} + e^{12 \pi i n/15} + e^{18 \pi i n/15} + e^{24 \pi i n/15} \right) \\ &amp;amp;\phantom{=} \phantom{\frac{11}{15}} + \frac{1}{3} \left( e^{10 \pi i n/15} + e^{20 \pi i n/15} \right) \\ &amp;amp;= \frac{11}{15} + \frac{2}{5} \left( e^{6 \pi i n/15} + e^{12 \pi i n/15} + e^{-12 \pi i n/15} + e^{-6 \pi i n/15} \right) \\ &amp;amp;\phantom{=} \phantom{\frac{11}{15}} + \frac{1}{3} \left( e^{10 \pi i n/15} + e^{-10 \pi i n/15} \right) \\ &amp;amp;= \frac{11}{15} + \frac{2}{5} \left( 2 \cos \left( \frac{6 \pi n}{15} \right) + 2 \cos \left( \frac{12 \pi n}{15} \right) \right) \\ &amp;amp;\phantom{=} \phantom{\frac{11}{15}} + \frac{1}{3} \left( 2 \cos \left( \frac{10 \pi n}{15} \right) \right) \\ &amp;amp;= \frac{11}{15} + \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right) + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right). \end{align*} This gives us exactly the same function \( f(n) \) we obtained earlier. The difference is only in how we arrived there. Working out Fourier coefficients by hand is slow and mechanical. In practice these sums are almost always computed automatically by numerical software or computer algebra systems. Still, this exercise shows that our humble Fizz Buzz index function can be expressed precisely using the machinery of Fourier analysis.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;To summarise, we have defined the Fizz Buzz sequence as \[ (s_{f(n)}(n))_{n = 1}^{\infty} \] where \[ f(n) = \frac{11}{15} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right). \] and \( s_0(n) = n, \) \( s_1(n) = \mathtt{Fizz}, \) \( s_2(n) = \mathtt{Buzz} \) and \( s_3(n) = \mathtt{FizzBuzz}. \) A Python program to print the Fizz Buzz sequence based on this definition was presented earlier. That program can be written more succinctly as follows:&lt;/p&gt;
    &lt;code&gt;
 from math import cos, pi
for n in range(1, 101):
    print([n, 'Fizz', 'Buzz', 'FizzBuzz'][round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3) + (4 / 5) * (cos(2 * pi * n / 5) + cos(4 * pi * n / 5)))])
&lt;/code&gt;
    &lt;p&gt;We can also wrap this up nicely in a shell one-liner, in case you want to share it with your friends and family and surprise them:&lt;/p&gt;
    &lt;code&gt;python3 -c 'from math import cos, pi; [print([n, "Fizz", "Buzz", "FizzBuzz"][round(11/15 + (2/3) * cos(2*pi*n/3) + (4/5) * (cos(2*pi*n/5) + cos(4*pi*n/5)))]) for n in range(1, 101)]'&lt;/code&gt;
    &lt;p&gt;We have taken a simple counting game and turned it into a trigonometric construction: a finite Fourier series with a constant term \( 11/15 \) and three cosine terms with coefficients \( 2/3, \) \( 4/5 \) and \( 4/5. \) None of this makes Fizz Buzz any easier, of course, but it does show that every \( \mathtt{Fizz} \) and \( \mathtt{Buzz} \) now owes its existence to a particular set of Fourier coefficients. We began with the modest goal of making this simple problem more complicated. I think it is safe to say that we did not fall short.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006598</guid><pubDate>Fri, 21 Nov 2025 17:28:25 +0000</pubDate></item><item><title>Helping Valve to power up Steam devices</title><link>https://www.igalia.com/2025/11/helpingvalve.html</link><description>&lt;doc fingerprint="961b0d5348912672"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Helping Valve to Power Up Steam Devices&lt;/head&gt;
    &lt;p&gt;Last week, Valve stunned the computer gaming world by unveiling three new gaming devices at once: the Steam Frame, a wireless VR headset; the Steam Machine, a gaming console in the vein of a PlayStation or Xbox; and the Steam Controller, a handheld game controller. Successors to the highly successful Valve Index and Steam Deck, these devices are set to be released in the coming year.&lt;/p&gt;
    &lt;p&gt;Igalia has long worked with Valve on SteamOS, which will power the Machine and Frame, and is excited to be contributing to these new devices, particularly the Frame. The Frame, unlike the Machine or Deck which have x86 CPUs, runs on an ARM-based CPU.&lt;/p&gt;
    &lt;p&gt;Under normal circumstances, this would mean that only games compiled to run on ARM chips could be played on the Frame. In order to get around this barrier, a translation layer called FEX is used to run applications compiled for x86 chips (which are used in nearly all gaming PCs) on ARM chips by translating the x86 machine code into ARM64 machine code.&lt;/p&gt;
    &lt;p&gt;âIf you love video games, like I do, working on FEX with Valve is a dream come true,â said Paulo Matos, an engineer with Igaliaâs Compilers Team. Even so, the challenges can be daunting, because making sure the translation is working often requires manual QA rather than automated testing. âYou have to start a game, sometimes the error shows up in the colors or sound, or how the game behaves when you break down the door in the second level. Just debugging this can take a while,â said Matos. âFor optimization work I did early last year, I used a game called Psychonauts to test it. I must have played the first 3 to 4 minutes of the game many, many times for debugging. Looking at my history, Steam tells me I played it for 29 hours, but it was always the first few minutes, nothing else.â&lt;/p&gt;
    &lt;p&gt;Beyond the CPU, the Qualcomm Adreno 750 GPU used in the Steam Frame introduced its own set of challenges when it came to running desktop games, and other complex workloads, on these devices. Doing so requires a rock-solid Vulkan driver that can ensure correctness, eliminating major rendering bugs, while maintaining high performance. This is a very difficult combination to achieve, and yet thatâs exactly what weâve done for Valve with Mesa3D Turnip, a FOSS Vulkan driver for Qualcomm Adreno GPUs.&lt;/p&gt;
    &lt;p&gt;Before we started our work, critical optimizations such as LRZ (which you can learn more about from our blog post here) or the autotuner (and its subsequent overhaul) werenât in place. Even worse, there wasnât support for the Adreno 700-series GPUs at all, which we eventually added along with support for tiled rendering.&lt;/p&gt;
    &lt;p&gt;âWe implemented many Vulkan extensions and reviewed numerous others,â said Danylo Piliaiev, an engineer on the Graphics Team. âOver the years, we ensured that D3D11, D3D12, and OpenGL games rendered correctly through DXVK, vkd3d-proton, and Zink, investigating many rendering issues along the way. We achieved higher correctness than the proprietary driver and, in many cases, Mesa3D Turnip is faster as well.â&lt;/p&gt;
    &lt;p&gt;Weâve worked with many wonderful people from Valve, Google, and other companies to iterate on the Vulkan driver over the years in order to introduce new features, bug fixes, performance improvements, as well as debugging workflows. Some of those people decided to join Igalia later on, such as our colleague and Graphics Team developer Emma Anholt. âIâve been working on Mesa for 22 years, and itâs great to have a home now where I can keep doing that work, across hardware projects, where the organization prioritizes the work experience of its developers and empowers them within the organization.â&lt;/p&gt;
    &lt;p&gt;Valveâs support in all this cannot be understated, either. Their choice to build their devices using open software like Mesa3D Turnip and FEX means theyâre committed to working on and supporting improvements and optimizations that become available to anyone who uses the same open-source projects.&lt;/p&gt;
    &lt;p&gt;âWeâve received a lot of positive feedback about significantly improved performance and fewer rendering glitches from hobbyists who use these projects to run PC games on Android phones as a result of our work,â said Dhruv Mark Collins, another Graphics Team engineer working on Turnip. âAnd it goes both ways! Weâve caught a couple of nasty bugs because of that widespread testing, which really emphasizes why the FOSS model is beneficial for everyone involved.â&lt;/p&gt;
    &lt;p&gt;An interesting area of graphics driver development is all the compiler work that is involved. Vulkan drivers such as Mesa3D Turnip need to process shader programs sent by the application to the GPU, and these programs govern how pixels in our screens are shaded or colored with geometry, textures, and lights while playing games. Job Noorman, an engineer from our Compilers Team, made significant contributions to the compiler used by Mesa3D Turnip. He also contributed to the Mesa3D NIR shader compiler, a common part that all Mesa drivers use, including RADV (most popularly used on the Steam Deck) or V3DV (used on Raspberry Pi boards).&lt;/p&gt;
    &lt;p&gt;As is normal for Igalia, while we focused on delivering results for our customer, we also made our work as widely useful as possible. For example: âWhile our target throughout our work has been the Snapdragon 8 Gen 3 thatâs in the Frame, much of our work extends back through years of Snapdragon hardware, and we regression test it to make sure it stays Vulkan conformant,â said Anholt. This means that Igaliaâs work for the Frame has consistently passed Vulkanâs Conformance Test Suite (CTS) of over 2.8 million tests, some of which Igalia is involved in creating.&lt;/p&gt;
    &lt;p&gt;Our very own Vulkan CTS expert Ricardo GarcÃa says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Igalia and other Valve contractors actively participate in several areas inside the Khronos Group, the organization maintaining and developing graphics API standards like Vulkan. We contribute specification fixes and feedback, and we are regularly involved in the development of many new Vulkan extensions. Some of these end up being critical for game developers, like mesh shading. Others ensure a smooth and efficient translation of other APIs like DirectX to Vulkan, or help take advantage of hardware features to ensure applications perform great across multiple platforms, both mobile like the Steam Frame or desktop like the Steam Machine. Having Vulkan CTS coverage for these new extensions is a critical step in the release process, helping make sure the specification is clear and drivers implement it correctly, and Igalia engineers have contributed millions of source code lines and tests since our collaboration with Valve started.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A huge challenge we faced in moving forward with development is ensuring that we didnât introduce regressions, small innocent-seeming changes can completely break rendering on games in a way that even CTS might not catch. What automated testing could be done was often quite constrained, but Igalians found ways to push through the barriers. âI made a continuous integration test to automatically run single-frame captures of a wide range of games spanning D3D11, D3D9, D3D8, Vulkan, and OpenGL APIs,â said Piliaiev, about the development covered in his recent XDC 2025 talk, âensuring that we donât have rendering or performance regressions.â&lt;/p&gt;
    &lt;p&gt;Looking ahead, Igaliaâs work for Valve will continue to deliver benefits to the wider Linux Gaming ecosystem. For example, the Steam Frame, as a battery-powered VR headset, needs to deliver high performance within a limited power budget. A way to address this is to create a more efficient task scheduler, which is something Changwoo Min of Igaliaâs Kernel Team has been working on. As he says, âI have been developing a customized CPU scheduler for gaming, named LAVD: Latency-criticality Aware Virtual Deadline scheduler.â&lt;/p&gt;
    &lt;p&gt;In general terms, a scheduler automatically identifies critical tasks and dynamically boosts their deadlines to improve responsiveness. Most task schedulers donât take energy consumption into account, but the Rust-based LAVD is different. âLAVD makes scheduling decisions considering each chipâs performance versus energy trade-offs. It measures and predicts the required computing power on the fly, then selects the best set of CPUs to meet that demand with minimal energy consumption,â said Min.&lt;/p&gt;
    &lt;p&gt;One of our other kernel engineers, Melissa Wen, has been working on AMD kernel display drivers to maintain good color management and HDR support for SteamOS across AMD hardware families, both for the Steam Deck and the Steam Machine. This is especially important with newer display hardware in the Steam Machine, which features some notable differences in color capabilities, aiming for more powerful and efficient color management which necessitated driver work.&lt;/p&gt;
    &lt;p&gt;â¦and thatâs a wrap! We will continue our efforts toward improving future versions of SteamOS, and with a partner as strongly supportive as Valve, we expect to do more work to make Linux gaming even better. If any of that sounded interesting and youâd like to work with us to tackle tricky problems of your own, please get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006616</guid><pubDate>Fri, 21 Nov 2025 17:29:59 +0000</pubDate></item><item><title>Discontinuation of ARM Notebook with Snapdragon X Elite SoC</title><link>https://www.tuxedocomputers.com/en/Discontinuation-of-ARM-notebooks-with-Snapdragon-X-Elite-SoC.tuxedo</link><description>&lt;doc fingerprint="600c35d002b48faf"&gt;
  &lt;main&gt;
    &lt;p&gt;We ship your order to almost all countries, in Europe mostly even free of charge! The respective shipping costs and the cost threshold above which we will cover the costs for you can be found here or for international shipping in the table below.&lt;/p&gt;
    &lt;p&gt;There are no shipping costs within Germany for goods worth €100 or more.&lt;/p&gt;
    &lt;p&gt;No matter how many small articles you order, such as USB stick card reader, LAN adapters or fan articles, with us, you pay a maximum of 7.99 € shipping costs.&lt;/p&gt;
    &lt;p&gt;You can check all occurring shipping costs or if we even deliver for free right before sending your order!&lt;/p&gt;
    &lt;p&gt;Here are the shipping costs as well as the amount threshold for your order. The threshold is referring to the total amount of your order, which enables free shipping.&lt;/p&gt;
    &lt;p&gt;For orders outside the EU there might be additional duties, taxes or charges needed to be paid by the customer. These don't have to be paid to the supplier, but to local authorities. Please check for any details with your local customs or tax authorities before ordering! But as a benefit you don't have to pay German taxes, this means you save up to 19%!&lt;lb/&gt; Due to the Brexit and the associated changes, there may be delays of several days in customs clearance on site for deliveries to the UK. This is not within our sphere of influence, so we ask for your understanding.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Country&lt;/cell&gt;
        &lt;cell&gt;Shipping Fee&lt;/cell&gt;
        &lt;cell&gt;Free Shipping From&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Albania&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Andorra&lt;/cell&gt;
        &lt;cell&gt;59,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Belgium&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bulgaria&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Denmark&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Estonia&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Faroe Islands&lt;/cell&gt;
        &lt;cell&gt;129,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Finland&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;France&lt;/cell&gt;
        &lt;cell&gt;9,99 EUR&lt;/cell&gt;
        &lt;cell&gt;120 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Greece&lt;/cell&gt;
        &lt;cell&gt;22,90 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;United Kingdom&lt;/cell&gt;
        &lt;cell&gt;9,99 EUR&lt;/cell&gt;
        &lt;cell&gt;120 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hong Kong&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;India&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ireland&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Island&lt;/cell&gt;
        &lt;cell&gt;129,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Italy&lt;/cell&gt;
        &lt;cell&gt;9,99 EUR&lt;/cell&gt;
        &lt;cell&gt;120 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Japan&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Canada&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Croatia&lt;/cell&gt;
        &lt;cell&gt;34,90 EUR&lt;/cell&gt;
        &lt;cell&gt;500 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Latvia&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Lithuania&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Luxembourg&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Macau&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Malta&lt;/cell&gt;
        &lt;cell&gt;34,90 EUR&lt;/cell&gt;
        &lt;cell&gt;500 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Macedonia&lt;/cell&gt;
        &lt;cell&gt;59,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Moldova&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Monaco&lt;/cell&gt;
        &lt;cell&gt;19,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Montenegro&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Netherlands&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Norway&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Austria&lt;/cell&gt;
        &lt;cell&gt;8,49 EUR&lt;/cell&gt;
        &lt;cell&gt;100 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Poland&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Portugal&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Romania&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;San Marino&lt;/cell&gt;
        &lt;cell&gt;9,99 EUR&lt;/cell&gt;
        &lt;cell&gt;120 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sweden&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Switzerland&lt;/cell&gt;
        &lt;cell&gt;13,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Serbia&lt;/cell&gt;
        &lt;cell&gt;34,90 EUR&lt;/cell&gt;
        &lt;cell&gt;500 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Singapore&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Slovakia&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Slovenia&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Spain (without Canary Islands)&lt;/cell&gt;
        &lt;cell&gt;14,99 EUR&lt;/cell&gt;
        &lt;cell&gt;150 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Czech Republic&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hungary&lt;/cell&gt;
        &lt;cell&gt;15,99 EUR&lt;/cell&gt;
        &lt;cell&gt;160 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;USA including Hawaii&lt;/cell&gt;
        &lt;cell&gt;99,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;United Arabic Emirates&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cyprus&lt;/cell&gt;
        &lt;cell&gt;34,90 EUR&lt;/cell&gt;
        &lt;cell&gt;500 EUR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qatar&lt;/cell&gt;
        &lt;cell&gt;199,00 EUR&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If not stated differently in the article's description, we deliver goods in:&lt;/p&gt;
    &lt;p&gt;For orders paid in advance, the delivery time starts with receipt of the payment. Please keep in mind that there is no delivery on Sundays or on holidays.&lt;lb/&gt; For goods delivered as download, there will be no shipping fees due.&lt;lb/&gt; Access data for downloads are sent out via e-mail 1-3 working days after contract formation. For orders with advanced payment, we will deliver after receiving the payment. You can download the item by using the link sent to you via e-mail.&lt;lb/&gt; Self-pick-up of orders is not possible, unfortunately.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46008156</guid><pubDate>Fri, 21 Nov 2025 19:46:34 +0000</pubDate></item><item><title>Pixar: The Early Days A never-before-seen 1996 interview</title><link>https://stevejobsarchive.com/stories/pixar-early-days</link><description>&lt;doc fingerprint="2501819f3360cdbd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pixar: The Early Days&lt;/head&gt;
    &lt;p&gt;A never-before-seen 1996 interview&lt;/p&gt;
    &lt;p&gt;November 18, 2025&lt;/p&gt;
    &lt;p&gt;To mark Toy Story’s 30th anniversary, we’re sharing a never-before-seen interview with Steve from November 22, 1996—exactly one year after the film debuted in theaters.&lt;/p&gt;
    &lt;p&gt;Toy Story was the world’s first entirely computer-animated feature-length film. An instant hit with audiences and critics, it also transformed Pixar, which went public the week after its premiere. Buoyed by Toy Story’s success, Pixar’s stock price closed at nearly double its initial offering, giving it a market valuation of approximately $1.5 billion and marking the largest IPO of 1995. The following year, Toy Story was nominated for three Academy Awards en route to winning a Special Achievement Oscar in March. In July, Pixar announced that it would close its television-commercial unit to focus primarily on feature films. By the time of the interview, the team had grown by 70 percent in less than a year; A Bug’s Life was in production; and behind the scenes, Steve was using his new leverage to renegotiate Pixar’s partnership with Disney.&lt;/p&gt;
    &lt;p&gt;In this footage, Steve reveals the long game behind Pixar’s seeming overnight success. With striking clarity, he explains how its business model gives artists and engineers a stake in their creations, and he reflects on what Disney’s hard-won wisdom taught him about focus and discipline. He also talks about the challenge of leading a team so talented that it inverts the usual hierarchy, the incentives that inspire people to stay with the company, and the deeper purpose that unites them all: to tell stories that last and put something of enduring value into the culture.&lt;/p&gt;
    &lt;p&gt;At Pixar, Steve collaborated closely with president Ed Catmull and refined a management approach centered on creating the conditions for talent to thrive. When he returned to Apple a few weeks after this interview, his experience at Pixar shaped how he saw his role as CEO: building a company on timeless ideas made new through technology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stay Hungry, Stay Foolish&lt;/head&gt;
    &lt;p&gt;Celebrating Steve’s timeless address&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46008769</guid><pubDate>Fri, 21 Nov 2025 20:45:06 +0000</pubDate></item><item><title>Arduino Terms of Service and Privacy Policy update: setting the record straight</title><link>https://blog.arduino.cc/2025/11/21/the-arduino-terms-of-service-and-privacy-policy-update-setting-the-record-straight/</link><description>&lt;doc fingerprint="b474a25b20f7aa5c"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;The Arduino Terms of Service and Privacy Policy update: setting the record straight&lt;/head&gt;
    &lt;p&gt;We’ve heard some questions and concerns following our recent Terms of Service and Privacy Policy updates. We are thankful our community cares enough to engage with us and we believe transparency and open dialogue are foundational to Arduino.&lt;/p&gt;
    &lt;p&gt;Let us be absolutely clear: we have been open-source long before it was fashionable. We’re not going to change now. The Qualcomm acquisition doesn’t modify how user data is handled or how we apply our open-source principles.&lt;/p&gt;
    &lt;p&gt;We periodically update our legal documents to reflect new features, evolving regulations, and best practices.&lt;/p&gt;
    &lt;head rend="h2"&gt;What remains the same&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Open Source and reverse-engineering. Any hardware, software or services (e.g. Arduino IDE, hardware schematics, tooling and libraries) released with Open Source licenses remain available as before. Restrictions on reverse-engineering apply specifically to our Software-as-a-Service cloud applications. Anything that was open, stays open.&lt;/item&gt;
      &lt;item&gt;Ownership of your creations. The Terms of Service clarifies that the content you choose to publish on the Arduino platform remains yours, and can be used to enable features you’ve requested, such as cloud services and collaboration tools.&lt;/item&gt;
      &lt;item&gt;Minors’ data and privacy. Our privacy disclosures have been strengthened, including enhanced protections for minors’ data. We’ve updated our data retention policies and age limits to provide age-appropriate services. We limit data retention for inactive users by automatically deactivating their accounts after 24 months of inactivity, in which case usernames would still be preserved in the Arduino Forum to address an explicit request from the Forum community to maintain attribution for user-generated content; where user requests account deletion, the username would be promptly removed and related posts would become anonymous.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why we updated our terms: clarity and compliance&lt;/head&gt;
    &lt;p&gt;These latest changes are about clarity, compliance, and supporting the innovative environment you expect.&lt;/p&gt;
    &lt;p&gt;Here’s what the updates actually cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enhanced transparency around data practices: We’ve made our privacy disclosures more precise and more detailed, including what data we retain, to protect your privacy.&lt;/item&gt;
      &lt;item&gt;New product capabilities and AI: As we introduce optional AI-powered features, such as those in the Arduino UNO Q and Arduino App Lab, we needed to update our terms to reflect these new capabilities and encourage their safe, responsible, and ethical use.&lt;/item&gt;
      &lt;item&gt;More precise commercial terms: For users of our Premium Services, we’ve clarified billing mechanics, recurring payments, and refund rights to make purchasing and returns easier.&lt;/item&gt;
      &lt;item&gt;Legal compliance: We’ve updated language to address US-specific privacy laws, export controls, and other regulatory requirements, while ensuring compliance with global standards.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our 20-year commitment to open-source is unwavering&lt;/head&gt;
    &lt;p&gt;We are very proud of the Arduino community, and we would like to reaffirm our fundamental, non-negotiable commitment to the principles that founded Arduino.&lt;/p&gt;
    &lt;p&gt;Please read the full Terms of Service and Privacy Policy, to appreciate how they support the innovative, collaborative environment you’ve come to expect.&lt;/p&gt;
    &lt;p&gt;If you have specific questions or concerns, please consult our detailed Q&amp;amp;A in our FAQ section or reach out to us at privacy@arduino.cc.&lt;/p&gt;
    &lt;p&gt;We are Arduino. We are open. We’re not going anywhere.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46009056</guid><pubDate>Fri, 21 Nov 2025 21:13:11 +0000</pubDate></item><item><title>LAPD helicopter tracker with real-time operating costs</title><link>https://lapdhelicoptertracker.com/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46009591</guid><pubDate>Fri, 21 Nov 2025 22:11:07 +0000</pubDate></item><item><title>Is Matrix Multiplication Ugly?</title><link>https://mathenchant.wordpress.com/2025/11/21/is-matrix-multiplication-ugly/</link><description>&lt;doc fingerprint="b305911521a69631"&gt;
  &lt;main&gt;
    &lt;p&gt;A few weeks ago I was minding my own business, peacefully reading a well-written and informative article about artificial intelligence, when I was ambushed by a passage in the article that aroused my pique. That’s one of the pitfalls of knowing too much about a topic a journalist is discussing; journalists often make mistakes that most readers wouldn’t notice but that raise the hackles or at least the blood pressure of those in the know.&lt;/p&gt;
    &lt;p&gt;The article in question appeared in The New Yorker. The author, Stephen Witt, was writing about the way that your typical Large Language Model, starting from a blank slate, or rather a slate full of random scribbles, is able to learn about the world, or rather the virtual world called the internet. Throughout the training process, billions of numbers called weights get repeatedly updated so as to steadily improve the model’s performance. Picture a tiny chip with electrons racing around in etched channels, and slowly zoom out: there are many such chips in each server node and many such nodes in each rack, with racks organized in rows, many rows per hall, many halls per building, many buildings per campus. It’s a sort of computer-age version of Borges’ Library of Babel. And the weight-update process that all these countless circuits are carrying out depends heavily on an operation known as matrix multiplication.&lt;/p&gt;
    &lt;p&gt;Witt explained this clearly and accurately, right up to the point where his essay took a very odd turn.&lt;/p&gt;
    &lt;p&gt;HAMMERING NAILS&lt;/p&gt;
    &lt;p&gt;Here’s what Witt went on to say about matrix multiplication:&lt;/p&gt;
    &lt;p&gt;“‘Beauty is the first test: there is no permanent place in the world for ugly mathematics,’ the mathematician G. H. Hardy wrote, in 1940. But matrix multiplication, to which our civilization is now devoting so many of its marginal resources, has all the elegance of a man hammering a nail into a board. It is possessed of neither beauty nor symmetry: in fact, in matrix multiplication, a times b is not the same as b times a.”&lt;/p&gt;
    &lt;p&gt;The last sentence struck me as a bizarre non sequitur, somewhat akin to saying “Number addition has neither beauty nor symmetry, because when you write two numbers backwards, their new sum isn’t just their original sum written backwards; for instance, 17 plus 34 is 51, but 71 plus 43 isn’t 15.”&lt;/p&gt;
    &lt;p&gt;The next day I sent the following letter to the magazine:&lt;/p&gt;
    &lt;p&gt;“I appreciate Stephen Witt shining a spotlight on matrices, which deserve more attention today than ever before: they play important roles in ecology, economics, physics, and now artificial intelligence (“Information Overload”, November 3). But Witt errs in bringing Hardy’s famous quote (“there is no permanent place in the world for ugly mathematics”) into his story. Matrix algebra is the language of symmetry and transformation, and the fact that a followed by b differs from b followed by a is no surprise; to expect the two transformations to coincide is to seek symmetry in the wrong place — like judging a dog’s beauty by whether its tail resembles its head. With its two-thousand-year-old roots in China, matrix algebra has secured a permanent place in mathematics, and it passes the beauty test with flying colors. In fact, matrices are commonplace in number theory, the branch of pure mathematics Hardy loved most.”&lt;/p&gt;
    &lt;p&gt;Confining my reply to 150 words required some finesse. Notice for instance that the opening sentence does double duty: it leavens my many words of negative criticism with a few words of praise, and it stresses the importance of the topic, preëmptively1 rebutting editors who might be inclined to dismiss my correction as too arcane to merit publication.&lt;/p&gt;
    &lt;p&gt;I haven’t heard back from the editors, and I don’t expect to. Regardless, Witt’s misunderstanding deserves a more thorough response than 150 words can provide. Let’s see what I can do with 1500 words and a few pictures.&lt;/p&gt;
    &lt;p&gt;THE GEOMETRY OF TRANSFORMATIONS&lt;/p&gt;
    &lt;p&gt;As a static object, matrices are “just” rectangular arrays of numbers, but that doesn’t capture what they’re really about. If I had to express the essence of matrices in a single word, that word would be “transformation”.&lt;/p&gt;
    &lt;p&gt;One example of a transformation is the operation f that takes an image in the plane and flips it from left to right, as if in a vertical mirror.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Another example is the operation g that that takes an image in the plane and reflects it across a diagonal line that goes from lower left to upper right.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;The key thing to notice here is that the effect of f followed by g is different from the effect of g followed by f. To see why, write a capital R on one side of a square piece of paper–preferably using a dark marker and/or translucent paper, so that you can still see the R even when the paper has been flipped over–and apply f followed by g; you’ll get the original R rotated by 90 degrees clockwise. But if instead, starting from that original R, you were to apply g followed by f, you’d get the original R rotated by 90 degrees counterclockwise.&lt;/p&gt;
    &lt;p&gt;Same two operations, different outcomes! Symbolically we write g ◦ f ≠ f ◦ g, where g ◦ f means “First do f, then do g” and f ◦ g means “First do g, then f”.2 The symbol ◦ denotes the meta-operation (operation-on-operations) called composition.&lt;/p&gt;
    &lt;p&gt;The fact that the order in which transformations are applied can affect the outcome shouldn’t surprise you. After all, when you’re composing a salad, if you forget to pour on salad dressing until after you’ve topped the base salad with grated cheese, your guests will have a different dining experience than if you’d remembered to pour on the dressing first. Likewise, when you’re composing a melody, a C-sharp followed by a D is different from a D followed by a C-sharp. And as long as mathematicians used the word “composition” rather than “multiplication”, nobody found it paradoxical that in many contexts, order matters.&lt;/p&gt;
    &lt;p&gt;THE ALGEBRA OF MATRICES&lt;/p&gt;
    &lt;p&gt;If we use the usual x, y coordinates in the plane, the geometric operation f can be understood as the numerical operation that sends the pair (x, y) to the pair (−x, y), which we can represented via the 2-by-2 array&lt;/p&gt;
    &lt;p&gt;where more generally the array&lt;/p&gt;
    &lt;p&gt;stands for the transformation that sends the pair (x, y) to the pair (ax+by, cx+dy). This kind of array is called a matrix, and when we want to compose two operations like f and g together, all we have to do is combine the associated matrices under the rule that says that the matrix&lt;/p&gt;
    &lt;p&gt;composed with the matrix&lt;/p&gt;
    &lt;p&gt;equals the matrix&lt;/p&gt;
    &lt;p&gt;For more about where this formula comes from, see my Mathematical Enchantments essay “What Is A Matrix?”.&lt;/p&gt;
    &lt;p&gt;There’s nothing special about 2-by-2 matrices; you could compose two 3-by-3 matrices, or even two 1000-by-1000 matrices. Going in the other direction (smaller instead of bigger), if you look at 1-by-1 matrices, the composition of&lt;/p&gt;
    &lt;p&gt;and&lt;/p&gt;
    &lt;p&gt;is just&lt;/p&gt;
    &lt;p&gt;so ordinary number-multiplication arises as a special case of matrix composition; turning this around, we can see matrix-composition as a sort of generalized multiplication. So it was natural for mid-19th-century mathematicians to start using words like “multiply” and “product” instead of words like “compose” and “composition”, at roughly the same time they stopped talking about “substitutions” and “tableaux” and started to use the word “matrices”.&lt;/p&gt;
    &lt;p&gt;In importing the centuries-old symbolism for number multiplication into the new science of linear algebra, the 19th century algebraists were saying “Matrices behave kind of like numbers,” with the proviso “except when they don’t”. Witt is right when he says that when A and B are matrices, A times B is not always equal to B times A. Where he’s wrong is in asserting that is a blemish on linear algebra. Many mathematicians regard linear algebra as one of the most elegant sub-disciplines of mathematics ever devised, and it often serves as a role model for the kind of sleekness that a new mathematical discipline should strive to achieve. If you dislike matrix multiplication because AB isn’t always equal to BA, it’s because you haven’t yet learned what matrix multiplication is good for in math, physics, and many other subjects. It’s ironic that Witt invokes the notion of symmetry to disparage matrix multiplication, since matrix theory and an allied discipline called group theory are the tools mathematicians use in fleshing out our intuitive ideas about symmetry that arise in art and science.&lt;/p&gt;
    &lt;p&gt;So how did an intelligent person like Witt go so far astray?&lt;/p&gt;
    &lt;p&gt;PROOFS VS CALCULATIONS&lt;/p&gt;
    &lt;p&gt;I’m guessing that part of Witt’s confusion arises from the fact that actually multiplying matrices of numbers to get a matrix of bigger numbers can be very tedious, and tedium is psychologically adjacent to distaste and a perception of ugliness. But the tedium of matrix multiplication is tied up with its symmetry (whose existence Witt mistakenly denies). When you multiply two n-by-n matrices A and B in the straightforward way, you have to compute n2 numbers in the same unvarying fashion, and each of those n2 numbers is the sum of n terms, and each of those n terms is the product of an element of A and an element of B in a simple way. It’s only human to get bored and inattentive and then make mistakes because the process is so repetitive. We tend to think of symmetry and beauty as synonyms, but sometimes excessive symmetry breeds ennui; repetition in excess can be repellent. Picture the Library of Babel and the existential dread the image summons.&lt;/p&gt;
    &lt;p&gt;G. H. Hardy, whose famous remark Witt quotes, was in the business of proving theorems, and he favored conceptual proofs over calculational ones. If you showed him a proof of a theorem in which the linchpin of your argument was a 5-page verification that a certain matrix product had a particular value, he’d say you didn’t really understand your own theorem; he’d assert that you should find a more conceptual argument and then consign your brute-force proof to the trash. But Hardy’s aversion to brute force was specific to the domain of mathematical proof, which is far removed from math that calculates optimal pricing for annuities or computes the wind-shear on an airplane wing or fine-tunes the weights used by an AI. Furthermore, Hardy’s objection to your proof would focus on the length of the calculation, and not on whether the calculation involved matrices. If you showed him a proof that used 5 turgid pages of pre-19th-century calculation that never mentioned matrices once, he’d still say “Your proof is a piece of temporary mathematics; it convinces the reader that your theorem is true without truly explaining why the theorem is true.”&lt;/p&gt;
    &lt;p&gt;If you forced me at gunpoint to multiply two 5-by-5 matrices together, I’d be extremely unhappy, and not just because you were threatening my life; the task would be inherently unpleasant. But the same would be true if you asked me to add together a hundred random two-digit numbers. It’s not that matrix-multiplication or number-addition is ugly; it’s that such repetitive tasks are the diametrical opposite of the kind of conceptual thinking that Hardy loved and I love too. Any kind of mathematical content can be made stultifying when it’s stripped of its meaning and reduced to mindless toil. But that casts no shade on the underlying concepts. When we outsource number-addition or matrix-multiplication to a computer, we rightfully delegate the soul-crushing part of our labor to circuitry that has no soul. If we could peer into the innards of the circuits doing all those matrix multiplications, we would indeed see a nightmarish, Borgesian landscape, with billions of nails being hammered into billions of boards, over and over again. But please don’t confuse that labor with mathematics.&lt;/p&gt;
    &lt;p&gt;Join the discussion of this essay over at Hacker News!&lt;/p&gt;
    &lt;p&gt;This essay is related to chapter 10 (“Out of the Womb”) of a book I’m writing, tentatively called “What Can Numbers Be?: The Further, Stranger Adventures of Plus and Times”. If you think this sounds interesting and want to help me make the book better, check out http://jamespropp.org/readers.pdf. And as always, feel free to submit comments on this essay at the Mathematical Enchantments WordPress site!&lt;/p&gt;
    &lt;p&gt;ENDNOTES&lt;/p&gt;
    &lt;p&gt;#1. Note the New Yorker-ish diaresis in “preëmptively”: as long as I’m being critical, I might as well be diacritical.&lt;/p&gt;
    &lt;p&gt;#2. I know this convention may seem backwards on first acquaintance, but this is how ◦ is defined. Blame the people who first started writing things like “log x” and “cos x“, with the x coming after the name of the operation. This led to the notation f(x) for the result of applying the function f to the number x. Then the symbol for the result of applying g to the result of applying f to x is g(f(x)); even though f is performed first, “f” appears to the right of “g“. From there, it became natural to write the function that sends x to g(f(x)) as “g ◦ f“.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46009660</guid><pubDate>Fri, 21 Nov 2025 22:17:11 +0000</pubDate></item><item><title>Personal blogs are back, should niche blogs be next?</title><link>https://disassociated.com/personal-blogs-back-niche-blogs-next/</link><description>&lt;doc fingerprint="3b8090076bc54e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Personal blogs are back, should niche blogs be next?&lt;/head&gt;
    &lt;p&gt;20 November 2025&lt;/p&gt;
    &lt;p&gt;When it comes to blogging there are few rules. Write content that is somehow meaningful might be one of them though. I think it’s down to the individual to determine what constitutes meaningful.&lt;/p&gt;
    &lt;p&gt;In the hey-day, the so-called golden age of blogging, there were plenty of people prepared to offer definitions of meaningful, and how to write accordingly. It was natural. The web was once awash with all sorts of blogs. Likewise people who wanted to show others how to blog “successfully”.&lt;/p&gt;
    &lt;p&gt;Again, the definition of successful resided with the individual, but it was obvious this involved monetary return for some people. And why not. If you’re going to invest time and energy in creating a resource that is useful to other people, why shouldn’t you earn money, make a living even, from it?&lt;/p&gt;
    &lt;p&gt;One of these people blogging about blogging was Melbourne based Australian writer and author Darren Rowse, who launched his blogging resource Problogger in 2004. Without going into detail, because you can look it up for yourself, Rowse, as one of the earlier bloggers about blogging, did, and still does presumably, rather well for himself.&lt;/p&gt;
    &lt;p&gt;Rowse’s writing, and that of his contributors, attracted numerous readers keen to learn what they could about blogging, and the potential to make money from it.&lt;/p&gt;
    &lt;p&gt;Problogger is what’s called a niche blog. As a blog about blogging, it has a reasonably singular focus. Some people considered this niche principle to be a core tenet of blogging. There was this idea, in the earlier days of blogging, which possibly still persists, that blogs would do better if they had a speciality. Not only were search engines said to be in favour the approach, but the author of a speciality, or niche blog, would generally be considered to be an expert, of some sort, in their field.&lt;/p&gt;
    &lt;p&gt;A master of one trade, rather than the proverbial jack of all trades.&lt;/p&gt;
    &lt;p&gt;Regardless, the world was once full of blogs on every topic imaginable. It was a great time to be alive. If you wanted to learn about something in particular, there was a blog for you. Some publications featured quality content, others required a little fact checking, while some were definitely to be taken with a pinch of salt.&lt;/p&gt;
    &lt;p&gt;But niche blogging was never a format that suited everyone. There are people who did, still do, well, writing about a range, sometimes a wide range, of topics. Kottke is one of the better known blogs that does not have a specific speciality. Here, the publication itself is the speciality. To repeat what I wrote in the first sentence of this article: the rules of blogging are few.&lt;/p&gt;
    &lt;p&gt;But the facets of blogging covered at Problogger, and numerous other similar websites, usually only applied to blogs of a commercial nature. That’s not to say one or two personal bloggers might have looked at the tips posted there for increasing their audience, or improving their writing though. But in my view, personal bloggers were not, are not, part of Problogger’s target audience.&lt;/p&gt;
    &lt;p&gt;It’s been a long time since I last wrote about Problogger, let alone visited the website, maybe fifteen plus years, but a recent mention of it by Kev Quick, via ldstephens, caught my eye. But I don’t believe Rowse is being critical, in any way, of personal bloggers because they do not adhere to a niche or speciality publishing format. That’s not what Problogger, or Rowse, is about.&lt;/p&gt;
    &lt;p&gt;But this started me thinking, and writing another of my long posts.&lt;/p&gt;
    &lt;p&gt;In an age where social media, and influencers, have usurped blogs and their A-List authors, in the jostle for supremacy, it has to be wondered what role websites like Problogger still have. Only a handful of blogs generate liveable incomes today. Despite the doom and gloom though, the form has not completely died off. A backlash against social media, and a growing IndieWeb/SmallWeb community, has precipitated a revival in personal websites.&lt;/p&gt;
    &lt;p&gt;This is a largely non-commercial movement. Of course, there’s nothing wrong with personal websites. Many of us started out with them in the early days of the web. But the web was not only intended for personal journals. It was a vehicle for sharing all manner of information. The web could also empower individuals, and partnerships, to not only set up shop online, be that blogs, or quite literally shops, but potentially make a living at the same time.&lt;/p&gt;
    &lt;p&gt;But with the revival of personal blogs well underway, I think it’s time to bring niche blogs back into the fold. I’m talking about well written, quality, topic focused resources. This is material fast vanishing from the web, leaving ever diminishing options to source useful and accurate information. What are the alternatives? The misinformation morass that is social media? Being served AI generated summaries in response to search engine queries? A web choke full of AI slop?&lt;/p&gt;
    &lt;p&gt;At the same time, I’m not advocating for a return of niche blogs plastered with adverts, and popup boxes urging visitors to subscribe to say a newsletter, before they’ve even had a chance to blink at what they came to read.&lt;/p&gt;
    &lt;p&gt;I’m talking about work produced by independent writers, with an interest in their subject matter, who are not backed by large media organisations, or private equity. This is bringing back reliable sources of information, that also recompenses the content writers in some way. Hopefully we’ve learned a few lessons about monetisation since the earlier wave of niche blogging. We know it is possible to generate revenue without compromising the reader experience.&lt;/p&gt;
    &lt;p&gt;A resurgence in personal blogging is the first step in rebuilding a vibrant, thriving, web, of if you like, blogosphere. Now the focus needs to be on restoring the flow of accessible and trusted information.&lt;/p&gt;
    &lt;p&gt;RELATED CONTENT&lt;/p&gt;
    &lt;p&gt;blogs, history, IndieWeb, self publishing, SmallWeb, technology, trends&lt;/p&gt;
    &lt;head rend="h3"&gt;There's 2 comments on this post&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt; On 22 November 2025 at 11:34 AM, Jorge Arango said:&lt;p&gt;Thanks for sharing. I’d like to believe a resurgence of personal blogs is underway. Is there data that substantiates this claim?&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46009894</guid><pubDate>Fri, 21 Nov 2025 22:40:28 +0000</pubDate></item><item><title>3D printing with unconventional vase mode</title><link>https://vorpal.se/posts/2025/jun/23/3d-printing-with-unconventional-vase-mode/</link><description>&lt;doc fingerprint="5fda99772b9413b8"&gt;
  &lt;main&gt;
    &lt;p&gt;This article targets an advanced audience who are already familiar with the 3D printing. In this article I will try to collect some information I haven’t found written down in a single place yet. In particular, a lot of the information is seemingly only available in the form of YouTube videos that take a long time to get to the point.&lt;/p&gt;
    &lt;p&gt;If you are new to 3D printing and/or CAD for 3D printing, this is not the right article for you. Come back when you have done a bit of printing/design and want to learn advanced tricks to save on print time and material usage.&lt;/p&gt;
    &lt;head rend="h1"&gt;Basics of vase mode&lt;/head&gt;
    &lt;p&gt;With that out of the way what is this about? Vase mode is a printing mode where the printer prints a spiral path, with no seams. This is fast, avoids the visual blemishes of the seam but also has some downsides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only a single perimeter. This potentially means weaker parts.&lt;/item&gt;
      &lt;item&gt;No disconnected areas (per layer), you have to print with a single path.&lt;/item&gt;
      &lt;item&gt;No internal geometry. No infill. No top layers.&lt;/item&gt;
      &lt;item&gt;No supports.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Typically, it gets used for vases and pots. Thus, the name. Here is a crude example (I’m not an aesthetics focused designer, so imagine something prettier than this. If it fits and functions, it ships in my book):&lt;/p&gt;
    &lt;p&gt;Of note here is that the model itself isn’t hollow, but the slicer will make it hollow for you (since it only prints a single perimeter). In PrusaSlicer this setting is found at “Print Settings” → “Layers and perimeters” → “Vertical shells” → “Spiral vase”. OrcaSlicer etc should have the same or similar setting as well somewhere else. I have no idea about Cura.&lt;/p&gt;
    &lt;p&gt;But there are some ways to stretch this mode to the limits, and that is what this article is about. This will make vase mode useful for more than just simple vases. And that can often be the fastest and lightest way to print a part, if you can pull it off.&lt;/p&gt;
    &lt;p&gt;To understand the tricks you do need to understand how vase mode works though. It takes solid geometry, and takes the outline of it. What is inside doesn’t matter. It will be ignored:&lt;/p&gt;
    &lt;p&gt;As can be seen, while the hole exists in the bottom solid layers, the slicer ignores it above that point.&lt;/p&gt;
    &lt;p&gt;So what can we do above that?&lt;/p&gt;
    &lt;head rend="h1"&gt;Internal geometry via slits&lt;/head&gt;
    &lt;p&gt;The idea comes from the RC plane 3D printing community, where they want to print lightweight but strong parts. In particular wings with internal supporting geometry.2&lt;/p&gt;
    &lt;p&gt;There are two main tricks for unconventional vase mode prints. Let’s start with slits, as the next trick builds upon this first trick. As I’m no aircraft wing designer I will use other geometry for illustration purposes. The idea is useful in other contexts than RC wings, that is the whole point of this article.&lt;/p&gt;
    &lt;p&gt;Make a slit into the part. The left is for demonstration only, you need the slit to be really thin, 0.00011 mm or so, as shown on the right:&lt;/p&gt;
    &lt;p&gt;If we extrude this into a block and slice it, PrusaSlicer will see this slit and print an outer perimeter going into the part, making a sort of internal support. You are basically modelling the infill yourself now:&lt;/p&gt;
    &lt;p&gt;If you try this, it will not work for you. This is because you are missing a crucial setting in PrusaSlicer. By default, PrusaSlicer will merge together close parts of the model. You need to change “Printer Settings” → “Advanced” → “Slicing” → “Slice gap closing radius”. Set it to 0.0.3 Otherwise, none of this will work.&lt;/p&gt;
    &lt;p&gt;For our example with a hole in the middle from the introduction we could get the following result:&lt;/p&gt;
    &lt;p&gt;Note that the slit will be visible and you can feel it with your fingers, but it will be a fairly smooth indentation, not a sharp edge.&lt;/p&gt;
    &lt;head rend="h1"&gt;Double walls&lt;/head&gt;
    &lt;p&gt;Now, let’s expand on this technique to make it even more useful: Have you ever wanted to use vase mode but with two perimeters? We can build upon the previous trick to make a double wall:&lt;/p&gt;
    &lt;p&gt;This is done by making a slit through into the hollow inside and making sure the part itself is exactly wide enough for two perimeters that touch. You can find the width you should use by going into PrusaSlicer (with the same settings that you plan to use to print with) and looking at the info text in “Print Settings” → “Layers and perimeters” → “Vertical shells”:&lt;/p&gt;
    &lt;p&gt;That is the value you want to use for this to work correctly.&lt;/p&gt;
    &lt;p&gt;We can build upon this to make our internal geometry touch the opposite wall, like so:&lt;/p&gt;
    &lt;p&gt;We can also use this to anchor a slit to the outside wall. This allows us to anchor internal geometry to the outside wall without poking through. In fact, to ensure we have a single continuous outline, all but one slit must be done like this. The following picture shows what you need to do (note that the double wall thickness is 0.87 mm in this example, it will change depending on other settings):&lt;/p&gt;
    &lt;p&gt;These two tricks presented so far form the basis of what I have seen called “unconventional vase mode”.4&lt;/p&gt;
    &lt;p&gt;But there are some more tricks related to vase mode that are worth knowing about.&lt;/p&gt;
    &lt;head rend="h1"&gt;Extrusion width&lt;/head&gt;
    &lt;p&gt;To make a vase mode stronger, you can increase the extrusion width. The general recommendation is that you can go to about 2x the nozzle diameter and keep good quality. This works, since the nozzle has a bit of a flat spot around the orifice.&lt;/p&gt;
    &lt;p&gt;However, British Youtuber “Lost in Tech” did some tests showing that you can go way further than that, but I haven’t tested this myself, and quality does eventually start going down. It might be worth looking into if this is useful to you.&lt;/p&gt;
    &lt;p&gt;In PrusaSlicer you can change this in “Print Settings” → “Advanced” → “Extrusion width”. For vase mode “External perimeters” is what matters (above the solid base layers, that is):&lt;/p&gt;
    &lt;p&gt;Remember to rescale any double walls to fit the new extrusion width. It might be a good idea to use a variable in your CAD model to make it easier to update (at least if you use parametric CAD like OnShape, FreeCAD or Fusion 360 which support variables).&lt;/p&gt;
    &lt;head rend="h1"&gt;Fake vase mode&lt;/head&gt;
    &lt;p&gt;Finally, if you absolutely cannot print something in vase mode you can still get most of the benefits by what I have seen called “fake vase mode”5. To understand this, we should first consider exactly what settings vase mode changes. In PrusaSlicer vase mode changes the following settings:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Single perimeter (except for the first few bottom layers).&lt;/item&gt;
      &lt;item&gt;No top layers.&lt;/item&gt;
      &lt;item&gt;No infill (except for the first few bottom layers).&lt;/item&gt;
      &lt;item&gt;No supports&lt;/item&gt;
      &lt;item&gt;Disables the setting “ensure vertical shell thickness”.&lt;/item&gt;
      &lt;item&gt;Prints in a continuous spiral path.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can do all of those except 6 by hand in the slicer. And you can mix and match those first five things as you see fit.&lt;/p&gt;
    &lt;p&gt;Let’s investigate this via a case study rather than simplified theoretical examples like we have done so far&lt;/p&gt;
    &lt;head rend="h2"&gt;Case study: spheres on sticks&lt;/head&gt;
    &lt;p&gt;I needed some spheres on the end of wooden sticks, to hold up a bird net over my strawberries on my balcony. I didn’t want the net to lie on the plants directly, and I needed something on the end of the sticks so that the net wouldn’t tear. Thus, spheres (or rather: truncated spheres for print bed adhesion and overhang reasons) on sticks.&lt;/p&gt;
    &lt;p&gt;Here is the basic design in a section view:&lt;/p&gt;
    &lt;p&gt;This doesn’t quite work in vase mode, because the top of the sphere has very shallow overhangs. And the top needs to be smooth. (The “roof” of the internal hole is fine, thanks to the cone shape.) It is so close, we can almost use vase mode.&lt;/p&gt;
    &lt;p&gt;So first I designed this in CAD. We have a slit from the outside to the centre, as well as some slits from the centre that goes almost to the outside. In fact, they go to the “recommended object thin wall thickness” mentioned before. (Note that the slits do not go down into the solid bottom layers, for some additional strength.)&lt;/p&gt;
    &lt;p&gt;This results in the following in PrusaSlicer:&lt;/p&gt;
    &lt;p&gt;Like true vase mode, I used zero infill. But I enabled “Ensure vertical shell thickness” and 1 top solid layer. This added a tiny bit of material just below the shallow top of the dome, making it printable, but still lighter than printing normally. Then I used a layer range modifier to disable “ensure vertical shell thickness” for the lower part of the print where it wasn’t needed, as PrusaSlicer wanted to add some material on the inside of the lower layers as well.&lt;/p&gt;
    &lt;p&gt;I also increased the extrusion width to 0.8 mm (with a 0.4 mm nozzle) to get additional strength, and I used scarf seams to make the outside seam almost invisible.&lt;/p&gt;
    &lt;p&gt;You can go further from true vase mode though: You could have an inner and outer perimeter like traditional non-vase slicing, but still model your own infill only where needed. You will get seams obviously, but you might still be able to print faster and save weight. We are moving further from true vase mode here, but only you can decide what exactly is best for your print:&lt;/p&gt;
    &lt;p&gt;In fact, when I printed some of these spheres, the version without a slit to the outside ended up the best looking6:&lt;/p&gt;
    &lt;p&gt;The slit is visible, but on the part printed without a slit extending to the outside there are no visible seams at all. The unevenness at the top is due to me filing away a small blob that the nozzle left behind as it pulled away at the end. It is smooth to the touch but reflects the light differently.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;Vase mode and “fake vase mode” is an often underused printing mode for functional parts, and it can be used to save weight and print time. The difference will be most noticeable on larger parts, on smaller parts 10 vs 15 minutes might not be worth the extra design effort (unless you are planning to print many copies of the same part).&lt;/p&gt;
    &lt;p&gt;I’m a bit disappointed that the slit was as visible from the outside as it was. From the videos about RC aircraft wings that I saw I expected this to be less noticeable. But “fake vase mode” still comes to the rescue here, offering most of the benefits. And when combined with scarf joint seams (which I found truly impressive, first time I tried it), I don’t really see the need for true vase mode any more. You might as well get the best of both worlds.&lt;/p&gt;
    &lt;p&gt;I did not find any written resource online summarizing these techniques, so I hope this post is useful not just to remind myself in the future, but also to others looking for this information. With that in mind, below is a cheat sheet of the important points and settings to remember.&lt;/p&gt;
    &lt;p&gt;These techniques require tuning settings in your slicer. This may not be possible if you are printing with at a commercial print farm, or targeting people slicing with a dumbed down web based slicer (as has recently been launched by both Printables and Makerworld). But it would be a shame if such dumbed down slicers restricted what we could design and publish. I will always try to make the most what both CAD and the slicer exposes to me.7&lt;/p&gt;
    &lt;p&gt;Do you have some other tips or tricks for vase mode? Did I get something wrong? Comment on Reddit or on Lemmy and I will likely see it (eventually).&lt;/p&gt;
    &lt;head rend="h2"&gt;Cheat sheet&lt;/head&gt;
    &lt;p&gt;Want to quickly remind yourself of the core ideas of this article when you are designing your next part? Here is a quick cheat sheet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Slits: Use slits to add internal geometry.&lt;list rend="ul"&gt;&lt;item&gt;0.0001 mm wide (or 0.001 if your CAD software doesn’t like you that day).&lt;/item&gt;&lt;item&gt;PrusaSlicer: Set “Print Settings” → “Advanced” → “Slicing” → “Slice gap closing radius” to 0.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Double walls: Use double walls for more strength and to connect slits to the opposite wall.&lt;list rend="ul"&gt;&lt;item&gt;PrusaSlicer: “Print Settings” → “Layers and perimeters” → “Vertical shells” (Look at info text to find width you need to use for your current print settings.)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Extrusion width: You can increase the extrusion width to 2x the nozzle diameter for additional strength with no quality downsides. You might be able to go even further, but eventually quality will start going down.&lt;list rend="ul"&gt;&lt;item&gt;PrusaSlicer: “Print Settings” → “Advanced” → “Extrusion width” → “External perimeters”&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Fake vase mode: You don’t need to use vase mode to get most of the benefits. You can mix and match all parts of normal vase mode except for the continuous spiral path. But consider scarf joints to hide seams.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You might need to experiment with the specific value to make your CAD program and slicer happy. With OnShape, sometimes 0.0001 mm works, sometimes only 0.001 mm works (or Onshape doesn’t see the slit), and I don’t know why exactly. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The first mention I can find of this is in this video by Tom Stanton, but I cannot say for sure that this is where it originated. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I found this solution from this forum post. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Unconventional vase mode is briefly mentioned in this excellent but long blog post about designing for 3D printing. I strongly recommend reading it if you want to make your CAD designs portable between different printers and for general tips on how to design to avoid supports, and in general make full use of the peculiarities of the manufacturing method. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I “semi-invented” this method myself, but then found out I wasn’t first. I was thinking “wouldn’t it be possible to…” and then I googled and found this video by “BV3D: Bryan Vines”, that already discussed this idea, though it takes a while to get to the point. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Printed with AddNorth Economy PETG, white. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I even considered using Full Control XYZ for a minute to have true vase mode and then switch back to non-vase mode on top. In the end I came to my senses and decided not to write my model with Python code. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46010173</guid><pubDate>Fri, 21 Nov 2025 23:10:17 +0000</pubDate></item><item><title>How I learned Vulkan and wrote a small game engine with it (2024)</title><link>https://edw.is/learning-vulkan/</link><description>&lt;doc fingerprint="23684b734fb50739"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I learned Vulkan and wrote a small game engine with it&lt;/head&gt;
    &lt;p&gt;tl;dr: I learned some Vulkan and made a game engine with two small game demos in 3 months.&lt;/p&gt;
    &lt;p&gt;The code for the engine and the games can be found here: https://github.com/eliasdaler/edbr&lt;/p&gt;
    &lt;head rend="h2"&gt;Table Of Contents&lt;/head&gt;
    &lt;p&gt;This article documents my experience of learning Vulkan and writing a small game/engine with it. It took me around 3 months to do it without any previous knowledge of Vulkan (I had previous OpenGL experience and some experience with making game engines, though).&lt;/p&gt;
    &lt;p&gt;The engine wasn’t implemented as a general purpose engine, which is probably why it took me a few months (and not years) to achieve this. I started by making a small 3D game and separated reusable parts into the “engine” afterwards. I can recommend everyone to follow the same process to not get stuck in the weeds (see “Bike-shedding” section below for more advice).&lt;/p&gt;
    &lt;head rend="h2"&gt;Preface&lt;/head&gt;
    &lt;p&gt;I’m a professional programmer, but I’m self-taught in graphics programming. I started studying graphics programming around 1.5 years ago by learning OpenGL and writing a 3D engine in it.&lt;/p&gt;
    &lt;p&gt;The engine I wrote in Vulkan is mostly suited for smaller level-based games. I’ll explain things which worked for me, but they might not be the most efficient. My implementation would probably still be a good starting point for many people.&lt;/p&gt;
    &lt;quote&gt;Hopefully, this article will help make some things about Vulkan clearer to you. But you also need to be patient. It took me months to implement what I have today and I did it by cutting corners in many places. But if a self-taught programmer like me can build something with Vulkan, then so can you!&lt;/quote&gt;
    &lt;head rend="h2"&gt;Learning graphics programming&lt;/head&gt;
    &lt;quote&gt;This is a very high level overview of how I learned some graphics programming myself. If there’s interest, I might write another article with more resources and helpful guidelines.&lt;/quote&gt;
    &lt;p&gt;If you haven’t done any graphics programming before, you should start with OpenGL. It’s much easier to learn it and not get overwhelmed by all the complexity that Vulkan has. A lot of your OpenGL and graphics programming knowledge will be useful when you start doing things with Vulkan later.&lt;/p&gt;
    &lt;p&gt;Ideally, you should at least get a textured model displayed on the screen with some simple Blinn-Phong lighting. I can also recommend doing some basic shadow mapping too, so that you learn how to render your scene from a different viewpoint and to a different render target, how to sample from depth textures and so on.&lt;/p&gt;
    &lt;p&gt;I can recommend using the following resources to learn OpenGL:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://learnopengl.com/&lt;/item&gt;
      &lt;item&gt;Anton’s OpenGL 4 Tutorials book&lt;/item&gt;
      &lt;item&gt;Thorsten ThormÃ¤hlen’s lectures lectures (watch the first 6 videos, the rest might be a bit too advanced)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sadly, most OpenGL resources don’t teach the latest OpenGL 4.6 practices. They make writing OpenGL a lot more enjoyable. If you learn them, transitioning to Vulkan will be much easier (I only learned about OpenGL 3.3 during my previous engine development, though, so it’s not a necessity).&lt;/p&gt;
    &lt;p&gt;Here are some resources which teach you the latest OpenGL practices:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://juandiegomontoya.github.io/modern_opengl.html&lt;/item&gt;
      &lt;item&gt;https://github.com/fendevel/Guide-to-Modern-OpenGL-Functions&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;It’s also good to have some math knowledge, especially linear algebra: how to work with vectors, transformation matrices and quaternions. My favorite book about linear algebra/math is 3D Math Primer for Graphics and Game Development by F. Dunn and I. Parbery. You don’t need to read it all in one go - use it as a reference if some math in the OpenGL resources above doesn’t make sense to you.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Bike-shedding and how to avoid it&lt;/head&gt;
    &lt;p&gt;https://en.wikipedia.org/wiki/Law_of_triviality&lt;/p&gt;
    &lt;p&gt;Ah, bike-shedding… Basically, it’s a harmful pattern of overthinking and over-engineering even the simplest things. It’s easy to fall into this trap when doing graphics programming (especially when doing Vulkan since you need to make many choices when implementing an engine with it).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Always ask yourself “Do I really need this?”, “Will this thing ever become a bottleneck?”.&lt;/item&gt;
      &lt;item&gt;Remember that you can always rewrite any part of your game/engine later.&lt;/item&gt;
      &lt;item&gt;Don’t implement something unless you need it right now. Don’t think “Well, a good engine needs X, right…?”.&lt;/item&gt;
      &lt;item&gt;Don’t try to make a general purpose game engine. It’s probably even better to not think about “the engine” at first and write a simple game.&lt;/item&gt;
      &lt;item&gt;Make a small game first - a Breakout clone, for example. Starting your engine development by doing a Minecraft clone with multiplayer support is probably not a good idea.&lt;/item&gt;
      &lt;item&gt;Be wary of people who tend to suggest complicated solutions to simple problems.&lt;/item&gt;
      &lt;item&gt;Don’t look too much at what other people do. I’ve seen many over-engineered engines on GitHub - sometimes they’re that complex for a good reason (and there are years of work behind them). But you probably don’t need most of that complexity, especially for simpler games.&lt;/item&gt;
      &lt;item&gt;Don’t try to make magical wrappers around Vulkan interfaces prematurely, especially while you’re still learning Vulkan.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Get it working first. Leave “TODO”/“FIXME” comments in some places. Then move on to the next thing. Try to fix “TODO”/“FIXME” places only when they really become problematic or bottleneck your performance. You’ll be surprised to see how many things won’t become a problem at all.&lt;/p&gt;
    &lt;quote&gt;Some of this advice only applies when you’re working alone on a hobby project. Of course, it’s much harder to rewrite something from scratch when others start to depend on it and a “temp hack” becomes a fundamental part of the engine which is very hard to change without breaking many things.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Why Vulkan?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Ask yourself if you need to learn a graphics API at all. If your main goal is to make a game as soon as possible, then you might be better off using something like Godot or Unreal Engine.&lt;/p&gt;
      &lt;p&gt;However, there’s nothing wrong with reinventing the wheel or doing something from scratch. Especially if you do it just for fun, to get into graphics programming or to get an in-depth knowledge about how something works.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The situation with graphic APIs in 2024 is somewhat complicated. It all depends on the use case: DirectX seems like the most solid choice for most AAA games. WebGL or WebGPU are the only two choices for doing 3D graphics on the web. Metal is the go-to graphics API on macOS and iOS (though you can still do Vulkan there via MoltenVK).&lt;/p&gt;
    &lt;p&gt;My use case is simple: I want to make small 3D games for desktop platforms (Windows and Linux mostly). I also love open source technology and open standards. So, it was a choice between OpenGL and Vulkan for me.&lt;/p&gt;
    &lt;p&gt;OpenGL is a good enough choice for many small games. But it’s very unlikely that it’ll get new versions in the future (so you can’t use some newest GPU capabilities like ray tracing), it’s deprecated on macOS and its future is uncertain.&lt;/p&gt;
    &lt;p&gt;WebGPU was also a possible choice. Before learning Vulkan, I learned some of it. It’s a pretty solid API, but I had some problems with it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It’s still not stable and there’s not a lot of tutorials and examples for it. This tutorial is fantastic, though.&lt;/item&gt;
      &lt;item&gt;WGSL is an okay shading language, but I just find its syntax not as pleasant as GLSL’s (note that you can write in GLSL and then load compiled SPIR-V on WebGPU native).&lt;/item&gt;
      &lt;item&gt;On desktop, it’s essentially a wrapper around other graphic APIs (DirectX, Vulkan, Metal).This introduces additional problems for me: &lt;list rend="ul"&gt;&lt;item&gt;It can’t do things some things that Vulkan or DirectX can do.&lt;/item&gt;&lt;item&gt;It has more limitations than native graphic APIs since it needs to behave similarly between them.&lt;/item&gt;&lt;item&gt;RenderDoc captures become confusing as they differ between the platforms (you can get DirectX capture on Windows and Vulkan capture on Linux) and you don’t have 1-to-1 mapping between WebGPU calls and native API calls.&lt;/item&gt;&lt;item&gt;Using Dawn and WGPU feels like using bgfx or sokol. You don’t get the same degree of control over the GPU and some of the choices/abstractions might not be the most pleasant for you.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;No bindless textures (WIP discussion here).&lt;/item&gt;
      &lt;item&gt;No push constants (WIP discussion here).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Still, I think that WebGPU is a better API than OpenGL/WebGL and can be more useful to you than Vulkan in some use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Validation errors are much better than in OpenGL/WebGL and not having global state helps a lot.&lt;/item&gt;
      &lt;item&gt;It’s also kind of similar to Vulkan in many things, so learning a bit of it before diving into Vulkan also helped me a lot.&lt;/item&gt;
      &lt;item&gt;It requires a lot less boilerplate to get things on the screen (compared to Vulkan).&lt;/item&gt;
      &lt;item&gt;You don’t have to deal with explicit synchronization which makes things much simpler.&lt;/item&gt;
      &lt;item&gt;You can make your games playable inside the browser.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Learning Vulkan&lt;/head&gt;
    &lt;p&gt;Learning Vulkan seemed like an impossible thing for me previously. It felt like you needed to have many years of AAA game graphics programming experience to be able to do things in it. You also hear people saying “you’re basically writing a graphics driver when writing in Vulkan” which also made Vulkan sounds like an incredibly complicated thing.&lt;/p&gt;
    &lt;p&gt;I have also checked out some engines written in Vulkan before and was further demotivated by seeing tons of scary abstractions and files named like &lt;code&gt;GPUDevice.cpp&lt;/code&gt; or &lt;code&gt;GPUAbstraction.cpp&lt;/code&gt; which had thousands of lines of scary C++ code.&lt;/p&gt;
    &lt;p&gt;The situation has changed over the years. Vulkan is not as complicated as it was before. First of all, Khronos realized that some parts of Vulkan were indeed very complex and introduced some newer features which made many things much simpler (for example, dynamic rendering). Secondly, some very useful libraries which reduce boilerplate were implemented. And finally, there are a lot of fantastic resources which make learning Vulkan much easier than it was before.&lt;/p&gt;
    &lt;p&gt;The best Vulkan learning resource which helped me get started was vkguide. If you’re starting from scratch, just go through it all (you might stop at “GPU driver rendering” chapter at first - many simple games probably won’t need this level of complexity)&lt;/p&gt;
    &lt;p&gt;Vulkan Lecture Series by TU Wien also nicely teaches Vulkan basics (you can probably skip “Real-Time Ray Tracing” chapter for now). I especially found a lecture on synchronization very helpful.&lt;/p&gt;
    &lt;p&gt;Here are some more advanced Vulkan books that also helped me:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3D Graphics Rendering Cookbook by Sergey Kosarevsky and Viktor Latypov. There is the second edition in the writing and it’s promising to be better than the first one. The second edition is not released yet, but the source code for it can be found here: https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook-Second-Edition&lt;/item&gt;
      &lt;item&gt;Mastering Graphics Programming with Vulkan by Marco Castorina, Gabriel Sassone. Very advanced book which explains some of the “cutting edge” graphics programming concepts (I mostly read it to understand where to go further, but didn’t have time to implement most of it). The source code for it can be found here: https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the result of my first month of learning Vulkan:&lt;/p&gt;
    &lt;p&gt;By this point I had:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;glTF model loading&lt;/item&gt;
      &lt;item&gt;Compute skinning&lt;/item&gt;
      &lt;item&gt;Frustum culling&lt;/item&gt;
      &lt;item&gt;Shadow mapping and cascaded shadow maps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, doing it for the 3rd time (I had it implemented it all in OpenGL and WebGPU before) certainly helped. Once you get to this point, Vulkan won’t seem as scary anymore.&lt;/p&gt;
    &lt;p&gt;Let’s see how the engine works and some useful things I learned.&lt;/p&gt;
    &lt;head rend="h2"&gt;Engine overview and frame analysis&lt;/head&gt;
    &lt;p&gt;https://github.com/eliasdaler/edbr&lt;/p&gt;
    &lt;p&gt;My engine is called EDBR (Elias Daler’s Bikeshed Engine) and was initially started as a project for learning Vulkan. It quickly grew into a somewhat usable engine which I’m going to use for my further projects.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;At the time of writing this article, the source code line counts are as follows:&lt;/p&gt;
      &lt;item&gt;Engine itself: 19k lines of code&lt;/item&gt;
      &lt;item&gt;6.7k LoC related to graphics,&lt;/item&gt;
      &lt;item&gt;2k LoC are light abstractions around Vulkan&lt;/item&gt;
      &lt;item&gt;3D cat game: 4.6k LoC&lt;/item&gt;
      &lt;item&gt;2D platformer game: 1.2k LoC&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;I copy-pasted some non-graphics related stuff from my previous engine (e.g. input handling and audio system) but all of the graphics and many other core systems were rewritten from scratch. I feel like it was a good way to do it instead of trying to cram Vulkan into my old OpenGL abstractions.&lt;/p&gt;
    &lt;quote&gt;You can follow the commit history which shows how I started from clearing the screen, drawing the first triangle, drawing a textured quad and so on. It might be easier to understand the engine when it was simpler and smaller.&lt;/quote&gt;
    &lt;p&gt;Let’s see how this frame in rendered:&lt;/p&gt;
    &lt;quote&gt;Most of the steps will be explained in more detail below.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Skinning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, models with skeletal animations are skinned in the compute shader. The compute shader takes unskinned mesh and produces a buffer of vertices which are then used instead of the original mesh in later rendering steps. This allows me to treat static and skinned meshes similarly in shaders and not do skinning repeatedly in different rendering steps.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CSM (Cascaded Shadow Mapping)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I use a 4096x4096 depth texture with 3 slices for cascaded shadow mapping. The first slice looks like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Geometry + shading&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All the models are drawn and shading is calculated using the shadow map and light info. I use a PBR model which is almost identical to the one described in Physically Based Rendering in Filament. The fragment shader is quite big and does calculation for all the lights affecting the drawn mesh in one draw call:&lt;/p&gt;
    &lt;p&gt;Everything is drawn into a multi-sampled texture. Here’s how it looks after resolve:&lt;/p&gt;
    &lt;p&gt;(Open the previous two screenshots in the next tab and flip between the tabs to see the difference more clearly)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Depth resolve&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Depth resolve step is performed manually via a fragment shader. I just go through all the fragments of multi-sample depth texture and write the minimum value into the non-MS depth texture (it’ll be useful in the next step).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Post FX&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some post FX is applied - right now it’s only depth fog (I use “depth resolve” texture from the previous step here), afterwards tone-mapping and bloom will also be done here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UI&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dialogue UI is drawn. Everything is done in one draw call (more is explained in “Drawing many sprites” section)&lt;/p&gt;
    &lt;p&gt;And that’s it! It’s pretty basic right now and would probably become much more complex in the future (see “Future work” section).&lt;/p&gt;
    &lt;head rend="h2"&gt;General advice&lt;/head&gt;
    &lt;head rend="h3"&gt;Recommended Vulkan libraries&lt;/head&gt;
    &lt;p&gt;There are a couple of libraries which greatly improve the experience of writing Vulkan. Most of them are already used in vkguide, but I still want to highlight how helpful they were to me.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;vk-bootstrap - https://github.com/charles-lunarg/vk-bootstrap&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;vk-bootstrap simplifies a lot of Vulkan boilerplate: physical device selection, swapchain creation and so on.&lt;/p&gt;
    &lt;p&gt;I don’t like big wrappers around graphic APIs because they tend to be very opinionated. Plus, you need to keep a mental map of “wrapper function vs function in the API spec” in your head at all times.&lt;/p&gt;
    &lt;p&gt;Thankfully, vk-bootstrap is not like this. It mostly affects the initialization step of your program and doesn’t attempt to be a wrapper around every Vulkan function.&lt;/p&gt;
    &lt;quote&gt;When I was learning Vulkan, I started doing Vulkan from scratch, without using any 3rd party libraries. Replacing big amounts of the initialization code with vk-bootstrap was a joy. It’s really worth it.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vulkan Memory Allocator (VMA) - https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’ll be honest, I used VMA without even learning about how to allocate memory in Vulkan manually. I read about it in the Vulkan spec later - I’m glad that I didn’t have to do it on my own.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;volk&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Volk was very useful for me for simplifying extension function loading. For example, if you want to use very useful &lt;code&gt;vkSetDebugUtilsObjectNameEXT&lt;/code&gt; for setting debug names for your objects (useful for RenderDoc captures and validation errors), you’ll need to do this if you don’t use volk:&lt;/p&gt;
    &lt;code&gt;// store this pointer somewhere
PFN_vkSetDebugUtilsObjectNameEXT pfnSetDebugUtilsObjectNameEXT;

// during your game init
pfnSetDebugUtilsObjectNameEXT = (PFN_vkSetDebugUtilsObjectNameEXT)
    vkGetInstanceProcAddr(instance, "vkSetDebugUtilsObjectNameEXT");

// and finally in your game code
pfnSetDebugUtilsObjectNameEXT(device, ...);
&lt;/code&gt;
    &lt;p&gt;With volk, all the extensions are immediately loaded after you call &lt;code&gt;volkInitialize&lt;/code&gt; and you don’t need to store these pointers everywhere. You just include &lt;code&gt;volk.h&lt;/code&gt; and call &lt;code&gt;vkSetDebugUtilsObjectNameEXT&lt;/code&gt; - beautiful!&lt;/p&gt;
    &lt;head rend="h3"&gt;GfxDevice abstraction&lt;/head&gt;
    &lt;p&gt;I have a &lt;code&gt;GfxDevice&lt;/code&gt; class which encapsulates most of the commonly used functionality and stores many objects that you need for calling Vulkan functions (&lt;code&gt;VkDevice&lt;/code&gt;, &lt;code&gt;VkQueue&lt;/code&gt; and so on). A single &lt;code&gt;GfxDevice&lt;/code&gt; instance is created on the startup and then gets passed around.&lt;/p&gt;
    &lt;p&gt;It handles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vulkan context initialization.&lt;/item&gt;
      &lt;item&gt;Swapchain creation and management.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;beginFrame&lt;/code&gt;returns a new&lt;code&gt;VkCommandBuffer&lt;/code&gt;which is later used in all the drawing steps.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;endFrame&lt;/code&gt;does drawing to the swapchain and does sync between the frames.&lt;/item&gt;
      &lt;item&gt;Image creation and loading textures from files.&lt;/item&gt;
      &lt;item&gt;Buffer creation.&lt;/item&gt;
      &lt;item&gt;Bindless descriptor set management (see “Bindless descriptors” section below).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That’s… a lot of things. However, it’s not that big: &lt;code&gt;GfxDevice.cpp&lt;/code&gt; is only 714 lines at the time of writing this article. It’s more convenient to pass one object into the function instead of many (&lt;code&gt;VkDevice&lt;/code&gt;, &lt;code&gt;VkQueue&lt;/code&gt;, &lt;code&gt;VmaAllocator&lt;/code&gt; and so on).&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling shaders&lt;/head&gt;
    &lt;p&gt;In Vulkan, you can use any shading language which compiles to SPIR-V - that means that you can use GLSL, HLSL and others. I chose GLSL because I already knew it from my OpenGL experience.&lt;/p&gt;
    &lt;p&gt;You can pre-compile your shaders during the build step or compile them on the fly. I do it during the build so that my shader loading runtime code is simpler. I also don’t have an additional runtime dependency on the shader compiler. Also, shader errors are detected during the build step and I don’t get compile errors during the runtime.&lt;/p&gt;
    &lt;p&gt;I use glslc (from shaderc project, it’s included in Vulkan SDK) which allows you to specify a &lt;code&gt;DEPFILE&lt;/code&gt; in CMake which is incredibly useful when you use shader includes. If you change a shader file, all files which include it are recompiled automatically. Without the &lt;code&gt;DEPFILE&lt;/code&gt;, CMake won’t be able to see which files shader files need to be recompiled and will only recompile the file which was changed.&lt;/p&gt;
    &lt;p&gt;My CMake script for building shaders looks like this:&lt;/p&gt;
    &lt;code&gt;function (target_shaders target shaders)
    set(SHADERS_BUILD_DIR "${CMAKE_CURRENT_BINARY_DIR}/shaders")
    file(MAKE_DIRECTORY "${SHADERS_BUILD_DIR}")
    foreach (SHADER_PATH ${SHADERS})
        get_filename_component(SHADER_FILENAME "${SHADER_PATH}" NAME)
        set(SHADER_SPIRV_PATH "${SHADERS_BUILD_DIR}/${SHADER_FILENAME}.spv")
        set(DEPFILE "${SHADER_SPIRV_PATH}.d")
        add_custom_command(
          COMMENT "Building ${SHADER_FILENAME}"
          OUTPUT "${SHADER_SPIRV_PATH}"
          COMMAND ${GLSLC} "${SHADER_PATH}" -o "${SHADER_SPIRV_PATH}" -MD -MF ${DEPFILE} -g
          DEPENDS "${SHADER_PATH}"
          DEPFILE "${DEPFILE}"
        )
        list(APPEND SPIRV_BINARY_FILES ${SHADER_SPIRV_PATH})
    endforeach()

    set(shaders_target_name "${target}_build_shaders")
    add_custom_target(${shaders_target_name}
      DEPENDS ${SPIRV_BINARY_FILES}
    )
    add_dependencies(${target} ${shaders_target_name})
endfunction()
&lt;/code&gt;
    &lt;p&gt;and then in the main CMakeLists file:&lt;/p&gt;
    &lt;code&gt;set(SHADERS
    skybox.frag
    skinning.comp
    ... // etc
)

# prepend shaders directory path
get_target_property(EDBR_SOURCE_DIR edbr SOURCE_DIR)
set(EDBR_SHADERS_DIR "${EDBR_SOURCE_DIR}/src/shaders/")
list(TRANSFORM SHADERS PREPEND "${EDBR_SHADERS_DIR}")

target_shaders(game ${SHADERS})
&lt;/code&gt;
    &lt;p&gt;Now, when you build a &lt;code&gt;game&lt;/code&gt; target, shaders get built automatically and the resulting SPIR-V files are put into the binary directory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Push constants, descriptor sets and bindless descriptors&lt;/head&gt;
    &lt;p&gt;Passing data to shaders in OpenGL is much simpler than it is in Vulkan. In OpenGL, you could just do this:&lt;/p&gt;
    &lt;p&gt;In shader:&lt;/p&gt;
    &lt;code&gt;uniform float someFloat;
&lt;/code&gt;
    &lt;p&gt;In C++ code:&lt;/p&gt;
    &lt;code&gt;const auto loc = glGetUniformLocation(shader, "someFloat");
glUseProgram(shader);
glUniform1f(loc, 42.f);
&lt;/code&gt;
    &lt;p&gt;You can also use explicit uniform location like this.&lt;/p&gt;
    &lt;p&gt;In shader:&lt;/p&gt;
    &lt;code&gt;layout(location = 20) uniform float someFloat;
&lt;/code&gt;
    &lt;p&gt;In code:&lt;/p&gt;
    &lt;code&gt;const auto loc = 20;
glUniform1f(loc, 42.f);
&lt;/code&gt;
    &lt;p&gt;In Vulkan, you need to group your uniforms into “descriptor sets”:&lt;/p&gt;
    &lt;code&gt;// set 0
layout (set = 0, binding = 0) uniform float someFloat;
layout (set = 0, binding = 1) uniform mat4 someMatrix;
// set 1
layout (set = 1, binding = 0) uniform float someOtherFloat;
... // etc.
&lt;/code&gt;
    &lt;p&gt;Now, this makes things a lot more complicated, because you need to specify descriptor set layout beforehand, use descriptor set pools and allocate descriptor sets with them, do the whole &lt;code&gt;VkWriteDescriptorSet&lt;/code&gt; + &lt;code&gt;vkUpdateDescriptorSets&lt;/code&gt; thing, call &lt;code&gt;vkCmdBindDescriptorSets&lt;/code&gt; for each descriptor set and so on.&lt;/p&gt;
    &lt;p&gt;I’ll explain later how I avoided using descriptor sets by using bindless descriptors and buffer device access. Basically, I only have one “global” descriptor set for bindless textures and samplers, and that’s it. Everything else is passed via push constants which makes everything much easier to handle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pipeline pattern&lt;/head&gt;
    &lt;p&gt;I separate drawing steps into “pipeline” classes.&lt;/p&gt;
    &lt;p&gt;Most of them look like this:&lt;/p&gt;
    &lt;code&gt;class PostFXPipeline {
public:
    void init(GfxDevice&amp;amp; gfxDevice, VkFormat drawImageFormat);
    void cleanup(VkDevice device);

    void draw(
        VkCommandBuffer cmd,
        GfxDevice&amp;amp; gfxDevice,
        const GPUImage&amp;amp; drawImage,
        const GPUImage&amp;amp; depthImage,
        const GPUBuffer&amp;amp; sceneDataBuffer);

private:
    VkPipelineLayout pipelineLayout;
    VkPipeline pipeline;

    struct PushConstants {
        VkDeviceAddress sceneDataBuffer;
        std::uint32_t drawImageId;
        std::uint32_t depthImageId;
    };
};
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;init&lt;/code&gt;loads needed shaders and initializes&lt;code&gt;pipeline&lt;/code&gt;and&lt;code&gt;pipelineLayout&lt;/code&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;void PostFXPipeline::init(GfxDevice&amp;amp; gfxDevice, VkFormat drawImageFormat)
{
    const auto&amp;amp; device = gfxDevice.getDevice();

    const auto pcRange = VkPushConstantRange{
        .stageFlags = VK_SHADER_STAGE_FRAGMENT_BIT,
        .offset = 0,
        .size = sizeof(PushConstants),
    };

    const auto layouts = std::array{gfxDevice.getBindlessDescSetLayout()};
    const auto pushConstantRanges = std::array{pcRange};
    pipelineLayout = vkutil::createPipelineLayout(device, layouts, pushConstantRanges);

    const auto vertexShader =
        vkutil::loadShaderModule("shaders/fullscreen_triangle.vert.spv", device);
    const auto fragShader =
        vkutil::loadShaderModule("shaders/postfx.frag.spv", device);
    pipeline = PipelineBuilder{pipelineLayout}
                   .setShaders(vertexShader, fragShader)
                   .setInputTopology(VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST)
                   .setPolygonMode(VK_POLYGON_MODE_FILL)
                   .disableCulling()
                   .setMultisamplingNone()
                   .disableBlending()
                   .setColorAttachmentFormat(drawImageFormat)
                   .disableDepthTest()
                   .build(device);
    vkutil::addDebugLabel(device, pipeline, "postFX pipeline");

    vkDestroyShaderModule(device, vertexShader, nullptr);
    vkDestroyShaderModule(device, fragShader, nullptr);
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;init&lt;/code&gt; function is usually called once during the engine initialization. &lt;code&gt;PipelineBuilder&lt;/code&gt; abstraction is described in vkguide here. I modified it a bit to use the Builder pattern to be able to chain the calls.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;cleanup&lt;/code&gt;does all the needed cleanup. It usually simply destroys the pipeline and its layout:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;void PostFXPipeline::cleanup(VkDevice device)
{
    vkDestroyPipeline(device, pipeline, nullptr);
    vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;draw&lt;/code&gt;is called each frame and all the needed inputs are passed as arguments. It’s assumed that the sync is performed outside of the&lt;code&gt;draw&lt;/code&gt;call (see “Synchronization” section below). Some pipelines are only called once per frame - some either take&lt;code&gt;std::vector&lt;/code&gt;of objects to draw or are called like this:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;for (const auto&amp;amp; mesh : meshes) {
    somePipeline.draw(cmd, gfxDevice, mesh, ...);
}
&lt;/code&gt;
    &lt;p&gt;The typical &lt;code&gt;draw&lt;/code&gt; function looks like this:&lt;/p&gt;
    &lt;code&gt;void PostFXPipeline::draw(
    VkCommandBuffer cmd,
    GfxDevice&amp;amp; gfxDevice,
    const GPUImage&amp;amp; drawImage,
    const GPUImage&amp;amp; depthImage,
    const GPUBuffer&amp;amp; sceneDataBuffer)
{
    // Bind the pipeline
    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_GRAPHICS, pipeline);

    // Bind the bindless descriptor set
    gfxDevice.bindBindlessDescSet(cmd, pipelineLayout);

    // Handle push constants
    const auto pcs = PushConstants{
        // BDA - explained below
        .sceneDataBuffer = sceneDataBuffer.address,
        // bindless texture ids - no need for desc. sets!
        // explained below
        .drawImageId = drawImage.getBindlessId(),
        .depthImageId = depthImage.getBindlessId(),
    };
    vkCmdPushConstants(
        cmd, pipelineLayout, VK_SHADER_STAGE_FRAGMENT_BIT, 0, sizeof(PushConstants), &amp;amp;pcs);

    // Finally, do some drawing. Here we're drawing a fullscreen triangle
    // to do a full-screen effect.
    vkCmdDraw(cmd, 3, 1, 0, 0);
}
&lt;/code&gt;
    &lt;p&gt;Note another thing: it’s assumed that &lt;code&gt;draw&lt;/code&gt; is called between &lt;code&gt;vkCmdBeginRendering&lt;/code&gt; and &lt;code&gt;vkCmdEndRendering&lt;/code&gt; - the render pass itself doesn’t care what texture it renders to - the caller of &lt;code&gt;draw&lt;/code&gt; is responsible for that. It makes things simpler and allows you to do several draws to the same render target, e.g.:&lt;/p&gt;
    &lt;code&gt;// handy wrapper for creating VkRenderingInfo
const auto renderInfo = vkutil::createRenderingInfo({
    .renderExtent = drawImage.getExtent2D(),
    .colorImageView = drawImage.imageView,
    .colorImageClearValue = glm::vec4{0.f, 0.f, 0.f, 1.f},
    .depthImageView = depthImage.imageView,
    .depthImageClearValue = 0.f,
    // for MSAA
    .resolveImageView = resolveImage.imageView,
});

vkCmdBeginRendering(cmd, &amp;amp;renderInfo.renderingInfo);

// draw meshes
for (const auto&amp;amp; mesh : meshesToDraw) {
    meshPipeline.draw(cmd, gfxDevice, mesh, ...);
}
// draw sky
skyboxPipeline.draw(cmd, gfxDevice, camera);

vkCmdEndRendering(cmd);
&lt;/code&gt;
    &lt;quote&gt;I use&lt;code&gt;VK_KHR_dynamic_rendering&lt;/code&gt;everywhere. I don’t use Vulkan render passes and subpasses at all. I’ve heard that they’re more efficient on tile-based GPUs, but I don’t care about mobile support for now.&lt;code&gt;VK_KHR_dynamic_rendering&lt;/code&gt;just makes everything much easier.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Using programmable vertex pulling (PVP) + buffer device address (BDA)&lt;/head&gt;
    &lt;p&gt;I have one vertex type for all the meshes. It looks like this:&lt;/p&gt;
    &lt;code&gt;struct Vertex {
    vec3 position;
    float uv_x;
    vec3 normal;
    float uv_y;
    vec4 tangent;
};
&lt;/code&gt;
    &lt;quote&gt;Of course, you can greatly optimize it using various methods, but it’s good enough for me for now. The&lt;code&gt;uv_x&lt;/code&gt;/&lt;code&gt;uv_y&lt;/code&gt;separation comes from vkguide - I think it’s a nice idea to get good alignment and not waste any bytes&lt;/quote&gt;
    &lt;p&gt;The vertices are accessed in the shader like this:&lt;/p&gt;
    &lt;code&gt;layout (buffer_reference, std430) readonly buffer VertexBuffer {
    Vertex vertices[];
};

layout (push_constant, scalar) uniform constants
{
    VertexBuffer vertexBuffer;
    ... // other stuff
} pcs;

void main()
{
    Vertex v = pcs.vertexBuffer.vertices[gl_VertexIndex];
    ...
}
&lt;/code&gt;
    &lt;p&gt;PVP frees you from having to define vertex format (no more VAOs like in OpenGL or &lt;code&gt;VkVertexInputBindingDescription&lt;/code&gt; + &lt;code&gt;VkVertexInputAttributeDescription&lt;/code&gt; in Vulkan). BDA also frees you from having to bind a buffer to a descriptor set - you just pass an address to your buffer which contains vertices in push constants and that’s it.&lt;/p&gt;
    &lt;code&gt;
  Also note the  scalar layout for push constants. I use it for all the buffers too. Compared to “std430” layout, it makes alignment a lot more easy to handle - it almost works the same as in C++ and greatly reduces the need for “padding” members in C++ structs.
&lt;/code&gt;
    &lt;head rend="h3"&gt;Bindless descriptors&lt;/head&gt;
    &lt;p&gt;Textures were painful to work with even in OpenGL - you had “texture slots” which were awkward to work with. You couldn’t just sample any texture from the shader if it wasn’t bound to a texture slot beforehand. &lt;code&gt;ARB_bindless_texture&lt;/code&gt; changed that and made many things easier.&lt;/p&gt;
    &lt;p&gt;Vulkan doesn’t have the exact same functionality, but it has something similar. You can create big descriptor sets which look like this:&lt;/p&gt;
    &lt;code&gt;// bindless.glsl
layout (set = 0, binding = 0) uniform texture2D textures[];
...
layout (set = 0, binding = 1) uniform sampler samplers[];
&lt;/code&gt;
    &lt;p&gt;You’ll need to maintain a list of all your textures using some “image manager” and when a new texture is loaded, you need to insert it into the &lt;code&gt;textures&lt;/code&gt; array. The index at which you inserted it becomes a bindless “texture id” which then can be used to sample it in shaders. Now you can pass these ids in your push constants like this:&lt;/p&gt;
    &lt;code&gt;layout (push_constant, scalar) uniform constants
{
  uint textureId;
  ...
} pcs;
&lt;/code&gt;
    &lt;p&gt;and then you can sample your texture in the fragment shader like this:&lt;/p&gt;
    &lt;code&gt;// bindless.glsl
#define NEAREST_SAMPLER_ID 0
...

vec4 sampleTexture2DNearest(uint texID, vec2 uv) {
    return texture(nonuniformEXT(sampler2D(textures[texID], samplers[NEAREST_SAMPLER_ID])), uv);
}

// shader.frag
vec4 color = sampleTexture2DNearest(pcs.textureId, inUV);
&lt;/code&gt;
    &lt;p&gt;Two things to note:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I chose separate image samplers so that I could sample any texture using different samplers. Common samplers (nearest, linear with anisotropy, depth texture samplers) are created and put into &lt;code&gt;samplers&lt;/code&gt;array on the startup.&lt;/item&gt;
      &lt;item&gt;The wrapper function makes the process of sampling a lot more convenient.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
  The placement of  nonuniformEXT is somewhat tricky and is explained very well here.
&lt;/code&gt;
    &lt;p&gt;I use bindless ids for the mesh material buffer which looks like this:&lt;/p&gt;
    &lt;code&gt;struct MaterialData {
    vec4 baseColor;
    vec4 metallicRoughnessEmissive;
    uint diffuseTex;
    uint normalTex;
    uint metallicRoughnessTex;
    uint emissiveTex;
};

layout (buffer_reference, std430) readonly buffer MaterialsBuffer {
    MaterialData data[];
} materialsBuffer;
&lt;/code&gt;
    &lt;p&gt;Now I can only pass material ID in my push constants and then sample texture like this in the fragment shader:&lt;/p&gt;
    &lt;code&gt;MaterialData material = materials[pcs.materialID];
vec4 diffuse = sampleTexture2DLinear(material.diffuseTex, inUV);
...
&lt;/code&gt;
    &lt;p&gt;Neat! No more bulky descriptor sets, just one int per material in the push constants.&lt;/p&gt;
    &lt;p&gt;You can also put different texture types into the same set like this (this is needed for being able to access textures of types other than &lt;code&gt;texture2D&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;layout (set = 0, binding = 0) uniform texture2D textures[];
layout (set = 0, binding = 0) uniform texture2DMS texturesMS[];
layout (set = 0, binding = 0) uniform textureCube textureCubes[];
layout (set = 0, binding = 0) uniform texture2DArray textureArrays[];
&lt;/code&gt;
    &lt;p&gt;And here’s how you can sample &lt;code&gt;textureCube&lt;/code&gt; with a linear sampler (note that we use &lt;code&gt;textureCubes&lt;/code&gt; here instead of &lt;code&gt;textures&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;vec4 sampleTextureCubeLinear(uint texID, vec3 p) {
    return texture(nonuniformEXT(samplerCube(textureCubes[texID], samplers[NEAREST_SAMPLER_ID])), p);
}
&lt;/code&gt;
    &lt;p&gt;Here’s a very good article on using bindless textures in Vulkan:&lt;/p&gt;
    &lt;p&gt;https://jorenjoestar.github.io/post/vulkan_bindless_texture/&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling dynamic data which needs to be uploaded every frame&lt;/head&gt;
    &lt;p&gt;I find it useful to pre-allocate big arrays of things and push stuff to them in every frame. Basically, you can pre-allocate an array of N structs (or matrices) and then start at index 0 at each new frame and push things to it from the CPU. Then, you can access all these items in your shaders. For example, I have all joint matrices stored in one big &lt;code&gt;mat4&lt;/code&gt; array and the skinning compute shader accesses joint matrices of a particular mesh using start index passed via push constants (more about it will be explained later).&lt;/p&gt;
    &lt;p&gt;Here are two ways of doing this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;Have N buffers on GPU and swap between them.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;vkguide explains the concept of “in flight” frames pretty well. To handle this parallelism properly, you need to have one buffer for the “currently drawing” frame and one buffer for “currently recording new drawing commands” frame to not have races. (If you have more frames in flight, you’ll need to allocate more than 2 buffers)&lt;/p&gt;
    &lt;p&gt;This means that you need to preallocate 2 buffers on GPU. You write data from CPU to GPU to the first buffer during the first frame. While you record the second frame, GPU reads from the first buffer while you write new data to the second buffer. On the third frame, GPU reads from the second buffer and you write new info to the first buffer… and so on.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;One buffer on GPU and N “staging” buffers on CPU&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This might be useful if you need to conserve some memory on the GPU.&lt;/p&gt;
    &lt;p&gt;Let’s see how it works in my engine:&lt;/p&gt;
    &lt;code&gt;class NBuffer {
public:
    void init(
        GfxDevice&amp;amp; gfxDevice,
        VkBufferUsageFlags usage,
        std::size_t dataSize,
        std::size_t numFramesInFlight,
        const char* label);

    void cleanup(GfxDevice&amp;amp; gfxDevice);

    void uploadNewData(
        VkCommandBuffer cmd,
        std::size_t frameIndex,
        void* newData,
        std::size_t dataSize,
        std::size_t offset = 0);

    const GPUBuffer&amp;amp; getBuffer() const { return gpuBuffer; }

private:
    std::size_t framesInFlight{0};
    std::size_t gpuBufferSize{0};
    std::vector&amp;lt;GPUBuffer&amp;gt; stagingBuffers;
    GPUBuffer gpuBuffer;
    bool initialized{false};
};

void NBuffer::init(
    GfxDevice&amp;amp; gfxDevice,
    VkBufferUsageFlags usage,
    std::size_t dataSize,
    std::size_t numFramesInFlight,
    const char* label)
{
    ...

    gpuBuffer = gfxDevice.createBuffer(
        dataSize, usage | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VMA_MEMORY_USAGE_AUTO_PREFER_DEVICE);
    vkutil::addDebugLabel(gfxDevice.getDevice(), gpuBuffer.buffer, label);

    for (std::size_t i = 0; i &amp;lt; numFramesInFlight; ++i) {
        stagingBuffers.push_back(gfxDevice.createBuffer(
            dataSize, usage | VK_BUFFER_USAGE_TRANSFER_SRC_BIT, VMA_MEMORY_USAGE_AUTO_PREFER_HOST));
    }

    ...
}
&lt;/code&gt;
    &lt;p&gt;Note how staging buffers are created using VMA’s &lt;code&gt;PREFER_HOST&lt;/code&gt; flag and the “main” buffer from which we read in the shader is using the &lt;code&gt;PREFER_DEVICE&lt;/code&gt; flag.&lt;/p&gt;
    &lt;p&gt;Here’s how new data is uploaded (full implementation):&lt;/p&gt;
    &lt;code&gt;void NBuffer::uploadNewData(
    VkCommandBuffer cmd,
    std::size_t frameIndex,
    void* newData,
    std::size_t dataSize,
    std::size_t offset) const
{
    assert(initialized);
    assert(frameIndex &amp;lt; framesInFlight);
    assert(offset + dataSize &amp;lt;= gpuBufferSize &amp;amp;&amp;amp; "NBuffer::uploadNewData: out of bounds write");

    if (dataSize == 0) {
        return;
    }

    // sync with previous read
    ... // READ BARRIER CODE HERE

    auto&amp;amp; staging = stagingBuffers[frameIndex];
    auto* mappedData = reinterpret_cast&amp;lt;std::uint8_t*&amp;gt;(staging.info.pMappedData);
    memcpy((void*)&amp;amp;mappedData[offset], newData, dataSize);

    const auto region = VkBufferCopy2{
        .sType = VK_STRUCTURE_TYPE_BUFFER_COPY_2,
        .srcOffset = (VkDeviceSize)offset,
        .dstOffset = (VkDeviceSize)offset,
        .size = dataSize,
    };
    const auto bufCopyInfo = VkCopyBufferInfo2{
        .sType = VK_STRUCTURE_TYPE_COPY_BUFFER_INFO_2,
        .srcBuffer = staging.buffer,
        .dstBuffer = gpuBuffer.buffer,
        .regionCount = 1,
        .pRegions = &amp;amp;region,
    };

    vkCmdCopyBuffer2(cmd, &amp;amp;bufCopyInfo);

    // sync with write
    ... // WRITE BARRIER CODE HERE
}
&lt;/code&gt;
    &lt;p&gt;I’d go with the first approach for most cases (more data on GPU, but no need for manual sync) unless you need to conserve GPU memory for some reason. I’ve found no noticeable difference in performance between two approaches, but it might matter if you are uploading huge amounts of data to GPU on each frame.&lt;/p&gt;
    &lt;head rend="h3"&gt;Destructors, deletion queue and cleanup&lt;/head&gt;
    &lt;p&gt;Now, this might be somewhat controversial… but I didn’t find much use of the deletion queue pattern used in vkguide. I don’t really need to allocated/destroy new objects on every frame.&lt;/p&gt;
    &lt;p&gt;Using C++ destructors for Vulkan object cleanup is not very convenient either. You need to wrap everything in custom classes, add move constructors and move &lt;code&gt;operator=&lt;/code&gt;… It adds an additional layer of complexity.&lt;/p&gt;
    &lt;p&gt;In most cases, the cleanup of Vulkan objects happens in one place - and you don’t want to accidentally destroy some in-use object mid-frame by accidentally destroying some wrapper object.&lt;/p&gt;
    &lt;p&gt;It’s also harder to manage lifetimes when you have cleanup in happening in the destructor. For example, suppose you have a case like this:&lt;/p&gt;
    &lt;code&gt;struct SomeClass {
    SomeOtherClass b;

    void init() {
        ...
    }

    void cleanup() {
        ...
    }
}
&lt;/code&gt;
    &lt;p&gt;If you want to cleanup &lt;code&gt;SomeOtherClass&lt;/code&gt; resources (e.g. the instance of &lt;code&gt;SomeOtherClass&lt;/code&gt; has a &lt;code&gt;VkPipeline&lt;/code&gt; object) during &lt;code&gt;SomeClass::cleanup&lt;/code&gt;, you can’t do that if the cleanup of &lt;code&gt;SomeOtherClass&lt;/code&gt; is performed in its destructor.&lt;/p&gt;
    &lt;p&gt;Of course, you can do this:&lt;/p&gt;
    &lt;code&gt;struct SomeClass {
    std::unique_ptr&amp;lt;SomeOtherClass&amp;gt; b;

    void init() {
        b = std::make_unique&amp;lt;SomeOtherClass&amp;gt;();
        ...
    }

    void cleanup() {
        b.reset();
        ...
    }
}
&lt;/code&gt;
    &lt;p&gt;… but I don’t like how it introduces a dynamic allocation and requires you to do write more code (and it’s not that much different from calling a &lt;code&gt;cleanup&lt;/code&gt; function manually).&lt;/p&gt;
    &lt;p&gt;Right now, I prefer to clean up stuff directly, e.g.&lt;/p&gt;
    &lt;code&gt;class SkyboxPipeline {
public:
    void cleanup(VkDevice device) {
        vkDestroyPipeline(device, pipeline, nullptr);
        vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
    }

private:
    VkPipelineLayout pipelineLayout;
    VkPipeline pipeline;
    ...
}

// in GameRenderer.cpp:
void GameRenderer::cleanup(VkDevice device) {
    ...
    skyboxPipeline.cleanup(device);
    ...
}
&lt;/code&gt;
    &lt;p&gt;This approach is not perfect - first of all, it’s easy to forget to call &lt;code&gt;cleanup&lt;/code&gt; function, This is not a huge problem since you get a validation error in case you forget to cleanup some Vulkan resources on shutdown:&lt;/p&gt;
    &lt;code&gt;Validation Error: [ VUID-vkDestroyDevice-device-05137 ] Object 0: handle = 0x4256c1000000005d, type = VK_OBJECT_TYPE_PIPELINE_LAYOUT; | MessageID = 0x4872eaa0 | vkCreateDevice():  OBJ ERROR : For VkDevice 0x27bd530[], VkPipelineLayout 0x4256c1000000005d[] has not been destroyed. The Vulkan spec states: All child objects created on device must have been destroyed prior to destroying device (https://vulkan.lunarg.com/doc/view/1.3.280.1/linux/1.3-extensions/vkspec.html#VUID-vkDestroyDevice-device-05137)
&lt;/code&gt;
    &lt;p&gt;VMA also triggers asserts if you forget to free some buffer/image allocated with it.&lt;/p&gt;
    &lt;p&gt;I find it convenient to have all the Vulkan cleanup happening explicitly in one place. It makes it easy to track when the objects get destroyed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Synchronization&lt;/head&gt;
    &lt;p&gt;Synchronization in Vulkan is difficult. OpenGL and WebGPU do it for you - if you read from some texture/buffer, you know that it will have the correct data and you won’t get problems with data races. With Vulkan, you need to be explicit and this is usually where things tend to get complicated.&lt;/p&gt;
    &lt;p&gt;Right now I manage most of the complexities of sync manually in one place. I separate my drawing into “passes”/pipelines (as described above) and then insert barriers between them. For example, the skinning pass writes new vertex data into GPU memory. Shadow mapping pass reads this data to render skinned meshes into the shadow map. Sync in my code looks like this:&lt;/p&gt;
    &lt;code&gt;// do skinning in compute shader
for (const auto&amp;amp; mesh : skinnedMeshes) {
    skinningPass.doSkinning(gfxDevice, mesh);
}

{
    // Sync skinning with CSM
    // This is a "fat" barrier and you can potentially optimize it
    // by specifying all the buffers that the next pass will read from
    const auto memoryBarrier = VkMemoryBarrier2{
        .sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER_2,
        .srcStageMask = VK_PIPELINE_STAGE_2_COMPUTE_SHADER_BIT,
        .srcAccessMask = VK_ACCESS_2_SHADER_WRITE_BIT,
        .dstStageMask = VK_PIPELINE_STAGE_2_VERTEX_SHADER_BIT,
        .dstAccessMask = VK_ACCESS_2_MEMORY_READ_BIT,
    };
    const auto dependencyInfo = VkDependencyInfo{
        .sType = VK_STRUCTURE_TYPE_DEPENDENCY_INFO,
        .memoryBarrierCount = 1,
        .pMemoryBarriers = &amp;amp;memoryBarrier,
    };
    vkCmdPipelineBarrier2(cmd, &amp;amp;dependencyInfo);
}

// do shadow mapping
shadowMappingPass.draw(gfxDevice, ...);
&lt;/code&gt;
    &lt;p&gt;Of course, this can be automated/simplified using render graphs. This is something that I might implement in the future. Right now I’m okay with doing manual sync. vkconfig’s “synchronization” validation layer also helps greatly in finding sync errors.&lt;/p&gt;
    &lt;p&gt;The following resources were useful for understanding synchronization:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;https://themaister.net/blog/2019/08/14/yet-another-blog-explaining-vulkan-synchronization/&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;More implementation notes&lt;/head&gt;
    &lt;head rend="h3"&gt;Drawing many sprites&lt;/head&gt;
    &lt;p&gt;With bindless textures, it’s easy to draw many sprites using one draw call without having to allocate vertex buffers at all.&lt;/p&gt;
    &lt;p&gt;First of all, you can emit vertex coordinates and UVs using &lt;code&gt;gl_VertexIndex&lt;/code&gt; in your vertex shader like this:&lt;/p&gt;
    &lt;code&gt;void main()
{
    uint b = 1 &amp;lt;&amp;lt; (gl_VertexIndex % 6);
    vec2 baseCoord = vec2((0x1C &amp;amp; b) != 0, (0xE &amp;amp; b) != 0);
    ...
}
&lt;/code&gt;
    &lt;p&gt;This snippet produces this set of values:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;gl_VertexIndex&lt;/cell&gt;
        &lt;cell role="head"&gt;baseCoord&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;(0,0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;(0,1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;(1,1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;(1,1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;(1,0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;(0,0)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All the sprite draw calls are combined into &lt;code&gt;SpriteDrawBuffer&lt;/code&gt; which looks like this in GLSL:&lt;/p&gt;
    &lt;code&gt;struct SpriteDrawCommand {
    mat4 transform; // could potentially be mat2x2...
    vec2 uv0; // top-left uv coord
    vec2 uv1; // bottom-right uv coord
    vec4 color; // color by which texture is multiplied
    uint textureID; // sprite texture
    uint shaderID; // explained below
    vec2 padding; // padding to satisfy "scalar" requirements
};

layout (buffer_reference, scalar) readonly buffer SpriteDrawBuffer {
    SpriteDrawCommand commands[];
};
&lt;/code&gt;
    &lt;p&gt;On CPU/C++ side, it looks almost the same:&lt;/p&gt;
    &lt;code&gt;struct SpriteDrawCommand {
    glm::mat4 transform;
    glm::vec2 uv0; // top-left uv coordinate
    glm::vec2 uv1; // bottom-right uv coodinate
    LinearColor color; // color by which texture is multiplied by
    std::uint32_t textureId; // sprite texture
    std::uint32_t shaderId; // explained below
    glm::vec2 padding; // padding
};

std::vector&amp;lt;SpriteDrawCommand&amp;gt; spriteDrawCommands;
&lt;/code&gt;
    &lt;p&gt;I create two fixed size buffers on the GPU and then upload the contents of &lt;code&gt;spriteDrawCommands&lt;/code&gt; (using techniques described above in the “Handling dynamic data” section).&lt;/p&gt;
    &lt;p&gt;The sprite renderer is used like this:&lt;/p&gt;
    &lt;code&gt;// record commands
renderer.beginDrawing();
{
    renderer.drawSprite(sprite, pos);
    renderer.drawText(font, "Hello");
    renderer.drawRect(...);
}
renderer.endDrawing();

// do actual drawing later:
renderer.draw(cmd, gfxDevice, ...);
&lt;/code&gt;
    &lt;code&gt;
  The same renderer also draws text, rectangles and lines in my engine. For example, the text is just N “draw sprite” commands for a string composed of N glyphs. Solid color rectangles and lines are achieved by using a 1x1 pixel white texture and multiplying it by  SpriteCommand::color in the fragment shader.
&lt;/code&gt;
    &lt;p&gt;And finally, here’s how the command to do the drawing looks like inside &lt;code&gt;SpriteRenderer::draw&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;vkCmdDraw(cmd, 6, spriteDrawCommands.size(), 0, 0);
// 6 vertices per instance, spriteDrawCommands.size() instances in total
&lt;/code&gt;
    &lt;p&gt;The complete sprite.vert looks like this:&lt;/p&gt;
    &lt;code&gt;#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_buffer_reference : require

#include "sprite_commands.glsl"

layout (push_constant) uniform constants
{
    mat4 viewProj; // 2D camera matrix
    SpriteDrawBuffer drawBuffer; // where sprite draw commands are stored
} pcs;

layout (location = 0) out vec2 outUV;
layout (location = 1) out vec4 outColor;
layout (location = 2) flat out uint textureID;
layout (location = 3) flat out uint shaderID;

void main()
{
    uint b = 1 &amp;lt;&amp;lt; (gl_VertexIndex % 6);
    vec2 baseCoord = vec2((0x1C &amp;amp; b) != 0, (0xE &amp;amp; b) != 0);

    SpriteDrawCommand command = pcs.drawBuffer.commands[gl_InstanceIndex];

    gl_Position = pcs.viewProj * command.transform * vec4(baseCoord, 0.f, 1.f);
    outUV = (1.f - baseCoord) * command.uv0 + baseCoord * command.uv1;
    outColor = command.color;
    textureID = command.textureID;
    shaderID = command.shaderID;
}
&lt;/code&gt;
    &lt;p&gt;All the parameters of the sprite draw command are self-explanatory, but &lt;code&gt;shaderID&lt;/code&gt; needs a bit of clarification. Currently, I use it to branch inside the fragment shader:&lt;/p&gt;
    &lt;code&gt;...

#define SPRITE_SHADER_ID 0
#define TEXT_SHADER_ID   1

void main()
{
    vec4 texColor = sampleTexture2DNearest(textureID, inUV);

    // text drawing is performed differently...
    if (shaderID == TEXT_SHADER_ID) {
        // glyph atlas uses single-channel texture
        texColor = vec4(1.0, 1.0, 1.0, texColor.r);
    }

    if (texColor.a &amp;lt; 0.1) {
        discard;
    }

    outColor = inColor * texColor;
}
&lt;/code&gt;
    &lt;p&gt;This allows me to draw sprites differently depending on this ID without having to change pipelines. Of course, it can be potentially bad for the performance. This can be improved by drawing sprites with the same shader ID in batches. You’ll only need to switch pipelines when you encounter a draw command with a different shader ID.&lt;/p&gt;
    &lt;p&gt;The sprite renderer is very efficient: it can draw 10 thousand sprites in just 315 microseconds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compute skinning&lt;/head&gt;
    &lt;p&gt;I do skinning for skeletal animation in a compute shader. This allows me to have the same vertex format for all the meshes.&lt;/p&gt;
    &lt;p&gt;Basically, I just take the mesh’s vertices (not skinned) and joint matrices and produce a new buffer of vertices which are used in later rendering stages.&lt;/p&gt;
    &lt;p&gt;Suppose you spawn three cats with identical meshes:&lt;/p&gt;
    &lt;p&gt;All three of them can have different animations. They all have an identical “input” mesh. But the “output” vertex buffer will differ between them, which means that you need to pre-allocate a vertex buffer for each instance of the mesh.&lt;/p&gt;
    &lt;p&gt;Here’s how the skinning compute shader looks like:&lt;/p&gt;
    &lt;code&gt;#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_buffer_reference : require

#include "vertex.glsl"

struct SkinningDataType {
    ivec4 jointIds;
    vec4 weights;
};

layout (buffer_reference, std430) readonly buffer SkinningData {
    SkinningDataType data[];
};

layout (buffer_reference, std430) readonly buffer JointMatrices {
    mat4 matrices[];
};

layout (push_constant) uniform constants
{
    JointMatrices jointMatrices;
    uint jointMatricesStartIndex;
    uint numVertices;
    VertexBuffer inputBuffer;
    SkinningData skinningData;
    VertexBuffer outputBuffer;
} pcs;

layout (local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

mat4 getJointMatrix(int jointId) {
    return pcs.jointMatrices.matrices[pcs.jointMatricesStartIndex + jointId];
}

void main()
{
    uint index = gl_GlobalInvocationID.x;
    if (index &amp;gt;= pcs.numVertices) {
        return;
    }

    SkinningDataType sd = pcs.skinningData.data[index];
    mat4 skinMatrix =
        sd.weights.x * getJointMatrix(sd.jointIds.x) +
        sd.weights.y * getJointMatrix(sd.jointIds.y) +
        sd.weights.z * getJointMatrix(sd.jointIds.z) +
        sd.weights.w * getJointMatrix(sd.jointIds.w);

    Vertex v = pcs.inputBuffer.vertices[index];
    v.position = vec3(skinMatrix * vec4(v.position, 1.0));

    pcs.outputBuffer.vertices[index] = v;
}
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I store all joint matrices in a big array and populate it every frame (and also pass the starting index in the array for each skinned mesh, &lt;code&gt;jointMatricesStartIndex&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Skinning data is not stored inside each mesh vertex, a separate buffer of &lt;code&gt;num_vertices&lt;/code&gt;elements is used.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After the skinning is performed, all the later rendering stages use this set of vertices Thee rendering process for static and skinned meshes becomes identical, thanks to that.&lt;/p&gt;
    &lt;quote&gt;Anton’s OpenGL 4 Tutorials book has the best skinning implementation guide I’ve ever read. Game Engine Architecture by Jason Gregory has nice explanations about skinning/skeletal animation math as well.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Game / renderer separation&lt;/head&gt;
    &lt;p&gt;I have a game/renderer separation which uses a simple concept of “draw commands”. In the game logic, I use entt, but the renderer doesn’t know anything about entities or “game objects”. It only knows about the lights, some scene parameters (like fog, which skybox texture to use etc) and meshes it needs to draw.&lt;/p&gt;
    &lt;p&gt;The renderer’s API looks like this in action:&lt;/p&gt;
    &lt;code&gt;void Game::generateDrawList()
{
    renderer.beginDrawing();

    // Add lights
    const auto lights = ...; // get list of all active lights
    for (const auto&amp;amp;&amp;amp; [e, tc, lc] : lights.each()) {
        renderer.addLight(lc.light, tc.transform);
    }

    // Render static meshes
    const auto staticMeshes = ...; // list of entities with static meshes
    for (const auto&amp;amp;&amp;amp; [e, tc, mc] : staticMeshes.each()) {
        // Each "mesh" can have multiple submeshes similar to how
        // glTF separates each "mesh" into "primitives".
        for (std::size_t i = 0; i &amp;lt; mc.meshes.size(); ++i) {
            renderer.drawMesh(mc.meshes[i], tc.worldTransform, mc.castShadow);
        }
    }

    // Render meshes with skeletal animation
    const auto skinnedMeshes = ...; // list of entities with skeletal animations
    for (const auto&amp;amp;&amp;amp; [e, tc, mc, sc] : skinnedMeshes.each()) {
        renderer.drawSkinnedMesh(
            mc.meshes, sc.skinnedMeshes, tc.worldTransform,
            sc.skeletonAnimator.getJointMatrices());
    }

    renderer.endDrawing();
}
&lt;/code&gt;
    &lt;p&gt;When you call &lt;code&gt;drawMesh&lt;/code&gt; or &lt;code&gt;drawSkinnedMesh&lt;/code&gt;, the renderer creates a mesh draw command and puts it in &lt;code&gt;std::vector&amp;lt;MeshDrawCommand&amp;gt;&lt;/code&gt; which are then iterated through during the drawing process. The &lt;code&gt;MeshDrawCommand&lt;/code&gt; looks like this:&lt;/p&gt;
    &lt;code&gt;
struct SkinnedMesh {
    GPUBuffer skinnedVertexBuffer;
};

struct MeshDrawCommand {
    MeshId meshId;
    glm::mat4 transformMatrix;
    math::Sphere worldBoundingSphere;

    const SkinnedMesh* skinnedMesh{nullptr};
    std::uint32_t jointMatricesStartIndex;
    bool castShadow{true};
};
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;meshId&lt;/code&gt;is used for looking up static meshes in&lt;code&gt;MeshCache&lt;/code&gt;- it’s a simple&lt;code&gt;std::vector&lt;/code&gt;of references to vertex buffers on GPU.&lt;/item&gt;
      &lt;item&gt;If the mesh has a skeleton, &lt;code&gt;jointMatricesStartIndex&lt;/code&gt;is used during compute skinning and&lt;code&gt;skinnedMesh-&amp;gt;skinnedVertexBuffer&lt;/code&gt;is used for all the rendering afterwards (instead of&lt;code&gt;meshId&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;worldBoundingSphere&lt;/code&gt;is used for frustum culling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This separation is nice because the renderer is clearly separated from the game logic. You can also do something more clever as described here if sorting draw commands becomes a bottleneck.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scene loading and entity prefabs&lt;/head&gt;
    &lt;p&gt;I use Blender as a level editor and export it as glTF. It’s easy to place objects, colliders and lights there. Here’s how it looks like:&lt;/p&gt;
    &lt;p&gt;Writing your own level editor would probably take months (years!), so using Blender instead saved me quite a lot of time.&lt;/p&gt;
    &lt;p&gt;It’s important to mention how I use node names for spawning some objects. For example, you can see an object named &lt;code&gt;Interact.Sphere.Diary&lt;/code&gt; selected in the screenshot above. The part before the first dot is the prefab name (in this case “Interact”). The “Sphere” part is used by the physics system to create a sphere physics body for the object (“Capsule” and “Box” can also be used, otherwise the physics shape is created using mesh vertices).&lt;/p&gt;
    &lt;p&gt;Some models are pretty complex and I don’t want to place them directly into the level glTF file as it’ll greatly increase each level’s size. I just place an “Empty-&amp;gt;Arrows” object and name it something like “Cat.NearStore”. This will spawn “Cat” prefab and attach “NearStore” tag to it for runtime identification.&lt;/p&gt;
    &lt;p&gt;Prefabs are written in JSON and look like this:&lt;/p&gt;
    &lt;code&gt;{
  "scene": {
    "scene": "assets/models/cato.gltf"
  },
  "movement": {
    "maxSpeed": [4, 4, 4]
  },
  "physics": {
    "type": "dynamic",
    "bodyType": "virtual_character",
    "bodyParams": {
        ...
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;During the level loading process, if the node doesn’t have a corresponding prefab, it’s loaded as-is and its mesh data is taken from the glTF file itself (this is mostly used for static geometry). If the node has a corresponding prefab loaded, it’s created instead. Its mesh data is loaded from the external glTF file - only transform is copied from the original glTF node (the one in the level glTF file).&lt;/p&gt;
    &lt;quote&gt;Once glTFX is released and the support for it is added to Blender, things might be even easier to handle as you’ll be able to reference external glTF files with it.&lt;/quote&gt;
    &lt;head rend="h3"&gt;MSAA&lt;/head&gt;
    &lt;p&gt;Using forward rendering allowed me to easily implement MSAA. Here’s a comparison of how the game looks without AA and with MSAA on:&lt;/p&gt;
    &lt;p&gt;MSAA is explained well here: https://vulkan-tutorial.com/Multisampling&lt;/p&gt;
    &lt;p&gt;Here’s another good article about MSAA: https://therealmjp.github.io/posts/msaa-overview/ and potential problems you can have with it (especially with HDR and tone-mapping).&lt;/p&gt;
    &lt;head rend="h3"&gt;UI&lt;/head&gt;
    &lt;p&gt;My UI system was inspired by Roblox’s UI API: https://create.roblox.com/docs/ui&lt;/p&gt;
    &lt;p&gt;Basically, the UI can calculate its own layout without me having to hard code each individual element’s size and position. Basically it relies on the following concepts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Origin is an anchor around which the UI element is positioned. If origin is &lt;code&gt;(0, 0)&lt;/code&gt;, setting UI element’s position to be&lt;code&gt;(x,y)&lt;/code&gt;will make its upper-left pixel have (x,y) pixel coordinate. If the origin is&lt;code&gt;(1, 1)&lt;/code&gt;, then the element’s bottom-right corner will be positioned at&lt;code&gt;(x, y)&lt;/code&gt;. If the origin is (0.5, 1) then it will be positioned using bottom-center point as the reference.&lt;/item&gt;
      &lt;item&gt;Relative size makes the children’s be proportional to parent’s size. If (1,1) then the child element will have the same size as the parent element. If it’s (0.5, 0.5) then it’ll have half the size of the parent. If the parent uses children’s size as a guide, then if a child has (0.5, 0.25) relative size, the parent’s width will be 2x larger and the height will be 4x larger.&lt;/item&gt;
      &lt;item&gt;Relative position uses parent’s size as a guide for positioning. It’s useful for centering elements, for example if you have an element with (0.5, 0.5) origin and (0.5, 0.5) relative position, it’ll be centered inside its parent element.&lt;/item&gt;
      &lt;item&gt;You can also set pixel offsets for both position and size separately (they’re called &lt;code&gt;offsetPosition&lt;/code&gt;and&lt;code&gt;offsetSize&lt;/code&gt;in my codebase).&lt;/item&gt;
      &lt;item&gt;You can also set a fixed size for the elements if you don’t want them to ever be resized.&lt;/item&gt;
      &lt;item&gt;The label/image element size is determined using its content.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some examples of how it can be used to position child elements:&lt;/p&gt;
    &lt;p&gt;a) The child (yellow) has relative size (0.5, 1), relative position of (0.5, 0.5) and origin (0.5, 0.5) (alternatively, the relative position can be (0.5, 0.0) and origin at (0.5, 0.0) in this case). Its parent (green) will be two times wider, but will have the same height. The child element will be centered inside the parent.&lt;/p&gt;
    &lt;p&gt;b) The child (yellow) has origin (1, 1), fixed size (w,h) and absolute offset of (x,y) - this way, the item can be positioned relative to the bottom-right corner of its parent (green)&lt;/p&gt;
    &lt;p&gt;Let’s see how sizes and positions of UI elements are calculated (implementation in EDBR).&lt;/p&gt;
    &lt;p&gt;First, sizes of all elements are calculated recursively. Then positions are computed based on the previously computed sizes and specified offset positions. Afterwards all elements are drawn recursively - parent element first, then its children etc.&lt;/p&gt;
    &lt;p&gt;When calculating the size, most elements either have a “fixed” size (which you can set manually, e.g. you can set some button to always be 60x60 pixels) or their size is computed based on their content. For example, for label elements, their size is computed using the text’s bounding box. For image elements, their size equals the image size and so on.&lt;/p&gt;
    &lt;p&gt;If an element has an “Auto-size” property, it needs to specify which child will be used to calculate its size. For example, the menu nine-slice can have several text labels inside the “vertical layout” element - the bounding boxes will be calculated first, then their sizes will be summed up - then, the parent’s size is calculated.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at a simple menu with bounding boxes displayed:&lt;/p&gt;
    &lt;p&gt;Here, root &lt;code&gt;NineSliceElement&lt;/code&gt; is marked as “Auto-size”. To compute its size, it first computes the size of its child (&lt;code&gt;ListLayoutElement&lt;/code&gt;). This recursively computes the sizes of each button, sums them up and adds some padding (&lt;code&gt;ListLayoutElement&lt;/code&gt; also makes the width of each button the same based on the maximum width in the list).&lt;/p&gt;
    &lt;head rend="h3"&gt;Dear ImGui and sRGB issues&lt;/head&gt;
    &lt;p&gt;I love Dear ImGui. I used it to implement many useful dev and debug tools (open the image in a new tab to see them better):&lt;/p&gt;
    &lt;p&gt;It has some problems with sRGB, though. I won’t explain it in detail, but basically if you use sRGB framebuffer, Dear ImGui will look wrong in many ways, see the comparison:&lt;/p&gt;
    &lt;p&gt;Sometimes you can see people doing hacks by doing &lt;code&gt;pow(col, vec4(2.2))&lt;/code&gt; with Dear ImGui’s colors but it still doesn’t work properly with alpha and produces incorrect color pickers.&lt;/p&gt;
    &lt;p&gt;I ended up writing my own Dear ImGui backend and implementing DilligentEngine’s workaround which is explained in detail here and here.&lt;/p&gt;
    &lt;quote&gt;Writing it wasn’t as hard as I expected. I only need to write the rendering part, while “logic/OS interaction” part (input event processing, clipboard etc.) is still handled by default Dear ImGui SDL backend in my case.&lt;/quote&gt;
    &lt;p&gt;There are some additional benefits of having my own backend:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It supports bindless texture ids, so I can draw images by simply calling &lt;code&gt;ImGui::Image(bindlessTextureId, ...)&lt;/code&gt;. Dear ImGui’s Vulkan backend requires you to “register” textures by calling&lt;code&gt;ImGui_ImplVulkan_AddTexture&lt;/code&gt;for each texture before you can call&lt;code&gt;ImGui::Image&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;It can properly draw linear and non-linear images by passing their format into backend (so that sRGB images are not gamma corrected twice when they’re displayed)&lt;/item&gt;
      &lt;item&gt;Initializing and dealing with it is easier as it does Vulkan things in the same way as the rest of my engine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Other stuff&lt;/head&gt;
    &lt;p&gt;There are many parts of the engine not covered there because they’re not related to Vulkan. I still feel like it’s good to mention them briefly for the sake of completion.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I use Jolt Physics for physics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Integrating it into the engine was pretty easy. Right now I mostly use it for collision resolution and basic character movement.&lt;/p&gt;
    &lt;p&gt;The samples are fantastic. The docs are very good too.&lt;/p&gt;
    &lt;p&gt;I especially want to point out how incredible &lt;code&gt;JPH::CharacterVirtual&lt;/code&gt; is. It handles basic character movement so well. I remember spending days trying to get proper slope movement in Bullet to work. With Jolt, it just worked “out of the box”.&lt;/p&gt;
    &lt;p&gt;Here’s how it basically works (explaining how it works properly would probably require me to write quite a big article):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You add your shapes to Jolt’s world.&lt;/item&gt;
      &lt;item&gt;You run the simulation.&lt;/item&gt;
      &lt;item&gt;You get new positions of your physics objects and use these positions to render objects in their current positions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I use entt for the entity-component-system part.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It has worked great for me so far. Previously I had my own ECS implementation, but decided to experiment with a 3rd party ECS library to have less code to maintain.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I use openal-soft, libogg and libvorbis for audio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The audio system is mostly based on these articles: https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I use Tracy for profiling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Integrating it was very easy (read the PDF doc, it’s fantastic!) and it helped me avoid tons of bike-shedding by seeing how little time something, which I thought was “inefficient”, really took.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I gained from switching to Vulkan&lt;/head&gt;
    &lt;p&gt;There are many nice things I got after switching to Vulkan:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No more global state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This makes abstractions a lot easier. With OpenGL abstractions/engines, you frequently see “shader.bind()” calls, state trackers, magic RAII, which automatically binds/unbinds objects and so on. There’s no need for that in Vulkan - it’s easy to write functions which take some objects as an input and produce some output - stateless, more explicit and easier to reason about.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;API is more pleasant to work with overall - I didn’t like “binding” things and the whole “global state machine” of OpenGL.&lt;/item&gt;
      &lt;item&gt;You need to write less abstractions overall.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With OpenGL, you need to write a lot of abstractions to make it all less error-prone… Vulkan’s API requires a lot less of this, in my experience. And usually the abstractions that you write map closer to Vulkan’s “raw” functions, compared to OpenGL abstractions which hide manipulation of global state and usually call several functions (and might do some stateful things for optimization).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Better validation errors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Validation errors are very good in Vulkan. While OpenGL has &lt;code&gt;glDebugMessageCallback&lt;/code&gt;, it doesn’t catch that many issues and you’re left wondering why your texture looks weird, why your lighting is broken and so on. Vulkan has more extensive validation which makes the debugging process much better.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debugging in RenderDoc&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I can now debug shaders in RenderDoc. It looks like this:&lt;/p&gt;
    &lt;p&gt;With OpenGL I had to output the values to some texture and color-pick them… which took a lot of time. But now I can debug vertex and fragment shaders easily.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More consistent experience across different GPUs and OSes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With OpenGL, drivers on different GPUs and OSes worked differently from each other which made some bugs pop up only on certain hardware configurations. It made the process of debugging them hard. I still experienced some slight differences between different GPUs in Vulkan, but it’s much less prevalent compared to OpenGL.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ability to use better shading languages in the future&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GLSL is a fine shading language, but there are some new shading languages which promise to be more feature-complete, convenient and readable, for example:&lt;/p&gt;
    &lt;p&gt;I might explore them in the future and see if they offer me something that GLSL lacks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More control over every aspect of the graphics pipeline.&lt;/item&gt;
      &lt;item&gt;Second system effect, but good&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My first OpenGL engine was written during the process of learning graphics programming from scratch. Many abstractions were not that good and rewriting them with some graphics programming knowledge (and some help from vkguide) helped me implement a much cleaner system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Street cred&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And finally, it makes me proud to be able to say “I have a custom engine written in Vulkan and it works”. Sometimes people start thinking about you as a coding wizard and it makes me happy and proud of my work. :)&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;There are many things that I plan to do in the future, here’s a list of some of them:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sign-distance field font support (good article about implementing them)&lt;/item&gt;
      &lt;item&gt;Loading many images and generating mipmaps in parallel (or use image formats which already have mipmaps stored inside of them)&lt;/item&gt;
      &lt;item&gt;Bloom.&lt;/item&gt;
      &lt;item&gt;Volumetric fog.&lt;/item&gt;
      &lt;item&gt;Animation blending.&lt;/item&gt;
      &lt;item&gt;Render graphs.&lt;/item&gt;
      &lt;item&gt;Ambient occlusion.&lt;/item&gt;
      &lt;item&gt;Finishing the game? (hopefully…)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Overall, I’m quite satisfied with what I managed to accomplish. Learning Vulkan was quite difficult, but it wasn’t as hard as I imagined. It taught me a lot about graphics programming and modern APIs and now I have a strong foundation to build my games with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46010329</guid><pubDate>Fri, 21 Nov 2025 23:28:40 +0000</pubDate></item><item><title>Sharper MRI scans may be on horizon thanks to new physics-based model</title><link>https://news.rice.edu/news/2025/sharper-mri-scans-may-be-horizon-thanks-new-physics-based-model</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46010806</guid><pubDate>Sat, 22 Nov 2025 00:30:26 +0000</pubDate></item><item><title>The death of tech idealism and rise of the homeless in Northern California</title><link>https://lithub.com/on-the-death-of-tech-idealism-and-rise-of-the-homeless-in-northern-california/</link><description>&lt;doc fingerprint="b60491d83ba3026b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;On the Death of Tech Idealism (and Rise of the Homeless) in Northern California&lt;/head&gt;&lt;head rend="h2"&gt;“It’s as though the city feels it’s been invaded by the unhoused. But turn San José inside out and it’s a giant homeless camp being invaded by a city.”&lt;/head&gt;&lt;p&gt;Fuckers. I couldn’t get the word out of my head, because he wouldn’t stop saying it. I was sitting in the tiled courtyard of the Mediterranean-style home of an old acquaintance, a venture capitalist and serial tech entrepreneur, who lived a few blocks from Zuckerberg in Palo Alto. Next to us was a massive stone slab over which water dribbled into a reflecting pool. A Buddha sat on the stone, contemplating the flow. Above us sprawled the canopy of a century-old olive tree, which had been raining its fruit onto the courtyard.&lt;/p&gt;&lt;p&gt;It would have been an idyllic scene, were it not for the presence of my acquaintance, who kept smacking and berating his dog, a puffy, pure-white Alaskan-looking thing, who wouldn’t stop eating the olives.&lt;/p&gt;&lt;p&gt;In my previous career, I was a landscape designer, and this person was my client. I’d lived in Santa Cruz then, a hippie-surfer town about an hour away on the other side of the mountains that separate the Valley from the ocean. I was not alone in commuting over those mountains—many of Santa Cruz’s hippies and surfers make the trek to stick their straws where the wealth is. I went to college at the University of California in Santa Cruz—home of the fighting Banana Slugs!—and spent the entirety of my twenties there.&lt;/p&gt;&lt;p&gt;When I drove over after arriving in Cupertino, however, a camp lined the main road into town; hundreds of unhoused residents inhabited another area along the river.&lt;/p&gt;&lt;p&gt;When thirty approached, I began to think of things like owning a home, which even an hour from the Valley’s gravitational center was out of reach with my income at the time. So a few months after the Great Recession hit, I moved back to Georgia, where I’d grown up. I bought a house on seven acres for $90,000.&lt;/p&gt;&lt;p&gt;I’d been away from California for twelve years. Much had changed. The real estate costs I’d fled had tripled; 2008 prices now seem quaintly affordable. I don’t remember ever seeing a tent on the streets of Santa Cruz back then. It was known as a place with a lot of panhandlers and drug users, but not so many that they made their dwellings in places obvious to the casual observer. When I drove over after arriving in Cupertino, however, a camp lined the main road into town; hundreds of unhoused residents inhabited another area along the river.&lt;/p&gt;&lt;p&gt;My client had also changed. I remembered him as a charming, progressive guy, but he’d grown older, crankier, and more libertarian in the decade since I last saw him. Apparently he’d become fond of calling people fuckers, and when I broached the topic of homelessness, he erupted in a lava flow. Employees of NGOs who concoct idealistic plans to address the housing crisis? Fuckers. Activists who valiantly defend the less fortunate among us? Fuckers. He couldn’t stand to go to San Francisco anymore because of the hordes sleeping and shitting right there on the sidewalk, in front of businesses run by people who actually contribute to the economy.&lt;/p&gt;&lt;p&gt;“If we can figure out how to get a package from China to your doorstep in two days, we can figure this out,” he said. Whether it’s houses made of shipping containers or building artificial islands in the Bay to house the homeless, he assured me that “innovators” like himself could come up with a solution—if only the incompetent, na.ve, and corrupt fuckers in the public sector would get out of the way.&lt;/p&gt;&lt;p&gt;In fact, he would personally love to dig his entrepreneurial hands into the issue. But the poverty space was dominated by inefficient nonprofits and he wouldn’t waste his time consorting with them—the profit motive is what drives efficiency, after all, and efficiency paves the way to viable solutions. Nonprofits are in the business of self-congratulation, not getting things done, he said. His evidence: They hadn’t fixed the problem yet. “It’s like a car or your phone,” he said. “Either it works or it doesn’t.”&lt;/p&gt;&lt;p&gt;The last time I’d seen my client he was plotting his first trip to Burning Man. He’d shown me some of his paintings and we’d chatted about organic farming and his time in a kibbutz. Though he worked sixteen hours a day (he claimed to sleep no more than a few hours a night), he nevertheless found time to feed his omnivorous intellect, which snacked on cybernetics and chewed on transcendentalism after dinner. He was the archetypal boomer tech entrepreneur, kissed by the antiestablishment, but in the business of re-establishing the establishment in his own image.&lt;/p&gt;&lt;p&gt;The Valley overlaps geographically with the hippie homeland of San Francisco, Berkeley, and their environs, and there’s long been cross-pollination, if not orgiastic copulation, between the two spheres. As a barefoot college dropout in the early seventies, Steve Jobs’s interests included Ram Dass, Hare Krishnas, and fruitarianism; his connection to apples stemmed from a stint at the All One Farm, a commune where he worked in a Gravenstein orchard.&lt;/p&gt;&lt;p&gt;The commune fell apart as residents realized they were being conned by the spiritual leader, a close friend of Jobs, into providing free labor for his apple cider business. The apple cider guru later became a billionaire mining magnate notorious for labor and environmental abuses. Jobs, however, sought to bring his spiritual values with him in founding a company to disseminate what he considered the ultimate tool of enlightenment—the personal computer—to the masses.&lt;/p&gt;&lt;p&gt;This trajectory is such a prominent feature among the Valley’s founding fathers that it has spawned a minor field of academic study. “To pursue the development of individualized, interactive computing technology was to pursue the New Communalist dream of social change,” writes Fred Turner, a Stanford historian, in his book From Counterculture to Cyberculture: Stewart Brand, the Whole Earth Network, and the Rise of Digital Utopianism.&lt;/p&gt;&lt;p&gt;Turner’s book focuses on Stewart Brand, a Merry Prankster turned evangelist of enlightened capitalism, who once roamed from commune to commune selling back-to-the-land supplies out of his 1963 Dodge truck. The Whole Earth Truck Store, as he called it, morphed into the Whole Earth Catalog magazine, which begat the Whole Earth Software Catalog, which begat Wired magazine.&lt;/p&gt;&lt;p&gt;Unhoused communities don’t randomly burble up from the sidewalk. They are born of the housed communities around them, which in the Valley’s case is a particularly curious one.&lt;/p&gt;&lt;p&gt;Brand, writes Turner, “brokered a long-running encounter between San Francisco flower power and the emerging technological hub of Silicon Valley,” in which “counterculturalists and technologists alike joined together to reimagine computers as tools for personal liberation, the building of virtual and decidedly alternative communities, and the exploration of bold new social frontiers.”&lt;/p&gt;&lt;p&gt;One can imagine a young Steve Jobs digging the communalism of today’s Bay Area camps, whose countercultural idealism shares many threads with that of the Valley’s early hippie-nerds—ironic given the bulldozing of camps in the shadows of contemporary tech campuses and their tightly conformist corporate cultures. The commonalities don’t stretch very far: A rather thick thread in the hippie-techie braid is individualism, a whole lot of which hid behind the Me generation’s “New Communalist” movement. The marriage of these Bay Area cultures is alive and well, but today has more of a New Age–Burning Man vibe.&lt;/p&gt;&lt;p&gt;Brand, now in his eighties, is an ardent Burner. He’s gone from libertarian to the even-harder-to-define “post-libertarian” and is building a five-hundred-foot clock inside a Texas mountain owned by Jeff Bezos, which is designed to tick once per year for ten thousand years. A cuckoo will emerge at the end of each millennium.&lt;/p&gt;&lt;p&gt;Brand shares a certain intellectual hubris with my acquaintance, who asked if I would like to read an eighty-two-page white paper he wrote regarding the human brain and why Darwin’s survival-of-the-fittest theory applies not just to biological evolution, but to the optimization of social structure. I stared at the Buddha and tried to think of a way to change the subject.&lt;/p&gt;&lt;p&gt;Unhoused communities don’t randomly burble up from the sidewalk. They are born of the housed communities around them, which in the Valley’s case is a particularly curious one. The Valley’s valley is wide and smoggy enough that some days you can’t see the mountain ranges that form it. The scorching Diablo Range, where cattle roam oceans of desiccated grass, lies to the east.&lt;/p&gt;&lt;p&gt;On the other side, the lusher Santa Cruz Mountains, a place of dank redwood forests, organic farming communes, and uppity vineyards, form a verdant curtain between the Valley and the ocean. Here the tech elite build their villas and take to the fog-kissed ravines for athleisure-clad recreation.&lt;/p&gt;&lt;p&gt;The valley started to become the Valley in 1943 when IBM opened a factory to manufacture punch cards in San José. At the time, orchards carpeted much of the region. When the trees blossomed in early spring, the honey-scented flowers intoxicated bees and lovers alike. During the late summer harvest, the air was a punch bowl. Maps referred to it then as the Santa Clara Valley, but romantic minds of the day christened it the Valley of Heart’s Delight, after a 1927 poem by a local writer with Wordsworthian sensibilities, named Clara Louise Lawrence.&lt;/p&gt;&lt;p&gt;No brush can paint the picture&lt;lb/&gt; No pen describe the sight&lt;lb/&gt; That one can find in April&lt;lb/&gt; In “The Valley of Heart’s Delight.”&lt;/p&gt;&lt;p&gt;Cupertino did not exist back then. The Glendenning family farmed the land where the Apple Spaceship now sits. Prunes were their specialty. The farm was on Pruneridge Avenue—the valley was considered the prune capital of the world, supplying 30 percent of the global market—which passed through their orchards near the present location of Steve Jobs Theater, a smaller circular building next to the mothership.&lt;/p&gt;&lt;p&gt;But Apple bought the road from the city—$23,814,257 for a half mile—so you can’t drive through there anymore. Between the steel bars of the fence you can still catch a glimpse of the Glendennings’ old fruit-drying barn, which has been renovated and is now storage for landscaping equipment. The new orchards and the old barn help soften the Pentagon vibe with a little farm-to-table ambience.&lt;/p&gt;&lt;p&gt;The Valley’s valley is not a stereotypical one because it lacks a mighty river meandering between the mountain ranges. Instead, there is the southern leg of San Francisco Bay, a shallow, brackish estuary fed by measly creeks that barely run in the dry season. It’s a bird and crustacean paradise, but the lack of fresh water and ocean currents make for a putrid aroma that’s further intensified by the landfills, wastewater treatment plants, and commercial salt-harvesting operations clustered around the waterfront.&lt;/p&gt;&lt;p&gt;The smell is so intense that it’s spawned a South Bay Odor Stakeholders Group “dedicated to identifying and resolving odor issues.” One finds Reddit threads with titles like South Bay Fucking Smell: “south bay people, you know what i mean. where the fuck is this rancid ass smell coming from. it’s pretty common for it to smell like shit here, i’ve smelled it my whole life, but i just want to know where it’s comin from. My guess is the shitty salty shallow south bay water spewing out smelly air, but idk.”&lt;/p&gt;&lt;p&gt;“That, or else it’s your mom,” replied another user, who referred to the odor as “the ass cloud.” The poetics of the region have shifted since Lawrence’s day.&lt;/p&gt;&lt;p&gt;The military forefathers of the Valley must have been horrified at the hippies their children became, though by the eighties the arc of flower power had bent toward the common ground of Wall Street.&lt;/p&gt;&lt;p&gt;The ass cloud did not dissuade the early tech settlers, who followed the money flowing from the patron saint of the Valley’s venture capitalists: DARPA, the Department of Defense’s secretive research agency, which commissioned much of the basic science from which the IT revolution sprang. While farms like the Glendennings’ continued to pump out prunes on the arable land between the Bay and the mountains, the military-industrial complex set up along the mud flats. The Navy built an eight-acre dirigible hangar in Mountain View, still one of the largest freestanding structures ever erected. The CIA quietly rooted itself among the reeds and spread rhizomatically. During the Cold War, aerospace companies blossomed between DOD installations. Lockheed was the Valley’s biggest employer when Kent and Steve Jobs were growing up in the suburbs that slowly consumed the orchards.&lt;/p&gt;&lt;p&gt;The American tech industry was born in the Bay Area because its defense industry parents came here to ward off the Japanese—during World War II, this was the gateway to the “Pacific Theater,” as the Asian front of the war was euphemistically referred to. This first generation of the Valley “seeded companies that repurposed technologies built for war to everyday life,” writes Margaret O’Mara, a tech industry historian. “Today’s tech giants all contain some defense-industry DNA.”&lt;/p&gt;&lt;p&gt;Jeff Bezos’s grandfather, for instance, was a high-ranking official at the US Atomic Energy Commission and at ARPA, the precursor to DARPA. Jerry Wozniak, father of Apple’s other Steve—Steve “The Woz” Wozniak, the company cofounder and part of the gang tweaking on computers in the Jobs’ garage—was an engineer at Lockheed. The military forefathers of the Valley must have been horrified at the hippies their children became, though by the eighties the arc of flower power had bent toward the common ground of Wall Street.&lt;/p&gt;&lt;p&gt;The Navy’s dirigible hangar still looms over the Bay, but Google now rents the property from the government for the parking of private jets. The company dominates the neighborhood to the west of the hangar, a spread of dull office buildings revolving around the central Googleplex, with its employee swimming pools, volleyball courts, and eighteen cafeterias. There are no houses or apartments in the neighborhood, though there are residential districts—of a sort. These are surprisingly affordable, which means that some of the folks who smear avocado on the techies’ toast and stock the kombucha taps have the good fortune to live nearby.&lt;/p&gt;&lt;p&gt;It’s easy to miss their humble abodes, however. An out-of-towner who gets off at the Google exit to take a leak could be forgiven for thinking they’d stumbled across some sort of RV convention. But those aren’t recreational vehicles lining the backstreets of the Google-burbs—those are homes on wheels.&lt;/p&gt;&lt;p&gt;RVs parked on the side of the road are the new desirable real estate, and like the old industrial cores of American cities that have evolved from roughshod hangouts for unemployed artists to haute loft developments for upwardly mobile professionals, their inhabitants aren’t immune to class stratification. Most of the rigs are older, ramshackle models, but here and there shiny coaches broadcast the relative wealth of their inhabitants—techies who could afford an apartment but don’t want to waste their money on rent.&lt;/p&gt;&lt;p&gt;They roll out of bed, hop on a company bike, and are at the office in three minutes, in the meantime saving up for a big house in the outer, outer, outer burbs, where you can still get a McMansion for under $3 million. Some already have the McMansion and use their RV as a workweek crash pad.&lt;/p&gt;&lt;p&gt;The more-rickety RVs belong to the avocado smearers and lawn mower operators. Crisanto Avenue, five minutes from the Googleplex, is the Latin America of Mountain View’s homes-on-wheels community. It’s like a museum of 1980s RVs—Toyota Escapers, Winnebago Braves, Chevy Lindys, Fleetwood Jamborees—most of them emanating Spanish banter, many with blue tarps over the roof, and some leaking unmentionable juices from onboard septic tanks. Apartments line one side of Crisanto, but the side with the RVs fronts onto train tracks. A shaded strip of earth along the tracks, maybe twelve feet wide, serves as a communal front yard, complete with potted plants and patio furniture, for pets and kids to play.&lt;/p&gt;&lt;p&gt;She’d held America in high esteem before living here. “La vida en los Estados Unidos es terrible,” she said.&lt;/p&gt;&lt;p&gt;An older Peruvian woman named Ida invited me into her RV, where a half-eaten pineapple sat serenely on an otherwise empty table. She used to live in a two-bedroom apartment with sixteen other people—“Fue imposible!” she said—until she learned of the RV scene. She couldn’t afford to purchase one, but there’s a growing industry in the Valley for old-school RV rentals; residents on Crisanto told me they pay between $500 and $1,000 per month, depending on the RV, plus a $75 fee to pump sewage.&lt;/p&gt;&lt;p&gt;Since Ida arrived in the US in 2003, she has worked mainly as a nanny, often for around six dollars per hour. Work was sparse during the pandemic, so she accepted whatever pay she was offered. One family gave her twenty dollars for taking care of their two children for twelve hours. She’d held America in high esteem before living here. “La vida en los Estados Unidos es terrible,” she said.&lt;/p&gt;&lt;p&gt;My visual experience of the Valley began to shift. My eyes had once flashed at views of the water, clever billboards (“Hey Facebook, our planet doesn’t like your climate posts”), and homes with the billowy, buff-colored grasses and scrawny wildflowers that signify the aesthetics of people who can afford expensive landscaping designed to look feral.&lt;/p&gt;&lt;p&gt;But the more time I spent with the Valley’s have-nots, the more my focus became trained on the visual language of the income inequality ecosystem: the camouflage patterns of desiccated vegetation pocked with blue tarps and plastic bags flapping in the branches; the hulking silhouettes of recreational vehicles parked in non-recreational environments; the bodies splayed out on the sidewalk.&lt;/p&gt;&lt;p&gt;Here and there, artistic aberrations emerge in the motif. I met a thirty-year old man named Ariginal who lived with his family and dogs in a 1983 Chevy camper van that he’d hand-painted marine blue with school-bus-yellow trim. A blue neon light mounted to the undercarriage illuminated the street in a cool glow as they motored around in their Scooby-Doo mobile at night. Ariginal went to school to be a fireman but became an Uber driver. He’s also a rapper, fashion model, and inventor—there are a few things he’s hoping to patent, and he wanted to show me the drawings, but his daughter was napping in the van. “I have a lot of dreams,” he said.&lt;/p&gt;&lt;p&gt;Within twelve minutes of meeting Ariginal I learned that he recently “discovered a couple of lumps . . . uh, in my testicles.” They were cancerous. He’d just had the tumors removed and would soon be undergoing radiation to make sure they don’t come back. “Just another obstacle,” he sighed.&lt;/p&gt;&lt;p&gt;“Vanlife has become the norm here,” a veteran gig worker named Chase, who’s driven for Uber, Instacart, and Amazon Flex, told me. He was not talking about hipsters who move into a home on wheels because it sounds like a fun and Instagrammable lifestyle. He was referring to his colleagues who have no other choice.&lt;/p&gt;&lt;p&gt;I found there is significant overlap between the gig work community and the unhoused community. Some full-time gig workers end up living in their vehicles; some camp residents become part-time gig workers because it’s a way to make a buck that doesn’t require a home address or the scrutiny of a human boss, only a working phone. Rudy, for instance, began delivering for food apps—using Lime scooters he rents by the hour—after he became homeless.&lt;/p&gt;&lt;p&gt;Their camps keep getting razed, but like the marshland reeds, they sprout right back.&lt;/p&gt;&lt;p&gt;The mobile communities stretch along the Bay up to Meta’s headquarters at 1 Hacker Way, on the outskirts of Palo Alto. East Palo Alto, the historically Black community surrounding the Meta campus, is one of the least gentrified, most impoverished places in the Valley—a 2017 study found that 42 percent of students in the local school district were homeless. A sixty-acre nature preserve across the street from the Meta campus is home to endangered species such as the salt marsh harvest mouse and Ridgway’s rail, a chicken-sized bird with a long, pointy beak and special glands that allow it to drink salt water.&lt;/p&gt;&lt;p&gt;A local variety of Homo sapiens lives there too, who are endangered in a different sort of way. The authorities want them out because their “presence is compromising the health of the estuary,” according to Palo Alto Weekly. Their poop and trash are considered pollution under the federal Clean Water Act—grounds for eviction. “The welfare of wildlife and the health of Baylands ecosystems is pitted against the very real human needs of people,” the paper opined. Their camps keep getting razed, but like the marshland reeds, they sprout right back.&lt;/p&gt;&lt;p&gt;Different regions of the Valley lend themselves to different expressions of homelessness. In the suburban areas, there are lots of vehicle dwellers because it’s (relatively) easy to find a place to park. In densely developed San Francisco, homelessness is largely centered along sidewalks, pushing the lives of unhoused individuals up close and personal, but keeping camps small and dispersed. Golden Gate Park is a would-be homeless haven, but local authorities have managed to keep camps from proliferating.&lt;/p&gt;&lt;p&gt;In San José, however, local green spaces have been commandeered by the unhoused, with camps that have developed into villages, especially along the Guadalupe River, where the Crash Zone was located, and its tributaries.&lt;/p&gt;&lt;p&gt;San José’s waterways are hideously un-scenic—views are of rubble and trash; the vegetation appears to be in a fight for its life. And although the Guadalupe is called a river, it’s more like a creek that bulges into a torrent on the rare occasion of a multiday rainstorm. Its abused hydrology forms the armature of a shadow city—a new form of urban infrastructure that is unplanned and unwelcome—within a city that does not acknowledge its shadow.&lt;/p&gt;&lt;p&gt;At a community meeting in 2017 to solicit input on a homeless shelter the city wanted to build, a horde of angry residents expressed their discontent over efforts to accommodate the unhoused: “Build a wall,” they chanted.&lt;/p&gt;&lt;p&gt;The Guadalupe River and its camps pierce downtown San José, meandering past the Zoom campus and Adobe’s four-tower headquarters to the Children’s Discovery Museum on Woz Way (as in Apple’s Wozniak, the Apple cofounder), where a community of Latinx campers have chiseled terraced gardens into the riverbank to grow food. People call it the Shelves.&lt;/p&gt;&lt;p&gt;Downstream from the Crash Zone, the Guadalupe flows north along the edge of the airport on its way to the Bay, passing dozens of camps and acres of office parks populated by household names—PayPal, Google, Hewlett-Packard, Roku, Cisco, Intel. One of the biggest camps in this part of town emerged on vacant land owned by Apple, just north of the airport, where the company plans to build yet another campus. Located on Component Drive, it was known as Component Camp.&lt;/p&gt;&lt;p&gt;The Jungle, they said, was “a crime syndicate ruled by gangs, where police do not enter.”&lt;/p&gt;&lt;p&gt;As word spread that displaced Crash Zone residents might soon inundate the place, the company announced they would spend “several million dollars” on a high-end sweep—evicted residents were given vouchers for nine months in a hotel—just weeks before the Crash Zone sweep began.&lt;/p&gt;&lt;p&gt;The Guadalupe’s tributaries tell further stories. Los Gatos Creek leads to the headquarters of eBay and Netflix, as well as Downtown West, a neighborhood being built from the ground up by Google. The city approved a development proposal that included office space for twenty-five thousand folks, but only four thousand units of housing—the sort of job-supply-to-housing-demand ratio that helps put a $3 million sticker on a bungalow.&lt;/p&gt;&lt;p&gt;A report by an economic research firm found that the project’s job-to-housing imbalance would translate to a $765-per month increase for San José renters over a decade—to offset upward pressure on rents, they said more than four times as many units would be needed, a third of them at subsidized rates. The San José Chamber of Commerce declared the report “fundamentally flawed” and dismissed findings like the $765 rent increase. “I don’t think that the stark reality presented in the report is realistic,” a representative told San José Spotlight, “nor something we can expect to happen in the next 8 to 10 years.” In the previous decade, however, median rent in the area had gone up by exactly $763.&lt;/p&gt;&lt;p&gt;Coyote Creek is home to a camp called the Jungle—I never deduced whether this was a reference to hobo jungles or the dense vegetation, or both—on which national media descended in 2014 as it was being swept. It was similar to the Crash Zone in scale, and headlines touting it as the “nation’s largest homeless camp” became a mantra. It was a feast of poverty porn.&lt;/p&gt;&lt;p&gt;“Living in The Jungle means learning to live in fear,” said The Atlantic, quoting a resident who displayed “a machete that he carries up his sleeve at night.” For the UK’s Daily Mail, it was an opportunity to get Dickensian. “Dilapidated, muddy and squalid though it was, it was all they had to call home—a shantytown in the heart of Silicon Valley,” the reporter lamented. “In the last month, one resident tried to strangle another with a cord of wire and another was nearly beaten to death with a hammer.” The Jungle, they said, was “a crime syndicate ruled by gangs, where police do not enter.”&lt;/p&gt;&lt;p&gt;The New York Times was more restrained, striking a valiant tone, with a photo of the mayor himself helping a resident to “wheel his belongings up a muddy embankment.” The local CBS station reported that displaced residents immediately formed a “New Jungle” a half mile away. Before long, they recolonized the original Jungle.&lt;/p&gt;&lt;p&gt;The Crash Zone had grown to the size of the original Jungle, if not larger, by the time I first visited. The fields outside the airport fence technically belong to a city park, but when driving by they appeared like a cross between a junkyard and a refugee camp, in which explosives were periodically detonated. RVs in various states of disrepair butted up to tarp compounds that overflowed with debris, from bottles and cans to appliances and vehicles.&lt;/p&gt;&lt;p&gt;This visual buffet was a mix of freshly prepared, putrefied, and charred—one resident had a tidily landscaped yard with a pair of pink plastic flamingos, while other homesteads were a mix of rotting garbage, blackened grass, melted tarps, and burnt-out vehicles. My eyes flowed over suitcases and furniture and school buses to unexpected items, such as a piano, several boats, and a limousine. The first residents I met cautioned me against wandering into certain sections, where I might be mistaken for an undercover cop—the confetti labyrinth of structures left many a hidden nook where bad things might happen.&lt;/p&gt;&lt;p&gt;One guy had cobbled together a two-story wood cabin; around it were huge piles of wood chips and logs. I wanted to knock but was told the owner wielded an axe and did not like visitors. They called him the Woodchucker.&lt;/p&gt;&lt;p&gt;It was midsummer when I first visited the Crash Zone, height of the dry season in California. Large portions of the landscape were barren earth, and the powder-dry soil coated the skin of its residents. Here and there, people sifted through the loose dirt with their hands; occasionally someone held up a small trinket they’d discovered, inspecting it in the harsh light of the sun.&lt;/p&gt;&lt;p&gt;A woman walked by with a mixing bowl and a toy unicorn, stooping to extract a scrap of blue tarp from the earth, before she continued on. A minimally dressed man pulled clothes from a dumpster and tried them on, not necessarily in the way they were designed to be worn, and quickly took them off again. He spoke incomprehensibly to himself as he did this, tsking and looking annoyed, as though he just couldn’t find the outfit he was looking for. He was thin, barefoot; I wondered how he stayed alive.&lt;/p&gt;&lt;p&gt;I saw a man thrashing his body in anger as he crossed the street. A dreadlocked white guy in a hoodie wandered by with a baseball bat in one hand and a small, sweet-looking dog in the other. The wind picked up; a dust devil spun. A car crawled out of one of the fields with a flat tire, its rim scraping the asphalt as it entered the street. Every five minutes or so, a plane roared overhead like an angry avian dinosaur.&lt;/p&gt;&lt;p&gt;The Crash Zone spilled from its gills, extending beyond the end-of-the-runway fields and into the surrounding cityscape. One end spilled into a series of baseball diamonds, the dugout now housing, the clubhouse a drug den, the bathrooms given over to the sex trade—“People pull through in $100,000 cars trolling for people to blow them in the car,” a resident told me.&lt;/p&gt;&lt;p&gt;On an adjacent street, a man on crutches lived on a bench next to what was once a demonstration garden for drought-tolerant plants, according to a small sign, which had devolved into a garden of rocks and bare earth. The street proceeds across a large bridge where a solitary tent occupied one of the little nooks designed as a place for pedestrians to linger and look out over the Guadalupe. The bike and pedestrian paths that weave through the riparian corridor below provided access to a neighborhood of tents and shacks, a leafy suburb of the Crash Zone known as the Enchanted Forest. Its inhabitants pulled their cars over the curb, using the paths as driveways. Joggers and cyclists and parents pushing strollers paraded through nonetheless.&lt;/p&gt;As San José’s camps have spread, the Guadalupe River parklands have become the frontlines of a local culture war.&lt;p&gt;The tents flowed along the river to the San José Heritage Rose Garden, where thousands of rare and forgotten varieties have been arranged in concentric rings of paths and beds. Some of those varieties disappeared following the Crash Zone’s pandemic-era population explosion, when incidents of arson and “rose rustling”—horticultural theft—were reported by garden volunteers on the site’s Facebook page, the insinuation of who was responsible clearly legible between the lines of the posts. The tents trickled past the roses and collected in clumps along the edges of the Historic Orchard, whose few remaining trees appear murdered and maimed, where they bumped into the six-foot fence that protects the children at the Rotary PlayGarden, a gift to the city from the local Rotary Club. A gate attendant keeps the you-know-who from wandering in to the $6 million playscape.&lt;/p&gt;&lt;p&gt;As San José’s camps have spread, the Guadalupe River parklands have become the frontlines of a local culture war. “The city’s homeless problem is becoming a PR problem,” a CBS anchor said in a 2019 segment. “From their airplane windows, arriving visitors are greeted by a shantytown of tents, blue tarps, and RVs,” they said, describing the trail network that parallels the river as “an eleven-mile stretch of human misery and suffering.”&lt;/p&gt;&lt;p&gt;The campers swim in the animosity that drenches the airwaves and cyberspaces around them. I wondered how my new friends on the street felt when they heard these things. How much does the angst directed toward them undermine their prospects of recovery? I found myself reading the Tripadvisor reviews of Guadalupe River Park, which encompasses much of the trail system. They felt like a beating.&lt;/p&gt;&lt;p&gt;“The once beautiful walking, running and biking trail has been taken over by homeless, garbage, rats,” wrote Robin G. “Really bad,” wrote hob0525. “It was basically . . . a tour of homeless camps. We walked for over an hour thinking it would get better. . . . It did not.”&lt;/p&gt;&lt;p&gt;In a 2019 survey by the Guadalupe River Park Conservancy, 77 percent of respondents did not feel “welcome and safe” in the park. “It’s something that I’ve never seen before, honestly,” Jason Su, the conservancy’s director, told The Mercury News. “This is just on a scale that’s just so, so large.”&lt;/p&gt;&lt;p&gt;It’s as though the city feels it’s been invaded by the unhoused. But turn San José inside out and it’s a giant homeless camp being invaded by a city.&lt;/p&gt;&lt;p&gt;__________________________________&lt;/p&gt;&lt;p&gt;From Front Street: Resistance and Rebirth in the Tent Cities of Techlandia by Brian Barth. Used with the permission of the publisher, Astra Publishing House. Copyright © 2025 by Brian Barth.&lt;/p&gt;&lt;head rend="h4"&gt;Brian Barth&lt;/head&gt;&lt;p&gt;Brian Barth is an award-winning independent journalist with bylines in the New Yorker, National Geographic, Washington Post, The New Republic and Mother Jones, among other publications. He lives between the Bay Area and California's remote Lost Coast region, where he is developing a spiritual refuge—open to seekers, broken souls, and all of humankind—amid a foggy, fern-filled forest. Front Street is his first book.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46011521</guid><pubDate>Sat, 22 Nov 2025 02:28:28 +0000</pubDate></item><item><title>Microsoft Will Preload Windows 11 File Explorer to Fix Bad Performance</title><link>https://blogs.windows.com/windows-insider/2025/11/21/announcing-windows-11-insider-preview-build-26100-7271-dev-beta-channels/</link><description>&lt;doc fingerprint="358ecd733633e5df"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing Windows 11 Insider Preview Build 26220.7271 (Dev &amp;amp; Beta Channels)&lt;/head&gt;
    &lt;p&gt;Hello Windows Insiders, today we are releasing Windows 11 Insider Preview Build 26220.7271 (KB5070307) to the Dev &amp;amp; Beta Channels.&lt;/p&gt;
    &lt;p&gt;As a reminder we are offering the same builds to both the Dev &amp;amp; Beta Channels on Windows 11, version 25H2.&lt;/p&gt;
    &lt;p&gt;If you are an Insider in the Dev Channel, you now have a window to switch from the Dev Channel to the Beta Channel if you would like. This window will only be open for as long as we’re releasing the same 25H2-based updates across both the Dev and Beta Channels. After we move Dev Channel forward to a higher build number, the opportunity to switch between these channels will close. When the Dev Channel jumps ahead, things might not be as stable as the Dev Channel is today, so we highly encourage you to evaluate which channel you would like to be in during the time in which the window to switch is open.&lt;/p&gt;
    &lt;p&gt;Changes in Dev &amp;amp; Beta Channel builds and updates are documented in two buckets: new features, improvements, and fixes that are being gradually rolled out for Insiders who have turned on the toggle to get the latest updates as they are available (via Settings &amp;gt; Windows Update*) and then new features, improvements, and fixes rolling out to everyone in the Dev &amp;amp; Beta Channels. For more information, see the Reminders section at the bottom of this blog post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing the Xbox full screen experience for PC&lt;/head&gt;
    &lt;p&gt;Alongside today’s general availability of the Xbox full screen experience (FSE) on more Windows handhelds, today’s preview to Windows Insiders also expands availability of FSE to additional Windows 11 PC form factors.&lt;/p&gt;
    &lt;p&gt;Designed with console-style navigation in mind, the Xbox full screen experience delivers a clean, distraction-free interface for controller-first gaming. Pair a controller to your PC for smooth task switching and a streamlined gaming experience on your desktop, laptop, or tablet.&lt;/p&gt;
    &lt;p&gt;How to enter Xbox full screen experience:&lt;/p&gt;
    &lt;p&gt;You can access Xbox full screen experience from Task View, Game Bar settings, or use Win + F11 hotkey to toggle FSE.&lt;/p&gt;
    &lt;p&gt;The Xbox full screen experience begins as a gradual rollout to Windows Insiders on the Dev &amp;amp; Beta Channels, who are also registered Xbox Insiders. We expect to expand this later to all Insiders on the Dev &amp;amp; Beta Channels without requiring Xbox program registration. If you want to be among the first to try out these new features on your PC, join the Xbox Insiders Program and opt into the PC gaming preview through the Xbox Insiders Hub.&lt;/p&gt;
    &lt;p&gt;For more information about the Xbox full screen experience, visit Full screen experience expands to more Windows 11 PC form factors.&lt;/p&gt;
    &lt;p&gt;Feedback: Share your thoughts in Feedback Hub (WIN + F) under Gaming and Xbox &amp;gt; Gaming Full Screen Experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;New features gradually being rolled out with toggle on*&lt;/head&gt;
    &lt;head rend="h3"&gt;Point-in-time restore for Windows&lt;/head&gt;
    &lt;p&gt;We’re excited to introduce point-in-time restore for Windows, now available to Insiders in the Beta and Dev Channels! This flexible recovery feature empowers you to quickly roll your device back to a previous state—helping minimize downtime and simplify troubleshooting when disruptions strike. Whether you’re dealing with a widespread outage or a one-off issue, point-in-time restore helps recover your system (including apps, settings, and user files) to get you back to productivity faster. For more details, check out our documentation.&lt;/p&gt;
    &lt;p&gt;Feedback: Share your thoughts in Feedback Hub (WIN + F) under Recovery and Uninstall &amp;gt; Point-in-time restore.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introducing Fluid Dictation in Voice Typing&lt;/head&gt;
    &lt;p&gt;Following the introduction of Fluid dictation for voice access users, we’re also now introducing it for voice typing users on NPU devices. Fluid dictation makes voice-based dictation smoother and smarter by automatically correcting grammar, punctuation, and filler words as you speak, reducing the need for manual editing. Powered by on-device small language models (SLMs), it ensures fast and private processing.&lt;/p&gt;
    &lt;p&gt;To use it, set focus to a text field and launch voice typing by pressing the Windows key plus H and complete setup if you’re a first-time user. Fluid Dictation is enabled by default—you can check or toggle it via the settings flyout—so all you need to do is start talking!&lt;/p&gt;
    &lt;p&gt;Feedback: Share your thoughts in Feedback Hub (WIN + F) under Input and Language &amp;gt; Voice Typing (Windows key plus H).&lt;/p&gt;
    &lt;head rend="h2"&gt;Changes and Improvements gradually being rolled out with toggle on*&lt;/head&gt;
    &lt;head rend="h3"&gt;[Seamlessly resume more apps from your Android phone to your PC]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Following the ability to resume Spotify tracks from your phone onto your PC, we’re excited to share that: &lt;list rend="ul"&gt;&lt;item&gt;vivo Android phone users can also now continue your browsing activity from vivo Browser on your phone, onto your default browser on your PC.&lt;/item&gt;&lt;item&gt;Honor, Huawei, Oppo, Samsung and vivo Android phone users can also now continue online files opened on M365 Copilot app from your phone onto your PC. Word, Excel, and PowerPoint files will open in the respective app on your PC if you have it installed, or if you don’t they’ll open in the default browser on your PC. Note – offline files (stored locally on the phone) are not currently supported.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FEEDBACK: Please file feedback in Feedback Hub (WIN + F) under Devices and Drivers &amp;gt; Linked Phone.&lt;/p&gt;
    &lt;head rend="h3"&gt;[Click to Do]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’re testing and refining the Click-to-Do top bar to determine the best experience for future updates. Functionality will vary by device and market.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[File Explorer]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’re making a few refinements to the context menu aimed at reducing the space taken by less commonly used actions, while keeping them easy to access. We’ve also updated the ordering of actions to group similar tasks. This includes: &lt;list rend="ul"&gt;&lt;item&gt;We’ve moved Compress to ZIP file, Copy as Path, Set as Desktop Background, and Rotate Right, and Rotate Left into a new Manage file flyout.&lt;/item&gt;&lt;item&gt;We’ve moved cloud provider options, like Always Keep on this Device and Free Up Space, into their relevant cloud provider flyout.&lt;/item&gt;&lt;item&gt;We’ve moved Send to My Phone next to the cloud provider options.&lt;/item&gt;&lt;item&gt;We’ve moved Open Folder Location to now be next to Open and Open with.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note, the name Manage file may change in a future Insider update. If you have feedback, please file it in the Feedback Hub under Desktop Environment &amp;gt; Right-Click Context Menu&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’re exploring preloading File Explorer in the background to help improve File Explorer launch performance. This shouldn’t be visible to you, outside of File Explorer hopefully launching faster when you need to use it. If you have the change, if needed there is an option you can uncheck to disable this called “Enable window preloading for faster launch times” in File Explorer’s Folder Options, under View. Looking forward to your feedback! If you do encounter any issues, please file them in the Feedback Hub under Files Folders and Online Storage &amp;gt; File Explorer Performance, or Files Folders and Online Storage &amp;gt; File Explorer.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[Microsoft Store]&lt;/head&gt;
    &lt;p&gt;Based on user feedback, we have added support for uninstalling Store-managed apps from the Store’s library page. Simply find an installed app in your library, click the three-dot menu, and click uninstall. Please let us know what you think!&lt;/p&gt;
    &lt;p&gt;Windows Insiders across all channels running Microsoft Store version 22510.1401.x.x and higher will see this improvement.&lt;/p&gt;
    &lt;p&gt;FEEDBACK: Please file feedback in Feedback Hub (WIN + F) under Microsoft Store.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixes gradually being rolled out with toggle on*&lt;/head&gt;
    &lt;head rend="h3"&gt;[Taskbar and System Tray]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixed an issue which could cause the taskbar to hang after receiving certain notifications.&lt;/item&gt;
      &lt;item&gt;Fixed an issue where the battery icon in the taskbar might unexpectedly show its own backplate when hovering over the icon in the system tray (instead of combined with wi-fi and volume).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[Internet]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Made some underlying improvements to help address an issue which could lead to not having internet after resuming from disconnected standby. Please don’t hesitate to file feedback under Network and Internet in the Feedback Hub if you continue experiencing issues.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[File Explorer]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixed an issue where if you opened the Recycle bin and had “Empty recycle bin” visible in the command bar, it might stay showing after you navigated away.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[Settings]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixed an issue where Settings might crash when navigating to Privacy &amp;amp; Security &amp;gt; Camera, Location, or Microphone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[Display and Graphics]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixed an issue where recently certain games might show a message saying “Unsupported graphics card detected”, although a supported graphics card was being used.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[Task Manager]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you’re using Die or CAMM memory form factor, Task Manager will now show that in Performance under Memory &amp;gt; Form Factor, instead of a blank.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[.NET Framework and Visual Studio]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The issue causing Insiders with ARM64 PCs to potentially observe crashes with Visual Studio or applications that depend on .NET Framework should be resolved if you have installed the latest .NET Framework update.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Known issues&lt;/head&gt;
    &lt;head rend="h3"&gt;[Xbox full screen experience for PC]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[NEW] The virtual keyboard is not shown for controller users on devices without a touch screen. Please use the physical keyboard as a workaround for now.&lt;/item&gt;
      &lt;item&gt;[NEW] Some apps may behave unexpectedly when using FSE, particularly those that expect to be locked to a given size or launch additional windows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[Taskbar &amp;amp; System Tray]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’re investigating an issue which is causing the Start menu to not open for some Insiders on click, although it will open if you press the Windows key. It’s believed this issue may also potentially impact the notification center (which you can open with WIN + N).&lt;/item&gt;
      &lt;item&gt;We’re investigating an issue where for some Insiders apps aren’t showing in the system tray when they should be.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[File Explorer]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scrollbar and footer are missing and showing a white block instead when text is scaled in dark mode version of the copy dialog.&lt;/item&gt;
      &lt;item&gt;[NEW] We’re investigating an issue where File Explorer has started showing a white flash when navigating between pages after the previous flight.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;[Bluetooth]&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[NEW] We’re investigating an issue causing Bluetooth device battery level to not show for some Insiders.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Reminders for Windows Insiders in the Dev &amp;amp; Beta Channels&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updates are based on Windows 11, version 25H2 via an enablement package (Build 26220.xxxx).&lt;/item&gt;
      &lt;item&gt;Many features are rolled out using Controlled Feature Rollout technology, starting with a subset of Insiders and ramping up over time as we monitor feedback to see how they land before pushing them out to everyone in this channel.&lt;/item&gt;
      &lt;item&gt;For Windows Insiders who want to be the first to get features gradually rolled out to you, you can turn ON the toggle to get the latest updates as they are available via Settings &amp;gt; Windows Update*. Over time, we will increase the rollouts of features to everyone with the toggle turned on. Should you keep this toggle off, new features will gradually be rolled out to your PC over time once they are ready.&lt;/item&gt;
      &lt;item&gt;Features and experiences included in these builds may never get released as we try out different concepts and get feedback. Features may change over time, be removed, or replaced and never get released beyond Windows Insiders. Some of these features and experiences could show up in future Windows releases when they’re ready.&lt;/item&gt;
      &lt;item&gt;Some features in active development we preview with Windows Insiders may not be fully localized and localization will happen over time as features are finalized. As you see issues with localization in your language, please report those issues to us via Feedback Hub.&lt;/item&gt;
      &lt;item&gt;Check out Flight Hub for a complete look at what build is in which Insider channel.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks,&lt;lb/&gt; Windows Insider Program Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46011569</guid><pubDate>Sat, 22 Nov 2025 02:38:42 +0000</pubDate></item></channel></rss>